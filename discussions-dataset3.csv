1888870,A,numpy : How to convert an array type quickly I find the astype() method of numpy arrays not very efficient. I have an array containing 3 million of Uint8 point. Multiplying it by a 3x3 matrix takes 2 second but converting the result from uint16 to uint8 takes another second. More precisely :  print time.clock() imgarray = np.dot(imgarray M)/255 print time.clock() imgarray = imgarray.clip(0 255) print time.clock() imgarray = imgarray.astype('B') print time.clock() dot product and scaling takes 2 sec clipping takes 200 msec type conversion takes 1 sec Given the time taken by the other operations I would expect astype to be faster. Is there a faster way to do type conversion or am I wrong when guesstimating that type conversion should not be that hard ? Edit : the goal is to save the final 8 bit array to a file Why do you need to go to uint16 and back again? Is it possible to have `M` as a uint8 matrix then you don't need the conversion. the result of the dot product will exceed the uint8 range. I originally was using a float M matrix and thought going to integer would give me some improvement but this is not true. What takes all that time probably is accessing all the memory locations. Sounds hard to fix. But clipping is also accessing all memory locations yet it is fast. Hopefully clipping does not need to modify a lot of locations. Similar operation done in C don't have this memory bandwith problem so I don't buy the memory acessing problem Interesting here it takes 0.2s 0.02s and 0.01s respectively for those three operations. Sure my machine seems to be faster than yours but the astype() operation certainly doesn't take anywhere near as long as the multiplication. When you use imgarray = imgarray.astype('B') you get a copy of the array cast to the specified type. This requires extra memory allocation even though you immediately flip imgarray to point to the newly allocated array. If you use imgarray.view('uint8') then you get a view of the array. This uses the same data except that it is interpreted as uint8 instead of imgarray.dtype. (np.dot returns a uint32 array so after the np.dot imgarray is of type uint32.) The problem with using view however is that a 32-bit integer becomes viewed as 4 8-bit integers and we only care about the value in the last 8-bits. So we need to skip to every 4th 8-bit integer. We can do that with slicing: imgarray.view('uint8')[:::4] IPython's %timeit command shows there is a significant speed up doing things this way: In [37]: %timeit imgarray2 = imgarray.astype('B') 10000 loops best of 3: 107 us per loop In [39]: %timeit imgarray3 = imgarray.view('B')[:::4] 100000 loops best of 3: 3.64 us per loop Can I save this view to a file @shodanex: Yes you could use np.save(). See http://docs.scipy.org/doc/numpy-1.3.x/reference/generated/numpy.save.html @shodanex: For other format options see also http://docs.scipy.org/doc/numpy-1.3.x/reference/routines.io.html it is then implicitly architecture-dependent since which slice to use depends on endianness. @kaizer.se: Yes that's true. Do you know a nice way to make the code non-architecture-dependent?,python numpy
1825857,A,"How much of NumPy and SciPy is in C? Python newbie here. Are parts of NumPy and/or SciPy programmed in C/C++? And how does the overhead of calling C from Python compare to the overhead of calling C from Java and/or C#? I'm just wondering if Python is a better option than Java or C# for scientific apps. If I look at the shootouts Python loses by a huge margin. But I guess this is because they don't use 3rd-party libraries in those benchmarks. The shootout python code (e.g. http://shootout.alioth.debian.org/u32/benchmark.php?test=regexdna&lang=python&id=1) does not use numpy/scipy. Don't forget about Fortran. Python plays nicely with Fortran too @~unutbu It's kind-of puzzling that you would expect the regex-dna program to use numpy. If you look closer you'll find an ""interesting alternative"" Python program that does use numpy http://shootout.alioth.debian.org/u32/benchmark.php?test=spectralnorm&lang=python&id=2 There is a better comparison here (not a benchmark but shows ways of speeding up Python). NumPy is mostly written in C. The main advantage of Python is that there are a number of ways of very easily extending your code with C (ctypes swigf2py) / C++ (boost.python weave.inline weave.blitz) / Fortran (f2py) - or even just by adding type annotations to Python so it can be processed to C (cython). I don't think there are many things comparably easy for C# or Java - at least that so seemlessly handle passing numerical arrays of different types (although I guess proponents would argue since they don't have the performance penalty of Python there is less need to).  A lot of it is written in C or fortran. You can re-write the hot loops in C (or use one of the gazillion ways to speed python up boost/weave is my favorite) but does it really matter? Your scientific app will be run once. The rest is just debugging and development and those can be much quicker on Python. ""Your scientific app will be run once. The rest is just debugging and development and those can be much quicker on Python."" -- Normally I'd agree. But this app could run for days or even weeks so cutting back just a little bit on processing time will save a lot of real time. It will be run more than once. really - you should jus ttry it: use Python Numeric from a Python interactuive console to create some matricesand make some operatins with them ""live"". -- It gives you an ease of use and flexibility that goes unsurpassed in other tools - which sppeds up any development as new ideas and usage patterns can be tried right away. The SciPy interactive prompt is oftenly used as an alternative to MatLab and other expensive (and somehow limited) scientific tools.  I would question any benchmark which doesn't show the source for each implementation (or did I miss something)? It's entirely possible that either or both of those solutions are coded badly which would result in an unfair appraisal of either or both language's performance. [Edit] Oops now I see the source. As others have pointed out though it's not using the NumPy/SciPy libraries so those benchmarks are not going to help you make a decision. I believe the vast majority of NumPy and SciPy is written in C and wrapped in Python for ease of use. It probably depends what you're doing in any of those languages as to how much overhead there is for a particular application. I've used Python for data processing and analysis for a couple of years now so I would say it's certainly fit for purpose. What are you trying to achieve at the end of the day? If you want a fast way to develop readable code Python is an excellent option and certainly fast enough for a first stab at whatever it is you're trying to solve. Why not have a bash at each for a small subset of your problem and benchmark the results in terms of development time and run time? Then you can make an objective decision based on some relevant data ...or at least that's what I'd do :-) The source code is available by navigating to a specific program. Scroll down to the bottom and click on one of the ""Python CPython"" links. An example: http://shootout.alioth.debian.org/u32/benchmark.php?test=mandelbrot&lang=python&id=5 +1 for now. After downloading the NumPy source code I can confirm it is mostly C wrapped in Python. By ""for now"" I mean it's an excellent answer I'll accept it if no-one produces a good comparison of different costs for C interop in Python Java and C#. Also I'll follow your advice and prototype a part of the app in all 3 languages. ""or did I miss something"" Put your [Edit] at the top where everyone will read your mistake. Out of curiosity did you look at more than that one page you were referred to?  Most of NumPy is in C but a large portion of the C code is ""boilerplate"" to handle all the dirty details of the Python/C interface. I think the ratio C vs. Python is around 50/50 ATM for NumPy. I am not too familiar with vm-based low-level details but I believe the interface cost would be higher because of the restrictions put on the jvm and the .clr. One of the reason why numpy is often faster than similar environments is the memory representation and how arrays are shared/passed between functions. Whereas most environments (Matlab and R as well I believe) use Copy-On-Write to pass arrays between functions NumPy use references. But doing so in e.g. the JVM would be hard (because of restrictions on how to use pointer etc...). It is doable (an early port of NumPy for Jython exists) but I don't know how they solve this issue. Maybe C++/Cli would make this easier but I have zero experience with that environment. @DavidCournapeaud passing an array from C# to a native dll is as easy as passing a pointer. In fact no copy (like for Java) of the array is made. The array _is_ passed as (pinned) reference with very little overhead.  It always depends on your own capability to handle the langue so the language is able to generate fast code. Out of my experience numpy is several times slower then good .NET implementations. And I expect JAVA to be similar fast. Their optimizing JIT compilers have improved significantly over the years and produce very efficient instructions. numpy on the other hand comes with a syntax wich is easier to use for those which are attuned to scripting languages. But if it comes to application development those advantages often turn to obstacles and you will yearn for typesafety and enterprise IDEs. Also the syntactic gap is already closing with C#. A growing number of scientific libraries exist for Java and .NET.Personally I tend towards C# bacause it provides better syntax for multidimensional arrays and somehow feels more 'modern'. But of course this is only my personal experience.",python performance numpy scipy scientific-computing
1791791,A,"Stacking numpy recarrays without losing their recarrayness Suppose I make two recarrays with the same dtype and stack them: >>> import numpy as np >>> dt = [('foo' int) ('bar' float)] >>> a = np.empty(2 dtype=dt).view(np.recarray) >>> b = np.empty(3 dtype=dt).view(np.recarray) >>> c = np.hstack((ab)) Although a and b are recarrays c is not: >>> c.foo Traceback (most recent call last): File ""<stdin>"" line 1 in <module> AttributeError: 'numpy.ndarray' object has no attribute 'foo' >>> d = c.view(np.recarray) >>> d.foo array([ 0 111050731618561 0 7718048 8246760947200437872]) I can obviously turn it into a recarray again as shown with d above but that is inconvenient. Is there a reason why stacking two recarrays does not produce another recarray? Incidentally you can also use: c = np.concatenate((ab)) or c = np.r_[a b] ( Source: this mailing list message ) They don't preserve the recarrayness either.  Alternatively there are some helper utilities in numpy.lib.recfunctions which I stumbled across here. This module has functions for both merging and stacking recarrays: from numpy.lib.recfunctions import stack_arrays c = stack_arrays((a b) asrecarray=True usemask=False) c.foo >>> array([ 140239282560000 4376479720 -4611686018427387904 4358733828 4365061216]) If one wants to add extra columns to a recarray this can be done using merge_arrays: import numpy as np from numpy.lib.recfunctions import merge_arrays dt1 = [('foo' int) ('bar' float)] dt2 = [('foobar' int) ('barfoo' float)] aa = np.empty(6 dtype=dt1).view(np.recarray) bb = np.empty(6 dtype=dt2).view(np.recarray) cc = merge_arrays((aa bb) asrecarray=True flatten=True) type(cc) >>> numpy.core.records.recarray (Although not an answer to the question I'm posting the latter example as a reference)  I don't know. Most likely it's a bug/feature that's never been implemented. numpy.hstack is basically a wrapper around a function in numpy.core.fromnumeric. Numeric is one of the two predecessors of numpy. Most functions in numpy have a convention to output the same type as the input by calling the method __array_wrap__ of the input on the output and the resulting output is should have the same data but ""wrapped"" in the new class. Perhaps the concept of ""wrapping"" was not in numeric andnever got added to this function. You can use this technique to make a smarter stacking function def hstack2(arrays) : return arrays[0].__array_wrap__(numpy.hstack(arrays)) This works for both recarrays and regular arrays >>> f = hstack2((ab)) >>> type(f) <class 'numpy.core.records.recarray'> >>> f.foo array([ 140633760262784 111050731618561 140633760262800 7536928 8391166428122670177]) >>> x = numpy.random.rand(3) >>> y = numpy.random.rand(2) >>> z = hstack2((xy)) >>> type(z) <type 'numpy.ndarray'> I'm not sure what you're planning but you might want to ask on the numpy mailing list is there's a better way than using the documented but double-underscored method and what their reasoning is for not doing the wrapping themselves.",python numpy recarray
1929973,A,"PyTables problem - different results when iterating over subset of table I am new to PyTables and am looking at using it to process data generated from an agent-based modeling simulation and stored in HDF5. I'm working with a 39 MB test file and am experiencing some strangeness. Here's the layout of the table:  /example/agt_coords (Table(2000000)) '' description := { ""agent"": Int32Col(shape=() dflt=0 pos=0) ""x"": Float64Col(shape=() dflt=0.0 pos=1) ""y"": Float64Col(shape=() dflt=0.0 pos=2)} byteorder := 'little' chunkshape := (20000) Here's how I'm accessing it in Python: from tables import * >>> h5file = openFile(""alternate_hose_test.h5"" ""a"") h5file.root.example.agt_coords /example/agt_coords (Table(2000000)) '' description := { ""agent"": Int32Col(shape=() dflt=0 pos=0) ""x"": Float64Col(shape=() dflt=0.0 pos=1) ""y"": Float64Col(shape=() dflt=0.0 pos=2)} byteorder := 'little' chunkshape := (20000) >>> coords = h5file.root.example.agt_coords Now here's where things get weird. [x for x in coords[1:100] if x['agent'] == 1] [(1 25.0 78.0) (1 25.0 78.0)] >>> [x for x in coords if x['agent'] == 1] [(1000000 25.0 78.0) (1000000 25.0 78.0)] >>> [x for x in coords.iterrows() if x['agent'] == 1] [(1000000 25.0 78.0) (1000000 25.0 78.0)] >>> [x['agent'] for x in coords[1:100] if x['agent'] == 1] [1 1] >>> [x['agent'] for x in coords if x['agent'] == 1] [1 1] I don't understand why the values are screwed up when I iterate over the whole table but not when I take a small subset of the whole set of rows. I'm sure this is an error in how I'm using the library so any help in this matter would be extremely appreciated. This is a very common point of confusion when iterating over Table object When you iterate over a Table the type of item you get is not the data at the item but an accessor to the table at the current row. So with [x for x in coords if x['agent'] == 1] you create a list of row accessors that all point to the ""current"" row of the table the last row. But when you do [x[""agent""] for x in coords if x['agent'] == 1] you use the accessor as you build the list. The solution to get all the data you need as you build the list by using the accessor on each iteration. There are two options [x[:] for x in coords if x['agent'] == 1] or [x.fetch_all_fields() for x in coords if x['agent'] == 1] The former builds a list of tuples. The latter returns a NumPy void object. IIRC the second is faster but the former might make more sense for you purposes. Here's a good explanation from the PyTables developer. In future releases printing a row accessor object may not simply show the data but state that it's a row accessor object. Thank you for the very detailed explanation of the unexpected behavior. I really wish that the tutorial (e.g. http://www.pytables.org/moin/HintsForSQLUsers  http://www.pytables.org/docs/manual/ch03.html (see 3.1.6) made note of what you explained",python numpy pytables
1236695,A,"Is 'for x in array' always result in sorted x? [Python/NumPy] For arrays and lists in Python and Numpy are the following lines equivalent: itemlist = [] for j in range(len(myarray)): item = myarray[j] itemlist.append(item) and: itemlist = [] for item in myarray: itemlist.append(item) I'm interested in the order of itemlist. In a few examples that I have tried they are identical but is it guaranteed? For example I know that the foreach statement in C# doesn't guarantee order and that I should be careful with it. Oops yes I did mean to throw in a range() Did you mean: for j in range(len(myarray)): ? Yes it's entirely guaranteed. for item in myarray (where myarray is a sequence which includes numpy's arrays builtin lists Python's array.arrays etc etc) is in fact equivalent in Python to: _aux = 0 while _aux < len(myarray): item = myarray[_aux] ...etc... for some phantom variable _aux;-). Btw both of your constructs are also equivalent to itemlist = list(myarray) Thanks! Yeah I wasnt really trying to convert an array to a list just exploring the order.  It is guaranteed for lists. I think the more relevant Python parallel to your C# example would be to iterate over the keys in a dictionary which is NOT guaranteed to be in any order. # Always prints 0-9 in order a_list = [0123456789] for x in a_list: print x # May or may not print 0-9 in order. Implementation dependent. a_dict = {'0':0'1':1'2':2'3':3'4':4'5':5'6':6'7':7'8':8'9':9} for x in a_dict: print x The for <element> in <iterable> structure only worries that the iterable supplies a next() function which returns something. There is no general guarantee that these elements get returned in any order over the domain of the for..in statement; lists are a special case. Thanks for highlighting that arrays have a notion of order that it is implemented via the next() function. The for ... in ... structure takes advantage of next() and so data with an intrinsic order (arrays) implements next() differently than data without an intrinsic order (such as dictionaries.)  Yes the Python Language Reference guarantees this (emphasis is mine):  for_stmt ::= ""for"" target_list ""in"" expression_list "":"" suite [""else"" "":"" suite] ""The suite is then executed once for each item provided by the iterator in the order of ascending indices.""",python arrays list numpy
1800187,A,replace values in an array as a replacement value for another within an operation with arrays or how to search within an array and replace a value by another for example: array ([[NaN 1. 1. 1. 1. 1. 1.] [1. NaN 1. 1. 1. 1. 1.] [1. 1. NaN 1. 1. 1. 1.] [1. 1. 1. NaN 1. 1. 1.] [1. 1. 1. 1. NaN 1. 1.] [1. 1. 1. 1. 1. NaN 1.] [1. 1. 1. 1. 1. 1. NaN]]) where it can replace NaN by 0. thanks for any response these days there is the special function: a = numpy.nan_to_num(a) Just saved my bacon while doing in Inverse Filter. [image-processing] But this will involve a temporary variable with same type and shape as ```a``` it will matter on large matrices.  You could do this: import numpy as np x=np.array([[np.NaN 1. 1. 1. 1. 1. 1.][1. np.NaN 1. 1. 1. 1. 1.][1. 1. np.NaN 1. 1. 1. 1.] [1. 1. 1. np.NaN 1. 1. 1.] [1. 1. 1. 1. np.NaN 1. 1.][1. 1. 1. 1. 1. np.NaN 1.] [1. 1. 1. 1. 1. 1. np.NaN]]) x[np.isnan(x)]=0 np.isnan(x) returns a boolean array which is True wherever x is NaN. x[ boolean_array ] = 0 employs fancy indexing to assign the value 0 wherever the boolean array is True. For a great introduction to fancy indexing and much more see also the numpybook. NameError: name 'x' is not defined not work @ricardo: Let x be your numpy array. hi excellent response thanks,python numpy
1727669,A,"Contruct 3d array in numpy from exist 2d array during preparing data for numpy calculate i curious about way to contruct myarray.shape => (21818) from d1.shape => (1818) d2.shape => (1818) i try to use numpy command hstack([[d1][d2]]) but it looks not work!! hstack and vstack do no change the number of dimensions of the arrays: they merely put them ""side by side"". Thus combining 2-dimensional arrays creates a new 2-dimensional array (not a 3D one!). You can do what Daniel suggested. You can alternatively convert your arrays to 3D arrays before stacking them by adding a new dimension to each array: d3 = vstack([ d1[newaxis...] d2[newaxis...] ]) # shape = (2 18 18) In fact d1[newaxis...].shape == (1 18 18) and you can stack both 3D arrays directly and get the new 3D array (d3) that you wanted. :) thank EOL  now i 'll know more about vstackhstack  Just doing d3 = array([d1d2]) seems to work for me: >>> from numpy import array >>> # ... create d1 and d2 ... >>> d1.shape (1818) >>> d2.shape (1818) >>> d3 = array([d1 d2]) >>> d3.shape (2 18 18) oh its work thank Daniel :)",python numpy
442218,A,How do I use a 2-d boolean array to select from a 1-d array on a per-row basis in numpy? Let me illustrate this question with an example: import numpy matrix = numpy.identity(5 dtype=bool) #Using identity as a convenient way to create an array with the invariant that there will only be one True value per row the solution should apply to any array with this invariant base = numpy.arange(5305) #This could be any 1-d array provided its length is the same as the length of axis=1 of matrix from above result = numpy.array([ base[line] for line in matrix ]) result now holds the desired result but I'm sure there is a numpy-specific method for doing this that avoids the explicit iteration. What is it? Here is another ugly way of doing it: n.apply_along_axis(base.__getitem__ 0 matrix).reshape((51))  If I understand your question correctly you can simply use matrix multiplication: result = numpy.dot(matrix base) If the result must have the same shape as in your example just add a reshape: result = numpy.dot(matrix base).reshape((51)) If the matrix is not symmetric be careful about the order in dot.  My try: numpy.sum(matrix * base axis=1),python numpy
877479,A,"What's the simplest way to extend a numpy array in 2 dimensions? I have a 2d array that looks like this: XX xx What's the most efficient way to add an extra row and column: xxy xxy yyy For bonus points I'd like to also be able to knock out single rows and columns so for example in the matrix below I'd like to be able to knock out all of the a's leaving only the x's - specifically I'm trying to delete the nth row and the nth column at the same time - and I want to be able to do this as quickly as possible: xxaxx xxaxx aaaaa xxaxx xxaxx The shortest in terms of lines of code i can think of is for the first question. >>> import numpy as np >>> p = np.array([[12][34]]) >>> p = np.append(p [[56]] 0) >>> p = np.append(p [[7][8][9]]1) >>> p array([[1 2 7] [3 4 8] [5 6 9]]) And the for the second question  p = np.array(range(20)) >>> p.shape = (45) >>> p array([[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]]) >>> n = 2 >>> p = np.append(p[:n]p[n+1:]0) >>> p = np.append(p[...:n]p[...n+1:]1) >>> p array([[ 0 1 3 4] [ 5 6 8 9] [15 16 18 19]])  Answer to the first question: Use numpy.append. http://docs.scipy.org/doc/numpy/reference/generated/numpy.append.html#numpy.append Answer to the second question: Use numpy.delete http://docs.scipy.org/doc/numpy/reference/generated/numpy.delete.html  maybe you need this. >>> x = np.array([1122]) >>> y = np.array([1876]) >>> z = np.array([135]) >>> np.concatenate((xyz)) array([11 22 18 7 6 1 3 5])  Another elegant solution to the first question may be the insert command: p = np.array([[12][34]]) p = np.insert(p 2 values=0 axis=1) # insert values before column 2 Leads to: array([[1 2 0] [3 4 0]]) insert may be slower than append but allows you to fill the whole row/column with one value easily. As for the second question delete has been suggested before: p = np.delete(p 2 axis=1) Which restores the original array again: array([[1 2] [3 4]])  A useful alternative answer to the first question using the examples from tomeedee’s answer would be to use numpy’s vstack and column_stack methods: Given a matrix p >>> import numpy as np >>> p = np.array([ [12]  [34] ]) an augmented matrix can be generated by: >>> p = np.vstack( [ p  [5  6] ] ) >>> p = np.column_stack( [ p  [ 7  8  9 ] ] ) >>> p array([[1 2 7] [3 4 8] [5 6 9]]) These methods may be convenient in practice than np.append() as they allow 1D arrays to be appended to a matrix without any modification in contrast to the following scenario: >>> p = np.array([ [ 1  2 ]  [ 3  4 ]  [ 5  6 ] ] ) >>> p = np.append( p  [ 7  8  9 ]  1 ) Traceback (most recent call last): File ""<stdin>"" line 1 in <module> File ""/usr/lib/python2.6/dist-packages/numpy/lib/function_base.py"" line 3234 in append return concatenate((arr values) axis=axis) ValueError: arrays must have same number of dimensions In answer to the second question a nice way to remove rows and columns is to use logical array indexing as follows: Given a matrix p >>> p = np.arange( 20 ).reshape( ( 4  5 ) ) suppose we want to remove row 1 and column 2: >>> r  c = 1  2 >>> p = p [ np.arange( p.shape[0] ) != r  : ] >>> p = p [ :  np.arange( p.shape[1] ) != c ] >>> p array([[ 0 1 3 4] [10 11 13 14] [15 16 18 19]]) Note - for reformed Matlab users - if you wanted to do these in a one-liner you need to index twice: >>> p = np.arange( 20 ).reshape( ( 4  5 ) ) >>> p = p [ np.arange( p.shape[0] ) != r  : ] [ :  np.arange( p.shape[1] ) != c ] This technique can also be extended to remove sets of rows and columns so if we wanted to remove rows 0 & 2 and columns 1 2 & 3 we could use numpy's setdiff1d function to generate the desired logical index: >>> p = np.arange( 20 ).reshape( ( 4  5 ) ) >>> r = [ 0  2 ] >>> c = [ 1  2  3 ] >>> p = p [ np.setdiff1d( np.arange( p.shape[0] ) r )  : ] >>> p = p [ :  np.setdiff1d( np.arange( p.shape[1] )  c ) ] >>> p array([[ 5 9] [15 19]])  I find it much easier to ""extend"" via assigning in a bigger matrix. E.g. import numpy as np p = np.array([[12] [34]]) g = np.array(range(20)) g.shape = (45) g[0:2 0:2] = p Here are the arrays: p  array([[1 2] [3 4]]) g: array([[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19]]) and the resulting g after assignment:  array([[ 1 2 2 3 4] [ 3 4 7 8 9] [10 11 12 13 14] [15 16 17 18 19]])",python arrays math numpy
1518779,A,"How to install numpy and scipy on Windows XP I have a problem installing Numpy and Scipy from http://www.scipy.org/Installing%5FSciPy/Windows I went to download page and downloaded .exe files for Python26. I have Python26 on my machine. After installation I tried >>> import nympy scipy Traceback (most recent call last): File ""<stdin>"" line 1 in <module> ImportError: No module named nympy >>> How to proceed? try with numpy instead of nympy My bad Thanks!!",python numpy scipy
1724504,A,Just curious about result from NumPy function! I have used NumPy for my Master thesis. I've converted parts of the code from MATLAB code but I have doubts in NumPy/Python when I reference: m = numpy.ones((102)) m[:0] which returns: array([ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]) and when I ref to: m[:0:1] it returns: array([[ 1.] [ 1.] [ 1.] [ 1.] [ 1.] [ 1.] [ 1.] [ 1.] [ 1.] [ 1.]]) that I think it should be cause same result with MATLAB!!! It would be very helpful if you'd proofread your question and format it properly. oops  sorry about that :) this should help a bit though I still don't understand the question exactly! thank you very much for formatting Amro :) I'm still learning Python myself but I think the way that slicing works is that indices point to in-between locations therefore 0:1 only gets you the first column. Is this what you were asking about? This is what the documentation has to say: One way to remember how slices work is to think of the indices as pointing between characters with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n for example:  +---+---+---+---+---+ | H | e | l | p | A | +---+---+---+---+---+ 0 1 2 3 4 5 -5 -4 -3 -2 -1 No m[:0:1] is the equivalent of the matlab statement as shown in the question - this extracts column 0 equivalent to column 1 in Matlab. 0:2 is a slice covering 01 which is both columns (so the whole array in this case) I think this what the OP was trying to do since (aside from the starting index) in MATLAB: `m(:1:2)` gets you two columns while in Python/NumPy: `m[:0:1]` returns one column. I guess this is something to be clarified.. Agree its not clear - I read the question as asking about extracting a column - in matlab m(:1) gives a 2d column vector - the same as m[:0:1] in numpy but m[:0] in numpy gives a 1d array - I think the question was why this is different to matlab but the slices translate as you say... Yea it's not definitely clear in the question. I meant the same as Amro: MATLAB m(:1:2) NumPy m[:0:2]  I forget what numpy does but Matlab indexes vectors from 1 not 0. So array(:0) is an error in Matlab.  This is because numpy has the concept of 1d arrays which Matlab doesn't have. Coupled with numpys broadcasting this provides a powerful simplification (less worrying about inserting transposes everywhere) but does mean you have to think a little bit about translating from Matlab. In this case extracting a single column with a scalar Numpy simplifies the result to a 1d array - but with a slice it preserves the original dimensions. If you want to stay closer to Matlab semantics you could try using the Matrix class. See NumPy for matlab users page for details. In this case you could do either of the following: m[:0][:newaxis] # gives same as matlab np.matrix(m)[:0] # gives same as matlab But remember if you use matrix class * becomes matrix multiplication and you need to use multiply() for elementwise. (This is all covered in NumPy for Matlab Users page). Generally I would recommend trying to get used to using 1d arrays where you would have column or row vector in matlab and generally things just work. You only need to worry about column vs row when reassembling them into a 2d array. You may be interested in automated matlab to python converters such as OMPC (paper) (I think there are others as well). ohh great i got it :D thank very much  thrope!!! no problem... If you feel this answers the question you could click on the tick by the answer to accept it... :),python matlab numpy
1949225,A,"""painting"" one array onto another using python / numpy I'm writing a library to process gaze tracking in Python and I'm rather new to the whole numpy / scipy world. Essentially I'm looking to take an array of (xy) values in time and ""paint"" some shape onto a canvas at those coordinates. For example the shape might be a blurred circle. The operation I have in mind is more or less identical to using the paintbrush tool in Photoshop. I've got an interative algorithm that trims my ""paintbrush"" to be within the bounds of my image and adds each point to an accumulator image but it's slow(!) and it seems like there's probably a fundamentally easier way to do this. Any pointers as to where to start looking? Sounds like you want some form of quick blit. However I lack knowledge of python to suggest a good answer. OpenCV uses numpy arrays and has basic drawing functions: circles elipses polylines... To draw a line you can call cv.line(arrayprevious_pointnew_pointcolourthickness=x) each time you get a mouse event.  Have you looked into Tkinter? Python Image Library may be some help too. Tkinter alarms me a bit -- I'm really leery of using a GUI tooklit to do array math. And I'm familiar with PIL but don't really see how this solves anything that numpy doesn't. I'd still need to do some odd jiggery-pokery to add the arrays together...?  In your question you describe a Gaussian filter for which scipy has support via a package. For example: from scipy import * # rand from pylab import * # figure imshow from scipy.ndimage import gaussian_filter # random ""image"" I = rand(100 100) figure(1) imshow(I) # gaussian filter J = gaussian_filter(I sigma=10) figure(2) imshow(J) Of course you can apply this on the whole image or just on a patch using slicing: J = array(I) # copy image J[30:70 30:70] = gaussian_filter(I[30:70 30:70] sigma=1) # apply filter to subregion figure(2) imshow(2) For basic image manipulation the Python Image library (PIL) is probably what you want. NOTE: for ""painting"" with a ""brush"" I think you could just create a boolean mask array with your brush. For instance: # 7x7 boolean mask with the ""brush"" (example: a _crude_ circle) mask = array([[0 0 1 1 1 0 0] [0 1 1 1 1 1 0] [1 1 1 1 1 1 1] [1 1 1 1 1 1 1] [1 1 1 1 1 1 1] [0 1 1 1 1 1 0] [0 0 1 1 1 0 0]] dtype=bool) # random image I = rand(100 100) # apply filter only on mask # compute the gauss. filter only on the 7x7 subregion not the whole image I[40:47 40:47][mask] = gaussian_filter(I[40:47 40:47][mask] sigma=1) Hm this may put me on the right track -- I think some slicing magic will help me do what I need. I added an example with a boolean mask maybe that's what you need. this is nice +1 :)  Doing a little of math in Fourier space may help: a translation (convolution by a dirac) is equal to a simple multiplication by a phase in Fourier... this makes your paintbrush move to the exact place (a similar solution than catchmeifyoutry & dwf but this allows a translation finer than the pixel like 2.5 alas with some ringing). Then a sum of such strokes is the sum of these operations. In code: import numpy import pylab from scipy import mgrid def FTfilter(image FTfilter): from scipy.fftpack import fftn fftshift ifftn ifftshift from scipy import real FTimage = fftshift(fftn(image)) * FTfilter return real(ifftn(ifftshift(FTimage))) def translate(image vec): """""" Translate image by vec (in pixels) """""" u = ((vec[0]+image.shape[0]/2) % image.shape[0]) - image.shape[0]/2 v = ((vec[1]+image.shape[1]/2) % image.shape[1]) - image.shape[1]/2 f_x f_y = mgrid[-1:1:1j*image.shape[0] -1:1:1j*image.shape[1]] trans = numpy.exp(-1j*numpy.pi*(u*f_x + v*f_y)) return FTfilter(image trans) def occlude(image mask): # combine in oclusive mode return numpy.max(numpy.dstack((image mask)) axis=2) if __name__ == '__main__': Image = numpy.random.rand(100 100) X Y = mgrid[-1:1:1j*Image.shape[0] -1:1:1j*Image.shape[1]] brush = X**2 + Y**2 < .05 # relative size of the brush # shows the brush pylab.imshow(brush) # move it to some other position / use a threshold to avoid ringing brushed = translate(brush [20 -10.51]) > .6 pylab.imshow(brushed) pylab.imshow(occlude(Image brushed)) more_strokes = [[40 -15.1] [-40 -15.1] [-25 15.1] [20 10] [0 -10] [25 -10.51]] for stroke in more_strokes: brushed = brushed + translate(brush stroke) > .6 pylab.imshow(occlude(Image brushed))  You should really look into Andrew Straw's motmot and libcamiface. He uses it for fly behaviour experiments but it's a flexible library for doing just the kind of image acquisition and processing you're doing I think. There's a video of his presentation at SciPy2009. As for the paintbrush scenario you mention I'd make a copy of the image with the .copy() method keep the paintbrush image in an array and simply add it with arr[first_br_row:last_br_row first_br_col:last_br_col] += brush[first_row:last_row first_col:last_col] where you set first_br_row last_br_row first_br_col last_br_col to address the subimage where you want to add the brush and first_row last_row first_col last_col to clip the brush (normally set them to 0 and # rows/cols - 1 but adjust when you're near enough to the image boundary to only want to paint part of the brush). Hope all that helps. Thanks! Turns out that yes slicing and indexing tricks will help a bunch. However! The Right Answer is probably to draw individual points and apply the brush and blur both as kernels. Thanks for the bump on the libraries; however this is really a post-processing task: the data have already been collected.",python image-processing numpy scipy
1782114,A,Why don't these two math functions return the same result? I'm trying to use fancy indexing instead of looping to speed up a function in Numpy. To the best of my knowledge I've implemented the fancy indexing version correctly. The problem is that the two functions (loop and fancy-indexed) do not return the same result. I'm not sure why. It's worth pointing out that the functions do return the same result if a smaller array is used (e.g. 20 x 20 x 20). Below I've included everything necessary to reproduce the error. If the functions do return the same result then the line find_maxdiff(data) - find_maxdiff_fancy(data) should return an array full of zeroes. from numpy import * def rms(data axis=0): return sqrt(mean(data ** 2 axis)) def find_maxdiff(data): samples channels epochs = shape(data) window_size = 50 maxdiff = zeros(epochs) for epoch in xrange(epochs): signal = rms(data[: : epoch] axis=1) for t in xrange(window_size alen(signal) - window_size): amp_a = mean(signal[t-window_size:t] axis=0) amp_b = mean(signal[t:t+window_size] axis=0) the_diff = abs(amp_b - amp_a) if the_diff > maxdiff[epoch]: maxdiff[epoch] = the_diff return maxdiff def find_maxdiff_fancy(data): samples channels epochs = shape(data) window_size = 50 maxdiff = zeros(epochs) signal = rms(data axis=1) for t in xrange(window_size alen(signal) - window_size): amp_a = mean(signal[t-window_size:t] axis=0) amp_b = mean(signal[t:t+window_size] axis=0) the_diff = abs(amp_b - amp_a) maxdiff[the_diff > maxdiff] = the_diff return maxdiff data = random.random((600 20 100)) find_maxdiff(data) - find_maxdiff_fancy(data) data = random.random((20 20 20)) find_maxdiff(data) - find_maxdiff_fancy(data) What magnitude of difference is there between the two? This isn't the typical floating point accuracy issue that catches so many people out is it? At what value between 20x20x20 and 600x20x100 do things start to go wrong? Do things go wrong gradually and more and more or all at once? The magnitude of the differences is rather large to just be floating point errors. First in fancy your signal is now 2D if I understand correctly - so I think it would be clearer to index it explicitly (eg amp_a = mean(signal[t-window_size:t:] axis=0). Similarly with alen(signal) - this should just be samples in both cases so I think it would be clearer to use that. It is wrong whenever you are actually doing something in the t loop - when samples < window_lenght as in the 20x20x20 example that loop never gets executed. As soon as that loop is executed more than once (ie samples > 2 *window_length+1) then the errors come. Not sure why though - they do look equivalent to me.  The problem is this line: maxdiff[the_diff > maxdiff] = the_diff The left side selects only some elements of maxdiff but the right side contains all elements of the_diff. This should work instead: replaceElements = the_diff > maxdiff maxdiff[replaceElements] = the_diff[replaceElements] or simply: maxdiff = maximum(maxdiff the_diff) As for why 20x20x20 size seems to work: This is because your window size is too large so nothing gets executed. Thanks for helping me better understand how fancy assignment works. Also I should have caught the silly reason why the smaller array was working :) Thanks again.,python numpy scipy
1975704,A,"Python - pickling fails for numpy.void objects  >>> idmapfile = open(""idmap"" mode=""w"") >>> pickle.dump(idMap idmapfile) >>> idmapfile.close() >>> idmapfile = open(""idmap"") >>> unpickled = pickle.load(idmapfile) >>> unpickled == idMap False idMap[1] {1537: (552 1 1537 17.793827056884766 3) 1540: (4220 1 1540 19.31205940246582 3) 1544: (592 1 1544 18.129131317138672 3) 1675: (529 1 1675 18.347782135009766 3) 1550: (4048 1 1550 19.31205940246582 3) 1424: (1528 1 1424 19.744396209716797 3) 1681: (1265 1 1681 19.596025466918945 3) 1560: (3457 1 1560 20.530569076538086 3) 1690: (477 1 1690 17.395542144775391 3) 1691: (554 1 1691 13.446117401123047 3) 1436: (3010 1 1436 19.596025466918945 3) 1434: (3183 1 1434 19.744396209716797 3) 1441: (3570 1 1441 20.589576721191406 3) 1435: (476 1 1435 19.640911102294922 3) 1444: (527 1 1444 17.98480224609375 3) 1478: (1897 1 1478 19.596025466918945 3) 1575: (614 1 1575 19.371648788452148 3) 1586: (2189 1 1586 19.31205940246582 3) 1716: (3470 1 1716 19.158674240112305 3) 1590: (2278 1 1590 19.596025466918945 3) 1463: (991 1 1463 19.31205940246582 3) 1594: (1890 1 1594 19.596025466918945 3) 1467: (1087 1 1467 19.31205940246582 3) 1596: (3759 1 1596 19.744396209716797 3) 1602: (3011 1 1602 20.530569076538086 3) 1547: (490 1 1547 17.994071960449219 3) 1605: (658 1 1605 19.31205940246582 3) 1606: (1794 1 1606 16.964881896972656 3) 1719: (1826 1 1719 19.596025466918945 3) 1617: (583 1 1617 11.894925117492676 3) 1492: (3441 1 1492 20.500667572021484 3) 1622: (3215 1 1622 19.31205940246582 3) 1628: (2761 1 1628 19.744396209716797 3) 1502: (1563 1 1502 19.596025466918945 3) 1632: (1108 1 1632 15.457141876220703 3) 1468: (3779 1 1468 19.596025466918945 3) 1642: (3970 1 1642 19.744396209716797 3) 1518: (612 1 1518 18.570245742797852 3) 1647: (854 1 1647 16.964881896972656 3) 1650: (2099 1 1650 20.439058303833008 3) 1651: (540 1 1651 18.552841186523438 3) 1653: (613 1 1653 19.237197875976563 3) 1532: (537 1 1532 18.885730743408203 3)} >>> unpickled[1] {1537: (64880 1638 56700 -1.0808743559293829e+18 152) 1540: (64904 1638 0 0.0 0) 1544: (54472 1490 0 0.0 0) 1675: (6464 1509 0 0.0 0) 1550: (43592 1510 0 0.0 0) 1424: (43616 1510 0 0.0 0) 1681: (0 0 0 0.0 0) 1560: (400 152 400 2.1299736657737219e-43 0) 1690: (408 152 408 2.7201111331839077e+26 34) 1435: (424 152 61512 1.0122952080313192e-39 0) 1436: (400 152 400 20.250289916992188 3) 1434: (424 152 62080 1.0122952080313192e-39 0) 1441: (400 152 400 12.250144958496094 3) 1691: (424 152 42608 15.813941955566406 3) 1444: (400 152 400 19.625289916992187 3) 1606: (424 152 42432 5.2947192852601414e-22 41) 1575: (400 152 400 6.2537390010262572e-36 0) 1586: (424 152 42488 1.0122601755697111e-39 0) 1716: (400 152 400 6.2537390010262572e-36 0) 1590: (424 152 64144 1.0126357235581501e-39 0) 1463: (400 152 400 6.2537390010262572e-36 0) 1594: (424 152 32672 17.002994537353516 3) 1467: (400 152 400 19.750289916992187 3) 1596: (424 152 7176 1.0124003054161436e-39 0) 1602: (400 152 400 18.500289916992188 3) 1547: (424 152 7000 1.0124003054161436e-39 0) 1605: (400 152 400 20.500289916992188 3) 1478: (424 152 42256 -6.0222748507426518e+30 222) 1719: (400 152 400 6.2537390010262572e-36 0) 1617: (424 152 16472 1.0124283313854301e-39 0) 1492: (400 152 400 6.2537390010262572e-36 0) 1622: (424 152 35304 1.0123190301052127e-39 0) 1628: (400 152 400 6.2537390010262572e-36 0) 1502: (424 152 63152 19.627988815307617 3) 1632: (400 152 400 19.375289916992188 3) 1468: (424 152 38088 1.0124213248931084e-39 0) 1642: (400 152 400 6.2537390010262572e-36 0) 1518: (424 152 63896 1.0127436235399031e-39 0) 1647: (400 152 400 6.2537390010262572e-36 0) 1650: (424 152 53424 16.752857208251953 3) 1651: (400 152 400 19.250289916992188 3) 1653: (424 152 50624 1.0126497365427934e-39 0) 1532: (400 152 400 6.2537390010262572e-36 0)} The keys come out fine the values are screwed up. I tried same thing loading file in binary mode; didn't fix the problem. Any idea what I'm doing wrong? Edit: Here's the code with binary. Note that the values are different in the unpickled object. >>> idmapfile = open(""idmap"" mode=""wb"") >>> pickle.dump(idMap idmapfile) >>> idmapfile.close() >>> idmapfile = open(""idmap"" mode=""rb"") >>> unpickled = pickle.load(idmapfile) >>> unpickled==idMap False >>> unpickled[1] {1537: (12176 2281 56700 -1.0808743559293829e+18 152) 1540: (0 0 15934 2.7457842047810522e+26 108) 1544: (400 152 400 4.9518498821046956e+27 53) 1675: (408 152 408 2.7201111331839077e+26 34) 1550: (456 152 456 -1.1349175514578289e+18 152) 1424: (432 152 432 4.5939047815653343e-40 11) 1681: (408 152 408 2.1299736657737219e-43 0) 1560: (376 152 376 2.1299736657737219e-43 0) 1690: (376 152 376 2.1299736657737219e-43 0) 1435: (376 152 376 2.1299736657737219e-43 0) 1436: (376 152 376 2.1299736657737219e-43 0) 1434: (376 152 376 2.1299736657737219e-43 0) 1441: (376 152 376 2.1299736657737219e-43 0) 1691: (376 152 376 2.1299736657737219e-43 0) 1444: (376 152 376 2.1299736657737219e-43 0) 1606: (25784 2281 376 -3.2883343074537754e+26 34) 1575: (24240 2281 376 2.1299736657737219e-43 0) 1586: (24240 2281 376 2.1299736657737219e-43 0) 1716: (24240 2281 376 -3.0093091599657311e-35 26) 1590: (24240 2281 376 2.1299736657737219e-43 0) 1463: (24240 2281 376 2.1299736657737219e-43 0) 1594: (24240 2281 376 -4123208450048.0 196) 1467: (25784 2281 376 2.1299736657737219e-43 0) 1596: (25784 2281 376 2.1299736657737219e-43 0) 1602: (25784 2281 376 -5.9963281433905448e+26 76) 1547: (25784 2281 376 -218106240.0 139) 1605: (25784 2281 376 -3.7138649803377281e+27 56) 1478: (376 152 376 2.1299736657737219e-43 0) 1719: (25784 2281 376 2.1299736657737219e-43 0) 1617: (25784 2281 376 -1.4411779941597184e+17 237) 1492: (25784 2281 376 2.8596493694487798e-30 80) 1622: (25784 2281 376 184686084096.0 93) 1628: (1336 152 1336 3.1691839245470052e+29 179) 1502: (1272 152 1272 -5.2042207205116645e-17 99) 1632: (1208 152 1208 2.1299736657737219e-43 0) 1468: (1144 152 1144 2.1299736657737219e-43 0) 1642: (1080 152 1080 2.1299736657737219e-43 0) 1518: (1016 152 1016 4.0240902787680023e+35 145) 1647: (952 152 952 -985172619034624.0 237) 1650: (888 152 888 12094787289088.0 66) 1651: (824 152 824 2.1299736657737219e-43 0) 1653: (760 152 760 0.00018310768064111471 238) 1532: (696 152 696 8.8978061885676389e+26 125)} OK I've isolated the problem but don't know why it's so. First apparently what I'm pickling are not tuples (though they look like it) but instead numpy.void types. Here is a series to illustrate the problem. first = run0.detections[0] >>> first (1 19 1578 82.637763977050781 1) >>> type(first) <type 'numpy.void'> >>> firstTuple = tuple(first) >>> theFile = open(""pickleTest"" ""w"") >>> pickle.dump(first theFile) >>> theTupleFile = open(""pickleTupleTest"" ""w"") >>> pickle.dump(firstTuple theTupleFile) >>> theFile.close() >>> theTupleFile.close() >>> first (1 19 1578 82.637763977050781 1) >>> firstTuple (1 19 1578 82.637764 1) >>> theFile = open(""pickleTest"" ""r"") >>> theTupleFile = open(""pickleTupleTest"" ""r"") >>> unpickledTuple = pickle.load(theTupleFile) >>> unpickledVoid = pickle.load(theFile) >>> type(unpickledVoid) <type 'numpy.void'> >>> type(unpickledTuple) <type 'tuple'> >>> unpickledTuple (1 19 1578 82.637764 1) >>> unpickledTuple == firstTuple True >>> unpickledVoid == first False >>> unpickledVoid (7936 1705 56700 -1.0808743559293829e+18 152) >>> first (1 19 1578 82.637763977050781 1) show us the same code with binary-mode writing and reading. There is nothing wrong with this code. I tried it out and it worked just fine. I wasn't sure exactly what you were pickling however I used an array of dicts because that looked similar. Works for me in Python 2.4.5 2.5.2 and with the obvious changes in Python 3.0. Please post actual test data for which it fails. Also what version of pickle are you using? pickle or cPickle? idMap is a list of dictionaries where dictionary is int-> tuple object. idMap[1] is available at http://pastebin.com/f44a58355 I'm using pickle not cPickle. I'm using Python 2.6.4 on WindowsXP (r264:75708 Oct 26 2009 08:23:19) [MSC v.1500 32 bit (Intel)] In my response below I used your data with pickle (not cPickle) on both python2.51 (Cygwin) and python3.1 (Windows) using all protocols and it works so pickling will only work with top level module functions and classes and will not pickle class data so if some numpy class code/data are required to produce a representation of the numpy void type pickling isn't going to work as expected. It may be that the numpy package has implemented an internal __repr__ to print the void type as a tuple if this is the case then what you pickled certainly is not going to be what you printed. jottos - if you submit that comment as an answer I'll accept it and close the question. That must be what's happening internally. I'm just wrapping it in a tuple call and the pickling works as expected. Something with your system (filesystem?) ; I would try pickling in binary mode; use dump(idMap idmapfile protocol=2) Did not fix the problem - same result.  So using python31 I made just a small change to your example and it worked fine. Note that I added the ""b"" for binary in the file open's I tried this with all protocols and it worked for each idmapfile = open(""idmap"" mode=""wb"") pickle.dump(idMap idmapfile) idmapfile.close() idmapfile = open(""idmap"" ""rb"") unpickled = pickle.load(idmapfile) print ('they are equal' unpickled == idMap) src> ./pick.py they are equal True  I agree. I think there is a problem with serializing numpy.void example that doesn't work (Python 2.7.3 numpy 1.6.1): import pickle numpy as np my_array = np.array([('hello' 45.5 'world')] dtype=[('a' str 10) ('b' float) ('c' str10)]) my_void = my_array[0] print my_void print pickle.loads(pickle.dumps(my_void)) which will print: ('hello' 45.5 'world') ('\xc0\x00llo' 45.5 'world') The first looks like a tuple but it is actually a numpy.void So to avoid this you can't have numpy.void you should instead wrap your void with numpy.array() or call .tolist() on your numpy.void. Edit: There is a bug in numpy https://github.com/numpy/numpy/pull/3188  so pickling will only work with top level module functions and classes and will not pickle class data so if some numpy class code/data are required to produce a representation of the numpy void type pickling isn't going to work as expected. It may be that the numpy package has implemented an internal repr to print the void type as a tuple if this is the case then what you pickled certainly is not going to be what you printed. – jottos Dec 29 '09 at 18:42",python serialization numpy marshalling pickle
111983,A,"python.array versus numpy.array If you are creating a 1d array in Python is there any benefit to using the NumPy package? It all depends on what you plan to do with the array. If all you're doing is creating arrays of simple data types and doing I/O the array module will do just fine. If on the other hand you want to do any kind of numerical calculations the array module doesn't provide any help with that. NumPy (and SciPy) give you a wide variety of operations between arrays and special functions that are useful not only for scientific work but for things like advanced image manipulation or in general anything where you need to perform efficient calculations with large amounts of data. Numpy is also much more flexible e.g. it supports arrays of any type of Python objects and is also able to interact ""natively"" with your own objects if they conform to the array interface.",python numpy
601477,A,"Best way to create a NumPy array from a dictionary? I'm just starting with NumPy so I may be missing some core concepts... What's the best way to create a NumPy array from a dictionary whose values are lists? Something like this: d = { 1: [102030]  2: [5060] 3: [100200300400500] } Should turn into something like: data = [ [102030??] [5060???] [100200300400500] ] I'm going to do some basic statistics on each row eg: deviations = numpy.std(data axis=1) Questions: What's the best / most efficient way to create the numpy.array from the dictionary? The dictionary is large; a couple of million keys each with ~20 items. The number of values for each 'row' are different. If I understand correctly numpy wants uniform size so what do I fill in for the missing items to make std() happy? Update: One thing I forgot to mention - while the python techniques are reasonable (eg. looping over a few million items is fast) it's constrained to a single CPU. Numpy operations scale nicely to the hardware and hit all the CPUs so they're attractive. numpy dictionary You can use a structured array to preserve the ability to address a numpy object by a key like a dictionary. import numpy as np dd = {'a':1'b':2'c':3} dtype = eval('[' + ''.join([""('%s' float)"" % key for key in dd.keys()]) + ']') values = [tuple(dd.values())] numpy_dict = np.array(values dtype=dtype) numpy_dict['c'] will now output array([ 3.]) However the resulting array has a nested tuple so it might be slower for some operations.  You don't need to create numpy arrays to call numpy.std(). You can call numpy.std() in a loop over all the values of your dictionary. The list will be converted to a numpy array on the fly to compute the standard variation. The downside of this method is that the main loop will be in python and not in C. But I guess this should be fast enough: you will still compute std at C speed and you will save a lot of memory as you won't have to store 0 values where you have variable size arrays. If you want to further optimize this you can store your values into a list of numpy arrays so that you do the python list -> numpy array conversion only once. if you find that this is still too slow try to use psycho to optimize the python loop. if this is still too slow try using Cython together with the numpy module. This Tutorial claims impressive speed improvements for image processing. Or simply program the whole std function in Cython (see this for benchmarks and examples with sum function ) An alternative to Cython would be to use SWIG with numpy.i. if you want to use only numpy and have everything computed at C level try grouping all the records of same size together in different arrays and call numpy.std() on each of them. It should look like the following example. example with O(N) complexity: import numpy list_size_1 = [] list_size_2 = [] for row in data.itervalues(): if len(row) == 1: list_size_1.append(row) elif len(row) == 2: list_size_2.append(row) list_size_1 = numpy.array(list_size_1) list_size_2 = numpy.array(list_size_2) std_1 = numpy.std(list_size_1 axis = 1) std_2 = numpy.std(list_size_2 axis = 1) I'm doing the numpy.std in a loop now and you're right the memory savings are important. I would like to at least do a speed comparison with the numpy version though. The problem is that numpy.std() was made to accept only fix size array. So the only way I see to do this test is to group all the records of same size together and call numpy.std() on each of them. Shouldn't CPython really be Cython? Have I got it wrong? Yes correct. Fixed. Grouping same sized records simple but effective. I like it.  While there are already some pretty reasonable ideas present here I believe following is worth mentioning. Filling missing data with any default value would spoil the statistical characteristics (std etc). Evidently that's why Mapad proposed the nice trick with grouping same sized records. The problem with it (assuming there isn't any a priori data on records lengths is at hand) is that it involves even more computations than the straightforward solution: at least O(N*logN) 'len' calls and comparisons for sorting with an effective algorithm O(N) checks on the second way through the list to obtain groups(their beginning and end indexes on the 'vertical' axis) Using Psyco is a good idea (it's strikingly easy to use so be sure to give it a try). It seems that the optimal way is to take the strategy described by Mapad in bullet #1 but with a modification - not to generate the whole list but iterate through the dictionary converting each row into numpy.array and performing required computations. Like this: for row in data.itervalues(): np_row = numpy.array(row) this_row_std = numpy.std(np_row) # compute any other statistic descriptors needed and then save to some list In any case a few million loops in python won't take as long as one might expect. Besides this doesn't look like a routine computation so who cares if it takes extra second/minute if it is run once in a while or even just once. A generalized variant of what was suggested by Mapad: from numpy import array mean std def get_statistical_descriptors(a): if ax = len(shape(a))-1 functions = [mean std] return f(a axis = ax) for f in functions def process_long_list_stats(data): import numpy groups = {} for key row in data.iteritems(): size = len(row) try: groups[size].append(key) except KeyError: groups[size] = ([key]) results = [] for gr_keys in groups.itervalues(): gr_rows = numpy.array([data[k] for k in gr_keys]) stats = get_statistical_descriptors(gr_rows) results.extend( zip(gr_keys zip(*stats)) ) return dict(results) Thanks Maleev this is essentially what I ended up doing. One thing I forgot to mention - while looping in Python is fast I believe I'm only using a single CPU with this method. Matrix operations hit all CPUs so they're attractive. Why would you need to sort the rows before grouping vectors by length? Only grouping is needed. Moreover I would be careful with the big O notation: here N ~ 1000000 but the speed between a Python and C program can be ~100 times slower. So N -> 1000 is not really tending to the infinity 2 Parand: You're right taking multi-threading into account does really make sense. 2 Mapad: If I'm not terribly mistaken grouping is essentially equivalent to sorting. How then do you suggest to group them? Python code whether it is just loop over the rows or grouping is executed in any of the cases. So talking exclusively of python code's asymptotic complexity we've got difference in p*O(NlogN) - p*O(N) = p*O(NlogN). Besides C code looping through rows inside groups adds up c*O(N) to it. You say c << p. Sure. But that still leaves that p*O(NlogN) difference. Unless you prove you can really do grouping in O(N) in average and worst case. I agree with your simplifications I just want to recall they assume N >> p (p being computation time introduced by Python loop compared to C loop to process a record). Since all process here take O(N) (see my example) I would not throw away the p with the big O notation to check complexity",python numpy
1350174,A,"What does matrix**2 mean in python/numpy? I have a python ndarray temp in some code I'm reading that suffers this: x = temp**2 Is this the dot square (ie equivalent to m.*m) or the matrix square (ie m must be a square matrix)? In particular I'd like to know whether I can get rid of the transpose in this code: temp = num.transpose(whatever) num.sum(temp**2axis=1)) and turn it into this: num.sum(whatever**2axis=0) That will save me at least 0.1ms and is clearly worth my time. Thanks! The ** operator is ungooglable and I know nothing! a ** is the raise-to-power operator in Python so x**2 means ""x squared"" in Python -- including numpy. Such operations in numpy always apply element by element so x**2 squares each element of array x (whatever number of dimensions) just like say x*2 would double each element or x+2 would increment each element by two (in each case x proper is unaffected -- the result is a new temporary array of the same shape as x!). Edit: as @kaizer.ze points out while what I wrote holds for numpy.array objects it doesn't apply to numpy.matrix objects where multiplication means matrix multiplication rather than element by element operation like for array (and similarly for raising to power) -- indeed that's the key difference between the two types. As the Scipy tutorial puts it for example: When we use numpy.array or numpy.matrix there is a difference. A*x will be in the latter case matrix product not elementwise product as with array. i.e. as the numpy reference puts it: A matrix is a specialized 2-d array that retains its 2-d nature through operations. It has certain special operators such as * (matrix multiplication) and ** (matrix power). Well it is sadly not so simple as I answered; the differing behaviors of `array` and `matrix` can confuse this and operators such as `*` and `**` change meaning! (If A * B is matrix multiplication whith A B matrix A**2 has to be matrix exponentiation of course.) Yes there's a difference between matrix and array -- though `**` is of course still the raise-to-power operation operations on a matrix apply to ""the matrix"" on an array to ""the elements"". Good point let me edit to clarify.  You should read NumPy for Matlab Users. The elementwise power operation is mentioned there and you can also see that in numpy some operators apply differently to array and matrix. >>> from numpy import * >>> a = arange(4).reshape((22)) >>> print a**2 [[0 1] [4 9]] >>> print matrix(a)**2 [[ 2 3] [ 6 11]]  It's just the square of each element. from numpy import * a = arange(4).reshape((22)) print a**2 prints [[0 1] [4 9]] Woot thanks. Fifteeeeenherewecome. You're welcome. (I signed back into point out the probably obvious note that if you're ndarray are >2 dimensions I don't think the transposing axis swapping thing will work.) I can see where this might be confusing. Without knowing Python and understanding that for real (and complex) numbers squaring means ""multiply a number by itself"" it would have been reasonable to assume that it meant ""multiply a matrix by itself"" for matricies. This means that the matrix has equal numbers of rows and columns of course.",python numpy
1579218,A,"numpy.extract and numpy.any functions is it possible to make it simpler way? If there is any possibility to make this code simpler I'd really appreciate it! I am trying to get rid of rows with zeros. The first column is date. If all other columns are zero they have to be deleted. Number of columns varies. import numpy as np condition = [ np.any( list(x)[1:] ) for x in r] r = np.extract( condition r ) numpy.extract docs seems simple enough to me. what are you not happy about? just felt that for ndarray had to be a better way conversion to list and then list comprehension looked weird You can avoid the list comprehension and instead use fancy indexing: #!/usr/bin/env python import numpy as np import datetime r=np.array([(datetime.date(200011)01) (datetime.date(200011)11) (datetime.date(200011)10) (datetime.date(200011)00) ]) r=r[r[:1:].any(axis=1)] print(r) # [[2000-01-01 0 1] # [2000-01-01 1 1] # [2000-01-01 1 0] if r is an ndarray then r[:1:] is a view with the first column removed. r[:1:].any(axis=1) is a boolean array which you can then use as a ""fancy index"" Beautiful! I like it. Ugh I can't read this. Please format as ""Code Sample"". Edit your text select the code and click on the button that looks like ""101/010"". This is a promising answer but I don't think it is quite correct yet. He wants rows compressed if they are *all* zeroes not if any single value is zero. Okay I've modified my code per steveha's comment absolutely what I needed thanks a lot should have known ndarray indexing better!",python numpy
1208118,A,"Using numpy to build an array of all combinations of two arrays I'm trying to run over the parameters space of a 6 parameter function to study it's numerical behavior before trying to do anything complex with it so I'm searching for a efficient way to do this. My function takes float values given a 6-dim numpy array as input. What I tried to do initially was this: 1) First I created a function that takes 2 arrays and generate an array with all combinations of values from the two arrays from numpy import * def comb(ab): c = [] for i in a: for j in b: c.append(r_[ij]) return c The I used reduce to apply that to m copies of the same array: def combs(am): return reduce(comb[a]*m) And then I evaluate my function like this: values = combs(np.arange(010.1)6) for val in values: print F(val) This works but it's waaaay too slow. I know the space of parameters is huge but this shouldn't be so slow. I have only sampled (10)^6 = a million points in this example and it took more then 15 seconds just to create the array 'values'. Do you know any more efficient way of doing this with numpy? I can modify the way the function F take it's arguments if it's necessary. itertools.combinations is in general the fastest way to get combinations from a Python container (if you do in fact want combinations i.e. arrangements WITHOUT repetitions and independent of order; that's not what your code appears to be doing but I can't tell whether that's because your code is buggy or because you're using the wrong terminology). If you want something different than combinations perhaps other iterators in itertools product or permutations might serve you better. For example it looks like your code is roughly the same as: for val in itertools.product(np.arange(0 1 0.1) repeat=6): print F(val) All of these iterators yield tuples not lists or numpy arrays so if your F is picky about getting specifically a numpy array you'll have to accept the extra overhead of constructing or clearing and re-filling one at each step. Thanks. This is exactly what I needed. I'm still not used to some python concepts like iterators. @Rafael glad to know I've been of help!  It looks like you want a grid to evaluate your function in which case you can use numpy.ogrid (open) or numpy.mgrid (fleshed out): import numpy my_grid = numpy.mgrid[[slice(010.1)]*6]  You can do something like this import numpy as np def cartesian_coord(*arrays): grid = np.meshgrid(*arrays) coord_list = [entry.ravel() for entry in grid] points = np.vstack(coord_list).T return points a = np.arange(4) # fake data print(cartesian_coord(*6*[a]) which gives array([[0 0 0 0 0 0] [0 0 0 0 0 1] [0 0 0 0 0 2] ... [3 3 3 3 3 1] [3 3 3 3 3 2] [3 3 3 3 3 3]])  The following numpy implementation should be approx. 2x the speed of the given answer: def cartesian2(arrays): arrays = [np.asarray(a) for a in arrays] shape = (len(x) for x in arrays) ix = np.indices(shape dtype=int) ix = ix.reshape(len(arrays) -1).T for n arr in enumerate(arrays): ix[: n] = arrays[n][ix[: n]] return ix  Here's a pure-numpy implementation. It's ca. 5× faster than using itertools.  import numpy as np def cartesian(arrays out=None): """""" Generate a cartesian product of input arrays. Parameters ---------- arrays : list of array-like 1-D arrays to form the cartesian product of. out : ndarray Array to place the cartesian product in. Returns ------- out : ndarray 2-D array of shape (M len(arrays)) containing cartesian products formed of input arrays. Examples -------- >>> cartesian(([1 2 3] [4 5] [6 7])) array([[1 4 6] [1 4 7] [1 5 6] [1 5 7] [2 4 6] [2 4 7] [2 5 6] [2 5 7] [3 4 6] [3 4 7] [3 5 6] [3 5 7]]) """""" arrays = [np.asarray(x) for x in arrays] dtype = arrays[0].dtype n = np.prod([x.size for x in arrays]) if out is None: out = np.zeros([n len(arrays)] dtype=dtype) m = n / arrays[0].size out[:0] = np.repeat(arrays[0] m) if arrays[1:]: cartesian(arrays[1:] out=out[0:m1:]) for j in xrange(1 arrays[0].size): out[j*m:(j+1)*m1:] = out[0:m1:] return out Nice Pauli this solves my 2D interpolation problem. Defining the data point coords for griddata was giving some trouble. Does this function make into the master numpy code? why not creat `out` with `np.ndarray` that saves time. ever consider submitting this to be included in numpy? this is not the first time I've gone looking for this functionality and found your post. This rocks! Saved me some real time! Thanks There is bug in this implementation. For arrays of strings for example: arrays[0].dtype = ""|S3"" and arrays[1].dtype = ""|S5"". So there is a need in finding the longest string in input and use its type in out = np.zeros([n len(arrays)] dtype=dtype) Can I just say thank you so much. Thats like 2 hours of my time I could have saved had I gotten here first. FYI: seems to have made it into the scikit-learn package at `from sklearn.utils.extmath import cartesian`",python arrays multidimensional-array numpy
1382846,A,How can I create a numpy array holding values of a multi-variable function? I want to create an array holding a function f(xyz). If it were a function of one variable I'd do for instance: sinx = numpy.sin(numpy.linspace(-55100)) to get sin(x) for x in [-55] How can I do the same to get for instance sin(x+y+z)? What would be the values for xyz and/or how do you plan to generate them? xyz would be the cartesian product of `numpy.linspace(-55100)` over all three dimensions. I don't know the best way to generate them. I guess that's a pre-requisite for the question. xyz = numpy.mgrid[-5:5-5:5-5:5] sinxyz = numpy.sin(xyz[0]+xyz[1]+xyz[2])  I seem to have found a way: # define the range of xyz x_range = numpy.linspace(x_minx_maxx_num) y_range = numpy.linspace(y_miny_maxy_num) z_range = numpy.linspace(z_minz_maxz_num) # create arrays xyz in the correct dimensions # so that they create the grid xyz = numpy.ix_(x_rangey_rangez_range) # calculate the function of x y and z sinxyz = numpy.sin(x+y+z)  The numpy.mgrid function would work equally well: xyz = numpy.mgrid[x_min:x_max:x_num y_min:y_max:y_num z_min:z_max:z_num] sinxyz = numpy.sin(x+y+z) edit: to get it to work x_num y_num and z_num have to be explicit numbers followed by j e.g. xy = numpy.mgrid[-1:1:10j -1:1:10j],python function multidimensional-array numpy
1055131,A,"How to modify a NumPy.recarray using its two views I am new to Python and Numpy and I am facing a problem that I can not modify a numpy.recarray when applying to masked views. I read recarray from a file then create two masked views then try to modify the values in for loop. Here is an example code. import numpy as np import matplotlib.mlab as mlab dat = mlab.csv2rec(args[0] delimiter=' ') m_Obsr = dat.is_observed == 1 m_ZeroScale = dat[m_Obsr].scale_mean < 0.01 for d in dat[m_Obsr][m_ZeroScale]: d.scale_mean = 1.0 But when I print the result newFile = args[0] + "".no-zero-scale"" mlab.rec2csv(dat[m_Obsr][m_ZeroScale] newFile delimiter=' ') All the scale_means in the files are still zero. I must be doing something wrong. Is there a proper way of modifying values of the view? Is it because I am applying two views one by one? Thank you. I think you have a misconception in this term ""masked views"" and should (re-)read The Book (now freely downloadable) to clarify your understanding. I quote from section 3.4.2: Advanced selection is triggered when the selection object obj is a non-tuple sequence object an ndarray (of data type integer or bool) or a tuple with at least one sequence object or ndarray (of data type integer or bool). There are two types of advanced indexing: integer and Boolean. Advanced selection always returns a copy of the data (contrast with basic slicing that returns a view). What you're doing here is advanced selection (of the Boolean kind) so you're getting a copy and never binding it anywhere -- you make your changes on the copy and then just let it go away then write a new fresh copy from the original. Once you understand the issue the solution should be simple: make your copy once make your changes on that copy and write that same copy. I.e.: dat = mlab.csv2rec(args[0] delimiter=' ') m_Obsr = dat.is_observed == 1 m_ZeroScale = dat[m_Obsr].scale_mean < 0.01 the_copy = dat[m_Obsr][m_ZeroScale] for d in the_copy: d.scale_mean = 1.0 newFile = args[0] + "".no-zero-scale"" mlab.rec2csv(the_copy newFile delimiter=' ') Yes you were right I did not realise I am doing an advanced selection and that the later returns a temporary copy. And my understanding of ""masked views"" is indeed very hazy. Thank you for the quote. I was reading the same section 3 earlier but did not get to this §. Your fix will work. Though my question whether it is possible to modify the original data in dat without taking a copy of the part of the array. I need the order of the original to be preserved while modify only subset. Will simple iterative approach work? Or is there anything nicer? To modify dat itself I know nothing nicer than the ""simple iterative approach"" (generally on the .flat view as that IS indeed a view!-). BTW SO etiquette is that you upvote good answers and accept one that does answer your question or fix your problem -- since you comment ""your fix will work"" by that etiquette you should upvote and accept (and probably open another question specifically about selective modification in the main array itself -- somebody might give you a better approach than iteration if they saw that question but may not notice the question if you just ask it in a comment!) I did accept the solution because it is indeed the solution to the described problem. I can't up-vote as I don't have enough rating yet :). Thank you for your interest in the problem.",python numpy matplotlib
558216,A,"Function to determine if two numbers are nearly equal when rounded to n significant decimal digits I have been asked to test a library provided by a 3rd party. The library is known to be accurate to n significant figures. Any less-significant errors can safely be ignored. I want to write a function to help me compare the results: def nearlyequal( a b sigfig=5 ): The purpose of this function is to determine if two floating-point numbers (a and b) are approximately equal. The function will return True if a==b (exact match) or if a and b have the same value when rounded to sigfig significant-figures when written in decimal. Can anybody suggest a good implementation? I've written a mini unit-test. Unless you can see a bug in my tests then a good implementation should pass the following: assert nearlyequal(1 1 5) assert nearlyequal(1.0 1.0 5) assert nearlyequal(1.0 1.0 5) assert nearlyequal(-1e-9 1e-9 5) assert nearlyequal(1e9 1e9 + 1  5) assert not nearlyequal( 1e4 1e4 + 1 5) assert nearlyequal( 0.0 1e-15 5 ) assert not nearlyequal( 0.0 1e-4 6 ) Additional notes: Values a and b might be of type int float or numpy.float64. Values a and b will always be of the same type. It's vital that conversion does not introduce additional error into the function. Lets keep this numerical so functions that convert to strings or use non-mathematical tricks are not ideal. This program will be audited by somebody who is a mathematician who will want to be able to prove that the function does what it is supposed to do. Speed... I've got to compare a lot of numbers so the faster the better. I've got numpy scipy and the standard-library. Anything else will be hard for me to get especially for such a small part of the project. Thanks! [Comparing Floating Point Numbers 2012 Edition](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/) There is a function assert_approx_equal in numpy.testing (source here) which may be a good starting point. def assert_approx_equal(actualdesiredsignificant=7err_msg=''verbose=True): """""" Raise an assertion if two items are not equal up to significant digits. .. note:: It is recommended to use one of `assert_allclose` `assert_array_almost_equal_nulp` or `assert_array_max_ulp` instead of this function for more consistent floating point comparisons. Given two numbers check that they are approximately equal. Approximately equal is defined as the number of significant digits that agree. Great! This will do exactly what I need. Thank you. unittest.assertAlmostEqual() fooled me by ignoring insignificant digits.  This is a fairly common issue with floating point numbers. I solve it based on the discussion in Section 1.5 of Demmel[1]. (1) Calculate the roundoff error. (2) Check that the roundoff error is less than some epsilon. I haven't used python in some time and only have version 2.4.3 but I'll try to get this correct. Step 1. Roundoff error def roundoff_error(exact approximate): return abs(approximate/exact - 1.0) Step 2. Floating point equality def float_equal(float1 float2 epsilon=2.0e-9): return (roundoff_error(float1 float2) < epsilon) There are a couple obvious deficiencies with this code. Division by zero error if the exact value is Zero. Does not verify that the arguments are floating point values. Revision 1. def roundoff_error(exact approximate): if (exact == 0.0 or approximate == 0.0): return abs(exact + approximate) else: return abs(approximate/exact - 1.0) def float_equal(float1 float2 epsilon=2.0e-9): if not isinstance(float1float): raise TypeError""First argument is not a float."" elif not isinstance(float2float): raise TypeError""Second argument is not a float."" else: return (roundoff_error(float1 float2) < epsilon) That's a little better. If either the exact or the approximate value is zero than the error is equal to the value of the other. If something besides a floating point value is provided a TypeError is raised. At this point the only difficult thing is setting the correct value for epsilon. I noticed in the documentation for version 2.6.1 that there is an epsilon attribute in sys.float_info so I would use twice that value as the default epsilon. But the correct value depends on both your application and your algorithm. [1] James W. Demmel Applied Numerical Linear Algebra SIAM 1997.  ""Significant figures"" in decimal is a matter of adjusting the decimal point and truncating to an integer. >>> int(3.1415926 * 10**3) 3141 >>> int(1234567 * 10**-3) 1234 >>> Can you provide an algorithm to do this? Different from the code I already posted? What more do you need to know?  There is a interesting solution to this by B. Dawson (with C++ code) at ""Comparing Floating Point Numbers"". His approach relies on strict IEEE representation of two numbers and the enforced lexicographical ordering when said numbers are represented as unsigned integers.  There are already plenty of great answers but here's a think: def closeness(a b): """"""Returns measure of equality (for two floats) in unit of decimal significant figures."""""" if a == b: return float(""infinity"") difference = abs(a - b) avg = (a + b)/2 return math.log10( avg / difference ) if closeness(1000 1000.1) > 3: print ""Joy!""  I believe your question is not defined well enough and the unit-tests you present prove it: If by 'round to N sig-fig decimal places' you mean 'N decimal places to the right of the decimal point' then the test assert nearlyequal(1e9 1e9 + 1  5) should fail because even when you round 1000000000 and 1000000001 to 0.00001 accuracy they are still different. And if by 'round to N sig-fig decimal places' you mean 'The N most significant digits regardless of the decimal point' then the test assert nearlyequal(-1e-9 1e-9 5) should fail because 0.000000001 and -0.000000001 are totally different when viewed this way. If you meant the first definition then the first answer on this page (by Triptych) is good. If you meant the second definition please say it I promise to think about it :-) Actually you are right about ""assert nearlyequal(-1e-9 1e-9 5)"" - it breaks the rules! +1  There are lots of ways of comparing two numbers to see if they agree to N significant digits. Roughly speaking you just want to make sure that their difference is less than 10^-N times the largest of the two numbers being compared. That's easy enough. But what if one of the numbers is zero? The whole concept of relative-differences or significant-digits falls down when comparing against zero. To handle that case you need to have an absolute-difference as well which should be specified differently from the relative-difference. I discuss the problems of comparing floating-point numbers -- including a specific case of handling zero -- in this blog post: http://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/  Here's a take. def nearly_equal(absig_fig=5): return ( a==b or int(a*10**sig_fig) == int(b*10**sig_fig) ) +1: Beat me by 30 seconds! Haha @S.Lott - I've definitely been on the other end of that. you don't really need that if statement. you can just return result of comparison: it's either True or False it's shorter and surely more pythonic will this work for the _nearlyequal((1e91+1e9 5)_? good point @silentghost. Made the change. This fails nearlyequal(1e9 1e9 + 1  5) - it does not work for big numbers. The issue is that ""significant figures"" depends on whether the number is >1 or <1. >1 you have to use sig_fig-math.log10(n) to shift ""to the right"". If <1 you simple use sig_fig to shift ""to the left"". For the purposes of the problem the numbers can be any number which can be represented by floating-point.  Oren Shemesh got part of the problem with the problem as stated but there's more: assert nearlyequal( 0.0 1e-15 5 ) also fails the second definition (and that's the definition I learned in school.) No matter how many digits you are looking at 0 will not equal a not-zero. This could prove to be a headache for such tests if you have a case whose correct answer is zero.",python math floating-point numpy
1687566,A,"Why does an assignment for double-sliced numpy arrays not work? why do the following lines not work as I expect? import numpy as np a = np.array([01211]) a[a==1][1:] = 3 print a >>> [0 1 2 1 1] # I would expect [0 1 2 3 3] Is this a 'bug' or is there another recommended way to this? On the other hand the following works: a[a==1] = 3 print a >>> [0 3 2 3 3] Cheers Philipp It appears you simply can't do an assignment through a double-slice like that. This works though: a[numpy.where(a==1)[0][1:]] = 3  This does what you want a[2:][a[2:]==1]=3 But that requires knowing in advance that the first occurrence of 1 is at position 1.  Because the a[a==1] part isn't actually a slice. It creates a new array. It makes sense when you think about it-- you're only taking the elements that satisfy the boolean condition (like a filter operation). @tom10 - It's not a ""hack"". It's part of the implementation of the array class. Behavior is documented for instance here: http://www.scipy.org/Tentative_NumPy_Tutorial#head-864862d3f2bb4c32f04260fac61eb4ef34788c4c. ""Slicing an array returns a view of it"" i.e. not a copy. normal list works that way too -- you can assign to slices (only with iterables) `l = range(10); l[5:] = range(5)` @Dave - Certainly a slice in numpy is a view but a[a==1] is not a slice and not a view. I added a second example that works like I expect. I don't really see the difference. Shouldn't the assignment being ""piped through""? I don't think this is quite right. If you do `a[a==1] = 3` that actually changes the contents of a. @Dave - I think this is perimosocodiae is correct and that your counter-example is due to something more like a hack in the numpy internals to create the appearance of an in-place operation.  It's related to how fancy indexing works. There is a thorough explanation here. It is done this way to allow inplace modification with fancy indexing (ie a[x>3] *= 2). A consequence of this is that you can't assign to a double index as you have found. Fancy indexing always returns a copy rather than a view. Actually your solution modifies the first occurrence of 1 which is not what he wants. Right - took it off before your comment. Ps hi Philip it's Robin! Hey Robin - what chance is that to meet here... Cheers from munich!",python numpy variable-assignment slicing
1698036,A,"Convert little endian string to integer I have read samples out of a wave file using the wave module but it gives the samples as a string it's out of wave so it's little endian (for example '`\x00'). What is the easiest way to convert this into a python integer or a numpy.int16 type? (It will eventually become a numpy.int16 so going directly there is fine). Code needs to work on little endian and big endian processors. The general case is: http://stackoverflow.com/questions/444591/convert-a-string-of-bytes-into-an-int-python struct is fine if you have to convert one or a small number of 2-byte strings to integers but array and numpy itself are better options. Specifically numpy.fromstring (called with the appropriate dtype argument) can directly convert the bytes from your string to an array of (whatever that dtype is). (If numpy.little_endian is false you'll then have to swap the bytes -- see here for more discussion but basically you'll want to call the byteswap method on the array object you just built with fromstring). This is really good to know as well I'm going to go with the struct solution though so that I don't need to worry about correcting endianess manually. If you're in no hurry or have very few data points that's fine. Otherwise you _can_ codify endianness as a string as part of the `dtype` (I don't know the details offhand though).  The struct module converts packed data to Python values and vice-versa. >>> import struct >>> struct.unpack(""<h"" ""\x00\x05"") (1280) >>> struct.unpack(""<h"" ""\x00\x06"") (1536) >>> struct.unpack(""<h"" ""\x01\x06"") (1537) ""h"" means a short int or 16-bit int. ""<"" means use little-endian.",python numpy wav
1735025,A,"How to normalize a NumPy array to within a certain range? After doing some processing on an audio or image array it needs to be normalized within a range before it can be written back to a file. This can be done like so: # Normalize audio channels to between -1.0 and +1.0 audio[:0] = audio[:0]/abs(audio[:0]).max() audio[:1] = audio[:1]/abs(audio[:1]).max() # Normalize image to between 0 and 255 image = image/(image.max()/255.0) Is there a less verbose convenience function way to do this? matplotlib.colors.Normalize() doesn't seem to be related. You can also rescale using sklearn. The advantages are that you can adjust normalize the standard deviation in addition to mean-centering the data and that you can do this on either axis by features or by records. from sklearn.preprocessing import scale X = scale( X axis=0 with_mean=True with_std=True copy=True ) The keyword arguments axis with_mean with_std are self explanatory and are shown in their default state. The argument copy performs the operation in-place if it is set to False. Documentation here.  You can use the ""i"" (as in idiv imul..) version and it doesn't look half bad: image /= (image.max()/255.0) For the other case you can write a function to normalize an n-dimensional array by colums: def normalize_columns(arr): rows cols = arr.shape for col in xrange(cols): arr[:col] /= abs(arr[:col]).max() Can you clarify this? The parentheses make it behave differently than without? @endolith: I think I was first but that doesn't matter. Accept the answer you like the most. parantheses don't change anything. the point was to use `/=` instead of `= .. / .. ` Oh you just both answered with the same thing at the same time?  audio /= np.max(np.abs(audio)axis=0) image *= (255.0/image.max()) Using /= and *= allows you to eliminate an intermediate temporary array thus saving some memory. Multiplication is less expensive than division so image *= 255.0/image.max() # Uses 1 division and image.size multiplications is marginally faster than image /= image.max()/255.0 # Uses 1+image.size divisions Since we are using basic numpy methods here I think this is about as efficient a solution in numpy as can be. Why is multiplication less expensive than division? I don't know exactly why. However I am confident of the claim having checked it with timeit. With multiplication you can work with one digit at a time. With division especially with large divisors you have to work with many digits and ""guess"" how many times the divisor goes into the dividend. You end up doing many multiplication problems to solve one division problem. The computer algorithm for doing division may not be the same as human long division but nevertheless I believe it's more complicated than multiplication. Probably worth mentioning a divide by zero for blank images.",python arrays numpy scipy convenience-methods
1799527,A,Numpy - show decimal values in array results how do I calculate that an array of python numpy or me of all the calculate decimals and not skip like. >> A = numpy.array ([[123] [456] [789]]). >> C = numpy.array ([[789] [123] [456]]). >> A / C array ([[0 0 0] [4 2 2] [1 1 1]]) but in the first vector would not have to be given to absolute zero [0.143 0.250 0.333] This is an English speaking forum. Your title needs editing to be in English. Hola Ricardo el título en inglés por favor. :-) I have no idea what the question is about I hope I translated correctly :) hello excuse was a mistake not happen again Try converting one of the arrays A or C into an array of floats. For instance: A = A * 1.0 Then the division will be floating point division.  Numpy arrays may have different types. You may also create a float array it will always divide correctly: >>> A = numpy.array ([[123] [456] [789]] dtype=float) >>> A/2 array([[ 0.5 1.  1.5] [ 2.  2.5 3. ] [ 3.5 4.  4.5]]) Notice the dtype= argument to numpy.array  To avoid integer division use numpy.true_divide(AC). You can also put from __future__ import division at the top of the file to default to this behavior. thanks for response,python numpy
1550130,A,"""Cloning"" row or column vectors Sometimes it is useful to ""clone"" a row or column vector to a matrix. By cloning I mean converting a row vector such as [123] Into a matrix [[123] [123] [123] ] or a column vector such as [1 2 3 ] into [[111] [222] [333] ] In matlab or octave this is done pretty easily:  x = [123] a = ones(31) * x a = 1 2 3 1 2 3 1 2 3 b = (x') * ones(13) b = 1 1 1 2 2 2 3 3 3 I want to repeat this in numpy but unsuccessfully In [14]: x = array([123]) In [14]: ones((31)) * x Out[14]: array([[ 1. 2. 3.] [ 1. 2. 3.] [ 1. 2. 3.]]) # so far so good In [16]: x.transpose() * ones((13)) Out[16]: array([[ 1. 2. 3.]]) # DAMN # I end up with In [17]: (ones((31)) * x).transpose() Out[17]: array([[ 1. 1. 1.] [ 2. 2. 2.] [ 3. 3. 3.]]) Why wasn't the first method (In[16]) working? Is there a way to achieve this task in python in a more elegant way? In Matlab note that it is much faster to use `repmat`: `repmat([1 2 3]31)` or `repmat([1 2 3].'13)` Octave also has `repmat`. First note that with numpy's broadcasting operations it's usually not necessary to duplicate rows and columns. See this and this for descriptions. But to do this repeat and newaxis are probably the best way In [12]: x = array([123]) In [13]: repeat(x[:newaxis] 3 1) Out[13]: array([[1 1 1] [2 2 2] [3 3 3]]) In [14]: repeat(x[newaxis:] 3 0) Out[14]: array([[1 2 3] [1 2 3] [1 2 3]]) This example is for a row vector but applying this to a column vector is hopefully obvious. repeat seems to spell this well but you can also do it via multiplication as in your example In [15]: x = array([[1 2 3]]) # note the double brackets In [16]: (ones((31))*x).transpose() Out[16]: array([[ 1. 1. 1.] [ 2. 2. 2.] [ 3. 3. 3.]]) @AFoglia - Good point. I updated my answer to point this out. newaxis has the additional benefit that it doesn't actually copy the data until it needs to. So if you are doing this to multiply or add to another 3x3 array the repeat is unnecessary. Read up on numpy broadcasting to get the idea.  Here's an elegant Pythonic way to do it: >>> array([[123]]*3) array([[1 2 3] [1 2 3] [1 2 3]]) >>> array([[123]]*3).transpose() array([[1 1 1] [2 2 2] [3 3 3]]) the problem with [16] seems to be that the transpose has no effect for an array. you're probably wanting a matrix instead: >>> x = array([123]) >>> x array([1 2 3]) >>> x.transpose() array([1 2 3]) >>> matrix([123]) matrix([[1 2 3]]) >>> matrix([123]).transpose() matrix([[1] [2] [3]]) (transpose works for 2D arrays e.g. for the square one in the example or when turning into a `(N1)`-shape array using `.reshape(-1 1)`)  I think using the broadcast in numpy is the best and faster I did a compare as following import numpy as np b = np.random.randn(1000) In [105]: %timeit c = np.tile(b[: newaxis] (1100)) 1000 loops best of 3: 354 µs per loop In [106]: %timeit c = np.repeat(b[: newaxis] 100 axis=1) 1000 loops best of 3: 347 µs per loop In [107]: %timeit c = np.array([b]*100).transpose() 100 loops best of 3: 5.56 ms per loop about 15 times faster using broadcast  Use numpy.tile: >>> tile(array([123]) (3 1)) array([[1 2 3] [1 2 3] [1 2 3]]) or for repeating columns: >>> tile(array([[123]]).transpose() (1 3)) array([[1 1 1] [2 2 2] [3 3 3]]) Upvote! On my system for a vector with 10000 elements repeated 1000 times the `tile` method is 19.5 times faster than the method in the currently accepted answer (using the multiplication-operator-method).",python matlab numpy octave linear-algebra
1642730,A,"How to delete columns in numpy.array I would like to delete selected columns in a numpy.array . This is what I do: n [397]: a = array([[ NaN 2. 3. NaN] .....: [ 1. 2. 3. 9]]) In [398]: print a [[ NaN 2. 3. NaN] [ 1. 2. 3. 9.]] In [399]: z = any(isnan(a) axis=0) In [400]: print z [ True False False True] In [401]: delete(a z axis = 1) Out[401]: array([[ 3. NaN] [ 3. 9.]]) In this example my goal is to delete all the columns that contain NaN's. I expect the last command to result in: array([[2. 3.] [2. 3.]]) How can I do that? From Numpy Documentation np.delete(arr obj axis=None) Return a new array with sub-arrays along an axis deleted. >>> arr array([[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]]) >>> np.delete(arr 1 0) array([[ 1 2 3 4] [ 9 10 11 12]]) >>> np.delete(arr np.s_[::2] 1) array([[ 2 4] [ 6 8] [10 12]]) >>> np.delete(arr [135] None) array([ 1 3 5 7 8 9 10 11 12])  Another way is to use masked arrays: import numpy as np a = np.array([[ np.nan 2. 3. np.nan] [ 1. 2. 3. 9]]) print(a) # [[ NaN 2. 3. NaN] # [ 1. 2. 3. 9.]] The np.ma.masked_invalid method returns a masked array with nans and infs masked out: print(np.ma.masked_invalid(a)) [[-- 2.0 3.0 --] [1.0 2.0 3.0 9.0]] The np.ma.compress_cols method returns a 2-D array with any column containing a masked value suppressed: a=np.ma.compress_cols(np.ma.masked_invalid(a)) print(a) # [[ 2. 3.] # [ 2. 3.]] See manipulating-a-maskedarray  This creates another array without those columns:  b = a.compress(logical_not(z) axis=1) cool. I wish matlab's syntax worked here: ""a(:z) = []"" is much simpler similar: b = a[:[12]] @bpowah: indeed. the more general way would be b = a[:z]. You might want to update your answer accordingly  Given its name I think the standard way should be delete: A = scipy.delete(A 1 0) # delete second row of A B = scipy.delete(B 2 0) # delete third row of B C = scipy.delete(C 1 1) # delete second column of C  In your situation you can extract the desired data with: a[: -z] ""-z"" is the logical negation of the boolean array ""z"". This is the same as: a[: logical_not(z)]  From HERE a=array([[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]]) delete(a s_[1:3] axis=0) # remove rows 1 and 2 output: array([[ 0 1 2 3] [12 13 14 15]]) delete(a s_[1:3] axis=1) # remove columns 1 and 2 output: array([[ 0 3] [ 4 7] [ 8 11] [12 15]]) Hope that helps",python numpy scipy
1929045,A,Implementing tridiagonal matrix algorithm (TDMA) with NumPy I'm implementing TDMA in Python using NumPy. The tridiagonal matrix is stored in three arrays: a = array([...]) b = array([...]) c = array([...]) I'd like to calculate alpha-coefficients efficiently. The algorithm is as follows: # n = size of the given matrix - 1 alpha = zeros(n) alpha[0] = b[0] / c[0] for i in range(n-1): alpha[i+1] = b[i] / (c[i] - a[i] * alpha[i]) However this is not efficient because of Python's for loop. Want I want is something like this approach: # n = size of the given matrix - 1 alpha = zeros(n) alpha[0] = b[0] / c[0] alpha[1:] = b[1:] / (c[1:] - a[1:] * alpha[:-1]) In this latter case the result is incorrect because NumPy stores the right part of the last expression in a temprorary array and then assigns references to its elements to alpha[1:]. Therefore a[1:] * alpha[:-1] is just an array of zeros. Is there a way to tell NumPy to use values of alpha calculated on previous steps within its internal loop? Thanks. this isn't possible. see http://stackoverflow.com/questions/1587367/python-numpy-tricky-slicing-problem for a similar issue. if you really need the speed increase try cython. If its tridiagonal systems you want to solve there is solve_banded() in numpy.linalg. Not sure if that's what you're looking for. i don't think there is any provision for banded matrix solving  in numpy or even in scipy  there is no sp.sparse as far as i know. I think it's my bad  scipy.sparse exists(it doesn't exist on my system  it existsin docs so it must actually exist)  but not solve_banded() in numpy.linalg  may have been when present in 2009  not now.  Apparently there is no way to do this in Python without using C or its pythonic variations.,python numpy numerical-methods
318390,A,Running numpy from cygwin I am running a windows machine have installed Python 2.5. I also used the windows installer to install NumPy. This all works great when I run the Python (command line) tool that comes with Python. However if I run cygwin and then run Python from within it cannot find the numpy package. What environment variable do I need to set? What value should it be set to? Cygwin comes with its own version of Python so it's likely that you have two Python installs on your system; one that installed under Windows and one which came with Cygwin. To test this try opening a bash prompt in Cygwin and typing which python to see where the Python executable is located. If it says /cygdrive/c/Python25/python.exe or something similar then you'll know you're running the Windows executable. If you see /usr/local/bin/python or something like that then you'll know that you're running the Cygwin version. I recommend opening a DOS prompt and running Python from there when you need interactive usage. This will keep your two Python installs nicely separate (it can be very useful to have both; I do this on my own machine). Also you may have some problems running a program designed for Windows interactive console use from within a Cygwin shell. You were right! This was helpful. Thanks Note: numpy can be installed directly from the Cygwin setup.exe.  numpy built for windows is not compatible with cygwin python. You have to build it by yourself on cygwin.  You're running a separate copy of python provided by cygwin. You can run /cygdrive/c/python25/python (or wherever you installed it) to get your win32 one or just install another copy of numpy.  Ensure that PYTHONPATH has NumPy. Refer The Module Search Path (section 6.1.2) and Modifying Python's Search Path (section 4.1).,python numpy
560283,A,"How do you construct an array suitable for numpy sorting? I need to sort two arrays simultaneously or rather I need to sort one of the arrays and bring the corresponding element of its associated array with it as I sort. That is if the array is [(5 33) (4 44) (3 55)] and I sort by the first axis (labeled below dtype='alpha') then I want: [(3.0 55.0) (4.0 44.0) (5.0 33.0)]. These are really big data sets and I need to sort first ( for nlog(n) speed ) before I do some other operations. I don't know how to merge my two separate arrays though in the proper manner to get the sort algorithm working. I think my problem is rather simple. I tried three different methods: import numpy x=numpy.asarray([543]) y=numpy.asarray([334455]) dtype=[('alpha'float) ('beta'float)] values=numpy.array([(x)(y)]) values=numpy.rollaxis(values1) #values = numpy.array(values dtype=dtype) #a=numpy.array(valuesdtype=dtype) #q=numpy.sort(aorder='alpha') print ""Try 1:\n"" values values=numpy.empty((len(x)2)) for n in range (len(x)): values[n][0]=y[n] values[n][1]=x[n] print ""Try 2:\n"" values #values = numpy.array(values dtype=dtype) #a=numpy.array(valuesdtype=dtype) #q=numpy.sort(aorder='alpha') ### values = [(x[0] y[0]) (x[1]y[1])  (x[2]y[2])] print ""Try 3:\n"" values values = numpy.array(values dtype=dtype) a=numpy.array(valuesdtype=dtype) q=numpy.sort(aorder='alpha') print ""Result:\n""q I commented out the first and second trys because they create errors I knew the third one would work because that was mirroring what I saw when I was RTFM. Given the arrays x and y (which are very large just examples shown) how do I construct the array (called values) that can be called by numpy.sort properly? *** Zip works great thanks. Bonus question: How can I later unzip the sorted data into two arrays again? I think you just need to specify the axis that you are sorting on when you have made your final ndarray. Alternatively argsort one of the original arrays and you'll have an index array that you can use to look up in both x and y which might mean you don't need values at all. (scipy.org seems to be unreachable right now or I would post you a link to some docs) Given that your description doesn't quite match your code snippet it's hard to say with certainty but I think you have over-complicated the creation of your numpy array.  I couldn't get a working solution using Numpy's sort function but here's something else that works: import numpy x = [543] y = [334455] r = numpy.asarray([(x[i]y[i]) for i in numpy.lexsort([x])]) lexsort returns the permutation of the array indices which puts the rows in sorted order. If you wanted your results sorted on multiple keys e.g. by x and then by y use numpy.lexsort([xy]) instead.  Simon suggested argsort as an alternative approach; I'd recommend it as the way to go. No messy merging zipping or unzipping: just access by index. idx = numpy.argsort(x) ans = [ (x[idx[i]]y[idx[i]]) for i in idx]  for your bonus question -- zip actually unzips too: In [1]: a = range(10) In [2]: b = range(10 20) In [3]: c = zip(a b) In [4]: c Out[4]: [(0 10) (1 11) (2 12) (3 13) (4 14) (5 15) (6 16) (7 17) (8 18) (9 19)] In [5]: d e = zip(*c) In [6]: d e Out[6]: ((0 1 2 3 4 5 6 7 8 9) (10 11 12 13 14 15 16 17 18 19))  zip() might be inefficient for large arrays. numpy.dstack() could be used instead of zip: ndx = numpy.argsort(x) values = numpy.dstack((x[ndx] y[ndx])) it actually turns out np.take(xndx) is faster than fancy indexing. [http://wesmckinney.com/blog/?p=215](http://wesmckinney.com/blog/?p=215) @user545424: looks interesting. thanks for the link thankyou as far as I can tell this is the fastest answer given (I haven't done any speed tests but I really doubt that building a zip and then unzipping it is the fastest)  I think what you want is the zip function. If you have x = [123] y = [456] then zip(xy) == [(14)(25)(36)] So your array could be constructed using a = numpy.array(zip(xy) dtype=dtype) +1: zip works well with generators so you don't have to create giant in-memory lists you can use generator functions instead. How can I later unzip the sorted data into two arrays again? once you have `a` above you can do `c d = zip(*a)` to unzip.",python algorithm arrays numpy
362489,A,How do I add a guard ring to a matrix in NumPy? Using NumPy a matrix A has n rows and m columns and I want add a guard ring to matrix A. That guard ring is all zero. What should I do? Use Reshape? But the element is not enough to make a n+1 m+1 matrix. Or etc.? Thanks in advance I mean an extra ring of cells that always contain 0 surround matrix A.Basically there is a Matrix B has n+2rows m+2columns where the first row and columns and the last row and columns are all zeroand the rest of it are same as matrix A. This is a less general but easier to understand version of Alex's answer: >>> a = numpy.array(range(9)).reshape((33)) >>> a array([[0 1 2] [3 4 5] [6 7 8]]) >>> b = numpy.zeros(a.shape + numpy.array(2) a.dtype) >>> b array([[0 0 0 0 0] [0 0 0 0 0] [0 0 0 0 0] [0 0 0 0 0] [0 0 0 0 0]]) >>> b[1:-11:-1] = a >>> b array([[0 0 0 0 0] [0 0 1 2 0] [0 3 4 5 0] [0 6 7 8 0] [0 0 0 0 0]])  Following up on your comment: >>> import numpy >>> a = numpy.array(range(9)).reshape((33)) >>> b = numpy.zeros(tuple(s+2 for s in a.shape) a.dtype) >>> b[tuple(slice(1-1) for s in a.shape)] = a >>> b array([[0 0 0 0 0] [0 0 1 2 0] [0 3 4 5 0] [0 6 7 8 0] [0 0 0 0 0]]) Wanted to mention that at least with the version of NumPy (1.2.0 release) and Python (2.5.2) I have the conversion to tuple is unnecessary and omitting it appeared to save time in my own limited testing. I forgot you could use slices like this!! Python is like the Feng Shui of programming. And by 'omitting' I mean using [] instead of tuple(). Oops :),python numpy
1201817,A,"Adding a field to a structured numpy array What is the cleanest way to add a field to a structured numpy array? Can it be done destructively or is it necessary to create a new array and copy over the existing fields? Are the contents of each field stored contiguously in memory so that such copying can be done efficiently? def add_field(a descr): """"""Return a new array that is like ""a"" but has additional fields. Arguments: a -- a structured numpy array descr -- a numpy type description of the new fields The contents of ""a"" are copied over to the appropriate fields in the new array whereas the new fields are uninitialized. The arguments are not modified. >>> sa = numpy.array([(1 'Foo') (2 'Bar')] \ dtype=[('id' int) ('name' 'S3')]) >>> sa.dtype.descr == numpy.dtype([('id' int) ('name' 'S3')]) True >>> sb = add_field(sa [('score' float)]) >>> sb.dtype.descr == numpy.dtype([('id' int) ('name' 'S3') \ ('score' float)]) True >>> numpy.all(sa['id'] == sb['id']) True >>> numpy.all(sa['name'] == sb['name']) True """""" if a.dtype.fields is None: raise ValueError ""`A' must be a structured numpy array"" b = numpy.empty(a.shape dtype=a.dtype.descr + descr) for name in a.dtype.names: b[name] = a[name] return b  If you're using numpy 1.3 there's also numpy.lib.recfunctions.append_fields(). For many installations you'll need to import numpy.lib.recfunctions to access this. import numpy will not allow one to see the numpy.lib.recfunctions",python numpy
773030,A,"Why are 0d arrays in Numpy not considered scalar? Surely a 0d array is scalar but Numpy does not seem to think so... am I missing something or am I just misunderstanding the concept? >>> foo = numpy.array(1.11111111111 numpy.float64) >>> numpy.ndim(foo) 0 >>> numpy.isscalar(foo) False >>> foo.item() 1.11111111111 One should not think too hard about it. It's ultimately better for the mental health and longevity of the individual. The curious situation with Numpy scalar-types was bore out of the fact that there is no graceful and consistent way to degrade the 1x1 matrix to scalar types. Even though mathematically they are the same thing they are handled by very different code. If you've been doing any amount of scientific code ultimately you'd want things like max(a) to work on matrices of all sizes even scalars. Mathematically this is a perfectly sensible thing to expect. However for programmers this means that whatever presents scalars in Numpy should have the .shape and .ndim attirbute so at least the ufuncs don't have to do explicit type checking on its input for the 21 possible scalar types in Numpy. On the other hand they should also work with existing Python libraries that does do explicit type-checks on scalar type. This is a dilemma since a Numpy ndarray have to individually change its type when they've been reduced to a scalar and there is no way of knowing whether that has occurred without it having do checks on all access. Actually going that route would probably make bit ridiculously slow to work with by scalar type standards. The Numpy developer's solution is to inherit from both ndarray and Python scalars for its own scalary type so that all scalars also have .shape .ndim .T etc etc. The 1x1 matrix will still be there but its use will be discouraged if you know you'll be dealing with a scalar. While this should work fine in theory occasionally you could still see some places where they missed with the paint roller and the ugly innards are exposed for all to see: >>> from numpy import * >>> a = array(1) >>> b = int_(1) >>> a.ndim 0 >>> b.ndim 0 >>> a[...] array(1) >>> a[()] 1 >>> b[...] array(1) >>> b[()] array(1) There's really no reason why a[...] and a[()] should return different things but it does. There are proposals in place to change this but looks like they forgot to finish the job for 1x1 arrays. A potentially bigger and possibly non-resolvable issue is the fact that Numpy scalars are immutable. Therefore ""spraying"" a scalar into a ndarray mathematically the adjoint operation of collapsing an array into a scalar is a PITA to implement. You can't actually grow a Numpy scalar it cannot by definition be cast into an ndarray even though newaxis mysteriously works on it: >>> b[0123] = 1 Traceback (most recent call last): File ""<stdin>"" line 1 in <module> TypeError: 'numpy.int32' object does not support item assignment >>> b[newaxis] array([1]) In Matlab growing the size of a scalar is a perfectly acceptable and brainless operation. In Numpy you have to stick jarring a = array(a) everywhere you think you'd have the possibility of starting with a scalar and ending up with an array. I understand why Numpy has to be this way to play nice with Python but that doesn't change the fact that many new switchers are deeply confused about this. Some have explicit memory of struggling with this behaviour and eventually persevering while others who are too far gone are generally left with some deep shapeless mental scar that frequently haunts their most innocent dreams. It's an ugly situation for all. +1 for the philosophical introduction :-) Have you considered a writing side career?  You have to create the scalar array a little bit differently: >>> x = numpy.float64(1.111) >>> x 1.111 >>> numpy.isscalar(x) True >>> numpy.ndim(x) 0 It looks like scalars in numpy may be a bit different concept from what you may be used to from a purely mathematical standpoint. I'm guessing you're thinking in terms of scalar matricies?",python numpy
962343,A,"How to use numpy with 'None' value in python? i'm a pretty new user of python and numpy so i hope my question won't annoy you. Well i'd like to calculate the mean of an array in python in this form : Matrice = [1 2 None] I'd just like to have my None value ignored by the numpy.mean calculation but i can't figure out how to do it. If anybody can help me that would be great! PS : sorry for my poor english. +1: this question can be particularly relevant for arrays that are imported from a database where values can sometimes be NULL. You can use scipy for that: import scipy.stats.stats as st m=st.nanmean(vec) Thanks this is just what I needed! Yes you are right it works on numpy.nan not on None. It's most useful when calculating the mean on numpy vector. This doesn't work. `a = [12None]` and then `st.nanmean(a)` results in a TypeError.  You can also use filter pass None to it it will filter non True objects also 0 :D So use it when you dont need 0 too. >>> filter(None[1 2 None]) [1 2]  You are looking for masked arrays. Here's an example. import MA a = MA.array([1 2 None] mask = [0 0 1]) print ""average ="" MA.average(a) Unfortunately masked arrays aren't thoroughly supported in numpy so you've got to look around to see what can and can't be done with them. a member function that helped a lot was `filled`. that brought the masked array back to a normal array filled with a value that I would recognize as invalid (NaN -9999 whatever your users need).  haven't used numpy but in standard python you can filter out None using list comprehensions or the filter function >>> [i for i in [1 2 None] if i != None] [1 2] >>> filter(lambda x: x != None [1 2 None]) [1 2] and then average the result to ignore the None `x != None` is usually written `x is not None` (PEP 8: ""Comparisons to singletons like None should always be done with 'is' or 'is not' never the equality operators."")  You might also be able to kludge with values like NaN or Inf. In [1]: array([1 2 None]) Out[1]: array([1 2 None] dtype=object) In [2]: array([1 2 NaN]) Out[2]: array([ 1. 2. NaN]) Actually it might not even be a kludge. Wikipedia says: NaNs may be used to represent missing values in computations. Actually this doesn't work for the mean() function though so nevermind. :) In [20]: mean([1 2 NaN]) Out[20]: nan Actually `mean(a[~isnan(a)])` explicitly choosing all non-NaN values works. @kaizer your comment is a gem. great solution thanks!",python numpy mean
1761419,A,Matlab-like structure 'Cell Array' in Numpy? i try to use create Cell Array in Numpy Anyone have an Information ? Matlab cell arrays are most similar to Python lists since they can hold any object - but scipy.io.loadmat imports them as numpy object arrays - which is an array with dtype=object. To be honest though you are just as well off using Python lists - if you are holding general objects you will loose almost all of the advantages of numpy arrays (which are designed to hold a sequence of values which each take the same amount of memory). Thank thrope!:) The one thing that I am missing by using python lists is the possibility of indexing the array with an array of indexes. Thanks for making me discover numpy object arrays :),python matlab numpy
1704823,A,"Initializing numpy matrix to something other than zero or one I have the following code: r = numpy.zeros(shape = (width height 9)) It creates a width x height x 9 matrix filled with zeros. Instead I'd like to know if there's a function or way to initialize them instead to NaN. Is there any? Without having to resort to manually doing loops and such? Thanks One caveat is that NumPy doesn't have an integer NA value (unlike R). See [pandas list of gotchas](http://pandas.pydata.org/pandas-docs/stable/gotchas.html). Hence `np.nan` goes wrong when converted to int. Are you familiar with numpy.nan? You can create your own method such as: def nans(shape dtype=float): a = numpy.empty(shape dtype) a.fill(numpy.nan) return a Then nans([34]) would output array([[ NaN NaN NaN NaN] [ NaN NaN NaN NaN] [ NaN NaN NaN NaN]]) I found this code in a mailing list thread.  You rarely need loops for vector operations in numpy. You can create an uninitialized array and assign to all entries at once: >>> a = numpy.empty((33)) >>> a[:] = numpy.NAN >>> a array([[ NaN NaN NaN] [ NaN NaN NaN] [ NaN NaN NaN]]) I have timed the alternatives a[:] = numpy.nan here and a.fill(numpy.nan) as posted by Blaenk: $ python -mtimeit ""import numpy as np; a = np.empty((100100));"" ""a.fill(np.nan)"" 10000 loops best of 3: 54.3 usec per loop $ python -mtimeit ""import numpy as np; a = np.empty((100100));"" ""a[:] = np.nan"" 10000 loops best of 3: 88.8 usec per loop The timings show a preference for ndarray.fill(..) as the faster alternative. OTOH I like numpy's convenience implementation where you can assign values to whole slices at the time the code's intention is very clear. I agree that your code's intention is clearer. But thanks for the unbiased timings (or rather the fact that you still posted them) I appreciate it :) I like this one: `a = numpy.empty((3 3)) * numpy.nan`. It timed faster than `fill` but slower than the assignment method but it is a oneliner!! Please look at this answer: http://stackoverflow.com/questions/10871220/making-a-matrix-square-and-padding-it-with-desired-value-in-numpy I prefer the `.fill()` method but the difference in speeds reduces to practically nothing as the arrays get larger.",python numpy
1408311,A,NumPy array slice using None This had me scratching my head for a while. I was unintentionally slicing an array with None and getting something other than an error (I expected an error). Instead it returns an array with an extra dimension. >>> import numpy >>> a = numpy.arange(4).reshape(22) >>> a array([[0 1] [2 3]]) >>> a[None] array([[[0 1] [2 3]]]) Is this behavior intentional or a side-effect? If intentional is there some rationale for it? Using None is equivalent to using numpy.newaxis so yes it's intentional. In fact they're the same thing but of course newaxis spells it out better. The docs A related SO question.,python arrays numpy
1871536,A,"Euclidean distance between points in two different Numpy arrays not within I have two arrays of x-y coordinates and I would like to find the minimum Euclidean distance between each point in one array with all the points in the other array. The arrays are not necessarily the same size. For example: xy1=numpy.array( [[ 243 3173] [ 525 2997]]) xy2=numpy.array( [[ 682 2644] [ 277 2651] [ 396 2640]]) My current method loops through each coordinate xy in xy1 and calculates the distances between that coordinate and the other coordinates. mindist=numpy.zeros(len(xy1)) minid=numpy.zeros(len(xy1)) for ixy in enumerate(xy1): dists=numpy.sqrt(numpy.sum((xy-xy2)**2axis=1)) mindist[i]minid[i]=dists.min()dists.argmin() Is there a way to eliminate the for loop and somehow do element-by-element calculations between the two arrays? I envision generating a distance matrix for which I could find the minimum element in each row or column. Another way to look at the problem. Say I concatenate xy1 (length m) and xy2 (length p) into xy (length n) and I store the lengths of the original arrays. Theoretically I should then be able to generate a n x n distance matrix from those coordinates from which I can grab an m x p submatrix. Is there a way to efficiently generate this submatrix? If you need to speed up your code you should remove the unnecessary numpy.sqrt (and only take the square root of the minimum squared distance when you have found it). For what you're trying to do: dists = numpy.sqrt((xy1[: 0 numpy.newaxis] - xy2[: 0])**2 + (xy1[: 1 numpy.newaxis - xy2[: 1])**2) mindist = numpy.min(dists axis=1) minid = numpy.argmin(dists axis=1) Edit: Instead of calling sqrt doing squares etc. you can use numpy.hypot: dists = numpy.hypot(xy1[: 0 numpy.newaxis]-xy2[: 0] xy1[: 1 numpy.newaxis]-xy2[: 1]) Oh my that's amazing. I did not realize that element-by-element could work that way too. So `xy1[:0numpy.newaxis]` effectively replaces my for loop by being a column vector from which all the *x*-values of `xy2` are subtracted. Very cool thank you. Yes. For a more general and elegant method see Alex's answer. @fideli: help(numpy.subtract.outer) tells you that the numpy.newaxis trick of Alok is what is also at work in Alex's answer.  (Months later) scipy.spatial.distance.cdist( X Y ) gives all pairs of distances for X and Y 2 dim 3 dim ... It also does 22 different norms detailed here . # cdist example: (nxdim) (nydim) -> (nxny) from __future__ import division import sys import numpy as np from scipy.spatial.distance import cdist #............................................................................... dim = 10 nx = 1000 ny = 100 metric = ""euclidean"" seed = 1 # change these params in sh or ipython: run this.py dim=3 ... for arg in sys.argv[1:]: exec( arg ) np.random.seed(seed) np.set_printoptions( 2 threshold=100 edgeitems=10 suppress=True ) title = ""%s dim %d nx %d ny %d metric %s"" % ( __file__ dim nx ny metric ) print ""\n"" title #............................................................................... X = np.random.uniform( 0 1 size=(nxdim) ) Y = np.random.uniform( 0 1 size=(nydim) ) dist = cdist( X Y metric=metric ) # -> (nx ny) distances #............................................................................... print ""scipy.spatial.distance.cdist: X %s Y %s -> %s"" % ( X.shape Y.shape dist.shape ) print ""dist average %.3g +- %.2g"" % (dist.mean() dist.std()) print ""check: dist[03] %.3g == cdist( [X[0]] [Y[3]] ) %.3g"" % ( dist[03] cdist( [X[0]] [Y[3]] )) # (trivia: how do pairwise distances between uniform-random points in the unit cube # depend on the metric ? With the right scaling not much at all: # L1 / dim ~ .33 +- .2/sqrt dim # L2 / sqrt dim ~ .4 +- .2/sqrt dim # Lmax / 2 ~ .4 +- .2/sqrt dim Hello I actually just came across this last week. Much faster too! @LWZ just what you have -- `np.array([ dist( x y ) for x y in zip( X Y )])` @denis cdist calculate distances between ALL pairs. How can I distance only between corresponding elements for example `[ dist(X[0]Y[0]) dist(X[1]Y[1]) ... dist(X[N]Y[N]) ]` assuming `X` and `Y` are of same length `N`? This works! And pretty fast. Just a note that the elements to calculate must be of length 2 otherwise python will raise an error. For a list of points in a contour detected by opencv2 I needed to use reshape function of numpy to reshape it first... @denis: well e.g. the result of cv2.findContours is something like this: [ [[x1 y1]] [[x2 y2]].....[[xn yn]] ]. I don't know why they put the coordinates in 2 square bracket but if you apply cdist directly to the list each element will only have length 1 (a list containing 1 list inside) I have to reshape it so the contour become the list of length-2 element (which means flat out the double bracket) @Jim Raynor right -- cdist expects arrays with ndim == 2 should check can you post an example of how to use it? there are none in the docs  To compute the m by p matrix of distances this should work: >>> def distances(xy1 xy2): ... d0 = numpy.subtract.outer(xy1[:0] xy2[:0]) ... d1 = numpy.subtract.outer(xy1[:1] xy2[:1]) ... return numpy.hypot(d0 d1) the .outer calls make two such matrices (of scalar differences along the two axes) the .hypot calls turns those into a same-shape matrix (of scalar euclidean distances). +1: just learned about the properties of Numpy's ufuncs! This method is faster I would go for cdist in this case but +1'd and I've learnt from this solution cool stuff Faster it's numpy!",python numpy euclidean-distance
643699,A,"How can I use numpy.correlate to do autocorrelation? I need to do auto-correlation of a set of numbers which as I understand it is just the correlation of the set with itself. I've tried it using numpy's correlate function but I don't believe the result as it almost always gives a vector where the first number is not the largest as it ought to be. So this question is really two questions: What exactly is numpy.correlate doing? How can I use it (or something else) to do auto-correlation? See also: http://stackoverflow.com/questions/12269834/is-there-any-numpy-autocorrellation-function-with-standardized-output for information about normalized autocorrelation. Auto-correlation comes in two versions: statistical and convolution. They both do the same except for a little detail: The former is normalized to be on the interval [-11]. Here is an example of how you do the statistical one: def acf(x length=20): return numpy.array([1]+[numpy.corrcoef(x[:-i] x[i:]) \ for i in range(1 length)]) You want ``numpy.corrcoef[x:-i] x[i:])[01]`` in the second line as the return value of ``corrcoef`` is a 2x2 matrix  1) Here is the documentation for numpy.correlate. The code inside the file looks like this: mode = _mode_from_name(mode) return multiarray.correlate(avmode) multiarray.correlate points to a .pyd file (i.e. a DLL file) so to get the inner workings you should probably ask the numpy developers. 2) If you don't believe the numpy results you might try SciPy's correlate function. The scipy and numpy correlate functions are both in C. numpy multiarray source code is somewhere in here: http://svn.scipy.org/svn/numpy/trunk/numpy/core/src/multiarray/ and scipy correlate source code is here: http://svn.scipy.org/svn/scipy/trunk/scipy/signal/correlate_nd.c.src  To answer your first question Numpy.correlate(a v mode) is performing the convolution of a with the reverse of v and giving the results clipped by the specified mode. Because of the definition of convolution the correlation C(t) = Sum for -inf < i < inf of (a[i] * v[t + i]) where -inf < t < inf. Even though this definition of the correlation would allow for results from -infinity to infinity you obviously can't store an infinitely long array. So it has to be clipped and that is where the mode comes in. There are 3 different modes: full same & valid. 'full' mode returns results for every t where both a and v have some overlap. 'same' mode returns a result with the same length as the shortest vector (a or v). 'valid' mode returns results only when a and v completely overlap each other. The documentation for Numpy.convolve gives more detail on the modes. For your second question I think Numpy.correlate is giving you the autocorrelation it is just giving you a little more as well. The autocorrelation is used to find how similar a signal or function is to itself at a certain time difference. At a time difference of 0 the auto-correlation should be the highest because the signal is identical to itself so you expected that the first element in the auto-correlation result array would be the greatest. However the correlation is not starting at a time difference of 0. It starts at a negative time difference closes to 0 and then goes positive. That is you were expecting: Autocorrelation(a) = Sum for -inf < i < inf (a[i] * v[t + i]) where 0 <= t < inf But what you got was: Autocorrelation(a) = Sum for -inf < i < inf (a[i] * v[t + i]) where -inf < t < inf What you need to do is take the last half of your correlation result and that should be the auto-correlation you are looking for. A simple python function to do that would be: def autocorr(x): result = numpy.correlate(x x mode='full') return result[result.size/2:] You will of course need error checking to make sure that x is actually a 1-d array. Also this explanation probably isn't the most mathematically rigorous. I've been throwing around infinities because the definition of convolution uses them but that doesn't necessarily apply for auto-correlation. So the theoretical portion of this explanation may be slightly wonky but hopefully the practical results are helpful. These pages on auto-correlation are pretty helpful and can give you a much better theoretical background if you don't mind wading through the notation and heavy concepts. In current builds of numpy the mode 'same' can be specified to achieve exactly what the A. Levy proposed. The body of the function could then read `return numpy.correlate(x x mode='same')` @DavidZwicker but the resultings are different! `np.correlate(xxmode='full')[len(x)//2:] != np.correlate(xxmode='same')`. For example `x = [12312]; np.correlate(xxmode='full');` {`>>> array([ 2 5 11 13 19 13 11 5 2])`} `np.correlate(xxmode='same');` {`>>> array([11 13 19 13 11])`}. The correct one is: `np.correlate(xxmode='full')[len(x)-1:];` {`>>> array([19 13 11 5 2])`} see **the first item** is **the largest one**. Note that this answer gives the unnormalized autocorrelation.  As I just ran into the same problem I would like to share a few lines of code with you. In fact there are several rather similar posts about autocorrelation in stackoverflow by now. If you define the autocorrelation as a(x L) = sum(k=0N-L-1)((xk-xbar)*(x(k+L)-xbar))/sum(k=0N-1)((xk-xbar)**2) [this is the definition given in IDL's a_correlate function and it agrees with what I see in answer 2 of question #12269834] then the following seems to give the correct results: import numpy as np import matplotlib.pyplot as plt # generate some data x = np.arange(0.6.120.01) y = np.sin(x) # y = np.random.uniform(size=300) yunbiased = y-np.mean(y) ynorm = np.sum(yunbiased**2) acor = np.correlate(yunbiased yunbiased ""same"")/ynorm # use only second half acor = acor[len(acor)/2:] plt.plot(acor) plt.show() As you see I have tested this with a sin curve and a uniform random distribution and both results look like I would expect them. Note that I used mode=""same"" instead of mode=""full"" as the others did.  Using the numpy.corrcoef function instead of numpy.correlate to calculate the statistical correlation for a lag of t: def autocorr(x t=1): numpy.corrcoef(numpy.array([x[0:len(x)-t] x[t:len(x)]]))",python math numpy numerical-methods
1903462,A,"How can I ""zip sort"" parallel numpy arrays? If I have two parallel lists and want to sort them by the order of the elements in the first it's very easy: >>> a = [2 3 1] >>> b = [4 6 2] >>> a b = zip(*sorted(zip(ab))) >>> print a (1 2 3) >>> print b (2 4 6) How can I do the same using numpy arrays without unpacking them into conventional Python lists? @YGA will your input array ""a"" ever have non-unique values? If so how would you like the sort to behave in that case? Arbitrary order? Stable sort? Secondary sort using corresponding values in array ""b""? Your question is the answer to my question -- I was searching for that kind of sort. b[a.argsort()] should do the trick. Here's how it works. First you need to find a permutation that sorts a. argsort is a method that computes this: >>> a = numpy.array([2 3 1]) >>> p = a.argsort() >>> p [2 0 1] You can easily check that this is right: >>> a[p] array([1 2 3]) Now apply the same permutation to b. >>> b = numpy.array([4 6 2]) >>> b[p] array([2 4 6]) This doesn't use `b` for ""auxiliary sorting"" for example when `a` has elements that repeat. Please see my answer for details. otoh auxiliary sorting is not always desired.  Here's an approach that creates no intermediate Python lists though it does require a NumPy ""record array"" to use for the sorting. If your two input arrays are actually related (like columns in a spreadsheet) then this might open up an advantageous way of dealing with your data in general rather than keeping two distinct arrays around all the time in which case you'd already have a record array and your original problem would be answered merely by calling sort() on your array. This does an in-place sort after packing both arrays into a record array: >>> from numpy import array rec >>> a = array([2 3 1]) >>> b = array([4 6 2]) >>> c = rec.fromarrays([a b]) >>> c.sort() >>> c.f1 # fromarrays adds field names beginning with f0 automatically array([2 4 6]) Edited to use rec.fromarrays() for simplicity skip redundant dtype use default sort key use default field names instead of specifying (based on this example). Thanks! I really wish I could accept two answers. This one is less simple but more general. I've upvoted it though as the least I could do :-)  This might the simplest and most general way to do what you want. (I used three arrays here but this will work on arrays of any shape whether two columns or two hundred). import numpy as NP fnx = lambda : NP.random.randint(0 10 6) a b c = fnx() fnx() fnx() abc = NP.column_stack((a b c)) keys = (abc[:0] abc[:1]) # sort on 2nd column resolve ties using 1st col indices = NP.lexsort(keys) # create index array ab_sorted = NP.take(abc indices axis=0) One quirk w/ lexsort is that you have to specify the keys in reverse order i.e. put your primary key second and your secondary key first. In my example i want to sort using the 2nd column as the primary key so i list it second; the 1st column resolves ties only but it is listed first). nice catch Brendan thanks.",python sorting numpy
1613249,A,"NumPy: Comparing Elements in Two Arrays Anyone ever come up to this problem? Let's say you have two arrays like the following a = array([123456]) b = array([145]) Is there a way to compare what elements in a exist in b? For example c = a == b # Wishful example here print c array([145]) # Or even better array([True False False True True False]) I'm trying to avoid loops as it would take ages with millions of elements. Any ideas? Cheers what data do you have in the arrays? Index-like unique integers like the example? Numpy has a set function numpy.setmember1d() that works on sorted and uniqued arrays and returns exactly the boolean array that you want. If the input arrays don't match the criteria you'll need to convert to the set format and invert the transformation on the result. import numpy as np a = np.array([6123456]) b = np.array([145]) # convert to the uniqued form a_set a_inv = np.unique1d(a return_inverse=True) b_set = np.unique1d(b) # calculate matching elements matches = np.setmea_set b_set) # invert the transformation result = matches[a_inv] print(result) # [False True False False True True False] Edit: Unfortunately the setmember1d method in numpy is really inefficient. The search sorted and assign method you proposed works faster but if you can assign directly you might as well assign directly to the result and avoid lots of unnecessary copying. Also your method will fail if b contains anything not in a. The following corrects those errors: result = np.zeros(a.shape dtype=np.bool) idxs = a.searchsorted(b) idxs = idxs[np.where(idxs < a.shape[0])] # Filter out out of range values idxs = idxs[np.where(a[idxs] == b)] # Filter out where there isn't an actual match result[idxs] = True print(result) My benchmarks show this at 91us vs. 6.6ms for your approach and 109ms for numpy setmember1d on 1M element a and 100 element b. That's a nice solution. I'll try out your suggestion and what I just wrote to see what's more optimal in speed. Many thanks everyone for your help! The method I wrote is a bit faster. For a 10000 element array the time it took using timeit in iPython is roughly 3 µs. The setmember1d method took 3 ms. I think your method is more elegant but I need the speed. you forgot to close a parenthesis in the 3rd line. you should fix it before some computer science professor notices it... ebressert: seems that you're right setmember1d has an absolutely terrible implementation in numpy. But the method you're using seems to be using nan values for no good reason you might just as well use the result array directly. I'll edit with the corresponding example. Ants Aasma: Your edit is good. I implemented pieces of it to my code and increased the speed once more. Rather than doing nans I put in -1 and then filtered on match = b >= 0. I'm dealing with indexing in my case so there are no indexes of -1. That's why I used np.nan which would work for the more general case. Thanks for your input. My code is really flying now.  Your example implies set-like behavior caring more about existance in the array than having the right element at the right place. Numpy does this differently with its mathematical arrays and matrices it will tell you only about items at the exact right spot. Can you make that work for you? >>> import numpy >>> a = numpy.array([123]) >>> b = numpy.array([133]) >>> a == b array([ True False True] dtype=bool) sorry this example doesn't work if you try it; moreover you would have to sort the arrays first. @dalloligom: Uh I copied from my interactive session so at least it works exactly like that for some version of Python and Numpy. ok but it doesn't work if the two arrays have different length; in any case you have to sort them first (try array([123])==array([231]). he wants to know which elements of an array exists in another. and by the way even sorting the arrays won't work... you have to use a set structure. @dalloliogm: Did you read my answer? Does it seem like I didn't understand all that?  Thanks for your reply kaizer.se. It's not quite what I was looking for but with a suggestion from a friend and what you said I came up with the following. import numpy as np a = np.array([145]).astype(np.float32) b = np.arange(10).astype(np.float32) # Assigning matching values from a in b as np.nan b[b.searchsorted(a)] = np.nan # Now generating Boolean arrays match = np.isnan(b) nonmatch = match == False It's a bit of a cumbersome process but it beats writing loops or using weave with loops. Cheers  Use np.intersect1d. #!/usr/bin/env python import numpy as np a = np.array([123456]) b = np.array([145]) c=np.intersect1d(ab) print(c) # [1 4 5] Note that np.intersect1d gives the wrong answer if a or b have nonunique elements. In that case use np.intersect1d_nu. There is also np.setdiff1d setxor1d setmember1d and union1d. See Numpy Example List With Doc +1: Excellent. The right function for this task.  Actually there's an even simpler solution than any of these: import numpy as np a = array([123456]) b = array([145]) c = np.in1d(ab) The resulting c is then: array([ True False False True True False] dtype=bool) Is there an ""almost_equal"" version of this? Where you can specify the condition used to test for equality?  ebresset your answer won't work unless a is a subset of b (and a and b are sorted). Otherwise the searchsorted will return false indices. I had to do something similar and combining that with your code: # Assume a and b are sorted idxs = numpy.mod(b.searchsorted(a)len(b)) idxs = idxs[b[idxs]==a] b[idxs] = numpy.nan match = numpy.isnan(b)",python numpy
1477144,A,"Compile Matplotlib for Python on Snow Leopard I've killed half a day trying to compile matplotlib for python on Snow Leopard. I've used the googles and found this helpful page (http://blog.hyperjeff.net/?p=160) but I still can't get it to compile. I see comments from other users on that page so I know I'm not alone. I already installed zlib libpng and freetype independently. I edited the make.osx file to contain this at the top: PREFIX=/usr/local PYVERSION=2.6 PYTHON=python${PYVERSION} ZLIBVERSION=1.2.3 PNGVERSION=1.2.33 FREETYPEVERSION=2.3.5 MACOSX_DEPLOYMENT_TARGET=10.6 ## You shouldn't need to configure past this point PKG_CONFIG_PATH=""${PREFIX}/lib/pkgconfig"" CFLAGS=""-Os -arch x86_64 -arch i386 -I${PREFIX}/include"" LDFLAGS=""-arch x86_64 -arch i386 -L${PREFIX}/lib"" CFLAGS_DEPS=""-arch i386 -arch x86_64 -I${PREFIX}/include -I${PREFIX}/include/freetype2 -isysroot /Developer/SDKs/MacOSX10.6.sdk"" LDFLAGS_DEPS=""-arch i386 -arch x86_64 -L${PREFIX}/lib -syslibroot/Developer/SDKs/MacOSX10.6.sdk"" I then run: sudo make -f make.osx mpl_build which gives me: export PKG_CONFIG_PATH=""/usr/local/lib/pkgconfig"" &&\ export MACOSX_DEPLOYMENT_TARGET=10.6 &&\ export CFLAGS=""-Os -arch x86_64 -arch i386 -I/usr/local/include"" &&\ export LDFLAGS=""-arch x86_64 -arch i386 -L/usr/local/lib"" &&\ python2.6 setup.py build ... snip ... gcc-4.2 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -Os -arch x86_64 -arch i386 -I/usr/local/include -pipe -DPY_ARRAYAUNIQUE_SYMBOL=MPL_ARRAY_API -I/Library/Python/2.6/site-packages/numpy/core/include -I. -I/Library/Python/2.6/site-packages/numpy/core/include/freetype2 -I./freetype2 -I/System/Library/Frameworks/Python.framework/Versions/2.6/include/python2.6 -c src/ft2font.cpp -o build/temp.macosx-10.6-universal-2.6/src/ft2font.o cc1plus: warning: command line option ""-Wstrict-prototypes"" is valid for C/ObjC but not for C++ In file included from src/ft2font.h:13 from src/ft2font.cpp:1: /usr/local/include/ft2build.h:56:38: error: freetype/config/ftheader.h: No such file or directory ... snip ... src/ft2font.cpp:98: error: ‘FT_Int’ was not declared in this scope /Library/Python/2.6/site-packages/numpy/core/include/numpy/__multiarray_api.h:1174: warning: ‘int _import_array()’ defined but not used lipo: can't open input file: /var/tmp//ccDOGx37.out (No such file or directory) error: command 'gcc-4.2' failed with exit status 1 make: *** [mpl_build] Error 1 I'm just lost. `make.osx` will automatically download freetype/libpng/zlib. You should really ask this on the matplotlib-users mailing list. It's monitored by actual matplotlib developers which StackOverflow (AFAIK) isn't.  as suggested elsewhere macports works fine on multiple architecture and versions of MacOsX + allows updates and more: $ port search matplot py-matplotlib @0.99.0 (python graphics math) matlab-like syntax for creating plots in python py-matplotlib-basemap @0.99.4 (python graphics math) matplotlib toolkit for plotting data on map projections py25-matplotlib @0.99.0 (python graphics math) matlab-like syntax for creating plots in python py25-matplotlib-basemap @0.99.4 (python graphics math) matplotlib toolkit for plotting data on map projections py26-matplotlib @0.99.0 (python graphics math) matlab-like syntax for creating plots in python py26-matplotlib-basemap @0.99.4 (python graphics math) matplotlib toolkit for plotting data on map projections Found 6 ports. $ in your case simply issue : $ sudo port install py26-matplotlib it features the macosx backend (native cocoa) as default  This solution worked for me on OSX 10.8.3: ln -s /usr/local/include/freetype2/freetype/ /usr/include/freetype (Credit really goes to: http://simpleyuan.blogspot.com/2012/08/matplotlib-error-mac-os-x.html) This solution worked for me on OSX 10.8.3: `ln -s /opt/local/include/freetype2/freetype/ /usr/include/freetype` For Homebrew: `ln -s /usr/local/include/freetype2/ /usr/include/freetype`  I just got it to compile. I added freetype2 in the include path for the CFLAGS in the make.osx file. Now the top of make.osx is: PREFIX=/usr/local PYVERSION=2.6 PYTHON=python${PYVERSION} ZLIBVERSION=1.2.3 PNGVERSION=1.2.33 FREETYPEVERSION=2.3.5 MACOSX_DEPLOYMENT_TARGET=10.6 ## You shouldn't need to configure past this point PKG_CONFIG_PATH=""${PREFIX}/lib/pkgconfig"" CFLAGS=""-Os -arch x86_64 -arch i386 -I${PREFIX}/include -I${PREFIX}/include/freetype2"" LDFLAGS=""-arch x86_64 -arch i386 -L${PREFIX}/lib"" CFLAGS_DEPS=""-arch i386 -arch x86_64 -I${PREFIX}/include -I${PREFIX}/include/freetype2 -isysroot /Developer/SDKs/MacOSX10.6.sdk"" LDFLAGS_DEPS=""-arch i386 -arch x86_64 -L${PREFIX}/lib -syslibroot/Developer/SDKs/MacOSX10.6.sdk"" Then I ran these commands and it compiled and installed perfectly. sudo make -f make.osx mpl_build sudo make -f make.osx mpl_install You sure did -- thanks a lot whatnick! Well then I got answer - spot on.  According to your error message you have missing freetype headers. Can you locate them using system search functionalities. I will not lecture on using a pre-built package since I love scratching my head and compiling from the start as well. Thanks whatnick. That helped me edit the make.osx makefile.  Try with this one: http://macinscience.org/?page%5Fid=6 or as an alternative with easy_install. Thanks for the link but judging from the comments on that page it doesn't seem that superpack works with Snow Leopard. and have you tried with enthought (http://www.enthought.com/products/epd.php)? It is free for academic usage. I'm no longer an academic so it won't be free for me. And I don't like the idea of paying just to figure out how to compile something.  You can also build by using $ python setup.py build with the following patch applied to setupext.py Index: setupext.py =================================================================== --- setupext.py (revision 7917) +++ setupext.py (working copy) @@ -3346 +3348 @@ module.include_dirs.extend(incdirs) module.include_dirs.append('.') + module.include_dirs.append('/usr/local/include') + module.include_dirs.append('/usr/local/include/freetype2') module.library_dirs.extend(libdirs) def getoutput(s): This worked for me.  For Python.org 2.7.1: I used a mix of the instructions. It basically worked by using the libpng in OSX's /usr/X11 Downloaded built and installed (make install) freetype2 v2.4.4 & zlib v1.2.5. Did not use make deps. Modified setupext.py to have module.include_dirs.extend(incdirs) module.include_dirs.append('.') module.include_dirs.append('/usr/local/include') module.include_dirs.append('/usr/local/include/freetype2') module.include_dirs.append('/usr/X11/include') module.library_dirs.extend(libdirs) module.library_dirs.append('/usr/local/lib') module.library_dirs.append('/usr/X11/lib') Modified make.osx to include the same /usr/X11 info png version 1.2.5 is OSX 10.6.6 current native PYVERSION=2.7 PYTHON=python${PYVERSION} ZLIBVERSION=1.2.5 PNGVERSION=1.2.44 FREETYPEVERSION=2.4.4 MACOSX_DEPLOYMENT_TARGET=10.6 OSX_SDK_VER=10.6 ARCH_FLAGS=""-arch i386-arch x86_64"" PREFIX=/usr/local MACPREFIX=/usr/X11 PKG_CONFIG_PATH=""${PREFIX}/lib/pkgconfig"" CFLAGS=""-arch i386 -arch x86_64 -I${PREFIX}/include -I${PREFIX}/include/freetype2 -I${MAXPREFIX}/include -isysroot /Developer/SDKs/MacOSX${OSX_SDK_VER}.sdk"" LDFLAGS=""-arch i386 -arch x86_64 -L${PREFIX}/lib -L/usr/X11/lib -syslibroot/Developer/SDKs/MacOSX${OSX_SDK_VER}.sdk"" FFLAGS=""-arch i386 -arch x86_64"" Then the standard sudo make -f make.osx mpl_build sudo make -f make.osx mpl_install sudo python setup.py install Crikey... seems to work. Now have Image & MDP & pylab & matplotlib with 2.7.1 on 10.6.6 Image module (Imaging-1.7.7) works okay as long as you install libjpeg. I used jpegsrc.v8c and it seemed happy enough.",python osx-snow-leopard numpy compilation matplotlib
1708775,A,Combining two record arrays I have two Numpy record arrays that have exactly the same fields. What is the easiest way to combine them into one (i.e. append one table on to the other)? #!/usr/bin/env python import numpy as np desc = {'names': ('gender''age''weight') 'formats': ('S1' 'f4' 'f4')} a = np.array([('M'64.075.0)('F'25.060.0)] dtype=desc) b = np.array([('M'64.075.0)('F'25.060.0)] dtype=desc) alen=a.shape[0] blen=b.shape[0] a.resize(alen+blen) a[alen:]=b[:] This works with structured arrays though not recarrays. Perhaps this is a good reason to stick with structured arrays. Is there a reason why this does not work with recarrays? I thought recarrays were just structured arrays with an extra __getattribute__/__setattr__ arguments? I don't know why. I only know that when I try the same thing with recarrays I get a ValueError: cannot resize this array: it does not own its own data. Having run into problems like this with recarrays in the past I tend to use structured arrays instead of recarrays. The syntactic sugar isn't worth the trouble.  for i in array1: array2.append(i) Or (if implemented) array1.extend(array2) Now array1 contains also all elements of array2  Use numpy.hstack(): >>> import numpy >>> desc = {'names': ('gender''age''weight') 'formats': ('S1' 'f4' 'f4')} >>> a = numpy.array([('M'64.075.0)('F'25.060.0)] dtype=desc) >>> numpy.hstack((aa)) array([('M' 64.0 75.0) ('F' 25.0 60.0) ('M' 64.0 75.0) ('F' 25.0 60.0)] dtype=[('gender' '|S1') ('age' '<f4') ('weight' '<f4')]),python numpy recarray
1822417,A,Simple question about numpy matrix in python Let's suppose I have a numpy matrix variable called MATRIX with 3 coordinates: (x y z). Is acessing the matrix's value through the following code myVar = MATRIX[000] equal to myVar = MATRIX[00][0] or myVar = MATRIX[0][00] ? What about if I have the following code? myTuple = (00) myScalar = 0 myVar = MATRIX[myTuple myScalar] Is the last line equivalent to doing myVar = MATRIX[myTuple[0] myTuple[1] myScalar] I have done simple tests and it seems so but maybe that is not so in all the cases. How do square brackets work in python with numpy matrices? Since day one I felt confused as how they work. Thanks Are you sure? I get a `TypeError` when I try any of this. If you have a tuple `MYTUPLE=(123)` then the only possible indices are `MYTUPLE[0]` `MYTUPLE[1]` and `MYTUPLE[2]`. Sorry now that I see it it's numpy matrices that I was refering to. It's not possible to index a tuple with another tuple so none of that code is valid.  I assume you have a array instance rather than a matrix since the latter only can have two dimensions. m[0 0 0] gets the element at position (0 0 0). m[0 0] gets a whole subarray (a slice) which is itself a array. You can get the first element of this subarray like this: m[0 0][0] which is why both syntaxes work (even though m[i j k] is preferred because it doesn't have the unnecessary intermediate step). Take a look at this ipython session: rbonvall@andy:~$ ipython Python 2.5.4 (r254:67916 Sep 26 2009 08:19:36) [...] In [1]: import numpy.random In [2]: m = numpy.random.random(size=(3 3 3)) In [3]: m Out[3]: array([[[ 0.68853531 0.8815277  0.53613676] [ 0.9985735  0.56409085 0.03887982] [ 0.12083102 0.0301229  0.51331851]] [[ 0.73868543 0.24904349 0.24035031] [ 0.15458694 0.35570177 0.22097202] [ 0.81639051 0.55742805 0.5866573 ]] [[ 0.90302482 0.29878548 0.90705737] [ 0.68582033 0.1988247  0.9308886 ] [ 0.88956484 0.25112987 0.69732309]]]) In [4]: m[0 0] Out[4]: array([ 0.68853531 0.8815277  0.53613676]) In [5]: m[0 0][0] Out[5]: 0.6885353066709865 It only works like this for numpy arrays. Python built-in tuples and lists are not indexable by tuples just by integers. hmm and what would m[000] yield? The same as `m[0 0][0]` given that `len(m.shape) == 3`.,python numpy
367565,A,How do I build a numpy array from a generator? How can I build a numpy array out of a generator object? Let me illustrate the problem: >>> import numpy >>> def gimme(): ... for x in xrange(10): ... yield x ... >>> gimme() <generator object at 0x28a1758> >>> list(gimme()) [0 1 2 3 4 5 6 7 8 9] >>> numpy.array(xrange(10)) array([0 1 2 3 4 5 6 7 8 9]) >>> numpy.array(gimme()) array(<generator object at 0x28a1758> dtype=object) >>> numpy.array(list(gimme())) array([0 1 2 3 4 5 6 7 8 9]) In this instance gimme() is the generator whose output I'd like to turn into an array. However the array constructor does not iterate over the generator it simply stores the generator itself. The behaviour I desire is that from numpy.array(list(gimme())) but I don't want to pay the memory overhead of having the intermediate list and the final array in memory at the same time. Is there a more space-efficient way? This is an interesting issue. I came accross this by `from numpy import *; print any(False for i in range(1))` - which shadows the built-in [`any()`](http://docs.python.org/library/functions.html#any) and produces the opposite result (as I know now). @moooeeeep that's terrible. if `numpy` can't (or doesn't want to) to treat generators as Python does at least it should raise an exception when it receives a generator as an argument. @max I stepped on exact same mine. Apparently this was raised [on the NumPy list](http://thread.gmane.org/gmane.comp.python.numeric.general/47681/focus=47702) (and [earlier](http://thread.gmane.org/gmane.comp.python.numeric.general/13197)) concluding that this will not be changed to raise exception and one should always use namespaces. Numpy arrays require their length to be set explicitly at creation time unlike python lists. This is necessary so that space for each item can be consecutively allocated in memory. Consecutive allocation is the key feature of numpy arrays: this combined with native code implementation let operations on them execute much quicker than regular lists. Keeping this in mind it is technically impossible to take a generator object and turn it into an array unless you either: (a) can predict how many elements it will yield when run: my_array = numpy.zeros(predict_length()) for i el in enumerate(gimme()): my_array[i] = el (b) are willing to store its elements in an intermediate list : my_array = numpy.array(list(gimme())) (c) can make two identical generators run through the first one to find the total length initialize the array and then run through the generator again to find each element: length = sum(1 for el in gimme()) my_array = numpy.zeros(length) for i el in enumerate(gimme()): my_array[i] = el (a) is probably what you're looking for. (b) is space inefficient and (c) is time inefficient (you have to go through the generator twice). Thanks that makes alot of sense. The builtin `array.array` is a contiguous non-linked list and you can simply `array.array('f' generator)`. To say say it's impossible is misleading. It's just dynamic allocation. Why numpy.array doesn't do the memory allocation the same way as the builtin array.array as Cuadue says. What is the tradeof? I ask because there is contiguous allocated memory in both examples. Or not? numpy assumes its array sizes to not change. It relies heavily on different views of the same chunk of memory so allowing arrays to be expanded and reallocated would require an additional layer of indirection to enable views for example.  One google behind this stackoverflow result I found that there is a numpy.fromiter(data dtype count). The default count=-1 takes all elements from the iterable. It requires a dtype to be set explicitly. In my case this worked: numpy.fromiter(something.generate(from_this_input) float) Interesting I shall try it the next time I need it. how would you apply this to the question? `numpy.fromiter(gimme() float count=-1)` does not work. What does `something` stand for? something.generate is just the name of the generator @Matthias009 `numpy.fromiter(gimme() float count=-1)` works for me. A thread explaining why `fromiter` only works on 1D arrays: http://mail.scipy.org/pipermail/numpy-discussion/2007-August/028898.html. fwiw `count=-1` does not need to be specified as it is the default.  Somewhat tangential but if your generator is a list comprehension you can use numpy.where to more effectively get your result (I discovered this in my own code after seeing this post),python numpy generator
1587367,A,"Python/numpy tricky slicing problem I have a problem with some numpy stuff. I need a numpy array to behave in an unusual manner by returning a slice as a view of the data I have sliced not a copy. So heres an example of what I want to do: Say we have a simple array like this: a = array([1 0 0 0]) I would like to update consecutive entries in the array (moving left to right) with the previous entry from the array using syntax like this: a[1:] = a[0:3] This would get the following result: a = array([1 1 1 1]) Or something like this: a[1:] = 2*a[:3] # a = [1248] To illustrate further I want the following kind of behaviour: for i in range(len(a)): if i == 0 or i+1 == len(a): continue a[i+1] = a[i] Except I want the speed of numpy. The default behavior of numpy is to take a copy of the slice so what I actually get is this: a = array([1 1 0 0]) I already have this array as a subclass of the ndarray so I can make further changes to it if need be I just need the slice on the right hand side to be continually updated as it updates the slice on the left hand side. Am I dreaming or is this magic possible? Update: This is all because I am trying to use Gauss-Seidel iteration to solve a linear algebra problem more or less. It is a special case involving harmonic functions I was trying to avoid going into this because its really not necessary and likely to confuse things further but here goes. The algorithm is this: while not converged: for i in range(len(u[:0])): for j in range(len(u[0:])): # skip over boundary entries ij == 0 or len(u) u[ij] = 0.25*(u[i-1j] + u[i+1j] + u[i j-1] + u[ij+1]) Right? But you can do this two ways Jacobi involves updating each element with its neighbours without considering updates you have already made until the while loop cycles to do it in loops you would copy the array then update one array from the copied array. However Gauss-Seidel uses information you have already updated for each of the i-1 and j-1 entries thus no need for a copy the loop should essentially 'know' since the array has been re-evaluated after each single element update. That is to say every time we call up an entry like u[i-1j] or u[ij-1] the information calculated in the previous loop will be there. I want to replace this slow and ugly nested loop situation with one nice clean line of code using numpy slicing: u[1:-11:-1] = 0.25(u[:-21:-1] + u[2:1:-1] + u[1:-1:-2] + u[1:-12:]) But the result is Jacobi iteration because when you take a slice: u[:-21:-1] you copy the data thus the slice is not aware of any updates made. Now numpy still loops right? Its not parallel its just a faster way to loop that looks like a parallel operation in python. I want to exploit this behaviour by sort of hacking numpy to return a pointer instead of a copy when I take a slice. Right? Then every time numpy loops that slice will 'update' or really just replicate whatever happened in the update. To do this I need slices on both sides of the array to be pointers. Anyway if there is some really really clever person out there that awesome but I've pretty much resigned myself to believing the only answer is to loop in C. just to clarify again this is not what you want: a[1:] = 2*a[:3]; a[:3] = [100] so we now have a = [1200] but our a[:3] is now [120] so a becomes [1240] and once again a[:3] is [124] so we assign that to a[1:] and a becomes [1248] -- and now finally a[1:] ([248]) = 2*a[:3] (2*[124]) so our assignment is finished. what you're saying is that that's NOT what you want to happen? just to put an analogy on what you're trying to do since most people don't seem to understand it: it's somewhat like pointing a video camera at a television screen displaying its own output. so what you want is some sort of recursive assignment -- but i don't believe there is any guarantee that this will settle down into a constant value. sure it does in your case but not in general -- for example: `a[:] = 2*a[:]` would loop forever. so no what you want is not possible in numpy without explicitly looping and comparing until propagation of values is done. @daver I don't think you're getting how numpy works. When you say a[1:] = 2*a[:3] there are _two_ loops. The first one is 2*a[:3] which makes a temporary array. And then a second loop does the assignment. a[1:] is not getting assigned from itself but from a temporary. The problem is not that a[:3] isn't a view into a because it is but that 2*a[:3] is a completely different array. Numpy is giving you the speed of C but at the cost of more temporaries. Once you grasp this you'll see why you can't easily do what you want in NumPy without delving into lower-level stuff. @daver: unfortunately what i described before will not work in general in numpy since there is no convergence guarantee. as i said before a[:] = 2*a[:] would loop forever. for example if a = [1 1 1] then going through a[:] = 2*a[:] gives a = 2*[111] so a[:] is assigned [222]. so now we have a[:] ([222]) = 2*[222] which aren't equal so we go again: a[:] ([444]) = 2*[444]... etc it'll never converge. the only fixed point of a[:] = 2*a[:] is if a = [000] anything else will grow exponentially. What you described is *exactly* what I want to happen. Numpy appears to assign a bunch of stuff at once in reality it is just looping somewhere else. Default behaviour is like this first round: a = [1200]  second round: a = [1200] third round: a = [1200]. If the slice a[:3] were a pointer to a's data (view I think its called in numpy) then after the first round that slice would have updated so it would then be: a[:3] = [120] then a becomes: [1240] then the slice is pointing at a's data so a[:3] = [124] and infally we end up with a = [1248]. @daver: can you post a proper example using a loop? for now the loop is the same as a[1:]=a[1]. the for loop is the same of a[1:]=a[1]... Andrea don't try to think about how you could solve this trivial example in a simpler way think about the mechanics of what is going on yes the output is the same as a[1:] = a[0] but in the general case not even close. Why would that loop forever? And no it is not some sort of recursive assignment it is simple iteration. I just want like a pointer instead of a deep copy of my array when I take a slice of it its not really that hard to comprehend surely?? The problem with the ""kind of behavior... a[i+1]=a[i]"" is that it is such a trivial case that even though many of the posted solutions fulfil it you're not satisfied and neither is anyone else and no one can recognize a possible answer as an actual answer. That's why I asked you to give a simple but NON-TRIVIAL example of what you want using numbers other than 1 and 0. Do this and you may get a good answer. (But not from me I've read this problem too many times already.) Dude I'm sorry man but you asked me to update my question so that it is clearer I have done that and I have provided a case other than 1 and 0. You even replied to a comment I made where I gave you a cut down version of the 2-d thing I am using this on. Not one single response addresses the problem none of them so what you just said is simply not true. I'm sorry man. I even know the technical details of what I am after I'm specifically asking: can I get a slice to return a pointer to my data instead of a deep copy? And read my question it is in there. Did you look at the link in my answer to the SciPy page on Performance Python. The actual example of solving Laplace's equation using Gauss-Seidel iteration. They explicitly mention having to use a temporary to do what you want if you stick to pure NumPy and then give more advanced techniques to avoid that. Thanks for the link but it doesn't solve the problem I'm afraid. I've already implemented exactly what they have on that page but they even admit that it is not updating the entries in the manner I want to: ""... However since the NumPy expression uses temporaries internally only the old value of u[11] will be used."". It works but its called Jacobi iteration not Gauss-Seidel. Unfortunately I have to use Gauss-Seidel for an assignment. Thanks anyway. I am sorry but I don't understand very well your question... Anyway have you tried with a.copy()? What you're asking for is largely senseless. Why should you be able to say `a[0:3]` and have is mean `a[0]` only? That's incomprehensible. -1: I don't get your question either. Can you formalize what the behavior should be and break it down into well defined steps? First implement a simple for-loop version then we can maybe help you to optimize it. Did you ever get solved this? I just did exactly what is indicated here and usign Jacobi negates the speed of numpy because you need many more iterations. Late answer but this turned up on Google so I probably point to the doc the OP wanted. Your problem is clear: when using NumPy slices temporaries are created. Wrap your code in a quick call to weave.blitz to get rid of the temporaries and have the behaviour your want. Read the weave.blitz section of PerformancePython tutorial for full details.  It is not the correct logic. I'll try to use letters to explain it. Image array = abcd with abcd as elements. Now array[1:] means from the element in position 1 (starting from 0) on. In this case:bcd and array[0:3] means from the character in position 0 up to the third character (the one in position 3-1) in this case: 'abc'. Writing something like: array[1:] = array[0:3] means: replace bcd with abc To obtain the output you want now in python you should use something like: a[1:] = a[0] I've just read his comment above. The question is pretty unclear so. The fist part of my post explain why it does not work as the OP thinks. I'm waiting for a question improvement to try to solve his problem. It is not clear as he would want to update it. I did edit it it should be very clear now. Sorry this is my first post what I want to know is why this question got 3 downvotes? I commented on the question. I didn't downvoted this but I think it got downvoted because of the unclear exposition. Try to provide an alternative working way to solve the problem or at least to explain the algorithm you'd like to be used in the details. Andrea I have updated again with even more information I'm afraid I don't think it makes it any clearer but its there if you are interested. And yeah I got downvotes after I clarified things so I'm thinking theres a bit of people confusing not understanding the problem with my explanation of it going on. Yes the original question was vague and oversimplified. Found clarification in a comment. I wonder why people are afraid to edit original posts? The OP is trying to do something more sophisticated than that. The goal is to update elements of a slice based on element values that were updated in the current operation. It's hard to comprehend but it's much faster than traditional python loops.  You could have a look at np.lib.stride_tricks. There is some information in these excellent slides: http://mentat.za.net/numpy/numpy_advanced_slides/ with stride_tricks starting at slide 29. I'm not completely clear on the question though so can't suggest anything more concrete - although I would probably do it in cython or fortran with f2py or with weave. I'm liking fortran more at the moment because by the time you add all the required type annotations in cython I think it ends up looking less clear than the fortran. There is a comparison of these approaches here: www. scipy. org/ PerformancePython (can't post more links as I'm a new user) with an example that looks similar to your case.  Numpy must be checking if the target array is the same as the input array when doing the setkey call. Luckily there are ways around it. First I tried using numpy.put instead In [46]: a = numpy.array([1000]) In [47]: numpy.put(a[123]a[0:3]) In [48]: a Out[48]: array([1 1 1 1]) And then from the documentation of that I gave using flatiters a try (a.flat) In [49]: a = numpy.array([1000]) In [50]: a.flat[1:] = a[0:3] In [51]: a Out[51]: array([1 1 1 1]) But this doesn't solve the problem you had in mind In [55]: a = np.array([1000]) In [56]: a.flat[1:] = 2*a[0:3] In [57]: a Out[57]: array([1 2 0 0]) This fails because the multiplication is done before the assignment not in parallel as you would like. Numpy is designed for repeated application of the exact same operation in parallel across an array. To do something more complicated unless you can find decompose it in terms of functions like numpy.cumsum and numpy.cumprod you'll have to resort to something like scipy.weave or writing the function in C. (See the PerfomancePython page for more details.) (Also I've never used weave so I can't guarantee it will do what you want.)  It must have something to do with assigning a slice. Operators however as you may already know do follow your expected behavior: >>> a = numpy.array([1000]) >>> a[1:]+=a[:3] >>> a array([1 1 1 1]) If you already have zeros in your real-world problem where your example does then this solves it. Otherwise at added cost set them to zero either by multiplying by zero or assigning to zero (whichever is faster) edit: I had another thought. You may prefer this: numpy.put(a[123]a[:3])  In the end I came up with the same problem as you. I had to resort to use Jacobi iteration and weaver:  while (iter_n < max_time_steps): expr = ""field[1:-1 1:-1] = (field[2: 1:-1] ""\ ""+ field[:-2 1:-1]+""\ ""field[1:-1 2:] +""\ ""field[1:-1 :-2] )/4."" weave.blitz(expr check_size=0) #Toroidal conditions field[:0] = field[:self.flow.n_x - 2] field[:self.flow.n_x -1] = field[:1] iter_n = iter_n + 1 It works and is fast but is not Gauss-Seidel so convergence can be a bit tricky. The only option of doing Gauss-Seidel as a traditional loop with indexes.  accumulate is designed to do what you seem to want; that is to proprigate an operation along an array. Here's an example: from numpy import * a = array([1000]) a[1:] = add.accumulate(a[0:3]) # a = [1 1 1 1] b = array([1111]) b[1:] = multiply.accumulate(2*b[0:3]) # b = [1 2 4 8] Another way to do this is to explicitly specify the result array as the input array. Here's an example: c = array([2000]) multiply(c[:3] c[:3] c[1:]) # c = [ 2 4 16 256]  i would suggest cython instead of looping in c. there might be some fancy numpy way of getting your example to work using a lot of intermediate steps... but since you know how to write it in c already just write that quick little bit as a cython function and let cython's magic make the rest of the work easy for you.  Just use a loop. I can't immediately think of any way to make the slice operator behave the way you're saying you want it to except maybe by subclassing numpy's array and overriding the appropriate method with some sort of Python voodoo... but more importantly the idea that a[1:] = a[0:3] should copy the first value of a into the next three slots seems completely nonsensical to me. I imagine that it could easily confuse anyone else who looks at your code (at least the first few times). No no no its not copying the first value into the next three its updating each consecutive entry with data from the previous entry. However each time it updates I want it to be aware of the previous update. Yes I can loop but its extremely slow and clumsy for the purpose I have in mind. But heres what it would look like in a loop: for i in a: *Dammit sorry i hit tab and it updated my comment... the code: for i in a: a[i+1] = a[i] The point is this is for a 2 dimensional array and is a specific numeric algorithm that I need to implement. why not just a[1:4]=a[0]? unknown: You really don't want the array copy to work that way. How would you copy parts of the array if it worked that way? gnibbler: That completely misses the point of the algorithm this is a simple example what I am doing is Gauss-Seidel iteration which infers information about a location in a matrix by using data that has already been inferred in previous entries. Deep in the machinery of numpy as I understand it it performs this loop. However it loops over the copy of the original data in the slice. I want it to loop over the slice and update the slice as it goes. Maybe I could make it clearer like this: a[1:] = 2*a[0:3] with the expected result being: a = [1248]. Oh yeah and no I do not want my array copy to work like that I want my array slice to not be a copy I want my array slice to be a view of the data contained in the array. Please fix the question. Do no correct your question in the comments on an answer. Please update the question with this revised description of the problem. One of the main advantages to numpy is the ability to avoid expensive python iterations. (aka broadcasting). The question is perfectly valid and a reasonable thing to expect numpy to do.",python numpy slice
1025379,A,"Decimal alignment formatting in Python This should be easy. Here's my array (rather a method of generating representative test arrays): >>> ri = numpy.random.randint >>> ri2 = lambda x: ''.join(ri(09x).astype('S')) >>> a = array([float(ri2(x)+ '.' + ri2(y)) for xy in ri(110(102))]) >>> a array([ 7.99914000e+01 2.08000000e+01 3.94000000e+02 4.66100000e+03 5.00000000e+00 1.72575100e+03 3.91500000e+02 1.90610000e+04 1.16247000e+04 3.53920000e+02]) I want a list of strings where '\n'.join(list_o_strings) would print:  79.9914 20.8 394.0 4661.0 5.0 1725.751 391.5 19061.0 11624.7 353.92 I want to space pad to the left and the right (but no more than necessary). I want a zero after the decimal if that is all that is after the decimal. I do not want scientific notation. ..and I do not want to lose any significant digits. (in 353.98000000000002 the 2 is not significant) Yeah it's nice to want.. Python 2.5's %g %fx.x etc. are either befuddling me or can't do it. I have not tried import decimal yet. I can't see that NumPy does it either (although the array.__str__ and array.__repr__ are decimal aligned (but sometimes return scientific). Oh and speed counts. I'm dealing with big arrays here. My current solution approaches are: to str(a) and parse off NumPy's brackets to str(e) each element in the array and split('.') then pad and reconstruct to a.astype('S'+str(i)) where i is the max(len(str(a))) then pad It seems like there should be some off-the-shelf solution out there... (but not required) Top suggestion fails with when dtype is float64: >>> a array([ 5.50056103e+02 6.77383566e+03 6.01001513e+05 3.55425142e+08 7.07254875e+05 8.83174744e+02 8.22320510e+01 4.25076609e+08 6.28662635e+07 1.56503068e+02]) >>> ut0 = re.compile(r'(\d)0+$') >>> thelist = [ut0.sub(r'\1' ""%12f"" % x) for x in a] >>> print '\n'.join(thelist) 550.056103 6773.835663 601001.513 355425141.8471 707254.875038 883.174744 82.232051 425076608.7676 62866263.55 156.503068 Please post the code that doesn't work. Sorry but after thorough investigation I can't find any way to perform the task you require without a minimum of post-processing (to strip off the trailing zeros you don't want to see); something like: import re ut0 = re.compile(r'(\d)0+$') thelist = [ut0.sub(r'\1' ""%12f"" % x) for x in a] print '\n'.join(thelist) is speedy and concise but breaks your constraint of being ""off-the-shelf"" -- it is instead a modular combination of general formatting (which almost does what you want but leaves trailing zero you want to hide) and a RE to remove undesired trailing zeros. Practically I think it does exactly what you require but your conditions as stated are I believe over-constrained. Edit: original question was edited to specify more significant digits require no extra leading space beyond what's required for the largest number and provide a new example (where my previous suggestion above doesn't match the desired output). The work of removing leading whitespace that's common to a bunch of strings is best performed with textwrap.dedent -- but that works on a single string (with newlines) while the required output is a list of strings. No problem we'll just put the lines together dedent them and split them up again: import re import textwrap a = [ 5.50056103e+02 6.77383566e+03 6.01001513e+05 3.55425142e+08 7.07254875e+05 8.83174744e+02 8.22320510e+01 4.25076609e+08 6.28662635e+07 1.56503068e+02] thelist = textwrap.dedent( '\n'.join(ut0.sub(r'\1' ""%20f"" % x) for x in a)).splitlines() print '\n'.join(thelist) emits:  550.056103 6773.83566 601001.513 355425142.0 707254.875 883.174744 82.232051 425076609.0 62866263.5 156.503068 I can't guarantee that %12f won't lose significant digits. (I made an edit and changed the way my test arrays are generated to reflect this.) If I increase to %20 or more to guarantee this then there is simply too much padding to the left. (want the largest value to have no leading spaces) I'll take back-of-the-cupboard solutions too!  Pythons string formatting can both print out only the necessary decimals (with %g) or use a fixed set of decimals (with %f). However you want to print out only the necessary decimals except if the number is a whole number then you want one decimal and that makes it complex. This means you would end up with something like: def printarr(arr): for x in array: if math.floor(x) == x: res = '%.1f' % x else: res = '%.10g' % x print ""%*s"" % (15-res.find('.')+len(res) res) This will first create a string either with 1 decimal if the value is a whole number or it will print with automatic decimals (but only up to 10 numbers) if it is not a fractional number. Lastly it will print it adjusted so that the decimal point will be aligned. Probably though numpy actually does what you want because you typically do want it to be in exponential mode if it's too long.",python formatting numpy code-golf
573487,A,"Any way to create a NumPy matrix with C API? I read the documentation on NumPy C API I could find but still wasn't able to find out whether there is a possibility to construct a matrix object with C API — not a two-dimensional array. The function is intended for work with math matrices and I don't want strange results if the user calls matrix multiplication forgetting to convert this value from an array to a matrix (multiplication and exponentiation being the only difference that matrix subclass has). What do you mean by ""matrix""? Is it `numpy.matrix` class? You can call any python callable with the PyObject_Call* functions. PyObject *numpy = PyImport_ImportModule(""numpy""); PyObject *numpy_matrix = PyObject_GetAttrString(numpy ""matrix""); PyObject *my_matrix = PyObject_CallFunction(numpy_matrix ""(s)"" ""0 0; 0 0""); This will create a matrix my_matrix of size 2x2. EDIT: Changed references to numpy.zeros/numpy.ndarray to numpy.matrix instead. I also found a good tutorial on the subject: http://starship.python.net/crew/hinsen/NumPyExtensions.html OP asks `numpy.matrix` but `zeros` returns `ndarray`.  numpy.matrix is an ordinary class defined in numpy/core/defmatrix.py. You can construct it using C API as any other instance of user-defined class in Python.",python numpy python-c-api
1966207,A,"Converting NumPy array into Python List structure? How do I convert a NumPy array to a Python List (for example [[123][456]] ) and do it reasonably fast? Arrays are *already* array_like as are lists. Do you mean ""how do I convert an array to a list""? classic SO crankiness directly above! Use tolist(): import numpy as np >>> np.array([[123][456]]).tolist() [[1 2 3] [4 5 6]]  NumPy arrays have a tolist method: In [1]: arr=np.array([[123][456]]) In [2]: arr.tolist() Out[2]: [[1 2 3] [4 5 6]]",python numpy
1624395,A,"Removing Array Elements in Python while keeping track of their position I'v got two numpy arrays. The first array contains some zeros (which are distributed randomly over the length of the array) which I would like to remove. My issue is that I would also like to remove the entries of the second array at the index positions where the first array elements are zero. I only came up with a very cumbersome for-loop. Does anyone have an ""elegant"" method for doing this? Thx! You can use boolean indexing. x!=0 gives you a boolean array with True where x!=0 false where x==0. If you index either x or y with this array (ie x_nozeros=x[x!=0]) then you will get only the elements where x!=0. eg: In [1]: import numpy as np In [2]: x = np.array([120304]) In [3]: y = np.arange(17) In [4]: indx = x!=0 In [5]: x_nozeros = x[indx] In [6]: y_nozeros = y[indx] In [7]: x_nozeros Out[7]: array([1 2 3 4]) In [8]: y_nozeros Out[8]: array([1 2 4 6])  Is it what you want? I am a NumPy newbie. In [1]: import numpy as np In [2]: a = np.array([120304]) In [3]: b = np.array([123456]) In [4]: b[np.where(a)] Out[4]: array([1 2 4 6]) In [5]: np.where(a) Out[5]: (array([0 1 3 5])) In [6]: a[np.where(a)] Out[6]: array([1 2 3 4]) That's exactly what I looked for thanks! @AFoglia thanks for the introduction of np.extract. it's really cool. @Dzz Glad it helped :) This is the way I usually do it but there is an even simpler method. `np.extract(ab)`. It does the same as `b[np.where(a)]`.",python arrays numpy
1939228,A,"Constructing a python set from a numpy matrix I'm trying to execute the following >> from numpy import * >> x = array([[323][444]]) >> y = set(x) TypeError: unhashable type: 'numpy.ndarray' How can I easily and efficiently create a set from a numpy array? The immutable counterpart to an array is the tuple hence try convert the array of arrays into an array of tuples: >> from numpy import * >> x = array([[323][444]]) >> x_hashable = map(tuple x) >> y = set(x_hashable) set([(3 2 3) (4 4 4)]) and how to I easily/efficiently transform back to a list? `map(array y)`  If you want a set of the elements: >> y = set(e for r in x for e in r) set([2 3 4]) For a set of the rows: >> y = set(tuple(r) for r in x) set([(3 2 3) (4 4 4)])  The above answers work if you want to create a set out of the elements contained in an ndarray but if you want to create a set of ndarray objects – or use ndarray objects as keys in a dictionary – then you'll have to provide a hashable wrapper for them. See the code below for a simple example: from hashlib import sha1 from numpy import all array uint8 class hashable(object): r'''Hashable wrapper for ndarray objects. Instances of ndarray are not hashable meaning they cannot be added to sets nor used as keys in dictionaries. This is by design - ndarray objects are mutable and therefore cannot reliably implement the __hash__() method. The hashable class allows a way around this limitation. It implements the required methods for hashable objects in terms of an encapsulated ndarray object. This can be either a copied instance (which is safer) or the original object (which requires the user to be careful enough not to modify it). ''' def __init__(self wrapped tight=False): r'''Creates a new hashable object encapsulating an ndarray. wrapped The wrapped ndarray. tight Optional. If True a copy of the input ndaray is created. Defaults to False. ''' self.__tight = tight self.__wrapped = array(wrapped) if tight else wrapped self.__hash = int(sha1(wrapped.view(uint8)).hexdigest() 16) def __eq__(self other): return all(self.__wrapped == other.__wrapped) def __hash__(self): return self.__hash def unwrap(self): r'''Returns the encapsulated ndarray. If the wrapper is ""tight"" a copy of the encapsulated ndarray is returned. Otherwise the encapsulated ndarray itself is returned. ''' if self.__tight: return array(self.__wrapped) return self.__wrapped Using the wrapper class is simple enough: >>> from numpy import arange >>> a = arange(0 1024) >>> d = {} >>> d[a] = 'foo' Traceback (most recent call last): File ""<input>"" line 1 in <module> TypeError: unhashable type: 'numpy.ndarray' >>> b = hashable(a) >>> d[b] = 'bar' >>> d[b] 'bar'  If you want a set of the elements here is another probably faster way: y = set(x.flatten()) PS: after performing comparisons between x.flat x.flatten() and x.ravel() on a 10x100 array I found out that they all perform at about the same speed. For a 3x3 array the fastest version is the iterator version: y = set(x.flat) which I would recommend because it is the less memory expensive version (it scales up well with the size of the array). PS: There is also a NumPy function that does something similar: y = numpy.unique(x) This does produce a NumPy array with the same element as set(x.flat) but as a NumPy array. This is very fast (almost 10 times faster) but if you need a set then doing set(numpy.unique(x)) is a bit slower than the other procedures (building a set comes with a large overhead). @musicinmybrain: very good points! Thank you! Good suggestion! You could also use set(x.ravel()) which does the same thing but creates a copy only if needed. Or better use set(x.flat). x.flat is an iterator over the elements of the flattened array but does not waste time actually flattening the array @conradlee: This solution is indeed designed to give the set of all the numbers found in the array. WARNING: this answer *will not* give you a set of vectors but rather a set of numbers. If you want a set of vectors then see miku's answer below which converts the vectors to tuples",python arrays numpy set
1836966,A,Passing numpy.arange() an argument I'm trying to pass the values that I want numpy.arange to use. The code is: for x in numpy.arange(argument) where argument is: argument = (.16.3.1) (tuple) TypeError: arange: scaler arguements expected instead of a tuple arguement = [.16.3.1] (list) TypeError: unsupported operand type(s) for -: 'str' and 'int' arguement = '.16.3.1' (string) TypeError: unsupported operand type(s) for -: 'str' and 'int' and I've tried putting the tuple and list in a string. None of these have worked. I've searched the literature and can find no reference to this. Any insights would be appreciated. arange is like python's range function. Perhaps you were looking for numpy.array? Or maybe you really did want the range to be from 0.1 to 6.3 in steps of 0.1. In that case use Python's argument unpacking syntax: arguments = (.1 6.3 .1) numpy.arange(*arguments) I had to change 'argument =...' to 'arguments =...' and then it worked. I'll have to read up on Python's argument unpacking sytax. Thanks,python numpy
1909994,A,"How do I add rows and columns to a NUMPY array? Hello I have a 1000 data series with 1500 points in each. They form a (1000x1500) size Numpy array created using np.zeros((1500 1000)) and then filled with the data. Now what if I want the array to grow to say 1600 x 1100? Do I have to add arrays using hstack and vstack or is there a better way? I would want the data already in the 1000x1500 piece of the array not to be changed only blank data (zeros) added to the bottom and right basically. Thanks. If you want zeroes in the added elements my_array.resize((1600 1000)) should work. Note that this differs from numpy.resize(my_array (1600 1000)) in which previous lines are duplicated which is probably not what you want. Otherwise (for instance if you want to avoid initializing elements to zero which could be unnecessary) you can indeed use hstack and vstack to add an array containing the new elements; numpy.concatenate() (see pydoc numpy.concatenate) should work too (it is just more general as far as I understand). In either case I would guess that a new memory block has to be allocated in order to extend the array and that all these methods take about the same time. Just a note that this doesn't appear to keep the data in place in the case when you merely want to extend the data set: >>> a = numpy.array([[12][34]]) >>> a array([[1 2] [3 4]]) >>> a.resize((24)) Traceback (most recent call last): File """" line 1 in ValueError: cannot resize an array references or is referenced by another array in this way. Use the resize function >>> a = numpy.array(a) >>> a.resize((24)) >>> a array([[1 2 3 4] [0 0 0 0]])  No matter what you'll be stuck reallocating a chunk of memory so it doesn't really matter if you use arr.resize() np.concatenate hstack/vstack etc. Note that if you're accumulating a lot of data sequentially Python lists are usually more efficient.  This should do what you want (ie using 3x3 array and 4x4 array to represent the two arrays in the OP) >>> import numpy as NP >>> a = NP.random.randint(0 10 9).reshape(3 3) >>> a >>> array([[1 2 2] [7 0 7] [0 3 0]]) >>> b = NP.zeros((4 4)) mapping a on to b: >>> b[:3:3] = a >>> b array([[ 1. 2. 2. 0.] [ 7. 0. 7. 0.] [ 0. 3. 0. 0.] [ 0. 0. 0. 0.]]) @kηives yes a typo thank you--editing my post now. I got an error from that code. Shouldn't the last line be b[:3 :3] = a ? All the same plus one since when I did that it worked and that is what I was looking for.  You should use reshape() and/or resize() depending on your precise requirement. If you want chapter and verse from the authors you are probably better off posting on the numpy discussion board.",python arrays numpy reshape
1401712,A,"How can the euclidean distance be calculated with numpy? I have two points in 3D: (xa ya za) (xb yb zb) And I want to calculate the distance: dist = sqrt((xa-xb)^2 + (ya-yb)^2 + (za-zb)^2) What's the best way to do this with Numpy or with Python in general? I have: a = numpy.array((xa ya za)) b = numpy.array((xb yb zb)) Another instance of this problem solving method. As soon as I submitted the question I got it: def dist(xy): return numpy.sqrt(numpy.sum((x-y)**2)) a = numpy.array((xayaza)) b = numpy.array((xbybzb)) dist_a_b = dist(ab) can you use numpy's sqrt and/or sum implementations? That should make it faster (?). Thanks! I'll update the answer I found this on the other side of the interwebs `norm = lambda x: N.sqrt(N.square(x).sum())` ; `norm(x-y)` scratch that. it had to be somewhere. here it is: `numpy.linalg.norm(x-y)`  Can be done like this don't know how fast it is but its no numpy. from math import sqrt a = (123) #data point 1 b = (456) #data point 2 print sqrt(sum( (a - b)**2 for a b in zip(a b)))  I find a 'dist' function in matplotlib.mlab but i don't think it's handy enough. I'm posting it here just for reference. import numpy as np import matplotlib as plt a = np.array([123]) b = np.array([234]) # distance between a and b dis = plt.mlab.dist(ab)  Use numpy.linalg.norm: dist = numpy.linalg.norm(a-b) I knew there was a reason for me not to accept my own answer :-). Just for the record I managed to see Mark Lavin's answer before he deleted it. I liked it better for the link to Python's docs and the explanation. Can you add some details? The linalg.norm docs can be found here: http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html My only real comment was sort of pointing out the connection between a norm (in this case the Frobenius norm/2-norm which is the default for norm function) and a metric (in this case Euclidean distance).  There's a function for that in SciPy it's called Euclidean example: from scipy.spatial import distance a = (123) b = (456) dst = distance.euclidean(ab)  dist = numpy.linalg.norm(a-b) Is a nice one line answer. However if speed is a concern I would recommend experimenting on your machine. I found that using the math library's sqrt with the ** operator for the square is much faster on my machine than the one line numpy solution. I ran my tests using this simple program: #!/usr/bin/python import math import numpy from random import uniform def fastest_calc_dist(p1p2): return math.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2 + (p2[2] - p1[2]) ** 2) def math_calc_dist(p1p2): return math.sqrt(math.pow((p2[0] - p1[0]) 2) + math.pow((p2[1] - p1[1]) 2) + math.pow((p2[2] - p1[2]) 2)) def numpy_calc_dist(p1p2): return numpy.linalg.norm(numpy.array(p1)-numpy.array(p2)) TOTAL_LOCATIONS = 1000 p1 = dict() p2 = dict() for i in range(0 TOTAL_LOCATIONS): p1[i] = (uniform(01000)uniform(01000)uniform(01000)) p2[i] = (uniform(01000)uniform(01000)uniform(01000)) total_dist = 0 for i in range(0 TOTAL_LOCATIONS): for j in range(0 TOTAL_LOCATIONS): dist = fastest_calc_dist(p1[i] p2[j]) #change this line for testing total_dist += dist print total_dist On my machine math_calc_dist runs much faster than numpy_calc_dist: 1.5 seconds versus 23.5 seconds. To get a measurable difference between fastest_calc_dist and math_calc_dist I had to up TOTAL_LOCATIONS to 6000. Then fastest_calc_dist takes ~50 seconds while math_calc_dist takes ~60 seconds. You can also experiment with numpy.sqrt and numpy.square though both were slower than the math alternatives on my machine. My tests were run with Python 2.6.6. You're badly misunderstanding how to use numpy... _Don't_ use loops or list comprehensions. If you're iterating through and applying the function to _each_ item then yeah the numpy functions will be slower. The whole point is to vectorize things. If I move the numpy.array call into the loop where I am creating the points I do get better results with numpy_calc_dist but it is still 10x slower than fastest_calc_dist. If I have that many points and I need to find the distance between each pair I'm not sure what else I can do to advantage numpy. I realize this thread is old but I just want to reinforce what Joe said. You are not using numpy correctly. What you are calculating is the sum of the distance from every point in p1 to every point in p2. The solution with numpy/scipy is over 70 times quicker on my machine. Make p1 and p2 into an array (even using a loop if you have them defined as dicts). Then you can get the total sum in one step `scipy.spatial.distance.cdist(p1 p2).sum()`. That is it. Or use `numpy.linalg.norm(p1-p2).sum()` to get the sum between each point in p1 and the corresponding point in p2 (i.e. not every point in p1 to every point in p2). And if you do want every point in p1 to every point in p2 and don't want to use scipy as in my previous comment then you can use np.apply_along_axis along with numpy.linalg.norm to still do it much much quicker then your ""fastest"" solution. Previous versions of NumPy had very slow norm implementations. In current versions there's no need for all this.  You can just substract the vectors and then innerproduct. Following your example a = numpy.array((xayaza)) b = numpy.array((xbybzb)) tmp = a - b result = numpy.dot( tmp.T  tmp) Simple Code an easy to understand. this will give me the square of the distance. you're missing a sqrt here.",python numpy euclidean-distance
934616,A,How do I find out if a numpy array contains integers? I know there is a simple solution to this but can't seem to find it at the moment. Given a numpy array I need to know if the array contains integers. Checking the dtype per-se is not enough as there are multiple int dtypes (int8 int16 int32 int64 ...). Please specify whether you want to check whether the **type** is an integer or whether the **value** is an integer (see [my solution](http://stackoverflow.com/a/7236784/866007)). Checking for an integer type does not work for floats that are integers e.g. 4. Better solution is np.equal(np.mod(x 1) 0) as in: >>> import numpy as np >>> def isinteger(x): ... return np.equal(np.mod(x 1) 0) ... >>> foo = np.array([0. 1.5 1.]) >>> bar = np.array([-5 1 2 3 -4 -2 0 1 0 0 -1 1]) >>> isinteger(foo) array([ True False True] dtype=bool) >>> isinteger(bar) array([ True True True True True True True True True True True True] dtype=bool) >>> isinteger(1.5) False >>> isinteger(1.) True >>> isinteger(1) True  Found it in the numpy book! Page 23: The other types in the hierarchy deﬁne particular categories of types. These categories can be useful for testing whether or not the object returned by self.dtype.type is of a particular class (using issubclass). issubclass(n.dtype('int8').type n.integer) >>> True issubclass(n.dtype('int16').type n.integer) >>> True  This also works:  n.dtype('int8').kind == 'i' For unsigned dtype kind = 'u'. A more general test should be: **some_dtype.kind in ('u''i')**,python numpy
1530960,A,"Incrementally building a numpy array and measuring memory usage I have a series of large text files (up to 1 gig) that are output from an experiment that need to be analysed in Python. They would be best loaded into a 2D numpy array which presents the first question: As the number of rows is unknown at the beginning of the loading how can a very large numpy array be most efficiently built row by row? Simply adding the row to the array would be inefficient in memory terms as two large arrays would momentarily co-exist. The same problem would seem to be occur if you use numpy.append. The stack functions are promising but ideally I would want to grow the array in place. This leads to the second question: What is the best way to observe the memory usage of a Python program that heavily uses numpy arrays? To study the above problem I've used the usual memory profiling tools - heapy and pympler - but am only getting the size of the outer array objects (80 bytes) and not the data they are containing. Asides from a crude measuring of how much memory the Python process is using how can I get at the ""full"" size of the arrays as they grow? Local details: OSX 10.6 Python 2.6 but general solutions are welcome. Have you tried using memmap file? You can iterate through your input file (in chunks if possible) and convert the incoming data and insert them as rows into a memory-mapped numpy array. The downside is incurring more disk i/o in case there is insufficient main memory and paging from swap becomes necessary. See: http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html Another alternative is PyTables. You'll need to construct some special sql-like table but it's fairly simple. In fact it provides transparent disk persistence (automated serialization) and hierarchical organization for your data. It also limits the amount of main memory used. See: www.pytables.org/moin/HowToUse Best of luck!  On possible option is to do a single pass through the file first to count the number of rows without loading them. The other option is to double your table size each time which has two benefits: You will only re-alloc memory log(n) times where n is the number of rows. You only need 50% more ram than your largest table size If you take the dynamic route you could measure the length of the first row in bytes then guess the number of rows by calculating (num bytes in file / num bytes in first row). Start with a table of this size. Thanks all. Given the size of the file I was reluctant to do an initial pass just to count lines but it seems the easiest and most efficient way to solve the memory problem. I had a co-worker ask a similar question recently and I came up with another possibility that could save you from the initial pass. If you know the approximate size of an ""element"" in the file you can divide it into the file size. Add some padding for safety and you can then write to the entire memory. To hide the extra uninitialized you can use a view of only the elements with data. You will need to make sure you don't go over. It's not perfect but if your file reads are slow and your data is consistently laid out it might work.  There's no way to ensure you can grow the array in place other than creating an empty array (numpy.empty) of the maximum possible size and then using a view of that at the end. You can't start small because there's no guarantee that you can expand whatever memory the map is without clobbering some other data. (And all of this is much lower level than python allows you to get from inside the interpreter.) Your best bet is probably numpy.fromiter. Looking at the source as the number of items increase the array is expanded by a little over 50% each time. If you can easily get the number of rows (say from counting the lines) you can even pass it a count.  The problem is essentially the text file. When your input data is stored in a more advanced from such problems can be avoided. Take for example a look at the h5py project. It is worth the trouble to first convert your data to HDF5 files and then run analysis scripts on the HDF5 files.",python numpy memory-management
417664,A,"How can I use Numerical Python with Python 2.6 I'm forced to upgrade to Python 2.6 and am having issues using Numerical Python (NumPy) with Python 2.6 in Windows. I'm getting the following error... Traceback (most recent call last): File ""<pyshell#0>"" line 1 in <module> from numpy.core.numeric import arraydotall File ""C:\svn\svn_urbansim\UrbanSimDev\Builds\working\urbansim\Tools\Python26\lib\site-packages\numpy\__init__.py"" line 39 in <module> import core File ""C:\svn\svn_urbansim\UrbanSimDev\Builds\working\urbansim\Tools\Python26\lib\site-packages\numpy\core\__init__.py"" line 5 in <module> import multiarray ImportError: Module use of python25.dll conflicts with this version of Python. It appears that the existing module is trying to use the python25.dll file. Is there any way I can tell it to use the python26.dll file instead without modifying the source code? NumPy 1.3.0 is available for Python 2.6 now.  How did you install it? NumPy doesn't currently have a Python 2.6 binary. If you have LAPACK/ATLAS/BLAS etc. and a development environment you should be able to compile numpy from sources. Otherwise I think you're stuck with using Python 2.5 on Windows if you need NumPy. The next version of NumPy should have a 2.6 binary and it's likely to be out within the next month or so. [Edit]: It appears that a pygame developer created a NumPy 1.2.1 binary for Python 2.6 on Windows available here.",python windows numpy
1988091,A,"Poor numpy.cross() performance I've been doing some performance testing in order to improve the performance of a pet project I'm writing. It's a very number-crunching intensive application so I've been playing with Numpy as a way of improving computational performance. However the result from the following performance tests were quite surprising.... Test Source Code (Updated with test cases for hoisting and batch submission) import timeit numpySetup = """""" import numpy left = numpy.array([1.00.00.0]) right = numpy.array([0.01.00.0]) """""" hoistSetup = numpySetup +'hoist = numpy.cross\n' pythonSetup = """""" left = [1.00.00.0] right = [0.01.00.0] """""" numpyBatchSetup = """""" import numpy l = numpy.array([1.00.00.0]) left = numpy.array([l]*10000) r = numpy.array([0.01.00.0]) right = numpy.array([r]*10000) """""" pythonCrossCode = """""" x = ((left[1] * right[2]) - (left[2] * right[1])) y = ((left[2] * right[0]) - (left[0] * right[2])) z = ((left[0] * right[1]) - (left[1] * right[0])) """""" pythonCross = timeit.Timer(pythonCrossCode pythonSetup) numpyCross = timeit.Timer ('numpy.cross(left right)'  numpySetup) hybridCross = timeit.Timer(pythonCrossCode numpySetup) hoistCross = timeit.Timer('hoist(left right)' hoistSetup) batchCross = timeit.Timer('numpy.cross(left right)' numpyBatchSetup) print 'Python Cross Product : %4.6f ' % pythonCross.timeit(1000000) print 'Numpy Cross Product : %4.6f ' % numpyCross.timeit(1000000) print 'Hybrid Cross Product : %4.6f ' % hybridCross.timeit(1000000) print 'Hoist Cross Product : %4.6f ' % hoistCross.timeit(1000000) # 100 batches of 10000 each is equivalent to 1000000 print 'Batch Cross Product : %4.6f ' % batchCross.timeit(100) Original Results Python Cross Product : 0.754945 Numpy Cross Product : 20.752983 Hybrid Cross Product : 4.467417 Final Results Python Cross Product : 0.894334 Numpy Cross Product : 21.099040 Hybrid Cross Product : 4.467194 Hoist Cross Product : 20.896225 Batch Cross Product : 0.262964 Needless to say this wasn't the result I expected. The pure Python version performs almost 30x faster than Numpy. Numpy performance in other tests has been better than the Python equivalent (which was the expected result). So I've got two related questions: Can anyone explain why NumPy is performing so poorly in this case? Is there something I can do to fix it? To reduce the numpy calling overhead you might try using cython as an intermediate to call into the numpy functions. See Fast numerical computations with Cython (SciPy 2009) for details.  Excellent post! I think that the comparison is not actually fair. Batch Cross Product gives an array containing the cross products of all vectors while Python Cross Product gives one vector at a time. If you need to compute all cross products at once of course Batch is better but if you need to compute every cross product separately you should include the overhead of accessing the array. Also if a cross product if a function of the previous cross product the Batch implementation should be modified.  You can see the source code yourself here: http://www.google.com/codesearch/p?hl=en#5mAq98l-MUw/trunk/dnumpy/numpy/core/numeric.py&q=cross%20package:numpy&sa=N&cd=1&ct=rc numpy.cross just handles lots of cases and does some extra copies. In general numpy is going to be plenty fast enough for slow things like matrix multiplication or inversion - but operations on small vectors like that have a lot of overhead.  Try this with larger arrays. I think that just the cost of calling the methods of numpy here overruns the simple several list accesses required by the Python version. If you deal with larger arrays I think you'll see large wins for numpy. In this particular case 3 component arrays (xyz co-ordinates) are by far the most common case. What's also a bit weird is that even reading from numpy arrays the python code is still faster. If it was call overhead I'd expect that to be slowed down even more than the pure NumPy solution. @Adam: but by reading from numpy's arrays you save the overhead of calling the `cross` function itself which is a dynamically loaded extension so it goes through at least a couple of pointers. For such short arrays it indeed makes sense as a micro-optimization to unroll the call to `cross` I just added a test case where I batched the arrays together and saw a considerable performance boost. So I'd say the overhead theory is correct. Looks like if I want to use Numpy for a performance boost I'll need to find a way of batching these operations together.",python performance numpy
1066758,A,"find length of sequences of identical values in a numpy array In a pylab program (which could probably be a matlab program as well) I have a numpy array of numbers representing distances: d[t] is the distance at time t (and the timespan of my data is len(d) time units). The events I'm interested in are when the distance is below a certain threshold and I want to compute the duration of these events. It's easy to get an array of booleans with b = d<threshold and the problem comes down to computing the sequence of the lengths of the True-only words in b. But I do not know how to do that efficiently (i.e. using numpy primitives) and I resorted to walk the array and to do manual change detection (i.e. initialize counter when value goes from False to True increase counter as long as value is True and output the counter to the sequence when value goes back to False). But this is tremendously slow. How to efficienly detect that sort of sequences in numpy arrays ? Below is some python code that illustrates my problem : the fourth dot takes a very long time to appear (if not increase the size of the array) from pylab import * threshold = 7 print '.' d = 10*rand(10000000) print '.' b = d<threshold print '.' durations=[] for i in xrange(len(b)): if b[i] and (i==0 or not b[i-1]): counter=1 if i>0 and b[i-1] and b[i]: counter+=1 if (b[i-1] and not b[i]) or i==len(b)-1: durations.append(counter) print '.' durations = [] counter = 0 for bool in b: if bool: counter += 1 elif counter > 0: durations.append(counter) counter = 0 if counter > 0: durations.append(counter) sure this is more consise but just as inefficient ; what I want to do is move the loop down to the C layer by means of using some clever combination of numpy calls... check my edited answer I now offer one such ""clever combinations"" (always trying hard not to be TOO clever though;-) -- but do measure the speed of that one AND the itertools.groupby-based solution and let us know which one is faster (and by how much) in examples realistic-for-you!  Here is a solution using only arrays: it takes an array containing a sequence of bools and counts the length of the transitions. >>> from numpy import array arange >>> b = array([000111000111100] dtype=bool) >>> sw = (b[:-1] ^ b[1:]); print sw [False False True False False True False False True False False False True False] >>> isw = arange(len(sw))[sw]; print isw [ 2 5 8 12] >>> lens = isw[1::2] - isw[::2]; print lens [3 4] sw contains a true where there is a switch isw converts them in indexes. The items of isw are then subtracted pairwise in lens. Notice that if the sequence started with an 1 it would count the length of the 0s sequences: this can be fixed in the indexing to compute lens. Also I have not tested corner cases such sequences of length 1.  Just in case anyone is curious (and since you mentioned MATLAB in passing) here's one way to solve it in MATLAB: threshold = 7; d = 10*rand(1100000); % Sample data b = diff([false (d < threshold) false]); durations = find(b == -1)-find(b == 1); I'm not too familiar with Python but maybe this could help give you some ideas. =) thanks for this answer as well this is exactly the kind of stuff I was looking for diff() exists in numpy too so this is more or less what you want though replace find(foo) with where(foo)[0].  While not numpy primitives itertools functions are often very fast so do give this one a try (and measure times for various solutions including this one of course): def runs_of_ones(bits): for bit group in itertools.groupby(bits): if bit: yield sum(group) If you do need the values in a list just can use list(runs_of_ones(bits)) of course; but maybe a list comprehension might be marginally faster still: def runs_of_ones_list(bits): return [sum(g) for b g in itertools.groupby(bits) if b] Moving to ""numpy-native"" possibilities what about: def runs_of_ones_array(bits): # make sure all runs of ones are well-bounded bounded = numpy.hstack(([0] bits [0])) # get 1 at run starts and -1 at run ends difs = numpy.diff(bounded) run_starts = numpy.where(difs > 0) run_ends = numpy.where(difs < 0) return run_ends - run_starts Again: be sure to benchmark solutions against each others in realistic-for-you examples! Hmmmmm... that last one looks familiar. ;) Thanks a lot ! The diff/where solution is exactly what I had in mind (not to mention it is about 10 times faster than the other solutions). Call that ""not too clever"" if you like but I wish I was clever enough to come up with it :-) @gnovice I don't do matlab (funny enough my daughter now a PhD candidate in advanced radio engineering does;-) but now looking at your answer I do see the analogies -- get the end-of-runs minus the start-of-runs get those by locating <0 and >0 spot in the differences and pad the bits with zeros to make sure all runs-of-ones do end. Guess there aren't that many ways to skin this ""run lengths"" problem!-) @Gyom you're welcome -- as @gnovice hints the matlab solution is also similar or so I guess it would be if one knew matlab -- so it must be that neither is very clever;-)... it's more a question of having had to do run-length coding stuff before (most of the time in my edit was about translating from Numeric which is what I still tend instinctively to turn to to much-better numpy -- but where I actually first learned such things was with APL 30 years ago when I was still a hardware designer...!-).",python matlab numpy pylab
1846836,A,"the best shortest path algorithm what is the difference between the ""Floyd-Warshall algorithm"" and ""Dijkstra's Algorithm"" and which is the best for finding the shortest path in a graph? I need to calculate the shortest path between all the pairs in a net and save the results to an array as follows: **A B C D E** A 0 10 15 5 20 B 10 0 5 5 10 C 15 5 0 10 15 D 5 5 10 0 15 E 20 10 15 15 0 thanks for your answers but the other one was closed mostly because of the user's bad english and one of the solutions named these exact two algorithms as alternatives. If we close this as dup how will the author find out more about the previous question? Will we really all be nice enough to go over there and vote to reopen? hi sorry but wanted to add an array example with respect to a picture but I did not do thanks SilentGhost for re-edit my question Shouldn't DE in that graph be 15? In the meanwhile better algorithms for the single source shortest path problem are known. A practically relevant one is a derivation of Dijkstra's algorithm by Torben Hagerup. The algorithm has the same worst case complexity as Djikstra's but in the average case the expected runtime is linear in the size of the graph which is much faster than the pure Dijkstra. The idea of the algorithm is based on the idea that there is no need to always poll the minimum edge from the queue. It is possible poll an edge from the queue whose weight is 1+k times as large as the minimum edge weight where k is some number larger 0. Even if such an edge is chosen the algorithm will still find the shortest path.  Dijkstra's algorithm finds the shortest path between a node and every other node in the graph. You'd run it once for every node. Weights must be non-negative so if necessary you have to normalise the values in the graph first. Floyd-Warshall calculates the shortest routes between all pairs of nodes in a single run! Cycle weights must be non-negative and the graph must be directed (your diagram is not). Johnson's algorithm is using Dijkstra's algorithm to find all pairs in a single pass and is faster for sparse trees (see the link for analysis). From the wikipedia link you cite for Dijkstra: ""the algorithm finds the path with lowest cost between that vertex and **every** other vertex"" (my emphasis). You thus don't need to run it for every pair of vertex but only for every vertex. thx Andreas fixed You can convert an undirected graph to a directed graph by replacing every edge uv with two edges (uv) and (vu) with the same weight. Then presumably Floyd-Warshall should work just fine? err .. floyd-warshall does not require it to have non-negative edges from wikipedia ""is a graph analysis algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles)""  Floyd Warshall find the paths between all pairs of vertices but Dijkstra only finds the path from one vertex to all others. Floyd Warshall is O(|V|3) and Dikstra is O(|E| + |V| log |V|) but you'll have to run it V times to find all pairs which gives a complexity of O(|E * V| + |V2| log |V|) I guess. This means it's possibly faster to use Dijsktra repeatedly than the FW algorithm I would try both approaches and see which one is fastest in the actual case. Francis Haart's comment: ""@Andreas Brinck in a complete graph E=(V^2-V)/2 and dijkstra's would be no faster.""  Use the Floyd-Warshall algorithm if you want to find the shortest path between all pairs of vertexes as it has a (far) higher running time than Dijkstra's algorithm. The Floyd-Warshall algorithm has a worst case performance of O(|V|3) where as Dijkstra's has a worse case performance of O(|E| + |V|log |V|)  Dijkstra's is mainly for single pair shortest path finding i.e. from one node to all other nodes where as Floyd-Warshall is for all-pair shortest path i.e. shortest path between all pair of vertices. The Floyd-Warshall algorithm has a worst case performance of O(|V|3) where as Dijkstra's has a worse case performance of O(|E| + |V|log |V|) Also Dijkstra's cannot be used for negative weights ( we use Bellmann Ford for the same ). but for Floyd-Warshall we can use negative weights but no negative cycles  Dijkstra finds the shortest path from only one vertex Floyd-Warshall finds it between all of them.",python algorithm numpy shortest-path
1564000,A,List of tuples to Numpy recarray Given a list of tuples where each tuple represents a row in a table e.g. tab = [('a'1)('b'2)] Is there an easy way to convert this to a record array? I tried np.recarray(tabdtype=[('name'str)('value'int)]) which doesn't seem to work. try np.rec.fromrecords(tab) rec.array([('a' 1) ('b' 2)] dtype=[('f0' '|S1') ('f1' '<i4')]),python numpy
1250367,A,"How to pickle numpy's Inf objects? When trying to pickle the object Inf as defined in numpy (I think) the dumping goes Ok but the loading fails: >>> cPickle.dump(Inf file(""c:/temp/a.pcl""'wb')) >>> cPickle.load(file(""c:/temp/a.pcl""'rb')) Traceback (most recent call last): File ""<pyshell#257>"" line 1 in <module> cPickle.load(file(""c:/temp/a.pcl""'rb')) ValueError: could not convert string to float >>> type(Inf) <type 'float'> Why is that? And moreover - is there a way to fix that? I want to pickle something that has Inf in it - changing it to something else will flaw the elegance of the program... Thanks If you specify a pickle protocol more than zero it will work. Protocol is often specified as -1 meaning use the latest and greatest protocol: >>> cPickle.dump(Inf file(""c:/temp/a.pcl""'wb') -1) >>> cPickle.load(file(""c:/temp/a.pcl""'rb')) 1.#INF -- may be platform dependent what prints here.  Try this solution at SourceForge which will work for any arbitrary Python object: y_serial.py module :: warehouse Python objects with SQLite ""Serialization + persistance :: in a few lines of code compress and annotate Python objects into SQLite; then later retrieve them chronologically by keywords without any SQL. Most useful ""standard"" module for a database to store schema-less data."" http://yserial.sourceforge.net",python numpy pickle
1783369,A,String preallocation in numpy.arrays >>> import numpy as np >>> a = np.array(['zero' 'one' 'two' 'three']) >>> a[1] = 'thirteen' >>> print a ['zero' 'thirt' 'two' 'three'] >>> As you can see the second element has been truncated to the maximum number of characters in the original array. Is it possible to workaround this problem? If you don't know the maximum length element then you can use dtype=object >>> import numpy as np >>> a = np.array(['zero' 'one' 'two' 'three'] dtype=object) >>> a[1] = 'thirteen' >>> print a ['zero' 'thirteen' 'two' 'three'] >>> But then you lose the performance advantages of having an allocated contigous block of memory so you may as well use a python list.  Use the dtype argument in numpy.array e.g.: >>> import numpy as np >>> a = np.array(['zero' 'one' 'two' 'three'] dtype='S8') >>> a[1] = 'thirteen' >>> print(a) ['zero' 'thirteen' 'two' 'three'],python numpy
500328,A,"Identifying numeric and array types in numpy Is there an existing function in numpy that will tell me if a value is either a numeric type or a numpy array? I'm writing some data-processing code which needs to handle numbers in several different representations (by ""number"" I mean any representation of a numeric quantity which can be manipulated using the standard arithmetic operators + - * / **). Some examples of the behavior I'm looking for >>> is_numeric(5) True >>> is_numeric(123.345) True >>> is_numeric('123.345') False >>> is_numeric(decimal.Decimal('123.345')) True >>> is_numeric(True) False >>> is_numeric([1 2 3]) False >>> is_numeric([1 '2' 3]) False >>> a = numpy.array([1 2.3 4.5 6.7 8.9]) >>> is_numeric(a) True >>> is_numeric(a[0]) True >>> is_numeric(a[1]) True >>> is_numeric(numpy.array([numpy.array([1]) numpy.array([2])]) True >>> is_numeric(numpy.array(['1']) False If no such function exists I know it shouldn't be hard to write one something like isinstance(n (int float decimal.Decimal numpy.number numpy.ndarray)) but are there other numeric types I should include in the list? What should return `is_numeric([123])` and `is_numeric([1 '2' 3])`? False in both cases. I'll edit that into the question. What about `numpy.array([numpy.array([1]) numpy.array([2])])`? `numpy.array(['1'])`? ""Testing by trying to do math"" will not catch `bool` values or numpy arrays of them. In general the flexible fast and pythonic way to handle unknown types is to just perform some operation on them and catch an exception on invalid types. try: a = 5+'5' except TypeError: print ""Oops"" Seems to me that this approach is easier than special-casing out some function to determine absolute type certainty. It is not an answer to the question but I agree completely. The problem is that '5'*5 does work. @JF that's true but I still like this answer to this question. I think the str*int bug will be easier to catch than the is_numeric function will be to write. Also consider that IIRC no other mathematical operation is defined where the two operands are str and int. I like it too... if only I could accept more than one ;-) `TrueFalse` (and numpy arrays of them) ""look like numbers"" by this test.  isinstance(numpy.int32(4) numbers.Number) returns False so that doesn't quite work. operator.isNumberType() does work on all the variants of numpy numbers however including numpy.array([1]).  Your is_numeric is ill-defined. See my comments to your question. Other numerical types could be: long complex fractions.Fraction numpy.bool_ numpy.ubyte ... operator.isNumberType() returns True for Python numbers and numpy.array. Since Python 2.6 you can use isinstance(d numbers.Number) instead of deprecated operator.isNumberType(). Generally it is better to check the capabilities of the object (e.g. whether you can add an integer to it) and not its type. Yeah but if I had an exact definition in mind I could have written the function ;-) I edited in some more information.  As others have answered there could be other numeric types besides the ones you mention. One approach would be to check explicitly for the capabilities you want with something like def is_numeric(obj): attrs = ['__add__' '__sub__' '__mul__' '__div__' '__pow__'] return all(hasattr(obj attr) for attr in attrs) This works for all your examples except the last one numpy.array(['1']). That's because numpy.ndarray has the special methods for numeric operations but raises TypeError if you try to use them inappropriately with string or object arrays. You could add an explicit check for this like  ... and not (isinstance(obj ndarray) and obj.dtype.kind in 'OSU') This may be good enough. But... you can never be 100% sure that somebody won't define another type with the same behavior so a more foolproof way is to actually try to do a calculation and catch the exception something like def is_numeric_paranoid(obj): try: obj+obj obj-obj obj*obj obj**obj obj/obj except ZeroDivisionError: return True except Exception: return False else: return True but depending on how often you plan to call use it and with what arguments this may not be practical (it can be potentially slow e.g. with large arrays). `TrueFalse` and numpy arrays of them look like numbers by these tests so they'd need to be included in the `and not` clause; assuming that you don't want to do math on them.  Also numpy has numpy.isreal and other similar functions (numpy.is + Tab should list them). They all have their fun corner cases but one of those could be useful.",python numpy
1599754,A,"Is there easy way in python to extrapolate data points to the future? I have a simple numpy array for every date there is a data point. Something like this: >>> import numpy as np >>> from datetime import date >>> from datetime import date >>> x = np.array( [(date(200835) 4800 ) (date(2008315) 4000 ) (date(20083 20) 3500 ) (date(200845) 3000 ) ] ) Is there easy way to extrapolate data points to the future: date(200851) date(2008 5 20) etc? I understand it can be done with mathematical algorithms. But here I am seeking for some low hanging fruit. Actually I like what numpy.linalg.solve does but it does not look applicable for the extrapolation. Maybe I am absolutely wrong. Actually to be more specific I am building a burn-down chart (xp term): 'x=date and y=volume of work to be done' so I have got the already done sprints and I want to visualise how the future sprints will go if the current situation persists. And finally I want to predict the release date. So the nature of 'volume of work to be done' is it always goes down on burn-down charts. Also I want to get the extrapolated release date: date when the volume becomes zero. This is all for showing to dev team how things go. The preciseness is not so important here :) The motivation of dev team is the main factor. That means I am absolutely fine with the very approximate extrapolation technique. When you googled for ""statistics python"" what did you find? Any questions on any of the statistical packages you found? It is hard to talk about any extrapolation without knowing the nature of the data in question. The above as far as one can see could be anything (not excluding random values) so to talk about any practical approach would be just speculating. Refine the question. you are absolutely right! refined. A simple way of doing extrapolations is to use interpolating polynomials or splines: there are many routines for this in scipy.interpolate and there are quite easy to use (just give the (x y) points and you get a function [a callable precisely]). Now as as been pointed in this thread you cannot expect the extrapolation to be always meaningful (especially when you are far from your data points) if you don't have a model for your data. However I encourage you to play with the polynomial or spline interpolations from scipy.interpolate to see whether the results you obtain suit you. like this definitely going to try thanks a lot!  The mathematical models are the way to go in this case. For instance if you have only three data points you can have absolutely no indication on how the trend will unfold (could be any of two parabola.) Get some statistics courses and try to implement the algorithms. Try Wikibooks. absolutely agree do understand it but want to clarify I am just checking if by some chance there is numpy.extrapolate function already in place with argument ""choose extrapolation method"" :) That's why I call it ""low hanging fruit""  It's all too easy for extrapolation to generate garbage; try this. Many different extrapolations are of course possible; some produce obvious garbage some non-obvious garbage many are ill-defined. """""" extrapolate ymd data with scipy UnivariateSpline """""" import numpy as np from scipy.interpolate import UnivariateSpline # pydoc scipy.interpolate.UnivariateSpline -- fitpack unclear from datetime import date from pylab import * # ipython -pylab __version__ = ""denis 23oct"" def daynumber( ymd ): """""" 200511 -> 0 200611 -> 365 ... """""" return date( ymd ).toordinal() - date( 200511 ).toordinal() days values = np.array([ (daynumber(200511) 1.2 ) (daynumber(200541) 1.8 ) (daynumber(200591) 5.3 ) (daynumber(2005101) 5.3 ) ]).T dayswanted = np.array([ daynumber( year month 1 ) for year in range( 2005 2006+1 ) for month in range( 1 12+1 )]) np.set_printoptions( 1 ) # .1f print ""days:"" days print ""values:"" values print ""dayswanted:"" dayswanted title( ""extrapolation with scipy.interpolate.UnivariateSpline"" ) plot( days values ""o"" ) for k in (123): # line parabola cubicspline extrapolator = UnivariateSpline( days values k=k ) y = extrapolator( dayswanted ) label = ""k=%d"" % k print label y plot( dayswanted y label=label ) # pylab legend( loc=""lower left"" ) grid(True) savefig( ""extrapolate-UnivariateSpline.png"" dpi=50 ) show() Added: a Scipy ticket says ""The behavior of the FITPACK classes in scipy.interpolate is much more complex than the docs would lead one to believe"" -- imho true of other software doc too. very good example! thank you!  You have to swpecify over which function you need extrapolation. Than you can use regression http://en.wikipedia.org/wiki/Regression%5Fanalysis to find paratmeters of function. And extrapolate this in future. For instance: translate dates into x values and use first day as x=0 for your problem the values shoul be aproximatly (01.2) (4001.8)(9005.3) Now you decide that his points lies on function of type a+b*x+c*x^2 Use the method of least squers to find ab and c http://en.wikipedia.org/wiki/Linear%5Fleast%5Fsquares (i will provide full source but later beacuase I do not have time for this)",python numpy interpolation spline burndowncharts
1520234,A,"How to check which version of Numpy I'm using? How can I check which version of Numpy I'm using? I'm using Mac OS X 10.6.1 Snow Leopard. numpy.version.version +1 - That looks like a better way of doing it than mine. @Dominic Rodger: yeah but your is more general to any module that cares to set a `__version__`. This is not the public API numpy.__version__ is. Actually `import numpy ; numpy.version.version` . The lack of `import numpy` through me an obvious newbie. Since the use of `__version__` in recommended in PEP8 and most packages support `__version__` vs the non standard `version.version` I think that this answer should be treated more as a curiosity than an accepted method. Use `numpy.__version__` or `.__version__` as [Dominic Rodger's answer recommends](http://stackoverflow.com/a/1520264/298607) Parse the version (and create your own version strings) as recommended in PEP 386 / PEP 440.  You can also check if your version is using MKL with: import numpy numpy.show_config()  from the command line you can simply issue: python -c ""import numpy; print numpy.version.version"" or python -c ""import numpy; print numpy.__version__""  numpy.version.version type it. This answer doesn't improve on the identical answer given (and accepted) four years before this post.  >> import numpy >> print numpy.__version__ Thank you for your help! @keijo - You might want to consider changing your accepted answer to SilentGhost's. This is the API we numpy developers will support. numpy.version.version is an implementation detail that should not be relied upon. The same works for scipy ! ....",python numpy
1727950,A,"Just Curious about Python+Numpy to Realtime Gesture Recognition i 'm just finish labs meeting with my advisor previous code is written in matlab and it run offline mode not realtime mode so i decide to convert to python+numpy (in offline version) but after labs meeting my advisor raise issue about speed of realtime recognition so i have doubt about speed of python+numpy to do this project. or better in c? my project is about using electronic glove (2x sensors) to get realtime data and do data processing recognition process ohh don't know about accepting answers thankfor that :) What size arrays (N) are you dealing with? I've found for some problems the speed advantage of numpy over pure python only become recognizable when N > 10000 or so. In fact pure python can be faster than numpy when dealing with small arrays since there is a performance hit associated with importing numpy and creating the arrays. NumPy is very fast if you follow some basic rules. You should avoid Python loops using the operators provided by NumPy instead whenever you can. This and this should be a good starting points. After reading through that why don't you write some simple code in both Matlab and NumPy and compare the performance? If it performs well in NumPy it should be enough to convince your advisor especially if the code is representative of the actual algorithms you are using in your project. Note: you should also see that your algorithm really is suited for realtime recognition.  You might look at OpenCV which has Python libs ctypes-opencv and opencv-cython; I haven't used these myself. Ideally you want to combine a fast-running C inner loop with a flexible Python/Numpy play-with-algorithms. Bytheway google ""opencv gesture recognition"" → 6680 hits.  I think the answer depends on three things: how well you code in Matlab how well you code in Python/Numpy and your algorithm. Both Matlab and Python can be fast for number crunching if you're diligent about vectorizing everything and using library calls. If your Matlab code is already very good I would be surprised if you saw much performance benefit moving to Numpy unless there's some specific idiom you can use to your advantage. You might not even see a large benefit moving to C. I this case your effort would likely be better spent tuning your algorithm. If your Matlab code isn't so good you could 1) write better Matlab code 2) rewrite in good Numpy code or 3) rewrite in C.",python c numpy gesture-recognition
310459,A,"Adding a dimension to every element of a numpy.array I'm trying to transform each element of a numpy array into an array itself (say to interpret a greyscale image as a color image). In other words: >>> my_ar = numpy.array((0510)) [0 5 10] >>> transformed = my_fun(my_ar) # In reality my_fun() would do something more useful array([ [ 0 0 0] [ 5 10 15] [10 20 30]]) >>> transformed.shape (3 3) I've tried: def my_fun_e(val): return numpy.array((val val*2 val*3)) my_fun = numpy.frompyfunc(my_fun_e 1 3) but get: my_fun(my_ar) (array([[0 0 0] [ 5 10 15] [10 20 30]] dtype=object) array([None None None] dtype=object) array([None None None] dtype=object)) and I've tried: my_fun = numpy.frompyfunc(my_fun_e 1 1) but get: >>> my_fun(my_ar) array([[0 0 0] [ 5 10 15] [10 20 30]] dtype=object) This is close but not quite right -- I get an array of objects not an array of ints. Update 3! OK. I've realized that my example was too simple beforehand -- I don't just want to replicate my data in a third dimension I'd like to transform it at the same time. Maybe this is clearer? Use map to apply your transformation function to each element in my_ar: import numpy my_ar = numpy.array((0510)) print my_ar transformed = numpy.array(map(lambda x:numpy.array((xx*2x*3)) my_ar)) print transformed print transformed.shape  I propose:  numpy.resize(my_ar (33)).transpose() You can of course adapt the shape (my_ar.shape[0])*2 or whatever  Does numpy.dstack do what you want? The first two indexes are the same as the original array and the new third index is ""depth"". >>> import numpy as N >>> a = N.array([[123][456][789]]) >>> a array([[1 2 3] [4 5 6] [7 8 9]]) >>> b = N.dstack((aaa)) >>> b array([[[1 1 1] [2 2 2] [3 3 3]] [[4 4 4] [5 5 5] [6 6 6]] [[7 7 7] [8 8 8] [9 9 9]]]) >>> b[11] array([5 5 5])  Does this do what you want: tile(my_ar (113))",python arrays numpy
795570,A,"Correlate one set of vectors to another in numpy? Let's say I have a set of vectors (readings from sensor 1 readings from sensor 2 readings from sensor 3 -- indexed first by timestamp and then by sensor id) that I'd like to correlate to a separate set of vectors (temperature humidity etc -- also all indexed first by timestamp and secondly by type). What is the cleanest way in numpy to do this? It seems like it should be a rather simple function... In other words I'd like to see: > a.shape (36520) > b.shape (365 5) > correlations = magic_correlation_function(ab) > correlations.shape (20 5) Cheers /YGA P.S. I've been asked to add an example. Here's what I would like to see: $ In [27]: x $ Out[27]: array([[ 0 0 0] [-1 0 -1] [-2 0 -2] [-3 0 -3] [-4 0.1 -4]]) $ In [28]: y $ Out[28]: array([[0 0] [1 0] [2 0] [3 0] [4 0.1]]) $ In [28]: magical_correlation_function(x y) $ Out[28]: array([[-1.  0.70710678 1. ] [-0.70710678 1.  0.70710678]]) Ps2: whoops mis-transcribed my example. Sorry all. Fixed now. It's not obvious to me what you're trying to do could you maybe post example input and output (for some smaller size data)? What formula are you using to arrive at those numbers? I can't seem to reproduce them with any normal correlation/covariance formulas (but then I'm no expert in statistics). The simplest thing that I could find was using the scipy.stats package In [8]: x Out[8]: array([[ 0.  0.  0. ] [-1.  0.  -1. ] [-2.  0.  -2. ] [-3.  0.  -3. ] [-4.  0.1 -4. ]]) In [9]: y Out[9]: array([[0.  0. ] [1.  0. ] [2.  0. ] [3.  0. ] [4.  0.1]]) In [10]: import scipy.stats In [27]: (scipy.stats.cov(yx) /(numpy.sqrt(scipy.stats.var(yaxis=0)[:numpy.newaxis])) /(numpy.sqrt(scipy.stats.var(xaxis=0)))) Out[27]: array([[-1.  0.70710678 -1. ] [-0.70710678 1.  -0.70710678]]) These aren't the numbers you got but you've mixed up your rows. (Element [00] should be 1.) A more complicated but purely numpy solution is In [40]: numpy.corrcoef(x.Ty.T)[numpy.arange(x.shape[1])[numpy.newaxis:] numpy.arange(y.shape[1])[:numpy.newaxis]] Out[40]: array([[-1.  0.70710678 -1. ] [-0.70710678 1.  -0.70710678]]) This will be slower because it computes the correlation of each element in x with each other element in x which you don't want. Also the advanced indexing techniques used to get the subset of the array you desire can make your head hurt. If you're going to use numpy intensely get familiar with the rules on broadcasting and indexing. They will help you push as much down to the C-level as possible. I've updated the question with the ""right"" inputs -- prob. makes sense to update the response just so as not to confuse people :-) Done. I've also added links to some helpful documentation.  Will this do what you want? correlations = dot(transpose(a) b)  As David said you should define the correlation you're using. I don't know of any definitions of correlation that gives sensible numbers when correlating empty and non-empty signals.",python numpy
1544948,A,"python numpy savetxt Can someone indicate what I am doing wrong here? import numpy as np a = np.array([12345]dtype=int) b = np.array(['a''b''c''d''e']dtype='|S1') np.savetxt('test.txt'zip(ab)fmt=""%i %s"") The output is: Traceback (most recent call last): File ""loadtxt.py"" line 6 in <module> np.savetxt('test.txt'zip(ab)fmt=""%i %s"") File ""/Users/tom/Library/Python/2.6/site-packages/numpy/lib/io.py"" line 785 in savetxt fh.write(format % tuple(row) + '\n') TypeError: %d format: a number is required not numpy.string_ I think the problem you are having is that you are passing tuples through the formating string and it can't interpret the tuple with %i. Try using fmt=""%s"" assuming this is what you are looking for as the output: 1 a 2 b 3 c 4 d 5 e that's just wrong. `fmt=""%s""` works for entirely different reasons `fmt=""%s %s""` works too btw. You're right as soon as I posted I realized it worked but not for the reason I thought. My bad. The post by SilentGhost is much better. Thanks.  You need to construct you array differently: z = np.array(zip([12345] ['a''b''c''d''e']) dtype=[('int' int) ('str' '|S1')]) np.savetxt('test.txt' z fmt='%i %s') when you're passing a sequence savetext performs asarray(sequence) call and resulting array is of type |S4 that is all elements are strings! that's why you see this error. one small comment - it should be savetxt() instead of savetext()  If you want to save a CSV file you can also use the function rec2csv (included in matplotlib.mlab) >>> from matplotlib.mlab import rec2csv >>> rec = array([(1.0 2) (3.0 4)] dtype=[('x' float) ('y' int)]) >>> rec = array(zip([12345] ['a''b''c''d''e']) dtype=[('x' int) ('y' str)]) >>> rec2csv(rec 'recordfile.txt' delimiter=' ') hopefully one day pylab's developers will implement a decent support to writing csv files.",python numpy
1057666,A,"Using Python to replace MATLAB: how to import data? I want to use some Python libraries to replace MATLAB. How could I import Excel data in Python (for example using NumPy) to use them? I don't know if Python is a credible alternative to MATLAB but I want to try it. Is there a a tutorial? One way to interpret the original question is: how can I read a (proprietary) Excel (.xls .xlsx) file into Python. An answer to this could be useful but I don't have one. So the general flow here is to export in another format from Excel namely CSV. Depending on what kind of computations you are doing with MATLAB (and on which toolboxes you are using) Python could be a good alternative to MATLAB. Python + NumPy + SciPy + Matplotlib are the right combination to start. For the data you can for example save your data directly in text file (assuming that you are not directly concerned by floating-point precision issues) and read it in Python. If your data are Excel data where each value is separated by a "";"" you can for example read the file line by line and use the split() method (with "";"" as argument) to get each value. For MATLAB up to version 7.1 it is possible to directly load .mat files from Python with the scipy.io.matlab.mio module.  If you saved you data in MATLAB format use: from scipy.io import loadmat datafile = ""yourfile.mat"" data = loadmat(datafile matlab_compatible=True) var1 = data['nameOfYourVariable'].squeeze() var2 = data['nameOfYourOtherVariable'].squeeze()  Pandas is a Python data analysis library that can import/export from Excel pretty easily. Here's how to do it: http://pandas.pydata.org/pandas-docs/stable/10min.html#excel Crash course: import pandas as pd data = pd.read_excel('foo.xlsx' 'Sheet1' index_col=None na_values=['NA'])  I had a look at mlabwrap as a step to easing some MATLAB developers into using Python more. But I have been unable to cleanly build it and I don't run production installs here so I'm dead in the water.  If you come from the MATLAB world Pylab will ease your transition. Once you have converted your data to ASCII pylab.load() will do the rest: pylab.load(fname comments='#' delimiter=None converters=None skiprows=0 usecols=None unpack=False dtype=<type 'numpy.float64'>)  There are probably hundreds of ways you could import text data into Python. But since you want to replace MATLAB you're going be using NumPy and probably SciPy. Keep things simple: use NumPy's standard text-loading: import numpy imported_array = numpy.loadtxt('file.txt'delimiter='\t') # Assuming tab-delimiter print imported_array.shape  There's Matplotlib for plots and the csv module for reading Excel data (assuming you can dump to CSV). Here's a tutorial about replacing MATLAB with Python. SAGE (mentionend in the tutorial see http://sagemath.org/) is a pretty cool stack based on Python. Thanx a lot. I'm going to try SAGE.  ""I don't know if Python is a credible alternative to MATLAB"" For me (experimental physics) Python is not only a full replacement for MATLAB (when including SciPy and Matplotlib as mentioned above) but it is useful for many things other than data crunching and visualisation (such are general programming needs). ""I'm going to try SAGE."" It is worth noting that there are a couple of servers running Sage which offer the notebook environmet (check Try Sage online in http://www.sagemath.org/). This is pretty neat given the fact that all you need it is an Internet browser and access (no installation required). As for the question as interpreted by Kevin Buchs (in another answer) reading proprietary Excel to Python can be done in several methods some are platform (OS) dependent: A nice resource (platform independent) - http://www.python-excel.org/ An example using xlrd which I once found useful (this is what I used when I needed it): http://code.activestate.com/recipes/483742/ for an example based on xlrd (platform independent) pyexcelerator is another option. I hope this helps. If not I can try to arrange some example code myself (though the ones I have are over six years old...). I personally prefer as was proposed in the other answers to use the CSV or ASCII format.",python excel matlab numpy
1730600,A,"Principal component analysis in Python I'd like to use principal component analysis (PCA) for dimensionality reduction. Does numpy or scipy already have it or do I have to roll my own using numpy.linalg.eigh? I don't just want to use singular value decomposition (SVD) because my input data are quite high-dimensional (~460 dimensions) so I think SVD will be slower than computing the eigenvectors of the covariance matrix. I was hoping to find a premade debugged implementation that already makes the right decisions for when to use which method and which maybe does other optimizations that I don't know about. Here is another implementation of a PCA module for python using numpy scipy and C-extensions. The module carries out PCA using either a SVD or the NIPALS (Nonlinear Iterative Partial Least Squares) algorithm which is implemented in C.  You do not need full Singular Value Decomposition (SVD) at it computes all eigenvalues and eigenvectors and can be prohibitive for large matrices. scipy and its sparse module provide generic linear algrebra functions working on both sparse and dense matrices among which there is the eig* family of functions : http://docs.scipy.org/doc/scipy/reference/sparse.linalg.html#matrix-factorizations Scikit-learn provides a Python PCA implementation which only support dense matrices for now. Timings : In [1]: A = np.random.randn(1000 1000) In [2]: %timeit scipy.sparse.linalg.eigsh(A) 1 loops best of 3: 802 ms per loop In [3]: %timeit np.linalg.svd(A) 1 loops best of 3: 5.91 s per loop Ah now I see that that the relative speed of the sparse vs nonsparse methods depends on the size of the matrix. If I use your example where A is a 1000*1000 matrix then `eigsh` and `svds` are faster than `eigh` and `svd` by a factor of ~3 but if A is smaller say 100*100 then `eigh` and `svd` are quicker by factors of ~4 and ~1.5 respectively. T would still use sparse SVD over sparse eigenvalue decomposition though. Indeed I think I am biased toward large matrices. To me large matrices are more like 10⁶ * 10⁶ than 1000 * 1000. In those case you often can't even store the covariance matrices ... You don't need to compute sparse matrices from dense matrices. The algorithms provided in the sparse.linalg module rely only on the matrice vector multiplication operation through the matvec method of the Operator object. For dense matrices this is just something like matvec=dot(A x). For the same reason you don't need to compute the covariance matrix but only to provide the operation dot(A.T dot(A x)) for A. Not really a fair comparison since you still need to compute the covariance matrix. Also it's probably only worth using the sparse linalg stuff for very large matrices since it seems to be quite slow to construct sparse matrices from dense matrices. for example `eigsh` is actually ~4x slower than `eigh` for nonsparse matrices. The same is true for `scipy.sparse.linalg.svds` versus `numpy.linalg.svd`. I would always go with SVD over eigenvalue decomposition for the reasons that @dwf mentioned and perhaps use sparse version of SVD if the matrices get really huge.  You can use sklearn: import sklearn.decomposition as deco import numpy as np x = (x - np.mean(x 0)) / np.std(x 0) # You need to normalize your data first pca = deco.PCA(n_components) # n_components is the components number after reduction x_r = pca.fit(x).transform(x) print ('explained variance (first %d components): %.2f'%(n_components sum(pca.explained_variance_ratio_))) Upvoted because this works nicely for me - I have more than 460 dimensions and even though sklearn uses SVD and the question requested non-SVD I think 460 dimensions is likely to be OK.  PCA using scipy.linalg.svd is super easy. Here's a simple demo: import numpy as np import matplotlib.pyplot as plt from scipy.linalg import svd from scipy.misc import lena # the underlying signal is a sinusoidally modulated image img = lena() t = np.arange(100) time = np.sin(0.1*t) real = time[:np.newaxisnp.newaxis] * img[np.newaxis...] # we add some noise noisy = real + np.random.randn(*real.shape)*255 # (observations features) matrix M = noisy.reshape(noisy.shape[0]-1) # singular value decomposition factorises your data matrix such that: # # M = U*S*V.T (where '*' is matrix multiplication) # # * U and V are the singular matrices containing orthogonal vectors of # unit length in their rows and columns respectively. # # * S is a diagonal matrix containing the singular values of M - these # values squared divided by the number of observations will give the # variance explained by each PC. # # * if M is considered to be an (observations features) matrix the PCs # themselves would correspond to the rows of S^(1/2)*V.T. if M is # (features observations) then the PCs would be the columns of # U*S^(1/2). # # * since U and V both contain orthonormal vectors U*V.T is equivalent # to a whitened version of M. U s Vt = svd(M full_matrices=False) V = Vt.T # sort the PCs by descending order of the singular values (i.e. by the # proportion of total variance they explain) ind = np.argsort(s)[::-1] U = U[: ind] s = s[ind] V = V[: ind] # if we use all of the PCs we can reconstruct the noisy signal perfectly S = np.diag(s) Mhat = np.dot(U np.dot(S V.T)) print ""Using all PCs MSE = %.6G"" %(np.mean((M - Mhat)**2)) # if we use only the first 20 PCs the reconstruction is less accurate Mhat2 = np.dot(U[: :20] np.dot(S[:20 :20] V[::20].T)) print ""Using first 20 PCs MSE = %.6G"" %(np.mean((M - Mhat2)**2)) fig [ax1 ax2 ax3] = plt.subplots(1 3) ax1.imshow(img) ax1.set_title('true image') ax2.imshow(noisy.mean(0)) ax2.set_title('mean of noisy images') ax3.imshow((s[0]**(1./2) * V[:0]).reshape(img.shape)) ax3.set_title('first spatial PC') plt.show()  SVD should work fine with 460 dimensions. It takes about 7 seconds on my Atom netbook. The eig() method takes more time (as it should it uses more floating point operations) and will almost always be less accurate. If you have less than 460 examples then what you want to do is diagonalize the scatter matrix (x - datamean)^T(x - mean) assuming your data points are columns and then left-multiplying by (x - datamean). That might be faster in the case where you have more dimensions than data. can you describe more in detail this trick when you have more dimensions than data? Basically you assume that the eigenvectors are linear combinations of the data vectors. See Sirovich (1987). ""Turbulence and the dynamics of coherent structures.""  You can quite easily ""roll"" your own using scipy.linalg (assuming a pre-centered dataset data): covmat = data.dot(data.T) evs evmat = scipy.linalg.eig(covmat) Then evs are your eigenvalues and evmat is your projection matrix. If you want to keep d dimensions use the first d eigenvalues and first d eigenvectors. Given that scipy.linalg has the decomposition and numpy the matrix multiplications what else do you need? cov matrix is np.dot(data.Tdataout=covmat) where data must be centered matrix.  Months later here's a small class PCA and a picture: #!/usr/bin/env python """""" a small class for Principal Component Analysis Usage: p = PCA( A fraction=0.90 ) In: A: an array of e.g. 1000 observations x 20 variables 1000 rows x 20 columns fraction: use principal components that account for e.g. 90 % of the total variance Out: p.U p.d p.Vt: from numpy.linalg.svd A = U . d . Vt p.dinv: 1/d or 0 see NR p.eigen: the eigenvalues of A*A in decreasing order (p.d**2). eigen[j] / eigen.sum() is variable j's fraction of the total variance; look at the first few eigen[] to see how many PCs get to 90 % 95 % ... p.npc: number of principal components e.g. 2 if the top 2 eigenvalues are >= `fraction` of the total. It's ok to change this; methods use the current value. Methods: The methods of class PCA transform vectors or arrays of e.g. 20 variables 2 principal components and 1000 observations using partial matrices U' d' Vt' parts of the full U d Vt: A ~ U' . d' . Vt' where e.g. U' is 1000 x 2 d' is diag([ d0 d1 ]) the 2 largest singular values Vt' is 2 x 20. Dropping the primes d . Vt 2 principal vars = p.vars_pc( 20 vars ) U 1000 obs = p.pc_obs( 2 principal vars ) U . d . Vt 1000 obs p.obs( 20 vars ) = pc_obs( vars_pc( vars )) fast approximate A . vars using the `npc` principal components Ut 2 pcs = p.obs_pc( 1000 obs ) V . dinv 20 vars = p.pc_vars( 2 principal vars ) V . dinv . Ut 20 vars p.vars( 1000 obs ) = pc_vars( obs_pc( obs )) fast approximate Ainverse . obs: vars that give ~ those obs. Notes: PCA does not center or scale A; you usually want to first A -= A.mean(A axis=0) A /= A.std(A axis=0) with the little class Center or the like below. See also: http://en.wikipedia.org/wiki/Principal_component_analysis http://en.wikipedia.org/wiki/Singular_value_decomposition Press et al. Numerical Recipes (2 or 3 ed) SVD PCA micro-tutorial iris-pca .py .png """""" from __future__ import division import numpy as np dot = np.dot # import bz.numpyutil as nu # dot = nu.pdot __version__ = ""2010-04-14 apr"" __author_email__ = ""denis-bz-py at t-online dot de"" #............................................................................... class PCA: def __init__( self A fraction=0.90 ): assert 0 <= fraction <= 1 # A = U . diag(d) . Vt O( m n^2 ) lapack_lite -- self.U self.d self.Vt = np.linalg.svd( A full_matrices=False ) assert np.all( self.d[:-1] >= self.d[1:] ) # sorted self.eigen = self.d**2 self.sumvariance = np.cumsum(self.eigen) self.sumvariance /= self.sumvariance[-1] self.npc = np.searchsorted( self.sumvariance fraction ) + 1 self.dinv = np.array([ 1/d if d > self.d[0] * 1e-6 else 0 for d in self.d ]) def pc( self ): """""" e.g. 1000 x 2 U[: :npc] * d[:npc] to plot etc. """""" n = self.npc return self.U[: :n] * self.d[:n] # These 1-line methods may not be worth the bother; # then use U d Vt directly -- def vars_pc( self x ): n = self.npc return self.d[:n] * dot( self.Vt[:n] x.T ).T # 20 vars -> 2 principal def pc_vars( self p ): n = self.npc return dot( self.Vt[:n].T (self.dinv[:n] * p).T ) .T # 2 PC -> 20 vars def pc_obs( self p ): n = self.npc return dot( self.U[: :n] p.T ) # 2 principal -> 1000 obs def obs_pc( self obs ): n = self.npc return dot( self.U[: :n].T obs ) .T # 1000 obs -> 2 principal def obs( self x ): return self.pc_obs( self.vars_pc(x) ) # 20 vars -> 2 principal -> 1000 obs def vars( self obs ): return self.pc_vars( self.obs_pc(obs) ) # 1000 obs -> 2 principal -> 20 vars class Center: """""" A -= A.mean() /= A.std() inplace -- use A.copy() if need be uncenter(x) == original A . x """""" # mttiw def __init__( self A axis=0 scale=True verbose=1 ): self.mean = A.mean(axis=axis) if verbose: print ""Center -= A.mean:"" self.mean A -= self.mean if scale: std = A.std(axis=axis) self.std = np.where( std std 1. ) if verbose: print ""Center /= A.std:"" self.std A /= self.std else: self.std = np.ones( A.shape[-1] ) self.A = A def uncenter( self x ): return np.dot( self.A x * self.std ) + np.dot( x self.mean ) #............................................................................... if __name__ == ""__main__"": import sys csv = ""iris4.csv"" # wikipedia Iris_flower_data_set # 5.13.51.40.2 # Iris-setosa ... N = 1000 K = 20 fraction = .90 seed = 1 exec ""\n"".join( sys.argv[1:] ) # N= ... np.random.seed(seed) np.set_printoptions( 1 threshold=100 suppress=True ) # .1f try: A = np.genfromtxt( csv delimiter="""" ) N K = A.shape except IOError: A = np.random.normal( size=(N K) ) # gen correlated ? print ""csv: %s N: %d K: %d fraction: %.2g"" % (csv N K fraction) Center(A) print ""A:"" A print ""PCA ...""  p = PCA( A fraction=fraction ) print ""npc:"" p.npc print ""% variance:"" p.sumvariance * 100 print ""Vt[0] weights that give PC 0:"" p.Vt[0] print ""A . Vt[0]:"" dot( A p.Vt[0] ) print ""pc:"" p.pc() print ""\nobs <-> pc <-> x: with fraction=1 diffs should be ~ 0"" x = np.ones(K) # x = np.ones(( 3 K )) print ""x:"" x pc = p.vars_pc(x) # d' Vt' x print ""vars_pc(x):"" pc print ""back to ~ x:"" p.pc_vars(pc) Ax = dot( A x.T ) pcx = p.obs(x) # U' d' Vt' x print ""Ax:"" Ax print ""A'x:"" pcx print ""max |Ax - A'x|: %.2g"" % np.linalg.norm( Ax - pcx np.inf ) b = Ax # ~ back to original x Ainv A x back = p.vars(b) print ""~ back again:"" back print ""max |back - x|: %.2g"" % np.linalg.norm( back - x np.inf ) # end pca.py Fyinfo there's an excellent talk on [Robust PCA](http://videolectures.net/nipsworkshops2010_caramanis_rcf/snippet/) by C. Caramanis January 2011. Thanks for posting this. Concur with Pete; thanks very much. is this code will output that image(Iris PCA)? If not can you post an alternative solution in which the out would be that image. IM having some difficulties in converting this code to c++ because I'm new in python :) oh thats fine.Still thank you so much for this code :)  You might have a look at MDP. I have not had the chance to test it myself but I've bookmarked it exactly for the PCA functionality. +1 great extension!  matplotlib.mlab has a PCA implementation. the link for [PCA of matplotlib](http://matplotlib.sourceforge.net/api/mlab_api.html#matplotlib.mlab.PCA) is updated. The matplotlib.mlab implementation of PCA uses SVD. Here's [a more detailed description](http://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml) of the its functions and how to use.  I just finish reading the book Machine Learning: An Algorithmic Perspective. All code examples in the book was written by Python(and almost with Numpy). The code snippet of chatper10.2 Principal Components Analysis maybe worth a reading. It use numpy.linalg.eig. By the way I think SVD can handle 460 * 460 dimensions very well. I have calculate a 6500*6500 SVD with numpy/scipy.linalg.svd on a very old PC:Pentium III 733mHz. To be honest the script needs a lot of memory(about 1.xG) and a lot of time(about 30 minutes) to get the SVD result. But I think 460*460 on a modern PC will not be a big problem unless u need do SVD a huge number of times. You should never use eig() on a covariance matrix when you can simply use svd(). Depending on how many components you plan on using and the size of your data matrix the numerical error introduced by the former (it does more floating point operations) can become significant. For the same reason you should never explicitly invert a matrix with inv() if what you're really interested in is the inverse times a vector or matrix; you should use solve() instead. @dwf thanks for the info!",python numpy scipy pca
1697557,A,"Numpy problem with long arrays I have two arrays (a and b) with n integer elements in the range (0N). typo: arrays with 2^n integers where the largest integer takes the value N = 3^n I want to calculate the sum of every combination of elements in a and b (sum_ij_ = a_i_ + b_j_ for all ij). Then take modulus N (sum_ij_ = sum_ij_ % N) and finally calculate the frequency of the different sums. In order to do this fast with numpy without any loops I tried to use the meshgrid and the bincount function. AB = numpy.meshgrid(ab) A = A + B A = A % N A = numpy.reshape(AA.size) result = numpy.bincount(A) Now the problem is that my input arrays are long. And meshgrid gives me MemoryError when I use inputs with 2^13 elements. I would like to calculate this for arrays with 2^15-2^20 elements. that is n in the range 15 to 20 Is there any clever tricks to do this with numpy? Any help will be highly appreciated. -- jon And how big is N? Is numpy really going to be that efficient? I'd guess you'd be better off in c++ writing your own functions and optimizing as you can. From what it sounds like numpy can't handle array that large. Although I must say if you have two arrays with 2^15 to 2^20 elements then if you look at all of their different sums then you'll end up with an array of 2^30 to 2^40 elements. Which is a lot.. @unutbu: N~3^n @liberalkid: I guess you're right. Tho my c++ skills are not that good. Check your math that's a lot of space you're asking for: 2^20*2^20 = 2^40 = 1 099 511 627 776 If each of your elements was just one byte that's already one terabyte of memory. Add a loop or two. This problem is not suited to maxing out your memory and minimizing your computation.  try chunking it. your meshgrid is an NxN matrix block that up to 10x10 N/10xN/10 and just compute 100 bins add them up at the end. this only uses ~1% as much memory as doing the whole thing. probably the largest you can make a block and still keep it safely tucked in ram. I guess this is the way to go but is there a clever way to do this with numpy arrays. Minimizing the use of for loops. Hey is there an optimal size for a block?  Edit in response to jonalm's comment: jonalm: N~3^n not n~3^N. N is max element in a and n is number of elements in a. n is ~ 2^20. If N is ~ 3^n then N is ~ 3^(2^20) > 10^(500207). Scientists estimate (http://www.stormloader.com/ajy/reallife.html) that there are only around 10^87 particles in the universe. So there is no (naive) way a computer can handle an int of size 10^(500207). jonalm: I am however a bit curios about the pv() function you define. (I do not manage to run it as text.find() is not defined (guess its in another module)). How does this function work and what is its advantage? pv is a little helper function I wrote to debug the value of variables. It works like print() except when you say pv(x) it prints both the literal variable name (or expression string) a colon and then the variable's value. If you put #!/usr/bin/env python import traceback def pv(var): (filenameline_numberfunction_nametext)=traceback.extract_stack()[-2] print('%s: %s'%(text[text.find('(')+1:-1]var)) x=1 pv(x) in a script you should get x: 1 The modest advantage of using pv over print is that it saves you typing. Instead of having to write print('x: %s'%x) you can just slap down pv(x) When there are multiple variables to track it's helpful to label the variables. I just got tired of writing it all out. The pv function works by using the traceback module to peek at the line of code used to call the pv function itself. (See http://docs.python.org/library/traceback.html#module-traceback) That line of code is stored as a string in the variable text. text.find() is a call to the usual string method find(). For instance if text='pv(x)' then text.find('(') == 2 # The index of the '(' in string text text[text.find('(')+1:-1] == 'x' # Everything in between the parentheses I'm assuming n ~ 3^N and n~2**20 The idea is to work module N. This cuts down on the size of the arrays. The second idea (important when n is huge) is to use numpy ndarrays of 'object' type because if you use an integer dtype you run the risk of overflowing the size of the maximum integer allowed. #!/usr/bin/env python import traceback import numpy as np def pv(var): (filenameline_numberfunction_nametext)=traceback.extract_stack()[-2] print('%s: %s'%(text[text.find('(')+1:-1]var)) You can change n to be 2**20 but below I show what happens with small n so the output is easier to read. n=100 N=int(np.exp(1./3*np.log(n))) pv(N) # N: 4 a=np.random.randint(Nsize=n) b=np.random.randint(Nsize=n) pv(a) pv(b) # a: [1 0 3 0 1 0 1 2 0 2 1 3 1 0 1 2 2 0 2 3 3 3 1 0 1 1 2 0 1 2 3 1 2 1 0 0 3 # 1 3 2 3 2 1 1 2 2 0 3 0 2 0 0 2 2 1 3 0 2 1 0 2 3 1 0 1 1 0 1 3 0 2 2 0 2 # 0 2 3 0 2 0 1 1 3 2 2 3 2 0 3 1 1 1 1 2 3 3 2 2 3 1] # b: [1 3 2 1 1 2 1 1 1 3 0 3 0 2 2 3 2 0 1 3 1 0 0 3 3 2 1 1 2 0 1 2 0 3 3 1 0 # 3 3 3 1 1 3 3 3 1 1 0 2 1 0 0 3 0 2 1 0 2 2 0 0 0 1 1 3 1 1 1 2 1 1 3 2 3 # 3 1 2 1 0 0 2 3 1 0 2 1 1 1 1 3 3 0 2 2 3 2 0 1 3 1] wa holds the number of 0s 1s 2s 3s in a wb holds the number of 0s 1s 2s 3s in b wa=np.bincount(a) wb=np.bincount(b) pv(wa) pv(wb) # wa: [24 28 28 20] # wb: [21 34 20 25] result=np.zeros(Ndtype='object') Think of a 0 as a token or chip. Similarly for 123. Think of wa=[24 28 28 20] as meaning there is a bag with 24 0-chips 28 1-chips 28 2-chips 20 3-chips. You have a wa-bag and a wb-bag. When you draw a chip from each bag you ""add"" them together and form a new chip. You ""mod"" the answer (modulo N). Imagine taking a 1-chip from the wb-bag and adding it with each chip in the wa-bag. 1-chip + 0-chip = 1-chip 1-chip + 1-chip = 2-chip 1-chip + 2-chip = 3-chip 1-chip + 3-chip = 4-chip = 0-chip (we are mod'ing by N=4) Since there are 34 1-chips in the wb bag when you add them against all the chips in the wa=[24 28 28 20] bag you get 34*24 1-chips 34*28 2-chips 34*28 3-chips 34*20 0-chips This is just the partial count due to the 34 1-chips. You also have to handle the other types of chips in the wb-bag but this shows you the method used below: for icount in enumerate(wb): partial_count=count*wa pv(partial_count) shifted_partial_count=np.roll(partial_counti) pv(shifted_partial_count) result+=shifted_partial_count # partial_count: [504 588 588 420] # shifted_partial_count: [504 588 588 420] # partial_count: [816 952 952 680] # shifted_partial_count: [680 816 952 952] # partial_count: [480 560 560 400] # shifted_partial_count: [560 400 480 560] # partial_count: [600 700 700 500] # shifted_partial_count: [700 700 500 600] pv(result) # result: [2444 2504 2520 2532] This is the final result: 2444 0s 2504 1s 2520 2s 2532 3s. # This is a test to make sure the result is correct. # This uses a very memory intensive method. # c is too huge when n is large. if n>1000: print('n is too large to run the check') else: c=(a[:]+b[:np.newaxis]) c=c.ravel() c=c%N result2=np.bincount(c) pv(result2) assert(all(r1==r2 for r1r2 in zip(resultresult2))) # result2: [2444 2504 2520 2532] Dear Ubuntu. I see there is an inconsistency in my notation. What I really meant is size(a)=2^n (not n as I wrote in the first post) max(a)=3^n (=N) with n as high as posible. a[:]+b[:np.newaxis] %N can do n=14 but not higher. I would like to have n~20 => max(a)=3^20 < 2^32 so I need a clever way to deal with the data. The point I was trying to make was that with the n and N I need bincount(a) contains more elements than a so I do not think your method will be efficient for the problem. That said I like the pv() function. Thank you so much for the answer I like this method. But I don't think this will help me as all the numbers in array a (and in b) are different (did not mention that my bad). bincount(a) will only consist of 1 and 0. N~3^n not n~3^N. N is max element in a and n is number of elements in a. I am however a bit curios about the pv() function you define. (I do not manage to run it as text.find() is not defined (guess its in another module)). How does this function work and what is its advantage? Hi jonalm I edited my answer to respond to your comment. Note that `c %= N` does work (and may use twice as less memory). @EOL yes c %= N is better. However defining `c=(a[:]+b[:np.newaxis])` means you've already lost the battle since this is a huge 2-d array of shape (nn) while the above solution uses nothing more than a couple of 1-d arrays of shape (N).",python math numpy
568962,A,How do I create an empty array/matrix in NumPy? I'm sure I must be being very dumb but I can't figure out how to use an array or matrix in the way that I would normally use a list. I.e. I want to create an empty array (or matrix) and then add one column (or row) to it at a time. At the moment the only way I can find to do this is like: mat = None for col in columns: if mat is None: mat = col else: mat = hstack((mat col)) Whereas if it were a list I'd do something like this: list = [] for item in data: list.append(item) Is there a way to use that kind of notation for NumPy arrays or matrices? (Or a better way -- I'm still pretty new to python!) A NumPy array is a very different data structure from a list and is designed to be used in different ways. Your use of hstack is potentially very inefficient... every time you call it all the data in the existing array is copied into a new one. (The append function will have the same issue.) If you want to build up your matrix one column at a time you might be best off to keep it in a list until it is finished and only then convert it into an array. e.g.  mylist = [] for item in data: mylist.append(item) mat = numpy.array(mylist) item can be a list an array or any iterable as long as each item has the same number of elements. In this particular case (data is some iterable holding the matrix columns) you can simply use  mat = numpy.array(data) (Also note that using list as a variable name is probably not good practice since it masks the built-in type by that name which can lead to bugs.) EDIT: If for some reason you really do want to create an empty array you can just use numpy.array([]) but this is rarely useful! Are numpy arrays/matrices fundamentally different from Matlab ones? @levesque Look [HERE](http://www.scipy.org/NumPy_for_Matlab_Users)  ARRAY OBJECTS Array objects consist of one- or multidimensional homogeneous fixed-size structures i.e. they have a fixed number of elements all of the same datatype (which allows much faster methods than Python's list object). An array element is retrieved as A[ijk..] (list elements are retrieved as L[i][j][k]..). A matrix is a two-dimensional array. Arrays a have attributes a.attr which can be distinguished in properties and methods a.meth(). The former return values (such as the shape and type) that belong to the array; the latter specify actions that are to be performed on the array. Often there is a function that performs the same action as a method; function and method then have the same name. Howeverthe default settings of the parameters may differ for methods and their corresponding functions. Array construction Arrays can be created in various ways: with the function array(obj) where 'obj' is a (nested) sequence e.g. a list [ ] or a tuple ( ). When 'obj' is an array a copy of this array is returned. The function matrix does the same for matrices. with the function copy(obj) where 'obj' is another array or matrix or a (nested) sequence with the function asarray(obj) which is like 'copy' except that no copy is made if 'obj' is already an array. with the function empty(shape dtype) which produces an uninitialized array with specified shape and typecode or with the function empty_like(a) which produces an uninitialized array with the same shape and typecode as its argument a with the function ones(shape dtype) producing an array initialized with ones or ones_like(a). with the function zeros(shape dtype) producing an array initialized with zeros or zeros_like(a) with the function identity(ndtype) producing a 2-d n*n identity matrix with the function arange(..) which does the same as array(range(..)) with the function concatenate(..) which concatenates sequences to an array. When you copy-paste documentation in place of an answer you should cite a source.  You have the wrong mental model for using NumPy efficiently. NumPy arrays are stored in contiguous blocks of memory. If you want to add rows or columns to an existing array the entire array needs to be copied to a new block of memory creating gaps for the new elements to be stored. This is very inefficient if done repeatedly to build an array. In the case of adding rows your best bet is to create an array that is as big as your data set will eventually be and then add data to it row-by-row: >>> import numpy >>> a = numpy.zeros(shape=(52)) >>> a array([[ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.]]) >>> a[0] = [12] >>> a[1] = [23] >>> a array([[ 1. 2.] [ 2. 3.] [ 0. 0.] [ 0. 0.] [ 0. 0.]]) There is also numpy.empty() if you don't need to zero the array. What's the benefit of using empty() over zeros()? that if you're going to initialize it with your data straight away you save the cost of zeroing it.  To create an empty multidimensional array in NumPy (e.g. a 2D array m*n to store your matrix) in case you don't know m how many rows you will append and don't care about the computational cost Stephen Simmons mentioned (namely re-buildinging the array at each append) you can squeeze to 0 the dimension to which you want to append to: X = np.empty(shape=[0 n]). This way you can use for example (here m = 5 which we assume we didn't know when creating the empty matrix and n = 2): n = 2 X = np.empty(shape=[0 n]) for i in range(5): for j in range(2): X = np.append(X [[i j]] axis=0) print X which will give you: [[ 0. 0.] [ 0. 1.] [ 1. 0.] [ 1. 1.] [ 2. 0.] [ 2. 1.] [ 3. 0.] [ 3. 1.] [ 4. 0.] [ 4. 1.]]  If you absolutely don't know the final size of the array you can increment the size of the array like this: my_arr = numpy.zeros((05)) for i in range(3): my_arr=numpy.concatenate( ( my_arr numpy.ones((15)) ) ) print(my_arr) [[ 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1.]] Notice the 0 in the first line. numpy.append is another option. It calls numpy.concatenate.  You can use the append function. For rows: >>> from numpy import * >>> a = array([102030]) >>> append(a [[123]] axis=0) array([[10 20 30] [1 2 3]]) For columns: >>> append(a [[15][15]] axis=1) array([[10 20 30 15] [1 2 3 15]]) EDIT Of course as mentioned in other answers unless you're doing some processing (ex. inversion) on the matrix/array EVERY time you append something to it I would just create a list append to it then convert it to an array.  I looked into this a lot because I needed to use a numpy.array as a set in one of my school projects and I needed to be initialized empty... I didn't found any relevant answer here on Stack Overflow so I started doodling something. # Initialize your variable as a empty list first In [32]: x=[] # and now cast it as a numpy ndarray In [33]: x=np.array(x) The result will be: In [34]: x Out[34]: array([] dtype=float64) Therefore you can directly initialize an np array as follows: In [36]: x= np.array([] dtype=np.float64) I hope this helps.,python arrays numpy
1520379,A,"How to update Numpy on Mac OS X Snow Leopard? How can I update Numpy into the newest one? Should I download .dmg file from here: http://sourceforge.net/projects/numpy/files/ Is this .dmg only for 10.5? I have installed numpy using these instructions: http://www.scipy.org/Installing_SciPy/Mac_OS_X My current Numpy is 1.2.1. I'm running on Mac OS X 10.6.1 Snow Leopard. Thanks! Use pip install -U numpy instead as easy_install is deprecated in favor of pip  For some reason easy_install -U numpy didn't work. print numpy.__version__ would always give 1.2.1 So I first removed numpy 1.2.1 by finding it and deleting the entire folder: import numpy print numpy.__file__ I downloaded the GNU Fortran Compiler from: http://r.research.att.com/gfortran-4.2.3.dmg I used easy_install to install numpy. In retrospect easy_install -U numpy might have worked if I had the Fortran compiler installed.  sudo easy_install -U numpy Installing via setuptools will get the new numpy on the sys.path for non-system utilties (I've been told that some Apple utilities rely on the system-numpy). In general setuptools will ""do the right"" thing on OS X. As noted by Austin you have to install http://r.research.att.com/gfortran-4.2.3.dmg first. Tested on a new 10.6.4 install I had similar problems trying sudo easy_install -U numpy So I did the following: sudo easy_install pip to install pip. Than installed numpy via `pip` pip install numpy which worked great.  as suggested elsewhere macports works fine on multiple architecture and versions of MacOsX + allows updates and more: $ port search numpy py-numpy @1.3.0 (python) The core utilities for the scientific library scipy for Python py25-numpy @1.3.0 (python) The core utilities for the scientific library scipy for Python py25-symeig @1.4 (python science) Symeig - Symmetrical eigenvalue routines for NumPy. py26-numpy @1.3.0 (python) The core utilities for the scientific library scipy for Python py26-scikits-audiolab @0.10.2 (python science audio) Audiolab is a python toolbox to read/write audio files from numpy arrays Found 5 ports. $ in your case simply issue : $ sudo port install py26-numpy alternatively if you want / need to compile yourself the instructions in HJBlog are very useful. I tested and could easily compile the 64-bit version of matplotlib.",python osx osx-snow-leopard numpy
221386,A,"Removing a sequence of characters from a large binary file using python I would like to trim long sequences of the same value from a binary file in python. A simple way of doing it is simply reading in the file and using re.sub to replace the unwanted sequence. This will of course not work on large binary files. Can it be done in something like numpy? You need to make your question more precise. Do you know the values you want to trim ahead of time? Assuming you do I would probably search for the matching sections using subprocess to run ""fgrep -o -b <search string>"" and then change the relevant sections of the file using the python file object's seek read and write methods.  AJMayorga suggestion is fine unless the sizes of the replacement strings are different. Or the replacement string is at the end of the chunk. I fixed it like this: def ReplaceSequence(inFilename outFilename oldSeq newSeq): inputFile = open(inFilename ""rb"") outputFile = open(outFilename ""wb"") data = """" chunk = 1024 oldSeqLen = len(oldSeq) while 1: data = inputFile.read(chunk) dataSize = len(data) seekLen= dataSize - data.rfind(oldSeq) - oldSeqLen if seekLen > oldSeqLen: seekLen = oldSeqLen data = data.replace(oldSeq newSeq) outputFile.write(data) inputFile.seek(-seekLen 1) outputFile.seek(-seekLen 1) if dataSize < chunk: break inputFile.close() outputFile.close()  dbr's solution is a good idea but a bit overly complicated all you really have to do is rewind the file pointer the length of the sequence you are searching for before you read your next chunk. def ReplaceSequence(inFilename outFilename oldSeq newSeq): inputFile = open(inFilename ""rb"") outputFile = open(outFilename ""wb"") data = """" chunk = 1024 while 1: data = inputFile.read(chunk) data = data.replace(oldSeq newSeq) outputFile.write(data) inputFile.seek(-len(oldSequence) 1) outputFile.seek(-len(oldSequence) 1) if len(data) < chunk: break inputFile.close() outputFile.close()  If two copies fit in memory then you can easily make a copy. The second copy is the compressed version. Sure you can use numpy but you can also use the array package. Additionally you can treat your big binary object as a string of bytes and manipulate it directly. It sounds like your file may be REALLY large and you can't fit two copies into memory. (You didn't provide a lot of details so this is just a guess.) You'll have to do your compression in chunks. You'll read in a chunk do some processing on that chunk and write it out. Again numpy array or simple string of bytes will work fine.  This generator-based version will keep exactly one character of the file content in memory at a time. Note that I am taking your question title quite literally - you want to reduce runs of the same character to a single character. For replacing patterns in general this does not work: import StringIO def gen_chars(stream): while True: ch = stream.read(1) if ch: yield ch else: break def gen_unique_chars(stream): lastchar = '' for char in gen_chars(stream): if char != lastchar: yield char lastchar=char def remove_seq(infile outfile): for ch in gen_unique_chars(infile): outfile.write(ch) # Represents a file open for reading infile = StringIO.StringIO(""1122233333444555"") # Represents a file open for writing outfile = StringIO.StringIO() # Will print ""12345"" remove_seq(infile outfile) outfile.seek(0) print outfile.read()  If you don't have the memory to do open(""big.file"").read() then numpy wont really help.. It uses the same memory as python variables do (if you have 1GB of RAM you can only load 1GB of data into numpy) The solution is simple - read the file in chunks.. f = open(""big.file"" ""rb"") then do a series of f.read(500) remove the sequence and write it back out to another file object. Pretty much how you do file reading/writing in C.. The problem then is if you miss the pattern you are replacing.. For example: target_seq = ""567"" input_file = ""1234567890"" target_seq.read(5) # reads 12345 doesn't contain 567 target_seq.read(5) # reads 67890 doesn't contain 567 The obvious solution is to start at the first character in the file check len(target_seq) characters then go forward one character check forward again. For example (pseudo code!): while cur_data != """": seek_start = 0 chunk_size = len(target_seq) input_file.seek(offset = seek_start whence = 1) #whence=1 means seek from start of file (0 + offset) cur_data = input_file.read(chunk_size) # reads 123 if target_seq == cur_data: # Found it! out_file.write(""replacement_string"") else: # not it shove it in the new file out_file.write(cur_data) seek_start += 1 It's not exactly the most efficient way but it will work and not require keeping a copy of the file in memory (or two). Thanks that helps a lot. I was hoping numpy would have some auto memory management for large files - I'm not too familiar with it.",python numpy binaryfiles
1767865,A,"Using Numpy to find average value across data sets with some missing data I have several (10 or so) CSV-formatted data sets. Each column of a data set represents one aspect of a running system (available RAM CPU usage open TCP connections and so forth). Each row contains the values for these columns at one moment in time. The data sets were captured during individual runs of the same test. The number of rows is not guaranteed to be the same in each data set (i.e.: some tests ran longer than others). I want to produce a new CSV file that represents the ""average"" value across all data sets for a given time offset and a given column. Ideally values missing in one data set would be ignored. If necessary though missing values could be assumed to be the same as the last known value or the average of known values for that row. A simplified example: +---------------+ +---------------+ +---------------+ | Set 1 | | Set 2 | | Average | +---+-----+-----+ +---+-----+-----+ +---+-----+-----+ | t | A | B | | t | A | B | | t | A | B | +---+-----+-----+ +---+-----+-----+ +---+-----+-----+ | 1 | 10 | 50 | | 1 | 12 | 48 | | 1 | 11 | 49 | | 2 | 13 | 58 | | 2 | 7 | 60 | | 2 | 10 | 59 | | 3 | 9 | 43 | | 3 | 17 | 51 | => | 3 | 13 | 47 | | 4 | 14 | 61 | | 4 | 12 | 57 | | 4 | 13 | 59 | | : | : | : | | : | : | : | | : | : | : | | 7 | 4 | 82 | | 7 | 10 | 88 | | 7 | 7 | 86 | +---+-----+-----+ | 8 | 15 | 92 | | 8 | 15 | 92 | | 9 | 6 | 63 | | 9 | 6 | 63 | +---+-----+-----+ +---+-----+-----+ I'm new to numpy having picked it up specifically for this project. What's the best way to do this? For data sets with the same number of rows (which I've been forcing by chopping longer data sets short) I just do: d_avg = sum(dsets) / float(len(dsets)) where ""dsets"" is a list of the ndarrays containing the data from each CSV file. This works well but I don't want to discard the data from the longer runs. I can also resize the shorter runs to the length of the longest but all the new fields are filled with ""NoneType"". Later operations then error when adding (for example) a float and a NoneType. Any suggestions? I think the average for row 7 is wrong I knew I was going to miss one of those rows! Updated. Edit: I've revised my method abandoning scipy.nanmean in favor of masked arrays. If it is unclear what the code is doing at any point first try putting print statements in. If it is still unclear feel free to ask; I'll try my best to explain. The trick part is getting the t-values merged. (That was done with numpy array's searchsorted method.) Playing with numpy has led me to believe that its speed advantages may not exist until the datasets get quite big (maybe you'll need at least 10000 rows per data set). Otherwise a pure python solution may be both easier to write and faster. Here are the toy datasets I used: % cat set1 1 10 50 2 13 58 3943 41461 7 4 82 % cat set2 1 12 48 2 7 60 31751 41257 71088 81592 9663 And here is the code: #!/usr/bin/env python import numpy as np filenames=('set1''set2') # change this to list all your csv files column_names=('t''a''b') # slurp the csv data files into a list of numpy arrays data=[np.loadtxt(filename delimiter='') for filename in filenames] # Find the complete list of t-values # For each elt in data elt[ab] is the value in the a_th row and b_th column t_values=np.array(list(reduce(set.union(set(elt[:0]) for elt in data)))) t_values.sort() # print(t_values) # [ 1. 2. 3. 4. 7. 8. 9.] num_rows=len(t_values) num_columns=len(column_names) num_datasets=len(filenames) # For each data set we compute the indices of the t_values that are used. idx=[(t_values.searchsorted(data[n][:0])) for n in range(num_datasets)] data2=np.ma.zeros((num_rowsnum_columnsnum_datasets)) for n in range(num_datasets): data2[idx[n]:n]=data[n][::] data2=np.ma.masked_equal(data2 0) averages=data2.mean(axis=-1) print(averages) # [[1.0 11.0 49.0] # [2.0 10.0 59.0] # [3.0 13.0 47.0] # [4.0 13.0 59.0] # [7.0 7.0 85.0] # [8.0 15.0 92.0] # [9.0 6.0 63.0]] Nice! I didn't know about 'loadtxt'. I was using the 'tabular' module which turned out to be overkill. Thanks.  Why not just us numpy's ma (masked array) module? maxLen = reduce(lambda ab : max(a b.shape[0]) dSets 0) all = N.ma.zeros((maxLen)+ dSets[0].shape[1:] + (len(dSets)) dtype=float) # set the dtype to whatever all.mask = True for i set in enumerate(dSets): all.mask[:len(set)...i] = False all[:len(set)...i] = set mean = all.mean(axis=-1) Of course this only works if you can guarantee that the time in each row is the same across all arrays i.e. set[i0] == set[j0] for all ij This works great. Thanks! One thing: the reduce/lambda construct can fail when an early value is the highest: 'int' has no method 'shape'. replaced with: maxLen = max([a.shape[0] for a in dSets]) Yes you're right I ballsed up the lambda. Edited to correct. Cheers! Even if the time isn't the same you can use masked arrays. You just need to be smarter in setting up the masked array so the data for each time is in the same row.  Well one way to do it would be to iterate over each row of each data set and append a given column value to an array that's stored in a dictionary where the time index is used for its key value. You then iterate over the dictionary and pull the average for each array stored there. This isn't particularly efficient -- the other option is to find the longest array iterate over it and query the other datasets to create an temporary array to average. This way you save the secondary iteration over the dictionary. I was really hoping that numpy with its array-oriented efficiency would provide a way to do exactly that. You're right though I'll have to fall back to the method you suggest if there's no existing operation for it. If you're really wanting to stay in numpy take a look at masked arrays here: http://docs.scipy.org/doc/numpy/reference/maskedarray.generic.html It's not so much numpy itself that I want. It's clean easy-to-understand code! Frankly I'd drop Python for (hypothetically) R if that meant an elegant solution. But I know even less about R than numpy. Thanks for the tip on masked arrays. I'll check it out.",python numpy
911871,A,"Detect if a NumPy array contains at least one non-numeric value? I need to write a function which will detect if the input contains at least one value which is non-numeric. If a non-numeric value is found I will raise an error (because the calculation should only return a numeric value). The number of dimensions of the input array is not known in advance - the function should give the correct value regardless of ndim. As an extra complication the input could be a single float or numpy.float64 or even something oddball like a zero-dimensional array. The obvious way to solve this is to write a recursive function which iterates over every iterable object in the array until it finds a non-iterabe. It will apply the numpy.isnan() function over every non-iterable object. If at least one non-numeric value is found then the function will return False immediately. Otherwise if all the values in the iterable are numeric it will eventually return True. That works just fine but it's pretty slow and I expect that NumPy has a much better way to do it. What is an alternative that is faster and more numpyish? Here's my mockup: def contains_nan( myarray ): """""" @param myarray : An n-dimensional array or a single float @type myarray : numpy.ndarray numpy.array float @returns: bool Returns true if myarray is numeric or only contains numeric values. Returns false if at least one non-numeric value exists Not-A-Number is given by the numpy.isnan() function. """""" return True Your description for `contains_nan` looks suspicious: ""Returns false if at least one non-numeric value exists"". I would have expected `contains_nan` to return `True` if the array contains NaN. With numpy 1.3 or svn you can do this In [1]: a = arange(10000.).reshape(100100) In [3]: isnan(a.max()) Out[3]: False In [4]: a[5050] = nan In [5]: isnan(a.max()) Out[5]: True In [6]: timeit isnan(a.max()) 10000 loops best of 3: 66.3 µs per loop The treatment of nans in comparisons was not consistent in earlier versions.  This should be faster than iterating and will work regardless of shape. numpy.isnan(myarray).any() Edit: 30x faster: import timeit s = 'import numpy;a = numpy.arange(10000.).reshape((100100));a[1010]=numpy.nan' ms = [ 'numpy.isnan(a).any()' 'any(numpy.isnan(x) for x in a.flatten())'] for m in ms: print "" %.2f s"" % timeit.Timer(m s).timeit(1000) m Results:  0.11 s numpy.isnan(a).any() 3.75 s any(numpy.isnan(x) for x in a.flatten()) Bonus: it works fine for non-array NumPy types: >>> a = numpy.float64(42.) >>> numpy.isnan(a).any() False >>> a = numpy.float64(numpy.nan) >>> numpy.isnan(a).any() True with numpy 1.7 the flatten() version is only twice as fast as the first one",python numpy
1632673,A,"Python File Slurp w/ endian conversion It was recently asked how to do a file slurp in python and the accepted answer suggested something like: with open('x.txt') as x: f = x.read() How would I go about doing this to read the file in and convert the endian representation of the data? For example I have a 1GB binary file that's just a bunch of single precision floats packed as a big endian and I want to convert it to little endian and dump into a numpy array. Below is the function I wrote to accomplish this and some real code that calls it. I use struct.unpack do the endian conversion and tried to speed everything up by using mmap. My question then is am I using the slurp correctly with mmap and struct.unpack? Is there a cleaner faster way to do this? Right now what I have works but I'd really like to learn how to do this better. Thanks in advance! #!/usr/bin/python from struct import unpack import mmap import numpy as np def mmapChannel(arrayName fileName channelNo line_count sample_count): """""" We need to read in the asf internal file and convert it into a numpy array. It is stored as a single row and is binary. Thenumber of lines (rows) samples (columns) and channels all come from the .meta text file Also internal format files are packed big endian but most systems use little endian so we need to make that conversion as well. Memory mapping seemed to improve the ingestion speed a bit """""" # memory-map the file size 0 means whole file # length = line_count * sample_count * arrayName.itemsize print ""\tMemory Mapping..."" with open(fileName ""rb"") as f: map = mmap.mmap(f.fileno() 0 access=mmap.ACCESS_READ) map.seek(channelNo*line_count*sample_count*arrayName.itemsize) for i in xrange(line_count*sample_count): arrayName[0 i] = unpack('>f' map.read(arrayName.itemsize) )[0] # Same method as above just more verbose for the maintenance programmer. # for i in xrange(line_count*sample_count): #row # be_float = map.read(arrayName.itemsize) # arrayName.itemsize should be 4 for float32 # le_float = unpack('>f' be_float)[0] # > for big endian < for little endian # arrayName[0 i]= le_float map.close() return arrayName print ""Initializing the Amp HH HV and Phase HH HV arrays..."" HHamp = np.ones((1 line_count*sample_count) dtype='float32') HHphase = np.ones((1 line_count*sample_count) dtype='float32') HVamp = np.ones((1 line_count*sample_count) dtype='float32') HVphase = np.ones((1 line_count*sample_count) dtype='float32') print ""Ingesting HH_Amp..."" HHamp = mmapChannel(HHamp 'ALPSRP042301700-P1.1__A.img' 0 line_count sample_count) print ""Ingesting HH_phase..."" HHphase = mmapChannel(HHphase 'ALPSRP042301700-P1.1__A.img' 1 line_count sample_count) print ""Ingesting HV_AMP..."" HVamp = mmapChannel(HVamp 'ALPSRP042301700-P1.1__A.img' 2 line_count sample_count) print ""Ingesting HV_phase..."" HVphase = mmapChannel(HVphase 'ALPSRP042301700-P1.1__A.img' 3 line_count sample_count) print ""Reshaping...."" HHamp_orig = HHamp.reshape(line_count -1) HHphase_orig = HHphase.reshape(line_count -1) HVamp_orig = HVamp.reshape(line_count -1) HVphase_orig = HVphase.reshape(line_count -1) I wanted to add to this for anyone else who finds this post useful. Running the original code I had takes about 80 seconds or so. Running the solution provided by Alex Martelli and J F Sebastian is less than a second. The program that calls this function does so many times. As such the running time has dropped considerably. Thank you both for the help and for teaching me something =) with open(fileName ""rb"") as f: arrayName = numpy.fromfile(f numpy.float32) arrayName.byteswap(True) Pretty hard to beat for speed AND conciseness;-). For byteswap see here (the True argument means ""do it in place""); for fromfile see here. This works as is on little-endian machines (since the data are big-endian the byteswap is needed). You can test if that is the case to do the byteswap conditionally change the last line from an unconditional call to byteswap into for example: if struct.pack('=f' 2.3) == struct.pack('<f' 2.3): arrayName.byteswap(True) i.e. a call to byteswap conditional on a test of little-endianness. that is remarkably straightforward. thank you what's weird is i had seen those when trying to figure out how to do this but it just didn't register for some reason. comes with experience i suppose =) numpy.float32 has native byte order that might not be always big-endian. http://stackoverflow.com/questions/1632673/python-file-slurp-w-endian-conversion/1633525#1633525 Indeed it will mostly be little-endian but if you're running e.g. on a Power PC machine it will be big endian (if that's an issue just conditionally omit the byteswap call -- let me edit the answer to add that bit). Testing sys.byteorder is a little more straightforward than using struct.pack.  Slightly modified @Alex Martelli's answer: arr = numpy.fromfile(filename numpy.dtype('>f4')) # no byteswap is needed regardless of endianess of the machine  I'd expect something like this to be faster arrayName[0] = unpack('>'+'f'*line_count*sample_count map.read(arrayName.itemsize*line_count*sample_count)) Please don't use map as a variable name  You could coble together an ASM based solution using CorePy. I wonder though if you might be able to gain enough performance from the some other part of your algorithm. I/O and manipulations on 1GB chunks of data are going to take a while which ever way you slice it. One other thing you might find helpful would be to switch to C once you have prototyped the algorithm in python. I did this for manipulations on a whole-world DEM (height) data set one time. The whole thing was much more tolerable once I got away from the interpreted script.",python struct numpy endianness mmap
1711865,A,Accessing a matrix element by matrix[(a b) c] instead of matrix[a b c] I want to achieve the following: Have a AxBxC matrix (where ABC are integers). Access that matrix not as matrix[a b c] but as matrix[(a b) c] this is I have two variables var1 = (x y) and var2 = z and want access my matrix as matrix[var1 var2]. How can this be done? I am using numpy matrix if it makes any difference. I know I could use matrix[var1[0] var1[1] var2] but if possible I'd like to know if there is any other more elegant way. Thanks! matrix[ab][c] ? Add that as asnwer and I'll rate it. Thanks! If var1 = (xy) and var2 = z you can use matrix[var1][var2]  I think you can simply subclass the NumPy matrix type with a new class of your own; and overload the __getitem__() nethod to accept a tuple. Something like this: class SpecialMatrix(np.matrix): def __getitem__(self arg1 arg2 arg3=None): try: i j = arg1 k = arg2 assert(arg3 is None) x = super(SpecialMatrix self).__getitem__(i j k) except TypeError: assert(arg3 is not None) return super(SpecialMatrix self).__getitem__(arg1 arg2 arg3) And do something similar with __setitem__(). I'm not sure if __getitem__() takes multiple arguments like I'm showing here or if it takes a tuple or what. I don't have NumPy available as I write this answer sorry. EDIT: I re-wrote the example to use super() instead of directly calling the base class. It has been a while since I did anything with subclassing in Python. EDIT: I just looked at the accepted answer. That's totally the way to do it. I'll leave this up in case anyone finds it educational but the simple way is best.,python numpy
1658714,A,Range of valid numpy values I'm interested in finding for a particular Numpy type (e.g. np.int64 np.uint32 np.float32 etc.) what the range of all possible valid values is (e.g. np.int32 can store numbers up to 2**31-1). Of course I guess one can theoretically figure this out for each type but is there a way to do this at run time to ensure more portable code? You can use numpy.iinfo(arg).max to find the max value for integer types of arg and numpy.finfo(arg).max to find the max value for float types of arg. >>> numpy.iinfo(numpy.uint64).min 0 >>> numpy.iinfo(numpy.uint64).max 18446744073709551615L >>> numpy.finfo(numpy.float64).max 1.7976931348623157e+308 >>> numpy.finfo(numpy.float64).min -1.7976931348623157e+308 iinfo only offers min and max but finfo also offers useful values such as eps (the smallest number > 0 representable) and resolution (the approximate decimal number resolution of the type of arg).  Quoting from a numpy dicussion list: That kind of information is available via numpy.finfo() and numpy.iinfo(): In [12]: finfo('d').max Out[12]: 1.7976931348623157e+308 In [13]: iinfo('i').max Out[13]: 2147483647 In [14]: iinfo(uint8).max Out[14]: 255 The link is here: link to numpy discussion group page,python numpy
1803860,A,Performing operations on a NumPy arrray but masking values along the diagonal from these operations as I can perform operations on arrays so that does nothing on the diagonal is calculated such that all but the diagonal array ([[0. 1.37 1. 1.37 1. 1.37 1.] [1.37 0.  1.37 1.73 2.37 1.73 1.37] [1.  1.37 0.  1.37 2.  2.37 2. ] [1.37 1.73 1.37 0.  1.37 1.73 2.37] [1.  2.37 2.  1.37 0.  1.37 2. ] [1.37 1.73 2.37 1.73 1.37 0.  1.37] [1.  1.37 2.  2.37 2.  1.37 0. ]]) to avoid the NaN value but retained the value zero on the diagonal in all responses What are you trying to do? What is the operation involved? Are you trying to do matrix multiplication or inversion? Your question is very unclear. yes I need to do is Can you just do the calculation as normal then afterwards set the diagonal back to zero? yes I need to do is Then do this after each calculation: for i in range(len(array)):array[i][i]=0  I wonder if masked arrays might do what you want e.g. import numpy as NP A = NP.random.random_integers(0 9 16).reshape(4 4) dg = NP.r_[ [NP.nan] * 4 ] # proper syntax is 'nan' not 'NaN' dg = NP.diag(dg) A += dg # a 4x4 array w/ NaNs down the main diagonal NP.sum(A axis=1) # doesn't work gives: array([ NaN NaN NaN NaN]) from numpy import ma as MA Am = **MA.masked_invalid**(A) NP.sum(Am axis=1) # now it works (treats 'nan' as 0) The other way to do this of is of course to first convert the NaNs to 0s then mask the 0s: NP.nan_to_num(A) MA.masked_equal(A 0) Finally it's often efficient to mask and convert the NaNs in one step: MA.fix_invalid(A) Pretty straightforward just keep in mind that 'ma' might not yet be in your namespace and also that these functions deal with 'NaNs' and 'infs' which is usually what you want.  >>> arr = [ ... [0. 1.37 1. 1.37 1. 1.37 1.] ... [1.37 0.  1.37 1.73 2.37 1.73 1.37] ... [1.  1.37 0.  1.37 2.  2.37 2. ] ... [1.37 1.73 1.37 0.  1.37 1.73 2.37] ... [1.  2.37 2.  1.37 0.  1.37 2. ] ... [1.37 1.73 2.37 1.73 1.37 0.  1.37] ... [1.  1.37 2.  2.37 2.  1.37 0. ] ... ] >>> for i in range(6): ... for y in range(6): ... if (i <> y): ... print arr[i][y]*arr[y][i] ... 1.8769 1.0 1.8769 1.0 1.8769 1.8769 1.8769 2.9929 5.6169 2.9929 1.0 1.8769 1.8769 4.0 5.6169 1.8769 2.9929 1.8769 1.8769 2.9929 1.0 5.6169 4.0 1.8769 1.8769 1.8769 2.9929 5.6169 2.9929 1.8769 Depends on what you need to calculate  Do your calculation as normal and then myarray[arange(len(array)) arange(len(array))] = 0.,python arrays numpy scipy
695794,A,"more efficient way to pickle a string The pickle module seems to use string escape characters when pickling; this becomes inefficient e.g. on numpy arrays. Consider the following z = numpy.zeros(1000 numpy.uint8) len(z.dumps()) len(cPickle.dumps(z.dumps())) The lengths are 1133 characters and 4249 characters respectively. z.dumps() reveals something like ""\x00\x00"" (actual zeros in string) but pickle seems to be using the string's repr() function yielding ""'\x00\x00'"" (zeros being ascii zeros). i.e. (""0"" in z.dumps() == False) and (""0"" in cPickle.dumps(z.dumps()) == True) You should add a specific question to your post here. What do you want to serialize a Python string or a numpy array of bytes? should be len(cPickle.dumps(z)) Try using a later version of the pickle protocol with the protocol parameter to pickle.dumps(). The default is 0 and is an ASCII text format. Ones greater than 1 (I suggest you use pickle.HIGHEST_PROTOCOL). Protocol formats 1 and 2 (and 3 but that's for py3k) are binary and should be more space conservative.  Solution: import zlib cPickle def zdumps(obj): return zlib.compress(cPickle.dumps(objcPickle.HIGHEST_PROTOCOL)9) def zloads(zstr): return cPickle.loads(zlib.decompress(zstr)) >>> len(zdumps(z)) 128 Here's something more on the subject: http://tinyurl.com/3ymhaj5 . Basically if you're serializing to disk you can just do gzip.open() instead of open. @slack3r that link is dead. 'ascii' codec can't encode character u'\xda' in position 1: ordinal not in range(128)  An improvement to vartec's answer that seems a bit more memory efficient (since it doesn't force everything into a string): def pickle(fname obj): import cPickle gzip cPickle.dump(obj=obj file=gzip.open(fname ""wb"" compresslevel=3) protocol=2) def unpickle(fname): import cPickle gzip return cPickle.load(gzip.open(fname ""rb"")) (1) Then py2 code won't read py3 objects. (2) the header says ""an improvement to vartec's answer"" which was using compression -- I think it used less mem but it could have been a false impression... (3) fixed -1 (1) Don't hard-code protocol numbers use `-1` or `HIGHEST_PROTOCOL`. (2) Subsequent compression is an ADD-ON and is irrelevant to his question. (3) Specifying `compresslevel` when decompressing is pointless; any such information that may be necessary to decompress the file would be stored in the header of the compressed file -- otherwise how would you be able to decompress a file if you didn't know what compression level was used?  z.dumps() is already pickled string i.e. it can be unpickled using pickle.loads(): >>> z = numpy.zeros(1000 numpy.uint8) >>> s = z.dumps() >>> a = pickle.loads(s) >>> all(a == z) True",python numpy pickle space-efficiency
1796597,A,"import array in python how can I import an array to python (numpy) from a file and that way the file must be written. For example a matrix to and from that file type (extention). thanks for any response Checkout the entry on the numpy example list. Here is the entry on .loadtxt() >>> from numpy import * >>> >>> data = loadtxt(""myfile.txt"") # myfile.txt contains 4 columns of numbers >>> tz = data[:0] data[:3] # data is 2D numpy array >>> >>> txyz = loadtxt(""myfile.txt"" unpack=True) # to unpack all columns >>> tz = loadtxt(""myfile.txt"" usecols = (03) unpack=True) # to select just a few columns >>> data = loadtxt(""myfile.txt"" skiprows = 7) # to skip 7 rows from top of file >>> data = loadtxt(""myfile.txt"" comments = '!') # use '!' as comment char instead of '#' >>> data = loadtxt(""myfile.txt"" delimiter=';') # use ';' as column separator instead of whitespace >>> data = loadtxt(""myfile.txt"" dtype = int) # file contains integers instead of floats hello thanks for answering I can only doubt as he defines the path of where the file  Another option is numpy.genfromtxt e.g: import numpy as np data = np.genfromtxt(""myfile.dat""delimiter="""") This will make data a numpy array with as many rows and columns as are in your file  (I know the question is old but I think this might be good as a reference for people with similar questions) If you want to load data from an ASCII/text file (which has the benefit or being more or less human-readable and easy to parse in other software) numpy.loadtxt is probably what you want: http://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html If you just want to quickly save and load numpy arrays/matrices to and from a file take a look at numpy.save and numpy.load: http://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html http://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html  Have a look at SciPy cookbook. It should give you an idea of some basic methods to import /export data. If you save/load the files from your own Python programs you may also want to consider the Pickle module or cPickle. Pickling is inappropriate for arrays - while you can do it it will be slow as hell. Use np.save() to save in the .npy format or np.savez() to save a zipped archive of several arrays.",python numpy
118370,A,"How do you use the ellipsis slicing syntax in Python? This came up in Hidden features of Python but I can't see good documentation or examples that explain how the feature works. You'd use it in your own class since no builtin class makes use of it. Numpy uses it as stated in the documentation. Some examples here. In your own class you'd use it like this: >>> class TestEllipsis(object): ... def __getitem__(self item): ... if item is Ellipsis: ... return ""Returning all items"" ... else: ... return ""return %r items"" % item ... >>> x = TestEllipsis() >>> print x[2] return 2 items >>> print x[...] Returning all items Of course there is the python documentation and language reference. But those aren't very helpful. looks quite broken since the ""propper"" way to say all items is >>> x[:] >>> x[: 1:2] @Ronny: The point was to demonstrate some custom usage of Ellipsis. so true that docs are utterly cryptic  The ellipsis is used to slice higher-dimensional data structures. It's designed to mean at this point insert as many full slices (:) to extend the multi-dimensional slice to all dimensions. Example: >>> from numpy import arange >>> a = arange(16).reshape(2222) Now you have a 4-dimensional matrix of order 2x2x2x2. To select all first elements in the 4th dimension you can use the ellipsis notation >>> a[... 0].flatten() array([ 0 2 4 6 8 10 12 14]) which is equivalent to >>> a[:::0].flatten() array([ 0 2 4 6 8 10 12 14]) In your own implementations you're free to ignore the contract mentioned above and use it for whatever you see fit. This was a great explanation and good examples. Thanks!  Python documentation aren't very clear about this but there is another use of ellipsis. It is used as a representation of infinite data structures in case of Python. This question discusses how and some actual applications. This doesn't actually use the Python ellipsis object; it just uses ... when representing infinite structures as strings.  This is another use for Ellipsis which has nothing to do with slices: I often use it in intra-thread communication with queues as a mark that signals ""Done""; it's there it's an object it's a singleton and its name means ""lack of"" and it's not the overused None (which could be put in a queue as part of normal data flow). YMMV. P.S: I don't mind downvotes when what I say in an answer is not useful in relation to the question; then I try to improve my answer. But I sure can't understand how one can downvote any of the answers in this question— when the question is “how do you use the Ellipsis in Python”… It seems that people think that downvoting means “I disagree” or “I don't like this”. Never thought of that nice idea! Mightn't it be clearer to just say: ""Done = object()"" somewhere and just use that? Not necessarily - it requires you to actually _say_ Done=object() somewhere. Sentinel values aren't necessarily a bad thing -- and using otherwise nearly-useless Python singletons as sentinels isn't so horrible IMO (Ellipsis and () are the ones I've used where None would be confusing). +1 to counter unexplained anonymous downvotes. Regarding Done = object() I think using Ellipsis is better especially if you're using it for communication with queues. If you go from intra-thread to intra-process communication id(Done) will not be the same in the other process and there is nothing to distinguish one object from another. The id of Ellipsis won't be the same either but at least the type will be the same - this is the point of a singleton.",python numpy subclass slicing ellipsis
1511354,A,Using lambda for a constraint function import numpy from numpy import asarray Initial = numpy.asarray [2.0 4.0 5.0 3.0 5.0 6.0] # Initial values to start with bounds = [(1 5000) (1 6000) (2 100000) (1 50000) (1.0 5000) (2 1000000)] # actual passed bounds b1 = lambda x: numpy.asarray([1.4*x[0] - x[0]]) b2 = lambda x: numpy.asarray([1.4*x[1] - x[1]]) b3 = lambda x: numpy.asarray([x[2] - x[3]]) constraints = numpy.asarray([b1 b2 b3]) opt= optimize.fmin_slsqp(funcInitialieqcons=constraintsbounds=bounds full_output=Trueiter=200iprint=2 acc=0.01) Problem: I want to pass in inequality constraints. Consider that I have 6 parameters [ a b c d e f] in the Initial values and my constraints are: a<=e<=1.4*a ('e' varies from a to 1.4*a) b<=f<=1.4*b ('f' varies from b to 1.4*b) c>d ('c' must always be greater than d) But this is not working properly. I don't know what the mistake is. Is there any better way to pass my constraints as a function? Please help me. I don't know numpy but are a and b negative? otherwise I can't seee how any values of e and f can satisfy 1.4*a <= e <= a and 1.4*b <= f <= b. It would help if you state clearly what it is exactly that you are doing what is it you want to happen and what actually happens instead of just pasting a code fragment. @pear I've tried to answer your question but as hughdbrown says the constraints in your code above don't seem to work for positive numbers. Maybe the signs are backwards on the first two? Sorry all are positive values greater than 0. I have changed i hope its correct. Based on the comment from Robert Kern I have removed my previous answer. Here are the constraints as continuous functions: b1 = lambda x: x[4]-x[0] if x[4]<1.2*x[0] else 1.4*x[0]-x[4] b2 = lambda x: x[5]-x[1] if x[5]<1.2*x[1] else 1.4*x[1]-x[5] b3 = lambda x: x[2]-x[3] Note: Python 2.5 or greater is required for this syntax.1 To get the constraint a<=e<=1.4*a note that 1.2*a is the halfway point between a and 1.4*a. Below this point that is all e<1.2*a we use the continuous function e-a. Thus the overall constraint function is negative when e<a handling the lower out-of-bounds condition zero on the lower boundary e==a and then positive for e>a up to the halfway point. Above the halfway point that is all e>1.2*a we use instead the continuous function 1.4*a-e. This means the overall constraint function is is negative when e>1.4*a handling the upper out-of-bounds condition zero on the upper boundary e==1.4*a and then positive when e<1.4*a down to the halfway point. At the halfway point where e==1.2*a both functions have the same value. This means that the overall function is continuous. Reference: documentation for ieqcons. 1 - Here is pre-Python 2.5 syntax: b1 = lambda x: (1.4*x[0]-x[4] x[4]-x[0])[x[4]<1.2*x[0]] system Pause as Robert says function has to continuous. so i have made lambda x: ([1.4*x[0] - x[0]]) ie.lambda x:(max-min). But what am i made is correct or not i don't know. @system Pause Thank you it seems b1 and b2 are working good but b3 = lambda x: x[2]-x[3] not working as expected.For example some times func() passes [150 192 1.8487 2.07364 194 216] here b1 and b2 are well defined but b3 is violated. Like this it happens couple of times.what to do? any better way of defining 'b2'? No this will not work. The constraint functions should be continuous as much as possible >0 when they are satisfied <0 when they are not and ==0 when they are precisely on the boundary. @pear Sorry I don't know why `b3` is violated. Subtracting `c-d` (aka `x[2]-x[3]`) is a continuous function that is be positive when `c>d` and zero/negative when `c<=d`. But clearly 1.8487 < 2.07364 so I am stumped. @pear I don't think that can be correct as it does not use *e* (aka `x[4]`) at all in the expression. That expression is equivalent to `1.4*a - a` which is always going to have the value `0.4a`.,python lambda numpy scipy
1721802,A,What is the equivalent of MATLAB's repmat in NumPy I would like to execute the equivalent of the following MATLAB code using NumPy: repmat([1; 1] [1 1 1]). How would I accomplish this? Note that some of the reasons you'd need to use MATLAB's repmat are taken care of by NumPy's broadcasting mechanism which allows you to do various types of math with arrays of similar shape. So if you had say a 1600x1400x3 array representing a 3-color image you could (elementwise) multiply it by [1.0 0.25 0.25] to reduce the amount of green and blue at each pixel. See the above link for more information. thank kwatford :)  Here is a much better (official) NumPy for Matlab Users link - I'm afraid the mathesaurus one is quite out of date. The numpy equivalent of repmat(a m n) is tile(a (m n)). This works with multiple dimensions and gives a similar result to matlab. (Numpy gives a 3d output array as you would expect - matlab for some reason gives 2d output - but the content is the same). Matlab: >> repmat([1;1][111]) ans = 1 1 Python: In [46]: a = np.array([[1][1]]) In [47]: np.tile(a [111]) Out[47]: array([[[1] [1]]]) when i try size(repmat([1;1][112])) it get ans = 2 1 2 [in matlab] but in python np.tile(a[112]).shape it get(1 2 2)  i want numpy give result as same as matlab np.tile(a[:np.newaxis][112]) - it gives the same. Problem is tile promotes `a` to the dimension of the tile argument by *prepending* new axes as necessary. Matlab seems to work the other way. Similarly with 4d tiling you will need newaxis twice... so `np.tile(a[:newaxisnewaxis][1234]) = size(repmat(a[1 2 3 4]))` as required...  See NumPy for Matlab users. Matlab: repmat(a 2 3) Numpy: numpy.kron(numpy.ones((23)) a) I don't think the mathesaurus link is very good. There is an Numpy For Matlab Users page as part of the official documentation which is more comprehensive and up to date: http://www.scipy.org/NumPy_for_Matlab_Users thank you rcs i will try it I've edited the answer to point to the official docs. This is probably slow but it works nicely  Know both tile and repeat. x = numpy.arange(5) print numpy.tile(x 2) print x.repeat(2),python matlab numpy
1530598,A,"Definition of mathematical operations (sin…) on NumPy arrays containing objects I would like to provide ""all"" mathematical functions for the number-like objects created by a module (the uncertainties.py module which performs calculations with error propagation)—these objects are numbers with uncertainties. What is the best way to do this? Currently I redefine most of the functions from math in the module uncertainties.py so that they work on numbers with uncertainties. One drawback is that users who want to do from math import * must do so after doing import uncertainties. The interaction with NumPy is however restricted to basic operations (an array of numbers with uncertainties can be added etc.); it does not (yet) include more complex functions (e.g. sin()) that would work on NumPy arrays that contain numbers with uncertainties. The approach I have taken so far consists in suggesting that the user define sin = numpy.vectorize(math.sin) so that the new math.sin function (which works on numbers with uncertainties) is broadcast to the elements of any Numpy array. One drawback is that this has to be done for each function of interest by the user which is cumbersome. So what is the best way to extend mathematical functions such as sin() so that they work conveniently with simple numbers and NumPy arrays? The approach chosen by NumPy is to define its own numpy.sin rather than modifying math.sin so that it works with Numpy arrays. Should I do the same for my uncertainties.py module and stop redefining math.sin? Furthermore what would be the most efficient and correct way of defining sin so that it works both for simple numbers numbers with uncertainties and Numpy arrays? My redefined math.sin already handles simple numbers and numbers with uncertainties. However vectorizing it with numpy.vectorize is likely to be much slower on ""regular"" NumPy arrays than numpy.sin. It looks like following what NumPy itself does keeps things clean: ""extended"" mathematical operations (sin…) that work on new objects can be put in a separate name space. Thus NumPy has numpy.sin etc. These operations are mostly compatible with those from math but also work on NumPy arrays. Therefore it seems to me that mathematical functions that should work on usual numbers and NumPy arrays and their counterparts with uncertainties are best defined in a separate name space. For instance the user could do: from uncertainties import sin or from uncertainties import * # sin cos etc. For optimization purposes an alternative might be to provide two distinct sets of mathematical functions: those that generalize functions to simple numbers with uncertainties and those that generalize them to arrays with uncertainties: from uncertainties.math_ops import * # Work on scalars and scalars with uncertainty or from uncertainties.numpy_ops import * # Work on everything (scalars arrays numbers with uncertainties arrays with uncertainties)",python arrays numpy operations
1295994,A,"Numpy: Is there an array size limit? I'm learning to use Numpy and I wanted to see the speed difference in the summation of a list of numbers so I made this code: np_array = numpy.arange(1000000) start = time.time() sum_ = np_array.sum() print time.time() - start sum_ >>> 0.0 1783293664 python_list = range(1000000) start = time.time() sum_ = sum(python_list) print time.time() - start sum_ >>> 0.390000104904 499999500000 The python_list sum is correct. If I do the same code with the summation to 1000 both print the right answer. Is there an upper limit to the length of the Numpy array or is it with the Numpy sum function? Thanks for your help The standard list switched over to doing arithmetic with the long type when numbers got larger than a 32-bit int. The numpy array did not switch to long and suffered from integer overflow. The price for speed is smaller range of values allowed. >>> 499999500000 % 2**32 1783293664L  Numpy is creating an array of 32-bit unsigned ints. When it sums them it sums them into a 32-bit value. if 499999500000L % (2**32) == 1783293664L: print ""Overflowed a 32-bit integer"" You can explicitly choose the data type at array creation time: a = numpy.arange(1000000 dtype=numpy.uint64) a.sum() -> 499999500000  Notice that 499999500000 % 2**32 equals exactly 1783293664 ... i.e. numpy is doing operations modulo 2**32 because that's the type of the numpy.array you've told it to use. Make np_array = numpy.arange(1000000 dtype=numpy.uint64) for example and your sum will come out OK (although of course there are still limits with any finite-size number type). You can use dtype=numpy.object to tell numpy that the array holds generic Python objects; of course performance will decay as generality increases.",python numpy
1588224,A,"Is there any possibilty to convert recarray to ndarray and change ndim? I am getting recarray from matplotlib.mlab.csv2rec function. My expectation was it would have 2 dimensions like 'x' but it has 1 dimension like 'y'. Is there any way to get x from y? >>> import numpy as np >>> from datetime import date >>> x=np.array([(date(200011)01) ... (date(200011)11) ... (date(200011)10) ... (date(200011)00) ... ]) >>> x array([[2000-01-01 0 1] [2000-01-01 1 1] [2000-01-01 1 0] [2000-01-01 0 0]] dtype=object) >>> y = np.rec.fromrecords( x ) >>> y rec.array([(datetime.date(2000 1 1) 0 1) (datetime.date(2000 1 1) 1 1) (datetime.date(2000 1 1) 1 0) (datetime.date(2000 1 1) 0 0)] dtype=[('f0' '|O4') ('f1' '<i4') ('f2' '<i4')]) >>> x.ndim 2 >>> y.ndim 1 >>> x.shape (4 3) >>> y.ndim 1 >>> y.shape (4) >>> If your csv file has two columns csv2rec should create an array with two dimensions. Can you provide an example of the file you are parsing and the call to csv2rec that you make? moreover notice that you can use the new numpy.genfromtxt instead of csv2rec: it works better but you must pass it dtype=None as a parameter. actually it has 7 columns first one is date in a format dd/mm/yyyy then 6 doubles can different types be the cause? Well there might be a more efficient way than this but here is one way: #!/usr/bin/env python import numpy as np from datetime import date x=np.array([(date(200011)01) (date(200011)11) (date(200011)10) (date(200011)00) ]) y=np.rec.fromrecords( x ) z=np.empty((len(y)len(y.dtype))dtype='object') for idxfield in enumerate(y.dtype.names): z[:idx]=y[field] assert (x==z).all() sure but I can't believe there is no elegant way for such conversion :(  Sounds weird but... I can save to csv by using matplotlib.mlab.rec2csv and then read to ndarray by using numpy.loadtxt. My case is simpler as I already have csv file. Here is an example how it works. >>> a = np.loadtxt( 'name.csv' skiprows=1 delimiter='' converters = {0: lambda x: 0} ) >>> a array([[ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.  0.  0.  0.  0. ] [ 0.  0.29 0.29 0.43 0.29 0. ] [ 0.  0.71 0.29 0.57 0.  0. ] [ 0.  1.  0.57 0.71 0.  0. ] [ 0.  0.43 0.29 0.14 0.14 0. ] [ 0.  1.  0.43 0.71 0.  0. ] [ 0.  0.57 0.57 0.29 0.14 0. ] [ 0.  1.43 0.43 0.86 0.43 0. ] [ 0.  1.  0.71 0.57 0.  0. ] [ 0.  1.14 0.57 0.29 0.  0. ] [ 0.  1.43 0.29 0.71 0.29 0.29] [ 0.  1.14 0.43 1.  0.29 0.29] [ 0.  0.43 1.14 0.86 0.43 0.14] [ 0.  1.14 0.86 0.86 0.29 0.29]]) >>> t = a.any( axis = 1 ) >>> t array([False False False False False False False False False False False False False False False False True True True True True True True True True True True True True] dtype=bool) >>> a.ndim 2 Also in my case I don't need a first column for making a decision.  You can do it via pandas: import pandas as pd pd.DataFrame(y).values array([[2000-01-01 0 1] [2000-01-01 1 1] [2000-01-01 1 0] [2000-01-01 0 0]] dtype=object) But I would consider doing my project in pandas if I were you. Support for named columns is built much more deeply into pandas than into regular numpy. >>> z = pd.DataFrame.from_records(y index=""f0"") >>> z f1 f2 f0 2000-01-01 0 1 2000-01-01 1 1 2000-01-01 1 0 2000-01-01 0 0 >>> z[""f1""] f0 2000-01-01 0 2000-01-01 1 2000-01-01 1 2000-01-01 0 Name: f1 +1 for pandas which ought to make all of this easier but I believe the proper way to access a DataFrame as a numpy array is ``.values`` not ``.__array__()``. Changed __array__() to values; thanks @DanAllan",python numpy
1322380,A,"gotchas where Numpy differs from straight python? Folks is there a collection of gotchas where Numpy differs from python points that have puzzled and cost time ? ""The horror of that moment I shall never never forget !"" ""You will though"" the Queen said ""if you don't make a memorandum of it."" For example NaNs are always trouble anywhere. If you can explain this without running it give yourself a point -- from numpy import array NaN isnan pynan = float(""nan"") print pynan is pynan pynan is NaN NaN is NaN a = (0 pynan) print a a[1] is pynan any([aa is pynan for aa in a]) a = array(( 0 NaN )) print a a[1] is NaN isnan( a[1] ) (I'm not knocking numpy lots of good work there just think a FAQ or Wiki of gotchas would be useful.) Edit: I was hoping to collect half a dozen gotchas (surprises for people learning Numpy). Then if there are common gotchas or better common explanations we could talk about adding them to a community Wiki (where ?) It doesn't look like we have enough so far. should be community wiki No one mentioned primitive types. Does this mean a python float is equivalent to a np.float and so on? The biggest gotcha for me was that almost every standard operator is overloaded to distribute across the array. Define a list and an array >>> l = range(10) >>> l [0 1 2 3 4 5 6 7 8 9] >>> import numpy >>> a = numpy.array(l) >>> a array([0 1 2 3 4 5 6 7 8 9]) Multiplication duplicates the python list but distributes over the numpy array >>> l * 2 [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9] >>> a * 2 array([ 0 2 4 6 8 10 12 14 16 18]) Addition and division are not defined on python lists >>> l + 2 Traceback (most recent call last): File ""<stdin>"" line 1 in <module> TypeError: can only concatenate list (not ""int"") to list >>> a + 2 array([ 2 3 4 5 6 7 8 9 10 11]) >>> l / 2.0 Traceback (most recent call last): File ""<stdin>"" line 1 in <module> TypeError: unsupported operand type(s) for /: 'list' and 'float' >>> a / 2.0 array([ 0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5]) Numpy overloads to treat lists like arrays sometimes >>> a + a array([ 0 2 4 6 8 10 12 14 16 18]) >>> a + l array([ 0 2 4 6 8 10 12 14 16 18]) Yes that got me too. A simple table with columns: op python numpy would settle that. Actually ``a + a`` **is** defined for lists. It concatenates them.  A 0-d array of None looks like None but it is not the same: In [1]: print None None In [2]: import numpy In [3]: print numpy.array(None) None In [4]: numpy.array(None) is None Out[4]: False In [5]: numpy.array(None) == None Out[5]: False In [6]: print repr(numpy.array(None)) array(None dtype=object)  print pynan is pynan pynan is NaN NaN is NaN This tests identity that is if it is the same object. The result should therefore obviously be True False True because when you do float(whatever) you are creating a new float object. a = (0 pynan) print a a[1] is pynan any([aa is pynan for aa in a]) I don't know what it is that you find surprising with this. a = array(( 0 NaN )) print a a[1] is NaN isnan( a[1] ) This I did have to run. :-) When you stick NaN into an array it's converted into a numpy.float64 object which is why a[1] is NaN fails. This all seems fairly unsurprising to me. But then I don't really know anything much about NumPy. :-)  NaN is not a singleton like None so you can't really use the is check on it. What makes it a bit tricky is that NaN == NaN is False as IEEE-754 requires. That's why you need to use the numpy.isnan() function to check if a float is not a number. Or the standard library math.isnan() if you're using Python 2.6+. Well it's in the definition of NaN. def isnan(x): return (x != x)  Not such a big gotcha: With boolean slicing I sometimes wish I could do x[ 3<= y < 7 ] like the python double comparison. Instead I have to write x[ np.logical_and( 3<=y y<7) ] (Unless you know something better?) Also np.logical_and and np.logical_or only take two arguments each I would like them to take a variable number or a list so I could feed in more than just two logical clauses. (numpy 1.3 maybe this has all changed in later versions.) What about x[(3<=y) & (y<7)] ?  from Neil Martinsen-Burrell in numpy-discussion 7 Sept -- The ndarray type available in Numpy is not conceptually an extension of Python's iterables. If you'd like to help other Numpy users with this issue you can edit the documentation in the online documentation editor at numpy-docs  I found the fact that multiplying up lists of elements just creates view of elements caught me out. >>> a=[0]*5 >>>a [00000] >>>a[2] = 1 >>>a [00100] >>>b = [np.ones(3)]*5 >>>b [array([ 1. 1. 1.]) array([ 1. 1. 1.]) array([ 1. 1. 1.]) array([ 1. 1. 1.]) array([ 1. 1. 1.])] >>>b[2][1] = 2 >>>b [array([ 1. 2. 1.]) array([ 1. 2. 1.]) array([ 1. 2. 1.]) array([ 1. 2. 1.]) array([ 1. 2. 1.])] So if you create a list of elements like this and intend to do different operations on them you are scuppered ... A straightforward solution is to iteratively create each of the arrays (using a 'for loop' or list comprehension) or use a higher dimensional array (where e.g. each of these 1D arrays is a row in your 2D array which is generally faster).  Because __eq__ does not return a bool using numpy arrays in any kind of containers prevents equality testing without a container-specific work around. Example: >>> import numpy >>> a = numpy.array(range(3)) >>> b = numpy.array(range(3)) >>> a == b array([ True True True] dtype=bool) >>> x = (a 'banana') >>> y = (b 'banana') >>> x == y Traceback (most recent call last): File ""<stdin>"" line 1 in <module> ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() This is a horrible problem. For example you cannot write unittests for containers which use TestCase.assertEqual() and must instead write custom comparison functions. Suppose we write a work-around function special_eq_for_numpy_and_tuples. Now we can do this in a unittest: x = (array1 'deserialized') y = (array2 'deserialized') self.failUnless( special_eq_for_numpy_and_tuples(x y) ) Now we must do this for every container type we might use to store numpy arrays. Furthermore __eq__ might return a bool rather than an array of bools: >>> a = numpy.array(range(3)) >>> b = numpy.array(range(5)) >>> a == b False Now each of our container-specific equality comparison functions must also handle that special case. Maybe we can patch over this wart with a subclass? >>> class SaneEqualityArray (numpy.ndarray): ... def __eq__(self other): ... return isinstance(other SaneEqualityArray) and self.shape == other.shape and (numpy.ndarray.__eq__(self other)).all() ... >>> a = SaneEqualityArray( (2 3) ) >>> a.fill(7) >>> b = SaneEqualityArray( (2 3) ) >>> b.fill(7) >>> a == b True >>> x = (a 'banana') >>> y = (b 'banana') >>> x == y True >>> c = SaneEqualityArray( (7 7) ) >>> c.fill(7) >>> a == c False That seems to do the right thing. The class should also explicitly export elementwise comparison since that is often useful.  In [1]: bool([]) Out[1]: False In [2]: bool(array([])) Out[2]: False In [3]: bool([0]) Out[3]: True In [4]: bool(array([0])) Out[4]: False So don't test for the emptiness of an array by checking its truth value. Use size(array).  No one seems to have mentioned this so far: >>> all(False for i in range(3)) False >>> from numpy import all >>> all(False for i in range(3)) True >>> any(False for i in range(3)) False >>> from numpy import any >>> any(False for i in range(3)) True numpy's any and all don't play nicely with generators and don't raise any error warning you that they don't.  Slicing creates views not copies. >>> l = [1 2 3 4] >>> s = l[2:3] >>> s[0] = 5 >>> l [1 2 3 4] >>> a = array([1 2 3 4]) >>> s = a[2:3] >>> s[0] = 5 >>> a array([1 2 5 4]) not always: ""There are two kinds of fancy indexing in numpy which behave similarly ..."" http://mail.scipy.org/pipermail/numpy-discussion/2008-January/031101.html  (Related but a NumPy vs. SciPy gotcha rather than NumPy vs Python) Slicing beyond an array's real size works differently: >>> import numpy scipy.sparse >>> m = numpy.random.rand(2 5) # create a 2x5 dense matrix >>> print m[:3 :] # works like list slicing in Python: clips to real size [[ 0.12245393 0.20642799 0.98128601 0.06102106 0.74091038] [ 0.0527411 0.9131837 0.6475907 0.27900378 0.22396443]] >>> s = scipy.sparse.lil_matrix(m) # same for csr_matrix and other sparse formats >>> print s[:3 :] # doesn't clip! IndexError: row index out of bounds So when slicing scipy.sparse arrays you must make manually sure your slice bounds are within range. This differs from how both NumPy and plain Python work.  The truth value of a Numpy array differs from that of a python sequence type where any non-empty sequence is true. >>> import numpy as np >>> l = [0123] >>> a = np.arange(4) >>> if l: print ""Im true"" ... Im true >>> if a: print ""Im true"" ... Traceback (most recent call last): File ""<stdin>"" line 1 in <module> ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() >>> The numerical types are true when they are non-zero and as a collection of numbers the Nupy array inherits this definition. But with a collection of numbers truth could reasonably mean ""all elements are non-zero"" or ""at least one element is non-zero"". Numpy refuses to guess which definition is meant and raises the above exception. Using the .any() and .all() methods allows one to specify which meaning of true is meant. >>> if a.any(): print ""Im true"" ... Im true >>> if a.all(): print ""Im true"" ... >>>  I think this one is funny: >>> import numpy as n >>> a = n.array([[12][34]]) >>> a[1] a[0] = a[0] a[1] >>> a array([[1 2] [1 2]]) For Python lists on the other hand this works as intended: >>> b = [[12][34]] >>> b[1] b[0] = b[0] b[1] >>> b [[3 4] [1 2]] Funny side note: numpy itself had a bug in the shuffle function because it used that notation :-) (see here). The reason is that in the first case we are dealing with views of the array so the values are overwritten in-place. Could you explain how the numpy array ends up being so?",python numpy
1764859,A,How to compute laplacian of a field? I'm trying to compute the laplacian of a 2d field A using scipy.ndimage.convolve. stencil = numpy.array([[0 1 0][1 -4 1] [0 1 0]]) scipy.ndimage.convolve(A stencil mode='wrap') This doesn't seem to give me the right answer though. Any ideas where I'm going wrong or are there better ways of computing the laplacian in numpy? how does it seem not right ? do you have an example image to compare to ? I'm testing this out on a gaussian in 2d. So I have an array where I've evaluated the laplacian of the gaussian analytically and then I try my numerical laplacian on the gaussian itself. When I take the difference between the two the results are not close to 0. i seem to remember that a convolution with a laplace kernel is only an approximation of a laplace transform... By laplacian I mean: d^2(phi)/dx^2 + d^2(phi)/dy^2. The stencil I used is supposed to be a finite difference approximation to the laplacian. did you try another laplace convolution kernel like [[111][1-81][111]] ? Just tried it and it also doesn't seem to work.  I got another idea: did you take into account that your stencil in order to approximate the Laplacian should be divided by step**2 where step is the step size of your grid? Only then can you compare the ndimage.convolve result with the analytical result. In fact with a Gaussian I obtain results that indicate that ndimage.convolve works quite well: from scipy import ndimage stencil = numpy.array([[0 1 0][1 -4 1] [0 1 0]]) x = linspace(-10 10 100) y = linspace(-10 10 100) xx yy = meshgrid(x y) image = exp(-xx**2-yy**2) # Standard deviation in x or y: 1/sqrt(2) laplaced = ndimage.convolve(image stencil)/(x[1]-x[0])**2 # stencil from original post expected_result = -4*image + 8*(xx**2+yy**2)*image # Very close to laplaced in most points! I did actually take that into account when I worked with the result matrix. I think the problem I was seeing was that the grid I was using was too small.  I tried your example and it does work with the latest SciPy on my machine. I would suggest that you plot image-ndimage.convolve(…) in order to see how the convolution changes your image. Example: image = vstack(arange(-10 10)**2 for _ in range(20)) ndimage.convolve(image stencil mode='wrap') yields array([[-38 2 2 2 2 2 2 2 2 2 2 2 2...) which is quite correct (the second order derivative of x**2 being 2)—except for the left border. I get the same result and in 1d this works for the gaussian but the 2d gaussian still doesn't work for me.,python numpy scipy
1398822,A,"The Assignment Problem a numpy function? Since an assignment problem can be posed in the form of a single matrix I am wandering if numpy has a function to solve such a matrix. So far I have found none. Maybe one of you guys know if numpy/scipy has an assignment-problem-solve function? Edit: In the meanwhile I have found a python (not numpy/scipy) implementation at http://www.clapper.org/software/python/munkres/. Still I suppose a numpy/scipy implementation could be much faster right? What a shame it was not implemented with numpy. Not only might it be faster but the algorithm must be much easier to express with numpy as well. There is an implementation of the Munkres' algorithm as a python extension module which has numpy support. I've used it successfully on my old laptop. However it does not work on my new machine - I assume there is a problem with ""new"" numpy versions (or 64bit arch).  There is now a numpy implementation of the munkres algorithm in scikit-learn under sklearn/utils/linear_assignment_.py its only dependency is numpy. I tried it with some approximately 20x20 matrices and it seems to be about 4 times as fast as the one linked to in the question. cProfiler shows 2.517 seconds vs 9.821 seconds for 100 iterations.  No NumPy contains no such function. Combinatorial optimization is outside of NumPy's scope. It may be possible to do it with one of the optimizers in scipy.optimize but I have a feeling that the constraints may not be of the right form. NetworkX probably also includes algorithms for assignment problems.",python numpy scipy
910930,A,"Storing information on points in a 3d space I'm writing some code (just for fun so far) in Python that will store some data on every point in a 3d space. I'm basically after a 3d matrix object that stores arbitary objects that will allow me to do some advanced selections like: Get the point where x=1y=2z=3. Getting all points where y=2. Getting all points within 3 units of position x=1y=2z=3. Getting all points where point.getType() == ""Foo"" In all of the above I'd need to end up with some sort of output that would give me the original position in the space and the data stored at that point. Apparently numpy can do what I want but it seems highly optimised for scientific computing and working out how to get the data like I want above has so far eluded me. Is there a better alternative or should I return to banging my head on the numpy wall? :) EDIT: some more info the first three answers made me realise I should include: I'm not worried about performance this is purely a proof-of-concept where I'd prefer clean code to good performance. I will also have data for every point in the given 3d space so I guess a Spare Matrix is bad? Yeah just use a 3D array more complex structures are just for optimizing some operations in some precise cases. when you say ""points in space"" I assume that xyx are continuous variables e.g. floats but your example implies ints. If ints a sparse matrix is ok. Otherwise use tuples or objects. Personally I would recommend the climb over the numpy wall. It's green pasture on the other side! Well ... If you expect to really fill that space then you're probably best off with a densely packed matrix-like structure basically voxels. If you don't expect to fill it look into something a bit more optimized. I would start by looking at octrees which are often used for things like this. voxels are not a data structure it's a technique to display a 3D grid.  Here's an approach that may work. Each point is a 4-tuple (xyzdata) and your database looks like this: database = [ (xyzdata) (xyzdata) ... ] Let's look at your use cases. Get the point where x=1y=2z=3. [ (xyzdata) for xyzdata in database if (xyz) == (123) ] Getting all points where y=2. [ (xyzdata) for xyzdata in database if y == 2 ] Getting all points within 3 units of position x=1y=2z=3. [ (xyzdata) for xyzdata in database if math.sqrt((x-1)**2+(y-2)**2+(z-3)**2)<=3.0 ] Getting all points where point.getType() == ""Foo"" [ (xyzdata) for xyzdata in database if type(data) == Foo ] This is going to be insanely slow for searching! @Ed Woodcock: It's how RDBMS's work so there is a precedent for this architecture. Further indexing can be added to this if the data volume warrants it. Since we don't know the volume of data or the frequency of queries we don't know enough to reject this yet. The ""arbitrary object"" requirement is a small problem here: if it is immutable it requires you to do three steps to change an entry: 1. remove old tuple 2. build new tuple 3. append new tuple. Of course that won't be a problem if you use your own class for the data object. @RoadieRich: Doesn't matter what class the extra piece of data is. We do rebuild bunches of tuples. But these tuples contain the same original objects unchanged by being reincorporated into numerous new tuples. The underlying objects in all these tuples never change.  Here's another common approach class Point( object ): def __init__( self x y z data ): self.x self.y self.z = x y z self.data = data def distFrom( self x y z ) return math.sqrt( (self.x-x)**2 + (self.y-y)**2 + (self.z-z)**2 ) database = [ Point(xyzdata) Point(xyzdata) ... ] Let's look at your use cases. Get the point where x=1y=2z=3. [ p for p in database if (p.x p.y p.z) == ( 1 2 3 ) ] Getting all points where y=2. [ p for p in database if p.y == 2 ] Getting all points within 3 units of position x=1y=2z=3. [ p for p in database if p.distFrom( 1 2 3 ) <= 3.0 ] Getting all points where point.getType() == ""Foo"" [ p for p in database if type(p.data) == Foo ] Thanks! I was getting mixed up in data structures more complicated than they needed to be. This works very well and lets me do any sort of data selection as I want to each time.  You can do the first 2 queries with slicing in numpy : a = numpy.zeros((4 4 4)) a[1 2 3] # The point at x=1y=2z=3 a[: 2 :] # All points where y=2 For the third one if you mean ""getting all points within a sphere of radius 3 and centered at x=1y=2z=3"" you will have to write a custom function to do that ; if you want a cube you can proceed with slicing e.g.: a[1:3 1:3 1:3] # The 2x2x2 array sliced from the center of 'a' For the fourth query if the only data stored in your array is the cells type you could encode it as integers: FOO = 1 BAR = 2 a = numpy.zeros((4 4 4) dtype=""i"") a[1 2 3] = FOO a[3 2 1] = BAR def filter(a type_code): coords = [] for z in range(4): for y in range(4): for x in range(4): if a[x y z] == type_code: coords.append((x y z)) return coords filter(a FOO) # => [(1 2 3)] numpy looks like the good tool for doing what you want as the arrays will be smaller in memory easily accessible in C (or even better cython !) and extended slicing syntax will avoid you writing code.  One advantage of numpy is that it is blazingly fast e.g. calculating the pagerank of a 8000x8000 adjacency matrix takes milliseconds. Even though numpy.ndarray will only accept numbers you can store number/id-object mappings in an external hash-table i.e. dictionary (which in again is a highly optimized datastructure). The slicing would be as easy as list slicing in python: >>> from numpy import arange >>> the_matrix = arange(64).reshape(4 4 4) >>> print the_matrix[0][1][2] 6 >>> print the_matrix[0][1] [4 5 6 7] >>> print the_matrix[0] [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] If you wrap some of your desired functions (distances) around some core matrix and a id-object-mapping hash you could have your application running within a short period of time. Good luck! Actually it's far from pointless it's the method I'm going with now. I just store my own custom class at each point. The only reason I dislike it is that a lot of numpy seems to assume you know matrix math/terminology and since I don't it's just something else to swot up on. I thought ndarray could only store numbers too but it can store anything! numpy.ndarray([whd]object) This is what I'm currently wrestling with but it's annoying enough I posted here for better solutions that involve less knwledge of matrix stuff! ok than my suggestion is somewhat pointless i'll delete it; but why do you want to avoid ``matrix stuff``; matrices are i wouldn't say fun but somewhat neat. and for your slicing access the ``ndarray`` is great - so why not use it?  It depends upon the precise configuration of your system but from the example you give you are using integers and discrete points so it would probably be appropriate to consider Sparse Matrix data structures.  Using a dictionary with xyz tuples as keys is another solution if you want a relatively simple solution with the standard library. import math #use indexing to get point at (123): points[(123)] get_points(points x=None y=None x=None): """"""returns dict of all points with given xy and/or z values. Call using keywords (eg get_points(points x=3)"""""" filteredPoints = points.items() if x: filteredPoints = [p for p in filteredPoints if p[0][0] == x] if y: filteredPoints = [p for p in filteredPoints if p[0][1] == y] if z: filteredPoints = [p for p in filteredPoints if p[0][0] == x] return dict(filteredPoints) get_point_with_type(points type_): """"""returns dict of points with point.getType() == type_"""""" filteredPoints = points.items() return dict((positiondata) for positiondata in points.iterItems() if data.getType == type_) get_points_in_radius(pointsxyzr): """"""Returns a dict of points within radius r of point (xyz)"""""" def get_dist(x1y1z1x2y2z3): return math.sqrt((x1-x2)*(x1-x2)+(y1-y2)*(y1-y2)+(z1-z2)*(z1-z2)) return dict((positiondata) for positiondata in points.iterItems() if get_dist(xyz *position) <= r)) And due to python referencing you can alter ""points"" in the returned dictionaries and have the original points change as well (I think).  When to use Binary Space Partitioning Quadtree Octree? 3d array imo is worthless. Especially if your world is dynamic. You should decide between BSP Quadtree or Octtree. BSP would do just fine. Since your world is in 3d you need planes when splitting the bsp not lines. Cheers ! Edit I will also have data for every point in the given 3d space so I guess a Spare Matrix is bad? I guess this is alright if always know how large you data set is and that it never changes i.e. if more points are added to it that in turn are out of bound. You would have to resize the 3d array in that case.",python data-structures 3d matrix numpy
49926,A,Open source alternative to MATLAB's fmincon function? Is there an open-source alternative to MATLAB's fmincon function for constrained linear optimization? I'm rewriting a MATLAB program to use Python / NumPy / SciPy and this is the only function I haven't found an equivalent to. A NumPy-based solution would be ideal but any language will do. The open source Python packageSciPy has quite a large set of optimization routines including some for multivariable problems with constraints (which is what fmincon does I believe). Once you have SciPy installed type the following at the Python command prompt help(scipy.optimize) The resulting document is extensive and includes the following which I believe might be of use to you.  Constrained Optimizers (multivariate) fmin_l_bfgs_b -- Zhu Byrd and Nocedal's L-BFGS-B constrained optimizer (if you use this please quote their papers -- see help) fmin_tnc -- Truncated Newton Code originally written by Stephen Nash and adapted to C by Jean-Sebastien Roy. fmin_cobyla -- Constrained Optimization BY Linear Approximation  Python optimization software: OpenOpt http://openopt.org (this one is numpy-based as you wish with automatic differentiation by FuncDesigner) Pyomo https://software.sandia.gov/trac/coopr/wiki/Package/pyomo CVXOPT http://abel.ee.ucla.edu/cvxopt/ NLPy http://nlpy.sourceforge.net/  I don't know if it's in there but there's a python distribution called Enthought that might have what you're looking for. It was designed specifically for data analysis has over 60 additional libraries. Two other people added links that weren't sure whether or not their suggestions would have what the original poster wanted. Why the down votes. A comment would be nice here.  For numerical optimization in Python you may take a look at OpenOpt solvers: http://openopt.org/NLP http://openopt.org/Problems  GNU Octave is another MATLAB clone that might have what you need.  Is your problem convex? Linear? Non-linear? I agree that SciPy.optimize will probably do the job but fmincon is a sort of bazooka for solving optimization problems and you'll be better off if you can confine it to one of the categories below (in increasing level of difficulty to solve efficiently) Linear Program (LP) Quadratic Program (QP) Convex Quadratically-Constrained Quadratic Program (QCQP) Second Order Cone Program (SOCP) Semidefinite Program (SDP) Non-Linear Convex Problem Non-Convex Problem There are also combinatoric problems such as Mixed-Integer Linear Programs (MILP) but you didn't mention any sort of integrality constraints suffice to say that they fall into a different class of problems. The CVXOpt package will be of great use to you if your problem is convex. If your problem is not convex you need to choose between finding a local solution or the global solution. Many convex solvers 'sort of' work in a non-convex domain. Finding a good approximation to the global solution would require some form Simulated Annealing or Genetic Algorithm. Finding the global solution will require an enumeration of all local solutions or a combinatorial strategy such as Branch and Bound.  Have a look at http://www.aemdesign.com/downloadfsqp.htm. There you will find C code which provides the same functionality as fmincon. (However using a different algorithm. You can read the manual if you are interested in the details.) It's open source but not under GPL.  There is a program called SciLab that is a MATLAB clone. I haven't used it at all but it is open source and might have the function you are looking for.,python matlab numpy numerical scientific-computing
893657,A,"How do I calculate r-squared using Python and Numpy? I'm using Python and Numpy to calculate a best fit polynomial of arbitrary degree. I pass a list of x values y values and the degree of the polynomial I want to fit (linear quadratic etc.). This much works but I also want to calculate r (coefficient of correlation) and r-squared(coefficient of determination). I am comparing my results with Excel's best-fit trendline capability and the r-squared value it calculates. Using this I know I am calculating r-squared correctly for linear best-fit (degree equals 1). However my function does not work for polynomials with degree greater than 1. Excel is able to do this. How do I calculate r-squared for higher-order polynomials using Numpy? Here's my function: import numpy # Polynomial Regression def polyfit(x y degree): results = {} coeffs = numpy.polyfit(x y degree) # Polynomial Coefficients results['polynomial'] = coeffs.tolist() correlation = numpy.corrcoef(x y)[01] # r results['correlation'] = correlation # r-squared results['determination'] = correlation**2 return results @leif -- The request boils down to ""do it like Excel does"". I'm getting the feeling from these answers that the users may be reading too much into the r-squared value when using a non-linear best-fit curve. Nonetheless I'm not a math wizard and this is the requested functionality. Note: you use the degree only in the calculation of coeffs. tydok is correct. You are calculating the correlation of x and y and r-squared for y=p_0 + p_1 * x. See my answer below for some code that should work. If you don't mind me asking what is your ultimate goal? Are you doing model selection (choosing what degree to use)? Or something else? I have been using this successfully where x and y are array-like. def rsquared(x y): """""" Return R^2 where x and y are array-like."""""" slope intercept r_value p_value std_err = scipy.stats.linregress(x y) return r_value**2  From the numpy.polyfit documentation it is fitting linear regression. Specifically numpy.polyfit with degree 'd' fits a linear regression with the mean function E(y|x) = p_d * x**d + p_{d-1} * x **(d-1) + ... + p_1 * x + p_0 So you just need to calculate the R-squared for that fit. The wikipedia page on linear regression gives full details. You are interested in R^2 which you can calculate in a couple of ways the easisest probably being SST = Sum(i=1..n) (y_i - y_bar)^2 SSReg = Sum(i=1..n) (y_ihat - y_bar)^2 Rsquared = SSReg/SST Where I use 'y_bar' for the mean of the y's and 'y_ihat' to be the fit value for each point. I'm not terribly familiar with numpy (I usually work in R) so there is probably a tidier way to calculate your R-squared but the following should be correct import numpy # Polynomial Regression def polyfit(x y degree): results = {} coeffs = numpy.polyfit(x y degree) # Polynomial Coefficients results['polynomial'] = coeffs.tolist() # r-squared p = numpy.poly1d(coeffs) # fit values and mean yhat = p(x) # or [p(z) for z in x] ybar = numpy.sum(y)/len(y) # or sum(y)/len(y) ssreg = numpy.sum((yhat-ybar)**2) # or sum([ (yihat - ybar)**2 for yihat in yhat]) sstot = numpy.sum((y - ybar)**2) # or sum([ (yi - ybar)**2 for yi in y]) results['determination'] = ssreg / sstot return results Thank you this explanation is very clear to me. I'm going to try this one out. No problem glad to help. Exactly what I was looking for. I just want to point out that using the numpy array functions instead of list comprehension will be much faster e.g. numpy.sum((yi - ybar)**2) and easier to read According to wiki page http://en.wikipedia.org/wiki/Coefficient_of_determination the most general definition of R^2 is `R^2 = 1 - SS_err/SS_tot` with `R^2 = SS_reg/SS_tot` being just a special case.  The wikipedia article on r-squareds suggests that it may be used for general model fitting rather than just linear regression.  R-squared is a statistic that only applies to linear regression. Essentially it measures how much variation in your data can be explained by the linear regression. So you calculate the ""Total Sum of Squares"" which is the total squared deviation of each of your outcome variables from their mean. . . \sum_{i}(y_{i} - y_bar)^2 where y_bar is the mean of the y's. Then you calculate the ""regression sum of squares"" which is how much your FITTED values differ from the mean \sum_{i}(yHat_{i} - y_bar)^2 and find the ratio of those two. Now all you would have to do for a polynomial fit is plug in the y_hat's from that model but it's not accurate to call that r-squared. Here is a link I found that speaks to it a little. This seems to be the root of my problem. How does Excel get a different r-squared value for a polynomial fit vs. a linear regression then? are you just giving excel the fits from a linear regression and the fits from a polynomial model? It's going to calculate the rsq from two arrays of data and just assume that you're giving it the fits from a linear model. What are you giving excel? What is the 'best fit trendline' command in excel? It's part of the graphing functions of Excel. You can plot some data right-click on it then choose from several different types of trend lines. There is the option to see the equation of the line as well as an r-squared value for each type. The r-squared value is also different for each type. @Travis Beale -- you are going to get a different r-squared for each different mean function you try (unless two models are nested and the extra coeffecients in the larger model all work to be 0). So of course Excel gives a different r-squared values. @Baltimark -- this is linear regression so it is r-squared.  A very late reply but just in case someone needs a ready function for this: scipy.stats.stats.linregress i.e. slope intercept r_value p_value std_err = scipy.stats.linregress(x y) as in @Adam Marples's answer. It's reasonable to analyze with *coefficient of correlation* and then to do the bigger job *regression*.",python math statistics numpy curve-fitting
1987694,A,"Print the full numpy array When I print a numpy array I get a truncated representation but I want the full array. Is there any way to do this? Examples: >>> numpy.arange(10000) array([ 0 1 2 ... 9997 9998 9999]) >>> numpy.arange(10000).reshape(25040) array([[ 0 1 2 ... 37 38 39] [ 40 41 42 ... 77 78 79] [ 80 81 82 ... 117 118 119] ... [9880 9881 9882 ... 9917 9918 9919] [9920 9921 9922 ... 9957 9958 9959] [9960 9961 9962 ... 9997 9998 9999]]) Are you using `numpy` specifically? It looks like he is.. Is there a way to do it on a ""one off"" basis? That is to print out the full output once but not at other times in the script? @Matt O'Brien see ZSG's answer below This sounds like you're using numpy. If that's the case you can add: set_printoptions(threshold=nan) That will disable the corner printing. For more information see this NumPy Tutorial.  Here is a one-off way to do this if you don't want to change your default settings: def fullprint(*args **kwargs): from pprint import pprint import numpy opt = numpy.get_printoptions() numpy.set_printoptions(threshold='nan') pprint(*args **kwargs) numpy.set_printoptions(**opt)  To clarify on Reed's reply import numpy numpy.set_printoptions(threshold=numpy.nan) Note that the reply as given above works with an initial 'from numpy import *' which is not advisable. This also works for me numpy.set_printoptions(threshold='nan') For full documentation see http://docs.scipy.org/doc/numpy/reference/generated/numpy.set_printoptions.html.",python arrays numpy
1962980,A,"Selecting rows from a NumPy ndarray I want to select only certain rows from a NumPy array based on the value in the second column. For example this test array has integers from 1 to 10 in the second column. >>> test = numpy.array([numpy.arange(100) numpy.random.randint(1 11 100)]).transpose() >>> test[:10 :] array([[ 0 6] [ 1 7] [ 2 10] [ 3 4] [ 4 1] [ 5 10] [ 6 6] [ 7 4] [ 8 6] [ 9 7]]) If I wanted only rows where the second value is 4 it is easy: >>> test[test[: 1] == 4] array([[ 3 4] [ 7 4] [16 4] ... [81 4] [83 4] [88 4]]) But how do I achieve the same result when there is more than one wanted value? The wanted list can be of arbitrary length. For example I may want all rows where the second column is either 2 4 or 6: >>> wanted = [2 4 6] The only way I have come up with is to use list comprehension and then convert this back into an array and seems too convoluted although it works: >>> test[numpy.array([test[x 1] in wanted for x in range(len(test))])] array([[ 0 6] [ 3 4] [ 6 6] ... [90 2] [91 6] [92 2]]) Is there a better way to do this in NumPy itself that I am missing? numpy.in1d is what you are looking for: print test[numpy.in1d(test[:1] wanted)] It should easily be the fastest solution if wanted is large; plus it is the most readable one id say.  test[numpy.logical_or.reduce([test[:1] == x for x in wanted])] The result should be faster than the original version since NumPy's doing the inner loops instead of Python. This solution goes through the array len(wanted) times. It is usually faster to go through the array in a single pass. Thanks Amnon. This is the solution that I decided to accept. I think it is clear to understand and is about 20 x faster than my original solution.  The following solution should be faster than Amnon's solution as wanted gets larger: wanted_set = set(wanted) # Much faster look up than with lists for larger lists @numpy.vectorize def selected(elmt): return elmt in wanted_set # Or: selected = wanted_set.__contains__ print test[selected(test[: 1])] In fact it has the advantage of searching through the test array only once (instead of len(wanted) times). It also uses Python's built-in fast element look up in sets which are much faster for this than lists. It is also fast because it uses Numpy's fast loops. You also get the optimization of the in operator: once a wanted element matches the remaining elements do not have to be tested (as opposed to the ""logical or"" approach of Amnon were all the elements in wanted are tested no matter what). Alternatively you could use the following one-liner which also goes through your array only once: test[numpy.apply_along_axis(lambda x: x[1] in wanted 1 test)] This is much much slower though as this extracts the element in the second column at each iteration (instead of doing it in one pass as in the first solution of this answer). @Amnon: Good point and interesting results. Thanks! These solutions call python for every element instead of using numpy's comparison. According to my tests your first solution is faster than mine for len(wanted)=50 but slower for len(wanted)=5. EOL many thanks for your time and effort. Your explanations were clear. I chose to use Amnon's solution because for my usual scenario (len(test) about 1000 and len(wanted) about 3-5) that was faster than your first solution. The speed difference is not huge but I also found it clearer. But it was good to be reminded of numpy's vectorize and I am sure I will find a use for it soon.  This is two times faster than Amnon's variant for len(test)=1000: wanted = (246) wanted2 = numpy.expand_dims(wanted 1) print test[numpy.any(test[: 1] == wanted2 0) :] @ahatchkins: Some typos in your version. What you are suggesting is this - # convert wanted list into a one column array wanted = numpy.array(wanted).reshape((len(wanted)1)) # print test[numpy.any(test[:1] == wanted 0)] In my tests this is about 2 times faster than Amnon's solution. Yes there was a typo: s/wanted2/wanted/ . Fixed Hmm yes. You are right. Two typos in fact. And only 2 times faster. )",python numpy
1029207,A,"Interpolation in SciPy: Finding X that produces Y Is there a better way to find which X gives me the Y I am looking for in SciPy? I just began using SciPy and I am not too familiar with each function. import numpy as np import matplotlib.pyplot as plt from scipy import interpolate x = [70 80 90 100 110] y = [49.7 80.6 122.5 153.8 163.0] tck = interpolate.splrep(xys=0) xnew = np.arange(701111) ynew = interpolate.splev(xnewtckder=0) plt.plot(xy'x'xnewynew) plt.show() tck=tck yToFind = 140 print interpolate.sproot((tc-yToFindk)) #Lowers the spline at the abscissa Can you elaborate on what you want to be better? Performance accuracy conciseness? The UnivariateSpline class in scipy makes doing splines much more pythonic. x = [70 80 90 100 110] y = [49.7 80.6 122.5 153.8 163.0] f = interpolate.UnivariateSpline(x y s=0) xnew = np.arange(701111) plt.plot(xy'x'xnewf(xnew)) To find x at y then do: yToFind = 140 yreduced = np.array(y) - yToFind freduced = interpolate.UnivariateSpline(x yreduced s=0) freduced.roots() I thought interpolating x in terms of y might work but it takes a somewhat different route. It might be closer with more points. Wouldn't this require twice the amount of CPU calculations since are interpolating practically the same data set two times? @JcMaco the first use of UnivariateSpline is just to make a pretty plot. The second usage is what actually gives the values. Should that be `freduced.roots()` on the last line? Craig is right can you correct it on your example as it's otherwise great! Fixed the typo. Thanks Craig.  If all you need is linear interpolation you could use the interp function in numpy. Your question didn't specify what type of interpolation you needed -- if linear isn't good enough for your problem then I don't think interp will help. I'd prefer spline interpolation. How would the interp function help me solve my problem more easily?  I may have misunderstood your question if so I'm sorry. I don't think you need to use SciPy. NumPy has a least squares function. #!/usr/bin/env python from numpy.linalg.linalg import lstsq def find_coefficients(data exponents): X = tuple((tuple((pow(xp) for p in exponents)) for (xy) in data)) y = tuple(((y) for (xy) in data)) x resids rank s = lstsq(Xy) return x if __name__ == ""__main__"": data = tuple(( (1.47 52.21) (1.50 53.12) (1.52 54.48) (1.55 55.84) (1.57 57.20) (1.60 58.57) (1.63 59.93) (1.65 61.29) (1.68 63.11) (1.70 64.47) (1.73 66.28) (1.75 68.10) (1.78 69.92) (1.80 72.19) (1.83 74.46) )) print find_coefficients(data range(3)) This will return [ 128.81280358 -143.16202286 61.96032544]. >>> x=1.47 # the first of the input data >>> 128.81280358 + -143.16202286*x + 61.96032544*(x**2) 52.254697219095988 0.04 out not bad The problem is finding which number will give me 52.21. Of course there can be many solutions if the interpolation is quadratic (or higher power).",python numpy scipy interpolation scientific-computing
790960,A,"How to synthesize sounds? I'd like to produce sounds that would resemble audio from real instruments. The problem is that I have very little clue how to get that. What I know this far from real instruments is that sounds they output are rarely clean. But how to produce such unclean sounds? This far I've gotten to do this it produces quite plain sound from which I'm not sure it's even using the alsa correctly. import numpy from numpy.fft import fft ifft from numpy.random import random_sample from alsaaudio import PCM PCM_NONBLOCK PCM_FORMAT_FLOAT_LE pcm = PCM()#mode=PCM_NONBLOCK) pcm.setrate(44100) pcm.setformat(PCM_FORMAT_FLOAT_LE) pcm.setchannels(1) pcm.setperiodsize(4096) def sine_wave(x freq=100): sample = numpy.arange(x*4096 (x+1)*4096 dtype=numpy.float32) sample *= numpy.pi * 2 / 44100 sample *= freq return numpy.sin(sample) for x in xrange(1000): sample = sine_wave(x 100) pcm.write(sample.tostring()) As other people said not a trivial topic at all. There are challenges both at the programming side of things (especially if you care about low-latency) and the synthesis part. A goldmine for sound synthesis is the page by Julius O. Smith. There is a lot of techniques for synthesis http://ccrma-www.stanford.edu/~jos/.  Sound synthesis is a complex topic which requires many years of study to master. It is also not an entirely solved problem although relatively recent developments (such as physical modelling synthesis) have made progress in imitating real-world instruments. There are a number of options open to you. If you are sure that you want to explore synthesis further then I suggest you start by learning about FM synthesis. It is relatively easy to learn and implement in software at least in basic forms and produces a wide range of interesting sounds. Also check out the book ""The Computer Music Tutorial"" by Curtis Roads. It's a bible for all things computer music and although it's a few years old it is the book of choice for learning the fundamentals. If you want a quicker way to produce life-like sound consider using sampling techniques: that is record the instruments you want to reproduce (or use a pre-existing sample bank) and just play back the samples. It's a much more straightforward (and often more effective) approach. I wouldn't like to use sample banks. I want something that resembles instruments not life-like at all.  Cheery if you want to generate (from scratch) something that really sounds ""organic"" i.e. like a physical object you're probably best off to learn a bit about how these sounds are generated. For a solid introduction you could have a look at a book such as Fletcher and Rossings The Physics of Musical Instruments. There's lots of stuff on the web too you might want to have a look at a the primer James Clark has here Having at least a skim over this sort of stuff will give you an idea of what you are up against. Modeling physical instruments accurately is very difficult! If what you want to do is have something that sounds physical rather something that sounds like instrument X your job is a bit easier. You can build up frequencies quite easily and stack them together add a little noise and you'll get something that at least doesn't sound anything like a pure tone. Reading a bit about Fourier analysis in general will help as will Frequency Modulation (FM) techniques. Have fun!  I agree that this is very non-trivial and there's no set ""right way"" but you should consider starting with a (or making your own) MIDI SoundFont.",python numpy alsa
1601613,A,"python contour for binary 2D matrix I want to calculate a convex hull around a shape in a binary NxM matrix. The convex hull algorithm expects a list of coordinates so I take numpy.argwhere(im) to have all shape point coordinates. However most of those points are not contributing to the convex hull (they lie on the inside of the shape). Because convex hull computation time is at least proportional to the number of points that it gets as input I devised an idea to filter the plethora of useless points on beforehand and only pass those that span the outline. The idea is quite simple that for each row in the binary NxM matrix I take only the minimal and maximal indices. So for example: im = np.array([[1110] [1011] [1101] [0000] [0111]] dtype=np.bool) outline = somefunc(im) Then outline should read (in tuples or as a 5x2 numpy array I don't mind): [(00)(02)(10)(13)(20)(23)(41)(43)] Any convex hull tight around this shape (im) must a subset of these points (outline). In other words if ""somefunc()"" is efficient in filtering the inside points then it saves time for the convex hull computation. I have code that does the above trick but I am hoping someone has a more clever (read faster) approach since I need to run it many many times. The code I have is: # I have a 2D binary field. random for the purpose of demonstration. import numpy as np im = np.random.random((320360)) > 0.9 # This is my algorithm so far. Notice that coords is sorted. coords = np.argwhere(im) left = np.roll(coords[:0] 1 axis=0) != coords[:0] outline = np.vstack([coords[left] coords[left[1:]] coords[-1]]) Another idea I had was to use Python's reduce() so I'd need to run over the list of coords only once. But I have difficulty finding a good reducing function. Any help would greatly be appreciated! edit In the meanwhile I have found a faster way to go from im directly to outline. At least with large images this is significantly faster. In the apparent absence of an external solution I am positing it as the solution to this question. Still if somebody knows an even faster method please speak up :) @Paul: Probably a good approach to complain because parts of your question are unclear and unhelpful. If we can't get your question (including your tags) we're pretty much unable to help. `fast` is not very helpful tag *sigh* I'm looking for answers wise guy. In the absence of an acceptable answer I post my best working code as the solution. def outline(im): ''' Input binary 2D (NxM) image. Ouput array (2xK) of K (yx) coordinates where 0 <= K <= 2*M. ''' topbottom = np.empty((12*im.shape[1]) dtype=np.uint16) topbottom[00:im.shape[1]] = np.argmax(im axis=0) topbottom[0im.shape[1]:] = (im.shape[0]-1)-np.argmax(np.flipud(im) axis=0) mask = np.tile(np.any(im axis=0) (2)) xvalues = np.tile(np.arange(im.shape[1]) (12)) return np.vstack([topbottomxvalues])[:mask].T  For more general solution you could use somekind of edge detection method to find only the edge points. I believe (Google..) that NumPy has built-in sobel filter which will do that. The filter will give you the bitmap/matrix where you can find all the indices like you did in your code. Oh wait you got the example image with sobel and it has too many points? yes I know the sobel filter and it is marvelous. In fact that is how I got to these binary images. the filter does not give indices however. You're right. I want to fit a polygon around the (thresholded) sobel. And therefore first I'd like to reduce the number of points. This last part is where I am trying to find fast code for. In machine vision application I've been using there's contour output in blob tool but unfortunately I don't see such feature in OpenCV. That would've been elegant solution..  This assignment seems to accomplish the same thing as your last two steps: outline = np.array(dict(reversed(coords)).items() + dict(coords).items()) Don't know if it's any faster however. Too bad it's 10 times(!) slower. That is an interesting approach. Trying it now...",python algorithm numpy contour
971678,A,Iterating through a multidimensional array in Python I have created a multidimensional array in Python like this: self.cells = np.empty((rc)dtype=np.object) Now I want to iterate through all elements of my twodimensional array and I do not care about the order. How do I achieve this? Just iterate over one dimension then the other. for row in self.cells: for cell in row: do_something(cell) Of course with only two dimensions you can compress this down to a single loop using a list comprehension or generator expression but that's not very scalable or readable: for cell in (cell for row in self.cells for cell in row): do_something(cell) If you need to scale this to multiple dimensions and really want a flat list you can write a flatten function. Wow this answer is ancient. You're right; will fix. Isn't the way he did it fine? It's just a generator expression instead of a list comprehension...am I missing something? O.o You got it wrong. It should be: for cell in [cell for row in self.cells for cell in row]: do_something(cell) The 'for's used to be backwards. I edited it since.  It's clear you're using numpy. With numpy you can just do: for cell in self.cells.flat: do_somethin(cell)  How about this: import itertools for cell in itertools.chain(*self.cells): cell.drawCell(surface posx posy) `itertools.chain.from_iterable(self.cells)`  If you need to change the values of the individual cells then ndenumerate (in numpy) is your friend. Even if you don't it probably still is! for indexvalue in ndenumerate( self.cells ): do_something( value ) self.cells[index] = new_value,python arrays multidimensional-array numpy iteration
1589706,A,"Iterating over arbitrary dimension of numpy.array Is there function to get an iterator over an arbitrary dimension of a numpy array? Iterating over the first dimension is easy... In [63]: c = numpy.arange(24).reshape(234) In [64]: for r in c : ....: print r ....: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]] But iterating over other dimensions is harder. For example the last dimension: In [73]: for r in c.swapaxes(20).swapaxes(12) : ....: print r ....: [[ 0 4 8] [12 16 20]] [[ 1 5 9] [13 17 21]] [[ 2 6 10] [14 18 22]] [[ 3 7 11] [15 19 23]] I'm making a generator to do this myself but I'm surprised there isn't a function named something like numpy.ndarray.iterdim(axis=0) to do this automatically. I'd use the following: c = numpy.arange(2 * 3 * 4) c.shape = (2 3 4) for r in numpy.rollaxis(c 2): print(r) The function rollaxis creates a new view on the array. In this case it's moving axis 2 to the front equivalent to the operation c.transpose(2 0 1). +1: Quite direct but unfortunately a tad slower than the simple `c[::i]` approach (not sure why).  There is special Ellipsis object in python which can be passed to __getitem__ or written as ... in slice operations: >>> c[... ... 0] array([[ 0 4 8] [12 16 20]]) @Denis: The ellipsis is less appropriate here than "":"": the ellipsis is intended to represent any number of "":"". Your `c[......0]` would thus be better written as `c[...0]`.  I guess there is no function. When I wrote my function I ended up taking the iteration EOL also suggested. For future readers here it is: def iterdim(a axis=0) : a = numpy.asarray(a); leading_indices = (slice(None))*axis for i in xrange(a.shape[axis]) : yield a[leading_indices+(i)]  What you propose is quite fast but the legibility can be improved with the clearer forms: for i in range(c.shape[-1]): print c[::i] or better (faster and more explicit): for i in range(c.shape[-1]): print c[...i] However the first approach above appears to be about twice as slow as the swapaxes() approach: python -m timeit -s 'import numpy; c = numpy.arange(24).reshape(234)' 'for r in c.swapaxes(20).swapaxes(12): u = r' 100000 loops best of 3: 3.69 usec per loop python -m timeit -s 'import numpy; c = numpy.arange(24).reshape(234)' 'for i in range(c.shape[2]): u = c[::i]' 100000 loops best of 3: 6.08 usec per loop python -m timeit -s 'import numpy; c = numpy.arange(24).reshape(234)' 'for r in numpy.rollaxis(c 2): u = r' 100000 loops best of 3: 6.46 usec per loop I would guess that this is because swapaxes() does not copy any data and because the handling of c[::i] might be done through general code (that handles the case where : is replaced by a more complicated slice). Note however that the more explicit second solution c[...i] is both quite legible and quite fast: python -m timeit -s 'import numpy; c = numpy.arange(24).reshape(234)' 'for i in range(c.shape[2]): u = c[...i]' 100000 loops best of 3: 4.74 usec per loop  So one can iterate over the first dimension easily as you've shown. Another way to do this for arbitrary dimension is to use numpy.rollaxis() to bring the given dimension to the first (the default behavior) and then use the returned array (which is a view so this is fast) as an iterator. In [1]: array = numpy.arange(24).reshape(234) In [2]: for array_slice in np.rollaxis(array 1): ....: print array_slice.shape ....: (2 4) (2 4) (2 4) EDIT: I'll comment that I submitted a PR to numpy to address this here: https://github.com/numpy/numpy/pull/3262. The concensus was that this wasn't enough to add to the numpy codebase. I think using np.rollaxis is the best way to do this and if you want an interator wrap it in iter().",python numpy iterate
1735263,A,"Help with Python UnboundLocalError: local variable referenced before assignment I have post the similar question beforehoweverI think I may have misinterpreted my questionso may I just post my origin code hereand looking for someone can help meI am really stuck now..thanks alot. from numpy import * import math as M #initial condition All in SI unit G=6.673*10**-11 #Gravitational constant ms=1.9889*10**30 #mass of the sun me=5.9742*10**24 #mass of the earth dt=10 #time step #Creat arrays vs=array([[000]]) #1st element stand for x component of V of earth ve=array([[2977000]]) rs=array([[000]]) re=array([[01.4960*10**110]]) #First update velocity in order to start leapfrog approximation fs=-G*ms*me*((rs-re)/(M.sqrt((rs-re)[0][0]**2+(rs-re)[0][1]**2+(rs-re)[0][2]**2))**3) fe=-fs vs=vs+fs*dt/ms ve=ve+fe*dt/me n=input('please enter the number of timestep you want it evolve:') #update force def force(nmsmersreG): rsre=update_r(rsrendt) fs=-G*ms*me*((rs-re)/(M.sqrt((rs-re)[0][0]**2+(rs-re)[0][1]**2+(rs-re)[0][2]**2))**3) fe=-fs return fsfe #update velocities def update_v(nvsvemsmedtfsfe): fsfe=force(nmsmersreG) i=arange(n) vs=vs+fs[:]*i[:newaxis]*dt/ms ve=ve+fe[:]*i[:newaxis]*dt/me return vsve #update position def update_r(rsrendt): vsve=update_v(nvsvemsmedtfsfe) i=arange(n) rs=rs+vs[:]*i[:newaxis]*dt re=re+ve[:]*i[:newaxis]*dt return rsre #there is start positionvrf all have initial arrays(when n=0). #then it should calculate f(n=1) then use this to update v(n=0) #to v(n=1)then use v(n=1) update r(n=0) to r(n=1)then use r(n=1) #update f(n=1) to f(n=2)....and so on until finish n.but this code seems doesnt do thishow can I make it? – when i call force python gives: please enter the number of timestep you want it evolve:4Traceback (most recent call last): File ""<pyshell#391>"" line 1 in <module> force(nmsmersreG) File ""/Users/Code.py"" line 24 in force rsre=update_r(rsrendt) File ""/Users/Code.py"" line 39 in update_r vsve=update_v(nvsvemsmedtfsfe) UnboundLocalError: local variable 'vs' referenced before assignment can anyone give me some tips?thanks...... The use of ""from numpy import *"" is a bad practice. It pollutes the global namespace. ""import numpy as np"" is better. If you have specific functions you use a lot and you are tired of writing np.sin() np.cos etc you should import those specifically (""from numpy import sin""). cheers. thanks for the tips:) Put an additional global statement containing all your globals after each def statement. Otherwise all globals are transformed into locals within your def without it. def update_v(nvsvemsmedtfsfe): global vs ve ...  where do you call force in this code? In any event the problem is in update_r. You reference vs in the first line of update_r even though vs is not defined in this function. Python is not looking at the vs defined above. Try adding global vs as the first line of update_r or adding vs to the parameter list for update_r  In the first line of update_r you have vsve=update_v(nvsvemsmedtfsfe). Look at the function that you are calling. You are calling update_v with a bunch of parameters. One of these parameters is vs. However that is the first time in that function that vs appears. The variable vs does not have a value associated with it yet. Try initializing it first and your error should disappear there is a global definition of vs. normally it would have been used. the only reason that it's not is the vs in the left hand side of the assignment. that defines an unintialized local variable with the same name. I did intend to convey that but perhaps I should have worded my answer better. Sorry for the miscommunicaton thanks for replayhoweverin my understanding(newb's)is that vs on the left hand side doesnt not really matter if it has the sam name with one of the global variables it is local.please help me with understanding this Okay so you're assigning a local variable called vs to the return value of some computation on the global variable vs. While you and I understand this distinction the Python interpreter does not. When you create the local vs (which is being assigned) then the next call to vs will be to the local vs. So what happens is that the Python interpreter is trying to perform the computation on the local vs (which has no value yet). I would fix it by doing this: temp_vsve=update_v(nvsvemsmedtfsfe) vs = temp_vs This should fix your problem. Also do what foosion says (before the vs init)  On line 39 you do vsve=update_v(nvsvemsmedtfsfe) while you are inside a function. Since you defined a global variable called vs you would expect this to work. It would have worked if you had: vs_newve_new = update_v(nvsvemsmedtfsfe) because then the interpreter knows vs in the function arguments is the global one. But since you had vs in the left hand side you created an uninitialized local variable. But dude you have a much bigger problem in your code: update_r calls update_v update_v calls force and force calls update_r - you will get a stack overflow :) thanks for replay..the thing is how can I solve this bigger problemthat is actually the reason I post thisit does really make me feel badplease help me..",python numpy
1100100,A,"FFT-based 2D convolution and correlation in Python Is there a FFT-based 2D cross-correlation or convolution function built into scipy (or another popular library)? There are functions like these: scipy.signal.correlate2d - ""the direct method implemented by convolveND will be slow for large data"" scipy.ndimage.correlate - ""The array is correlated with the given kernel using exact calculation (i.e. not FFT)."" scipy.fftpack.convolve.convolve which I don't really understand but seems wrong Numarray had a correlate2d() function with a 'fft=True' switch (http://structure.usc.edu/numarray/node61.html) but I guess numarray was folded into numpy and I can't find if this function was included. note that using exact calculation (no FFT) is exactly the same as saying it is slow :) More exactly the FFT-based method will be much faster if you have a signal and a kernel of approximately the same size (if the kernel is much smaller than the input then FFT may actually be slower than the direct computation). Oh you're not talking about zero padding you're talking about matching a 5x5 image with a 2000x2000 image. Why can't the algorithm just guess whether the FFT would be more efficient and do it whichever way is faster? Ideally the FFT algorithm would automatically take care of zero-padding things to the right size for best speed. scipy has an fftconvolve function http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.fftconvolve.html#scipy.signal.fftconvolve I wrote a cross-correlation/convolution wrapper that takes care of padding & nans and includes a simple smooth wrapper here. It's not a popular package but it also has no dependencies besides numpy (or fftw for faster ffts). I've also implemented an FFT speed testing code here in case anyone's interested. It shows - surprisingly - that numpy's fft is faster than scipy's at least on my machine. EDIT: moved code to N-dimensional version here  I've lost track of the status of this package in scipy but I know we include ndimage as part of the stsci_python release package as a convenience for our users: http://www.stsci.edu/resources/software_hardware/pyraf/stsci_python/current/download or you should be able pull it from the repository if you prefer: https://www.stsci.edu/svn/ssb/stsci_python/stsci_python/trunk/ndimage/ Turns out stsci is being removed from SciPy (which is why it doesn't work) and the stsci_python version is now the authoritative one so I'm moving this to be the accepted answer. According to SciPy docs it's not FFT-based though as I mentioned in the question. http://www.scipy.org/SciPyPackages/Ndimage Oh! I can just import that python file directly if I remove the reference to _correlate. The FFT correlation is all in Python. Now I've got it working. :) Thanks! The convolve package is also available from the stsci_python repository. It includes the correlate2d function that has the fft=True switch that you also mentioned. https://www.stsci.edu/svn/ssb/stsci_python/stsci_python/trunk/convolve/lib/Convolve.py Also the 1-D convolve/correlate is not FFT-accelerated. :(  I think you want the scipy.stsci package: http://docs.scipy.org/doc/scipy/reference/stsci.html In [30]: scipy.__version__ Out[30]: '0.7.0' In [31]: from scipy.stsci.convolve import convolve2d correlate2d I saw that too but it doesn't seem to be included in SciPy anymore? >>> import scipy.stsci.convolve Traceback (most recent call last): File """" line 1 in ImportError: No module named convolve Hi - I pasted the output from my prompt above. What's your version? Clearly something is wrong: http://pastebin.com/mdd2bc6d Good to know it exists though. Werid. From your ipython prompt I see you're using python 2.6. I have python 2.5.2. I have no idea why scipy would have a different release per version. Maybe it's easier to just re-install scipy and see if the problem persists? It works on my Windows machine with 2.6 but not on other Ubuntu machines so it must be a packaging issue with Ubuntu. https://bugs.launchpad.net/bugs/397217 you could use correlate2d from scipy.signal: it uses more or less the same implementation technique as the stsci.convolve one (no FFT). The 2.6 problem is weird - may be related to a distutils thing.  I found scipy.signal.fftconvolve as also pointed out by magnus but didn't realize at the time that it's n-dimensional. Since it's built-in and produces the right values it seems like the ideal solution. From Example of 2D Convolution: In [1]: a = asarray([[ 1 2 3] ...: [ 4 5 6] ...: [ 7 8 9]]) In [2]: b = asarray([[-1-2-1] ...: [ 0 0 0] ...: [ 1 2 1]]) In [3]: scipy.signal.fftconvolve(a b mode = 'same') Out[3]: array([[-13. -20. -17.] [-18. -24. -18.] [ 13. 20. 17.]]) Correct! The STSCI version on the other hand requires some extra work to make the boundaries correct? In [4]: stsci.convolve2d(a b fft = True) Out[4]: array([[-12. -12. -12.] [-24. -24. -24.] [-12. -12. -12.]]) (The STSCI method also requires compiling which I was unsuccessful with (I just commented out the non-python parts) has some bugs like this and modifying the inputs ([1 2] becomes [[1 2]]) etc. So I changed my accepted answer to the built-in fftconvolve() function.) Correlation of course is the same thing as convolution but with one input reversed: In [5]: a Out[5]: array([[3 0 0] [2 0 0] [1 0 0]]) In [6]: b Out[6]: array([[3 2 1] [0 0 0] [0 0 0]]) In [7]: scipy.signal.fftconvolve(a b[::-1 ::-1]) Out[7]: array([[ 0. -0. 0. 0. 0.] [ 0. -0. 0. 0. 0.] [ 3. 6. 9. 0. 0.] [ 2. 4. 6. 0. 0.] [ 1. 2. 3. 0. 0.]]) In [8]: scipy.signal.correlate2d(a b) Out[8]: array([[0 0 0 0 0] [0 0 0 0 0] [3 6 9 0 0] [2 4 6 0 0] [1 2 3 0 0]]) and the latest revision has been sped up by using power-of-two sizes internally (and then I sped it up more by using real FFT for real input and using 5-smooth lengths instead of powers of 2 :D ).  look at scipy.signal.fftconvolve signal.convolve and signal.correlate (there is a signal.correlate2d but it seems to return an shifted array not centered). I changed my accepted answer to this as explained below http://stackoverflow.com/questions/1100100/fft-based-2d-convolution-and-correlation-in-python/1768140#1768140",python image numpy signal-processing fft
1734626,A,"Each looping return a result I am a beginner and got an issue really head around now. Here is the code: n=3 #time step #f v and r are arrayseg [345] #rvf all have initial array which is when n=0 def force(): r=position() f=r*2 return f def position(n): v=velocity(n) for i in range(n): #This part may wrong... r=v*i #How can I return results when i=01...5? return r def velocity(n): f=force for i in range(n): v=f*i #Same problem here..... return v Another problem is force. It is a function of position which is a function of velocity and velocity is a function of force. So it's kind of a logic loop. I can't even start. Physically it should initially start from force at time=0 then carrying on looping. But I just don't know how to do it in Python. Also how can I make the row of rv to be the results with the evolution of the time? You can use yield.  def velocity(n): f=force for i in range(n): v=f*i yield(v)  for vel in velocity(n): //do something One working example. It will print the output of function test as soon as it yields. So you do not need to wait for the next iteration of the loop.  import time def test(): for i in range(10): time.sleep(i) yield(i)  for k in test(): print k thanks for replayhowever i dont quite understand whats does yield do? +1 for using generators. A much under valued/used feature in my opinion. I have changed to yieldhowever it gives '''' when I call the function..it seems is a mistake..also how can I make these three functions run?they messed together how to start?thanks what exactly you are trying to achieve and what are you initial inputs? sorry about the confusing i made. The initial input is just n(timestep)and i want is the code gives two arraysone for r one for vwhich the rows of them are correspond to i=0123...n this code is trying to simulate gravity force between 2 stars.Thanks for helping AFAIK your code says: force depends on position position depends on velocity and velocity depends on force. So there is no start position you are stuck in a circle. Or may be try to clarify the problem statement. Or is this some sort of http://en.wikipedia.org/wiki/Recurrence_relation? there is start positionvrf all have initial arrays(when n=0).then it should calculate f(n=1) then use this to update v(n=0) to v(n=1)then use v(n=1) update r(n=0) to r(n=1)then use r(n=1) update f(n=1) to f(n=2)....and so on until finish n.  You need to use append to add to the list. def position(n): v=velocity(n) r = array() for i in range(n): #this part may wrong... r.append(v*i) #how can I return results when i=01...5? return r thanks for replay yesi am going to use appendbut it doesnt really solve the question seems..  You could use a list comprehension: def position(n): v=velocity(n) return [v*i for i in range(n)] Or since you are using numpy: v=np.array([123]) # array([1 2 3]) you can use numpy broadcasting to express the entire calculation in one blow: i=np.arange(5) # array([0 1 2 3 4]) v[:]*i[:np.newaxis] # array([[ 0 0 0] # [ 1 2 3] # [ 2 4 6] # [ 3 6 9] # [ 4 8 12]]) In the above calculation the scalar values in i (e.g. 01234) are each multiplied against the array v. The results are collected in a 2-d numpy array. Each row corresponds to a different value of i. See http://www.scipy.org/EricsBroadcastingDoc for an introduction to numpy broadcasting. @OP: To address your question regarding a ""logic loop"": Typically what you do is define a ""state"" of the system. Perhaps in your case a state would consist of a tuple (timepositionvelocity). You would then define a function which is given a state-tuple as an input and return a new state-tuple as output. Given a (timepositionvelocity) the force could be computed (mainly from the old position). From the force you then compute the new velocity. From the velocity you compute a new position. Don't write code first. In this case sit down with paper and pencil and grind out the calculation by hand with a concrete example. Do enough iterations until you see clearly the pattern of how the calculation is done. What is the order of the steps? What parts get repeated? Once you see how to do it by hand it will be much clearer how to write the python code. thanks for replaybut i do need use array unluckly.. thanks alotit helps! however still left a bit of question see my functions relationship..really messhow can i sort it out?thanks again I've added a bit of advice on how to resolve the ""logic loop"". thanks alotwhat you have said is exactly the same as my tutor said which is really important and i think i have already know clearly about the physicsjust dont know how to translate to python code to add a bit more: there is start positionvrf all have initial arrays(when n=0).then it should calculate f(n=1) then use this to update v(n=0) to v(n=1)then use v(n=1) update r(n=0) to r(n=1)then use r(n=1) update f(n=1) to f(n=2)....and so on until finish n.  It looks like you're trying to do an Euler algorithm and getting a bit mixed up in the looping. Here's how I think it should look (and I'll assume this is for a game and not homework... if it is for homework you should state that clearly so we don't give the full answer like I'm doing here.) This example is for a ball on a spring which I think you're aiming for. I'm the example my initial conditions are to thrown diagonally along the x-z axis and I've also included gravity (if you didn't intend to you vectors you can just replace all the vector quantities with scalars e.g. t x v = 0. 0. 2.; etc). from numpy import * # set the start conditions etc. n_timesteps = 100 dt m k = .1 1. 2. # timestep mass spring-const (I'll write the equations correctly so the units make sense) t x v = 0. array([0.0.0.]) array([2. 0. 2.]) # initial values gravity = array([0. 0. -9.8]) # to make the problem a little more interesting result = zeros((4 n_timesteps)) # run the simulation looping through the timesteps for n in range(n_timesteps): # do the calculation f = -k*x + gravity a = f/m v += a*dt x += v*dt # store the results t += dt # just for easy record keeping result[0n] = t result[1:4 n] = x Note that for loop is looping over the timesteps (and the all looping over the vectors is handles by numpy broadcasting e.g. f = -k*x+gravity what could be easier?). Also note that the force is set first and then we work our way down the chain of integrating the derivatives then go back to the top and start at the force again. (You're right that this is a bit asymmetric and really we should update them all at the same time or something like that and this is a deficiency of the Euler method but it works well enough for small timesteps.) Here's what the plots look like... the ball oscillates as expected Edit: To clarify your question: Basically your code's issue is not a question of ""starting the functions"" as you imply; instead your code is approaching the problem in the wrong way so you need to fix the approach. It looks like you're trying to iterate your timesteps within each function. This is incorrect! Instead you need to do an enveloping iteration through the timesteps and for each timestep update the current state of each variable used in the calculation at that timestep. You can write this update as a separate function or for example you can do it inline like I did. But it makes no sense to iterate the timesteps within each variable calculation function. Instead for your example to make sense force velocity and other functions should have as inputs things at the present timestep and return an update to the state of that variable to be used in the next timestep. See how my example does this: it just cycles through the timesteps and within each timestep cycle it sequentially updates all the variables basing each updated variable on the variables that were updated just before it in this current timestep. Thanks for replayhoweverthe real problem in my question is how can I start these functions(I got initial conditions)thanks",python arrays numpy
993984,A,"Why NumPy instead of Python lists? Is it worth my learning NumPy? I have approximately 100 financial markets series and I am going to create a cube array of 100x100x100 = 1 million cells. I will be regressing (3-variable) each x with each y and z to fill the array with standard errors. I have heard that for ""large matrices"" I should use NumPy as opposed to Python lists for performance and scalability reasons. Thing is I know Python lists and they seem to work for me. Is the scale of the above problem worth moving to NumPy? What if I had 1000 series (that is 1 billion floating point cells in the cube)? Note also that there is support for timeseries based on numpy in the timeseries scikits: http://pytseries.sourceforge.net For regression I am pretty sure numpy will be order of magnitude faster and more convenient than lists even for the 100^3 problem  Numpy is not just more efficient it is also more convenient. You get a lot of vector and matrix operations for free which sometimes allow one to avoid unnecessary work. And they are also efficiently implemented. For example you could read your cube directly from a file into an array: x = numpy.fromfile(file=open(""data"") dtype=float).reshape((100 100 100)) Sum along the second dimension: s = x.sum(axis=1) Find which cells are above a threshold: (x > 0.5).nonzero() Remove every even-indexed slice along the third dimension: x[: : ::2] Also other libraries that could be useful for you work on numpy arrays. For example statistical analysis and visualization libraries. Even if you don't have performance problems learning numpy is worth the effort. Thanks - you have provided another good reason in your third example as indeed I will be searching the matrix for cells above threshold. Moreover I was loading up from sqlLite. The file approach will be much more efficient.  Alex mentioned memory efficiency and Roberto mentions convenience and these are both good points. For a few more ideas I'll mention speed and functionality. Functionality: You get a lot built in with Numpy FFTs convolutions fast searching basic statistics linear algebra histograms etc. And really who can live without FFTs? Speed: Here's a test on doing a sum over a list and a numpy array showing that the sum on the numpy array is 10x faster (in this test -- mileage may vary). from numpy import arange from timeit import Timer Nelements = 10000 Ntimeits = 10000 x = arange(Nelements) y = range(Nelements) t_numpy = Timer(""x.sum()"" ""from __main__ import x"") t_list = Timer(""sum(y)"" ""from __main__ import y"") print ""numpy: %.3e"" % (t_numpy.timeit(Ntimeits)/Ntimeits) print ""list: %.3e"" % (t_list.timeit(Ntimeits)/Ntimeits) which on my systems (while I'm running a backup) gives: numpy: 3.004e-05 list: 5.363e-04  Here's a nice answer from the FAQ on the scipy.org website: What advantages do NumPy arrays offer over (nested) Python lists? Python’s lists are efficient general-purpose containers. They support (fairly) efficient insertion deletion appending and concatenation and Python’s list comprehensions make them easy to construct and manipulate. However they have certain limitations: they don’t support “vectorized” operations like elementwise addition and multiplication and the fact that they can contain objects of differing types mean that Python must store type information for every element and must execute type dispatching code when operating on each element. This also means that very few list operations can be carried out by efficient C loops – each iteration would require type checks and other Python API bookkeeping.  Speed wise I'm not so sure of. Here is a quick example: I've created a function(of x) that returns a list of prime numbers between 2 and x: regular python function using lists: def findprimeupto(x): primes = [] n_primes = [] for i in range(2 x): if not (i in n_primes): primes.append(i) n_primes.append(i) for j in range(len(primes)): if i > n_primes[j]: n_primes[j] += primes[j] return primes import time start_time = time.time() findprimeupto(10000) print(""--- %s seconds ---"" % str(time.time() - start_time)) and C like python function using numpy arrays: import numpy def findprimeupto(x): primes = numpy.array(numpy.zeros(x) dtype=numpy.int32) n_primes = numpy.array(numpy.zeros(x) dtype=numpy.int32) primeslen = 0 for i in range(2 x): flag = 1 for j in range(primeslen): if n_primes[j] == i: flag = 0 break if flag: primes[primeslen] = i n_primes[primeslen] = i primeslen += 1 for j in range(primeslen): if i > n_primes[j]: n_primes[j] += primes[j] return [primeslen primes] import time start_time = time.time() result = findprimeupto(10000) #for i in range(result[0]): # print('{:d} '.format(result[1][i]) end="""") print() print(""--- %s seconds ---"" % str(time.time() - start_time)) The former supposedly slow implementation using lists is executed in 0.6 seconds and the later supposedly fast numpy implementation is needs 50 seconds. If someone can point out why I'd greatly appreciate it. BTW pure C program which is more or less a copy of numpy version of the function executes in less than 0.04s. The speed of C is even more obvious with large x:  #include <stdio.h> #include <stdlib.h> #include <time.h> void findprimesupto(int n int *primeslen int *primes int *n_primes) { int i j flag; *primeslen = 0; for (i=2; i <= n; i++) { for (j=0 flag=1; j < *primeslen; j++) if (n_primes[j] == i) { flag = 0; break; } if (flag) { primes[*primeslen] = i; n_primes[*primeslen] = i; (*primeslen)++; } for (j=0; j < *primeslen; j++) if (i > n_primes[j]) n_primes[j] += primes[j]; } } int main() { int n = 10000 primeslen = 0 i; int *primes *n_primes; clock_t start diff; start = clock(); primes = malloc(n * sizeof(int)); n_primes = malloc(n * sizeof(int)); findprimesupto(n &primeslen primes n_primes); /* for (i=0; i < primeslen; i++) printf(""%d "" primes[i]); printf(""\n""); */ diff = clock() - start; printf(""Time: %f s\n"" (float) diff / (float) CLOCKS_PER_SEC); free(primes); free(n_primes); return 0; } I'm trying to find appropriate changes to the code using native numpy functions but this seems to be particularly difficult case as it differs significantly from standard linear algebra problems numpy is well suited for. I suspect numpy array access functions are slow in fact I've just posted a test in another thread that demonstrates that but I don't see how I can get around it. There is one other thing. If I define arrays of floats instead of integers namely if I change declarations of primes and n_primes to: primes = numpy.zeros(x) n_primes = numpy.zeros(x) then runtime drops from 50s to 9s. This would indicate issues with typecasting (i.e. all numpy functions assume arguments are type float) which indeed could gobble up time. All in all numpy is not well suited for general purpose integer arrays. Shame though. NumPy's speed depends on making good use of NumPy's capabilities for vectorized operations. You need to rewrite your NumPy example in a more NumPy-friendly style replacing Python-level for loops with NumPy vectorized operations. You can't just replace lists with NumPy arrays and expect code to run faster.  NumPy's arrays are more compact than Python lists -- a list of lists as you describe in Python would take at least 20 MB or so while a NumPy 3D array with single-precision floats in the cells would fit in 4 MB. Access in reading and writing items is also faster with NumPy. Maybe you don't care that much for just a million cells but you definitely would for a billion cells -- neither approach would fit in a 32-bit architecture but with 64-bit builds NumPy would get away with 4 GB or so Python alone would need at least about 12 GB (lots of pointers which double in size) -- a much costlier piece of hardware! The difference is mostly due to ""indirectness"" -- a Python list is an array of pointers to Python objects at least 4 bytes per pointer plus 16 bytes for even the smallest Python object (4 for type pointer 4 for reference count 4 for value -- and the memory allocators rounds up to 16). A NumPy array is an array of uniform values -- single-precision numbers takes 4 bytes each double-precision ones 8 bytes. Less flexible but you pay substantially for the flexibility of standard Python lists! Alex - always the good answer. Thank you - point made. I'll go with Numpy for scalability and indeed for efficiency. I'm thinking I'll also soon be needing to learn parallel programming in Python and invest in some OpenCL capable hardware ;) Tx @Thomas always glad to help! It's super alex here to answer NumPy questions in the blink of an eye :)",python numpy
752482,A,"Can I compile numpy & scipy as eggs for free on Windows32? I've been asked to provide Numpy & Scipy as python egg files. Unfortunately Numpy and Scipy do not make official releases of their product in .egg form for a Win32 platform - that means if I want eggs then I have to compile them myself. At the moment my employer provides Visual Studio.Net 2003 which will compile no version of Numpy later than 1.1.1 - every version released subsequently cannot be compiled with VS2003. What I'd really like is some other compiler I can use perhaps for free but at a push as a free time-limited trial... I can use that to compile the eggs. Is anybody aware of another compiler that I can get and use without paying anything and will definitely compile Numpy on Windows? Please only suggest something if you know for a fact that that it will compile Numpy - no speculation! Thanks Notes: I work for an organization which is very sensitive about legal matters so everything I do has to be totally legit. I've got to do everything according to licensed terms and will be audited. My environment: Windows 32 Standard C Python 2.4.4 Try compiling the whole Python stack with MinGW32. This is a GCC-Win32 development environment that can be used to build Python and a wide variety of software. You will probably have to compile the whole Python distribution with it. Here is a guide to compiling Python with MinGW. Note that you will probably have to provide a python distribution that is compiled with MinGW32 as well. If recompiling the Python distro is not a goer I believe that Python 2.4 is compiled using VS2003. You are probably stuck with back-porting Scipy and Numpy to VS2003 or paying a consultant to do it. I would dig out the relevant mailing lists or contact the maintainers and get some view of the effort that would be required to do it. Another alternative would be to upgrade the version of Python to a more recent one but you will probably have to regression test your application and upgrade the version of Visual Studio to 2005 or 2008. No good - we use a stock CPython from python.org. We cannot use a special python. :-( There is absolutely no need to build your own python to build numpy (or any package for that matter) with mingw.  If you just need the compiler it is part of the .NET framework. For instance you can find the 3.5 framework (Which is used be visual studio 2008) in: ""C:\Windows\Microsoft.NET\Framework\v3.5"" You cannot reliably compile numpy (or any python extension) for python 2.4 with VS 2008. You need to use the same compiler as the one python itself was built with which is VS 2003 in python 2.4 case  You could try GCC for Windows. GCC is the compiler most often used for compiling Numpy/Scipy (or anything else really) on Linux so it seems reasonable that it should work on Windows too. (Never tried it myself though) And of course it's distributed under the GPL so there shouldn't be any legal barriers. The GCC coming with mingw is support by numpy. It is actually the compiler I use to build the official windows installers.",python windows numpy scipy
316410,A,Is there a good NumPy clone for Jython? I'm a relatively new convert to Python. I've written some code to grab/graph data from various sources to automate some weekly reports and forecasts. I've been intrigued by the Jython concept and would like to port some Python code that I've written to Jython. In order to do this quickly I need a NumPy clone for Jython (or Java). Is there anything like this out there? I can't find anything that's a clone of numpy but there's a long list of Java numerics packages here - these should all be usable from Jython. Which one meets your requirements depends on what you're doing with numpy I guess.  Wilberforce is essentially corrrect. However I suggest looking at the Apache Commons Math library -- that would be a better choice for a replacement Java numerics package than any of those listed in wilberforce's answer. JScience Java library is an amazingly powerful library that covers many aspects of math in Java even symbolic calculus - http://www.jscience.org/  There is a build called JNumeric available on sourceforge: The sourceforge version has not had a release in a long time but it seems like an updated version for Jython 2.51 is also available (have not tried it myself): http://bitbucket.org/zornslemon/jnumeric-ra/downloads/  Incanter a Clojure scientific/statistical computing library uses the Parallel Colt Java libraries with great success: http://incanter.org/. One route may be to start using the PColt classes in Jython and slowly build up Python-esque bindings for it as Incanter provides? (Let me know if you have interest in this.)  If you need analysis package for numerical calculations you can try SCaVis numerical calculations that uses Jython,java python numpy jython
1972877,A,Matlab's griddata3 for numpy? I realize that there is a griddata for numpy via matplotlib but is there a griddata3 (same has griddata but for higher dimensions). In other words I have (xyzd(xyz)) where (xyz) form an irregular grid and d(xyz) is a scalar function of three variables. I need to generate d(xiyizi) for a new set of (xiyizi) points using some kind of interpolation that can handle the non-uniformity of the original (xyz) data. Ultimately the (xiyizid(xiyizi)) data will have to be rendered as a surface somehow but that's a problem for later. I also do not have an analytical form for the d(.) function; I just have data for it. Any help appreciated. Not sure how you intend to render a surface of a scalar function of 3 variables except perhaps using cutplanes or something similar. Mayavi (really VTK which powers Mayavi) has support for efficient Delaunay triangulation via enthought.mayavi.mlab.pipeline.delaunay3d which is the core of the algorithm used by griddata3. See the 2D example code they have posted just add one dimension (and use delaunay3d instead). I don't know of a way to explicitly get the interpolated values used to render the surface but there might be a way to sample it through Mayavi you could dig through the documentation or ask on one of the Enthought mailing lists. Alternatively one of the C functions in the NCAR natgrid library may be useful i.e. dsgrid3d. There is a partial wrapper implemented as a matplotlib toolkit. thanks! I will look into these.  I am not familiar with griddata3 but you might want to look into meshgrid and this related post.  Scipy 0.9 (at the moment a first beta is out) has a new griddata function that can handle N-dimensional data. yay! great work!,python numpy analysis interpolation scientific-computing
1894981,A,"How best to hold 1000 different data series using TimeSeries module in Python? I want to create a massive TimeSeries object which will hold 1000 different financial markets data series each storing 1500 daily-data points. I'm quite new to the TimeSeries module and am a little confused as to how I would best go about it. So a few basic questions: 1) Should I use a huge numpy array of 1000x1500 and simply feed that to the time series constructor function time_series()? 2) If I do this how will I index each series by name (eg ""S&P500"" or ""GOLD"" for example)? I know I will be able to access the array by date but will I have to have a separate data structure to link series names with their column numbers in the large array? 3) Or should I use a structured data type as per the example given in the docs(http://pytseries.sourceforge.net/core.timeseries.html)? If so how do I append series one by one to the timeseries since I don't want to create a massive non-numpy structure to feed to the time_series() constructor in one shot? Advice on where I can get some good examples for financial markets and timeseries module in general would also be appreciated. Thanks. 1) i once implemented a pagerank algorithm for a small set (~10K) of linked documents therefore in during the calculation a 10Kx10K matrix had to be handled for which the numpy array implementation was - as i recall - blazingly fast. 2) imho storing metadata like series name externally does not hurt that much .. 3) i haven't worked with scikits.timeseries but would definitely look into it; as far as i can see the project lives around the same scipy orbit as numpy ..  For help on this have a look at Quantlib which is a useful library for financial work and which has an active users mailing list. In addition read this book review for a book entitled Financial Modeling in Python. Thanks Michael - I know Quantlib. Excellent for pricing stuff not so great for doing stats on large datasets. As for the book - thank you ** 10 because I have been looking for such a book for a while! Do have a look at the quantlib-users mailing list and try asking your question there as well. Even though quantlib doesn't solve this particular technical problem it is likely that the quantlib community of users have some experience wrestling with time-series data.",python numpy finance
1783251,A,Growing matrices columnwise in NumPy In pure Python you can grow matrices column by column pretty easily: data = [] for i in something: newColumn = getColumnDataAsList(i) data.append(newColumn) NumPy's array doesn't have the append function. The hstack function doesn't work on zero sized arrays thus the following won't work: data = numpy.array([]) for i in something: newColumn = getColumnDataAsNumpyArray(i) data = numpy.hstack((data newColumn)) # ValueError: arrays must have same number of dimensions So my options are either to remove the initalization iside the loop with appropriate condition: data = None for i in something: newColumn = getColumnDataAsNumpyArray(i) if data is None: data = newColumn else: data = numpy.hstack((data newColumn)) # works ... or to use a Python list and convert is later to array: data = [] for i in something: newColumn = getColumnDataAsNumpyArray(i) data.append(newColumn) data = numpy.array(data) Both variants seem a little bit awkward to be. Are there nicer solutions? Generally it is expensive to keep reallocating the NumPy array - so your third solution is really the best performance wise. However I think hstack will do what you want - the cue is in the error message ValueError: arrays must have same number of dimensions` I'm guessing that newColumn has two dimensions (rather than a 1D vector) so you need data to also have two dimensions... for example data = np.array([[]]) - or alternatively make newColumn a 1D vector (generally if things are 1D it is better to keep them 1D in NumPy so broadcasting etc. work better). in which case use np.squeeze(newColumn) and hstack or vstack should work with your original definition of the data.  The hstack can work on zero sized arrays: import numpy as np N = 5 M = 15 a = np.ndarray(shape = (N 0)) for i in range(M): b = np.random.rand(N 1) a = np.hstack((a b))  NumPy actually does have an append function which it seems might do what you want e.g. import numpy as NP my_data = NP.random.random_integers(0 9 9).reshape(3 3) new_col = NP.array((5 5 5)).reshape(3 1) res = NP.append(my_data new_col axis=1) your second snippet (hstack) will work if you add another line e.g. my_data = NP.random.random_integers(0 9 16).reshape(4 4) # the line to add--does not depend on array dimensions new_col = NP.zeros_like(my_data[:-1]).reshape(-1 1) res = NP.hstack((my_data new_col)) hstack gives the same result as concatenate((my_data new_col) axis=1) i'm not sure how they compare performance-wise. While that's the most direct answer to your question i should mention that looping through a data source to populate a target via append while just fine in python is not idiomatic NumPy. Here's why: initializing a NumPy array is relatively expensive and with this conventional python pattern you incur that cost more or less at each loop iteration (i.e. each append to a NumPy array is roughly like initializing a new array with a different size). For that reason the common pattern in NumPy for iterative addition of columns to a 2D array is to initialize an empty target array once(or pre-allocate a single 2D NumPy array having all of the empty columns) the successively populate those empty columns by setting the desired column-wise offset (index)--much easier to show than to explain: >>> # initialize your skeleton array using 'empty' for lowest-memory footprint >>> M = NP.empty(shape=(10 5) dtype=float) >>> # create a small function to mimic step-wise populating this empty 2D array: >>> fnx = lambda v : NP.random.randint(0 10 v) populate NumPy array as in the OP except each iteration just re-sets the values of M at successive column-wise offsets >>> for index itm in enumerate(range(5)): M[:index] = fnx(10) >>> M array([[ 1. 7. 0. 8. 7.] [ 9. 0. 6. 9. 4.] [ 2. 3. 6. 3. 4.] [ 3. 4. 1. 0. 5.] [ 2. 3. 5. 3. 0.] [ 4. 6. 5. 6. 2.] [ 0. 6. 1. 6. 8.] [ 3. 8. 0. 8. 0.] [ 5. 2. 5. 0. 1.] [ 0. 6. 5. 9. 1.]]) of course if you don't known in advance what size your array should be just create one much bigger than you need and trim the 'unused' portions when you finish populating it >>> M[:3:3] array([[ 9. 3. 1.] [ 9. 6. 8.] [ 9. 7. 5.]]) @JohnBarça thanks for the feedback. You might be right that the details of my code snippet should have been more carefully chosen--ie in my example the value of 'index' at every iteration is indeed the same as the value of the loop variable. That's an artifact though--the values of these two variables are likely to be not equal in practice (eg the iterable is a list containing values to pass to a function which creates the 1D arrays which are then 'inserted' into the target array). Very helpful post for a numpy newbie. Quick question: is there any reason why you use `for index itm in enumerate(range(5)):` rather than just for example `for x in range(5):` seeing as index and itm have the same value and only one is used.  Usually you don't keep resizing a NumPy array when you create it. What don't you like about your third solution? If it's a very large matrix/array then it might be worth allocating the array before you start assigning its values: x = len(something) y = getColumnDataAsNumpyArray.someLengthProperty data = numpy.zeros( (xy) ) for i in something: data[i] = getColumnDataAsNumpyArray(i),python arrays numpy
432112,A,Is there a Numpy function to return the first index of something in an array? I know there is a method for python list to return the first index of something l = list(123) l.index(2) >>> 1 Is there something like that for numpy arrays? Thanks for your help :) If you're going to use this as an index into something else you can use boolean indices if the arrays are broadcastable; you don't need explicit indices. The absolute simplest way to do this is to simply index based on a truth value. other_array[first_array == item] Any boolean operation works: a = numpy.arange(100) other_array[first_array > 50] The nonzero method takes booleans too: index = numpy.nonzero(first_array == item)[0][0] The two zeros are for the tuple of indices (assuming first_array is 1D) and then the first item in the array of indices.  There are lots of operations in numpy that could perhaps be put together to accomplish this. This will return indices of elements equal to item: numpy.nonzero(array - item) You could then take the first elements of the lists to get a single element. wouldn't that give the indices of all elements that are *not* equal to item? Yeah this is close but gives the not equals...  If you need the index of the first occurrence of only one value you can use nonzero (or where which amounts to the same thing in this case): >>> t = array([1 1 1 2 2 3 8 3 8 8]) >>> nonzero(t == 8) (array([6 8 9])) >>> nonzero(t == 8)[0][0] 6 If you need the first index of each of many values you could obviously do the same as above repeatedly but there is a trick that may be faster. The following finds the indices of the first element of each subsequence: >>> nonzero(r_[1 diff(t)[:-1]]) (array([0 3 5 6 7 8])) Notice that it finds the beginning of both subsequence of 3s and both subsequences of 8s: [1 1 1 2 2 3 8 3 8 8] So it's slightly different than finding the first occurrence of each value. In your program you may be able to work with a sorted version of t to get what you want: >>> st = sorted(t) >>> nonzero(r_[1 diff(st)[:-1]]) (array([0 3 5 7])) Could you please explain what `r_` is? @Geoff `r_` concatenates; or more precisely it translates slice objects to concatenation along each axis. I could have used [`hstack`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html) instead; that may have been less confusing. See [the documentation](http://docs.scipy.org/doc/numpy/reference/generated/numpy.r_.html) for more information about `r_`. There is also a [`c_`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html). +1 nice one! (vs NP.where) your solution is a lot simpler (and probably faster) in the case where it's only the first occurrence of a given value in a 1D array that we need  to index on any criteria you can so something like the following: In [1]: from numpy import * In [2]: x = arange(125).reshape((555)) In [3]: y = indices(x.shape) In [4]: locs = y[:x >= 120] # put whatever you want in place of x >= 120 In [5]: pts = hsplit(locs len(locs[0])) In [6]: for pt in pts: .....: print(' '.join(str(p[0]) for p in pt)) 4 4 0 4 4 1 4 4 2 4 4 3 4 4 4 [edit] and here's a quick function to do what list.index() does except doesn't raise an exception if it's not found. beware -- this is probably very slow on large arrays. you can probably monkeypatch this on to arrays if you'd rather use it as a method. def ndindex(ndarray item): if len(ndarray.shape) == 1: try: return [ndarray.tolist().index(item)] except: pass else: for i subarray in enumerate(ndarray): try: return [i] + ndindex(subarray item) except: pass In [1]: ndindex(x 103) Out[1]: [4 0 3]  you can also convert a Numpy array to list in the air and get its index . for example l = [12345] #python list a = numpy.array(l) #numpy array i = a.tolist()[0].index(2) # i will return index of 2 from 0th row print i Will print 1.  Yes here is the answer given a Numpy array array and a value item to search for. itemindex = numpy.where(array==item) The result is a tuple with first all the row indices then all the column indices. For example if array is two dimensions and it contained your item at two locations then array[itemindex[0][0]][itemindex[1][0]] would be equal to your item and so would array[itemindex[0][1]][itemindex[1][1]] numpy.where If you are looking for the first row in which an item exists in the first column this works (although it will throw an index error if none exist) `rows columns = np.where(array==item); first_idx = sorted([r for r c in zip(rows columns) if c == 0])[0]` Also have a look at this question: http://stackoverflow.com/questions/7632963/numpy-find-first-index-of-value-fast/7654768#7654768,python arrays numpy
1273203,A,"Can't import Numpy in Python I'm trying to write some code that uses Numpy. However I can't import it: Python 2.6.2 (r262 May 15 2009 10:22:27) [GCC 3.4.2] on linux2 Type ""help"" ""copyright"" ""credits"" or ""license"" for more information. >>> import numpy Traceback (most recent call last): File ""<stdin>"" line 1 in <module> ImportError: No module named numpy I tried the suggestions in this question: >>> import sys >>> print sys.path ['' '/usr/intel/pkgs/python/2.6.2/lib/python26.zip' '/usr/intel/pkgs/python/2.6.2/lib/python2.6' '/usr/intel/pkgs/python/2.6.2/lib/python2.6/plat-linux2' '/usr/intel/pkgs/python/2.6.2/lib/python2.6/lib-tk' '/usr/intel/pkgs/python/2.6.2/lib/python2.6/lib-old' '/usr/intel/pkgs/python/2.6.2/lib/python2.6/lib-dynload' '/usr/intel/pkgs/python/2.6.2/lib/python2.6/site-packages'] and I searched for files named numpy in that path: $ find /usr/intel/pkgs/python/2.6.2/bin/python -iname numpy\* But nothing came up. So... Are there any other places in which Python modules are commonly installed? How can I install numpy locally in my account if it turns out that it isn't installed in the central areas? What disto are you using? Suse 9 64-bit on a corporate machine I was trying to import numpy in python 3.2.1 on windows 7. Followed suggestions in above answer for numpy-1.6.1.zip as below after unzipping it cd numpy-1.6 python setup.py install but got an error with a statement as below unable to find vcvarsall.bat For this error I found a related question here which suggested installing mingW. MingW was taking some time to install. In the meanwhile tried to install numpy 1.6 again using the direct windows installer available at this link the file name is ""numpy-1.6.1-win32-superpack-python3.2.exe"" Installation went smoothly and now I am able to import numpy without using mingW. Long story short try using windows installer for numpy if one is available.  Your sys.path is kind of unusual as each entry is prefixed with /usr/intel. I guess numpy is installed in the usual non-prefixed place e.g. it. /usr/share/pyshared/numpy on my Ubuntu system. Try find / -iname '*numpy*' That would be because I work at Intel...  Have you installed it? On debian/ubuntu: aptitude install python-numpy On windows: http://sourceforge.net/projects/numpy/files/NumPy/ On other systems: http://sourceforge.net/projects/numpy/files/NumPy/ $ tar xfz numpy-n.m.tar.gz $ cd numpy-n.m $ python setup.py install Good question. I had assumed that it's part of standard distributions. How can I install it in a private area? I don't have root permissions on the machine use the --prefix install option. `python setup.py install --prefix=/usr/intel` - see distutils docs for more details at http://docs.python.org/install/",python import numpy
1803054,A,"Speeding up computations with numpy matrices I have two matrices. Both are filled with zeros and ones. One is a big one (3000 x 2000 elements) and the other is smaller ( 20 x 20 ) elements. I am doing something like: newMatrix = (size of bigMatrix) filled with zeros l = (a constant) for y in xrange(0 len(bigMatrix[0])): for x in xrange(0 len(bigMatrix)): for b in xrange(0 len(smallMatrix[0])): for a in xrange(0 len(smallMatrix)): if (bigMatrix[x y] == smallMatrix[x + a - l y + b - l]): newMatrix[x y] = 1 Which is being painfully slow. Am I doing anything wrong? Is there a smart way to make this work faster? edit: Basically I am for each (xy) in the big matrix checking all the pixels of both big matrix and the small matrix around (xy) to see if they are 1. If they are 1 then I set that value on newMatrix. I am doing a sort of collision detection. You actually have this expression: smallMatrix[x + a - l y + b - l]) Whe you use teh ""big matrix indices"" xy to address an element on the small matrix - is this correct? It is correct . Your example code makes no sense but the description of your problem sounds like you are trying to do a 2d convolution of a small bitarray over the big bitarray. There's a convolve2d function in scipy.signal package that does exactly this. Just do convolve2d(bigMatrix smallMatrix) to get the result. Unfortunately the scipy implementation doesn't have a special case for boolean arrays so the full convolution is rather slow. Here's a function that takes advantage of the fact that the arrays contain only ones and zeroes: import numpy as np def sparse_convolve_of_bools(a b): if a.size < b.size: a b = b a offsets = zip(*np.nonzero(b)) n = len(offsets) dtype = np.byte if n < 128 else np.short if n < 32768 else np.int result = np.zeros(np.array(a.shape) + b.shape - (11) dtype=dtype) for o in offsets: result[o[0]:o[0] + a.shape[0] o[1]:o[1] + a.shape[1]] += a return result On my machine it runs in less than 9 seconds for a 3000x2000 by 20x20 convolution. The running time depends on the number of ones in the smaller array being 20ms per each nonzero element. I don't get what that code does but I've tried and it seems to crash saying result = numpy.zeros(numpy.array(a.shape) + b.shape - (11) dtype=dtype) ValueError: shape mismatch: objects cannot be broadcast to a single shape Try substituting it with result = np.zeros((a.shape[0] + b.shape[0] - 1 a.shape[1] + b.shape[1] - 1 dtype=dtype)  If your bits are really packed 8 per byte / 32 per int and you can reduce your smallMatrix to 20x16 then try the following here for a single row. (newMatrix[x y] = 1 when any bit of the 20x16 around xy is 1 ?? What are you really looking for ?) python -m timeit -s ' """""" slide 16-bit mask across 32-bit pairs bits[j] bits[j+1] """""" import numpy as np bits = np.zeros( 2000 // 16 np.uint16 ) # 2000 bits bits[::8] = 1 mask = 32+16 nhit = 16 * [0] def hit16( bits mask nhit ): """""" slide 16-bit mask across 32-bit pairs bits[j] bits[j+1] bits: long np.array( uint16 ) mask: 16 bits int out: nhit[j] += 1 where pair & mask != 0 """""" left = bits[0] for b in bits[1:]: pair = (left << 16) | b if pair: # np idiom for non-0 words ? m = mask for j in range(16): if pair & m: nhit[j] += 1 # hitposition = jb*16 + j m <<= 1 left = b # if any(nhit): print ""hit16:"" nhit ' \ ' hit16( bits mask nhit ) ' # 15 msec per loop bits[::4] = 1 # 11 msec per loop bits[::8] = 1 # mac g4 ppc  I can think of a couple of optimisations there - As you are using 4 nested python ""for"" statements you are about as slow as you can be. I can't figure out exactly what you are looking for - but for one thing if your big matrix ""1""s density is low you can certainly use python's ""any"" function on bigMtarix's slices to quickly check if there are any set elements there -- you could get a several-fold speed increase there: step = len(smallMatrix[0]) for y in xrange(0 len(bigMatrix[0] step)): for x in xrange(0 len(bigMatrix) step): if not any(bigMatrix[x: x+step y: y + step]): continue (...) At this point if still need to interact on each element you do another pair of indexes to walk each position inside the step - but I think you got the idea. Apart from using inner Numeric operations like this ""any"" usage you could certainly add some control flow code to break-off the (ba) loop when the first matching pixel is found. (Like inserting a ""break"" statement inside your last ""if"" and another if..break pair for the ""b"" loop. I really can't figure out exactly what your intent is - so I can't give you more specifc code. I don't know how I could forget what you mention in your last paragraph that break idea. But I need to break out of 2 loops. Is there any way to do it in python without having to break the inner loop and having to use a flag to check whenever I should break the outter one?",python matrix numpy
1794010,A,"How to use numpy with portaudio to extract bass mid treble As in this example http://stackoverflow.com/questions/259451/how-to-extract-frequency-information-from-an-input-audio-stream-using-portaudio I'm curious about portaudio and numpy... I'm not 100% sure about fft how can I pass numpy a chunk and get back three values from -1.0 to 1.0 for bass mid and treble ? I don't mind if this just for one channel as I can make sense of the audio part of this it's the maths that swim in front of me when I look at them :) What do you want the -1 to 1.0 to mean? What frequency ranges do you use to define bass mid and treble? The Fourier Transform mentioned in the selected answer to the SO question you point to gives you the ""spectrum"" -- a large collection of values giving the sound intensity in each of various ranges/slices of frequencies (expressed for example in Hertz). How to translate (say) a thousand intensities (one per each 10-Hertz slice of the spectrum say) into just three numbers as you desire is of course quite a heuristic issue -- for example you could just decide which ranges of frequencies correspond to ""bass"" and ""treble"" with everything in-between being ""mid"" and compute the average intensities in each. For what it's worth I believe a common convention for ""bass"" is up to 250Hz for ""treble"" 6KHz and above (in-between being the ""midrange"") cfr e.g. this page -- but it's rather an arbitrary convention so ""pick your poison""!-) Once you have the relative levels you'll want to normalize them with respect to each other and scale them appropriately to lie in your desired range (presumably on a logarithmic scale because that's how human hearing works;-). Choosing the frequencies is a good point for now it would be enough to use three equal sized ranges. I'd rather be able to get the 3 slices back from numpy instead of use python to convert the spectrum for speed. As this is for graphics this may be enough if not I can mess around with frequencies later. The main priority is performance and not doing too much processing in pure python.  Actually you would not use a Fourier transform to do this. Splitting any audio signal in bass mid and treble is usually done using filters. A filter is a signal processing device that attenuates certain frequency ranges. Filters can be build digitally or electrically. For example they are used in the audio crossover systems in loudspeakers. To get the low-frequency bass part you would use a low-pass filter. Low-pass filters filter out high frequencies. They are also called 'high-cut' filters. To get the mid-frequency mid part you would use a band-pass filter. Band-pass filters filter out both low and high frequencies. They are also called 'bell-filters'. To get the high-frequency treble part you would use a high-pass filter. High-pass filters filter out any low frequencies. They are also called 'low-cut' filters. Actually you could also only use the high-pass and low-pass filter. If you subtract both filtered signals from the original signal the result would be a band-pass filtered signal. This saves you one filter. Each filter will have a threshold frequency. The threshold frequency is a special frequency from which the filter should start filtering. Depending on the filter order the signal will be attenuated by 6 dB/oct (1st order) 12 dB/oct (2nd order) 18 dB/oct (3rd order) etc. For your application a 2nd order design is probably fine. Note that filters in general mess with your signal in some ways and the higher the order the more audible this can get. By the way this is pure physics and true for all signal processing including Fourier transforms. Using these three filters is (can be) equivalent to doing a Fourier transform with only three spectral points.",python audio numpy fft portaudio
1058434,A,"Flags in Python I'm working with a large matrix (250x250x30 = 1875000 cells) and I'd like a way to set an arbitrary number of flags for each cell in this matrix in some manner that's easy to use and reasonably space efficient. My original plan was a 250x250x30 list array where each element was something like: [""FLAG1""""FLAG8""""FLAG12""]. I then changed it to storing just integers instead: [1812]. These integers are mapped internally by getter/setter functions to the original flag strings. This only uses 250mb with 8 flags per point which is fine in terms of memory. My question is: am I missing another obvious way to structure this sort of data? Thanks all for your suggestions. I ended up rolling a few suggestions into one sadly I can only pick one answer and have to live with upvoting the others: EDIT: erm the initial code I had here (using sets as the base element of a 3d numpy array) used A LOT of memory. This new version uses around 500mb when filled with randint(02**1000). import numpy FLAG1=2**0 FLAG2=2**1 FLAG3=2**2 FLAG4=2**3 (xyz) = (25025030) array = numpy.zeros((xyz) dtype=object) def setFlag(locationflag): array[location] |= flag def unsetFlag(locationflag): array[location] &= ~flag How many flags do you need to support? I'm not sure I wouldn't be confident saying anything more accurate than ""more than 5 and less than 500"". BitSet is what you want since it allows you to store many flags at once using only an fixed size integer (Int type)  You can define some constants with different power of two values as: FLAG1 = 0x01 FLAG8 = 0x02 FLAG12 = 0x04 ... And use them with boolean logic to store the flags in only one integer p.e.: flags = FLAG1 | FLAG8 To check if a flag is enabled you can use the & operator: flag1_enabled = flags & FLAG1 If the flag is enabled this expression will return a non-zero value that will be evaluated as True in any boolean operation. If the flag is disabled the expression will return 0 that is evaluated as False in boolean operations. Thanks for this suggestion. Although I'm going to use sets instead of bitfields (I don't need the extra space savings here) I definitely will need something like this in future. Actually I am! see sample code in the question and thanks for the idea. This saves me about 20% of the required memory even if I use up to 100 objects very interesting! One question how do I remove a flag? Maybe not the best approach if you really might have up to 500 flags although Python does support arbitrarily long integers. To remove a flag you have to add its negation (flags &= ~FLAG1) @Alex Jurkiewicz: new_flags = old_flags & ~flags_to_remove.  Your solution is fine if every single cell is going to have a flag. However if you are working with a sparse dataset where only a small subsection of your cells will have flags what you really want is a dictionary. You would want to set up the dictonary so the key is a tuple for the location of the cell and the value is a list of flags like you have in your solution. allFlags = {(111):[123] (25025030):[456]} Here we have the 111 cell have the flags 12 and 3 and the cell 25025030 have the flags 45 and 6 edit- fixed key tuples thanks Andre and dictionary syntax. Actually shouldn't that be {(111):[123] ...). dict(key=val) syntax only works if your keys are python identifiers. He's using a three dimensional matrix so you would need so say something like dict((111)=[123]...  Consider using Flyweight pattern to share cell properties: http://en.wikipedia.org/wiki/Flyweight_pattern  I would generally use a numpy array (presumably of short ints 2 bytes each since you may need more than 256 distinct values) -- that would take less than 4MB for the <2 million cells. If for some reason I couldn't afford the numpy dependency (e.g on App Engine which doesn't support numpy) I'd use the standard library array module - it only supports 1-dimensional arrays but it's just as space-efficient as numpy for large homogeneous arrays and the getter/setter routines you mention can perfectly well ""linearize"" a 3-items tuple that's your natural index into the single integer index into the 1-D array. In general consider numpy (or array) any time you have large homogeneous dense vectors or matrices of numbers -- Python built-in lists are highly wasteful of space in this use case (due to their generality which you're not using and don't need here!-) and saving memory indirectly translates to saving time too (better caching fewer levels of indirection etc etc). Thanks for this idea. I've ended up using a 3d numpy array of sets (on the assumption that a set is better suited for this than a list). erm sets sucked. Sample code I pasted in the question uses the magic of python long's. Thanks for the numpy array tip. Thanks for the numpy suggestion. One question about the data type. Since a ushort can hold values up to 2**16 can't I only have 16 flags if I use ushort? Sorry... I was still thinking of bitflags as in a previous answer. I see what you mean now a 4d array where a[0][0][0] would return a 1d array correct? Hmm I had read your question differently with just a single flag (or none) per cell (with up to 500 different values) as opposed to up to 500 flags for a single cell. For your actual question a 4-D array (with 512 bits per cell 64 bytes) is indeed needed so that's 128 MB (and the translation to/from bit by shift and mask as other answers have mentioned) -- *IF* you do need to be as ""dense"" as that (if the _typical_ cell has a few flags then lists are again attractive esp. in numpy where you only need lists at the lowest level).  Taking Robbie's suggestion one step further... flags = set() x y flag = 34 201 3 flags.add((x y flag)) # set flag 3 at position (34 201) if (3 2 1) in flags: # check if flag 1 is at position (3 2) # do something else: # do something else You can also create a helper class. class Flags(object): def __init__(self): self.data = set() def add(self x y flag): self.data.add((x y flag)) def remove(self x y flag): self.data.remove((x y flag)) def contains(self x y flag): return (x y flag) in self.data You could also implement Python's special methods like __contains__ to make it easier to work with.",python matrix numpy flags
524930,A,"NumPy PIL adding an image I'm trying to add two images together using NumPy and PIL. The way I would do this in MATLAB would be something like: >> M1 = imread('_1.jpg'); >> M2 = imread('_2.jpg'); >> resM = M1 + M2; >> imwrite(resM 'res.jpg'); I get something like this: Using a compositing program and adding the images the MATLAB result seems to be right. In Python I'm trying to do the same thing like this: from PIL import Image from numpy import * im1 = Image.open('/Users/rem7/Desktop/_1.jpg') im2 = Image.open('/Users/rem7/Desktop/_2.jpg') im1arr = asarray(im1) im2arr = asarray(im2) addition = im1arr + im2arr resultImage = Image.fromarray(addition) resultImage.save('/Users/rem7/Desktop/a.jpg') and I get something like this: Why am I getting all those funky colors? I also tried using ImageMath.eval(""a+b"" a=im1 b=im2) but I get an error about RGB unsupported. I also saw that there is an Image.blend() but that requires an alpha. What's the best way to achieve what I'm looking for? Source Images (images have been removed): Humm OK well I added the source images using the add image icon and they show up when I'm editing the post but for some reason the images don't show up in the post. (images have been removed) 2013 05 09 Using PIL's blend() with an alpha value of 0.5 would be equivalent to (im1arr + im2arr)/2. Blend does not require that the images have alpha layers. Try this: from PIL import Image im1 = Image.open('/Users/rem7/Desktop/_1.jpg') im2 = Image.open('/Users/rem7/Desktop/_2.jpg') Image.blend(im1im20.5).save('/Users/rem7/Desktop/a.jpg') this is especially nice for getting the job done without dragging in numpy.  To clamp numpy array values: >>> c = a + b >>> c[c > 256] = 256 It assumes that type of elements is larger than uint8.  Your sample images are not showing up form me so I am going to do a bit of guessing. I can't remember exactly how the numpy to pil conversion works but there are two likely cases. I am 95% sure it is 1 but am giving 2 just in case I am wrong. 1) 1 im1Arr is a MxN array of integers (ARGB) and when you add im1arr and im2arr together you are overflowing from one channel into the next if the components b1+b2>255. I am guessing matlab represents their images as MxNx3 arrays so each color channel is separate. You can solve this by splitting the PIL image channels and then making numpy arrays 2) 1 im1Arr is a MxNx3 array of bytes and when you add im1arr and im2arr together you are wrapping the component around. You are also going to have to rescale the range back to between 0-255 before displaying. Your choices are divide by 2 scale by 255/array.max() or do a clip. I don't know what matlab does Are the images still not showing? I edited the question and after that it works here. works for me now. It definitely looks like a wrapping/saturation issue. It would also be nice if you posted your source images. I think the pil conversion does make it a MxNx3 since im1arr.shape prints this: (2477 3700 3). Option two seems to be correct. It seems that I had to divide both images first before I can add them even if I do a clip(min=0 max=255) on the result the overflow already happened so the funky colors are still there. The alternative to dividing first is to cast from a byte array over to an int (or short) array and then do your math.  It seems the code you posted just sums up the values and values bigger than 256 are overflowing. You want something like ""(a + b) / 2"" or ""max(a + b 256)"". The latter seems to be the way that your Matlab example does it. Yes matlab clamps values when doing arithmetic on uint8 values (e.g. it implicitly does the equivalent to max(double(a)+double(b)256) )"" When I try to do max(im1arr+im2arr256) I get the error: ""ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"" I do (im1arr+im2arr)/2 I get funky colors only dimmer max value 127 so I did: addition=(im1arr/2)+(im2arr/2) and that seems to work.  As everyone suggested already the weird colors you're observing are overflow. And as you point out in the comment of schnaader's answer you still get overflow if you add your images like this: addition=(im1arr+im2arr)/2 The reason for this overflow is that your NumPy arrays (im1arr im2arr) are of the uint8 type (i.e. 8-bit). This means each element of the array can only hold values up to 255 so when your sum exceeds 255 it loops back around 0: >>>array([25510100]dtype='uint8') + array([110160]dtype='uint8') array([ 0 20 4] dtype=uint8) To avoid overflow your arrays should be able to contain values beyond 255. You need to convert them to floats for instance perform the blending operation and convert the result back to uint8: im1arrF = im1arr.astype('float') im2arrF = im2arr.astype('float') additionF = (im1arrF+im2arrF)/2 addition = additionF.astype('uint8') You should not do this: addition = im1arr/2 + im2arr/2 as you lose information by squashing the dynamic of the image (you effectively make the images 7-bit) before you perform the blending information. MATLAB note: the reason you don't see this problem in MATLAB is probably because MATLAB takes care of the overflow implicitly in one of its functions. Thanks your explanation was very clear. Why 'float'? A 'uint16' would be sufficient. There was no rational reason for choosing float uint16 would have been enough indeed.",python image-processing numpy python-imaging-library
1970680,A,Integer overflow in numpy arrays import numpy as np a = np.arange(1000000).reshape(10001000) print(a**2) With this code I get this answer. Why do I get negative values? [[ 0 1 4 ... 994009 996004 998001] [ 1000000 1002001 1004004 ... 3988009 3992004 3996001] [ 4000000 4004001 4008004 ... 8982009 8988004 8994001] ... [1871554624 1873548625 1875542628 ... -434400663 -432404668 -430408671] [-428412672 -426416671 -424420668 ... 1562593337 1564591332 1566589329] [1568587328 1570585329 1572583332 ... -733379959 -731379964 -729379967]] python integers don't have this problem since they automatically upgrade to python long integers when they overflow. so if you do manage to overflow the int64's one solution is to use python int's in the numpy array: import numpy a=numpy.arange(1000dtype=object) a**20 What are the performance implications of using an object?  numpy integer types are fixed width and you are seeing the results of integer overflow.  A solution to this problem is as follows (taken from here): ...change in class StringConverter._mapper (numpy/lib/_iotools.py) from: {{{ _mapper = [(nx.bool_ str2bool False) (nx.integer int -1) (nx.floating float nx.nan) (complex _bytes_to_complex nx.nan + 0j) (nx.string_ bytes asbytes('???'))] }}} to {{{ _mapper = [(nx.bool_ str2bool False) (nx.int64 int -1) (nx.floating float nx.nan) (complex _bytes_to_complex nx.nan + 0j) (nx.string_ bytes asbytes('???'))] }}} This solved a similar problem that I had with numpy.genfromtxt for me Note that the author describes this as a 'temporary' and 'not optimal' solution. However I have had no side effects using v2.7 (yet?!).  np.arange returns an array of dtype 'int32' : In [1]: np.arange(1000000).dtype Out[1]: dtype('int32') Each element of the array is a 32-bit integer. Squaring leads to a result which does not fit in 32-bits. The result is cropped to 32-bits and still interpreted as a 32-bit integer however which is why you see negative numbers. Edit: In this case you can avoid the integer overflow by constructing an array of dtype 'int64' before squaring: a=np.arange(1000000dtype='int64').reshape(10001000) Note that the problem you've discovered is an inherent danger when working with numpy. You have to choose your dtypes with care and know before-hand that your code will not lead to arithmetic overflows. For the sake of speed numpy can not and will not warn you when this occurs. See http://mail.scipy.org/pipermail/numpy-discussion/2009-April/041691.html for a discussion of this on the numpy mailing list.,python numpy
1598251,A,Adding row to numpy recarray Is there an easy way to add a record/row to a numpy recarray without creating a new recarray? Let's say I have a recarray that takes 1Gb in memory I want to be able to add a row to it without having python take up 2Gb of memory temporarily. You can call yourrecarray.resize with a shape which has one more row then assign to that new row. Of course. numpy might still have to allocate completely new memory if it just doesn't have room to grow the array in-place but at least you stand a chance!-) Since an example was requested here comes modified off the canonical example list...: >>> import numpy >>> mydescriptor = {'names': ('gender''age''weight') 'formats': ('S1' 'f4' 'f4')} >>> a = numpy.array([('M'64.075.0)('F'25.060.0)] dtype=mydescriptor) >>> print a [('M' 64.0 75.0) ('F' 25.0 60.0)] >>> a.shape (2) >>> a.resize(3) >>> a.shape (3) >>> print a [('M' 64.0 75.0) ('F' 25.0 60.0) ('' 0.0 0.0)] >>> a[2] = ('X' 17.0 61.5) >>> print a [('M' 64.0 75.0) ('F' 25.0 60.0) ('X' 17.0 61.5)] Could you show some demonstration code? My attempt at calling arr.resize() ended with ValueError: cannot resize this array: it does not own its data @unutbu sure edited answer to supply simple example. You may be meeting issues discussed in this thread: http://aspn.activestate.com/ASPN/Mail/Message/numpy-discussion/3042521 -- then you can fix them as Travis Oliphant mentions there by adding the refcheck=0 argument to the resize call (unless you HAVE shared the data in which case there can be no resizing in-place any more (note that what Travis mentions as a feature of the SVN head of numpy has been part of regularly released numpy for a long time by now -- that thread is 3+ years old;-). Thank you! a.resize(3refcheck=0) did the trick for me. I wish the numpy's developers had thought of a better way to add a row to a dataset. It is a very common operation and I don't understand why it should be so inefficient.,python numpy
1517129,A,"Python: how do I install SciPy on 64 bit Windows? How do I install SciPy on my system? Update 1: for the NumPy part (that SciPy depends on) there is actually an installer for 64 bit Windows: numpy-1.3.0.win-amd64-py2.6.msi (is direct download URL 2310144 bytes). Running the SciPy superpack installer results in this message in a dialog box: ""Cannot install. Python version 2.6 required which was not found in the registry."" I already have Python 2.6.2 installed (and a working Django installation in it) but I don't know about any Registry story. The registry entries seems to already exist: REGEDIT4 [HKEY_LOCAL_MACHINE\SOFTWARE\Python] [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore] [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6] [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\Help] [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\Help\Main Python Documentation] @=""D:\\Python262\\Doc\\python262.chm"" [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\InstallPath] @=""D:\\Python262\\"" [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\InstallPath\InstallGroup] @=""Python 2.6"" [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\Modules] [HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\PythonPath] @=""D:\\Python262\\Lib;D:\\Python262\\DLLs;D:\\Python262\\Lib\\lib-tk"" What I have done so far: Step 1 Downloaded the NumPy superpack installer numpy-1.3.0rc2-win32-superpack-python2.6.exe (direct download URL 4782592 bytes). Running this installer resulted in the same message ""Cannot install. Python version 2.6 required which was not found in the registry."". Update: there is actually an installer for NumPy that works - see beginning of the question. Step 2 Tried to install NumPy in another way. Downloaded the zip package numpy-1.3.0rc2.zip (direct download URL 2404011 bytes) extracted the zip file in a normal way to a temporary directory D:\temp7\numpy-1.3.0rc2 (where setup.py and README.txt is). I then opened a command line window and: d: cd D:\temp7\numpy-1.3.0rc2 setup.py install This ran for a long time and also included use of cl.exe (part of Visual Studio). Here is a nearly 5000 lines long transcript (230 KB). This seemed to work. I can now do this in Python: import numpy as np np.random.random(10) with this result: array([ 0.35667511 0.56099423 0.38423629 0.09733172 0.81560421 0.18813222 0.10566666 0.84968066 0.79472597 0.30997724]) Step 3 Downloaded the SciPy superpack installer scipy-0.7.1rc3- win32-superpack-python2.6.exe (direct download URL 45597175 bytes). Running this installer resulted in the message listed in the beginning Step 4 Tried to install SciPy in another way. Downloaded the zip package scipy-0.7.1rc3.zip (direct download URL 5506562 bytes) extracted the zip file in a normal way to a temporary directory D:\temp7\scipy-0.7.1 (where setup.py and README.txt is). I then opened a command line window and: d: cd D:\temp7\scipy-0.7.1 setup.py install This did not achieve much - here is a transcript (about 95 lines). And it fails: >>> import scipy as sp2 Traceback (most recent call last): File ""<stdin>"" line 1 in <module> ImportError: No module named scipy Platform: Python 2.6.2 installed in directory D:\Python262 Windows XP 64 bit SP2 8 GB RAM Visual Studio 2008 Professional Edition installed. The startup screen of the installed Python is: Python 2.6.2 (r262:71605 Apr 14 2009 22:46:50) [MSC v.1500 64 bit (AMD64)] on win32 Type ""help"" ""copyright"" ""credits"" or ""license"" for more information. >>> Value of PATH result from SET in a command line window: Path=D:\Perl64\site\bin;D:\Perl64\bin;C:\Program Files (x86)\PC Connectivity Solution\;D:\Perl\site\bin;D:\Perl\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\Program Files (x86)\ATI Technologies\ATI.ACE\Core-Static;d:\Program Files (x86)\WinSCP\;D:\MassLynx\;D:\Program Files (x86)\Analyst\bin;d:\Python262;d:\Python262\Scripts;D:\Program Files (x86)\TortoiseSVN\bin;D:\Program Files\TortoiseSVN\bin;C:\WINDOWS\system32\WindowsPowerShell\v1.0;D:\Program Files (x86)\IDM Computer Solutions\UltraEdit\ There are lots of other packages here: http://www.lfd.uci.edu/~gohlke/pythonlibs/ but I don't see one named ""SciPy"". I see ""ScientificPython"" and SciPy subpackages but not SciPy itself. Does this mean it's still not available 3 months later or am I missing something? @endolith https://pypi.python.org/pypi/scipy/0.7.0 @Inversus: http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy @endolith niiice. Thx For completeness: Enthought has a Python distribution which includes SciPy; however it's not free. Caveat: I've never used it.  I was getting this same error on a 32-bit machine. I fixed it by registering my Python installation using the script at: http://effbot.org/zone/python-register.htm It's possible that the script would also make the 64-bit superpack installers work.  Unofficial 64-bit installers for NumPy and SciPy are available at http://www.lfd.uci.edu/~gohlke/pythonlibs/ This seems to be working for me!  I haven't tried it but you may want to download this version of Portable Python. It comes with Scipy-0.7.0b1 running on Python 2.5.4. Thanks! It works great and is by far the easiest way to get it working (although it is a 32 bit version and thus not a 64 bit version of SciPy). And it doesn't mess with the existing 64 bit version installation of Python. Although it solved his problem it didn't exactly answer the question... For Python 3 a roughly equivalent version is available [here](http://portablepython.com/wiki/PortablePython3.2.5.1/)  Install Python distribution http://www.python.org/download/. Download and Install Anaconda Python Distribution. Make Anaconda Python distribution link to py3.3 if you want Numpy Scipy or Matplotlib to work in py3.3 or just use it like that to have only py2.7 and older functionality. The link below provides more detail about Anaconda: http://infodatatech.blogspot.com/2013/04/anaconda-python-distribution-python-33.html.  Try to install Python 2.6.3 over your 2.6.2 (this should also add correct Registry entry) or to register your existing installation using this script. Installer should work after that. Building SciPy requires a Fortran compiler and libraries - BLAS and LAPACK. When I run Fredrik Lundh's script I get: ""*** Unable to register! *** You probably have another Python installation!"". I have updated the question with registry entries on my system. (Some of the variables are: pythonpath: 'd:\\Python262;d:\\Python262\\Lib\\;d:\\Python262\\DLLs\\' regpath: 'SOFTWARE\\Python\\Pythoncore\\2.6\\'). If `HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\2.6\` exists then try to remove it and run it again. Also - do you run it with enough privileges? I tried it and got: ""--- Python 2.6 is now registered!"". However with the same result when running scipy-0.7.1rc3-win32-superpack-python2.6.exe. Is it expected to work on a 64 bit version of Python? (And yes I have far too many privileges :-) (administrator). I know I shouldn't for security reasons.)  Another alternative: http://www.pythonxy.com/ Free and includes lots of stuff meant to work together smoothly. This person says Did you try linux.pythonxy ? ( http://linux.pythonxy.com ). It's 64 bit ready ... Though I'm not quite sure what that means.  WinPython is an open-source distribution that has 64-bit numpy and scipy.  Short answer: windows 64 support is still work in progress at this time. The superpack will certainly not work on a 64 bits python (but it should work fine on a 32 bits python even on windows 64). The main issue with windows 64 is that building with mingw-w64 is not stable at this point: it may be our's (numpy devs) fault python's fault or mingw-w64. Most likely a combination of all those :). So you have to use proprietary compilers: anything other than MS compiler crashes numpy randomly; for the fortran compiler ifort is the one to use. As of today both numpy and scipy source code can be compiled with VS 2008 and ifort (all tests passing) but building it is still quite a pain and not well supported by numpy build infrastructure. Thanks for the good explanation of the issues. Any idea when it will be ready? Enthought provides 64 bits build of EPD which is free to use for academic usage (they supported the win64 port) and uses the MKL for speed. There is also another unofficial set of binaries linked below Is 64 bit available for academic use? It would be great - from the website it looks like only 32bit is freely available.  As the transcript for SciPy told you SciPy isn't really supposed to work on Win64: Warning: Windows 64 bits support is experimental and only available for testing. You are advised not to use it for production. So I would suggest to install the 32-bit version of Python and stop attempting to build SciPy yourself. If you still want to try anyway you first need to compile BLAS and LAPACK as PiotrLegnica says. See the transcript for the places where it was looking for compiled versions of these libraries. NumPy (at least v1.5.1) doesn't give this warning anymore.",python windows 64bit numpy scipy
1300648,A,Multidimensional list(array) reassignment problem Good day coders and codereses I am writing a piece of code that goes through a pile of statistical data and returns what I ask from it. To complete its task the method reads from one multidimensional array and writes into another one. The piece of code giving me problems is: writer.variables[variable][: : : :] = reader.variables[variable][offset: 0 0:5 3] The size of both slices is 27:1:6:1 but it throws up an exception: ValueError: total size of new array must be unchanged I am flabbergasted. Thank you. Sheesh. I'll try not to ask any more question until the sleeping pattern is sorted out. The size of a slice with 0:5 is not 6 as you say: it's 5. The upper limit is excluded in slicing (as it most always is in Python). Don't know whether that's your actual problem or just a typo in your question... Oh wow. I've spent an hour trying to debug this just because I have perceived the bug being of enormous complexity. Thank you sir. @Rince you're welcome!,python list numpy scipy netcdf
1827489,A,"Numpy meshgrid in 3D Numpy's meshgrid is very useful for converting two vectors to a coordinate grid. What is the easiest way to extend this to three dimensions? So given three vectors x y and z construct 3x3D arrays (instead of 2x2D arrays) which can be used as coordinates. Here is the source code of meshgrid: def meshgrid(xy): """""" Return coordinate matrices from two coordinate vectors. Parameters ---------- x y : ndarray Two 1-D arrays representing the x and y coordinates of a grid. Returns ------- X Y : ndarray For vectors `x` `y` with lengths ``Nx=len(x)`` and ``Ny=len(y)`` return `X` `Y` where `X` and `Y` are ``(Ny Nx)`` shaped arrays with the elements of `x` and y repeated to fill the matrix along the first dimension for `x` the second for `y`. See Also -------- index_tricks.mgrid : Construct a multi-dimensional ""meshgrid"" using indexing notation. index_tricks.ogrid : Construct an open multi-dimensional ""meshgrid"" using indexing notation. Examples -------- >>> X Y = np.meshgrid([123] [4567]) >>> X array([[1 2 3] [1 2 3] [1 2 3] [1 2 3]]) >>> Y array([[4 4 4] [5 5 5] [6 6 6] [7 7 7]]) `meshgrid` is very useful to evaluate functions on a grid. >>> x = np.arange(-5 5 0.1) >>> y = np.arange(-5 5 0.1) >>> xx yy = np.meshgrid(x y) >>> z = np.sin(xx**2+yy**2)/(xx**2+yy**2) """""" x = asarray(x) y = asarray(y) numRows numCols = len(y) len(x) # yes reversed x = x.reshape(1numCols) X = x.repeat(numRows axis=0) y = y.reshape(numRows1) Y = y.repeat(numCols axis=1) return X Y It is fairly simple to understand. I extended the pattern to an arbitrary number of dimensions but this code is by no means optimized (and not thoroughly error-checked either) but you get what you pay for. Hope it helps: def meshgrid2(*arrs): arrs = tuple(reversed(arrs)) #edit lens = map(len arrs) dim = len(arrs) sz = 1 for s in lens: sz*=s ans = [] for i arr in enumerate(arrs): slc = [1]*dim slc[i] = lens[i] arr2 = asarray(arr).reshape(slc) for j sz in enumerate(lens): if j!=i: arr2 = arr2.repeat(sz axis=j) ans.append(arr2) return tuple(ans) In the case of a 3d mesh-grid using a sample like the one provided in numpy doc for meshgrib this would return ZYX instead of XYZ. Replacing the return statement by `return tuple(ans[::-1])` can fix this. @Paul if the length of x or y array are long then x.repeat() command crashes and sends Memory Error. Is there any way to avoid this error? @Dalek How long are the arrays? Could it be that you run out of memory? For example if there are 3 arrays 4096 entries each and each entry holds a double (i.e. 8bytes) then for entries alone we need (8 * 4 * 2**10)**3 bytes = 2**45 bytes = 32 * 2**40 bytes = 32 TB of memory which is obviously enourmous. I hope I have not made a mistake here.  Instead of writing a new function numpy.ix_ should do what you want. it would be nice if you could tell how?...  Can you show us how you are using np.meshgrid? There is a very good chance that you really don't need meshgrid because numpy broadcasting can do the same thing without generating a repetitive array. For example import numpy as np x=np.arange(2) y=np.arange(3) [XY] = np.meshgrid(xy) S=X+Y print(S.shape) # (3 2) # Note that meshgrid associates y with the 0-axis and x with the 1-axis. print(S) # [[0 1] # [1 2] # [2 3]] s=np.empty((32)) print(s.shape) # (3 2) # x.shape is (2). # y.shape is (3). # x's shape is broadcasted to (32) # y varies along the 0-axis so to get its shape broadcasted we first upgrade it to # have shape (31) using np.newaxis. Arrays of shape (31) can be broadcasted to # arrays of shape (32). s=x+y[:np.newaxis] print(s) # [[0 1] # [1 2] # [2 3]] The point is that S=X+Y can and should be replaced by s=x+y[:np.newaxis] because the latter does not require (possibly large) repetitive arrays to be formed. It also generalizes to higher dimensions (more axes) easily. You just add np.newaxis where needed to effect broadcasting as necessary. See http://www.scipy.org/EricsBroadcastingDoc for more on numpy broadcasting.  i think what you want is X Y Z = numpy.mgrid[-10:10:100j -10:10:100j -10:10:100j] for example. Thanks but this is not quite what I need - meshgrid actually uses the values of the vectors to generate the 2D array and the values can be irregularly spaced.  In case someone comes past this numpy (as of 1.8 I think) support higher that 2D generation of position grids with meshgrid. One important addition which really helped me is the ability to chose the indexing order (either xy or ij for Cartesian or matrix indexing respectively) which I verified with the following example: import numpy as np x_ = np.linspace(0. 1. 10) y_ = np.linspace(1. 2. 20) z_ = np.linspace(3. 4. 30) x y z = np.meshgrid(x_ y_ z_ indexing='ij') assert np.all(x[:00] == x_) assert np.all(y[0:0] == y_) assert np.all(z[00:] == z_) Thank you everyone who contributes to numpy!  Here is a multidimensional version of meshgrid that I wrote: def ndmesh(*args): args = map(np.asarrayargs) return np.broadcast_arrays(*[x[(slice(None))+(None)*i] for i x in enumerate(args)]) Note that the returned arrays are views of the original array data so changing the original arrays will affect the coordinate arrays. did the trick for me thanks a lot!",python numpy
1706665,A,Loading and saving numpy matrix I'm having troubles loading a numpy matrix. I successfully saved it to disk through: self.q.dump(fileName) and now I want to be able to load it. From what I understand the load command should do the trick: self.q.load(fileName) but it seems not. Anyone knows what might be wrong? Maybe the function is not called load? help(numpy.ndarray)  | dump(...) | a.dump(file) | | Dump a pickle of the array to the specified file. | The array can be read back with pickle.load or numpy.load. | | Parameters | ---------- | file : str | A string naming the dump file. numpy.load should work fine. Yes but it doesn't. AttributeError: 'numpy.ndarray' object has no attribute 'load' @d.e.: I'ts not a method on an array it's a function. Be sure to call it like `numpy.load(filename)` not `q.load(..)` (where q is an array).,python numpy
944863,A,"Numpy: Should I use newaxis or None? In numpy one can use the 'newaxis' object in the slicing syntax to create an axis of length one e.g.: import numpy as np print np.zeros((35))[:np.newaxis:].shape # shape will be (315) The documentation states that one can also use None instead of newaxis the effect is exactly the same. Is there any reason to choose one over the other? Is there any general preference or style guide? My impression is that newaxis is more popular probably because it is more explicit. So is there any reason why None is allowed? None is allowed because numpy.newaxis is merely an alias for None. In [1]: import numpy In [2]: numpy.newaxis is None Out[2]: True The authors probably chose it because they needed a convenient constant and None was available. As for why you should prefer newaxis over None: mainly it's because it's more explicit and partly because someday the numpy authors might change it to something other than None. (They're not planning to and probably won't but there's no good reason to prefer None.) Thanks I hadn't noticed that np.newaxis is actually None. I will go with newaxis then. They actually say you can use None though so they can't change it now: ""The newaxis object can be used in the basic slicing syntax discussed above. None can also be used instead of newaxis.""",python numpy
365395,A,How to truncate matrix using NumPy (Python) just a quick question if I have a matrix has n rows and m columns how can I cut off the 4 sides of the matrix and return a new matrix? (the new matrix would have n-2 rows m-2 columns). Thanks in advance A more general answer is: a[[slice(1 -1) for _ in a.shape]]  a[1:-1 1:-1] That is fantastically compact! I had only previously seen this done using a tuple of slice(1-1) objects. numpy has some really nice ways of handling indexing and slicing. I miss the more advanced slicing features when I use Matlab (esp. the broadcasting features).,python numpy
1565731,A,"Strange numpy.float96 behaviour What am I missing: In [66]: import numpy as np In [67]: np.float(7.0 / 8) Out[67]: 0.875 #OK In [68]: np.float32(7.0 / 8) Out[68]: 0.875 #OK In [69]: np.float96(7.0 / 8) Out[69]: -2.6815615859885194e+154 #WTF In [70]: sys.version Out[70]: '2.5.4 (r254:67916 Dec 23 2008 15:10:54) [MSC v.1310 32 bit (Intel)]' Edit. On cygwin the above code works OK: $ python Python 2.5.2 (r252:60911 Dec 2 2008 09:26:14) [GCC 3.4.4 (cygming special gdc 0.12 using dmd 0.125)] on cygwin Type ""help"" ""copyright"" ""credits"" or ""license"" for more information. >>> import numpy as np >>> np.float(7.0 / 8) 0.875 >>> np.float96(7.0 / 8) 0.875 For the completeness I checked this code in plain python (not Ipython): C:\temp>python Python 2.5.4 (r254:67916 Dec 23 2008 15:10:54) [MSC v.1310 32 bit (Intel)] on win32 Type ""help"" ""copyright"" ""credits"" or ""license"" for more information. >>> import numpy as np >>> np.float(7.0 / 8) 0.875 >>> np.float96(7.0 / 8) -2.6815615859885194e+154 >>> EDIT I saw three bug reports on Numpy's trac site (976 902 and 884) but this one doesn't seem to be related to string representation. Therefore I have opened a new bug (1263). Will update here the progress I would be interested in seeing the outcome of the bug report :) I can't reproduce this on linux (ubuntu 64 bit) because it doesn't have a `float96` only a `float128` so the problem may be Windows specific. Cannot reproduce on mac snow leopard same reason. There were a few fixes for long double formatting issues on Windows in 1.3.0; at least http://projects.scipy.org/numpy/changeset/6219 http://projects.scipy.org/numpy/changeset/6218 http://projects.scipy.org/numpy/changeset/6217  This works fine for me: In [1]: import numpy as np In [2]: np.float(7.0/8) Out[2]: 0.875 In [3]: np.float96(7.0/8) Out[3]: 0.875 What Numpy are you using? I'm using Python 2.6.2 and Numpy 1.3.0 and I'm on 64 bit Vista. I tried this same thing on another computer that is running 32 bit XP with Python 2.5.2 and Numpy 1.2.1 and to my surprise I get: In [2]: np.float96(7.0/8) Out[2]: -2.6815615859885194e+154 After some investigation installing Python 2.6.3 and Numpy 1.3.0 on 32 bit XP I've found: In [2]: np.float96(7.0/8) Out[2]: 0.875 So it must be a bug in either the old version of Numpy or a bug in the old version of Python...  The problem is caused by incompatibilities between mingw compiler (the one used for the official numpy binary) and the MS runtime (the one printf is coming from). MS compiler consider long double and double to be equivalent types and so does the MS C runtime (printf included). Mingw for some reason define long double as big enough to hold 80 bits extended precision numbers but of course the MS printf does not know about it and cannot print long double correctly. We circumvented around some problems by using our own formatting functions but I think the real fix is to force long double to be a synonym to double when built with mingw. This will be done for numpy 1.5.0 I think.",python numpy
214549,A,"How to create a numpy record array from C On the Python side I can create new numpy record arrays as follows: numpy.zeros((3) dtype=[('a' 'i4') ('b' 'U5')]) How do I do the same from a C program? I suppose I have to call PyArray_SimpleNewFromDescr(nd dims descr) but how do I construct a PyArray_Descr that is appropriate for passing as the third argument to PyArray_SimpleNewFromDescr? Use PyArray_DescrConverter. Here's an example: #include <Python.h> #include <stdio.h> #include <numpy/arrayobject.h> int main(int argc char *argv[]) { int dims[] = { 2 3 }; PyObject *op *array; PyArray_Descr *descr; Py_Initialize(); import_array(); op = Py_BuildValue(""[(s s) (s s)]"" ""a"" ""i4"" ""b"" ""U5""); PyArray_DescrConverter(op &descr); Py_DECREF(op); array = PyArray_SimpleNewFromDescr(2 dims descr); PyObject_Print(array stdout 0); printf(""\n""); Py_DECREF(array); return 0; } Thanks to Adam Rosenfield for pointing to Section 13.3.10 of the Guide to NumPy.  See the Guide to NumPy section 13.3.10. There's lots of different ways to make a descriptor although it's not nearly as easy as writing [('a' 'i4') ('b' 'U5')]. Thanks the Guide mentioned `PyArray_DescrConverter` which works. I've posted an example as a separate answer as it doesn't fit in a comment. @JoelVroom: I don't know what happened to the original link but I was able to find another link to the same document easily enough. That link doesn't work for me. Anyone have an updated link?",python c numpy
384759,A,"PIL and numpy Alright I'm toying around with converting a PIL image object back and forth to a numpy array so I can do some faster pixel by pixel transformations than PIL's PixelAccess object would allow. I've figured out how to place the pixel information in a useful 3D numpy array by way of: pic = Image.open(""foo.jpg"") pix = numpy.array(pic.getdata()).reshape(pic.size[0] pic.size[1] 3) But I can't seem to figure out how to load it back into the PIL object after I've done all my awesome transforms. I'm aware of the putdata() method but can't quite seem to get it to behave. Any thoughts? Open I as an array: >>> I = numpy.asarray(Image.open('test.jpg')) Do some stuff to I then convert it back to an image: >>> im = Image.fromarray(numpy.uint8(I)) Filter numpy images with FFT Python If you want to do it explicitly for some reason there are pil2array() and array2pil() functions using getdata() on this page in correlation.zip. NameError: name ""Image"" is not defined. @ArditS.: Did you `import Image` first? Do you have PIL installed? I fixed it. I didn't have PIL installed. Is the `uint8` conversion necessary?  You're not saying how exactly putdata() is not behaving. I'm assuming you're doing >>> pic.putdata(a) Traceback (most recent call last): File ""...blablabla.../PIL/Image.py"" line 1185 in putdata self.im.putdata(data scale offset) SystemError: new style getargs format but argument is not a tuple This is because putdata expects a sequence of tuples and you're giving it a numpy array. This >>> data = list(tuple(pixel) for pixel in pix) >>> pic.putdata(data) will work but it is very slow. As of PIL 1.1.6 the ""proper"" way to convert between images and numpy arrays is simply >>> pix = numpy.array(pic) although the resulting array is in a different format than yours (3-d array or rows/columns/rgb in this case). Then after you make your changes to the array you should be able to do either pic.putdata(pix) or create a new image with Image.fromarray(pix). First shouldn't it be pic.putdata(data)? And numpy.asarray(pic) produces a readonly array so you need to call numpy.array(pic) and you didn't answer the question... from the link you provided it appears to be pic = Image.fromarray(pix). Fix your answer and I'll accept it. Thanks for this...`Image.fromarray` is not listed in the PIL documentation (!) so I'd never have found it if it weren't for this.",python image numpy python-imaging-library
1420235,A,How can I generate a complete histogram with numpy? I have a very long list in a numpy.array. I want to generate a histogram for it. However Numpy's built in histogram requires a pre-defined number of bins. What's the best way to generate a full histogram with one bin for each value? If you have an array of integers and the max value isn't too large you can use numpy.bincount: hist = dict((keyval) for key val in enumerate(numpy.bincount(data)) if val) Edit: If you have float data or data spread over a huge range you can convert it to integers by doing: bins = numpy.unique(data) bincounts = numpy.bincount(numpy.digitize(data bins) - 1) hist = dict(zip(bins bincounts)) thanks. didn't know about `bincount()`  A bin for every value sounds a bit strange but wouldn't bins=a.max()-a.min() give a similar result?,python numpy histogram
1001634,A,"Array division- translating from MATLAB to Python I have this line of code in MATLAB written by someone else: c=a.'/b I need to translate it into Python. a b and c are all arrays. The dimensions that I am currently using to test the code are: a: 18x1 b: 25x18 which gives me c with dimensions 1x25. The arrays are not square but I would not want the code to fail if they were. Can someone explain exactly what this line is doing (mathematically) and how to do it in Python? (i.e. the equivalent for the built-in mrdivide function in MATLAB if it exists in Python?) I think you have a typo. If ""a"" is 1-by-18 you don't need the transpose. It's not a typo the Matlab code works perfectly. @Emily: Then ""a"" has to be 18-by-1 (before the transpose) not 1-by-18. Otherwise MATLAB throws an error. Yes A-transpose/B to rightly occur A-transponse and B should have the same no. of columns. And so A's size should be 18x1 as gnovice pointed out. Ok sorry about that it is 18x1 at first. The line c = a.' / b computes the solution of the equation c b = aT for c. Numpy does not have an operator that does this directly. Instead you should solve bT cT = a for cT and transpose the result: c = numpy.linalg.lstsq(b.T a.T)[0].T Thanks for pointing that out. I'll edit my answer. You may have the ""a"" and ""b"" in your equation flipped relative to the OPs ""a"" and ""b"". The line ""c = a.'/b"" is solving an equation of the form ""x*b = a.'"" which becomes ""(b.')*(x.') = a"". The OPs ""b"" (i.e. the matrix of equation coefficients) should be the first input to lstsq transposed of course.  [edited] As Suvesh pointed out i was completely wrong before. however numpy can still easily do the procedure he gives in his post: A = numpy.matrix(numpy.random.random((18 1))) # as noted by others your dimensions are off B = numpy.matrix(numpy.random.random((25 18))) C = A.T * B.T * (B * B.T).I The / operator in Python is defined as standard matrix division for square matrices i.e. A*inv(B). In her example she's trying to achieve right division for any size matrices. So your code would not work. +1 for the change and translating my math into Python code. :) I don't know Python but in any case I wanted her to work that out so I didn't bother to post code along with my math. This at least gives me a result matrix c with the right dimensions but the values in that matrix do not match the values in Matlab- any ideas? did you make sure that the A and B you use in numpy are the same A and B you used in matlab? You should NOT do this. The so called normal equations should not be implemented directly but should use the least square approximation which is lstsq function in numpy.linalg. @Autoplectic: yes. @David: can you please explain how to use this function to replace this line in Python? I tried to use it after I read the help for lstsq but I am getting getting values in the matrix that are different from the ones in Matlab though they are the same dimensions.  In Matlab A.' means transposing the A matrix. So mathematically what is achieved in the code is AT/B. How to go about implementing matrix division in Python (or any language) (Note: Let's go over a simple division of the form A/B; for your example you would need to do AT first and then AT/B next and it's pretty easy to do the transpose operation in Python |left-as-an-exercise :)|) You have a matrix equation C*B=A (You want to find C as A/B) RIGHT DIVISION (/) is as follows: C*(B*BT)=A*BT You then isolate C by inverting (B*BT) i.e. C = A*BT*(B*BT)' ----- [1] Therefore to implement matrix division in Python (or any language) get the following three methods. Matrix multiplication Matrix transpose Matrix inverse Then apply them iteratively to achieve division as in [1]. Only you need to do AT/B therefore your final operation after implementing the three basic methods should be: AT*BT*(B*BT)' Note: Don't forget the basic rules of operator precedence :)  The symbol ""/"" is the matrix right division operator in MATLAB which calls the MRDIVIDE function. From the documentation matrix right division is related to matrix left division in the following way: B/A = (A'\B')' If A is a square matrix B/A is roughly the same as B*inv(A) (although it's computed in a different way). Otherwise X = B/A is the solution in the least squares sense to the under- or overdetermined system of equations XA = B. More detail about the algorithms used for solving the system of equations is given in the link to the MRDIVIDE documentation above. Most use LAPACK or BLAS. It's all rather complicated and you should check to see if Python already has an MRDIVIDE-like function before you try to do it yourself. EDIT: The NumPy package for Python contains a routine lstsq for computing the least-squares solution to a system of equations as mentioned in a comment by David Cournapeau. This routine will likely give you comparable results to using the MRDIVIDE function in MATLAB but it is unlikely to be exact. Any differences in the underlying algorithms used by each function will likely result in answers that differ slightly from one another (i.e. one may return a value of 1.0 whereas the other may return a value of 0.999). The relative size of this error could end up being larger depending heavily on the specific system of equations you are solving. To use lstsq you may have to adjust your problem slightly. It appears that you want to solve an equation of the form cB = a where B is 25-by-18 a is 1-by-18 and c is 1-by-25. Applying a transpose to both sides gives you the equation BTcT = aT which is a more standard form (i.e. Ax = b). The arguments to lstsq should be (in this order) BT (an 18-by-25 array) and aT (an 18-element array). lstsq should return a 25-element array (cT). Disclaimer: I don't know if Python makes any distinction between a 1-by-N or N-by-1 array so transposes may not be necessary for 1-dimensional arrays. MATLAB certainly considers them as different and will yell at you for it. =) Numpy does not make any distinction between a 1-by-N or N-by-1 array. A 1D array is a 1D array. If you make a 2D array with one dimension of length 1 then you can differentiate but there's usually no reason to do so.",python matlab numpy linear-algebra
1582525,A,"Longest string in numpy object_ array I'm using a numpy object_ array to store variable length strings e.g. a = np.array(['hello''world''!']dtype=np.object_) Is there an easy way to find the length of the longest string in the array without looping over all elements? No as the only place the length of each string is known is by the string. So you have to find out from every string what its length is.  Say I want to get the longest string in the second column: data_array = [['BFNN' 'Forested bog without permafrost or patterning no internal lawns'] ['BONS' 'Nonpatterned open shrub-dominated bog']] def get_max_len_column_value(data_array column): return len(max(data_array[:[column]] key=len)[0]) get_max_len_column_value(data_array 1) >>>64  If you store the string in a numpy array of dtype object then you can't get at the size of the objects (strings) without looping. However if you let np.array decide the dtype then you can find out the length of the longest string by peeking at the dtype: In [64]: a = np.array(['hello''world''!''Oooh gaaah booo gaah?']) In [65]: a.dtype Out[65]: dtype('|S21') In [72]: a.dtype.itemsize Out[72]: 21  max(a key=len) gives you the longest string (and len(max(a key=len)) gives you its length) without requiring you to code an explicit loop but of course max will do its own looping internally as it couldn't possibly identify ""the longest string"" in any other way. Great answer!!!",python arrays numpy
902761,A,"Saving a Numpy array as an image I have a matrix in the type of a Numpy array. How would I write it to disk it as an image? Any format works (png jpeg bmp...). One important constraint is that PIL is not present. I'd just like to note that some of the answers below and surely some of the people coming and finding this question do not meet the constraint listed above of being _without_ [PIL](http://www.pythonware.com/products/pil/). Since some askers and some answers both avoid that constraint I encourage anyone who's here and doesn't mind having PIL to look below and any non-PIL answers (new or old) to mention that they're a PIL-is-used type of answer to distinguish themselves from answers meeting the original constraint. If you happen to use [Py]Qt already you may be interested in qimage2ndarray. Starting with version 1.4 (just released) PySide is supported as well and there will be a tiny imsave(filename array) function similar to scipy's but using Qt instead of PIL. With 1.3 just use something like the following: qImage = array2qimage(image normalize = False) # create QImage from ndarray success = qImage.save(filename) # use Qt's image IO functions for saving PNG/JPG/.. (Another advantage of 1.4 is that it is a pure python solution which makes this even more lightweight.) The 1.4 release is out now. :-) (I edited the answer accordingly.)  Maybe you have scipy: import scipy scipy.misc.imsave('outfile.jpg' image_array) imsave lives in .../scipy/misc/pilutil.py which uses PIL Ah I was not aware. Thank you for the reference. Be careful when converting to jpg since it is lossy and so you may not be able to recover the exact data used to generate the image. `numpy` does not imply `scipy`  Pure Python a snippet without 3rd party dependencies. This function writes true-color RGBA PNG's using Python's gzip module. def write_png(buf width height): """""" buf: must be bytes or a bytearray in py3 a regular string in py2. formatted RGBARGBA. """""" import zlib struct # reverse the vertical line order and add null bytes at the start width_byte_4 = width * 4 raw_data = b''.join(b'\x00' + buf[span:span + width_byte_4] for span in range((height - 1) * width * 4 -1 - width_byte_4)) def png_pack(png_tag data): chunk_head = png_tag + data return (struct.pack(""!I"" len(data)) + chunk_head + struct.pack(""!I"" 0xFFFFFFFF & zlib.crc32(chunk_head))) return b''.join([ b'\x89PNG\r\n\x1a\n' png_pack(b'IHDR' struct.pack(""!2I5B"" width height 8 6 0 0 0)) png_pack(b'IDAT' zlib.compress(raw_data 9)) png_pack(b'IEND' b'')]) ... The data should be written directly to a file opened as binary as in: data = write_png(buf 64 64) with open(""my_image.png"" 'wb') as fd: fd.write(data) Original source: https://developer.blender.org/diffusion/B/browse/master/release/bin/blender-thumbnailer.py$155 Example usage thanks to @Evgeni Sergeev: http://stackoverflow.com/a/21034111/432509 @PhilMacKay the data just has to be written to a binary file. added comment. This seems to be exactly what I'm looking for but could you add some comments? I don't see how this writes to a file. Do you have to write the output in a previously opened file? Thanks! Can someone specify what format the image (`buf`) is supposed to be in? It does not seem to be a numpy array... @christianmbrodbeck a bytearray (RGBARGBA...)  With matplotlib: import matplotlib matplotlib.image.imsave('name.png' array) Works with matplotlib 1.3.1 I don't know about lower version. From the docstring: Arguments: *fname*: A string containing a path to a filename or a Python file-like object. If *format* is *None* and *fname* is a string the output format is deduced from the extension of the filename. *arr*: An MxN (luminance) MxNx3 (RGB) or MxNx4 (RGBA) array. Using this. But suffering from memory leak  You can use PyPNG. It's a pure Python (no dependencies) open source PNG encoder/decoder and it supports writing NumPy arrays as images.  given a numpy array ""A"": import Image im = Image.fromarray(A) im.save(""your_file.jpeg"") you can replace ""jpeg"" with almost any format you want. More details about the formats here Image is a module of PIL. Do ""print Image.__file__"" Very helpful for those of us who wandered here and do have PIL - I think I'll use `from PIL import Image` to keep it clear... If you've got an RGB image you can get the image using im = Image.fromarray(A).convert('RGB') More info: http://stackoverflow.com/questions/4711880/pil-using-fromarray-with-binary-data-and-writing-coloured-text  Addendum to @ideasman42's answer: def saveAsPNG(array filename): import struct if any([len(row) != len(array[0]) for row in array]): raise ValueError ""Array should have elements of equal size"" #First row becomes top row of image. flat = []; map(flat.extend reversed(array)) #Big-endian unsigned 32-byte integer. buf = b''.join([struct.pack('>I' ((0xffFFff & i32)<<8)|(i32>>24) ) for i32 in flat]) #Rotate from ARGB to RGBA. data = write_png(buf len(array[0]) len(array)) f = open(filename 'wb') f.write(data) f.close() So you can do: saveAsPNG([[0xffFF0000 0xffFFFF00] [0xff00aa77 0xff333333]] 'test_grid.png') Producing test_grid.png: (Transparency also works by reducing the high byte from 0xff.)  If you have matplotlib you can do: import matplotlib.pyplot as plt plt.imshow(matrix) #Needs to be in rowcol order plt.savefig(filename) Befoe imshow one has to add plt.figure() and plt.show() No for the pyplot interface the plt.figure() is superfluous. Also you only need the plt.show() if you want to see a figure window as well--in this case only saving an image file was desired so there was no need to call show().  matplotlib svn has a new function to save images as just an image -- no axes etc. it's a very simple function to backport too if you don't want to install svn (copied straight from image.py in matplotlib svn removed the docstring for brevity): def imsave(fname arr vmin=None vmax=None cmap=None format=None origin=None): from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas from matplotlib.figure import Figure fig = Figure(figsize=arr.shape[::-1] dpi=1 frameon=False) canvas = FigureCanvas(fig) fig.figimage(arr cmap=cmap vmin=vmin vmax=vmax origin=origin) fig.savefig(fname dpi=1 format=format)",python image numpy
872376,A,numpy linear algebra basic help This is what I need to do- I have this equation- Ax = y Where A is a rational m*n matrix (m<=n) and x and y are vectors of the right size. I know A and y I don't know what x is equal to. I also know that there is no x where Ax equals exactly y. I want to find the vector x' such that Ax' is as close as possible to y. Meaning that (Ax' - y) is as close as possible to (000...0). I know that I need to use either the lstsq function: http://www.scipy.org/doc/numpy_api_docs/numpy.linalg.linalg.html#lstsq or the svd function: http://www.scipy.org/doc/numpy_api_docs/numpy.linalg.linalg.html#svd I don't understand the documentation at all. Can someone please show me how to use these functions to solve my problem. Thanks a lot!!! SVD is for the case of m < n because you don't really have enough degrees of freedom. The docs for lstsq don't look very helpful. I believe that's least square fitting for the case where m > n. If m < n you'll want SVD. For a discussion of the SVD from a computing perspective take a look at section 2.6 of the numerical recipes book (http://www.nrbook.com/a/bookcpdf.php)  The updated documentation may be a bit more helpful... looks like you want numpy.linalg.lstsq(A y) Yeah it looks like x = numpy.linalg.lstsq(A y) will solve your problem (and based on the return values it appears to solve the least-squares problem by using the SVD). Beware that for m <= n you are likely to suffer from overfitting that is you have enough degrees of freedom to fit y exactly but only because your model is too flexible and is fitting the noise in addition to the underlying signal.  The SVD of matrix A gives you orthogonal matrices U and V and diagonal matrix Σ such that A = U Σ V T where U UT = I ; V VT = I Hence if x A = y then x U Σ V T = y x U Σ V T V = y V x U Σ = y V U T x Σ = y V x Σ = U y V x = Σ -1 U T y V x = V T Σ -1 U T y So given SVD of A you can get x. Although for general matrices A B != B A it is true for vector x that x U == U T x. For example consider x = ( x y ) U = ( a b ; c d ): x U = ( x y ) ( a b ; c d ) = ( xa+yc xb+yd ) = ( ax+cy bx+dy ) = ( a c; b d ) ( x; y ) = U T x It's fairly obvious when you look at the values in x U being the dot products of x and the columns of U and the values in UTx being the dot products of the x and the rows of UT and the relation of rows and columns in transposition In general matrix multiplication is not commutative so in general xA != Ax.,python numpy scipy linear-algebra svd
1519956,A,"NumPy and memmap: [Errno 24] Too many open files I am working with large matrixes so I am using NumPy's memmap. However I am getting an error as apparently the file descriptors used by memmap are not being closed. import numpy import tempfile counter = 0 while True: temp_fd temporary_filename = tempfile.mkstemp(suffix='.memmap') map = numpy.memmap(temporary_filename dtype=float mode=""w+"" shape=1000) counter += 1 print counter map.close() os.remove(temporary_filename) From what I understand the memmap file is closed when the method close() is called. However the code above cannot loop forever as it eventually throws the ""[Errno 24] Too many open files"" error:  1016 1017 1018 1019 Traceback (most recent call last): File ""./memmap_loop.py"" line 11 in <module> File ""/usr/lib/python2.5/site-packages/numpy/core/memmap.py"" line 226 in __new__ EnvironmentError: [Errno 24] Too many open files Error in sys.excepthook: Traceback (most recent call last): File ""/usr/lib/python2.5/site-packages/apport_python_hook.py"" line 38 in apport_excepthook ImportError: No module named packaging_impl Original exception was: Traceback (most recent call last): File ""./memmap_loop.py"" line 11 in <module> File ""/usr/lib/python2.5/site-packages/numpy/core/memmap.py"" line 226 in __new__ EnvironmentError: [Errno 24] Too many open files Does anybody know what I am overlooking? Since the memmap does not take the open file descriptor but the file name I suppose you leak the temp_fd file descriptor. Does os.close(temp_fd) help? Great that it works. Since you can pass numpy.memmap a file-like object you could create one from the file descriptor you already have temp_fd. fobj = os.fdopen(temp_fd ""w+"") numpy.memmap(fobj ... It helps. It's working now. Thanks.",python memory-management numpy
