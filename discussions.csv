0,A,"How do you mock params when testing a Rails model's setter? Given the code from the Complex Form part III how would you go about testing the virtual attribute?  def new_task_attributes=(task_attributes) task_attributes.each do |attributes| tasks.build(attributes) end end I am currently trying to test it like this:  def test_adding_task_to_project p = Project.new params = {""new_tasks_attributes"" => [{ ""name"" => ""paint fence""}]} p.new_tasks_attributes=(params) p.save assert p.tasks.length == 1 end But I am getting the following error: NoMethodError: undefined method `stringify_keys!' for ""new_tasks_attributes"":String Any suggestions on improving this test would be greatly appreciated. Can we see the whole stack trace? Where does it think String#stringify_keys! is being called? Also params looks odd to me. Is tasks.build() expecting input like this: [""new_tasks_attribute"" {""name"" => ""paint fence""}] ? If not maybe you actually want Hash#each_key() instead of Hash#each()? Need more data. Also you might consider a Ruby tag to accompany your Rails tag.  It looks as if new_task_attributes= is expecting an array of hashes but you're passing it a hash. Try this: def test_adding_task_to_project p = Project.new new_tasks_attributes = [{ ""name"" => ""paint fence""}] p.new_tasks_attributes = (new_tasks_attributes) p.save assert p.tasks.length == 1 end"1,A,"What's the best way to tell if a method is a property from within Policy Injection? I've got a custom handler applied to a class (using the Policy Injection Application Block in entlib 4) and I would like to know whether the input method is a property when Invoke is called. Following is what my handler looks like. [ConfigurationElementType(typeof(MyCustomHandlerData))] public class MyCustomHandler : ICallHandler { public IMethodReturn Invoke(IMethodInvocation input GetNextHandlerDelegate getNext) { if (input.MethodBase.IsPublic && (input.MethodBase.Name.Contains(""get_"") || input.MethodBase.Name.Contains(""set_""))) { Console.WriteLine(""MyCustomHandler Invoke called with input of {0}"" input.MethodBase.Name); } return getNext().Invoke(input getNext); } public int Order { get; set; } } As you can see from my code sample the best way I've thought of so far is by parsing the method name. Isn't there a better way to do this? A couple of you mentioned using the ""IsSpecialName"" property of the MethodBase type. While it is true that the will return true for property ""gets"" or ""sets"" it will also return true for operator overloads such as add_EventName or remove_EventName. So you will need to examine other attributes of the MethodBase instance to determine if its a property accessor. Unfortunately if all you have is a reference to a MethodBase instance (which I believe is the case with intercepting behaviors in the Unity framework) there is not real ""clean"" way to determine if its a property setter or getter. The best way I've found is as follows: C#: bool IsPropertySetter(MethodBase methodBase){ return methodBase.IsSpecialName && methodBase.Name.StartsWith(""set_""); } bool IsPropertyGetter(MethodBase methodBase){ return methodBase.IsSpecialName && methodBase.Name.StartsWith(""get_""); } VB:  Private Function IsPropertySetter(methodBase As MethodBase) As Boolean Return methodBase.IsSpecialName AndAlso methodBase.Name.StartsWith(""set_"") End Function Private Function IsPropertyGetter(methodBase As MethodBase) As Boolean Return methodBase.IsSpecialName AndAlso methodBase.Name.StartsWith(""get_"") End Function  You could check the IsSpecialName property; it will be true for property getters and setters. However it will also be true for other special methods like operator overloads.  You can also check IsSpecialName is true. this will be true in a property (amongst other things) At the il level the methods are exposed as follows (using Environment.ExitCode as example): .method public hidebysig specialname static int32 get_ExitCode() cil managed .method public hidebysig specialname static void set_ExitCode(int32 'value') cil managed If you wanted to get fancy you could verify after extracting the name that said property exists but to be honest if (m.IsSpecialName && (m.Attributes & MethodAttributes.HideBySig) != 0)) as well as starts with get_ or set_ then you should be good even for people using nasty names (faking the hidebysig is easy enough faking the IsSpecialName would be very tricky) Nothing is guaranteed though. Someone could emit a class with a set_Foo method that looked just like a real set method but actually wasn't a set on a read only property. Unless you check whether the property CanRead/CanWrite as well. This strikes me as madness for you though you aren't expecting deliberate circumvention. A simple utility/extension method on MethodInfo which did this logic wouldn't be too hard and including IsSpecialName would almost certainly cover all your needs.  I'm not familiar with that application block but assuming that MethodBase property is of type System.Reflection.MethodBase you could take a look at the IsSpecialName property. System.Reflection.MethodBase.IsSpecialName on MSDN"2,A,Avoiding double-thunking with C++/CLI properties I've read (in Nish Sivakumar's book C++/CLI In Action among other places) that you should use the __clrcall decorator on function calls to avoid double-thunking in cases where you know that the method will never be called from unmanaged code. Nish also says that if the method signature contains any CLR types then the JIT compiler will automatically add the __clrcall. What is not clear to me is if I need to include __clrcall when I create C++/CLI properties. In one sense properties are only accessible from .NET languages on the other hand the C++/CLI compiler (I think) just generates methods (e.g. ***_get() ) that are callable from both managed and unmanaged code. So do I need to use the __clrcall modifier on my properties and if so where does it go? On the get/set functions themselves? @Mike B - Thanks for the tip on ildasm - I didn't know about that tool. It appears that I misread/misunderstood Nish - the __clrcall modifier and the double-thunking problem it eliminates only apply to methods of NATIVE classes. All methods of Managed classes are __clrcall by default - which seems obvious in retrospect. Evidently Marcus Heege's book Expert C++/CLI is available as a free download and it has a nice table on page 215 that summarizes the calling conventions. Thanks for the follow-up.3,A,CakePHP ACL Database Setup: ARO / ACO structure? I'm struggling to implement ACL in CakePHP. After reading the documentation in the cake manual as well as several other tutorials blog posts etc I found Aran Johnson's excellent tutorial which has helped fill in many of the gaps. His examples seem to conflict with others I've seen though in a few places - specifically in the ARO tree structure he uses. In his examples his user groups are set up as a cascading tree with the most general user type being at the top of the tree and its children branching off for each more restricted access type. Elsewhere I've usually seen each user type as a child of the same generic user type. How do you set up your AROs and ACOs in CakePHP? Any and all tips appreciated! I got here looking for isMine() which I tried defining but it looks like it already exists. CakePHP's built-in ACL system is really powerful but poorly documented in terms of actual implementation details. A system that we've used with some success in a number of CakePHP-based projects is as follows. It's a modification of some group-level access systems that have been documented elsewhere. Our system's aims are to have a simple system where users are authorised on a group-level but they can have specific additional rights on items that were created by them or on a per-user basis. We wanted to avoid having to create a specific entry for each user (or more specifically for each ARO) in the aros_acos table. We have a Users table and a Roles table. Users user_id user_name role_id Roles id role_name Create the ARO tree for each role (we usually have 4 roles - Unauthorised Guest (id 1) Authorised User (id 2) Site Moderator (id 3) and Administrator (id 4)) : cake acl create aro / Role.1 cake acl create aro 1 Role.2 ... etc ... After this you have to use SQL or phpMyAdmin or similar to add aliases for all of these as the cake command line tool doesn't do it. We use 'Role-{id}' and 'User-{id}' for all of ours. We then create a ROOT ACO - cake acl create aco / 'ROOT' and then create ACOs for all the controllers under this ROOT one: cake acl create aco 'ROOT' 'MyController' ... etc ... So far so normal. We add an additional field in the aros_acos table called _editown which we can use as an additional action in the ACL component's actionMap. CREATE TABLE IF NOT EXISTS `aros_acos` ( `id` int(11) NOT NULL auto_increment `aro_id` int(11) default NULL `aco_id` int(11) default NULL `_create` int(11) NOT NULL default '0' `_read` int(11) NOT NULL default '0' `_update` int(11) NOT NULL default '0' `_delete` int(11) NOT NULL default '0' `_editown` int(11) NOT NULL default '0' PRIMARY KEY (`id`) KEY `acl` (`aro_id``aco_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; We can then setup the Auth component to use the 'crud' method which validates the requested controller/action against an AclComponent::check(). In the app_controller we have something along the lines of: private function setupAuth() { if(isset($this->Auth)) { .... $this->Auth->authorize = 'crud'; $this->Auth->actionMap = array( 'index' => 'read' 'add' => 'create' 'edit' => 'update' 'editMine' => 'editown' 'view' => 'read' ... etc ... ); ... etc ... } } Again this is fairly standard CakePHP stuff. We then have a checkAccess method in the AppController that adds in the group-level stuff to check whether to check a group ARO or a user ARO for access: private function checkAccess() { if(!$user = $this->Auth->user()) { $role_alias = 'Role-1'; $user_alias = null; } else { $role_alias = 'Role-' . $user['User']['role_id']; $user_alias = 'User-' . $user['User']['id']; } // do we have an aro for this user? if($user_alias && ($user_aro = $this->User->Aro->findByAlias($user_alias))) { $aro_alias = $user_alias; } else { $aro_alias = $role_alias; } if ('editown' == $this->Auth->actionMap[$this->action]) { if($this->Acl->check($aro_alias $this->name 'editown') and $this->isMine()) { $this->Auth->allow(); } else { $this->Auth->authorize = 'controller'; $this->Auth->deny('*'); } } else { // check this user-level aro for access if($this->Acl->check($aro_alias $this->name $this->Auth->actionMap[$this->action])) { $this->Auth->allow(); } else { $this->Auth->authorize = 'controller'; $this->Auth->deny('*'); } } } The setupAuth() and checkAccess() methods are called in the AppController's beforeFilter() callback. There's an isMine method in the AppControler too (see below) that just checks that the user_id of the requested item is the same as the currently authenticated user. I've left this out for clarity. That's really all there is to it. You can then allow / deny particular groups access to specific acos - cake acl grant 'Role-2' 'MyController' 'read' cake acl grant 'Role-2' 'MyController' 'editown' cake acl deny 'Role-2' 'MyController' 'update' cake acl deny 'Role-2' 'MyController' 'delete' I'm sure you get the picture. Anyway this answer's way longer than I intended it to be and it probably makes next to no sense but I hope it's some help to you ... -- edit -- As requested here's an edited (purely for clarity - there's a lot of stuff in our boilerplate code that's meaningless here) isMine() method that we have in our AppController. I've removed a lot of error checking stuff too but this is the essence of it: function isMine($model=null $id=null $usermodel='User' $foreignkey='user_id') { if(empty($model)) { // default model is first item in $this->uses array $model = $this->uses[0]; } if(empty($id)) { if(!empty($this->passedArgs['id'])) { $id = $this->passedArgs['id']; } elseif(!empty($this->passedArgs[0])) { $id = $this->passedArgs[0]; } } if(is_array($id)) { foreach($id as $i) { if(!$this->_isMine($model $i $usermodel $foreignkey)) { return false; } } return true; } return $this->_isMine($model $id $usermodel $foreignkey); } function _isMine($model $id $usermodel='User' $foreignkey='user_id') { $user = Configure::read('curr.loggedinuser'); // this is set in the UsersController on successful login if(isset($this->$model)) { $model = $this->$model; } else { $model = ClassRegistry::init($model); } //read model if(!($record = $model->read(null $id))) { return false; } //get foreign key if($usermodel == $model->alias) { if($record[$model->alias][$model->primaryKey] == $user['User']['id']) { return true; } } elseif($record[$model->alias][$foreignkey] == $user['User']['id']) { return true; } return false; } simple and better than most articles @David: am using cakephp 2.x & trying to add aco something like this cake acl create aco controllers Users then cake acl create aco Users index Running 1st command will give me error :-) thanks for such a great tutorial. It saved me too.. :-)4,A,"Programmatically accessing Data in an ASP.NET 2.0 Repeater This is an ASP.Net 2.0 web app. The Item template looks like this for reference: <ItemTemplate> <tr> <td class=""class1"" align=center><a href='url'><img src=""img.gif""></a></td> <td class=""class1""><%# DataBinder.Eval(Container.DataItem""field1"") %></td> <td class=""class1""><%# DataBinder.Eval(Container.DataItem""field2"") %></td> <td class=""class1""><%# DataBinder.Eval(Container.DataItem""field3"") %></td> <td class=""class1""><%# DataBinder.Eval(Container.DataItem""field4"") %></td> </tr> </ItemTemplate> Using this in codebehind: foreach (RepeaterItem item in rptrFollowupSummary.Items) { string val = ((DataBoundLiteralControl)item.Controls[0]).Text; Trace.Write(val); } I produce this: <tr> <td class=""class1"" align=center><a href='url'><img src=""img.gif""></a></td> <td class=""class1"">23</td> <td class=""class1"">1/1/2000</td> <td class=""class1"">-2</td> <td class=""class1"">11</td> </tr> What I need is the data from Field1 and Field4 I can't seem to get at the data the way I would in say a DataList or a GridView and I can't seem to come up with anything else on Google or quickly leverage this one to do what I want. The only way I can see to get at the data is going to be using a regex to go and get it (Because a man takes what he wants. He takes it all. And I'm a man aren't I? Aren't I?). Am I on the right track (not looking for the specific regex to do this; forging that might be a followup question ;) ) or am I missing something? The Repeater in this case is set in stone so I can't switch to something more elegant. Once upon a time I did something similar to what Alison Zhou suggested using DataLists but it's been some time (2+ years) and I just completely forgot about doing it this way. Yeesh talk about overlooking something obvious. . . So I did as Alison suggested and it works fine. I don't think the viewstate is an issue here even though this repeater can get dozens of rows. I can't really speak to the question if doing it that way versus using the instead (but that seems like a fine solution to me otherwise). Obviously the latter is less of a viewstate footprint but I'm not experienced enough to say when one approach might be preferrable to another without an extreme example in front of me. Alison one question: why literals and not labels? Euro Micelli I was trying to avoid a return trip to the database. Since I'm still a little green relative to the rest of the development world I admit I don't necessarily have a good grasp of how many database trips is ""just right"". There wouldn't be a performance issue here (I know the app's load enough to know this) but I suppose I was trying to avoid it out of habit since my boss tends to emphasize fewer trips where possible. I don't understand what you mean by ""get"" data. Doesn't the fact that you have access to the rptrFollowUpSummary mean that you already have the data in its raw form? Amen - not very clear about what he is after... The title of the post says it all. This is a good question. If you can afford a smidge more overhead in the generation go for DataList and use the DataKeys property which will save the data fields you need. You could also use labels in each of your table cells and be able to reference items with e.Item.FindControl(""LabelID"").  Since you are working with tabular data I'd recommend using the GridView control. Then you'll be able to access individual cells. Otherwise you can set the td's for Field1 and Field4 to runat=""server"" and give them ID's. Then in the codebehind access the InnerText property for each td.  Off the top of my head you can try something like this: <ItemTemplate> <tr> <td ""class1""><asp:Literal ID=""litField1"" runat=""server"" Text='<%# Bind(""Field1"") %>'/></td> <td ""class1""><asp:Literal ID=""litField2"" runat=""server"" Text='<%# Bind(""Field2"") %>'/></td> <td ""class1""><asp:Literal ID=""litField3"" runat=""server"" Text='<%# Bind(""Field3"") %>'/></td> <td ""class1""><asp:Literal ID=""litField4"" runat=""server"" Text='<%# Bind(""Field4"") %>'/></td> </tr> </ItemTemplate> Then in your code behind you can access each Literal control as follows: foreach (RepeaterItem item in rptrFollowupSummary.Items) { Literal lit1 = (Literal)item.FindControl(""litField1""); string value1 = lit1.Text; Literal lit4 = (Literal)item.FindControl(""litField4""); string value4 = lit4.Text; } This will add to your ViewState but it makes it easy to find your controls.  @peacedog: Correct; Alison's method is perfectly acceptable. The trick with the database roundtrips: they are not free obviously but web servers tend to be very ""close"" (fast low-latency connection) to the database while your users are probably ""far"" (slow high-latency connection). Because of that sending data to/from the browser via cookies ViewState hidden fields or any other method can actually be ""worse"" than reading it again from your database. There are also security implications to keep in mind (Can an ""evil"" user fake the data coming back from the browser? Would it matter if they do?). But quite often it doesn't make any difference in performance. That's why you should do what works more naturally for your particular problem and worry about it only if performance starts to be a real-world issue. Good luck!  The <%#DataBinder.Eval(...) %> mechanism is not Data Binding in a ""strict"" sense. It is a one-way technique to put text in specific places in the template. If you need to get the data back out you have to either: Get it from your source data Populate the repeater with a different mechanism Note that the Repeater doesn't save the DataSource between postbacks You can't just ask it to give you the data later. The first method is usually easier to work with. Don't assume that it's too expensive to reacquire your data from the source unless you prove it to yourself by measuring; it's usually pretty fast. The biggest problem with this technique is if the source data can change between calls. For the second method a common technique is to use a Literal control. See Alison Zhou's post for an example of how to do it. I usually personally prefer to fill the Literal controls inside of the OnItemDataBound instead"5,A,64 bit enum in C++? Is there a way to have a 64 bit enum in C++? Whilst refactoring some code I came across bunch of #defines which would be better as an enum but being greater than 32 bit causes the compiler to error. For some reason I thought the following might work: enum MY_ENUM : unsigned __int64 { LARGE_VALUE = 0x1000000000000000 }; I don't think that's possible. The underlying representation of enums is up to the compiler. You are better off using: const __int64 LARGE_VALUE = 0x1000000000000000L; This is what I resorted to in the end but I was curious as to whether 64 bit enums are possible even with a compiler specific extension.  An enum in C++ can be any integral type. You can for example have an enum of chars. IE: enum MY_ENUM { CHAR_VALUE = 'c' }; I would assume this includes __int64. Try just enum MY_ENUM { LARGE_VALUE = 0x1000000000000000 }; According to my commenter sixlettervariables in C the base type will be an int always while in C++ the base type is whatever is large enough to fit the largest included value. So both enums above should work. @Doug T.: while ANSI C dictates that enumerations are the size of the 'int' data type ISO C++ dictates that enumerations are of the size at least as large as required to represent all values.  Enum type is normally determined by the data type of the first enum initializer. If the value should exceed the range for that integral datatype then c++ compiler will make sure it fits in by using a larger integral data type.If compiler finds that it does not belong to any of the integral data type then compiler will throw error. Ref: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1905.pdf Edit: However this is purely depended on machine architecture  C++11 supports this using this syntax: enum class Enum2 : __int64 {Val1 Val2 val3}; I was close then. I must of read about the `: type` syntax somewhere. note that `class` is still optional. If you want old style enum but with a custom type you can also `enum moo : long long {...}`  In MSVC++ you can do this: enum MYLONGLONGENUM:__int64 { BIG_KEY=0x3034303232303330 ... }; better use int64-max than a magic wild guess.  If the compiler doesn't support 64 bit enums by compilation flags or any other means I think there is no solution to this one. You could create something like in your sample something like: namespace MyNamespace { const uint64 LARGE_VALUE = 0x1000000000000000; }; and using it just like an enum using MyNamespace::LARGE_VALUE or using MyNamespace; .... val = LARGE_VALUE; And loose type safety though. Unfortunatelly yes The speed of our conversation reminds me of correspondence chess :D  The current draft of so called C++0x it is n3092 says in 7.2 Enumeration declarations paragraph 6: It is implementation-defined which integral type is used as the underlying type except that the underlying type shall not be larger than int unless the value of an enumerator cannot fit in an int or unsigned int. The same paragraph also says: If no integral type can represent all the enumerator values the enumeration is ill-formed. My interpretation of the part unless the value of an enumerator cannot fit in an int or unsigned int is that it's perfectly valid and safe to initialise enumerator with 64-bit integer value as long as there is 64-bit integer type provided in a particular C++ implementation. For example: enum MyEnum { Undefined = 0xffffffffffffffffULL }; If you don't want count out your f's you can just do `Undefined = ~0x0ULL`  Since you are working in C++ another alternative might be const __int64 LARVE_VALUE = ... This can be specified in an H file. LARVE? I cheated 9 chars with first try.  The answers refering to __int64 miss the problem. The enum is valid in all C++ compilers that have a true 64 bit integral type i.e. any C++11 compiler or C++03 compilers with appropriate extensions. Extensions to C++03 like __int64 work differently across compilers including its suitability as a base type for enums.  your snipplet of code is not c++ standard: enum MY_ENUM : unsigned __int64 does not make sense. use const __int64 instead as Torlack suggests he has prolly seen it in another curly brace language (e.g. C# supports it) or in the upcoming C++ standard where this will be allowed.6,A,"VS 2003 Reports ""unable to get the project file from the web server"" when opening a solution from VSS When attempting to open a project from source control on a newly formatted pc I receive an ""unable to get the project file from the web server"" after getting the sln file from VSS. If I attempt to open the sln file from explorer I also receive the same error. Any pointers or ideas? Thanks! Is there anything odd in your sln file? Have you opened it with a text editor to see if it is linking to a remote resource? I am not. I am using VSS Locally  This question is very old so you have probably solved the issue but just in case: Does the project file use IIS? If so then it is probably trying to read the project file from IIS and the virtual directory does not exist on the newly formatted computer. Also there should be more detail about the message in the Output window when you open the solution which should help you find the cause. With VS2003 you also need to add your user account to the ""Debugger Users"" and ""VS Developers"" and possibly the account that is running the AppPool (possibly Network Server ASPNET or IUSER_xxx). This may depend on the type of authentication you are using as well. Occasionally I had to add those group permissions the the virtual directory location as well. It's been a while since I have used VS2003 with web projects though.  Try deleting the .csproj files (back them up first though)."7,A,"Setting a div's height in HTML with CSS I am a CSS newbie trying to layout a table-like page with two columns. I want the rightmost column to dock to the right of the page and this column should have a distinct background color. The content in the right side is almost always going to be smaller than that on the left. I would like the div on the right to always be tall enough to reach the separator for the row below it. How can I make my background color fill that space? <html> <body> <style type=""text/css""> .rightfloat { color: red; background-color: #BBBBBB; float: right; width: 200px; } .left { font-size: 20pt; } .separator { clear: both; width: 100%; border-top: 1px solid black; } </style> <div class=""separator""> <div class=""rightfloat""> Some really short content. </div> <div class=""left""> Some really really really really really really really really really really big content </div> </div> <div class=""separator""> <div class=""rightfloat""> Some more short content. </div> <div class=""left""> Some really really really really really really really really really really big content </div> </div> </body> </html> Edit: I agree that this example is very table-like and an actual table would be a fine choice. But my ""real"" page will eventually be less table-like and I'd just like to first master this task! Also for some reason when I create/edit my posts in IE7 the code shows up correctly in the preview view but when I actually post the message the formatting gets removed. Editing my post in Firefox 2 seems to have worked FWIW. Another edit: Yeah I unaccepted GateKiller's answer. It does indeed work nicely on my simple page but not in my actual heavier page. I'll investigate some of the links y'all have pointed me to. Thanks! The short answer to your question is that you must set the height of 100% to the body and html tag then set the height to 100% on each div element you want to make 100% the height of the page.  Some browsers support CSS tables so you could create this kind of layout using the various CSS display: table-* values. There's more information on CSS tables in this article (and the book of the same name) by Rachel Andrew: Everything You Know About CSS is Wrong If you need a consistent layout in older browsers that don't support CSS tables you need to do two things: Make your ""table row"" element clear its internal floated elements. The simplest way of doing this is to set overflow: hidden which takes care of most browsers and zoom: 1 to trigger the hasLayout property in older versions of IE. There are many other ways of clearing floats if this approach causes undesirable side effects you should check the question which method of 'clearfix' is best and the article on having layout for other methods. Balance the height of the two ""table cell"" elements. There are two ways you could approach this. Either you can create the appearance of equal heights by setting a background image on the ""table row"" element (the faux columns technique) or you can make the heights of the columns match by giving each a large padding and equally large negative margin. Faux columns is the simpler approach and works very well when the width of one or both columns is fixed. The other technique copes better with variable width columns (based on percentage or em units) but can cause problems in some browsers if you link directly to content within your columns (e.g. if a column contained <div id=""foo""></div> and you linked to #foo) Here's an example using the padding/margin technique to balance the height of the columns. HTML: <div class=""row""> <div class=""right-column"">Right column content</div> <div class=""left-column"">Left column content</div> </div> <div class=""row""> <div class=""right-column"">Right column content</div> <div class=""left-column"">Left column content</div> </div> CSS: html body { height: 100%; } div.row { zoom: 1; /* Clear internal floats in IE */ overflow: hidden; /* Clear internal floats */ } div.right-column div.left-column { padding-bottom: 1000em; /* } Balance the heights of the columns */ margin-bottom: -1000em; /* } */ } div.right-column { width: 20%; float: right; } div.left-column { width: 79%; float: left; } This Barcamp demo by Natalie Downe may also be useful when figuring out how to add additional columns and nice spacing and padding: Equal Height Columns and other tricks (it's also where I first learnt about the margin/padding trick to balance column heights) Worked for me . Thanks :)  I had the same problem on my site (shameless plug). I had the nav section ""float: right"" and the main body of the page has a background image about 250px across aligned to the right and ""repeat-y"". I then added something with ""clear: both"" to it. Here is the W3Schools and the CSS clear property. I placed the clear at the bottom of the ""page"" classed div. My page source looks something like this. body -> header (big blue banner) -> headerNav (green bar at the top) -> breadcrumbs (invisible at the moment) -> page -> navigation (floats to the right) -> content (main content) -> clear (the quote at the bottom) -> footerNav (the green bar at the bottom) -> clear (empty but still does something) -> footer (blue thing at the bottom) I hope that helps :)  Here's an example of equal-height columns - Equal Height Columns - revisited You can also check out the idea of ""Faux Columns"" as well - Faux Columns Don't go the table route. If it's not tabular data don't treat it as such. It's bad for accessibility and flexibility.  This should work for you. Set the height to 100% in your css for the html and body. You can then adjust the height to your needs in the div html { height: 100%; } body { height: 100%; } div { height: 100%; /* Set Div Height */ }  Just trying to help here so the code is more readable... but remember that you can insert code snippets by clicking on the button at the top with ""101010"" Just enter your code then highlight it and click the button. Like this: <html> <body> <style type=""text/css""> .rightfloat { color: red; background-color: #BBBBBB; float: right; width: 200px; } .left { font-size: 20pt; } .separator { clear: both; width: 100%; border-top: 1px solid black; } </style>  Give this a try: <html>  <head>  <style>  htmlbody {  height: 100%  }  #left {  float: left;  width: 25%;  height: 100%  }  #right {  width: 75%;  height: 100%  }  </style>  </head>  <body>  <div id=""left"">  Content  </div>  <div id=""right"">  Content  </div>  </body> </html>  I can think of 2 options Use javascript to resize the smaller column on page load. Fake the equal heights by setting the background-color for the column on the container <div/> instead (<div class=""separator""/>) with repeat-y  I gave up on strictly CSS and used a little jquery: var leftcol = $(""#leftcolumn""); var rightcol = $(""#rightcolumn""); var leftcol_height = leftcol.height(); var rightcol_height = rightcol.height(); if (leftcol_height > rightcol_height) rightcol.height(leftcol_height); else leftcol.height(rightcol_height);  A 2 column layout is a little bit tough to get working in CSS (at least until CSS3 is practical.) Floating left and right will work to a point but it won't allow you to extend the background. To make backgrounds stay solid you'll have to implement a technique known as ""faux columns"" which basically means your columns themselves won't have a background image. Your 2 columns will be contained inside of a parent tag. This parent tag is given a background image that contains the 2 column colors you want. Make this background only as big as you need it to (if it is a solid color only make it 1 pixel high) and have it repeat-y. AListApart has a great walkthrough on what is needed to make it work. http://www.alistapart.com/articles/fauxcolumns/  Ahem The short answer to your question is that you must set the height of 100% to the body and html tag then set the height to 100% on each div element you want to make 100% the height of the page. actually 100% height will not work in most design situations - this may be short but it is not a good answer. Google ""any column longest"" layouts. The best way is to put the left and right cols inside a wrapper div float the left and right cols and then float the wrapper - this makes it stretch to the height of the inner containers - then set background image on the outer wrapper. But watch for any horizontal margins on the floated elements in case you get the IE double margin float bug.  You can just use css width property to do so... Example : <style type=""text/css"">; td { width:25%; height:100%; float:left; } </style> Welcome to Stack Overflow! Thanks for your post! Please do not use signatures/taglines in your posts. Your user box counts as your signature and you can use your profile to post any information about yourself you like. [FAQ on signatures/taglines](http://stackoverflow.com/faq#signatures)"8,A,"How can I know why one of my vxWorks task is pended? In vxWorks I can issue the ""i"" command in the shell and I get the list of tasks in my system along with some information like the following example:  NAME ENTRY TID PRI STATUS PC SP ERRNO DELAY ---------- ------------ -------- --- ---------- -------- -------- ------- ----- tJobTask 1005a6e0 103bae00 0 PEND 100e5860 105fffa8 0 0 tExcTask 10059960 10197cbc 0 PEND 100e5860 101a0ef4 0 0 tLogTask logTask 103bed78 0 PEND 100e37cd 1063ff24 0 0 tNbioLog 1005b390 103bf210 0 PEND 100e5860 1067ff54 0 0 For the tasks that are pended I would like to know what they are pended on. Is there a way to do this? The ""w"" command will do exactly what you want:  NAME ENTRY TID STATUS DELAY OBJ_TYPE OBJ_ID OBJ_NAME ---------- ---------- ---------- ---------- ----- ---------- ---------- -------- tJobTask 0x1005a6e0 0x103bae00 PEND 0 SEM_B 0x10184088 N/A tExcTask 0x10059960 0x10197cbc PEND 0 SEM_B 0x10183ff8 N/A tLogTask logTask 0x103bed78 PEND 0 MSG_Q(R) 0x103be358 N/A tNbioLog 0x1005b390 0x103bf210 PEND 0 SEM_B 0x103bf198 N/A"9,A,"Why is my PDF footer text invisible? I'm creating PDFs on-demand with ColdFusion's CFDocument tag like so: <cfdocument format=""PDF"" filename=""#attributes.fileName#"" overwrite=""true""> <cfdocumentitem type=""footer""> <table border=""0"" cellpadding=""0"" cellspacing=""0"" width=""100%""> <tr> <td align=""left""><font face=""Tahoma"" color=""black""><strong>My Client's Corporation</strong><br/>Street address<br/>City ST 55555</font></td> <td align=""right""><font face=""Tahoma"" color=""black"">Phone: 555.555.5555<br/>Fax: 555.555.5555<br/>Email: info@domain.com</font></td> </tr> </table> </cfdocumentitem> <html> <body> <table border=""0"" cellpadding=""0"" cellspacing=""0"" width=""100%""> <!--- some content here ... ---> </table> </body> </html> </cfdocument> The problem I'm having is that sometimes (actually most of the time but not always) some of the footer text is there but invisible. I can highlight it and copy/paste it into notepad where I can see it all -- but in the generated PDF only the first line of the left column of the footer is visible the rest is invisible. Hence why I added the font color of black in the code. Any ideas on how to correct this? A PDF is what I'm after so I'm not sure how outputting another format would help. As it turns out the footer space just wasn't enough to fit all of this text; verified by the fact that changing the font size to 4pt would fit it all in without a problem. I spent some time attempting to rewrite the footer code using DDX as outlined here and the CFPDF tag to implement it; but even after several hours of hacking away and finally getting a valid DDX as reported by the new isDDX function the CFPDF tag reported that it was invalid DDX for some reason. At this point I decided I had wasted enough of the client's time/money and just reformatted the footer to be 2 lines of centered text which was good enough.  Usually when PDF shows blank text it's because the font metrics are embedded in the document but the glyphs are not. I know nothing about ColdFusion but you might try the following: Try a font other than Tahoma as a test. All PDF readers must support 14 basic fonts including 4 Helvetica variants 4 Times variants 4 Courier variants Symbol and ZapfDingbats so those are always safe choices See if ColdFusion offers any control over font embedding Try a list of alternatives in your font declaration like ""TahomaHelveticasans-serif"""10,A,"bitwise indexing in C? I'm trying to implement a data compression idea I've had and since I'm imagining running it against a large corpus of test data I had thought to code it in C (I mostly have experience in scripting languages like Ruby and Tcl.) Looking through the O'Reilly 'cow' books on C I realize that I can't simply index the bits of a simple 'char' or 'int' type variable as I'd like to to do bitwise comparisons and operators. Am I correct in this perception? Is it reasonable for me to use an enumerated type for representing a bit (and make an array of these and writing functions to convert to and from char)? If so is such a type and functions defined in a standard library already somewhere? Are there other (better?) approaches? Is there some example code somewhere that someone could point me to? Thanks - Try using bitfields. Be careful the implementation can vary by compiler. http://publications.gbdirect.co.uk/c_book/chapter6/bitfields.html  Theory There is no C syntax for accessing or setting the n-th bit of a built-in datatype (e.g. a 'char'). However you can access bits using a logical AND operation and set bits using a logical OR operation. As an example say that you have a variable that holds 1101 and you want to check the 2nd bit from the left. Simply perform a logical AND with 0100: 1101 0100 ---- AND 0100 If the result is non-zero then the 2nd bit must have been set; otherwise is was not set. If you want to set the 3rd bit from the left then perform a logical OR with 0010: 1101 0010 ---- OR 1111 You can use the C operators && (for AND) and || (for OR) to perform these tasks. You will need to construct the bit access patterns (the 0100 and 0010 in the above examples) yourself. The trick is to remember that the least significant bit (LSB) counts 1s the next LSB counts 2s then 4s etc. So the bit access pattern for the n-th LSB (starting at 0) is simply the value of 2^n. The easiest way to compute this in C is to shift the binary value 0001 (in this four bit example) to the left by the required number of places. As this value is always equal to 1 in unsigned integer-like quantities this is just '1 << n' Example unsigned char myVal = 0x65; /* in hex; this is 01100101 in binary. */ /* Q: is the 3-rd least significant bit set (again the LSB is the 0th bit)? */ unsigned char pattern = 1; pattern <<= 3; /* Shift pattern left by three places.*/ if(myVal && (char)(1<<3)) {printf(""Yes!\n"");} /* Perform the test. */ /* Set the most significant bit. */ myVal |= (char)(1<<7); This example hasn't been tested but should serve to illustrate the general idea.  There is a standard library container for bits: std::vector. It is specialised in the library to be space efficient. There is also a boost dynamic_bitset class. These will let you perform operations on a set of boolean values using one bit per value of underlying storage. Boost dynamic bitset documentation For the STL documentation see your compiler documentation. Of course you can also address the individual bits in other integral types by hand. If you do that you should use unsigned types so that you don't get undefined behaviour if decide to do a right shift on a value with the high bit set. However it sounds like you want the containers. To the commenter who claimed this takes 32x more space than necessary: boost::dynamic_bitset and vector are specialised to use one bit per entry and so there is not a space penalty assuming that you actually want more than the number of bits in a primitive type. These classes allow you to address individual bits in a large container with efficient underlying storage. If you just want (say) 32 bits by all means use an int. If you want some large number of bits you can use a library container. That uses 32x more data than is necessary. The original poster said he was interested in data compression. This sounds like the opposite! No the point about vector and boost::dynamic_bitset is that they use one bit per bool. To store 1024 bools they will use 128 bytes plus the class overhead. How did you calculate 32x more storage than necessary?  It is possible. To set the nth bit use OR: x |= (1 << 5); // sets the 5th-from right To clear a bit use AND: x &= ~(1 << 5); // clears 5th-from-right To flip a bit use XOR: x ^= (1 << 5); // flips 5th-from-right To get the value of a bit use shift and AND: (x & (1 << 5)) >> 5 // gets the value (0 or 1) of the 5th-from-right note: the shift right 5 is to ensure the value is either 0 or 1. If you're just interested in 0/not 0 you can get by without the shift.  IF you want to index a bit you could: bit = (char & 0xF0) >> 7; gets the msb of a char. You could even leave out the right shift and do a test on 0. bit = char & 0xF0; if the bit is set the result will be > 0; obviousuly you need to change the mask to get different bits (NB: the 0xF is the bit mask if it is unclear). It is possible to define numerous masks e.g. #define BIT_0 0x1 // or 1 << 0 #define BIT_1 0x2 // or 1 << 1 #define BIT_2 0x4 // or 1 << 2 #define BIT_3 0x8 // or 1 << 3 etc... This gives you: bit = char & BIT_1; You can use these definitions in the above code to sucessfully index a bit within either a macro or a function. To set a bit: char |= BIT_2; To clear a bit: char &= ~BIT_3 To toggle a bit char ^= BIT_4 This help?  To query state of bit with specific index: int index_state = variable & ( 1 << bit_index ); To set bit: varabile |= 1 << bit_index; To restart bit: variable &= ~( 1 << bit_index );  Have a look at the answers to this question.  Following on from what Kyle has said you can use a macro to do the hard work for you. It is possible. To set the nth bit use OR: x |= (1 << 5); // sets the 5th-from right To clear a bit use AND: x &= ~(1 << 5); // clears 5th-from-right To flip a bit use XOR: x ^= (1 << 5); // flips 5th-from-right Or... #define GetBit(var bit) ((var & (1 << bit)) != 0) // Returns true / false if bit is set #define SetBit(var bit) (var |= (1 << bit)) #define FlipBit(var bit) (var ^= (1 << bit)) Then you can use it in code like: int myVar = 0; SetBit(myVar 5); if (GetBit(myVar 5)) { // Do something } This is very helpful thanks very much.  Individual bits can be indexed as follows. Define a struct like this one: struct { unsigned bit0 : 1; unsigned bit1 : 1; unsigned bit2 : 1; unsigned bit3 : 1; unsigned reserved : 28; } bitPattern; Now if I want to know the individual bit values of a var named ""value"" do the following: CopyMemory( &input &value sizeof(value) ); To see if bit 2 is high or low: int state = bitPattern.bit2; Hope this helps."11,A,Is it possible to use .htaccess to send six digit number URLs to a script but handle all other invalid URLs as 404s? Is it possible to use .htaccess to process all six digit URLs by sending them to a script but handle every other invalid URL as an error 404? For example: http://mywebsite.com/132483 would be sent to: http://mywebsite.com/scriptname.php?no=132483 but http://mywebsite.com/132483a or http://mywebsite.com/asdf would be handled as a 404 error. I presently have this working via a custom PHP 404 script but it's kind of kludgy. Seems to me that .htaccess might be a more elegant solution but I haven't been able to figure out if it's even possible. <IfModule mod_rewrite.c> RewriteEngine on RewriteRule ^([0-9]{6})$ scriptname.php?no=$1 [L] </IfModule> To preserve the clean URL http://mywebsite.com/132483 while serving scriptname.php use only [L]. Using [R=301] will redirect you to your scriptname.php?no=xxx You may find this useful http://www.addedbytes.com/download/mod_rewrite-cheat-sheet-v2/pdf/ Thank you so much for your help! That works perfectly.  In your htaccess file put the following RewriteEngine On RewriteRule ^([0-9]{6})$ /scriptname.php?no=$1 [L] The first line turns the mod_rewrite engine on. The () brackets put the contents into $1 - successive () would populate $2 $3... and so on. The [0-9]{6} says look for a string precisely 6 characters long containing only characters 0-9. The [L] at the end makes this the last rule - if it applies rule processing will stop. Oh the ^ and $ mark the start and end of the incoming uri. Hope that helps! Doh. You beat me to it by a few seconds. :) The R=301 is optional. It should only be used if you don't want the numeric version visible in the URL (or to spiders). yeah - i think 301 is the default anyway  Yes it's possible with mod_rewrite. There are tons of good mod_rewrite tutorials online a quick Google search should turn up your answer in no time. Basically what you're going to want to do is ensure that the regular expression you use is just looking for digits and no other characters and to ensure the length is 6. Then you'll redirect to scriptname.?no= with the number you captured. Hope this helps!12,A,"Are there many users of PRADO out there? After making some comments I've been inspired to get some feedback on the PHP MVC framework PRADO. I've been using it for over a year now and I've very much enjoyed working with it however I notice that throughout Stack Overflow it doesn't seem to rate a mention when symfony or CakePHP are being talked about as potential candidates for a framework. Is anybody using Stack Overflow using PRADO now? If so how do you find it? Has anyone used it in the past but left it behind and if so why? Can anybody appraise its strengths and weaknesses against Cake or symfony? I found that the active controls were pretty slick. It makes doing all kinds of ajaxy things really easy. Unfortunately when you need to do something slightly different it's pretty obfuscated and difficult to figure out what's up. I felt like I often got something simple and great working and then one small additional requirement would require me to tear the whole thing apart and come up with a much more complicated solution.  I've played with PRADO some but I felt that if I'm going to be forced into post-back-hell i might as well do it on the platform that it was built for in the beginning - .NET other then that PRADO is relatively ""untalked"" about in the blogs etc. I don't know why really though. What is it about post-backs that you find hellish and what alternatives are you aware of? I've found post-backs to be pretty clumsy in the time i've worked with PRADO and with .NET but haven't really known any alterantives.  Prado is dead now. Also the documentation is poor.  I think Prado never really caught on because it's an event-driven framework which is a bit hard to wrap your head around. Especially for the many PHP developers coming from a more procedural background.  The first time I looked into PRADO I spent about 10 days using it and kept saying to myself: ""This framework is amazing!"". A couple of months later I started working on a big project where the customer had chosen to use PRADO... And Hell began... As long as we kept using PRADO's base components everything was perfect and development was fast. But as soon as the customer wanted an out-of-the-box thing we literaly spent 2 to 3 times the amount of time we would have done it with another framework. And I'm not talking about big customizations. The PRADO framework forces the application to have a particular structure and workflow. If that logic is not working for you then check out another framework. This ended up being my exact experience. It was great for a while then became a nightmare. I wish you'd answered this a year ago! Would've saved me a couple of months of horror ;)  PRADO would have been my choice for a framework if I hadn't run across QCodo. I like the event-driven approach -- QCodo just suits me more. I too use QCodo and love it. It's QCubed (http://qcu.be) fork is the one that's actively developed at the moment. Nice to know there's an actively developed fork - I was starting to give up hope on it.  We are working with PRADO framework since 4 years. We are developing huge (+4000 programs) web apps for e-Goverment with Oraracle and MySql databases containing more than 60 millon records. As infrastructure for development we use SVN+TRAC+ our own tools for project control AND phpEdit w/tortoiseSVN as client tools. Currently we are thinking on changing to Yii."13,A,"On a wiki is it acceptable to restructure people's headings? I dislike the fact that people use h3 rather h1 or skip a level just because of the way it looks. IMHO headings signify document structure rather than layout. The layout is just a side effect customizable through themes. Having ""wrong"" heading levels also makes it more difficult to generate proper TOCs and/or export to other formats. While wiki gardening is it acceptable to completely restructure other people's pages to use proper nested heading levels? The good thing about wikis is that you are free to edit it! if you think restructuring it makes it better then by all means do it.  Yes it is acceptable to restructure peoples' headers to following the site's established or de-fact standards. When you edit someone's post the community will decide if your edit was deserving or superficial by either rolling back your changes keeping your changes or editing your changes. See the Wikipedia Manual of Style  As others have mentioned it's best to start a conversation on the discussion page for the article if what you're doing is a major over-haul. If someone has one H1 and two H3s in a fairly short article I don't think it's a big deal to change those. When in doubt start a discussion. Another thing that might be pertinent is if you can't find editing or style guidelines regarding use of headers their effect on the table of contents for an article the styles used to control their appearance etc then you could always start discussion about that. If there is disagreement about what is the correct layout for an article then the wiki will be really hard to read. Whatever is decided by such a discussion (make sure to get consensus and quorum or whatever's appropriate for wiki-spanning decisions on the wiki in question) can more assuredly be unilaterally edited into any article not conforming.  It depends what the coding/layout guidelines state. If they say to use h3 when they have used h2 then you could edit it. However if you are editing someone else's post based solely on your personal preferences then I would say that's wrong. Aside from that you can always ask the author if you can find them.  I agree with Teifion. It depends on the individual wiki's guidelines and/or rules but also the rules of conduct. On some sites it's ok to edit whatever page you want on some sites it is considered best to notify the original (or maintaining) author. The other thing that I'd say is that it would be more courteous to present your arguments to the initial page author or most recent maintaining author. You could also add a comment on the discussion page stating your opinion. Because while it is generally considered good style to use H1 first then H2 based on content level and not skip it really is a matter of personal preference. Yay someone agrees with me!"14,A,"How do I do an Upsert Into Table? I have a view that has a list of jobs in it with data like who they're assigned to and the stage they are in. I need to write a stored procedure that returns how many jobs each person has at each stage. So far I have this (simplified): DECLARE @ResultTable table ( StaffName nvarchar(100) Stage1Count int Stage2Count int ) INSERT INTO @ResultTable (StaffName Stage1Count) SELECT StaffName COUNT(*) FROM ViewJob WHERE InStage1 = 1 GROUP BY StaffName INSERT INTO @ResultTable (StaffName Stage2Count) SELECT StaffName COUNT(*) FROM ViewJob WHERE InStage2 = 1 GROUP BY StaffName The problem with that is that the rows don't combine. So if a staff member has jobs in stage1 and stage2 there's two rows in @ResultTable. What I would really like to do is to update the row if one exists for the staff member and insert a new row if one doesn't exist. Does anyone know how to do this or can suggest a different approach? I would really like to avoid using cursors to iterate on the list of users (but that's my fall back option). I'm using SQL Server 2005. Edit: @Lee: Unfortunately the InStage1 = 1 was a simplification. It's really more like WHERE DateStarted IS NOT NULL and DateFinished IS NULL. Edit: @BCS: I like the idea of doing an insert of all the staff first so I just have to do an update every time. But I'm struggling to get those UPDATE statements correct. The following query on your result table should combine the rows again. This is assuming that InStage1 and InStage2 are never both '1'. select distinct(rt1.StaffName) rt2.Stage1Count rt3.Stage2Count from @ResultTable rt1 left join @ResultTable rt2 on rt1.StaffName=rt2.StaffName and rt2.Stage1Count is not null left join @ResultTable rt3 on rt1.StaffName=rt2.StaffName and rt3.Stage2Count is not null  I managed to get it working with a variation of BCS's answer. It wouldn't let me use a table variable though so I had to make a temp table. CREATE TABLE #ResultTable ( StaffName nvarchar(100) Stage1Count int Stage2Count int ) INSERT INTO #ResultTable (StaffName) SELECT StaffName FROM ViewJob GROUP BY StaffName UPDATE #ResultTable SET Stage1Count= ( SELECT COUNT(*) FROM ViewJob V WHERE InStage1 = 1 AND V.StaffName = @ResultTable.StaffName COLLATE Latin1_General_CI_AS GROUP BY V.StaffName) Stage2Count= ( SELECT COUNT(*) FROM ViewJob V WHERE InStage2 = 1 AND V.StaffName = @ResultTable.StaffName COLLATE Latin1_General_CI_AS GROUP BY V.StaffName) SELECT StaffName Stage1Count Stage2Count FROM #ResultTable DROP TABLE #ResultTable  To get a real ""upsert"" type of query you need to use an if exists... type of thing and this unfortunately means using a cursor. However you could run two queries one to do your updates where there is an existing row then afterwards insert the new one. I'd think this set-based approach would be preferable unless you're dealing exclusively with small numbers of rows.  IIRC there is some sort of ""On Duplicate"" (name might be wrong) syntax that lets you update if a row exists (MySQL) Alternately some form of: INSERT INTO @ResultTable (StaffName Stage1Count Stage2Count) SELECT StaffName00 FROM ViewJob GROUP BY StaffName UPDATE @ResultTable Stage1Count= ( SELECT COUNT(*) AS count FROM ViewJob WHERE InStage1 = 1 @ResultTable.StaffName = StaffName) UPDATE @ResultTable Stage2Count= ( SELECT COUNT(*) AS count FROM ViewJob WHERE InStage2 = 1 @ResultTable.StaffName = StaffName) the ""on duplicate"" you mention is INSERT ... ON DUPLICATE KEY UPDATE and the documentation is here: http://dev.mysql.com/doc/refman/5.0/en/insert-on-duplicate.html  You could just check for existence and use the appropriate command. I believe this really does use a cursor behind the scenes but it's the best you'll likely get: IF (EXISTS (SELECT * FROM MyTable WHERE StaffName = @StaffName)) begin UPDATE MyTable SET ... WHERE StaffName = @StaffName end else begin INSERT MyTable ... end SQL2008 has a new MERGE capability which is cool but it's not in 2005.  Actually I think you're making it much harder than it is. Won't this code work for what you're trying to do? SELECT StaffName SUM(InStage1) AS 'JobsAtStage1' SUM(InStage2) AS 'JobsAtStage2' FROM ViewJob GROUP BY StaffName"15,A,"How do I install modperl under OS X Leopard's default Apache 2? My attempts to install modperl under the default vanilla Leopard Apache 2 have failed and all I can find online are variations on this: I would like if possible not to rely on MacPorts or Fink though if they can be made to work with the default Apache 2 install that would probably be ok. Get the latest mod_perl and set the following var: export ARCHFLAGS=""-arch x86_64"" Compile/install as usual. Taken from this post ""Building mod_perl2 on Leopard"" which also links to further details on how to get Apache2::Request (libapreq) working as well. - (Not that I've been able to test it since I'm personally back on Tiger running Apache 1.3!) (And let's see if stackoverflow manages to lift this answer to the top since it is the only ""correct"" answer) the x86_64 architecture will not work on PPC systems that run Leopard. ppc and i386 are the preferred build types in MacPorts presently.  I asked a very similar question a few days ago and got some good answers: ""How do I use a vendor Apache with a self-compiled Perl and mod_perl?""  Why not just give up and build/install your own or port versions of perl apache2 and mod_perl2? Probably easier than fighting with it. (Worked for me.) (as per comment) Mmmkay! Sorry I didn't intend that to be snarky or imply that it's not a valid question. I guess I'll delete this (if I can.) Would it be useful to edit the question to add your rational rationale for not having a separate installation? Because it's not my machine. Because it has a whole load of stuff already set up and running under the default Apache. Because it's a valid question in any case. In the past. for my own purposes I have always ignored the default install. But this time I can't. Mmmkay?  The mc ports install of mod_perl tries to install apache 1.3 even if you specify just the mod perl so thats not a good option. mod_perl2 is available in MacPorts for Apache 2  Try this: http://www.unibia.com/unibianet/node/32 This is a good solution but not for my question since it requires installing a new version of Apache.  Macports has it (think apt-get and the likes on linux but on OS X) (you can see it listed here) Haven't installed myself though.... But would that work with the default Apache 2 installation? Surely this would install the MacPorts version of Apache first as a dependency? And Perl too in all likelihood from what I remember of previous dalliances_ This does not require the default Apache installation. MacPorts will create its own copy that it can modify as you add/remove packages."16,A,"Perl Regex Match and Removal I have a string which starts with //#... goes upto the newline characater. I have figured out the regex for the which is this ..#([^\n]*). My question is how do you remove this line from a file if the following condition matches I don't think your regex is correct. First you need to start with ^ or else it will match this pattern anywhere on the line. Second the .. should be \/\/ or else it will match any two characters. ^\/\/#[^\n]* is probably what you want. Then do what EricSchaefer says and read the file line by line only writing lines that don't match. -- bmb  You really don't need perl for this. sed '/^\/\/#/d' inputfile > outputfile I <3 sed.  Iterate over each line in the file and skip the line if it matches the pattern:  my $fh = new FileHandle 'filename' or die ""Failed to open file - $!""; while (my $line = $fh->getline) { next if $line =~ m{^//#}; print $line; } close $fh; This will print all lines from the file except the line that starts with '//#'.  As others have pointed out if the end goal is only to remove lines starting with //# for performance reasons you are probably better off using grep or sed: grep -v '^\/\/#' filename.txt > filename.stripped.txt sed '/^\/\/#/d' filename.txt > filename.stripped.txt or sed -i '/^\/\/#/d' filename.txt if you prefer in-place editing. Note that in perl your regex would be m{^//#} which matches two slashes followed by a # at the start of the string. Note that you avoid ""backslashitis"" by using the match operator m{pattern} instead of the more familiar /pattern/. Train yourself on this syntax early since it's a simple way to avoid excessive escaping. You could write m{^//#} just as effectively as m%^//#% or m#^//\## depending on what you want to match. Strive for clarity - regular expressions are hard enough to decipher without a prickly forest of avoidable backslashes killing readability. Seriously m/^\/\/#/ looks like an alligator with a chipped tooth and a filling or a tiny ASCII painting of the Alps. One problem that might come up in your script is if the entire file is slurped up into a string newlines and all. To defend against that case use the /m (multiline) modifier on the regex: m{^//#}m This allows ^ to match at the beginning of the string and after a newline. You would think there was a way to strip or match the lines matching m{^//#.*$} using the regex modifiers /g /m and /s in the case where you've slurped the file into a string but you don't want to make a copy of it (begging the question of why it was slurped into a string in the first place.) It should be possible but it's late and I'm not seeing the answer. However one 'simple' way of doing it is: my $cooked = join qq{\n} (grep { ! m{^//} } (split m{\n} $raw)); even though that creates a copy instead of an in-place edit on the original string $raw.  Read the file line by line and only write those lines to a new file that don't match the regex. You cannot just remove a line.  Does it start at the begining of a line or can it appear anywhere? If the former s/old/new is what you want. If the latter I'll have to figure that out. I suspect that back referances could be used somehow.  Try the following: perl -ne 'print unless m{^//#}' input.txt > output.txt If you are using windows you need double quotes instead of single quotes. You can do the same with grep grep -v -e '^//#' input.txt > output.txt  To filter out all the lines in a file that match a certain regex: perl -n -i.orig -e 'print unless /^#/' file1 file2 file3 The '.orig' after the -i switch creates a backup of the file with the given extension (.orig). You can skip it if you don't need a backup (just use -i). The -n switch causes perl to execute your instructions (-e ' ... ') for each line in the file. The line is stored in $_ (which is also the default argument for many instructions in this case: print and regex matching). Finally the argument to the -e switch says ""print the line unless it matches a # character at the start of the line. PS. There is also a -p switch which behaves like -n except the lines are always printed (good for searching and replacing)  Your regex is badly chosen on several points: Instead of matching two slashes specifically you use .. to match two characters that can be anything at all presumably because you dont know how to match slashes when youre also using them as delimiters. (Actually dots match almost anything as well see in #3.) Within a slash-delimited regex literal // you can match slashes simply by protecting them with backslashes eg. /\/\//. The nicer variant however is to use the longer form of regex literal m// where you can choose the delimiter eg. m!!. Since you use something other than slashes for delimitation you can then write them without escaping them: m!//!. See perldoc perlop. Its not anchored to the start of the string so it will match anywhere. Use the ^ start-of-string assertion in front. You wrote [^\n] to match any character except newline when there is a much simpler way to write that which is just the . wildcard. It does exactly that  match any character except newline. You are using parentheses to group a part of the match but the group is neither quantified (you are not specifying that it can match any other number of times than exactly once) nor are you interested in keeping it. So the parentheses are superfluous. Altogether that makes it m!^//#.*!. But putting an uncaptured .* (or anything with a * quantifier) at the end of a regex is meaningless since it never changes whether a string will match or not: the * is happy to match nothing at all. So that leaves you with m!^//#!. As for removing the line from the file as everyone else explained read it in line by line and print all the lines you want to keep back to another file. If you are not doing this within a larger program use perls command line switches to do it easily: perl -ni.bak -e'print unless m!^//#!' somefile.txt Here the -n switch makes perl put a loop around the code you provide which will read all the files you pass on the command line in sequence. The -i switch (for in-place) says to collect the output from your script and overwrite the original contents of each file with it. The .bak parameter to the -i option tells perl to keep a backup of the original file in a file named after the original file name with .bak appended. For all of these bits see perldoc perlrun. If you want to do this within the context of a larger program the easiest way to do it safely is to open the file twice once for reading and separately with IO::AtomicFile another time for writing. IO::AtomicFile will replace the original file only if its successfully closed. Excellent detailed answer. Thank you very much for the very informative post!"17,A,PLS-00306 error on call to cursor I think I might be missing something here. Here is the relevant part of the trigger: CURSOR columnNames (inTableName IN VARCHAR2) IS SELECT COLUMN_NAME FROM USER_TAB_COLUMNS WHERE TABLE_NAME = inTableName; /* Removed for brevity */ OPEN columnNames('TEMP'); And here is the error message that I'm getting back  27/20 PLS-00306: wrong number or types of arguments in call to 'COLUMNNAMES' 27/2 PL/SQL: Statement ignored If I am understanding the documentation correctly that should work but since it is not I must be doing something wrong. Any ideas? @Matthew - I appreciate the help but the reason that I am confused is because this bit of code isn't working for me and is raising the errors referenced. We have other triggers in the database with code almost exactly the as that so I'm not sure if it is something that I did wrong or something with how I am trying to store the trigger etc. @Matthew - Well now I get to feel embarrassed. I did a copy/paste of the code that you provided into a new trigger and it worked fine. So I went back into the original trigger and tried it and received the error message again except this time I started to delete stuff out of the trigger and after getting rid of this line FOR columnName IN columnNames LOOP Things saved fine. So it turns out that where I thought the error was wasn't actually were the error was. To clarify the cause of the issue. As you state OPEN columnNames('TEMP'); worked while FOR columnName IN columnNames LOOP did not. The FOR statement would work fine if it also included the parameter like so: FOR columnName IN columnNames('TEMP') LOOP You don't show the code where you fetch the rows so I can't tell your purpose but where I work OPEN is commonly used to fetch the first row (in this case the first column name of the given table) while the FOR is used to iterate through all returned rows. @Rob's comment. I'm not allowed to comment so updating here instead. The missing parameter is what I describe above. You added a response stating you simply deleted the FOR loop. It did not look like you at the time understood why deleting it made a difference. Which is why I attempted to explain since depending on your need the FOR loop might be a better solution. Frode - The reason I was getting the error was because I was trying to make a call to a parametrized query in the FOR LOOP without providing a parameter.  @Rob If you cut/paste the code I have here does it work? How/where are you calling your code? its in a trigger is it? The query you have written here is that actually the code producing the error or just an example (eg can you reproduce the error with the query you have above)  Works fine for me. create or replace procedure so_test_procedure as CURSOR columnNames (inTableName IN VARCHAR2) IS SELECT COLUMN_NAME FROM USER_TAB_COLUMNS WHERE TABLE_NAME = inTableName; BEGIN OPEN columnNames('TEMP'); CLOSE columnNames; END; procedure so_test_procedure Compiled. execute so_test_procedure(); anonymous block completed18,A,"Transform Columns into Rows I have a very simple problem which requires a very quick and simple solution in SQL Server 2005. I have a table with x Columns. I want to be able to select one row from the table and then transform the columns into rows. TableA Column1 Column2 Column3 SQL Statement to ruturn ResultA Value of Column1 Value of Column2 Value of Column3 @Kevin: I've had a google search on the topic but alot of the example where overly complex for my example are you able to help further? @Mario: The solution I am creating has 10 columns which stores the values 0 to 6 and I must work out how many columns have the value 3 or more. So I thought about creating a query to turn that into rows and then using the generated table in a subquery to say count the number of rows with Column >= 3 Look at my blog: http://sql-tricks.blogspot.com/2011/04/sql-server-rows-transpose.html Hm... now that's something I've never tried. The solutions that came to my mind are all too tricky and too ugly and I'm sure there's something much more elegant. I've done a search on UNPIVOT too and it looks like that's the path you should go. I'll take this one as a puzzle to solve during the next days. If you have a fixed set of columns and you know what they are you can basically do a series of subselects (SELECT Column1 AS ResultA FROM TableA) as R1 and join the subselects. All this in a single query.  UNION should be your friend: SELECT Column1 FROM table WHERE idColumn = 1 UNION ALL SELECT Column2 FROM table WHERE idColumn = 1 UNION ALL SELECT Column3 FROM table WHERE idColumn = 1 but it can also be your foe on large result sets.  You should take a look at the UNPIVOT clause. Update1: GateKiller strangely enough I read an article (about something unrelated) about it this morning and I'm trying to jog my memory where I saw it again had some decent looking examples too. It'll come back to me I'm sure. Update2: Found it: http://weblogs.sqlteam.com/jeffs/archive/2008/04/23/unpivot.aspx  SELECT IDColumn NumberOfColumnsGreaterThanThree = (CASE WHEN Column1 >= 3 THEN 1 ELSE 0 END) + (CASE WHEN Column2 >= 3 THEN 1 ELSE 0 END) + (Case WHEN Column3 >= 3 THEN 1 ELSE 0 END) FROM TableA;  I'm not sure of the SQL Server syntax for this but in MySQL I would do SELECT IDColumn ( IF( Column1 >= 3 1 0 ) + IF( Column2 >= 3 1 0 ) + IF( Column3 >= 3 1 0 ) + ... [snip ] ) AS NumberOfColumnsGreaterThanThree FROM TableA; EDIT: A very (very) brief Google search tells me that the CASE statement does what I am doing with the IF statement in MySQL. You may or may not get use out of the Google result I found FURTHER EDIT: I should also point out that this isn't an answer to your question but an alternative solution to your actual problem.  I had to do this for a project before. One of the major difficulties I had was explaining what I was trying to do to other people. I spent a ton of time trying to do this in SQL but I found the pivot function woefully inadequate. I do not remember the exact reason why it was but it is too simplistic for most applications and it isn't full implemented in MS SQL 2000. I wound up writing a pivot function in .NET. I'll post it here in hopes it helps someone someday.  ''' <summary>  ''' Pivots a data table from rows to columns  ''' </summary>  ''' <param name=""dtOriginal"">The data table to be transformed</param>  ''' <param name=""strKeyColumn"">The name of the column that identifies each row</param>  ''' <param name=""strNameColumn"">The name of the column with the values to be transformed from rows to columns</param>  ''' <param name=""strValueColumn"">The name of the column with the values to pivot into the new columns</param>  ''' <returns>The transformed data table</returns>  ''' <remarks></remarks>  Public Shared Function PivotTable(ByVal dtOriginal As DataTable ByVal strKeyColumn As String ByVal strNameColumn As String ByVal strValueColumn As String) As DataTable  Dim dtReturn As DataTable  Dim drReturn As DataRow  Dim strLastKey As String = String.Empty  Dim blnFirstRow As Boolean = True  ' copy the original data table and remove the name and value columns  dtReturn = dtOriginal.Clone  dtReturn.Columns.Remove(strNameColumn)  dtReturn.Columns.Remove(strValueColumn)  ' create a new row for the new data table  drReturn = dtReturn.NewRow  ' Fill the new data table with data from the original table  For Each drOriginal As DataRow In dtOriginal.Rows  ' Determine if a new row needs to be started  If drOriginal(strKeyColumn).ToString <> strLastKey Then  ' If this is not the first row the previous row needs to be added to the new data table  If Not blnFirstRow Then  dtReturn.Rows.Add(drReturn)  End If  blnFirstRow = False  drReturn = dtReturn.NewRow  ' Add all non-pivot column values to the new row  For Each dcOriginal As DataColumn In dtOriginal.Columns  If dcOriginal.ColumnName <> strNameColumn AndAlso dcOriginal.ColumnName <> strValueColumn Then  drReturn(dcOriginal.ColumnName.ToLower) = drOriginal(dcOriginal.ColumnName.ToLower)  End If  Next  strLastKey = drOriginal(strKeyColumn).ToString  End If  ' Add new columns if needed and then assign the pivot values to the proper column  If Not dtReturn.Columns.Contains(drOriginal(strNameColumn).ToString) Then  dtReturn.Columns.Add(drOriginal(strNameColumn).ToString drOriginal(strValueColumn).GetType)  End If  drReturn(drOriginal(strNameColumn).ToString) = drOriginal(strValueColumn)  Next  ' Add the final row to the new data table  dtReturn.Rows.Add(drReturn)  ' Return the transformed data table  Return dtReturn  End Function"19,A,"What's the difference between a ""script"" and an ""application""? I'm referring to distinctions such as in this answer. As programmer who's worked in many languages this seems to be C Java and other compiled language snobbery. I'm not looking for reenforcement of my opinion or hand-wavy answers. Rather I'm genuinely want to what technical differences is being referred to. (And I use C in my day job so I'm not just being defensive.) And what's the difference between those and a program and a solution (-: Usually it is ""script"" versus ""program"". I am with you that this distinction is mostly ""compiled language snobbery"" or to quote Larry Wall and take the other side of the fence ""a script is what the actors have a programme is given to the audience"". +1 ""a script is what the actors have a programme is given to the audience"".  An application is big and will be used over and over by people and maybe sold to a customer. A script starts out small stays small if you're lucky is rarely sold to a customer and might either be run automatically or fall into disuse.  Taking perl as an example you can write perl scripts or perl applications. A script would imply a single file or a single namespace. (e.g. updateFile.pl). An application would be something made up of a collection of files or namespaces/classes (e.g. an OO-designed perl application with many .pm module files).  A script generally runs as part of a larger application inside a scripting engine eg. JavaScript -> Browser This is in contrast to both traditional static typed compiled languages and to dynamic languages where the code is intended to form the main part of the application.  What about: Script: A script is text file (or collection of text files) of programming statements written in a language which allows individual statements written in it to be interpreted to machine executable code directly before each is executed and with the intention of this occurring. Application: An application is any computer program whose primary functionality involves providing service to a human Actor. A script-based program written in a scripting language can therefore theoretically have its textual statements altered while the script is being executed (at great risk of  of course). The analogous situation for compiled programs is flipping bits in memory. Any takers? :)  John Ousterhout (the inventor of TCL) has a good article at http://www.tcl.tk/doc/scripting.html where he proposes a distinction between system programming languages (for implementing building blocks emphasis on correctness type safety) vs scripting languages (for combining building blocks emphasis on responsiveness to changing environments and requirements easy conversion in and out of textual representations). If you go with that categorisation system then 99% of programmers are doing jobs that are more appropriate to scripting languages than to system programming languages. Yet here we are some time after that. And two years later I updated the link because I thought this was a good reference. PS on the mechanics of Stack Overflow: I think the article is relevant and notable enough to be worth mentioning but there's an uncomfortable ""the party's over everyone's gone home"" feeling about adding to an topic whose answers were clustered into 48 minutes four months ago.  I would say a script is usually a set of commands or instructions written in plain text that are executed by a hosting application (browser command interpreter or shell...). It does not mean it's not powerfull or not compiled in some way when it's actually executed. But a script cannot do anything by itself it's just plain text. By nature it can be a fragment only needing to be combined to build a program or an application but extended and fully developed scripts or set of scripts can be considered programs or applications when executed by the host just like a bunch of source files can become an application once compiled.  Actually the difference between a script ( or a scripting language) and an application is that a script don't require it to be compiled into machine language.. You run the source of the script with an interpreter.. A application compiles the source into machine code so that you can run it as a stand alone application.  I would say that an application tends to be used interactively where a script would run its course suitable for batch work. I don't think it's a concrete distinction. Good point but doesn't address why there are 'programming languages' vs 'scripting languages'. You can write non-interactive jobs in either. I think I agree with this answer. I'm realizing that it might have been better to ask about scripting vs. programming languages. I like this definition. Would not have thought of it otherwise.  It's often just a semantic argument or even a way of denigrating certain programming languages. As far as I'm concerned a ""script"" is a type of program and the exact definition is somewhat vague and varies with context. I might use the term ""script"" to mean a program that primarily executes linearly rather than with lots of sequential logic or subroutines much like a ""script"" in Hollywood is a linear sequence of instructions for an actor to execute. I might use it to mean a program that is written in a language embedded inside a larger program for the purpose of driving that program. For example automating tasks under the old Mac OS with AppleScript or driving a program that exposes itself in some way with an embedded TCL interface. But in all those cases a script is a type of program. The term ""scripting language"" has been used for dynamically interpreted (sometimes compiled) languages usually these have a lot of common features such as very high level instructions built in hashes and arbitrary-length lists and other high level data structures etc. But those languages are capable of very large complicated modular well-designed programs so if you think of a ""script"" as something other than a program that term might confuse you. See also Is it a Perl program or a Perl script? in perlfaq1.  This is an interesting topic and I don't think there are very good guidelines for the differentiating a ""script"" and a ""application."" Let's take a look at some Wikipedia articles to get a feel of the distinction. Script (Wikipedia -> Scripting language): A scripting language script language or extension language is a programming language that controls a software application. ""Scripts"" are often treated as distinct from ""programs"" which execute independently from any other application. At the same time they are distinct from the core code of the application which is usually written in a different language and by being accessible to the end user they enable the behavior of the application to be adapted to the user's needs. Application (Wikipedia -> Application software -> Terminology) In computer science an application is a computer program designed to help people perform a certain type of work. An application thus differs from an operating system (which runs a computer) a utility (which performs maintenance or general-purpose chores) and a programming language (with which computer programs are created). Depending on the work for which it was designed an application can manipulate text numbers graphics or a combination of these elements. Reading the above entries seems to suggest that the distinction is that a script is ""hosted"" by another piece of software while an application is not. I suppose that can be argued such as shell scripts controlling the behavior of the shell and perl scripts controlling the behavior of the interpreter to perform desired operations. (I feel this may be a little bit of a stretch so I may not completely agree with it.) When it comes down to it it is in my opinion that the colloquial distinction can be made in terms of the scale of the program. Scripts are generally smaller in scale when compared to applications. Also in terms of the purpose a script generally performs tasks that needs taken care of say for example build scripts that produce multiple release versions for a certain piece of software. On the otherhand applications are geared toward providing functionality that is more refined and geared toward an end user. For example Notepad or Firefox.  Script to me implies line-by-line interpretation of the code. You can open a script and view its programmer-readable contents. An application implies a stand-alone compiled executable.  A script tends to be a series of commands that starts runs and terminates. It often requires no/little human interaction. An application is a ""program""... it often requires human interaction it tends to be larger.  Traditionally a program is compiled and a script is interpreted but that is not really important anymore. You can generate a compiled version of most scripts if you really want to and other 'compiled' languages like Java are in fact interpreted (at the byte code level.) A more modern definition might be that a program is intended to be used by a customer (perhaps an internal one) and thus should include documentation and support while a script is primarily intended for the use of the author. The web is an interesting counter example. We all enjoy looking things up with the Google search engine. The bulk of the code that goes into creating the 'database' it references is used only by its authors and maintainers. Does that make it a script?  A scripting language doesn't have a standard library or platform (or not much of one). It's small and light designed to be embedded into a larger application. Bash and Javascript are great examples of scripting languages because they rely absolutely on other programs for their functionality. Using this definition a script is code designed to drive a larger application (suite). A Javascript might call on Firefox to open windows or manipulate the DOM. A Bash script executes existing programs or other scripts and connects them together with pipes. You also ask why not scripting languages so: Are there even any unit-testing tools for scripting languages? That seems a very important tool for ""real"" applications that is completely missing. And there's rarely any real library bindings for scripting languages. Most of the times scripts could be replaced with a real light language like Python or Ruby anyway. Python and Ruby are sometimes slighted as ""scripting languages"". ;-) Python and Ruby are scripting languages IMHO :) Python and Ruby are not scripting languages unless ""scripting"" has been redefined to meaninglessness. and what is a ""Javacript Application"" then. Snakeoil? :P Yes since it's not possible to write an entire program using Javascript. You have to write (or re-use) a separate platform like Firefox or Rhino. These are not part of the Javascript language and thus do not count when comparing languages. Anti-JavaScript snobbery! Every language uses a separate platform on some level to parse and execute the code. JavaScript is just another language capable of writing ""programs"" or ""scripts"". When you talk about a web app as a ""javascript application"" you are implicitly comparing to a ""javascript not-an-application"" or a simple webpage. I think ""web application"" is a more correct and less confusing term - JS apps also need HTML and CSS which aren't part of JS. @Jason: Sorry but not all languages use separate platforms. Machine code is so named because it works directly on the machine and C compilers make machine code. Indeed. But however you can generate html entirely with javascript in the web browser. and you can have a user enter a singular javascript driven page and never leave it again for the entire time of their stay. ( GMail for example. I would hardly call gmail ""merely a web page with a script"" ) @Jason Please point me to documentation on Javascript's standard library. Especially topics like reading files or printing output. @Kent: I'm not denying that it's possible to build large or complex systems in a scripting language. But it's still *just* a scripting language. It's relying entirely on the browser's defined evironment and cannot run outside it. For example you can't run GMail in Rhino or Apple Dashboard. @John Okay the lack of I/O in JavaScript is a bit of problem isn't it? :) You _could_ make the argument that the code embedded in the machine is a ""platform"" and that the JS interpreter is the equivalent of the standard I/O library for C. But pretty soon we'd re-define ourselves into stupidity. Im going to complain I cant run C on my coffee machine or that objects instantiated in the virtual reality of the computer cant survive in the real world. And thus everything is a script. :p ( reality has a different API. Damn things undocumented ) Turtles all the way down. Ah but nobody claimed that you could run machine code on arbitrary machines! Only that machine code interacts with the machine without anything in between. ;-) And that was never the real issue anyway because interpretation is not the critical characteristic of scripting languages. X-P Man I would love to get a peek at the API docs for reality. Maybe just the section on DNA. Now that would be some hard-core programming! ""A scripting language doesn't have a standard library or platform (or not much of one)."" So what does that make PERL? That's got a significantly larger set of libraries than C++ so it must be a programming language. By contrast C (no STL) must be a scripting language. ""PERL...must be a programming language"" -- I agree. ""C...must be a scripting language"" -- why? Haven't you heard of libc?  First of all I would like to make it crystal clear that a script is a program. In other words a script is a set of instructions. Program: A set of instructions which is going to be compiled is known as a Program. Script: A set of instructions which is going to be interpreted is known as a Script.  Personally I think the separation is a step back from the actual implementation. In my estimation an application is planned. It has multiple goals it has multiple deliverables. There are tasks set aside at design time in advance of coding that the application must meet. A script however is just thrown together as suits and little planning is involved. Lack of proper planning does not however downgrade you to a script. Possibly it makes your application a poorly organized collection of poorly planned scripts. Further more an application can contain scripts that aggregated comprise the whole. But a script can only reference an application. Your definition of a script (""thrown together as suits and little planning is involved"") does not match my experience programming and maintaining them. Scripts can be ""thrown together"" but in my case they take about as much planning as writing a function in C.  An application is a collection of scripts geared toward a common set of problems. A script is a bit of code for performing one fairly specific task. IMO the difference has nothing whatsoever to do with the language that's used. It's possible to write a complex application with bash and it's possible to write a simple script with C++."20,A,"How to get Emacs to unwrap a block of code? Say I have a line in an emacs buffer that looks like this: foo -option1 value1 -option2 value2 -option3 value3 \ -option4 value4 ... I want it to look like this: foo -option1 value1 \ -option2 value2 \ -option3 value3 \ -option4 value4 \ ... I want each option/value pair on a separate line. I also want those subsequent lines indented appropriately according to mode rather than to add a fixed amount of whitespace. I would prefer that the code work on the current block stopping at the first non-blank line or line that does not contain an option/value pair though I could settle for it working on a selected region. Anybody know of an elisp function to do this? In this case I would use a macro. You can start recording a macro with C-x ( and stop recording it with C-x ). When you want to replay the macro type C-x e. In this case I would type C-a C-x ( C-s v a l u e C-f C-f \ RET SPC SPC SPC SPC C-x ) That would record a macro that searches for ""value"" moves forward 2 inserts a slash and newline and finally spaces the new line over to line up. Then you could repeat this macro a few times. EDIT: I just realized your literal text may not be as easy to search as ""value1"". You could also search for spaces and cycle through the hits. For example hitting C-s a few times after the first match to skip over some of the matches. Note: Since your example is ""ad-hoc"" this solution will be too. Often you use macros when you need an ad-hoc solution. One way to make the macro apply more consistently is to put the original statement all on one line (can also be done by a macro or manually). EDIT: Thanks for the comment about ( versus C-( you were right my mistake! C-x ( runs the command kmacro-start-macro not C-x C-(. Same for C-x ) etc  Nobody had what I was looking for so I decided to dust off my elisp manual and do it myself. This seems to work well enough though the output isn't precisely what I asked for. In this version the first option goes on a line by itself instead of staying on the first line like in my original question. (defun tcl-multiline-options () ""spread option/value pairs across multiple lines with continuation characters"" (interactive) (save-excursion (tcl-join-continuations) (beginning-of-line) (while (re-search-forward "" -[^ ]+ +"" (line-end-position) t) (goto-char (match-beginning 0)) (insert "" \\\n"") (goto-char (+(match-end 0) 3)) (indent-according-to-mode) (forward-sexp)))) (defun tcl-join-continuations () ""join multiple continuation lines into a single physical line"" (interactive) (while (progn (end-of-line) (char-equal (char-before) ?\\)) (forward-line 1)) (while (save-excursion (end-of-line 0) (char-equal (char-before) ?\\)) (end-of-line 0) (delete-char -1) (delete-char 1) (fixup-whitespace)))  Your mode may support this already. In C mode and Makefile mode at least M-q (fill-paragraph) will insert line continuations in the fill-column and wrap your lines. What mode are you editing this in? tcl-mode. Fill-paragraph is not what I want because I don't want the paragraph filled I want it split into lines one option/value pair per line  Personally I do stuff like this all the time. But I don't write a function to do it unless I'll be doing it every day for a year. You can easily do it with query-replace like this: m-x (query-replace "" -option"" ""^Q^J -option"") I say ^Q^J as that is what you'll type to quote a newline and put it in the string. Then just press 'y' for the strings to replace and 'n' to skip the wierd corner cases you'd find. Another workhorse function is query-replace-regexp that can do replacements of regular expressions. and also grep-query-replace which will perform query-replace by parsing the output of a grep command. This is useful because you can search for ""foo"" in 100 files then do the query-replace on each occurrence skipping from file to file."21,A,"Attribute & Reflection libraries for C++? Most mature C++ projects seem to have an own reflection and attribute system i.e for defining attributes which can be accessed by string and are automatically serializable. At least many C++ projects I participated in seemed to reinvent the wheel. Do you know any good open source libraries for C++ which support reflection and attribute containers specifically: Defining RTTI and attributes via macros Accessing RTTI and attributes via code Automatic serialisation of attributes Listening to attribute modifications (e.g. OnValueChanged) I looked at these things for quite a while but they tend to be very heavy-handed. They might prevent you from using inheritance or having strange constructors etc etc. In the end they ended up being too much of a burden instead of a convenience. This approach for exposing members that I now use is quite lightweight and lets you explore a class for serialization or setting all fields called ""x"" to 0 for example. It's also statically determined so is very very fast. No layers of library code or code-gen to worry about messing with the build process. It generalises to hierarchies of nested types. Set your editor up with some macros to automate writing some of these things. struct point { int x; int y; // add this to your classes template <typename Visitor> void visit(Visitor v) { v->visit(x ""x""); v->visit(y ""y""); } }; /** Outputs any type to standard output in key=value format */ struct stdout_visitor { template <typename T> void visit(const T& rhs) { rhs.visit(this); } template <typename Scalar> void visit (const Scalar& s const char* name) { std::cout << name << "" = "" << s << "" ""; } } That is a good approach. I once made a preprocessor that generates `visit()` function templates for you. And then a few functors to serialize and deserialize JSON binary text etc... No ugly macros. https://groups.google.com/d/msg/comp.lang.c++/Ila1Tn09mm4/nJVxl3SzpFUJ While i like it it should be noted that it isn't a general solution - e.g. it breaks as soon as the actual static type isn't known.  This is a notorious weakness of the C++ language in general because the things that would need to be standardized to make reflection implementations portable and worthwhile aren't standard. Calling conventions object layouts and symbol mangling come to mind but there are others as well. The lack of direction from the standard means that compiler implementers will do some things differently which means that very few people have the motivation to write a portable reflection library which means that people who need reflection re-invent the wheel but only just enough for what they need. This happens to ad infinitum and here we are.  Not a general one but QT supports this via a meta compiler and is GPL. My understanding from talking to the QT people was that this isn't possible with pure C++ hence the need for the moc.  Looked at this for a while too. The current easiest solution seems to be BOOST_FUSION_ADAPT_STRUCT. Practically once you have a library/header you only need to add your struct fields into the BOOST_FUSION_ADAPT_STRUCT() macro as the last segment of the code shows. Yes it has restrictions many other people have mentioned. And it does not support listeners directly. The other promising solutions I looked into are CAMP and XRTTI/gccxml however both seem to be a hurdle to bring external tools dependency into your project. Years ago I used perl c2ph/pstruct to dump the meta info from the output of gcc -gstabs that is less intrusive but needs more work though it worked perfectly for me. Does this enable /reflection/? I can see how this adapts a struct to model the Sequence concept of Boost MPL/Fusion so you can iterate over it's fields. However getting the names of fields or using the names to access fields is out of the question IIRC. That's not even speaking about the struct type itself @sehe: As you see the field types are printed by __cxa_demangle(typeid(t).name()). The same function can be applied to struct decoder and array decoder to print all struct types and array types including the very top level. If you think java vm keeps a list of all classes/objects in an internal binary list the meta data this example dumps is roughly at that level. From that low level information (plus runtime address information) you need to write code to access fields (by names in string or char[] format). That would be the other half of the whole reflection picture. Ah. So the most important portion of the answer isn't actually there. Hmmm that was surprising. Anyways I'll **+1 for `c2ph/pstruct`**. Oldschool but will work in most places really. __cxa* isn't exactly portable. And ADAPT_STRUCT is both not very relevant easy to work with and not easily automated. If you're using macros you might as well use macros the full way (and not depend on Boost or GCC ABI) Actually the most important part for me is least runtime cost and least intrusive process. c2ph/pstruct is not very portable as it may appear to be. I had to spend a large effort to fix pstruct for the specific gcc stabs dump. After dumping the type info and retrieve the address info from the built binary only a memory access interface is needed to be built into the runtime. Everything else can be done out of the box (I do embedded programming and use reflection for mainly testing). Thus this ADAPT_STRUCT is the easiest solution so far I can find.  You could have a look at the two tools below. I've never used either of them so I can't tell you how (im)practical they are. XRTTI: Xrtti is a tool and accompanying C++ library which extends the standard runtime type system of C++ to provide a much richer set of reflection information about classes and methods to manipulate these classes and their members. OpenC++: OpenC++ is C++ frontend library (lexer+parser+DOM/MOP) and source-to-source translator. OpenC++ enables development of C++ language tools extensions domain specific compiler optimizations and runtime metaobject protocols. XRTTI still has an updated version for late Linux distributions. OpenC++ seems to be too old.  There is a new project providing reflection in C++ using a totally different approach: CAMP. http://dev.tegesoft.com/projects/camp CAMP doesn't use a precompiler the classes/properties/functions/... are declared manually using a syntax similar to boost.python or luabind. Of course people can use a precompiler like gccxml or open-c++ to generate this declaration if they prefer. It's based on pure C++ and boost headers only and thanks to the power of template meta-programming it supports any kind of bindable entity (inheritance and strange constructors are not a problem for example). It is distributed under the LGPL licence.  This is what you get when C++ meets Reflection: Whatever you choose it'll probably have horrible macros hard to debug code or weird build steps. I've seen one system automatically generate the serialisation code from DevStudio's PDB file. Seriously though for small projects it'll be easier to write save/load functions (or use streaming operators). In fact that might hold for big projects too - it's obvious what's going on and you'd usually need to change code anyway if the structure changes. +1 : for the photo no really it says a lot :D The wagon lettering has a typo should be l337! Unless my app already uses Qt I create streaming operators for each of my classes that need to be saved. +1 : for the photo ;) kidding. I agree with the conclusion. Use handcraft serialisers or use another langage :)  Automatic introspection/reflection toolkit. Use meta compiler like Qt's and adding meta information directly into object files. Intuitive easy to use. No external dependencies. Even allow automatically reflect std::string and then use it in scripts. Please visit IDK"22,A,How to Determine the Installed ASP.NET Version of Host from a Web Page I have a site running in a Windows shared hosting environment. In their control panel for the shared hosting account I have it set to use ASP.NET version 3.0 but it doesn't say 3.5 SP1 specifically. How can I view the installed version running on the server where my website is hosted in an asp.net page? The hint from Brian Boatright by putting the <%=Environment.Version%> on a page and save it in DotNetVersion.aspx upload it at testing on the right URL world great. Sadly it was an too old version for me: 1.1.4322.2443  One way is to throw an exception in Page Load but don't catch it. At the bottom of the page you'll see the version number.  Thanks! I just dropped <%=Environment.Version%> on a page and got 2.0.50727.3053  @Jon Limjap: Unfortunately this tells you the .NET CLR (runtime library) version not the version of the .NET Framework. These two version numbers are not always the same; in particular the .NET Framework 3.0 and 3.5 both use the .NET CLR 2.0. So the OP may indeed have only .NET 2.0 SP1 as the Environment.Version indicates or he may also have the .NET 3.5 SP1 which he is looking for.23,A,Visual Studio equivalent to Delphi bookmarks I use Delphi for many years and although I have now moved on to Visual Studio I still fondly remember numbered bookmarks (CTRL+K+1 to set bookmark 1 CRTL+Q+1 to goto bookmark 1). Is there a Visual Studio equivalent? I'm find the dumb bookmarks in VS a chore after Delphi. I want to bookmark then return to a specific place in the file. Strictly speaking they are known as **Brief bookmarks** after the text editor by UnderWare called BRIEF. http://en.wikipedia.org/wiki/Brief_(text_editor) I use: CTRL-F2 toggle bookmark F2 next bookmark SHIFT-F2 previous bookmark CTRL-SHIFT-F2 clear all bookmarks BTW after using Visual Studio for years I just found about a couple of months ago that you can press ALT and drag mouse to mark a column or a square.  DPack can give you numbered bookmarks in VisualStudio. +1 for ***excellent*** DPack. It gives so many things to Visual Studio - VS developers have no idea how much easier life can be. (Like thinking a command line is great without having used a GUI)  Just to amplify Lars Truijens answer. DPack is a GExperts like plugin for visual studio. I found it great help when moving from the Delphi IDE to Visual Studio.  More a comment on your original question than an actual answer but Delphi has had much easier to remember (and type) keyboard shortcuts than what you quote available for quite some time now: Set bookmark 1: Ctrl-Shift-1 Go to bookmark 1: Ctrl-1 If you ever go back to Delphi this should make your life so much easier! ;) Cheers Oliver  There is a Bookmark Window. Go to menu View/Bookmark Window (Ctrl+K Ctrl+W). In there you can see all your bookmarks and rename them. That is a lot better than just seeing the numbers. For some reason they don't allow to order that list by clicking in a column header but you can drag the bookmarks and accommodate them in the order you want.  Ctrl K + Ctrl K - Add/Remove Bookmark on Line Ctrl K + Ctrl N - Go to Next Bookmark Ctrl K + Ctrl P - Go to Previous Bookmark There are other options as well. Look under Edit->Bookmarks menu Unfortunately those aren't Brief Bookmarks and Brief Bookmarks in Visual Studio are buggy (http://stackoverflow.com/questions/7982970/how-to-drop-a-brief-bookmark-in-visual-studio)  I find this one also very useful: Ctrl K + Ctrk L - Clear alll bookmarks24,A,Can regex capture and substitution be used with an Apache DirectoryMatch directive? Does anyone know if it's possible to use regex capture within Apache's DirectoryMatch directive? I'd like to do something like the following: <DirectoryMatch ^/home/www/(.*)> AuthType Basic AuthName $1 AuthUserFile /etc/apache2/svn.passwd Require group $1 admin </DirectoryMatch> but so far I've had no success. Specifically I'm trying to create a group-based HTTP Auth for individual directories/vhosts on a server in Apache 2.0. For example Site A pointing to /home/www/a will be available to all users in group admin and group a site b at /home/www/b will be available to all users in group admin and group b etc. I'd like to keep everything based on the directory name so I can easily script adding htpasswd users to the correct groups and automate this as much as possible but other suggestions for solving the problem are certainly welcome. What you are trying to do looks very similar to per-user home directories. The way Apache handles these is through file system permissions and .htaccess files. I don't believe there is any way to use regex capture in the enclosed directives (AuthName etc).  You could tackle the problem from a completely different angle: enable the perl module and you can include a little perl script in your httpd.conf. You could then do something like this: <Perl> my @groups = qw/ foo bar baz /; foreach ( @groups ) { push @PerlConfig qq| <Directory /home/www/$_> blah </Directory> |; } </Perl> That way you could even read your groups and other information from a database or by simply globbing /home/www or whatever else tickles your fancy.25,A,"What's the cleanest way to simulate pass-by-reference in Actionscript 3.0? Actionscript 3.0 (and I assume Javascript and ECMAScript in general) lacks pass-by-reference for native types like ints. As a result I'm finding getting values back from a function really clunky. What's the normal pattern to work around this? For example is there a clean way to implement swap( intA intB ) in Actionscript? You could also use a wrapper instead of int: public class Integer { public var value:int; public function Integer(value:int) { this.value = value; } } Of course this would be more useful if you could use operator overloading...  Just look at some Java code. Java has had the convention that reference types are passed by reference and primitive types are passed by value since it's inception. It's a very good model in many ways. But talking about swap the best and easiest way to do a swap in Java/AS3 is with the following three lines:  var temp:int = array[i]; array[j] = array[i]; array[i] = temp; Theres not really any reason to use a function to do a simple swap when you can do it faster with just 3 lines.  If ActionScript works like Javascript [ab] = [ba] Sorry but that doesn't work in ActionScript  Destructuring assignment (e.g. [ab] = [ba]) isn't defined in the ECMA-262 3 specification and it's not implemented in JavaScript 1.5 which is the version equivalent to the JScript implementation in IE. I've seen this syntax in the AS4 specifications preview though and I believe it's part of JavaScript 1.7.  It is annoying. But if you use different idioms than in e.g. C# you can get reasonable-quality results. If you need to pass a lot of parameters back and forth pass in an object filled with the needed data and change the object's parameters when you return. The Object class is for just this sort of thing. If you just need to return a bunch of data return an Object. This is more in keeping with the ECMAScript style than pass-by-ref semantics.  This is nitpicking but int String Number and the others are passed by reference it's just that they are immutable. Of course the effect is the same as if they were passed by value.  I Believe the best you can do is pass a container object as an argument to a function and change the values of some properties in that object: function swapAB(aValuesContainer:Object):void { if (!(aValuesContainer.hasOwnProperty(""a"") && aValuesContainer.hasOwnProperty(""b""))) throw new ArgumentError(""aValuesContainer must have properties a and b""); var tempValue:int = aValuesContainer[""a""]; aValuesContainer[""a""] = aValuesContainer[""b""]; aValuesContainer[""b""] = tempValue; } var ints:Object = {a:13 b:25}; swapAB(ints); hasseg I'm holding out hope somebody knows a trick that's more elegant but it seems likely that your answer is the best one. I guess the natural follow up is: Does this limitation drive anyone else crazy or is it just me? :) it drives me a little crazy every now and then. I don't think you're all too alone on that one.  I suppose an alternative would be somewhere defining this sort of thing ... public class Reference { public var value:*; } Then use functions that take some number of Reference arguments to act as ""pointers"" if you're really just looking for ""out"" parameters and either initialize them on the way in or not and your swap would become: function swap(Reference a Reference b) { var tmp:* = a.value; a.value = b.value; b.value = tmp; } And you could always go nuts and define specific IntReference StringReference etc."26,A,designing Panels without a parent Form in VS? Does anyone know of any tools/plugins that allow you to design a Panel independently of a Form (Windows not Web Form) within Visual Studio? I've been using the designer and manually extracting the bits I want from the source but surely there is a nicer way. You could do all the design work inside of a UserControl. If you go that route instead of just copying the bits out of the user control simply use the user control itself.  You could just write the code by hand!  As Chris Karcher said you should probably use a user control. This will allow easy VS-supported/-integrated reuse without having to manually fiddle with designer code.27,A,"Are delegates not just shorthand interfaces? Suppose we have: interface Foo { bool Func(int x); } class Bar: Foo { bool Func(int x) { return (x>0); } } class Baz: Foo { bool Func(int x) { return (x<0); } } Now we can toss around Bar and Baz as a Foos and call their Func methods. Delegates simplify this a little bit: delegate bool Foo(int x); bool Bar(int x) { return (x<0); } bool Baz(int x) { return (x>0); } Now we can toss around Bar and Baz as Foo delegates. What is the real benefit of delegates except for getting shorter code? You can pass delegates as parameters in functions (Ok technically delegates become objects when compiled but that's not the point here). You could pass an object as a parameter (obviously) but then you are tying that type of object to the function as a parameter. With delegates you can pass any function to execute in the code that has the same signature regardless of where it comes from.  There is a slight difference delegates can access the member variables of classes in which they are defined. In C# (unlike Java) all inner class are consider to be static. Therefore if you are using an interface to manage a callback e.g. an ActionListener for a button. The implementing inner class needs to be passed (via the constructor) references to the parts of the containing class that it may need to interact with during the callback. Delegates do not have this restriction therefore reduces the amount of code required to implement the callback. Shorter more concise code is also a worthy benefit. @DanielEarwicker: At least two reasons: (1) Suppose an a routine will call an `Action` in some situation and in that situation I want it to call `Foo(x)` with the present value of local variable `x`. Passing such a request as a delegate requires the creation of two heap instances: a temporary class object to hold `x` and a delegate to call it. Passing the request as an interface type would require the creation of one heap instance (an implementation of the interface). Passing it as an interface-constrained generic might not require the creation of any. (2) Interfaces can have open generic methods whereas delegates cannot. Suppose I have a collection of objects have no common base type but all are of types constrained to implement IFoo and IBar. I'd like to be able to run an externally-supplied routine which takes a parameter constrained to IFoo and IBar. There's no way to specify that a delegate takes a parameter of type `T:IFooIBar` but it is possible to declare an interface method that does so. (1) ""heap instances"" are not especially expensive in CLR. See http://smellegantcode.wordpress.com/2009/03/01/the-amazing-speed-of-the-net-garbage-collector/ (2) What's wrong with `delegate void MyDelegate(T v) where T : IFoo IBar;` ? @DanielEarwicker: A routine in which a closure is defined defines a special ""hidden"" class to hold local variables creates an instance of such a class any time those variables enter scope and uses the fields of that class instead of ""normal"" local variables. Scenarios which use closures would actually work better with interfaces than with delegates. @supercat Correct. But why do you think interfaces would ""work better"" than delegates? A delegate is logically the same thing as a single-method interface. Ah you mean like this? http://smellegantcode.wordpress.com/2010/04/17/how-many-methods-does-this-interface-have/#more-283 @DanielEarwicker: The problem with it is that the routine which creates the delegate has to specify type `T`. Suppose a class has a collection of objects which all implement `I1` and `I2` but they do not have a common base type which implements both; it wishes to expose an `ForAll` method. If such a method accepted an object of type `interface IActUponConstrained{void Act(T param) where T:I1I2;}` it could call `Act` on each item without the entity that created the `IActUponConstrained` having to know the types of the individual items. @DanielEarwicker: If one doesn't want to use Reflection the generic interface is necessary because generic typing parameters can only move ""down"" (deeper into) the call stack. A collection of abstract class `ThingImplementing` may hold instances of derived classes `ThingImplementing.WithType where ActualType:I1I2`. Each instance is bound to the generic type with which it was created but if other code is going to be called using that type parameter the instance is going to have to do the call itself. If `Foo` and `Bar` are unrelated but implement... ...both interfaces there's no way to define a delegate with a type parameter constrained to both `I1` and `I2` to which a `ThingImplementing.WithType` could pass a `Foo` and to which a `ThingImplementing.WithType` could pass a `Bar`. One could however define an implementation of `IActUponConstrained` pass it to both objects and have the first call `Act(Foo param)` while the second calls `Act(Bar param)`. @DanielEarwicker: Precisely. The ability to have the consumer of an interface type supply a generic type parameter is unlike anything which can be done with .net delegates. Actually I'm not sure there'd be any real need for delegates in the framework if compilers could do for generic interfaces a few of the things they do for delegates (e.g. given an interface with an `Invoke` method having a certain signature construct a class whose constructor takes two implementations of that interface and which implements invoke by calling both instances). Of course... ...generics didn't exist in .net 1.0 so quasi-generic delegates were a necessary stopgap. Incidentally I wonder why compiler-generated derivatives of `Delegate` don't define their own type-specific `Combine` and `Remove` methods? Such methods could properly operate with contravariant delegate types whereas `Delegate.Combine` cannot. It is interesting to note that delegates can access members. A class wrapped method to be passed could easily act as a courier for the resultant data but direct access is a much faster way to go about it. It's not just member variables. It's even local variables or parameters of the method an anonymous delegate is defined within.  Interfaces and delegates are two utterly different things although I understand the temptation to describe delegates in interface-like terms for ease of understanding...however not knowing the truth may lead to confusion down the line. Delegates were inspired (partly) because of the black art of C++ method pointers being inadequate for certain purposes. A classic example is implementing a message-passing or event-handling mechanism. Delegates allow you to define a method signature without any knowledge of a class' types or interfaces - I could define a ""void eventHandler(Event* e)"" delegate and invoke it on any class that implemented it. For some insight into this classic problem and why delegates are desirable read this and then this.  One can think of delegates as an interface for a method which defines what arguments and return type a method must have to fit the delegate  From a Software Engineering perspective you are right delegates are much like function interfaces in that they prototype a function interface. They can also be used much in the same kind of way: instead of passing a whole class in that contains the method you need you can pass in just a delegate. This saves a whole lot of code and creates much more readable code. Moreover with the advent of lambda expressions they can now also be defined easily on fly which is a huge bonus. While it is POSSIBLE to build classes on the fly in C# it's really a huge pain in the butt. Comparing the two is an interesting concept. I hadn't previously considered how much alike the ideas are from a use case and code structuring standpoint.  In at least one proposal for adding closures (i.e. anonymous delegates) to Java they are equivalent to interfaces with a single member method.  A delegate does share a lot in common with a interface reference that has a single method from the caller's point of view. In the first example Baz and Bar are classes which can be inherited and instantiated. In the second example Baz and Bar are methods. You can't apply interface references to just any class that matches the interface contract. The class must explicitly declare that it supports the interface. You can apply a delegate reference to any method that matches the signature. You can't include static methods in an interface's contract. (Although you can bolt static methods on with extension methods). You can refer to static methods with a delegate reference.  A delegate is a typed method pointer. This gives you more flexibility than interfaces because you can take advantage of covariance and contravariance and you can modify object state (you'd have to pass the this pointer around with interface based functors). Also delegates have lots of nice syntactic sugar which allows you to do things like combine them together easily.  Yes a delegate can be thought of as an interface with one method.  No delegates are for method pointers. Then you can make sure that the signature of the method associated w/ the delegate is correct. Also then you don't need to know the structure of the class. This way you can use a method that you have written to pass into a method in another class and define the functionality you want to have happen. Take a look at the List<> class with the Find method. Now you get to define what determines if something is a match or not without requiring items contained in the class to implement IListFindable or something similar."28,A,"How to remove xmlns attribute with .NET XML API XmlElement.Attributes.Remove* methods are working fine for arbitrary attributes resulting in the removed attributes being removed from XmlDocument.OuterXml property. Xmlns attribute however is different. Here is an example: XmlDocument doc = new XmlDocument(); doc.InnerXml = @""<Element1 attr1=""""value1"""" xmlns=""""http://mynamespace.com/"""" attr2=""""value2""""/>""; doc.DocumentElement.Attributes.RemoveNamedItem(""attr2""); Console.WriteLine(""xmlns attr before removal={0}"" doc.DocumentElement.Attributes[""xmlns""]); doc.DocumentElement.Attributes.RemoveNamedItem(""xmlns""); Console.WriteLine(""xmlns attr after removal={0}"" doc.DocumentElement.Attributes[""xmlns""]); The resulting output is xmlns attr before removal=System.Xml.XmlAttribute xmlns attr after removal= <Element1 attr1=""value1"" xmlns=""http://mynamespace.com/"" /> The attribute seems to be removed from the Attributes collection but it is not removed from XmlDocument.OuterXml. I guess it is because of the special meaning of this attribute. The question is how to remove the xmlns attribute using .NET XML API. Obviously I can just remove the attribute from a String representation of this but I wonder if it is possible to do the same thing using the API. @Edit: I'm talking about .NET 2.0. I just ran into this issue. Good find! Many thanks to Ali Shah this thread solved my problem perfectly! here's a C# conversion: var dom = new XmlDocument(); dom.Load(""C:/ExampleFITrade.xml)); var loaded = new XDocument(); if (dom.DocumentElement != null) if( dom.DocumentElement.NamespaceURI != String.Empty) { dom.LoadXml(dom.OuterXml.Replace(dom.DocumentElement.NamespaceURI """")); dom.DocumentElement.RemoveAllAttributes(); loaded = XDocument.Parse(dom.OuterXml); }  Yes because its an ELEMENT name you can't explicitly remove it. Using XmlTextWriter's WriteStartElement and WirteStartAttribute and replacing the attribute with empty spaces will likely to get the job done. I'm checking it out now. will update.  Maybe trough the XmlNamespaceManager ? http://msdn.microsoft.com/en-us/library/system.xml.xmlnamespacemanager.removenamespace.aspx but it's just a guess. I've tried this but haven't been able to figure out how to remove a namespace with this class.  public static string RemoveXmlns(string xml) { //Prepare a reader StringReader stringReader = new StringReader(xml); XmlTextReader xmlReader = new XmlTextReader(stringReader); xmlReader.Namespaces = false; //A trick to handle special xmlns attributes as regular //Build DOM XmlDocument xmlDocument = new XmlDocument(); xmlDocument.Load(xmlReader); //Do the job xmlDocument.DocumentElement.RemoveAttribute(""xmlns""); //Prepare a writer StringWriter stringWriter = new StringWriter(); XmlTextWriter xmlWriter = new XmlTextWriter(stringWriter); //Optional: Make an output nice ;) xmlWriter.Formatting = Formatting.Indented; xmlWriter.IndentChar = ' '; xmlWriter.Indentation = 2; //Build output xmlDocument.Save(xmlWriter); return stringWriter.ToString(); } A little narrative description of your proposed answer would be helpful.  .NET DOM API doesn't support modifying element's namespace which is what you are essentially trying to do. So in order to solve your problem you have to construct a new document one way or another. You can use the same .NET DOM API and create a new element without specifying its namespace. Alternatively you can create an XSLT stylesheet that transforms your original ""namespaced"" document to a new one in which the elements will be not namespace-qualified. I'm not sure but as long as there is no positive answer I believe it is true.  We can convert the xml to a string remove the xmlns from that string and then create another XmlDocument using this string which will not have the namespace.  Wasn't this supposed to remove namespaces? XmlNamespaceManager mgr = new XmlNamespaceManager(""xmlnametable""); mgr.RemoveNamespace(""prefix"" ""uri""); But anyway on a tangent here the XElement XDocument and XNameSpace classes from System.Xml.Linq namespace (.Net 3.0) are a better lot than the old XmlDocument model. Give it a go. I am addicted. Thanks for the suggestion. Will definitely give it a try.  I saw the various options in this thread and come to solve my own solution for removing xmlns attributes in xml. This is working properly and has no issues: 'Remove the Equifax / Transunian / Experian root node attribute that have xmlns and load xml without xmlns attributes. If objXMLDom.DocumentElement.NamespaceURI <> String.Empty Then objXMLDom.LoadXml(objXMLDom.OuterXml.Replace(objXMLDom.DocumentElement.NamespaceURI """")) objXMLDom.DocumentElement.RemoveAllAttributes() ResponseXML = objXMLDom.OuterXml End If There is no need to do anything else to remove xmlns from xml."29,A,"Sending a mail as both HTML and Plain Text in .net I'm sending mail from my C# Application using the SmtpClient. Works great but I have to decide if I want to send the mail as Plain Text or HTML. I wonder is there a way to send both? I think that's called multipart. I googled a bit but most examples essentially did not use SmtpClient but composed the whole SMTP-Body themselves which is a bit ""scary"" so I wonder if something is built in the .net Framework 3.0? If not is there any really well used/robust Third Party Library for sending e-Mails? I'm just going to put a note here for anyone that's having problems and finds their way to this page - sometimes Outlook SMTP servers will reconvert outgoing email. If you're seeing your plain-text body vanish entirely and nothing but base64-encoded attachments it might be because your server is reencoding the email. Google's SMTP server does not reencode email - try sending through there and see what happens. I did indeed find my way here (over four years later) looking for that. Any chance you or anyone reading this know how to make it stop it on Microsoft Exchange Server 2010 SP3?  Just want to add that you can use defined constants MediaTypeNames.Text.Html and MediaTypeNames.Text.Plain instead of ""text/html"" and ""text/plain"" which is always a preferable way. It's in System.Net.Mime namespace. So in the example above it would be: AlternateView htmlView = AlternateView.CreateAlternateViewFromString(htmlContent null MediaTypeNames.Text.Html); I would prefere: var htmlView = AlternateView.CreateAlternateViewFromString(htmlContent new ContentType(MediaTypeNames.Text.Html));  What you want to do is use the AlternateViews property on the MailMessage http://msdn.microsoft.com/en-us/library/system.net.mail.mailmessage.alternateviews.aspx  The MSDN Documentation seems to miss one thing though I had to set the content type manually but otherwise it works like a charm :-) MailMessage msg = new MailMessage(username nu.email subject body); msg.BodyEncoding = Encoding.UTF8; msg.SubjectEncoding = Encoding.UTF8; AlternateView htmlView = AlternateView.CreateAlternateViewFromString(htmlContent); htmlView.ContentType = new System.Net.Mime.ContentType(""text/html""); msg.AlternateViews.Add(htmlView); It looks like the MSDN Documentation has been updated: the C# version of their example code uses a version of AlternateView.CreateAlternateViewFromString that takes the content type as a parameter. The C++ version does not (they may have neglected to update it). @jep I was able to prevent my smtp server (one similar to Exchange) from re-encoding the plain text version to BASE64 by using msg.BodyEncoding = Encoding.ASCII. The htmlView is unharmed. +1 Cancelled out that down vote because this was useful to me."30,A,"Any advice for speeding up the compile time in Flex Builder 3? I run Flex Builder 3 on a mac and as my project grows - the compile time gets longer and longer and longer. I am using some SWC's and there is a fair amount of code but it shouldn't take minutes to build and crash daily should it? You may want to explore the command-line compiler found in the Flex SDK mxmlc. As I recall Flex Builder 3 seems to hide all the compiler details but perhaps there are arguments you can append that will help you speed up the compilation. For example you may want to set optimize=false which will skip the step of optimizing the bytecode (perhaps reducing compilation time)? This of course comes at the price of performance and file size of the actual application. More documentation on mxmlc can be found at: http://livedocs.adobe.com/flex/3/html/compilers_13.html. Good luck!  Usually the first build takes the longest and then it's pretty quick after that. That's using Vista x64 w/ core 2 duo. Otherwise I am nearly certain a Intel Core i7 Extreme Edition 965 3.2GHz upgrade processor would speed your Flex building up nicely .. :) :) :)  First of all comments on some of the response: There is no need to explicitly specify -incremental in Flex Builder because it uses incremental compilation by default. -keep-generated-actionscript is a performance killer because it instructs the compiler to write out AS3 codes generated for MXML components in the middle of the compilation. File I/O in the middle of a compilation means unnecessary pauses and low CPU utilizations. -optimize slows down linking because it instructs the linker to produce smaller SWFs. Note that -optimize=true|false doesn't have any effect on building SWCs because SWCs are libraries and have to be unoptimized. I rarely mess with JVM settings because JVM knows its jobs well and tunes itself quite well at runtime. Most people make matter worse by setting various GC tuning parameters. That said there are 3 settings most people understand and set correctly for their usage: -Xmx (max heap size) -server or -client (HotSpot Server or Client VM) -XX:+UseSerialGC or -XX:+UseParallelGC (or other non-serial GC) -server consistently outperforms -client by about 30% when running the Flex compiler. -XX:+UseParallelGC turns on the parallel garbage collector. ideal for multicore computer and when the computer still has CPU cycles to spare. You may also want to check out HellFire Compiler Daemon (http://bytecode-workshop.com/). It uses multiple processor cores to compile multiple Flex applications at the same time. You can also run the compiler on a second machine via sockets (assuming that your second machine has faster CPUs and more memory). In my opinion use more modules than libraries and use HFCD. Hope this helps. -Clement I don't understand your point on -keep-generated-actionscript. Are you saying the I/O cost of caching these resources outweighs the cost of recompiling them EVERY SINGLE BUILD? I find that unbelievable given that I build my code dozens of times every day.  Go to Project->Properties->Flex Applications. All of the applications listed are compiled each time (even though you have a default set). If you remove everything but the default (don't worry it won't delete the actual files) it only compiles the default app. This resulted in a significant speed up for me. If you change your default app it ADDs it to the Flex Applications list - adding to your compile time. You will need to maintain this list to get the quickest compile.  I always disable ""automatic compile"" for Flex. It compiles too much takes too long and so interrupts my work. If you have many different project files and all of those needs to be recompiled but you also have other projects open and don't want to close them always you're doing a build you can also use Eclipse Working Sets. Unfortunately the default Flex Navigator does not support working sets. But you can open the Package Explorer with Window / Show View / .... Click on the little white downward arrow to the topright and select Top Level Elements: Working Sets. You can then add Working Sets (aka groups of projects). Each project needs to be in at least one working set (""Other Projects"" being the default) but can be in several. Now with Project / Build Working Set / ... you can instruct Eclipse to build all the projects in this working set but none of the others. This is especially useful if you suspect your project references to be sometimes broken - otherwise building the 'topmost' project should trigger subsequent builds automatically.  In addition to the suggestions already mentioned close any projects that you have open that you are not using. Rich click on the Project in the Navigator view and select ""Close Unrelated Projects"". Depending on how many projects you have open this can lead to a significant improvements in compile time as well as all around performance. mike chambers mesh@adobe.com Sadly even on my 8 core Mac Pro w/ 12 GB of RAM Flex builds take forever for a small piece of code. I waste a lot of time waiting for Flex to build. mxmlc on my dual core Xeon Linux box isn't much if any faster. ""Close unrelated projects"" doesn't matter for me; I only have one project open. We have a really big Flash Builder 4 project and compile time is still a problem even on the fastest machines. The problem is that the smallest change even to the internals of a private function seems to cause Flash Builder/Flex to rebuild the world completely unnecessarily. Perhaps they plan class caching etc for a future release!  The SDK 4.x.x introduced silly bug (see Adobe bugsystem issue FB-27440) which causes projects with SVN or CVS meta data compile much slower than with SDK 3.x.x. On how it can be fixed see here.  You can use WORKING SETS to compile just a set of your components that are part of the application that you are changing and not the whole project http://livedocs.adobe.com/flex/3/html/help.html?content=build_6.html  There's no need to use mxmlc on the command line just to be able to add compiler flags. Right click your project in the Flex Navigator select Properties and then Flex Compiler in the dialog that appears. There you can add any extra compiler flags. Not sure that there's very much to do though more code means more compile time that's just the way it is. If you're not doing a release build (or whatever it's called in Flex Builder) it's unlikely that your compiler settings include optimize to begin with. Better choices to try would be -incremental (which only recompiles the parts that have changed) and -keep-generated-actionscript (which stops the compiler from deleting the ActionScript files it has generated from your application's MXML files). I very much prefer using mxmlc on the command line (by way of Ant) compared to Flex Builder. Although I don't think that the latter compiles any slower it feels more sluggish in every way. Using Ant also makes it possible to do more than just compilation when building and conditional compilation (only compile a SWF or SWC if the source code has actually changed). Check out a blog post of mine for more info on that. What you could try is the Flex Compiler Shell another command line tool that can speed things up. Basically it tries to keep as much as possible in memory between builds so no need to wait for things like the JVM starting up (the Flex compiler is a Java application). On the other hand this is sort of what Flex Builder does anyway.  You want at least 4 gigs on your computer if possible and make sure to override the default memory settings that eclipse/flexbuilder gives to the application. If you're not sure how to do this you can find the flexbuilder app in /Applications right click and choose ""Show Package Contents"". Then go into the contents file and edit the eclipse.ini file. Edit that file have memory settings of at least: -vmargs -Xms768m -Xmx768m -XX:PermSize=128m -XX:MaxPermSize=128m It's also worthwhile to go into the eclipse/flexbuilder preferences and to check the ""Show heap status"" box under Windows->Preferences->General (This is in eclipse with the FB plugin I'm assuming it's also there for standalone FB). This shows the current memory in the lower right of the window and has a little trash icon so you can force garbage collection. I'd also suggest turning off automatic building of the project when your files change (you can force a build with cmd-B). We had a huge project with quite a few modules files and performance in FlexBuilder 3 was decent with these steps.  I don't use Flex Builder but I use the Flex SDK compiler everyday and I was wasting tons of time waiting for the MXMLC compiler to do its job until I found Flex Compiler SHell: http://blog.zarate.tv/2008/12/07/theres-something-called-flex-compiler-shell/ Although in theory Flex Builder already uses this optimizations might be worth checking. Sorry just realized that Theo linked to it : |  I created RAM Disk with workspace and it gives up to 10% of better compilation time. Not much but something.  Slow compile time is most often caused by having large numbers of embedded resources ([Embed] or @Embed). Option 2 on this article might help you: [http://www.rogue-development.com/blog2/2007/11/slow-flex-builder-compile-and-refresh-solution-modules/]  As Clement said use the HellFire Compiler Daemon. If you have multiple modules and more CPU cores on your machine it can compile them in parallel. Another option is to use IntelliJ (the commercial version) which offers the same feature."31,A,"Favorite Windows keyboard shortcuts I'm a keyboard junkie. I love having a key sequence to do everything. What are your favorite keyboard shortcuts? I'll start by naming a couple of mine: 1 - Alt-Space to access the windows menu for the current window 2 - F2 to rename a file in Windows Explorer Might be smart to rename your ""question"" to an actual question so that people don't downvote it. Maybe ""What Windows keyboard shortcuts do you find most useful?"" or maybe just upvote it. this needs not be a question as it's community-wiki. It's just an article with a title. Amazing how many closer-to-being-related-to-programming questions have been closed but this one managed to survive... *23 for ""vote to close"". *22 for me In calc F5 F6 F7 F8 cycle between Hex Dec Oct Bin mode.  Win + Pause/Break to bring up computer information and to access environment variables under the Advanced tab. Win + R to go straight to the run box (though I barely use this anymore since I started with Launchy). Of course Alt + Tab but also Alt + Shift + Tab for going backwards. Oh and personally I hate Ctrl + F4 for closing tabs - too much of a pinky stretch. Oh and try Win + Tab on Windows 7 (with Aero on). IMO default shortcut of Launchy sucks because Alt-Space is too useful for me to let L to hi-jack it. So I re-map Launchy to Win-Space. What is your preference? alt space is fine by me win-space woudl be harder to hit but to each his own! you can do anything alt-space provides with other hot keys so it's okay for launchy to take over it. However Launchy is found to consume too much resource for me i just pin the most frequently used programs to my start menu. It's good. Ctrl+W closes tabs or documents in all good apps.  Win+D Win+R Win+E Win+1 (Firefox)  How is this not here? +Pause to System Information. Then the system PATH variable is only 2 clicks away (Advanced system settingsEnvironment Variables...)  Repeat CTRL + ALT + DEL Twice!  win-M to minimise all. Useful for quick trips to the desktop. Win+Shift+M to unminimise.  Press the Backspace key in any Windows Explorer window (including the common dialog windows) to go up one level in the folder hierarchy. This is a shortcut for a button next to the folder combo-box. Microsoft removed this functionality in Windows Vista and later in order to make Windows Explorer more like a web browser; now the Backspace key operates as a ""Back"" button. (If anyone knows of a way to go up one level in Vista and later please comment!) I never thought of that. Thank you! [Alt-D] "".."" ENTER Alt-D to give focus to the menu bar .. to go up a level ENTER to execute  On Windows Vista if you bring up the Start menu and search for a program pressing Ctrl+Shift+Enter will run the selected program as Administrator. So to open an Administrator command prompt: Windows key type ""cmd"" Ctrl+Shift+Enter  I am used to setting up shortcut keys to program shortcuts in start menu (standard Windows feature). Once the program is started pressing such shortcut brings the focus to the window instead of starting another copy. For example to pause Winamp I just press Ctrl+Alt+W C (and I can have it working without tray or taskbar icons). The only drawback is that some program names start with the same letter so I have to pick up other letters for them. =) You can get two per letter so Ctrl+Alt+w is different than Ctrl+Alt+W. Numbers work too.  Win+D to show the desktop and then Win+D to bring all the windows back again.  Not really an answer but a hint for a good source to look from - if no one cited it above wikipedia has all ( for the most important OS's) - not the best  Win-D to minimize all applications Ctrl-Shift-Esc to open Task Manager I always wanted to use Win-D more but it's a one way action. There doesn't seem to be a way to restore all the windows to the state they were at unfortunately. @Zigdon: Win-D is a toggle. Press it again and the windows should be restored. Are you confusing this with Win-M? (I use XP) It is - Win-D. At least on XP. Another way is to use Win-M to minimize Win-Shift-M to undo. @gabr PRECISELY!  In any dialog with tabs Ctrl-Page Up/Down to cycle between the tabs.  Ctrl + Shift + ESC : Run Task Manager Ctrl/Shift + Insert : Copy/Paste Shift + Delete : Cut (text) Win + L : Lock System Win + R : Run Ctrl + Pause Break : Break Loop (Programming) Ctrl + Tab : Tab Change  A few basic keyboard shortcuts for clipboard operations text selection and navigation that work in most Windows programs: Clipboard Ctrl+X - Clipcoard Cut Ctrl+C - Clipboard Copy Ctrl+V - Clipboard Paste Selecting Text Ctrl+A - Select All (in the current field or document) Shift + [navigate with arrow keys Home/End or PgUp/PgDn] - Select text between the caret's previous and new positions. Continue to hold Shift and navigate to select more text. Navigation Ctrl + left arrow / Ctrl + right arrow - Move the caret to the previous/next word Ctrl+Home / Ctrl+End - Go to beginning/end of the current field or document Bonus Tip! Before submitting a web form where you've entered a lot of text into a text field (for example an email in a web-based mail client -- or a new question or answer on Stack Overflow!) do a quick Ctrl+A Ctrl+C on the field. That way if something goes wrong with the submit (even if the browser crashes) you haven't lost your work -- you have a copy of it sitting on the clipboard.  Win + E to open an WindowsExplorer reference Win + R from the Run box Ctrl + Esc to open the start menu And of course Alt + F4 to close things.  F5 to execute seems to be the one I use the most  Win-L to lock the computer..  My personal favourite is WinKey U Enter - shuts Windows down! ;-) win -> U -> U also works.  I try to stick to my keyboard as well. I frequently use... Win+L to Lock my system Alt+F4 to close a program Win+R to launch from the Run Window (Used for frequent programs instead of going through QuickLaunch) F2 to rename a file Win+D to go to Desktop Alt+Tab and Alt+Tab+Shift to cycle through open programs Visual Studio Alt D (debug) P (process) W (webdev process) Alt T (Tools) P (process) W (webdev process) for VS 2008 Alt M O to collapse to definitions F5 to launch F9 F10 and F11 for stepping through debugger Alt+K D to format a document Alt+K C to comment Alt+K U to uncomment Browser Alt+W to close tab F6 to focus on the address bar  Ctrl + Shift + Esc -> Open Task Manager Ctrl + W -> closes windows in MDIs where Ctrl+F4 doesn't work Those and the Win + Number is Vista are used constantly. Also a nice trick is Win + Tab -> cycles through program groups on task bar in Windows Xp and Server 2003. (i.e. same as Vista without the previews).  Win-[type name of program] to launch a program in Vista Win-E for explorer Win-F for find Alt-Tab to swap between programs Ctrl-Tab to swawp between tabs Not really a 'Windows' shortcut but the Ctrl-Alt-numpad and Ctrl-Alt-arrows to move and resize windows and move them to another monitor using WinSplit Revolution are absolutely great. I would never use large or multiple monitors without them. Just downloaded WinSplit - wowza that's useful!  Alt-F4 to close a program. WindowsKey + L to lock my workstation Ctr-Shift-Ins to copy text from a textbox Alt-Print Screen to capture a shot of just a window WindowsKey + R to open the ""Run"" dialog (XP Pro only- does something else on XP Home) `Ctrl`-`Shift`-`Ins` OMG! That's absolutely great. Didn't know about that. I have used 3rd party SW for this purpose. From which Windows' version is it supported? Thanks for that! :) Alt+PrintScreen is handy especially on XPs. Vista comes with a handy accessory - Snipping Tool. At least the enterprise edition does. Just `Ctrl+Ins` is copy. `Shift+Ins` is paste!  Many say that Win-D minimises all applications. Not true. It simply shows the desktop. Use Win-M to minimise all open windows. Use Win-Shift-M to restore them to their previous state. By the way did you notice that the Sift key can be combined with most of the usual shortcuts? e.g. Alt+Tab : cycle through applications 1->2->3->4->...1 Add Shift to the shortcut and you will be cycling in the opposite direction 1<-2<-3<-4<- ...1 Control+Tab to switch between Tabs in most Windows applications (sadly not in Eclipse) - you can already guess what Ctr+Shift+Tab will do. Especially handy in Firefox IE etc... where you have more than one Tab open and try going to the previous one. Very handy. And one more tip this is soooo handy I love it. Only found out about it a couple of weeks ago: FireFox users: tired of rightclick->Open Link in New Tab? Click a link with MIDDLE mouse button and it will open in a new tab (depends on your Tabs settings in Tools->Options but by default would work). The magical thing about this is that it works even for the browser's Back button! Also when you type a search term into the Google box (usually in top right corner) and middle-click the search button the search results are opened in a new tab. Closing tabs is also much easier with the middle mouse button (of course you can do Ctrl+W but sometimes the mouse is simply in your hand). You don't have to click the tab's red button to close it. Simply middle-click anywhere on the tab and it will be closed. EDIT I just tried the middle button in IE 7 and seems to work just like it does in FF except for the Back-button and Search widget.  The most ""important"" shortcut is the Secure Attention Sequence Ctrl+Alt+Del. On XP you usually have to enable it otherwise it just runs Task Manager. While logged in SAS brings up the Windows Security dialog on its own Desktop which will get you out of almost anything (such as a hung full-screen DirectX app). The Task Manager shortcut is now (and always was) Ctrl+Shift+Esc. (this answer applies to NT4 2000 XP and 2003. I can't speak for Vista)  I mapped some global hotkeys: In Winamp I use Ctrl+Alt+Backspace (same as AltGr+Backspace for me) to Pause. If someone wants my attention while I've got headphones on far easier to press a couple of buttons than click the mouse on a button that's about five pixels wide. I use Ctrl+Alt+C to run calc. Astonishing how many people have a physical calculator on their desk because they can't figure this out.  Ctrl+Shift+Esc to go straight to the task manager without any intermediate dialogs.  Win+Pause/Break for System Properties Win+E: open windows explorer Win+F: find Win+R: run Win+M: minimize all windows Win+Shift+M: restore all windows Alt+F4: close program Alt+Tab: switch between tasks Ctrl+Alt+Del: task manager  I don't have favorites among keyboard shortcuts -- they are all utility entities to me... Except for Win-L which means another coffee break!  I use the free AutoHotKey then I define my own shortcuts: dobule tap F4 quickly => Close active Windows (like Alt+F4 but with one finger only) double tap Right Alt quickly => Find and Run Robot task manager F12 => open Find and Run Robot Locate32 plugin (I use it like a very lightweight desktop search) Ctrl+Up / Down in a command window => scroll back / forward command line like the mouse wheel Ctrl+w in a command windows => close window etc.  All those of you that mentioned Alt Tab and Ctrl Tab missed out the shift versions too CTRL-SHIFT-TAB - move one tab back ALT-SHIFT-TAB - move one window back in task switcher  F4 in windows explorer to access the location bar trivially. Menu key (next to the right-hand windows key) + W + F to create a new folder in explorer. ALT + E just accesses the location bar.  For when you have a window stuck under an appbar and can't get at that window's system menu to move it: alt-spacebar -> M -> arrow keys -> return  It's not a keyboard shortcut but my favourite trick is to bind the large thumb button on the rat to move window the smaller thumb button to resize. That way windows can be moved and resized very easily and naturally. You can probably to that in windows too. As for keyboard tricks I use right ctrl+keypad to pick (one of nine) virtual screens. Very quick and natural.  Win + 1 .. 9 -- Start quick launch shortcut at that index (WindowsVista). Ctrl + Scroll Lock Scroll Lock -- Crash your computer: Windows feature lets you generate a memory dump file by using the keyboard @gabr -- Win + D is show desktop Win + M minimizes all windows. Hitting Win + D twice brings everything back as it has only shown the desktop window in front of the other windows. In Windows7 Win+1..9 is for programs pinned to task bar (in case you wanted to try it on restored QuickLanuch toolbar). I picked yours as the ""answer"" b/c I have never heard of the ctrl-scroll lock scroll lock one before. Wow what the devil.. I've never heard about this one What am I doing wrong? I keep trying the scroll lock combination and NOTHING happens. You have to enable it via the registry. Follow the link from the answer. Of course Win+D twice does NOT actually restore the window order correctly oh no that would have been too nice of MS. If you have 15 windows spread on three screens chances are that you will get a completely different set of foreground windows after doing it twice.  To maximize a window: Alt+Space X To restore a window: Alt+Space R To minimize a window: Alt+Space N To close a window: Alt+Space C  Alt + f4 Alt + tab ctrl + tab win + tab ctrl + x ctrl + v ctrl + c alt + r alt + e alt + d ctrl + space (VS IDE)  ctrl+alt+del to open task manager alt+f4 to close window I've removed the ""Windows"" key from my keyboard along with a few other non-standard keys - they're too much of a nuisance. Interesting... I use the Windows key a lot. In fact I've made a point of ""pinning"" shortcuts to the menu with unique letters. That way I can hit Windows-Q or Windows-X or whatever to launch my favorite apps in two keystrokes."32,A,"Should I be doing JSPX instead of JSP? Using JDeveloper I started developing a set of web pages for a project at work. Since I didn't know much about JDev at the time I ran over to Oracle to follow some tutorials. The JDev tutorials recommended doing JSPX instead of JSP but didn't really explain why. Are you developing JSPX pages? Why did you decide to do so? What are the pros/cons of going the JSPX route? The main difference is that a JSPX file (officially called a 'JSP document') may be easier to work with because the requirement for well-formed XML may allow your editor to identify more typos and syntax errors as you type. However there are also disadvantages. For example well-formed XML must escape things like less-than signs so your file could end up with content like: <script type=""text/javascript""> if (number &lt; 0) { The XML syntax may also be more verbose. In my personal experience JSPX enforces controller view separation because you cant execute java code in JSPX but XML syntax sometimes is overwhelming Escaping the code with CDATA is the way to go. In most IDE's you can add code snippets/templates so you can make one to output a basic tag including the CDATA comment. You shouldn't have JS in your pages anyway. Why? If some scripting snippet is only applicable to some limited markup in only one page IMHO a `"33,A,"Uncollapsible CollapsiblePanelExtender I have a CollapsiblePanelExtender that will not collapse. I have ""collapsed"" set to true and all the ControlID set correctly. I try to collapse and it goes through the animation but then expands almost instantly. This is in an User Control with the following structure. <asp:UpdatePanel ID=""UpdatePanel1"" runat=""server""> <ContentTemplate> <asp:GridView ID=""GridView1"" runat=""server"" AutoGenerateColumns=""False"" DataSourceID=""odsPartners"" Width=""450px"" BorderWidth=""0"" ShowHeader=""false"" ShowFooter=""false"" AllowSorting=""true"" onrowdatabound=""GridView1_RowDataBound""> <Columns> <asp:TemplateField HeaderText=""Contract Partners"" SortExpression=""Name""> <ItemTemplate> <asp:Panel id=""pnlRow"" runat=""server""> <table> ...Stuff... </table> </asp:Panel> <ajaxToolkit:CollapsiblePanelExtender runat=""server"" ID=""DDE"" Collapsed=""true"" ImageControlID=""btnExpander"" ExpandedImage=""../Images/collapse.jpg"" CollapsedImage=""../Images/expand.jpg"" TargetControlID=""DropPanel"" CollapseControlID=""btnExpander"" ExpandControlID=""btnExpander"" /> <asp:Panel ID=""DropPanel"" runat=""server"" CssClass=""CollapsedPanel""> <asp:Table ID=""tblContracts"" runat=""server""> <asp:TableRow ID=""row"" runat=""server""> <asp:TableCell ID=""spacer"" runat=""server"" Width=""30"">&nbsp;</asp:TableCell> <asp:TableCell ID=""cellData"" runat=""server"" Width=""400""> <uc1:ContractList ID=""ContractList1"" runat=""server"" PartnerID='<%# Bind(""ID"") %>' /> </asp:TableCell> </asp:TableRow> </asp:Table> </asp:Panel> </ItemTemplate> </asp:TemplateField> </Columns> </asp:GridView> </ContentTemplate> <Triggers> <asp:AsyncPostBackTrigger ControlID=""tbFilter"" EventName=""TextChanged"" /> </Triggers> </asp:UpdatePanel> I don't remember closing it...hmmm. I reopened it. ""Not a programming question""? I am sorry I do not have time to trouble-shoot your code so this is from the hip. There is a good chance that this a client-side action that is failing. Make certain that your page has the correct doctype tag if you took it out of your page or masterPage. Furthermore attempt to set the ClientState as well: DDE.ClientState = true; The issue is you have that thing wrapped inside of your TemplateField. I have ran into issues using the AjaxControlToolkit on repeated fields and usually side with using a lighter weight client-side option up to and including rolling your own show/hide method that can be reused just by passing in an DOM understood id.  Also check that the you have the following property set: AutoExpand=""False"" One of the features of the collapsible panel is that it will auto expand when you put your mouse over it and this tag will make sure that doesn't happen.  After checking the AutoExpand (which strangley had no visible effect) I checked the DOC Type. Sure enough. That was the culprit. This is the correct one: <!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.1//EN"" ""http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"" > Thanks Ian!"34,A,"Updating Legacy Code from System.Web.Mail to System.Net.Mail in Visual Studio 2005: Problems sending E-Mail Using the obsolete System.Web.Mail sending email works fine here's the code snippet:  Public Shared Sub send(ByVal recipent As String ByVal from As String ByVal subject As String ByVal body As String) Try Dim Message As System.Web.Mail.MailMessage = New System.Web.Mail.MailMessage Message.To = recipent Message.From = from Message.Subject = subject Message.Body = body Message.BodyFormat = MailFormat.Html Try SmtpMail.SmtpServer = MAIL_SERVER SmtpMail.Send(Message) Catch ehttp As System.Web.HttpException critical_error(""Email sending failed reason: "" + ehttp.ToString) End Try Catch e As System.Exception critical_error(e ""send() in Util_Email"") End Try End Sub and here's the updated version: Dim mailMessage As New System.Net.Mail.MailMessage() mailMessage.From = New System.Net.Mail.MailAddress(from) mailMessage.To.Add(New System.Net.Mail.MailAddress(recipent)) mailMessage.Subject = subject mailMessage.Body = body mailMessage.IsBodyHtml = True mailMessage.Priority = System.Net.Mail.MailPriority.Normal Try Dim smtp As New Net.Mail.SmtpClient(MAIL_SERVER) smtp.Send(mailMessage) Catch ex As Exception MsgBox(ex.ToString) End Try I have tried many different variations and nothing seems to work I have a feeling it may have to do with the SmtpClient is there something that changed in the underlying code between these versions? There are no exceptions that are thrown back. Everything you are doing is correct. Here's the things i would check. Double check that the SMTP service in IIS is running right. Make sure it's not getting flagged as spam. those are usually the biggest culprits whenever we have had issues w/ sending email. Also just noticed you are doing MsgBox(ex.Message). I believe they blocked MessageBox from working asp.net in a service pack so it might be erroring out you just might not know it. check your event log.  I've tested your code and my mail is sent successfully. Assuming that you're using the same parameters for the old code I would suggest that your mail server (MAIL_SERVER) is accepting the message and there's a delay in processing or it considers it spam and discards it. I would suggest sending a message using a third way (telnet if you're feeling brave) and see if that is successful. EDIT: I note (from your subsequent answer) that specifying the port has helped somewhat. You've not said if you're using port 25 (SMTP) or port 587 (Submission) or something else. If you're not doing it already using the sumission port may also help solve your problem. Wikipedia and rfc4409 have more details.  Are you setting the credentials for the E-Mail? smtp.Credentials = New Net.NetworkCredential(""xyz@gmail.com"" ""password"") I had this error however I believe it threw an exception.  Have you tried adding smtp.UseDefaultCredentials = True before the send? Also what happens if you try changing: mailMessage.From = New System.Net.Mail.MailAddress(from) mailMessage.To.Add(New System.Net.Mail.MailAddress(recipent)) to this: mailMessage.From = New System.Net.Mail.MailAddress(fromrecipent) -- Kevin Fairchild  I added the port number for the mail server and it started working sporadically it seems that it was a problem with the server and a delay in sending the messages. Thanks for your answers they were all helpful!  The System.Net.Mail library uses the config files to store the settings so you may just need to add a section like this  <system.net> <mailSettings> <smtp from=""test@foo.com""> <network host=""smtpserver1"" port=""25"" userName=""username"" password=""secret"" defaultCredentials=""true"" /> </smtp> </mailSettings> </system.net> he's passing the smtp server into the constructor of SmtpClient."35,A,"What is a good maintainability index using Visual Studio 2008 code analysis? My company recently purchased TFS and I have started looking into the code analysis tools to help drive up code quality and noticed a good looking metric ""maintainability index"". Is anyone using this metric for code reviews/checkins/etc? If so what is an acceptable index for developers to work toward? Visual Studio 2008 - Code Metrics  The maintainability index is not as much a fixed value you look at it's more of an indication that code is hard to understand test and/or debug. I usually try to keep high level code (basically anything except for the real plumbing code) above 80 where 90+ would be good. It adds a competitive element to programming as maintainable as possible to me. The code analysis tool really shines in the area of dependencies and number of branches within a method though. More branches means harder testing which makes it more error prone. Dependencies same thing. In other people's code I use the maintainability index to spot possible bad parts in the code so I know where to review it. Also methods/classes with a high number of lines are an indication of poor code to me (unless it can't be avoided again the plumbing works). In the end I think it mainly depends on how often your code will change. Code that's expected to change a lot has to score higher in maintainability than your typical 'write once' code."36,A,"cx_Oracle: how do I get the ORA-xxxxx error number? In a try/except block how do I extract the Oracle error number? try: cursor.execute(""select 1 / 0 from dual"") except cx_Oracle.DatabaseError exc: error = exc print ""Code:"" error.code print ""Message:"" error.message This results in the following output: Code: 1476 Message: ORA-01476: divisor is equal to zero"37,A,"Setting Attributes in Webby Layouts I'm working with Webby and am looking for some clarification. Can I define attributes like title or author in my layout? I've never used it but the tutorial here: Makes it look like the answer to your question is ""yes"". Specifically I'm looking under the ""Making Changes"" header on that page. What you (and the tutorial) mention is valid for the content pages. It is not true for the layouts however.  Not really. The layout has access to the page attributes rather than the other way. The easiest way to do what you want is to populate the SITE.page_defaults hash in your site's Rakefile (probably build.rake). Add something like the following: SITE.page_defaults['title'] = ""My awesome title"" SITE.page_defaults['author'] = ""Shazbug"" SITE.page_defaults['is_mando_awesome'] = ""very yes"" You can now access those hash members in your template: Written by <%= @page.author %> You can find more info about Webby's page default stuff on the Google Group specifically here: http://groups.google.com/group/webby-forum/browse_thread/thread/f3dc1f4187959634/c30d7883705f6218?lnk=gst&q=SITE#c30d7883705f6218"38,A,"php scripts writing to non-world-writable files How can you allow a php script to write to a file with high security restrictions such as only allowing a single user to write to it? The difficulty seems to be that a php script is running as a low-permissions user (maybe apache or www or nobody?) and even if I chown apache the_writable_file the directory it's in might not be writable for the low-level user. In general what's the usual way that php can work with local files in a secure way? Sure chgrp apache the_writable_file and chmod g+w the_writable_file. After that only your secure user and the apache user will be able to write to the file. Since the apache user is typically forbidden from logging in you only have to worry about web users writing to your secure file using through the http daemon.  All the containing folders need to have execute permissions. For example if the file's in /foo/bar/the_writable_file the directories ""foo"" and ""bar"" both need to have executable permission to access the_writable_file even if they don't have read/write permission.  Unfortunately in shared hosts that use mod_php there is no way to restrict access to secure files to your web app and login user. The solution is to run your web app as your login user. When you do that UNIX file permissions can correctly lock everyone else out. There are several ways to implement that including SuExec suPHP or running PHP with FastCGI with mod_fcgid or mod_proxy_fcgid. FastCGI is my favorite way. Another solution is to use a dedicated host or virtual private server."39,A,"How do I get the unix find command to print out the file size with the file name? If I issue the find command as follows: $find . -name *.ear It prints out: ./dir1/dir2/earFile1.ear ./dir1/dir2/earFile2.ear ./dir1/dir3/earFile1.ear What I want to 'print' to the command line is the name and the size: ./dir1/dir2/earFile1.ear 5000 KB ./dir1/dir2/earFile2.ear 5400 KB ./dir1/dir3/earFile1.ear 5400 KB  $ find . -name ""test*"" -exec du -sh {} \; 4.0K ./test1 0 ./test2 0 ./test3 0 ./test4 $ Scripter World reference  Using gnu find I think this is what you want. It finds all real files and not directories (-type f) and for each one prints the filename (%p) a tab (\t) the size in kilobytes (%k) the suffix "" KB"" and then a newline (\n). find . -type f -printf '%p\t%k KB\n' If the printf command doesn't format things the way you want you can use exec followed by the command you want to execute on each file. Use {} for the filename and terminate the command with a semicolon (;). On most shells all three of those characters should be escaped with a backslash. Here's a simple solution that finds and prints them out using ""ls -lh"" which will show you the size in human-readable form (k for kilobytes M for megabytes): find . -type f -exec ls -lh {} \; As yet another alternative ""wc -c"" will print the number of characters (bytes) in the file: find . -type f -exec wc -c {} \;  find . -name '*.ear' -exec du -h {} \; This gives you the filesize only instead of all the unnecessary stuff.  find . -name ""*.ear"" -exec ls -l {} \;  find . -name ""*.ear"" | xargs ls -sh  Why not use du -a ? E.g. find . -name ""*.ear"" -exec du -a {} \; Works on a Mac  a simple solution is to use the -ls option in find: find . -name \*.ear -ls That gives you each entry in the normal ""ls -l"" format. Or to get the specific output you seem to be looking for this: find . -name \*.ear -printf ""%p\t%k KB\n"" Which will give you the filename followed by the size in KB.  Awk can fix up the output to give just what the questioner asked for. On my Solaris 10 system find -ls prints size in KB as the second field so: % find . -name '*.ear' -ls | awk '{print $2 $11}' 5400 ./dir1/dir2/earFile2.ear 5400 ./dir1/dir2/earFile3.ear 5400 ./dir1/dir2/earFile1.ear Otherwise use -exec ls -lh and pick out the size field from the output. Again on Solaris 10: % find . -name '*.ear' -exec ls -lh {} \; | awk '{print $5 $9}' 5.3M ./dir1/dir2/earFile2.ear 5.3M ./dir1/dir2/earFile3.ear 5.3M ./dir1/dir2/earFile1.ear  find . -name '*.ear' -exec ls -lh {} \; just the h extra from jer.drab.org's reply. saves time converting to MB mentally ;) This version will exec ""ls"" process for each file. If you have many files (say over a thousand) you better optimize that by either: `find . -name '*.ear' -exec ls -lh {} + \;` (GNU extension) or `find . -name '*.ear' -print0 | xargs -0 ls -lh`. Also you may like to add `-type f` if you're only interested in files (or add `-d` to ls if you want directories themselves included without their contents).  You need to use -exec or -printf. Printf works like this: find . -name *.ear -printf ""%p %k KB\n"" -exec is more powerful and lets you execute arbitrary commands - so you could use a version of 'ls' or 'wc' to print out the filename along with other information. 'man find' will show you the available arguments to printf which can do a lot more than just filesize. [edit] -printf is not in the official POSIX standard so check if it is supported on your version. However most modern systems will use GNU find or a similarly extended version so there is a good chance it will be implemented. +1 This seems more cleaner. To format the output I would prefer add `column` command. `find . -name *.ear -printf ""%p %k KB\n"" | column -t` It looks like your example is more precise but I can't seem to get the example to work on Solaris 10. I'm afraid Solaris find does not support -printf at all: http://jrwren.wrenfam.com/blog/2006/10/07/solaris-find-sucks/ http://www.cs.bgu.ac.il/~arik/usail/man/solaris/find.1.html You could install GNU find if you can be bothered otherwise you need to use exec or | as suggested by others.  I struggled with this on Mac OS X where the find command doesn't support -printf. A solution that I found that admittedly relies on the 'group' for all files being 'staff' was... ls -l -R | sed 's/\(.*\)staff *\([0-9]*\)..............\(.*\)/\2 \3/' This splits the ls long output into three tokens the stuff before the text 'staff' the file size the file name And then outputs tokens 2 and 3 i.e. output is number of bytes and then filename 8071 sections.php 54681 services.php 37961 style.css 13260 thumb.php 70951 workshops.php  You could try this: find. -name *.ear -exec du {} \; This will give you the size in bytes. But the du command also accepts the parameters -k for KB and -m for MB. It will give you an output like 5000 ./dir1/dir2/earFile1.ear 5400 ./dir1/dir2/earFile2.ear 5400 ./dir1/dir3/earFile1.ear  This should get you what you're looking for formatting included (i.e. file name first and size afterward): find . -type f -iname ""*.ear"" -exec du -ah {} \; | awk '{print $2""\t"" $1}' sample output (where I used -iname ""*.php"" to get some result): ./plugins/bat/class.bat.inc.php 20K ./plugins/quotas/class.quotas.inc.php 8.0K ./plugins/dmraid/class.dmraid.inc.php 8.0K ./plugins/updatenotifier/class.updatenotifier.inc.php 4.0K ./index.php 4.0K ./config.php 12K ./includes/mb/class.hwsensors.inc.php 8.0K  Unix find command options with practical examples Scripter"40,A,Alternative to FreeLists for hosting my (developer/software) maling lists? For many years I have had a low-traffic mailing list at freelists.org to provide users of my hobby software projects with updates about releases and other news. Recently I noticed they now send out monthly reminders to everyone subscribing to any of their hosted lists informing them which lists the user is subscribed to. I don't want this extra noise on my mailing list so I am now looking for a new provider. Any suggestions or recommendations? Update Both Google Groups and SourceForge mailing list seems to be viable options. My project is hosted on Sourceforge and I have a Google account so which is better and why? I use GoogleGroups for the same reason. It is also handy if you already have a Google-account for any of their services.  If your software is open source you get all of that (and more) on Sourceforge.41,A,"Why am I getting a malloc: double free error with realloc()? I've tried to write a string replace function in C which works on a char * which has been allocated using malloc(). It's a little different in that it will find and replace strings rather than characters in the starting string. It's trivial to do if the search and replace strings are the same length (or the replace string is shorter than the search string) since I have enough space allocated. If I try to use realloc() I get an error that tells me I am doing a double free - which I don't see how I am since I am only using realloc(). Perhaps a little code will help: void strrep(char *input char *search char *replace) { int searchLen = strlen(search); int replaceLen = strlen(replace); int delta = replaceLen - searchLen; char *find = input; while (find = strstr(find search)) { if (delta > 0) { realloc(input strlen(input) + delta); find = strstr(input search); } memmove(find + replaceLen find + searchLen strlen(input) - (find - input)); memmove(find replace replaceLen); } } The program works until I try to realloc() in an instance where the replaced string will be longer than the initial string. (It still kind of works it just spits out errors as well as the result). If it helps the calling code looks like: #include <stdio.h> #include <string.h> #include <stdlib.h> void strrep(char *input char *search char *replace); int main(void) { char *input = malloc(81); while ((fgets(input 81 stdin)) != NULL) { strrep(input ""Noel"" ""Christmas""); } } As a general rule you should never do a free or realloc on a user provided buffer. You don't know where the user allocated the space (in your module in another DLL) so you cannot use any of the allocation functions on a user buffer. Provided that you now cannot do any reallocation within your function you should change a little its behavior like doing only one replacement so the user will be able to compute the resulting string max length and provide you with a buffer long enough for this one replacement to occur. Then you could create another function to do the multiple replacements but you will have to allocate the whole space for the resulting string and copy the user input string. Then you must provide a way to delete the string you allocated. Resulting in: void strrep(char *input char *search char *replace); char* strrepm(char *input char *search char *replace); void strrepmfree(char *input);  This seems to work; char *strrep(char *string const char *search const char *replace) {  char *p = strstr(string search);  if (p) {  int occurrence = p - string;  int stringlength = strlen(string);  int searchlength = strlen(search);  int replacelength = strlen(replace);  if (replacelength > searchlength) {  string = (char *) realloc(string strlen(string)  + replacelength - searchlength + 1);  }  if (replacelength != searchlength) {  memmove(string + occurrence + replacelength  string + occurrence + searchlength  stringlength - occurrence - searchlength + 1);  }  strncpy(string + occurrence replace replacelength);  }  return string; } Sigh is there anyway to post code without it sucking?  Note try to edit your code to get rid of the html escape codes. Well though it has been a while since I used C/C++ realloc that grows only reuses the memory pointer value if there is room in memory after your original block. For instance consider this: (xxxxxxxxxx..........) If your pointer points to the first x and . means free memory location and you grow the memory size pointed to by your variable by 5 bytes it'll succeed. This is of course a simplified example as blocks are rounded up to a certain size for alignment but anyway. However if you subsequently try to grow it by another 10 bytes and there is only 5 available it will need to move the block in memory and update your pointer. However in your example you are passing the function a pointer to the character not a pointer to your variable and thus while the strrep function internally might be able to adjust the variable in use it is a local variable to the strrep function and your calling code will be left with the original pointer variable value. This pointer value however has been freed. In your case input is the culprit. However I would make another suggestion. In your case it looks like the input variable is indeed input and if it is it shouldn't be modified at all. I would thus try to find another way to do what you want to do without changing input as side-effects like this can be hard to track down.  I've tried using input = realloc(...) but that appears to just case some other problems which considering this is a slightly ""non-recommended"" practice of altering strings that I don't know where they've come from I will take lassevk's advice of doing it another way... ...Since this isn't really production code (it's for learning only) and the specification said ""you can limit input to 80 characters"" and the only increasing replace is ""Noel"" -> ""Christmas"" I can just initially allocate 161 characters and leave it be.  Someone else apologized for being late to the party - two and a half months ago. Oh well I spend quite a lot of time doing software archaeology. I'm interested that no-one has commented explicitly on the memory leak in the original design or the off-by-one error. And it was observing the memory leak that tells me exactly why you are getting the double-free error (because to be precise you are freeing the same memory multiple times - and you are doing so after trampling over the already freed memory). Before conducting the analysis I'll agree with those who say your interface is less than stellar; however if you dealt with the memory leak/trampling issues and documented the 'must be allocated memory' requirement it could be 'OK'. What are the problems? Well you pass a buffer to realloc() and realloc() returns you a new pointer to the area you should use - and you ignore that return value. Consequently realloc() has probably freed the original memory and then you pass it the same pointer again and it complains that you're freeing the same memory twice because you pass the original value to it again. This not only leaks memory but means that you are continuing to use the original space -- and John Downey's shot in the dark points out that you are misusing realloc() but doesn't emphasize how severely you are doing so. There's also an off-by-one error because you do not allocate enough space for the NUL '\0' that terminates the string. The memory leak occurs because you do not provide a mechanism to tell the caller about the last value of the string. Because you kept trampling over the original string plus the space after it it looks like the code worked but if your calling code freed the space it too would get a double-free error or it might get a core dump or equivalent because the memory control information is completely scrambled. Your code also doesn't protect against indefinite growth -- consider replacing 'Noel' with 'Joyeux Noel'. Every time you would add 7 characters but you'd find another Noel in the replaced text and expand it and so on and so forth. My fixup (below) does not address this issue - the simple solution is probably to check whether the search string appears in the replace string; an alternative is to skip over the replace string and continue the search after it. The second has some non-trivial coding issues to address. So my suggested revision of your called function is: char *strrep(char *input char *search char *replace) { int searchLen = strlen(search); int replaceLen = strlen(replace); int delta = replaceLen - searchLen; char *find = input; while ((find = strstr(find search)) != 0) { if (delta > 0) { input = realloc(input strlen(input) + delta + 1); find = strstr(input search); } memmove(find + replaceLen find + searchLen strlen(input) + 1 - (find - input)); memmove(find replace replaceLen); } return(input); } This code does not detect memory allocation errors - and probably crashes (but if not leaks memory) if realloc() fails. See Steve Maguire's 'Writing Solid Code' book for an extensive discussion of memory management issues. Thanks this is a really good analysis of what I was doing wrong (and that the double-free was in a sense a by-product of the several things I was doing wrong.) I think I had it in my head that realloc() just extended the memory allocation - which makes no sense at all when I think about it!  My quick hints. Instead of: void strrep(char *input char *search char *replace) try: void strrep(char *&input char *search char *replace) and than in the body: input = realloc(input strlen(input) + delta); Generally read about passing function arguments as values/reference and realloc() description :).  realloc is strange complicated and should only be used when dealing with lots of memory lots of times per second. i.e. - where it actually makes your code faster. I have seen code where realloc(bytes smallerSize); was used and worked to resize the buffer making it smaller. Worked about a million times then for some reason realloc decided that even if you were shortening the buffer it would give you a nice new copy. So you crash in a random place 1/2 a second after the bad stuff happened. Always use the return value of realloc.  Just a shot in the dark because I haven't tried it yet but when you realloc it returns the pointer much like malloc. Because realloc can move the pointer if needed you are most likely operating on an invalid pointer if you don't do the following: input = realloc(input strlen(input) + delta); And if realloc fails it returns NULL and leaves the existing buffer alone. You've just lost the pointer... :-(  First off sorry I'm late to the party. This is my first stackoverflow answer. :) As has been pointed out when realloc() is called you can potentially change the pointer to the memory being reallocated. When this happens the argument ""string"" becomes invalid. Even if you reassign it the change goes out of scope once the function ends. To answer the OP realloc() returns a pointer to the newly-reallocated memory. The return value needs to be stored somewhere. Generally you would do this: data *foo = malloc(SIZE * sizeof(data)); data *bar = realloc(foo NEWSIZE * sizeof(data)); /* Test bar for safety before blowing away foo */ if (bar != NULL) {  foo = bar;  bar = NULL; } else {  fprintf(stderr ""Crap. Memory error.\n"");  free(foo);  exit(-1); } As TyBoer points out you guys can't change the value of the pointer being passed in as the input to this function. You can assign whatever you want but the change will go out of scope at the end of the function. In the following block ""input"" may or may not be an invalid pointer once the function completes: void foobar(char *input int newlength) {  /* Here I ignore my own advice to save space. Check your return values! */  input = realloc(input newlength * sizeof(char)); } Mark tries to work around this by returning the new pointer as the output of the function. If you do that the onus is on the caller to never again use the pointer he used for input. If it matches the return value then you have two pointers to the same spot and only need to call free() on one of them. If they don't match the input pointer now points to memory that may or may not be owned by the process. Dereferencing it could cause a segmentation fault. You could use a double pointer for the input like this: void foobar(char **input int newlength) {  *input = realloc(*input newlength * sizeof(char)); } If the caller has a duplicate of the input pointer somewhere that duplicate still might be invalid now. I think the cleanest solution here is to avoid using realloc() when trying to modify the function caller's input. Just malloc() a new buffer return that and let the caller decide whether or not to free the old text. This has the added benefit of letting the caller keep the original string!"42,A,"Java Singleton vs static - is there a real performance benefit? I am merging a CVS branch and one of the larger changes is the replacement wherever it occurs of a Singleton pattern with abstract classes that have a static initialisation block and all static methods. Is this something that's worth keeping since it will require merging a lot of conflicts what sort of situation would I be looking at for this refactoring to be worthwhile? We are running this app under Weblogic 8.1 (so JDK 1.4.2) sorry Thomas let me clarify.. the HEAD version has the traditional singleton pattern (private constructor getInstance() etc) the branch version has no constructor is a 'public abstract class' and modified all the methods on the object to be 'static'. The code that used to exist in the private constructor is moved into a static block. Then all usages of the class are changed which causes multiple conflicts in the merge. There are a few cases where this change was made. From a strict runtime performance point of view the difference is really negligible. The main difference between the two lies down in the fact that the ""static"" lifecycle is linked to the classloader whereas for the singleton it's a regular instance lifecycle. Usually it's better to stay away from the ClassLoader business you avoid some tricky problems especially when you try to reload the web application. A singleton is tied to ClassLoader lifetime of the class loader that loaded it as much as a static is. And? Everything is tied to the lifetime of the ClassLoader but by putting a singleton you gain an extra lifecycle layer with more chances (finalize) to handle things properly.  From my experience the only thing that matters is which one is easier to mock in unit tests. I always felt Singleton is easier and natural to mock out. If your organization lets you use JMockit it doesn't matter since you can overcome these concerns.  Static is bad for extensibility since static methods and fields cannot be extended or overridden by subclasses. It's also bad for unit tests. Within a unit test you cannot keep the side effects of different tests from spilling over since you cannot control the classloader. Static fields initialized in one unit test will be visible in another or worse running tests concurrently will yield unpredictable results. Singleton is generally an ok pattern when used sparingly. I prefer to use a DI framework and let that manage my instances for me (possibly within different scopes as in Guice).  Does this discussion help? (I don't know if it's taboo to link to another programming forum but I'd rather not just quote the whole discussion =) ) Sun Discussion on this subject The verdict seems to be that it doesn't make enough of a difference to matter in most cases though technically the static methods are more efficient.  Write some code to measure the performance. The answer is going to be dependent on the JVM(Sun's JDK might perform differently than JRockit) and the VM flags your application uses.  If my original post was the correct understanding and the discussion from Sun that was linked to is accurate (which I think it might be) then I think you have to make a trade off between clarity and performance. Ask yourself these questions: Does the Singleton object make what I'm doing more clear? Do I need an object to do this task or is it more suited to static methods? Do I need the performance that I can gain by not using a Singleton? The question is actually about the performance gain :-) Consider you have a controller that handles 100 requests per second. is this relevant for choosing between singleton or class as a stateless service ?  I would use a singleton if it needed to store any state and static classes otherwise. There's no point in instantiating something even a single instance unless it needs to store something."43,A,"Is the C# static constructor thread safe? In other words is this Singleton implementation thread safe: public class Singleton { private static Singleton instance; private Singleton() { } static Singleton() { instance = new Singleton(); } public static Singleton Instance { get { return instance; } } } It is thread-safe. Suppose several threads want to get the property `Instance` at once. One of the threads will be told to first run the type initializer (also known as the static constructor). Meanwhile all other threads wanting to read the `Instance` property will be ***locked*** until the type initializer has finished. Only after the field initializer has concluded will threads be allowed to get the `Instance` value. So no-one can see `Instance` being `null`. Using a static constructor actually is threadsafe. The static constructor is guaranteed to be executed only once. From the C# language specification http://msdn.microsoft.com/en-us/library/aa645612(VS.71).aspx: The static constructor for a class executes at most once in a given application domain. The execution of a static constructor is triggered by the first of the following events to occur within an application domain: An instance of the class is created. Any of the static members of the class are referenced. So yes you can trust that your singleton will be correctly instantiated. Zooba made an excellent point (and 15 seconds before me too!) that the static constructor will not guarantee thread-safe shared access to the singleton. That will need to be handled in another manner.  Static constructors are guaranteed to be run only once per application domain before any instances of a class are created or any static members are accessed. http://msdn.microsoft.com/en-us/library/aa645612.aspx The implementation shown is thread safe for the initial construction that is no locking or null testing is required for constructing the Singleton object. However this does not mean that any use of the instance will be synchronised. There are a variety of ways that this can be done; I've shown one below. public class Singleton { private static Singleton instance; // Added a static mutex for synchronising use of instance. private static System.Threading.Mutex mutex; private Singleton() { } static Singleton() { instance = new Singleton(); mutex = new System.Threading.Mutex(); } public static Singleton Acquire() { mutex.WaitOne(); return instance; } // Each call to Acquire() requires a call to Release() public static void Release() { mutex.ReleaseMutex(); } } For others who might be tripped up by this: Any static field members with initializers are initialized _before_ the static constructor is called. Simple thread-safe singleton blank for everyday use: http://ilyatereschuk.blogspot.com/2013/12/c-simple-singleton-blank-for-everyday.html @user2604650 There's nothing thread-safe about this except for instanciating the singleton which it would already be if it were initialized in the static constructor. Personally I think using a pattern like that for a singleton would be overkill since in most cases the only time you access the singleton would be during the accessing of the static instance property which in result is then the time of calling the static constructor. The answer these days is to use `Lazy` - anyone who uses the code I originally posted is doing it wrong (and honestly it wasn't that good to begin with - 5-years-ago-me wasn't as good at this stuff as current-me is :) ). Why use a `Mutex` here? The `Acquire` method just returns a reference to an instance (this is a reference type). Whether the object referenced is mutable or immutable is irrelevant. The reference is to the same object no matter if the object mutates. Why do locking? Even if the instance was not ""singleton"" or ""read-only"" i.e. even if the field reference could change since [reference assignments are atomic](http://stackoverflow.com/questions/5209623/) there would still be no need for locking/`Mutex`. I will downvote this answer @ +109 net upvotes... @JeppeStigNielsen The idea is that you keep the mutex while you're using the object and then release it when you're done. Again there are better ways to do it depending on how much overhead you want to force upon your users. I'm more a fan of an IDisposable/using() pattern now but the concept is the same. Note that if your singleton object is immutable using a mutex or any synchronization mechanism is an overkill and should not be used. Also I find the above sample implementation extremely brittle :-). All code using Singleton.Acquire() is expected to call Singleton.Release() when it's done using the singleton instance. Failing to do this (e.g. returning prematurely leaving scope via exception forgetting to call Release) next time this Singleton is accessed from a different thread it will deadlock in Singleton.Acquire(). Agreed though I'd go further. If your singleton is immutable using a singleton is overkill. Just define constants. Ultimately using a singleton properly requires that the developers know what they are doing. As brittle as this implementation is it is still better than the one in the question where those errors manifest randomly rather than as an obviously unreleased mutex. One way to lessen the brittleness of the Release() method is to use another class with IDisposable as a sync handler. When you acquire the singleton you get the handler and can put the code requiring the singleton into a using block to handle the release.  Static constructors are guaranteed to fire only once per App Domain so your approach should be OK. However it is functionally no different from the more concise inline version: private static readonly Singleton instance = new Singleton(); Thread safety is more of an issue when you are lazily initializing things. Andrew that is not fully equivalent. By not using a static constructor some of the guarantees about when the initializer will be executed are lost. Please see these links for in-depth explanation: * * Derek I'm familiar with the *beforefieldinit* ""optimization"" but personally I never worry about it.  Although other answers are mostly correct there is yet another caveat with static constructors. As per section II.10.5.3.3 Races and deadlocks of the ECMA-335 Common Language Infrastructure Type initialization alone shall not create a deadlock unless some code called from a type initializer (directly or indirectly) explicitly invokes blocking operations. The following code results in a deadlock using System.Threading; class MyClass { static void Main() { /* Wont run... the static constructor deadlocks */ } static MyClass() { Thread thread = new Thread(arg => { }); thread.Start(); thread.Join(); } } Original author is Igor Ostrovsky see his post here.  The Common Language Infrastructure specification guarantees that ""a type initializer shall run exactly once for any given type unless explicitly called by user code."" (Section 9.5.3.1.) So unless you have some whacky IL on the loose calling Singleton::.cctor directly (unlikely) your static constructor will run exactly once before the Singleton type is used only one instance of Singleton will be created and your Instance property is thread-safe. Note that if Singleton's constructor accesses the Instance property (even indirectly) then the Instance property will be null. The best you can do is detect when this happens and throw an exception by checking that instance is non-null in the property accessor. After your static constructor completes the Instance property will be non-null. As Zoomba's answer points out you will need to make Singleton safe to access from multiple threads or implement a locking mechanism around using the singleton instance.  Just to be pedantic but there is no such thing as a static constructor but rather static type initializers here's a small demo of cyclic static constructor dependency which illustrates this point.  While all of these answers are giving the same general answer there is one caveat. Remember that all potential derivations of a generic class are compiled as individual types. So use caution when implementing static constructors for generic types. class MyObject<T> { static MyObject() { //this code will get executed for each T. } } EDIT: Here is the demonstration: static void Main(string[] args) { var obj = new Foo<object>(); var obj2 = new Foo<string>(); } public class Foo<T> { static Foo() { System.Diagnostics.Debug.WriteLine(String.Format(""Hit {0}"" typeof(T).ToString())); } } In the console: Hit System.Object Hit System.String typeof(MyObject) != typeof(MyObject); I think that is the point i'm trying to make. Generic types are compiled as individual types based upon which generic parameters are used so the static constructor can and will be called multiple times. This is right when T is of value type for reference type T only one generic type would be generated @sll: Not True... See my Edit See [Generics in the Runtime](http://msdn.microsoft.com/en-us/library/f4a6ta2h(v=vs.80).aspx) `When a generic type is first constructed with a value type as a parameter the runtime creates a specialized generic type with the supplied parameter or parameters substituted in the appropriate places in the MSIL. Specialized generic types are created once for each unique value type used as a parameter` Interesting but really static cosntructor called for all types just tried for multiple reference types @Brian Rudolph: what is shared is the _definition_ of the static constructor take a look at [this code](http://ideone.com/vZC55g)  Static constructor is guaranteed to be thread safe. Also check out the discussion on Singleton at DeveloperZen: http://www.developerzen.com/2007/07/15/whats-wrong-with-this-code-1-discussion/  Here's the Cliffnotes version from the above MSDN page on c# singleton: Use the following pattern always you can't go wrong: public sealed class Singleton { private static readonly Singleton instance = new Singleton(); private Singleton(){} public static Singleton Instance { get { return instance; } } } Beyond the obvious singleton features it gives you these two things for free (in respect to singleton in c++): lazy construction (or no construction if it was never called) synchronization Lazy if the class is doesn't have any other unrelated statics (like consts). Otherwise accessing any static method or property will result instance creation. So I wouldn't call it lazy."44,A,How to set the order in subnodes of a tree structure I have a tree representation of pages in a CMS application. I understand how to persist the tree in the database. However I don't have a good way to: A) Reorder subpages under a particular parent page. B) Provide a UI implementation that allows the user to change the order. Any suggestions? A) I have a similar CMS app and I store an ordinal value with the page for a particular tree and sort on this value -- because lots of my pages appear in completely different sites I have to maintain the ordinal number against a page / tree combination. B) I too would like a better way to do this. Currently they click on the node in the treeview and in the main page screen they can move the page around. I've tried drag and drop with java script and other solutions but my users could never work with it without lots of hand holding. I'll be interested in the responses to this one.  Changing the order itself will require you store some sort of ordering along with each page in the database. Just the current highest / lowest value +/- 1 would probably be a fine starting point. Once you've got that ordering in there reordering becomes a case of swapping two values or changing the value for one page to be between two others (you could use floats I guess but you may need to renumber if you split it too many times). Anyway once you've got that you need a UI. I've seen a very simple 'swap this with the one above/below' approach which can be a simple web link or an AJAX call. You could also present all the page values to the user and ask them to renumber them as they see fit. If you want to get fancy JavaScript drag and drop might be a good approach. I've used ExtJS and Mootools as frameworks in this kind of area. If you don't need all the Extjs widgets I'd say well away from it in future and look at something like the Mootools Dynamic Sortables demo.45,A,"KVM/QEMU network TAP problems with libvirt I'm trying to use libvirt with virsh to manage my kvm/qemu vms. The problem I have is with getting it to work with public IPs. The server is running ubuntu 8.04. libvirt keeps trying to run it as: /usr/bin/kvm -M pc -m 256 -smp 3 -monitor pty -no-acpi \ -drive file=/opt/virtual-machines/calculon/root.qcow2if=ideboot=on \ -net nicvlan=0model=virtio -net tapfd=10vlan=0 -usb -vnc 127.0.0.1:0 Which boots but does not have any network access (pings go nowhere). Running it without fd=10 makes it work right with kvm creating the necessary TAP device for me and networking functioning inside the host. All the setup guides I've seen focus on setting up masquerading while I just want a simple bridge and unfiltered access to the net (both the guests and host must use public IPs). Running ifconfig on the host gives this the bridge is manually setup in my /etc/network/interfaces file. : br0 Link encap:Ethernet HWaddr 00:1e:c9:3c:59:b8 inet addr:12.34.56.78 Bcast:12.34.56.79 Mask:255.255.255.240 inet6 addr: fe80::21e:c9ff:fe3c:59b8/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3359 errors:0 dropped:0 overruns:0 frame:0 TX packets:3025 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:180646 (176.4 KB) TX bytes:230908 (225.4 KB) eth0 Link encap:Ethernet HWaddr 00:1e:c9:3c:59:b8 inet6 addr: fe80::21e:c9ff:fe3c:59b8/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:6088386 errors:0 dropped:0 overruns:0 frame:0 TX packets:3058 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:680236624 (648.7 MB) TX bytes:261696 (255.5 KB) Interrupt:33 Any help would be greatly appreciated. This is a programming site not tech support. I disagree. The lines are blurred. System administration and configuration are as vital to a program as the code itself and present similar challenges. I followed the bridged networking guide at https://help.ubuntu.com/community/KVM and have the following in /etc/network/interfaces: auto eth0 iface eth0 inet manual auto br0 iface br0 inet static address 192.168.0.10 network 192.168.0.0 netmask 255.255.255.0 broadcast 192.168.0.255 gateway 192.168.0.1 bridge_ports eth0 bridge_fd 9 bridge_hello 2 bridge_maxage 12 bridge_stp off I have not changed any libvirt network settings and my kvm images are booted like: /usr/bin/kvm -M pc -no-kqemu -m 256 -smp 1 -monitor pty -boot c -hda \ /libvirt/apt.img -net nicmacaddr=00:16:3e:77:32:1dvlan=0 -net \ tapfd=11script=vlan=0 -usb -vnc 127.0.0.1:0 I then specify the static network settings in the kvm image as normal. Has all worked ok since I followed the guide. I do have the following settings in my xml files in /etc/libvirt/qemu/ though under devices: <interface type='bridge'> <mac address='00:16:3e:77:32:1d'/> <source bridge='br0'/> </interface>  i guess your tap device should be shown in ifconfig. run ""brctl show "" it will show bridge and tunnel device connection. you may have to put iptable entry show that all the packets will be routed through bridge iptables -I INPUT -i br0 -j ACCEPT"46,A,"catching button clicks in javascript without server interaction I've got a sign up form that requires the user to enter their email and password both are in two separate text boxes. I want to provide a button that the user can click so that the password (which is masked) will appear in a popup when the user clicks the button. Currently my JavaScript code for this is as follows:  function toggleShowPassword() { var button = $get('PASSWORD_TEXTBOX_ID'); var password; if (button) { password = button.value; alert(password); button.value = password; } } The problem is that every time the user clicks the button the password is cleared in both Firefox and IE. I want them to be able to see their password in clear text to verify without having to retype their password. My questions are: Why does the password field keep getting reset with each button click? How can I make it so the password field is NOT cleared once the user has seen his/her password in clear text? You do not need to do button.value = password; since reading the value does not change it. I'm not sure why it's being cleared maybe JavaScript does not allow password field values to be modified.  hah! the answer if here: http://forums.asp.net/p/1067527/1548528.aspx I figured out the solution... the fix was simple change  OnClientClick=""myOnClick()"" to  OnClientClick=""return myOnClick()"" Here's the fully corrected code... function myOnClick() { //perform some other actions... return false; } Untitled Page  I did a quick example up of a working version: <html> <head> <script type=""text/javascript"" src=""prototype.js""></script> <script type=""text/javascript""> function toggleShowPassword() { var textBox = $('PasswordText'); if (textBox) { alert(textBox.value); } } </script> </head> <body> <input type=""password"" id=""PasswordText"" /><input type=""button"" onclick=""toggleShowPassword();"" value=""Show Password"" /> </body> </html> The key is that the input is of type button and not submit. I used the prototype library for retrieving the element by ID.  In your HTML: <input type=""button"" onclick=""toggleShowPassword();""> You need to use ""button"" rather than ""submit"" to prevent your form from posting.  It sounds that you're doing a request to the server on each click the password box being reset in each page load is typical behavior of the browsers.  I would assume that the browser has some issue with the script attempting to set the value of a password field: button.value = password; This line of code has no real purpose. password.value is not affected in the previous lines where you are reading the value and using it in the alert(). This should be a simpler version of your code: function toggleShowPassword() { var button = $get('PASSWORD_TEXTBOX_ID'); if (button) { alert(button.value); } } edit: actually I just did a quick test and Firefox has no problem setting the password field's value with code such as button.value = ""blah"". So it doesn't seem like this would be the case ... I would check if your ASP.NET code is causing a postback as others have suggested.  You didn't say you were using ASP.NET but... By design ASP.NET clears during postback the value of TextBox controls whose Mode is Password. I work around this in a subclass with the following code:  // If the TextMode is ""password"" the Text property won't work if ( TextMode == System.Web.UI.WebControls.TextBoxMode.Password ) Attributes[ ""value"" ] = stringValue;  If you don't want the button to submit the form then be sure it has type 'button' rather than 'submit'. For example you might do something like this: <input type=""button"" value=""Show My Password"" onclick=""toggleShowPassword()""/>"47,A,OSCache vs. EHCache Never used a cache like this before. The problem is that I want to load 500000 + records out of a database and do some selecting/filtering wicked fast. I'm thinking about using a cache and preliminarily found EHCache and OSCache any opinions? Why do you think using the cache will be faster than selecting/filtering in the database? That's kind of what they do. :) It sort of depends on your needs. If you're doing the work in memory on one machine then ehcache will work perfectly assuming you have enough RAM or a fast enough hard disk so that the overflow doesn't cause disk paging/thrashing. if you find you need to achieve scalability even despite this particular operation happening a lot then you'll probably want to do clustering. JGroups /TreeCache from JBoss support this so does EHcache (I think) and I know it definitely works if you use Ehcache with terracotta which is a very slick integration. This answer doesn't speak directly to the merits of EHcache and OSCache so here's that answer: EHcache seems to have the most inertia (used to be the default well known active development including a new cache server) and OSCache seemed (at least at one point) to have slightly more features but I think that with the options mentioned above those advantages are moot/superseded. Ah the other thing I forgot to mention is that transactionality of the data is important and your requirements will refine the list of valid choices.  I have used oscache on several spring projects with spring-modules using the aop based configuration. Recently I looked to use oscache + spring modules on a Spring 3.x project but found spring-modules annotation-based caching is not supported (even by the fork). I recently found out about this project - http://code.google.com/p/ehcache-spring-annotations/ Which supports spring 3.x with declarative annotation-based caching using ehcache.  I've used JCS (http://jakarta.apache.org/jcs/) and it seems solid and easy to use programatically.  They're both pretty solid projects. If you have pretty basic caching needs either one of them will probably work as well as the other. You may also wish to consider doing the filtering in a database query if it's feasible. Often using a tuned query that returns a smaller result set will give you better performance than loading 500000 rows into memory and then filtering them.  Either way I recommend using them with Spring Modules. The cache can be transparent to the application and cache implementations are trivially easy to swap. In addition to OSCache and EHCache Spring Modules also support Gigaspaces and JBoss cache. As to comparisons.... OSCache is easier to configure EHCache has more configuration options They are both rock solid both support mirroring cache both work with Terracotta both support in-memory and to-disk caching.  I mainly use EhCache because it used to be the default cache provider for Hibernate. There is a list of caching solutions on Java-Source.net. I used to have a link that compared the main caching solutions. If I find it I will update this answer.  Other answers discuss pros/cons for caches; but I am wondering whether you actually benefit from cache at all. It is not quite clear exactly what you plan on doing here and why a cache would be beneficial: if you have the data set at your use just access that. Cache only helps reuse things between otherwise independent tasks. If this is what you are doing yes caching can help. But if it is a big task that can carry along its data set caching would add no value.  Judging by their releases page OSCache has not been actively maintained since 2007. This is not a good thing. EhCache on the other hand is under constant development. For that reason alone I would choose EhCache. Edit Nov 2013: OSCache like the rest of OpenSymphony is dead. +1 This is a very big factor in deciding what open source software to use http://www.opensymphony.com/ OpenSymphony has now publicly announced that they are dead.48,A,Should I always use the AndAlso and OrElse operators? Is there ever a circumstance in which I would not want to use the AndAlso operator rather than the And operator? _or in which I would not want to use the OrElse operator rather than the Or operator? From MSDN: Short-Circuiting Trade-Offs Short-circuiting can improve performance by not evaluating an expression that cannot alter the result of the logical operation. However if that expression performs additional actions short-circuiting skips those actions. For example if the expression includes a call to a Function procedure that procedure is not called if the expression is short-circuited and any additional code contained in the Function does not run. If your program logic depends on any of that additional code you should probably avoid short-circuiting operators. One could argue that relying on your logic code to run a function is obscure and your logic should be designed to not rely on that for clarity and maintainability. I agree. The real lesson here is don't write code that doesn't clearly indicate it's side effects!  Sure: if you want to make sure that both sides of the expression are evaluated. This might be the case if for example both sides are method calls that return booleans as a result of some other operation. But in general you should AndAlso/OrElse whenever you would use &&/|| in C/C++/C# which of course is the vast majority of the time.49,A,"Interpreting Stacks in Windows Minidumps As someone who is just starting to learn the intricacies of computer debugging for the life of me I can't understand how to read the Stack Text of a dump in Windbg. I've no idea of where to start on how to interpret them or how to go about it. Can anyone offer direction to this poor soul? ie (the only dump I have on hand with me actually) >b69dd8f0 bfa1e255 016d2fc0 89efc000 00000040 nv4_disp+0x48b94 b69dd8f4 016d2fc0 89efc000 00000040 00000006 nv4_disp+0x49255 b69dd8f8 89efc000 00000040 00000006 bfa1dcc0 0x16d2fc0 b69dd8fc 00000000 00000006 bfa1dcc0 e1e71018 0x89efc000 I know the problem is to do with the Nvidia display driver but what I want to know is how to actually read the stack (eg what is b69dd8f4?) :-[ http://support.microsoft.com/kb/315263 and http://www.networkworld.com/news/2005/041105-windows-crash.html  It might help to include an example of the stack you are trying to read. A good tip is to ensure you have correct debug symbols for all modules shown in the stack. This includes symbols for modules in the OS Microsoft has made their symbol server publicly available.  First you need to have the proper symbols configured. The symbols will allow you to match memory addresses to function names. In order to do this you have to create a local folder in your machine in which you will store a local cache of symbols (for example: C:\symbols). Then you need to specify the symbols server path. To do this just go to: File > Symbol File Path and type: SRV*c:\symbols*http://msdl.microsoft.com/download/symbols You can find more information on how to correctly configure the symbols here. Once you have properly configured the Symbols server you can open the minidump from: File > Open Crash Dump. Once the minidump is opened it will show you on the left side of the command line the thread that was executing when the dump was generated. If you want to see what this thread was executing type: kpn 200 This might take some time the first you execute it since it has to download the necessary public Microsoft related symbols the first time. Once all the symbols are downloaded you'll get something like: 01 MODULE!CLASS.FUNCTIONNAME1(...) 02 MODULE!CLASS.FUNCTIONNAME2(...) 03 MODULE!CLASS.FUNCTIONNAME3(...) 04 MODULE!CLASS.FUNCTIONNAME4(...) Where: THE FIRST NUMBER: Indicates the frame number MODULE: The DLL that contains the code CLASS: (Only on C++ code) will show you the class that contains the code FUNCTIONAME: The method that was called. If you have the correct symbols you will also see the parameters. You might also see something like 01 MODULE!+989823 This indicates that you don't have the proper Symbol for this DLL and therefore you are only able to see the method offset. So what is a callstack? Imagine you have this code: void main() { method1(); } void method1() { method2(); } int method2() { return 20/0; } In this code method2 basically will throw an Exception since we are trying to divide by 0 and this will cause the process to crash. If we got a minidump when this occurred we would see the following callstack: 01 MYDLL!method2() 02 MYDLL!method1() 03 MYDLL!main() You can follow from this callstack that ""main"" called ""method1"" that then called ""method2"" and it failed. In your case you've got this callstack (which I guess is the result of running ""kb"" command) b69dd8f0 bfa1e255 016d2fc0 89efc000 00000040 nv4_disp+0x48b94 b69dd8f4 016d2fc0 89efc000 00000040 00000006 nv4_disp+0x49255 b69dd8f8 89efc000 00000040 00000006 bfa1dcc0 0x16d2fc0 b69dd8fc 00000000 00000006 bfa1dcc0 e1e71018 0x89efc000 The first column indicates the Child Frame Pointer the second column indicates the Return address of the method that is executing the next three columns show the first 3 parameters that were passed to the method and the last part is the DLL name (nv4_disp) and the offset of the method that is being executed (+0x48b94). Since you don't have the symbols you are not able to see the method name. I doubt tha NVIDIA offers public access to their symbols so I gues you can't get much information from here. I recommend you run ""kpn 200"". This will show you the full callstack and you might be able to see the origin of the method that caused this crash (if it was a Microsoft DLL you should have the proper symbols in the steps that I provided you). At least you know it's related to a NVIDIA bug ;-) Try upgrading the DLLs of this driver to the latest version. In case you want to learn more about WinDBG debugging I recommend the following links: If broken it is fix it you should TechNet Webcast: Windows Hang and Crash Dump Analysis Delicious.com popular links on WinDBG Thanks man! Helping me a lot! This answer is awesome. SO shouuld have more answers like this.  A really good tutorial on interpreting a stack trace is available here: http://www.codeproject.com/KB/debug/cdbntsd2.aspx However even with a tutorial like that it can be very difficult (or near impossible) to interpret a stack dump without the proper symbols available/loaded."50,A,"C++ : Opening a file in non exclusive mode I have to develop an application which parses a log file and sends specific data to a server. It has to run on both Linux and Windows. The problem appears when I want to test the log rolling system (which appends .1 to the name of the creates a new one with the same name). On Windows (haven't tested yet on Linux) I can't rename a file that I have opened with std::ifstream() (exclusive access?) even if I open it in ""input mode"" (ios::in). Is there a cross-platform way to open file in a non-exclusive way? Win32 filesystem semantics require that a file you rename not be open (in any mode) at the time you do the rename. You will need to close the file rename it and then create the new log file. Unix filesystem semantics allow you to rename a file that's open because the filename is just a pointer to the inode.  Is there a way to open file in a non-exclusive way Yes using Win32 passing the various FILE_SHARE_Xxxx flags to CreateFile. is it cross platform? No it requires platform-specific code. Due to annoying backwards compatibility concerns (DOS applications being single-tasking assume that nothing can delete a file out from under them i.e. that they can fclose() and then fopen() without anything going amiss; Win16 preserved this assumption to make porting DOS applications easier Win32 preserved this assumption to make porting Win16 applications easier and it's awful) Windows defaults to opening files exclusively. The underlying OS infrastructure supports deleting/renaming open files (although I believe it does have the restriction that memory-mapped files cannot be deleted which I think isn't a restriction found on *nix) but the default opening semantics do not. C++ has no notion of any of this; the C++ operating environment is much the same as the DOS operating environment--no other applications running concurrently so no need to control file sharing.  It's not the reading operation that's requiring the exclusive mode it's the rename because this is essentially the same as moving the file to a new location. I'm not sure but I don't think this can be done. Try copying the file instead and later delete/replace the old file when it is no longer read.  I'd make sure you don't keep files open. This leads to weird stuff if your app crashes for example. What I'd do: Abstract (reading / writing / rolling over to a new file) into one class and arrange closing of the file when you want to roll over to a new one in that class. (this is the neatest way and since you already have the roll-over code you're already halfway there.) If you must have multiple read/write access points need all features of fstreams and don't want to write that complete a wrapper then the only cross platform solution I can think of is to always close the file when you don't need it and have the roll-over code try to acquire exclusive access to the file a few times when it needs to roll-over before giving up.  If you are only reading from the file I know it can be done with windows api CreateFile. Just specify FILE_SHARE_DELETE | FILE_SHARE_READ | FILE_SHARE_WRITE as the input to dwShareMode. Unfortunally this is not crossplatform. But there might be something similar for Linux. See msdn for more info on CreateFile. EDIT: Just a quick note about Greg Hewgill comment. I've just tested with the FILE_SHARE* stuff (too be 100% sure). And it is possible to both delete and rename files in windows if you open read only and specify the FILE_SHARE* parameters."51,A,"Should I provide accessor methods / Getter Setters for public/protected components on a form? If I have .Net Form with a component/object such as a textbox that I need to access from a parent or other form I obviously need to ""upgrade"" the modifier to this component to an Internal or Public level variable. Now if I were providing a public variable of an int or string type etc. in my form class I wouldn't think twice about using Getters and (maybe) Setters around this even if they didn't do anything other than provide direct access to the variable. However the VS designer doesn't seem to implement such Getters/Setters for those public objects that are components on a form (and therefore does not comply with good programming practice). So the question is; In order to do the ""right thing"" should I wrap such VS designer components or objects in a Getter and/or Setter?? The reason for not implementing Getters and Setters for components on a form I believe is cause they wouldn't be ""Thread Safe"" .Net objects are suppose to be only modified by the form thread that created them If you put on getter and setters you are potentially opening it up for any thread. Instead your suppose to implement a delegate system where changes to these objects are delegated to the thread that created them and ran there.  I always do that and if you ARE following an MVP design creating getter/setters for your view components would be a design requirement. I do not understand what you mean by ""does not comply with good programming practice"". Microsoft violates a lot of good programming practices to make it easier to create stuff on Visual Studio (for the sake of rapid app development) and I do not see the lack of getters/setters for controls as evidence of violating any such best practices.  This is a classic example of encapsulation in object-oriented design. A Form is an object whose responsibility is to present UI to the user and accept input. The interface between the Form object and other areas of the code should be a data-oriented interface not an interface which exposes the inner implementation details of the Form. The inner workings of the Form (ie the controls) should remain hidden from any consuming code. A mature solution would probably involve the following design points: Public methods or properties are behavior (show hide position) or data-oriented (set data get data update data). All event handlers implemented by the Form are wrapped in appropriate thread delegation code to enforce Form thread-execution rules. Controls themselves would be data-bound to the underlying data structure (where appropriate) to reduce code. And that's not even mentioning meta-development things like unit tests.  ""However the VS designer doesn't seem to implement such Getters/Setters for those public objects that are components on a form (and therefore does not comply with good programming practice)."" If you mean the controls you're dragging and dropping onto the form these are marked as private instance members and are added to the form's Controls collection. Why would they be otherwise? A form could have forty or fifty controls it'd be somewhat unnecessary and unwieldy to provide a getter/setter for every control on the form. The designer leaves it up to you to provide delegated access to specific controls via public getter/setters. The designer does the right thing here."52,A,How to check if element in groovy array/hash/collection/list? How do I figure out if an array contains an element? I thought there might be something like [123].includes(1) which would evaluate as true Can you find the index out also of where this equal element is in the list? @AtharvaJohri `assert [124233].indexOf(42) == 1` .contains() is the best method for lists but for maps you will need to use .containsKey() or .containsValue() [a:1b:2c:3].containsValue(3) [a:1b:2c:3].containsKey('a')  [123].contains(1) == true Probably you wanted to say [123].contains(1). Because I am guessing contains function itself already returns a boolean. Why do you want to again compare it with a hardcoded 'true'.  If you really want your includes method on an ArrayList just add it: ArrayList.metaClass.includes = { i -> i in delegate }  Some syntax sugar 1 in [123] == true The `==true` is not necessary.53,A,"How do I use Django templates without the rest of Django? I want to use the Django template engine in my (Python) code but I'm not building a Django-based web site. How do I use it without having a settings.py file (and others) and having to set the DJANGO_SETTINGS_MODULE environment variable? If I run the following code: >>> import django.template >>> from django.template import Template Context >>> t = Template('My name is {{ my_name }}.') I get: ImportError: Settings cannot be imported because environment variable DJANGO_SETTINGS_MODULE is undefined. I would also recommend jinja2. There is a nice article on django vs. jinja2 that gives some in-detail information on why you should prefere the later.  Jinja2 syntax is pretty much the same as Django's with very few differences and you get a much more powerfull template engine which also compiles your template to bytecode (FAST!). I use it for templating including in Django itself and it is very good. You can also easily write extensions if some feature you want is missing. Here is some demonstration of the code generation: >>> import jinja2 >>> print jinja2.Environment().compile('{% for row in data %}{{ row.name | upper }}{% endfor %}' raw=True) from __future__ import division from jinja2.runtime import LoopContext Context TemplateReference Macro Markup TemplateRuntimeError missing concat escape markup_join unicode_join name = None def root(context environment=environment): l_data = context.resolve('data') t_1 = environment.filters['upper'] if 0: yield None for l_row in l_data: if 0: yield None yield unicode(t_1(environment.getattr(l_row 'name'))) blocks = {} debug_info = '1=9' I'm using Jinja in a project of mine because I wanted something that I was fairly familiar with but didn't want my users (since it's a distributable app) to have to install Django. A plus is that Jinja can be installed with easy_install. Django can be installed with easy_install as well.  Found this: http://snippets.dzone.com/posts/show/3339  Thanks for the help folks. Here is one more addition. The case where you need to use custom template tags. Let's say you have this important template tag in the module read.py from django import template register = template.Library() @register.filter(name='bracewrap') def bracewrap(value): return ""{"" + value + ""}"" This is the html template file ""temp.html"": {{var|bracewrap}} Finally here is a Python script that will tie to all together import django from django.conf import settings from django.template import Template Context import os #load your tags from django.template.loader import get_template django.template.base.add_to_builtins(""read"") # You need to configure Django a bit settings.configure( TEMPLATE_DIRS=(os.path.dirname(os.path.realpath(__file__)) ) ) #or it could be in python #t = Template('My name is {{ my_name }}.') c = Context({'var': 'stackoverflow.com rox'}) template = get_template(""temp.html"") # Prepare context .... print template.render(c) The output would be {stackoverflow.com rox}  Google AppEngine uses the Django templating engine have you taken a look at how they do it? You could possibly just use that.  I echo the above statements. Jinja 2 is a pretty good superset of Django templates for general use. I think they're working on making the Django templates a little less coupled to the settings.py but Jinja should do well for you.  The solution is simple. It's actually well documented but not too easy to find. (I had to dig around -- it didn't come up when I tried a few different Google searches.) The following code works: >>> from django.template import Template Context >>> from django.conf import settings >>> settings.configure() >>> t = Template('My name is {{ my_name }}.') >>> c = Context({'my_name': 'Daryl Spitzer'}) >>> t.render(c) u'My name is Daryl Spitzer.' See the Django documentation (linked above) for a description of some of the settings you may want to define (as keyword arguments to configure). And to get it from a file: settings.configure( TEMPLATE_DIRS=(""."") ) t = get_template('test.html')  Don't. Use StringTemplate instead--there is no reason to consider any other template engine once you know about it. bold claim! link?  Any particular reason you want to use Django's templates? Both Jinja and Genshi are in my opinion superior. If you really want to then see the Django documentation on settings.py. Especially the section ""Using settings without setting DJANGO_SETTINGS_MODULE"". Use something like this: from django.conf import settings settings.configure (FOO='bar') # Your settings go here  While running the manage.py shell: >>> from django import template >>> t = template.Template('My name is {{ me }}.') >>> c = template.Context({'me': 'ShuJi'}) >>> t.render(c)  I would say Jinja as well. It is definitely more powerful than Django Templating Engine and it is stand alone. If this was an external plug to an existing Django application you could create a custom command and use the templating engine within your projects environment. Like this; manage.py generatereports --format=html But I don't think it is worth just using the Django Templating Engine instead of Jinja."54,A,"What is WCF in simple terms? What is WCF in simple terms? It's hard to distill the meaning from the Wikipedia page. See the [WCF Tag Page](http://stackoverflow.com/tags/wcf/info). WCF stands for Windows Cummunication Foundation. It's Microsoft's attempt to sort out and simplify network programming. It's provide an easy to use level of abstraction over different communication protocols and transportation methods. It's allows you to concentrate more on what you want to implement rather than thinking on how to implement. Read Mircosoft's WCF FAQ for more info.  WCF allows you to create ""services"" without specifying that it's a Windows service or a Web service or which protocols are used to communicate with it or how the data is serialized. All those details may be specified externally either programmatically in a service host or via the config file.  WCF is Microsoft's new .NET do-all extensible communications framework meant to replace functionality previously available in DCOM .NET Remoting and ASMX web services.  WCF - Windows Communication Framework - is Microsoft's framework to make inter-process communication easier. It let's you do this communication through various means plain old asmx web services Remoting MS Message Queuing and a couple more. It let's you talk with other .NET apps or non-Microsoft technologies (like J2EE). It's extensible enough to allow for newer stuff like REST too (I don't think REST is built-in). REST is built in with .NET v3.5"55,A,"Fade splash screen in and out In a C# windows forms application. I have a splash screen with some multi-threaded processes happening in the background. What I would like to do is when I display the splash screen initially I would like to have it appear to ""fade in"". And then once all the processes finish I would like it to appear as though the splash screen is ""fading out"". I'm using C# and .NET 2.0. Thanks. You can use the Opacity property for the form to alter the fade (between 0.0 and 1.0).  While(this.Opacity !=0) { this.Opacity -= 0.05; Thread.Sleep(50);//This is for the speed of the opacity... and will let the form redraw } so you're ""fading"" out in 2 iterations? I can hardly call that a fade out... more so I believe your Thread.Sleep will block the UI thread thus preventing it from updating. Idan calm down. First I had wrote -0.5 and it was a typo in fact 0.05 could be a better value. But you can use the one you want... The Thread.Sleep() doesn't block the UI since it gives room to the Redraw. Not the best solution maybe but it does work.  You could use a timer to modify the Form.Opacity level.  When using Opacity property have to remember that its of type double where 1.0 is complete opacity and 0.0 is completely transparency.  private void fadeTimer_Tick(object sender EventArgs e) { this.Opacity -= 0.01; if (this.Opacity <= 0) { this.Close(); } } This is fade-out not fade-in. If the timer were set to a fast 50ms and Opacity is a percentage then wouldn't this fade-out take about 8 minutes? @mackenir If your timer is 50ms/tick then it will take 5 seconds. (1.0 / 0.01) = 100 * 50ms = 5000ms => 5sec Hi @aarontfoley I've taken the liberty of editing your answer to remove the mention of Opacity being a 'percentage' as it is contradicted by the text that follows (Opacity being a value between 0 and 1). Feel free to revert and make the change yourself or not. I know how annoying it can be to have your answer edited by somebody else. Cheers. @mackenir np thanks for the edit."56,A,How can I find the current DNS server? I'm using Delphi and need to get the current Windows DNS server IP address so I can do a lookup. What function should I call to find it? The only solution I have right now does an ipconfig/all to get it which is horrible. Found a nice one using the function GetNetworkParams().Seems to work quite good. You can find it here: http://www.swissdelphicenter.ch/torry/showcode.php?id=2452 This code worked nicely. Thank you. you're welcome :)  See GetNetowrkParams method (Platform SDK: IP Helper)  Do you really need to know what is DNS server to do a lookup? Here is a solution how to get a IP address using 2 functions: GetHostName and GetHostByName. I assume the GetHostByName function does the lookup you need for you or am I wrong? I'm wanting to get the DNS server so I can do an MX lookup. The code you link to seems to show how to find your own IP? Yes. I assumed you wanted just a name->IP lookup so gave you the GetHostByName function :)57,A,How can I Java webstart multiple dependent native libraries? Example: I have two shared objects (same should apply to .dlls). The first shared object is from a third-party library we'll call it libA.so. I have wrapped some of this with JNI and created my own library libB.so. Now libB depends on libA. When webstarting both libraries are places in some webstart working area. My java code attempts to load libB. At this point the system loader will attempt to load libA which is not in the system library path (java.library.path won't help this). The end result is that libB has an unsatisfied link and cannot be used. I have tried loading libA before libB but that still does not work. Seems the OS wants to do that loading for me. Is there any way I can make this work other than statically compiling? Static compilation proved to be the only way to webstart multiple dependent native libraries.  I'm not sure if this would be handled exactly the same way for webstart but we ran into this situation in a desktop application when dealing with a set of native libraries (dlls in our case). Loading libA before libB should work unless one of those libraries has a dependency that is unaccounted for and not in the path. My understanding is that once it gets to a system loadLibrary call (i.e. Java has found the library in its java.library.path and is now telling the OS to load it) - it is completely dependent on the operating system to find any dependent libraries because at that point it is the operating system that is loading the library for the process and the OS only knows how to look in the system path. That seems hard to set in the case of a Webstart app but there is a way around this that does not involve static compiling. You may be able to shuffle where your libraries are - I am unsure If you use a custom classloader you can override loadLibrary and findLibrary so that it can locate your libraries from within a jar in your classpath and if you also make it aware of your native library dependencies (i.e. libB depends on libA depends on libX then when loading libB you can catch yourself and ensure you load libA first and in checking that notice and load libX first. Then the OS doesn't try to find a library that isn't in your path. It's klunky and a bit painful but ensuring Java finds them and loads them all in the correct order can work. Classloaders cannot fix the problem since the OS resolves the native dependecies.  Are both native libraries packaged into a signed jar which is listed as <nativelib ...> In the JNLP file?58,A,"How to use combinations of sets as test data I would like to test a function with a tuple from a set of fringe cases and normal values. For example while testing a function which returns true whenever given three lengths that form a valid triangle I would have specific cases negative / small / large numbers values close-to being overflowed etc.; what is more main aim is to generate combinations of these values with or without repetition in order to get a set of test data. (inf0-1) (5101000) (1055) (0-15) (1000infinf) ... As a note: I actually know the answer to this but it might be helpful for others and a challenge for people here! --will post my answer later on. Interesting question! I would do this by picking combinations something like the following in python. The hardest part is probably first pass verification i.e. if f(123) returns true is that a correct result? Once you have verified that then this is a good basis for regression testing. Probably it's a good idea to make a set of test cases that you know will be all true (e.g. 345 for this triangle case) and a set of test cases that you know will be all false (e.g. 01inf). Then you can more easily verify the tests are correct.  # xpermutations from http://code.activestate.com/recipes/190465 from xpermutations import * lengths=[-10151001000'inf'] for c in xselections(lengths3): # or xuniqueselections print c  (-1-1-1); (-1-10); (-1-11); (-1-15); (-1-110); (-1-10); (-1-11000); (-1-1inf); (-10-1); (-100); ...  With the brand new Python 2.6 you have a standard solution with the itertools module that returns the Cartesian product of iterables : import itertools print list(itertools.product([123] [456])) [(1 4) (1 5) (1 6) (2 4) (2 5) (2 6) (3 4) (3 5) (3 6)] You can provide a ""repeat"" argument to perform the product with an iterable and itself: print list(itertools.product([12] repeat=3)) [(1 1 1) (1 1 2) (1 2 1) (1 2 2) (2 1 1) (2 1 2) (2 2 1) (2 2 2)] You can also tweak something with combinations as well : print list(itertools.combinations('123' 2)) [('1' '2') ('1' '3') ('2' '3')] And if order matters there are permutations : print list(itertools.permutations([1234] 2)) [(1 2) (1 3) (1 4) (2 1) (2 3) (2 4) (3 1) (3 2) (3 4) (4 1) (4 2) (4 3)] Of course all that cool stuff don't exactly do the same thing but you can use them in a way or another to solve you problem. Just remember that you can convert a tuple or a list to a set and vice versa using list() tuple() and set().  While it's possible to create lots of test data and see what happens it's more efficient to try to minimize the data being used. From a typical QA perspective you would want to identify different classifications of inputs. Produce a set of input values for each classification and determine the appropriate outputs. Here's a sample of classes of input values valid triangles with small numbers such as (1 billion 2 billion 2 billion) valid triangles with large numbers such as (0.000001 0.00002 0.00003) valid obtuse triangles that are 'almost'flat such as (10 10 19.9999) valid acute triangles that are 'almost' flat such as (10 10 0000001) invalid triangles with at least one negative value invalid triangles where the sum of two sides equals the third invalid triangles where the sum of two sides is greater than the third input values that are non-numeric ... Once you are satisfied with the list of input classifications for this function then you can create the actual test data. Likely it would be helpful to test all permutations of each item. (e.g. (234) (243) (324) (342) (423) (432)) Typically you'll find there are some classifications you missed (such as the concept of inf as an input parameter). Random data for some period of time may be helpful as well that can find strange bugs in the code but is generally not productive. More likely this function is being used in some specific context where additional rules are applied.(e.g. only integer values or values must be in 0.01 increments etc.) These add to the list of classifications of input parameters.  Absolutely especially dealing with lots of these permutations/combinations I can definitely see that the first pass would be an issue. Interesting implementation in python though I wrote a nice one in C and Ocaml based on ""Algorithm 515"" (see below). He wrote his in Fortran as it was common back then for all the ""Algorithm XX"" papers well that assembly or c. I had to re-write it and make some small improvements to work with arrays not ranges of numbers. This one does random access I'm still working on getting some nice implementations of the ones mentioned in Knuth 4th volume fascicle 2. I'll an explanation of how this works to the reader. Though if someone is curious I wouldn't object to writing something up. /** [combination c n p x]  * get the [x]th lexicographically ordered set of [p] elements in [n]  * output is in [c] and should be sizeof(int)*[p] */ void combination(int* cint nint p int x){  int irk = 0;  for(i=0;i<p-1;i++){  c[i] = (i != 0) ? c[i-1] : 0;  do {  c[i]++;  r = choose(n-c[i]p-(i+1));  k = k + r;  } while(k < x);  k = k - r;  }  c[p-1] = c[p-2] + x - k; } ~""Algorithm 515: Generation of a Vector from the Lexicographical Index""; Buckles B. P. and Lybanon M. ACM Transactions on Mathematical Software Vol. 3 No. 2 June 1977. @mkb Of course. What does `choose()` do? Does that basically return `n-c[i]` choose `p-(i+1)1`?  I think you can do this with the Row Test Attribute (available in MbUnit and later versions of NUnit) where you could specify several sets to populate one unit test."59,A,"Should a software engineer know how to assemble their own computer? Coming from a Mac background I've never really spent much time tinkering with / assembling / tweaking my own computer beyond occasional RAM upgrades and swapping out hard disks. I feel like I have a good grasp on how a computer works at a conceptual level CPU bus memory etc but I haven't really got much practical experience in putting it all together / taking it apart. My question is is there anything to be gained in terms of software engineering skills by learning to assemble my own computer? If you have spent your whole life putting bits of hardware together how has it influenced the way you write or think about software? Yes but not in the sense you're thinking. I think it's incredibly valuable to learn about basic microprocessor design memory architecture etc. Even better if you can get a project kit to do something basic with a PIC or Stamp. Get the abstractions out of the way and learn a little at the bit level. A review of the relevant Patterson and Hennessy books is also recommended. Building a PC up from components is a good one-time exercise just to understand the compatibility issues of the various parts.  If you wear shoes you should probably know how to tie one. But most likely if you're smart enough to wear them you can figure it out. If not you've got problems.  Eh. I honestly feel that the actual act of assembling a computer does not really fundamentally improve your understanding of computers. You're not building these parts you're assembling them into a case by following instructions. There is some improvement to be gained though in making the operating system work correctly with the components and debugging issues relating to that. And Aku's point about saving money is a good one.  I have built my own computer but I do not think it has added my ability to be a software engineer. Nor would knowing how to build one help make you a better engineer. Understanding how a computer works and how software interacts with hardware is what makes you a better engineer. Just because I can put a multi-core CPU into a mother board doesn't mean I understand why parallel processes help me or what it means to enter a critical part of a function call. And because it has a 4 MB cache that doesn't mean I understand what the cache does and how paging works. There are allot of people who can build computers but there are fewer that can engineer software for them. If that was the case then software engineers would make far less than they do.  If everything in the series ""Write Great Code"" makes sense then I'd say no there's not much more you're going to get out of actually building up a system for your job that is. Now working as a tech and having to diagnose problems hardware and software adds to your ability to tackle problems and would help. The problem is its not something you can learn by putting a couple computers together for fun. You end up missing out on all the little weird things various manufactures do and if you can apply it it can help broaden your overall picture of good and bad design choices.  This is the similar to the question about Software and Electronic Engineering.  No not neccessarily but I think programmers should know about how code can be optimised for the hardware it runs on. CPU memory network and disk resources have a big effect on performance. Programmers should know when and how to use caching to increase the performance of the application and also how to exploit code to use large resources when available. Being a geek I would say learn how to assemble a computer for fun.  To quote one of our founding fathers (Jeff Atwood): In my book one of the best ways to understand the hardware is to get your hands dirty and put one together including installing the OS yourself. It's a shame Apple programmers can't do this... While I'm not so sure it makes you that much of a better programmer - but it is the tool you use. These machines constant break or need upgrades. I think its a good idea to be able to do this and building one from scratch is THE way to learn this.  Well honestly I wouldn't know which parts to pick to assemble a top class development or gaming PC. I know how everything fits together I know software bottlenecks etc. What I do when I have to assemble a PC is find the purpose first. Dev PC? gaming? multimedia? Office? Then I think about the requirements. Obviously a simple office PC works with a lot less harddrive space an onboard GPU etc. When I figured that out I scour the internet for reviews already composed systems tom's hardware benchmarks to try to find the best combination within my budget. Another thing to take into account is new developments. If for instance Intel is releasing a new CPU model within the month I'd wait for that so the prices of the lower models will drop. Once I have the list of required components I either go to my local computer store or find the best price on the net. I prefer a local store because that saves me a lot of time when I get a dead component or need to claim my warranty. Usually a bit more expensive though. Assembling is mainly logical thinking a bit of patience and staying focused ;)  You don't learn much that's usefull to a programmer from putting together a computer. Knowing where to put the pci cards and how to plug in a processor isn't really useful for anything apart from building computers. I do think you need some low level knowledge about how computers work but all the stuff that's interesting for programmers is already soldered together on the mainboard. You'll learn much more from programming c or assembly and maybe reading Charles Petzold's book Code...  Benefits are obvious: You can save a couple of bucks for good use If you lose your software engineer position for hanging to much on SO you can make money assembling computers. Last time I bought assembled computer I found that there was no heat-conducting paste between CPU and radiator. As for software engineering skills I didn't find any benefits. True enough - Assuming I don't account for my time that is :)  Building your own computers makes you a better generalist not a better specialist. If you look on programming as the specialized application of code in an editor turning into applications then building your own systems does nothing to help that practice. However if you need to put a server in a colo administer it figure out when it crashes whether it's your application or a bad drive or a bad fan than went south then yah knowing what all the pieces are understanding what they do which ones you bought and why whether the hardware or the software is likely to be the problem then building a computer is the minimum first step down that road. If you aren't responsible for the actual execution and utility of your code then it's a waste of time. A human being should be able to change a diaper plan an invasion butcher a hog conn a ship design a building write a sonnet balance accounts build a wall set a bone comfort the dying take orders give orders cooperate act alone solve equations analyze a new problem pitch manure program a computer cook a tasty meal fight efficiently die gallantly. Specialization is for insects.  Beware of programmers who carry screwdrivers. (Leonard Brandwein) You don't learn anything as a programmer from assembling a PC. I can use a toilet just fine without installing a sewerage system.  I tend to think anytime you learn something and expand your knowledge and horizons it is a good thing so I am going to say 'yes'.  As a programmer who has built computers for years I would say that it has not given me a sizable influence over the quality or structure of my code but it has given me a good basis of how the quality of your hardware affects the end user experience of running code. Additionally and I think perhaps this is the greatest benefit: building your own PC gives you enough knowledge to say whether or not a development system given to you as an employee is a good one or not. Giving an informed opinion to an IT/Management person about why the system they have provided is inferior may be the key to getting an upgrade.  Depends on what kind of learner you are. If you have a good imagination you can probably learn all there is to know about hardware without touching anything more than a book there are people who needs to look at things to improve their understanding. That said you could learn a lot if you take the opportunity of building a new computer as a learning one for instance take the time to look at where the southbridge is which chip is it and look it up find where is the SATA controller which brand is it and look it up see their characteristics and so on and so forth. If you just put the pieces together as fast as you can youll probably learn what to do in case of failure and save a few bucks nothing more. Also building things increases curiosity about them which can't be bad.  Anything that deals with the technology you directly work on will help you to become better at all tasks related to that technology. Knowing computer hardware will make you a better programmer. It may not do so directly but there will be instances in which it benefits you. It really depends on what you want your skills to be. It would be a waste of time messing with hardware if you want to specialize in complex mathematics or specific types of software (A.I. for example). However if you want to do things such as build robots with minds of their own you will need to know both software and hardware. If you want to develop games especially as an indie or even solo you will be required to emulate multiple jobs and specialties. What a nightmare it would be if an indie developer didn't understand how hardware differentiates between systems. IMO you cannot call yourself a real nerd unless you know how to do both software and hardware. When I meet programmers who don't know how to install a graphics card I am conflicted as to whether I should laugh or cry. Of course not everyone is smart enough or enthusiastic enough about technology to be interested in it to the point of wanting to know how the hardware is created let alone assembled and then programmed. As I have grown up I have learned to laugh less often at others and instead understand they may not have a need to learn about hardware while focusing on software. It is certainly not a requirement.  I used to build my own Windows and Linux computers before I joined the Cult of Mac. I did learn a bit I don't think it made me a better programmer. It's not useless knowledge though. It's like a chef who also knows a little about plumbing. The knowledge doesn't really apply to your core tasks but it can be handy when something goes wrong. Edit: There is a big difference between understanding the fundamentals of how computers work and putting one together with parts from Newegg. An understanding of how the CPU cache and memory function are invaluable for programming. Knowing how upgrade your video is a useful skill but it doesn't help in day-to-day software development."60,A,"Why the claim that c# guys don't get object-oriented programming? (vs class-oriented) This caught my attention last night. On the latest ALT.NET Podcast Scott Bellware discusses how as opposed to Ruby languages like c# java et al. are not truly object oriented rather opting for the phrase ""class-oriented"". They talk about this distinction in very vague terms without going into much detail or discussing the pros and cons much. What is the real difference here and how much does it matter? What are other languages then are ""object-oriented""? It sounded pretty interesting but I don't want to have to learn Ruby just to know what if anything I am missing. Update: After reading some of the answers below it seems like people generally agree that the reference is to duck-typing. What I'm not sure I understand still though is the claim that this ultimately changes all that much. Especially if you are already doing proper tdd with loose coupling blah blah blah. Can someone show me an example of a wonderous thing I could do with ruby that I cannot do with c# and that exemplifies this different oop approach? Maybe they are alluding to the difference between duck typing and class hierarchies? if it walks like a duck and quacks like a duck just pretend it's a duck and kick it. In C# Java etc. the compiler fusses a lot about: Are you allowed to do this operation on that object? Object Oriented vs. Class Oriented could therefore mean: Does the language worry about objects or classes? For instance: In Python to implement an iterable object you only need to supply a method __iter__() that returns an object that has a method named next(). That's all there is to it: No interface implementation (there is no such thing). No subclassing. Just talking like a duck / iterator. EDIT: This post was upvoted while I rewrote everything. Sorry won't ever do that again. The original content included advice to learn as many languages as possible and to nary worry about what the language doctors think / say about a language.  IMO it's really overly defining ""object-oriented"" but what they are referring to is that Ruby unlike C# C++ Java et al does not make use of defining a class -- you really only ever work directly with objects. Conversely in C# for example you define classes that you then must instantiate into object by way of the new keyword. The key point being you must declare a class in C# or describe it. Additionally in Ruby everything -- even numbers for example -- is an object. In contrast C# still retains the concept of an object type and a value type. This in fact I think illustrates the point they make about C# and other similar languages -- object type and value type imply a type system meaning you have an entire system of describing types as opposed to just working with objects. Conceptually I think OO design is what provides the abstraction for use to deal complexity in software systems these days. The language is a tool use to implement an OO design -- some make it more natural than others. I would still argue that from a more common and broader definition C# and the others are still object-oriented languages.  Object Oriented is a concept. This concept is based upon certain ideas. The technical names of these ideas (actually rather principles that evolved over the time and have not been there from the first hour) have already been given above I'm not going to repeat them. I'm rather explaining this as simple and non-technical as I can. The idea of OO programming is that there are objects. Objects are small independent entities. These entities may have embedded information or they may not. If they have such information only the entity itself can access it or change it. The entities communicate with each other by sending messages between each other. Compare this to human beings. Human beings are independent entities having internal data stored in their brain and the interact with each other by communicating (e.g. talking to each other). If you need knowledge from someone's else brain you cannot directly access it you must ask him a question and he may answer that to you telling you what you wanted to know. And that's basically it. This is real idea behind OO programming. Writing these entities define the communication between them and have them interact together to form an application. This concept is not bound to any language. It's just a concept and if you write your code in C# Java or Ruby that is not important. With some extra work this concept can even be done in pure C even though it is a functional language but it offers everything you need for the concept. Different languages have now adopted this concept of OO programming and of course the concepts are not always equal. Some languages allow what other languages forbid for example. Now one of the concepts that involved is the concept of classes. Some languages have classes some don't. A class is a blueprint how an object looks like. It defines the internal data storage of an object it defines the messages an object can understand and if there is inheritance (which is not mandatory for OO programming!) classes also defines from which other class (or classes if multiple inheritance is allowed) this class inherits (and which properties if selective inheritance exists). Once you created such a blueprint you can now generate an unlimited amount of objects build according to this blueprint. There are OO languages that have no classes though. How are objects then build? Well usually dynamically. E.g. you can create a new blank object and then dynamically add internal structure like instance variables or methods (messages) to it. Or you can duplicate an already existing object with all its properties and then modify it. Or possibly merge two objects into a new one. Unlike class based languages these languages are very dynamic as you can generate objects dynamically during runtime in ways not even you the developer has thought about when starting writing the code. Usually this dynamic has a price: The more dynamic a language is the more memory (RAM) objects will waste and the slower everything gets as program flow is extremely dynamically as well and it's hard for a compiler to generate effective code if it has no chance to predict code or data flow. JIT compilers can optimize some parts of that during runtime once they know the program flow however as these languages are so dynamically program flow can change at any time forcing the JIT to throw away all compilation results and re-compile the same code over and over again. But this is a tiny implementation detail - it has nothing to do with the basic OO principle. It is nowhere said that objects need to be dynamic or must be alterable during runtime. The Wikipedia says it pretty well: Programming techniques may include features such as information hiding data abstraction encapsulation modularity polymorphism and inheritance. http://en.wikipedia.org/wiki/Object-oriented_programming They may or they may not. This is all not mandatory. Mandatory is only the presence of objects and that they must have ways to interact with each other (otherwise objects would be pretty useless if they cannot interact with each other). @Meckl: I would note that the concept of objects communicating by passing messages it quite irrelevant in the modern world outside of academia. It was an important concept during the development of the technology but no longer matters as a distinct concept.  That was an abstract-podcast indeed! But I see what they're getting at - they just dazzled by Ruby Sparkle. Ruby allows you to do things that C-based and Java programmers wouldn't even think of + combinations of those things let you achieve undreamt of possibilities. Adding new methods to a built-in String class coz you feel like it passing around unnamed blocks of code for others to execute mixins... Conventional folks are not used to objects changing too far from the class template. Its a whole new world out there for sure.. As for the C# guys not being OO enough... dont take it to heart.. Just take it as the stuff you speak when you are flabbergasted for words. Ruby does that to most people. If I had to recommend one language for people to learn in the current decade.. it would be Ruby. I'm glad I did.. Although some people may claim Python. But its like my opinion.. man! :D ""Adding new methods to a built-in String class coz you feel like it"". sigh and MS listened to this and added extensions methods to C#. You're gonna love it with C#4.0 :) Muddy waters between static and dynamic languages now. Google out Anders PDC talk on Future of C#  You asked: ""Can someone show me an example of a wonderous thing I could do with ruby that I cannot do with c# and that exemplifies this different oop approach?"" One good example is active record the ORM built into rails. The model classes are dynamically built at runtime based on the database schema. Don't get me wrong I've since started learning ruby and I like it very much but how exactly is ActiveRecord an example of OOP principles that are not possible in C#? It seems like an example of the capabilities of dynamic languages rather than OO. By the way there are several popular ActiveRecord implementations in C# - the most popular one being Monorail's gotcha thanks for the clarification Firstly I was specifically talking about active record the library not the pattern. The library uses ruby's metaprogramming to dynamically define types that match the database schema. You can't do that with a class-oriented language since the types must be defined at compile time. Castle active record requires that the class model matches the database model (or a mapping is provided). Im using the definition of OO given above by Scott Bellware and Ben Scheirman which is only possible in dynamic languages.  OO is sometimes defined as message oriented. The idea is that a method call (or property access) is really a message sent to another object. How the recieveing object handles the message is completely encapsulated. Often the message corresponds to a method which is then executed but that is just an implementation detail. You can for example create a catch-all handler which is executed regardless of the method name in the message. Static OO like in C# does not have this kind of encapsulation. A massage has to correspond to an existing method or property otherwise the compiler will complain. Dynamic languages like Smalltalk Ruby or Python does however support ""message-based"" OO. So in this sense C# and other statically typed OO languages are not true OO sine thay lack ""true"" encapsulation. That's an interesting viewpoint. But even in python or ruby you should be injecting your dependencies in the constructor. So if you have a dependency on CookieEatingService you're going to have to inject something! Even if somehow the interpreter takes care of it for you and injects nil then calling service.EatCookie() will cause an error. It will only work if when a specific implementation is not found a stub is provided. This is something that C# can do too (albeit with more difficulty) George there's nothing in all of OO that suggests that dependencies must - or even should - be ""injected"" via constructor arguments. This is a bias of static language programming in the age of DI frameworks. It's a particular cultural bias and isn't reflected in the values of the Dependency Inversion Principle.  This is really probably getting down to what these people see others doing in c# and java as opposed to c# and java supporting OOP. Most languages cane be used in different programming paradigms. For example you can write procedural code in c# and scheme and you can do functional-style programming in java. It is more about what you are trying to do and what the language supports.  I don't think this is specifically about duck typing. For instance C# supports limited duck-typing already - an example would be that you can use foreach on any class that implements MoveNext and Current. The concept of duck-typing is compatible with statically typed languages like Java and C# it's basically an extension of reflection. This is really the case of static vs dynamic typing. Both are proper-OO in as much as there is such a thing. Outside of academia it's really not worth debating. Rubbish code can be written in either. Great code can be written in either. There's absolutely nothing functional that one model can do that the other can't. The real difference is in the nature of the coding done. Static types reduce freedom but the advantage is that everyone knows what they're dealing with. The opportunity to change instances on the fly is very powerful but the cost is that it becomes hard to know what you're deaing with. For instance for Java or C# intellisense is easy - the IDE can quickly produce a drop list of possibilities. For Javascript or Ruby this becomes a lot harder. For certain things for instance producing an API that someone else will code with there is a real advantage in static typing. For others for instance rapidly producing prototypes the advantage goes to dynamic. It's worth having an understanding of both in your skills toolbox but nowhere near as important as understanding the one you already use in real depth.  In an object-oriented language objects are defined by defining objects rather than classes although classes can provide some useful templates for specific cookie-cutter definitions of a given abstraction. In a class-oriented language like C# for example objects must be defined by classes and these templates are usually canned and packaged and made immutable before runtime. This arbitrary constraint that objects must be defined before runtime and that the definitions of objects are immutable is not an object-oriented concept; it's class oriented.  There are three pillars of OOP Encapsulation Inheritance Polymorphism If a language can do those three things it is a OOP language. I am pretty sure the argument of language X does OOP better than language A will go on forever. yup - we will never stop fighting about that!  Update: Its the new wave.. which suggest everything that we've been doing till now is passe.. Seems to be propping up quite a bit in podcasts and books.. Maybe this is what you heard. Till now we've been concerned with static classes and not unleashed the power of object oriented development. We've been doing 'class based dev.' Classes are fixed/static templates to create objects. All objects of a class are created equal. e.g. Just to illustrate what I've been babbling about... let me borrow a Ruby code snippet from PragProg screencast I just had the privilege of watching. 'Prototype based development' blurs the line between objects and classes.. there is no difference. animal = Object.new # create a new instance of base Object def animal.number_of_feet=(feet) # adding new methods to an Object instance. What? @number_of_feet = feet end def animal.number_of_feet @number_of_feet end cat = animal.clone #inherits 'number_of_feet' behavior from animal cat.number_of_feet = 4 felix = cat.clone #inherits state of '4' and behavior from cat puts felix.number_of_feet # outputs 4 The idea being its a more powerful way to inherit state and behavior than traditional class based inheritance. It gives you more flexibility and control in certain ""special"" scenarios (that I've yet to fathom). This allows things like Mix-ins (re using behavior without class inheritance).. By challenging the basic primitives of how we think about problems 'true OOP' is like 'the Matrix' in a way... You keep going WTF in a loop. Like this one.. where the base class of Container can be either an Array or a Hash based on which side of 0.5 the random number generated is. class Container < (rand < 0.5 ? Array : Hash) end Ruby javascript and the new brigade seem to be the ones pioneering this. I'm still out on this one... reading up and trying to make sense of this new phenomenon. Seems to be powerful.. too powerful.. Useful? I need my eyes opened a bit more. Interesting times.. these. Gishu rather than trying to ""fathom"" this power why not get hands-on in an extended project that is enabled by metaprogramming? This is what helped me get past my own cognitive constraints coming from the C# world (and Java before then). If I hadn't got real tangible practical and practicable experience I might not have realized that I was trying to assess the qualities of a given paradigm without having left behind the biases of the one that I'm most influenced by.  I'll take a stab at this. Python and Ruby are duck-typed. To generate any maintainable code in these languages you pretty much have to use test driven development. As such it is very important for a developer to easily inject dependencies into their code without having to create a giant supporting framework. Successful dependency-injection depends upon on having a pretty good object model. The two are sort of two sides of the same coin. If you really understand how to use OOP then you should by default create designs where dependencies can be easily injected. Because dependency injection is easier in dynamically typed languages the Ruby/Python developers feel like their language understands the lessons of OO much better than other statically typed counterparts.  I've only listened to the first 6-7 minutes of the podcast that sparked your question. If their intent is to say that C# isn't a purely object-oriented language that's actually correct. Everything in C# isn't an object (at least the primitives aren't though boxing creates an object containing the same value). In Ruby everything is an object. Daren and Ben seem to have covered all the bases in their discussion of ""duck-typing"" so I won't repeat it. Whether or not this difference (everything an object versus everything not an object) is material/significant is a question I can't readily answer because I don't have sufficient depth in Ruby to compare it to C#. Those of you who on here who know Smalltalk (I don't though I wish I did) have probably been looking at the Ruby movement with some amusement since it was the first pure OO language 30 years ago.  The duck typing comments here are more attributing to the fact that Ruby and Python are more dynamic than C#. It doesn't really have anything to do with it's OO Nature. What (I think) Bellware meant by that is that in Ruby everything is an object. Even a class. A class definition is an instance of an object. As such you can add/change/remove behavior to it at runtime. Another good example is that NULL is an object as well. In ruby everything is LITERALLY an object. Having such deep OO in it's entire being allows for some fun meta-programming techniques such as method_missing."61,A,"Is there a way to insert assembly code into C? I remember back in the day with the old borland DOS compiler you could do something like this: asm { mov axex etc etc... } Is there a semi-platform independent way to do this now? I have a need to make a BIOS call so if there was a way to do this without asm code that would be equally useful to me. A good start would be reading this article which talk about inline assembly in C/C++: http://www.codeproject.com/KB/cpp/edujini_inline_asm.aspx Example from the article: #include <stdio.h> int main() { /* Add 10 and 20 and store result into register %eax */ __asm__ ( ""movl $10 %eax;"" ""movl $20 %ebx;"" ""addl %ebx %eax;"" ); /* Subtract 20 from 10 and store result into register %eax */ __asm__ ( ""movl $10 %eax;"" ""movl $20 %ebx;"" ""subl %ebx %eax;"" ); /* Multiply 10 and 20 and store result into register %eax */ __asm__ ( ""movl $10 %eax;"" ""movl $20 %ebx;"" ""imull %ebx %eax;"" ); return 0 ; }  In GCC there's more to it than that. In the instruction you have to tell the compiler what changed so that its optimizer doesn't screw up. I'm no expert but sometimes it looks something like this:  asm (""lock; xaddl %0%2"" : ""=r"" (result) : ""0"" (1) ""m"" (*atom) : ""memory""); It's a good idea to write some sample code in C then ask GCC to produce an assembly listing then modify that code.  Non-x86 Microsoft compilers do not support in-line assembly. You have to define the whole function in a separate assembly source file and pass it to an assembler. You're highly unlikely to be able to call into the BIOS under a protected-mode operating system and should use whatever facilities are available on that system. Even if you're in kernel mode it's probably unsafe - the BIOS may not be correctly synchronized with respect to OS state if you do so.  Using GCC __asm__(""movl %edx %eax\n\t"" ""addl $2 %eax\n\t""); Using VC++ __asm { mov eax edx add eax 2 } VS compiler does not support inline assembly ! I had burned my finger trying to do that a few years ago ! @JayD: the VS compiler supports inline assembly depending on the target. For example 32-bit x86 is supported; 64-bit x86-64 is not. @Michael : u r correct ! I forgot to mention 64 bit . C++Builder from CodeGear also supports the __asm keyword."62,A,Resize Infragistics GanttChart task with the mouse I loaded a custom DataTable into an UltraChart of type GanttChart. The data loads succesfully. Do you know if it possible to add support for mouse resize(drag) to the tasks that show up into the chart? I have not been able to figure out if this is supported by the Infragistics control. Thank you in advance for your answers. Dragos In this forum post an Infragistics employee states that this is not implemented (as of Feb '08) but may be doable handling FillSceneGraph.63,A,What program can I use to generate diagrams of SQL view/table structure? I've been tasked with redesigning part of a ms-sql database structure which currently involves a lot of views some of which contain joins to other views. Anyway I wonder if anyone here could recommend a utility to automatically generate diagrams to help me visualise the whole structure. What's the best program you've used for such problems? I upmodded Mark's post about Toad Data Modeler and wanted to point out that they have a beta version that is fully functional and free. The only downsides are the occasional bug and built in expiration (typically around the time a new beta is available) but for this poor bloke it does wonders until I can get my boss to chip in for a license.  Toad Data Modeller from Quest does a nice job on this and is reasonably priced. Embarcadero E/R studio is good too as Bruce mentioned.  If you are talking about MS SQL Server tables I like the diagram support in SQL Server Management Studio. You just drag the tables from the explorer onto the canvas and they are laid out for you along with lines for relationships. You'll have to do some adjusting by hand for the best looking diagrams but it is a decent way to get diagrams.  I am a big fan of Embarcadero's ER/Studio. It is very powerful and produces excellent on-screen as well as printed results. They have a free trial as well so you should be able to get in and give it a shot without too much strife. Good luck! Link is broken. Can now be found here: http://www.embarcadero.com/products/er-studio  OP asked about diagramming views and view dependencies SQL Management Studio and Enterprise Manager doesn't allow you to diagram views. I can't vouch for the other tools. The LINQ to SQL designer for Visual Studio does allow you to drop views on the design surface but there isn't a easy way to model the dependencies between the views. I'm not sure which tool has this type of diagramming functionality. You could take a look at Red Gate's SQLDoc tool but it just provides text based output.  For me I haven't found anything yet that tops Visio's database reverse engineering wizard. If you can get your hands on that I'd strongly recommend it. Has been removed from Visio 2013 Pro.64,A,Any good tools for creating timelines? I need to create a historical timeline starting from 1600's to the present day. I also need to have some way of showing events on the timeline so that they do not appear cluttered when many events are close together. I have tried using Visio 2007 as well as Excel 2007 Radar Charts but I could not get the results I wanted. the timeline templates in Visio are not great and using Radar charts in Excel leads to cluttered data. Are there any other tools or techniques I could use to create these? @Darren: The first link looks great. Thanks! The second link did not work in Firefox and was rendered as ASCII. It opened up fine in IE. And yes this is for the end users. So I want it to look as presentable as possible if you know what I mean. Thanks again! @Pascal this page? http://tools.mscorlib.com/timeline/Default.aspx. If it's looking like ascii maybe look for a js error but that renders on my system fine. If all else fails it's a decent js library by the MIT team as it is so you could wire up your own implementation  If you need a timeline from RSS Feeeds give xTimeline a try. I just used it http://lifehacker.com/software/rss/create-a-timeline-from-rss-feeds-with-xtimeline-283098.php  I also recommend Simile Timeline... I just implemented a webpage that uses it and JQuery and produces fantastic results. The downside is that you need to implement it through some html page hook it up with the js and create some xml files so it probably won't do for a presentational tool. http://infosthetics.com/ is a good data visualization blog maybe you find something there. Also check flowingdata.com For webbased timelines there is also: circavie: http://flowingdata.com/2007/10/25/create-share-and-embed-custom-timelines-with-circavie/ dipity (looks killer): http://flowingdata.com/2008/08/18/tell-stories-with-interactive-timelines-from-dipity/  SIMILIE Timeline would probably suit your needs. http://simile.mit.edu/timeline/ Timeline .NET: http://www.codeplex.com/timelinenet Oh i guess i should ask... for personal use or for display to end users? that might change what i would suggest but this could work for internal purposes too i suppose.  Lifehacker has a good overview and tutorial of SIMILIE Timeline. They seem to like it quite a bit.  You can used this great timeline tool built with JavaScript. You can download it for free here: http://timeline.verite.co/#examples65,A,"How can Perl's system() print the command that it's running? In Perl you can execute system commands using system() or `` (backticks). You can even capture the output of the command into a variable. However this hides the program execution in the background so that the person executing your script can't see it. Normally this is useful but sometimes I want to see what is going on behind the scenes. How do you make it so the commands executed are printed to the terminal and those programs' output printed to the terminal? This would be the .bat equivalent of ""@echo on"". Another technique to combine with the others mentioned in the answers is to use the tee command. For example: open(F ""ls | tee /dev/tty |""); while (<F>) { print length($_) ""\n""; } close(F); This will both print out the files in the current directory (as a consequence of tee /dev/tty) and also print out the length of each filename read.  I don't know of any default way to do this but you can define a subroutine to do it for you: sub execute { my $cmd = shift; print ""$cmd\n""; system($cmd); } my $cmd = $ARGV[0]; execute($cmd); And then see it in action: pbook:~/foo rudd$ perl foo.pl ls ls file1 file2 foo.pl  Here's an updated execute that will print the results and return them: sub execute { my $cmd = shift; print ""$cmd\n""; my $ret = `$cmd`; print $ret; return $ret; }  Hmm interesting how different people are answering this different ways. It looks to me like mk and Daniel Fone interpreted it as wanting to see/manipulate the stdout of the command (neither of their solutions capture stderr fwiw). I think Rudd got closer. One twist you could make on Rudd's response is to overwite the built in system() command with your own version so that you wouldn't have to rewrite existing code to use his execute() command. using his execute() sub from Rudd's post you could have something like this at the top of your code: if ($DEBUG) { *{""CORE::GLOBAL::system""} = \&{""main::execute""}; } I think that will work but I have to admit this is voodoo and it's been a while since I wrote this code. Here's the code I wrote years ago to intercept system calls on a local (calling namespace) or global level at module load time:  # importing into either the calling or global namespace _must_ be # done from import(). Doing it elsewhere will not have desired results. delete($opts{handle_system}); if ($do_system) { if ($do_system eq 'local') { *{""$callpkg\::system""} = \&{""$_package\::system""}; } else { *{""CORE::GLOBAL::system""} = \&{""$_package\::system""}; } } The big problem with replacing the global `system` command is that `system` has a complex prototype that can't be replicated by the user-specified prototype system. As a result some code will just not work if you replace `system` with your custom version.  As I understand system() will print the result of the command but not assign it. Eg. [daniel@tux /]$ perl -e '$ls = system(""ls""); print ""Result: $ls\n""' bin dev home lost+found misc net proc sbin srv System tools var boot etc lib media mnt opt root selinux sys tmp usr Result: 0 Backticks will capture the output of the command and not print it: [daniel@tux /]$ perl -e '$ls = `ls`; print ""Result: $ls\n""' Result: bin boot dev etc home lib etc... Update: If you want to print the name of the command being system()'d as well I think Rudd's approach is good. Repeated here for consolidation: sub execute { my $cmd = shift; print ""$cmd\n""; system($cmd); } my $cmd = $ARGV[0]; execute($cmd);  Use open instead. Then you can capture the output of the command. open(LS""|ls""); print LS;"66,A,"What is the most efficient way to handle the lifecycle of an object with COM interop? I have a Windows Workflow application that uses classes I've written for COM automation. I'm opening Word and Excel from my classes using COM. I'm currently implementing IDisposable in my COM helper and using Marshal.ReleaseComObject(). However if my Workflow fails the Dispose() method isn't being called and the Word or Excel handles stay open and my application hangs. The solution to this problem is pretty straightforward but rather than just solve it I'd like to learn something and gain insight into the right way to work with COM. I'm looking for the ""best"" or most efficient and safest way to handle the lifecycle of the classes that own the COM handles. Patterns best practices or sample code would be helpful. Basically you should not rely on hand code to call Dispose() on your object at the end of the work. You probably have something like this right now: MyComHelper helper = new MyComHelper(); helper.DoStuffWithExcel(); helper.Dispose(); ... Instead you need to use try blocks to catch any exception that might be triggered and call dispose at that point. This is the canonical way: MyComHelper helper = new MyComHelper(); try { helper.DoStuffWithExcel(); } finally() { helper.Dispose(); } This is so common that C# has a special construct that generates the same exact code [see note] as shown above; this is what you should be doing most of the time (unless you have some special object construction semantics that make a manual pattern like the above easier to work with): using(MyComHelper helper = new MyComHelper()) { helper.DoStuffWithExcel(); } EDIT: NOTE: The actual code generated is a tiny bit more complicated than the second example above because it also introduces a new local scope that makes the helper object unavailable after the using block. It's like if the second code block was surrounded by { }'s. That was omitted for clarify of the explanation.  I can not see what failure you have that does not calls the Dispose() method. I made a test with a sequential workflow that contains only a code activity which just throws an exception and the Dispose() method of my workflow is called twice (this is because of the standard WorkflowTerminated event handler). Check the following code: Program.cs  class Program { static void Main(string[] args) { using(WorkflowRuntime workflowRuntime = new WorkflowRuntime()) { AutoResetEvent waitHandle = new AutoResetEvent(false); workflowRuntime.WorkflowCompleted += delegate(object sender WorkflowCompletedEventArgs e) { waitHandle.Set(); }; workflowRuntime.WorkflowTerminated += delegate(object sender WorkflowTerminatedEventArgs e) { Console.WriteLine(e.Exception.Message); waitHandle.Set(); }; WorkflowInstance instance = workflowRuntime.CreateWorkflow(typeof(WorkflowConsoleApplication1.Workflow1)); instance.Start(); waitHandle.WaitOne(); } Console.ReadKey(); } } Workflow1.cs  public sealed partial class Workflow1: SequentialWorkflowActivity { public Workflow1() { InitializeComponent(); this.codeActivity1.ExecuteCode += new System.EventHandler(this.codeActivity1_ExecuteCode); } [DebuggerStepThrough()] private void codeActivity1_ExecuteCode(object sender EventArgs e) { Console.WriteLine(""Throw ApplicationException.""); throw new ApplicationException(); } protected override void Dispose(bool disposing) { if (disposing) { // Here you must free your resources // by calling your COM helper Dispose() method Console.WriteLine(""Object disposed.""); } } } Am I missing something? Concerning the lifecycle-related methods of an Activity (and consequently of a Workflow) object please check this post: Activity ""Lifetime"" Methods. If you just want a generic article about disposing check this. You are welcome. Once again you're spot on. The problem wasn't with my COm handling (which I've been doing for years) but with the way I was handling the workflow runtime. Thanks very much!"67,A,"Best practice for webservices I've created a webservice and when I want to use its methods I instantiate it in the a procedure call the method and I finally I dispose it however I think also it could be okay to instantiate the webservice in the ""private void Main_Load(object sender EventArgs e)"" event. The thing is that if I do it the first way I have to instantiate the webservice every time I need one of its methods but in the other way I have to keep a webservice connected all the time when I use it in a form for example. I would like to know which of these practices are better or if there's a much better way to do it Strategy 1 private void btnRead_Click(object sender EventArgs e) { try { //Show clock this.picResult.Image = new Bitmap(pathWait); Application.DoEvents(); //Connect to webservice svc = new ForPocketPC.ServiceForPocketPC(); svc.Credentials = new System.Net.NetworkCredential(Settings.UserName Settings.Password); svc.AllowAutoRedirect = false; svc.UserAgent = Settings.UserAgent; svc.PreAuthenticate = true; svc.Url = Settings.Url; svc.Timeout = System.Threading.Timeout.Infinite; svc.CallMethod(); ... } catch (Exception ex) { ShowError(ex); } finally { if (svc != null) svc.Dispose(); } } Strategy 2 private myWebservice svc; private void Main_Load(object sender EventArgs e) { //Connect to webservice svc = new ForPocketPC.ServiceForPocketPC(); svc.Credentials = new System.Net.NetworkCredential(Settings.UserName Settings.Password); svc.AllowAutoRedirect = false; svc.UserAgent = Settings.UserAgent; svc.PreAuthenticate = true; svc.Url = Settings.Url; svc.Timeout = System.Threading.Timeout.Infinite; } private void btnRead_Click(object sender EventArgs e) { try { //Show clock this.picResult.Image = new Bitmap(pathWait); Application.DoEvents(); svc.CallMethod(); ... } catch (Exception ex) { ShowError(ex); } } private void Main_Closing(object sender CancelEventArgs e) { svc.Dispose(); } It depends on how often you are going to be calling the web service. If you're going to be calling it almost constantly it would probably be better to use method #2. However if it's not going to be getting called quite so often you are better off using method #1 and only instantiating it when you need it.  Right now I made a solution for a mobile device and it turns to be used on irregular times it could be used in 10 minutes 1 hour 4 hours its very variable it seems that the better aproach is the first strategy. Last year we went on a project where we used webservices the fact is that we instantiated our webservices at the Sub New() procedure and it run it very well however sometimes some users claimed at us that they woke up from their chairs and when they returned and tried to continue on the application they received a timeout error message and they had to re-login again. We thougth that maybe that was Ok because maybe the users went out for a very long time out of their seats but once in a presentation of the application with the CEOs it happened exactly the same scenario and personally I didn't like that behaviour and that's why the question. Thanks for the answer."68,A,"Database Table and Column Naming Conventions? Whenever I design a database I always wonder if there is a best way of naming an item in my database. Quite often I ask myself the following questions: Should table names be plural? Should column names be singular? Should I prefix tables or columns? Should I use any case in naming items? Are there any recommended guidelines out there for naming items in a database? why did you finally chose the answer you chose? there are quite many voting for plural table names the one you chose votes for singular. just curious? I think the best answer for a question like this that comes down to a matter of preference should summarize the competing choices. phasetwenty's answer below links to several; it would be good to inline those links here. I think we should name plural for Tables and singular for columns. I see a table as ""storage"" with multiple items not single ""entity"" so I name it plural. When I mapped tables into objects I would name the objects singular. This is just my personal opinion.  --Example SQL CREATE TABLE D001_Students ( StudentID INTEGER CONSTRAINT nnD001_STID NOT NULL ChristianName NVARCHAR(255) CONSTRAINT nnD001_CHNA NOT NULL Surname NVARCHAR(255) CONSTRAINT nnD001_SURN NOT NULL CONSTRAINT pkD001 PRIMARY KEY(StudentID) ); CREATE INDEX idxD001_STID on D001_Students; CREATE TABLE D002_Classes ( ClassID INTEGER CONSTRAINT nnD002_CLID NOT NULL StudentID INTEGER CONSTRAINT nnD002_STID NOT NULL ClassName NVARCHAR(255) CONSTRAINT nnD002_CLNA NOT NULL CONSTRAINT pkD001 PRIMARY KEY(ClassID StudentID) CONSTRAINT fkD001_STID FOREIGN KEY(StudentID) REFERENCES D001_Students(StudentID) ); CREATE INDEX idxD002_CLID on D002_Classes; CREATE VIEW V001_StudentClasses ( SELECT D001.ChristianName D001.Surname D002.ClassName FROM D001_Students D001 INNER JOIN D002_Classes D002 ON D001.StudentID = D002.StudentID ); These are the conventions I was taught but you should adapt to whatever you developement hose uses. Plural. It is a collection of entities. Yes. The attribute is a representation of singular property of an entity. Yes prefix table name allows easily trackable naming of all constraints indexes and table aliases. Pascal Case for table and column names prefix + ALL caps for indexes and constraints. ChristianName ... that's an odd convention. @Bobby: especially for non-catholics @Ian Boyd: especially because it's `NOT NULL` Serial numbers on your tables? Does anyone seriously think this makes sense **works** for the developers? **!@Shock@!** is hereby expressed at numeric object prefixes.  I think the best answer to each of those questions would be given by you and your team. It's far more important to have a naming convention then how exactly the naming convention is. As there's no right answer to that you should take some time (but not too much) and choose your own conventions and - here's the important part - stick to it. Of course it's good to seek some information about standards on that which is what you're asking but don't get anxious or worried about the number of different answers you might get: choose the one that seems better for you. Just in case here are my answers: Yes. A table is a group of records teachers or actors so... plural. Yes. I don't use them. The database I use more often - Firebird - keeps everything in upper case so it doesn't matter. Anyway when I'm programming I write the names in a way that it's easier to read like releaseYear.  I hear the argument all the time that whether or not a table is pluralized is all a matter of personal taste and there is no best practice. I don't believe that is true especially as a programmer as opposed to a DBA. As far as I am aware there are no legitimate reasons to pluralize a table name other than ""It just makes sense to me because it's a collection of objects"" while there are legitimate gains in code by having singular table names. For example: It avoids bugs and mistakes caused by plural ambiguities. Programmers aren't exactly known for their spelling expertise and pluralizing some words are confusing. For example does the plural word end in 'es' or just 's'? Is it persons or people? When you work on a project with large teams this can become an issue. For example an instance where a team member uses the incorrect method to pluralize a table he creates. By the time I interact with this table it is used all over in code I don't have access to or would take to long to fix. The result is I have to remember to spell the table wrong every time I use it. Something very similar to this happened to me. The easier you can make it for every member of the team to consistently and easily use the exact correct table names without errors or having to look up table names all the time the better. The singular version is much easier to handle in a team environment. If you use the singular version of a table name AND prefix the primary key with the table name you now have the advantage of easily determining a table name from a primary key or vice versa via code alone. You can be given a variable with a table name in it concatenate ""Id"" to the end and you now have the primary key of the table via code without having to do an additional query. Or you can cut off ""Id"" from the end of a primary key to determine a table name via code. If you use ""id"" without a table name for the primary key then you cannot via code determine the table name from the primary key. In addition most people who pluralize table names and prefix PK columns with the table name use the singular version of the table name in the PK (for example statuses and statusId) making it impossible to do this at all. If you make table names singular you can have them match the class names they represent. Once again this can simplify code and allow you to do really neat things like instantiating a class by having nothing but the table name. It also just makes your code more consistent which leads to... If you make the table name singular it makes your naming scheme consistent organized and easy to maintain in every location. You know that in every instance in your code whether it's in a column name as a class name or as the table name it's the same exact name. This allows you to do global searches to see everywhere that data is used. When you pluralize a table name there will be cases where you will use the singular version of that table name (the class it turns into in the primary key). It just makes sense to not have some instances where your data is referred to as plural and some instances singular. To sum it up if you pluralize your table names you are losing all sorts of advantages in making your code smarter and easier to handle. There may even be cases where you have to have lookup tables/arrays to convert your table names to object or local code names you could have avoided. Singular table names though perhaps feeling a little weird at first offer significant advantages over pluralized names and I believe are best practice.  I prefer singular name with PascalCase and without prefixes. For example: it is more natural to work with entities in singular if you use EF or if you create your scheme (LINQ to SQL) in Visual Studio. Yes If you have small project and you use just one database it is no reason for using table prefixes. If you have one database and you want to use it for more projects you should use prefixes. prj_Category oth_Category... Dont use prefixes like tbl_. IMHO in the most situations is no reason for using column prefixes. PascalCase or prefix_PascalCase This practices I use for web applications creating on MS products. Its just as natural to work with them in in EF if they are plural too. I see no difference.  Throughout the years I have added new columns at the end of my tables in the app I developed and market. Sometimes I use english names in my columns sometimes I use spanish and sometimes I re-use columns for something else instead of deleting them and adding a new column with a proper descriptive name for what it is used. I purposely did this in order to OBFUSCATE my source code in case someone else tries to hack or reverse-engineer my code. Only I can understand it someone else will get frustrated!..This way they always have to rely on me for anything! ITS MY OWN BUSINESS I DONT WORK FOR ANYONE ELSE!.. plus I am so familiar with the column names and code that I can quickly make changes while hanging from a pole blindfolded.. I DO THIS TO PROTECT MY SOURCE JUST INCASE SOMEONE SHOULD EVER GET A HOLD OF IT.. YOU NEVER KNOW! The only person who will ever work on my app is my Son who already understands my obfuscated code and his Son will also work on it and so on!.. I'd rather see it disappear than fall into someone elses hands I did not want it to. Too many hackers out there trying to steal proprietary things. Why do you think many large outfits are obfuscating their code? Frank I have to clean up the kind of legacy 'work' you do. Trust me this is not somethign to brag about. I know it makes sense to you but we live in a connected world. I certainly hope this is trolling. If not then we have a serious problem. I'm not sure if he was kidding or being serious. What kind of applications do you write? It seems strange that you chose to make *your* life harder as a developer (in the name of job security) instead of letting a tool automatically obfuscate the application for you. I have no problem understanding my own work because I have a method for camouflaging my code. **!@Shock@!** is hereby expressed once again.  Table names should always be singular because they represent a set of objects. As you say herd to designate a group of sheep or flock do designate a group of birds. No need for plural. When a table name is composition of two names and naming convention is in plural it becomes hard to know if the plural name should be the first word or second word or both. Its the logic  Object.instance not objects.instance. Or TableName.column not TableNames.column(s). Microsoft SQL is not case sensitive its easier to read table names if upper case letters are used to separate table or column names when they are composed of two or more names.  I'm also in favour of a ISO/IEC 11179 style naming convention noting they are guidelines rather than being prescriptive. See Data element name on Wikipedia: ""Tables are Collections of Entities and follow Collection naming guidelines. Ideally a collective name is used: eg. Personnel. Plural is also correct: Employees. Incorrect names include: Employee tblEmployee and EmployeeTable."" As always there are exceptions to rules e.g. a table which always has exactly one row may be better with a singular name e.g. a config table. And consistency is of utmost importance: check whether you shop has a convention and if so follow it; if you don't like it then do a business case to have it changed rather than being the lone ranger. Hi are there some practical examples somewhere? -1: The referenced text has nothing to do with ISO/IEC 11179. The referenced wikipedia page should not be trusted; read the actual standard instead (http://metadata-standards.org/11179/#A5) @onedaywhen: I don't know enough about the subject to correct the wikipedia page; Also the wikipedia page is not so much wrong as it is misleading - it doesn't explicitly say that ISO/IEC 11179 includes the database naming conventions it just says that ""ISO/IEC 11179 is applicable when naming tables and columns within a relational database"". It then goes on to provide an example of naming conventions that might be used for relational database. It lets you think that the example is something taken from the standard when it's really something made up by the writer of the wikipedia article. @mkadunc: You are downvoting this answer because wikipedia isn't to be trusted?! I think SO users are familiar with the wikipedia model (e.g. not the trusted source but provides links to the trusted source) and its reliability as regards The Truth. You sound like you know the subject better than I so why aren't you doing your duty and correcting the wikipedia page in question? @mkadunc: why not just correct the parts you do feel qualified to tackle or add notes to clarify or report the problems...? While you are in there please correct ""tables and columns within a relational database"" to ""relational variables (relvars) and their attributes within a relational database"" thanks :)  Take a look at ISO 11179-5: Naming and identification principles You can get it here: http://metadata-standards.org/11179/#11179-5 I blogged about it a while back here: ISO-11179 Naming Conventions Hi are there some practical examples somewhere? Your answer would be more accessible (=better) if you gave a summary here. Great pointer though! +1 to @SQLMenace and +1 to @Ola Eldy  Essential Database Naming Conventions (and Style) (click here for more detailed description) table names choose short unambiguous names using no more than one or two words distinguish tables easily facilitates the naming of unique field names as well as lookup and linking tables give tables singular names never plural (update: i still agree with the reasons given for this convention but most people really like plural table names so ive softened my stance)... follow the link above please Oracle's naming convention was the funniest of them all.. `e.g. PATIENTS would have a primary key called pa_patient_id_pk` !! complete detail with examples are provided on the link attached. please follow the link although what Oracle suggest it totally opposite to link linke above. find what Oracle says here..http://ss64.com/ora/syntax-naming.html [This answer has already been given almost 3 years ago...](http://stackoverflow.com/questions/7662/database-table-and-column-naming-conventions/226710#226710)  our preference: Should table names be plural? Never. The arguments for it being a collection make sense but you never know what the table is going to contain (01 or many items). Plural rules make the naming unnecessarily complicated. 1 House 2 houses mouse vs mice person vs people and we haven't even looked at any other languages. Update person set property = 'value' acts on each person in the table. Select * from person where person.name = 'Greg' returns a collection/rowset of person rows. Should column names be singular? Usually yes except where you are breaking normalisation rules. Should I prefix tables or columns? Mostly a platform preference. We prefer to prefix columns with the table name. We don't prefix tables but we do prefix views (v_) and stored_procedures (sp_ or f_ (function)). That helps people who want to try to upday v_person.age which is actually a calculated field in a view (which can't be UPDATEd anyway). It is also a great way to avoid keyword collision (delivery.from breaks but delivery_from does not). It does make the code more verbose but often aids in readability. bob = new person() bob.person_name = 'Bob' bob.person_dob = '1958-12-21' ... is very readable and explicit. This can get out of hand though: customer.customer_customer_type_id indicates a relationship between customer and the customer_type table indicates the primary key on the customer_type table (customer_type_id) and if you ever see 'customer_customer_type_id' whilst debugging a query you know instantly where it is from (customer table). or where you have a M-M relationship between customer_type and customer_category (only certain types are available to certain categories) customer_category_customer_type_id ... is a little (!) on the long side. Should I use any case in naming items? Yes - lower case :) with underscores. These are very readable and cross platform. Together with 3 above it also makes sense. Most of these are preferences though. - As long as you are consistent it should be predictable for anyone that has to read it.  SELECT UserID FirstName MiddleInitial Lastname FROM Users ORDER BY Lastname Note the standards used: tables hold multiple things users have one first name T-SQL keywords in uppercase table definitions in Pascal case.  Late answer here but in short: My preference is plural Yes Tables: *Usually* no prefixes is best. Columns: No. Both tables and columns: Pascal casing. Elaboration: (1) What you must do. There are very few things that you must do a certain way every time but there are a few. Name your primary keys using ""[singularOfTableName]ID"" format. That is whether your table name is Customer or Customers the primary key should be CustomerID. Further foreign keys must be named consistently in different tables. It should be legal to beat up someone who does not do this. I would submit that while defined foreign key constraints are often important consistent foreign key naming is always important You database must have internal conventions. Even though in later sections you'll see me being very flexible within a database naming must be very consistent . Whether your table for customers is called Customers or Customer is less important than that you do it the same way throughout the same database. And you can flip a coin to determine how to use underscores but then you must keep using them the same way. If you don't do this you are a bad person who should have low self-esteem. (2) What you should probably do. Fields representing the same kind of data on different tables should be named the same. Don't have Zip on one table and ZipCode on another. To separate words in your table or column names use PascalCasing. Using camelCasing would not be intrinsically problematic but that's not the convention and it would look funny. I'll address underscores in a moment. (You may not use ALLCAPS as in the olden days. OBNOXIOUSTABLE.ANNOYING_COLUMN was okay in DB2 20 years ago but not now.) Don't artifically shorten or abbreviate words. It is better for a name to be long and clear than short and confusing. Ultra-short names is a holdover from darker more savage times. Cus_AddRef. What on earth is that? Custodial Addressee Reference? Customer Additional Refund? Custom Address Referral? (3) What you should consider. I really think you should have plural names for tables; some think singular. Read the arguments elsewhere. Column names should be singular however. Even if you use plural table names tables that represent combinations of other tables might be in the singular. For example if you have a Promotions and an Items table a table representing an item being a part of a promotion could be Promotions_Items but it could also legitimately be Promotion_Items I think (reflecting the one-to-many relationship). Use underscores consistently and for a particular purpose. Just general tables names should be clear enough with PascalCasing; you don't need underscores to separate words. Save underscores either (a) to indicate an associative table or (b) for prefixing which I'll address in the next bullet. Prefixing is neither good or bad. It usually is not best. In your first db or two I would not suggest using prefixes for general thematic grouping of tables. Tables end up not fitting your categories easily and it can actually make it harder to find tables. With experience you can plan and apply a prefixing scheme that does more good than harm. I worked in a db once where data tables began with tbl config tables with ctbl views with vew proc's sp and udf's fn and a few others; it was meticulously consistently applied so it worked out okay. The only time you NEED prefixes is when you have really separate solutions that for some reason reside in the same db; prefixing them can be very helpful in grouping the tables. Prefixing is also okay for special situations like for temporary tables that you want to stand out. Very seldom (if ever) would you want to prefix columns. @Triynko if you use just ""ID"" it also become programatically impossible to determine the table it belongs to. With the table name prefix you can simply cut off the last two digits of a primary key and know the table name it belongs to via code. A lot of times IT and DBA people don't realize that there are coding advantages for programmers in designing databases in certain ways. I think the primary key should just be ""ID"". Such a simple convention makes the primary key predictable and quickly identifiable. I would however prepend the table name (""PersonID"") when its used as a foreign key in other tables. This convention could help distinguish between a primary key and foreign keys in the same table. ""Fields representing the same kind of data on different tables should be named the same. Don't have Zip on one table and ZipCode on another."" Yes yes a million times yes. Can you tell our database was not designed that way? A personid might be refered to in any of a dozen different ways very annoying to maintain. I've always kept to this rule in any database I had control over designing and it makes life much simpler. @Tryinko Using ID all over the place is LIVING HELL for anyone doing joins of multiple tables. There's no possible way that the slight advantage of knowing this is the PK outweighs the incredible annoyance of re-aliasing the dang ID column in every bloody query over and over again. If you want a way to denote PK in a table make it the first column. Also denoting FKs in the names of columns is in my mind another solidly evil anti-pattern.  Table Name: It should be singular as it is a singular entity representing a real world object and not objects which is singlular. Column Name: It should be singular only then it conveys that it will hold an atomic value and will confirm to the normalization theory. If however there are n number of same type of properties then they should be suffixed with 1 2 ... n etc. Prefixing Tables / Columns: It is a huge topic will discuss later. Casing: It should be Camel case My friend Patrick Karcher I request you to please not write anything which may be offensive to somebody as you wrote ""Further foreign keys must be named consistently in different tables. It should be legal to beat up someone who does not do this."". I have never done this mistake my friend Patrick but I am writing generally. What if they together plan to beat you for this? :) oh please...... So you are saying the table is the entity? Or is the row in the table the entity? To me a table is a collection of rows - hence a collection of entities which implies plural.  In my opinion: Table names should be plural. Column names should be singular. No. Either CamelCase (my preferred) or underscore_separated for both table names and column names. However like it has been mentioned any convention is better than no convention. No matter how you choose to do it document it so that future modifications follow the same conventions.  Very late to the party but I still wanted to add my two cents about column prefixes There seem to be two main arguments for using the table_column (or tableColumn) naming standard for columns both based on the fact that the column name itself will be unique across your whole database: 1) You do not have to specify table names and/or column aliases in your queries all the time 2) You can easily search your whole code for the column name I think both arguments are flawed. The solution for both problems without using prefixes is easy. Here's my proposal: Always use the table name in your SQL. E.g. always use table.column instead of column. It obviously solves 2) as you can now just search for table.column instead of table_column. But I can hear you scream how does it solve 1)? It was exactly about avoiding this. Yes it was but the solution was horribly flawed. Why? Well the prefix solution boils down to: To avoid having to specify table.column when there's ambiguity you name all your columns table_column! But this means you will from now on ALWAYS have to write the column name every time you specify a column. But if you have to do that anyways what's the benefit over always explicitly writing table.column? Exactly there is no benefit it's the exact same number of characters to type. edit: yes I am aware that naming the columns with the prefix enforces the correct usage whereas my approach relies on the programmers As you mentioned you cannot rely on every case having table.column. Programmers will forget in one place and then your global find and replace just broke your whole program. Or you'll make it a rule and someone will think he's fulfilling the rule by using an alias of the table thus again foiling a global find. In addition if you want to organize your code by having some sort of database class (which any good programmer will) there will be times when you'll just pass a column name to a db function or just have the column name alone in a variable.  Ok since we're weighing in with opinion: I believe that table names should be plural. Tables are a collection (a table) of entities. Each row represents a single entity and the table represents the collection. So I would call a table of Person entities People (or Persons whatever takes your fancy). For those who like to see singular ""entity names"" in queries that's what I would use table aliases for: SELECT person.Name FROM People person A bit like LINQ's ""from person in people select person.Name"". As for 2 3 and 4 I agree with @Lars. @John Topley: I bet you don't say ""the data are"" and thus you should have no real conceptual problem with things that are multiple being called by a singular name. If you want to name things plural I see that as a simple preference that doesn't relate to the number of rows in the table. If you saw a database full of singular table names such as Person Invoice Address and so on would you really get confused and think they only had one row in each of them? @Emtucifor: In English we don't say ""Look at all the person out there in that crowd of person!"" Having a conceptual problem with things that are multiple being referred to by a singular word is to be expected. It's neither usual nor proper. ""Data"" is exceptional and often used to refer to a piece of a volume of substance much like ""cake"". ""Would you like (a piece of) cake?"" Naming a table ""People"" because it contains information on multiple individuals makes far more sense than naming it ""Person"". A data class named ""Person"" for the ROW makes sense as do singular column names. @Triynko I guess it all determines how you think about tables and how you use them. Tables names are used in queries; whether they fit properly into a certain rigid English sentence pattern is up to your personal preference. Here's an English sentence that handles your grammar objection no problem: ""Give me all Person rows from the table of the same name."" Anyway my point is that your conceptual/language argument is not automatic or global--it's just how you like to think about it. Other ways are just as logical. @Triynko: Given that a table is known by all to be a collection of rows I wouldn't ever name it ""Person Collection"" just like I wouldn't name it ""Person Table"". Again it all depends on how you think about it. Restrooms say Men and Women but they could just as easily say Male and Female (rather than Males and Females). I'm just going to say again: If you saw a database full of singular table names such as Person Invoice Address and so on would you really get confused and think they only had one row in each of them? @Emtucifor: Ultimately all language is arbitrary and conventional. I was just arguing that conventionally we refer to a collection of items as the plural of the type of item therein. So a collection of rows where each row has information about a single person would be refferred to as a collection of People. But if you want to refer to it as a collection of Person go right ahead. @Emtucifor: Yes lol. Naming the table ""PersonCollection"" would be equivalent to naming it ""People"". Contrast that with naming such a collection just ""Person"" which does not make sense :) @Triynko: You mean the Person collection? :) @Emtucifor: Then let's think of it from another angle to put the naming convention in a context. Suppose you have object classes for representing both the row and the table. ""Person"" obviously makes sense for the class that represents a row of data. If you're table was also named ""Person"" then you might have a naming conflict or some confusion. I just think that it makes more sense to name objects with accurate plurality. A row with data about a person should be called Person and a table with information about people or multiple persons is called People PersonCollection Persons etc. @Josh M. I'm curious - how do you name your collection variables in code? If you have an array of people do you call the variable ""person""? The ""People person"" alias is rediculous. No need to add to confusion just name the table ""Person"" in the first place. The name of a table should describe what an entry\row in the table is. Of course it would be something plural like ""People."" But that doesn't mean the table name should be. Use the SQL example in this answer as proof. Doing SELECT People.Name is very odd. @Josh M. Opinions vary. ""SELECT People.Name"" does look wrong but naming a collection of people (which is all the table is) ""Person"" also looks wrong. Yep. Either way you go it ""feels"" half wrong. @Josh M. Well not *either* way you go. If you go with my way you can alias the People table as ""person"" and have SELECT person.Name. Problem solved. ;-)  Naming conventions allow the development team to design discovereability and maintainability at the heart of the project. A good naming convention takes time to evolve but once its in place it allows the team to move forward with a common language. A good naming convention grows organically with the project. A good naming convention easily copes with changes during the longest and most important phase of the software lifecycle - service management in production. Here are my answers: Yes table names should be plural when they refer to a set of trades securities or counterparties for example. Yes. Yes. SQL tables are prefixed with tb views are prefixed vw stored procedures are prefixed usp_ and triggers are prefixed tg_ followed by the database name. Column name should be lower case separated by underscore. Naming is hard but in every organisation there is someone who can name things and in every software team there should be someone who takes responsibility for namings standards and ensures that naming issues like sec_id sec_value and security_id get resolved early before they get baked into the project. So what are the basic tenets of a good naming convention and standards: - Use the language of your client and your solution domain Be descriptive Be consistent Disambiguate reflect and refactor Dont use abbreviations unless they are clear to everyone Dont use SQL reserved keywords as column names why a downvote? this is a good answer. Well I'll balance that out right here . . .  Definitely keep table names singular person not people Same here No. I've seen some terrible prefixes going so far as to state what were dealing with is a table (tbl_) or a user store procedure (usp_). This followed by the database name... Don't do it! Yes. I tend to PascalCase all my table names I've always liked the way that the select statement sounds better if it is plural. `SELECT idname FROM contacts WHERE email_address LIKE '%gmail%'` tables plural columns singular. Again always a matter of personal opinion. OMG. NO. Table names DEFINITELY plural. It's a COLLECTION. It has multiple things in it. ""select * from PEOPLE"". You're not selecting from a single person you're selecting from multiple PEOPLE!  Here's a link that offers a few choices. I was searching for a simple spec I could follow rather than having to rely on a partially defined one. http://justinsomnia.org/writings/naming_conventions.html  I work in a database support team with three DBAs and our considered options are: Any naming standard is better than no standard. There is no ""one true"" standard we all have our preferences If there is standard already in place use it. Don't create another standard or muddy the existing standards. We use singular names for tables. Tables tend to be prefixed with the name of the system (or its acronym). This is useful if the system complex as you can change the prefix to group the tables together logically (ie. reg_customer reg_booking and regadmin_limits). For fields we'd expect field names to be include the prefix/acryonm of the table (i.e. cust_address1) and we also prefer the use of a standard set of suffixes ( _id for the PK _cd for ""code"" _nm for ""name"" _nb for ""number"" _dt for ""Date""). The name of the Foriegn key field should be the same""as the Primary key field. i.e. SELECT cust_nm cust_add1 booking_dt FROM reg_customer INNER JOIN reg_booking ON reg_customer.cust_id = reg_booking.cust_id When developing a new project I'd recommend you write out all the preferred entity names prefixes and acronyms and give this document to your developers. Then when they decide to create a new table they can refer to the document rather than """"guess"""" what the table and fields should be called. cry for sure... sheesh Note to self: Never hire Frank. Certainly makes the SQL unreadable; but i think i can translate. cust_nm should be **CustomerName** booking_dt should be **BookingDate**. reg_customer well i have no idea what that is. Especially for number 3 we had a agroup of folks who all got hired from the same company and they tried to impose their old naming standard (which none of the rest of us used) on anything they did. Very annoying. i agree that a particular standard is not as important as having a consistent standard. But some standards are wrong. DB2 and column names like CSPTCN CSPTLN CSPTMN CSDLN. People should learn that long names have been invented - we can afford to make things readable. @Ian. The intention is that you stick to the naming convension your used to and keep it consistent. I ALWAYS know that any date field is _dt any name field is _nm. 'reg' is an example of a """"registration"""" system (bookings customers etc) and all the related tables would have the same prefix. But each to their own... Throughout the years I have added new columns at the end of my tables in the app I developed and market. Sometimes I use english names in my columns sometimes I use spanish and sometimes I re-use columns for something else instead of deleting them and adding a new column with a proper descriptive name for what it is used. I purposely did this in order to OBFUSCATE my source code in case someone else tries to hack or reverse-engineer my code. Only I can understand it someone else will get frustrated!..This way they always have to rely on me for anything! @Frank not sure if I sould laugh or cry...  Table names singular. Let's say you were modelling a realtionship between someone and their address. For example if you are reading a datamodel would you prefer 'each person may live at 01 or many address.' or 'each people may live at 01 or many addresses.' I think its easier to pluralise address rather than have to rephrase people as person. Plus collective nouns are quite often dissimlar to the singular version.  I recommend checking out Microsoft's SQL Server sample databases: http://codeplex.com/SqlServerSamples The AdventureWorks sample uses a very clear and consistent naming convention that uses schema names for the organization of database objects. Singular names for tables Singular names for columns Schema name for tables prefix (E.g.: SchemeName.TableName) Pascal casing I wouldn't rely on Microsoft for any standard - if you look at their northwind database you'll see they use Plural Tables Singular Column Names Schema Prefixes for Tables Table Prefixes for Primary Key Columns Hungarian-esque Constraint Prefixes and worst of all SPACES """" """" for multi-word table names. Additionally system tables for SQLServer use plurals so it seems AdventureWorks was the black sheep in this bunch. I think the main issue here is that the Singular table name crowd seem to consider the table as the entity rather than the row in the table which the Plural crowd does. You have to ask your self which it is. If the table is just a container of rows isn't it more logical to use plural naming? You would never name a collection in code singular then why would you name the table singular? Why the inconsistency? I hear all the arguments about how they sort and use in joins but those all seem very flimsy arguments. If it all comes down to preference I will go with the consistency and pluralize. Also consider which direction the tide is going. It seems like its going in the direction of plural table names especially since all the system tables in SQL Server are all plural and the default for Entity Framework is plural out of the box. If that's Microsoft's stance I want to go the direction where we will be in 20 years. Even Oracle's database conventions say plural table names. Just think how many c# developers hated the """"var"""" keyword when it was introduced now its the widely accepted way to define variables. I'm glad the correct answer made it to the top. Think about it this way... what do you call a container that holds other things? A table is a row-container... like a bag of rocks can of worms. @Jasmine: but do you name your tables """"Bag"""" and """"Can"""" or """"Rock"""" and """"Worm""""? Table names are often named for what they contain (in this example the latter pair of choices) versus some kind of container name (the former pair). Calling a container of Rocks simply """"Rock"""" doesn't make any sense to the plural-favoring crowd. @Derek no it's about what you call the container - you don't say """"bags of rocks"""" unless there is more than one bag. Since it would be redundant to name our tables as """"TableOfInvoices"""" for example we label the tables with the names of the objects it contains - so the table is named """"Invoice"""" because it defines an invoice container and there is only one invoice container called Invoice. @Jasmine - I see your point of view though I think you inadvertently named your example table backwards. """"TableOfInvoices"""" should be shortened to """"Invoices"""" which is what I prefer. You probably instead meant """"InvoiceTable"""" which makes sense to shorten """"Invoice."""" And this is hardly a big issue when it comes to database design. When you look at all the other problems people seem to have naming conventions look fairly trivial. That's probably old fashion but I prefer to use lower case with underscore for items naming in order to avoid any problem when switching between systems taking in account lower case or not (ex.: Linux/Windows). What about Stored Procedures and Functions @Emtucifor: Fair catch. I had interpreted it to mean tables named like `production_BillOfMatterials`. I've edited urini's answer for clarification (and so that I could undo my down vote.) It's actually a non-trivial effort to find the actual AdventureWorks table names if one is not on Windows! @Stu Thompson: It sounds like he's saying use a schema name *instead of* a prefix so are you sure about that -1? http://www.wilsonmar.com/sql_adventureworks.htm is an excellent analysis of the AdventureWorks schema.  No. A table should be named after the entity it represents. Person not persons is how you would refer to whoever one of the records represents. Again same thing. The column FirstName really should not be called FirstNames. It all depends on what you want to represent with the column. NO. Yes. Case it for clarity. If you need to have columns like """"FirstName"""" casing will make it easier to read. Ok. Thats my $0.02 """"A table should be named after the entity it represents"""" A table is a collection of entities. While a table is also an entity it is an entity of type """"Table"""" which is pointless to add to its name. Adding some clarity to number 3 - prefixes are a way of embedding metadata into the column name. There should be no need to do this in any modern DB for the same reasons as (overuse of) Hungarian notation. @Ian Boyd: You actually write SQL by hand? ;) @Lars: i certainly don't use ORM `select top 15 from order' or 'select top 15 from orders'? The latter is my (human) preference. @Lars M_hlum: I write ALL my SQL by hand. I would wager that I do it faster than someone using a GUI too. Though... perhaps you were being sarcastic? @Ian Boyd: Yep: SELECT TOP 100 * FROM Report R INNER JOIN VisitReport VR ON R.ReportID = VR.ReportID. It all depends on how you think about it. If you put a picture of a lemon on a canister you'd know there were lemons inside without needing two lemons on the outside to indicate that it could be plural. Sure you might label it with the written word """"lemons."""" But it might just as well be """"lemon"""". To acquire the resource named """"lemon"""" go here. add $0.01 for using UpperCase in column names and add another $0.01 for using underscore in column names so that its easier to distinguish column names in plain sight. Total = My $0.02 donation to you!  My opinions on these are: 1) No table names should be singular. While it appears to make sense for the simple selection (select * from Orders) it makes less sense for the OO equivalent (Orders x = new Orders). A table in a DB is really the set of that entity it makes more sense once you're using set-logic: select Orders.* from Orders inner join Products on Orders.Key = Products.Key That last line the actual logic of the join looks confusing with plural table names. I'm not sure about always using an alias (as Matt suggests) clears that up. 2) They should be singular as they only hold 1 property 3) Never if the column name is ambiguous (as above where they both have a column called [Key]) the name of the table (or its alias) can distinguish them well enough. You want queries to be quick to type and simple - prefixes add unnecessary complexity. 4) Whatever you want I'd suggest CapitalCase I don't think there's one set of absolute guidelines on any of these. As long as whatever you pick is consistent across the application or DB I don't think it really matters.  I know this is late to the game and the question has been answered very well already but I want to offer my opinion on #3 regarding the prefixing of column names. All columns should be named with a prefix that is unique to the table they are defined in. E.g. Given tables """"customer"""" and """"address"""" let's go with prefixes of """"cust"""" and """"addr"""" respectively. """"customer"""" would have """"cust_id"""" """"cust_name"""" etc. in it. """"address"""" would have """"addr_id"""" """"addr_cust_id"""" (FK back to customer) """"addr_street"""" etc. in it. When I was first presented with this standard I was dead-set against it; I hated the idea. I couldn't stand the idea of all that extra typing and redundancy. Now I've had enough experience with it that I'd never go back. The result of doing this is that all of the columns in your database schema are unique. There is one major benefit to this which trumps all arguments against it (in my opinion of course): You can search your entire code base and reliably find every line of code that touches a particular column. The benefit from #1 is incredibly huge. I can deprecate a column and know exactly what files need to be updated before the column can safely be removed from the schema. I can change the meaning of a column and know exactly what code needs to be refactored. Or I can simply tell if data from a column is even being used in a particular portion of the system. I can't count the number of times this has turned a potentially huge project into a simple one nor the amount of hours we've saved in development work. Another relatively minor benefit to it is that you only have to use table-aliases when you do a self join: SELECT cust_id cust_name addr_street addr_city addr_state FROM customer INNER JOIN address ON addr_cust_id = cust_id WHERE cust_name LIKE 'J%'; I've being prefixing my column names for more than 25 years!.. I dont even have to use """"table.column"""" notation in my SQL statements unless absolutely necessary because the engine knows which column belongs to which table in the data dictionary. Don't you ever do `select * `? Agreed it's a bad practice but certainly there are situations that require it. @Raveren - I use """"SELECT *"""" all the time; I was just being explicit. Then you can no longer `reliably find every line of code that touches a particular column`... Isn't that the point? The alternative is just to use the `.` as intended and stop reinventing the wheel. @Raveren - You still can. If all you do is """"SELECT *"""" then the query is irrelevant for this purpose. When/If later you use the results of that query you have to use the column name to do something with its data so that is the place you need to worry about in your code not the SQL statement. I wouls be curious as to what situations require SELECT *? I certainly would not want anyone to use that in production code. Yes it is useful for ad hoc queries and for finding out which piece of data is making your multiple join query results be odd but I can think of no place in production code where it is required. @HLGEM - Nothing requires it of course. But if you have one query to pull data but multiple places that would run that query to use its data it can be easier for maintenance tasks. Then as the various """"consumers"""" need more or less of the columns you only have to update the consumer not the producer also. @Raveren As far as I'm concerned you shouldn't use select * its better to specify the specific columns That's precisely the reason why there are ALIAS for tables.""",,69,A,How to copy the contents of an FTP directory to a shared network path? I have the need to copy the entire contents of a directory on a FTP location onto a shared networked location. FTP Task has you specify the exact file name (not a directory) and File System Task does not allow accessing a FTP location. EDIT: I ended up writing a script task. When I need to do this sort of thing I use a batch file to call FTP on the command line and use the mget command. Then I call the batch from the DTS/DTSX package.  Nothing like reviving a really old thread... but there is a solution to this. To copy the all files from a directory then specify your remote path to be /[directory name]/* Or for just files and not directories /[directory name]/. Or specific file types; /[directory name]/*.csv  I've had some similar issues with the FTP task before. In my case the file names changed based on the date and some other criteria. I ended up using a Script Task to perform the FTP operation. It looks like this is what you ended up doing as well. I'd be curious if anyone else can come up with a better way to use the FTP task. It's nice to have...but VERY limited.70,A,"Is there an easy way to change the color of a bullet in a list? All I want is to be able to change the color of a bullet in a list to a light gray. It defaults to black and I can't figure out how to change it. I know I could just use an image; I'd rather not do that if I can help it. I would just go with an image myself and avoid the extra markup. It's probably the more efficient solution Who cares about ""extra"" markup like this... seriously you lose nothing by doing it. <ul> <li style=""color:#ddd;""><span style=""color:#000;"">List Item</span></li> </ul>  Just do a bullet in a graphics program and use list-style-image: ul { list-style-image:url('gray-bullet.gif'); } The OP explicitly didn't want to use a picture but I came across this looking for a general way to replace the bullet. So I think this answer is nice for completeness.  This was impossible in 2008 but it's becoming possible in 2013! According to http://www.w3.org/TR/css3-lists/#marker-pseudoelement you can have full control over any number glyph or other symbol generated before a list item with the ::marker pseudo-element. To apply this to the most voted answer's solution: <ul> <li>item #1</li> <li>item #2</li> <li>item #3</li> </ul> li::marker { color: red; /* bullet color */ } li { color: black /* text color */ } JSFiddle Example Note though that as of June 2014 this solution is only a W3C proposition and does not work in any major browsers yet.  Wrap the text within the list item with a span (or some other element) and apply the bullet color to the list item and the text color to the span.  The bullet gets its color from the text. So if you want to have a different color bullet than text in your list you'll have to add some markup. Wrap the list text in a span: <ul> <li><span>item #1</span></li> <li><span>item #2</span></li> <li><span>item #3</span></li> </ul> Then modify your style rules slightly: li { color: red; /* bullet color */ } li span { color: black; /* text color */ } +1 for Nice and well worked answer.. seems to be not working in Mobile Safari Exactly what I needed! I have a list of links and the links already have a different color style applied to them anyway. +1 good answer @Prestaul  You'll want to set a ""list-style"" via CSS and give it a color: value. Example: ul.colored {list-style: color: green;}  <ul style=""color: red;""> <li>One</li> <li>Two</li> <li>Three</li> </ul> One Two Three  You could use CSS to attain this. By specifying the list in the color and style of your choice you can then also specify the text as a different color. Follow the example at http://www.echoecho.com/csslists.htm.  Just use CSS: <li style='color:#e0e0e0'>something</li> Yeah that's what I thought too... but styling li color changes the color of the text as well as the bullet.  I managed this without adding markup but instead using li:before. This obviously has all the limitations of :before (no old IE support) but it seems to work with IE8 Firefox and Chrome after some very limited testing. It's working in our controller environment wondering if anyone could check this. The bullet style is also limited by what's in unicode. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"" ""http://www.w3.org/TR/html4/loose.dtd""> <html> <head> <style type=""text/css""> li { list-style: none; } li:before { /* For a round bullet */ content:'\2022'; /* For a square bullet */ /*content:'\25A0';*/ display: block; position: relative; max-width: 0px; max-height: 0px; left: -10px; top: -0px; color: green; font-size: 20px; } </style> </head> <body> <ul> <li>foo</li> <li>bar</li> </ul> </body> </html>  You can use Jquery if you have lots of pages and don't need to go and edit the markup your self. here is a simple example: $(""li"").each(function(){ var content = $(this).html(); var myDiv = $(""<div />"") myDiv.css(""color"" ""red""); //color of text. myDiv.html(content); $(this).html(myDiv).css(""color"" ""yellow""); //color of bullet });  <ul> <li style=""color: #888;""><span style=""color: #000"">test</span></li> </ul> the big problem with this method is the extra markup. (the span tag)  As per W3C spec The list properties ... do not allow authors to specify distinct style (colors fonts alignment etc.) for the list marker ... But the idea with a span inside the list above should work fine!  Hello maybe this answer is late but is the correct one to achieve this. Ok the fact is that you must specify an internal tag to make the LIst text be on the usual black (or what ever you want to get it). But is also true that you can REDEFINE any TAGS and internal tags with CSS. So the best way to do this use a SHORTER tag for the redefinition Usign this CSS definition: li { color: red; } li b { color: black; font_weight: normal; } .c1 { color: red; } .c2 { color: blue; } .c3 { color: green; } And this html code: <ul> <li><b>Text 1</b></li> <li><b>Text 2</b></li> <li><b>Text 3</b></li> </ul> You get required result. Also you can make each disc diferent color: <ul> <li class=""c1""><b>Text 1</b></li> <li class=""c2""><b>Text 2</b></li> <li class=""c3""><b>Text 3</b></li> </ul> you should use . also is deprecated. Counter point to @the_drow is not deprecated. http://stackoverflow.com/questions/1348683/will-the-b-and-i-tags-ever-become-deprecated"71,A,"What's the better database design: more tables or more columns? A former coworker insisted that a database with more tables with fewer columns each is better then one with fewer tables with more columns each. For example rather than a customer table with name address city state zip etc. columns you would have a name table an address table a city table etc. He argued this design was more efficient and flexible. Perhaps it is more flexible but I am not qualified to comment on its efficiency. Even if it is more efficient I think those gains may be outweighed by the added complexity. So are there any significant benefits to more tables with fewer columns over fewer tables with more columns? This is like arguing a tractor is better than a digger. It's not really about the tables it's about the purposes they will serve.  Like everything else: it depends. There is no hard and fast rule regarding column count vs table count. If your customers need to have multiple addresses then a separate table for that makes sense. If you have a really good reason to normalize the City column into its own table then that can go too but I haven't seen that before because it's a free form field (usually). A table heavy normalized design is efficient in terms of space and looks ""textbook-good"" but can get extremely complex. It looks nice until you have to do 12 joins to get a customer's name and address. These designs are not automatically fantastic in terms of performance that matters most: queries. Avoid complexity if possible. For example if a customer can have only two addresses (not arbitrarily many) then it might make sense to just keep them all in a single table (CustomerID Name ShipToAddress BillingAddress ShipToCity BillingCity etc.). Here's Jeff's post on the topic.  I think balance is in order in this case. If it makes sense to put a column in a table then put it in the table if it doesn't then don't. Your coworkers approach would definately help to normalize the database but that might not be very useful if you have to join 50 tables together to get the information you need. I guess what my answer would be is use your best judgement.  When you design your database you should be as close as possible from the meaning of data and NOT your application need ! A good database design should stand over 20 years without a change. A customer could have multiple adresses that's the reality. If you decided that's your application is limited to one adresse for the first release it's concern the design of your application not the data ! It's better to have multiple table instead of multiple column and use view if you want to simplify your query. Most of time you will have performance issue with a database it's about network performance (chain query with one row result fetch column you don't need etc) not about the complexity of your query.  First normalize your tables. This ensures you avoid redundant data giving you less rows of data to scan which improves your queries. Then if you run into a point where the normalized tables you are joining are causing the query to take to long to process (expensive join clause) denormalize where more appropriate.  Hmm. I think its a wash and depends on your particular design model. Definitely factor out entities that have more than a few fields out into their own table or entities whose makeup will likely change as your application's requirements changes (for instance - I'd factor out address anyways since it has so many fields but I'd especially do it if you thought there was any chance you'd need to handle foreign country addresses which can be of a different form. The same with phone numbers). That said when you're got it working keep an eye out on performance. If you've spun an entity out that requires you to do large expensive joins maybe it becomes a better design decision to spin that table back into the original.  A fully normalized design (i.e ""More Tables"") is more flexible easier to maintain and avoids duplication of data which means your data integrity is going to be a lot easier to enforce. Those are powerful reasons to normalize. I would choose to normalize first and then only denormalize specific tables after you saw that performance was becoming an issue. My experience is that in the real world you won't reach the point where denormalization is necessary even with very large data sets.  There are many sides to this but from an application efficiency perspective mote tables can be more efficient at times. If you have a few tables with a bunch of columns every time the db as to do an operation it has a chance of making a lock more data is made unavailable for the duration of the lock. If locks get escalated to page and tables (well hopefully not tables :) ) you can see how this can slow down the system.  I have a few fairly simple rules of thumb I follow when designing databases which I think can be used to help make decisions like this.... Favor normalization. Denormalization is a form of optimization with all the requisite tradeoffs and as such it should be approached with a YAGNI attitude. Make sure that client code referencing the database is decoupled enough from the schema that reworking it doesn't necessitate a major redesign of the client(s). Don't be afraid to denormalize when it provides a clear benefit to performance or query complexity. Use views or downstream tables to implement denormalization rather than denormalizing the core of the schema when data volume and usage scenarios allow for it. The usual result of these rules is that the initial design will favor tables over columns with a focus on eliminating redundancy. As the project progresses and denormalization points are identified the overall structure will evolve toward a balance that compromises with limited redundancy and column proliferation in exchange for other valuable benefits. crappy answer... What exactly is a 'downstream table'? I mean ""downstream"" in the context of a ""data flow"". Which essentially means you have a process which uses the normalized tables as a source and transforms the data somehow and then deposits the result somewhere else.  i would consider normalizing as the first step so cities counties states countries would be better as separate columns... the power of SQL language together with today DBMS-es allows you to group your data later if you need to view it in some other non-normalized view. When the system is being developed you might consider 'unnormalizing' some part if you see that as an improvement. My 2 cents: I have to disagree; doing that kind of optimization during design is a classic case of premature optimization. Wait until you see that performance is a problem *before* you sacrifice a good design.  I would argue in favor of more tables but only up to a certain point. Using your example if you separated your user's information into two tables say USERS and ADDRESS this gives you the flexibility to have multiple addresses per user. One obvious application of this is a user who has separate billing and shipping addresses. The argument in favor of having a separate CITY table would be that you only have to store each city's name once then refer to it when you need it. That does reduce duplication but in this example I think it's overkill. It may be more space efficient but you'll pay the price in joins when you select data from your database.  There are advantages to having tables with fewer columns but you also need to look at your scenario above and answer these questions: Will the customer be allowed to have more than 1 address? If not then a separate table for address is not necessary. If so then a separate table becomes helpful because you can easily add more addresses as needed down the road where it becomes more difficult to add more columns to the table.  It depends on your database flavor. MS SQL Server for example tends to prefer more narrower tables. That's also the more 'normalized' approach. Other engines might prefer it the other way around. Mainframes tend to fall in that category.  There are huge benefits to queries using as few columns as possible. But the table itself can have a large number. Jeff says something on this as well. Basically make sure that you don't ask for more than you need when doing a query - performance of queries is directly related to the number of columns you ask for.  I think you have to look at the kind of data you're storing before you make that decision. Having an address table is great but only if the likelihood of multiple people sharing the same address is high. If every person had different addresses keeping that data in a different table just introduces unnecessary joins. I don't see the benefit of having a city table unless cities in of themselves are entities you care about in your application. Or if you want to limit the number of cities available to your users. Bottom line is decisions like this have to take the application itself into considering before you start shooting for efficiency. IMO.  The multi-table database is a lot more flexible if any of these one to one relationships may become one to many or many to many in the future. For example if you need to store multiple addresses for some customers it's a lot easier if you have a customer table and an address table. I can't really see a situation where you might need to duplicate some parts of an address but not others so separate address city state and zip tables may be a bit over the top. I have 40 unique fields about user information which are unique and they are one to one from User Authentication System. Do you think its ok if I keep those 40 columns in one table? If I separate them I need to write more joins in my queries :-(. Can you suggest  It doesn't sound so much like a question about tables/columns but about normalization. In some situations have a high degree of normalization (""more tables"" in this case) is good and clean but it typically takes a high number of JOINs to get relevant results. And with a large enough dataset this can bog down performance. Jeff wrote a little about it regarding the design of StackOverflow. See also the post Jeff links to by Dare Obasanjo. In my experience this is patently false. I've worked with queries that join dozens of tables *each* containing 1million + rows and as long as you're joining on primary keys the results come back very quickly. What's 'quickly'? If you're running a website trying to serve thousands of pageviews a second 'fast enough' as an entirely different meaning than a single user database where all you're concerned about is response time for the user. ""as long as you're joining on primary keys the results come back very quickly"" Well yeah. But in my experience with more tables the more likely it is for joins to happen on non-pk's non-indexed columns etc. Normalisation and the subsequent joining of tables usually helps performance since by definition you can be more selective and avoid table scans - the slowest method of selecting. Poor design is usually the biggest factor in poor performance not normalisation. Aye I had a real-time data processing app and the joins killed the queries. I de-normalised the data and all was good it gets integrated back into the normalised database at the end of the day when the number of requests chill out.  Each table should only include columns that pertain to the entity that's uniquely identified by the primary key. If all the columns in the database are all attributes of the same entity then you'd only need one table with all the columns. If any of the columns may be null though you would need to put each nullable column into its own table with a foreign key to the main table in order to normalize it. This is a common scenario so for a cleaner design you're likley to be adding more tables than columns to existing tables. Also by adding these optional attributes to their own table they would no longer need to allow nulls and you avoid a slew of NULL-related issues."72,A,"When is it NIH vs. being a sensible investment of time? So I'm sitting here playing catch up listening to Stack Overflow Podcast #20 and Joel is talking about the Excel guys at MS who wrote their own compiler to improve the performance of code that worked with pointers and it got me wondering  when is it NIH versus being a sensible investment of time? Given the situation back then that the current compiler performance was abysmal I can see that selling this particular case to management was fairly simple given that you had a specific area of optimisation that you wanted to add. But nowadays can you imagine trying to go to management and saying you wanted time to rewrite gcc? So how do people determine if an idea is worthwhile pursuing versus a ""tilting at windmills"" pursuit dripping in NIH syndrome? I'm asking because we need to get some time allocated to improve some aspects of our operational platform. I'm interested in how to go ""with cap in hand"" to ask The Man for some time to be allocated to make improvements that most people know will definitely pay off in the future. Edit: I forgot to highlight that we are a service company providing a platform for a major broadcaster. It is this platform that has grown organicaly over the last 15 years that needs some improvements. The provision of this service is our core business function. Joel himself answered that his best advice (which seems reasonable) is: If it's a core business function -- do it yourself no matter what. To that I could add if it's not a core function then you better have both economical and technical good reasons. By the way in what way the improvements you need time to do are covered by NIH?  It's all about competitive advantage. The guys on the Excel team wrote their own compiler so they could make Excel much smaller than it otherwise would have been. This reduced their load-time and also the amount of memory the program used; it made their product better than the competition. Similarly Jeff could have used PhpBB to make this site but then he'd be just like the competition. He'd just be another dude with a forum. He wrote his own forum engine so that he could differentiate his site from the competition. You should only deploy NIH where doing so gives you a clear competitive advantage or where doing so differentiates you from your competition.  Memory allocation is a ""core"" requirement of all my software and hence my business but I wouldn't generally write my own allocator or collector. Like everything else in engineering there's a set of trade-offs in every case. These require experience to make can be successfully handled in different ways and can't be summarised in a blogger's sound-bite. Try to get good at assessing the quality of 3rd-party stuff. log4net which is what started this discussion around here is neglect-ware which has recently been dropped by some well-known projects (like nUnit). That would have been enough to put me off. I didn't know that why did nUnit drop the lib? He said ""function"" as in **feature**. Not requirement. Your memory allocation point is moot.  @Will Memory allocation is not a core business function for your business unless you are writing embedded software or otherwise memory sensitive applications. Even then if your business value is doing something else than being more memory efficient than the competition it still wouldn't be a core business function. And if you are writing that kind of software and it differentiates itself through memory efficiency how did you manage? Yes of course you will always have to take care of the trade offs as always but that hardly adds any value to an answer. ""It depends"" is always a strictly correct answer thus meaningless. @Mel I agree. To me core business functions are functions relating to your main business that differentiate you from your competition. In your accounting example I doubt that the calculation you mention is a core asset of the accounting company but something different like customer service or better knowledge of the law or whatever. And I also disagree if you'll always use something that works even when that part of the application is supposedly your advantage (what will encourage people to use/buy your app.) then it stops being your advantage. Let's just remember that competitive advantage is not always (almost never in the big scheme of things) due to software and thus many many people can use existing components without risking their business.  We had a (vaguely) similar issue a few months ago needing time to re-do something we had already built a different way to cater for instability of a downstream library. It seemed fairly easy to get it approved in the end - the app was suffering and I suppose they believed what we were saying. PS Probably stupid question and definitely not an answer but whats NIH? Not Invented Here  Vinko it depends on your definition of ""Core business function"". If my business is Accounting then calculating loan interest would quite definitely be a core business function but it is certainly not unique to my company. Something ""standard"" like this should almost NEVER be written in-house. If an available component does what I need then I'll use it every time. If I have to do a lot of wrangling to make it work then I might consider writing my own instead. You have to also take legal issues into account. There might be a perfectly good library that I can't use in my commercial application because of a viral license (GPL vs LGPL).  This question is so timely and its very interesting that the development of Excel inspired this question. We have an in-house project in which we needed some Excel functionality programmed into a PHP application. One developer has worked on this projected for over a year and a half. One does not realize how hard it is to rewrite Excel until you've tried it. Anyway last January I noticed that PHP had a PHPExcel library written for it by a 3rd party that is actively being developed and supported (it even supports Excel 2007). So did it completely make sense for us to reinvent Excel (especially considering the turn over of programmers in our department - this guy is leaving us next week). To answer this question you have to ask yourself if the additions or changes you want to make to your project's code will have a great exponential effect on the current feature set or allow you to add more features in the future with exponentially less time spent. You need to analyze and document the upside and downside effects that your code changes will have on the overall project now and in the future. Also consider if the changes will make it easier to maintain and contribute to future growth of the project - if so that adds value to the application especially if there is a possibility that you or others involved may be moving on to another job someday or someone else may need to make changes while you are busy on other projects. By the way ""NIH"" means Not-Invented-Here. See this link by Joel in his blog entry on the topic: In Defense of Not-Invented-Here Syndrome"73,A,"How does google make make those awesome PDF reports in Analytics and when you print a Google Doc etc? When you print from Google Docs (using the ""print"" link not File/Print) you end up printing a nicely formated PDF file instead of relying on the print engine of the browser. Same is true for some of the reports in Google Analytics . . . the printed reports as PDF's are beautiful. How do they do that? I can't imagine they use something like Adobe Acrobat to facilitate it but maybe they do. I've seen some expensive HTML to PDF converters online from time to time but have never tired it. Any thoughts? If you are specifically looking at how Google does it. If you look at the PDF Properties page they use Prince 6.0 (see princexml.com) There are lots of other PDF generators out there. I've had great success with PDFlib for tricky jobs.  Rendering a PDF is hard complex problem. However generating them is not. Simply make up some entities and generate. It's about same problem domain as generating HTML for webpage vs. displaying (rendering) it.  iTextSharp and iText are opensource and free PDF generation libraries for .NET and Java respectively. I've used them to generate report PDF's before and was quite happy with the results. http://itextsharp.sourceforge.net/ http://www.lowagie.com/iText/  I have had success with pd4ml. It has a tag library so you can turn any existing HTML into PDF by <pd4ml:transform> <!-- Your HTML is here --> <c:import url=""/page.html"" /> </pd4ml:transform>  Well I doubt it's as easy as generating HTML . . . I mean first of all PDF is not a human readable format and it's not plain text (like SVG). In fact I would compare a SVG file to a PDF file in that with both you have precise control over the layout on a printed page. But SVG is different in that it's XML (and also in that it's not supported completely in the browser . . . still looking into SVG too). Come to think of it SVG should probably will be my next question. I know Google doesn't use .NET and I doubt they use Java so there must be some other libraries they use for generating the PDF files. More importantly how do they create the PDF's without having to rewrite everything as a PDF instead of as HTML? I mean there has to be some shared code for between when they generate the HTML view as opposed to the PDF view. Come to think of it maybe the PDF view and the HTML view are completely separate and they just have two views and hence why the MVC development style seems to be the way to go.  Great free alternative to PrinceXML: wkhtmltopdf . There are plenty of wrapper libraries for various languages - but I've only used Ruby ones. However the product itseld is on par with PrinceXML IMHO."74,A,"How to make user controls know about css classes in ASP.NET Since there are no header sections for user controls in asp.net user controls have no way of knowing about stylesheet files. So css classes in the user controls are not recognized by visual studio and produces warnings. How can I make a user control know that it will relate to a css class so if it is warning me about a non-existing css class it means that the class really do not exist? Edit: Or should I go for a different design like exposing css classes as properties like ""HeaderStyle-CssClass"" of GridView? You Can use CSS direct in userControl. Use this in UserControl:  <head> <title></title> <style type=""text/css""> .wrapper { margin: 0 auto -142px; /* the bottom margin is the negative value of the footer's height */ } </style> </head> This will work. This means that you would have to have CSS duplicated in each ascx file which is bad in many ways. No you can use this usercontrol in MasterFile.Then there is no need to write this in each file You can also attach stylesheet in usercontrol as given below. Yes you can put a reference to to the UserControl in the MasterPage but the question is asking about how to make the UserControl recognize that a CSS rule exists in a separate .css file. You can also attach stylesheet in usercontrol as given below.  Here's what I did: <link rel=""Stylesheet"" type=""text/css"" href=""Stylesheet.css"" id=""style"" runat=""server"" visible=""false"" /> It fools Visual Studio into thinking you've added a stylesheet to the page but it doesn't get rendered. Here's an even more concise way to do this with multiple references; <% if (false) { %> <link rel=""Stylesheet"" type=""text/css"" href=""Stylesheet.css"" /> <script type=""text/javascript"" src=""js/jquery-1.2.6.js"" /> <% } %> As seen in this blog post from Phil Haack. Thank you so much @Adam for the solution. outside of the fact that it works can you explain what the condition actually means in your second solution? if what is false?... thanks! @blachawk `if (false)` means it will never be executed the code is there purely as a hint to VS. Thanks a lot. It worked for me. `+1` Clever solution!  If you are creating composite UserControl then you can set the CSSClass property on the child controls.. If not then you need to expose properties that are either of the Style type or (as I often do) string properties that apply CSS at the render type (i.e. take them properties and add a style attribute to the HTML tags when rendering). Should the Style link be pointing to http://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.style.aspx ?  Add the style on your usercontrol and import css in it.  <%@ Control Language=""vb"" AutoEventWireup=""false"" CodeBehind=""WCReportCalendar.ascx.vb"" Inherits=""Intra.WCReportCalender"" %> <style type='text/css'> @import url(""path of file.css""); // This is how i used jqueryui css @import url(""http://code.jquery.com/ui/1.10.0/themes/base/jquery-ui.css""); </style> your html"75,A,"PHP mail using Gmail In my PHP webapp I want to be notified via email whenever certain errors occur. I'd like to use my Gmail account for sending these. How could this be done? Swift Mailer while generally considered bloated in comparison to other mailing classes has an approach for this too: http://www.swiftmailer.org/wikidocs/v3/connections/smtp#if_you_need_to_use_authentication  Gmail's SMTP-server requires a very specific configuration. From Gmail help: Outgoing Mail (SMTP) Server (requires TLS) - smtp.gmail.com - Use Authentication: Yes - Use STARTTLS: Yes (some clients call this SSL) - Port: 465 or 587 Account Name: your full email address (including @gmail.com) Email Address: your email address (username@gmail.com) Password: your Gmail password You can probably set these settings up in Pear::Mail or PHPMailer. Check out their documentation for more details.  You could use PEAR's mail function with Gmail's SMTP Server Note that when sending e-mail using Gmail's SMTP server it will look like it came from your Gmail address despite what you value is for $from. (following code taken from About.com Programming Tips ) <?php require_once ""Mail.php""; $from = ""Sandra Sender <sender@example.com>""; $to = ""Ramona Recipient <recipient@example.com>""; $subject = ""Hi!""; $body = ""Hi\n\nHow are you?""; // stick your GMAIL SMTP info here! ------------------------------ $host = ""mail.example.com""; $username = ""smtp_username""; $password = ""smtp_password""; // -------------------------------------------------------------- $headers = array ('From' => $from 'To' => $to 'Subject' => $subject); $smtp = Mail::factory('smtp' array ('host' => $host 'auth' => true 'username' => $username 'password' => $password)); $mail = $smtp->send($to $headers $body); if (PEAR::isError($mail)) { echo(""<p>"" . $mail->getMessage() . ""</p>""); } else { echo(""<p>Message successfully sent!</p>""); } ?>"76,A,"How to put text in the upper right or lower right corner of a ""box"" using css How would I get the here and and here to be on the right on the same lines as the lorem ipsums? See the following: Lorem Ipsum etc........here blah....................... blah blah.................. blah....................... lorem ipsums.......and here Float right the text you want to appear on the right and in the markup make sure that this text and its surrounding span occurs before the text that should be on the left. If it doesn't occur first you may have problems with the floated text appearing on a different line. <html> <body> <div> <span style=""float:right"">here</span>Lorem Ipsum etc<br/> blah<br/> blah blah<br/> blah<br/> <span style=""float:right"">and here</span>lorem ipsums<br/> </div> </body> </html> Note that this works for any line not just the top and bottom corners. But the ""and here"" appears on a new line instead of the same line as the final ""blah"" In which browser? It worked fine for me in the major three as long as you have the floated text first. Absolute positioning will certainly work if you only care about the corners of the box but this allows you to float text on arbitrary lines all the way to the right.  <div style=""position: relative; width: 250px;""> <div style=""position: absolute; top: 0; right: 0; width: 100px; text-align:right;""> here </div> <div style=""position: absolute; bottom: 0; right: 0; width: 100px; text-align:right;""> and here </div> Lorem Ipsum etc <br /> blah <br /> blah blah <br /> blah <br /> lorem ipsums </div> Gets you pretty close may need to tweak the ""top"" and ""bottom"" values.  You need to put ""here"" into a <div> or <span> with style=""float: right"".  You only need to float the div element to the right and give it a margin. Make sure dont use ""absolute"" for this case. #date { margin-right:5px; position:relative; float:right; }  <style> #content { width: 300px; height: 300px; border: 1px solid black; position: relative; } .topright { position: absolute; top: 5px; right: 5px; text-align: right; } .bottomright { position: absolute; bottom: 5px; right: 5px; text-align: right; } </style> <div id=""content""> <div class=""topright"">here</div> <div class=""bottomright"">and here</div> Lorem ipsum etc................ </div>  You may be able to use absolute positioning. The container box should be set to position: relative. The top-right text should be set to position: absolute; top: 0; right: 0. The bottom-right text should be set to position: absolute; bottom: 0; right: 0. You'll need to experiment with padding to stop the main contents of the box from running underneath the absolute positioned elements as they exist outside the normal flow of the text contents.  The first line would consist of 3 <div>s. One outer that contains two inner <div>s. The first inner <div> would have float:left which would make sure it stays to the left the second would have float:right which would stick it to the right. <div style=""width:500;height:50""><br> <div style=""float:left"" >stuff </div><br> <div style=""float:right"" >stuff </div> ... obviously the inline-styling isn't the best idea - but you get the point. 23 and 4 would be single <div>s. 5 would work like 1.  If the position of the element containing the Lorum Ipsum is set absolute you can specify the position via CSS. The ""here"" and ""and here"" elements would need to be contained in a block level element. I'll use markup like this. print(""<div id=""lipsum"">""); print(""<div id=""here"">""); print("" here""); print(""</div>""); print(""<div id=""andhere"">""); print(""and here""); print(""</div>""); print(""blah""); print(""</div>""); Here's the CSS for above.  #lipsum {position:absolute;top:0;left:0;} /* example */ #here {position:absolute;top:0;right:0;} #andhere {position:absolute;bottom:0;right:0;} Again the above only works (reliably) if #lipsum is positioned via absolute. If not you'll need to use the float property.  #here #andhere {float:right;} You'll also need to put your markup in the appropriate place. For better presentation your two divs will probably need some padding and margins so that the text doesn't all run together.  Or even better use HTML elements that fit your need. It's cleaner and produces leaner markup. Example: <dl> <dt>Lorem Ipsum etc <em>here</em></dt> <dd>blah</dd> <dd>blah blah</dd> <dd>blah</dd> <dt>lorem ipsums <em>and here</em></dt> </dl> Float the em to the right (with display: block) or set it to position: absolute with its parent as position: relative."77,A,"How do you redirect HTTPS to HTTP? How do you redirect HTTPS to HTTP?. That is the opposite of what (seemingly) everyone teaches. I have a server on HTTPS for which I paid an SSL certification for and a mirror for which I haven't and keep around for just for emergencies so it doesn't merit getting a certification for. On my client's desktops I have SOME shortcuts which point to http://production_server and https://production_server (both work). However I know that if my production server goes down then DNS forwarding kicks in and those clients which have ""https"" on their shortcut will be staring at https://mirror_server (which doesn't work) and a big fat Internet Explorer 7 red screen of uneasyness for my company. Unfortunately I can't just switch this around at the client level. These users are very computer illiterate: and are very likely to freak out from seeing HTTPS ""insecurity"" errors (especially the way Firefox 3 and Internet Explorer 7 handle it nowadays: FULL STOP kind of thankfully but not helping me here LOL). It's very easy to find Apache solutions for http->https redirection but for the life of me I can't do the opposite. Ideas? This has not been tested but I think this should work using mod_rewrite RewriteEngine On RewriteCond %{HTTPS} on RewriteRule (.*) http://%{HTTP_HOST}%{REQUEST_URI} How do I make it work (what do I have to change from this code to my domain to make this code work)? Enve: Just add to your site's vhost_ssl.conf configuration (or .htaccess at the root of the site). Nothing needs to be changed it will dynamically use the same host name and url path. I think you might also want to catch query strings. I'm not sure but I think the above snippet will not forward query strings from https to http. Although this works it appears wrong per Apache (http://httpd.apache.org/docs/current/rewrite/avoid.html) Looks like you are supposed to use mod alias instead. As pointed by Kieron below this wouldn't work if the mirror server has no valid certificate. You would still see a big red warning because of invalid certificate. Once you start to use https you are basically stuck with it. Be prepared to pay for it throughout the rest of your life. If you stop paying people who bookmarked the https links will not be able to come through.  ejunker's got it right. For all the examples that have you redirect from port 80 substitute port 443 as the conditional.  Based on ejunker's answer this is the solution working for me not on a single server but on a cloud enviroment Options +FollowSymLinks RewriteEngine On RewriteCond %{ENV:HTTPS} on RewriteRule (.*) http://%{HTTP_HOST}%{REQUEST_URI} [R=301L]  Keep in mind that the Rewrite engine only kicks in once the HTTP request has been received - which means you would still need a certificate in order for the client to set up the connection to send the request over! However if the backup machine will appear to have the same hostname (as far as the client is concerned) then there should be no reason you can't use the same certificate as the main production machine.  If none of the above solutions work for you (they did not for me) here is what worked on my server: RewriteCond %{HTTPS} =on RewriteRule ^(.*)$ http://%{HTTP_HOST}/$1 [LR=301] Often times you won't want the `L` (which means ""Last rule""). If you are using wordpress or another CMS the `L` flag may prevent the page request from being properly routed. Instead use: `RewriteRule ^(.*)$ http://%{HTTP_HOST}/$1 [R=301]`  As far as I'm aware of a simple meta refresh also works without causing errors: <meta http-equiv=""refresh"" content=""0;URL='http://www.yourdomain.com/path'""> I wish down voters would be required to leave comments explaining the reasons for down votes. Personally I would not choose this answer unless you as a developer did not have access to the server you were developing for but you did have access to the page. One problem is that you'll have to hardcode every path on every page to get this to work. If you can assume that JavaScript is enabled for your important use cases you'd be better off using JavaScript to change to http. The above answers are better because they do not require javascript since they happen at the server. Simply: because htaccess is far better option than that. Also it doesnt' will fix the problem to redirect the https protocol to http if you doesn't have a certificate. The action should be processed by the server not 'a' document it may serve."78,A,"How do I use my own compiler with Nant? Nant seems very compiler-centric - which is guess is because it's considered a .NET development system. But I know it can be done! I've seen it. The platform we're building on has its own compiler and doesn't use 'cl.exe' for c++. We're building a C++ app on a different platform and would like to override with our own compiler. Can anyone point me at a way to do that or at least how to set up a target of my own that will use our target platform's compiler? Here is one I did for Delphi. Each 'arg' is a separate param with a value defined elsewhere. The target is called with the params set up before calling it. <target name=""build.application""> <exec program=""dcc32"" basedir=""${Delphi.Bin}"" workingdir=""${Application.Folder}"" verbose=""true""> <arg value=""${Application.Compiler.Directive}"" /> <arg value=""-Q"" /> <arg value=""/B"" /> <arg value=""/E${Application.Output.Folder}"" /> <arg value=""/U${Application.Lib.Folder};${Application.Search.Folder}"" /> <arg value=""${Application.Folder}\${Delphi.Project}"" /> </exec> </target> Nice answer. We did that same type of thing for our Borland/CodeGear C++ compiler. We can pretty much make nant run your dish washer with the proper exec task. :)  You need to write your own task. This is a nice reference.  You could also use the <exec> task.  Initially use the <exec> task to run an executable passing in any required information as parameters and/or environment variables. For future use you could also investigate writing your own task. I know with standard ant this is done with the <taskdef> task and a java class. I'm not sure of the Nant equivalent unfortunately."79,A,"WPF - Programmatic Binding on a BitmapEffect I would like to be able to programmatically bind some data to the dependency properties on a BitmapEffect. With a FrameworkElement like TextBlock there is a SetBinding method where you can programmatically do these bindings like: myTextBlock.SetBinding(TextBlock.TextProperty new Binding(""SomeProperty"")); And I know you can do it in straight XAML (as seen below) <TextBlock Width=""Auto"" Text=""Some Content"" x:Name=""MyTextBlock"" TextWrapping=""Wrap"" > <TextBlock.BitmapEffect> <BitmapEffectGroup> <OuterGlowBitmapEffect x:Name=""MyGlow"" GlowColor=""White"" GlowSize=""{Binding Path=MyValue}"" /> </BitmapEffectGroup> </TextBlock.BitmapEffect> </TextBlock> But I can't figure out how to accomplish this with C# because BitmapEffect doesn't have a SetBinding method. I've tried: myTextBlock.SetBinding(OuterGlowBitmapEffect.GlowSize new Binding(""SomeProperty"") { Source = someObject }); But it doesn't work. Thanks in advance. You can use BindingOperation.SetBinding: Binding newBinding = new Binding(); newBinding.ElementName = ""SomeObject""; newBinding.Path = new PropertyPath(SomeObjectType.SomeProperty); BindingOperations.SetBinding(MyGlow OuterGlowBitmapEffect.GlowSizeProperty newBinding); I think that should do what you want."80,A,"Does a YUI Compressor GUI App Exist? I recently discovered Yahoo's YUI Compressor software and started using it on my website. What I'm disappointed about with the software is its lack of a GUI as well as a lack of a way to combine files. Does anyone know of a project which uses the YUI Compressor as a backend that provides a GUI front-end with the ability to combine files? I want to create a GUI application based on the YUI Compressor but if one already exists I'd rather use that. I found this one: http://code.google.com/p/yuicompressorgui/  Check out this one http://smallerapp.com It's the best GUI for YUI Compressor I've ever found for Mac OS X.  I wrote a Window Batch file to call the YUI Compressor from command line automatically. So you can do batch compression without the need to call the javac manually one by one in the command line. http://jeeshenlee.wordpress.com/2010/07/30/batch-process-yui-compressor/  If you are after a reasonable simple implementation try: http://www.refresh-sf.com/yui/.  I've written a series about writing efficient web applications for The View. Since I'm a big fan of the YUI Compressor I had a friend create a GUI for it and create a JAVA application. You can find YUI GUI for download here: http://eview.com/eview/volr6.nsf/852561460065adc3852561130021446a/caef1ae990071cb9852574b9006616fc?OpenDocument Maybe a later release will include GZIP support as well GZIP:ing the results of the YUI Compressor all in one go. One less step you have to do yourself... ;)  I recently open-sourced a PHP based web gui for the YuiCompressor. It does concatenation reporting and mutlifile upload. You can download or fork it from Github here https://github.com/johnhunter/yui-compressor-webapp.  I am using Google Closure-Compiler as an online tool. But it would be great if someone made something like WinLess for Javascript...  use http://refresh-sf.com/yui/............. don't use YUI GUI recommended by Joacim Boive..........it failed  doesn't work at all  Me too I wrote a Windows batch just like you in the beginning. But an idea hit me. So I open my Ecplipse IDE and give YUI Compressor a GUI in Java Swing http://xp-rience.blogspot.com/2010/08/yui-compressor-with-gui.html Tested only with Windows XP with JRE6 installed. Never have time to test it out with other OS. Hope that helps.  I was recently looking for the same thing. After some time I came across this link: http://yuilibrary.com/forum/viewtopic.php?f=94&t=91&p=262 Windows-based GUI app for the YUI compressor self-contained with installer. Interface is pretty nice too. Nice...but doesn't combine files does it? Looks pretty nifty! Thanks for sharing!  I know this thread is old but you can try out 'Minimus' (free) at: http://www.webmaster-source.com/minimus/ Also there is a commercial app called 'Smaller': http://smallerapp.com/ Both apps are for the Mac platform.  Yes someone has created a web-based GUI for YUI compressor.  I don't believe a GUI exists for YUI Compressor...probably because in most projects YUI Compressor is run by an automatic build process and its command line parameters are of the ""figure 'em out once forget about 'em"" nature. In terms of combining files this too would be easy to do with the current YUI Compressor and a command line (as I'm sure you already know) but the projects that I've worked on have opted for combining JS files at runtime before sending down to the client. This allows for greater flexibility of picking and choosing appropriate JS packages for a specific users' current needs (and thus minimizing the amount of JS being sent to the client while trying of course to balance these packages in an appropriate browser caching scheme). Steve Souders' ""High Performance Web Sites"" has great discussion on this topic. That being said a GUI for YUI Compressor certainly sounds like something worth doing if for no other reason than helping expose all of YUI Compressor's power to those users who are only aware of the most basic command line parameters.  You can also take a look at Javascript Obfuscator - it has better compression ratio than yui. worth mentioning is that this is a paid app.  For OS X you should try Minifire - it's small simple and open-source  I made a .bat file to do the compression and placed it in the sendto folder to have it available in the send to contextual menu in win xp. this is the code enjoy:  @eecho off set str=%1 set str=%str:.js=.min.js% set str=%str:.css=.min.css% java -jar ""D:\YUICOM~1\build\yuicompressor-2.4.2.jar"" %1 -o %str%"81,A,What's the most current good practice and easiest way to use sessions in PHP? Sessions in PHP seemed to have changed since the last time I used them so I'm looking for a simple way of using sessions but at the same time for it to be relatively secure and a good common practice. While database might be more secure for sessions you should focus on what you're storing in the session in the first place - it should not really contain anything but an ID to identify the user (and MAYBE a firstname or a temporary variable between pages). I would suggest simply using the default cookies. Database sessions give an extra hit ON EVERY PAGE and even though not every site is slashdot there's no harm in pre-optimizing something as simple as this. For usage I would recommend the standard global variable: $_SESSION['yourvar'] = 'somevalue'; If you use that method in all your code you can easily change the back-end later through the use of session_set_save_handler which gives a unified way of implementing session backends. Note that you can use an object to contain all the session handling simply give arrays to each entry - array('Staticclass' 'staticmethod'). For more in-depth usage I would recommend you take a look at how sessions are handled in KohanaPHP.  Encapsulate the $SESSION array in a Session() Object that allows you to get variables from session get and post with a similar (yet dissociable) way including automatic security filters flash variables (var that are used once then distroyed) and default value setters. Have a look to the behaviour of Symfony on that point its very helpful.  As far as simplicity it doesn't get any better than: # Start the session manager session_start(); # Set a var $_SESSION['foo'] = 'whatever'; # Access the var print $_SESSION['foo'];  You can store PHP sessions in database as described in this book. I have used this method and I find it secure and easy to implement so I would reccomend it.  First off use cookie based only unless you have a very specific good business reason not to. I had a client that insisted on url based sessions only for a project. very insecure and a pain to work with. One good idea is to regenerate the session on each request. this makes hijack much less likely. For example. session_start(); $old_sessionid = session_id(); session_regenerate_id(); $new_sessionid = session_id(); Another thing that is good practice is if you are doing some kind of user login as part of the system completely invalidate and empty the session data on a logout to insure that the user is truly logged out of the system. I have seen systems where logout is just accomplished by removing the session cookie.  Session management changed some time back (I think it was around 4.4). The old mechanism still works but is deprecated. It's rather confusing so I recommend staying clear of it. Today you use sessions by accessing the global variable $_SESSION (It's an array). You can put object instances in there but you need to load the class definitions for those objects before starting the session on the next page. Using autoload can help you out here. You must start a session before you can use $_SESSION. Since starting the session sends headers you can't have any output before. This can be solved in one of two ways: Either you always begin the session at the start of your script. Or you buffer all output and send it out at the end of the script. One good idea is to regenerate the session on each request. this makes hijack much less likely. That's (slightly) bad advice since it can make the site inaccessible. You should regenerate the session-id whenever a users privileges changes though. In general that means whenever they log in. This is to prevent session-fixation (A form of session-hijacking). See this recent thread @ Sitepoint for more on the subject. Using cookiebased sessions only is OK but if you regenerate session id's on login it doesn't add any additional security and it lowers accessibility a bit. I have gone over to that sitepoint forum and taken a look. from what I can see they actually seem to recommend id regeneration as well as using cookie only overall. Look closely. Regenerate session-id's yes but *only* when you need to. That means when the user logs in. not everyone in the thread agrees with that stance82,A,"In SQL whats the difference between count(*) and count('x')? I have the following code: SELECT <column> count(*) FROM <table> GROUP BY <column> HAVING COUNT(*) > 1; Is there any difference to the results or performance if I replace the COUNT(*) with COUNT('x')? (This question is related to a previous one) The major performance difference is that COUNT(*) can be satisfied by examining the primary key on the table. i.e. in the simple case below the query will return immediately without needing to examine any rows. select count(*) from table I'm not sure if the query optimizer in SQL Server will do so but in the example above if the column you are grouping on has an index the server should be able to satisfy the query without hitting the actual table at all. To clarify: this answer refers specifically to SQL Server. I don't know how other DBMS products handle this.  MySQL: According to the MySQL website COUNT(*) is faster for single table queries when using MyISAM: http://dev.mysql.com/doc/refman/5.0/en/group-by-functions.html#function_count I'm guessing with a having clause with a count in it may change things.  This question is slightly different that the other referenced. In the referenced question it was asked what the difference was when using count(*) and count(SomeColumnName) and SQLMenace's answer was spot on. To address this question essentially there is no difference in the result. Both count(*) and count('x') and say count(1) will return the same number. The difference is that when using "" * "" just like in a SELECT all columns are returned then counted. When a constant is used (e.g. 'x' or 1) then a row with one column is returned and then counted. The performance difference would be seen when "" * "" returns many columns. Update: The above statement about performance is probably not quite right as discussed in other answers but does apply to subselect queries when using EXISTS and NOT EXISTS Does that mean COUNT('x') would be faster if the table had many columns compared to COUNT(*)? I think this behavior depends on the database and the query optimization applied. It's an obvious optimization to perform when you see COUNT(*). It can only mean one thing you want the total count of rows regardless of how many columns the table has.  I believe this one has been answered in: http://stackoverflow.com/questions/59294/in-sql-whats-the-difference-between-countcolumn-and-count That's very similar (and may indeed be the same answer) but I wondered if there is a difference between referencing a specific column (i.e. COUNT(column)) compared to referencing an arbitrary string (i.e. COUNT('x')).  To say that SELECT COUNT(*) vs COUNT(1) results in your DBMS returning ""columns"" is pure bunk. That may have been the case long long ago but any self-respecting query optimizer will choose some fast method to count the rows in the table - there is NO performance difference between SELECT COUNT(*) COUNT(1) COUNT('this is a silly conversation') Moreover SELECT(1) vs SELECT(*) will NOT have any difference in INDEX usage -- most DBMS will actually optimize SELECT( n ) into SELECT(*) anyway. See the ASK TOM: Oracle has been optimizing SELECT(n) into SELECT(*) for the better part of a decade if not longer: http://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1156151916789 problem is in count(col) to count() conversion **03/23/00 05:46 pm *** one workaround is to set event 10122 to turn off count(col) ->count() optimization. Another work around is to change the count(col) to count() it means the same when the col has a NOT NULL constraint. The bug number is 1215372. One thing to note - if you are using COUNT(col) (don't!) and col is marked NULL then it will actually have to count the number of occurrences in the table (either via index scan histogram etc. if they exist or a full table scan otherwise). Bottom line: if what you want is the count of rows in a table use COUNT(*) It's not correct to say there's not a difference between select(n) and select(*). If you have a covering index that includes n you get the data straight from the leaf level of the index and don't have to go back to the table which is much faster. The DBMS optimizer *will* realize this and choose the correct index for the job. Provided there is an index rare is the day that I've seen a DBMS actually **count** rows on the table. Moreover the presence of NULLs often cause semantic bugs. When you want the # of rows in a table use COUNT(*)!!! @Matt Just a note on the tone of your answer... If you want to get excited about someone else's apparent ignorance the appropriate place might be the ""comments"" rather than your own answer. Lacing your answer with slights at others is most decidedly ""not helpful"". TI: I disagree: if someone is incorrect and has upvotes I find it unlikely that comments will 1) spur upvoters to change their votes or 2) that potential upvoters will read the comments before voting. The ""comments (n)"" link is too easily overlooked. @Matt I was thinking more of using comments to tell the answerer that their answer was poorly informed so they might fix it rather than to sway other voters. Furthermore there's a reason downvoting isn't as impactful as upvoting: to encourage spotlighting good answers over burying bad ones. @Matt To put it simply if your answer is a good one it will continue to gain upvotes and hence prominence on the page which will in turn push the bad ones out of prominence and pinch off any continuing erroneous upvotes they might have gotten otherwise. Harsh language is completely unnecessary. @TI: I agree that commenting is perfectly suited to nudge answers in the right direction. A wrong upvoted answer that shows no evidence of any investigation ought to be called out. We want this site to be the arbiter of the ""correct"" answer. Wouldn't an upvoted wrong answer be exactly opposite? @Matt Surely it would if it somehow managed to get to the top of the page. But if it's already at the bottom or if there haven't been many answers yet beating the bad answer into submission seems a prematurely strong reaction unlikely to be necessary in the end. @Matt I guess the crux of my point is that the disparity in score between the good answer and the bad is the real indicator of quality. Not whether the score is positive or negative. And I would prefer for myself to use the goodwill of upvotes for good answers to create that disparity. YMMV. @TI: I understand and can see how you'd feel it's ""[unnecessarily] harsh"". I don't fully agree (it's a matter of style I think) but I do agree with your assessment of the situation. I'll avoid pulling that trigger so quickly. Hopefully never! :) Thanks for your feedback. @Matt Glad we could settle amicably. That's really my biggest concern. I may have exaggerated the harshness some. I was hit by two things. One was a strong fear that if we aren't diligent in kindness & support with our criticism this place might become the next slashdot. @Matt The other thing that hit me was a concern over the future relevance of remarks on other people's answers in an environment where those answers can be deleted. @TI: The next /.? THE HORROR!!! :) @TI: Indeed as one of my comments was to a posting that now no longer exists. :( It's unfortunate that users like TI want to turn this into Gym Class where A's are awarded for participation not actual skill. Sadly our generation did this to him. Told him that the answers to tests were less important than how he FELT about them."83,A,"How to attach a ChangeEvent handler to an inherited dependency property? How would you attach a propertychanged callback to a property that is inherited? Like such: class A { DependencyProperty prop; } class B : A { //... prop.AddListener(PropertyChangeCallback); } @MojoFilter Jon's last suggestion link will give you what you're looking for: it uses weak references to register listening to changes by wrapping properties in a new object. Scroll to the bottom of ""PropertyDescriptor AddValueChanged Alternative"". You'll have to change the Binding code around a bit since BindingOperations doesn't exist.  (edited to remove recommendation to use DependencyPropertyDescriptor which is not available in Silverlight) PropertyDescriptor AddValueChanged Alternative None of the proposed solutions can be used at least directly with Silverlight. Removed options that don't work in Silverilght. This one should as verified by @Dimebrain  Have you tried a two way data binding between the two dependency properties? Agreed. Seems to me to be the most straight forward way - is there a reason this isn't being proposed as a higher-ranking solution?"84,A,Can DTS Test for Presence of MS-Access Table I have an Access database in which I drop the table and then create the table afresh. However I need to be able to test for the table in case the table gets dropped but not created (i.e. when someone stops the DTS package just after it starts -roll-eyes- ). If I were doing this in the SQL database I would just do: IF (EXISTS (SELECT * FROM sysobjects WHERE name = 'Table-Name-to-look-for')) BEGIN drop table 'Table-Name-to-look-for' END But how do I do that for an Access database? Optional answer: is there a way to have the DTS package ignore the error and just go to the next step rather than checking to see if it exists? SQL Server 2000 Microsoft Access has a system table called MSysObjects that contains a list of all database objects including tables. Table objects have Type 1 4 and 6. It is important to reference the type: ... Where Name='TableName' And Type In (146) Otherwise what is returned could be a some object other than a table.  Try the same T-SQL but in MS ACCESS the sys objects table is called: MSysObjects. Try this: SELECT * FROM MSysObjects WHERE Name = 'your_table'; and see if it works from there. You can take a look at these tables if you go to Tools -> Options -> View (a tab) -> and check Hidden Objects System Objects. So you can see both. If you open the table you should see your table names queries etc. Do not change this manually or the DB could panic :) Martin. P.D.: Your If Exists should also check of object type: IF EXISTS (SELECT * FROM sysobjects WHERE id = object_id(N'[dbo].[Your_Table_Name]') AND OBJECTPROPERTY(id N'IsUserTable') = 1)  I'm not sure whether you can query the system objects table in an Access database from a DTS package. If that doesn't work why not just try doing a SELECT * from the Access table in question and then catch the error if it fails?85,A,"C# Database Access: DBNull vs null We have our own ORM we use here and provide strongly typed wrappers for all of our db tables. We also allow weakly typed ad-hoc SQL to be executed but these queries still go through the same class for getting values out of a data reader. In tweaking that class to work with Oracle we've come across an interesting question. Is it better to use DBNull.Value or null? Are there any benefits to using DBNull.Value? It seems more ""correct"" to use null since we've separated ourselves from the DB world but there are implications (you can't just blindly ToString() when a value is null for example) so its definitely something we need to make a conscious decision about. Use DBNull. We encouintered some sort of problems when using null. If I recall correctly you cannot INSERT a null value to a field only DBNull. Could be Oracle related only sorry I do not know the details anymore.  I find it better to use null instead of DB null. The reason is because as you said you're separating yourself from the DB world. It is generally good practice to check reference types to ensure they aren't null anyway. You're going to be checking for null for things other than DB data and I find it is best to keep consistency across the system and use null not DBNull. In the long run architecturally I find it to be the better solution.  If you've written your own ORM then I would say just use null since you can use it however you want. I believe DBNull was originally used only to get around the fact that value types (int DateTime etc.) could not be null so rather than return some value like zero or DateTime.Min which would imply a null (bad bad) they created DBNull to indicate this. Maybe there was more to it but I always assumed that was the reason. However now that we have nullable types in C# 3.0 DBNull is no longer necessary. In fact LINQ to SQL just uses null all over the place. No problem at all. Embrace the future... use null. ;-) Interesting. While using an ORM for DB access I don't often explicitly use ExecuteScalar in my code. I usually end up retrieving full entity objects and looking at their values that way. So if the entity itself is null that's one thing and if the value is null that's something else. Just different ways of doing it. I admit that ExecuteScalar is more efficient if you just need one single value. I think there are still some uses for DBNull. At least with sqlite I can test if a value returned by ExecuteScalar() is explicitly stored in the database or not. Consider Table foo(a int b int); with a single row (1 null). If we were to ExecuteScalar on a command ""SELECT b FROM foo WHERE A = 1"" it would return DBNull.Value (the row exists a null is stored). If I were to call ExecuteScalar on ""SELECT b FROM foo WHERE A = 2"" it would return null (no such row exists). Presumably you could achieve the same result with a DataReader but I do appreciate the convenience.  From the experience I've had the .NET DataTables and TableAdapters work better with DBNull. It also opens up a few special methods when strongly typed such as DataRow.IsFirstNameNull when in place. I wish I could give you a better technical answer than that but for me the bottom line is use DBNull when working with the database related objects and then use a ""standard"" null when I'm dealing with objects and .NET related code."86,A,"Is there a list of browser conditionals for use including stylesheets? I've seen people doing things like this in their HTML: <!--[if IE]> <link rel=""stylesheet"" href=""ie.css"" type=""text/css"" /> <![endif]--> Does this work across all modern browsers and is there a list of browser types that will work with that kind of if statement? Edit Thanks Ross. Interesting to find out about gt lt gte & lte. Conditional comments are purely for IE (version 5 and later). The official Microsoft documentation is here. If you are going to use them the best strategy is to conditionally include external stylesheets or javascript files after your normal includes. This means that on IE your IE-specific code will override everything else. On any other browser the code will be treated as comments and ignored by the parser.  This works across all browsers because anything except IE sees <!--IGNORED COMMENT-->. Only IE reads the comment if it contains a conditional clause. Have a look at this article You can also specify which version of IE. For example: <!--[if IE 8]> <link rel=""stylesheet type=""text/css"" href=""ie8.css"" /> <![endif]--> To add a bit this is officially supported by MS and is generally considered a developer best practice in cases where you absolutely have to target IE (vs. all the ugly CSS hacks web devs used previously). Here's an MSDN article w/full syntax: http://msdn.microsoft.com/en-us/library/ms537512.aspx  Further to Ross' answer you can only target the Internet Explorer rendering engine with conditional comments; there is no similar construct for other browsers. For example you can't write conditional comments that target Firefox but are ignored by Internet Explorer. The way I achieve the same effect as your example above is to sniff the user agent string. I then deliver a suitable CSS file for that browser. This isn't perfect because sometimes people change their user-agent string for compatibility. The other way to target different browsers is to utilise browser specific hacks. These are particularly nasty because they usually rely on bugs in the browser and bugs are liable to be fixed! User-agent sniffing is the best all-round solution in my opinion.  If you can use Javascript there are several options: navigator.appName navigator.appVersion link Or something more robust by using a library such as jQuery. Finally you could use the BrowserDetect object from QuirksMode. Once you have the browser name and version you can then insert HTML to link to a style sheet or include other tags."87,A,"Best Ruby on Rails social networking framework I'm planning on creating a social networking + MP3 lecture downloading / browsing / commenting / discovery website using Ruby on Rails. Partially for fun and also as a means to learn some Ruby on Rails. I'm looking for a social networking framework that I can use as a basis for my site. I don't want to re-invent the wheel. Searching the web I found three such frameworks. Which of these three would you recommend using and why? http://portal.insoshi.com/ http://www.communityengine.org/ http://lovdbyless.com/ I created and open sourced Brevidy a video social network: https://github.com/iwasrobbed/Brevidy ok so this question was asked about 6 years ago.. and insoshi wasn't [touched](https://github.com/insoshi/insoshi) for like 4 years it doesn't use bundler and it doesn't have a gemfile.. i mean comon.. any more up to date solutions? Keep googling around and finding everything 'Ruby On Rails Social Network' pointing back to here apart from over at https://www.ruby-toolbox.com/categories/social_networking (which is a comprehensive list - though still includes some solutions that are not maintained) I've not worked with these but am aware of this comparison: ""Unlike Insoshi and Lovd By Less which are full social networking Rails applications Community Engine is a plugin that can add social networking features to existing Rails applications"" from http://www.rubyinside.com/community-engine-rails-plugin-that-adds-social-networking-to-your-app-901.html Thanks for the link. True that Community Engine is a plug-in and might therefore be better suited to adding into an existing application but I'm starting from scratch. So I could theoretically use any of the three options.  i'm currently testing both lovdbyless and insoshi. i was able to install and get insoshi up and running fairly quickly whereas lovdbyless is giving me a harder time. if you're in novice mode i suggest getting the book from Head First. http://www.headfirstlabs.com/books/hfrails/ it is probably one of the better books out there for beginners. atleast in my opinion because i went through a few that was just way too confusing.  It depends what your priorities are. If you really want to learn RoR do it all from scratch. Seriously. Roll your own. It's the best way to learn far better than hacking through someone else's code. If you do that sometimes you'll be learning Rails but sometimes you'll just be learning that specific social network framework. And you won't know which is which... The type of site you're suggesting sounds perfect for a Rails project. If you get stuck then go browse the repositories of these frameworks. Who cares if you're reinventing the wheel? It's your site your vision your rules. If you just want a site up and running then I would pick Insoshi or LovdbyLess simply because they're out of the box apps so you'll have to do less to do get running. I suggest trying to install them both and introducing yourself in the Google Groups. That'll give you a good indication of wether you're going to get along.  Update: Insoshi's license has changed to the MIT license which means you're basically free to do with it as you please. But still review the license for any code you are considering before you get too invested in it. Something to keep in mind when deciding is the license for the code. Insoshi is licensed under the GNU Affero General Public License http://insoshi.com/license. This means that you have to distribute the source code to your Insoshi-based web application to anyone who uses that web application. You might not want to do that in which case you'll need to pay Insoshi a license fee (they dual license like MySQL). LovdByLess is distributed under an MIT license http://github.com/stevenbristol/lovd-by-less/tree/master/LICENSE. This means you can use the source code however you want to. @EskoLuontola anything more up to date than insoshi folks? insoshi hasn't been [touched](https://github.com/insoshi/insoshi) for 4 years Insoshi appears to use the MIT license since 2009: https://github.com/insoshi/insoshi/blob/master/LICENSE  Just a quick update EngineY now supports Rails 2.3.5 and just released this weekend is support for themes. This goes along with existing features including groups blogs photos REST API status updates Facebook Connect forums private messages user profiles activity feeds wall posts and more... Check it out at http://www.enginey.com or on GitHub at http://github.com/timothyf/enginey  Another option for anyone who wants to create a social site without having to build it from scratch is the EngineY framework. EngineY is a social networking framework written in Ruby and Rails. It provides alot of popular social networking features such as activity streams groups photos message boards status updates events blogs wall posts integrated twitter feeds and more. EngineY is also under active development with new features being added all the time. You can read more about EngineY and download it from: http://www.enginey.com  One other positive to Community Engine is that it is using Engines which is an advanced type of plugin that is becoming a part of rails in 2.3. So what you learn from using Community Engine (and therefore Engines) will be useful going forward.  Use Rails 3 and roll your own. Don't copy and paste code though look through the source and try to understand the reasoning or motive behind certain design decisions only then will you learn.  Regarding RailsSpace that's a very nicely built Rails 1.2 application and I think it was updated for compatibility with Rails 2.x. There's even a terrific book that was written about the RailsSpace application (or rather RailsSpace and the book were written together). But RailsSpace became Insoshi when the authors were so inspired by the amount of interest in a social networking site built in Rails. So while RailsSpace might be an interesting learning exercise it's dead in terms of development. All of the authors' efforts (for more than a year now I think) have been going into Insoshi instead so that's where you should be looking."88,A,"Django Calendar Widget? Does anyone know of any existing packages or libraries that can be used to build a calendar in a django app? Calendaring is a vast field. What features do you need? Event display? Export in standard formats? Appointment scheduling? Resource scheduling? Etc. svn checkout http://django-calendar.googlecode.com/svn/trunk/ django-calendar-read-only svn: URL 'http://django-calendar.googlecode.com/svn/trunk' doesn't exist so google search may reveal but it's no longer exists. The project can only be downloaded as a tarball from the project page. In other words they aren't using google code for SVN.  Today I ran into django-swingtime. Worth checking out.  It seems that django-calendar has become django-agenda: http://github.com/dokterbob/django-agenda There is also a github https://github.com/noisyboiler/django-calendar but I'm not sure which one is which since I never saw the original. I couldn't find any immediate references to a fork from the django-calendar project.  The django-schedule code originally from thauber (thauber/django-schedule) has been forked and worked into the glamkit/glamkit-eventtools code for Galleries Libraries Museums and Archives. It has also been forked and updated by a variety of other folks e.g. boskee/django-schedule and my guess is that that might have fewer dependencies and be easier to integrate into another project. It says: Django-schedule: A calendaring/scheduling application featuring: one-time and recurring events calendar exceptions (occurrences changed or cancelled) occurrences accessible through Event API and Period API relations of events to generic objects ready to use nice user interface view day week month three months and year project sample which can be launched immediately and reused in your project See the github ""network"" tab for a graphical navigation from the point of view of a given branch to see how other branches relate to it (i.e. what is available for merging). @Onlyjus Sorry - I have not. I was just trying to raise the visibility of some of the options in this confusing space. Have you played with any of them?  Great Tipps django-swingtime lives on http://github.com/dakrauth/django-swingtime  A quick google search reveals django-gencal which looks like exactly what you need. It would also be worth looking at the snippets under the calendar tag on Django Snippets at http://www.djangosnippets.org/tags/calendar/. It says _For real documentation see the project's homepage_ but the link to http://code.justinlilly.com/django-gencal/ returns a domain-not-found and justinlilly.com doesn't refer to it. Must have been a temporary glitch http://code.justinlilly.com/django-gencal/ works for me. The link you supply responds with: Your client does not have permission to get URL /p/django-calendar/ from this server. Changed the link to a current project which looks like what the questioner was after.  There are also some calendar functions built into Python itself you can see a simple implementation here. You're referring to HTMLCalendar from Python 2.5. And to clarify the link shows how that can be incorporated into Django. +1.  There is another calendar alternative here Django Event Calendar from 3captus that offers something a bit simpler. I'm trying it out now but it looks like a better fit for me. From the features list: Full feature calendar display using python calendar class Support month scrolling (forward or backward) AJAX add modify delete GUI Require mimimum knowledge of Django should be a good compliment after you are done with django tutorial (http://www.djangoproject.com/documentation/tutorial01/) Calendar and Event class can be used in any python project Full unit test included"89,A,"What is Inversion of Control? Inversion of Control (or IoC) can be quite confusing when it is first encountered. What is it? What problems does it solve? When is it appropriate and when not? The problem with most of these answers is the terminology used. What's a container? Inversion? Dependency? Explain it in layman terms without the big words. See also on Programmers.SE: [Why is Inversion of Control named that way?](http://programmers.stackexchange.com/questions/205681/why-is-inversion-of-control-named-that-way) A very simple written explanation can be found here http://binstock.blogspot.in/2008/01/excellent-explanation-of-dependency.html it says ""Any nontrivial application is made up of two or more classes that collaborate with each other to perform some business logic. Traditionally each object is responsible for obtaining its own references to the objects it collaborates with (its dependencies). When applying DI the objects are given their dependencies at creation time by some external entity that coordinates each object in the system. In other words dependencies are injected into objects.""  IoC / DI to me is pushing out dependencies to the calling objects. Super simple. The non-techy answer is being able to swap out an engine in a car right before you turn it on. If everything hooks up right (the interface) you are good.  Lots of good answers here but I was looking for a simple explanation with code excerpts and graphics. Here you go: http://www.codeproject.com/Articles/380748/Inversion-of-Control-Overview-with-Examples  The Inversion of Control (IoC) and Dependency Injection (DI) patterns are all about removing dependencies from your code. For example say your application has a text editor component and you want to provide spell checking. Your standard code would look something like this: public class TextEditor { private SpellChecker checker; public TextEditor() { checker = new SpellChecker(); } } What we've done here is create a dependency between the TextEditor and the SpellChecker. In an IoC scenario we would instead do something like this: public class TextEditor { private ISpellChecker checker; public TextEditor(ISpellChecker checker) { this.checker = checker; } } Now the client creating the TextEditor class has the control over which SpellChecker implementation to use. We're injecting the TextEditor with the dependency. This is just a simple example there's a good series of articles by Simone Busoli that explains it in greater detail. So this answer only shows a *way* of achieving IoC rather than explaining the concept is that right? @Pangea it's *Rogerio* The original dependency graph looks like (main -> TextEditor -> SpellChecker) but in the second code snippet it looks like (main -> TextEditor main -> SpellChecker). The dependency from TextEditor to SpellChecker has been broken which is useful so I can see that this is an example of dependency injection. However the thread of control still starts with main which invokes code in the texteditor. So I'm unclear why this would an example of 'inversion of control'. The example you gave is not of IOC it is Dependency Injection example. @urini I am confused.. from this line `public TextEditor(ISpellChecker checker)` isn't `TextEditor` again depending upon `ISpellChecker` ?? ISpellChecker is an interface not a defined class. Therefore any class implementing that interface can be 'injected' into TextEditor. Whoever creates TextEditor can determine what implementation of ISpellChecker it uses. And this answer describes Dependency Injection nothing to do with IoC. This is good but I think it's worst extending why it is useful to reduce the dependencies. For example what if you wanted a French Spellchecker and a Spanish Spellchecker. Add to the this to the example. what about 3rd question?( When is it appropriate and when not?) Good clear example. However suppose rather than requiring the ISpellChecker interface be passed to the object's constructor we exposed it as a settable property (or SetSpellChecker method). Would this still constitute IoC? chainguy1337 - yes it would. Using setters like that is called setter injection as opposed to constructor injection (both dependency injection techniques). IoC is a fairly generic pattern but dependency injection acheives IoC Despite the many up-votes this answer is incorrect. Please see http://martinfowler.com/articles/injection.html#InversionOfControl. In particular note the part saying ""Inversion of Control is too generic a term and thus people find it confusing. As a result with a lot of discussion with various IoC advocates we settled on the name Dependency Injection"". I agree with @Rogeria. this doesn't explain why it is called the IoC and I am surprised by the number of up votes ;-) I side with @Rogerio and @Pangea. This may be a good example for [constructor injection](http://www.martinfowler.com/articles/injection.html#ConstructorInjectionWithPicocontainer) but not a good answer to the original question. IoC as defined by [Fowler](http://martinfowler.com/bliki/InversionOfControl.html) can be realised without using injection of any kind e.g. by using a [service locator](http://www.martinfowler.com/articles/injection.html#UsingAServiceLocator) or even simple inheritance. Thank you so much  I browsed for nearly 10 days and was breaking my head to understand the concepts your link took me in the right path. Thanks a lot for help  IoC is also known as dependency injection (DI). It is a process whereby objects define their dependencies that is the other objects they work with only through constructor arguments arguments to a factory method or properties that are set on the object instance after it is constructed or returned from a factory method. The container then injects those dependencies when it creates the bean. This process is fundamentally the inverse hence the name Inversion of Control (IoC) of the bean itself controlling the instantiation or location of its dependencies by using direct construction of classes or a mechanism such as the Service Locator pattern Spring-framework-referance.pfd page 34 http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/pdf/spring-framework-reference.pdf  Inversion of control is a pattern used for decoupling components and layers in the system. The pattern is implemented through injecting dependencies into a component when it is constructed. These dependences are usually provided as interfaces for further decoupling and to support testability. IoC / DI containers such as Castle Windsor Unity are tools (libraries) which can be used for providing IoC. These tools provide extended features above and beyond simple dependency management including lifetime AOP / Interception policy etc. a. Alleviates a component from being responsible for managing it's dependencies. b. Provides the ability to swap dependency implementations in different environments. c. Allows a component be tested through mocking of dependencies. d. Provides a mechanism for sharing resources throughout an application. a. Critical when doing test-driven development. Without IoC it can be difficult to test because the components under test are highly coupled to the rest of the system. b. Critical when developing modular systems. A modular system is a system whose components can be replaced without requiring recompilation. c. Critical if there are many cross-cutting concerns which need to addressed partilarly in an enterprise application. Actually IoC isn't mainly about managing dependencies. Please see http://martinfowler.com/articles/injection.html#InversionOfControl In particular note the part saying ""Inversion of Control is too generic a term and thus people find it confusing. As a result with a lot of discussion with various IoC advocates we settled on the name Dependency Injection"".  Inversion of Controls is about separating concerns. It is nothing new. I remember when I was in University and studying computer science at 1993 I was learning separating concerns to resolve complex software issues by decoupling. Whitout IoC: You have a laptop computer and you accidentally break the screen. And darn you find the same brand laptop screen is not anywhere in the market. So you stuck. With IoC: You have a desktop computer and you accidentally break the screen. You find you can just grap any brand monitor from the market and it just works well with your desktop. desktop successfully implements the IoC in this case. It just accept any type of monitor while the laptop does not it has to require a specific screen to get fixed.  For example task#1 is to create object. Without IOC concept task#1 is supposed to be done by Programmer.But With IOC concept task#1 would be done by container. In short Control gets inverted from Programmer to container. So it is called as inversion of control. I found one good example here. A container is a concept in IoC where the object model including dependencies (relationships between ""user"" object and ""used"" object) and object instances reside and is managed -- e.g. contained. The container is usually provided by a IoC framework such as Spring. Think of it as a runtime repository for the objects that make up your application. pretty nice example What is a container?  So number 1 above. http://stackoverflow.com/questions/3058/what-is-inversion-of-control#99100 Maintenance is the number one thing it solves for me. It guarantees I am using interfaces so that two classes are not intimate with each other. In using a container like Castle Windsor it solves maintenance issues even better. Being able to swap out a component that goes to a database for one that uses file based persistence without changing a line of code is awesome (configuration change you're done). And once you get into generics it gets even better. Imagine having a message publisher that receives records and publishes messages. It doesn't care what it publishes but it needs a mapper to take something from a record to a message. public class MessagePublisher<RECORDMESSAGE> { public MessagePublisher(IMapper<RECORDMESSAGE> mapperIRemoteEndpoint endPointToSendTo) { //setup } } I wrote it once but now I can inject many types into this set of code if I publish different types of messages. I can also write mappers that take a record of the same type and map them to different messages. Using DI with Generics has given me the ability to write very little code to accomplish many tasks. Oh yeah there are testability concerns but they are secondary to the benefits of IoC/DI. I am definitely loving IoC/DI. 3 . It becomes more appropriate the minute you have a medium sized project of somewhat more complexity. I would say it becomes appropriate the minute you start feeling pain. http://ferventcoder.com/archive/2008/12/14/the-real-reason-to-use-a-dependency-injection-container-like.aspx  Wikipedia Article. To me inversion of control is turning your sequentially written code and turning it into an delegation structure. Instead of your program explicitly controlling everything your program sets up a class or library with certain functions to be called when certain things happen. It solves code duplication. For example in the old days you would manually write your own event loop polling the system libraries for new events. Nowadays most modern APIs you simply tell the system libraries what events you're interested in and it will let you know when they happen. Inversion of control is a practical way to reduce code duplication and if you find yourself copying an entire method and only changing a small piece of the code you can consider tackling it with inversion of control. Inversion of control is made easy in many languages through the concept of delegates interfaces or even raw function pointers. It is not appropriate to use in all cases because the flow of a program can be harder to follow when written this way. It's a useful way to design methods when writing a library that will be reused but it should be used sparingly in the core of your own program unless it really solves a code duplication problem. I find that Wikipedia article very confusing and in need of fixing up. Check out the discussion page for a laugh.  Before using Inversion of Control you should be well aware of the fact that it has its pros and cons and you should know why you use it if you do so. Pros: Your code gets decoupled so you can easily exchange implementations of an interface with alternative implementations It is a strong motivator for coding against interfaces instead of implementations It's very easy to write unit tests for your code because it depends on nothing else than the objects it accepts in its constructor/setters and you can easily initialize them with the right objects in isolation. Cons: IoC not only inverts the control flow in your program it also clouds it considerably. This means you can no longer just read your code and jump from one place to another because the connection between your code is not in the code anymore. Instead it is in XML configuration files or annotations and the in the code of your IoC container that interprets these metadata. There arises a new class of bugs where you get your XML config or your annotations wrong and you can spend a lot of time finding out why your IoC container injects a null reference into one of your objects under certain conditions. Personally I see the strong points of IoC and I really like them but I tend to avoid IoC whenever possible because it turns your software into a collection of classes that no longer constitute a ""real"" program but just something that needs to be put together by XML configuration or annotation metadata and would fall (and falls) apart without it. That is where Resharper helps - ""click go to implementation"" against the interface. Avoiding IoC (or more specifically DI from your example) probably also means you aren't testing properly Re: _it turns your software into a collection of classes that no longer constitute a ""real"" program but just something that needs to be put together by XML configuration or annotation metadata and would fall (and falls) apart without it_ -- I think this is very misleading. The same could be said of any program that is written on top of a framework. The difference with a good IoC container is that you should be able to if your program is well designed & written take it out and plop in another one with minimal changes to your code or toss out IoC altogether and construct your objects by hand. The first con is incorrect. Ideally there should only be 1 use of IOC container in your code and that is your main method. Everything else should cascade down from there I think what he means is you can't just read: myService.DoSomething() and go to the definition of DoSomething because with IoC myService is just an interface and the actual implementation is unknown to you unless you go look it up in xml config files or the main method where your ioc gets setup.  I agree with NilObject but I'd like to add to this: if you find yourself copying an entire method and only changing a small piece of the code you can consider tackling it with inversion of control If you find yourself copying and pasting code around you're almost always doing something wrong. Codified as the design principle Once and Only Once.  What is Inversion of Control? If you follow these simple two steps you have done inversion of control: Separate what-to-do part from when-to-do part. Ensure that when part knows as little as possible about what part; and vice versa. There are several techniques possible for each of these steps based on the technology/language you are using for your implementation. -- The inversion part of the Inversion of Control (IoC) is the confusing thing; because inversion is the relative term. The best way to understand IoC is to forget about that word! -- Examples Event Handling. Event Handlers (what-to-do part) -- Raising Events (when-to-do part) Interfaces. Component client (when-to-do part) -- Component Interface implementation (what-to-do part) xUnit fixure. Setup and TearDown (what-to-do part) -- xUnit frameworks calls to Setup at the beginning and TearDown at the end (when-to-do part) Template method design pattern. template method when-to-do part -- primitive subclass implementation what-to-do part DLL container methods in COM. DllMain DllCanUnload etc (what-to-do part) -- COM/OS (when-to-do part) +1 for this: *The inversion part of the Inversion of Control (IoC) is the confusing thing; because inversion is the relative term. The best way to understand IoC is to forget about that word!* I love your Examples ! especially Events example... <3 this is a fantastic answer; rather than just giving an example it explains the actual concepts. Thank you! I agree. Nice explanation in simple words. This is a far far better answer than the originally accepted answer.  Let to say that we make some meeting in some hotel. Many people many carafes of water many plastic cups. When somebody want to drink she fill cup drink and throw cup on the floor. After hour or something we have a floor covered of plastic cups and water. Let invert control. The same meeting in the same place but instead of plastic cups we have a waiter with one glass cup (Singleton) and she all of time offers to guests drinking. When somebody want to drink she get from waiter glass drink and return it back to waiter. Leaving aside the question of the hygienic last form of drinking process control is much more effective and economic. And this is exactly what the Spring (another IoC container for example: Guice) does. Instead of let to application create what it need using new keyword (taking plastic cup) Spring IoC container all of time offer to application the same instance (singleton) of needed object(glass of water). Think about yourself as organizer of such meeting. You need the way to message to hotel administration that meeting members will need glass of water but not piece of cake. Example:- public class MeetingMember { private GlassOfWater glassOfWater; ... public void setGlassOfWater(GlassOfWater glassOfWater){ this.glassOfWater = glassOfWater; } //your glassOfWater object initialized and ready to use... //spring IoC called setGlassOfWater method itself in order to //offer to meetingMember glassOfWater instance } Useful links:- http://adfjsf.blogspot.in/2008/05/inversion-of-control.html http://martinfowler.com/articles/injection.html http://www.shawn-barrett.com/blog/post/Tip-of-the-day-e28093-Inversion-Of-Control.aspx  Inversion of Control is what you get when your program callbacks e.g. like a gui program. For example in an old school menu you might have: print ""enter your name"" read name print ""enter your address"" read address etc... store in database thereby controlling the flow of user interaction. In a GUI program or somesuch instead we say when the user types in field a store it in NAME when the user types in field b store it in ADDRESS when the user clicks the save button call StoreInDatabase So now control is inverted... instead of the computer accepting user input in a fixed order the user controls the order in which the data is entered and when the data is saved in the database. Basically anything with an event loop callbacks or execute triggers falls into this category. I now get it why it is sometimes facetiously referred to as the ""Hollywood Principle: Don't call us we'll call you"" +1 as per me this should accepted answer for the question here. Thanks Mark. dont mark this guy down. technically he is correct http://martinfowler.com/bliki/InversionOfControl.html IoC is a very general principal. Flow of control is ""inverted"" by dependency injection because you have effectively delegated dependancies to some external system (e.g. IoC container) Agreed with Schneider's comment. 5 downvotes? The mind boggles since this is the only answer that's really correct. Note the opening: '**like** a gui program.' Dependency injection is only the most commonly-seen realization of IoC. Indeed this is one of the few *correct* anwsers! Guys IoC is *not* fundamentally about dependencies. Not at all. This is confusing. -5 votes?? People who have downvoted this answer should have specified the reason for doing so as well. +1 - This is a good description (with example) of the following statement by Martin Fowler - ""Early user interfaces were controlled by the application program. You would have a sequence of commands like ""Enter name"" ""enter address""; your program would drive the prompts and pick up a response to each one. With graphical (or even screen based) UIs the UI framework would contain this main loop and your program instead provided event handlers for the various fields on the screen. The main control of the program was inverted moved away from you to the framework.""  When you go to a restaurant and ready to order food. Without IoC: You have to order a specific food in advance and every time you ask more you get same food. So for example you can ask for apple and you are always served apple when you ask more food. Darn isn't it boring? With IoC: You have to order food in advance but instead of any specific food you can just ask for fruit. Then when you get served each time you can get different fruit for example apple orange pear peach and water melon. Don't be afraid to add your favorite food to the list it will be served. So obviously IoC is preferred when you like the varieties.  It seems that the most confusing thing about ""IoC"" the acronym and the name for which it stands is that it's too glamorous of a name - almost a noise name. Do we really need a name by which to describe the difference between procedural and event driven programming? OK if we need to but do we need to pick a brand new ""bigger than life"" name that confuses more than it solves? IoC != event driven. Similarities (and in some cases overlap) but they are not principally the same paradigm. Good question. Event driven programming IS certainly IoC. We write event handlers and they get called from event loop. But IoC is more generic concept than Event driven programming.. If you override a method in subclass it is also a kind of IoC. You write a code that would get invoked when appropriate reference (instance) was used.  Inversion of Control is about getting freedom more flexibility and less dependency. When you are using a desktop computer you are slaved (or say controlled). You have to sit before a screen and look at it. Using keyboard to type and using mouse to navigate. And a bad written software can slave you even more. If you replaced your desktop with a laptop then you somewhat inverted control. You can easily take it and move around. So now you can control where you are with your computer instead of computer controlling it. By implementing Inversion of Control a software/object consumer get more controls/options over the software/objects instead of being controlled or having less options. i like how your metaphors contradict each other  But I think you have to be very careful with it. If you will overuse this pattern you will make very complicated design and even more complicated code. Like in this example with TextEditor: if you have only one SpellChecker maybe it is not really necessary to use IoC ? Unless you need to write unit tests or something ... Anyway: be reasonable. Design pattern are good practices but not Bible to be preached. Do not stick it everywhere."90,A,"What is a magic number and why is it bad? What is a magic number? Why should it be avoided? Are there cases where it's appropriate? A magic number is a direct usage of a number in the code. public class Foo { public void setPassword(String password) { // don't do this if (password.length() > 7) { throw new InvalidArgumentException(""password""); } } } This should be refactored to: public class Foo { public static final int MAX_PASSWORD_SIZE = 7; public void setPassword(String password) { if (password.length() > MAX_PASSWORD_SIZE) { throw new InvalidArgumentException(""password""); } } } It improves readability of the code and it's easier to maintain. Imagine the case where I set the size of the password field in the GUI. If I use a magic number whenever the max size changes I have to change in two code locations. If I forget one this will lead to inconsistencies. The JDK is full of examples like in Integer Character and Math classes. PS.: Static analysis tools like FindBugs and PMD detects the use of magic numbers in your code and suggests the refactoring. @paulwhit checkstyle can do the same in java. @JonathanParker Disagree depends on the context. other good natural constants include ONE_MINUTE = 60 ONE_HOUR = 3600 ONE_DAY = 86400 (most programmers will be working with epoch times)... or @JonathanParker what do you mean by ""0 and 1 are exceptions to this rule"" ? I'd say that that code is LESS readable why would you want to have to go hunting for the definition of MAX_PASSWORD_SIZE when it won't change? you're literally just making it more spaghettish. @BrendanLong It doesn't always have to be `TRUE` / `FALSE` as exception. Example: `if(array.length == 0)` (or `< 1`) In the .NET world FxCop and the Visual Studio 2008 code metrics should point these out for you. So are they bad and if so why? 0 and 1 are exceptions to this rule. Should I write: `public static final HUNDRED_PERCENTS = 100;` ? @Kirill: If you expect the definition of ""Hundred Percents"" to change then yes. A better approach would be to have the variable from what it is to what it represents i.e. public static final MAX_DOWNLOAD_PERCENTAGE = 100. Although even that wouldn't make sense because ""100 percent"" is very well defined. On the other hand the fact that Passwords can be a maximum of 7 characters long is not globally defined and actually differs so that is a candidate for a variable. @Jonathan Parker except when they're not (`TRUE`/`FALSE`) Just because a magic number will never change doesn't mean it shouldn't be replaced by a constant. My code is full of global constants like HzPerMHz and msecPerSecond. These will never change but they make the meaning clearer and provide some protection against typos.  A Magic Number is a hard-coded value that may change at a later stage but that can be therefore hard to update. For example let's say you have a Page that displays the last 50 Orders in a ""Your Orders"" Overview Page. 50 is the Magic Number here because it's not set through standard or convention it's a number that you made up for reasons outlined in the spec. Now what you do is you have the 50 in different places - your SQL script (SELECT TOP 50 * FROM orders) your Website (Your Last 50 Orders) your order login (for (i = 0; i < 50; i++)) and possibly many other places. Now what happens when someone decides to change 50 to 25? or 75? or 153? You now have to replace the 50 in all the places and you are very likely to miss it. Find/Replace may not work because 50 may be used for other things and blindly replacing 50 with 25 can have some other bad side effects (i.e. your Session.Timeout = 50 call which is also set to 25 and users start reporting too frequent timeouts). Also the code can be hard to understand i.e. ""if a < 50 then bla"" - if you encounter that in the middle of a complicated function other developers who are not familiar with the code may ask themselves ""WTF is 50???"" That's why it's best to have such ambiguous and arbitrary numbers in exactly 1 place - ""const int NumOrdersToDisplay = 50"" because that makes the code more readable (""if a < NumOrdersToDisplay"" it also means you only need to change it in 1 well defined place. Places where Magic Numbers are appropriate is everything that is defined through a standard i.e. SmtpClient.DefaultPort = 25 or TCPPacketSize = whatever (not sure if that is standardized). Also everything only defined within 1 function might be acceptable but that depends on Context. Even if it can't change it's still a bad idea because it's not clear what's going on.  A magic number can also be a number with special hardcoded semantics. For example I once saw a system where record IDs > 0 were treated normally 0 itself was ""new record"" -1 was ""this is the root"" and -99 was ""this was created in the root"". 0 and -99 would cause the WebService to supply a new ID. What's bad about this is that you're reusing a space (that of signed integers for record IDs) for special abilities. Maybe you'll never want to create a record with ID 0 or with a negative ID but even if not every person who looks either at the code or at the database might stumble on this and be confused at first. It goes without saying those special values weren't well-documented. Arguably 22 7 -12 and 620 count as magic numbers too. ;-)  It is worth noting that sometimes you do want non-configurable ""hard-coded"" numbers in your code. There are a number of famous ones including 0x5F3759DF which is used in the optimized inverse square root algorithm. In the rare cases where I find the need to use such Magic Numbers I set them as a const in my code and document why they are used how they work and where they came from. In my opinion the magic number code smell refers specifically to *unexplained* constants. As long as you're putting them in a named constant it shouldn't be a problem.  What about return variables? I specially find it challenging when implementing stored procedures. Imagine the next stored procedure (wrong syntax I know just to show an example): int procGetIdCompanyByName(string companyName); It return the Id of the company if it exists in a particular table. Otherwise it returns -1. Somehow it's a magic number. Some of the recommendations I've read so far says that I'll really have to do design somthing like that: int procGetIdCompanyByName(string companyName bool existsCompany); By the way what should it return if the company does not exists? Ok: it will set existesCompany as false but also will return -1. Antoher option is to make two separate functions: bool procCompanyExists(string companyName); int procGetIdCompanyByName(string companyName); So a pre-condition for the second stored procedure is that company exists. But i'm afraid of concurrency because in this system a company can be created by another user. The bottom line by the way is: what do you think about using that kind of ""magic numbers"" that are relatively known and safe to tell that something is unsuccessful or that something does not exists? Thank you.  A problem that has not been mentioned with using magic numbers... If you have very many of them the odds are reasonably good that you have two different purposes that you're using magic numbers for where the values happen to be the same. And then sure enough you need to change the value... for only one purpose. This doesn't look all that probable when talking about numbers (at least not to me) but I ran into it with strings and it is a hit: first you have to read a lot of code to see where its used than you have to notice that it's being used for different things...not my favourite pastime.  I've always used the term ""magic number"" differently as an obscure value stored within a data structure which can be verified as a quick validity check. For example gzip files contain 0x1f8b08 as their first three bytes Java class files start with 0xcafebabe etc. You often see magic numbers embedded in file formats because files can be sent around rather promiscuously and lose any metadata about how they were created. However magic numbers are also sometimes used for in-memory data structures like ioctl() calls. A quick check of the magic number before processing the file or data structure allows one to signal errors early rather than schlep all the way through potentially lengthy processing in order to announce that the input was complete balderdash.  What about initializing a variable at the top of the class with a default value? For example: public class SomeClass { private int maxRows = 15000; ... // Inside another method for (int i = 0; i < maxRows; i++) { // Do something } public void setMaxRows(int maxRows) { this.maxRows = maxRows; } public int getMaxRows() { return this.maxRows; } In this case 15000 is a magic number (according to CheckStyles). To me setting a default value is okay. I don't want to have to do: private static final int DEFAULT_MAX_ROWS = 15000; private int maxRows = DEFAULT_MAX_ROWS; Does that make it more difficult to read? I never considered this until I installed CheckStyles. I think `static final` constants are overkill when you're using them in one method. A `final` variable declared at the top of the method is more readable IMHO. I think this would be okay if the constructor initializes the value. Otherwise if the value is initialized outside of the constructor I just see it as a hassle and as something harder to read.  @eed3si9n: I'd even suggest that '1' is a magic number. :-) A principle that's related to magic numbers is that every fact your code deals with should be declared exactly once. If you use magic numbers in your code (such as the password length example that @marcio gave you can easily end up duplicating that fact and when your understand of that fact changes you've got a maintenance problem. IOW code should be written like this: `factorial n = if n == BASE_CASE then BASE_VALUE else n * factorial (n - RECURSION_INPUT_CHANGE); RECURSION_INPUT_CHANGE = 1; BASE_CASE = 0; BASE_VALUE = 1`  I assume this is a response to my answer to your earlier question. In programming a magic number is an embedded numerical constant that appears without explanation. If it appears in two distinct locations it can lead to circumstances where one instance is changed and not another. For both these reasons it's important to isolate and define the numerical constants outside the places where they're used.  In programming a ""magic number"" is a value that should be given a symbolic name but was instead slipped into the code as a literal usually in more than one place. It's bad for the same reason SPOT (Single Point of Truth) is good: If you wanted to change this constant later you would have to hunt through your code to find every instance. It is also bad because it might not be clear to other programmers what this number represents hence the ""magic"". People sometimes take magic number elimination further by moving these constants into separate files to act as configuration. This is sometimes helpful but can also create more complexity than it's worth. Can you be more specific on why eliminating maginc numbers ISN'T alway good? In math formulas like e^pi + 1 = 0 I think he meant it isn't always good to move magic numbers to config files which I agree. Marcio: When you do things like ""const int EIGHT = 8;"" and then requirements change and you end up with ""const int EIGHT = 9;"" Sorry but that's simply an example of bad naming or a base usage for the constant.  It seems nobody mentioned a REALLY GOOD use of magic numbers... I have found that there is 1 case where I PREFER magic numbers... and that is unit tests. In unit tests you often want to construct and use your objects with data and assert how that data was used and abused. When you write your tests with ""magic numbers"" rather than factored constants I feel it actually improves the quality of the tests... it often makes the tests a bit more clearer what they are doing but more importantly makes the tests even more isolated. With unit tests you want each one to be isolated and independent of all the others. It should fail for exactly 1 reason which does not depend on the usage or results of other tests. Thus if you have magic numbers you are implicitly tying all your tests to those magic numbers... unless you are defining constants like ZERO and ONE and TWO which is less readable than just 0 1 and 2. In a test I want to be able to see everything related to the test right there in the test itself magic numbers and all. This may be a subjective test style of mine by the way. In general though magic numbers tend to be bad for all the reasons others have described. Best answer - Logical reason. A useful to have the best of both worlds in a unit test is to define local number variables with a more descriptive name to show why exactly you chose that number. Avoids all the constant making while helping to make a story out of the code. In some unit tests this might make things more difficult especially when other programmers use the code. For instance I'm currently debugging some failing unit tests where a hardcoded value was asserted (e.g. `assertEquals(27 myArray.size())`. Unfortunately there is no context for why the number 27 was chosen in the first place. Granted this might be okay if the code is properly documented but having magic numbers like 27 can make things difficult for others if not sufficiently explained. @Thunderforge I agree completely. Mike I agree it is good practice to NOT use constants defined in the code you are testing from your tests. However I have found that NOT using constants defined WITHIN test classes to cause repeated head scratching. Not always of course but often enough and hard enough to be very cautious about allowing nonchalant use of magic values even in unit tests. Very good remark!  A magic number is a sequence of characters at the start of a file format or protocol exchange. This number serves as a sanity check. Example: Open up any GIF file you will see at the very start: GIF89. ""GIF89"" being the magic number. Other programs can read the first few characters of a file and properly identify GIFs. The danger is that random binary data can contain these same characters. But it is very unlikely. As for protocol exchange you can use it to quickly identify that the current 'message' that is being passed to you is corrupted or not valid. Magic numbers are still useful. I don't think that's the magic number he was refering to Maybe you should remove the ""file-format"" and ""networking"" tags you added because he's clearly not talking about those kinds of magic numbers. It's still very useful to know that magic numbers may refer to more than simply a code issue. -Adam If the subject read: ""What is a magic number in terms of source code"" then the tags shouldn't be there. But he did not specify this. So having my extra information is good. I think Kyle Landon and Marcio are wrong. There was also no way to determine which one he was looking for. Since I was the first post I couldn't guess which one he was looking for.  Have you taken a look at the wikipedia entry for magic number? It goes into a bit of detail about all of the ways the magic number reference is made. Here's a quote about magic number as a bad programming practice The term magic number also refers to the bad programming practice of using numbers directly in source code without explanation. In most cases this makes programs harder to read understand and maintain. Although most guides make an exception for the numbers zero and one it is a good idea to define all other numbers in code as named constants. Good example of RTFW :)"91,A,"How to make embedded servlet engine instantiate servlets eagerly? The problem is simple but I'm struggling a bit already. Server server = new Server(8080); Context context = new Context(server ""/"" Context.NO_SESSIONS); context.addServlet(MainPageView.class ""/""); context.addServlet(UserView.class ""/signup""); server.start(); That's a pretty standard piece of code that you can find anywhere in Jetty world. I have an application that embeds Jetty as a servlet engine and has some servlets. Instantiation of some of these servlets requires heavy work on startup. Say  reading additional config files connecting to the database etc. How can I make the servlet engine instantiate all servlets eagerly so that I can do all the hard work upfront and not on the first user request? I'm not sure why using Guice make's Justin's option not work for you. What exactly is getting injected in? I'm not sure if this would help you at all because it is very similar to what Justin wrote above but if you do it this way Jetty will do the actually instantiating. Context context = new Context(server ""/"" Context.NO_SESSIONS); ServletHolder mainPageViewHolder = new ServletHolder(MainPageView.class); // Do this to force Jetty to instantiate the servlet mainPageViewHolder.getServlet(); context.addServlet(mainPageViewHolder ""/""); Private fields of the servlet are injected. This answer is better but I still kinda hoped for some unknown missing flag that I can set and jety will instantiate everything eagerly. Anyway I think I'll have to live with that.  Use the Context.addServlet overload that takes a ServletHolder. ServletHolder is a class that accepts either a Class or a Servlet instance. Servlet myServlet = new MyServlet(); ServletHolder holder = new ServletHolder(myServlet); context.addServlet(holder ""/""); This assumes Jetty 6. I think it will work for Jetty 7 as well. Justin that'd do the trick. But for a few reasons (i.e. hidden Guice-based dependency injection) I need Jetty to eagerly instantiate the servlets for me."92,A,Roaming settings with LocalFileSettingsProvider On my way through whipping up a Windows Forms application I thought it might have been a good idea to use the settings file to store miscellaneous application options (instead of the registry) and user parameters (window positions column orderings etc.). Out of the box quick and simple or so I thought. All works as per MSDN using the default SettingsProvider (LocalFileSettingsProvider) but I do have concerns about where it gets stored and hopefully somebody can provide a solution. Essentially the file ends up in the local application data and in an unsavoury sub-directory structure. (AppData / Local / company / *namespace_StrongName_gibberish* / version ). Is there a way to tell the LocalFileSettingsProvider to store the configuration file so the data will roam and perhaps in a less crazy folder structure? (or maybe an implementation of SettingsProvider that already does this?) You pretty much have to implement your own. This is a good starting point however. I was hoping that wasnt going to be the answer thanks.  You can use SettingsManageabilityAttribute for storing settings in roaming directory: [SettingsManageability(SettingsManageability.Roaming)] Nice I didn't know about that!  see http://blogs.msdn.com/rprabhu/articles/433979.aspx for some good info about the settings topic93,A,"Deserializing Client-Side AJAX JSON Dates Given the following JSON Date representation: ""\/Date(1221644506800-0700)\/"" How do you deserialize this into it's JavaScript Date-type form? I've tried using MS AJAX JavaScrioptSerializer as shown below: Sys.Serialization.JavaScriptSerializer.deserialize(""\/Date(1221644506800-0700)\/"") However all I get back is the literal string date. Are you using jQuery maybe? Check my blog post to auto convert dates so you don't have to do it manually. http://erraticdev.blogspot.com/2010/12/converting-dates-in-json-strings-using.html See my comments below. Your blog's code fails on dates before the epoch. The regular expression used in the ASP.net AJAX deserialize method looks for a string that looks like ""\/Date(1234)\/"" (The string itself actually needs to contain the quotes and slashes). To get such a string you will need to escape the quote and back slash characters so the javascript code to create the string looks like ""\""\\/Date(1234)\\/\"""". This will work. Sys.Serialization.JavaScriptSerializer.deserialize(""\""\\/Date(1221644506800)\\/\"""") It's kind of weird but I found I had to serialize a date then serialize the string returned from that then deserialize on the client side once. Something like this. Script.Serialization.JavaScriptSerializer jss = new Script.Serialization.JavaScriptSerializer(); string script = string.Format(""alert(Sys.Serialization.JavaScriptSerializer.deserialize({0}));"" jss.Serialize(jss.Serialize(DateTime.Now))); Page.ClientScript.RegisterStartupScript(this.GetType() ""ClientScript"" script true);  For those who don't want to use Microsoft Ajax simply add a prototype function to the string class. E.g.  String.prototype.dateFromJSON = function () { return eval(this.replace(/\/Date\((\d+)\)\//gi ""new Date($1)"")); }; Don't want to use eval? Try something simple like var date = new Date(parseInt(jsonDate.substr(6))); As a side note I used to think Microsoft was misleading by using this format. However the JSON specification is not very clear when it comes to defining a way to describe dates in JSON. That regex won't work for dates before the epoch which have a negative value. You need this: `/\/Date\((-?\d+)(?:-\d+)?\)\//i`  Provided you know the string is definitely a date I prefer to do this :  new Date(parseInt(value.replace(""/Date("" """").replace("")/"""""") 10)) This worked for me my ToJson method was serializing DateTimes as ""\/Date(1251795081950)\/"" Thank you. `var date = new Date(parseInt(jsonDate.substr(6)));` works too.  The big number is the standard JS time new Date(1221644506800) Wed Sep 17 2008 19:41:46 GMT+1000 (EST)  A JSON value is a string number object array true false or null. So this is just a string. There is no official way to represent dates in JSON. This syntax is from the asp.net ajax implementation. Others use the ISO 8601 format. You can parse it like this: var s = ""\/Date(1221644506800-0700)\/""; var m = s.match(/^\/Date\((\d+)([-+]\d\d)(\d\d)\)\/$/); var date = null; if (m) date = new Date(1*m[1] + 3600000*m[2] + 60000*m[3]); +1 for making it clear what a JSON value can be and that dates are not one of them but a custom format. That regex won't work for dates before the epoch which have a negative value. You need this: `/\/Date\((-?\d+)(?:-\d+)?\)\//i` @Sjoerd: Do you have to add the timezone or subtract it to convert it to UTC? I am confused because if I see a time as `1/1/2000 0:0:0 +0500` (i think) I have to subtract 5 hours to get UTC time. This didn't work for me My toJson method was spitting out serialized dates as ""\/Date(1251795070160)\/"" which your code doesn't parse. I'll work out why later just posting here for others  Bertrand LeRoy who worked on ASP.NET Atlas/AJAX described the design of the JavaScriptSerializer DateTime output and revealed the origin of the mysterious leading and trailing forward slashes. He made this recommendation: run a simple search for ""\/Date((\d+))\/"" and replace with ""new Date($1)"" before the eval (but after validation) I implemented that as: var serializedDateTime = ""\/Date(1271389496563)\/""; document.writeln(""Serialized: "" + serializedDateTime + ""<br />""); var toDateRe = new RegExp(""^/Date\\((\\d+)\\)/$""); function toDate(s) { if (!s) { return null; } var constructor = s.replace(toDateRe ""new Date($1)""); if (constructor == s) { throw 'Invalid serialized DateTime value: ""' + s + '""'; } return eval(constructor); } document.writeln(""Deserialized: "" + toDate(serializedDateTime) + ""<br />""); This is very close to the many of the other answers: Use an anchored RegEx as Sjoerd Visscher did -- don't forget the ^ and $. Avoid string.replace and the 'g' or 'i' options on your RegEx. ""\/Date(1271389496563)\/\/Date(1271389496563)\/"" shouldn't work at all. That regex won't work for dates before the epoch which have a negative value. You need something like this: `/\/Date\((-?\d+)(?:-\d+)?\)\//i` A very good answer.... shame i found it at the bottom of the list for the question."94,A,"What's the definitive Java Swing starter guide and reference? Obviously the Java API reference but what else is there that you all use? I've been doing web development my entire career. Lately I've been messing around a lot with Groovy and I've decided to do a small application in Griffon just to experiment more with Groovy and also break some ground in desktop development. The only thing is I'm totally green when it comes to desktop apps. So world where's a good place to start? When it comes to developing java desktop applications I would highly recommend using the IDE environment Netbeans. Especially when it comes to the development of Swing based applications.  The Swing Tutorial is very good. Apart from that the Swing API is obviously the reference however it's also a treasure trove of fairly good source code! Add the API source to your IDE and you can jump directly to the implementation to all the Swing classes. This is a great way to explore the functionality see how various Swing components work and learn a good Swing ""style"". Furthermore it's great to be able to step through the API classes if things don't seem to work and you have no idea why! Adding the API source to the IDE has the additional benefit that you get all the JavaDocs along with it although all modern IDEs can also pull them from the net -- you do not want to program desktop Java without the documentation available from within the IDE! NetBeans and other IDEs do make the creation of IDEs very easy but be aware that there is a lot more to Swing than just containers and layout managers. In fact containers and layout managers are among the easier things and I'd recommend learning to use them by hand too. There is nothing at all wrong with using a GUI builder but in some cases it's overkill and then it's nicer to just quickly whip up a GUI from source. In other cases you need to be able to create a GUI dynamically and then GUI builders are no use at all! For creating very complex layouts from source I recommend FormLayout which has its own set of quirks but which does scale (in terms of programming effort) to very big frames and layouts. If you've only done Groovy so far you'll be surprised how well documented Swing and the rest of the Java API is and how well everything is integrated. It might also take some getting used to a different style of programming using the debugger more often and println-debugging less etc. There might also be some ""boiler-plate"" code that will be very annoying. ;) Enjoy. I've actually been doing a lot of Java development but all with a web front end (mostly Struts) so I'm very familiar with debugging and the usefulness of IDEs. I guess I'm looking for more of a refcard type resource that I can use to help until I become more familiar with the components and widets  I recommend you to play around with netbeans. It will allow you to build complete GUIs using only your mouse. Once you get familiar with Swing components start using the Java API. Thats how I started.  The O'Reilly Swing Book is a pretty good reference it has a good overview of general Swing concepts and covers each of the major classes. I used it recently when I had to refresh my memory on Swing.  The Sun Java tutorials are pretty good. I cannot vouch specifically for the Swing one as it has been ages since I've done any Swing development and I have not read it myself. Creating a GUI with JFC/Swing"95,A,"App does not run with VS 2008 SP1 DLLs previous version works with RTM versions Since our switch from Visual Studio 6 to Visual Studio 2008 we've been using the MFC90.dll and msvc[pr]90.dlls along with the manifest files in a private side-by-side configuration so as to not worry about versions or installing them to the system. Pre-SP1 this was working fine (and still works fine on our developer machines). Now that we've done some testing post-SP1 I've been pulling my hair out since yesterday morning. First off our NSIS installer script pulls the dlls and manifest files from the redist folder. These were no longer correct as the app still links to the RTM version. So I added the define for _BIND_TO_CURRENT_VCLIBS_VERSION=1 to all of our projects so that they will use the SP1 DLLs in the redist folder (or subsequent ones as new service packs come out). It took me hours to find this. I've double checked the generated manifest files in the intermediate files folder from the compilation and they correctly list the 9.0.30729.1 SP1 versions. I've double and triple checked depends on a clean machine: it all links to the local dlls with no errors. Running the app still gets the following error: The application failed to initialize properly (0xc0150002). Click on OK to terminate the application. None of the searches I've done on google or microsoft have come up with anything that relates to my specific issues (but there are hits back to 2005 with this error message). Any one had any similar problem with SP1? Options: Find the problem and fix it so it works as it should (preferred) Install the redist dig out the old RTM dlls and manifest files and remove the #define to use the current ones. (I've got them in an earlier installer build since Microsoft blasts them out of your redist folder!) Edit: I've tried re-building with the define turned off (link to RTM dlls) and that works as long as the RTM dlls are installed in the folder. If the SP1 dlls are dropped in it gets the following error: c:\Program Files\...\...\X.exe This application has failed to start because the application configuration is incorrect. Reinstalling the application may fix this problem. Has no-one else had to deal with this issue? Edit: Just for grins I downloaded and ran the vcredist_x86.exe for VS2008SP1 on my test machine. It works. With the SP1 DLLs. And my RTM linked app. But NOT in a private side-by-side distribution that worked pre-SP1. I just remembered another trick that I used to find out which static libraries were ill-behaving: 'grep' through the static libraries for the string '21022'. HOWEVER don't use the 'normal' grep tools like wingrep because they won't show you these strings (they think it's a binary file and look for the raw non-unicode string). Use the 'strings' utility from the resource kit (now in the Russinovich site I think). That one will grep through binaries ok. So you let this 'strings' go through your whole source tree and you'll see the binary files (dlls and static libraries) that contain references to the wrong manifest (or to the manifest with the wrong version in it).  For your third option you can probably find the DLLs and manifests for the 9.0.21022 version in the C:\WINDOWS\WinSxS directory on your dev machine. If you can then you can setup your own redist directory and install those files with your app. Alternatively you can use the 9.0.30729.1 ones supplied with Visual Studio and forge the manifest you install with your app to report that it supplies the 9.0.21022 DLLs and not 9.0.30729.1. The runtime linker doesn't seem to mind. See this blog which has been immensely helpful for solving these problems for more information. Both workarounds fixed the problems I had with deploying the DLLs as private assemblies with VS2008 Express. Roel's answer is the way to go for your first option (""fix it right"") but if you depend on a library that depends on 9.0.21022 (and your manifest therefore lists both versions) then the third option may be the only way to go if you don't want to run vcredist_x86.exe.  I have battled this problem myself last week and consider myself somewhat of an expert now ;) I'm 99% sure that not all dlls and static libraries were recompiled with the SP1 version. You need to put #define _BIND_TO_CURRENT_MFC_VERSION 1 #define _BIND_TO_CURRENT_CRT_VERSION 1 into every project you're using. For every project of a real-world size it's very easy to forget some small lib that wasn't recompiled. There are more flags that define what versions to bind to; it's documented on http://msdn.microsoft.com/en-us/library/cc664727%28v=vs.90%29.aspx . As an alternative to the lines above you can also put #define _BIND_TO_CURRENT_VCLIBS_VERSION 1 which will bind to the latest version of all VC libs (CRT MFC ATL OpenMP). Then check what the embedded manifest says. Download XM Resource Editor: http://www.wilsonc.demon.co.uk/d10resourceeditor.htm. Open every dll and exe in your solution. Look under 'XP Theme Manifest'. Check that the 'version' attribute on the right-hand side is '9.0.30729.1'. If it's '9.0.21022' some static library is pulling in the manifest for the old version. What I found is that in many cases both versions were included in the manifest. This means that some libraries use the sp1 version and others don't. A great way to debug which libraries don't have the preprocessor directives set: temporarily modify your platform headers so that compilation stops when it tries to embed the old manifest. Open C:\Program Files\Microsoft Visual Studio 9.0\VC\crt\include\crtassem.h. Search for the '21022' string. In that define put something invalid (change 'define' to 'blehbleh' or so). This way when you're compiling a project where the _BIND_TO_CURRENT_CRT_VERSION preprocessor flag is not set your compilation will stop and you'll know that you need to add them or made sure that it's applied everywhere. Also make sure to use Dependency Walker so that you know what dlls are being pulled in. It's easiest to install a fresh Windows XP copy with no updates (only SP2) on a virtual machine. This way you know for sure that there is nothing in the SxS folder that is being used instead of the side-by-side dlls that you supplied. *Raises hands to the sky* Thank you thank you thank you! Not only did that work but I actually understand why. A combination of sxstrace to determine which component was crying about what sxs dependency and these defines to make the manifest point in the right direction woot! Depends never showed what was missing on my clean VM (might need to make sure I've got the latest version!) but using the Resource Editor I found the offending DLL. It was a DLL we compile but that is not part of our project. Thanks for the help. I got the newer version of Dependency Walker and it called out problems immediately with the specific DLLs that were a problem. The flag _BIND_TO_CURRENT_VCLIBS_VERSION also works. Roel's crtassem.h edit technique is great for finding compilation units that don't include stdafx.h that may still be binding the wrong lib versions.  Another nice tool for viewing exe and dll manifests is Manifest View which fittingly enough will not run on a clean install of XP because it depends on 9.0.21022.  To understand the problem I think it is important to realize that there are four version numbers involved: (A) The version of the VC header files to which the .exe is compiled. (B) The version of the manifest file that is embedded in the resources section of that .exe. By default this manifest file is automatically generated by Visual Studio. (C) The version of the VC .DLLs (part of the side-by-side assembly) you copy in the same directory as the .exe. (D) The version of the VC manifest files (part of the side-by-side assembly) you copy in the same directory as the .exe. There are two versions of the VC 2008 DLL's in the running: v1: 9.0.21022.8 v2: 9.0.30729.4148 For clarity I'll use the v1/v2 notation. The following table shows a number of possible situations: Situation | .exe (A) | embedded manifest (B) | VC DLLs (C) | VC manifests (D) ----------------------------------------------------------------------------- 1 | v2 | v1 | v1 | v1 2 | v2 | v1 | v2 | v2 3 | v2 | v1 | v2 | v1 4 | v2 | v2 | v2 | v2 The results of these situations when running the .exe on a clean Vista SP1 installation are: Situation 1: a popup is shown saying: ""The procedure entry point XYZXYZ could not be located in the dynamic link library"". Situation 2: nothing seems to happen when running the .exe but the following event is logged in Windows' ""Event Viewer / Application log"": Activation context generation failed for ""C:\Path\file.exe"".Error in manifest or policy file ""C:\Path\Microsoft.VC90.CRT.MANIFEST"" on line 4. Component identity found in manifest does not match the identity of the component requested. Reference is Microsoft.VC90.CRTprocessorArchitecture=""x86""publicKeyToken=""1fc8b3b9a1e18e3b""type=""win32""version=""9.0.21022.8"". Definition is Microsoft Situation 3: everything seems to work fine. This is remicles2's solution. Situation 4: this is how it should be done. Regrettably as Roel indicates it can be rather hard to implement. Now my situation (and I think it is the same as crashmstr's) is nr 1. The problem is that Visual Studio for one reason or another generates client code (A) for v2 but for one reason or another generates a v1 manifest file (B). I have no idea where version (A) can be configured. Note that this whole explanation is still in the context of private assemblies. Update: finally I start to understand what is going on. Apparently Visual Studio generates client code (A) for v2 by default contrary to what I've read on some Microsoft blogs. The _BIND_TO_CURRENT_VCLIBS_VERSION flag only selects the version in the generated manifest file (B) but this version will be ignored when running the application. Conclusion An .exe that is compiled by Visual Studio 2008 links to the newest versions of the VC90 DLLs by default. You can use the _BIND_TO_CURRENT_VCLIBS_VERSION flag to control which version of the VC90 libraries will be generated in the manifest file. This indeed avoids situation 2 where you get the error message ""manifest does not match the identity of the component requested"". It also explains why situation 3 works fine as even without the _BIND_TO_CURRENT_VCLIBS_VERSION flag the application is linked to the newest versions of the VC DLLs. The situation is even weirder with public side-by-side assemblies where vcredist was run putting the VC 9.0 DLLs in the Windows SxS directory. Even if the .exe's manifest file states that the old versions of the DLLs should be used (this is the case when the _BIND_TO_CURRENT_VCLIBS_VERSION flag is not set) Windows ignores this version number by default! Instead Windows will use a newer version if present on the system except when an ""application configuration file"" is used. Am I the only one who thinks this is confusing? So in summary: For private assemblies use the _BIND_TO_CURRENT_VCLIBS_VERSION flag in the .exe's project and all dependent .lib projects. For public assemblies this is not required as Windows will automatically select the correct version of the .DLLs from the SxS directory. How ironic that this MESS is supposed to solve the DLL hell. Now we have SxS hell which is even worse."96,A,"How do you log errors (Exceptions) in your ASP.NET apps? I'm looking for the best way to log errors in an ASP.NET application. I want to be able to receive emails when errors occurs in my application with detailed information about the Exception and the current Request. In my company we used to have our own ErrorMailer catching everything in the Global.asax Application_Error. It was ""Ok"" but not very flexible nor configurable. We swithed recently to NLog. It's much more configurable we can define different targets for the errors filter them buffer them (not tried yet). It's a very good improvement. But I discovered lately that there's a whole Namespace in the .Net framework for this purpose : System.Web.Management and it can be configured in the healthMonitoring section of web.config. Have you ever worked with .Net health monitoring? What is your solution for error logging? Thanks Vincent I've also seen System.Web.Management but I've never used it. I'd love to hear any feedback on whether it works well. I've been using the Enterprise Library's Logging objects. It allows you to have different types of logging (flat file e-mail and/or database). It's pretty customizable and has a pretty good interface for updating your web.config for the configuration of the logging. Usually I call my logging from the On Error in the Global.asax. Here's a link to the MSDN  I recently built an asp.net webservice with NLog which I use for all my desktop apps. The logging works fine when I'm debugging in Visual Studio but as soon as I switch to IIS the log file isn't created; I've not yet determined why but it the fact that I need to look for a solution makes me want to try something else for my asp.net needs!  I use elmah. It has some really nice features and here is a CodeProject article on it. I think the StackOverflow team uses elmah also!  I've been using Log4net configured to email details of fatal errors. It's also set up to log everything to a log file which is invaluable when trying to debug problems. The other benefit is that if that standard functionality doesn't do what you want it to it's fairly easy to write a custom appender which can process the logging information as required. Having said that I'm using this in tandem with a custom error handler which sends out a html email with a bit more information than is included in the standard log4net emails - page session variables cookies http server variables etc. These are both wired up in the Application_OnError event where the exception is logged as a fatal exception in log4net (which then causes it to be emailed to a specified email address) and also handled using the custom error handler. First heard about Elmah from the Coding Horror blog entry Crash Responsibly and although it looks promising I'm yet to implement it any projects. +1 Couldn't have said it better myself!  I use CALM because it logs exceptions to the database along with the user's IP address session ID page name request parms and other useful information and sends me an email (via a separate notification service) with the details. It also provides a dashboard for all of my apps at a glance. caveat: i am the author of CALM ;-) My apologies for the broken link. Hosting is being moved and CALM is being revamped for v2 with new pricing and a free version; email steven [dot] lowe [at] nov8r.com if interested. This comment will be removed when the link is live again. @so the link is still not live. Bummer being down for 2 years :p @Rune FS: [facepalm] I forgot all about it; the product is off the market pending updates for all the new .NET stuff but if you want a version that works with .NET 2.0 send me an email  I use log4net and where ever I expect an exception I log it to the appropriate level. I tend not to re-throw the exception because it doesn't really allow for as-nice user experience there is less info you can provide at the current state. I'll have Application_Error also configured to catch any exception which was not expected and the error is logged as a Fatal priority through log4net (well 404's are detected and logged as Info as they aren't that high severity).  We use a custom home grown logging util we wrote. It requires you to implement logging on your own everywhere you need it. But it also allows your to capture alot more then just the exception. For exmaple our code would like like this:  Try Dim p as New Person() p.Name = ""Joe"" p.Age = 30 Catch ex as Exception Log.LogException(ex""Err creating person and assigning name/age"") Throw ex End Try This way our logger will write all the info we need to a sql db. We have email alerts set up at the db level to look for certain errors or frequently occurring errrors. It helps us identify exactly where the errors are coming from. This might not be exactly what you're looking for. Another approach similar to using Global.asax is to us a code injection technique like AOP with PostSharp. This allows you to inject custom code at the beginning and end of every method or on every exception. It' an interesting approach but I believe it may have a heavy performance overhead.  My team uses log4net from Apache. It's pretty lightweight and easy to setup. Best of all it's completely configurable from the web.config file so once you've got the hooks in your code setup you can completely change the way logging is done just by changing the web.config file. log4net supports logging to a wide variety of locations - database email text file Windows event log etc. My team has it configured to send detailed error information to a database and also send an email to the entire team with enough information for us to determine in which part of the code the error originated. Then we know who is responsible for that piece of code and they can go to the database to get more detailed information."97,A,"Strong SSL with Tomcat 6 I'm trying to create a self signed certificate for use with Apache Tomcat 6. Every certificate I can make always results in the browser connecting with AES-128. The customer would like me to demonstrate that I can create a connection at AES-256. I've tried java's keytool and openssl. I've tried with a variety of parameters but can't seem to specify anything about the keysize just the signature size. How can I get the browser-tomcat connection to use AES-256 with a self signed certificate? danivo so long as the server's cert is capable of AES encryption the level of encryption between the browser and the server is independent of the cert itself -- that level of encryption is negotiated between the browser and server. In other words my understanding is that the cert doesn't specify the level of encryption just the type of encryption (e.g. AES). See this link (PDF) for verification of this and how the cert resellers upsell ""256-bit-capable"" certs despite the cert not being what determines 256-bit capability. So you're just fine with the cert you have that supports AES-128 -- and they key is to figure out how to get Tomcat to support AES-256 (since most if not all major browsers certainly support it).  I think what you are looking for is http://www.sslshopper.com/article-how-to-disable-weak-ciphers-and-ssl-2-in-tomcat.html and http://docs.oracle.com/javase/1.5.0/docs/guide/security/jsse/JSSERefGuide.html#AppA Depending on whether you want good security and compatibility or PCI certification.  The strength of the SSL connection is negotiated between the browser and the server (or whatever is providing SSL). It might be their browser asking for a weaker cypher. Have they ever seen a 256-AES SSL connection on this browser? AES-128 is still a very secure algorithm so unless they have something that they want to protect from off line (think: supercomputer brute force generating 2^128 keys wikipedia) attack 128-bit should be fine. If they really need that much protection they probably should be using a more stable solution for data access than a website a secure ssh tunnel to their server is bulletproof (you can tell them they can have their 256-bit AES and 4096-bit RSA too) or a vpn depending upon implementation.  Okie doke I think I just figured this out. As I said above the key bit of knowledge is that the cert doesn't matter so long as it's generated with an algorithm that supports AES 256-bit encryption (e.g. RSA). Just to make sure that we're on the same page for my testing I generated my self-signed cert using the following: keytool -genkey -alias tomcat -keyalg RSA Now you have to make sure that your Java implementation on your server supports AES-256 and this is the tricky bit. I did my testing on an OS X (OS 10.5) box and when I checked to see the list of ciphers that it supported by default AES-256 was NOT on the list which is why using that cert I generated above only was creating an AES-128 connection between my browser and Tomcat. (Well technically TLS_RSA_WITH_AES_256_CBC_SHA was not on the list -- that's the cipher that you want according to this JDK 5 list.) For completeness here's the short Java app I created to check my box's supported ciphers: import java.util.Arrays; import javax.net.ssl.SSLSocketFactory; public class CipherSuites { public static void main(String[] args) { SSLSocketFactory sslsf = (SSLSocketFactory) SSLSocketFactory.getDefault(); String[] ciphers = sslsf.getDefaultCipherSuites(); Arrays.sort(ciphers); for (String cipher : ciphers) { System.out.println(cipher); } } } It turns out that JDK 5 which is what this OS X box has installed by default needs the ""Unlimited Strength Jurisdiction Policy Files"" installed in order to tell Java that it's OK to use the higher-bit encryption levels; you can find those files here (scroll down and look at the top of the ""Other Downloads"" section). I'm not sure offhand if JDK 6 needs the same thing done but the same policy files for JDK 6 are available here so I assume it does. Unzip that file read the README to see how to install the files where they belong and then check your supported ciphers again... I bet AES-256 is now on the list. If it is you should be golden; just restart Tomcat connect to your SSL instance and I bet you'll now see an AES-256 connection. Unlimited Strength Jurisdiction Policy Files must be installed in jdk home folder or just in tomcat/lib folder? They need to be in your JDK's /lib/security subdirectory (e.g. $JAVA_HOME/lib/security). This information is in the README of the files linked above."98,A,"Where can I find a ""Math topic dependency tree"" to assist my self-guided refresher on the subject? I'm trying to reteach myself some long forgotten math skills. This is part of a much larger project to effectively ""teach myself software development"" from the ground up (the details are here if you're interested in helping out). My biggest stumbling block so far has been math - how can I learn about algorithms and asymptotic notation without it?? What I'm looking for is some sort of ""dependency tree"" showing what I need to know. Is calculus required before discrete? What do I need to know before calculus (read: components to the general ""pre-calculus"" topic)? What can I cut out to fast track the project (""what can I go back for later"")? Thank! @Bill I didn't ask for the migration so much to get fresh answers so much as I just felt this question was on the wrong SX. If the powers that be are fine with leaving it unmigrated then that's fine with me. We don't typically migrate questions this old. It would be better to re-ask your question on Math.SE to get fresh answers from that community. Usually an overview of each field is a good thing to have when looking at any topic but it's rare to have a genuine dependence the way we'd think of it. Algebra is always needed. I can't think of a time I've needed any trigonometry. (except to expand it with new things from calculus) I'm even quite sure people wouldn't agree on what a dependency graph would look like or even in which field each topic belongs. I think the right way to approach it is to just collect a wide range of topics from all of branches and read them in whatever order you feel like recording dependencies between topics as you go. (respecting them or not as you please.) This should have the far more important property of keeping the student interested. It's also my experience that if something just has you stumped just mark it and set it aside for later. As for my school well it was similar to Harrison's: cominatorics linear algebra calculus numerical analysis (error analysis in particular.) logic statistics (with operations research / queueing therory.)  My advice is to lazily evaluate your own dependency tree. Study something you think is interesting -- when you hit something you don't know go learn about it. I always find it easier to learn something new when I already have a context in which I want to use it.  Here's how my school did it: base: algebra trigonometry analytic geometry track 1 track 2 track 3 calc 1 linear algebra statistics calc 2 discrete math 1 calc 3 (multivariable) discrete math 2 differential equations The base courses were a prerequisite for everything the tracks were independent and taken in order. So to answer your specific question only algebra is needed for discrete. If you want to fast track do one of these: algebra discrete algebra linear algebra discrete (if you want to cover matrices first) HTH... It about killed me when I returned to school and took these but I'm a much better programmer for it. Good Luck!  Take a look at MathWorld. Browse topics or search for one you'll get your position in the overall tree.  This is a particularly cool site for visualizing how everything in the math world fits together: http://www.math.niu.edu/~rusin/known-math/ It's also got short summaries of many subfields you've probably never heard of which is fun."99,A,"What is the proper way to do a Subversion merge in Eclipse? I'm pretty used to how to do CVS merges in Eclipse and I'm otherwise happy with the way that both Subclipse and Subversive work with the SVN repository but I'm not quite sure how to do merges properly. When I do a merge it seems to want to stick the merged files in a seperate directory in my project rather than overwriting the old files that are to be replaced in the merge as I am used to in CVS. The question is not particular to either Subclipse or Subversive. Thanks for the help! Merging an entire branch into trunk Inspect the Branch project history to determine the version from which the branch was taken by default Eclipse Team ""History"" only shows the past 25 revisions so you will have to click the button in that view labeled ""Show All"" when you say ""Show All"" it will take you back past the branch date and show you all the history for trunk as well so you'll have to search for your comment where you branched NOTE: if you use Tortise SVN for this same task (navigate to the branch and select ""Show Log"") it will show you only the branch history so you can tell exactly where the branch began So now I know that 82517 was the first version ID of the branch history. So all versions of the branch past 82517 have changes that I want to merge into trunk Now go to the ""trunk"" project in your Eclipse workspace and select ""right click - Team - Merge"" The default view is the 1 url merge select the URL of the branch from which you are merging under Revisions select ""All"" press OK This will take you to the ""Team Synchronizing"" perspective (if it doesn't you should go there yourself) in order to resolve conflicts (see below) Re-Merging more branch changes into trunk Insepct the trunk project history to determine the last time you merged into trunk (you should have commented this) for the sake of argument let's say this version was 82517 So now I know that any version greater than 82517 in the branch needs to be merged into trunk Now go to the ""trunk"" project in your Eclipse workspace and select ""right click - Team - Merge"" The default view is the 1 url merge select the URL of the branch from which you are merging under Revisions select ""Revisions"" radio button and click ""Browse"" this will open up a list of the latest 25 branch revisions select all the revisions with a number greater than 82517 press OK (you should see the revision list in the input field beside the radio button) press OK This will take you to the ""Team Synchronizing"" perspective (if it doesn't you should go there yourself) in order to resolve conflicts (see below) Resolving Conflicts You should be at the ""Team Synchronizing"" perspective. This will look like any regular synchronization for commit purposes where you see files that are new and files that have conflicts. For every file where you see a conflict choose ""right click - Edit Conflicts"" (do not double click the file it will bring up the commit diff version tool this is VERY different) if you see stuff like ""<<<<<<< .working"" or "">>>>>>> .merge-right.r84513"" then you are in the wrong editing mode once you have resolved all the conflicts in that file tell the file to ""mark as merged"" once all the files are free of conflicts you can then synchronize your Eclipse project and commit the files to SVN This should be marked as the answer! 40 upvotes for this vs 2 upvotes for the accepted answer agrees with you @HDave If I want to remerge a branch into trunk as you described in the second section would I get the same result if I always select ""all"" revisions instead of selecting the past revisions one-by-one?  Use Eclipse integration it works perfectly fine. The main change from CVS is that you only merge deltas from a branch ie changes from one revision to another. That is to say you have to track the correct start revision somehow (unless you have svn 1.5 merge history) If you got that right it's only up to you to get the changes right with the compare editor. It is worth pointing out that in order for merge history to work the client the server AND the repository all need to be upgraded to at least 1.5: see http://subversion.tigris.org/svn_1.5_releasenotes.html#mt-compatibility  I typically check out both branches and then use the compare to each other option which does a synchronize-like compare of the two source trees. After integrating the changes into one branch you can recommit back to the repository.  openCollabNet's merge tool for subclipse is pretty neat. There are many merging types available and the merging I just performed with it when seamlessly. I recommend it.  Remember that with svn reverting a modified tree to a clean state is fairly easy. Simply have a clean workspace on the merge destination branch and run the merge command to import the modifications from the merge source branch then synchronize your workspace and you will get your usual eclipse comparison window showing all the merge modified files and the conflicts. If for some reason you can't solve the conflicts you can svn revert on the project and go back to a clean state otherwise you do the merge in place and once you are done you can commit. Note that you don't have to commit once you are done handling the conflicts you can also return to the dev view verify that the code compiles run your unit tests whatever and then synchronize again and commit (once the conflict are locally resolved they won't come back) last time I looked when you use subclipse merge command it will overwrite the merged file (using conflict markers to show conflicting areas) and put the original left and right side of the merge in the same place. it shouldn't put anything in different directories. As a rule of thumb it is best to commit all merge modifications in a single commit and to only have the merge modifications in the commit so that you can rollback the merge later if needed.  The one thing that syncrhonize view in eclipse lacks is check-in capability. In Team synchronization view I can view all my changes and resolve conflicts so it would be rather intuitive to check-in right there instead of going back to java view and do check-in.  I would advise not trying to use Eclipse's plugins as your primary access to Subversion. If you are developing on Windows TortoiseSVN is the best program that I have seen for Subversion access. Explore to the directory of which you wish to merge right click on it and use the Tortoise SVN merge option. Assuming a non-interactive merge once you get conflicts you'll have to go through each conflicted file and edit the conflicts before marking them as resolved. For this process I recommend a program called KDiff3 which shows your local repository copy (what was stored in the .svn before the merge) your local copy (including any changes) and the copy coming from the repository and allows you to easily see (and even hand-modify if needed) the result of the merging. It also handles a bunch of minor conflicts automatically. KDiff3 is portable TortoiseSVN is a windows shell extension so if you're using another environment I would try to just use SVN to merge. But that would be much more of a pain :) And sometimes it doesn't. Especially if it basically proposes an effectively single-platform alternative for a question regarding a multi-platform IDE. Downvoter - care to add a reason? I'd say it was down-voted only because the question was specific about how to do it in Eclipse. Sometimes the answer has to be out of the box.  Firstly if you are seeing "">>>>>"" and such in your files when you view them in Eclipse this probably means that you are not looking at the file with the proper compare editor. Try right-clicking on the file in the Project view or Synchronize view and selecting ""Edit Conflicts"" to bring up a compare editor that will show you the conflicting regions graphically rather than as text. Note that the compare editor that comes up for ""Edit Conflicts"" is different from the one that you get when you just doubleclick on a file in the Synchronize view -- the doublieclick compare editor shows the differences between your current file and the way it existed when you last checked it out or updated it while the Edit Conflicts compare dialog shows the differences between two sources of changes (for instance the changes you merged versus the changes that existed in your workspace before you merged). Secondly you may wish to be aware of a bug in some versions of the Eclipse subversive plugin which causes all files that accepted merge changes to be incorrectly marked as having conflicts. This bug has been fixed but a lot of people don't seem to have updated to get the fix yet. Further details here: https://bugs.eclipse.org/bugs/show_bug.cgi?id=312585"100,A,Best way to keep an ordered list of windows (from most-recently created to oldest)? What is the best way to manage a list of windows (keeping them in order) to be able to promote the next window to the top-level when the current top-level window is closed. This is for a web application so we're using jQuery Javascript. We'd talked through a few simplistic solutions such as using an array and just treating [0] index as the top-most window. I'm wondering if there's any potentially more efficient or useful alternative to what we had brainstormed. A stack if you want to just close the window on top. A queue if you also need to open windows at the end.  I don't really know javascript but couldn't you create a stack of windows?  Stack/queue in JS is a simple array which can be manipulated with .push(val) .pop() .shift(val) and .unshift().101,A,"Help getting .Net WinForms apps to support Vista Aero Glass There are a couple of tricks for getting glass support for .Net forms. I think the original source for this method is here: http://blogs.msdn.com/tims/archive/2006/04/18/578637.aspx Basically: //reference Desktop Windows Manager (DWM API) [DllImport( ""dwmapi.dll"" )] static extern void DwmIsCompositionEnabled( ref bool pfEnabled ); [DllImport( ""dwmapi.dll"" )] static extern int DwmExtendFrameIntoClientArea( IntPtr hWnd ref MARGINS pMarInset ); //then on form load //check for Vista if ( Environment.OSVersion.Version.Major >= 6 ) { //check for support bool isGlassSupported = false; DwmIsCompositionEnabled( ref isGlassSupported ); if ( isGlassSupported ) DwmExtendFrameIntoClientArea( this.Handle ref margins ); ... //finally on print draw a black box over the alpha-ed area //Before SP1 you could also use a black form background That final step is the issue - any sub controls drawn over that area seem to also treat black as the alpha transparency mask. For instance a tab strip over the class area will have transparent text. Is there a way around this? Is there an easier way to do this? The applications I'm working on have to work on both XP and Vista - I need them to degrade gracefully. Are there any best practices here? DannySmurf said it. You don't have direct ""managed"" access to these APIs though the .NET framework (I tried this myself a few weeks ago). I ended up doing something nasty. Created my own UI with GDI+. (Buttons rounded labels etc). It looks the same regardless of the Windows version. Win.Forms is really ugly but that's all you got on the XP < side.  I think you forgot to set the TransparencyKey of the area you want to be glass. From the article In your Windows Forms application you simply need to set the TransparencyKey property to a color that you won't use elsewhere in the application (I use Gainsboro for reasons that will become apparent later). Then you can create one or more panels that are docked to the margins of your form and set the background color for the panel to the transparency key. Now when you call DwmExtendFrameIntoClientArea the glass will show within its margins wherever you've set something of the appropriate transparency key. Thanks but that doesn't work after SP1 - it seems like the box has to be black.  A cheap hack you can use is to place a transparent Panel control over your form and place your controls on it -- black will then be black.  There really isn't an easier way to do this. These APIs are not exposed by the .NET Framework (yet) so the only way to do it is through some kind of interop (or WPF). As for working with both Windows versions the code you have should be fine since the runtime does not go looking for the entry point to the DLL until you actually call the function.  I don't mind the unmanaged calls - it's the hack of using a black box to mimic the alpha behaviour and the effect it then has on black element in some components on top that's the problem."102,A,"How do you back up your development machine? How do you back up your development machine so that in the event of a catastrophic hardware malfunction you are up and running in the least amount of time possible? like a few others I have a clean copy of my virtual pc that I can grab and start fresh at anytime and all code is stored in subversion.  I use xcopy to copy all my personal files to an external hard drive on startup. Here's my startup.bat: xcopy d:\files f:\backup\files /D /E /Y /EXCLUDE:BackupExclude.txt This recurses directories only copies files that have been modified and suppresses the message to replace an existing file the list of files/folders in BackupExclude.txt will not be copied.  All important files are in version control (Subversion) My subversion layout generally matches the file layout on my web server so I can just do a checkout and all of my library files and things are in the correct places. Twice-daily backups to an external hard drive Nightly rsync backups to a remote server. This means that I send stuff on my home server over to my webhost and all files & databases on my webhost back home so I'm not screwed if I lose either my house or my webhost.  If you use a Mac it's a no brainer - just plug in an external hard drive and the built in Time Machine software will back up your whole system then maintain an incremental backup on the schedule you define. This has got me out of a hole many a time when I've messed up my environment; it also made it super easy to restore my system after installing a bigger hard drive. For offsite backups I like JungleDisk - it works on Mac Windows and Linux and backs up to Amazon S3 (or added very recently the Rackspace cloud service). This is a nice solution if you have multiple machines (or even VMs) and want to keep certain directories backed up without having to think about it.  A combination of RAID1 Acronis xcopy DVDs and ftp. See: http://successfulsoftware.net/2008/02/04/your-harddrive-will-fail-its-just-a-question-of-when/  I would like a recommendation for an external RAID container or perhaps just an external drive container preferably interfacing using FireWire 800. I also would like a recommendation for a manufacturer for the backup drives to go into the container. I read so many reviews of drives saying that they failed I'm not sure what to think. I don't like backup services like Mozy because I don't want to trust them to not look at my data.  Virtual machines and CVS. Desktops are rolled out with ghost and are completely vanilla. Except they have VirtualBox. Then developers pull the configured baseline development environment down from CVS. They log into the development VM image as themselves refresh the source and libraries from CVS and they're up and working agian. This also makes doing develpment and maintenance at the same time a lot easier. (I know some people won't like CVS or VirtualBox so feel free to substiture your tools of choice) oh and You check you work into a private branch off Trunk daily. There you go. Total time to recover : 1 hour (tops) Time to ""adopt"" a shbiy new laptop for a customer visit : 1 hour ( tops) And a step towards CMMI Configuration Management.  At work NetBackup or PureDisk depending on the box at home rsync.  If you are talking absolute least amount of restore time... I've often setup machines to do Ghost (Symantec or something similar) backups on a nightly basis to either an image or just a direct copy to another drive. That way all you have to do is reimage the machine from the image or just swap the drives. You can be back up in under 10 minutes... The setup I did before was in situation where we had some production servers that were redundant and it was acceptable for them to be offline long enough to clone the drive...but only at night. During the day they had to be up 100%...it saved my butt a couple times when a main drive failed... I just opened the case swapped the cables so the backup drive was the new master and was back online in 5 minutes.  There's an important distinction between backing up your development machine and backing up your work. For a development machine your best bet is an imaging solution that offers as near a ""one-click-restore"" process as possible. TimeMachine (Mac) and Windows Home Server (Windows) are both excellent for this purpose. Not only can you have your entire machine restored in 1-2 hours (depending on HDD size) but both run automatically and store deltas so you can have months of backups in relatively little space. There are also numerous ""ghosting"" packages though they usually do not offer incremental/delta backups so take more time/space to backup your machine. Less good are products such as Carbonite/Mozy/JungleDisk/RSync. These products WILL allow you to retrieve your data but you will still have to reinstall the OS and programs. Some have limited/no histories either. In terms of backing up your code and data then I would recommend a sourcecode control product like SVN. While a general backup solution will protect your data it does not offer the labeling/branching/history functionality that SCC packages do. These functions are invaluable for any type of project with a shelf-life. You can easily run a SVN server on your local machine. If your machine is backed up then your SVN database will be also. This IMO is the best solution for a home developer and is how I keep things. While TimeMachine/WHS may have advantages over the ""cloud"" backup services don't neglect to get frequent copies of your backups offsite. That is where those services shine.  I use TimeMachine.  Windows Home Server. My dev box has two drives with about 750GB of data between them (C: is a 300GB SAS 15K RPM drive with apps and system on it D: is a mirrored 1TB set with all my enlistments). I use Windows Home Server to back this machine up and have successfully restored it several times after horking it.  Home Server Warning! I installed Home Server on my development Server for two reasons: Cheap version of Windows Server 2003 and for backup reasons. The backup software side of things is seriously hit or miss. If you 'Add' a machine to the list of computers to be backed up right at the start of installing Home Server generally everything is great. BUT it seems it becomes a WHOLE lot harder to add any other machines after a certain amount of time has passed. (Case in point: I did a complete rebuild on my laptop tried to Add it - NOPE!) So i'm seriously doubting the reliability of this platform for backup purposes. Seems to be defeating the purpose if you can't trust it 100% I'm not alone in this Google 'WHS Windows Home Server connection problems' I prefer to use the built in Vista backup system these days with an external harddrive. I Wish this let me save to a network drive though! Your suggested Google brings up no hits that are related to your problem... I've been running WHS for over a year now (with frequent PC changes) without issue. Have you tried the WHS forums? I can't recall seeing anyone with your problem but people are very helpful http://tinyurl.com/bmnjzl are you serious? http://www.google.com.au/search?hl=en&q=WHS+Connection+issues&meta= found how to backup using vista's built in functionality. You can backup to a network share using a CMD prompt function 'wbadmin.exe start backup -backuptarget:\\server\share\ -include:c:' http://www.techinvasion.net/2007/11/12/vista-complete-pc-backup-to-network-share/  I don't. We do continuous integration submit code often to the central source control system (which is backed up like crazy!). If my machine dies at most I've lost a couple of days work. And all I need to do is get a clean disk at setup the dev environment from a ghost image or by spending a day sticking CDs in rebooting after Windows update etc. Not a pleasant day but I do get a nice clean machine.  I've finally gotten my ""fully automated data back-up strategy"" down to a fine art. I never have to manually intervene and I'll never lose another harddrive worth of data. If my computer dies I'll always have a full bootable back-up that is no more than 24 hours old and incremental back-ups no more than an hour old. Here are the details of how I do it. My only computer is a 160 gig MacBook running OSX Leopard. On my desk at work I have 2 external 500 gig harddrives. One of them is a single 500 gig partition called ""External"". The other has a 160 gig partition called ""Clone"" and a 340 gig partition called TimeMachine. TimeMachine runs whenever I'm at work constantly backing up my ""in progress"" files (which are also committed to Version Control throughout the day). Every weekday at 12:05 SuperDuper! automatically copies my entire laptop harddrive to the ""Clone"" drive. If my laptop's harddrive dies I can actually boot directly from the Clone drive and pick up work without missing a beat -- giving me some time to replace the drive (This HAS happened to me TWICE since setting this up!). (Technical Note: It actually only copies over whatever has changed since the previous weekday at 12:05... not the entire drive every time. Works like a charm.) At home I have a D-Link DNS-323 which is a 1TB (2x500 gig) Network Attached Storage device running a Mirrored RAID so that everything on the first 500 gig drive is automatically copied to the second 500 gig drive. This way you always have a backup and it's fully automated. This little puppy has a built-in Dynamic DNS client and FTP server. So on my WRT54G router I forward the FTP port (21) to my DNS-323 and leave its FTP server up. After the SuperDuper clone has been made rSync runs and synchronizes my ""External"" drive with the DNS-323 at home via FTP. That's it. Using 4 drives (2 external 2 in the NAS) I have: 1) An always-bootable complete backup less than 24 hours old Monday-Friday 2) A working-backup of all my in-progress files which is never more than 30 minutes old Monday-Friday (when I'm at work and connected to the external drives) 3) Access to all my MP3s (170GB) at documents at work on the ""External"" and at home on the NAS 4) Two complete backups of all my MP3s and documents on the NAS (External is original copy both drives on NAS are mirrors via ChronoSync) Why do I do all of this? Because: 1) In 2000 I dropped a 40 gig harddrive 1 inch and it cost me $2500 to get that data back. 2) In the past year I've had to take my MacBook in for repair 4 times. One dead harddrive two dead motherboards and a dead webcam. On the 4th time they replaced my MacBook with a newer better one at no charge and I haven't had a problem since. Thanks to my daily backups I didn't lose any work or productivity. If I hadn't had them though all my work would have been gone along with my MP3s and my writing and all the photos of my trips to Peru Croatia England France Greece Netherlands Italy and all my family photos. Can you imagine? I'm sure you can because I bet you have a pile of digital photos sitting on your computer right now... not backed-up in any way.  BTW your development machine should not contain anything of value. All your work (and your company's work) should be in central repositories (SVN).  I use Mozy and rarely think about it. That's one weight off my shoulders that I won't ever miss.  A little preparation helps: All my code is kept organized in one single directory (with categorized sub-directories). All email is kept in various PSTs. All code is also checked into source control at the end of every day. All documents are kept in one place as well. Backup: Backup your code email documents as often as it suits you (daily). Keep an image of your development environment always ready. Failure and Recovery If everything fails format and install the image. Copy back everything from backup and you are up and running. Of course there are tweaks here and there (incremental backup archiving etc.) which you have to do to make this process real.  SuperDuper complete bootable backups every few weeks Time Machine backups for my most important directories daily Code is stored in network subversion/git servers Mysql backups with cron on the web servers use ssh/rsync to pull it down onto our local servers also using cron nightly.  I use SuperDuper! and backup my Virtual Machine to another external drive (i have two). All the code is on a SVN server. I have a clean VM in case mine fails. But in either case it takes me a couple of hours to install WinXP+Vstudio. i don't use anything else in that box.  Maybe just a simple hardware hard disk raid would be a good start. This way if one drive fails you still have the other drive in the raid. If something other than the drives fail you can pop these drives into another system and get your files quickly.  I'm just sorting this out at work for the team. An image with all common tools is on Network. (We actually have a hotswap machine ready). All work in progress is on network too. So Developers machine goes boom. Use hotswap machine and continue. Downtime ~15 mins + coffee break.  I have the following backup scenarios and use rsync as a primary backup tool. (weekly) Windows backup for ""bare metal"" recovery Content of System drive C:\ using Windows Backup for quick recovery after physical disk failure as I don't want to reinstall Windows and applications from scratch. This is configured to run automatically using Windows Backup schedule. (daily and conditional) Active content backup using rsync Rsync takes care of all changed files from laptop phone other devices. I backup laptop every night and after significant changes in content like import of the recent photo RAWs from SD card to laptop. I've created a bash script that I run from Cygwin on Windows to start rsync: https://github.com/paravz/windows-rsync-backup  For my home and development machines I use Acronis True Image. In my opinion with the HD cheap prices nothing replaces a full incremental daily HD backup. I tried the trial for that. So much bloat. It wanted THREE exes to run on startup AND a service. Plus all my drives span up and clicked madly even 1 minute after running the program for the first time. Any program that feels it needs to index everything isn't worth having. I uninstalled it within minutes. Perhaps the non-trial version is better.  We have a corporate solution pushed down on us called Altiris which works when it wants to. It depends on whether or not it's raining outside. I think Altiris might be a rain-god and just doesn't know it. I am actually delighted when it's not working because it means that I can have my 99% of CPU usage back thank you very much. Other than that we don't have any rights to install other software solutions for backing things up or places we are permitted to do so. We are not permitted to move data off of our machines. So I end up just crossing my fingers while laughing at the madness.  My development machine is backed up using Retrospect and Acronis. These are nightly backups that run when I'm asleep - one to an external drive and one to a network drive. All my source code is in SVN repositories I keep all my repositories under a single directory so I have a scheduled task running a script that spiders a path for all SVN repositories and performs a number of hotcopies (using the hotcopy.py script) as well as an svndump of each repository. My work machine gets backed up however they handle it however I also have the same script running to do hotcopies and svndumps onto a couple of locations that get backed up. I make sure that of the work backups one location is NOT on the SAN yes it gets backed up and managed but when it is down it is down."103,A,"How can I kill a process using VBScript started by a particular user I have multiple users running attachemate on a Windows 2003 server. I want to kill attachemate.exe started by user_1 without killing attachemate.exe started by user_2. I want to use VBScript. You could use this to find out who the process owner is then once you have that you can use Win32_Process to kill the process by the process ID. MSDN Win32_Process class details MSDN Terminating a process with Win32_Process There is surely a cleaner way to do this but here's what I came up with. NOTE: This doesn't deal with multiple processes of the same name of course but I figure you can work that part out with an array to hold them or something like that. :) strComputer = ""."" strOwner = ""A111111"" strProcess = ""'notepad.exe'"" ' Connect to WMI service and Win32_Process filtering by name' Set objWMIService = GetObject(""winmgmts:{impersonationLevel=impersonate}!\\"" _ & strComputer & ""\root\cimv2"") Set colProcessbyName = objWMIService.ExecQuery(""Select * from Win32_Process Where Name = "" _ & strProcess) ' Get the process ID for the process started by the user in question' For Each objProcess in colProcessbyName colProperties = objProcess.GetOwner(strUsernamestrUserDomain) if strUsername = strOwner then strProcessID = objProcess.ProcessId end if next ' We have the process ID for the app in question for the user now we kill it' Set colProcessList = objWMIService.ExecQuery(""Select * from Win32_Process where ProcessId ="" & strProcessID) For Each objProcess in colProcess objProcess.Terminate() Next  Shell out to pskill from http://sysinternals.com/ Commandline: pskill -u user_1 attachemate.exe Needing to install pskill is not ideal. I'd prefer a solution that does not require me to install anything new."104,A,"Activating the main form of a single instance application In a C# Windows Forms application I want to detect if another instance of the application is already running. If so activate the main form of the running instance and exit this instance. What is the best way to achieve this? Aku that is a good resource. I answered a question similar to this one a while back. You can check my answer here. Even though this was for WPF you can use the same logic in WinForms. Actually I learned this trick from Sells book too. But Scott's article just seats among my bookmarks :)  You can use such detection and activate your instance after it:  // Detect existing instances string processName = Process.GetCurrentProcess().ProcessName; Process[] instances = Process.GetProcessesByName(processName); if (instances.Length > 1) { MessageBox.Show(""Only one running instance of application is allowed""); Process.GetCurrentProcess().Kill(); return; } // End of detection Thanks I really like your solution.  Here is what I'm currently doing in the application's Program.cs file. // Sets the window to be foreground [DllImport(""User32"")] private static extern int SetForegroundWindow(IntPtr hwnd); // Activate or minimize a window [DllImportAttribute(""User32.DLL"")] private static extern bool ShowWindow(IntPtr hWnd int nCmdShow); private const int SW_RESTORE = 9; static void Main() { try { // If another instance is already running activate it and exit Process currentProc = Process.GetCurrentProcess(); foreach (Process proc in Process.GetProcessesByName(currentProc.ProcessName)) { if (proc.Id != currentProc.Id) { ShowWindow(proc.MainWindowHandle SW_RESTORE); SetForegroundWindow(proc.MainWindowHandle); return; // Exit application } } Application.EnableVisualStyles(); Application.SetCompatibleTextRenderingDefault(false); Application.Run(new MainForm()); } catch (Exception ex) { } }  Scott Hanselman answers on you question in details."105,A,How to Detect if I'm Compiling Code With Visual Studio 2008? Is there any way to know if I'm compiling under Microsoft Visual Studio 2008 ? This is a little old but should get you started: //****************************************************************************** // Automated platform detection //****************************************************************************** // _WIN32 is used by // Visual C++ #ifdef _WIN32 #define __NT__ #endif // Define __MAC__ platform indicator #ifdef macintosh #define __MAC__ #endif // Define __OSX__ platform indicator #ifdef __APPLE__ #define __OSX__ #endif // Define __WIN16__ platform indicator #ifdef _Windows_ #ifndef __NT__ #define __WIN16__ #endif #endif // Define Windows CE platform indicator #ifdef WIN32_PLATFORM_HPCPRO #define __WINCE__ #endif #if (_WIN32_WCE == 300) // for Pocket PC #define __POCKETPC__ #define __WINCE__ //#if (_WIN32_WCE == 211) // for Palm-size PC 2.11 (Wyvern) //#if (_WIN32_WCE == 201) // for Palm-size PC 2.01 (Gryphon) //#ifdef WIN32_PLATFORM_HPC2000 // for H/PC 2000 (Galileo) #endif  In visual studio go to help | about and look at the version of Visual Studio that you're using to compile your app.  As a more general answer http://sourceforge.net/p/predef/wiki/Home/ maintains a list of macros for detecting specicic compilers operating systems architectures standards and more.  Yep _MSC_VER is the macro that'll get you the compiler version. The last number of releases of Visual C++ have been of the form <compiler-major-version>.00.<build-number> where 00 is the minor number. So _MSC_VER_ will evaluate to <major-version><minor-version>. You can use code like this: #if (_MSC_VER == 1500) // ... Do VC9/Visual Studio 2008 specific stuff #elif (_MSC_VER == 1600) // ... Do VC10/Visual Studio 2010 specific stuff #elif (_MSC_VER == 1700) // ... Do VC11/Visual Studio 2012 specific stuff #endif It appears updates between successive releases of the compiler have not modified the compiler-minor-version so the following code is not required: #if (_MSC_VER >= 1500 && _MSC_VER <= 1600) // ... Do VC9/Visual Studio 2008 specific stuff #endif Access to more detailed versioning information (such as compiler build number) can be found using other builtin pre-processor variables here. you just need to check == 1500 in that case @Jos true we could simplify it to just check for VC++ 9 with `_MSC_VER_ == 1500` however if MS did modify the `_MSC_VER` with compiler updates service packs etc (I don't think they ever have) then the `== 1500` check could break. Which is why I've coded it that way. __MSC_VER evaluate to the major and minor number components of the compiler's version. This will not change with an update there is _MSC_FULL_VER with include the build number too i have never need to use that. I will up-vote the answer if you edit it to clarify this. Bests Jose. @Jos: Answer updated to give a more correct and detailed answer.  By using Visual Studio specific macros more info is here.  By using the _MSC_VER macro.  _MSC_VER is what you need. You can also examine visualc.hpp in any recent boost install for some usage examples. Some values for the more recent versions of the compiler are: MSVC++ 12.0 _MSC_VER == 1800 (Visual Studio 2013) MSVC++ 11.0 _MSC_VER == 1700 (Visual Studio 2012) MSVC++ 10.0 _MSC_VER == 1600 (Visual Studio 2010) MSVC++ 9.0 _MSC_VER == 1500 (Visual Studio 2008) MSVC++ 8.0 _MSC_VER == 1400 (Visual Studio 2005) MSVC++ 7.1 _MSC_VER == 1310 (Visual Studio 2003) MSVC++ 7.0 _MSC_VER == 1300 MSVC++ 6.0 _MSC_VER == 1200 MSVC++ 5.0 _MSC_VER == 1100 The version number above of course refers to the major version of your Visual studio you see in the about box not to the year in the name. cl.exe /? will give a hint of the used version e.g.: c:\program files (x86)\microsoft visual studio 11.0\vc\bin>cl /? Microsoft (R) C/C++ Optimizing Compiler Version 17.00.50727.1 for x86 .....  _MSC_VER should be defined to a specific version number. You can either #ifdef on it or you can use the actual define and do a runtime test. (If for some reason you wanted to run different code based on what compiler it was compiled with? Yeah probably you were looking for the #ifdef. :))106,A,"Expose DependencyProperty When developing WPF UserControls what is the best way to expose a DependencyProperty of a child control as a DependencyProperty of the UserControl? The following example shows how I would currently expose the Text property of a TextBox inside a UserControl. Surely there is a better / simpler way to accomplish this? <UserControl x:Class=""WpfApplication3.UserControl1"" xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation"" xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml""> <StackPanel Background=""LightCyan""> <TextBox Margin=""8"" Text=""{Binding Text RelativeSource={RelativeSource FindAncestor AncestorType={x:Type UserControl}}}"" /> </StackPanel> </UserControl> using System; using System.Windows; using System.Windows.Controls; namespace WpfApplication3 { public partial class UserControl1 : UserControl { public static DependencyProperty TextProperty = DependencyProperty.Register(""Text"" typeof(string) typeof(UserControl1) new PropertyMetadata(null)); public string Text { get { return GetValue(TextProperty) as string; } set { SetValue(TextProperty value); } } public UserControl1() { InitializeComponent(); } } } That is how we're doing it in our team without the RelativeSource search rather by naming the UserControl and referencing properties by the UserControl's name. <UserControl x:Class=""WpfApplication3.UserControl1"" x:Name=""UserControl1"" xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation"" xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml""> <StackPanel Background=""LightCyan""> <TextBox Margin=""8"" Text=""{Binding Path=Text ElementName=UserControl1}"" /> </StackPanel> </UserControl> Sometimes we've found ourselves making too many things UserControl's though and have often times scaled back our usage. I'd also follow the tradition of naming things like that textbox along the lines of PART_TextDisplay or something so that in the future you could template it out yet keep the code-behind the same. this way works best in Silverlight 4 where there is no 'FindAncestor'  You can set DataContext to this in UserControl's constructor then just bind by only path. CS: DataContext = this; XAML: <TextBox Margin=""8"" Text=""{Binding Text} /> this only works on a limited scale. if you really need a datacontext you're screwed"107,A,Can I make Perl ithreads in Windows run concurrently? I have a Perl script that I'm attempting to set up using Perl Threads (use threads). When I run simple tests everything works but when I do my actual script (which has the threads running multiple SQL*Plus sessions) each SQL*Plus session runs in order (i.e. thread 1's sqlplus runs steps 1-5 then thread 2's sqlplus runs steps 6-11 etc.). I thought I understood that threads would do concurrent processing but something's amiss. Any ideas or should I be doing some other Perl magic? I and would guess others are curious to hear some follow-up on this question. Did you find a solution? Were the answers here helpful? What Frosty said... What was the problem? Check your database settings. You may find that it is set up in a conservative manner. That would cause even minor reads to block all access to that information. You may also need to call threads::yield.  A few possible explanations: Are you running this script on a multi-core processor or multi-processor machine? If you only have one CPU only one thread can use it at any time. Are there transactions or locks involved with steps 1-6 that would prevent it from being done concurrently? Are you certain you are using multiple connections to the database and not sharing a single one between threads? There was only one DB connection happening which wasn't what I expected. Thanks for the idea (never thought to check that) now I have work to do...  Actually you have no way of guaranteeing in which order threads will execute. So the behavior (if not what you expect) is not really wrong. I suspect you have some kind of synchronization going on here. Possibly SQL*Plus only let's itself be called once? Some programs do that... Other possiblilties: thread creation and process creation (you are creating subprocesses for SQL*Plus aren't you?) take longer than running the thread so thread 1 is finished before thread 2 even starts You are using transactions in your SQL scripts that force synchronization of database updates.108,A,"Flip an Image horizontally I need to flip an image so that a character faces in the right direction. This needs to be done ""on the fly' as they say. The issue I am having is that with Gif images I seem to lose the transparency. (The background goes white) Below is the code: (Alternatively someone could send me to a good example) $img = imagecreatefromgif(""./unit.gif""); $size_x = imagesx($img); $size_y = imagesy($img); $temp = imagecreatetruecolor($size_x $size_y); imagecolortransparent($img imagecolorallocate($img 0 0 0)); imagealphablending($img false); imagesavealpha($img true); $x = imagecopyresampled($temp $img 0 0 ($size_x-1) 0 $size_x $size_y 0-$size_x $size_y); if ($x) { $img = $temp; } else { die(""Unable to flip image""); } header(""Content-type: image/gif""); imagegif($img); imagedestroy($img); Shouldn't this: imagecolortransparent($img imagecolorallocate($img 0 0 0)); imagealphablending($img false); imagesavealpha($img true); ...be this: imagecolortransparent($temp imagecolorallocate($img 0 0 0)); imagealphablending($temp false); imagesavealpha($temp true); Note you should be calling these functions for the $temp image you have created not the source image.  If you can guarantee the presence of ImageMagick you can use their mogrify -flop command. It preserves transparency.  Final Results: $size_x = imagesx($img); $size_y = imagesy($img); $temp = imagecreatetruecolor($size_x $size_y); imagecolortransparent($temp imagecolorallocate($temp 0 0 0)); imagealphablending($temp false); imagesavealpha($temp true); $x = imagecopyresampled($temp $img 0 0 ($size_x-1) 0 $size_x $size_y 0-$size_x $size_y); if ($x) { $img = $temp; } else { die(""Unable to flip image""); } header(""Content-type: image/gif""); imagegif($img); imagedestroy($img); thanks for posting the updated code. That's helpful to me"109,A,"How can I find unused images and CSS styles in a website? Is there a tool or methodology (other than trial and error) I can use to find unused image files? How about CSS declarations for ID's and Classes that don't even exist in the site? It seems like there might be a way to just spider the site profile it and see which images and styles are never loaded. Hey Jon... just now (after reading the question and answers) I saw that it was you that asked the question. 4 years later I'm here looking for exactly the same thing! StackOverflow is really amazing... By the way: I just love the badge ""Works on my machine"" you have in your profile... I think I'll borrow this! :D More info at http://stackoverflow.com/questions/135657/tool-to-identify-unused-css-definitions CSS Redundancy Checker is a tool you run locally which you pass a stylesheet and either a list of URLs or a directory of HTML files. Here's the description given on the tool's site: A simple script that given a CSS stylesheet and either a .txt file listing URLs of HTML files or a directory of HTML files will iterate over them all and list the CSS statements in the stylesheet which are never called in the HTML. Basically it helps you keep your CSS files relevant and compact. And it's reasonably accurate.  As noted by Tim Murtaugh on the A List Apart blog post ""Two Tools to Keep Your CSS Clean"": csscss csscss will parse any CSS files you give it and let you know which rulesets have duplicated declarations. And most relevant to the question: helium-css Helium is a tool for discovering unused CSS across many pages on a web site. The tool is javascript-based and runs from the browser. Helium accepts a list of URLs for different sections of a site then loads and parses each page to build up a list of all stylesheets. It then visits each page in the URL list and checks if the selectors found in the stylesheets are used on the pages. Finally it generates a report that details each stylesheet and the selectors that were not found to be used on any of the given pages.  I seem to recall either Adobe Dreamweaver or Adobe Golive having a feature to find both orphaned styles and images; can't remember which now. Possibly both but the features were well-hidden.  You don't have to pay any web service or search for an addon you already have this in Google Chrome under F12 (Inspector)->Audits->Remove unused CSS rules Screenshot: This is great thanks for the tip! Good to use existing tools but this only scans the loaded page not the entire site? Awesome thanks. Be careful about responsive websites because you will have to reload for different sizes in order to know that one or more of these styles aren't being used. It only detects for the styles of the viewport being viewed. Any way to get the pruned up file of the style sheets rather then doing the removal process manually?  Try WARI - Web Application Resource Inspector. It finds unused images unused and duplicate CSS/JS. Link: wari.konem.net looks like the site is down now Tried it i have a huge php codebase. Doesn't look like it works  To answer your question about a tool to find unused image files you can use Xenu to spider your site to find all of the images that your site uses. Then Xenu prompts you for ftp access so that it can crawl your directories to find orphaned files. I have not yet used it on a production server but it sounds worthy to look into. EDIT: You just have to be careful not to delete images that are used by javascript.  TopStyle has a suite of tools for locating and dealing with orphan classes. It will also give you reports on where IDs and classes are used in the HTML allowing you to quickly open and skip to the relevant markup. Here's the blurb from the website regarding this feature: Site Reports: See at a glance where styles are used in your site. Find out where you've applied style classes that aren't defined in any style sheets or see what style classes you've defined that aren't being used. Very useful for dissecting unfamiliar websites. It doesn't find unused images though. Why is this answer voted down?  At a file level: use wget to aggressively spider the site and then process the http server logs to get the list of files accessed diff this with the files in the site diff \ <(sed some_rules httpd_log | sort -u) \ <(ls /var/www/whatever | sort -u) \ | grep something The mirror wget option is a good way to automatically prune un-referenced and unused files i.e. `wget -m `. The style sheets should be pruned from unused selectors first though - this looks like a good candidate for automatic that task: https://developers.google.com/speed/pagespeed/psol +1 for extra command-line geekiness!  Here is a really good article about cleaning up the CSS in different ways. In this article the writer use different tools and sum up different tools and ways to clean up your page. Just read the article and follow the given links in the article!  I found this tool that works with all versions of Firefox! It takes a little while to learn how it works but once it starts it seems pretty good. It will save a new version of the CSS with remarked out CSS selectors so you can quickly revert if you need to. CSS Usage - Firefox Addon  There's a Firefox extension that finds unused CSS selectors on a page. It has an option to spider the whole site. Version 3.01 should work with newer versions of Firefox. https://addons.mozilla.org/en-US/firefox/addon/dust-me-selectors/ And here's another option. https://addons.mozilla.org/en-US/firefox/addon/css-usage/ Yes this will only work on older version of FireFox but this: [CSS Usage - Firefox Addon](https://addons.mozilla.org/en-US/firefox/addon/css-usage/) is the same and will work also with the newest version."110,A,Which JavaScript library is recommended for neat UI effects? I need a JavaScript library that supports Ajax as well as help me in making simple and neat animation effects in a website I am working on. Which library do you recommend? http://script.aculo.us/ I think it fits your 'neat animation effects' requirement.  Stack Overflow uses jQuery if that matters. Scriptaculous tries pretty hard to do everything that you can do in Flash. Dojo has an SVG abstraction that lets you do things that are not directly supported in JavaScript.  Spry has a lot of effects that seem to be relatively easy to use. The downside (upside?) with Spry is its packaging. It's split into many separate pieces and parts. So if you want to use a lot of Spry you'll either be making several calls to external javascript files or you'll be gluing them together on your own. Spry won't do it for you neatly (like YUI does). However if you want to just use a single component or effect Spry is very lightweight!  Take a look at Dojo/Dijit/Dojox (http://dojotoolkit.org). They have a lot of cool special effects and a lot more that will come in handy to anyone working with Javascript. They also keep docs and related articles at http://dojocampus.org/  If you want to implement some basic animation jQuery is ok. Also personally I like the prototype.js For more difficult thing we using some features of Microsoft AJAX client library  I am a fan of YUI. It supports Animation and Ajax. In addition there is just a plethora of controls: menus movable windows tree controls sliders tabview the list goes on and on. I have used their code and I've had a good cross-browser experience with it. Doesn't surprise me. They do extensive testing on the toolkit.  Personally I'm a fan of MooTools' animation classes (Fx.Tween Fx.Morph Fx.Transitions). Very straight-forward and easy to use. For more advance animation Fx.Slide Fx.Scroll and Fx.Elements are also available... It also has a neat Ajax class (Request) that will take care of all your ajax needs. Obviously though this is my personal opinion... Any of the big ones (Yahoo UI jQuery MooTools Prototype etc...) will all be able to do both Ajax and Animation so I'd suggest looking at sample code from all those libraries and chose the one you like the most!  I would definitely recommend JQuery as the easiest to use and the one which requires you to write the least code. http://jquery.com/  That's a pretty broad question some of the top open source stacks are - YUI (Yahoo) - Prototype with Scriptaculuous - ExtJs - Dojo It's a pretty personal choice based on code style look and feel and which one you prefer.  I like ExtJS a lot. It's a great library for developing complex interfaces with javascript.  I've been playing with Scriptaculous and jQuery. Both are good although I'm leaning more toward jQuery.111,A,"Most effective form of CAPTCHA? Of all the forms of CAPTCHA available which one is the ""least crackable"" while remaining fairly human readable? I believe that CAPTCHA is dying. If someone really wants to break it it will be broken. I read (somewhere don't remember where) about a site that gave you free porn in exchange for answering CAPTCHAs to they can be rendered obsolete by bots. So why bother? Anyone who really wants to break this padlock can use a pair of bolt cutters so why bother with the lock? Anyone who really wants to steal this car can drive up with a tow truck so why bother locking my car? Anyone who really wants to open this safe can cut it open with an oxyacetylene torch so why bother putting things in the safe? Because using the padlock locking your car putting valuables in a safe and using a CAPTCHA weeds out a large spectrum of relatively unsophisticated or unmotivated attackers. The fact that it doesn't stop sophisticated highly motivated attackers doesn't mean that it doesn't work at all. Using a CAPTCHA isn't going to stop all spammers but it's going to tremendously reduce the amount that requires filtering or manual intervention. Heck look at the lame CAPTCHA that Jeff uses on his blog. Even a wimpy barrier like that still provides a lot of protection. Nice reasoning :D Try https://verswerks.com/verscaptcha. It's super easy to add to your form it's secure and by using it you're helping build a potential spam IP list that you can access to block unnecessary traffic from your website! While it's true that a CAPTCHA is better than nothing the fact that an attacker can overcome them points to using the easiest solution to integrate and for the end user to read and respond properly to! So the ""hardest to crack"" aspect should I feel not be a consideration. Point well said!  Just.. don't.. There are several reasons use of captcha is not advised. http://www.interfacegeek.com/dont-ever-use-captchas/  CAPTCHAS I believe should start being considered heavily when designing the UX. They're slow cumbersome and a very poor user experience. They are useful don't get me wrong but perhaps you should look into designing a honeypot. A honeypot is created by adding a hiddenfield at the bottom of the form. Because spam bots will fill in all the fields on the page blindly you can do a check: If honeypotfield <> Empty Then ""No Spam TY"" Else //Proceed with the form End If This works until there is a specifically designed spambot for your site so they can choose to fill out selected input fields. For more information: http://haacked.com/archive/2007/09/11/honeypot-captcha.aspx/  If you're just looking for a captcha to prevent spammers from bombing your blog the best option is something simple but unique. For example ask to write the word ""Cat"" into a box. The advantage of this is that no targeted captcha-breaker was developed for this solution and your small blog isn't important enough for someone to actually develop one. I've used such a captcha on my blog with some success for a couple of years now.  I wonder if a CAPTCHA mechanism that uses collage made of pictures and asks human to type what he sees in the collage image will be much more crack-proof than the text and number image one. Imagine that the mechanism stitches pictures of cat cup and car into a collage image and expects human visitor to tick (checkboxes) cat cup and car. How long do you think will hackers and crackers will come up with an algorithm to crack the mechanism (i.e. extract image elements from the collage and recognize the object depicted by each picture) ...  I agree with Thomas. Captcha is on its way out. But if you must use it reCAPTCHA is a pretty good provider with a simple API.  If you wanted you could try out the Microsoft Research project Asirra: http://research.microsoft.com/asirra/  Here is a cool link to create CAPTCHA..... http://www.codeproject.com/aspnet/CaptchaImage.asp  As far as I know the Google's one is the best that there is. It hasn't been broken by computer programs yet. What I know that the crackers have been doing is to copy the image and then send it to many phishing websites where humans solve them to enter those websites.  This information is hard to really know because I believe a CAPTCHA gets broken long before anybody knows about it. There is economic incentive for those that break them to keep it quiet. I used to work with a guy whose job revolved mostly around breaking CAPTCHA's and I can tell you the one giving them fits currently is reCAPTCHA. Now does that mean it will forever call me skeptical.  I believe that CAPTCHA is dying. If someone really wants to break it it will be broken. I read (somewhere don't remember where) about a site that gave you free porn in exchange for answering CAPTCHAs to they can be rendered obsolete by bots. So why bother? How do you people find some of this stuff? There must be a lot of money to be gained by people breaking these CAPTCHAs.  If you're a small enough site no one would bother. If you're still looking for a CAPTCHA I like tEABAG_3D by the OCR Research Team. It's complicated to break and uses your 3D vision. Plus it being developed by people who break CAPTCHAs for fun. And if you pay them they will have code that already breaks their own captchas  It doesn't matter if captchas are broken or not now -- there are Indian firms that do nothing but process captchas. I'm with the rest of the group in saying that Captchas are on their way out.  Death by Captcha can solve any Regular CAPTCHA (incude reCAPTCHA) but not Speedcoin Cryptocurrency Captcha. Death by Captcha - http://deathbycaptcha.com Speedcoin Captcha - http://speedcoin.co/info/captcha/Speedcoin_Captcha.html  I use uniqpin.com - it's easy to use and not annoying for users. So bots can recognise a text but can't recognize a image."112,A,"What is the easiest way using T-SQL / MS-SQL to append a string to existing table cells? I have a table with a 'filename' column. I recently performed an insert into this column but in my haste forgot to append the file extension to all the filenames entered. Fortunately they are all '.jpg' images. How can I easily update the 'filename' column of these inserted fields (assuming I can select the recent rows based on known id values) to include the '.jpg' extension? I wanted to adjust David B's ""Life Lesson"". I think it should be ""never use char for variable length string values"" -> There are valid uses for the char data type just not as many as some people think :)  If the original data came from a char column or variable (before being inserted into this table) then the original data had the spaces appended before becoming a varchar. DECLARE @Name char(10) @Name2 varchar(10) SELECT @Name = 'Bob' @Name2 = 'Bob' SELECT CASE WHEN @Name2 = @Name THEN 1 ELSE 0 END as Equal CASE WHEN @Name2 like @Name THEN 1 ELSE 0 END as Similiar Life Lesson : never use char. AH! It was an Excel import using the SQL Import tool  The solution is: UPDATE tablename SET [filename] = RTRIM([filename]) + '.jpg' WHERE id > 50 RTRIM is required because otherwise the [filename] column in its entirety will be selected for the string concatenation i.e. if it is a varchar(20) column and filename is only 10 letters long then it will still select those 10 letters and then 10 spaces. This will in turn result in an error as you try to fit 20 + 3 characters into a 20 character long field. Thanks very helpful Do ***var**char* values include spaces in expressions or just fixed length *char* fields? I'm not sure - the field I was working with was a **var**char(50) and it included spacing which I agree does seem against the idea of the varchar. Perhaps someone can enlighten us as to why the varchar included spaces when selected?  Nice easy one I think. update MyTable set filename = filename + '.jpg' where ... Edit: Ooh +1 to @MattMitchell's answer for the rtrim suggestion.  The answer to the mystery of the trailing spaces can be found in the ANSI_PADDING For more information visit: SET ANSI_PADDING (Transact-SQL) The default is ANSI_PADDIN ON. This will affect the column only when it is created but not to existing columns. Before you run the update query verify your data. It could have been compromised. Run the following query to find compromised rows: SELECT * FROM tablename WHERE LEN(RTRIM([filename])) > 46 -- The column size varchar(50) minus 4 chars -- for the needed file extension '.jpg' is 46. These rows either have lost some characters or there is not enough space for adding the file extension. Thanks for the reference  MattMitchell's answer is correct if the column is a CHAR(20) but is not true if it was a VARCHAR(20) and the spaces hadn't been explicitly entered. If you do try it on a CHAR field without the RTRIM function you will get a ""String or binary data would be truncated"" error. I'm not sure - the field I was working with was a **var**char(50) and it included spacing which I agree does seem against the idea of the varchar. Perhaps someone can enlighten us as to why the varchar included spaces when selected?"113,A,Most succinct way to determine if a variable equals a value from a 'list' of values If I have a variable in C# that needs to be checked to determine if it is equal to one of a set of variables what is the best way to do this? I'm not looking for a solution that stores the set in an array. I'm more curious to see if there is a solution that uses boolean logic in some way to get the answer. I know I could do something like this: int baseCase = 5; bool testResult = baseCase == 3 || baseCase == 7 || baseCase == 12 || baseCase == 5; I'm curious to see if I could do something more like this: int baseCase = 5; bool testResult = baseCase == (3 | 7 | 12 | 5); Obviously the above won't work but I'm interested in seeing if there is something more succinct than my first example which has to repeat the same variable over and over again for each test value. UPDATE: I decided to accept CoreyN's answer as it seems like the most simple approach. It's practical and still simple for a novice to understand I think. Unfortunately where I work our system uses the .NET 2.0 framework and there's no chance of upgrading any time soon. Are there any other solutions out there that don't rely on the .NET 3.5 framework besides the most obvious one I can think of: new List<int>(new int[] { 3 6 7 1 }).Contains(5); Here's a .NET 2.0 solution a bit more complex than Corey's : http://stackoverflow.com/questions/18407#153037  bool b = new int[] { 37125 }.Contains(5);  I usually use CoreyN's solution for simple cases like that. Anything more complex use a LINQ query.  You can do something similar with .NET 2.0 by taking advantage of the fact that an array of T implements IList<T> and IList<T> has a Contains method. Therefore the following is equivalent to Corey's .NET 3.5 solution though obviously less clear: bool b = ((IList<int>)new int[] { 3 7 12 5 }).Contains(5); I often use IList<T> for array declarations or at least for passing one-dimensional array arguments. It means you can use IList properties such as Count and switch from an array to a list easily. E.g. private readonly IList<int> someIntegers = new int[] { 12345 };  Since you did not specify what type of data you have as input I'm going to assume you can partition your input into powers of 2 -> 24816... This will allow you to use the bits to determine if your test value is one of the bits in the input. 4 => 0000100 16 => 0010000 64 => 1000000 using some binary math... testList = 4 + 16 + 64 => 1010100 testValue = 16 testResult = testList & testValue114,A,How to efficiently SQL select newest entries from a MySQL database? Possible Duplicate: SQL Query to get latest price I have a database containing stock price history. I want to select most recent prices for every stock that is listed. I know PostreSQL has a DISTINCT ON statement that would suit ideally here. Table columns are name closingPrice and date; name and date together form a unique index. The easiest (and very uneffective) way is SELECT * FROM stockPrices s WHERE s.date = (SELECT MAX(date) FROM stockPrices si WHERE si.name = s.name); Much better approach I found is SELECT * FROM stockPrices s JOIN (SELECT name MAX(date) AS date FROM stockPrices si GROUP BY name) lastEntry ON s.name = lastEntry.name AND s.date = lastEntry.date; What would be an efficient way to do this? What indexes should I create? duplicate of: SQL Query to get latest price See similar post I fail at search.  I think that your second approach is very efficient. What's its problem? You have to add indexes to name and date. Well you have to add an index if performance requires it. If it's ten stocks and a year's daily data I wouldn't be concerned: MySQL is relatively good at table scanning. If he had few data he wouldn't be asking for an efficient way to do this no? Even the first obvious approach would be enough.115,A,"When do function-level static variables get allocated/initialized? I'm quite confident that globally declared variables get allocated (and initialized if applicable) at program start time. int globalgarbage; unsigned int anumber = 42; But what about static ones defined within a function? void doSomething() { static bool globalish = true; // ... } When is the space for globalish allocated? I'm guessing when the program starts. But does it get initialized then too? Or is it initialized when doSomething() is first called? I try to test again code from Adam Pierce and added two more cases: static variable in class and POD type. My compiler is g++ 4.8.1 in Windows OS(MinGW-32). Result is static variable in class is treated same with global variable. Its constructor will be called before enter main function. Conclusion (for g++ Windows environment): Global variable and static member in class: constructor is called before enter main function (1). Local static variable: constructor is only called when execution reaches its declaration at first time. If Local static variable is POD type then it is also initialized before enter main function (1). Example for POD type: static int number = 10; (1): The correct state should be: ""before any function from the same translation unit is called"". However for simple as in example below then it is main function. include < iostream> #include < string> using namespace std; class test { public: test(const char *name) : _name(name) { cout << _name << "" created"" << endl; } ~test() { cout << _name << "" destroyed"" << endl; } string _name; static test t; // static member }; test test::t(""static in class""); test t(""global variable""); void f() { static test t(""static variable""); static int num = 10 ; // POD type init before enter main function test t2(""Local variable""); cout << ""Function executed"" << endl; } int main() { test t(""local to main""); cout << ""Program start"" << endl; f(); cout << ""Program end"" << endl; return 0; } result: static in class created global variable created local to main created Program start static variable created Local variable created Function executed Local variable destroyed Program end local to main destroyed static variable destroyed global variable destroyed static in class destroyed Anybody tested in Linux env ?  When is the space for globalish allocated? At startup. Space for all static variables is allocated at startup. The fact that globalish is initialized to a value known at compile-time will control how it will be stored in the object file. But it will have no influence on when the space for it will be allocated.  Static variables are allocated inside a code segment -- they are part of the executable image and so are mapped in already initialized. Static variables within function scope are treated the same the scoping is purely a language level construct. For this reason you are guaranteed that a static variable will be initialized to 0 (unless you specify something else) rather than an undefined value. There are some other facets to initialization you can take advantage off -- for example shared segments allow different instances of your executable running at once to access the same static variables. In C++ (globally scoped) static objects have their constructors called as part of the program start up under the control of the C runtime library. Under Visual C++ at least the order that objects are initialized in can be controlled by the init_seg pragma. This question is about function-scoped statics. At least when they have nontrivial constructors they are initialized on first entry into the function. Or more specifically when that line is reached. True -- but the question talks about the space allocated to the variable and uses simple data types. The space is still allocated in the code segment I don't see how code segment vs. data segment really matters here. I think we need clarification from the OP. He did say ""and initialized if applicable"". variables are never allocated inside the code segment; this way they wouldn't be write-able.  Some relevant verbiage from C++ Standard: 3.6.2 Initialization of non-local objects [basic.start.init] 1 The storage for objects with static storage duration (basic.stc.static) shall be zero-initialized (dcl.init) before any other initialization takes place. Objects of POD types (basic.types) with static storage duration initialized with constant expressions (expr.const) shall be initialized before any dynamic initialization takes place. Objects of namespace scope with static storage duration defined in the same translation unit and dynamically initialized shall be initialized in the order in which their definition appears in the translation unit. [Note: dcl.init.aggr describes the order in which aggregate members are initialized. The initialization of local static objects is described in stmt.dcl. ] [more text below adding more liberties for compiler writers] 6.7 Declaration statement [stmt.dcl] ... 4 The zero-initialization (dcl.init) of all local objects with static storage duration (basic.stc.static) is performed before any other initialization takes place. A local object of POD type (basic.types) with static storage duration initialized with constant-expressions is initialized before its block is first entered. An implementation is permitted to perform early initialization of other local objects with static storage duration under the same conditions that an implementation is permitted to statically initialize an object with static storage duration in namespace scope (basic.start.init). Otherwise such an object is initialized the first time control passes through its declaration; such an object is considered initialized upon the completion of its initialization. If the initialization exits by throwing an exception the initialization is not complete so it will be tried again the next time control enters the declaration. If control re-enters the declaration (recursively) while the object is being initialized the behavior is undefined. [Example:  int foo(int i) { static int s = foo(2*i); // recursive call - undefined return i+1; } --end example] 5 The destructor for a local object with static storage duration will be executed if and only if the variable was constructed. [Note: basic.start.term describes the order in which local objects with static storage duration are destroyed. ]  I was curious about this so I wrote the following test program and compiled it with g++ version 4.1.2. include <iostream> #include <string> using namespace std; class test { public: test(const char *name) : _name(name) { cout << _name << "" created"" << endl; } ~test() { cout << _name << "" destroyed"" << endl; } string _name; }; test t(""global variable""); void f() { static test t(""static variable""); test t2(""Local variable""); cout << ""Function executed"" << endl; } int main() { test t(""local to main""); cout << ""Program start"" << endl; f(); cout << ""Program end"" << endl; return 0; } The results were not what I expected. The constructor for the static object was not called until the first time the function was called. Here is the output: global variable created local to main created Program start static variable created Local variable created Function executed Local variable destroyed Program end local to main destroyed static variable destroyed global variable destroyed As a clarification: the static variable is initialized the first time execution hits its declaration not when the containing function is called. If you just have a static at the start of the function (e.g. in your example) these are the same but are not necessarily: e.g. if you have 'if (...) { static MyClass x; ... }' then 'x' will not be initialized at ALL during the first execution of that function in the case where the if statement's condition evaluates to false. But doesn't this lead to a runtime overhead since each time the static variable is used the program has to check if it has been previously used since if not it has to be initialized? In that case that kind of sucks a bit.  The compiler will allocate static variable(s) defined in a function foo at program load however the compiler will also add some additional instructions (machine code) to your function foo so that the first time it is invoked this additional code will initialize the static variable (e.g. invoking the constructor if applicable). @Adam: This behind the scenes injection of code by the compiler is the reason for the result you saw.  The memory for all static variables is allocated at program load. But local static variables are created and initialized the first time they are used not at program start up. There's some good reading about that and statics in general here. In general I think some of these issues depend on the implementation especially if you want to know where in memory this stuff will be located. not quite local statics are allocated and zero-initialized ""at program load"" (in quotes because that's not quite right either) and then reinitialized the first time the function they're in is entered.  Or is it initialized when doSomething() is first called? Yes it is. This among other things lets you initialize globally-accessed data structures when it is appropriate for example inside try/catch blocks. E.g. instead of int foo = init(); // bad if init() throws something int main() { try { ... } catch(...){ ... } } you can write int& foo() { static int myfoo = init(); return myfoo; } and use it inside the try/catch block. On the first call the variable will be initialized. Then on the first and next calls its value will be returned (by reference)."116,A,"Open one of a series of files using a batch file I have up to 4 files based on this structure (note the prefixes are dates) 0830filename.txt 0907filename.txt 0914filename.txt 0921filename.txt I want to open the the most recent one (0921filename.txt). how can i do this in a batch file? Thanks. Sorry for spamming this question but I just really feel like posting The Real Answer. If you want your BATCH script to parse and compare the dates in filenames then you can use something like this: @echo off rem Enter the ending of the filenames. rem Basically you must specify everything that comes after the date. set fn_end=filename.txt rem Do not touch anything bellow this line. set max_month=00 set max_day=00 for /F %%i in ('dir /B *%fn_end%') do call :check ""%%i"" call :open %max_month% %max_day% exit /B 0 :check set name=%~1 set date=%name:~04% set month=%date:~02% set day=%date:~22% if /I %month% GTR %max_month% ( set max_month=%month% set max_day=%day% ) else if /I %month% EQU %max_month% ( set max_month=%month% if /I %day% GTR %max_day% ( set max_day=%day% ) ) exit /B 0 :open set date=%~1 set month=%~2 set name=%date%%month%%fn_end% start ""dummy"" ""%name%"" exit /B 0 I dot't at this time but dang if that ain't nice!! MAN! Do you know of any good books on learning to write stuff like this?! you could probably answer this question too! http://beta.stackoverflow.com/questions/51054/batch-file-to-delete-files-older-than-n-days Actually there's no real need for books. You can simply type HELP in command line to get the list of all default commands. And then you can read the help of each individual command for more information - you just simple add the /? switch to the command. Or at least that's how I do it. I had no idea SET could do substrings. This helped me a ton. Thanks!  Here you go... (hope no-one beat me to it...) (You'll need to save the file as lasttext.bat or something) This will open up / run the oldest .txt file dir *.txt /b /od > systext.bak FOR /F %%i in (systext.bak) do set sysRunCommand=%%i call %sysRunCommand% del systext.bak /Y Probably XP only. BEHOLD The mighty power of DOS. Although this takes the latest filename by date - NOT by filename.. If you want to get the latest filename change /od to /on . If you want to sort on something else add a ""sort"" command to the second line. Your method will work but it will create unnecessary temp files. Also when using del in BATCH scripts I always add the /Y switch - otherwise the del command can be very annoying... :) good call - I shall edit. ___ Yours was very good AND FAST!! thanks for the response ___  One liner using EXIT trick: FOR /F %%I IN ('DIR *.TXT /B /O:-D') DO NOTEPAD %%I & EXIT EDIT: @pam: you're right I was assuming that the files were in date order but you can change the command to: FOR /F %%I IN ('DIR *.TXT /B /O:-N') DO NOTEPAD %%I & EXIT then you have the file list sorted by name in reverse order. NICE!!!! thanks.  Use regular expression to parse the relevant integer out and compare them. I think it's implied in the question that he only wants to use things which would be available from a command line. Do you know of a command line RE tool that would be available on Windows?  This method uses the actual file modification date to figure out which one is the latest file: @echo off for /F %%i in ('dir /B /O:-D *.txt') do ( call :open ""%%i"" exit /B 0 ) :open start ""dummy"" ""%~1"" exit /B 0 This method however chooses the last file in alphabetic order (or the first one in reverse-alphabetic order) so if the filenames are consistent - it will work: @echo off for /F %%i in ('dir /B *.txt^|sort /R') do ( call :open ""%%i"" exit /B 0 ) :open start ""dummy"" ""%~1"" exit /B 0 You actually have to choose which method is better for you. Just wanted to say I used the solution above to open the my latest to do text file everytime I start up my computer! Thanks for the help! Can you tell me what the %%i does in line 2 and what the %~1 does in line 7? Thanks! %%i is the loop variable (it will get the value of the first word in each line that the command inside parentheses writes to standard output). %1 is a simple way to access the command line argument passed to the script or the label (like in my case). %~1 however removes the quotes (if any). Vilnius Lithuania...?....hmmmm...I know a programmer there...Gintaras Didzgalvis he makes QuickMacros (http://QuickMacros.com). You should look him up sometime."117,A,"JavaScript interactive shell with completion For debugging and testing I'm searching for a JavaScript shell with auto completion and if possible object introspection (like ipython). The online JavaScript Shell is really nice but I'm looking for something local without the need for an browser. So far I have tested the standalone JavaScript interpreter rhino spidermonkey and google V8. But neither of them has completion. At least Rhino with jline and spidermonkey have some kind of command history via key up/down but nothing more. Any suggestions? This question was asked again here. It might contain an answer that you are looking for. This post by John Resig says that there are shells for Tamarin (Firefox 4?) and JavaScriptCore (Safari 3). I'm not sure if they have auto completion though.  jslibs (a standalone javascript runtime) could also be suitable for this purpose.  Rhino Shell since 1.7R2 has support for completion as well. You can find more information here.  In Windows you can run this file from the command prompt in cscript.exe and it provides an simple interactive shell. No completion. // shell.js // ------------------------------------------------------------------ // // implements an interactive javascript shell. // // from // http://kobyk.wordpress.com/2007/09/14/a-jscript-interactive-interpreter-shell-for-the-windows-script-host/ // // Sat Nov 28 00:09:55 2009 // var GSHELL = (function () { var numberToHexString = function (n) { if (n >= 0) { return n.toString(16); } else { n += 0x100000000; return n.toString(16); } }; var line scriptText previousLine result; return function() { while(true) { WScript.StdOut.Write(""js> ""); if (WScript.StdIn.AtEndOfStream) { WScript.Echo(""Bye.""); break; } line = WScript.StdIn.ReadLine(); scriptText = line + ""\n""; if (line === """") { WScript.Echo( ""Enter two consecutive blank lines to terminate multi-line input.""); do { if (WScript.StdIn.AtEndOfStream) { break; } previousLine = line; line = WScript.StdIn.ReadLine(); line += ""\n""; scriptText += line; } while(previousLine != ""\n"" || line != ""\n""); } try { result = eval(scriptText); } catch (error) { WScript.Echo(""0x"" + numberToHexString(error.number) + "" "" + error.name + "": "" + error.message); } if (result) { try { WScript.Echo(result); } catch (error) { WScript.Echo(""<<>>""); } } result = null; } }; })(); GSHELL(); If you want you can augment that with other utility libraries with a .wsf file. Save the above to ""shell.js"" and save the following to ""shell.wsf"": <job> <reference object=""Scripting.FileSystemObject"" /> <script language=""JavaScript"" src=""util.js"" /> <script language=""JavaScript"" src=""shell.js"" /> </job> ...where util.js is: var quit = function(x) { WScript.Quit(x);} var say = function(s) { WScript.Echo(s); }; var echo = say; var exit = quit; var sleep = function(n) { WScript.Sleep(n*1000); }; ...and then run shell.wsf from the command line.  Isn't Rhino Shell what you are looking for?  edit: after using the node REPL a bit more I've discovered this evaluation to be overly positive. There are some serious problems with its implementation including an inability to yank killed text issues with editing lines that are longer than the terminal width and some other problems. It might be better to just use rhino. The node.js REPL (node-repl on a system with node installed) is the best terminal-based system-context shell I've seen so far. I'm comparing it to rhino and the built-in v8 shell. It provides tab-completion and line editing history as well as syntax-colouring of evaluations. You can also import CommonJS modules or at least those modules implemented by node. Downside is that you have to build node. This is not a huge deal as building apps goes but might be a challenge if you don't normally do such things."118,A,"Why is Application.Restart() not reliable? Using the Method Application.Restart() in C# should restart the current Application: but it seems that this is not always working. Is there a reason for this Issue can somebody tell me why it doesn't work all the time? In my program I have a mutex to ensure only one instance of the application running on a computer. This was causing the newly started application to not start because the mutex had not been release in a timely fashion. As a result I put a value into Properties.Settings that indicates that the application is restarting. Before calling Application.Restart() the Properties.Settings value is set to true. In Program.Main() I also added a check for that specific property.settings value so that when true it is reset to false and there is a Thread.Sleep(3000); In your program you may have the logic: if (ShouldRestartApp) { Properties.Settings.Default.IsRestarting = true; Properties.Settings.Default.Save(); Application.Restart(); } In Program.Main() [STAThread] static void Main() { Mutex runOnce = null; if (Properties.Settings.Default.IsRestarting) { Properties.Settings.Default.IsRestarting = false; Properties.Settings.Default.Save(); Thread.Sleep(3000); } try { runOnce = new Mutex(true ""SOME_MUTEX_NAME""); if (runOnce.WaitOne(TimeSpan.Zero)) { Application.EnableVisualStyles(); Application.SetCompatibleTextRenderingDefault(false); Application.Run(new Form1()); } } finally { if (null != runOnce) runOnce.Close(); } } That's it. I used your `IsRestarting` idea in my VB translation [here](http://stackoverflow.com/questions/19367452/clickonce-application-doesnt-restart-after-upgrade-is-complete/22604449#22604449). It's a good idea but the code block itself doesn't work well with VB's Application Framework events. It took some doing but I finally got it working.  If the application was first launched from a network location and is unsigned (you get the warning dialog first) it won't restart and will only exit.  Try this code: bool appNotRestarted = true; This code must also be in the function: if (appNotRestarted == true) { appNotRestarted = false; Application.Restart(); Application.ExitThread(); }  Application.Restart(); Application.ExitThread(); this worked for me thanks. Please don't add ""thank you"" as an answer. Once you have sufficient [reputation](http://stackoverflow.com/help/whats-reputation) you will be able to [vote up questions and answers](http://stackoverflow.com/help/privileges/vote-up) that you found helpful. A little bit more information why this works would be nice.  In my case (NO single-instance) where Application.Restart(); didn't work System.Diagnostics.Process.Start(Application.ExecutablePath); Application.Exit(); did the job! In my case I have a single instance application and it worked quite well simply by replacing Application.Exit(); with Application.Current.Shutdown(); What about clickonce deployed apps??  The only time I've run into this kind of issue is when in my main form I had a custom FormClosing event handler that performed logic and canceled the event. EDIT: I have now run into another instance and based on your comments it possibly mirrors what you were experiencing. When running a single instance application using a Mutex I was calling Application.Restart() from a fairly embedded location that had a lot of cleanup to do. So it seems the restart was launching a new instance before the previous instance was complete so the Mutex was keeping the new instance from starting.  Try locking before dumping. Here's how I initiate a full app-dump. Might work for you might not. Context.Application.Lock(); Context.Session.Abandon(); Context.Application.RemoveAll(); Context.Application.Restart(); Context.Application.UnLock(); I guess it didn't work for them. Nice of them to have left a comment if that was the case. why is this no good? well i'm trying it - will post back... Context is for aspx apps right?  There could be a lot of reasons for this. It's not that the method doesn't work; rather many times programmers forget that they've put something in their code that would stop the application from automatically shutting down or starting up. Two examples: The Closing event on a form can stop an app's shutdown If you're doing checking for an already-running process the old one may not be closing fast enough to allow the new one to start up. Check your code for gotchas like that. If you're seeing this behaviour within a blank application then that's more likely to be a problem with the actual function than your code. @MADMap: Your single-instance check is the problem. Your old app can't die and then start the new one because after it's dead it can't do anything. I AM using an SingleInstance-Application but I didn't think that this can cause the problem. I rather thought .NET is smart enough to let the Process close till the next instance will start."119,A,"Shorthand conditional in C# similar to SQL 'in' keyword In C# is there a shorthand way to write this: public static bool IsAllowed(int userID) { return (userID == Personnel.JohnDoe || userID == Personnel.JaneDoe ...); } Like: public static bool IsAllowed(int userID) { return (userID in Personnel.JohnDoe Personnel.JaneDoe ...); } I know I could also use switch but there are probably 50 or so functions like this I have to write (porting a classic ASP site over to ASP.NET) so I'd like to keep them as short as possible. A nice little trick is to sort of reverse the way you usually use .Contains() like:- public static bool IsAllowed(int userID) { return new int[] { Personnel.JaneDoe Personnel.JohnDoe }.Contains(userID); } Where you can put as many entries in the array as you like. If the Personnel.x is an enum you'd have some casting issues with this (and with the original code you posted) and in that case it'd be easier to use:- public static bool IsAllowed(int userID) { return Enum.IsDefined(typeof(Personnel) userID); }  How about this? public static class Extensions { public static bool In<T>(this T testValue params T[] values) { return values.Contains(testValue); } } Usage: Personnel userId = Personnel.JohnDoe; if (userId.In(Personnel.JohnDoe Personnel.JaneDoe)) { // Do something } I can't claim credit for this but I also can't remember where I saw it. So credit to you anonymous Internet stranger. this is gorgeous!  Just another syntax idea: return new [] { Personnel.JohnDoe Personnel.JaneDoe }.Contains(userID);  Are permissions user-id based? If so you may end up with a better solution by going to role based permissions. Or you may end up having to edit that method quite frequently to add additional users to the ""allowed users"" list. For example enum UserRole { User Administrator LordEmperor } class User { public UserRole Role{get; set;} public string Name {get; set;} public int UserId {get; set;} } public static bool IsAllowed(User user) { return user.Role == UserRole.LordEmperor; } I'd love to do this but we're in way too deep with legacy code unfortunately.  I would encapsulate the list of allowed IDs as data not code. Then it's source can be changed easily later on. List<int> allowedIDs = ...; public bool IsAllowed(int userID) { return allowedIDs.Contains(userID); } If using .NET 3.5 you can use IEnumerable instead of List thanks to extension methods. (This function shouldn't be static. See this posting: using too much static bad or good ?.)  Here's the closest that I can think of: using System.Linq; public static bool IsAllowed(int userID) { return new Personnel[] { Personnel.JohnDoe Personnel.JaneDoe }.Contains((Personnel)userID); }  How about something like this: public static bool IsAllowed(int userID) { List<int> IDs = new List<string> { 12345 }; return IDs.Contains(userID); } (You could of course change the static status initialize the IDs class in some other place use an IEnumerable<> etc based on your needs. The main point is that the closest equivalent to the in operator in SQL is the Collection.Contains() function.)  Can you write an iterator for Personnel. public static bool IsAllowed(int userID) { return (Personnel.Contains(userID)) } public bool Contains(int userID) : extends Personnel (i think that is how it is written) { foreach (int id in Personnel) if (id == userid) return true; return false; }"120,A,"How do I add FTP support to Eclipse? I'm using Eclipse PHP Development Tools. What would be the easiest way to access a file or maybe create a remote project trough FTP and maybe SSH and SFTP?. Nice question. I was looking a long time for this but I always just went with Dreamweaver in the end. Just a word of warning about Aptana - I downloaded it because of this question. It's a vast plug-in that does a lot more than just ftp-type things. As such it adds things all over your Eclipse installation - including advertisements for Adobe. There also seems to be no documented way of removing it. For me this was a disaster that totally messed up my Eclipse configuration. So be warned - know what you're getting into with Aptana. have you checked RSE (Remote System Explorer) ? I think it's pretty close to what you want to achieve. a blog post about it with screenshots Yep this work a little to much setting up but that's eclipse thanks!! Nice I like it! The best thing is that when you re-open eclipse it takes you right to the directory that you were working in when you last closed. For UTF-8 right click on any file or folder and select properties. In the info tab change file encoding to ""UTF-8"". It gets applied to all files and folders for that connection.  Eclipse natively supports FTP and SSH. Aptana is not necessary. Native FTP and SSH support in Eclipse is in the ""Remote System Explorer End-User Runtime"" Plugin. Install it through Eclipse itself. These instructions may vary slightly with your version of Eclipse: Go to 'Help' -> 'Install New Software' (in older Eclipses this is called something a bit different) In the 'Work with:' drop-down select your version's plugin release site. Example: for Kepler this is Kepler - http://download.eclipse.org/releases/kepler In the filter field type 'remote'. Check the box next to 'Remote System Explorer End-User Runtime' Click 'Next'. It should now download and install. After install Eclipse may want to restart. Using it in Eclipse: Window -> Open Perspective -> (perhaps select 'Other') -> Remote System Explorer File -> New -> Other -> Remote System Explorer (folder) -> Connection (or type Connection into the filter field) Choose FTP from the 'Select Remote System Type' panel. Fill in your FTP host info in the next panel (username and password come later). In the Remote Systems panel right-click the hostname and click 'connect'. Enter username + password and you're good! Well not exactly 'good'. The RSE system is fairly unusual but you're connected. And you're one smart cookie! You'll figure out the rest. Edit: To change the default port follow the instructions on this page: http://ikool.wordpress.com/2008/07/25/tips-to-access-ftpssh-on-different-ports-using-eclipse-rse/ This is Awesome so nice and so straightforward. Thanks @Rendall RSE's FTP support isn't the same luxury as Aptana used to have it bothers me that customizing eclipse always has to be this demotivating. Shush pumpkin it's okay. You'll get it the way you like it. Just keep trying.  As none of the other solutions mentioned satisfied me I wrote a script that uses WinSCP to sync local directories in a project to a FTP(S)/SFTP/SCP Server when eclipse's autobuild feature is triggered. Obviously this is a Windows-only solution. Maybe someone finds this useful: http://rays-blog.de/2012/05/05/94/use-winscp-to-upload-files-using-eclipses-autobuild-feature/  Install Aptana plugin to your Eclipse installation. It has built-in FTP support and it works excellently. You can: Edit files directly from the FTP server Perform file/folder management (copy delete move rename etc.) Upload/download files to/from FTP server Synchronize local files with FTP server. You can make several profiles (actually projects) for this so you won't have to reinput over and over again. As a matter of fact the FTP support is so good I'm using Aptana (or Eclipse + Aptana) now for all my FTP needs. Plus I get syntax highlighting/whatever coding support there is. Granted Eclipse is not the speediest app to launch but it doesn't bug me so much. Aptana looks pretty cool. I'm going to download the standalone version and give it a try... thanks Not really an FTP plug-in more like a web developing plugin with FRP feature. @Rendall answer suited me better! for those who read this post read the warning post too - Aptana will mess up your Eclipse (I know what it means to have your workspace ruined and I don't recommend that ;))  I'm not sure if this works for you but when I do small solo PHP projects with Eclipse the first thing I set up is an Ant script for deploying the project to a remote testing environment. I code away locally and whenever I want to test it I just hit the shortcut which updates the remote site. Eclipse has good Ant support out of the box and the scripts aren't hard to make. Even for solo projects I usually use Subversion this is just an odd case. I guess that deploying with ant is very similar than subversion.  SFTP Plug-in: http://www.jcraft.com/eclipse-sftp/ :)"121,A,"Number of possible combinations How many possible combinations of the variables abcde are possible if I know that: a+b+c+d+e = 500 and that they are all integers and >= 0 so I know they are finite. That's a neat thing to think about. Is this a homework question? No a coworker asked me about it because I studied some Probability but I couldn't solve it I assume we're talking integers only here? Yes integers only You should clarify the integer restriction in the question. Take CSE566 at Ohio State and you'll learn all about this and other counting things. One of my favorite classes. If there is no top bound it's clearly infinite. Even if a b and c are zero you can still just choose d = x and e = -(d - 500) for all values of x >= 500. why so many vote downs on the question? If one of them is greater than 500 then it's impossible for them to add 500 since none can be negative... I just might have to take Michael Haren's advice and walk on down the street to get the answer to this. ;) @Kent: Read the part when it says abcde are all greater than or equal to zero. Why all the downvotes? I see nothing wrong with puzzles. I think a lot of people misread the puzzle. The ""integer"" and "">=0"" requirements make the puzzle doable. Actually the ""integer"" and "">=0"" requirements make the puzzle difficult. Without either of those the answer is easy: infinity! With those constraints the answer is some finite number Does there have to be 5 numbers or are you interested in solutions with 2-4? I'm interested in the solution for 5 but if the other solutions can help me figure it out then they are welcome (the solution for 2 is easy though: 501) I gave a solution before for arbitrary numbers of addends and sums. Does the order of the variables matter? If it does then that needs to be taken into account in the solution. The order matters: The answer should count both 500+0+0+0+0 and 0+0+0+0+50 In my code below I count the order properly. That is all permutations. @Chris. I did read that bit. There is no top bound specified only a lower bound. Hence any integer can reach positive infinity. Hence there is an infinite number of permutations. Am I missing something? Ah sorry I see - my bad. @Chris Conway answer is correct. I have tested with a simple code that is suitable for smaller sums.  long counter = 0; int sum=25; for (int a = 0; a <= sum; a++) { for (int b = 0; b <= sum ; b++) { for (int c = 0; c <= sum; c++) { for (int d = 0; d <= sum; d++) { for (int e = 0; e <= sum; e++) { if ((a+b+c+d+e)==sum) counter=counter+1L; } } } } } System.out.println(""counter e ""+counter);  If they are a real numbers then infinite ... otherwise it is a bit trickier. (OK for any computer representation of a real number there would be a finite count ... but it would be big!) They are greater than 0 so not that big he also stated integers  The answer to your question is 2656615626. Here's the code that generates the answer: public static long getNumCombinations( int summands int sum ) { if ( summands <= 1 ) return 1; long combos = 0; for ( int a = 0 ; a <= sum ; a++ ) combos += getNumCombinations( summands-1 sum-a ); return combos; } In your case summands is 5 and sum is 500. Note that this code is slow. If you need speed cache the results from summandsum pairs. I'm assuming you want numbers >=0. If you want >0 replace the loop initialization with a = 1 and the loop condition with a < sum. I'm also assuming you want permutations (e.g. 1+2+3+4+5 plus 2+1+3+4+5 etc). You could change the for-loop if you wanted a >= b >= c >= d >= e.  This would actually be a good question to ask on an interview as it is simple enough that you could write up on a white board but complex enough that it might trip someone up if they don't think carefully enough about it. Also you can also for two different answers which cause the implementation to be quite different. Order Matters If the order matters then any solution needs to allow for zero to appear for any of the variables; thus the most straight forward solution would be as follows: public class Combos { public static void main() { long counter = 0; for (int a = 0; a <= 500; a++) { for (int b = 0; b <= (500 - a); b++) { for (int c = 0; c <= (500 - a - b); c++) { for (int d = 0; d <= (500 - a - b - c); d++) { counter++; } } } } System.out.println(counter); } } Which returns 2656615626. Order Does Not Matter If the order does not matter then the solution is not that much harder as you just need to make sure that zero isn't possible unless sum has already been found. public class Combos { public static void main() { long counter = 0; for (int a = 1; a <= 500; a++) { for (int b = (a != 500) ? 1 : 0; b <= (500 - a); b++) { for (int c = (a + b != 500) ? 1 : 0; c <= (500 - a - b); c++) { for (int d = (a + b + c != 500) ? 1 : 0; d <= (500 - a - b - c); d++) { counter++; } } } } System.out.println(counter); } } Which returns 2573155876.  I solved this problem for my dad a couple months ago...extend for your use. These tend to be one time problems so I didn't go for the most reusable... a+b+c+d = sum i = number of combinations  for (a=0;a<=sum;a++) { for (b = 0; b <= (sum - a); b++) { for (c = 0; c <= (sum - a - b); c++) { //d = sum - a - b - c; i++ } } }  @Torlack @Jason Cohen: Recursion is a bad idea here because there are ""overlapping subproblems."" I.e. If you choose a as 1 and b as 2 then you have 3 variables left that should add up to 497; you arrive at the same subproblem by choosing a as 2 and b as 1. (The number of such coincidences explodes as the numbers grow.) The traditional way to attack such a problem is dynamic programming: build a table bottom-up of the solutions to the sub-problems (starting with ""how many combinations of 1 variable add up to 0?"") then building up through iteration (the solution to ""how many combinations of n variables add up to k?"" is the sum of the solutions to ""how many combinations of n-1 variables add up to j?"" with 0 <= j <= k). public static long getCombos( int n int sum ) { // tab[i][j] is how many combinations of (i+1) vars add up to j long[][] tab = new long[n][sum+1]; // # of combos of 1 var for any sum is 1 for( int j=0; j < tab[0].length; ++j ) { tab[0][j] = 1; } for( int i=1; i < tab.length; ++i ) { for( int j=0; j < tab[i].length; ++j ) { // # combos of (i+1) vars adding up to j is the sum of the # // of combos of i vars adding up to k for all 0 <= k <= j // (choosing i vars forces the choice of the (i+1)st). tab[i][j] = 0; for( int k=0; k <= j; ++k ) { tab[i][j] += tab[i-1][k]; } } } return tab[n-1][sum]; }  $ time java Combos 2656615626 real 0m0.151s user 0m0.120s sys 0m0.012s @PiotrekDe ""Combinations"" is a bad choice of words copied from the questioner. tab[i][j] is the number of distinct assignments of non-negative integers to i+1 variables such that the variables sum to j. For tab[2][1] let the variables by x y z; the assignments are (x=0;y=0;z=1) (x=0;y=1;z=0) and (x=1;y=0;z=0). @Chris I'm having problems understanding the meaning of values in the array you build in your solution. Could you please give me some example? In your array tab[2][1]=3 and means ""how many combinations of (2+1) vars add up to 1? What are these combinations? (I assume that since they are combinations the order doesn't matter) You're right dynamic programming is best! In my code solution above I suggest using a cache table of pairs but there's a LOT of pairs so DP is better. However I tried to work out the DP solution and it was too hard for little ol' me. :-) Actually just summand * sum pairs so that's not so bad at 5 x 500.  It has general formulae if a + b + c + d = N Then number of non-negative integral solution will be C(N + number_of_variable - 1 N)  One way of looking at the problem is as follows: First a can be any value from 0 to 500. Then if follows that b+c+d+e = 500-a. This reduces the problem by one variable. Recurse until done. For example if a is 500 then b+c+d+e=0 which means that for the case of a = 500 there is only one combination of values for bcd and e. If a is 300 then b+c+d+e=200 which is in fact the same problem as the original problem just reduced by one variable. Note: As Chris points out this is a horrible way of actually trying to solve the problem. link text  Including negatives? Infinite. Including only positives? In this case they wouldn't be called ""integers"" but ""naturals"" instead. In this case... I can't really solve this I wish I could but my math is too rusty. There is probably some crazy integral way to solve this. I can give some pointers for the math skilled around. being x the end result the range of a would be from 0 to x the range of b would be from 0 to (x - a) the range of c would be from 0 to (x - a - b) and so forth until the e. The answer is the sum of all those possibilities. I am trying to find some more direct formula on Google but I am really low on my Google-Fu today..."122,A,"Have you used any of the C++ interpreters (not compilers)? I am curious if anyone have used UnderC Cint and Ch (or any other C++ interpreter) and could share their experience. @GeorgFritzsche This question is about C++ not C. cint is the command processor for the particle physics analysis package ROOT. I use it regularly and it works very well for me. It is fairly complete and gets on well with compiled code (you can load compiled modules for use in the interpreter...) late edit:: Copied from a later duplicate because the poster on that questions didn't seem to want to post here: igcc. Never tried it personally but the web page looks promising. @littlenag To say root ""meets [our] needs"" is a little generous. The framework is horribly flawed on several basic levels and only continues to see widespread use because it's completely non-modular and thus nearly impossible to remove. CINT is a prime example of something you _have to_ install when all you want to do is read in a data file. +1 for link to igcc. I know several graduate students in physics that do the majority of their coding in cint/root and while they don't always have nice things to say it meets their needs for performance and flexibility. Well it is c++ with an add layer of complexity from needing to build the interperter<-->binary-code dictionaries. Plus the root class tree is a pain. But cint works. It works a lot better than COMIS did in the cernlib days.  I have (about a year ago) played around with Ch and found it to be pretty good.  Also long ago I used a product call Instant C but I don't know that it ever developed further  There is cling Cern's project of C++ interpreter based on clang - it's new approach based on 20 years of experience in ROOT cint and it's quite stable and recommended by Cern guys. Here is nice Google Talk: Introducing cling a C++ Interpreter Based on clang/LLVM. Cling actually works by compiling interactively it's more of a JIT compiler than an interpreter. Also as a ""CERN guy"" I feel obliged to comment on ""recommended by CERN guys"": many of us would argue that the main lesson after 20 years of ROOT is that monolithic software (ROOT) based around a single language (C++) is a mistake. Cling is a good crutch for those who continue to warship C++ as the be-all-end-all of languages but we aren't all hobbling away from CINT on another C++ interpreter: for interpenetrated code you can do far better than C++ use Python or Ruby.  I looked at using ch a while back to see if I could use it for black box testing DLLs for which I am responsible. Unfortunately I couldn't quite figure out how to get it to load and execute functions from DLLs. Then again I wasn't that motivated and there may well be a way.  NOTE: what follows is rather CINT specific but given that its probably the most widely used C++ interpreter it may be valid for them all. As a graduate student in particle physics who's used CINT extensively I should warn you away. While it does ""work"" it is in the process of being phased out and those who spend more than a year in particle physics typically learn to avoid it for a few reasons: Because of its roots as a C interpretor it fails to interpret some of the most critical components of C++. Templates for example don't always work so you'll be discouraged from using things which make C++ so flexible and usable. It is slower (by at least a factor of 5) than minimally optimized C++. Debugging messages are much more cryptic than those produced by g++. Scoping is inconsistent with compiled C++: it's quite common to see code of the form if (energy > 30) { float correction = 2.4; } else { float correction = 6.3; } somevalue += correction; whereas any working C++ compiler would complain that correcton has gone out of scope CINT allows this. The result is that CINT code isn't really C++ just something that looks like it. In short CINT has none of the advantages of C++ and all the disadvantages plus some. The fact that CINT is still used at all is likely more of a historical accident owing to its inclusion in the ROOT framework. Back when it was written (20 years ago) there was a real need for an interpreted language for interactive plotting / fitting. Now there are many packages which fill that role many which have hundreds of active developers. None of these are written in C++. Why? Quite simply C++ is not meant to be interpreted. Static typing for example buys you great gains in optimization during compilation but mostly serves to clutter and over-constrain your code if the computer is only allowed to see it at runtime. If you have the luxury of being able to use an interpreted language learn Python or Ruby the time it takes you to learn will be less than that you loose stumbling over CINT even if you already know C++. In my experience the older researchers who work with ROOT (the package you must install to run CINT) end up compiling the ROOT libraries into normal C++ executables to avoid CINT. Those in the younger generation either follow this lead or use Python for scripting. Incidentally ROOT (and thus CINT) takes roughly half an hour to compile on a fairly modern computer and will occasionally fail with newer versions of gcc. It's a package that served an important purpose many years ago but now it's clearly showing it's age. Looking into the source code you'll find hundreds of deprecated c-style casts huge holes in type-safety and heavy use of global variables. If you're going to write C++ write C++ as it's meant to be written. If you absolutely must have a C++ interpretor CINT is probably a good bet. While all your complaints about cint are perfectly valid (and you missed a few) you can take my word that the COMIS interpreter for PAW was much worse. Also that PAW provided a adequate environment of interactive plotting---it just had the scaling problems you would expect from a fortran 77 style of coding. @dmckee Believe me I'm glad we're not working with PAW. My point isn't that CINT is worse than _everything_ only that there are many things which would be better.  There is a program called c-repl which works by repeatedly compiling your code into shared libraries using GCC then loading the resulting objects. It seems to be evolving rapidly considering the version in Ubuntu's repository is written in Ruby (not counting GCC of course) while the latest git is in Haskell. :)  Long ago I used a C++ interpreter called CodeCenter. It was pretty nice although it couldn't handle things like bitfields or fancy pointer mangling. The two cool things about it were that you could watch when variables changed and that you could evaluate C/C++ code on the fly while debugging. These days I think a debugger like GDB is basically just as good. What would the interpreter do when it encountered a template instance? (or other preprocessing business). Was there some level of precompilation/preprocessing to handle templates or the preprocessor? Yes all the CPP stuff and templates were all part of the language being interpreted. Pretty nice."123,A,"How to print css applied background images with WebBrowser control I am using the webbrowser control in winforms and discovered now that background images which I apply with css are not included in the printouts. Is there a way to make the webbrowser print the background of the displayed document too? Edit: Since I wanted to do this programatically I opted for this solution: using Microsoft.Win32; ... RegistryKey regKey = Registry.CurrentUser .OpenSubKey(""Software"") .OpenSubKey(""Microsoft"") .OpenSubKey(""Internet Explorer"") .OpenSubKey(""Main""); //Get the current setting so that we can revert it after printjob var defaultValue = regKey.GetValue(""Print_Background""); regKey.SetValue(""Print_Background"" ""yes""); //Do the printing //Revert the registry key to the original value regKey.SetValue(""Print_Background"" defaultValue); Another way to handle this might be to just read the value and notify the user to adjust this himself before printing. I have to agree that tweaking with the registry like this is not a good practice so I am open for any suggestions. Thanks for all your feedback Another registry key would be : HKEY_CURRENT_USER\Software\Microsoft\Internet Explorer\PageSetup\Print_Background HKEY_LOCAL_MACHINE\Software\Microsoft\Internet Explorer\PageSetup\Print_Background  If you're going to go and change an important system setting make sure to first read the current setting and restore it when you are done. I consider this very bad practice in the first place but if you must do it then be kind. Registry.LocalMachine Also try changing LocalUser instead of LocalMachine - that way if your app crashes (and it will) then you'll only confounded the user not everyone who uses the machine.  The corresponding HKCU key for this setting is: HKEY_CURRENT_USER\Software\Microsoft\Internet Explorer\Main\Print_Background  By default the browser does not print background images at all. In Firefox * File > Page Setup > Check Off ""Print Background"" * File > Print Preview In IE * Tools > Internet Options > Advanced > Printing * Check Off ""Print Background Images and Colors"" In Opera * File > Print Options > Check Off ""Print Page Background"" * File > Print Preview (You may have to scroll down/up to see it refresh)  var sh = new ActiveXObject(""WScript.Shell""); key = ""HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\Main\\Print_Background""; var defaultValue = sh.RegRead(key); sh.RegWrite(key""yes""""REG_SZ""); document.frames['detailFrame'].focus(); document.frames['detailFrame'].print(); sh.RegWrite(keydefaultValue""REG_SZ""); return false;"124,A,"How to late bind 32bit/64 bit libs at runtime I've got a problem similar tobut subtly different from that described here (Loading assemblies and their dependencies). I have a C++ DLL for 3D rendering that is what we sell to customers. For .NET users we will have a CLR wrapper around it. The C++ DLL can be built in both 32 and 64bit versions but I think this means we need to have two CLR wrappers since the CLR binds to a specific DLL? Say now our customer has a .NET app that can be either 32 or 64bit and that it being a pure .NET app it leaves the CLR to work it out from a single set of assemblies. The question is how can the app code dynamically choose between our 32 and 64bit CLR/DLL combinations at run-time? Even more specifically is the suggested answer to the aforementioned question applicable here too (i.e. create a ResolveEvent handler)? Thanks in advance. I was able to do this about a year ago but I no longer remember all of the details. Basically you can use IntPtr.Size to determine which DLL to load then perform the actual LoadLibrary through p/Invoke. At that point you've got the module in memory and you ought to be able to just p/Invoke functions from inside of it -- the same module name shouldn't get reloaded again. I think though that in my application I actually had the C++ DLL register itself as a COM server and then accessed its functionality through a generated .NET wrapper -- so I don't know if I ever tested p/Invoking directly.  I encountered a similar scenario a while back. A toolkit I was using did not behave well in a 64-bit environment and I wasn't able to find a way to dynamically force the assemblies to bind as 32 bit. It is possible to force your assemblies to work in 32 bit mode but this requires patching the CLR header (there is a tool that does that in the Framework) and if your assemblies are strongly-named this does not work out. I'm afraid you'll need to build and publish two sets of binaries for 32 and 64 bit platforms.  I finally have an answer for this that appears to work. Compile both 32 & 64 bit versions - both managed & unmanaged - into separate folders. Then have the .NET app choose at run time which directory to load the assemblies from. The problem with using the ResolveEvent is that it only gets called if assemblies aren't found so it is all to easy to accidentally end up with 32 bit versions. Instead use a second AppDomain object where we can change the ApplicationBase property to point at the right folder. So you end up with code like: static void Main(String[] argv) { // Create a new AppDomain but with the base directory set to either the 32-bit or 64-bit // sub-directories. AppDomainSetup objADS = new AppDomainSetup(); System.String assemblyDir = System.IO.Path.GetDirectoryName(Application.ExecutablePath); switch (System.IntPtr.Size) { case (4): assemblyDir += ""\\win32\\""; break; case (8): assemblyDir += ""\\x64\\""; break; } objADS.ApplicationBase = assemblyDir; // We set the PrivateBinPath to the application directory so that we can still // load the platform neutral assemblies from the app directory. objADS.PrivateBinPath = System.IO.Path.GetDirectoryName(Application.ExecutablePath); AppDomain objAD = AppDomain.CreateDomain("""" null objADS); if (argv.Length > 0) objAD.ExecuteAssembly(argv[0]); else objAD.ExecuteAssembly(""MyApplication.exe""); AppDomain.Unload(objAD); } You end up with 2 exes - your normal app and a second switching app that chooses which bits to load. Note - I can't take credit for the details of this myself. One of my colleagues sussed that out given my initial pointer. If and when he signs up to StackOverflow I'll assign the answer to him"125,A,"SMO and Sql Server 7.0 Does anyone have a definitive answer to whether Sql Server Management Objects is compatible with Sql Server 7.0? The docs state: Because SMO is compatible with SQL Server version 7.0 SQL Server 2000 SQL Server 2005 and SQL Server 2008 you easily manage a multi-version environment. But trying to connect to a Sql 7 instance gets me: ""This SQL Server version (7.0) is not supported."" Has anyone been successful in getting these 2 to play nice? Looks like the docs are wrong (and have continued to be wrong for the last 3+ years!). I found this snippet with Reflector in Microsoft.SqlServer.Management.Common.ConnectionManager Microsoft.SqlServer.ConnectionInfo protected void CheckServerVersion(ServerVersion version) { if ((version.Major <= 7 || (version.Major > 9)) { throw new ConnectionFailureException( StringConnectionInfo.ConnectToInvalidVersion(version.ToString()) ); } } So it looks like only SQL 2000 and SQL 2005 are supported. Presumably SQL 2008 (version 10) has updated SMO assemblies. Bummer - guess it's back to SQL-DMO for this project.  Sorry for the late answer... there is partial support for SQL 2000 and SQL 7  you can use SMO to connect to SQL Server versions 7 2000 and 2005 but SMO does not support databases set to compatibility levels 60 65 and 70. for SQL Server 7.0 the compatibility level is 70 Obviously this is conflicting information...I assume if your compatibility level of your DB is 70 you can not connect. To check run: EXEC sp_dbcmptlevel 'databasename' Looking through this link it seems you might be able to change the compatibility level by running this: EXEC sp_dbcmptlevel 'databasename' 80 Obviously make a back up before changing anything.  Just to follow up on your commment SQL 2008 does have its own SMO package which supports SQL 2000 2005 and 2008 which is actually definitively documented on their download page! And you're right you can't connect SQL 2005 SMO to SQL 2008. There are some nice updates updates in Version 10 of the SMO in that if you access properties that do not existing on the version of SQL that you are connect to you get a sensible ""This property is not available on this Version of SQL"" exception or words to that effect. Microsoft SQL Server 2008 Management Objects The SQL Server Management Objects (SMO) is a .NET Framework object model that enables software developers to create client-side applications to manage and administer SQL Server objects and services. This object model will work with SQL Server 2000 SQL Server 2005 and SQL Server 2008."126,A,"PHP Session Security What are some guidelines for maintaining responsible session security with PHP? There's information all over the web and it's about time it all landed in one place! If you you use session_set_save_handler() you can set your own session handler. For example you could store your sessions in the database. Refer to the php.net comments for examples of a database session handler. DB sessions are also good if you have multiple servers otherwise if you are using file based sessions you would need to make sure that each webserver had access to the same filesystem to read/write the sessions.  There are a couple of things to do in order to keep your session secure: Use SSL when authenticating users or performing sensitive operations. Regenerate the session id whenever the security level changes (such as logging in). You can even regenerate the session id every request if you wish. Have sessions time out Don't use register globals Store authentication details on the server. That is don't send details such as username in the cookie. Check the $_SERVER['HTTP_USER_AGENT']. This adds a small barrier to session hijacking. You can also check the IP address. But this causes problems for users that have changing IP address due to load balancing on multiple internet connections etc (which is the case in our environment here). Lock down access to the sessions on the file system or use custom session handling For sensitive operations consider requiring logged in users to provide their authenication details again Don't regenerate session on every request. It's susceptible to race conditions and you'll lose session sooner or later. @grom Doesn't Chrome change it's user-agent automatically when it upgrades silently in the background while the user is using the browser? In this way you are blocking out real users for no real good reason. Don't forget that enhanced usability is also enhanced security. Using SSL only for some operations is not enough unless you have separate sessions for encrypted and unencrypted traffic. If you use single session over HTTPS and HTTP attacker will steal it on first non-HTTPS request. I agree with porneL. Also for number 6 if an attacker has your session id wouldn't they also have access to your user agent? If you regenerate the session id then the session id that an attacker steals on a non-HTTPS request is useless. -1 the user agent is trivial to spoof. What you are describing wastes code and is not a security system. Damn i wish i could give you another -1 for use of ssl. At no point can the cookie be leaked over http thats laid out in OWASP A3. @The Rook it may be a trivial barrier (the attacker can capture a victim's user-agent using their own site) and relies on security through obscurity but it is still one extra barrier. If the User-Agent HTTP was to change during the session use it would be extremely suspicious and most likely an attack. I never said you can use it alone. If you combine it with the other techniques you have a much more secure site. @grom I think its like putting a piece of scotch tape across your door and saying it will prevent people from breaking in. @The Rook yes the User Agent can be spoofed. Its just one small little barrier. And what do you mean by at no point the cookie can be leaked over http. Yes it can be stolen. http is plain text. @grom the only barrier is the one in your mind you are stopping no attack. If you're checking the user agent you'll block all requests from IE8 users when they toggle compatibility mode. See the fun I had tracking down this problem in my own code: http://serverfault.com/questions/200018/http-302-problem-on-ie7. I'm taking the user agent check out because it's such a trivial thing to spoof as others have said.  One guideline is to call session_regenerate_id every time a session's security level changes. This helps prevent session hijacking.  I think one of the major problems (which is being addressed in PHP 6) is register_globals. Right now one of the standard methods used to avoid register_globals is to use the $_REQUEST $_GET or $_POST arrays. The ""correct"" way to do it (as of 5.2 although it's a little buggy there but stable as of 6 which is coming soon) is through filters. So instead of: $username = $_POST[""username""]; you would do: $username = filter_input(INPUT_POST 'username' FILTER_SANITIZE_STRING); or even just: $username = filter_input(INPUT_POST 'username'); This has no relation to the question at all. Really? Then why in the accepted answer do they mention not to use register globals? Wouldn't as far as most run-of-the-mill developers are concerned register globals and form variable handling fall under the umbrella of ""sessions"" even if it isn't technically part of the ""session"" object? I agree this does not *fully* answer the question but it is definitely PART of the answer to the question. Again this fleshes out a bullet point in the accepted answer ""Don't use register globals"". This tells what to do instead. -1 This does not answer the question.  This session fixation paper has very good pointers where attack may come. See also session fixation page at Wikipedia.  The main problem with PHP sessions and security (besides session hijacking) comes with what environment you are in. By default PHP stores the session data in a file in the OS's temp directory. Without any special thought or planning this is a world readable directory so all of your session information is public to anyone with access to the server. As for maintaining sessions over multiple servers. At that point it would be better to switch PHP to user handled sessions where it calls your provided functions to CRUD (create read update delete) the session data. At that point you could store the session information in a database or memcache like solution so that all application servers have access to the data. Storing your own sessions may also be advantageous if you are on a shared server because it will let you store it in the database which you often times have more control over then the filesystem.  My two (or more) cents: Trust no one Filter input escape output (cookie session data are your input too) Avoid XSS (keep your HTML well formed take a look at PHPTAL or HTMLPurifier) Defense in depth Do not expose data There is a tiny but good book on this topic: Essential PHP Security by Chris Shiflett. On the home page of the book you will find some interesting code examples and sample chapters. You may use technique mentioned above (IP & UserAgent) described here: How to avoid identity theft +1 for XSS-prevention. Without that it's impossible to protect against CSRF and thus somebody can ""ride"" the session without even getting the session ID.  Using IP address isn't really the best idea in my experience. For example; my office has two IP addresses that get used depending on load and we constantly run into issues using IP addresses. Instead I've opted for storing the sessions in a separate database for the domains on my servers. This way no one on the file system has access to that session info. This was really helpful with phpBB before 3.0 (they've since fixed this) but it's still a good idea I think.  This is pretty trivial and obvious but be sure to session_destroy after every use. This can be difficult to implement if the user does not log out explicitly so a timer can be set to do this. Here is a good tutorial on setTimer() and clearTimer().  I set my sessions up like this- on the log in page: $_SESSION['fingerprint'] = md5($_SERVER['HTTP_USER_AGENT'] . PHRASE . $_SERVER['REMOTE_ADDR']); (phrase defined on a config page) then on the header that is throughout the rest of the site: session_start(); if ($_SESSION['fingerprint'] != md5($_SERVER['HTTP_USER_AGENT'] . PHRASE . $_SERVER['REMOTE_ADDR'])) { session_destroy(); header('Location: http://website login page/'); exit(); }  You need to be sure the session data are safe. By looking at your php.ini or using phpinfo() you can find you session settings. session.savepath_ tells you where they are saved. Check the permission of the folder and of its parents. It shouldn't be public (/tmp) or be accessible by other websites on your shared server. Assuming you still want to use php session You can set php to use an other folder by changing session.savepath_ or save the data in the database by changing session.savehandler_ . You might be able to set session.savepath_ in your php.ini (some providers allow it) or for apache + modphp in a .htaccess file in your site root folder: phpvalue session.savepath ""/home/example.com/html/session"". You can also set it at run time with _sessionsavepath() . Check Chris Shiflett's tutorial or ZendSessionSaveHandler_DbTable to set and alternative session handler.  I would check both IP and User Agent to see if they change if ($_SESSION['user_agent'] != $_SERVER['HTTP_USER_AGENT']   || $_SESSION['user_ip'] != $_SERVER['REMOTE_ADDR']) {   //Something fishy is going on here? } Yep but what about users that had static IP eq GSM and is changed every half hour. So stored IP in Session + host name WHEN IP != REMOTE_ADDR check host and compare hostanmes eq. 12.12.12.holand.nl-> when is holand.nl == true. But some host had IP based hostname Then need compare mask 88.99.XX.XX @jasondavis There is a browser called Chrome. IP can legitimately change if user is behind load-balanced proxy farm. And user_agent can change every time a user upgrades their browser. @scotts I agree with the IP part but for the browser upgrade you would set the session when they login so I don't see how they would upgrade there browser without creating a new session once they login again. I believe the user_agent can also change when toggling between compatibly mode in IE8. It's also very easy to fake.  php.ini session.cookie_httponly = 1 change session name from default PHPSESSID eq Apache add header: X-XSS-Protection 1 Can you elaborate? httpd.conf -> Header set X-XSS-Protection ""1"" Be aware that `X-XSS-Protection` isn't really useful at all. In fact the protecting algorithm itself could actually be exploited making it worse than before."127,A,"Querying like Linq when you don't have Linq I have a project that I'm currently working on but it currently only supports the .net framework 2.0. I love linq but because of the framework version I can't use it. What I want isn't so much the ORM side of things but the ""queryability"" (is that even a word?) of Linq. So far the closest is llblgen but if there was something even lighter weight that could just do the querying for me that would be even better. I've also looked at NHibernate which looks like it could go close to doing what I want but it has a pretty steep learning curve and the mapping files don't get me overly excited. If anyone is aware of something that will give me a similar query interface to Linq (or even better how to get Linq to work on the .net 2.0 framework) I'd really like to hear about it. You might want to check out Subsonic. It is an ORM that uses an ActiveRecord pattern. I'm pretty sure most of its features work with the .NET Framework 2.0.  LinqBridge works fine under .NET 2.0 and you get all the Linq extensions and query language. You need VS 2008 in order to use it but you already knew that. However Linq it not an ORM. It's a query syntax. If you want to use Linq to query a database you will need .NET 3.5. That's because 2.0 does not provide the mechanism needed to convert Linq code to your favorite database query language. In other words if an ORM is what you need LinqBridge will not help you. You need to check out some of the other suggestions provided.  There's a way to reference LINQ in the .NET 2.0 Framework but I have to warn you that it might be against the terms of use/EULA of the framework: http://stackoverflow.com/questions/2138/linq-on-the-net-20-runtime#2146  Have a look at this: http://www.albahari.com/nutshell/linqbridge.html Linq is several different things and I'm not 100% sure which bits you want but the above might be useful in some way. If you don't already have a book on Linq (I guess you don't) then I found ""Linq In Action"" to be be good.  First of all. Getting linq itself to work on 2.0 is out of the question. Its possible but really not something to do outside a testing environment. The closest you can get in terms of the ORM/Dynamic Querying part of it is imho SubSonic which I'll recommend for anyone stuck in C# 2.0  LinqBridge looks like a pretty nice place to start since I have VS2008 I just need to compile and deploy to a .net 2.0 server. I've looked at SubSonic and it's also an interesting alternative but linqbridge seems to provide a much closer fit so I'm not going to have to go and learn a new ORM / query syntax. LinqBridge only gives us Linq syntax on .NET 2.0. We still need an ORM.  To echo what Lance said - the SubSonic query language has a fluent interface which isn't as pretty as LINQ but gives you some of the benefits (compile time checking intellisense etc.)."128,A,"Can you animate a custom dependency property in Silverlight? I might be missing something really obvious. I'm trying to write a custom Panel where the contents are laid out according to a couple of dependency properties (I'm assuming they have to be DPs because I want to be able to animate them.) However when I try to run a storyboard to animate both of these properties Silverlight throws a Catastophic Error. But if I try to animate just one of them it works fine. And if I try to animate one of my properties and a 'built-in' property (like Opacity) it also works. But if I try to animate both my custom properties I get the Catastrophic error. Anyone else come across this? edit: The two DPs are ScaleX and ScaleY - both doubles. They scale the X and Y position of children in the panel. Here's how one of them is defined:  public double ScaleX { get { return (double)GetValue(ScaleXProperty); } set { SetValue(ScaleXProperty value); } } /// <summary> /// Identifies the ScaleX dependency property. /// </summary> public static readonly DependencyProperty ScaleXProperty = DependencyProperty.Register( ""ScaleX"" typeof(double) typeof(MyPanel) new PropertyMetadata(OnScaleXPropertyChanged)); /// <summary> /// ScaleXProperty property changed handler. /// </summary> /// <param name=""d"">MyPanel that changed its ScaleX.</param> /// <param name=""e"">DependencyPropertyChangedEventArgs.</param> private static void OnScaleXPropertyChanged(DependencyObject d DependencyPropertyChangedEventArgs e) { MyPanel _MyPanel = d as MyPanel; if (_MyPanel != null) { _MyPanel.InvalidateArrange(); } } public static void SetScaleX(DependencyObject obj double val) { obj.SetValue(ScaleXProperty val); } public static double GetScaleX(DependencyObject obj) { return (double)obj.GetValue(ScaleXProperty); } Edit: I've tried it with and without the call to InvalidateArrange (which is absolutely necessary in any case) and the result is the same. The event handler doesn't even get called before the Catastrophic error kicks off. It's a documented bug with Silverlight 2 Beta 2. You can't animate two custom dependancy properties on the same object.  I would try commenting out the InvalidateArrange in the OnPropertyChanged and see what happens.  I hope it's not bad form to answer my own question. Silverlight 2 Release Candidate 0 was released today I've tested this problem on it and it appears to have been fixed. Both Custom DPs in my test panel can now be animated properly so the app is behaving as expected. Which is nice. Note that this RC is only a developer-based RC so the standard build of Silverlight hasn't been updated. I'd expect it to be fully released in the next month though."129,A,"Capturing a repeated group I am attempting to parse a string like the following using a .NET regular expression: H3Y5NC8E-TGA5B6SB-2NVAQ4E0 and return the following using Split: H3Y5NC8E TGA5B6SB 2NVAQ4E0 I validate each character against a specific character set (note that the letters 'I' 'O' 'U' & 'W' are absent) so using string.Split is not an option. The number of characters in each group can vary and the number of groups can also vary. I am using the following expression: ([ABCDEFGHJKLMNPQRSTVXYZ0123456789]{8}-?){3} This will match exactly 3 groups of 8 characters each. Any more or less will fail the match. This works insofar as it correctly matches the input. However when I use the Split method to extract each character group I just get the final group. RegexBuddy complains that I have repeated the capturing group itself and that I should put a capture group around the repeated group. However none of my attempts to do this achieve the desired result. I have been trying expressions like this: (([ABCDEFGHJKLMNPQRSTVXYZ0123456789]{8})-?){4} But this does not work. Since I generate the regex in code I could just expand it out by the number of groups but I was hoping for a more elegant solution. Please note that the character set does not include the entire alphabet. It is part of a product activation system. As such any characters that can be accidentally interpreted as numbers or other characters are removed. e.g. The letters 'I' 'O' 'U' & 'W' are not in the character set. The hyphens are optional since a user does not need top type them in but they can be there if the user as done a copy & paste. BTW you can replace [ABCDEFGHJKLMNPQRSTVXYZ0123456789] character class with a more readable subtracted character class. [[A-Z\d]-[IOUW]] If you just want to match 3 groups like that why don't you use this pattern 3 times in your regex and just use captured 1 2 3 subgroups to form the new string? ([[A-Z\d]-[IOUW]]){8}-([[A-Z\d]-[IOUW]]){8}-([[A-Z\d]-[IOUW]]){8} In PHP I would return (I don't know .NET) return ""$1 $2 $3"";  I have discovered the answer I was after. Here is my working code:  static void Main(string[] args) { string pattern = @""^\s*((?<group>[ABCDEFGHJKLMNPQRSTVXYZ0123456789]{8})-?){3}\s*$""; string input = ""H3Y5NC8E-TGA5B6SB-2NVAQ4E0""; Regex re = new Regex(pattern); Match m = re.Match(input); if (m.Success) foreach (Capture c in m.Groups[""group""].Captures) Console.WriteLine(c.Value); }  What are the defining characteristics of a valid block? We'd need to know that in order to really be helpful. My generic suggestion validate the charset in a first step then split and parse in a seperate method based on what you expect. If this is in a web site/app then you can use the ASP Regex validation on the front end then break it up on the back end.  After reviewing your question and the answers given I came up with this: RegexOptions options = RegexOptions.None; Regex regex = new Regex(@""([ABCDEFGHJKLMNPQRSTVXYZ0123456789]{8})"" options); string input = @""H3Y5NC8E-TGA5B6SB-2NVAQ4E0""; MatchCollection matches = regex.Matches(input); for (int i = 0; i != matches.Count; ++i) { string match = matches[i].Value; } Since the ""-"" is optional you don't need to include it. I am not sure what you was using the {4} at the end for? This will find the matches based on what you want then using the MatchCollection you can access each match to rebuild the string.  Why use Regex? If the groups are always split by a - can't you use Split()?  If you're just checking the value of the group with group(i).value then you will only get the last one. However if you want to enumerate over all the times that group was captured use group(2).captures(i).value as shown below. system.text.RegularExpressions.Regex.Match(""H3Y5NC8E-TGA5B6SB-2NVAQ4E0""""(([ABCDEFGHJKLMNPQRSTVXYZ0123456789]+)-?)*"").Groups(2).Captures(i).Value  Sorry if this isn't what you intended but your string always has the hyphen separating the groups then instead of using regex couldn't you use the String.Split() method? Dim stringArray As Array = someString.Split(""-"")  You can use this pattern: Regex.Split(""H3Y5NC8E-TGA5B6SB-2NVAQ4E0"" ""([ABCDEFGHJKLMNPQRSTVXYZ0123456789]{8}+)-?"") But you will need to filter out empty strings from resulting array. Citation from MSDN: If multiple matches are adjacent to one another an empty string is inserted into the array.  Mike You can use character set of your choice inside character group. All you need is to add ""+"" modifier to capture all groups. See my previous answer just change [A-Z0-9] to whatever you need (i.e. [ABCDEFGHJKLMNPQRSTVXYZ0123456789])"130,A,"Lightbox style dialogs in MFC App Has anyone implemented Lightbox style background dimming on a modal dialog box in a MFC/non .net app. I think the procedure would have to be something like: steps: Get dialog parent HWND or CWnd* Get the rect of the parent window and draw an overlay with a translucency over that window allow the dialog to do it's modal draw routine e.g DoModal() Are there any existing libraries/frameworks to do this or what's the best way to drop a translucent overlay in MFC? edit Here's a mockup of what i'm trying to achieve if you don't know what 'lightbox style' means Some App: with a lightbox dialog box Here's what I did* based on Brian's links First create a dialog resource with the properties: border FALSE 3D look FALSE client edge FALSE Popup style static edge FALSE Transparent TRUE Title bar FALSE and you should end up with a dialog window with no frame or anything just a grey box. override the Create function to look like this: BOOL LightBoxDlg::Create(UINT nIDTemplate CWnd* pParentWnd) { if(!CDialog::Create(nIDTemplate pParentWnd)) return false; RECT rect; RECT size; GetParent()->GetWindowRect(&rect); size.top = 0; size.left = 0; size.right = rect.right - rect.left; size.bottom = rect.bottom - rect.top; SetWindowPos(m_pParentWndrect.leftrect.topsize.rightsize.bottomNULL); HWND hWnd=m_hWnd; SetWindowLong (hWnd  GWL_EXSTYLE GetWindowLong (hWnd  GWL_EXSTYLE ) | WS_EX_LAYERED ) ; typedef DWORD (WINAPI *PSLWA)(HWND DWORD BYTE DWORD); PSLWA pSetLayeredWindowAttributes; HMODULE hDLL = LoadLibrary (_T(""user32"")); pSetLayeredWindowAttributes = (PSLWA) GetProcAddress(hDLL""SetLayeredWindowAttributes""); if (pSetLayeredWindowAttributes != NULL) { /* * Second parameter RGB(255255255) sets the colorkey * to white LWA_COLORKEY flag indicates that color key * is valid LWA_ALPHA indicates that ALphablend parameter * is valid - here 100 is used */ pSetLayeredWindowAttributes (hWnd RGB(255255255) 100 LWA_COLORKEY|LWA_ALPHA); } return true; } then create a small black bitmap in an image editor (say 48x48) and import it as a bitmap resource (in this example IDB_BITMAP1) override the WM_ERASEBKGND message with: BOOL LightBoxDlg::OnEraseBkgnd(CDC* pDC) { BOOL bRet = CDialog::OnEraseBkgnd(pDC); RECT rect; RECT size; m_pParentWnd->GetWindowRect(&rect); size.top = 0; size.left = 0; size.right = rect.right - rect.left; size.bottom = rect.bottom - rect.top; CBitmap cbmp; cbmp.LoadBitmapW(IDB_BITMAP1); BITMAP bmp; cbmp.GetBitmap(&bmp); CDC memDc; memDc.CreateCompatibleDC(pDC); memDc.SelectObject(&cbmp); pDC->StretchBlt(00size.rightsize.bottom&memDc00bmp.bmWidthbmp.bmHeightSRCCOPY); return bRet; } Instantiate it in the DoModal of the desired dialog Create it like a Modal Dialog i.e. on the stack(or heap if desired) call it's Create manually show it then create your actual modal dialog over the top of it: INT_PTR CAboutDlg::DoModal() { LightBoxDlg Dlg(m_pParentWnd);//make sure to pass in the parent of the new dialog Dlg.Create(LightBoxDlg::IDD); Dlg.ShowWindow(SW_SHOW); BOOL ret = CDialog::DoModal(); Dlg.ShowWindow(SW_HIDE); return ret; } and this results in something exactly like my mock up above *there are still places for improvment like doing it without making a dialog box to begin with and some other general tidyups. as I can't accept my own answer If anyone posts the optimisations i mention then they will get the answer awarded  I think you just need to create a window and set the transparency. There is an MFC CGlassDialog sample on CodeProject that might help you. There is also an article on how to do this with the Win32 APIs. this looks like what i'm looking for. I'll try it out and report back :)"131,A,"How to export findbugs results from Eclipse findbugs plugin? I have findbugs plugin for eclipse which when run on my project will show results in Bugs explorer clubbed by the type of bug. I need to be able to do two things: Export all these to excel sheet Find out the bugs reported in a set of files (and be able to do it recursively w/o running for whole project and exporting and finding out the classes to be modified. Any suggestions? FYI I am using MyEclipse v 6.0.1 and FindBugs 1.3.4 Findbugs dumps its results into an XML file in your workspace's .metadata folder. Look for the subfolder that's named something like findbugs. You can also download a standalone version of Findbugs that will save the results wherever you like. Once you have the results file you might be able to import from XML to Excel and filter there. Alternatively you can use XSLT to transform to several CSV files and open them in Excel.  I had the same problem with findbugs some versions ago. I updated the plugin today to version 1.3.8 and found out that you can now export and even import reports as XML directly. All you have to do is right-click on a project either in the package explorer or in the findbugs ""Bug explorer"" and select ""Findbugs->Save XML"" or ""Findbugs->Load XML"". Finely a proper export and import functionality for a fantastic tool. Btw I use Eclipse 3.3.2."132,A,"Alternatives to System.exit(1) For various reasons calling System.exit is frowned upon when writing Java Applications so how can I notify the calling process that not everything is going according to plan? Edit: The 1 is a standin for any non-zero exit code. I think throwing an exception is what you should do when something goes wrong. This way if your application is not running as a stand-alone app the caller can react to it and has some information about what went wrong. It is also easier for debugging purposes because you as well get a better idea about what went wrong when you see a stack trace. One important thing to note is that when the exception reaches the top level and therefore causes the VM to quit the VM returns a return code of 1 therefore outside applications that use the return code see that something went wrong. The only case where I think System.exit() makes sense is when your app is meant to be called by applications which are not Java and therefore have to use return codes to see if your app worked or not and you want those applications to have a chance to react differently on different things going wrong i.e. you need different return codes.  The use of System.exit is frowned upon when the 'application' is really a sub-application (e.g. servlet applet) of a larger Java application (server): in this case the System.exit could stop the JVM and hence also all other sub-applications. In this situation throwing an appropriate exception which could be caught and handled by the application framework/server is the best option. If the java application is really meant to be run as a standalone application there is nothing wrong with using System.exit. in this case setting an exit value is probably the easiest (and also most used) way of communicating failure or success to the parent process.  System.exit will block and will create a deadlock if the thread that initiates it is used in a shutdown hook.  Throwing exceptions is the best way to send information about a certain error up and out of the app. A number doesn't tell you as much as: Exception at thread 'main': FileNotFoundException ""The file 'foo' doesn't exist"" (or something close to that)  It can be dangerous / problematic in web servlet environments also. Throwing an Exception is generally considered the other alternative.  Our company's policy is that it's OK (even preferred) to call System.exit(-1) but only in init() methods. I would definitely think twice before calling it during a program's normal flow.  It's frowned upon for normal exits. If ""not everything is going according to plan"" then System.exit is fine. Update: I should add that I assume your '1' has meaning that is documented somewhere.  I agree with the ""throw an Exception"" crowd. One reason is that calling System.exit makes your code difficult to use if you want other code to be able to use it. For example if you find out that your class would be useful from a web app or some kind of message consuming app it would be nice to allow those containers the opportunity to deal with the failure somehow. A container may want to retry the operation decide to log and ignore the problem send an email to an administrator etc. An exception to this would be your main() method; this could trap the Exception and call System.exit() with some value that can be recognized by the calling process or shell script."133,A,"Identifying ASP.NET web service references At my day job we have load balanced web servers which talk to load balanced app servers via web services (and lately WCF). At any given time we have 4-6 different teams that have the ability to add new web sites or services or consume existing services. We probably have about 20-30 different web applications and corresponding services. Unfortunately given that we have no centralized control over this due to competing priorities org structures project timelines financial buckets etc. it is quite a mess. We have a variety of services that are reused but a bunch that are specific to a front-end. Ideally we would have better control over this situation and we are trying to get control over it but that is taking a while. One thing we would like to do is find out more about what all of the inter-relationships between web sites and the app servers. I have used Reflector to find dependencies among assemblies but would like to be able to see the traffic patterns between services. What are the options for trying to map out web service relationships? For the most part we are mainly talking about internal services (web to app app to app batch to app etc.). Off the top of my head I can think of two ways to approach it: Analyze assemblies for any web references. The drawback here is that not everything is a web reference and I'm not sure how WCF connections are listed. However this would at least be a start for finding 80% of the connections. Does anyone know of any tools that can do that analysis? Like I said I've used Reflector for assembly references but can't find anything for web references. Possibly tap into IIS and passively monitor the traffic coming in and out and somehow figure out what is being called and where from. We are looking at enterprise tools that could help but it would be a while before they are implemented (and cost a lot). But is there anything out there that could help out quickly and cheaply? One tool in particular (AmberPoint) can tap into IIS on the servers and monitor inbound and outbound traffic adds a little special sauce and begin to build a map of the traffic. Very nice but costs a bundle. I know I know how the heck did you get into this mess in the first place? Beats me just trying to help us get control of it and get out of it. Thanks Matt The easiest way is to look through the logs but if that doesn't include the referrer than you may also want to monitor what is going out from your web to the app server. You can use tools like Wireshark or Microsoft Network Monitor to see this traffic. The other ""solution"" and I use this loosely is to bind a specific web server to app server and then run through a bundle and see what it is hitting on the app server. You could probably do this in a test environment to lesson the effects on the users of the site.  You need a service registry (UDDI??)... If you had a means to catalog these services and their consumers it would make this job of dependency discovery a lot easier. That is not an easy solution though. It takes time and documentation to get a catalog in place. I think the quickest solution would be to query your IIS logs and find source URLs which originate from your own servers. You would at least be able to track down which servers your consumers are coming from. Also if you already have some kind of authentication mechanism in place you could trace who is using a particular service based on login. You are right about AmberPoint. There are other tools that catalog the service traffic and provide reports showing what is happening to your services. Systinet SOA Software and Actional also has a products similar to Amberpoint but Amberpoint has a free-ware version I believe."134,A,"Cheat single inheritance in Java? I have heard there is a way to cheat single inheritance and implement multiple inheritance in Java. Does anyone know how to implement this(with out using interface)? Just out of curiosity ;-) Its a very different thought. I was thinking about this a little more and realised that while dynamic proxies will work (it's how RMI (used?) to work) if you really want this sort of functionality you would be better off looking at aspect oriented programming (AOP) using something like AspectJ (eclipse.org/aspectj). This way you can get several different aspects into a class giving you pseudo mixin inheritance without the hideously fragile inheritance heirarchies. As everyone else has pointed out wanting/needing multiple inheritance generally indicates you aren't approaching the problem from the right perspective. Remember the GoF principle of ""prefer composition over inheritance"" for a start!  Use interfaces. You can implement as many as you'd like. You can usually use some variant on the Composite Pattern (GoF) to be able to reuse implementation code if that's desirable.  JAVA doesn't support multiple Inheritence. You can get it to implement multiple interfaces and some see this as a way round the problem. Personally I have yet to use multiple inheritence so I can't really understand its appeal. Normally when someone suggests multiple inheritence within c# or JAVA its due to the fact that 'they could' in c++. Im a fan of 'just because you can doens't mean you should'. As c# & JAVA doesn't support it why try and force it to do something it wasn't designed to do. This is not to say that there are unique cases where it is a valid technique to empoly just the code can usually be refactored to not need it. You probably mean it doesn't support *multiple* inheritance. Trait inheritance (essentially mixin inheritance) in Scala shows a few ways in which multiple inheritance can be useful although Java's choice not to implement it is understandable as many implementations end up being needlessly complex to reason about.  Use of composition instead of inheritance tends to be the way around this. This actually also helps a lot with testability so it's good practice in general. If you just want your type to ""behave"" like several other types you can inherit from as many interfaces as you like though; you can't ""borrow"" implementation details from these though obviously.  SingleMultiple inheritance is not supported by Java instead it has got interfaces to serve the same purpose. In case you are adamant on using multiple inheritance it should be done in C++.  You need to be careful to distinguish interface inheritance (essentially inheritance of a contract to provide particular facilities) from implementation inheritance (inheritance of implementation mechanisms). Java provides interface inheritance by the implements mechanism and you can have multiple interface inheritance. Implementation inheritance is the extends mechanism and you've only got a single version of that. Do you really need multiple implementation inheritance? I bet you don't it's chock full of unpleasant consequences unless you're an Eiffel programmer anyway.  You can cheat it a little (and I stress a little) by using java.lang.reflect.Proxy instances. This really just allows you to add extra interfaces and delegate their calls to another instance at runtime. As someone who mentors and tutors new developers I would be horrified if somebody showed me code that did this. Reflection is one of those tools that you really need to understand and have a good understanding of Java before jumping in. I personally have only ever done this once and it was to make some code I didn't have control over implement some interfaces some other code I had no control over was expecting (it was a quick hack so I didn't have to write and maintain too much glue code). Yeah this is a ""neat trick"" but it's not something that anyone should really use in production code! but to use Proxy you need interfaces. OP: ""...with out using interface...""  By using Inner Classes this is what C++ sometimes prefers as well: Inner Class Idiom.  I believe that the fundamental reason that Java doesn't support multiple inheritance is the same as C#; all objects are ultimately derived from Object and it's having multiple paths to the same base class is ambiguous for the compiler. Ambiguous == Bad so the compiler doesn't allow it. Instead you can simulate multiple inheritance through delegation. See this article for an example. Actually sharing Object is not ambiguous for the compiler. It only gets ambiguous when methods are defined in different classes. So if one branch overrides hashcode and the other doesn't. In those cases the programmer will have to make a choice.  There was an effort to bring mixins into Java. Check this link out: http://www.disi.unige.it/person/LagorioG/jam/  Sure you can but it's tricky and you should really consider if that's the way you want to go. The idea is to use scope-based inheritance coupled with type-based one. Which is type-talk for saying that for internal purposes inner classes ""inherit"" methods and fields of the outer class. It's a bit like mixins where the outer class is mixed-in to the inner class but not as safe as you can change the state of the outer class as well as use its methods. Gilad Bracha (one of the main java language designers) have a paper discussing that. So suppose you want to share some methods for internal use between some unrelated classes (e.g for string manipulation) you can create sub classes of them as inner classes of a class that has all the needed methods and the sub classes could use methods both from their super classes and from the outer class. Anyway it's tricky for complex classes and you could get most of the functionality using static imports (from java 5 on). Great question for job interviews and pub quizzes though ;-) This is what I really needed..using inner classes..thanks there's a minor typo here  I'd fix it but I don't have enough rep yet. 'create sub classes of THEN as'  You could probably ""simulate"" it by managing the set of superclasses explicitly and using reflection to search all the superclasses for the target method. I wouldn't want to do this in production but it might an interesting toy program. You could probably do a lot of weird stuff by leveraging reflection creating classes on the fly and invoking the compiler programatically."135,A,"PHP/mySQL - regular recalcuation of benchmark values as new users submit their data I am confronted with a new kind of problem which I haven't encountered yet in my very young programming ""career"" and would like to know your opinion about how to tackle it best. The situation A research application (php/mysql) gathers stress related health data from users. User gets a an analyses after filling in the questionnaire. Value for each parameter is transformed into a percentile value using a benchmark (mean and standard devitation of existing data set). The task Since more and more ppl are filling in the questionnaire there is the potential to make the benchmark values (mean/SD) more accurate by recalculating them using the new user data. I would like the database to regularly run a script that updates the benchmark values. The question I've never used stored precedures so far and I only have a slight notion of what they are but somehow I have a feeling they could maybe help me with this? Or should I write the script as php and then set up a cron job? [edit]After the first couple of answers it looks like cron is clearly the way to go.[/edit] What you're considering could be done in a number of ways. You could setup a trigger in your DB to recalculate the values whenever a new record is updated. You could store the code needed to update the values in a sproc if necessary. You could write a PHP script and run it regularly via cron. #1 will slow down inserts to your database but will make sure your data is always up to date. #2 may lock the tables while it updates the new values and your data will only be accurate until the next update. #2 is much easier to back up as the script can easily be stored in your versioning system whereas you'd need to store the trigger and sproc creation scripts in whatever backup you'd make. Obviously you'll have to weigh up your requirements before you pick a method.  The easiest way to make this work is probably to write a script in the same language your website is using (sounds like PHP) and call it from cron. No need to make it more complicated than it needs to be by putting the logic in two places (your existing calculations and a stored procedure).  Go with the cron job way. Simple solid works. In the PHP/MySQL world I would say stored procedures are no-go.  PHP set up as a cron job lets you keep it in your source code management system and if you're using a database abstraction layer it'll be portable to other databases if you ever decide to switch. For those reasons I tend to go with scripts over stored procedures.  If the volume of data is big enough that calculating it on the fly is too much then either: Cron job with php script to denormalise the totals Trigger on inserts that increments totals"136,A,"Which ORM framework can best handle an MVCC database design? When designing a database to use MVCC (Multi-Version Concurrency Control) you create tables with either a boolean field like ""IsLatest"" or an integer ""VersionId"" and you never do any updates you only insert new records when things change. MVCC gives you automatic auditing for applications that require a detailed history and it also relieves pressure on the database with regards to update locks. The cons are that it makes your data size much bigger and slows down selects due to the extra clause necessary to get the latest version. It also makes foreign keys more complicated. (Note that I'm not talking about the native MVCC support in RDBMSs like SQL Server's snapshot isolation level) This has been discussed in other posts here on Stack Overflow. [todo - links] I am wondering which of the prevalent entity/ORM frameworks (Linq to Sql ADO.NET EF Hibernate etc) can cleanly support this type of design? This is a major change to the typical ActiveRecord design pattern so I'm not sure if the majority of tools that are out there could help someone who decides to go this route with their data model. I'm particularly interested in how foreign keys would be handled because I'm not even sure of the best way to data model them to support MVCC. To the best of my knowledge ORM frameworks are going to want to generate the CRUD code for you so they would have to be explicitly designed to implement a MVCC option; I don't know of any that do so out of the box. From an Entity framework standpoint CSLA doesn't implement persistence for you at all -- it just defines a ""Data Adapter"" interface that you use to implement whatever persistence you need. So you could set up code generation (CodeSmith etc.) templates to auto-generate CRUD logic for your CSLA entities that go along with a MVCC database architecture. This approach would work with any entity framework most likely not just CSLA but it would be a very ""clean"" implementation in CSLA.  Check out the Envers project - works nice with JPA/Hibernate applications and basically does that for you - keeps track of different versions of each Entity in another table and gives you SVN-like possibilities (""Gimme the version of Person being used 2008-11-05..."") http://www.jboss.org/envers/ /Jens Currently envers is integrated with hibernate  I might consider implementing the MVCC tier purely in the DB using stored procs and views to handle my data operations. Then you could present a reasonable API to any ORM that was capable of mapping to and from stored procs and you could let the DB deal with the data integrity issues (since it's pretty much build for that). If you went this way you might want to look at a more pure Mapping solution like IBatis or IBatis.net.  I always figured you'd use a db trigger on update and delete to push those rows out into a TableName_Audit table. That'd work with ORMs give you your history and wouldn't decimate select performance on that table. Is that a good idea or am I missing something?  I designed a database similarly (only INSERTs  no UPDATEs no DELETEs). Almost all of my SELECT queries were against views of only the current rows for each table (highest revision number). The views looked like this_ SELECT dbo.tblBook.BookId dbo.tblBook.RevisionId dbo.tblBook.Title dbo.tblBook.AuthorId dbo.tblBook.Price dbo.tblBook.Deleted FROM dbo.tblBook INNER JOIN ( SELECT BookId MAX(RevisionId) AS RevisionId FROM dbo.tblBook GROUP BY BookId ) AS CurrentBookRevision ON dbo.tblBook.BookId = CurrentBookRevision.BookId AND dbo.tblBook.RevisionId = CurrentBookRevision.RevisionId WHERE dbo.tblBook.Deleted = 0 And my inserts (and updates and deletes) were all handled by stored procedures (one per table). The stored procedures looked like this_ ALTER procedure [dbo].[sp_Book_CreateUpdateDelete] @BookId uniqueidentifier @RevisionId bigint @Title varchar(256) @AuthorId uniqueidentifier @Price smallmoney @Deleted bit as insert into tblBook ( BookId RevisionId Title AuthorId Price Deleted ) values ( @BookId @RevisionId @Title @AuthorId @Price @Deleted ) Revision numbers were handled per-transaction in the Visual Basic code_ Shared Sub Save(ByVal UserId As Guid ByVal Explanation As String ByVal Commands As Collections.Generic.Queue(Of SqlCommand)) Dim Connection As SqlConnection = New SqlConnection(System.Configuration.ConfigurationManager.ConnectionStrings(""Connection"").ConnectionString) Connection.Open() Dim Transaction As SqlTransaction = Connection.BeginTransaction Try Dim RevisionId As Integer = Nothing Dim RevisionCommand As SqlCommand = New SqlCommand(""sp_Revision_Create"" Connection) RevisionCommand.CommandType = CommandType.StoredProcedure RevisionCommand.Parameters.AddWithValue(""@RevisionId"" 0) RevisionCommand.Parameters(0).SqlDbType = SqlDbType.BigInt RevisionCommand.Parameters(0).Direction = ParameterDirection.Output RevisionCommand.Parameters.AddWithValue(""@UserId"" UserId) RevisionCommand.Parameters.AddWithValue(""@Explanation"" Explanation) RevisionCommand.Transaction = Transaction LogDatabaseActivity(RevisionCommand) If RevisionCommand.ExecuteNonQuery() = 1 Then 'rows inserted RevisionId = CInt(RevisionCommand.Parameters(0).Value) 'generated key Else Throw New Exception(""Zero rows affected."") End If For Each Command As SqlCommand In Commands Command.Connection = Connection Command.Transaction = Transaction Command.CommandType = CommandType.StoredProcedure Command.Parameters.AddWithValue(""@RevisionId"" RevisionId) LogDatabaseActivity(Command) If Command.ExecuteNonQuery() < 1 Then 'rows inserted Throw New Exception(""Zero rows affected."") End If Next Transaction.Commit() Catch ex As Exception Transaction.Rollback() Throw New Exception(""Rolled back transaction"" ex) Finally Connection.Close() End Try End Sub I created an object for each table each with constructors instance properties and methods create-update-delete commands a bunch of finder functions and IComparable sorting functions. It was a huge amount of code. One-to-one DB table to VB object... Public Class Book Implements iComparable #Region "" Constructors "" Private _BookId As Guid Private _RevisionId As Integer Private _Title As String Private _AuthorId As Guid Private _Price As Decimal Private _Deleted As Boolean ... Sub New(ByVal BookRow As DataRow) Try _BookId = New Guid(BookRow(""BookId"").ToString) _RevisionId = CInt(BookRow(""RevisionId"")) _Title = CStr(BookRow(""Title"")) _AuthorId = New Guid(BookRow(""AuthorId"").ToString) _Price = CDec(BookRow(""Price"")) Catch ex As Exception 'TO DO: log exception Throw New Exception(""DataRow does not contain valid Book data."" ex) End Try End Sub #End Region ... #Region "" Create Update & Delete "" Function Save() As SqlCommand If _BookId = Guid.Empty Then _BookId = Guid.NewGuid() End If Dim Command As SqlCommand = New SqlCommand(""sp_Book_CreateUpdateDelete"") Command.Parameters.AddWithValue(""@BookId"" _BookId) Command.Parameters.AddWithValue(""@Title"" _Title) Command.Parameters.AddWithValue(""@AuthorId"" _AuthorId) Command.Parameters.AddWithValue(""@Price"" _Price) Command.Parameters.AddWithValue(""@Deleted"" _Deleted) Return Command End Function Shared Function Delete(ByVal BookId As Guid) As SqlCommand Dim Doomed As Book = FindByBookId(BookId) Doomed.Deleted = True Return Doomed.Save() End Function ... #End Region ... #Region "" Finders "" Shared Function FindByBookId(ByVal BookId As Guid Optional ByVal TryDeleted As Boolean = False) As Book Dim Command As SqlCommand If TryDeleted Then Command = New SqlCommand(""sp_Book_FindByBookIdTryDeleted"") Else Command = New SqlCommand(""sp_Book_FindByBookId"") End If Command.Parameters.AddWithValue(""@BookId"" BookId) If Database.Find(Command).Rows.Count > 0 Then Return New Book(Database.Find(Command).Rows(0)) Else Return Nothing End If End Function Such a system preserves all past versions of each row but can be a real pain to manage. PROS: Total history preserved Fewer stored procedures CONS: relies on non-database application for data integrity huge amount of code to be written No foreign keys managed within database (goodbye automatic Linq-to-SQL-style object generation) I still haven't come up with a good user interface to retrieve all that preserved past versioning. CONCLUSION: I wouldn't go to such trouble on a new project without some easy-to-use out-of-the-box ORM solution. I'm curious if the Microsoft Entity Framework can handle such database designs well. Jeff and the rest of that Stack Overflow team must have had to deal with similar issues while developing Stack Overflow: Past revisions of edited questions and answers are saved and retrievable. I believe Jeff has stated that his team used Linq to SQL and MS SQL Server. I wonder how they handled these issues.  What we do is just use a normal ORM ( hibernate ) and handle the MVCC with views + instead of triggers. So there is a v_emp view which just looks like a normal table you can insert and update into it fine when you do this though the triggers handle actually inserting the correct data into the base table. Not.. I hate this method :) I'd go with a stored procedure API as suggested by Tim."137,A,What is the best way to setup an integration testing server? Setting up an integration server Im in doubt about the best approach regarding using multiple tasks to complete the build. Is the best way to set all in just one big-job or make small dependent ones? You definitely want to break up the tasks. Here is a nice example of CruiseControl.NET configuration that has different targets (tasks) for each step. It also uses a common.build file which can be shared among projects with little customization. http://code.google.com/p/dot-net-reference-app/source/browse/#svn/trunk  The approach I favour is the following setup (Actually assuming you are in a .NET project): CruiseControl.NET. NANT tasks for each individual step. Nant.Contrib for alternative CC templates. NUnit to run unit tests. NCover to perform code coverage. FXCop for static analysis reports. Subversion for source control. CCTray or similar on all dev boxes to get notification of builds and failures etc. On many projects you find that there are different levels of tests and activities which take place when someone does a checkin. Sometimes these can increase in time to the point where it can be a long time after a build before a dev can see if they have broken the build with a checkin. What I do in these cases is create three builds (or maybe two): A CI build is triggered by checkin and does a clean SVN Get Build and runs lightweight tests. Ideally you can keep this down to minutes or less. A more comprehensive build which could be hourly (if changes) which does the same as the CI but runs more comprehensive and time consuming tests. An overnight build which does everything and also runs code coverage and static analysis of the assemblies and runs any deployment steps to build daily MSI packages etc. The key thing about any CI system is that it needs to be organic and constantly being tweaked. There are some great extensions to CruiseControl.NET which log and chart build timings etc for the steps and let you do historical analysis and so allow you to continously tweak the builds to keep them snappy. It's something that managers find hard to accept that a build box will probably keep you busy for a fifth of your working time just to stop it grinding to a halt.  I use TeamCity with an nant build script. TeamCity makes it easy to setup the CI server part and nant build script makes it easy to do a number of tasks as far as report generation is concerned. Here is an article I wrote about using CI with CruiseControl.NET it has a nant build script in the comments that can be re-used across projects: Continuous Integration with CruiseControl  We use buildbot with the build broken down into discrete steps. There is a balance to be found between having build steps be broken down with enough granularity and being a complete unit. For example at my current position we build the sub-pieces for each of our platforms (Mac Linux Windows) on their respective platforms. We then have a single step (with a few sub steps) that compiles them into the final version that will end up in the final distributions. If something goes wrong in any of those steps it is pretty easy to diagnose. My advice is to write the steps out on a whiteboard in as vague terms as you can and then base your steps on that. In my case that would be: Build Plugin Pieces Compile for Mac Compile for PC Compile for Linux Make final Plugins Run Plugin tests Build intermediate IDE (We have to bootstrap building) Build final IDE Run IDE tests  Break your tasks up into discrete goal/operations then use a higher-level script to tie them all together appropriately. This makes your build process easier to understand for other people (you're documenting as you go so anyone on your team can pick it up right?) as well as increasing the potential for re-use. It's likely you won't reuse the high-level scripts (although this could be possible if you have similar projects) but you can definitely reuse (even if it's copy/paste) the discrete operations rather easily. Consider the example of getting the latest source from your repository. You'll want to group the tasks/operations for retrieving the code with some logging statements and reference the appropriate account information. This is the sort of thing that's very easy to reuse from one project to the next. For my team's environment we use NAnt since it provides a common scripting environment between dev machines (where we write/debug the scripts) and the CI server (since we just execute the same scripts in a clean environment). We use Jenkins to manage our builds but at their core each project is just calling into the same NAnt scripts and then we manipulate the results (ie archive the build output flag failing tests etc).  I would definitely break down the jobs. Chances are you're likely to make changes in the builds and it'll be easier to track down issues if you have smaller tasks instead of searching through one monolithic build. You should be able to create one big job from the smaller pieces anyways.  G'day As you're talking about integration testing my big (obvious) tip would be to make the test server built and configured as close as possible to the deployment environment as possible. </thebloodyobvious> (-: cheers Rob138,A,"What are the most important things to learn about .net as a Project Manager? Thinking about getting into .net technology project management I've had plenty of experience with PHP projects: I'm aware of most of the existing frameworks and libraries and I've written specs and case studies based on this knowledge. What should I know about .net? Which top resources would you recommend me to know so I can rapidly learn and later stay up to date on the technology? Edit (8.24.08): The answers I got so far essentially discuss being a good PM. Thanks but this is not what I meant. Any .net essentials would be appreciated. This question appears to be off-topic because project management questions are no longer on-topic. See http://pm.stackexchange.com. The number one rule is do NOT just ask for status updates. It is Especially annoying when phrases like ""where are we on this?"" are used. If you aren't directly involved in the details then just make sure you have established communication times or plans so that you know whats going on rather than asking for updates.  The #1 thing you need to be aware of (and I'm guessing you probably already are) is that the guys doing the coding should know what they are doing. Depending on the personailties of the members of your team you should be able to find someone who is willing and able to explain any of the intricacies to you on an as-required basis. In my experience the biggest hinderence to a project is the PM who understands the project but not how to accomplish it (not in itself a problem) but who is also unwilling to listen to what his team tell him. As with any project management accept that you can't know everything and be humble enough to ask for explanations where needed.  Start with the basics before you get to the higher level stuff like web services (though that is important too). The most important things you need to learn as a project manager are the things you're going to be questioning your underlings about later. For example my PM (also a PHP guy) has absolutely no knowledge of garbage collection and its implications which makes it incredibly difficult for me to explain to him why our .NET Windows service appears to be taking 80MB of RAM. Remember you are not the one who needs to know everything. You should be issuing overarching directives and let the people with the expertise sort out the details. That said study up on the technicals a bit so that they can communicate effectively with you. Edit (8/24/08):You should know something about the underlying technicals; not necessarily all .NET stuff either (garbage collection .config files pipes and services if you're running services adjacent to your project's main focus stuff like that). Higher-reaching concepts would probably include WPF (maybe Silverlight as well) LINQ (or your ORM of choice) as well as the Vista bridge and related bridging code if your project includes desktop apps at all. Those three things seem to be the focus for this round of .NET. Something else that's very important to have at least a passing knowledge of is the ways that .NET code can/must interoperate with native code: P/Invoke Runtime Callable Wrapping and COM Callable Wrapping. There are still a lot of native things that don't have a .NET equivalent. As for resources I'd highly recommend MSDN Magazine. They tend to preview upcoming technologies and tools well before average developers will ever see them.  This may be old but should get your started on the high-level overview of the .NET Framework. http://news.zdnet.co.uk/software/01000000121213420700.htm  The biggest thing you'll probably want to learn is the differences between Windows and non-Windows programmers. They approach fundamental things differently. Knowing the difference will be key to successfully managing the project. If you listen to the stack overflow podcast and Jeff and Joel have multiple discussions on this topic. Understanding the details of the underlying technology is mostly irrelevant and you'll never know it well enough to go toe to toe with someone who works in it day in and day out. You can probably pick it up as you go."139,A,"How can I catch all types of exceptions in one catch block? In C++ I'm trying to catch all types of exceptions in one catch (like catch(Exception) in C#). How is it done? And what's more how can one catch divide-by-zero exceptions? If catching all exceptions - including OS ones - is really what you need you need to take a look at your compiler and OS. For example on Windows you probably have ""__try"" keyword or compiler switch to make ""try/catch"" catch SEH exceptions or both. ebel gil : thank you your answer helped a lot !  You can of course use catch (...) { /* code here */ } but it really Depends On What You Want To Do. In C++ you have deterministic destructors (none of that finalisation rubbish) so if you want to mop up the correct thing to do is to use RAII. For example. instead of: void myfunc() { void* h = get_handle_that_must_be_released(); try { random_func(h); } catch (...) { release_object(h); throw; } release_object(h); } Do something like: #include<boost/shared_ptr.hpp> void my_func() { boost::shared_ptr<void> h(get_handle_that_must_be_released() release_object); random_func(h.get()); } Create your own class with a destructor if you don't use boost.  If you are on windows and need to handle errors like divide by zero and access violation you can use a structured exception translator. And then inside of your translator you can throw a c++ exception: void myTranslator(unsigned code EXCEPTION_POINTERS*) { throw std::exception(<appropriate string here>); } _set_se_translator(myTranslator); Note the code will tell you what the error was. Also you need to compile with the /EHa option (C/C++ -> Code Generatrion -> Enable C/C++ Exceptions = Yes with SEH Exceptions). If that doesn't make sense checkout the docs for _set_se_translator  If I recall correctly (it's been a while since I've looked at C++) I think the following should do the trick try { // some code } catch(...) { // catch anything } and a quick google(http://www.oreillynet.com/pub/a/network/2003/05/05/cpluspocketref.html) seems to prove me correct. potentially unsafe in Windows and likely others. It snags access violations and other Really Bad occurrences.  Make all your custom exception classes inherit from std::exception then you can simply catch std::exception. Here is some example code: class WidgetError : public std::exception { public: WidgetError() { } virtual ~WidgetError() throw() { } virtual const char *what() const throw() { return ""You got you a widget error!""; } }; this is good practice in order to reduce gobs of catch blocks. As others have noted catch(...) is usually unsafe. std::exception is a little messy when compiling under unicode. (because what() returns a char string not a wchar_t string)  You don't want to be using catch (...) (i.e. catch with the ellipsis) unless you really definitely most provable have a need for it. The reason for this is that some compilers (Visual C++ 6 to name the most common) also turn errors like segmentation faults and other really bad conditions into exceptions that you can gladly handle using catch (...). This is very bad because you don't see the crashes anymore. And technically yes you can also catch division by zero (you'll have to ""StackOverflow"" for that) but you really should be avoiding making such divisions in the first place. Instead do the following: If you actually know what kind of exception(s) to expect catch those types and no more and If you need to throw exceptions yourself and need to catch all the exceptions you will throw make these exceptions derive from std::exception (as Adam Pierce suggested) and catch that.  In C++ the standard does not define a divide-by-zero exception and implementations tend to not throw them.  catch (...) { // Handle exceptions not covered. } Important considerations: A better approach is to catch specific types of exception that you can actually recover from as opposed to all possible exceptions. catch(...) will also catch certain serious system level exceptions (varies depending on compiler) that you are not going to be able to recover reliably from. Catching them in this way and then swallowing them and continuing could cause further serious problems in your program. Depending on your context it can be acceptable to use catch(...) providing the exception is re-thrown. In this case you log all useful local state information and then re-throw the exception to allow it to propagate up. However you should read up on the RAII pattern if you choose this route. cant get it to catch MySQL errors (duplicate key)..."140,A,"How Does One Sum Dimensions of an Array Specified at Run-Time? I am working on a function to establish the entropy of a distribution. It uses a copula if any are familiar with that. I need to sum up the values in the array based on which dimensions are ""cared about."" Example: Consider the following example...  Dimension 0 (across) _ _ _ _ _ _ _ _ _ _ _ _ _ |_ 0 _|_ 0 _|_ 0 _|_ 2 _| Dimension 1 |_ 1 _|_ 0 _|_ 2 _|_ 0 _| (down) |_ 0 _|_ 3 _|_ 0 _|_ 6 _| |_ 0 _|_ 0 _|_ 0 _|_ 0 _| I ""care about"" dimension 0 only and ""don't care"" about the rest (dim 1). Summing this array with the above specifications will ""collapse"" the ""stacks"" of dimension 1 down to a single 4 x 1 array: _ _ _ _ _ _ _ _ _ _ _ _ _ |_ 1 _|_ 3 _|_ 2 _|_ 8 _| This can then be summed or have any operation performed. I need to do this with an array of 'n' dimensions which could feasibly be 20. Also I need to be able to do this caring about certain dimensions and collapsing the rest. I am having an especially hard time with this because I cant visualize 20 dimensions :p . If anyone could help me set up some c/c++ code to collapse/sum I would be very very grateful. Update: Just got home. Here is some info to answer your questions: Sorry for rolling back the edits I was hoping when I clicked roll-back it would show me the changes so I could see what I messed up a bit like wikipedia. This wasn't the case as I found out. @jeff - What doesnt make sense? I am using this great service for (what I think is) a legit reason. I want to get better at my hobby which is all it is as I am in high school. Many of my posts regard implementing a genetic algorithm (This post sparsearray rank an array pointer manipulation). I am using a sparse array representation as it is possible to exceed the number of molecules in the universe using a traditional (dense) array. For now the implementation of the sparsearray itself doesnt matter a whole lot as I am working to make it work with a standard array before going to a sparse representation. For those who havent seen my previous questions I am using a binary search tree as the structure to contain the sparse array points and a ""driver"" function to traverse the tree as necessary returning whatever the function is designed to do. This is flexible so I can accomodate a lot of different methods of accessing the array. The structure is a hypercube and the number of dimensions is specified at run time as well as the length of each dimension (which are all the same as it is a hypercube). Thanks everyone for your imput. Ed I'm looking at [your post history](http://stackoverflow.com/users/522/ed) and wondering if you're trying to pull an extended gag on Stack Overflow. **Do any of your questions make sense?** I beg to differ there is ALWAYS another way.. And if you really cannot refactor then you need to break the problem down into smaller parts.. Like I said establish which dimensions you need to sum then hit them one at a time.. Also stop changing the edits they are correcting your spelling errors they are trying to help you ;)  This could have applications. Lets say you implemented a 2D Conway's Game of Life (which defines a 2D plane 1 for 'alive' 0 for 'dead') and you stored the Games history for every iteration (which then defines a 3D cube). If you wanted to know how many bacteria there was alive over history you would use the above algorithm. You could use the same algorithm for a 3D (and 4D 5D etc.) version of Game of Life grid. I'd say this was a question for recursion I'm not yet a C programmer but I know it is possible in C. In python  def iter_arr(array): sum = 0 for i in array: if type(i) == type(list()): sum = sum + iter_arr(i) else: sum = sum + i return sum Iterate over each element in array If element is another array call the function again If element is not array add it to the sum Return sum You would then apply this to each element in the 'cared about' dimension. This is easier in python due to duck-typing though ...  x = number_of_dimensions; while (x > 1) { switch (x) { case 20: reduce20DimensionArray(); x--; break; case 19: ..... } } (Sorry couldn't resist.)  You're doing this in c/c++... so you have an array of array of array... you don't have to visualize 20 dimensions since that isn't how the data is laid out in memory for a 2 dimensional: [1] --> [123456...] [2] --> [123456...] [3] --> [123456...] [4] --> [123456...] [5] --> [123456...] . . . . . . so why can't you iterate across the first one summing it's contents? If you are trying to find the size then sizeof(array)/sizeof(int) is a risky approach. You must know the dimension to be able to process this data and set the memory up so you know the depth of recursion to sum. Here is some pseudo code of what it seems you should do sum( n_matrix depth ) running_total = 0 if depth = 0 then foreach element in the array running_total += elm else foreach element in the array running_total += sum( elm  depth-1 ) return running_total  When you say you don't know how many dimensions there are how exactly are you defining the data structures? At some point someone needs to create this array and to do that they need to know the dimensions of the array. You can force the creator to pass in this data along with the array. Unless the question is to define such a data structure...  Unfortunately there isnt really a simpler way to do this.  Actually by colllapsing the colums you already summed them so the dimension doesn't matter at all for your example. Did I miss something or did you?  This kind of thing is much easier if you use STL containers or maybe Boost.MultiArray. But if you must use an array: #include <iostream> #include <boost/foreach.hpp> #include <vector> int sum(int x) { return x; } template <class T unsigned N> int sum(const T (&x)[N]) { int r = 0; for(int i = 0; i < N; ++i) { r += sum(x[i]); } return r; } template <class T unsigned N> std::vector<int> reduce(const T (&x)[N]) { std::vector<int> result; for(int i = 0; i < N; ++i) { result.push_back(sum(x[i])); } return result; } int main() { int x[][2][2] = { { { 1 2 } { 3 4 } } { { 5 6 } { 7 8 } } }; BOOST_FOREACH(int v reduce(x)) { std::cout<<v<<""\n""; } }  @Jeff I actually think this is an interesting question. I'm not sure how useful it is but it is a valid question. @Ed Can you provide a little more info on this question? You said the dimension of the array is dynamic but is the number of elements dynamic as well? EDIT: I'm going to try and answer the question anyways. I can't give you the code off the top of my head (it would take a while to get it right without any compiler here on this PC) but I can point you in the right direction ... Let's use 8 dimensions (0-7) with indexes 0 to 3 as an example. You care about only 12 and 6. This means you have two arrays. First array_care[4][4][4] for 12 and 6. The array_care[4][4][4] will hold the end result. Next we want to iterate in a very specific way. We have the array input[4][4][4][4][4][4][4][4] to parse through and we care about dimensions 1 2 and 6. We need to define some temporary indexes: int dim[8] = {00000000}; We also need to store the order in which we want to increase the indexes: int increase_index_order[8] = {75430621}; int i = 0; This order is important for doing what you requested. Define a termination flag: bool terminate=false; Now we can create our loop: while (terminate) { array_care[dim[1]][dim[2]][dim[6]] += input[dim[0]][dim[1]][dim[2]][dim[3]][dim[4]][dim[5]][dim[6]][dim[7]]; while ((dim[increase_index_order[i]] = 3) && (i < 8)) { dim[increase_index_order[i]]=0; i++; } if (i < 8) { dim[increase_index_order[i]]++; i=0; } else { terminate=true; } } That should work for 8 dimensions caring about 3 dimensions. It would take a bit more time to make it dynamic and I don't have the time. Hope this helps. I apologize but I haven't learned the code markups yet. :(  If I understand correctly you want to sum all values in the cross section defined at each ""bin"" along 1 dimension. I suggest making a 1D array for your destination then looping through each element in your array adding the value to the destination with the index of the dimension of interest. If you are using arbitrary number of dimensions you must have a way of addressing elements (I would be curious how you are implementing this). Your implementation of this will affect how you set the destination index. But an obvious way would be with if statements checked in the iteration loops.  I think the best thing to do here would be one/both of two things: Rethink the design if its too complex find a less-complex way. Stop trying to visualise it.. :P Just store the dimensions in question that you need to sum then do them one at a time. Once you have the base code then look at improving the efficiency of your algorithm."141,A,"Free Project management software The company I work for has some project management issues. It's not that it doesn't work but that I get the impression it could work much better. I'm looking for a good web-based project management tool. I've been playing with dotproject which I believe is excellent but I'd like to hear some other approaches (if any). I'm specially interested in opinions of people who can tell me why their choice is better than dotproject as that's the project I'm familiar with. I'm well aware of this other question in stackoverflow) but any recommendations for free pm software? Edit: I'm interested in real life experience. I've had a look over all the free pms on Wikipedia and I really like dotproject but I'm about to show it to my boss so I don't want it to blow up in my face later on. Edit: Sorry about spelling I'm not a native English speaker and I don't see anything wrong with the text so if there's something wrong you can point it out or edit if you have edit permissions. Edit: Ok here's the thing this is like a personal project of mine. I'm trying to improve project management in my company so I decided to look up different project management tools input all the data for the project and then show it to my boss. That's why I need it to be free because if I tell him ""it's great but it's gonna cost you XXX$ he will just ignore it (every company is like Dilber's ...) so I need to tell him ""You're gonna get it for free"" Edit: Sorry I'm editing a lot but we're more or less 150 people working on developing the project (there're more people from marketing comercial etc) Yes. Also Google is your friend don't forget! Sorry but I couldn't figure out what you intended by ""I want the company to get it as free to improve possibilities for addoption"" so I just took it out. Feel free to put it or some variation back if you think it's important. Wikipedia's list of project management software -- includes open-source and proprietary systems along with some of the features each package includes.  Recently come across www.clockingit.com well worth a look and its free.  We use Trac for a 5 person team. I'd say it's about 95% of what you really need for a small group like ours especially if you're not looking to satisfy any bureaucratic demands. It's pleasantly minimalist and easy to tweak. On the other hand that missing 5% is sufficiently annoying that I'd probably go with something else if I were making the decision over again because it's stuff like ""user account management"" (there is none though there might be a plugin to handle web-based configuration) and ""modeling any kind of relationship between issues"". Also it's more work to set up than it really ought to be. JIRA is ridiculously over-featured has a basically miserable interface and is emphatically not open source although Atlassian offers free licenses for open projects and non-profits. Decent of them as far as it goes but I'd still avoid it like the plague. throw some links in  I'm using Redmine and think it is a great software easy to use and nice features. What I am missing in Redmine is a real integration into Eclipse with Mylyn. There is only a very basic support for this. In trac I am missing an easy multi project support.  While it's not exactly bug tracking software JIRA is what I use. It allows for pretty extensive workflows is open source software and there are a lot of good plugins. Looks great but it's not free. I need to tell them it will cost them nothing ... you know how things are in large companys...  Definitely take a look at OpenAtrium which is built upon Drupal and is completely open source and free. If you are looking for a freemium model with a web-based project management app take a look at Intervals.  Assembla? Has SVN/Trac wiki file hosting etc. All free. Very handy. Oh and it also has great support for multiple people to work on the project. Hundreds even. EDIT: Oh I should mention all of this is entirely web based. I'm not sure if theres any client-side management software from them. Sorry I forge to mention must be hosted in our own servers we work for defense deparment so security is quite an issue. Assembla is not free anymore  I'm trying to get the company where I'm working to start using Redmine which looks very good. I'm not that familiar with dotproject but I think Redmine's a bit more targeted towards software developers. I like mark down in Redmine. Looks nice I'll give it a try. I like the concept of per wiki project but I think dot project has more possibilities for project tracking We are using Redmine in our company. It's a very good piece of software when you dealing with one or two projects but it is unsuitable when you have dozens of projects on your hand. I agree that it is targeted more towards developers.  Redmine now supports Mylyn (searching Mylyn at redmine.org brings up several how to's). At least using Mylyn 3.0. The Bitnami stack for Redmine is pretty easy to get running for no cost. We are just starting to use it at our office so far it has everything we need. We are also looking at the Redmine plugin that will allow us to ingrate javadocs into the redmine system too along with several other useful looking plugins. Our old issue tracking software integrates with Sourcesafe but all the new Java goodness is in Subversion which redmine can work with.  We are pretty happy with trac at our company.  Well..... People tend to think they can solve issues inside their company by installing a piece of software on their PC computer. Does this sound rational ? :-p Maybe you need some new project managers that also have proper education. Actually the process is good enough but is all done manually! We got Mantis for issues reports we got our own propietary tool for night builds but then the project management is done by hand with the wrong metrics and not taking into account a lot of data because it cost to much to gather it. -1 Op is not asking for a lecture he's asking a specific question for software. What are you suggesting anyways - that all good educated pm's need is a piece of paper and a pen?  I've heard good things about Trac and Bugzilla has also been recommended to me often. I haven't personally had a large enough project (or a solid enough group of developers all working on the same project) to warrant a full project management tool but I've done a minor amount of playing on my own.  This is one of those things where I think it's worth spending some money. While I think some free solutions like Trac or DotProject are great I've had better experiences with Basecamp and FogBugz  Vote up for Teamlab. they constantly improve functions and it's still free. And what I also like is their support. I'm not that geek an sometimes have some problems- always get their help in just few moments."142,A,"""Greedy"" and in Visual Studio Is there the way to apply ""greedy"" behavior to and keys in Visual Studio? By ""greedy"" I mean such behavior when all whitespace between cursor position and next word bound can be deleted using one keystroke. Actually you will need to do this: Ctrl+Shift+Left+Right - this will give you only the space selected and then you can press delete. This is assuming that you are coming from the right and you have to delete the space to the left. Of course this is still 5 keystrokes... but it beats pressing backspace again and again....  Just Ctrl+Backspace... This will erase all space AND the last word too... I just tested it.  Well I don't think you can change the binding of the delete key or backspace key - but CTRL+DEL & CTRL+Backspace are pretty close to what you want.  You are looking for: Edit.DeleteHorizontalWhiteSpace I have it set to Ctrl+K Ctrl+\ which I think is the default but might not be  You can use Ctrl+Shift+Arrow keys to make the selection and then just hit Delete. You may need to hit the arrow key more than once while still pressing Ctrl+Shift combination but because the fingers are in the same position is very fast. This works also for selecting words incrementally.  Sounds like something you could write a macro for and then assign to a keyboard shortcut (like SHIFT+DEL). If you explore the EnvDTE namespaces you can do a lot to make changes to text in the active document window. I'd start by checking with something like...  Public Sub RemoveWhiteSpace() DTE.ActiveDocument.Selection.WordRight(True) DTE.ActiveDocument.Selection.Text = "" "" End Sub That's just a simple example but you can extend it further pretty easily  Ctrl+Back Space and Ctrl+Delete are also greedy they delete the nearest word in their respective direction.  OK I've got this < Ctrl > thing. And applying this knowledge I've found corresponding VS commands: Edit.WordDeleteToStart and Edit.WordDeleteToEnd. I've successfully remapped < Delete > and < Backspace > keys using Options->Environment->Keyboard dialog. Unfortunately this commands apply not only to whitespace as I'd wish to but still thanks everyone!"143,A,What are some good compilers to use when learning C++? What are some suggestions for easy to use C++ compilers for a beginner? Free or open-source ones would be preferred. I agree with Iulian erbnoiu: Code::Blocks is a very good solution usable both from Linux (it will use g++/gcc) and from Windows (it will use either the MS compiler or gcc) Note that you should at least once or twice try to compile using a good old makefile if only to understand the logic behind headers sources inclusion etc. etc.. As a beginner don't forget to read books about C++ (Scott Meyers and Herb Sutter books come to the mind when trying to learns the quirks of the language) and to study open source high profile projects to learn from their code style (they already encountered the problems you will encounter and probably found viable solutions...).  gcc with -Wall (enable all warnings) -Werror (change warnings into errors) -pedantic (get warnings for non-standard code) and -ansi (make the standard c++98). If a warning is something you're aware of and need to turn off you can always turn them back into warnings.  Microsoft Visual Studio Express Edition of their C++ compiler is good  I say GCC for simple things because for a more complicated project the build process isn't so easy True but I don't think understanding the build process of a large project is orthogonal to understanding the project itself. My last job I worked at they had a huge project that needed to build for the target platform (LynxOS) as well as an emulation environment (WinXP). They chose to throw everything into one .VCP file for on windows and build it as one big executable. On target it was about 50 individual processes so they wrote a makefile that listed all 3000 source files compiled them all into one big library and then linked the individual main.cpp's for each executable with the all-in-one library to make 50 executables (which shared maybe 10% of their code with the other executables). As a result no developer had a clue about what code depended on any other code. As a result they never bothered trying to define clean interfaces between anything because everything was easily accessible from everywhere. A hierarchical build system could have helped enforce some sort of order in an otherwise disorganized source code repository. If you don't learn how .cpp files produce object code what a static library is what a shared library is etc. when you are learning C/C++ you do still need to learn it at some point to be a competent C/C++ developer.  You can always use the C++ compiler from the Gnu Compiler Collection (GCC). It is available for almost any Unix system on earth BSDs Mac OS Linux and Windows (via Cygwin or mingw). A number of IDEs are supporting the GCC C++ compiler e.g. KDevelop under Linux/KDE or Dev-CPP as mentioned in other posts.  CodeBlocks is a very good IDE that can use besides many other compilers CL.EXE (from visual studio) and gcc. It comes also in a version with gcc included. Visual Studio Express edition is avery good choice also (with Platform SDK if you will develop application that call winapi functions).  One reason to use g++ or MingW/Cygwin that hasn't been mentioned yet is that starting and IDE will hide some of what is going on. It will be incredibly useful down the road to understand the differences between compiling and linking for instance. Learn it and understand it from the start and you won't even know you should be thanking yourself later. -Max  Visual Studio in command line behaves just like GCC. Just open the Visual Studio command line window and:  c:\temp> cl /nologo /EHsc /W4 foo.cpp c:\temp> dir /b foo.* foo.cpp <-- your source file foo.obj <-- result of compiling the cpp file foo.pdb <-- debugging symbols (friendly names for debugging) foo.exe <-- result of linking the obj with libraries  G++ is the GNU C++ compiler. Most *nix distros should have the package available.  I recommend gcc because it's designed to be used on the command line and you can compile simple programs and see exactly what's happening: g++ -o myprogram myprogram.cc ls -l myprogram One file in two files out. With Visual C++ most people use it with the GUI where you have to set up a project and the IDE generates a bunch of files which can get in the way if you're just starting out. If you're using Windows you'll choose between MingW or Cygwin. Cygwin is a little work to set up because you have to choose which packages to install but I don't have experience with MingW.  GCC is a good choice for simple things. Visual Studio Express edition is the free version of the major windows C++ compiler. If you are on Windows I would use VS. If you are on linux you should use GCC. *I say GCC for simple things because for a more complicated project the build process isn't so easy What to use on Linux if a project is complicated?  For a beginner: g++ --pedantic-errors -Wall It'll help enforce good programming from the start.  I'd recommend using Dev C++. It's a small and lightweight IDE that uses the mingw ports as the backend meaning you'll be compiling the the defacto C/C++ compiler gcc use this link for dev C++ instead http://sourceforge.net/projects/dev-cpp/  Eclipse is a good one for mac or Apple's own free Xcode which can be d/l'd off their development site.144,A,"2008: Resharper vs. CodeRush I know there are many discussions if Resharper or CodeRush is better. At my company we currently use Resharper and I'm fine with it. But last week I watched a screencast about CodeRush and thought it was amazing. There are just so many ""new"" refactorings that I immediately thought about a migration. What's your favorite tool for refactoring code-analysis navigation inside Visual Studio etc and why? At which areas do you think is Resharper better and at which areas CodeRush? What was the screencast? Things have changed. This question needs to be re-asked. See here for an updated discussion: http://stackoverflow.com/questions/2765841/resharper-vs-coderush-2010-remake I've been a long time user of CodeRush and Refactor! Pro. CodeRush is way more than refactoring. Once you learn the rules it sets out coding speed increases. However even before you learn the rules it intelligently applies itself to existing things you do and makes doing them easier. Indeed the thing I miss the most about coding in Ruby on Rails is that I don't have an IDE with CodeRush and Refactor! TextMate bundles get me part way but it's not the same. They have a free trial available so I encourage you to give it a run and see how you like it. I've never looked back.  I use resharper - just downloaded the CodeRush free thing.. but there's no items on the menu bar./.. nothing.. only a line between brackets... so I think I'll stick with resharper.. For more information on Coderush Xpress see http://community.devexpress.com/blogs/markmiller/archive/2009/06/25/coderush-xpress-for-c-and-visual-basic-2008.aspx  Even after a year I have to give credit to Mark Miller he gave a very fair comparison. Both products have improved since the original post and I do not have performance issues with either. One feature that Resharper has that is very difficult to live without is its collection of Move File refactorings. You can select multiple files in solution explorer and move them to a new folder and/or project and ReSharper will fix up the namespaces and references for you. I used this feature today on some legacy code and I estimate it saved me at least 4 hours of tedious work. The community plugin DX_MoveCode provides a bit of this behavior but without the ability to update the namespace and all references to the types moved it just doesn't compete. R# Code Formatting is real hard to live without especially if you work with legacy code. I love the fact that refactoring in Code Rush is so much cleaner and performant however those dialogs R# provide give me the ability to update variables comments and strings related to the refactoring. For me the cost of the dialogs is justified by this support. One thing that surprises me is how CR displays code issues: ticks on the left and right of the editing surface and a HUGE hint. The ticks are fine but give us an opton to turn off that hint. I'm surprised because CR normally does a good job of staying out of the way thise hints can be a pain at times. Working with options in CR is a major pain. Code Rush does not do a good job of making it easy or accessible to modify its rich set of options. For me if CR provided the same Move File features and Code Formatting R# has I'd buy it immediately. I'm a sucker for eye candy and CR has it in spades.  CodeRush is where it's at man. I didn't like resharpers intellisense so i turned it off which makes resharper less useful. I did the same thing but after turned it on again and gave it sometime now i find better than VS intellisense I haven't used R# at all but I was disappointed at the introduction of environment crashes to VS2008 when I tried out CR-Express. I stopped using it because it crashed the IDE at least 2-3 times per week. (I decided it was responsible b/c after I uninstalled it the IDE returned to it's typical boringly stable state. :) ) I would suggest checking out the current version of CodeRush Xpress. At version 9.2.4 it is a lot more stable and memopry efficient. See http://community.devexpress.com/blogs/markmiller/archive/2009/06/17/performance-and-memory-milestones-in-coderush-and-refactor-pro-preview-of-9-2.aspx Remember R# isnt blameless on IDE crashes either though (I once had a TeamCity issue which had quite a few DX bits in the call stack but the issue was firmly in the JB bits) - Its one complex ecosystem. But at the same time both JB and DX are good on tidying stuff like that appropriately quickly.  I dont think there is any comparison. I used Resharper for years but once I moved to CodeRush I am not looking back. The big reason I moved to CodeRush was Resharpers horrible performance with VS2010. R# made the IDE almost unusable. I have a dual core Intel with 6GB RAM laptop and it still had serious problems. Seems to be the consensus our there on this. CodeRush is like its not even there is VS2010. Very lightweigth and just a useful. I dont think one is so much better as far as features go and there are alot of things I like more in R# but Jet Brains really dropped the ball when VS2010 came out and I was pretty disappointed.  Is there any performance difference between the two? I was a long time ReSharper user but finally gave up when I couldn't stand the sluggish performance any longer. The more complex my project (forms etc) got the slower the machine got. I uninstalled it and performance was way way better. Is CodeRush as sluggish? I switched for the same reason and even on huge projects I have not had CodeRush become slow. (Sometimes it takes a while for the suggestions to appear but unlike ReSharper it never bogged the entire environment down). The performance in CodeRush+RefactorPro has increased a lot over the past year of releases especially for projects that have already been opened before. They changed a lot of back end stuff to achieve this and I am very happy. I've had the same experience. I have a rather old laptop at home with 1GB memory and Visual Studio 2010 + R#5 was basically unusable even for a file->new project sized solution. On my work laptop opening up a meager solution with about 40 classes and the same amount of files in one project and about the same number in another (unit tests) skyrocketed Visual Studio memory usage to 2.6GB memory and that's without solution analysis turned on. Refactor! Pro gave 300MB usage and had stellar performance. R# still has a way to go in this regard I think. I like some of the features of R# though.  I have done some big refactorings lately and would say that I can not work without Resharper any more. You can just find out so many things so fast like the usages of methods classes interfaces the inheritors etc. And if you want to apply refactorings there is a lot of support which makes Resharper a must- have for me  I used Coderush for a year pre TDD. I've now used Reshaper for nearly 1.5 years. I easily go for an hour or two without using the mouse now thanks to R# (only because I often run out of batteries) If you practice TDD and or learn to use the tools fully then Resharper is the way to go I hit many frustrations with CodeRush in TDD and it seems at the time effort was going to do javascript evaluation rather than evaluating productivity enhancement for core C#. If you're the type of person that uses Resharper for CTRL+T and mouse click refactoring then you'll probably get more from Coderush especially if property generation if high up on your consideration list. Day 1 impressions CodeRush wins equally Day X if you dont learn embrace the tools fully but by end of month one with a solid eval Resharper wins. Next time you reach for the mouse ask is there a shortcut for this try it. Resharper has a higher learning curve to get going (you need to become fluent with stuff beyond goto type) and works at its optimal potential with TDD approach. Half the problem IMO is that R# makes everything a separate shortcut - that and the fact that it stomps over other shortcuts. What specific bits did you find in CR's TDD support (I'm pretty happy with it but cant recall when it really got good)? Any specific info about how it helps TDD? I haven't used R# for TDD methodology but am curious to find out.  I've tried both. JetBrains Resharper is way better for me than DevExpress Coderush. JetBrains is better with the IDEs (they have their own IDE for java after all) just like DevExpress is better with the beautiful components.  Having tried both I'd say latest version of CR/R has the edge in terms of integrating with the programming workflow. Usually you won't see a dialog asking if you wanted to do this or that it just flows with what you're doing. You're always a keystroke away of all the available refactors etc. Latest version includes IMHO important refactors like push/move method. Navigating your classes is faster in CR/R. I have noticed also that performance in large projects tend to be better in CR/R than that of Resharper. Intellisense support is better in Resharper as well as code analysis.  The CodeRush refactoring experience is faster and smoother (fewer keystrokes fewer dialogs - zero fewer mouse movements) than ReSharper's. When you rename or change a signature that impacts many unopened files on disk CodeRush properly supports multi-file undo. By contrast ReSharper presents a dialog asking if you want undo support for this Rename and if you say yes ReSharper proceeds to open all the files touched by the refactoring. CodeRush has more refactorings than ReSharper although ReSharper has a nice Move member to class refactoring that has yet to hit CodeRush. ReSharper also has a nice rename feature that lets you rename variables that contain a class name when you rename that class. For example if I rename a ""Spaceship"" class to ""Spacecraft"" ReSharper finds identifiers with names like ""superSpaceship"" and suggests they be renamed to ""superSpacecraft"". CodeRush is a little bit faster than ReSharper on Visual Studio startup and on project opens. ReSharper uses more memory up to six times as much on really large solutions (e.g. 1000+ classes). ReSharper reports more of the background code issues than CodeRush and the code issues ReSharper shows are in general more useful (e.g. parameter type can be demoted to a class closer to object) however CodeRush includes a code issue that spots undisposed local variables that implement IDisposable which is very useful. CodeRush ships significantly more code templates (like VS code snippets) than ReSharper and CodeRush's templates are designed to be optimally efficient (for example ""ms"" builds a method that returns a string ""vb"" creates a variable of type bool and ""nl.i"" creates a new initialized List). Your fingers benefit from the efficiency but the templates take some practice getting used to. The CodeRush training window can ease this learning curve if you have it up while you code. Interestingly CodeRush templates effectively abstract away the programming language so developers working in more than one language (or transitioning from one language to another) can press the same keystrokes and get essentially the same code regardless of the language they are coding in. ReSharper has a nice Intellisense replacement and an interesting parameter tool tip replacement. ReSharper has a variable name suggestion feature that is useful. ReSharper also has a code reformatting feature that is very nice. CodeRush's TDD consume-first declaration features require fewer keystrokes than ReSharper's (see these in free CodeRush Xpress which includes most of the consume-first declaration features shipping in the full version of CodeRush). ReSharper's find all references window has multiple panes for each search performed which is nice. CodeRush provides only one pane for the last search performed. CodeRush has a neat Tab to Next Reference feature that takes you through all references to an identifier just by pressing the Tab key (Shift+Tab takes you back). CodeRush's Find All References appears faster than ReSharper's. CodeRush's Unit Test Runner released in 9.3 supports more test frameworks out of the box (NUnit MSTest xUnit and MbUnit). The CodeRush Test Runner also understands more of the framework attributes (for example NUnit's ExpectedException MatchType parameter) and also supports dynamically-generated tests (e.g. RowTests the Values attribute factories theories etc.). Support for test frameworks is extensible and CodeRush includes source code for each of the test framework plug-ins. @Mark - I by no means meant to question your integrity. I just thought it important that people know your affiliation. I personally love both products but CodeRush crashes my Visual Studio 2008 frequently while Resharper doesn't. Maybe I just have a bad install. Hopefully with the next 2010 VS and extensibility framework things will be much more stable. Jeffrey let's get that fixed. Contact support@devexpress.com and give us some details so we can reproduce. Let us know what version you're using and tell us a bit about your project and what leads to the issue. Also if you can attach to the VS process (with a second instance of VS) and get us a call stack that could be very helpful in narrowing down the cause of this. In the interest of full disclosure... it should be noted that Mark Miller works for DevExpress the makers of CodeRush. hehe Mark Miller _is_ DevExpress :-) Yes which is why I took special care to ensure that everything presented was accurate and verifiable. If there is any doubt on the accuracy of anything presented above I challenge you to verify it for yourself. There are free evaluation versions of both products readily available. I do offer my apologies for failing to make it clear at the top of the post that I work for DX. It didn't occur to me until I saw your comment that this might not be obvious to some readers. For the record CR doesnt crash VS with any regularity - all plugins are prone to issues and interactions with other components but I certainly havent seen a CR related issue in the last 2 years across many builds. And one one occasion CR showed up in a stack trace of a crash but it was actually the TeamCity plugin that was the root of my issue at that time. Speaking of support I must say that I am in love with DevExpress support. I've been using their class libraries for a while now (mostly at work but also somewhat on private projects) and the bugs/questions/issues I've encountered has been met with nothing but extraordinary support. Twice I've had beta-versions of next release available to me overnight by email fixing bugs I reported late the day before. Couple that with excellent products and I would say I'm a DevExpress fan for sure :) I do like the Alt+Enter to fix missing using-clauses in R# though (Mark/Rory hint hint :)) FWIW version 10.1 of CodeRush does introduce a single dialog. but Damn it's a good one. Really justifies itself. -> http://community.devexpress.com/blogs/markmiller/archive/2010/04/26/working-with-color-in-coderush.aspx If it counts for much Reshaper causes VS to crash often for me. (Nothing like your IDE just going away without so much as an error message.) Wow Mark! Really nice and honest comparison! Thank you  Both of these tool are excellent. I currently use Refactor! Pro. I've only just started using CodeRush Xpress as well. I used ReSharper a bit at a client site back in 2005. I purchased Refactor! Pro a couple of years ago because at that time it had wider language support. I was doing VB at the time and Refactor! supported it ReSharper didn't then. I prefer the UI paradigm in CR/R! but as I've not used a current version of ReSharper I can't judge what's best. However it does seem that ReSharper is more advanced in the code analysis area. The main point is that both tools are good and way better than what you get out of the box with Visual Studio! Code Issues (available in Coderush 9.1 and higher) are much more mature now than when this answer was first given.  It's Resharper for me. I've been using it since it's first EAP release and I love it to bits! As you say it far more than refactoring - it's the way it supports me in navigation and code comprehension that I use far more than the refactoring. Two of my favourite features are the code cleanup and type members layout. I find Tim's comment intriguing where he says: it intelligently applies itself to existing things you do I'd be interested to hear what this is in CodeRush. I have a lot of respect for Developer Express products. I find it interesting that when support towards Resharper is voiced its not considered an answer while Darrens effort in the opposite direction is the third most appreciated? :P Is this really the correct answer? It doesn't even answer the question. This answer is useless. It doesn't compare the two products; it merely acts as a cheerleader for one. If this is the correct answer then the *question* is wrong. He answers the question just fine.  ReSharper is the one for me. I have written about this here : http://www.tewari.info/2009/02/21/resharper-vs-coderush-refactor-pro/  I've been a long time user of CodeRush + RefactorPro whereas my friend in one team is using Resharper. I really would like to have code analysis at the same level that he has in R#. CR is just poor here. R# excells in code layout features as well and has decent test runner. In CR there is no code layout and formatting tools and test runner is not released yet. It is in planned stage. Regarding navigation we find both tools equal. My friend envies me templates that CR has. R# templates are far behind. Also learning curve of R# is worse. CR is much easier to grasp. Also refactorings provided by Refactor Pro are better than in R#. They are just easier to cope with. So in terms of writing code I find CR better. Recently we both tried to write plugins to implement features we see in opposite tool and would like to have. I was able to do this waaaay easier for CR. Extensibility of DXCore is amasing. All in all: R# has much more features but they are much harder to learn. Also if you find something missing CR is easier to extend.  I'm a long time IntelliJ and R# user and I'm sure I'd find it hard to switch but my curiousity about what I'm missing out on is strong enough to approach trying. From what others are talking about on this question page and elsewhere online I think I'd still miss the navigation and analysis features of R#. I found this page of videos that show CR/RP features. It might be useful to people like me who want to see what they're missing out on before installing anything: Training Videos and Online Tutorials - Coding Assistance and Refactoring Tools by DevExpress  Honestly they both are equal to the task. What I've found is that CodeRush/Refactor Pro has a steeper learning curve but once you have trained yourself and the environment (and switched a few keystrokes) CodeRush really becomes effective. ReSharper is equally good and don't let anyone mislead you on that point. They are not equal though and it will depend on your needs. CodeRush is way more extensible (IMO) but if you move from one to the other you will be missing a couple things from the other. Back in July I took a month and I did just what you are talking about doing (I migrated from ReSharper to CodeRush and blogged about it). I'm pretty happy with the end result (there's a couple things I'm missing... CodeRush's Code Analysis is still not quite at ReSharper's level and the ""Move"" refactoring.. beyond that I found everything I needed). Here are my blog posts so you can refer to them (if you want)... Hello CodeRush! After Week 1 2 weeks with CodeRush CR/RP Wrapup As I said though both tools are excellent and depending on your willingness to learn a new tool you may be better with what you have right now. What was the specific issue with CodeRush's move refactoring? I tried Resharper and as i recall it annoyed me by saying ""This could be better if only..."" and a little light bulb next to a lot of stuff. Needless to say it got removed almost right away. CodeRush offers the tools and abilities you need to get coding faster without getting in your way by pestering you with ""style recommendations"". (sorry if that sounded like a marketing line) @Frank Schwieterman: DX_MoveCode is just one example of what it doesnt do. What are you comparing in R# to CR and what do you mean by the question - i.e. do you believe he has a case or not? Would be nice to include version number as ReSharper now released 5.0. @RCIX: Most ReSharper users love those ""hints"" and ""suggestions"" especially since you can just hit Alt+Enter and RS will do it. AFAIK CodeRush has most of the same ""good coding suggestions"". The only ones I've ever found troublesome are the incorrect ones; for instance user code may only ever set a property but it doesn't work right in a designer without the getter. It can be bad about detecting ""possible NullReferenceExceptions"" as well. You can comment them out or just turn them off.  The reason I got R# for its code analysis. Showing me error in real time is awesome. CR doesn't have it or its weak. it's taking them foreevr to get right. I also love R# navigation shortcuts. CR is probably better in refactoring but I seldom do refactorings. I do them by hand. Refactor Pro has so many refactorings you might be buried. Learning curve is steep."145,A,"Should I move from C++ to Python? ... Or another language? In the company I work for we do a lot of file-based transaction processing. The processing centers around the conversion of files between numerous formats to suit numerous systems in numerous companies. The processing almost always involves an XML stage and can include a lot of text parsing database lookups data conversion and data validation. Currently the programs performing all these tasks are written in C++ and they perform quite quickly all on one average server. I'm investigating the possibilities of using a more ""modern"" language that newer graduate programmers are more likely to be familiar with. (Correct memory allocation in C++ seems to causes problems with a lot of newer programmers these days) Based on the brief information provided would a language such as python provide the required functionality and performance as well as addressing the memory allocation (and various other C++ related) problems which arise? I like the idea of not needing to compile the programs each time we make a change. I understand that the interpreted languages probably wont hit the same performance we currently get. Our systems are Linux based which also restrict some options. Any comments on the functionality and performance available with Python or suggestions of alternative languages would be much appreciated. Performance in Python can be a very big issue. Once I had to create program involving optimization algorithm on the list of tasks. I started with Python created it super-fast and clean then saw it will take ages to provide a result. Rewriting it line by line to C++ resulted in over 100x speed improvement... So sometimes it is not a matter of 5-10% performance loss as you can see. You should investigate it in your case (maybe little test?). Episodex: Please tell Why python generate results ""so lately"" & Why c++ generate results ""so early??."" Mostly because python is a script language which I think is slower by definition than compiled one. In my particular case I had to compute millions of operations on the lists of integers. Python's advanced lists are much slower than using just simple array with fixed length in C++ (I didn't need any of advanced features of these lists). And last but not least as far as I know C++ (or C) is the next top speed language after Assembler ;). thanks but I read at Stackoverflow in one of user comment that if Python is slow dont worry Because CPU time is much more cheaper than the Developer time.  Python would probably remove most of the low level stuff that you use in your application. Memory allocation wouldn't be an issue anymore. Also at least my university seems to be embracing Python as a programming language because students don't have to write all of that formal stuff to get started. Your only problem would be the performance part as Python will likely never be as fast as a compiled C++ program. I would advise you to take a couple of weeks to get to know the programming languages that you're considering. I'd check out Ruby also. Maybe toy around with Haskell a bit? As I understand it Python seems well equipped for dealing with everything you're talking about. XML database lookups validation parsing. It is usually a safe choice not just because of the easy and fun programming experience but if you're stuck there's an awesome community around the language who are just happy to help out.  I agree with others you should stick with C++. Switching to a non-compiled language is a step backwards. While many programmers may have trouble dealing with some of the troublesome aspects of the language (such as pointers) at least most programers have been exposed to some C++. I recommend you spend your time and money improving your codebase and programmers rather then switching languages. As for other languages you may want to keep your eye on GO lang. A friend of mine used it fairly extensively. It's a modern compiled language. It tends to be clear concise and modern. GO applications typically run at speeds comparable to those written in C++ and it interfaces well with the web. It's not very mature at this point but it looks promising. Good Luck! im sure your advice is great but i dont think its relevant anymore why are you posting on a question asked 4 years ago?  should move to python that languange make all possible in networking if you need faster move to c/c++  I hate to say this but f you want something that your incoming developers are going to be familiar with go with Java. Java is the language that most recent graduates will be most familiar with. You still have to compile but compile times will be shorter than C++. It'll run on Linux and pretty much anywhere else. It's got a good garbage collector. It's pretty fast. And did I mention your developers will be familiar with it? No it's not ""cool"" like Python but it's a very tried-and-true language. Honestly I doubt that you've got a lot of incoming developers who suck with C++ but would be awesome with Python anyway. The people who use Python well tend to be fine with manual memory management. The people who are bad with memory management actually tend to be bad with all languages. I do find it worrisome that you've got developers who are so bad with memory management that you want to switch languages. That's a sign indicating a problem but I'm not sure that the problem is with the language. -1 for Java (doesn't really help the OP much at all) but +1 for ""people who are bad with memory management tend to be bad with all languages"".  If you are fine with staying with a compiled language I would stay with C++ and suggest choosing a good set of libraries and teach newbies on the correct use and adherence to solid patterns. If you manage to find a pleasant set of libraries it will be easy for newbies to learn writing solid code. My (current) personal preference is the Qt class library because it makes memory handling easy and safe and is pleasant to work with. It also features support for XML parsing and generation has regexp's built in network capabilities is cross-platform ... and is also very useful for non-GUI systems. For me it's a huge difference between working with plain C++ std library and STL and working with a powerful library like Qt. Probably looking into boost goodies is also very worthwile. Oh just now realized this question is 2 y old. Never mind.  if the nature of the project you are doing allows you to even contemplate such a move then do move (assuming that you have some clue). In many C++ projects however your only choice is moving down one or two abstraction levels (e.g. to C or Assembly).  I like the idea of not needing to compile the programs each time we make a change. I understand that the interpreted languages probably wont hit the same performance we currently get. This is the biggest issue; can you live with the performance hit. You could try to use Python and extending it with your current C++ modules for the performance heavy parts. Still switching your entire system seems like a big effort if the only reason is the lack of C++ talent. Hiring people who know C++ seems like the cheaper option. @gbjbaanb On the flip side I'd rather poor programmers use Python than C++. Example a Python developer can't make inter-module globals. He can't get in trouble with initialization order of globals He's kind of forced to design things in a somewhat modular way and it's a little harder to get those disastrously bad designs in Python that can wreak havoc throughout the system. That said I'm mainly a C++ enthusiast (check my profile) but I've seen the disastrous kind of C++ coding first hand and think a language like Python would actually be better in the hands of less experienced programmers. There are less ways for such a developer to crash a program in Python. Accessing a variable with 'None' would cause an exception rather than undefined behavior (ex: access violation or segfault). There's no C-ish type casting/bitwise sort of logic (x-raying/bypassing types) that C++ developers sometimes do out of bad habit. There's also no way for them to slow down the build system by ignoring important idioms like pimpls for central headers. So assuming such code monkeys are going to have a hard time getting their code or design correct [...] [...] I'd rather trust them with Python than C++ as the result of poor coding is not quite as disastrous. Of course I'd much rather they just understood what they were doing and if there are any unemployed developers on this forum some of these top users really have great expertise when it comes to general engineering concepts and C++ usage. poor programmers tend to be poor with all languages so changing everything just to suit the numpties won't be a solution. I'd recommend teaching them how to be better instead it'll pay off significantly. (and use STL and a nice XML lib - tinyXML is good)  I would suggest to try groovy. The XML support is fine and parsing as well as data validation should be not to difficult. However some people pointed out that migrating might not be the brightest idea. Can't you try to factor out common stuff into ""macher objects"" and ""validating objects"" so that new programmers use your C++ library instead of trying to write error prone new code that only duplicates existing fragments? Also amke sure to use modern file IO (iostreams) and not C like IO in C++ that should help with memory problems a lot. Also looking to the boost libraries might be helpfull.  Or should try to store your parsing rules on a database instead of leaving them hard-coded inside your code. As Ken Downs rightly quoted minimize code maximize data. This way you would not need to recompile everytime a tiny rule changes.  Which is more important quickly getting the programs to work or getting the programs working quickly? If you're dealing with large numbers of large files then you may be better off staying in C++ and teaching your graduate programmers what a pointer is (!) Otherwise I'd strongly advise that you look at a scripting-based solution because development in these once you're up to speed is so much faster. And a lot more fun if we're honest for most people at least. If the per-record processing load is not high you may be surprised how little performance you lose: file IO will almost certainly be handled in a compiled (C) library so the interpreter overhead may be relatively low. Worth trying I'd suggest. Of the imperative languages Perl is an obvious option Python is popular and Ruby has a high profile (and probably cleaner OO features than the first two). Then there is the slightly more er esoteric realm of the functional languages but I'm not qualified to comment on those.  If you can get away with using Python Ruby Groovy or Perl vs. C++ you would be better off going with one of these higher level languages. Productivity will greatly increase. If you find that you need more performance then go with Java. Everyone should know at and use at least one dynamically typed language.  Another alternative is to embed Python in your C++ program. You could keep much of your application the same and make calls out to Python for the pieces that change often or need the flexibility that a scripting language provides. From the Python docs The previous chapters discussed how to extend Python that is how to extend the functionality of Python by attaching a library of C functions to it. It is also possible to do it the other way around: enrich your C/C++ application by embedding Python in it. Embedding provides your application with the ability to implement some of the functionality of your application in Python rather than C or C++. This can be used for many purposes; one example would be to allow users to tailor the application to their needs by writing some scripts in Python. You can also use it yourself if some of the functionality can be written in Python more easily."146,A,How does the Multiview control handle its Viewstate? Does the Multiview control contain the viewstate information for each of its views regardless of whether or not the view is currently visible? Yes it does all the views are still there just the inactive ones are hidden/disabled. http://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.multiview_properties.aspx  I believe so yes. It would be quite simple to confirm using a ViewState Decoder (google it there are tools available from Fritz Onion or as FireFox plugins).  I would have to assume that the viewstate contains information for each of a Multiview's views/controls. Otherwise there's no way it would be able to keep track of the state of the controls in each view- unless you were using some sort of custom state management.147,A,How should I translate from screen space coordinates to image space coordinates in a WinForms PictureBox? I have an application that displays an image inside of a Windows Forms PictureBox control. The SizeMode of the control is set to Zoom so that the image contained in the PictureBox will be displayed in an aspect-correct way regardless of the dimensions of the PictureBox. This is great for the visual appearance of the application because you can size the window however you want and the image will always be displayed using its best fit. Unfortunately I also need to handle mouse click events on the picture box and need to be able to translate from screen-space coordinates to image-space coordinates. It looks like it's easy to translate from screen space to control space but I don't see any obvious way to translate from control space to image space (i.e. the pixel coordinate in the source image that has been scaled in the picture box). Is there an easy way to do this or should I just duplicate the scaling math that they're using internally to position the image and do the translation myself? I wound up just implementing the translation manually. The code's not too bad but it did leave me wishing that they provided support for it directly. I could see such a method being useful in a lot of different circumstances. I guess that's why they added extension methods :) In pseudocode: // Recompute the image scaling the zoom mode uses to fit the image on screen imageScale ::= min(pictureBox.width / image.width pictureBox.height / image.height) scaledWidth ::= image.width * imageScale scaledHeight ::= image.height * imageScale // Compute the offset of the image to center it in the picture box imageX ::= (pictureBox.width - scaledWidth) / 2 imageY ::= (pictureBox.height - scaledHeight) / 2 // Test the coordinate in the picture box against the image bounds if pos.x < imageX or imageX + scaledWidth < pos.x then return null if pos.y < imageY or imageY + scaledHeight < pos.y then return null // Compute the normalized (0..1) coordinates in image space u ::= (pos.x - imageX) / imageScale v ::= (pos.y - imageY) / imageScale return (u v) To get the pixel position in the image you'd just multiply by the actual image pixel dimensions but the normalized coordinates allow you to address the original responder's point about resolving ambiguity on a case-by-case basis. Hi it would be great to see a sample of the code you put together if you still have it to hand. Sure thing I edited it into my response  Depending on the scaling the relative image pixel could be anywhere in a number of pixels. For example if the image is scaled down significantly pixel 2 10 could represent 2 10 all the way up to 20 100) so you'll have to do the math yourself and take full responsibility for any inaccuracies! :-)148,A,"Improving Python readability? I've been really enjoying Python programming lately. I come from a background of a strong love for C-based coding where everything is perhaps more complicated than it should be (but puts hair on your chest at least). So switching from C to Python for more complex things that don't require tons of speed has been more of a boon than a bane in writing projects. However coming from this land of brackets and parentheses and structs as far as the naked eye can see I come across a small problem: I find Python difficult to read. For example the following block of text is hard for me to decipher unless I stare at it (which I dislike doing): if foo: bar = baz while bar not biz: bar = i_am_going_to_find_you_biz_i_swear_on_my_life() did_i_not_warn_you_biz() my_father_is_avenged() The problem occurs at the end of that if block: all the tabbing and then suddenly returning to a jarring block feels almost disturbing. As a solution I've started coding my Python like this: if foo: bar = baz while bar not biz: bar = i_am_going_to_find_you_biz_i_swear_on_my_life() #-- while -- #-- if -- did_i_not_warn_you_biz() my_father_is_avenged() And this for some odd reason makes me more able to read my own code. But I'm curious: has anyone else with my strange problem found easier ways to make their tabbed-out code more readable? I'd love to find out if there's a better way to do this before this becomes a huge habit for me. Rather than focusing on making your existing structures more readable you should focus on making more logical structures. Make smaller blocks try not to nest blocks excessively make smaller functions and try to think through your code flow more. If you come to a point where you can't quickly determine the structure of your code you should probably consider refactoring and adding some comments. Code flow should always be immediately apparent -- the more you have to think about it the less maintainable your code becomes. I've discovered that my Python code is a lot flatter than my C++ code. Don't know why.  I would look in to understanding more details about Python syntax. Often times if a piece of code looks odd there usually is a better way to write it. For example in the above example: bar = foo if baz else None while bar not biz: bar = i_am_going_to_find_you_biz_i_swear_on_my_life() did_i_not_warn_you_biz() my_father_is_avenged() While it is a small change it might help the readability. Also in all honesty I've never used a while loop so there is a good change you would end up with a nice concise list comprehension or for loop instead. ;)  You could try increasing the indent size but in general I would just say relax it will come with time. I don't think trying to make Python look like C is a very good idea.  Perhaps the best thing would be to turn on ""show whitespace"" in your editor. Then you would have a visual indication of how far in each line is tabbed (usually a bunch of dots) and it will be more apparent when that changes. If the indentation isn't obvious from the whitespace itself then either it's not enough (four-column indents using spaces only per PEP 8) or you're not reading it in a monospaced font. Either of those need to be fixed before deciding to make whitespace look like something else.  I like to put blank lines around blocks to make control flow more obvious. For example: if foo: bar = baz while bar not biz: bar = i_am_going_to_find_you_biz_i_swear_on_my_life() did_i_not_warn_you_biz() my_father_is_avenged()  from __future__ import braces Need I say more? :) Seriously PEP 8 'Blank lines' 4 is the official way to do it.  Part of learning a new programming language is learning to read code in that language. A crutch like this may make it easier to read your own code but it's going to impede the process of learning how to read anyone else's Python code. I really think you'd be better off getting rid of the end of block comments and getting used to normal Python."149,A,Does anyone still use ObjectPal? ObjectPal is the programming language used by the Borland Paradox database application (now owned by Corel). I'm interested in just looking at it again for nostalgic value and nothing else. Anyone know if there is an emulator or something for it? I just inherited a Paradox 4.5 for DOS and Paradox 9 hybrid ERP app so here I am 5 1/2 years after your question just starting to learn Paradox. Unfortunately Paradox communities out there are dead these days. Most died out by 2009 or so... I found out by pure coincidence that I live less than 10 minutes from Mike Prestwood's consulting office. (Mike wrote most of the Paradox programming books out there.) His site is a good resource but even his site has let the Paradox content languish. I do periodically hire one of his guys for help so there are a few experts left. You mean not everyone is using ObjectPal anymore? Where have I been?  Visit The DB Community to meet with other current ObjectPAL developers and users. -Al.150,A,What is the best way to print screens from an ASP.NET page .NET1.1/.NET2.0 I have seen examples of printing from a windows application but I have not been able to find a good example of any way of doing this. What do you mean print screens ? If you want to print the page asp.net or not it's up to the browser to do that. I've used the print style sheet here's and article http://alistapart.com/stories/goingtoprint/ that will go through the way to set that up. Rather than setting up a special page that would need to be maintained as well.  If you just need to print your web page from the client-side use window.print(). Sample could be found here: http://www.javascriptkit.com/howto/newtech2.shtml. I would suggest preparing a special version of your page first with no dynamic content and with a layout which would look nice on print. If you need to send something to printer on the server-side that would be a little bit more complicated. Check out this MSDN article on how to do the basic printing.  Restating what others have said you just need to call window.print() in javascript. That and build a separate css for print.  The browser prints your pages. If you need to tweak the page so it looks better on the printer use CSS @media selectors.151,A,"Split data access class into reader and writer or combine them? This might be on the ""discussy"" side but I would really like to hear your view on this. Previously I have often written data access classes that handled both reading and writing which often led to poor naming like FooIoHandler etc. The rule of thumb that classes that are hard to name probably are poorly designed suggests that this is not a good solution. So I have recently started splitting the data access into FooWriter and FooReader which leads to nicer names and gives some additional flexibility but at the same time I kind of like keeping it together if the classes are not to big. Is a reader/writer separation a better design or should I combine them? If I should combine them what the heck should I name the class? Thanks /Erik ORM might be your best solution. Or use a repository type pattern with a ""thingContext"" object that is responsible for state persistence. Personally I use the activeRecord pattern where save logic is baked into a base class but I'm leaving it in favor of an nHibernate style repository pattern. The allowance for DDD and testing things without a db is very nice to have in a framework type situation where my business logic is now gaining traction for a new UI.  Something that reads and writes to a backend store could be called a data accessor or ReaderWriter or IO or Store. So how about one of: FooDataAccessor FooAccessor FooReaderWriter FooRW FooIO FooStore FooStorage Thanks for some useful other names. ""Data access classes"" is not synonymous with ""Database"". :)  I am now using Linq to Sql. This solves the problem entirely. However if you do not have that option (or some similar ORM tool) I don't see any reason to separate Read/Write methods. It just adds more classes and complicates data access. I have always designed it as follows: Component/Business Object: Car Data Access containing static Read and Write methods: CarDB Example Usage: Car car = new Car(); car.Manufacturer = ""Toyota"" car.Model = ""Camry"" car.Year = 2006; car.CarID = CarDB.InsertCar(car) car.OwnerID = 2; CarDB.UpdateCar(car); This also makes sense for data access where both Reads and Write need to be performed as part of the same transaction. If you split up the classes where would that go?  Ignoring ORM (not because I'm either for or against it) I would keep them in the same class. They are both facets of a single responsibility and separating them just makes you look in two places where I can't really think of a good reason you would want to do that.  When given the choice I generally subclass the reader to create the writer."152,A,E-mail Notifications In a .net system I'm building there is a need for automated e-mail notifications. These should be editable by an admin. What's the easiest way to do this? SQL table and WYSIWIG for editing? The queue is a great idea. I've been throwing around that type of process for awhile with my old company. Are you just talking about the interface and storage or the implementation of sending the emails as well? Yes a SQL table with FROM TO Subject Body should work for storage and heck a textbox or even maybe a RichText box should work for editing. Or is this a web interface? For actually sending it check out the System.Web.Mail namespace it's pretty self explanatory and easy to use :)  How about using the new Workflow components in .Net 3.0 (and 3.5)? That is what we use in combination with templates in my current project. The templates have the basic format and the the tokens that are replaced with user information.  Adam Haile writes: check out the System.Web.Mail namespace By which you mean System.Net.Mail in .Net 2.0 and above :)  I am thinking that if these are automated notifications then this means they are probably going out as a result of some type of event in your software. If this is a web based app and you are going to have a number of these being sent out then consider implementing an email queue rather than sending out an email on every event. A component can query the queue periodically and send out any pending items.  From a high level yes. :D The main thing is some place to store the templates. A database is a great option unless you're not already using one then file systems work fine. WSIWIG editors (such as fckeditor) work well and give you some good options regarding the features that you allow. Some sort of token replacement system is also a good idea if you need it. For example if someone puts %FIRSTNAME% in the email template the code that generates the email can do some simple pattern matching to replace known tokens with other known values that may be dynamic based on user or other circumstances.153,A,Testing StarTeam operations In a Java application I need to checkout files from Borland Starteam 2006 R2 using Starteam API by various parameters (date label). Is there any framework that helps to write automatic tests for such functionality? I'm not aware of any; the approach i'd take is a project which has sample files you can checkout by various criteria and then verify everything you expected arrived and it is the right file (hash matches). You're aware that they ship a command line client (stcmd) too right? For a lot of things you don't need to use the api at all.154,A,"How do you parse a filename in bash? I have a filename in a format like: system-source-yyyymmdd.dat I'd like to be able to parse out the different bits of the filename using the ""-"" as a delimiter. You can use the cut command to get at each of the 3 'fields' e.g.: $ echo ""system-source-yyyymmdd.dat"" | cut -d'-' -f2 source ""-d"" specifies the delimiter ""-f"" specifies the number of the field you require I'm curious why you added the # prompt. Normally that prompt indicates the root or superuser. In generally I'd think stuff like trying out the **cut** command would be better done as a regular user. I'd have used the $ prompt. Oh yeah - good point. I must admit I was logged in as root at the time and simply went for it - a bad habit I know. Having said that I think echo and cut are two of the least harmful commands :) But for the sake of completeness I'll certainly update the example right away. Cheers.  Use the cut command. e.g. echo ""system-source-yyyymmdd.dat"" | cut -f1 -d'-' will extract the first bit. Change the value of the -f parameter to get the appropriate parts. Here's a guide on the Cut command.  Another method is to use the shell's internal parsing tools which avoids the cost of creating child processes:  oIFS=$IFS IFS=- file=""system-source-yyyymmdd.dat"" set $file IFS=$oIFS echo ""Source is $2""  A nice and elegant (in my mind :-) using only built-ins is to put it into an array var='system-source-yyyymmdd.dat' parts=(${var//-/ }) Then you can find the parts in the array... echo ${parts[0]} ==> system echo ${parts[1]} ==> source echo ${parts[2]} ==> yyyymmdd.dat Caveat: this will not work if the filename contains ""strange"" characters such as space or heaven forbids quotes backquotes...  Depending on your needs awk is more flexible than cut. A first teaser: # echo ""system-source-yyyymmdd.dat"" \ |awk -F- '{printf ""System: %s\nSource: %s\nYear: %s\nMonth: %s\nDay: %s\n"" $1$2substr($314)substr($352)substr($372)}' System: system Source: source Year: yyyy Month: mm Day: dd Problem is that describing awk as 'more flexible' is certainly like calling the iPhone an enhanced cell phone ;-)"155,A,"Flat File Databases What are your best practices around creating flat file database structures in PHP? A lot of the more mature PHP flat file frameworks I see out there attempt to implement SQL-like query syntax which is over the top for my purposes in most cases (I would just use a database at that point). Are there any elegant tricks out there to get good performance and features with the small code overhead one would want by taking on this problem in the first place? In my opinion using a ""Flat File Database"" in the sense you're meaning (and the answer you've accepted) isn't neccesarily the best way to go about things. First of all using serialize() and unserialize() can cause MAJOR headaches if someone gets in and edits the file (they can in fact put arbritrary code in your ""database"" to be run each time.) Personally I'd say - why not look to the future? There have been so many times that I've had issues because I've been creating my own ""proprietary"" files and the project has exploded to a point where it needs a database and I'm thinking ""you know I wish I'd written this for a database to start with"" - because the refactoring of the code takes way too much time and effort. From this I've learnt that future proofing my application so that when it gets bigger I don't have to go and spend days refactoring is the way to go forward. How do I do this? SQLite. It works as a database uses SQL and is pretty easy to change over to mySQL (espescially if you're using abstracted classes for database manipulation like I do!) In fact espescially with the ""accepted answer""'s method it can drastically cut the memory usage of your app (you don't have to load all the ""RECORDS"" into PHP)  That's true. serialize() can be pretty useful for that as well. I think the trick to coming up with a viable system is finding some way to index the data nodes without killing yourself with complexity. post 100 :D upvote This is post #100!  Well what is the nature of the flat databases. Are they large or small. Is it simple arrays with arrays in them? if its something simple say userprofiles built as such: $user = array(""name"" => ""dubayou"" ""age"" => 20 ""websites"" => array(""dubayou.com""""willwharton.com""""codecream.com"") ""and_one"" => ""more""); and to save or update the db record for that user. $dir = ""../userdata/""; //make sure to put it bellow what the server can reach. file_put_contents($dir.$user['name']serialize($user)); and to load the record for the user function &get_user($name){ return unserialize(file_get_contents(""../userdata/"".$name)); } but again this implementation will vary on the application and nature of the database you need.  One framework I'm considering would be for a blogging platform. Since just about any possible view of data you would want would be sorted by date I was thinking about this structure: One directory per content node: ./content/YYYYMMDDHHMMSS/ Subdirectories of each node including /tags /authors /comments as well as simple text files in the node directory for pre- and post-rendered content and the like. This would allow a simple PHP glob() call (and probably a reversal of the result array) to query on just about anything within the content structure:glob(""content/*/tags/funny""); would return paths including all articles tagged ""funny"".  If you want a human-readable result you can also use this type of file : ofaurax|27|male|something| another|24|unknown|| ... This way you have only one file you can debug it (and manually fix) easily you can add fields later (at the end of each line) and the PHP code is simple (for each line split according to |). However the drawbacks is that you should parse the entire file to search something (if you have millions of entry it's not fine) and you should handle the separator in data (for example if the nick is WaR|ordz).  This one is inspiring as a practical solution: https://github.com/mhgolkar/FlatFire It uses multiple strategies to handling data... [Copied from Readme File] Free or Structured or Mixed - STRUCTURED Regular (table row column) format. [DATABASE] / \ TX TableY \_____________________________ |ROW_0 Colum_0 Colum_1 Colum_2| |ROW_1 Colum_0 Colum_1 Colum_2| |_____________________________| - FREE More creative data storing. You can store data in any structure you want for each (free) element its similar to storing an array with a unique ""Id"". [DATABASE] / \ EX ElementY (ID) \________________ |Field_0 Value_0 | |Field_1 Value_1 | |Field_2 Value_2 | |________________| recall [ID]: get_free(""ElementY"") --> array([Field_0]=>Value_0[Field_1]=>Value_1... - MIXD (Mixed) Mixed databases can store both free elements and tables.If you add a table to a free db or a free element to a structured db flat fire will automatically convert FREE or SRCT to MIXD database. [DATABASE] / \ EX TY  You might consider SQLite. It's almost as simple as flat files but you do get a SQL engine for querying. It works well with PHP too. SQLite was build into 5.0+ by default but discountinued (!) from PHP 5.4+ on !!! As I write this in July 2012 SQLite will not work on up-to-date systems anymore by default. Official statement [here](http://www.php.net/manual/en/sqlite.requirements.php) Installing the SQLite PDO driver is pretty trivial if you have server access. On Ubuntu/Debian running Apache2 just do apt-get install php5-sqlite service apache2 restart  Just pointing out a potential problem with a flat file database with this type of system: data|some text|more data row 2 data|bla hbalh|more data ...etc The problem is that the cell data contains a ""|"" or a ""\n"" then the data will be lost. Sometimes it would be easier to split by combinations of letters that most people wouldn't use. For example: Column splitter: #$% (Shift+345) Row splitter: ^&* (Shift+678) Text file: test data#$%blah blah#$%^&*new row#$%new row data 2 Then use: explode(""#$%"" $data); use foreach the explode again to separate columns Or anything along these lines. Also I might add that flat file databases are good for systems with small amounts of data (ie. less than 20 rows) but become huge memory hogs for larger databases.  Here's the code we use for Lilina. It stores each entry as a separate file which we found is efficient enough for use (no unneeded data is loaded and it's faster to save).  If you're going to use a flat file to persist data use XML to structure the data. PHP has a built-in XML parser.  One way to store flat-file content would be to save literal arrays to php files. For example: $data = array(); if( $_POST ) { $data = $_POST; $content = ""<?php\n""; $content .= '$data=' . var_export($data true) . ""\n""; $content .= ""?>""; save_to_file($filename $content); } // echo form `""IMHO you have two options if you want to avoid homebrewing something: 1) SQLite If you're familiar with PDO you can install a PDO driver that supports SQLite. Never used it but I have used PDO a ton with MySQL. I'm going to give this a shot on a current project. 2) XML Done this many times for relatively small amounts of data. XMLReader is a lightweight read-forward cursor-style class. SimpleXML makes it simple to read an XML document into an object that you can access just like any other class instance.  I have written two simple functions designed to store data in a file. You can judge for yourself if it's useful in this case. The point is to save a php variable (if it's either an array a string or an object) to a file. <?php function varname(&$var) { $oldvalue=$var; $var='AAAAB3NzaC1yc2EAAAABIwAAAQEAqytmUAQKMOj24lAjqKJC2Gyqhbhb+DmB9eDDb8+QcFI+QOySUpYDn884rgKB6EAtoFyOZVMA6HlNj0VxMKAGE+sLTJ40rLTcieGRCeHJ/TI37e66OrjxgB+7tngKdvoG5EF9hnoGc4eTMpVUDdpAK3ykqR1FIclgk0whV7cEn/6K4697zgwwb5R2yva/zuTX+xKRqcZvyaF3Ur0Q8T+gvrAX8ktmpE18MjnA5JuGuZFZGFzQbvzCVdN52nu8i003GEFmzp0Ny57pWClKkAy3Q5P5AR2BCUwk8V0iEX3iu7J+b9pv4LRZBQkDujaAtSiAaeG2cjfzL9xIgWPf+J05IQ=='; foreach($GLOBALS as $var_name => $value) { if ($value === 'AAAAB3NzaC1yc2EAAAABIwAAAQEAqytmUAQKMOj24lAjqKJC2Gyqhbhb+DmB9eDDb8+QcFI+QOySUpYDn884rgKB6EAtoFyOZVMA6HlNj0VxMKAGE+sLTJ40rLTcieGRCeHJ/TI37e66OrjxgB+7tngKdvoG5EF9hnoGc4eTMpVUDdpAK3ykqR1FIclgk0whV7cEn/6K4697zgwwb5R2yva/zuTX+xKRqcZvyaF3Ur0Q8T+gvrAX8ktmpE18MjnA5JuGuZFZGFzQbvzCVdN52nu8i003GEFmzp0Ny57pWClKkAy3Q5P5AR2BCUwk8V0iEX3iu7J+b9pv4LRZBQkDujaAtSiAaeG2cjfzL9xIgWPf+J05IQ==') { $var=$oldvalue; return $var_name; } } $var=$oldvalue; return false; } function putphp(&$var $file=false) { $varname=varname($var); if(!$file) { $file=$varname.'.php'; } $pathinfo=pathinfo($file); if(file_exists($file)) { if(is_dir($file)) { $file=$pathinfo['dirname'].'/'.$pathinfo['basename'].'/'.$varname.'.php'; } } file_put_contents($file'<?php'.""\n\$"".$varname.'='.var_export($var true)."";\n""); return true; } This is a four-year-old question with an accepted answer and many additional answers. Consider focusing on newer questions unless the accepted answer here is clearly wrong or inadequate.  That's true. serialize() can be pretty useful for that as well. You could serialize but this saves a step since you don't have to unserialize. Not sure if its actually better performance-wise. I think the trick to coming up with a viable system is finding some way to index the data nodes without killing yourself with complexity. Maybe have one central file that keeps track of all indexes? Like tags.txt which has a list of all post id's associated with each tags. May have to keep redundant data around to save time.  A low level object API flatfile database is Mimesis (an open source PHP flat file database low-level API)."156,A,"Getting Configuration value from web.config file using VB and .Net 1.1 I have the following web config file. I am having some difficulty in retrieving the value from the ""AppName.DataAccess.ConnectionString"" key. I know I could move it to the AppSetting block and get it realtively easily but I do not wnat to duplicate the key (and thereby clutter my already cluttered web.config file). Another DLL (one to which I have no source code) uses this block and since it already exists why not use it. I am a C# developer (using .Net 3.5) and this is VB code (using .Net 1.1 no less) so I am already in a strange place (where is my saftey semicolon?). Thanks for your help!! <?xml version=""1.0""?> <configuration> <configSections> <section name=""AppNameConfiguration"" type=""AppName.SystemBase.AppNameConfiguration SystemBase""/> </configSections> <AppNameConfiguration> <add key=""AppName.DataAccess.ConnectionString"" value=""(Deleted to protect guilty)"" /> </AppNameConfiguration> <appSettings> ...other key info deleted for brevity... </appSettings> <system.web> ... </system.web> </configuration> <section name=""AppNameConfiguration"" type=""AppName.SystemBase.AppNameConfiguration SystemBase""/> The custom section is supposed to have a class that defines how the various configuration data can be managed (This is in the Type section). Is this class not available for you to examine? MSDN has a decent explanation of how to create custom configuration sections in VB that may be helpful to you: http://msdn.microsoft.com/en-us/library/2tw134k3.aspx"157,A,"How to convince a company to switch their Source Control My current place of employment is currently in a transition new ownership has taken over things are finally getting standardized and proper guidelines are being enforced. But we are still using VSS there really isn't any reason for using it other then that's what whats initially setup. We don't use Visual Studio or any tool really that specifically requires it. What would be the absolute best argument I can bring up to help convince them that going to something like Subversion would be a much better solution in the long run. Get them to google for 'vss problem' 'source safe corruption' or simply look at the Wiki page for it. That ought to convince them that it's probably not a long-term viable thing for you to be betting such a vital part of your business on. How big is your team? (ie I mean how many members not whether or not you're salad dodgers) Once you start to get more than half a dozen quite active users VSS is going to give you headaches. I seriously doubt that Microsoft use it (in fact don't they use a customised Subversion or CVS variant?) and you've got to ask yourself - if the company don't eat their own dogfood why would you eat it?  Even if it ain't broke there's a potential benefit to migrating from VSS. First and most trivially you won't have to buy new VSS licenses. Second there are many examples of deficiencies in the VSS product (some also acknowledged by MS). The learning curve for SVN is at least as low as for VSS and if you have devs happier with their source control system they're more likely to use it early and often. That will translate to lots less risk for your company and that's a good benefit.  By just going over the features good source control brings: ability to easily see logs of who did what when and in what order to which files keep a history of past versions of everything easily go back and reproduce a specific version of your files from any past version to more easily reproduce bugs reported in older versions ability go retrieve deleted code or remove unwanted changes without having to worry about losing data in the process  @Jason: VSS is broken. I think the most powerful method for motivating a change away from VSS is to point out how critical an asset your source code is. Taking risks with its integrity is not a wise business choice. Add that your programmers are the creators of this asset and that making it easier for them to be productive means more value in your source code asset. Joel on Software often talks about how investing in his programmers is a big win for his company. The other answers here all describe specific reasons that you can point to when making your case.  I've previously written about why VSS is not a good idea. You might be able to gain some information from that. Also this article and this one contain further information. VSS 2005 has papered over some of the cracks in 6.0 but not in a particularly convincing way. The same brain-dead foundation remains.  being able to handle branching and forking is a start. Try using subversion for a while in parallel to vss you will most likely find many arguments to convince your boss. If you don't your boss is right no reason to switch.  VSS totally relies on the clients to manage the database. If a client drops connection in the middle of a write over the network at just the wrong time your file is trashed on the server. Not just the tip but all the history. Hope you have a good backup. I've been through it. It's bad news. VSS usage over VPN or other remote connections is abysmal. It's using SMB to transfer the data and you have to retrieve the file and all of its deltas just to get the tip. Nasty. I've seen VSS start to act up at 1GB of data. Database errors etc. MS (somewhere in a FAQ or KB) says that 2GB is really the max safe limit. There are no good management tools (the clients run the asylum) so you don't really get any warning about this. Anything with a server process to provide some level of transactions and integrity control is a superior solution.  @Adam Davis: Uhhh actually Adam VSS is a horrible source control system. It has a long history of corrupting history and losing data. It is terrible at merging doesn't handle multiple developers well and is very slow. Also the history is poor. Microsoft don't really support it any more you'll note that they never used it for their own internal development and now they don't even sell it in favour of a more modern solution (VSTS). In short if you have to choose between VSS and any other type of source control go with the alternative.  Why Subversion over VSS? Free software Easier to manage ""check-ins"" are atomic! Easy to Branch and Merge Continued development (i.e. VSS is dead end) Better tools for tracking changes and viewing logs Toolset and platform agnostic but also integrates with many tools I made the proposal to my manager and it was a pretty easy sell. I've found it to be much easier to use especially for branching (our project took 5 hours to ""share and pin"" in VSS and then each operation took extra time to complete!).  The internet is littered with well written articles on the flaws of VSS. I would collect this as a body of evidence for moving away from VSS. Find a key requirement that VSS can't support (remote working support on other OSs tools integration) and use it to drive your issue. You then need to find a source control system that is a good match for your organisation's requirements - are you sure Subversion is that system? Set up a demonstration of your chosen system and use this to prove its worth. I implemented this change at a previous employer (first to CVS and then to SVN) and while it was successful we had to build a lot of bits around the edge and rely on a lot of (sometimes unreliable) open source projects to get all the tools we needed. With hindsight I should have considered trying to evaluate professional tools such as Perforce Vault or even Team System. Having evaluated these I could have made a proper value judgement on whether CVS/SVN were worth their ""free"" price tag.  Basic answer is that you have to make the case that switching meets the needs of the business. For example: lower cost of development shorter schedule (another shade of #1) more apt for meeting process requirements (like software requirements traceability or build reproducibility etc). Making the case on these things also requires something quantitative not just ""we will lower costs because this is the right way to do it!"". One thing to watch out for is that it's too easy for a developer to convince themselves that it would be beneficial to make the change without first going through the basic business filters. Once that happens you end up with developers who are unhappy with their tools and are doubly frustrated because they think management won't listen. If you can't check off one of the things above them you'll have no chance of persuading management of anything (unless management is incompetent but that's for another question).  Any document that proves switching will lower costs. Failing that multi-colored graphs and charts. Maybe a power-point presentation. Lower costs is NOT the main driver for a source control system.  The best argument would have to be the reason why you want them to switch to subversion. :) I know absolutely nothing about VSS but the phrase ""if it ain't broken don't fix it"" comes to mind. You have to show your managers that VSS is broken and needs fixing. Even better if you can show management how it would save them money. Even if other people say VSS is broken and corrupts source if the company hasn't had bad experiences then they should be very cautious about changing. Changing source control system is a huge and risky undertaking even if you're going to something 'better'.  In addition to the technical points given in other answers there may be non-technical reasons lurking that you should be prepared to respond to: You should investigate whether your company has any sort of policy against (or misguided fear of) open source software. If the company or its lawyers dont understand the ins and outs of which licenses infect proprietary code and which dont as well as what you can do with open source code that doesnt affect your proprietary code you will have a hard time getting them to switch from a proprietary to an an open source tool. (And you may have a bigger education job on your hands.) In arguing for the switch from proprietary (e.g. VSS) to open source (e.g. subversion) youll also need to be prepared to defend the quality of the code and the lack of any need for a warranty or other contract rights regarding the code."158,A,"How can I create Debian install packages in Windows for a Visual Studio project? I'm developing some cross platform software targeting Mono under Visual Studio and would like to be able to build the installers for Windows and Linux (Ubuntu specifically) with a single button click. I figure I could do it by calling cygwin from a post-build event but I was hoping for at best a Visual Studio plugin or at worst a more Windows-native way of doing it. It seems like the package format is fairly simple and this must be a common need. edit: Re-asked question under other account due to duplicate login issue. Debian's .deb packages are just ""ar"" archives containing tarballs. You can manipulate both types of files using cygwin or msys quite easily: $ ar xv asciidoc_8.2.1-2_all.deb x - debian-binary x - control.tar.gz x - data.tar.gz $ tar -tzf control.tar.gz ./ ./conffiles ./md5sums ./control Or you can install all the ""standard"" Debian stuff using cygwin I suppose but most of that stuff won't benefit you much if you're building a .Net app anyway.  If you don't mind using Java tools it's possible to build Debian packages with jdeb in an Ant script. That's probably lighter than relying on Cygwin.  I am not aware of any plugin that does it natively especially since Mono users seem to prefer MonoDevelop. However it should be possible to use Cygwin and a custom MSBuild Task or Batch file in order to achieve that by using the native .deb creation tools.  this must be a common need. Some small percentage of software developers develop for .NET Some very small percentage of that group develop for mono Some small percentage of that group wants to provide .debs instead of just a zip Some very small percentage of that group wants to build their linux apps on windows instead of natively on linux It's just you :-) haha maybe..but I'm kinda surprised people really prefer MonoDevelop given it doesn't even have a debugger and there are free versions of Visual Studio available."159,A,"C++ unit testing framework I use the Boost Test framework for my C++ code but there are two problems with it that are probably common to all C++ test frameworks: There is no way to create automatic test stubs (by extracting public functions from selected classes for example). You cannot run a single test - you have to run the entire 'suite' of tests (unless you create lots of different test projects I guess). Does anyone know of a better testing framework or am I forever to be jealous of the test tools available to Java/.NET developers? CppUnit was the original homage to JUnit.  Great question! A few years ago I looked around forever for something worth using and came up short. I was looking for something that was very lightweight and did not require me to link in some libraries... you know something I could get up and running in minutes. However I persisted and ended up running across cxxtest. From the website: Doesn't require RTTI Doesn't require member template functions Doesn't require exception handling Doesn't require any external libraries (including memory management file/console I/O graphics libraries) Is distributed entirely as a set of header files (and a python script). Wow... super simple! Include a header file derive from the Test class and you're off and running. We've been using this for the past four years and I've still yet to find anything that I'm more pleased with. That link is no longer valid they migrated to github. https://github.com/CxxTest/cxxtest @leetNightshade Updated. Thank you. :) Maybe you should try CATCH. I say ""maybe"" because it does require exceptions and member template functions. The rest is the same as your list (without the Python script). See my answer for more info.  I'm a big fan of UnitTest++ it's very lightweight but does the job. You can run single tests there easily.  I like the Boost unit test framework principally because it is very lightweight. I never heard of a unit-test framework that would generate stubs. I am generally quite unconvinced by code generation if only because it gets obsolete very quickly. Maybe it becomes useful when you have a large number of classes? A proponent of Test Driven Development would probably say that it is fundamental that you run the whole test suite every time as to make sure that you have not introduced a regression. If running all the tests take too much time maybe your tests are too big or make too many calls to CPU intensive functions that should be mocked out? If it remains a problem a thin wrapper around the boost unit-tests should allow you to pick your tests and would probably be quicker than learn another framework and port all your tests. Generating stubs is the only way you'll be able to reduce the overhead of writing tests to a few keystrokes.  Take a look at the Google C++ Testing Framework. It's used by Google for all of their in-house C++ projects so it must be pretty good. http://googletesting.blogspot.com/2008/07/announcing-new-google-c-testing.html http://code.google.com/p/googletest  I'm using tut-framework  I too am a fan of UnitTest++. The snag is that the source distribution contains almost 40 seperate files. This is absurd. Managing the source control and build tasks for a simple project is dominated by looking after all these unit testing files. I have modified UnitTest++ so that it can be integrated with a project by adding one .h and .cpp file. This I have dubbed ""cutest"". Details are at http://ravenspoint.com/blog/index.php?entry=entry080704-063557 It does not automatically generate test stubs as asked for in the original question. I cannot help thinking that such a feature would be more trouble than it is worth generating vast amounts of useless code ""testing"" accessor functions.  I would imagine automatically stubbing out test functions would be more of a function (of scripts for the framework or) the development environment in question. Supposedly CodeGear's C++Builder applications will quickly generate test code for user functions.  Try WinUnit. It sounds excellent and is recommended by John Robbins. I'm using it fairly light and easy to get started. Test suites are organised into DLLs which are then loaded by the winunit exec. Possibility to launch single test see http://winunit.codeplex.com/wikipage?title=List%20of%20WinUnit%20command-line%20options&referringTitle=Home  Boost.Test does allow to run test case by name. Or test suite. Or several of them. Boost.Test does NOT insists on implementing main though it does make it easy to do so. Boost.Test is NOT necessary to use as a library. It has single header variant.  I'm trying out Igloo also a header only C++ test suite even it's two included dependencies are header only. So it's pretty straightforward and simple. Besides the included example on github there's examples and more details at the main site igloo-testing.org. I'll update this later as I get more experience with it and other frameworks.  I just responded to a very similar question. I ended up using Noel Llopis' UnitTest++. I liked it more than boost::test because it didn't insist on implementing the main program of the test harness with a macro - it can plug into whatever executable you create. It does suffer from the same encumbrance of boost::test in that it requires a library to be linked in. I've used CxxTest and it does come closer than anything else in C++-land to automatically generating tests (though it requires Perl to be part of your build system to do this). C++ just does not provide the reflection hooks that the .NET languages and Java do. The MsTest tools in Visual Studio Team System - Developer's Edition will auto-generate test stubs of unmanaged C++ but the methods have to be exported from a DLL to do this so it does not work with static libraries. Other test frameworks in the .NET world may have this ability too but I'm not familiar with any of those. So right now we use UnitTest++ for unmanaged C++ and I'm currently deciding between MsTest and NUnit for the managed libraries.  Andrew Marlow's Fructose library's worth checking out... http://fructose.sourceforge.net/ I recall his documents containing a fairly detailed analysis and comparison of other offering at the time he wrote Fructose but can't find a URL direct to that document.  http://groups.google.com/group/googletestframework but it's pretty new  I've just pushed my own framework CATCH out there. It's still under development but I believe it already surpasses most other frameworks. Different people have different criteria but I've tried to cover most ground without too many trade-offs. Take a look at my linked blog entry for a taster. My top five features are: Header only Auto registration of function and method based tests Decomposes standard C++ expressions into LHS and RHS (so you don't need a whole family of assert macros). Support for nested sections within a function based fixture Name tests using natural language - function/ method names are generated It doesn't do generation of stubs - but that's a fairly specialised area. I think Isolator++ is the first tool to truly pull that off. Note that Mocking/ stubbing frameworks are usually independent of unit testing frameworks. CATCH works particularly well with mock objects as test state is not passed around by context. It also has Objective-C bindings. [update] Just happened back across this answer of mine from a few years ago. Thanks for all the great comments! Obviously Catch has developed on a lot in that time. It now has support for BDD style testing (given/ when/ then) tags now in a single header and loads of internal improvements and refinements (e.g. richer command line clear and expressive output etc). A more up-to-date blog post is here. Excellent test suite. (+1) One the best and easiest to work with even in its early stages. The only thing missing is mockups although I require them rarely so I have no complaints. +1. Single header include == awesome. This rocks. The way you use expression templates to substitute the various assertion macros of other test suites is ingenious. Can it do parameterised tests? Also a floating point approximate equality assertion would be nice. Absolutely great ! Is there a way to speed up the build process? So I can get faster red-green-refactor-cycles? It takes 15sec to build .. I know it's not that much in absolute time but it feels like a lot. Anyways THANK YOU !!! I didn't use it but for me it looks like the single best c++ unit test framework. A pity that it's not integrated in something like boost which people usually use.  Eclipse/JUnit is a solid package for java and there are C++ extensions/equivalents for both. It can do what you're talking about. Of course you'd have to change IDEs...  Aeryn is another framework worth looking at Link is dead. Is this the same project? http://sourceforge.net/projects/aeryn/ @leetNightshade Yes. I have updated the link thanks  Visual Studio has a built-in unit testing framework this is a great link to setting up a test project for win32 console application: http://codeketchup.blogspot.ie/2012/12/unit-test-for-unmanaged-c-in-visual.html If you are working on a static DLL project it is much easier to set up as other have pointed out external tesing frameworks like GTest and Boost have extra features."160,A,"How do I create a status dialog box in Excel I have created a database report generator in Excel. I am trying to create a dialog box that displays status information as the program runs. When I generate the report although the dialog box appears I cannot refresh/update the information it displays. Most of the time the dialog box only partially appears. I have tried using the .repaint method but I still get the same results. I only see the complete dialog box after the report is generated. The dialog box is also running on the same UI thread. So it is too busy to repaint itself. Not sure if VBA has good multi-threading capabilities.  Try adding a DoEvents call in your loop. That should allow the form to repaint & accept other requests.  Insert a blank sheet in your workbook Rename the Sheet eg. ""information"" Sheets(""information"").Select Range(""C3"").Select ActiveCell.FormulaR1C1 = ""Updating Records"" Application.ScreenUpdating = False Application.Wait Now + TimeValue(""00:00:02"") Continue Macro Sheets(""information"").Select Range(""C3"").Select Application.ScreenUpdating = True ActiveCell.FormulaR1C1 = ""Preparing Information"" Application.ScreenUpdating = False Application.Wait Now + TimeValue(""00:00:02"") Continue Macro Etc Alternatively select a blank cell somewhere on the existing sheet instead of inserting a new sheet Range(""C3"").Select ActiveCell.FormulaR1C1 = ""Updating Records"" Application.ScreenUpdating = False Application.Wait Now + TimeValue(""00:00:02"") Etc  The code below works well when performing actions within Excel (XP or later). For actions that take place outside Excel for example connecting to a database and retrieving data the best this offers is the opportunity to show dialogs before and after the action (e.g. ""Getting data"" ""Got data"") Create a form called ""frmStatus"" put a label on the form called ""Label1"". Set the form property 'ShowModal' = false this allows the code to run while the form is displayed. Sub ShowForm_DoSomething() Load frmStatus frmStatus.Label1.Caption = ""Starting"" frmStatus.Show frmStatus.Repaint 'Load the form and set text frmStatus.Label1.Caption = ""Doing something"" frmStatus.Repaint 'code here to perform an action frmStatus.Label1.Caption = ""Doing something else"" frmStatus.Repaint 'code here to perform an action frmStatus.Label1.Caption = ""Finished"" frmStatus.Repaint Application.Wait (Now + TimeValue(""0:00:01"")) frmStatus.Hide Unload frmStatus 'hide and unload the form End Sub  I have used Excel's own status bar (in bottom left of window) to display progress information for a similar application I developed in the past. It works out very well if you just want to display textual updates on progress and avoids the need for an updating dialog at all. Ok @JonnyGold here's an example of the kind of thing I used... Sub StatusBarExample() Application.ScreenUpdating = False ' turns off screen updating Application.DisplayStatusBar = True ' makes sure that the statusbar is visible Application.StatusBar = ""Please wait while performing task 1..."" ' add some code for task 1 that replaces the next sentence Application.Wait Now + TimeValue(""00:00:02"") Application.StatusBar = ""Please wait while performing task 2..."" ' add some code for task 2 that replaces the next sentence Application.Wait Now + TimeValue(""00:00:02"") Application.StatusBar = False ' gives control of the statusbar back to the programme End Sub Hope this helps! Please could you add an example thanks."161,A,What's a good JavaScript plugin color picker? I make a lot of web applications and from time to time I need a color picker. What's one that I can use like an API and doesn't require a lot of code to plug in? I also need it to work in all browsers. I haven't personally implemented this but I have heard good things about it and it appears to be a great script: http://johndyer.name/post/2007/09/PhotoShop-like-JavaScript-Color-Picker.aspx  I like jscolor the most lightweight and lots of options.  Farbtastic is a nice jQuery color picker But apparently doesn't work in IE6 Here is another jQuery color picker that looks nice not sure about it compatibility though. It not ambiguous. If you call GPL'd JavaScript from your JavaScript and distribute it then you must release your JavaScript under the GPL also. This is the same as for any other programming language. So please avoid GPL'd JavaScript libraries. @SamWatkins - It *is* ambiguous. What if the library automatically runs on load? Since it's part of my page is it all one work or is each `162,A,"How do you use a variable in xsl when trying to select a node? I would have thought this would be an easy one to Google but I've been unsucessful. I want to assign a variable the value out of an attribute (easy so far) then use that variable to select another node based on the value of that attribute. Example: <xsl:variable name=""myId"" select=""@id"" /> <xsl value-of select=""//Root/Some/Other/Path/Where[@id='{@myId}']/@Name /> That does not work. If I replace the {@myId} with the value that is in the variable then it does find the right node but doign it this way produces nothing. I'm sure I'm missing something or perhaps there is a different way to do it. The context is that there is related data under different top-level nodes that share the same id value so I need to get the related nodes in my template. Ok I finally figured it out. Silly problem really I simply needed to leave out the quotes and the braces. One of those times when I thought that I'd already tried that. :D Oh and I mistyped @myId in the first example the code was actually $myId. <xsl:variable name=""myId"" select=""@id"" /> <xsl value-of select=""//Root/Some/Other/Path/Where[@id=$myId]/@Name"" /> There is still a quote that shouldn't be there. It is a very small edit so I cannot do it without changing also the text (minimum characters limit). They can obviously be combined so you can include a variable in an Attribute Value Template such as: <newElement Id=""{$myId}""/>"163,A,"What's the best way to calculate a 3D (or n-D) centroid? As part of a project at work I have to calculate the centroid of a set of points in 3D space. Right now I'm doing it in a way that seems simple but naive -- by taking the average of each set of points as in: centroid = average(x) average(y) average(z) where x y and z are arrays of floating-point numbers. I seem to recall that there is a way to get a more accurate centroid but I haven't found a simple algorithm for doing so. Anyone have any ideas or suggestions? I'm using Python for this but I can adapt examples from other languages. You got it. What you are calculating is the centroid or the mean vector.  Potentially more efficient: if you're calculating this multiple times you can speed this up quite a bit by keeping two standing variables N # number of points sums = dict(x=0y=0z=0) # sums of the locations for each point then changing N and sums whenever points are created or destroyed. This changes things from O(N) to O(1) for calculations at the cost of more work every time a point is created moves or is destroyed.  You vaguely mention ""a way to get a more accurate centroid"". Maybe you're talking about a centroid that isn't affected by outliers. For example the average household income in the USA is probably very high because a small number of very rich people skew the average; they are the ""outliers"". For that reason statisticians use the median instead. One way to obtain the median is to sort the values then pick the value halfway down the list. Maybe you're looking for something like this but for 2D or 3D points. The problem is in 2D and higher you can't sort. There's no natural order. Nevertheless there are ways to get rid of outliers. One way is to find the convex hull of the points. The convex hull has all the points on the ""outside"" of the set of points. If you do this and throw out the points that are on the hull you'll be throwing out the outliers and the points that remain will give a more ""representative"" centroid. You can even repeat this process several times and the result is kind like peeling an onion. In fact it's called ""convex hull peeling"". So if I understand this correctly if the centroid is like the mean of a linear set does convex hull peeling get you the point analogous to the median?  you can use increase accuracy summation - Kahan summation - was that what you had in mind? No I'm not looking to get a more accurate sum before averaging if that's what you mean. I was just wondering if I was calculating the centroid correctly. Thanks though -- I hadn't even heard of this.  Nope that is the only formula for the centroid of a collection of points. See Wikipedia: http://en.wikipedia.org/wiki/Centroid  Yes that is the correct formula. If you have a large number of points you can exploit the symmetry of the problem (be it cylindrical spherical mirror). Otherwise you can borrow from statistics and average a random number of the points and just have a bit of error. Specifically the mean of a random subset of points is an unbiased estimator of the mean of the whole group.  A ""more accurate centroid"" I believe centroid is defined the way you calculated it hence there can be no ""more accurate centroid""."164,A,"Configuring sendmail behind a firewall I'm setting up a server which is on a network behind a firewall and I want programs on this computer to be able to use sendmail to send emails to any email address. We have an SMTP server running on this network (let's call it mailrelay.example.com) which is how we're supposed to get outgoing emails through the firewall. So how do I configure sendmail to send all mail through mailrelay.example.com? Googling hasn't given me the answer yet and has only revealed that sendmail configuration is extremely complex and annoying. @eli: modifying sendmail.cf directly is not usually recommended since it is generated by the macro compiler. Edit /etc/mail/sendmail.mc to include the line:  define(`SMART_HOST'`mailrelay.example.com')dnl After changing the sendmail.mc macro configuration file it must be recompiled to produce the sendmail configuration file.  # m4 /etc/mail/sendmail.mc > /etc/sendmail.cf And restart the sendmail service (Linux):  # /etc/init.d/sendmail restart As well as setting the smarthost you might want to also disable name resolution configuration and possibly shift your sendmail to non-standard port or disable daemon mode. Disable Name Resolution Servers that are within fire-walled networks or using Network Address Translation (NAT) may not have DNS or NIS services available. This creates a problem for sendmail since it will use DNS by default and if it is not available you will see messages like this in mailq:  host map: lookup (mydomain.com): deferred) Unless you are prepared to setup an appropriate DNS or NIS service that sendmail can use in this situation you will typically configure name resolution to be done using the /etc/hosts file. This is done by enabling a 'service.switch' file and specifying resolution by file as follows: 1: Enable service.switch for sendmail Edit /etc/mail/sendmail.mc to include the lines:  define(`confSERVICE_SWITCH_FILE'`/etc/mail/service.switch')dnl 2: Configure service.switch for files Create or modify /etc/mail/service.switch to refer only to /etc/hosts for name resolution:  # cat /etc/mail/service.switch hosts files 3: Recompile sendmail.mc and restart sendmail for this setting to take effect. Shift sendmail to non-standard port or disable daemon mode By default sendmail will listen on port 25. You may want to change this port or disable the sendmail daemon mode altogether for various reasons: - if there is a security policy prohibiting the use of well-known ports - if another SMTP product/process is to be running on the same host on the standard port - if you don't want to accept mail via smtp at all just send it using sendmail 1: To shift sendmail to use non-standard port. Edit /etc/mail/sendmail.mc and modify the ""Port"" setting in the line:  DAEMON_OPTIONS(`Port=smtpAddr=127.0.0.1 Name=MTA') For example to get sendmail to use port 125:  DAEMON_OPTIONS(`Port=125Addr=127.0.0.1 Name=MTA') This will require sendmail.mc to be recompiled and sendmail to be restarted. 2: Alternatively to disable sendmail daemon mode altogether (Linux) Edit /etc/sysconfig/sendmail and modify the ""DAEMON"" setting to:  DAEMON=no This change will require sendmail to be restarted. Thanks! That tip about `service.switch` solved my problem.  @Espo: Thanks for the great advice on where to start. Your link would have been better if I had been configuring sendmail for its first use instead of taking an existing configuration and making this small change. However once I knew to look for stuff on ""SmartHost"" I found an easier way. All I had to do was edit my /etc/mail/sendmail.cf file to change DS to DSmailrelay.example.com then restart sendmail and it worked.  http://www.elandsys.com/resources/sendmail/smarthost.html Sendmail Smarthost A smarthost is a host through which outgoing mail is relayed. Some ISPs block outgoing SMTP traffic (port 25) and require their users to send out all mail through the ISP's mail server. Sendmail can be configured to use the ISP's mail server as the smart host. Read the linked article for instruction for how to set this up."165,A,Email Delivery Question This question comes on the heels of the question asked here. The email that comes from our web server comes from an IP address that is different than that for the Exchange server. Is this okay if the SPF and Domain keys are setup properly? It should just fine. However some spam filters will do a reverse lookup on the originating IP address and see if it's assigned to the domain name the email claims to be from and some may check to see if the IP is an actual MX for the domain. So the downside is that some recipients may never get the email and you may not know about it for a long time. I'd suggest routing your mail through an established MX rather than having a webserver do it directly (there are some security implications there too).  Short answer: Yes166,A,"Is there a good library for dealing with the Modbus protocol in .NET? Does anyone know of a good (preferably open source) library for dealing with the Modbus protocol? I have seen a few libraries but I am looking for some people's personal experiences not just the top ten Google hits. I figure there has to be at least one other person who deals with PLCs and automation hardware like I do out there. Open to any other materials that might have been a help to you as well... I like this one (Modbus via TCP): http://www.codeproject.com/Tips/16260/Modbus-TCP-class It's C# open source and was fairly easy to convert to VB.NET.  I have done a lot of communication with devices for the past few years since I work for a home automation company but we don't use Modbus. We do communication in a standard and open way using Web Services for Devices(WSD) which is also know as Devices Profile for Web Services(DPWS). During this time at one point I did hear of a project called NModbus. It is an open source library for working with modbus. I have not used it but looking at the site and the changesets on Google Code it looks pretty active. You may want to give it a look and even get involved in. This is the only library that I have heard of that targets .Net. Yeah I saw nmodbus and it looked ok but last time I tried it it was a bit rough around the edges. I will give it a try. Really I am now interested in reading up on WSD. Thanks for the link! Sure thing. WSD is really the way to go. Modbus is a very old protocol I know you may not have a choice but devices of today are starting to target WSD. Thumbs up for NModbus: works really well  Have a look at the offering from Colway Solutions http://www.colwaysolutions.com. They have a unique licensing scheme where you pay for each Modbus function code that you desire to use. Its not free but the pricing seems to be low. I also saw a few ports of the library to some popular microcontrollers and RTOS.  FieldTalk Modbus Library - handles all Modbus functions  Modbus is a very simple protocol to implement. All information you need can easily be found for free on the Internet. If you choose to implement it yourself I will be happy to answer any questions you have along the way. If you choose to go for a modbus master library I would look for: Modbust TCP support. Modbus RTU over TCP/UDP and COM-port. Configurable byte swapping word swapping Configurable ""base"" adress so you can choose adress 1 to actually be adress 0 (sounds stupid but I prefere to always specify adresses the same way they are documented) it must support reading several adresses as a block but it need to be flexible some modbus slaves will return error if any adress in the block is unused/reserved)."167,A,"Loading assemblies and its dependencies My application dynamically loads assemblies at runtime from specific subfolders. These assemblies are compiled with dependencies to other assemblies. The runtime trys to load these from the application directory. But I want to put them into the modules directory. Is there a way to tell the runtime that the dlls are in a seperate subfolder? You can use the <codeBase> element found in the application configuration file. More information on ""Locating the Assembly through Codebases or Probing"". Well the loaded assembly doesn't have an application configuration file. Well if you know the specific folders at runtime you can use Assembly.LoadFrom.  You can use the <probing> element in a manifest file to tell the Runtime to look in different directories for its assembly files. http://msdn.microsoft.com/en-us/library/823z9h8w.aspx e.g.: <configuration> <runtime> <assemblyBinding xmlns=""urn:schemas-microsoft-com:asm.v1""> <probing privatePath=""bin;bin2\subbin;bin3""/> </assemblyBinding> </runtime> </configuration>  One nice approach I've used lately is to add an event handler for the AppDomain's AssemblyResolve event. AppDomain currentDomain = AppDomain.CurrentDomain; currentDomain.AssemblyResolve += new ResolveEventHandler(MyResolveEventHandler); Then in the event handler method you can load the assembly that was attempted to be resolved using one of the Assembly.Load Assembly.LoadFrom overrides and return it from the method. EDIT: Based on your additional information I think using the technique above specifically resolving the references to an assembly yourself is the only real approach that is going to work without restructuring your app. What it gives you is that the location of each and every assembly that the CLR fails to resolve can be determined and loaded by your code at runtime... I've used this in similar situations for both pluggable architectures and for an assembly reference integrity scanning tool. A good example of this technique is the application LINQPad. It ships as a single exe so all the libraries are included as embedded resources. See http://www.albahari.com/nutshell/ch16.aspx for code and http://www.linqpad.net/HowLINQPadWorks.aspx for insight."168,A,Looking for code to render a form that displays a view of an object I've got the task of displaying a web form to represent the properties in a .NET class. In WinForms there's a pre-fab control named PropertyGrid that is a lot like what I need. I'm just looking for something to display a simple layout of property names next to an appropriate control like a textbox for strings or a dropdownlist for enum properties. Does anything like this already exist for ASP.NET or will I be rolling my own here? ASP.Net PropertyGrid169,A,"WPF Data Triggers and Story Boards I'm trying to trigger a progress animation when ever the ViewModel/Presentation Model is Busy. I have a IsBusy Property and the ViewModel is set as the DataContext of the UserControl. What is the best way to trigger a ""progressAnimation"" story board when the IsBusy property is true? Blend only let med add Event-Triggers on a UserControl level and I can only create property triggers in my data templates. The ""progressAnimation"" is defined as a resource in the user control. I tried adding the DataTriggers as a Style on the UserControl but when I try to start the StoryBoard I get the following error: 'System.Windows.Style' value cannot be assigned to property 'Style' of object'Colorful.Control.SearchPanel'. A Storyboard tree in a Style cannot specify a TargetName. Remove TargetName 'progressWheel'. ProgressWheel is the name of the object I'm trying to animate so removing target name is obvisouly NOT what I want. I was hoping to solve this in XAML using data binding techniques in stead of having to expose events and start/stop the animation through code. You can use Trigger.EnterAction to start an animation when a property is changed. <Trigger Property=""IsBusy"" Value=""true""> <Trigger.EnterActions> <BeginStoryboard x:Name=""BeginBusy"" Storyboard=""{StaticResource MyStoryboard}"" /> </Trigger.EnterActions> <Trigger.ExitActions> <StopStoryboard BeginStoryboardName=""BeginBusy"" /> </Trigger.ExitActions> </Trigger> Like I said this is at the user control level and I only accepts EventTriggers (not Property- or DataTriggers). Also the IsBusy is not a property on the UserControl but on the object set as the DataContext (the ViewModel)  I would recommend to use RoutedEvent instead of your IsBusy property. Just fire OnBusyStarted and OnBusyStopped event and use Event trigger on the appropriate elements. Well that was what I was hoping to avoid... But say I do that: any examples on how to implement a RoutedEvent in a class that does NOT derive from UIElement?  Although the answer that proposes attaching the animation directly to the element to be animated solves this problem in simple cases this isn't really workable when you have a complex animation that needs to target multiple elements. (You can attach an animation to each element of course but it gets pretty horrible to manage.) So there's an alternative way to solve this that lets you use a DataTrigger to run an animation that targets named elements. There are three places you can attach triggers in WPF: elements styles and templates. However the first two options don't work here. The first is ruled out because WPF doesn't support the use of a DataTrigger directly on an element. (There's no particularly good reason for this as far as I know. As far as I remember when I asked people on the WPF team about this many years ago they said they'd have liked to have supported it but didn't have time to make it work.) And styles are out because as the error message you've reported says you can't target named elements in an animation associated with a style. So that leaves templates. And you can use either control or data templates for this. <ContentControl> <ContentControl.Template> <ControlTemplate TargetType=""ContentControl""> <ControlTemplate.Resources> <Storyboard x:Key=""myAnimation""> <!-- Your animation goes here... --> </Storyboard> </ControlTemplate.Resources> <ControlTemplate.Triggers> <DataTrigger Binding=""{Binding MyProperty}"" Value=""DesiredValue""> <DataTrigger.EnterActions> <BeginStoryboard x:Name=""beginAnimation"" Storyboard=""{StaticResource myAnimation}"" /> </DataTrigger.EnterActions> <DataTrigger.ExitActions> <StopStoryboard BeginStoryboardName=""beginAnimation"" /> </DataTrigger.ExitActions> </DataTrigger> </ControlTemplate.Triggers> <!-- Content to be animated goes here --> </ControlTemplate> </ContentControl.Template> <ContentControl> With this construction WPF is happy to let the animation refer to named elements inside the template. (I've left both the animation and the template content empty here - obviously you'd populate that with your actual animation nd content.) The reason this works in a template but not a style is that when you apply a template the named elements it defines will always be present and so it's safe for animations defined within that template's scope to refer to those elements. This is not generally the case with a style because styles can be applied to multiple different elements each of which may have quite different visual trees. (It's a little frustrating that it prevents you from doing this even in scenarios when you can be certain that the required elements will be there but perhaps there's something that makes it very difficult for the animation to be bound to the named elements at the right time. I know there are quite a lot of optimizations in WPF to enable elements of a style to be reused efficiently so perhaps one of those is what makes this difficult to support.)  You can subscribe to the PropertyChanged event of the DataObject class and make a RoutedEvent fire from Usercontrol level. For RoutedEvent to work we need to have the class derived from DependancyObject I think you're right... Exposing a RoutedEvent from the UserControl seams like the most obvious solution... However I haven't given up on executing arbitrary storyboards based on data just yet.. But thanks for input!  What you want is possible by declaring the animation on the progressWheel itself: The XAML: <UserControl x:Class=""TriggerSpike.UserControl1"" xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation"" xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml"" Height=""300"" Width=""300""> <UserControl.Resources> <DoubleAnimation x:Key=""SearchAnimation"" Storyboard.TargetProperty=""Opacity"" To=""1"" Duration=""0:0:4""/> <DoubleAnimation x:Key=""StopSearchAnimation"" Storyboard.TargetProperty=""Opacity"" To=""0"" Duration=""0:0:4""/> </UserControl.Resources> <StackPanel> <TextBlock Name=""progressWheel"" TextAlignment=""Center"" Opacity=""0""> <TextBlock.Style> <Style> <Style.Triggers> <DataTrigger Binding=""{Binding IsBusy}"" Value=""True""> <DataTrigger.EnterActions> <BeginStoryboard> <Storyboard> <StaticResource ResourceKey=""SearchAnimation""/> </Storyboard> </BeginStoryboard> </DataTrigger.EnterActions> <DataTrigger.ExitActions> <BeginStoryboard> <Storyboard> <StaticResource ResourceKey=""StopSearchAnimation""/> </Storyboard> </BeginStoryboard> </DataTrigger.ExitActions> </DataTrigger> </Style.Triggers> </Style> </TextBlock.Style> Searching </TextBlock> <Label Content=""Here your search query""/> <TextBox Text=""{Binding SearchClause}""/> <Button Click=""Button_Click"">Search!</Button> <TextBlock Text=""{Binding Result}""/> </StackPanel> Code behind:  using System.Windows; using System.Windows.Controls; namespace TriggerSpike { public partial class UserControl1 : UserControl { private MyViewModel myModel; public UserControl1() { myModel=new MyViewModel(); DataContext = myModel; InitializeComponent(); } private void Button_Click(object sender RoutedEventArgs e) { myModel.Search(myModel.SearchClause); } } } The viewmodel:  using System.ComponentModel; using System.Threading; using System.Windows; namespace TriggerSpike { class MyViewModel:DependencyObject { public string SearchClause{ get;set;} public bool IsBusy { get { return (bool)GetValue(IsBusyProperty); } set { SetValue(IsBusyProperty value); } } public static readonly DependencyProperty IsBusyProperty = DependencyProperty.Register(""IsBusy"" typeof(bool) typeof(MyViewModel) new UIPropertyMetadata(false)); public string Result { get { return (string)GetValue(ResultProperty); } set { SetValue(ResultProperty value); } } public static readonly DependencyProperty ResultProperty = DependencyProperty.Register(""Result"" typeof(string) typeof(MyViewModel) new UIPropertyMetadata(string.Empty)); public void Search(string search_clause) { Result = string.Empty; SearchClause = search_clause; var worker = new BackgroundWorker(); worker.DoWork += worker_DoWork; worker.RunWorkerCompleted += worker_RunWorkerCompleted; IsBusy = true; worker.RunWorkerAsync(); } void worker_RunWorkerCompleted(object sender RunWorkerCompletedEventArgs e) { IsBusy=false; Result = ""Sorry no results found for: "" + SearchClause; } void worker_DoWork(object sender DoWorkEventArgs e) { Thread.Sleep(5000); } } } Hope this helps!"170,A,"How do I make AutoCompleteExtender render above select controls in IE6 When an AutoCompleteExtender is displayed in IE6 it seems to ignore z-index and renders below any select controls (like dropdownlists) in IE6. <asp:TextBox ID=""TextBox1"" runat=""server"" /> <cc1:AutoCompleteExtender ID=""AutoCompleteExtender1"" runat=""server"" TargetControlID=""TextBox1"" EnableCaching=""true"" CompletionSetCount=""5"" FirstRowSelected=""true"" ServicePath=""~/Services/Service1.asmx"" ServiceMethod=""GetSuggestion"" /> <asp:DropDownList ID=""DropDownList1"" runat=""server""> <asp:ListItem Text=""Item 1"" Value=""0"" /> <asp:ListItem Text=""Item 2"" Value=""1"" /> </asp:DropDownList> How do I make it render above dropdownlists? @Orion has this partially correct - there is one other way to deal with these and that is to cover the offending select lists with an iframe. This technique is used in Cody Lindley's ThickBox (written for jQuery). See the code for details on how to do it. Although I haven't got it to work 100% this is the solution which I have got closest with and proceeds with. Let me know if you need particular help I have working code I could probably share with you that would simplify things...If I remember and get time I will try to post it here - but no promises! Contact me via email if you can see my website etc. Seek and you shall find.  Nothing renders below select controls in IE6. It's one of the many ""features"" microsoft bestowed upon us when they gifted IE to the world You have to hide them then re-show them. Observe the standard lightbox script - which does exactly this (note that link is just to the first thing I found on google which had the source to lightbox.js as a demonstration. It's got nothing to do with anything else)"171,A,"How can I delete duplicate rows in a table I have a table with say 3 columns. There's no primary key so there can be duplicate rows. I need to just keep one and delete the others. Any idea how to do this is Sql Server? I'd SELECT DISTINCT the rows and throw them into a temporary table then drop the source table and copy back the data from the temp. EDIT: now with code snippet! INSERT INTO TABLE_2 SELECT DISTINCT * FROM TABLE_1 GO DELETE FROM TABLE_1 GO INSERT INTO TABLE_1 SELECT * FROM TABLE_2 GO that's the cleanest and most generic solution given that you have the disk space (the final frontier) So there's no way to do it using an SQL query? Actually that's three queries: INSERT INTO TABLE_2 SELECT DISTINCT * FROM TABLE_1 GO DELETE FROM TABLE_1 GO INSERT INTO TABLE_1 SELECT * FROM TABLE_2 GO I meant without creating a new table. This can fail if there are tables that depend on this table. Pretty unlikely: it's unsafe to create a FK to a table w/o primary key (if that's what you meant with ""depends"")...  How about: select distinct * into #t from duplicates_tbl truncate duplicates_tbl insert duplicates_tbl select * from #t drop table #t  The following example works as well when your PK is just a subset of all table columns. (Note: I like the approach with inserting another surrogate id column more. But maybe this solution comes handy as well.) First find the duplicate rows: SELECT col1 col2 count(*) FROM t1 GROUP BY col1 col2 HAVING count(*) > 1 If there are only few you can delete them manually: set rowcount 1 delete from t1 where col1=1 and col2=1 The value of ""rowcount"" should be n-1 times the number of duplicates. In this example there are 2 dulpicates therefore rowcount is 1. If you get several duplicate rows you have to do this for every unique primary key. If you have many duplicates then copy every key once into anoher table: SELECT col1 col2 col3=count(*) INTO holdkey FROM t1 GROUP BY col1 col2 HAVING count(*) > 1 Then copy the keys but eliminate the duplicates. SELECT DISTINCT t1.* INTO holddups FROM t1 holdkey WHERE t1.col1 = holdkey.col1 AND t1.col2 = holdkey.col2 In your keys you have now unique keys. Check if you don't get any result: SELECT col1 col2 count(*) FROM holddups GROUP BY col1 col2 Delete the duplicates from the original table: DELETE t1 FROM t1 holdkey WHERE t1.col1 = holdkey.col1 AND t1.col2 = holdkey.col2 Insert the original rows: INSERT t1 SELECT * FROM holddups btw and for completeness: In Oracle there is a hidden field you could use (rowid): DELETE FROM our_table WHERE rowid not in (SELECT MIN(rowid) FROM our_table GROUP BY column1 column2 column3... ; see: Microsoft Knowledge Site @Tony: That is correct. To my defense: I had this copied in my local programming wiki and wasn't even aware anymore where it came from. You should have mentioned you got this from Microsoft's support site. http://support.microsoft.com/kb/139444  Here's another way with test data create table #table1 (colWithDupes1 int colWithDupes2 int) insert into #table1 (colWithDupes1 colWithDupes2) Select 1 2 union all Select 1 2 union all Select 2 2 union all Select 3 4 union all Select 3 4 union all Select 3 4 union all Select 4 2 union all Select 4 2 select * from #table1 set rowcount 1 select 1 while @@rowcount > 0 delete #table1 where 1 < (select count(*) from #table1 a2 where #table1.colWithDupes1 = a2.colWithDupes1 and #table1.colWithDupes2 = a2.colWithDupes2 ) set rowcount 0 select * from #table1  Manrico Corazzi - I specialize in Oracle not MS SQL so you'll have to tell me if this is possible as a performance boost:- Leave the same as your first step - insert distinct values into TABLE2 from TABLE1. Drop TABLE1. (Drop should be faster than delete I assume much as truncate is faster than delete). Rename TABLE2 as TABLE1 (saves you time as you're renaming an object rather than copying data from one table to another).  Add an identity column to act as a surrogate primary key and use this to identify two of the three rows to be deleted. I would consider leaving the identity column in place afterwards or if this is some kind of link table create a compound primary key on the other columns. Adding an identity column will definitely help. SQL Server will generate a ghost column to make each record unique but you will not be able to query this column. The identity column will reduce some of that overhead and guarantee uniqueness.  This is a tough situation to be in. Without knowing your particular situation (table size etc) I think that your best shot is to add an identity column populate it and then delete according to it. You may remove the column later but I would suggest that you should keep it as it is really a good thing to have in the table  What about this solution : First you execute the following query :  select 'set rowcount ' + convert(varcharCOUNT(*)-1) + ' delete from MyTable where field=''' + field +'''' + ' set rowcount 0' from mytable group by field having COUNT(*)>1 And then you just have to execute the returned result set set rowcount 3 delete from Mytable where field='foo' set rowcount 0 .... .... set rowcount 5 delete from Mytable where field='bar' set rowcount 0 I've handled the case when you've got only one column but it's pretty easy to adapt the same approach tomore than one column. Let me know if you want me to post the code.  This is a way to do it with Common Table Expressions CTE. It involves no loops no new columns or anything and won't cause any unwanted triggers to fire (due to deletes+inserts). Inspired by this article. CREATE TABLE #temp (i INT) INSERT INTO #temp VALUES (1) INSERT INTO #temp VALUES (1) INSERT INTO #temp VALUES (2) INSERT INTO #temp VALUES (3) INSERT INTO #temp VALUES (3) INSERT INTO #temp VALUES (4) SELECT * FROM #temp ; WITH [#temp+rowid] AS (SELECT ROW_NUMBER() OVER (ORDER BY i ASC) AS ROWID * FROM #temp) DELETE FROM [#temp+rowid] WHERE rowid IN (SELECT MIN(rowid) FROM [#temp+rowid] GROUP BY i HAVING COUNT(*) > 1) SELECT * FROM #temp DROP TABLE #temp @Jonas - that my friend is very cool. And it just solved a problem i had. Thanks! Very nice. I am always amazed with what CTEs can do.  Can you add a primary key identity field to the table?  Here's the method I used when I asked this question - DELETE MyTable FROM MyTable LEFT OUTER JOIN ( SELECT MIN(RowId) as RowId Col1 Col2 Col3 FROM MyTable GROUP BY Col1 Col2 Col3 ) as KeepRows ON MyTable.RowId = KeepRows.RowId WHERE KeepRows.RowId IS NULL  After you clean up the current mess you could add a primary key that includes all the fields in the table. that will keep you from getting into the mess again. Of course this solution could very well break existing code. That will have to be handled as well.  I'm not sure if this works with DELETE statements but this is a way to find duplicate rows:  SELECT * FROM myTable t1 myTable t2 WHERE t1.field = t2.field AND t1.id > t2.id I'm not sure if you can just change the ""SELECT"" to a ""DELETE"" (someone wanna let me know?) but even if you can't you could just make it into a subquery."172,A,"What is the best way to format a localized string in AppleScript? When a script is saved as a bundle it can use the localized string command to find the appropriate string e.g. in Contents/Resources/English.lproj/Localizable.strings. If this is a format string what is the best way to fill in the placeholders? In other words what is the AppleScript equivalent of +[NSString stringWithFormat:]? One idea I had was to use do shell script with printf(1). Is there a better way? As ugly as it is calling out to printf(1) is the common solution. A cleaner though somewhat more complex solution is to use AppleScript Studio which allows you to call into Objective-C objects/classes from your AppleScript code with the call method syntax documented here. With that you'd be able to use something like this: call method ""stringWithFormat:"" of class ""NSString"" with parameters {formatString arguments} The downside of this of course is that you need to write an AppleScript Studio app instead of just writing a simple script. You do get a good bit more flexibility in general with Studio apps though so it's not all together a terrible route to go. Unfortunately there are cases where a script is required rather than an app and so AppleScript Studio is not an option. AppleScript Studio is now deprecated."173,A,"Setting Up MySQL Triggers I've been hearing about triggers and I have a few questions. What are triggers? How do I set them up? Are there any precautions aside from typical SQL stuff that should be taken? Triggers allow you to perform a function in the database as certain events happen (eg an insert into a table). I can't comment on mysql specifically. Precaution: Triggers can be very alluring when you first start using them they seem like a magic bullet to all kinds of problems. But they make ""magic"" stuff happen if you don't know the database inside out it can seem like really strange things happen (such as inserts into other tables input data changing etc). Before implementing things as a trigger I'd seriously consider instead enforcing the use of an API around the schema (preferably in the database but outside if you can't). Some things I'd still use triggers for Keeping track of ""date_created"" and ""date_last_edited"" fields Inserting ""ID""'s (in oracle where there is no auto id field) Keeping change history Things you wouldn't want to use triggers for business rules/logic anything which connects outside of the database (eg a webservice call) Access control Anything which isn't transactional ( anything you do in the trigger MUST be able to rollback with the transaction )  This question is old and other answers are very good but since the user asked about precautions that should be taken I want to add something: If you use replication in a complex environment don't make a massive use of Triggers and don't call stored procedures from triggers. Triggers are slow in MySQL. You can't use some SQL statements within triggers. And some statements are permitted but should be avoided like LOCK. The general rule is: if you don't fully understand the implications of what you are doing you shouldn't do it. Triggers can cause endless loops so be careful.  From dev.mysql.com a trigger is ...a named database object that is associated with a table and that is activated when a particular event occurs for the table. The syntax to create them is also documented at that site. Briefly CREATE [DEFINER = { user | CURRENT_USER }] TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt And they provide an example: CREATE TABLE account (acct_num INT amount DECIMAL(102)); CREATE TRIGGER ins_sum BEFORE INSERT ON account FOR EACH ROW SET @sum = @sum + NEW.amount; You at least need to abide by all the restrictions on stored functions. You won't be able to lock tables alter views or modify the table that triggered the trigger. Also triggers may cause replication problems.  A trigger is a named database object that is associated with a table and that is activated when a particular event occurs for the table. To create a trigger: CREATE TRIGGER triggerName [BEFORE|AFTER] [INSERT|UPDATE|DELETE|REPLACE] ON tableName FOR EACH ROW SET stuffToDoHERE; Even though I answered this part the other question still stands."174,A,What are the pros and cons of using RMI or JMS between web and business tiers? For a typical Web client -to- Servlet/WS -to- Business Tier (Spring or EJB) app what are the trade-offs of approaches like remote RPC or messaging for Web (Servlet) tier to remote Business tier aside from the basic sync/async aspects? We use RMI via Spring and find it very easy to use fairly robust and fast. Although our requirements were for a fairly responsive link and there was no real need to add a messaging component.  By web client do you mean web browser? If so looking at stuff like DWR or JAX-RS are my recommendations. RMI or JMS only really work when both sides are Java code. With any remoting technology the biggest issue using them tends to be how intrusive the technology becomes on your business objects. e.g. using RMI interface/exceptions everywhere or using the JMS APIs inside your business code. My recommendation is to use POJOs everywhere in Java then use a technology like Spring Remoting to layer on your middleware whether its RMI or JMS or whatever - but totally de-couple the middleware code from your business logic so you can switch between technologies at any time (and keep your business logic code simpler and focussed on your business problem). For example see the Camel implementation of Spring Remoting which then allows you to use any of these transports and protocols such as RMI JMS or even plain HTTP email files or XMPP - then switch between them trivially using a simple URI string change.  SUN RMI broke for us. The settings and garbage collection for a very long running application with continuous meassaging. We are patching to make it work continuously. JMS applications we run don't get the out of memory errors or gc problems that RMI does. Anything that needs to call System.gc() periodically and doesn't work with incremental collection to recover resources is coded wrong. RMI reliability improves with the JDK 6 and the correct property settings but JHC it's a bodgey framework. RMI would be vastly improved by using channels in nio and fixing the sun nio uses of system.gc(). The correct answer - seperate communication (mechanism) from the domain code. RPC is tightly coupled and the protocol and application can interfere with each other. JMS seperates the protocol from the application a much better paradigm.175,A,"Installer changes PATH variable changes don't show up in Command Shell I added a custom install action to my installer to add one of my installation directories to the System PATH environment variable. After I run the installer the PATH variable reflects the changes (when I access it through the Control Panel::System applet) but when I start a new command shell the PATH variable does not reflect the changes. Is there something I'm failing to do that causes this? Why are you using a CustomAction for this? The Windows Installer supports modifying environment variables natively. Also I think the Windows Installer sends a broadcast message to update the system when environment variables change. That may mean you don't need to reboot... but it's been a while since I tried so YMMV.  How are you adding the environment variable? Without using any external tools you can add it to the registry. Then your test of opening a new command window will reflect your change.  How are you starting the command shell? With the TaskManager? I suspect you might be starting it from Explorer - if I remember correctly this could meen that you are inheriting the parent processes (Windows Explorer in this case) PATH variable. Since that was set before your installer ran you see the old value. Not sure if this helps...  I think this depends on how you are starting the new Command shell. For example when you change the PATH environment variable under System properties the change isn't reflected until you open a new Command prompt. I think when you launch a new ""cmd"" process (from the Run dialog for example) you get a fresh copy of all environment variables but if you launch the command prompt a different way then you do not. For something done thru a script like that you may need to restart before you notice the change. Yep I've had scripts change the path variables and have found that no way of starting the Command prompt shows those changes until after a restart. This is in Windows XP.  http://support.microsoft.com/kb/310519 says that for system environment variables (which PATH is one of) requires a restart although I have a feeling that logging off and on may be enough."176,A,How do you create a weak reference to an object in Python? How do you create a weak reference to an object in Python? >>> import weakref >>> class Object: ... pass ... >>> o = Object() >>> r = weakref.ref(o) >>> # if the reference is still active r() will be o otherwise None >>> do_something_with_o(r()) See the wearkref module docs for more details. You can also use weakref.proxy to create an object that proxies o. Will throw ReferenceError if used when the referent is no longer referenced.177,A,"Add 1 to a field How do I turn the following 2 queries into 1 query $sql = ""SELECT level FROM skills WHERE id = $id LIMIT 1;""; $result = $db->sql_query($sql); $level = (int) $db->sql_fetchfield('level'); $db->sql_freeresult($result); ++$level; $sql = ""UPDATE skills SET level = $level WHERE id = $id;""; $result = $db->sql_query($sql); $db->sql_freeresult($result); I'm using it in a phpBB mod but the gist is that I grab the level add one to it then update it seems that it'd be much easier and faster if I could do it as one query. Edit: $id has already been forced to be an integer thus no escaping is needed this time. How about: UPDATE skills SET level = level + 1 WHERE id = $id;  $sql = ""UPDATE skills SET level = level + 1 WHERE id = $id""; I just hope you are properly sanitising $id elsewhere in your code! especially without quotes someone doesn't even have to escape the last quote with SQL Injection $id = ""'null' OR DELETE FROM skills;""; haha  Mat: That's what pasted in from the question. It hasn't been edited so I attribute that to a bug in Markdown. But oddly enough I have noticed. Also: yes mysql_escape_string()!  I get downmodded for this? $sql = ""UPDATE skills SET level = level+1 WHERE id = $id""; $result = $db->sql_query($sql); $db->sql_freeresult($result); In Teifion's specific case the phpBB DDL lists that particular field as NOT NULL so there's no danger of incrementing NULL. In the general case you should not use NULL to represent zero. Incrementing NULL should give an answer of NULL. If you're the kind of misguided developer who thinks NULL=0 step away from keyboard and find another pastime you're just making life hard for the rest of us. Of course this is the computer industry and who are we to say you're wrong? If you're not wrong use $sql = ""UPDATE skills SET level = COALESCE(level0)+1 WHERE id = $id""; ...but let's face it: you're wrong. If everyone starts at level 0 then your DDL should include level INT DEFAULT '0' NOT NULL in case the programmers forget to set it when they create a record. If not everyone starts on level 0 then skip the DEFAULT and force the programmer to supply a value on creation. If some people are beyond levels for whom having a level is a meaningless thing then adding one to their level equally has no meaning. In that case drop the NOT NULL from the DDL. I have an issue where if 'level' is null it will not increment.  try this UPDATE skills SET level = level + 1 WHERE id = $id  With PDO and prepared query: $query = $db->prepare(""UPDATE skills SET level = level + 1 WHERE id = :id"") $query->bindValue("":id"" $id); $result = $query->execute();  This way: UPDATE skills SET level = level + 1 WHERE id = $id  Josh: I downmodded you because as Mat said you had the completely wrong answer. What's showing up now isn't what I saw."178,A,"In C++/Windows how do I get the network name of the computer I'm on? In a C++ Windows (XP and NT if it makes a difference) application I'm working on I need to get the network name associated with the computer the code is executing on so that I can convert local filenames from C:\filename.ext to \\network_name\C$\filename.ext. How would I do this? Alternatively if there's a function that will just do the conversion I described that would be even better. I looked into WNetGetUniversalName but that doesn't seem to work with local (C drive) files. +1 to you. First time my google result was a Stack Overflow post. Thanks all. There are more than one alternatives: a. Use Win32's GetComputerName() as suggested by Stu. Example: http://www.techbytes.ca/techbyte97.html OR b. Use the function gethostname() under Winsock. This function is cross platform and may help if your app is going to be run on other platforms besides Windows. MSDN Reference: http://msdn.microsoft.com/en-us/library/ms738527(VS.85).aspx OR c. Use the function getaddrinfo(). MSDN reference: http://msdn.microsoft.com/en-us/library/ms738520(VS.85).aspx Unfortunatly the link for a) (techbytes.ca) doesn't seem to be available anymore.  I agree with Pascal on using winsock's gethostname() function. Here you go: #include <winsock2.h> //of course this is the way to go on windows only void GetHostName(std::string& host_name) { WSAData wsa_data; int ret_code; char buf[MAX_PATH]; WSAStartup(MAKEWORD(1 1) &wsa_data); ret_code = gethostname(bufe MAX_PATH); if (ret_code == SOCKET_ERROR) host_name = ""unknown"" else host_name = host_name; WSACleanup(); }  You'll want Win32's GetComputerName: http://msdn.microsoft.com/en-us/library/ms724295(VS.85).aspx"179,A,"Why does Python PEP-8 specify a maximum line length of 79 characters? Why in this millenium should Python PEP-8 specify a maximum line length of 79 characters? Pretty much every code editor under the sun can handle longer lines. What to do with wrapping should be the choice of the content consumer not the responsibility of the content creator. Are there any (legitimately) good reasons for adhering to 79 characters in this age? The answer to your question is *in* PEP-8. Yup! I believe that's where I got my answer from. ;) Shorter line lengths enhance productivity by increasing your KLOC. :p 79 characters (well actually 72 characters) is where most text-based email readers linewrap. So code cut-and-pasted into an email is a lot more readable.  I agree with Justin. To elaborate overly long lines of code are harder to read by humans and some people might have console widths that only accommodate 80 characters per line. The style recommendation is there to ensure that the code you write can be read by as many people as possible on as many platforms as possible and as comfortably as possible.  Keeping your code human readable not just machine readable. A lot of devices still can only show 80 characters at a time. Also it makes it easier for people with larger screens to multi-task by being able to set up multiple windows to be side by side. Readability is also one of the reasons for enforced line indentation. this should have been the marked answer 79 comes from the days of dot matrix printers they only printed 80 characters per line. Today we all sit with 16:9 ratio displays with a lot of width and limited vertical space... I follow every rule in PEP8 except this one I believe it should be brought in line with current technology. 79 characters also makes programmers use shorter more cryptic variable and function names to make everything fit. This is bad for readability. Yes granted. But why 79? Why not 100 or 120? Keeping things readable works both ways. Too much up-and-down reading of code is hard to grok too. It's true that a lot of devices can only show 80 characters. How many of them can't perform soft-wrapping? Most Python-specific editors don't do soft word wrapping. Also it is preferred to not have code wrap. From a user experience perspective it's unacceptable for most. There are some operating systems like MVS which cannot handle lines longer than 72 characters. PEP-8 won't help here. Setting an arbitrary limit of 79 characters makes no sense since how characters per line are good depends on the editor the monitor the personal preferences of the user and so on.  because if you push it beyond the 80th column it means that either you are writing a very long and complex line of code that does too much (and so you should refactor) or that you indented too much (and so you should refactor). Not to mention that if you name symbols in such a way that they are human readable e.g. ""users_directed_graph"" instead of ""usr_dir_gph"" then even a simple expresswion will eat up quite a few characters per line. I have always found in Python that if I exceed 80 chars its wise to stop and think about why that is. Usually bad design decision is at fault. -1 I don't think you can categorically say that any line past the 80 char boundary requires a refactor. Class methods are already indented twice add another indent for an ""if"" etc. and a simple list comprehension and it's pretty easy to cross the 80-char boundary.  Here's why I like the 80-character with: at work I use Vim and work on two files at a time on a monitor running at I think 1680x1040 (I can never remember). If the lines are any longer I have trouble reading the files even when using word wrap. Needless to say I hate dealing with other people's code as they love long lines.  Since whitespace has semantic meaning in Python some methods of word wrapping could produce incorrect or ambiguous results so there needs to be some limit to avoid those situations. An 80 character line length has been standard since we were using teletypes so 79 characters seems like a pretty safe choice. There's two different types of word-wrapping. There's hard-wrapping where the lines are broken up with newlines and there's soft-wrapping where they are only *displayed* with breaks but remain physically a single line. I don't see any problem with the latter. Most Python editors don't do soft word wrapping because it produces ambiguous hard to read code in a language where whitespace and indentation is important. It doesn't produce ambiguous or hard-to-read code so long as the wrapping is visually identified somehow. Kate does this and it works fine. If an editor doesn't handle this then that's a reason to file a bug against the editor not a reason to impose a coding style that avoids the bug. Even if it's indicated visually it still makes the code much more difficult to read which is why Python editors generally don't support it. Have you actually tried it for an extended period of time? I have. It doesn't make the code more difficult to read in my experience. Can you back up the claim that this is why Python editors don't include the feature? I've never heard that claim before.  I am a programmer who has to deal with a lot of code on a daily basis. Open source and what has been developed in house. As a programmer I find it useful to have many source files open at once and often organise my desktop on my (widescreen) monitor so that two source files are side by side. I might be programming in both or just reading one and programming in the other. I find it dissatisfying and frustrating when one of those source files is >120 characters in width because it means I can't comfortably fit a line of code on a line of screen. It upsets formatting to line wrap. I say '120' because that's the level to which I would get annoyed at code being wider than. After that many characters you should be splitting across lines for readability let alone coding standards. I write code with 80 columns in mind. This is just so that when I do leak over that boundary it's not such a bad thing. ""I write code with 80 columns in mind. This is just so that when I do leak over that boundary it's not such a bad thing."" Same for me.  I believe those who study typography would tell you that 66 characters per a line is supposed to be the most readable width for length. Even so if you need to debug a machine remotely over an ssh session most terminals default to 80 characters 79 just fits trying to work with anything wider becomes a real pain in such a case. You would also be suprised by the number of developers using vim + screen as a day to day environment. My ssh+screen+vim environemnts have no problem displaying long lines. ""66 characters per a line is supposed to be the most readable width for length"" I suppose we should write code in 2 or 3 columns since that's how newspapers are laid out? Emacs FTW! +1 though. I think the 79 limit comes from the early days of UNIX (and possibly MULTICS) that had 80x25 character terminals.  Much of the value of PEP-8 is to stop people arguing about inconsequential formatting rules and get on with writing good consistently formatted code. Sure no one really thinks that 79 is optimal but there's no obvious gain in changing it to 99 or 119 or whatever your preferred line length is. I think the choices are these: follow the rule and find a worthwhile cause to battle for or provide some data that demonstrates how readability and productivity vary with line length. The latter would be extremely interesting and would have a good chance of changing people's minds I think. Most reading studies are done in inches and not characters per line. The 66 character rule is based on studies done for reading newspapers. [Recent studies](http://www.usability.gov/articles/newsletter/pubs/082006news.html) have shown that when reading online articles reading speed increases up to about 120 characters per line (10 inches at size 12 font) with no loss in comprehension. Actually everybody who read into that topic *thinks* that 79 characters is optimal. That's why it was added to PEP8! This answer is actually wrong. [This one is the correct one](http://stackoverflow.com/a/88948/131120)  Printing a monospaced font at default sizes is (on A4 paper) 80 columns by 66 lines. Why do people print code in 2012? This reminds me of going to a technology conference and being handed a bag and a printed binder full of presentations. It's the 21st century people: e-mail me the slides or else that bag and binder are going straight into a landfill. I accept this standard; it's valid. But who prints code any more? Moreover who prints code from an environment which is intolerant of scaling or other formatting options? When was the last time anyone you know was stumped by their inability to render a line of 100 characters?"180,A,"Checklist for Database Schema Upgrades Having to upgrade a database schema makes installing a new release of software a lot trickier. What are the best practices for doing this? I'm looking for a checklist or timeline of action items such as 8:30 shut down apps 8:45 modify schema 9:15 install new apps 9:30 restart db etc showing how to minimize risk and downtime. Issues such as backing out of the upgrade if things go awry minimizing impact to existing apps ""hot"" updates while the database is running promoting from devel to test to production servers are especially of interest. And if the Scott Ambler paper whets your appetite I can recommend his book with Pramod J Sadolage called 'Refactoring Databases' - http://www.ambysoft.com/books/refactoringDatabases.html There is also a lot of useful advice and information at the Agile Database group at Yahoo - http://tech.groups.yahoo.com/group/agileDatabases/  Two quick notes: It goes without saying... So I'll say it twice. Verify that you have a valid backup. Verify that you have a valid backup. @mk. Check out Jeff's blog post on database version control (if you haven't already)  I guess you have considered the reads of Scott Ambler? http://www.agiledata.org/essays/databaseRefactoring.html  This is a topic that I was just talking about at work. Mainly the problem is that unless database migrations is handled for you nicely by your framework eg rails and their migration scripts then it is left up to you. The current way that we do it has apparent flaws and I am open to other suggestions. Have a schema dump with static data that is required to be there kept up to date and in version control. Every time you do a schema changing action ALTER CREATE etc. dump it to a file and throw it in version control. Make sure you update the original sql db dump. When doing pushes to live make sure you or your script applies the sql files to the db. Clean up old sql files that are in version control as they become old. This is by no means optimal and is really not intended as a ""backup"" db. It's simply to make pushes to live easy and to keep developers on the same page. There is probably something cool you could setup with capistrano as far as automating the application of the sql files to the db. Db specific version control would be pretty awesome. There is probably something that does that and if there isn't there probably should be.  I have a lot of experience with this. My application is highly iterative and schema changes happen frequently. I do a production release roughly every 2 to 3 weeks with 50-100 items cleared from my FogBugz list for each one. Every release we've done over the last few years has required schema changes to support new features. The key to this is to practice the changes several times in a test environment before actually making them on the live servers. I keep a deployment checklist file that is copied from a template and then heavily edited for each release with anything that is out of the ordinary. I have two scripts that I run on the database one for schema changes one for programmability (procedures views etc). The changes script is coded by hand and the one with the procs is scripted via Powershell. The change script is run when everything is turned off (you have to pick a time that annoys the least amount of users for this) and it is run command by command manually just in case anything goes weird. The most common problem I have run into is adding a unique constraint that fails due to duplicate rows. When preparing for an integration testing cycle I go through my checklist on a test server as if that server was production. Then in addition to that I go get an actual copy of the production database (this is a good time to swap out your offsite backups) and I run the scripts on a restored local version (which is also good because it proves my latest backup is sound). I'm killing a lot of birds with one stone here. So that's 4 databases total: Dev: all changes must be made in the change script never with studio. Test: Integration testing happens here Copy of production: Last minute deployment practice Production You really really need to get it right when you do it on production. Backing out schema changes is hard. As far as hotfixes I will only ever hotfix procedures never schema unless it's a very isolated change and crucial for the business."181,A,"Previewing HTML in Java What libraries/methods that you know of can do some basic HTML representation in Swing? Can you comment on your experience? Many of the Swing controls (like JLabel) can render basic HTML content. JEditorPane can be used to display HTML pages. However these controls are limited to HTML 3.2 support. For a richer experience I would use the JDesktop Integration Components. JDIC provides Java applications with access to functionalities and facilities provided by the native desktop. It consists of a collection of Java packages and tools. JDIC supports a variety of features such as embedding the native browser launching the desktop applications creating tray icons on the desktop registering file type associations creating JNLP installer packages etc.  Swing has a built-in compontent called BasicHTML. I've never used it but I think it should be sufficient for the basic stuff.  Came across Lobo java web browser the other day. Lobo is being actively developed with the aim to fully support HTML 4 Javascript and CSS2. No experience with it though but thought it may fit the bill for you.  This has historically been a major weak point for Java IMO. There are numerous ways to display limited markup but very few that offer full featured HTML capabilities. The previously mentioned JDIC component is one option however it is considered a ""heavyweight"" component and therefore does not always integrate well with Swing applications. I am hopeful however that the new Webkit based JWebPane project will provide more advanced capabilities without all of the issues that we've had to deal with in the past. And of course there are several commercial options as well (IceBrowser is pretty good as an example).  I haven't tried this in a while but a quick google search shows some possibilities: Java Sketchbook: The HTML Renderer Shootout Part 1 Cobra: Java HTML Renderer & Parser Are you trying to do this in an applet or an application? If it's an application (or signed applet) you could potentially instantiate IE or Firefox within your application. Webrenderer acts as a Swing wrapper for this.  I've just used SwingBox to display a quite simple HTML page with good results. The project includes a simple demo application which compares its BrowserPane component to JEditorPane showing a far better result on complex pages (but still not comparable with a modern web browser). The only issue I had is about unwanted scrollbars from the wrapping JScrollPane. The demo application seems to have the same problem. I can't tell where the issue originates. I'm using version 1.0. Here a code fragment to show how simple is to use the component: BrowserPane browserPane = new BrowserPane(); JScrollPane scrollPane = new JScrollPane(browserPane); someContainer.add(scrollPane); browserPane.setText(""<html><b>Some HTML here</b></html>""); // or... browserPane.setPage(new URL(""http://en.wikipedia.org""));  A good pure Java solution is JWebEngine. It render HTML 4 very good. It's commercial license is about 2000USD"182,A,"Conditional Display in ASPX Pages on Sharepoint I wonder what the best practice for this scenario is: I have a Sharepoint Site (MOSS2007) with an ASPX Page on it. However I cannot use any inline source and stuff like Event handlers do not work because Sharepoint does not allow Server Side Script on ASPX Pages per default. Two solutions: Change the PageParserPath in web.config as per this site <PageParserPaths> <PageParserPath VirtualPath=""/pages/test.aspx"" CompilationMode=""Always"" AllowServerSideScript=""true"" /> </PageParserPaths> Create all the controls and Wire them up to Events in the .cs File thus completely eliminating some of the benefits of ASP.net I wonder what the best practice would be? Number 1 looks like it's the correct choice but changing the web.config is something I want to use sparingly whenever possible. Well it's a page that hosts user controls. It's a custom .aspx Page that will be created on the site specially because I do not want to create WebParts. It's essentially an application running within Sharepoint utilizing Lists and other functions but all the functionality is only useful within the application so flooding the web part gallery with countless web parts that only work in one place is something i'd like to avoid.  What does the ASPX page do? What functionality does it add? How are you adding the page into the site? By the looks of it this is just a ""Web Part Page"" in a document library. I would have to do a little research to be 100% but my understanding is that inline code is ok providing it's in a page that remains ghosted and thereby trusted. Can you add your functionality into the site via a feature? I would avoide option 1 seems like bad advice to me. Allowing server side code in your page is a security risk as it then becomes possible for someone to inject malicious code. Sure you can secure the page but we are talking remote execution with likely some pretty serious permissions.  So in that case I would wrap it up in a feature and deploy it via a solution. This way I think you will avoid the issue you are seeing. This is especially useful if you plan to use this functionality within other sites too. You can also embed web parts directly in the page much like you do a WebControl thereby avoiding any gallery clutter.  Thanks so far. I've successfully tried Andrew Connel's solution: http://www.andrewconnell.com/blog/articles/UsingCodeBehindFilesInSharePointSites.aspx Wrapping it into a solution is part of that but the main problem was how to get the code into that and it's more leaning towards Option 2 without having to create the controls in code. What I was missing: In the .cs File it is required to manually add the ""protected Button Trigger;"" stuff because there is no automatically generated .designer.cs file when using a class library."183,A,"How to display the progress of a server script in jQuery? With this code I can show an animated gif while the server script is running: function calculateTotals() { $('#results').load('getResults.php' null showStatusFinished); showLoadStatus(); } function showLoadStatus() { $('#status').html(''); } function showStatusFinished() { $('#status').html('Finished.'); } However I would like to display a status of how far along the script is e.g. ""Processing line 342 of 20000..."" and have it count up until it is finished. How can I do that? I can make a server-script which constantly contains the updated information but where do I put the command to read this say every second? After reading your comments to Andrew's answer. You would read the status like this: function getStatus() { $.getJSON(""/status.php""{""session"":0 ""requestID"":12345} function(data) { //data is the returned JSON object from the server {name:""value""} setStatus(data.status); window.setTimeout(""getStatus()""intervalInMS) }); } Using this method you can open several simultaneous XHR request on the server. all your status.php as to output is : {""status"":""We are done row 1040/45983459""} You can however output as many information you want in the response and to process it accordingly (feeding a progress bar for example or performing an animation..) For more information on $.getJSON see http://docs.jquery.com/Ajax/jQuery.getJSON  Your server-side script should somehow keep its progress somewhere on server (file field in database memcached etc.). You should have AJAX function returning current progress. Poll this function once a second and render result accordingly.  Without knowing how your server side code works it's hard to say. However there are three stages to the process. Firstly you need to call a job creation script. This returns an id number and sets the server working. Next every second or so you need to call a status script which returns an status message that you want to display. That status script also needs to return a value indicating whether the job has finished or not. When the status script says the job has finished you stop polling. How you get this status script is to know the status of the job depends greatly on how server is set up but probably involves writing the message to a database table at various points during the job. The status script then reads this message from the database. Right but where do I put the line which reads the status script? That's what I don't understand. I can't do it in my showLoadStatus() function since it is only executed once.  I'm not down with the specifics for jQuery but a general answer that doesn't involve polling wold be: Use a variation of the forever frame technique. Basically create a hidden iframe and set the src of it to be 'getresults.php'. Inside getresults you ""stream"" back script blocks which are calls to a javascrpt function in the parent document that actually updates the progress. Here's an example that shows the basic idea behind a forever frame. (I wouldn't recommend using his actual JS or HTML though it's reasonably average)"184,A,"What does a PHP developer need to know about https / secure socket layer connections? I know next to nothing when it comes to the how and why of https connections. Obviously when I'm transmitting secure data like passwords or especially credit card information https is a critical tool. What do I need to know about it though? What are the most common mistakes you see developers making when they implement it in their projects? Are there times when https is just a bad idea? Thanks! In addition to the below you should use Wireshark to look at a simple HTTPS request/response just to see it ""in action."" And it's implicit in the answers below that you understand public key cryptography (at a high level). Be sure that when on an HTTPS page all elements on the page come from an HTTPS address. This means that elements should have relative paths (e.g. ""/images/banner.jpg"") so that the protocol is inherited or that you need to do a check on every page to find the protocol and use that for all elements. NB: This includes all outside resources (like Google Analytics javascript files)! The only down-side I can think of is that it adds (nearly negligible) processing time for the browser and your server. I would suggest encrypting only the transfers that need to be. Google Analytics's code detects HTTPS on its own and serves from a secure server. They've had this for over a year now and users using the older code can upgrade whenever they feel like. Hey thanks for that. I didn't know. I guess I should read their blog a little more often ;-) Doesn't it defeat the purpose of secure pages if you're going to let content in from third party sources. Espcially in the case of Javascript files (like Google analytics) that could potentially act as keyloggers and send secure information to the third party?  An HTTPS or Secure Sockets Layer (SSL) certificate is served for a site and is typically signed by a Certificate Authority (CA) which is effectively a trusted 3rd party that verifies some basic details about your site and certifies it for use in browsers. If your browser trusts the CA then it trusts any certificates signed by that CA (this is known as the trust chain). Each HTTP (or HTTPS) request consists of two parts: a request and a response. When you request something through HTTPS there are actually a few things happening in the background: The client (browser) does a ""handshake"" where it requests the server's public key and identification. At this point the browser can check for validity (does the site name match? is the date range current? is it signed by a CA it trusts?). It can even contact the CA and make sure the certificate is valid. The client then encrypts its request using the server's public key and only the server can decrypt that message using it's own private key. It also sends its own public key (generated on the fly usually). The server then processes the request normally and then encrypts the response using the client's public key and only the client can decrypt it. Certificates and Hostnames Certificates are assigned a Common Name (CN) which for HTTPS is the domain name. The CN has to match exactly eg a certificate with a CN of ""domain.com"" will NOT match the domain ""www.domain.com"" and users will get a warning in their browser. It is not possible to host multiple domain names on one IP. Because the certificate is fetched before the client even sends the actual HTTP request and the HTTP request contains the Host: header line that tells the server what URL to use there is no way for the server to know what certificate to serve for a given request - therefore each domain being secured needs its own IP address. You can serve other non-HTTPS sites on the same IP however. The one exception to this is with wildcard certificates. It is possible to get a certificate like "".domain.com"" in which case ""www.domain.com"" and ""foo.domain.com"" will both be valid for that certificate. However note that ""domain.com"" does not match "".domain.com"" and neither does ""foo.bar.domain.com"". If you use ""www.domain.com"" for your certificate you should redirect anyone at ""domain.com"" to the ""www."" site. If they request https://domain.com unless you host it on a separate IP and have two certificates the will get a certificate error. Forms Strictly speaking if you are submitting a form it doesn't matter if the form page itself is not encrypted as long as the submit URL goes to an https:// URL. In reality users have been trained (at least in theory) not to submit pages unless they see the little ""lock icon"" so even the form itself should be served via HTTPS to get this. Traffic and Server Load HTTPS traffic is much bigger than its equivalent HTTP traffic (due to encryption and certificate overhead) and it also puts a bigger strain on the server (encrypting and decrypting). If you have a heavily-loaded server it may be desirable to be very selective about what content is served using HTTPS. Best Practices The site should automatically redirect between HTTPS and HTTP as required. When you are doing anything involving sensitive data (logging in setting up users passwords looking at any data that other people should not see) you should be using HTTPS. Any resources on the page should come from the same scheme being used for the page. If you try to fetch images from http:// when the page is loaded with HTTPS the user will get security warnings. You should either use fully-qualified URLs or another easy way is to use absolute URLs that do not include the hostname (eg src=""/images/foo.png"") because they work for both. This includes external resources (eg Google Analytics) Don't do POSTs (form submits) when changing from HTTPS to HTTP. Most browsers will flag this as a security warning.  I'm not going to go in depth on SSL in general gregmac did a great job on that see below ;-). However some of the most common (and critical) mistakes made (not specifically PHP) with regards to use of SSL/TLS: Allowing HTTP when you should be enforcing HTTPS Retrieving some resources over HTTP from an HTTPS page (e.g. images IFRAMEs etc) Directing to HTTP page from HTTPS page unintentionally - note that this includes ""fake"" pages such as ""about:blank"" (I've seen this used as IFRAME placeholders) this will needlessly and unpleasantly popup a warning. Web server configured to support old unsecure versions of SSL (e.g. SSL v2 is common yet horribly broken) (okay this isn't exactly the programmer's issue but sometimes noone else will handle it...) Web server configured to support unsecure cipher suites (I've seen NULL ciphers only in use which basically provides absolutely NO encryption) (ditto) Self-signed certificates - prevents users from verifying the site's identity. Requesting the user's credentials from an HTTP page even if submitting to an HTTPS page. Again this prevents a user from validating the server's identity BEFORE giving it his password... Even if the password is transmitted encrypted the user has no way of knowing if he's on a bogus site - or even if it WILL be encrypted. Non-secure cookie - security-related cookies (such as sessionId authentication token access token etc.) MUST be set with the ""secure"" attribute set. This is important! If it's not set to secure the security cookie e.g. SessionId can be transmitted over HTTP (!) - and attackers can ensure this will happen - and thus allowing session hijacking etc. While you're at it (tho this is not directly related) set the HttpOnly attribute on your cookies too (helps mitigate some XSS). Overly permissive certificates - say you have several subdomains but not all of them are at the same trust level. For instance you have www.yourdomain.com dowload.yourdomain.com and publicaccess.yourdomain.com. So you might think about going with a wildcard certificate.... BUT you also have secure.yourdomain.com or finance.yourdomain.com - even on a different server. publicaccess.yourdomain.com will then be able to impersonate secure.yourdomain.com.... While there may be instances where this is okay usually you'd want some separation of privileges... That's all I can remember right now might re-edit it later... As far as when is it a BAD idea to use SSL/TLS - if you have public information which is NOT intended for a specific audience (either a single user or registered members) AND you're not particular about them retrieving it specifically from the proper source (e.g. stock ticker values MUST come from an authenticated source...) - then there is no real reason to incur the overhead (and not just performance... dev/test/cert/etc). However if you have shared resources (e.g. same server) between your site and another MORE SENSITIVE site then the more sensitive site should be setting the rules here. Also passwords (and other credentials) credit card info etc should ALWAYS be over SSL/TLS.  All very good tip here... but I just want to add something.. Ive seen some sites that gives you a http login page and only redirect you to https after you post your username/pass.. This means the username is transmitted in the clear before the https connection is established.. In short make the page where you login from ssl instead of posting to an ssl page.  I would say the most common mistakes when working with an SSL-enabled site are The site erroneously redirects users to http from a page as https The site doesn't automatically switch to https when it's necessary Images and other assets on an https page are being loading via http which will trigger a security alert from the browser. Make sure all assets are using fully-qualified URIs that specify https. The security certificate only works for one subdomain (such as www) but your site actually uses multiple subdomains. Make sure to get a wildcard certificate if you will need it.  I found that trying to <link> to a non-existent style sheet also caused security warnings. When I used the correct path the lock icon appeared.  I would suggest any time any user data is stored in a database and communicated use https. Consider this requirement even if the user data is mundane because even many of these mundane details are used by that user to identify themselves on other websites. Consider all the random security questions your bank asks you (like what street do you live on?). This can be taken from address fields really easily. In this case the data is not what you consider a password but it might as well be. Furthermore you can never anticipate what user data will be used for a security question elsewhere. You can also expect that with the intelligence of the average web user (think your grandmother) that that tidbit of information might make up part of that user's password somewhere else. One pointer if you use https make it so that if the user types http://www.website-that-needs-https.com/etc/yadda.php they will automatically get redirected to https://www.website-that-needs-https.com/etc/yadda.php (personal pet peeve) However if you're just doing a plain html webpage that will be essentially a one-way transmission of information from the server to the user don't worry about it.  So it is not safe to have the user login on a secured page e.g. https://yadayada.com/login.php and after a sucessful login redirect the user to an ordinary php page which has the ordinary session mgmt (According to AviD's 8th bullet)? ordinarypage.php: If that is the case how do I set the ""secure"" attribute of that session?"185,A,"How can I read the properties of a C# class dynamically? I can do an eval(""something()""); to execute the code dynamically in JavaScript. Is there a way for me to do the same thing in C#? What I am exactly trying to do is that I have an integer variable (say i) and I have multiple properties by the names: ""Property1"" ""Property2"" ""Property3"" etc. Now I want to perform some operations on the "" Property*i* "" property depending on the value of i. This is really simple with Javascript. Is there any way to do this with C#? c# call ironpython's eval. I tried it in c# 4.0. no experience with c# 2.0 @Peter Long where can I find documentation on IronPython's eval ? Take a look at [Mono's CSharp interactive shell](http://www.mono-project.com/CsharpRepl). It has [eval-like functions](http://www.go-mono.com/docs/index.aspx?link=N:Mono.CSharp). Unfortunately C# isn't a dynamic language like that. What you can do however is to create a C# source code file full with class and everything and run it through the CodeDom provider for C# and compile it into an assembly and then execute it. This forum post on MSDN contains an answer with some example code down the page somewhat: create a anonymous method from a string? I would hardly say this is a very good solution but it is possible anyway. What kind of code are you going to expect in that string? If it is a minor subset of valid code for instance just math expressions it might be that other alternatives exists. Edit: Well that teaches me to read the questions thoroughly first. Yes reflection would be able to give you some help here. If you split the string by the ; first to get individual properties you can use the following code to get a PropertyInfo object for a particular property for a class and then use that object to manipulate a particular object. String propName = ""Text""; PropertyInfo pi = someObject.GetType().GetProperty(propName); pi.SetValue(someObject ""New Value"" new Object[0]); Link: PropertyInfo.SetValue Method  My C# Eval program allows you to do this. It accepts a substantial subset of the C# language and compiles it at run time into dynamic methods. Look at my website KamimuCode.Com for the full details.  Unfortunately C# doesn't have any native facilities for doing exactly what you are asking. However my C# eval program does allow for evaluating C# code. It provides for evaluating C# code at runtime and supports many C# statements. In fact this code is usable within any .NET project however it is limited to using C# syntax. Have a look at my website http://csharp-eval.com for additional details.  This is an eval function under c#. I used it to convert anonymous functions (Lambda Expressions) from a string. Source: http://www.codeproject.com/KB/cs/evalcscode.aspx public static object Eval(string sCSCode) { CSharpCodeProvider c = new CSharpCodeProvider(); ICodeCompiler icc = c.CreateCompiler(); CompilerParameters cp = new CompilerParameters(); cp.ReferencedAssemblies.Add(""system.dll""); cp.ReferencedAssemblies.Add(""system.xml.dll""); cp.ReferencedAssemblies.Add(""system.data.dll""); cp.ReferencedAssemblies.Add(""system.windows.forms.dll""); cp.ReferencedAssemblies.Add(""system.drawing.dll""); cp.CompilerOptions = ""/t:library""; cp.GenerateInMemory = true; StringBuilder sb = new StringBuilder(""""); sb.Append(""using System;\n"" ); sb.Append(""using System.Xml;\n""); sb.Append(""using System.Data;\n""); sb.Append(""using System.Data.SqlClient;\n""); sb.Append(""using System.Windows.Forms;\n""); sb.Append(""using System.Drawing;\n""); sb.Append(""namespace CSCodeEvaler{ \n""); sb.Append(""public class CSCodeEvaler{ \n""); sb.Append(""public object EvalCode(){\n""); sb.Append(""return ""+sCSCode+""; \n""); sb.Append(""} \n""); sb.Append(""} \n""); sb.Append(""}\n""); CompilerResults cr = icc.CompileAssemblyFromSource(cp sb.ToString()); if( cr.Errors.Count > 0 ){ MessageBox.Show(""ERROR: "" + cr.Errors[0].ErrorText ""Error evaluating cs code"" MessageBoxButtons.OK MessageBoxIcon.Error ); return null; } System.Reflection.Assembly a = cr.CompiledAssembly; object o = a.CreateInstance(""CSCodeEvaler.CSCodeEvaler""); Type t = o.GetType(); MethodInfo mi = t.GetMethod(""EvalCode""); object s = mi.Invoke(o null); return s; } @sehe Whoops I corrected the typo (Lambada => Lambda). I didn't knew that the song is called Lambada so this one was unintentional. ;)  the correct answer is you need to cache all the result to keep the mem0ry usage low. an example would look like this TypeOf(Evaluate) { ""1+1"":2; ""1+2"":3; ""1+3"":5; .... ""2-5"":-3; ""0+0"":1 } and add it to a List List<string> results = new List<string>(); for() results.Add(result); save the id and use it in the code hope this helps Uhmmm... what do you mean? somebody confused evaluation with lookup. If you know all possible programs (I **think** that is _at least_ NP-Hard)... and you have a supermachine to precompile all possible results... **and** there are no sideeffects/external inputs... Yeah this idea _theoretically_ works. The code is one big syntax error though  Don't use reflection it's dirty (I really don't like it). Wait for .NET 4.0 and use Expression Trees: http://community.bartdesmet.net/blogs/bart/archive/2009/08/10/expression-trees-take-two-introducing-system-linq-expressions-v4-0.aspx ""dirty""? Data binding and serialisation are built on top of reflection so it can't be that bad. :-) I know it's very powerful. And with great power comes great responsibility. And a user who asks this type of question shouldn't touch reflection.  You can use reflection to get the property and invoke it. Something like this: object result = theObject.GetType().GetProperty(""Property"" + i).GetValue(theObject null); That is assuming the object that has the property is called ""theObject"" :)  I have written an open source project Dynamic Expresso that can convert text expression written using a C# syntax into delegates (or expression tree). Expressions are parsed and transformed into Expression Trees without using compilation or reflection. You can write something like: var interpreter = new Interpreter(); var result = interpreter.Eval(""8 / 2 + 2""); or var interpreter = new Interpreter() .SetVariable(""service"" new ServiceExample()); string expression = ""x > 4 ? service.SomeMethod() : service.AnotherMethod()""; Lambda parsedExpression = interpreter.Parse(expression new Parameter(""x"" typeof(int))); parsedExpression.Invoke(5); My work is based on Scott Gu article http://weblogs.asp.net/scottgu/archive/2008/01/07/dynamic-linq-part-1-using-the-linq-dynamic-query-library.aspx .  I don't now if you absolutely want to execute C# statements but you can already execute Javascript statements in C# 2.0. The open-source library Jint is able to do it. It's a Javascript interpreter for .NET. Pass a Javascript program and it will run inside your application. You can even pass C# object as arguments and do automation on it. Also if you just want to evaluate expression on your properties give a try to NCalc.  You could do it with a prototype function: void something(int i string P1) { something(i P1 String.Empty); } void something(int i string P1 string P2) { something(i P1 P2 String.Empty); } void something(int i string P1 string P2 string P3) { something(i P1 P2 P3 String.Empty); } and so on...  Uses reflection to parse and evaluate a data-binding expression against an object at run time. DataBinder.Eval Method Can you show an example how?  All of that would definitely work. Personally for that particular problem I would probably take a little different approach. Maybe something like this: class MyClass {  public Point point1 point2 point3;  private Point[] points;  public MyClass() {  //...  this.points = new Point[] {point1 point2 point3};  }  public void DoSomethingWith(int i) {  Point target = this.points[i+1];  // do stuff to target  } } When using patterns like this you have to be careful that your data is stored by reference and not by value. In other words don't do this with primitives. You have to use their big bloated class counterparts. I realized that's not exactly the question but the question has been pretty well answered and I thought maybe an alternative approach might help.  Not really. You can use reflection to achieve what you want but it won't be nearly as simple as in Javascript. For example if you wanted to set the private field of an object to something you could use this function:  protected static void SetField(object o string fieldName object value)  {  FieldInfo field = o.GetType().GetField(fieldName BindingFlags.Instance | BindingFlags.NonPublic);  field.SetValue(o value);  }  You also could implement a Webbrowser then load a html-file wich contains javascript. Then u go for the document.InvokeScript Method on this browser. The return Value of the eval function can be catched and converted into everything you need. I did this in several Projects and it works perfectly. Hope it helps"186,A,"Best way to access Exchange using PHP? I'm writing a CMS application in PHP and one of the requirements is that it must be able to interface with the customer's Exchange server. I've written up this functionality a few times before and have always used WebDAV to do it but now I'm leaning away from that. I will be running the site on IIS OR Apache (no preference) on Windows server 2008. A few things I would need to do include adding contacts to a given user's address book sending emails as a given user and running reports on contacts for a user. All of this is pretty easy to do with WebDAV but if there is a better way that doesn't require any functionality that is likely to be deprecated any time soon. Any ideas? Update: Justin I love the idea of using com objects I just worry about maintaining a 3rd product to make everything work... John I can write a web service in C# to interface with for these functions and access it with my PHP app but it's also a little bit out of the way. So far I'm not 100% convinced that either of these is better than WebDAV... Can anyone show me where I'm being silly? I vote for WebDAV. If it can do what you need stay with it. It's a simple well-defined interface. COM and consorts are the most flexible but sparsely documented and often unstable. Can you point to an example of connecting to exchange server with WEBDAV in php? I would like to add a calendar event to exchange server. I have released an open-source MIT licensed library that allows you to do some basic operations in PHP using Exchange Web Services. Exchange Web Services for PHP I have only tested it on Linux but I don't see any reason why it wouldn't work on a Windows installation of PHP as well. Hi can you elaborate on how can i create a calendar event on exchange server from my php script? Thanks so i need to host the exchange server and integrate it with my php website? And also integrate outlook and exchange server manually? Do you have an idea about php-exchange server integration? There's a function called ""create_event"". It should be pretty self-explanatory. https://github.com/rileydutton/Exchange-Web-Services-for-PHP/blob/master/Exchangeclient.php#L55  I can't recommend Dmitry Streblechenko's Redemption Data Objects library highly enough. It's a COM component that provides a sane API to Extended MAPI and is a joy to use. The Exchange API goalposts move from one release to the next: Use the M: drive! No use WebDAV! No use ExOLEDB!_ No use Web Services! with the only constant being good old MAPI.  I would look into IMAP IMAP POP3 and NNTP  Is your customer using Exchange 2007? If so I'd have a look at Exchange Web Services. If not as hairy as it can be I think WebDAV is your best bet. Personally I don't like using the Outlook.Application COM object route as its security prompts (""An application is attempting to access your contacts. Allow this?"" etc.) can cause problems on a server. I also think it would be difficult to accomplish your impersonation-like tasks using Outlook such as sending mail as a given user.  I'm not a PHP dev but Google says that PHP 5+ can instantiate COM components. If you can install Outlook on a box you could write a PHP web service around the COM component to handle the requests you need. $outlook = COM(""Outlook.Application"") Outlook API referance  The Zafara PHP MAPI extension looks like it could work.  I have not used PHP to do this but have experience in using C# to achieve the same thing. The Outlook API is a way of automating Outlook rather than connecting to Exchange directly. I have previously taken this approach in a C# application and it does work although can be buggy. If you wish to connect directly to the Exchange server you will need to research extended MAPI. In the past I used this wrapper MAPIEx: Extended MAPI Wrapper. It is a C# project but I believe you can use some .NET code on a PHP5 Windows server. Alternatively it has a C++ core DLL that you may be a able to use. I have found it to be very good and there are some good example applications. Update: Sorry for the delay no current way to keep track of posts yet. I do agree adding more layer on to your application and relying on 3rd party code can be scary (and rightfully so.) Today I read another interesting post tagged up as MAPI that is on a different subject. The key thing here though is that it has linked to this important MS article. I have been unaware of the issues until now on using managed code to interface to MAPI although the C++ code in the component should be unaffected by this error as it is unmanaged. This blog entry also suggests other ways to connect to MAPI/Exchange server. In this case due to these new facts http://us3.php.net/imap may be the answer as suggested by the other user.  I would recommend using ""PHP Exchange Web Services"" or short php-ews. Fair amount of documentation under the wiki helped me a lot."187,A,Cleanest way to implement collapsable entries in a table generated via asp:Repeater? Before anyone suggests scrapping the table tags altogether I'm just modifying this part of a very large system so it really wouldn't be wise for me to revise the table structure (the app is filled with similar tables). This is a webapp in C# .NET - data comes in from a webservice and is displayed onscreen in a table. The table's rows are generated with asp:Repeaters so that the rows alternate colers nicely. The table previously held one item of data per row. Now essentially the table has sub-headers... The first row is the date the second row shows a line of data and all the next rows are data rows until data of a new date comes in in which case there will be another sub-header row. At first I thought I could cheat a little and do this pretty easily to keep the current repeater structure- I just need to feed some cells the empty string so that no data appears in them. Now however we're considering one of those +/- collapsers next to each date so that they can collapse all the data. My mind immediately went to hiding rows when a button is pressed... but I don't know how to hide rows from the code behind unless the row has a unique id and I'm not sure if you can do that with repeaters. I hope I've expressed the problem well. I'm sure I'll find a way TBH but I just saw this site on slashdot and thought I'd give it a whirl :) When you build the row in the databinding event you can add in a unique identifier using say the id of the data field or something else that you use to make it unique. Then you could use a client side method to expand collapse if you want to fill it with data in the beginning toggling the style.display setting in Javascript for the table row element.  just wrap the contents of the item template in an asp:Panel then you have you have a unique id. Then throw in some jquery for some spice ;) edit: just noticed that you are using a table. put the id on the row. then toggle it.188,A,"How to implement Repository pattern withe LinqToEntities? How to implement Repository pattern withe LinqToEntities how to implement the interface Ive almost like this except for the ""Castle Windor"" stuff. Take a look at openticket.codeplex.com  I do the following: A service layer contains my business objects. It is passed the repository via an Inversion of Control (Castle Windor is my usual choice). The repository is in charge of mapping between the business objects and my entity framework objects. The advantages: You have no problems with object state or the context of the EF objects because you are just loading them during data manipulation on the repository side. This eases the situation when passing them to WCF/Web-Services. The disadvantages: You are losing some of the tracking functionality of Entity Framework you have to manually load the data object (ef objects) possibly if required manually to optimistic concurrency checks (via a timestamp on the business object for example). But generally I prefer this solution because it is possible to later change the repository. It allows me to have different repositories (for example my user object is actually using the ASPNetAuthenticationRepository instead of the EntityFrameworkRepository) but for my service layer it's transparent. With regards to the interface I would use the business objects from the service layer as your parameter objects and don't let those EF objects out of the repository layer. Hope that helps"189,A,"How do you list all triggers in a MySQL database? What is the command to list all triggers in a MySQL database? For showing a particular trigger in a particular schema you can try the following: select * from information_schema.triggers where information_schema.triggers.trigger_name like '%trigger_name%' and information_schema.triggers.trigger_schema like '%data_base_name%'  I hope following code will give you more information. select * from information_schema.triggers where information_schema.triggers.trigger_schema like '%your_db_name%' This will give you total 22 Columns in MySQL version: 5.5.27 and Above TRIGGER_CATALOG TRIGGER_SCHEMA TRIGGER_NAME EVENT_MANIPULATION EVENT_OBJECT_CATALOG EVENT_OBJECT_SCHEMA EVENT_OBJECT_TABLE ACTION_ORDER ACTION_CONDITION ACTION_STATEMENT ACTION_ORIENTATION ACTION_TIMING ACTION_REFERENCE_OLD_TABLE ACTION_REFERENCE_NEW_TABLE ACTION_REFERENCE_OLD_ROW ACTION_REFERENCE_NEW_ROW CREATED SQL_MODE DEFINER CHARACTER_SET_CLIENT COLLATION_CONNECTION DATABASE_COLLATION  The command is: show triggers or you can access the INFORMATION_SCHEMA table directly by: select trigger_schema trigger_name action_statement from information_schema.triggers You can do this from version 5.0.10 onwards. More information about the TRIGGERS table is here. I'd recommend going with using the ""SHOW TRIGGERS"" query instead of accessing information_schema directly - the latter will be very slow once you have more than thousand databases on the server while ""SHOW TRIGGERS"" still has an outstanding performance.  You can use below to find a particular trigger definition. SHOW TRIGGERS LIKE '%trigger_name%'\G or the below to show all the triggers in the database. It will work for MySQL 5.0 and above. SHOW TRIGGERS\G"190,A,"Reading from text file until EOF repeats last line The following C++ code uses a ifstream object to read integers from a text file (which has one number per line) until it hits EOF. Why does it read the integer on the last line twice? How to fix this? Code: #include <iostream> #include <fstream> using namespace std; int main() { ifstream iFile(""input.txt""); // input.txt has integers one per line while (!iFile.eof()) { int x; iFile >> x; cerr << x << endl; } return 0; } input.txt: 10 20 30 Output: 10 20 30 30 Note: I've skipped all error checking code to keep the code snippet small. The above behaviour is seen on Windows (Visual C++) cygwin (gcc) and Linux (gcc). int x; ifile >> x while (!iFile.eof()) { cerr << x << endl; iFile >> x; }  Without to much modifications of the original code it could become : while (!iFile.eof()) { int x; iFile >> x; if (!iFile) break; cerr << x << endl; } but I prefer the two other solutions above in general.  There's an alternative approach to this: #include <iterator> #include <algorithm> // ... copy(istream_iterator<int>(iFile) istream_iterator<int>() ostream_iterator<int>(cerr ""\n""));  I like this example which for now leaves out the check which you could add inside the while block: ifstream iFile(""input.txt""); // input.txt has integers one per line int x; while (iFile >> x) { cerr << x << endl; } Not sure how safe it is... What if 0 is a valid value like x==0?  Just follow closely the chain of events. Grab 10 Grab 20 Grab 30 Grab EOF Look at the second-to-last iteration. You grabbed 30 then carried on to check for EOF. You haven't reached EOF because the EOF mark hasn't been read yet (""binarically"" speaking its conceptual location is just after the 30 line). Therefore you carry on to the next iteration. x is still 30 from previous iteration. Now you read from the stream and you get EOF. x remains 30 and the ios::eofbit is raised. You output to stderr x (which is 30 just like in the previous iteration). Next you check for EOF in the loop condition and this time you're out of the loop. Try this: while (true) { int x; iFile >> x; if( iFile.eof() ) break; cerr << x << endl; } By the way there is another bug in your code. Did you ever try to run it on an empty file? The behaviour you get is for the exact same reason. Same spirit as up comment: instead of `while(true)` it seems better to write `while(file.good())` use 'while(iFile >> x)'. This reads the integer and returns the stream. When a stream is used as bool value it checks to see if the stream is valid. Valid means eof() and bad() are both false. see http://stackoverflow.com/questions/21647/c-reading-from-text-file-until-eof-repeats-last-line#21666"191,A,"Calling Table-Valued SQL Functions From .NET Scalar-valued functions can be called from .NET as follows: SqlCommand cmd = new SqlCommand(""testFunction"" sqlConn); //testFunction is scalar cmd.CommandType = CommandType.StoredProcedure; cmd.Parameters.Add(""retVal"" SqlDbType.Int); cmd.Parameters[""retVal""].Direction = ParameterDirection.ReturnValue; cmd.ExecuteScalar(); int aFunctionResult = (int)cmd.Parameters[""retVal""].Value; I also know that table-valued functions can be called in a similar fashion for example: String query = ""select * from testFunction(param1...)""; //testFunction is table-valued SqlCommand cmd = new SqlCommand(query sqlConn); SqlDataAdapter adapter = new SqlDataAdapter(cmd); adapter.Fill(tbl); My question is can table-valued functions be called as stored procedures like scalar-valued functions can? (e.g. replicate my first code snippet with a table-valued function being called and getting the returned table through a ReturnValue parameter). @ChuckConway Why dropping C# from the title? It was valid considering the C# tag... but that tag does not help it to show up in google. You might think of a different approach which has nothing to do with SQL. We use a net library named finaquant protos in our calculation engine for ecological population forecasts. This library has high-level table functions for table algebra aggregation filtering sampling allocation distribution function routing and so on. Data tables are represented with a class named MatrixTable and all the operations apply on MatrixTable objects as inputs and outputs.  No because you need to select them. However you can create a stored proc wrapper which may defeat the point of having a table function."192,A,"Simplest way to check if two integers have same sign? Which is the simplest way to check if two integers have same sign? Is there any short bitwise trick to do this? Assuming 32 bit ints: bool same = ((x ^ y) & >> 31) != 1; Slightly more terse: bool same = !((x ^ y) & >> 31); Those two code examples should ALWAYS ALWAYS ALWAYS be preceded by a code comment please (in real life) Oh of course. In real life I'd probably use something like same = Math.Sign(x) == Math.Sign(y). I just give evil bitwidily solutions when people ask for them. :D  For any size of int with two's complement arithmetic: #define SIGNBIT (~((unsigned int)-1 >> 1)) if ((x & SIGNBIT) == (y & SIGNBIT)) // signs are the same  assuming 32 bit if ( ((x^y) & 0x80000000) == 0) ... the answer if(x*y>0) is bad due to overflow  Just off the top of my head... int mask = 1 << 31; (a & mask) ^ (b & mask) < 0; only works for 32 bit ints which was not stipulated in the question  if (x * y) > 0... assuming non-zero and such.  branchless C version: int sameSign(int a int b) { return ~(a^b) & (1<<(sizeof(int)*8-1)); } C++ template for integer types: template <typename T> T sameSign(T a T b) { return ~(a^b) & (1<<(sizeof(T)*8-1)); }  if ((a ^ b) >= 0) sign is the same This one results in the delightfully compact ""xorl %edi %esi; sets %al"" on x86 only 6 bytes and two instructions. It's also an interesting case study as it's a concrete case where returning a 'bool' instead of an int produces dramatically better code. Oh nice! :-) I'm surprised that I missed this one. The really nice thing about this solution is it doesn't depend upon a particular bit cardinality in the underlying integer representation.  if (a*b < 0) sign is different else sign is the same (or a or b is zero) Doesn't work for overflow  I would be wary of any bitwise tricks to determine the sign of integers as then you have to make assumptions about how those numbers are represented internally. Almost 100% of the time integers will be stored as two's compliment but it's not good practice to make assumptions about the internals of a system unless you are using a datatype that guarentees a particular storage format. In two's compliment you can just check the last (left-most) bit in the integer to determine if it is negative so you can compare just these two bits. This would mean that 0 would have the same sign as a positive number though which is at odds with the sign function implemented in most languages. Personally I'd just use the sign function of your chosen language. It is unlikely that there would be any performance issues with a calculation such as this. Also those tricks decrease readability. The C Standard Lib provides a signbit() function. Probably tough to beat the optimiser with any ""bitwise tricks"" of your own devising.  Thinking back to my university days in most machine representations isn't the left-most bit of a integer a 1 when the number is negative and 0 when it's positive? I imagine this is rather machine-dependent though.  (integer1 * integer2) > 0 Because when two integers share a sign the result of multiplication will always be positive. You can also make it >= 0 if you want to treat 0 as being the same sign no matter what. Only works until the product overflows. also multiplication might be rather slow...  Assuming twos complement arithmetic (http://en.wikipedia.org/wiki/Two_complement): inline bool same_sign(int x int y) { return (x^y) >= 0; } This can take as little as two instructions and less than 1ns on a modern processor with optimization. Not assuming twos complement arithmetic: inline bool same_sign(int x int y) { return (x<0) == (y<0); } This may require one or two extra instructions and take a little longer. Using multiplication is a bad idea because it is vulnerable to overflow. 1 and 0 is returning true for both functions... @matias As it should. 0 and 1 have the same sign. Integers don't have a negative 0. obviously you are right i totally brain farted  As a technical note bit-twiddly solutions are going to be much more efficient than multiplication even on modern architectures. It's only about 3 cycles that you're saving but you know what they say about a ""penny saved""... a penny saved is the root of all evil say about 97% of the time?  int same_sign = !( (x >> 31) ^ (y >> 31) ); if ( same_sign ) ... else ...  What's wrong with return ((x<0) == (y<0)); ? +1 for language-generic answer Um... Nothing... Sad that we all missed the simple solution. Great answer simplicity is wonderful. What about Signed zero. -0.0 vs Unsigned zero 0.0 @lafur: OP specified integers not floats.  Here is a version that works in C/C++ that doesn't rely on integer sizes or have the overflow problem (i.e. x*y>=0 doesn't work) bool SameSign(int x int y) { return (x >= 0) ^ (y < 0); } Of course you can geek out and template: template <typename valueType> bool SameSign(typename valueType x typename valueType y) { return (x >= 0) ^ (y < 0); } Note: Since we are using exclusive or we want the LHS and the RHS to be different when the signs are the same thus the different check against zero. Quite nice the C/C++ hacker in me fully supports this code snippet. The software engineer in me questions why the user needs to know this in such a generic manner!  I'm not really sure I'd consider ""bitwise trick"" and ""simplest"" to be synonymous. I see a lot of answers that are assuming signed 32-bit integers (though it would be silly to ask for unsigned); I'm not certain they'd apply to floating-point values. It seems like the ""simplest"" check would be to compare how both values compare to 0; this is pretty generic assuming the types can be compared: bool compare(T left T right) { return (left < 0) && (right < 0); } If the signs are opposite you get false. If the signs are the same you get true. If both items are 0 you get true and you can have fun figuring out what sign you want to attribute to that. false && false == false I'm afraid that you would need to make that the negation of an XOR in order to make it correct."193,A,XNA File Load In XNA how do I load in a texture or mesh from a file without using the content pipeline? If you really want to load an Xna Xna.Framework.Graphics.Model on PC without the content pipeline (eg for user generated content) there is a way. I used SlimDX to load an X file and avoid the parsing code the some reflection tricks to instantiate the Model (it is sealed and has a private constructor so wasn't meant to be extended or customised). See here: http://contenttracker.codeplex.com/SourceControl/changeset/view/20704#346981  I believe Texture2D.FromFile(); is what you are looking for. It does not look like you can do this with a Model though. http://msdn.microsoft.com/en-us/library/microsoft.xna.framework.graphics.texture2d.fromfile.aspx XNA 4.0 seems to have no Texture2D.FromFile(). This might work only under 3.x versions. You can use Texture2D.FromStream(GraphicsDevice stream) in 4.0.  The .FromFile method will not work on xbox or zune. You have two choices: Just use the content pipeline ... on xbox or zune (if you care about them) you can't have user-supplied content anyways so it doesn't matter if you only use the content pipeline. Write code to load the texture (using .SetData) or of course to parse the model file and load the appropriate vertexbuffers etc.  This is a windows only Way to load a texture without loading it through the pipeline As Cory stated above all content must be compiled before loading it on the Xbox and Zune. Texture2D texture = Texture2D.FromFile(GraphicsDeviceManager.GraphicsDevice @Location of your Texture Here.png);  For anyone interested in loading a model from a file check out this tutorial: http://creators.xna.com/en-us/sample/winforms_series2 That method relies on the content pipeline. It actually builds the model file behind the scenes and then loads the Xnb via the content pipeline.194,A,"Get list of domains on the network Using the Windows API how can I get a list of domains on my network? Answered my own question: Use the NetServerEnum function passing in the SVTYPEDOMAIN_ENUM constant for the ""servertype"" argument. In Delphi the code looks like this: <snip> type NET_API_STATUS = DWORD; PSERVER_INFO_100 = ^SERVER_INFO_100; SERVER_INFO_100 = packed record sv100_platform_id : DWORD; sv100_name : PWideChar; end; function NetServerEnum( //get a list of pcs on the network (same as DOS cmd ""net view"") const servername : PWideChar; const level : DWORD; const bufptr : Pointer; const prefmaxlen : DWORD; const entriesread : PDWORD; const totalentries : PDWORD; const servertype : DWORD; const domain : PWideChar; const resume_handle : PDWORD ) : NET_API_STATUS; stdcall; external 'netapi32.dll'; function NetApiBufferFree( //memory mgmt routine const Buffer : Pointer ) : NET_API_STATUS; stdcall; external 'netapi32.dll'; const MAX_PREFERRED_LENGTH = DWORD(-1); NERR_Success = 0; SV_TYPE_ALL = $FFFFFFFF; SV_TYPE_DOMAIN_ENUM = $80000000; function TNetwork.ComputersInDomain: TStringList; var pBuffer : PSERVER_INFO_100; pWork : PSERVER_INFO_100; dwEntriesRead : DWORD; dwTotalEntries : DWORD; i : integer; dwResult : NET_API_STATUS; begin Result := TStringList.Create; Result.Clear; dwResult := NetServerEnum(nil100@pBufferMAX_PREFERRED_LENGTH @dwEntriesRead@dwTotalEntriesSV_TYPE_DOMAIN_ENUM PWideChar(FDomainName)nil); if dwResult = NERR_SUCCESS then begin try pWork := pBuffer; for i := 1 to dwEntriesRead do begin Result.Add(pWork.sv100_name); inc(pWork); end; //for i finally NetApiBufferFree(pBuffer); end; //try-finally end //if no error else begin raise Exception.Create('Error while retrieving computer list from domain ' + FDomainName + #13#10 + SysErrorMessage(dwResult)); end; end; <snip>  You will need to use some LDAP queries Here is some code I have used in a previous script (it was taken off the net somewhere and I've left in the copyright notices) ' This VBScript code gets the list of the domains contained in the ' forest that the user running the script is logged into ' --------------------------------------------------------------- ' From the book ""Active Directory Cookbook"" by Robbie Allen ' Publisher: O'Reilly and Associates ' ISBN: 0-596-00466-4 ' Book web site: http://rallenhome.com/books/adcookbook/code.html ' --------------------------------------------------------------- set objRootDSE = GetObject(""LDAP://RootDSE"") strADsPath = ""<GC://"" & objRootDSE.Get(""rootDomainNamingContext"") & "">;"" strFilter = ""(objectcategory=domainDNS);"" strAttrs = ""name;"" strScope = ""SubTree"" set objConn = CreateObject(""ADODB.Connection"") objConn.Provider = ""ADsDSOObject"" objConn.Open ""Active Directory Provider"" set objRS = objConn.Execute(strADsPath & strFilter & strAttrs & strScope) objRS.MoveFirst while Not objRS.EOF Wscript.Echo objRS.Fields(0).Value objRS.MoveNext wend Also a C# version"195,A,Measuring/benchmarking hard drive performance We know that disk speed is an important performance component of a development machine. I'd like to understand how the multiple drives in my Windows Vista machine compare. Vista will give me a performance index for the system but what's a good (preferably free) benchmarking tool for measuring each drive's performance? Try diskTT it's small and free benchmarking application for disk drives. Here you can find some usage reports on it. As usual you should try with different test file sizes in order to mimic the normal usage of your system/application. Some apps use many small files other some very large and the resulting access speed can vary dramatically.  Get a Linux live CD (there's a few) that has bonnie and hdparm installed. Boot it up and run hdparm in read only mode on each of your drives. The advantage of using a live CD is that you remove the overhead that Windows adds (swap background processes etc) which you can never truly stop. Bonnie is used to test filesystem performance not disk performance. Both are useful tools.  HDTune that is used in many review sites has also a free version (for personal use)  Disk Bench is basic but free and will probably do everything you need. There's probably a host of other tools out there but it did the job for me in the past.  I've recently discovered Iometer. I haven't tested it but I'll report back when I have.196,A,"Hide directories in wxGenericDirCtrl I am using a wxGenericDirCtrl and I would like to know if there is a way to hide directories I'd especially like to hide siblings of parent nodes. For example if my directory structure looks like this: +-a | +-b | | | +-whatever | +-c | | | +-d | | | +-e | | | +-f | +-g | +-whatever If my currently selected directory is /a/c/d is there any way to hide b and g so that the tree looks like this in my ctrl: +-a | +-c | +-[d] | +-e | +-f I'm currently working with a directory structure that has lots and lots directories that are irrelevant to most users so it would be nice to be able to clean it up. Edit: If it makes a difference I am using wxPython and so far I have only tested my code on linux using the GTK backend but I do plan to make it multi-platform and using it on Windows and Mac using the native backends. I don't think that's possible. It would be relatively easy to add this functionality to the underlying C++ wxWidgets control but since you're using wxPython you'd then have to rebuild that as well which is a tremendous issue. I have no problem rebuilding anything this is for an in-house app.  Listing/walking directories in Python is very easy so I would recommend trying to ""roll your own"" using one of the simple tree controls (such as TreeCtrl or CustomTreeCtrl). It should really be quite easy to call the directory listing code when some directory is expanded and return the result. Thanks but it's not worth the effort for me at this point. I'm gonna say this is the only solution to the problem since it doesn't seem like there's a way to do it with what exists"197,A,"Which way do you prefer to create your forms in MVC? Which way do you prefer to create your forms in MVC? <% Html.Form() { %> <% } %> Or <form action=""<%= Url.Action(""ManageImage"" ""UserAccount"") %>"" method=""post""> </form> I understand that Html.Form() as of PR5 now just uses the URL provided by the request. However something about that doesn't sit well with me especially since I will be getting all the baggage of any querystrings that are included. What is your take? The reason for using helpers is that they allow you to encapsulate common patterns in a consistent and DRY fashion. Think of them as a way of refactoring views to remove duplication just as you would with regular code. For example I blogged about some RESTful NHaml helpers that can build urls based on a model.  The second way definitely. The first way is programmer-centric which is not what the V part of MVC is about. The second way is more designer centric only binding to the model where it is necessary leaving the HTML as natural as possible. I find myself doing it the first way mostly. I'm a hypocrite.  I have to agree with both of you I am not really like this simplistic WebForms style that seems to be integrating its way in to MVC. This stuff almost seems like it should be a 3rd party library or at the very least an extensions library that can be included if needed or wanted.  I agree with Andrew Peters DRY. It should also be pointed out that you can specify your controller action and params to the .Form() helper and if they fit into your routing rules then no query string parameters will be used. I also understand what Will was saying about the V in MVC. In my opinion I do not think it is a problem to put code in the view as long as it is for the view. It is really easy to cross the line between controller and view if you are not careful. Personally I can not stand to use C# as a template engine without my eyes bleeding or getting the urge to murder someone. This helps me keep my logic separated controller logic in C# view logic in brail.  On the whole I think I'm kinda old-school as I prefer to roll my own HTML elements. I also prefer a view engine like like NHaml which makes writing HTML almost an order of magnitude simpler.  I am totally in the opinion of old school HTML that is what designers use. I don't like to include to much code centric syntax for this reason. I treat the web form view engine like a third party library because I replaced it with a different view engine. If you do not like the way the web form view model works or the direction it is going you can always go a different route. That is one of the main reasons I love ASP.NET MVC."198,A,"When to use unsigned values over signed ones? When is it appropriate to use an unsigned variable over a signed one? What about in a for loop? I hear a lot of opinions about this and I wanted to see if there was anything resembling a consensus. for (unsigned int i = 0; i < someThing.length(); i++) { SomeThing var = someThing.at(i); // You get the idea. } I know Java doesn't have unsigned values and that must have been a concious decision on Sun Microsystems' part. I found this helpful: http://codemines.blogspot.ca/2007/10/youre-probably-using-unsigned.html size_t is often a good choice for this or size_type if you're using an STL class. Only when you're dealing with the size of something in bytes.  In your example above when 'i' will always be positive and a higher range would be beneficial unsigned would be useful. Like if you're using 'declare' statements such as: #declare BIT1 (unsigned int 1) #declare BIT32 (unsigned int reallybignumber) Especially when these values will never change. However if you're doing an accounting program where the people are irresponsible with their money and are constantly in the red you will most definitely want to use 'signed'. I do agree with saint though that a good rule of thumb is to use signed which C actually defaults to so you're covered.  I would think that if your business case dictates that a negative number is invalid you would want to have an error shown or thrown. With that in mind I only just recently found out about unsigned integers while working on a project processing data in a binary file and storing the data into a database. I was purposely ""corrupting"" the binary data and ended up getting negative values instead of an expected error. I found that even though the value converted the value was not valid for my business case. My program did not error and I ended up getting wrong data into the database. It would have been better if I had used uint and had the program fail.  Naturally you're compiling with warnings turned all the way up right? And have you considered compiling with ""treat warnings as errors"" to take it that one step further? I do with my own code but when working with other people's stuff nothing would ever compile. My example was contrived I used to be fairly inconsistent but I generally stick to ""when in doubt use signed"".  C and C++ compilers will generate a warning when you compare signed and unsigned types; in your example code you couldn't make your loop variable unsigned and have the compiler generate code without warnings (assuming said warnings were turned on). Naturally you're compiling with warnings turned all the way up right? And have you considered compiling with ""treat warnings as errors"" to take it that one step further? The downside with using signed numbers is that there's a temptation to overload them so that for example the values 0->n are the menu selection and -1 means nothing's selected - rather than creating a class that has two variables one to indicate if something is selected and another to store what that selection is. Before you know it you're testing for negative one all over the place and the compiler is complaining about how you're wanting to compare the menu selection against the number of menu selections you have - but that's dangerous because they're different types. So don't do that.  I was glad to find a good conversation on this subject as I hadn't really given it much thought before. In summary signed is a good general choice - even when you're dead sure all the numbers are positive - if you're going to do arithmetic on the variable (like in a typical for loop case). If you're going to do bitwise things like masks unsigned starts to make more sense. Or if you're desperate to get that extra positive range by taking advantage of the sign bit. Personally I like signed because I don't trust myself to stay consistent and avoid mixing the two types (like the article warns against). Later in that thread it's shown that `unsigned` is vastly superior for detecting overflow in untrusted inputs. Unfortunately the proposed ""answers"" to the puzzle aren't all that great. Mine is `template bool range_check_sum( unsigned a unsigned b ) { return (a < limit) && (b < limit - a); }` If anyone has a similarly simple and straightforward answer using signed types I'd love to see it."199,A,"How to display ""12 minutes ago"" etc in a PHP webpage? Can anyone tell me how I can display a status message like ""12 seconds ago"" or ""5 minutes ago"" etc in a web page? This question was [previously asked](http://stackoverflow.com/questions/11/how-do-i-calculate-relative-time) the example code in answers should be pretty easy to convert to PHP. This has been covered (though with more of a C# focus) in [this thread](http://stackoverflow.com/questions/11/how-do-i-calculate-relative-time). There is a nice jquery plugin : timeago.js great question :) PHP's \DateTime::diff returns a \DateInterval object on which you can get the minutes by the public ""i"" property.  Here is the php code for the same: function time_since($since) { $chunks = array( array(60 * 60 * 24 * 365  'year') array(60 * 60 * 24 * 30  'month') array(60 * 60 * 24 * 7 'week') array(60 * 60 * 24  'day') array(60 * 60  'hour') array(60  'minute') array(1  'second') ); for ($i = 0 $j = count($chunks); $i < $j; $i++) { $seconds = $chunks[$i][0]; $name = $chunks[$i][1]; if (($count = floor($since / $seconds)) != 0) { break; } } $print = ($count == 1) ? '1 '.$name : ""$count {$name}s""; return $print; } The function takes the number of seconds as input and outputs text such as: 10 seconds 1 minute etc It only shows `33 minutes` no matter what I changing the date and time too @ErikEdgren I had same problem because I was sending date instead of seconds. Try this: `time_since(time() - strtotime($datetime))` Nice function :) Oh and don't forget to change those multiplications with the real values so that it won't be calculated every time it runs :) Since I was curious replacing the multiplication sequences with the evaluated products was ~1.2% faster.  function timeAgo($timestamp){ $datetime1=new DateTime(""now""); $datetime2=date_create($timestamp); $diff=date_diff($datetime1 $datetime2); $timemsg=''; if($diff->y > 0){ $timemsg = $diff->y .' year'. ($diff->y > 1?""'s"":''); } else if($diff->m > 0){ $timemsg = $diff->m . ' month'. ($diff->m > 1?""'s"":''); } else if($diff->d > 0){ $timemsg = $diff->d .' day'. ($diff->d > 1?""'s"":''); } else if($diff->h > 0){ $timemsg = $diff->h .' hour'.($diff->h > 1 ? ""'s"":''); } else if($diff->i > 0){ $timemsg = $diff->i .' minute'. ($diff->i > 1?""'s"":''); } else if($diff->s > 0){ $timemsg = $diff->s .' second'. ($diff->s > 1?""'s"":''); } $timemsg = $timemsg.' ago'; return $timemsg; }"200,A,"How to Autogenerate multiple getters/setters or accessors in Visual Studio Before I start I know there is this post and it doesn't answer my question: http://stackoverflow.com/questions/3017/how-to-generate-getters-and-setters-in-visual-studio In Visual Studio 2008 there is the ability to auto generate getters and setters (accessors) by right clicking on a private variable -> Refactor -> Encapsulate Field... This is great for a class that has 2 or 3 methods but come on MS! When have you ever worked with a class that has a few accessors? I am looking for a way to generate ALL with a few clicks (Eclipse folks out there will know what I am talking about - you can right click a class and select 'generate accessors'. DONE.). I really don't like spending 20 minutes a class clicking through wizards. I used to have some .NET 1.0 code that would generate classes but it is long gone and this feature should really be standard for the IDE. UPDATE: I might mention that I have found Linq to Entities and SQLMetal to be really cool ideas and way beyond my simple request in the paragraph above. If you have that many fields that it takes 20 minutes perhaps your class is trying to do too much Possibly a macro. There are also addins (like ReSharper which is great but commercial) capable of doing that quickly.  Sorry you really need to install Resharper to get approximately the same amount of refactoring support as you are used to in Eclipse. However Resharper gives you a dialog very similar to the one you are used to in Eclipse:  In 2008 I don't bother with Encapsulate Field. I use the new syntax for properties: public string SomeString { get; set; }  I have an ""info class generator"" application that you can use an excel sheet and it will generate the private members and the public get/set methods. You can download it for free from my website."