603114,A,Fonts for Carbon OpenGL app on OS X I'm trying to add text rendering to a Carbon OpenGL app I'm developing for OS X. Since the aglUseFont is now deprecated I'm looking for another way to add text as well as be able to query the glyph properties (i.e. width height spacing etc) So far I've investigated CoreText and ATSUI but both without much luck. Please help me!! Thanks! You could take a look at the FreeType project: it's an open source portable font rendering engine that supports OpenType TrueType Postscript Type 1 and other formats. There are several open source integrations of FreeType with OpenGL; see for example OGLFT. Or you could just roll your own: it's not hard to make FreeType generate bitmaps in some suitable pixel format and then pass these bitmaps to glTexImage2D.  In the end I just went with good old glBitmap for my fonts. Found an apple dev sample that created rendered each character and got its pertinent info (width height offset etc.) However if I get the time to do some more work on it later I plan on using the FreeType project as was suggested above. Thanks!,c++ osx opengl fonts carbon340185,A,"using GDAL/OGR api to read vector data (shapefile)--How? I am working on an application that involves some gis stuff. There would be some .shp files to be read and plotted onto an opengl screen. The current opengl screen is using the orthographic projection as set from glOrtho() and is already displaying a map using coordinates from a simple text file.. Now the map to be plotted is to be read from a shapefile. I have the following doubts: How to use the WGS84 projection of the .shp file(as read from the .prj file of the shapefileWKT format) into my existing glOrtho projection..is there any conversion that needs to be done? and how is it different from what the glOrtho() sets up?basically how to use this information? My application needs to be setup in such a way that i can know the exact lat/long of a point on the map.for eg. if i am hovering on X cityits correct lat/long could be fetched.I know that this can be done by using opensource utils/apis like GDAL/OGR but i am messed up as the documentation of these apis are not getting into my head. I tried to find some sample c++ progs but couldnt find one. I have already written my own logic to read the coordinates from a shapefile containing either points/polyline/polygon(using C-shapelib) and plotted over my opengl screen.I found a OGR sample code in doc to read a POINTS shapefile but none for POLYGON shapefile.And the problem is that this application has to be so dynamic that upon loading the shapefileit should correctly setup the projection of the opengl screen depending upon the projection of the .shp file being read..eg WGS84LCCEVEREST MODIFIED...etc. how to achieve this from OGR api? Kindly give your inputs on this problem.. I am really keen to make this work but im not getting the right start.. 1.Shapefile rendering is quite straight forward in OpenGL.You may require ""shapelib""a free shapefile parsing library in C(google for it).Use GL_POINTS for point shapefile GL_LINES for line shapefile and GL_LINE_LOOP for polygon shapefile.Set your bounding box coods to the Ortho. 2.What u read from .prj file is projection info.WGS84 gives u lat/long coods(Spherical). But ur display system is 2D(Rectangular).Sou need to convert 3D Spherical coods to 2D Rectangular coods(This is the meaning of Projection).Projection types are numerousdepending on the area of interest on the globe(remeber projection distorts area/shape/size of features).Projection types range from PolyconicModified EverestNADUTM etc. 3.If u simly need WGS84 then read bounding box coods of ur sh file and assign them to glOrtho.If u have any projection(eg:-UTM) then u convert ur bounding box coods into Projection coods and then assign the newly projected coods to glOrtho.For converting lat/long into any Projectionu may require projection libraries like ""Projlib"" or ""GeotransEngine"" and etc. For further clarifications u may contact me on dgplinux@ y a h o o . c o m  Please read the OGR API Tutorial where you can learn how to read vector data from sources like Shapefile. Next check the OGR Projections Tutorial where you can learn about how to use information about projection and spatial reference system read from OGR sources.  GDAL/OGR has everything you need to load a vector file then convert any coordinates. I understand your frustration with GDAL as the documentation is not the greatest. If you want a good intro to using the API look at gdalinfo.c and ogrinfo.cpp in the GDAL subversion tree. Source can be seen at https://svn.osgeo.org/gdal/trunk/gdal. If that does not help I have two basic examples that I use to parse vector info and do coordinate conversion. They are really bad but they may help get the point. Vector Loading Coordinate Conversion Finally if you are not familiar with GIS formats I would consider reading the ArcGIS introduction here under Guide Books/Map Projections. I can compete with experts despite no cartography training due to these guides. Another good source is Wikipedia. When in doubt just pick a UTM grid you want to stick with and use UTM as your coordinate system. It uses X (Easting) Y(Northing) and Z(Altitude). The only key is picking a single UTM Grid and making sure all coordinates use that as a reference. UTM is easy to test code with as there are a lot of guides online. You also can find conversion code using OGR/GDAL or other resources. Other projected coordinate systems are worthwhile and may be better but I would look at that to start with. Finally if all else fails take a look at the NGA GeoTrans. That is a great testing tool.",c++ linux opengl gdal1009150,A,"C++/OpenGL - Rotating a rectangle For my project i needed to rotate a rectangle. I thought that would be easy but i'm getting an unpredictable behavior when running it.. Here is the code:  glPushMatrix(); glRotatef(30.0f 0.0f 0.0f 1.0f); glTranslatef(vec_vehicle_position_.x vec_vehicle_position_.y 0); glEnable(GL_TEXTURE_2D); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glBegin(GL_QUADS); glTexCoord2f(0.0f 0.0f); glVertex2f(0 0); glTexCoord2f(1.0f 0.0f); glVertex2f(width_sprite_ 0); glTexCoord2f(1.0f 1.0f); glVertex2f(width_sprite_ height_sprite_); glTexCoord2f(0.0f 1.0f); glVertex2f(0 height_sprite_); glEnd(); glDisable(GL_BLEND); glDisable(GL_TEXTURE_2D); glPopMatrix(); The problem with that is that my rectangle is making a translation somewhere in the window while rotating. In other words the rectangle doesn't keep the position : vec_vehicle_position_.x and vec_vehicle_position_.y. What's the problem ? Thanks To elaborate on the previous answers. Transformations in OpenGL are performed via matrix multiplication. In your example you have: M_r_ - the rotation transform M_t_ - the translation transform v - a vertex and you had applied them in the order: M_r_ * M_t_ * v Using parentheses to clarify: ( M_r_ * ( M_t_ * v ) ) We see that the vertex is transformed by the closer matrix first which is in this case the translation. It can be a bit counter intuitive because this requires you to specify the transformations in the order opposite of that which you want them applied in. But if you think of how the transforms are placed on the matrix stack it should hopefully make a bit more sense (the stack is pre-multiplied together). Hence why in order to get your desired result you needed to specify the transforms in the opposite order. Indeed :) Thanks for the update !  Make sure the rotation is applied before the translation.  Inertiatic provided a very good response. From a code perspective your transformations will happen in the reverse order they appear. In other words transforms closer to the actual drawing code will be applied first. For example: glRotate(); glTranslate(); glScale(); drawMyThing(); Will first scale your thing then translate it then rotate it. You effectively need to ""read your code backwards"" to figure out which transforms are being applied in which order. Also keep in mind what the state of these transforms is when you push and pop the model-view stack.  You need to flip the order of your transformations: glRotatef(30.0f 0.0f 0.0f 1.0f); glTranslatef(vec_vehicle_position_.x vec_vehicle_position_.y 0); becomes glTranslatef(vec_vehicle_position_.x vec_vehicle_position_.y 0); glRotatef(30.0f 0.0f 0.0f 1.0f); Thanks a lot it works !",c++ opengl rotation1272120,A,What are the error messages for breaking the GLSL shader instruction limits? We're a small dev team working with some GLSL that may be too large for older graphics cards to compile. We want to display a sensible error message to the user (rather than just dump the info log or output a generic 'this shader didn't work' type of message) when this happens based on the type of error. The question is ATI and nVidia have different conventions for these error messages and the only way I've found to decide what type of error the shader had is to parse the error string generated by glGetShaderInfoLog. Given that is there a listing somewhere or does anyone know what the error output for both ATI and nVidia cards looks like? Or is there a better way to detect when the instruction limit has been exceeded? Even if you know what the error messages look like now nVidia and ATI are under no obligation to keep them the same in the next version(s) of their drivers. They basically can't be relied on for anything except debugging purposes. I would look and see if the vendor extensions might be able to provide you with more specific diagnostic information. http://petewarden.com/notes/archives/2005/06/fragment_progra_3.html did the trick.,c++ opengl glsl1218876,A,"Problem with using OpenGL's VBO I just tried to render the first redbook example ( the white Quad ) by using VBOs. It works fine with immediate mode and vertex arrays. But when using VBOs the screen stays black. I think i must have missed something important. init: unsigned int bufIds[2]; glGenBuffers( 2 bufIds ); GLfloat vertices[] = { 0.25 0.25 0.0 0.75 0.25 0.0 0.75 0.75 0.0 0.25 0.75 0.0 }; glBindBuffer( GL_ARRAY_BUFFER bufIds[0] ); glBufferData( GL_ARRAY_BUFFER 12 vertices GL_STATIC_DRAW ); glBindBuffer( GL_ARRAY_BUFFER 0 ); glClearColor( 0 0 0 1 ); glColor3f( 1 1 1 ); glOrtho( 0.0 1.0 0.0 1.0 -1.0 1.0 ); renderloop for VBO (not working): glClear( GL_COLOR_BUFFER_BIT ); glEnableClientState( GL_VERTEX_ARRAY ); glBindBuffer( GL_ARRAY_BUFFER bufIds[0] ); glVertexPointer( 3 GL_FLOAT 0 0 ); glDrawArrays( GL_QUADS 0 12 ); glBindBuffer( GL_ARRAY_BUFFER 0 ); glDisableClientState( GL_VERTEX_ARRAY ); renderloop for vertex arrays (working): glClear( GL_COLOR_BUFFER_BIT ); glEnableClientState( GL_VERTEX_ARRAY ); glBindBuffer( GL_ARRAY_BUFFER 0 ); glVertexPointer( 3 GL_FLOAT 0 vertices ); glDrawArrays( GL_QUADS 0 12 ); glDisableClientState( GL_VERTEX_ARRAY ); argh i just figured it out by trying to read back the contents of the buffer: i need to allocate the buffer with 12 * sizeof( GLfloat ) instead of only 12 glBufferData( GL_ARRAY_BUFFER 12 * sizeof( GLfloat ) vertices GL_STATIC_DRAW ); my read back code GLfloat vertices2[12]; glBindBuffer( GL_ARRAY_BUFFER bufIds[0] ); glGetBufferSubData ( GL_ARRAY_BUFFER 0 12 * sizeof( GLfloat ) vertices2 ); glBindBuffer( GL_ARRAY_BUFFER 0 ); for ( int i = 0; i < 4; i ++ ) { LOG_DEBUG << ""point "" << i << "": "" << vertices2[ i * 3 + 0 ] << "" / "" << vertices2[ i * 3 + 1 ] << "" / "" << vertices2[ i * 3 + 2 ]; } +1 Awesome! I had exactly the same problem thank you so much!",c++ opengl vbo640874,A,How do draw to a texture in OpenGL Now that my OpenGL application is getting larger and more complex I am noticing that it's also getting a little slow on very low-end systems such as Netbooks. In Java I am able to get around this by drawing to a BufferedImage then drawing that to the screen and updating the cached render every one in a while. How would I go about doing this in OpenGL with C++? I found a few guides but they seem to only work on newer hardware/specific Nvidia cards. Since the cached rendering operations will only be updated every once in a while i can sacrifice speed for compatability. glBegin(GL_QUADS); setColor(DARK_BLUE); glVertex2f(0 0); //TL glVertex2f(appWidth 0); //TR setColor(LIGHT_BLUE); glVertex2f(appWidth appHeight); //BR glVertex2f(0 appHeight); //BR glEnd(); This is something that I am especially concerned about. A gradient that takes up the entire screen is being re-drawn many times per second. How can I cache it to a texture then just draw that texture to increase performance? Also a trick I use in Java is to render it to a 1 X height texture then scale that to width x height to increase the performance and lower memory usage. Is there such a trick with openGL? I sincerely doubt drawing from a texture is less work than drawing a gradient. In drawing a gradient: Color is interpolated at every pixel In drawing a texture: Texture coordinate is interpolated at every pixel Color is still interpolated at every pixel Texture lookup for every pixel Multiply lookup color with current color Not that either of these are slow but drawing untextured polygons is pretty much as fast as it gets.  If you don't want to use Framebuffer Objects for compatibility reasons (but they are pretty widely available) you don't want to use the legacy (and non portable) Pbuffers either. That leaves you with the simple possibility of reading the contents of the framebuffer with glReadPixels and creating a new texture with that data using glTexImage2D. Let me add that I don't really think that in your case you are going to gain much. Drawing a texture onscreen requires at least texel access per pixel that's not really a huge saving if the alternative is just interpolating a color as you are doing now!  Consider using a display list rather than a texture. Texture reads (especially for large ones) are a good deal slower than 8 or 9 function calls. So would a display list that draws a full screen gradient be faster than just caching the result and drawing that over and over? It's something you'll want to benchmark yourself but yes I believe so. See Sorren's answer for the rationale.  Look into FBOs - framebuffer objects. It's an extension that lets you render to arbitrary rendertargets including textures. This extension should be available on most recent hardware. This is a fairly good primer on FBOs: OpenGL Frame Buffer Object 101 The whole point of the question is that he can't use FBOs because he wants the lowest possible hardware denominator.  Before doing any optimization you should make sure you fully understand the bottlenecks. You'll probably be surprised at the result.  Hey there thought I'd give you some insight in to this. There's essentially two ways to do it. Frame Buffer Objects (FBOs) for more modern hardware and the back buffer for a fall back. The article from one of the previous posters is a good article to follow on it and there's plent of tutorials on google for FBOs. In my 2d Engine (Phoenix) we decided we would go with just the back buffer method. Our class was fairly simple and you can view the header and source here: http://code.google.com/p/phoenixgl/source/browse/branches/0.3/libPhoenixGL/PhRenderTexture.h http://code.google.com/p/phoenixgl/source/browse/branches/0.3/libPhoenixGL/PhRenderTexture.cpp Hope that helps! links are dead: the branch 0.3 isn't anymore Here's a link to the equivalent file in the newer version of phoenix: https://github.com/jonparrott/PhoenixCore/blob/master/source/RenderTarget.h,c++ opengl graphics664698,A,"How to I draw pixels as a texture to a polygon in OpenGL? In C++ OpenGL I want to draw each pixel manually (as a texture I assume) on to a simple square primitive or indeed 2 polygons forming a square. I have no idea where to start or what phrases to look for. Am I looking for texture mapping or creating textures? Most examples are to load from a file but I dont want to do that. I've tried reading my OpenGL Programming Guide book but its just a maze as I'm quite new to OpenGL. Please help. In other words you want to draw a texture without perspective? After you see the right tutorials check my post over here -> http://stackoverflow.com/questions/503816/linux-fastest-way-to-draw/504440#504440 The geometry is to be rotated transformed etc. in a 3D sense. I'm not sure what you mean ""draw without perspective""... Thanks to Reed Copsey for pointing me towards glTexImage2D. Turns out this is very simple; just pass an array of GLubyte to the glTexImage2D function (as well as all the functions needed to bind the texture etc). Haven't tried this exact snippet of code but it should work fine. The array elements represent a serial version of the rows columns and channels. int pixelIndex = 0; GLubyte pixels[400]; for (int x = 0; x < 10; x++) { for (int y = 0; y < 10; x++) { for (int channel = 0; channel < 4; channel++) { // 0 = black 255 = white pixels[pixelIndex++] = 255; } } } glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA SIZE SIZE 0 GL_RGBA GL_UNSIGNED_BYTE pixels); I've read in the OpenGL book you can use a 2D array for monochrome images so I assume you could use a 3D array also.  If the square you are drawing to is 2 dimensional and not rotated you may be looking for glDrawPixels. It allows you to draw a rectangular region of pixels directly to the buffer without using any polygons.  If you are new then I would recommend starting from tutorial one on from NeHe.  Take a close look at glTexImage2D. This is the call that loads the image in OpenGL for a 2D Texture. glTexImage2D actually takes a raw pointer to the pixel data in the image. You can allocate memory yourself and set the image data directly (however you want) then call this function to pass the image to OpenGL. Once you've created the texture you can bind it to the state so that your triangles use that texture. There is nothing in OpenGL that directly requires images used as textures to be loaded from files. A good resource for textures can be found in this GameDev article.  What you are looking for is generally called ""render to texture."" http://developer.nvidia.com/object/gdc_oglrtt.html That tutorial is pretty old. I'd guess that some of those extensions aren't actually extensions in newer versions of OGL. Sorry I can't be more help. I don't work directly with graphics API's much any more and if I did I'd use D3D if I could (to avoid the extension soup and for other reasons I won't bore you with here.)  Do note that while glTexImage2D() takes a pointer to the pixels to load it does not care about any changes to that data after the call. That is the data is really loaded from the memory area you specify and OpenGL creates a copy of its own. This means that if the data changes you will need to either re-do the glTexImage2D() call or use something like glTexSubImage2D() to do a partial update. I would recommend testing and profiling to see if sub-texture updating is quick not sure how actual drivers optimize for that case.",c++ opengl textures1122203,A,"How do I get all the shader constants (uniforms) from a ID3DXEffect? I'm creating an effect using hr = D3DXCreateEffectFromFile( g_D3D_Device shaderPath.c_str() macros NULL 0 NULL &pEffect &pBufferErrors ); I would like to get all the uniforms that this shader is using. In OpenGL I used glGetActiveUniform and glGetUniformLocation to get constant's size type name etc. Is there a D3DX9 equivalent function? Thanks! D3DXHANDLE handle = m_pEffect->GetParameterByName( NULL ""Uniform Name"" ); if ( handle != NULL ) { D3DXPARAMETER_DESC desc; if ( SUCCEEDED( m_pEffect->GetParameterDesc( handle &desc ) ) ) { // You now have pretty much all the details about the parameter there are in ""desc"". } } You can also iterate through each parameter by doing the following: UINT index = 0; while( 1 ) { D3DXHANDLE handle = m_pEffect->GetParameter( NULL index ); if ( handle == NULL ) break; // Get parameter desc as above. index++; }",c++ opengl graphics 3d directx1824656,A,Hiding the Cursor I have a windows program with directx/opengl renderers and a custom mouse rendered as a quad. The program currently runs windowed. The problem is the standard windows mouse is overlaid ontop of my custom cursor. How do I hide it when its inside my window? Try ShowCursor(FALSE); when you init your window.,c++ windows opengl directx mouse1919251,A,Display image in opengl I am fairly new to openGL. I have a 3d game that I have running and it seems to go fairly well. What I would like to do is display an image straight onto the screen and I am not sure the easiest way to do that. My only idea is to draw a rectangle right in front of the screen and use the image as the texture. It seems like there should be an easier way. This is for menu screens and things so if there is a better way to do that as well please let me know. I would recommend setting up OpenGL for 2D rendering via gluOrtho2d(); then load the image into a texture and as you said draw it to the screen by creating a polygon and binding the texture to it. A good example can be found here.  You've got the basic idea. The other obvious alternative is to use glDrawPixels() but I think you'll find the texture method has much better performance. If you're feeling frisky you might also take a look at Pixel Buffer Objects. Good luck!,c++ opengl181731,A,Texture Sampling in Open GL i need to get the color at a particular coordinate from a texture. There are 2 ways i can do this by getting and looking at the raw png data or by sampling my generated opengl texture. Is it possible to sample an opengl texture to get the color (RGBA) at a given UV or XY coord? If so how? Off the top of my head your options are Fetch the entire texture using glGetTexImage() and check the texel you're interested in. Draw the texel you're interested in (eg. by rendering a GL_POINTS primitive) then grab the pixel where you rendered it from the framebuffer by using glReadPixels. Keep a copy of the texture image handy and leave OpenGL out of it. Options 1 and 2 are horribly inefficient (although you could speed 2 up somewhat by using pixel-buffer-objects and doing the copy asynchronously). So my favourite by FAR is option 3. Edit: If you have the GL_APPLE_client_storage extension (ie. you're on a Mac or iPhone) then that's option 4 which is the winner by a long way. Outside of option 2 you need a method for sampling based on the texcoords though which is the hard part -- this is described in my answer. My understanding was that the poster was asking how to implement the 'getTexel' part of your example not how to replicate the filtering (which depends on what kind of filtering he wants to replicate anyway). I was sort of asking about the getTexel part but he's right that 1 and 2 are too slow esp considering i'm working on a mobile device. Thanks :) Mobile device? Would that be an iPhone by any chance? See updated answer. indeed it would Mike F thanks for the tip... altho in this case it does turn out that the data can be pre processed as raw image data before the creation of the openGL texture.  The most efficient way I've found to do it is to access the texture data (you should have our PNG decoded to make into a texture anyway) and interpolate between the texels yourself. Assuming your texcoords are [01] multiply texwidth*u and texheight*v and then use that to find the position on the texture. If they're whole numbers just use the pixel directly otherwise use the int parts to find the bordering pixels and interpolate between them based on the fractional parts. Here's some HLSL-like psuedocode for it. Should be fairly clear: float3 sample(float2 coord texture tex) { float x = tex.w * coord.x; // Get X coord in texture int ix = (int) x; // Get X coord as whole number float y = tex.h * coord.y; int iy = (int) y; float3 x1 = getTexel(ix iy); // Get top-left pixel float3 x2 = getTexel(ix+1 iy); // Get top-right pixel float3 y1 = getTexel(ix iy+1); // Get bottom-left pixel float3 y2 = getTexel(ix+1 iy+1); // Get bottom-right pixel float3 top = interpolate(x1 x2 frac(x)); // Interpolate between top two pixels based on the fractional part of the X coord float3 bottom = interpolate(y1 y2 frac(x)); // Interpolate between bottom two pixels return interpolate(top bottom frac(y)); // Interpolate between top and bottom based on fractional Y coord }  As others have suggested reading back a texture from VRAM is horribly inefficient and should be avoided like the plague if you're even remotely interested in performance. Two workable solutions as far as I know: Keep a copy of the pixeldata handy (wastes memory though) Do it using a shader,c++ opengl680125,A,Can I use a grayscale image with the OpenGL glTexImage2D function? I have a texture which has only 1 channel as it's a grayscale image. When I pass the pixels in to glTexImage2D it comes out red (obviously because channel 1 is red; RGB). glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA dicomImage->GetColumns() dicomImage->GetRows() 0 GL_RGBA GL_UNSIGNED_BYTE pixelArrayPtr); Do I change GL_RGBA? If so what to? It appears that I should use GL_LUMINANCE instead of GL_RGBA for the 3rd argument. Edit (in reply to comments): When I set the 7th argument to GL_LUMINANCE (as well as the 3rd) the picture goes completely distorted. With the DICOM pixel format it appears that the 7th argument must be GL_RGBA for some reason. The strange behavior is because I'm using the DICOM standard. The particular DICOM reader I am using outputs integer pixel values (as pixel values may exceed the normal maximum of 255). For some strange reason the combination of telling OpenGL that I am using an RGBA format but passing in integer values rendered a perfect image. Because I was truncating the DICOM > 255 pixel values anyway it seemed logical to copy the values in to a GLbyte array. However after doing so a SIGSEGV (segmentation fault) occurred when calling glTexImage2D. Changing the 7th parameter to GL_LUMINANCE (as is normally required) returned the functionality to normal. Weird eh? So a note to all developers using the DICOM image format: You need to convert the integer array to a char array before passing it to glTexImage2D or just set the 7th argument to GL_RGBA (the later is probably not recommended). My case is unusual see my edit for explanation. It is argument #7 that counts here! Hmm when change #7 it goes red... but #3 has the desired effect. Can you explain this please? As ypnos says arguments 7 and 8 are what you're interested in. They tell OpenGL the format of the data which the pointer points to. The earlier arguments indicate the type of texture as I recall. Nick argument #3 is for internal storage #7 is for input format. GL will convert from #7 to #3. The effect you are getting can have several reasons I can't tell from here. In almost every case however you want both be the same anyway.  Change it to GL_LUMINANCE. See http://www.opengl.org/documentation/specs/man_pages/hardcopy/GL/html/gl/teximage2d.html It seems your link is broken. (It redirects to the home page). [glTextImage2D](http://www.opengl.org/sdk/docs/man/xhtml/glTexImage2D.xml) works but it doesn't mention GL_LUMINANCE anywhere. It seems they obsoleted it... Deprecated in OpenGL 4.0. But OpenGL 4.0 does not have a fixed function pipeline you need to bind your own shaders that's why there is no use in LUMINANCE ALPHA etc. any more. You need to use GL_RED and process the single channel accordingly.,c++ opengl textures1259461,A,"How to fix weird camera rotation while moving camera with sdl opengl in c++ I have a camera object that I have put together from reading on the net that handles moving forward and backward strafe left and right and even look around with the mouse. But when I move in any direction plus try to look around it jumps all over the place but when I don't move and look around its fine. I'm hoping someone can help me work out why I can move and look around at the same time? main.h #include ""SDL/SDL.h"" #include ""SDL/SDL_opengl.h"" #include <cmath> #define CAMERASPEED 0.03f // The Camera Speed struct tVector3 // Extended 3D Vector Struct { tVector3() {} // Struct Constructor tVector3 (float new_x float new_y float new_z) // Init Constructor { x = new_x; y = new_y; z = new_z; } // overload + operator tVector3 operator+(tVector3 vVector) {return tVector3(vVector.x+x vVector.y+y vVector.z+z);} // overload - operator tVector3 operator-(tVector3 vVector) {return tVector3(x-vVector.x y-vVector.y z-vVector.z);} // overload * operator tVector3 operator*(float number) {return tVector3(x*number y*number z*number);} // overload / operator tVector3 operator/(float number) {return tVector3(x/number y/number z/number);} float x y z; // 3D vector coordinates }; class CCamera { public: tVector3 mPos; tVector3 mView; tVector3 mUp; void Strafe_Camera(float speed); void Move_Camera(float speed); void Rotate_View(float speed); void Position_Camera(float pos_x float pos_yfloat pos_z float view_x float view_y float view_z float up_x float up_y float up_z); }; void Draw_Grid(); camera.cpp #include ""main.h"" void CCamera::Position_Camera(float pos_x float pos_y float pos_z float view_x float view_y float view_z float up_x float up_y float up_z) { mPos = tVector3(pos_x pos_y pos_z); mView = tVector3(view_x view_y view_z); mUp = tVector3(up_x up_y up_z); } void CCamera::Move_Camera(float speed) { tVector3 vVector = mView - mPos; mPos.x = mPos.x + vVector.x * speed; mPos.z = mPos.z + vVector.z * speed; mView.x = mView.x + vVector.x * speed; mView.z = mView.z + vVector.z * speed; } void CCamera::Strafe_Camera(float speed) { tVector3 vVector = mView - mPos; tVector3 vOrthoVector; vOrthoVector.x = -vVector.z; vOrthoVector.z = vVector.x; mPos.x = mPos.x + vOrthoVector.x * speed; mPos.z = mPos.z + vOrthoVector.z * speed; mView.x = mView.x + vOrthoVector.x * speed; mView.z = mView.z + vOrthoVector.z * speed; } void CCamera::Rotate_View(float speed) { tVector3 vVector = mView - mPos; tVector3 vOrthoVector; vOrthoVector.x = -vVector.z; vOrthoVector.z = vVector.x; mView.z = (float)(mPos.z + sin(speed)*vVector.x + cos(speed)*vVector.z); mView.x = (float)(mPos.x + cos(speed)*vVector.x - sin(speed)*vVector.z); } and the mousemotion code void processEvents() { int mid_x = screen_width >> 1; int mid_y = screen_height >> 1; int mpx = event.motion.x; int mpy = event.motion.y; float angle_y = 0.0f; float angle_z = 0.0f; while(SDL_PollEvent(&event)) { switch(event.type) { case SDL_MOUSEMOTION: if( (mpx == mid_x) && (mpy == mid_y) ) return; // Get the direction from the mouse cursor set a resonable maneuvering speed angle_y = (float)( (mid_x - mpx) ) / 1000; //1000 angle_z = (float)( (mid_y - mpy) ) / 1000; //1000 // The higher the value is the faster the camera looks around. objCamera.mView.y += angle_z * 2; // limit the rotation around the x-axis if((objCamera.mView.y - objCamera.mPos.y) > 8) objCamera.mView.y = objCamera.mPos.y + 8; if((objCamera.mView.y - objCamera.mPos.y) <-8) objCamera.mView.y = objCamera.mPos.y - 8; objCamera.Rotate_View(-angle_y); SDL_WarpMouse(mid_x mid_y); break; case SDL_KEYUP: objKeyb.handleKeyboardEvent(eventtrue); break; case SDL_KEYDOWN: objKeyb.handleKeyboardEvent(eventfalse); break; case SDL_QUIT: quit = true; break; case SDL_VIDEORESIZE: screen = SDL_SetVideoMode( event.resize.w event.resize.h screen_bpp SDL_OPENGL | SDL_HWSURFACE | SDL_RESIZABLE | SDL_GL_DOUBLEBUFFER | SDL_HWPALETTE ); screen_width = event.resize.w; screen_height = event.resize.h; init_opengl(); std::cout << ""Resized to width: "" << event.resize.w << "" height: "" << event.resize.h << std::endl; break; default: break; } } } does this happen only when you are looking and moving at the same time? yes individually they work together it goes weird. I agree with Goz. You need to use homegenous 4x4 matrices if you want to represent affine transformations such as rotate + translate Assuming row major representation then if there is no scaling or shearing your 4x4 matrix represents the following: Rows 0 to 2 : The three basis vectors of your local co-ordinate system ( i.e xyz ) Row 3 : the current translation from the origin So to move along your local x vector as Goz says because you can assume it's a unit vector if there is no scale/shear you just multiply it by the move step ( +ve or -ve ) then add the resultant vector onto Row 4 in the matrix So taking a simple example of starting at the origin with your local frame set to world frame then your matrix would look something like this 1 0 0 0 <--- x unit vector 0 1 0 0 <--- y unit vector 0 0 1 0 <--- z unit vector 0 0 0 1 <--- translation vector In terms of a way most game cameras work then the axes map like this: x axis <=> Camera Pan Left/Right y axis <=> Camera Pan Up/Down z axis <=> Camera Zoom In/Out So if I rotate my entire frame of reference to say look at a new point LookAt then as Goz puts in his BaseCamera overloaded constructor code you then construct a new local co-ordinate system and set this into your matrix ( all mCameraMat.Set( vLat vUp vDir vPos ) does typically is set those four rows of the matrix i.e VLat would be row 0 vUp row 1 vDir row 2 and vPos row 3 ) Then to zoom in/out would just become row 3 = row 2 * stepval Again as Goz rightly points out you then need to transform this back into world-space and this is done by multiplying by the inverse of the view matrix Do bear in mind that the above code is for a row major matrix. You will need to transpose the matrix for a column major matrix :) Yep absolutely true  I'm not entirely sure what you are doing above. Personally I would just allow a simple 4x4 matrix. Any implementation will do. To rotate you simply need to rotate using the change of mouse x and y as euler inputs for rotation around the y and x axes. There is lots of code available all over the internet that will do this for you. Some of those matrix libraries won't provide you with a ""MoveForward()"" function. If this is the case its ok moving forward is pretty easy. The third column (or row if you are using row major matrices) is your forward vector. Extract it. Normalise it (It really should be normalised anyway so this step may not be needed). Multiply it by how much you wish to move forward and then add it to the position (the 4th column/row). Now here is the odd part. A view matrix is a special type of matrix. The matrix above defines the view space. If you multiply your current model matrix by this matrix you will not get the answer you expect. Because you wish to transform it such that the camera is at the origin. As such you need to effectively undo the camera transformation to re-orient things to the view defined above. To do this you multiply your model matrix by the inverse of the view matrix. You now have an object defined in the correct view space. This is my very simple camera class. It does not handle the functionality you describe but hopefully will give you a few ideas on how to set up the class (Be warned I use row major ie DirectX style matrices). BaseCamera.h: #ifndef BASE_CAMERA_H_ #define BASE_CAMERA_H_ /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ #include ""Maths/Vector4.h"" #include ""Maths/Matrix4x4.h"" /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ class BaseCamera { protected: bool mDirty; MathsLib::Matrix4x4 mCameraMat; MathsLib::Matrix4x4 mViewMat; public: BaseCamera(); BaseCamera( const BaseCamera& camera ); BaseCamera( const MathsLib::Vector4& vPos const MathsLib::Vector4& vLookAt ); BaseCamera( const MathsLib::Matrix4x4& matCamera ); bool IsDirty() const; void SetDirty(); MathsLib::Matrix4x4& GetOrientationMatrix(); const MathsLib::Matrix4x4& GetOrientationMatrix() const; MathsLib::Matrix4x4& GetViewMatrix(); }; /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline MathsLib::Matrix4x4& BaseCamera::GetOrientationMatrix() { return mCameraMat; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline const MathsLib::Matrix4x4& BaseCamera::GetOrientationMatrix() const { return mCameraMat; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline bool BaseCamera::IsDirty() const { return mDirty; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline void BaseCamera::SetDirty() { mDirty = true; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ #endif BaseCamera.cpp: #include ""Render/stdafx.h"" #include ""BaseCamera.h"" /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera() : mDirty( true ) { } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera( const BaseCamera& camera ) : mDirty( camera.mDirty ) mCameraMat( camera.mCameraMat ) mViewMat( camera.mViewMat ) { } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera( const MathsLib::Vector4& vPos const MathsLib::Vector4& vLookAt ) : mDirty( true ) { MathsLib::Vector4 vDir = (vLookAt - vPos).Normalise(); MathsLib::Vector4 vLat = MathsLib::CrossProduct( MathsLib::Vector4( 0.0f 1.0f 0.0f ) vDir ).Normalise(); MathsLib::Vector4 vUp = MathsLib::CrossProduct( vDir vLat );//.Normalise(); mCameraMat.Set( vLat vUp vDir vPos ); } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera( const MathsLib::Matrix4x4& matCamera ) : mDirty( true ) mCameraMat( matCamera ) { } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ MathsLib::Matrix4x4& BaseCamera::GetViewMatrix() { if ( IsDirty() ) { mViewMat = mCameraMat.Inverse(); mDirty = false; } return mViewMat; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/",c++ linux opengl sdl1533380,A,c++ opengl converting model coordinates to world coordinates for collision detection (This is all in ortho mode origin is in the top left corner x is positive to the right y is positive down the y axis) I have a rectangle in world space which can have a rotation m_rotation (in degrees). I can work with the rectangle fine it rotates scales everything you could want it to do. The part that I am getting really confused on is calculating the rectangles world coordinates from its local coordinates. I've been trying to use the formula: x' = x*cos(t) - y*sin(t) y' = x*sin(t) + y*cos(t) where (x y) are the original points (x' y') are the rotated coordinates and t is the angle measured in radians from the x-axis. The rotation is counter-clockwise as written. -credits duffymo I tried implementing the formula like this: //GLfloat Ax = getLocalVertices()[BOTTOM_LEFT].x * cosf(DEG_TO_RAD( m_orientation )) - getLocalVertices()[BOTTOM_LEFT].y * sinf(DEG_TO_RAD( m_orientation )); //GLfloat Ay = getLocalVertices()[BOTTOM_LEFT].x * sinf(DEG_TO_RAD( m_orientation )) + getLocalVertices()[BOTTOM_LEFT].y * cosf(DEG_TO_RAD( m_orientation )); //Vector3D BL = Vector3D(AxAy0); I create a vector to the translated point store it in the rectangles world_vertice member variable. That's fine. However in my main draw loop I draw a line from (000) to the vector BL and it seems as if the line is going in a circle from the point on the rectangle (the rectangles bottom left corner) around the origin of the world coordinates. Basically as m_orientation gets bigger it draws a huge circle around the (000) world coordinate system origin. edit: when m_orientation = 360 it gets set back to 0. I feel like I am doing this part wrong: and t is the angle measured in radians from the x-axis. Possibly I am not supposed to use m_orientation (the rectangles rotation angle) in this formula? Thanks! edit: the reason I am doing this is for collision detection. I need to know where the coordinates of the rectangles (soon to be rigid bodies) lie in the world coordinate place for collision detection. what you do is rotation [ special linear transformation] of a vector with angle Q on 2d.It keeps vector length and change its direction around the origin. [linear transformation : additive L(m + n) = L(m) + L(n) where {m n} ‰âÂ vector  homogeneous L(k.m) = k.L(m) where m ‰âÂ vector and k ‰âÂ scalar ] So: You divide your vector into two pieces. Like m[1 0] + n[0 1] = your vector. Then as you see in the image rotation is made on these two pieces after that your vector take the form: m[cosQ sinQ] + n[-sinQ cosQ] = [m*cosQ - n*sinQ m*sinQ + n*cosQ] you can also look at Wiki Rotation If you try to obtain eye coordinates corresponding to your object coordinates you should multiply your object coordinates by model-view matrix in opengl. For M => model view matrix and transpose of [x y z w] is your object coordinates you do: M[x y z w]T = Eye Coordinate of [x y z w]T Thank you. I will work on this later tonight! :) OpenGL handles graphics not linear algebra operations. If he needs the transformed coordinates for collision detection that's a bad advice in my opinion. But +1 for a good explanation. If you use OpenGL to transform the mesh during the actual rendering remember to rotate about the Z axis.  This seems to be overcomplicating things somewhat: typically you would store an object's world position and orientation separately from its set of own local coordinates. Rotating the object is done in model space and therefore the position is unchanged. The world position of each coordinate is the same whether you do a rotation or not - add the world position to the local position to translate the local coordinates to world space. Any rotation occurs around a specific origin and the typical sin/cos formula presumes (00) is your origin. If the coordinate system in use doesn't currently have (00) as the origin you must translate it to one that does perform the rotation then transform back. Usually model space is defined so that (00) is the origin for the model making this step trivial.,c++ opengl matrix rotation trigonometry1733488,A,"Terrain minimap in OpenGL? So I have what is essentially a game... There is terrain in this game. I'd like to be able to create a top-down view minimap so that the ""player"" can see where they are going. I'm doing some shading etc on the terrain so I'd like that to show up in the minimap as well. It seems like I just need to create a second camera and somehow get that camera's display to show up in a specific box. I'm also thinking something like a mirror would work. I'm looking for approaches that I could take that would essentially give me the same view I currently have just top down... Does this seem feasible? Feel free to ask questions... Thanks! Well I don't have an answer to your specific question but it's common in games to render the world to an image using an orthogonal perspective from above and use that for the minimap. It would at least be less performance intensive than rendering it on the fly.  One way to do this is to create an FBO (frame buffer object) with a render buffer attached render your minimap to it and then bind the FBO to a texture. You can then map the texture to anything you'd like generally a quad. You can do this for all sorts of HUD objects. This also means that you don't have to redraw the contents of your HUD/menu objects as often as your main view; update the the associated buffer only as often as you require. You will often want to downsample (in the polygon count sense) the objects/scene you are rendering to the FBO for this case. The functions in the API you'll want to check into are: glGenFramebuffersEXT glBindFramebufferEXT glGenRenderbuffersEXT glBindRenderbufferEXT glRenderbufferStorageEXT glFrambufferRenderbufferEXT glFrambufferTexture2DEXT glGenerateMipmapEXT There is a write-up on using FBOs on gamedev.net. Another potential optimization is that if the contents of the minimap are static and you are simply moving a camera over this static view (truly just a map). You can render a portion of the map that is much larger than what you actually want to display to the player and fake a camera by adjusting the texture coordinates of the object it's mapped onto. This only works if your minimap is in orthographic projection. Would using multiple viewports also work? Can I render polygons to the frame buffer? Obviously I'm pretty noob to graphics programming haha. Yes you can draw to an FBO in exactly the same way that you draw normally in your window's frame buffer (the same as rendering to the back buffer before swapping with the front buffer). You can use different gluPerspective/gluLookAt's in each FBO. The benefit is that you don't have to redraw in the FBO if you don't need to for the current frame the FBO can be of any dimensions and it's very fast when used for textures as you don't need to use a glCopyTexImage. You can create many FBOs and switch between them with glBindFramebufferEXT (0 binds the window's framebuffer that your used to).",c++ opengl graphics662440,A,"Palette Animation in OpenGL I am making an old-school 2d game and I want to animate a specific color in my texture. Only ways I know are: opengl shaders. animating one color channel only. white texture under the color-animated texture. But I don't want to use shaders I want to make this game as simple as possible not many extra openGL functions etc.. And the color channel animating doesnt fit this because I need all color channels in my textures. Currently I am doing it with 2 textures: white texture under the other texture translated the specific pixel color into transparent then I change white texture color with glColor3f() function to what ever I want and I see the ""palet animation"" on that specific color. But that style sounds pretty hacky so I am wondering if there is some better trick to this? If you want your game to work on modern hardware the fragment shaders are pretty much your only option since paletted textures are deprecated and may not be available on newer hardware. The solution you have right now sounds reasonable especially since it will probably work everywhere.  How about just using paletted textures? There are extensions to do just that. If using extensions is out of question you can just make palette handling by your own. Just do your palette tricks there are lot of them and just write RGB texture using palette. Ofcourse this limits number of colors but thats whole point of using palette. Paint programs that are good for palette handling are nowadays rare. That's why I will not remove Deluxe Paint from my drive. ahh Deluxe Paint!  You may also use the stencil buffer to do something equivalent. Write into the stencil buffer at the positions where your animation should occur (= the positions that should have the animated color). Then render the box with the normal texture only on non-set positions of the stencil buffer using the corresponding stencil op. After that render the animated color into the resp. positions using different stencil op. I don't know if this really would be an improvement - depends also on in which way you have the data on your hands - but is probably less ""hackish"".  For a 2D game I think it's better to use ARB_vertex_program extension than GLSL. There are still OpenGL drivers out there that do not support GLSL but support ARB_fragment_program. Support for paletted textures is nowadays non-existent but pre-shader-age video cards may support it. See the links for statistics. The old ARB language is a low-level assembly language but you can use NVidia's high level Cg language to compile shaders to this format. Despite it being a NVidia technology it works for all GPUs. That said if you are programming the game for your own amusement it may not be worth the trouble.  While I am unfamiliar with the palette texture extension I still recommend using a fragment shader for this sort of effect. It is almost trivial to do a color-key replacement with a shader versus the other methods you mentioned and will be way faster than writing the palette functionality yourself. Here's an example GLSL fragment shader that would replace the color white in a texture for whatever color is passed in. uniform vec4 fvReplaceColor; uniform sampler2D baseMap; varying vec2 Texcoord; void main( void ) { vec4 fvBaseColor = texture2D( baseMap Texcoord); if(fvBaseColor == vec4(1.0 1.0 1.0 1.0)) fvBaseColor = fvReplaceColor; gl_FragColor = fvBaseColor; } Yes it does take a little bit extra to set up shader but but what it sounds like you are trying to do I feel it's the best approach.",c++ c opengl784445,A,"How could simply calling Pitch() and Yaw() cause the camera to eventually Roll()? I'm coding a basic OpenGL game and I've got some code that handles the mouse in terms of moving the camera. I'm using the following method: int windowWidth = 640; int windowHeight = 480; int oldMouseX = -1; int oldMouseY = -1; void mousePassiveHandler(int x int y) { int snapThreshold = 50; if (oldMouseX != -1 && oldMouseY != -1) { cam.yaw((x - oldMouseX)/10.0); cam.pitch((y - oldMouseY)/10.0); oldMouseX = x; oldMouseY = y; if ((fabs(x - (windowWidth / 2)) > snapThreshold) || (fabs(y - (windowHeight / 2)) > snapThreshold)) { oldMouseX = windowWidth / 2; oldMouseY = windowHeight / 2; glutWarpPointer(windowWidth / 2 windowHeight / 2); } } else { oldMouseX = windowWidth / 2; oldMouseY = windowHeight / 2; glutWarpPointer(windowWidth / 2 windowHeight / 2); } glutPostRedisplay(); } However after looking around in circles you'll find the camera starts to ""roll"" (rotate). Since I'm only calling Pitch and Yaw I don't see how this is possible. Here is the code I'm using for my Camera class: http://pastebin.com/m20d2b01e As far as I know my camera ""rolling"" shouldn't happen. It should simply pitch up and down or yaw left and right. NOT roll. What could be causing this? Well if you start off looking forward horizontal to the horizon pitch up 90 degrees then yaw left 90 degrees then pitch down 90 degrees you'd be looking in the same direction as you started but the horizon would be vertical (as if you'd rolled 90 degrees left). Edit: I think the problem is that yaw/pitch/roll would be appropriate if the camera is being treated like an airplane. What you probably want to do is treat it like a point in a sphere keeping track of where on the sphere you are pointing the camera. Instead of yaw/pitch use spherical coordinates keeping track of theta (latitude) and phi (longitude). They may sound similar but consider the extreme case where the camera is pointing directly up. With yaw/pitch you can still freely adjust the yaw and pitch from that straight-up direction. With theta/phi you could only adjust theta downward and no matter how much you adjusted phi decreasing theta would still give you a camera that is parallel to the horizon. This is how FPS camera's work (you can't look so far down that you're looking behind you). Edit 2: Looking at the camera code you linked to you want to be using the rotLati(float angle) and rotLongi(float angle) functions. If I start moving my mouse in tight circles on the screen you'll see a distinct and slow roll. It completely rolls all 360 degrees. It isn't inverting the view. +1 Beat me to it. If you're moving your mouse in tight circles you'll get small amounts of roll. That's just how the combination of movements work. well if you replace the 90's in my answer with 20's the answer would still be the same. moving in circles would do basically what i am describing. Nestor Kip just explained how pitching and yawing can be combined to get the net effect of a roll. His numbers are exaggerated to make the effect easier to see -- hold your hand in front of you and make those motions with it. You're getting smaller motions from your mouse which is why you have to move it around a lot more before you go 360 degrees but it's still the same issue.  You will probably need to use quaternions for composing rotations if you are not doing so already. This avoids the problem of gimbal lock which you can get when orienting a camera by rotation around the 3 axes. Here is how to convert between them. Use quaternions is the answer.  I believe this circumstance is called Gimbal Lock - there's an example of it with illustrations on the wikipedia page.  Mathematically the reason for this is that rotations in 3D space do not commute. What that means is that pitch() followed by yaw() is not the same as yaw() followed by pitch() but a consequence of that fact is that the three kinds of rotations are inextricably linked and you can't perform any two of them without getting some of the third. In other words any sequence of pitch()es and yaw()s will produce a noticeable roll() effect over time unless the second half of the sequence is the exact reverse of the first half. (There's a lot of fairly intricate math involved in this but the details aren't particularly relevant)  Congratulations -- you have discovered Lie group theory! Yes it's possible. The outcome of a series of transformations depends on the order in which they're executed. Doing a pitch followed by a yaw is not the same as a doing a yaw followed by a pitch. In fact in the limit of infinitesimally small yaws and pitches the difference amounts to a pure roll; the general case is a little more complicated. (Physicists call this the ""commutation relationships of the rotational group"".) If you're familiar with rotation matrices you can work it out quite easily. heh looks like we were thinking exactly the same thing ;-)  Pitch/yaw/roll are all relative to your vehicle's orientation. When you pitch up/down you change your axis of yaw. Likewise when you yaw you change your pitch axis. So it's possible to change your orientation in a way similar to a roll maneuver just by a combination and pitch & yaw maneuvers.",c++ opengl camera glut237899,A,"How to draw a filled envelop like a cone on OpenGL (using GLUT)? I am using freeglut for opengl rendering... I need to draw an envelop looking like a cone (2D) that has to be filled with some color and some transparency applied. Is the freeglut toolkit equipped with such an inbuilt functionality to draw filled geometries(or some trick)? or is there some other api that has an inbuilt support for filled up geometries.. Edit1: just to clarify the 2D cone thing... the envelop is the graphical interpretation of the coverage area of an aircraft during interception(of an enemy aircraft)...that resembles a sector of a circle..i should have mentioned sector instead.. and glutSolidCone doesnot help me as i want to draw a filled sector of a circle...which i have already done...what remains to do is to fill it with some color... how to fill geometries with color in opengl? Edit2: All the answers posted to this questions can work for my problem in a way.. But i would definitely would want to know a way how to fill a geometry with some color. Say if i want to draw an envelop which is a parabola...in that case there would be no default glut function to actually draw a filled parabola(or is there any?).. So to generalise this question...how to draw a custom geometry in some solid color? Edit3: The answer that mstrobl posted works for GL_TRIANGLES but for such a code: glBegin(GL_LINE_STRIP); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glEnd(); which draws a square...only a wired square is drawn...i need to fill it with blue color. anyway to do it? if i put some drawing commands for a closed curve..like a pie..and i need to fill it with a color is there a way to make it possible... i dont know how its possible for GL_TRIANGLES... but how to do it for any closed curve? just to clarify the 2D cone thing... the envelop is the graphical interpretation of the coverage area of an aircraft during interception(of an enemy aircraft)...that resembles a sector of a circle..i should have mentioned sector instead.. Edit yoru comment into question because it's where it belongs. edited my original question..as i am newbie to stackoverflow i thought that followups to orig questions should go into comments.. is this how stackoverflow works? if the original poster feels to add to the conversation..how he should proceed?through comments/answering to own question or editing ques Since you reclarified your question to ask for a pie: there's an easy way to draw that too using opengl primitives: You'd draw a solid sphere using gluSolidSphere(). However since you only want to draw part of it you just clip the unwanted parts away: void glClipPlane(GLenum plane const GLdouble * equation); With plane being GL_CLIPPLANE0 to GL_CLIPPLANEn and equation being a plane equation in normal form (a*x + b*y + c*z + d = 0 would mean equation would hold the values { a b c d }. Please note that those are doubles and not floats.  On the edit on colors: OpenGL is actually a state machine. This means that the current material and/or color position is used when drawing. Since you probably won't be using materials ignore that for now. You want colors. glColor3f(float r float g float b) // draw with r/g/b color and alpha of 1 glColor4f(float r float g float b float alpha) This will affect the colors of any vertices you draw of any geometry you render - be it glu's or your own - after the glColorXX call has been executed. If you draw a face with vertices and change the color inbetween the glVertex3f/glVertex2f calls the colors are interpolated. Try this: glBegin(GL_TRIANGLES); glColor3f(0.0 0.0 1.0); glVertex3f(-3.0 0.0 0.0); glColor3f(0.0 1.0 0.0); glVertex3f(0.0 3.0 0.0); glColor3f(1.0 0.0 0.0); glVertex3f(3.0 0.0 0.0); glEnd(); But I pointed at glColor4f already so I assume you want to set the colors on a per-vertex basis. And you want to render using display lists. Just like you can display lists of vertices you can also make them have a list of colors: all you need to do is enable the color lists and tell opengl where the list resides. Of course they need to have the same outfit as the vertex list (same order). If you had glEnableClientState(GL_VERTEX_ARRAY); glVertexPointer(3 GL_FLOAT 0 vertices_); glDisableClientState(GL_VERTEX_ARRAY); you should add colors this way. They need not be float; in fact you tell it what format it should be. For a color list with 1 byte per channel and 4 channels (R G B and A) use this: glEnableClientState(GL_VERTEX_ARRAY); glEnableClientState(GL_COLOR_ARRAY); glVertexPointer(3 GL_FLOAT 0 vertices_); glColorPointer(4 GL_UNSIGNED_BYTE 0 colors_); glDisableClientState(GL_COLOR_ARRAY); glDisableClientState(GL_VERTEX_ARRAY); EDIT: Forgot to add that you then have to tell OpenGL which elements to draw by calling glDrawElements.  On Edit3: The way I understand your question is that you want to have OpenGL draw borders and anything between them should be filled with colors. The idea you had was right but a line strip is just that - a strip of lines and it does not have any area. You can however have the lines connect to each other to define a polygon. That will fill out the area of the polygon on a per-vertex basis. Adapting your code: glBegin(GL_POLYGON); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glEnd(); Please note however that drawing a polygon this way has two limitations: The polygon must be convex. This is a slow operation. But I assume you just want to get the job done and this will do it. For the future you might consider just triangulating your polygon. Note that you don't need the multiple glColor3f() statements. OpenGL is a state machine and the state is retained. So one glColor3f()-statement will suffice. ThankYou mstroblthis is really what i wanted...ur solution made my day!  I'm not sure what you mean by ""an envelop"" but a cone is a primitive that glut has: glutSolidCone(radius height number_of_slices number_of_stacks) The easiest way to fill it with color is to draw it with color. Since you want to make it somewhat transparent you need an alpha value too: glColor4f(float red float green float blue float alpha) // rgb and alpha run from 0.0f to 1.0f; in the example here alpha of 1.0 will // mean no transparency 0.0 total transparency. Call before drawing. To render translucently blending has to be enabled. And you must set the blending function to use. What you want to do will probably be achieved with the following. If you want to learn more drop me a comment and I will look for some good pointers. But here goes your setup: glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); Call that before doing any drawing operations possibly at program initialization. :) I never remember those blending parameters.  I remember there was a subroutine for that. But it's neither too hard to do by yourself. But I don't understand the 2D -thing. Cone in 2D? Isn't it just a triangle? Anyway here's an algorithm to drawing a cone in opengl First take a circle subdivision it evenly so that you get a nice amount of edges. Now pick the center of the circle make triangles from the edges to the center of the circle. Then select a point over the circle and make triangles from the edges to that point. The size shape and orientation depends about the values you use to generate the circle and two points. Every step is rather simple and shouldn't cause trouble for you. First just subdivision a scalar value. Start from [0-2] -range. Take the midpoint ((start+end)/2) and split the range with it. Store the values as pairs. For instance subdividing once should give you: [(01) (12)] Do this recursively couple of times then calculate what those points are on the circle. Simple trigonometry just remember to multiply the values with ìÛ before proceeding. After this you have a certain amount of edges. 2^n where n is the amount of subdivisions. Then you can simply turn them into triangles by giving them one vertex point more. Amount of triangles ends up being therefore: 2^(n+1). (The amounts are useful to know if you are doing it with fixed size arrays. Edit: What you really want is a pie. (Sorry the pun) It's equally simple to render. You can again use just triangles. Just select scalar range [-0.25 - 0.25] subdivide project to circle and generate one set of triangles. The scalar - circle projection is simple as: x=cos(v*pi)r y=sin(vpi)*r where (xy) is the resulting vertex point r is a radius and trigonometric functions work on radiances not degrees. (if they work with degrees replace pi with 180) Use vertex buffers or lists to render it yourself. Edit: About the coloring question. glColor4f if you want some parts of the geometry to be different by its color you can assign a color for each vertex in vertex buffer itself. I don't right now know all the API calls to do it but API reference in opengl is quite understandable.",c++ opengl glut freeglut640095,A,"When using a GL_RGBA16F_ARB-texture it contains just crap but I get no error message I generate a texture like this: GLuint id; glGenTextures(1 &id); glBindTexture(GL_TEXTURE_2D id); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA16 //GL_RGBA16F_ARB //< Won't work 256 256 0 GL_RGBA GL_FLOAT NULL ); glBindTexture(GL_TEXTURE_2D 0); I attach it to a framebuffer object (FBO) for rendering to. All of this works like a charm when I set the internal format to GL_RGBA16. However I need a higher dynamic range and was thinking that GL_RGBA16F_ARB might do the trick. Unfortunately if I replace GL_RGBA16 with GL_RGBA16F_ARB in the code given above the texture seems to stop working. Nothing I try to render to the FBO/texture sticks and when I use the texture it contains random garbage. (A lot of purple as it turns out) This would not be so frustrating if I had gotten an error message hinting at what might be wrong but I can't seem to find one. In other words glGetError() returns 0 after the glTexImage2D-call and glCheckFramebufferStatusEXT(GL_FRAMEBUFFER_EXT) returns GL_FRAMEBUFFER_COMPLETE_EXT when I have attached the texture I haven't messed with glClampColorARB(...) ... yet :) Have I forgotten to check for errors in a place/way that I haven't thought of? Do GL_RGBA16F_ARB-textures require any special treatment that I haven't given? Is there anything else that might be wrong? I'm stumped since everything works smoothly with GL_RGBA16... :( EDIT: When using GL_RGBA16F_ARB the first frame I try to render to screen doesn't make it. Seems to me that I should be getting an error message somewhere..? EDIT: By inspecting ShadowIce's working code example I figured out that the problems disappeared if I changed the depth buffer on my FBO and give glRenderBufferStorageEXT(...) GL_DEPTH_COMPONENT24 as its second parameter rather than GL_DEPTH_COMPONENT16. I have no idea why this works but apparently it does work. Also ShadowIce's code breaks like mine if I do the opposite substitution there. How are you actually filling the texture? I am attaching it to an FBO and rendering to it. There shouldn't be anything special to do for setting up a framebuffer with float textures. Some things I would check: Is the FBO bound and the draw/read buffer set correctly before you call glCheckFramebufferStatusEXT? Also try testing it right before you draw to it. Does the texture look ok after a simple glClear with a specific clear color? If yes there might be something wrong with your shaders (if you use any) or the way you draw to the FBO. Are your drivers up to date? And does the problem still exist on a PC with different hardware? How about GL_RGBA32F_ARB? Edit: Check the id of your framebuffer and texture also check if the texture id matches the one attached to your fbo (with glGetFramebufferAttachmentParameteriv). Normally I would guess that everything is ok with that if it works with a RGBA texture but random data (especially purple) is a good sign that nothing was written to the texture or it wasn't cleared properly. I have written a small example application that should work maybe that helps. I have only tested it on windows so for linux you might need to change it a bit: link The example you made worked nicely. Now to isolate the differences... I ought to create sockpuppet-accounts so I could thank you properly ;) Hehe thanks. ;) 1. Yes. Verified by testing right before drawing 2. No it does not look OK 3. I get my drivers from the Ubuntu-repository. Should be reasonably up to date (nvidia). I get the same problem om a different machine 4. I get the same result sometimes but now it has apparently locked completely! (4. continued:) When breaking it in a debugger the stack I get to see is only one deep with ""libGL"" as the only element. The program is completely locked and uses 100% CPU :O Also: Thanks! If you have any more ideas I'm all ears :)  GL_HALF_FLOAT_ARB might work as the type instead of GL_FLOAT. Thanks. Unfortunately this gives the same result; It still works with GL_RGBA16 but fails with GL_RGBA*F_ARB :(",c++ c opengl fbo746034,A,glDrawPixels in grayscale? I have an image that I'm rendering like this: glDrawPixels(image->width image->height GL_BGR GL_UNSIGNED_BYTE image->imageData); Is there anyway I can draw it in grayscale instead (without loading it into a texture first)? I don't care if only say the blue component is used for the gray value rather than the L2 norm or something I just need a quick and dirty output. GL_LUMINANCE would be great except that it won't work on a 3-channel image. @timday: A perverse idea for you to try and I've no idea if it'll work: glPixelZoom(1.0f/3.0f1.0f); glDrawPixels(3*widthheightGL_LUMINANCEGL_UNSIGNED_BYTEdata); ie treat your 3-channel image as being a 1-channel (grayscale) image 3 times as wide and compensate for this by squishing the width using the x zoom factor. I believe GL always does nearest-neighbour sampling for zoomed glDrawPixels so it ought to consistently pick out the same component from each triple of samples as you require. Oh my God..... I can't believe that works! It sort of splits the image in half and one side grainier than the other but it works. Could be something to do with the precision with which 1.0/3.0 is maintained internally (especially if it works OK up to a certain size). I'd suggest adjusting the ratio very slightly or using a 4-bytes-per-pixel format and a zoom factor of 0.25.,c++ opengl1569829,A,Open Source C++ game engine math libraries? I'm looking for a free to use game engine math library. Specifically I'd like a good matrix and vector implementation. And everything needed to move objects in 3D space. Does anyone know any good ones? I'm targeting OpenGL. I'd like to write them myself but don't have the time. Besides Ogre 3D there's also Crystal Space. Here's an article that compares the two. That article appears to 404 on me. @darthcoder it looks like arcanoria.com did some reorganizing. I've updated the article's URL accordingly  I have good work with Open Dynamics Engine is Full and Stable Physics Engine the ode in a BSD License and have some functions for Matrix Manipulation Quaternion and rotations.  Take a look here https://sourceforge.net/projects/mg3d/ It is an opensource engine that has all former OpenGL transformation routines. The implementation here is very straightforward and clear. And it is very easy to include the module with the routines in your project.  If you want an entire 3D engine (which of course would contain the 3d maths you need) see Ogre 3D (LGPL) Actually MIT license now for current svn trunk and all coming releases.  I'd recommend OpenGL Mathematics (GLM) Though if you want physics with your math you could go with Bullet Physics Library Finally if you want an entire engine i'd go with OGRE I think GLM will work nicely... seems light weight enough and has what I need  You might want to consider Blitz++.,c++ opengl933020,A,Macro use depending on integer I have to use a macro multiple times inside a function and the macro that needs to be used depends on a number I pass into the function. e.g.  function(int number) { switch(number) { case 0: doStuff(MACRO0); break; case 1: doStuff(MACRO1); break; } } The problem is: I have a lot stuff to do per switch statement with the same macro. Is there are more elegant solution then having all that stuff in the switch statement? Like passing the macro itself to the function? I have read about eval() like methods in C++ but they just don't feel right to me. Another way could be do determine into what the macro expands but I haven't found any info on this. Oh it's openGL actually. This very much depends on what MACRO1 expands to. If it's a constant you can just call the function outside of the switch or do multiple fall-through cases. If it depends on the local context then you will have to evaluate it every time.  I would use a function object struct Method1 { void operator()() { ... } }; template<typename Method> void function(Method m) { ... m(); ... } int main() { function(Method1()); } Why not just use a function pointer? Because function objects using op() are faster and can be inlined. Given that he is doing OpenGL i think speed matters.  In addition to the above suggestions often I find using virtual inheritance can get rid of long conditionals especially switches-over-enums type code. I'm not sure what your particular situation so I'm not sure how applicable his will be but let's say the above was enum StyleID { STYLE0 = 0 STYLE1 STYLE2 /* ... */ }; void drawStyle(StyleID id) { switch(id) { case STYLE1: doDraw(MACROSTYLE1); break; /* ... */ }; } Long blocks of switches could be avoided via virtual inheritance: class StyleInterface { /* some methods */ virtual void doDraw() = 0; }; class Style1 : public StyleInterface { /* concrete impl */ virtual void doDraw() { doDraw(MACROSTYLE1); } }; void drawStyle(StyleInterface* id) { id->doDraw(); },c++ opengl macros366742,A,Creating an Environment Stack in OpenGL I'd like to create an abstraction in OpenGL of the environment settings(blending stenciling depth etc.) that works like the matrix stack. Push onto the stack make any changes you need draw your objects then pop the stack and go back to the prior settings. For example currently you might have drawing code like this: glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glDisable(GL_DEPTH_TEST); //Draw operations glEnable(GL_DEPTH_TEST); glDisable(GL_BLEND); But with an environment stack it would look like this: glPushEnv(); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glDisable(GL_DEPTH_TEST); //Draw operations glPopEnv(); As I see it there are only 2 ways to do this: Create my own 'flavor' of each environment setting function and call that. It will in turn update the current EnvStack data structure and call the OpenGL environment function. Alter the OpenGL environment functions to point to my environment functions which will again update the current EnvStack data structure and call the original OpenGL environment functions. So option 1 is obviously much simpler. But I run into a problem if I'm using other peoples code in that I don't necessarily know what changes it's making to the environement and therefor my data structure would be out of sync. And since the whole point is to have a simple method of ensuring the environment settings are correct this is not cool. So my question is in this context how do I change the functions that the OpenGL environment functions point to? OpenGL already contains this functionality. You want glPushAttrib(GL_ALL_ATTRIB_BITS); and glPopAttrib();. See http://opengl.org/documentation/specs/man_pages/hardcopy/GL/html/gl/pushattrib.html for more. Wow. I have never come across that. It seemed like something so useful they'd have to have a way to do it. Thanks.,c++ c opengl abstraction371665,A,"openGL into png I'm trying to convert an openGL [edit: ""card that I drew""(?):) thx unwind]containing a lot of textures (nothing moving) into one PNG file that I can use in another part of the framework I'm working with. Is there a C++ library that does that? thanks! What does ""card that I drew"" mean? it's a static constant scene that looks like a card... What is an ""OpenGL file""? OpenGL is a graphics API it doesn't specify any file formats. Do you mean a DDS file or something?  There are better ways to make a compose texture than drawing them with the graphics card. This is really something you would want to do before hand on the cpu store and then use as and when you need it with opengl Can you give me a clue on how I would do that?  If you simply mean ""take a scene rendered by OpenGL and save it as an image"" then it is fairly straightforward. You need to read the scene with glReadPixels() and then convert that data to an image format such as PNG (http://www.opengl.org/resources/faq/technical/miscellaneous.htm). There are also more efficient ways of achieving this such as using FBOs. Instead of rendering the scene directly into the framebuffer you can render it to a texture via an FBO then render that texture as a full-screen quad. You can then take this texture and save it to a file (using glGetTexImage for example).",c++ opengl png ldf52431,A,How do I draw text using OpenGL SDL and C++? I heard about SDL_TFF which I read about here but I don't understand how am I supposed to connect the TrueType2 library. Maybe there is something better out there? Here's a good answer if you decide to use ttf: http://stackoverflow.com/questions/5289447/using-sdl-ttf-with-opengl You can try to use FreeGLUT. If you like you can pull the text drawing files from the project.  I came across this great guide on linking in SDL extensions for those new to SDL which you may find useful. That said when I had your problem I eventually went with FTGL as the way SDL-ttf produces an SDL-Surface with its font rendered on it overcomplicated matters in my situation. This may not be the case in your situation though  For OpenGL things I usually use QT. This library is well documented and easy to use.  This article is about SDL and text output. Hope that helps.,c++ opengl sdl1719325,A,"FLTK in Cygwin using Eclipse (Linking errors) I have this assignment due that requires the usage of FLTK. The code is given to us and it should compile straight off of the bat but I am having linking errors and do not know which other libraries I need to include. I currently have ""opengl32"" ""fltk_gl"" ""glu32"" and ""fltk"" included (-l) each of which seem to reduce the number of errors. I compiled FLTK using make with no specified options. Including all of the produced library files doesn't fix the problem and I'm convinced that it's just some Windows specific problem. Compile log: **** Build of configuration Debug for project CG5 **** make all Building target: CG5.exe Invoking: Cygwin C++ Linker g++ -o""CG5.exe"" ./src/draw_routines.o ./src/gl_window.o ./src/my_shapes.o ./src/shape.o ./src/shapes_ui.o ./src/tesselation.o -lopengl32 -lfltk_z -lfltk_gl -lglu32 -lfltk /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x197): undefined reference to `_SelectPalette@12' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x1a7): undefined reference to `_RealizePalette@4' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x1fe): undefined reference to `_glDrawBuffer@4' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x20d): undefined reference to `_glReadBuffer@4' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x23a): undefined reference to `_glGetIntegerv@8' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x2c3): undefined reference to `_glOrtho@48' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x2f3): undefined reference to `_SwapBuffers@4' ...and lots more Thanks a ton for the help. EDIT: These first few lines are obviously OpenGL related although I'm still not sure what additional libraries need to be included. its probably not it but try changing the order so that -lglu32 is before the fltk libs. Also are you certain its: -lGLU32 -lOpenGL32 rather than -lGLU -lGL ? Sorry for the lack of closure but I just booted into my Linux netbook and got it working. -lfltk -lfltk_gl -lGLU -lGL -lXext -lX11 -lm  Just a guess: your makefile was written for Linux and on Cygwin some libraries are either missing or in a different place. You're going to have to examine the makefile locate the missing libraries and either move the libs to where the makefile expects them or change the makefile to look in the right place. The libraries it needs are listed on the line starting g++ (prepend 'lib' to the names after the -l flags)",c++ windows opengl linker-error fltk1167120,A,OpenGL Alpha blending with wrong color I am trying to create a simple ray tracer. I have a perspective view which shows the rays visibly for debugging purposes. In my example screenshot below I have a single white sphere to be raytraced and a green sphere representing the eye. Rays are drawn as lines with glLineWidth(10.0f) If a ray misses the sphere it is given color glColor4ub(100100100100); in my initialization code I have the following:  glEnable(GL_ALPHA_TEST); glAlphaFunc(GL_GREATER 0.0f); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHAGL_SRC_ALPHA); You can see in the screen shot that for some reason the rays passing between the perspective view point and the sphere are being color blended with the axis line behind the sphere rather than with the sphere itself. Here is a screenshot: Can anyone explain what I am doing wrong here? Thanks!! I edited to include your screenshot AlphaTest is only for discarding fragments - not for blending them. Check the spec By using it you are telling OpenGL that you want it to throw away the pixels instead of drawing them so you won't can any transparent blending. The most common blending function is glBlendFunc (GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); You can also check out the OpenGL Transparency FAQ.  You dont need the glAlphaFunc disable it. Light rays should be blended by adding to the buffer: glBlendFunc(GL_ONE GL_ONE) (for premultiplied alpha which you chose. Turn off depth buffer writing (not testing) when rendering the rays: glDepthMask(GL_FALSE) Render the rays last.  Is it a possibility you cast those rays before you draw the sphere? Then if Z-buffer is enabled the sphere's fragments simply won't be rendered as those parts of rays are closer. When you are drawing something semi-transparent (using blending) you should watch the order you draw things carefully. In fact I think you cannot use Z-buffer in any sensible way together with ray-tracing process. You'll have to track Z-order manually. While we are at it OpenGL might not be the best API to visualize ray-tracing process. (It will do so possibly much slower than pure software ray-tracer) @puddlesofjoy: any chance you could edit the question and add the result after this fix? It might be instructive for people to see what your intended result looked like. Not a biggie - just a suggestion. Glad this worked out! Well I'll be... just changing the order of my drawing code is all it took; I had no idea that could effect things so much!! Thank you sir! You are a gentleman and a scholar. Just to clear up my intent for the curious. The actual ray tracing is fully independent of the OpenGL drawing. I have just thrown in a 3d perspective view to help me debug while I get my ray tracer working. It just shows the rays and a copy of the raytraced image textured onto a quad in front of the raytracer eye point.,c++ qt opengl1191525,A,"I can't get the transparency in my images to work Stemming from this question of mine: http://stackoverflow.com/questions/1191093/im-seeing-artifacts-when-i-attempt-to-rotate-an-image In the source code there I am loading a TIF because I can't for the life of me get any other image format to load the transparency parts correctly. I've tried PNG GIF & TGA. I'd would like to be able to load PNGs. I hope the source code given in the question above will be enough if not then let me know. For a better description of what happens when I attempt to load some other format -- One of the images I was attempting was a 128*128 orange triangle. Depending on the format it would either make the entire 128*128 square orange or make the transparent parts of the image white. OK I'm new at OpenGL + SDL but here is what I have.. Loads all? formats SDL_image supports except I can't get .xcf to work and don't have a .lbm to test with. //called earlier.. glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); //load texture SDL_Surface* tex = IMG_Load(file.c_str()); if (tex == 0) { std::cout << ""Could not load "" << file << std::endl; return false; } glGenTextures(1 &texture); glBindTexture(GL_TEXTURE_2D texture); //nearest works but linear is best when scaled? glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); width = tex->w; height = tex->h; //IMG_is* doesn't seem to work right esp for TGA so use extension instead.. std::string ext = file.substr(file.length() - 4); bool isBMP = (ext.compare("".bmp"") == 0) || (ext.compare("".BMP"") == 0); bool isPNG = (ext.compare("".png"") == 0) || (ext.compare("".PNG"") == 0); bool isTGA = (ext.compare("".tga"") == 0) || (ext.compare("".TGA"") == 0); bool isTIF = ((ext.compare("".tif"") == 0) || (ext.compare("".TIF"") == 0) || (ext.compare(""tiff"") == 0) || (ext.compare(""TIFF"") == 0)); //default is RGBA but bmp and tga use BGR/A GLenum format = GL_RGBA; if(isBMP || isTGA) format = (tex->format->BytesPerPixel == 4 ? GL_BGRA : GL_BGR); //every image except png and bmp need to be converted if (!(isPNG || isBMP || isTGA || isTIF)) { SDL_Surface* fixedSurface = SDL_CreateRGBSurface(SDL_SWSURFACE width height 32 0x000000ff 0x0000ff00 0x00ff0000 0xff000000); SDL_BlitSurface(tex 0 fixedSurface 0); glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA width height 0 format GL_UNSIGNED_BYTE fixedSurface->pixels); SDL_FreeSurface(fixedSurface); } else { glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA width height 0 format GL_UNSIGNED_BYTE tex->pixels); } SDL_FreeSurface(tex); list = glGenLists(1); glNewList(list GL_COMPILE); GLint vertices[] = { 00 00 01 0height 11 widthheight 10 width0 }; glEnableClientState(GL_TEXTURE_COORD_ARRAY); glEnableClientState(GL_VERTEX_ARRAY); glBindTexture(GL_TEXTURE_2D texture); glTexCoordPointer(2 GL_INT 4*sizeof(GLint) &vertices[0]); glVertexPointer(2 GL_INT 4*sizeof(GLint) &vertices[2]); glDrawArrays(GL_POLYGON 0 4); glDisableClientState(GL_VERTEX_ARRAY); glDisableClientState(GL_TEXTURE_COORD_ARRAY); glEndList(); And then to draw I set the color to opaque white (doesn't affect transparency?) then just call the list.. glColor4f(1111); glCallList(list); And of course any help for my code would be much appreciated too! :) I used bits and pieces from your code and it finally worked. Turns out when I was loading the image ( I used my own function instead of IMG_Load() ) it was discarding the alpha layer as it was converting the BPPs. So thanks a lot for your code.  Make sure that you have alpha blending enabled with glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); otherwise primitives will draw solid colors where there should be transparency. You may need a different blendfunc. This is a common setup. I have that code in my previous question linked in the first post. Actually you have: glBlendFunc(GL_ONE GL_ONE_MINUS_SRC_ALPHA); // notice the GL_ONE instead of GL_SRC_ALPHA - this can make a difference depending on what you are seeing Sorry if you had the code already I wasn't able to scroll the code block on an iPhone. As Jim was saying the blendfunc that you have may not be correct. The blendfunc you're using is for blending a texture with pre-multiplied alpha. Ah well I tried your code then and now it just won't draw it at all :( Try to load in an image that has a known amount of transparency then look at the loaded buffer in the debugger to make sure that the alpha value contains the transparency you expect. If you don't see anything at all I suspect that the alpha could be 0.  I'm not familiar with SDL but since it's SDL that loading the image I would look closer at their docs. I use .png in my own work along with OpenGL and transparency works with no problem. (I use a .png parser called LightZPng.) Also I just noticed your linked post has: glBlendFunc(GL_ONE GL_ONE_MINUS_SRC_ALPHA); instead of: glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); This would have the affect of adding the pixels that should be transparent to whatever is in the background (assuming the alpha is 0 in those texels). Changing that makes the image not appear at all. Even with the TIF now Huh could it be that alpha is simply 0 for all your texels? I would look at your image data before passing it to glTexImage2D to confirm what the alpha values are for some texels. If they are simply RGB images (no alpha at all) then be sure to do a glColor4ub(255 255 255 255) before rendering to be sure your alpha is effectively set to fully opaque.",c++ gui opengl sdl1147249,A,Connecting Catmull-Rom splines together and calculating its length? I'm trying to create a class which takes in any number of points (position and control) and creates a catmull-rom spline based on the information given. What I'm doing - and I'm really unsure if this is the right way to do it - is storing each individual point in a class like so: class Point { public: Vector3 position; Vector3 control; } Where obviously position is the position of the point and control is the control point. My issue is connecting up the splines - obviously given the above class holding a point in the spline array indicates that any given position can only have one control point. So when having three or more points in a catmull rom spline the various individual catmull-rom splines which are being connected share one position and one control with another such spline. Now with position being the same is required - since I want to create splines which are continuous between themselves. However I really wonder should the control points also be the same between the two splines? With a bit of fiddling of the control points I can make it appear to be transitioning smoothly from one spline to another however I must emphasize that I'm not sure if the way they are transitioning is consistent with how catmull-rom splines form their shape. I'd much rather do it correctly than sit on my hands and rationalize that it's good enough. Obviously the second part of my question is self explanatory: Given two control and position points how do I calculate the length of a catmull-rom spline? With regards to measuring the lengths you can do this with calculus since its a polynomial spline. You need to do integrate the distance function across the line. Its described quite well on Wikipedia...  To define the spline between two control points the Catmull-Rom spline needs the control points and the tangent vector at each control point. However the tangent vector at internal (i.e. non-endpoint) control points is defined by the control points on either side of it: T(Pn) = (Pn+1 - Pn-1) / 2. For closed curves the spline is completely defined by the set of control points. For non-closed curves you need to also supply the tangent vector at the first and last control point. This is commonly done: T(P0) = P1 - P0 and T(Pn) = Pn - Pn-1. Thus for closed curves your data structure is just a list of control points. For general splines it's a list of points plus the first and last normal vector. If you want to have a cardinal spline then you can add a weighting factor to the tangent vector calculation as in the Wikipedia article. To calculate the length of such a spline one approach would be to approximate it by evaluating the spline at many points and then calculating the linear distance between each neighboring pair of points.,c++ opengl graphics directx spline23918,A,"OpenGL Rotation I'm trying to do a simple rotation in OpenGL but must be missing the point. I'm not looking for a specific fix so much as a quick explanation or link that explains OpenGL rotation more generally. At the moment I have code like this: glPushMatrix(); glRotatef(90.0 0.0 1.0 0.0); glBegin(GL_TRIANGLES); glVertex3f( 1.0 1.0 0.0 ); glVertex3f( 3.0 2.0 0.0 ); glVertex3f( 3.0 1.0 0.0 ); glEnd(); glPopMatrix(); But the result is not a triangle rotated 90 degrees. Edit Hmm thanks to Mike Haboustak - it appeared my code was calling a SetCamera function that use glOrtho. I'm too new to OpenGL to have any idea of what this meant but disabling this and rotating in the Z-axis produced the desired result. Thanks! The ""accepted answer"" is not fully correct - rotating around the Z will not help you see this triangle unless you've done some strange things prior to this code. Removing a glOrtho(...) call might have corrected the problem in this case but you still have a couple of other issues. Two major problems with the code as written: Have you positioned the camera previously? In OpenGL the camera is located at the origin looking down the Z axis with positive Y as up. In this case the triangle is being drawn in the same plane as your eye but up and to the right. Unless you have a very strange projection matrix you won't see it. gluLookat() is the easiest command to do this but any command that moves the current matrix (which should be MODELVIEW) can be made to work. You are drawing the triangle in a left handed or clockwise method whereas the default for OpenGL is a right handed or counterclockwise coordinate system. This means that if you are culling backfaces (which you are probably not but will likely move onto as you get more advanced) you would not see the triangle as expected. To see the problem put your right hand in front of your face and imagining it is in the X-Y plane move your fingers in the order you draw the vertices (11) to (32) to (31). When you do this your thumb is facing away from your face meaning you are looking at the back side of the triangle. You need to get into the habit of drawing faces in a right handed method since that is the common way it is done in OpenGL. The best thing I can recommend is to use the NeHe tutorials - http://nehe.gamedev.net/. They begin by showing you how to set up OpenGL in several systems move onto drawing triangles and continue slowly and surely to more advanced topics. They are very easy to follow.  Regarding Projection matrix you can find a good source to start with here: http://msdn.microsoft.com/en-us/library/bb147302(VS.85).aspx It explains a bit about how to construct one type of projection matrix. Orthographic projection is the very basic/primitive form of such a matrix and basically what is does is taking 2 of the 3 axes coordinates and project them to the screen (you can still flip axes and scale them but there is no warp or perspective effect). transformation of matrices is most likely one of the most important things when rendering in 3D and basically involves 3 matrix stages: Transform1 = Object coordinates system to World (for example - object rotation and scale) Transform2 = World coordinates system to Camera (placing the object in the right place) Transform3 = Camera coordinates system to Screen space (projecting to screen) Usually the 3 matrix multiplication result is referred to as the WorldViewProjection matrix (if you ever bump into this term) since it transforms the coordinates from Model space through World then to Camera and finally to the screen representation. Have fun  When I had a first look at OpenGL the NeHe tutorials (see the left menu) were invaluable.  Ensure that you're modifying the modelview matrix by putting the following before the glRotatef call: glMatrixMode(GL_MODELVIEW); Otherwise you may be modifying either the projection or a texture matrix instead.  Do you get a 1 unit straight line? It seems that 90deg rot. around Y is going to have you looking at the side of a triangle with no depth. You should try rotating around the Z axis instead and see if you get something that makes more sense. OpenGL has two matrices related to the display of geometry the ModelView and the Projection. Both are applied to coordinates before the data becomes visible on the screen. First the ModelView matrix is applied transforming the data from model space into view space. Then the Projection matrix is applied with transforms the data from view space for ""projection"" on your 2D monitor. ModelView is used to position multiple objects to their locations in the ""world"" Projection is used to position the objects onto the screen. Your code seems fine so I assume from reading the documentation you know what the nature of functions like glPushMatrix() is. If rotating around Z still doesn't make sense verify that you're editing the ModelView matrix by calling glMatrixMode. IMPORTANT! See also Perry's answer below.  I'd like to recommend a book: 3D Computer Graphics : A Mathematical Introduction with OpenGL by Samuel R. Buss It provides very clear explanations and the mathematics are widely applicable to non-graphics domains. You'll also find a thorough description of orthographic projections vs. perspective transformations.",c++ opengl glut721802,A,"What disadvantages could I have using OpenGL for GUI design in a desktop application? There are tons of GUI libraries for C/C++ but very few of them are based on the idea that opengl is a rather multiplatform graphics library. Is there any big disadvantage on using this OpenGL for building my own minimal GUI in a portable application? Blender is doing that and it seems that it works well for it. EDIT: The point of my question is not about using an external library or making my own. My main concern is about the use of libraries that use opengl as backend. Agar CEGUI or Blender's GUI for instance. Thanks. Here's an oddball one that bit a large physics experiment I worked on: because an OpenGL GUI bypasses some of the usual graphics abstraction layers it may defeat remote viewing applications. In the particular instance I'm thinking of we wanted to allow remote shift operations over VNC. Everything worked fine except for the one program (which we only needed about once per hour but we really needed) that used an OpenGL interface. We had to delay until a remote version of the OpenGL interface could be prepared. I was not involved in the solution but they arranged to display the OpenGL on the remote host. A little clunky but it meant I could sit shift in my PJs in my usual time-zone so I was happy. Yes this is the kind of answer I was looking for. Thanks dmckee! Good point. Generally anything that uses 3D acceleration will be invisible to apps like VNC. There are some ways to make it work but it causes a lot of performance issues and so isn't a great idea. I think to fix this you have to think an app as GUI-core-decoupled so you don't need VNC to control it. Actually there are several possibilities to use OpenGL enabled applications remotely such as for example xvnc/xf4vnc virtualgl or even just plain X11. The best solution depends on whether you want the OpenGL hardware acceleration to be done on the remote machine or locally on the viewer.  You're losing the native platforms capabilities for accessibility. For example on Windows most controls provide information to screen readers or other tools supporting accessibility impaired users. Basically unless you have a real reason to do this you shouldn't.  Re-inventing the Wheel: Yeah you'd be doing it. But I note OP used the word ""minimal"" in the problem statement so assuming it really doesn't need to scale up to all that it may be a small enough wheel as to not matter. The product I currently work on supports OpenGL on three platforms (Win Mac Linux) and we built all our own widgets (text boxes buttons dialogs). It's a lot of work but now that we've done it we own a huge chunk of our stack and don't have to debug into third party frameworks when things don't work as expected. It's nice having complete control of the experience. There's always something you want to do that a framework doesn't support. Like everything in our business it is a trade-off and you just have to weigh your needs against your need to finish on time. Portability: Yes you will still have to write platform specific code to boot-strap everything. This will be difficult if you've not done it before as it requires you to understand all the target platforms. Windows Drivers: We've found that graphics card manufacturers have much better support for DirectX on Windows than OpenGL since that's what is required to get MSFT certification. Often low- to mid-range cards have bugs missing functionality or outright crashes in their OpenGL support.  With Qt 4.5 you can select if you want to use OpenGL as window renderer. More info: http://blog.qt.digia.com/blog/2008/10/22/so-long-and-thanks-for-the-blit/ Read the comments to know about the problems about this. Your link is dead. Here's the new link: http://blog.qt.digia.com/blog/2008/10/22/so-long-and-thanks-for-the-blit/ link updated :)  The obvious one is that you're basically building the GUI elements yourself instead of having a nice designed like for wxWidgets or Qt.  You cant just use opengl you need a platform specific code to set up a window for opengl. From the freely available opengl red book: OpenGL is designed as a streamlined hardware-independent interface to be implemented on many different hardware platforms. To achieve these qualities no commands for performing windowing tasks or obtaining user input are included in OpenGL; instead you must work through whatever windowing system controls the particular hardware you're using. There are multiplatform solutions for this like glut qt wxWidgets ... And if you have to use them anyway why not use built in GUI elements. They also give you the opportunity to build your own and make use of the framework to handle mouse/keyboard events and stuff. It's very easy to make a multiplatform opengl program using preprococessor conditions and glut. It's far from using a huge library like qt for the whole GUI system. It very similar to make a multiplatform opengl program using qt: derive from QGLWidget; override initialiseGL paintGL resizeGL; assign your new class as mainwidget to a QApplication and thats it. The library is bigger yes. But i dont see the problem with that? Can you please tell me how they give you the opportunity to build your own GUI elements?  If you want a GUI as in windows/button etc. Don't do it yourself. There a lot of free solutions for this wxwidgetsqt or GTK. All have OpenGL support if you want a 3d window",c++ gui opengl1691538,A,"What 3D graphics framework should I use for a real world game engine? I'm a C++ programmer with very extensive server programming experience. I'm however fairly bored at the moment and I decided to tackle a new area: 3D game programming for learning purposes. Additionally I think this learning project may turn out to be good resume material in the future should I decide to work in this area. Instead of creating a 3D engine from scratch I decided to emulate as exactly as I'm able an existing one: World of Warcraft's. If you are curious as to why (feel free to skip this): It's a real world successful game All the map textures models and what not are already done (I'm not interested in learning how to actually draw a texture with photoshop or whatever) Their file formats have been more or less completely reverse engineered There is already an identical open source project (wowmapview) that I can look at if I'm in trouble. ok that was a long preface.. Now my main question is the following: Should I use DirectX OpenGL wrapper libraries such as sdl or what? What's the most used one in the real world? And something else that perplexes me: World of Warcraft appears to be using both! In fact normally it uses DirectX but you can use opengl by starting it with the ""-opengl"" switch via command line. Is this something common in games? Why did they do it? I imagine it's a lot of work and from my understanding nobody uses OpenGL anyway (very very few people know about the secret switch at all). If it's something usually done do programmers usually create their own 3d engine ""wrapper"" something like SDL made in house and based on switches / #defines / whatnot decide which API function to ultimately call (DirectX or OpenGL)? Or is this functionality already built in in sdl (you can switch between DirectX and OpenGL at will)? And finally do you have any books to suggest? Thanks! ""Why did they do it?"" Because Mac OsX doesn't have DirectX support? SDL is merely a platform abstraction layer it substitutes neither OpenGL nor DirectX. When SDL is used though it is more commonly used with OpenGL since both cross platform libraries. Also worth noting is that SDL doesn't provide a widget toolkit so if that's a requirement (not very common in fullscreen 3D games) something like Qt might be a better choice for a cross platform application. @gf: good point. Makes me wonder now why they don't support linux natively since they went through all the trouble to make OpenGL work for mac.. should only need relatively trivial changes. oh well guess I'll never find out. Blizzard do or at least did at some point have an internal unsupported build for Linux but it's never been released. I'm pretty sure they're using SDL too since Slouken WoW's lead interface programmer is also the author of SDL... take a look at this as well http://gamedev.stackexchange.com/questions/90/why-is-it-so-hard-to-develop-a-mmo this is my favorite post when I think I am boring and decide to do some game for fun especially WOW like . ^_^ You might want to look at some projects that encapsulate the low level 3d api in a higher level interface that is api independent such as Ogre3D. As you are doing this to learn I assume you probably will be more interesting in implementing the low level detail yourself but you could probably learn a lot from such a project. Or irrlicht or openscenegraph Indeed all good.  if you are really only interested in the rendering part i can suggest ogre3d. it is clean c++ tested and cross-platform. i used it in two projects and compared to other experiences (torque3d for example) i liked the good support (tutorials/wiki/forum) and the not so steep learning curve. i think someone can also learn a lot by looking at the sourcecode and the concepts they have used in the design. there is a accompanying book as well which is a bit outdated but it is good for the start the problem with this is you will be thinking inside this engine and soon you will need gameplay-like (timers events) elements for simulating and testing your effects or whatever you want to do. so you will end up working around ogre3ds shortcomings (it is not a game engine) and implement in on your own or use another middleware. so if you really want to touch 3d rendering first i would take some computer graphics books (gpu gems shaderx) and see some tutorials and demos on the web and start building my own basic framework. this is for the experience and i think you will learn the most from this approach. at least i did ...  If you just want it to work on Windows then DirectX is a good choice. Your first decision will be to look at what is available for both OpenGL and DirectX and decide which will give you more of what you want with less work. If you have an old version of DirectX and don't want to upgrade you may want to use OpenGL but the main reason for the switch is for support on non-windows machines. Since you are using C++ you can pick either option. If you use a library or wrapper it will simplify your life but then you will need to design within the limits of the libraries. You may want to use Lua for much of the top-level parts of your application as that is what WoW is using and then where you need you can go lower-level to limit how much low-level work you need to do. I'm already familiar with Lua and I've used it extensively in the non performance critical parts of my server projects I love it! However I wanted to focus on the actual 3d and rendering engine not the interface so I don't think I will be using it for this project. Then just first decide what requirements you have such as OS and peripheral support as support will differ between OpenGL and DirectX. I decided I'll go with DirectX (at least at first) thanks! Nice choice it comes with so much when you have just installed it. Try to avoid making shapes with individual triangles fans will be your friend. :)  I realize you already accepted an answer but I think this deserves some more comments. Sorry to quote you out of order I'm answering by what I think is important. Instead of creating a 3D engine from scratch I decided to emulate as exactly as I'm able an existing one: World of Warcraft's. However I wanted to focus on the actual 3d and rendering engine not the interface so I don't think I will be using it [lua] for this project. From these two snippets I can tell you that you are not trying to emulate the game engine. Just the 3D rendering backend. It's not the same thing and the rendering backend part is very small part compared to the full game engine. This by the way can help answer one of your questions: World of Warcraft appears to be using both! In fact normally it uses DirectX but you can use opengl by starting it with the ""-opengl"" switch via command line. Yep they implemented both. The amount of work to do that is non-negligeable but the rendering back-end in my experience is at most 10% of the total code usually less. So it's not that outraging to implement multiple ones. More to the point the programming part of a game engine today is not the biggest chunk. It's the asset production that is the bulk (and that includes most game programming. Most lua scripts are considered on that side of things e.g.) For WoW OSX support meant OpenGL. So they did it. They wanted to support older hardware too... So they support DX8-level hardware. That's already 3 backends. I'm not privy to how many they actually implement but it all boils down to what customer base they wanted to reach. Multiple back-ends in a game engine is something that is more or less required to scale to all graphics cards/OSs/platforms. I have not seen a single real game engine that did not support multiple backends (even first party titles tend to support an alternate back-end for debugging purposes). ok that was a long preface.. Now my main question is the following: Should I use DirectX OpenGL wrapper libraries such as sdl or what? Well this depends strongly on what you want to get out of it. I might add that your option list is not quite complete: DirectX9 DirectX10 DirectX11 OpenGL < 3.1 (before deprecated API is removed) OpenGL >= 3.1 OpenGL ES 1.1 (only if you need to. It's not programmable) OpenGL ES 2.0 Yep those APIs are different enough that you need to decide which ones you want to handle. If you want to learn the very basics of 3D rendering any of those can work. OpenGL < 3.1 tends to hide a lot of things that ultimately has to happen in user code for the other ones (e.g. Matrix manipulation see this plug). The DX SDKs do come with a lot of samples that help understand the basic concepts but they also tend to use the latest and greatest features of DX when it's not necessarily required when starting (e.g. using Geometry shader to render sprites...) On the other hand most GL tutorials tend to use features that are essentially non-performant on modern hardware (e.g. glBegin/glEnd selection/picking ... see the list of things that got removed from GL 3.1 or this other plug) and tend to seed the wrong concepts for a lot of things. What's the most used one in the real world? For games DirectX9 is the standard today in PC world. By a far margin. However I'm expecting DirectX11 to grab more market share as it allows for some more multithreaded work. It's unfortunately significantly more complicated than DX9. nobody uses OpenGL anyway (very very few people know about the secret switch at all). Ask the Mac owners what they think. Side question do you really think hardware vendors would spend any energy in OpenGL drivers if this was really the case (I realize I generalize your comment sorry)? there are real world usages of it. Not much in games though. And Apple makes OpenGL more relevant through the iphone (well OpenGL ES really). If it's something usually done do programmers usually create their own 3d engine ""wrapper"" It's usually a full part of the engine design. Mind you it's not abstracting the API at the same level it's usually more at a ""draw this with all its bells and whistles over there"". Which rendering algorithm to apply on that draw tends to be back-end specific. This however is very game engine dependent. If you want to understand better you could look at UE3 it just got released free (beer) for non-commercial use (I have not looked at it yet so I don't know if they exposed the backends but it's worth a look). To get back to my comment that game engine does not just mean 3D look at this. +1 for a very thorough and thoughtful answer! Great answer marked this one as accepted. Thanks ! wow I did not even know you could change the accepted answer after the fact.  I think the primary benefit of using OpenGL over DirectX is the portability. DirectX only runs on windows. However this is often not a problem (many games only run on Windows). DirectX also provides other libraries which are useful for games which are unrelated to graphics such as sound and input. I believe there are equivalents which are often used with OpenGL but I don't think they're actually part of OpenGL itself. If you're going to be locking into windows with DirectX and you are willing to/interested in learning C# and managed code I have found XNA to be and extremely easy platform to learn. It allows you to learn most of the concepts without dealing with a lot of the really tricky details of DirectX (or OpenGL). You can still use shader code and have the full power of DirectX but in a much friendlier environment. It would be my recomendation but again you'd have to switch to C# (mind you you can also put that on you're resume). DirectX no longer supports C#: http://social.msdn.microsoft.com/Forums/en-US/gametechnologiesdirectx101/thread/e9f458e0-65c6-4936-9af4-4276fc92f111. The last version I used with support was 9.0c Huge +1 for XNA given the askers background and future applicability of the technology True you cannot program DirectX directly with C# however XNA is a C# library which uses DirectX underneath. It is not the same as DirectX and is very much in use today version 3.0 just came out recently.  I'm doing some OpenGL work right now (on both Windows and Mac). Compared to my midnight game programming using the Unity3D engine usingOpenGL is a bit like having to chop down your own trees to make a house versus buying the materials. Unity3D runs on everything (Mac PC and iPhone Web player etc) and lets you concentrate on the what makes a game a game. To top it off it's faster than anything I could write. You code for it in C# Java or Boo. I just used Unity to mock up a demo for a client who wants something made in OpenGL so it has it's real world uses also. -Chris PS: The Unity Indie version recently became free. A little correction: You can program in Javascript not Java! :-)",c++ opengl directx sdl944665,A,"Designing a Qt + OpenGL application in Eclipse I'm starting a C++ project using OpenGL and Qt for the UI in eclipse. I would like to create a UI where a portion of the window contains a frame for OpenGL rendering and the rest would contain other Qt widgets such as buttons and so on. I haven't used Qt or the GUI editor in eclipse before and I'm wondering what the best approach would be? Should I create the UI by hand coding or would it be easier to use eclipse's GUI designer - I had a quick look at this and there doesn't seem to be an OpenGL widget built in. Thanks any particular aversion to QtCreator? This might be handy http://doc.trolltech.com/4.5/qtopengl.html If you are using Qt Designer (which I think is available via Eclipse Integration) you can place a base QWidget in the layout and then ""promote"" that widget to a QGLWidget. To do this: Add the QWidget to the desired place in the layout Right-click on the widget Select ""Promote To"" Enter QGLWidget as the class name and as the header Hit Add Select the QGLWidget from the list of promoted widgets at the top of the dialog Hit Promote This way you don't have to go through the placeholder route and create an additional layer. Thanks. That was just what I was after :-)  I had the same problem while using Qt Designer. I used a simple frame as a placeholder for the OpenGL widget then in main window constructor I created OpenGL widget manually and assigned it to the placeholder frame (as a child). The main advantage here is that you see where the OpenGL widget should be while designing your interface. The main disadvantage is that some coding is still required to set up the GUI.  Why won't you use Qt Eclipse Integration? It works flawlessly enables you to edit UIs directly from Eclipse. i am using that...",c++ eclipse gui qt opengl538810,A,How do I get the current mouse position in C++ / OpenGL? I know that I can use a Mouse callback function for when a user clicks the mouse but what if I want to know the current x/y position without the user clicking? Will I have to use a different callback that gets called on any mouse movement and keep track of the x/y myself or is there a function I can call within GLUT/OpenGL to get it? This is more related to glut than OpenGL. Things like mouse position are usually handled by the OS and have little to do with either OpenGL or C++. Register a glutPassiveMotionFunc callback function See info about callbacks @crashmstr Now? It is working now.  You need to use the glutMotionFunc/glutPassiveMotionFunc callback to track mouse movement independent of mouse clicks. 7.6 glutMotionFunc glutPassiveMotionFunc,c++ opengl mouse glut679210,A,How can I use a dynamically sized texture array with glTexImage2D? Currently I'm able to load in a static sized texture which I have created. In this case it's 512 x 512. This code is from the header: #define TEXTURE_WIDTH 512 #define TEXTURE_HEIGHT 512 GLubyte textureArray[TEXTURE_HEIGHT][TEXTURE_WIDTH][4]; Here's the usage of glTexImage2D: glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA TEXTURE_WIDTH TEXTURE_HEIGHT 0 GL_RGBA GL_UNSIGNED_BYTE textureArray); And here's how I'm populating the array (rough example not exact copy from my code): for (int i = 0; i < getTexturePixelCount(); i++) { textureArray[column][row][0] = (GLubyte)pixelValue1; textureArray[column][row][1] = (GLubyte)pixelValue2; textureArray[column][row][2] = (GLubyte)pixelValue3; textureArray[column][row][3] = (GLubyte)pixelValue4; } How do I change that so that there's no need for TEXTURE_WIDTH and TEXTURE_HEIGHT? Perhaps I could use a pointer style array and dynamically allocate the memory... Edit: I think I see the problem in C++ it can't really be done. The work around as pointed out by Budric is to use a single dimensional array but use all 3 dimensions multiplied to represent what would be the indexes: GLbyte *array = new GLbyte[xMax * yMax * zMax]; And to access for example x/y/z of 1/2/3 you'd need to do: GLbyte byte = array[1 * 2 * 3]; However the problem is I don't think the glTexImage2D function supports this. Can anyone think of a workaround that would work with this OpenGL function? Edit 2: Attention OpenGL developers this can be overcome by using a single dimensional array of pixels... [0]: column 0 > [1]: row 0 > [2]: channel 0 ... n > [n]: row 1 ... n > [n]: column 1 .. n ... no need to use a 3 dimensional array. In this case I've had to use this work around as 3 dimensional arrays are apparently not strictly possible in C++. You don't simply multiply the indices. It's (rowIndex * numColumns * numColourComponents + columnIndex + colourComponent); Oops. Should be (rowIndex * numColumns * numColourComponents + columnIndex*numColourComponents + colourComponent); You could always wrap it up in a class. If you are loading the image from a file you get the height and width out with the rest of the data (how else could you use the file?) you could store them in a class that wraps the file loading instead of using preprocessor defines. Something like: class ImageLoader { ... ImageLoader(const char* filename ...); ... int GetHeight(); int GetWidth(); void* GetDataPointer(); ... }; Even better you could hide the function calls to glTexImage2d in there with it. class GLImageLoader { ... ImageLoader(const char* filename ...); ... GLuint LoadToTexture2D(); // returns texture id ... };  Ok since this took me ages to figure this out here it is: My task was to implement the example from the OpenGL Red Book (9-1 p373 5th Ed.) with a dynamic texture array. The example uses: static GLubyte checkImage[checkImageHeight][checkImageWidth][4]; Trying to allocate a 3-dimensional array as you would guess won't do the job. Someth. like this does NOT work: GLubyte***checkImage; checkImage = new GLubyte**[HEIGHT]; for (int i = 0; i < HEIGHT; ++i) { checkImage[i] = new GLubyte*[WIDTH]; for (int j = 0; j < WIDTH; ++j) checkImage[i][j] = new GLubyte[DEPTH]; } You have to use a one dimensional array: unsigned int depth = 4; GLubyte *checkImage = new GLubyte[height * width * depth]; You can access the elements using this loops: for(unsigned int ix = 0; ix < height; ++ix) { for(unsigned int iy = 0; iy < width; ++iy) { int c = (((ix&0x8) == 0) ^ ((iy&0x8)) == 0) * 255; checkImage[ix * width * depth + iy * depth + 0] = c; //red checkImage[ix * width * depth + iy * depth + 1] = c; //green checkImage[ix * width * depth + iy * depth + 2] = c; //blue checkImage[ix * width * depth + iy * depth + 3] = 255; //alpha } } Don't forget to delete it properly: delete [] checkImage; Hope this helps... +1 for putting so much effort in to a 3 year old question. Helped me out too ! SO take my vote...This is one of those little intricacies of OpenGL  You can use int width = 1024; int height = 1024; GLubyte * texture = new GLubyte[4*width*height]; ... glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA width height 0 GL_RGBA GL_UNSIGNED_BYTE textureArray); delete [] texture; //remove the un-needed local copy of the texture; However you still need to specify the width and height to OpenGL in glTexImage2D call. This call copies texture data and that data is managed by OpenGL. You can delete resize change your original texture array all you want and it won't make a different to the texture you specified to OpenGL. Edit: C/C++ deals with only 1 dimensional arrays. The fact that you can do texture[a][b] is hidden and converted by the compiler at compile time. The compiler must know the number of columns and will do texture[a*cols + b]. Use a class to hide the allocation access to the texture. For academic purposes if you really want dynamic multi dimensional arrays the following should work: int rows = 16 cols = 16; char * storage = new char[rows * cols]; char ** accessor2D = new char *[rows]; for (int i = 0; i < rows; i++) { accessor2D[i] = storage + i*cols; } accessor2D[5][5] = 2; assert(storage[5*cols + 5] == accessor2D[5][5]); delete [] accessor2D; delete [] storage; Notice that in all the cases I'm using 1D arrays. They are just arrays of pointers and array of pointers to pointers. There's memory overhead to this. Also this is done for 2D array without colour components. For 3D dereferencing this gets really messy. Don't use this in your code. @Budric my array needs to have 3 dimensions not 2. I'm accessing it in the style of texture[column][row][channel] Hmm this would work only when assigning values to the array I get this error: invalid types ‰Û÷unsigned char[int]‰Ûª for array subscript Are you using texture[i][j]? You can't use that with the code above. texture[row * width + column].,c++ opengl glteximage2d1569939,A,Rendering different triangle types and triangle fans using vertex buffer objects? (OpenGL) About half of my meshes are using triangles another half using triangle fans. I'd like to offload these into a vertex buffer object but I'm not quite sure how to do this. The triangle fans all have a different number of vertices... for example one might have 5 and another 7. VBO's are fairly straight forward using plain triangles however I'm not sure how to use them with triangle fans or with different triangle types. I'm fairly sure I need an index buffer but I'm not quite sure what I need to do this. I know how many vertices make up each fan during run time... I'm thinking I can use that to call something like glArrayElement Any help here would be much appreciated! VBOs and index buffers are an orthogonal things. If you're not using index buffers yet maybe it is wiser to move one step at a time. So... regarding your question. If you put all your triangle fans in a vbo the only thing you need to draw them is to setup your vbo and pass the index in it for your fan start  glBindBuffer(GL_VERTEX_BUFFER buffer); glVertexPointer(3 GL_FLOAT 0 NULL); // 3 floats per vertex for each i in fans glDrawArrays(GL_TRIANGLE_FAN indef_of_first_vertex_for_fan[i] fan_vertex_count[i]) Edit: I'd have to say that you're probably better off transforming your fans to a regular triangle set and use glDrawArrays(GL_TRIANGLES) for all your triangles. A call per primitive is rarely efficient. Well if you have 200K triangle fans then you have that many calls **per frame**. Think about the cost compared to some load-time (or better off-line process time) that's required to transform the fans to triangles. sun proposed an extension because this was too much of an overhead *10 years ago*. The overhead/intersting work ratio only increased since. http://www.opengl.org/registry/specs/SUN/triangle_list.txt To be complete I should add that [primitive restart](http://www.opengl.org/registry/specs/NV/primitive_restart.txt) is an extension that you could be using as well (but requires indexing of your primitives which you should look at anyways). [glMultiDrawArrays](http://www.opengl.org/sdk/docs/man/xhtml/glMultiDrawArrays.xml) is yet another attempt at reducing the overhead but is rarely actually optimized in the drivers. Bahbar Thanks for the tip! I had read about transforming the fans into triangles... However I might have 200000 triangle fan primitives. Granted I'd only need to do this once so in your opinion is it cheaper to transform them and then do glDrawArrays or should I just do the call for each prim? Thanks Good stuff thanks man. It is so hard to find good examples/documentation of this kind of stuff,c++ opengl graphics 3d vertex-buffer1977737,A,"OpenGL Rotations around World Origin when they should be around Local Origin I'm implementing a simple camera system in OpenGL. I set up gluPerspective under the projection matrix and then use gluLookAt on the ModelView matrix. After this I have my main render loop which checks for keyboard events and if any of the arrow keys are pressed modifies angular and forward speeds (I only rotate through the y axis and move through the z (forwards)). Then I move the view using the following code (deltaTime is the amount of time since the last frame was rendered in seconds in order to decouple movement from framerate): //place our camera newTime = RunTime(); //get the time since app start deltaTime = newTime - time; //get the time since the last frame was rendered time = newTime; glRotatef(view.angularSpeed*deltaTime010); //rotate glTranslatef(00view.forwardSpeed*deltaTime); //move forwards //draw our vertices draw(); //swap buffers Swap_Buffers(); Then the code loops around again. My draw algorithm begins with a glPushMatrix() and ends in a glPopMatrix(). Each call to glRotatef() and glTranslatef() pushes the view forwards by the forwards speed in the direction of view. However when I run the code my object is drawn in the correct place but when I move the movement is done with the orientation of the world origin (000 - facing along the Z axis) as opposed to the local orientation (where I'm pointing) and when I rotate the rotation is done about (000) and not the position of the camera. I end up with this strange effect of my camera orbiting (000) as opposed to rotating on the spot. I do not call glLoadIdentity() at all anywhere inside the loop and I am sure that the Matrix Mode is set to GL_MODELVIEW for the entire loop. Another odd effect is if I put a glLoadIdentity() call inside the draw() function (between the PushMatrix and PopMatrix calls the screen just goes black and no matter where I look I can't find the object I draw. Does anybody know what I've messed up in order to make this orbit (000) instead of rotate on the spot? Thanks in advance for all of your help Please put code into code tags! glRotate() rotates the ModelView Matrix around the World Origin so to rotate around some arbitrary point you need to translate your matrix to have that point at the origin rotate and then translate back to where you started. I think what you need is this float x y z;//point you want to rotate around glTranslatef(00view.forwardSpeed*deltaTime); //move forwards glTranslatef(xyz); //translate to origin glRotatef(view.angularSpeed*deltaTime010); //rotate glTranslatef(-x-y-z); //translate back //draw our vertices draw(); //swap buffers Swap_Buffers(); Note that the comments on the 2nd and 3rd glTranslatef() calls are incorrect. They should be ""translate to origin"" and ""translate from origin"" respectively. Good catch! Thanks  Swap your rotate and translate calls around :) Since they post-multiply the matrix stack the last the be called is the 'first' to be applied conceptually if you care about that sort of thing. I tried that but it just gives the same effect. Since the rotate and translate calls all have such minute values in them and since they stack on top of each other as the while loop iterates there is little difference in their application order. Oh all your transformations are relative! If you reset the camera position each time (glLoadIdentity()) and do an absolute translation and rotation each time then things should work :)  I think you first have to translate your camera to point (000) then rotate then translate it back.",c++ opengl matrix rotation transform1816431,A,QT Webkit & OpenGL Rendering Context Would it be possible to create a window with a webpage using a webkit component using QT4 then embed an OpenGL context into the middle in the same way a java applet or a flash applet may appear normally? Sure you can embed any QWidget into a web page shown through a QWebView including a QGLWidget. This would be a starting point in the docs: http://doc.trolltech.com/4.5/qwebpage.html#setPluginFactory . I gogoled using the references you mentioned and it lead me to this: http://daniel-albuschat.blogspot.com/2008/12/embedding-qt-widgets-into-qtwebkit.html ^_^ Since you gave me the lead I needed Ill accept the answer thanks!,c++ qt opengl1135901,A,"Windows message loop Theres some reason for this code not reach the first else? I got it exactly the same from vairous sources. Than I did my own encapsulation. Everything goes fine. Window is created messages are treated events are generated to keyborad input in the client area the gl canvas works fine (when I force it to draw). The only problem is that message loop never leaves the first if. :/ I'm really stuck. while (!done) { if (::PeekMessage (&msg NULL 0 0 PM_REMOVE)) { if (msg.message == WM_QUIT) { done = TRUE; } else { ::TranslateMessage (&msg); ::DispatchMessage (&msg); } } else { // Code is never reaching this! draw (); ::SwapBuffers(hDC); idle (); } } return msg.wParam; Obviously something it posting new messages into the queue while Translate/Dispatch is done. You should just list all messages retrieved and deduce what message it is and why it appears. Using spy i got a hard flow of WM_PAINT with hdc 0. No idea how this is being generated. In your case the message queue must never be empty - why? Well it depends on what the rest of your program is doing. Some possibilities: Your code is posting new messages to the queue in a manner such that the queue doesn't get empty. I'd suggest logging out the message ids as they are handled. You aren't handling paint messages - from msdn: ""The PeekMessage function normally does not remove WM_PAINT messages from the queue. WM_PAINT messages remain in the queue until they are processed. However if a WM_PAINT message has a NULL update region PeekMessage does remove it from the queue."" Hope this helps. [Edit] To handle WM_PAINT either call BeginPaint and EndPaint or forward to DefWindowProc looks like you can also do it by forwarding wm_paint to DefWindowProc but it probably does the same thing looks like thats the problem. But the my MainWndProc is returning 0 from WM_PAINT. What I know is that every message you treat you return 0. So in theory no message should be left. He's handling WM_PAINT because he's seeing things appearing. Added this code and it start to work as expected. But would like to have some explanation if possible please. case WM_PAINT: BeginPaint(hwnd &ps); EndPaint(hwnd &ps); return 0; WM_PAINT isn't a real message - it's just a flag on the window that says it has invalidated regions (WM_PAINT never handled.) As long as that flag is present WM_PAINT will be generated. there isnt another way to acomplish that? looks like this will just add more processing to my program instead if just discard the message.  PeekMessage will return 0 only if there are no messages in the message queue. Since there are messages to be dispatched in the message queue it is returning a non-zero value and your else condition is never executed. I guess he is asking about this - why doesn't it ever happen that there're no messages left.  Make sure you are processing the WM_PAINT correctly. By this I mean make sure you are calling BeginPaint and EndPaint from inside the WM_PAINT message otherwise you will be confusing Windows into thinking your application still needs to be painted.  May be there is always a message waiting ? If you have a new question please ask it by clicking the [Ask Question](http://stackoverflow.com/questions/ask) button. Include a link to this question if it helps provide context.",c++ winapi opengl363302,A,openGL textures that are not 2^x in dimention I'm trying to display a picture in an openGL environment. The picture's origninal dimensions are 3648x2432 and I want to display it with a 256x384 image. The problem is 384 is not a power of 2 and when I try to display it it looks stretched. How can I fix that? If GL_EXT_texture_rectangle is true then use GL_TEXTURE_RECTANGLE_EXT for the first param in glEnable() and GLBindTexture() calls.  You can resize your texture so it is a power of two (skew your texture so that when it is mapped onto the object it looks correct). I am under the impression that textures are always a power of two because the hardware would use the same amount of resources anyways. But I could be incorrect. Yes this is incorrect in modern hardware.  There's three ways of doing this that I know of - The one Albert suggested (resize it until it fits). Subdivide the texture into 2**n-sized rectangles and piece them together in some way. See if you can use GL_ARB_texture_non_power_of_two. It's probably best to avoid it though since it looks like it's an Xorg-specific extension. Option 2 is great - prevents loss of fidelity  ARB_texture_rectangle is probably what you're looking for. It lets you bind to GL_TEXTURE_RECTANGLE_ARB instead of GL_TEXTURE_2D and you can load an image with non power-of-2 dimensions. Be aware that your texture coordinates will range from [0..w]x[0..h] instead of [0..1]x[0..1].,c++ opengl ldf1095378,A,"How do I destruct data associated with an object after the object no longer exists? I'm creating a class (say C) that associates data (say D) with an object (say O). When O is destructed O will notify C that it soon will no longer exist :( ... Later when C feels it is the right time C will let go of what belonged to O namely D. If D can be any type of object what's the best way for C to be able to execute ""delete D;""? And what if D is an array of objects? My solution is to have D derive from a base class that C has knowledge of. When the time comes C calls delete on a pointer to the base class. I've also considered storing void pointers and calling delete but I found out that's undefined behavior and doesn't call D's destructor. I considered that templates could be a novel solution but I couldn't work that idea out. Here's what I have so far for C minus some details:  // This class is C in the above description. There may be many instances of C. class Context { public: // D will inherit from this class class Data { public: virtual ~Data() {} }; Context(); ~Context(); // Associates an owner (O) with its data (D) void add(const void* owner Data* data); // O calls this when he knows its the end (O's destructor). // All instances of C are now aware that O is gone and its time to get rid // of all associated instances of D. static void purge (const void* owner); // This is called periodically in the application. It checks whether // O has called purge and calls ""delete D;"" void refresh(); // Side note: sometimes O needs access to D Data *get (const void *owner); private: // Used for mapping owners (O) to data (D) std::map _data; }; // Here's an example of O class Mesh { public: ~Mesh() { Context::purge(this); } void init(Context& c) const { Data* data = new Data; // GL initialization here c.add(this new Data); } void render(Context& c) const { Data* data = c.get(this); } private: // And here's an example of D struct Data : public Context::Data { ~Data() { glDeleteBuffers(1 &vbo); glDeleteTextures(1 &texture); } GLint vbo; GLint texture; }; }; P.S. If you're familiar with computer graphics and VR I'm creating a class that separates an object's per-context data (e.g. OpenGL VBO IDs) from its per-application data (e.g. an array of vertices) and frees the per-context data at the appropriate time (when the matching rendering context is current). So if I understand correctly you want reference counting? Thanks all. In my program there is an update thread and a render thread. The render thread is responsible for rendering twice once for each eye to create a stereo image. Each 3D object O has eye-specific rendering data D for each context (i.e. for each O there are two copies of D). The problem: O can be created or destroyed in the update thread but O's eye-specific rendering data must be destroyed in the render thread and only when rendering the associated eye. C needs to know if O is destructed so that in the render thread it will know to destroy both instances of D. The question is rather vague on the requirements so it's hard to give a good concrete answer. I hope the following helps. If you want the data to disappear immediately when its owner dies have the owner delete it (and notify C if the C instances need to know). If you want C to do the deletion at its leisure your solution looks fine. Deriving from Data seems to me the right thing to do. (Of course it is crucial that ~Data() be virtual as you have done.) What if D is an array of objects? There are two interpretations of this question. If you mean that D is always an array let it be an array (or vector<>) of pointers to Data. Then in C::purge() walk the vector and delete the objects. If you mean that D could be an array of objects but could also be a single object there are two ways to go. Either decide that it is always an array (possibly of size 1) or that it is a single object (derived from Data) which can be a class wrapping the array of the actual objects (or pointers to them). In the latter case the wrapper class destructor should walk the array and do the deletions. Note that if you want the array (or vector<>) to contains the actual objects not pointers to them (in which case you won't have to walk the array and delete manually) then you'll have the following limitations. 1. All objects in the array will have to be of the same actual type. 2. You will have to declare the array to be of that type. This will lose you all the benfits of polymorphism.  To answer the question ""What if D is an array of objects ?"" I'd suggest a vector<> but you'd have to associate it with D:  struct D_vector :D { vector<whatever> vw; };  What you're looking for is Boost::shared_ptr or some similar smart-pointer system.",c++ opengl graphics723762,A,"Programs causing static noise in speakers? Does anyone know a reason why my programs could be causing my speakers to output some soft static? The programs themselves don't have a single element that outputs sound to anything yet when I run a few of my programs I can hear a static coming from my speakers. It even gets louder when I run certain programs. Moving the speakers around doesn't help so it must be coming from inside the computer. I'm not sure what other details to put down since this seems very odd. They are OpenGL programs written in C++ with MS Visual C++. Edit: It seems to be that swapping the framebuffers inside an infinite loop is making the noise as when I stop swapping I get silence... I get exactly the same thing with a number of programs - particularly with ObjectDock (a Mac Dock imitation for Windows) - but only when I move my mouse if the cursor is over the dock. Quite strange. I don't know if ObjectDock is supposed to do that - seems weird that it would. Since you say you don't touch sound in your programs I doubt it's your code doing this. Does it occur if you run any other graphics-intensive programs? Also what happens if you mute various channels in the mixer (sndvol32.exe on 32-bit windows)? Not knowing anything else I'd venture a guess that it could be related to the fan on your graphics card. If your programs cause the fan to turn on and it's either close to your sound card or the fan's power line crosses an audio cable it could cause some static. Try moving any audio cables as far as possible from the fan and power cables and see what happens. It could also be picking up static from a number of other sources and I wouldn't say it's necessarily unusual. If non-graphics-intensive programs cause this as well it could be hard-disk access or even certain frequencies of CPU/power usage being picked up on an audio line like an antenna. You can also try to reduce the number of loops in your audio wires and see if it helps but no guarantees. I opened Sim City muted game sounds and I heard the same static. I guess I never noticed because I always play with sound. =/ Turns out it was some power wires hanging over my sound card. I moved them and silence!  Most electronic devices give off some kind of electromagnetic interference. Your speakers or sound hardware may be picking up something as simple as the signaling on your video cable or the graphics card itself. Cheap speakers and poorly-protected audio devices tend to be fairly sensitive to this kind of radiation in my experience.  There is interference on your motherboard that is leaking onto your sound bus. This is usually because of the quality of your motherboard or the age of it. Also the layout of the equipment inside your computer (close together over lapping) often will make interesting EM fields. My old laptop used to do this a lot easier as it got older. So as things are winding up or down you'll hear it. Try to see if it happens on a different computer. Try different computers of different ages and different configurations (external soundcard or a physical sound card etc). Hope that helps.  Computers consume a different amount of power when executing code. This fluctuation of current acts like a RF transmitter and can be picked up by audio equipment and it will be essentially ""decoded"" much like a AM modulated signal. As the execution usually does not produce a recognizable signal it sounds like white noise. A good example of audio equippment picking up a RF signal is if you hold your (GSM) cell phone close to an audio amplifier when receiving a call. You most likely will hear a characteristic pumping buzz from the cell phone's transmitter. Go here to learn more about Electromagnetic compatibility. There are multiple ways a signal can couple into your audio. As you mentioned a power cord to be the source it was most likely magnetic inductive coupling. This is exactly what it is. You either need better sheilding on your cables/speakers or on your computer. If your PC case is plastic it won't block any RF generated from within. If the case is steel it acts like a Faraday Cage. I don't think thats the reason. The current level is too low in the circuits to cause audible sound. Also there is no system in the computer which is built to catch RF frequency. This noise should be generated by actually getting a electric signal.  :) You will be surprised to know that the speaker input is picking up static from the hard disk. When you do something memory/disk intensive (like swapping framebuffers) so that the hard disk has to rotate fast the sound will appear. I had the same problem some years back I solved it too. But I am sorry that I don't remember how I did it. Hope the diagnosis helps in remedying the problem. UPDATE: I remembered. If you are using Windows go to volume control and mute all the external inputs/outputs like CD input etc. Just keep the two basic ones.  Crappy audio hardware on motherboards especially the ones that end up in office PCs. The interior of a PC case is full of electrical noise. If that couples to the audio hardware you'll hear it. Solution: Get a pair of headphones with a volume control on the cord. Turn the volume on the headphones down and turn the volume on the PC up full. This will increase the signal level relative to the noise level in most cases.  tempest dvbt",c++ opengl audio665688,A,"Is stereoscopy (3D stereo) making a come back? I'm working on a stereoscopy application in C++ and OpenGL (for medical image visualization). From what I understand the technology was quite big news about 10 years ago but it seems to have died down since. Now many companies seem to be investing in the technology... Including nVidia it would seem. Stereoscopy is also known as ""3D Stereo"" primarily by nVidia (I think). Does anyone see stereoscopy as a major technology in terms of how we visualize things? I'm talking in both a recreational and professional capacity. With nVidia's 3D kit you don't need to ""make a stereoscopy application"" drivers and video card take care of that. 10 years ago there was good quality stereoscopy with polarized glasses and extremely expensive monitors and low quality stereoscopy with red/cyan glasses. What you have now is both cheap and good quality. Right now all you need is 120Hz LCD entry level graphics card and $100 shutter glasses. So no doubt about it it will be the next big thing. At least in entertainment. I'm struggling to find a 120Hz LCD... Could someone name a few brands or models? Ones explicitly named by Nvidia: Samsung SyncMaster 2233RZ and ViewSonic FuHzion VX2265wm. I'm sure there are more. It's a bit hard to find them though as refresh Hz it's not something that's important LCD spec so most don't even specify this. Ah interesting. I was under the impression that 120Hz LCDs weren't available - perhaps I've been living in the past with the CRT sitting on my desk. Could you point me in the right direction? For example Nvidia bundles it's kit with Samsung SyncMaster 2233RZ. Sony sales 200Hz LCD Bravia Z since last year. 120Hz LCD is pretty much standard now. http://en.wikipedia.org/wiki/Motion_interpolation BTW. You're serious about having CRT on your desk? Wow! Haha yes I'd read in so many places that LCD doesn't support over 80Hz and I believed it since that appeared to be common knowledge; I guess all the material I read must have been ancient and defunct. As a result of this I bought a CRT. Now I feel a little embarrassed! I bought it about 2 years ago mind. Since when has >80Hz been around? Well it was true about 2 years ago. The thing is that fast LCD are TN which don't give you colors as rich as IPS or VA. BTW. most entry level LCD still are 60Hz although as long as you don't need stereoscopy it doesn't really mater. @Zeus one for each eye perfect for stereoscopy ;-) @vartec - I have 2 crt's on my desk :(  Enthusiasm for stereo display seems to come and go in cycles of hype and disappointment (e.g cinema). I don't expect TV and PCs will be any different. For medical visualisation if it was that useful there would be armies of clinicians sitting in front of expensive displays wearing shutter glasses already. Big hint: there aren't. And that market doesn't need 3D display tech to reach ""impulse purchase"" pricing levels as an enabler. You make a very good point. Actually in the medical field stereo 3D is quickly gaining popularity and is actively being used in multiple practices including robotics and surgery. For example with the DaVinci robot surgeons perform cases every day using a 3D display to operate with and the technicians moving tools in and out of the robot/person's body (using a monitor) don't want to push anything too far into the patient and stab them. Normally the robot would prevent you from doing this but it tends to malfunction and people just turn it off. So the need for depth perception is very important.  One reason why it is probably coming back is due to the fact that we know have screens with high enough refreshrate so that 3D is possible. I think I read that you will need somewhere around 100Hz for 3D-TV. So no need for bulky glasses anymore. Edit: To reiterate: You no longer need glasses in order to have 3D TV. This article was posted in a swedish magazine a few weeks ago: http://www.nyteknik.se/nyheter/it_telekom/tv/article510136.ece. What it says is basically that instead of glasses you use a technique with vertical lenses on the screen. Problem with CRT is that they are not flat. Our more modern flat screens obviously hasn't got this problem. The second problem is that you need high frequency (at least 100 Hz as that makes the eye get 50 frames per second) and a lot of pixels since each eye only gets half the pixels. TV sets that support 3D without glasses have been sold by various companies since 2005. It seems to me that 3D TVs are still quite rare I remember when Sharp tried this but I haven't really seen that product recently (I think they stopped marketing it) - I think Zalman do one right? Who else does them? What's the marketing keyword used most ""3D TV""? Zalman does ones that use polarization the 120Hz thing is based shutter glasses. The 3D-without-glasses displays are technically known as ""autosteroscopic"". I've seen the Philips one and it's quite impressive (as a tradeshow gimmick anyway).",c++ opengl nvidia stereo-3d stereoscopy327642,A,"OpenGL and monochrome texture Is it possible to pump monochrome (graphical data with 1 bit image depth) texture into OpenGL? I'm currently using this: glTexImage2D( GL_TEXTURE_2D 0 1 game->width game->height 0 GL_LUMINANCE GL_UNSIGNED_BYTE game->culture[game->phase] ); I'm pumping it with square array of 8 bit unsigned integers in GL_LUMINANCE mode (one 8 bit channel represents brightness of all 3 channels and full alpha) but it is IMO vastly ineffective because the onlu values in the array are 0x00 and 0xFF. Can I (and how) use simple one-bit per pixel array of booleans instead somehow? The excessive array size slows down any other operations on the array :( After some research I was able to render the 1-bit per pixel image as a texture with the following code: static GLubyte smiley[] = /* 16x16 smiley face */ { 0x03 0xc0 /* **** */ 0x0f 0xf0 /* ******** */ 0x1e 0x78 /* **** **** */ 0x39 0x9c /* *** ** *** */ 0x77 0xee /* *** ****** *** */ 0x6f 0xf6 /* ** ******** ** */ 0xff 0xff /* **************** */ 0xff 0xff /* **************** */ 0xff 0xff /* **************** */ 0xff 0xff /* **************** */ 0x73 0xce /* *** **** *** */ 0x73 0xce /* *** **** *** */ 0x3f 0xfc /* ************ */ 0x1f 0xf8 /* ********** */ 0x0f 0xf0 /* ******** */ 0x03 0xc0 /* **** */ }; float index[] = {0.0 1.0}; glPixelStorei(GL_UNPACK_ALIGNMENT1); glPixelMapfv(GL_PIXEL_MAP_I_TO_R 2 index); glPixelMapfv(GL_PIXEL_MAP_I_TO_G 2 index); glPixelMapfv(GL_PIXEL_MAP_I_TO_B 2 index); glPixelMapfv(GL_PIXEL_MAP_I_TO_A 2 index); glTexImage2D(GL_TEXTURE_2D0GL_RGBA16160GL_COLOR_INDEXGL_BITMAPsmiley); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); and here is the result: Nice! Have you compared its performance to the 1 byte per pixel variant? I found swapping the internal format to `LUMINANCE` produces the same visuals with a quarter the memory. If there are any formats smaller I would be interested. Idk how to get S3TC formats. I don't think pyOpenGL has those extensions. It does not appear that you can clamp this texture without it changing shape. I was just trying to get rid of the cross border bleeding.  The smallest uncompressed texture-format for luminance images uses 8 bits per pixel. However 1 bit per pixel images can be compressed without loss to the S3TC or DXT format. This will still not be 1 bit per pixel but somewhere between 2 and 3 bits. If you really need 1 bit per pixel you can do so with a little trick. Load 8 1 bit per pixel textures as one 8 bit Alpha-only texture (image 1 gets loaded into bit 1 image 2 into bit 2 and so on). Once you've done that you can ""address"" each of the sub-textures using the alpha-test feature and a bit of texture environment programming to turn alpha into a color. This will of only work if you have 8 1 bit per pixel textures and tricky to get right though. The 8 textures trick sounds interesting but it is not really what I am looking for. I'm using the OpenGL to visualize data from my application so such trick would slow down standard operation on the array (not speaking of programming complications).",c++ opengl textures791687,A,OpenGL: glTexImage2D conflicts with glGenLists & glCallList? I have a simple OpenGL application where I have 2 objects displayed on screen: 1) particle system where each particle is texture mapped with glTexImage2D() call. In the drawEvent function I draw it as a GL_TRIANGLE_STRIP with 4 glVertex3f. 2) a 3D text loaded from an object file where each point is loaded using glNewList/glGenLists and store each point as glVertex. I draw it by making call to glCallList. The problem is as soon as I call glTexImage2D() to map my particle with a .bmp file the 3D text would not show on the screen. The particle would look fine. (this is not expected) If I dont call glTexImage2D I see both the 3D text and particle system. In this case particle system looks horrible because I didnt texture map with anything. (this is expected) Does anyone know why using call list and glTexImage2D might conflict with each other? EDIT i also forgot to mention: I do call glBindTexture(GL_TEXTURE_2D this->texture); inside the drawLoop before particle system is called. EDIT2 i only call glTexImage2D() once at system start up (when I texture mapped the bitmap) glTexImage2D uploads the texture to the video-ram (simplified said). If OpenGL would allow you to place a glTexImage2D call inside the list it had to store the pixel-data in the list as well. Now what happends if you would execute the list? You would upload the same image data into the same texture all over gain. That makes no sense therefore it's left out. If you want to change textures between draw calls use glBindTexure. That call sets the current texture. It's much faster. Regarding your image-upload via glTexImage2D: Do that only once for each texture. Either at the start of your program (if it's small) or each time you load new content from disk. hi nils..yes i only call glTextImage2D once (at the start of the program) if you do so why do you want to put it into a list? hi..i'm sorry i dont explain clearly. i put all of my 3D fonts into a list. for particle system i use glGenTextures(1 &this->texture); glBindTexture(GL_TEXTURE_2D this->texture); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_WRAP_S GL_REPEAT); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_WRAP_T GL_REPEAT); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_MAG_FILTER GL_LINEAR); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_MIN_FILTER GL_LINEAR);  I already solve this problem. Before use GL_TEXTURE_2D you need set enable function=> glEnable(GL_TEXTURE_2D); And before use glCallList you also need set disable function => glDisable(GL_TEXTURE_2D);  It might not be a conflict of glTexImage2D and glCallList at all. Is your texture mapping ok ? have you set the texture coordinates propperly? Try checking for a vertex to texture coordinates missmatch. texture mapping looks ok for my particle system. the two when instantiated independently work fine. it's only when i to have them both displayed on the same screen it doesnt work.,c++ c opengl1626081,A,"OpenGL - Textures loading improperly UPDATE: I've posted the Renderer code below since this code here doesn't seem to be the problem. I'm having a problem with some code of mine where when I try to upload multiple textures to openGL one at a time it fails somewhat spectacularly with the renderer only ending up using a single texture. I've done a bit of debugging to trace the error to this function but I'm having problems figuring out what part of the function is at fault. Are there are particularly obvious screwups I'm making that I'm simply not seeing or is there a more subtle flaw in my code? Here're the structs I use for storing texture information and generally just keeping track of all my pointers typedef struct { float Width; float Height; } texInfo; typedef struct { dshlib::utfstr ResourceName; texInfo * TextureInfo; GLuint TextureNum; SDL_Surface * Image; } texCacheItem; And here's the current WIP graphics loader. Basically it loads a named .png file out of a .zip archive using a prewritten library (incidentally it's being tested with this program). Then it's loaded with libpng and then loaded up as a texture with caching thrown in to speed loading up and avoid loading a single texture more than once. I omitted the #include statements since they were just cruft. texCacheItem * loadGraphics(dshlib::utfstr FileName) { for(int i = 0; i < NumTexCached; i++) { //First see if this texture has already been loaded if(TextureCache[i]->ResourceName == FileName) return TextureCache[i]; } dshlib::utfstr FullFileName = ""Data/Graphics/""; //If not create the full file path in the archive FullFileName += FileName; dshlib::FilePtr file = resourceCtr.OpenFile(FullFileName); //And open the file if (!file->IsOk()) { //If the file failed to load... EngineState = ENGINESTATE_ERR; return NULL; } SDL_Surface * T = loadPNG(file); texCacheItem * Texture = new texCacheItem; Texture->TextureInfo = new texInfo; glGenTextures(1 &Texture->TextureNum); //Allocate one more texture and save the name to the texCacheItem glBindTexture(GL_TEXTURE_2D Texture->TextureNum); //Then create it glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA8 T->w T->h 0 GL_RGBA GL_UNSIGNED_BYTE T->pixels); Texture->TextureInfo->Width = (float)T->w; //Write the useful data Texture->TextureInfo->Height = (float)T->h; Texture->ResourceName = FileName; //And the caching info needed Texture->Image = T; //And save the image for if it's needed later and for deleting if (!TexCacheSize) { //If this is the first load this is 0 so allocate the first 8 Cache slots. TexCacheSize = 8; TextureCache = new texCacheItem*[8]; } if(NumTexCached == TexCacheSize) { //If we're out of cache space if (TexCacheSize == 32768) { //If too many cache items error out EngineState = ENGINESTATE_ERR; return NULL; } TexCacheSize <<= 1; //Double cache size texCacheItem ** NewSet = new texCacheItem*[TexCacheSize]; memcpy(NewSet TextureCache NumTexCached * sizeof(texCacheItem*)); //And copy over the old cache delete TextureCache; //Delete the old cache TextureCache = NewSet; //And assign the pointer to the new one } TextureCache[NumTexCached++] = Texture; //Store the texCacheItem to the Cache file->Close(); //Close the file file = NULL; //And NULL the smart pointer. [NTS: Confirm with Disch this is what won't cause a memory leak] return Texture; //And return the loaded texture in texCacheItem form. } SDL_Surface *loadPNG(dshlib::FilePtr File) { Uint8 *PNGFile = new Uint8[(long)File->GetSize()]; File->GetAr<Uint8>(PNGFile (long)File->GetSize()); return IMG_LoadPNG_RW(SDL_RWFromMem(PNGFile (long)File->GetSize())); } Here's the renderer code file. It's quite messy at the moment apologies for that. level->activeMap basically tells the renderer which ""layer"" of the tilemap (0 being the front 3 the back) to draw the sprites above. #include ""../MegaJul.h"" void render(void) { //Render the current tilemap to the screen glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT); glLoadIdentity(); glTranslatef(0.0f 0.0f -4.0f); if (level) { glBegin(GL_QUADS); float dT = 32.0f / level->dTex; float sX fX fXa sY tX tY sYa sYb sXa tXa tYa; unsigned long m = level->mapDimensions[0] * level->mapDimensions[1]; float ai; long long t; Sint16 * p; glBindTexture(GL_TEXTURE_2D level->tilemap->TextureNum); for (int i = 3; i >= 0; i--) { if (level->layers[i]->mapPosition[0] > 0) level->layers[i]->mapPosition[0] = 0; if (level->layers[i]->mapPosition[0] < 0 - (signed long)((level->mapDimensions[0] - 21) * 32)) level->layers[i]->mapPosition[0] = 0 - (signed long)((level->mapDimensions[0] - 21) * 32); if (level->layers[i]->mapPosition[1] < 0) level->layers[i]->mapPosition[1] = 0; if (level->layers[i]->mapPosition[1] > (signed long)((level->mapDimensions[1] - 16) * 32)) level->layers[i]->mapPosition[1] = (signed long)((level->mapDimensions[1] - 16) * 32); if (i == level->activeMap) { for (int j = 0; j < NumSprites; j++) { glBindTexture(GL_TEXTURE_2D Sprites[j]->Graphics->TextureNum); Sprites[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); } for (int j = 0; j < NumBullets; j++) { glBindTexture(GL_TEXTURE_2D Bullets[j]->Texture->TextureNum); Bullets[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); } } glBindTexture(GL_TEXTURE_2D level->tilemap->TextureNum); t = 0 - ((level->layers[i]->mapPosition[0] - (level->layers[i]->mapPosition[0] % 32)) /32) + (((level->layers[i]->mapPosition[1] - (level->layers[i]->mapPosition[1] % 32)) /32) * level->mapDimensions[0]); ai = (float)(3 - i); //Invert Z-Index sX = (float)((level->layers[i]->mapPosition[0] % 32)); sY = (float)((level->layers[i]->mapPosition[1] % 32)); if (sX > 0) sX -= 32; if (sY < 0) sY += 32; fX = sX /= 32.0f; sY /= 32.0f; fXa = sXa = sX + 1.0f; sYa = sY + 14.0f; sYb = sY + 15.0f; for (int y = 0; y < 16; y++) { for (int x = 0; x < 21; x++) { p = level->tiles[level->layers[i]->map[t]]->position; tX = p[0] / level->dTex; tY = p[1] / level->dTex; tXa = tX + dT; tYa = tY + dT; glTexCoord2f(tX tYa); glVertex3f(fX sYa ai); // Bottom Left Of The Texture and Quad glTexCoord2f(tXatYa); glVertex3f(fXa sYa ai); // Bottom Right Of The Texture and Quad glTexCoord2f(tXatY); glVertex3f(fXa sYb ai); // Top Right Of The Texture and Quad glTexCoord2f(tX tY); glVertex3f(fX sYb ai); // Top Left Of The Texture and Quad fX += 1.0f; fXa += 1.0f; t++; if (t >= m) break; } sYb -= 1.0f; sYa -= 1.0f; fXa = sXa; fX = sX; t += level->mapDimensions[0] - 21; //21 is the number of tiles drawn on a line (20 visible + 1 extra for scrolling) } } glEnd(); } SDL_GL_SwapBuffers(); } Here's the code segments that set the tilemap data for sprites and the level: Level: void loadLevel(dshlib::utfstr FileName) { -snip- texCacheItem * Tex = loadGraphics(FileName); if (!Tex) { //Load the tile graphics for the level unloadLevel(); EngineState = ENGINESTATE_ERR; return; } else { level->dTex = Tex->TextureInfo->Width; level->tilemap = Tex; } -snip- } Sprite: void SpriteBase::created() { this->Graphics = loadGraphics(DefaultGFX()); -snip- } UPDATE 2: Sid Farkus noted one big mistake I made with the renderer so here's an updated renderer.cpp: #include ""../MegaJul.h"" void render(void) { //Render the current tilemap to the screen glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT); glLoadIdentity(); glTranslatef(0.0f 0.0f -4.0f); if (level) { float dT = 32.0f / level->dTex; float sX fX fXa sY tX tY sYa sYb sXa tXa tYa; unsigned long m = level->mapDimensions[0] * level->mapDimensions[1]; float ai; long long t; Sint16 * p; for (int i = 3; i >= 0; i--) { if (level->layers[i]->mapPosition[0] > 0) level->layers[i]->mapPosition[0] = 0; if (level->layers[i]->mapPosition[0] < 0 - (signed long)((level->mapDimensions[0] - 21) * 32)) level->layers[i]->mapPosition[0] = 0 - (signed long)((level->mapDimensions[0] - 21) * 32); if (level->layers[i]->mapPosition[1] < 0) level->layers[i]->mapPosition[1] = 0; if (level->layers[i]->mapPosition[1] > (signed long)((level->mapDimensions[1] - 16) * 32)) level->layers[i]->mapPosition[1] = (signed long)((level->mapDimensions[1] - 16) * 32); if (i == level->activeMap) { for (int j = 0; j < NumSprites; j++) { glBindTexture(GL_TEXTURE_2D Sprites[j]->Graphics->TextureNum); glBegin(GL_QUADS); Sprites[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); glEnd(); } for (int j = 0; j < NumBullets; j++) { glBindTexture(GL_TEXTURE_2D Bullets[j]->Texture->TextureNum); glBegin(GL_QUADS); Bullets[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); glEnd(); } } glBindTexture(GL_TEXTURE_2D level->tilemap->TextureNum); glBegin(GL_QUADS); -snipped out renderer since it was bloat glEnd(); } SDL_GL_SwapBuffers(); } Your code looks fine have you verified your PNG loader is working correctly? Create a 4x4 texture and check the values to make sure you've got the right things out of the loader and that the byte ordering matches what you pass to glTexImage2D. Barring that I'd focus on your rendering code. Btw weird naming convention to start variable names with capital letter. I was confused for good 10 seconds when I first looked at your code. (not to say it's wrong just weird) I'm assuming you can't do any sort of debugging or logging for this? If you could I'd expect this would be trivial to diagnose. The main thing that looks dangerous to me is that you're not checking the return value from loadPNG. I'd put something there as the very first thing I did. I'd consider commenting out the initial check for an already-cached texture too. If things start working at that point you know it's a problem with the resource names or filenames (or the comparison of them). As an aside I'm surprised that you're using classes and smart pointers but rolling your own std::vector with bare pointers and arrays. ;) I have done debugging - and I've had no luck which is why I'm asking here. As far as the intermix goes well I'm sure I'll get around to making it make sense soon enough.  In your renderer are you calling glBindTexture appropriately? It sounds like your renderer is just using whatever the last texture you uploaded was since that was the last time you called glBindTexture. glBindTexture is what tells OpenGL texture to use for your polygons. Nope I'm making sure I always call glBindTexture() in the renderer. Would it help if I posted the relevant parts of it for reference? Yeah definitely since there is nothing obvious wrong in the loading code. (This assumes you are actually loading different textures of course. :) ) Print out the TextureNum before you call bind to be sure you are in fact at least setting a different opengl texture. Also try using GLintercept to see if you can spot anything using that. (I use this all the time to track down my opengl problems.) Running a debugger check on those shows that the sprite has TextureNum = 2 and level has TextureNum = 1. Thanks for the glIntercept tip though! ..And that solved my problem thanks! I should clarify; the GLIntercept program showed me what I did wrong. I had the last glEnd() one curly-brace too far down and wasn't noticing that.  With your rendering code I can see you're calling BindTexture in a glBegin/End block. From the opengl docs: GL_INVALID_OPERATION is generated if glBindTexture is executed between the execution of glBegin and the corresponding execution of glEnd. Move your BindTexture calls outside the glBegin()/glEnd() block and you should be golden. You'll probably have to have multiple blocks to accommodate your rendering style. edit: With the updated code make sure of a couple things; your sprite positions are visible on the screen with the current projection/model view matrix and your sprite texture ids are valid textures. There's nothing technically wrong that jumps out at me now but your values may be off. Hm now I'm getting the same issue but instead the *other* texture is used... Copying in the newer version knowing me I completely goofed it up... Make sure inside your render() calls you don't do anything wierd either. Remember there are a limited subset of functions you're permitted to call between glBegin and glEnd. The only things the sprite's render does is calculate some offsets and then call glTexCoord2f and glVertex3f.",c++ opengl textures819953,A,"How to start writing a music visualizer in C++? I'm interested in learning to use OpenGL and I had the idea of writing a music visualizer. Can anyone give me some pointers of what elements I'll need and how I should go about learning to do this? with a ""#include"" perhaps? sorry couldn't help myself :) You can find implementation of FFT algorithms and other useful informations in Numerical Recipes in C book. The book is free AFAIK. There is also Numerical Recipes in C++ book.  For the music analysis part you should study the basis of Fourier series then pick a free implementation of a DFFT (digital fast fourier transform) algorithm.  From my point of view...check this site: http://nehe.gamedev.net/ really good Information and Tutorials for using OpenGL edit: http://www.opengl.org/code/  If you use C++/CLI here's an example that uses WPF four (fourier that is;) display. He references this site that has considerable information about what your asking here's anoutline from the specific page; How do we split sound into frequencies? Our ears do it by mechanical means mathematicians do it using Fourier transforms and computers do it using FFT. The Physics of Sound 1.2. Harmonic Oscillator Sampling Sounds Fourier Analysis Complex Numbers Digital Fourier Transform FFT Ahhh I found this a few minutes later it's a native C++ analyzer. Code included that should get you off and running.  My approach for creating BeatHarness (http://www.beatharness.com) : record audio in real time have a thread that runs an FFT on the audio to get the frequency intensities calculate audio-volume for left and right channel filter the frequencies in bands (bass midtones treble) now you have some nice variables to use in your graphics display. For example show a picture where the size is multiplied by the bass - this will give you a picture that'll zoom in on the beat. From there on it's your own imagination ! :)  Are you trying to write your own audio/music player? Perhaps you should try writing a plugin for an existing player so you can focus on graphics rather than the minutia of codecs dsp and audio output devices. I know WinAMP and Foobar have APIs for visualization plugins. I'm sure Windows Media Player and iTunes also have them. Just pick a media player and start reading. Some of them may even have existing OpenGL plugins from which you can start so you can focus on pure OpenGL. I'm not trying to write a player or anything just something that will take in an MP3 and visualize it but I don't know anything about how to connect all those pieces. I'm on Linux so I'd stick to something simple. Haven't really thought about integrating it into anything else. consider using a library like mpg123 libmad or ffmpeg to decode then mp3 into audio samples. From there you'll want to use DFT(FFT) to convert audio to frequency information (see FFTW). At this point you'll have raw frequency data similar to what you see on most visuliazers (winamp/xmms moving lines w/ peaks). After that you need to figure out what to visual based on frequency and changes in frequency.  If you're just after some basic 3D or accelerated 2D then I'd recommend purchasing a copy of Dave Astle's ""Beginning OpenGL Game Programming"" which covers the basics of OpenGL in C++.  You may want to consider using libvisual's FFT/DCT functions over FFTW; they're much simpler to work with and provide data that's similarly easy to work with for the sake of generating visuals. Several media players and visualization plugins use libvisual to some extent for their visuals. Examples: Totem (player) GOOM (plugin for Totem and other players) PsyMP3 2.x (player)",c++ opengl audio visualization1738636,A,"OpenGL Keyboard Camera Controls I have been following this tutorial series for OpenGL: GLUT: http://www.lighthouse3d.com/opengl/glut/ I have reached the stage of implementing camera controls using the keyboard: http://www.lighthouse3d.com/opengl/glut/index.php?8 When doing the advanced tutorial it stops working. I've pretty much just copied and pasted it. When I run its version of the code it works. Mine just doesn't seem to work. It should rotate the camera view when moving left and right and move forward and backwards when using up and down keys. My code is here broken down: This part of my code renders components in the scene with init() which initilizes values etc.: void display(void) { if (deltaMove) moveMeFlat(deltaMove); if (deltaAngle) { angle += deltaAngle; orientMe(angle); } glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glBindTexture(GL_TEXTURE_2D texture[0]); RenderSkyDome(); glBindTexture(GL_TEXTURE_2D NULL); glutSwapBuffers(); } void init(void) { glClearColor(0.0 0.0 0.0 0.0); glClearDepth(1.0f); glColor3f(0.0 0.0 1.0); glMatrixMode(GL_PROJECTION); glLoadIdentity(); glEnable(GL_TEXTURE_2D); glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glHint(GL_PERSPECTIVE_CORRECTION_HINT GL_NICEST); LoadTextures(""clouds2.bmp"" 0); GenerateDome(600.0f 5.0f 5.0f 1.0f 1.0f); snowman_display_list = createDL(); } This is my main loop function: int main () { glutInitDisplayMode(GLUT_DEPTH | GLUT_DOUBLE | GLUT_RGBA); glutInitWindowSize(800 600); glutInitWindowPosition(0 0); glutCreateWindow(""Captain Ed's Adventures: Great Wall of China""); init(); //Glut Input Commands glutIgnoreKeyRepeat(1); glutSpecialFunc(pressKey); glutSpecialUpFunc(releaseKey); glutKeyboardFunc(processNormalKeys); glutDisplayFunc(display); glutIdleFunc(display); glutReshapeFunc(reshape); // This redraws everything on screen when window size is changed. glutMainLoop(); return 0; } Here are my input functions which are called: void pressKey(int key int x int y) { switch (key) { case GLUT_KEY_LEFT : deltaAngle = -0.10f;break; case GLUT_KEY_RIGHT : deltaAngle = 0.10f;break; case GLUT_KEY_UP : deltaMove = 50;break; case GLUT_KEY_DOWN : deltaMove = -50;break; } } void releaseKey(int key int x int y) { switch (key) { case GLUT_KEY_LEFT : if (deltaAngle < 0.0f) deltaAngle = 0.0f; break; case GLUT_KEY_RIGHT : if (deltaAngle > 0.0f) deltaAngle = 0.0f; break; case GLUT_KEY_UP : if (deltaMove > 0) deltaMove = 0; break; case GLUT_KEY_DOWN : if (deltaMove < 0) deltaMove = 0; break; } } void processNormalKeys(unsigned char key int x int y) { if (key == 27) exit(0); } Variables that get used by the camera and the functions that should change it: static float angle=0.0deltaAngle = 0.0ratio; static float x=0.0fy=1.75fz=5.0f; static float lx=0.0fly=0.0flz=-1.0f; static int deltaMove=0; void orientMe(float ang) { lx = sin(ang); lz = -cos(ang); glLoadIdentity(); gluLookAt(x y z x + lxy + lyz + lz 0.0f1.0f0.0f); } void moveMeFlat(int i) { x = x + i*(lx)*0.1; z = z + i*(lz)*0.1; glLoadIdentity(); gluLookAt(x y z x + lxy + lyz + lz 0.0f1.0f0.0f); } I believe that's pretty much everything I am working with so basically what should happen is that when I press UP key deltaMove should = 50 and when this happens the if statement in void display(void) should do moveMeFlat(deltaMove); I don't know if I am doing this wrong or if there is a better result.... I can move ""moveMeFlat(deltaMove)"" within the relevant switch cases but this does not allow me to have the movement that I want. It seems to work using the source code from the tutorials above with the right functionality but not in my case so my assumption is that it's to do with the if statement in display. The end result I am looking for is to be able to have forwards and backwards working with left and right rotating the camera. I would like to be able to press forward and left key and see the camera swerve left like in a racing game... This question has had many points looked into over at http://www.gamedev.net/community/forums/topic.asp?topic_id=553415 still no solution let me know if you can help thanks. You can download my sourcecode for this @ http://www.anicho.com/downloads/StageTwo.zip Try getting rid of static in front of all of your global variable declarations. The last use of static is as a global variable inside a file of code. In this case the use of static indicates that source code in other files that are part of the project cannot access the variable. Only code inside the single file can see the variable. (It's scope -- or visibility -- is limited to the file.) This technique can be used to simulate object oriented code because it limits visibility of a variable and thus helps avoid naming conflicts. This use of static is a holdover from C. -http://www.cprogramming.com/tutorial/statickeyword.html Also instead of this: if (deltaMove) moveMeFlat(deltaMove); if (deltaAngle) { angle += deltaAngle; orientMe(angle); } try this: if (deltaMove != 0) moveMeFlat(deltaMove); if (deltaAngle != 0.0) { angle += deltaAngle; orientMe(angle); } http://www.gamedev.net/community/forums/topic.asp?topic_id=553415 solution can be found here  Is GLUT idle when a key press is occurring? I think it is not idle. Your display() function is only called when the window is created and when GLUT is idle which doesn't allow your position to be changed. I suggest adding: glutTimerFunc(40 display 1); to the end of your display function. This will cause display to be called every 40 milliseconds. Also don't register display as the idle function or you'll have the idle function starting lots of series of timer functions. P.S. I would call glFlush(); just before swapping the buffers in your display function. http://www.gamedev.net/community/forums/topic.asp?topic_id=553415 thanks for the glFlush tip the solution can be found on this forum post",c++ opengl input keyboard glut1450023,A,"openGL and STL? I am using openGL and am currently passing it a vertex array. The problem is that I have to create many vertices and add them in between one another (for order). This means that using a regular array is pretty annoying/inefficient. I want to use a data structure from STL so that I can efficiently (and easily) put new vertices at any index. The problem is that openGL expects a regular array. Does anyone know how to go about this? Is there an easy way to convert from an STL vector to an array? I am using openGL 1.1 Thanks vector<int> array; ..... functionThatAcceptsArray(&array[0]); // yes this is standard  It depends on how much control you need over your intermediate data representations and how much you are able to precalculate. For a compromise between freedom and memory usage read about the Winged Edge data structure. The structure allows quick access and traversal between vertices edges and faces and works like a doubled linked-list. If you implement the concept and make an iterator implementation for it you can use std::copy to copy the data into any STL container. As the rest of the folks have already mentioned use std::vector as the final representation when OpenGL needs the data. And lastly: Don't be afraid to have several instances of the same data!  OpenGL requires a contiguous array of elements. For hopefully obvious reasons there is no efficient way to insert a single element into a contiguous array. It is necessarily at least O(N). However you potentially could add N elements in less than the O(N^2) that the vector achieves for N random insertions. For example if you don't actually add new vertices ""at any index"" but always close to the previous one you could add all the elements to a std::list (O(1) per element O(N) total) then copy the std::list to a std::vector. In fact it doesn't have to be the previous element just a previous element so if the order is based on a recursive traversal of some tree then you might still be able to do this. If new vertices are added at an index determined by some linear order then add all the elements to a std::map or std::multi_map (O(log N) per element O(N log N) total) then copy that to a vector. So the lowest-complexity way of doing it depends on how the order is determined. Whether these lower-complexity solutions are actually faster than the vector depends on N. They have much higher overheads (O(N) allocations instead of O(log N) for the vector) so N might have to be pretty big before the asymptotic behaviour kicks in. If you do use either of the solutions I describe then the easy/efficient way to copy either a list or a map to a vector is like this: std::vector<glVertex3f> vec; vec.reserve(listormap.size()); vec.insert(vec.begin() listormap.begin() listormap.end());  You can use a pointer to the first address of the vector as an array pointer. STL vectors are guaranteed to keep their elements in contiguous memory. So you can just do something like: &vertices[0] where vertices is your vector.",c++ opengl stl205522,A,openGL SubTexturing I have image data and i want to get a sub image of that to use as an opengl texture. glGenTextures(1 &m_name); glGetIntegerv(GL_TEXTURE_BINDING_2D &oldName); glBindTexture(GL_TEXTURE_2D m_name); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA m_width m_height 0 GL_RGBA GL_UNSIGNED_BYTE m_data); How can i get a sub image of that image loaded as a texture. I think it has something to do with using glTexSubImage2D but i have no clue how to use it to create a new texture that i can load. Calling: glTexSubImage2D(GL_TEXTURE_2D 0 xOffset yOffset xWidth yHeight GL_RGBA GL_UNSIGNED_BYTE m_data); doe nothing that i can see and calling glCopyTexSubImage2D just takes part of my framebuffer. Thanks Edit: Use glPixelStorei. You use it to set GL_UNPACK_ROW_LENGTH to the width (in pixels) of the entire image. Then you call glTexImage2D (or whatever) passing it a pointer to the first pixel of the subimage and the width and height of the subimage. Don't forget to restore GL_UNPACK_ROW_LENGTH to 0 when you're finished with it. Ie: glPixelStorei( GL_UNPACK_ROW_LENGTH img_width ); char *subimg = (char*)m_data + (sub_x + sub_y*img_width)*4; glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA sub_width sub_height 0 GL_RGBA GL_UNSIGNED_BYTE subimg ); glPixelStorei( GL_UNPACK_ROW_LENGTH 0 ); Or if you're allergic to pointer maths: glPixelStorei( GL_UNPACK_ROW_LENGTH img_width ); glPixelStorei( GL_UNPACK_SKIP_PIXELS sub_x ); glPixelStorei( GL_UNPACK_SKIP_ROWS sub_y ); glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA sub_width sub_height 0 GL_RGBA GL_UNSIGNED_BYTE m_data ); glPixelStorei( GL_UNPACK_ROW_LENGTH 0 ); glPixelStorei( GL_UNPACK_SKIP_PIXELS 0 ); glPixelStorei( GL_UNPACK_SKIP_ROWS 0 ); Edit2: For the sake of completeness I should point out that if you're using OpenGL-ES then you don't get GL_UNPACK_ROW_LENGTH. In which case you could either (a) extract the subimage into a new buffer yourself or (b)... glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA sub_width sub_height 0 GL_RGBA GL_UNSIGNED_BYTES NULL ); for( int y = 0; y < sub_height; y++ ) { char *row = m_data + ((y + sub_y)*img_width + sub_x) * 4; glTexSubImage2D( GL_TEXTURE_2D 0 0 y sub_width 1 GL_RGBA GL_UNSIGNED_BYTE row ); } Basically i have an image ( as raw data ) and i want to use part of that image as a texture. I know how to load the entire image as a texture but don't know how to use just a bit of it. Poster answered this. If you have an image in memory it is stored somehow. If you used glTexImage2D(...GL_RGBA GL_UNSIGNED_BYTE) to load then there's a byte for R G B and alpha value. The images are stored with pixel (0 0) starts at position 0. Pixel x at row y starts at image[width*y+x][0] since i am using openGL ES i ended up doing the first option u gave that is extracting subimage into a new buffer. sadly on the device it no longer works so i'm going to rework the texture so that i don't need to solve the problem.,c++ opengl1993431,A,"OpenGL: Rendering more than 8 lights how? How should I implement more than 8 lights in OpenGL? I would like to render unlimited amounts of lights efficiently. So whats the preferred method for doing this? OpenGL lights is a simplistic system that as far as I know is already in the deprecated list. You should handle lights yourself by writing a shader. Take a look here. the tutorial says ""therefore the best approach is to compile different shaders for different numbers of lights."" umm... thats not really what i want to do since i can move in the world and sometimes there might be 100 lights rendered sometimes 10 lights... i wouldnt want to write up to 2000 shaders for every possible combination of visible lights. Or did i understand something horribly wrong :/  Deferred shading. In a nutshell you render your scene without any lights. Instead you store the normals and world positions along with the textured pixels into multiple frame-buffers (so called render targets). You can even do this in a single pass if you use a multiple render-target extension. Once you have your buffers prepared you start to render a bunch of full-screen quads each with a pixel shader program that reads out the normals and positions and computes the light for one or multiple light-sources. Since light is additive you can render as much full-screen quads as you want and accumulate the light for as much light-sources as you want. A final step does a composition between your light and the unlit textured frame-buffer. That's more or less the state-of-the-art way to do it. Getting fog and transparency working with such a system is a challenge though.",c++ opengl lights765629,A,"Learning OpenGL through Java I'm interested in learning OpenGL and my favorite language at the time is Java. Can I reap its full (or most) benefits using things like JOGL or should I instead focus on getting stronger C++ skills? Btw which is your Java OpenGL wrapper library of choice and why? if you really want to get serious at 3d programming you have to learn C/C++ C++ is the standard for programming 3d games Games aren't the only commercial application of 3d graphics there's plenty of other things kitchen design software for example?  I have done some basic OpenGL development in Delphi and Java as well. I used JOGL as mentioned in others' replies and I must conclude that although there is very little difference in programming OpenGL in Java using JOGL and programming OpenGL in other languages (Delphi C++ etc...) it just doesn't feel right. It was driving me crazy to set it all up correctly and then writing stuff like gl.glBegin (GL.GL_QUADS) there seems to be a lot of superfuous gl GL GLU to be written and it just gets in your way. Also the performance would be I believe much better if you used C++ or similar NOT Java. I am not saying hands off JOGL (Java+OpenGL) it can be done and it really isn't too different but... as I said. Give c++ a try if you can. imho gl GL GLU is not superfluous since they allow to direct translate in Java their C code or tutorial I mean I prefer calling glBegin(GL_QUADS) instead of gl.glBegin(GL.GL_QUADS) :) then use ""import static"" :-) I partially agree. The ""gl"" prefix on the method names is not necessary.  JOGL is a wrapper library that allows OpenGL to be used in the Java programming language. It is currently the reference implementation for JSR-231 (Java Bindings for OpenGL) so it should be your first choice  Do it in C++ the bits you'll be using will basically be the same because there won't be much messing about with pointers so it's not a major leap of intuition. Also the C++ bindings are more reliable and (in my experience) the fastest out of all the langauges available (Python in particular REALLY can't do OpenGL). Also C++ is always a good thing to learn it makes you think about programming concepts in much more detail as opposed to just popping in an ArrayList<> or whatever pre-built class kind-of-serves your purposes =]  If you are also interested in just doing 3D stuff in Java without worrying about all that low-level stuff check out Java3D. Worst case you can look at how they leverage OpenGL for some good learning material.",java c++ opengl618829,A,openGL glDrawElements with interleaved buffers Thus far i have only used glDrawArrays and would like to move over to using an index buffer and indexed triangles. I am drawing a somewhat complicated object with texture coords normals and vertex coords. All this data is gathered into a single interleaved vertex buffer and drawn using calls similar to ( Assuming all the serup is done correctly ): glVertexPointer( 3 GL_FLOAT 22 (char*)m_vertexData ); glNormalPointer( GL_SHORT 22 (char*)m_vertexData+(12) ); glTexCoordPointer( 2 GL_SHORT 22 (char*)m_vertexData+(18) ); glDrawElements(GL_TRIANGLES m_numTriangles GL_UNSIGNED_SHORT m_indexData ); Does this allow for m_indexData to also be interleaved with the indices of my normals and texture coords as well as the standard position index array? Or does it assume a single linear list of inidices that apply to the entire vertex format ( POS NOR TEX )? If the latter is true how is it possible to render the same vertex with different texture coords or normals? I guess this question could also be rephrased into: if i had 3 seperate indexed lists ( POS NOR TEX ) where the latter 2 cannot be rearranged to share the same index list as the first what is the best way to render that. You cannot have different indexes for the different lists. When you specify glArrayElement(3) then OpenGL is going to take the 3rd element of every list. What you can do is play with the pointer you specify since essentially the place in the list which is eventually accessed is the pointer offset from the start of the list plus the index you specify. This is useful if you have a constant offset between the lists. if the lists are just a random permutation then this kind of play for every vertex is probably going to be as costy as just using plain old glVertex3fv() glNormal3fv() and glTexCoord3fv() Just to confirm the indices used in glDrawElements apply to your entire vertex format every time a vertex occurs on that mesh it must have the same normal and texture coords? i ask because certain 3D apps export models where a given vertex position might have different normal & texture coords If you want the same vertex to appear in a mesh with two different sets normals and texture coords then you'll need to add the vertex twice to arrays. As far as OpenGL is concerned they are two different vertices.  I am having similar trouble attempting to do the same in Direct3D 9.0 For my OpenGL 3 implementation it was rather easy and my source code is available online if it might help you any... https://github.com/RobertBColton/enigma-dev/blob/master/ENIGMAsystem/SHELL/Graphics_Systems/OpenGL3/GL3model.cpp,c++ opengl vertex-buffer1815513,A,"Problem with a volumetric fog in OpenGL Good day. I am trying to make a volumetric fog in OpenGL using glFogCoordfEXT. Why does a fog affect to all object of my scene even if they're not in fog's volume? And these objects become evenly gray as a fog itself. Here is a pic Code: void CFog::init() { glEnable(GL_FOG); glFogi(GL_FOG_MODE GL_LINEAR); glFogfv(GL_FOG_COLOR this->color); glFogf(GL_FOG_START 0.0f); glFogf(GL_FOG_END 1.0f); glHint(GL_FOG_HINT GL_NICEST); glFogi(GL_FOG_COORDINATE_SOURCE_EXT GL_FOG_COORDINATE_EXT); } void CFog::draw() { glBlendFunc(GL_SRC_ALPHA GL_SRC_ALPHA); glEnable(GL_BLEND); glPushMatrix(); glTranslatef(this->coords[0] this->coords[1] this->coords[2]); if(this->angle[0] != 0.0f) glRotatef(this->angle[0] 1.0f 0.0f 0.0f); if(this->angle[1] != 0.0f) glRotatef(this->angle[1] 0.0f 1.0f 0.0f); if(this->angle[2] != 0.0f) glRotatef(this->angle[2] 0.0f 0.0f 1.0f); glScalef(this->size this->size this->size); GLfloat one = 1.0f; GLfloat zero = 0.0f; glColor4f(0.0 0.0 0.0 0.5); glBegin(GL_QUADS); // Back Wall glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glEnd(); GLenum err; if((err = glGetError()) != GL_NO_ERROR) { char * str = (char *)glGetString(err); } glBegin(GL_QUADS); // Floor glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f-2.5f 15.0f); glEnd(); glBegin(GL_QUADS); // Roof glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f 2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f 2.5f 15.0f); glEnd(); glBegin(GL_QUADS); // Right Wall glFogCoordfEXT( zero); glVertex3f( 2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f 2.5f 15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glEnd(); glBegin(GL_QUADS); // Left Wall glFogCoordfEXT( zero); glVertex3f(-2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f 2.5f 15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glEnd(); glPopMatrix(); glDisable(GL_BLEND); //glDisable(GL_FOG); } Maybe there's no lighting in the scene? If all light sources are disabled all objects are going to appear in a flat ambient color. Check to see what happens if you disable fog altogether. Does this still happen? No lighting is there if I comment fog draw and init in my scene render functon all is fine  You seem to not be clear on how GL_fog_coord_EXT works. You're saying that an object is ""outside the fog volume"" but OpenGL does not have any notion of a fog volume. At any point either Fog is completely off or it's on in which case the fog equation will be applied with a fog coefficient that depends both on the fog mode (LINEAR in your case) and the fog coordinate. Regarding the fog coordinate. when using  glFogi(GL_FOG_COORDINATE_SOURCE_EXT GL_FOG_COORDINATE_EXT); You're telling OpenGL that every time you'll provide a vertex you'll also provide which fog coordinate to use through glFogCoordfEXT So what does it mean in your case ? Assuming you're not calling glFogCoordfEXT in your teapot drawing code you'll end up with the value of your last call to glFogCoordfEXT which looks like a glFogCoordf(one). So everything drawn in that case will be fully in fog which is what you observe. Now I'm not sure exactly what you're trying to achieve so I don't know how to help you solve the issue exactly. However if the goal is to use your quads to mimic fog simply turn fog off when drawing the scene and turn it on only when drawing the cube (I'm pretty sure it won't look like nice fog though). Thank you Bahbar!! this: ""you'll end up with the value of your last call to glFogCoordfEXT which looks like a glFogCoordf(one). "" helped! I just changed a drawing order and everything became ok:))",c++ opengl graphics648619,A,"Resizing an OpenGL window causes it to fall apart For some reason when I resize my OpenGL windows everything falls apart. The image is distorted the coordinates don't work and everything simply falls apart. I am sing Glut to set it up. //Code to setup glut glutInitWindowSize(appWidth appHeight); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA); glutCreateWindow(""Test Window""); //In drawing function glMatrixMode(GL_MODELVIEW); glLoadIdentity(); glClear(GL_COLOR_BUFFER_BIT); //Resize function void resize(int w int h) { glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } The OpenGL application is strictly 2D. This is how it looks like initially: http://www.picgarage.net/images/Corre_53880_651.jpeg this is how it looks like after resizing: http://www.picgarage.net/images/wrong_53885_268.jpeg The images are no longer available. You should not forget to hook the GLUT 'reshape' event: glutReshapeFunc(resize); And reset your viewport: void resize(int w int h) { glViewport(0 0 width height); //NEW glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } A perspective projection would have to take the new aspect ratio into account: void resizeWindow(int width int height) { double asratio; if (height == 0) height = 1; //to avoid divide-by-zero asratio = width / (double) height; glViewport(0 0 width height); //adjust GL viewport glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(FOV asratio ZMIN ZMAX); //adjust perspective glMatrixMode(GL_MODELVIEW); } And replace the C-style cast with static_cast<> since you're working with C++ :)",c++ opengl graphics glut1963157,A,OpenGL Video Memory Usage is there an API or profiler application that can track the video memory usage of my application? I am using C++/OpenGL on Windows but I am open to suggestions on other platforms as well. On Mac OS X you have OpenGL Profiler.app which comes with the Developer Tools (on the OS DVD or from http://developer.apple.com) On Windows you could try gDEBugger - a good commercial OpenGL Profiling tool.,c++ memory opengl profiling video-memory1302025,A,Game engine map editor. SDL->wxWidgets I have been writing an OpenGL game engine for a while which uses SDL for all the window management and for portability. I would like to create a level editor using the full engine. The engine itself isn't at all tied in with SDL except for input. I would like to use wxWidgets for the GUI and I have been looking at some OpenGL samples which are quite simple and easy to understand. Would it be simpler to try and integrate SDL with wxWidgets and use both or switch between them for use in different applications? What would be the best way to switch between the two systems? In all likelihood it would be easier to use one GUI API per application as opposed to merging the two together (i.e. easier to have SDL/OpenGL in your game and wxWidgets/OpenGL in your level editor). Usually these libraries are not built to be merged together or used in conjunction with other libraries so it would be nearly impossible to use them both in one program. For example I don't know much about SDL but a correctly-written wxWidgets program uses internal macros to generate int main() and start up its message pump among other things. If SDL required you to do the same thing (run some special initialization code in your int main() or allow SDL to generate its own int main()) you would be unable to initialize SDL properly without breaking wxWidgets and vice versa. Again I don't know if this particular conflict actually exists between the two libraries but it's just an example of how the two can interact and interfere with each other. That said in a perfect world it would be better to choose one of the libraries and use it both for your engine and level editor (i.e. use SDL/OpenGL for the engine and editor or wxWidgets/OpenGL for the engine and editor) but if you're happy maintaining two different API codebases then if it ain't broke don't fix it. SDL doesn't use a main() macro and can be initialized in parts but I'd guess that the message loops would overlap. (SDL requires you to implement your own loop.) Yeah if not at int main() then at the message loop will there be issues. wxWidgets creates its own native message loop completely hidden from the end-user; modifying it even slightly would easily break the library.,c++ opengl wxwidgets sdl279358,A,"Invalid lock sequence error in an OpenSceneGraph application I have an application that is built against OpenSceneGraph (2.6.1) and therefore indirectly OpenGL. The application initializes and begins to run but then I get the following exception ""attempt was made to execute an invalid lock sequence"" in OpenGL32.dll. When I re-run it I sometimes get this exception and sometimes an exception about a ""privileged instruction"". The call stack looks like it is corrupted so I can't really tell exactly where the exception is being thrown from. I ran the app quite a bit a couple of days ago and never saw this behavior. Since then I have added an else clause to a couple of ifs and that is all. My app is a console application is built with Visual Studio 2008 and it sets OpenScenGraph to SingleThreaded mode. Anybody seen this before? Any debugging tips? Can you reproduce it with one of the standard examples? Can you create a minimal app that causes this? Do you have a machine with a different brand video card you can test it on (eg Nvidia vs. ATI) there are some issues with openscenegraph and bad OpenGL drivers. Have you tried posting to osg-users@lists.openscenegraph.org  The problem turned out to be our app was picking up an incorrect version of the OpenGL DLL  instead of the one installed in System32.",c++ opengl openscenegraph1759397,A,"the quake 2 md2 file format (theory) i am trying to load md2 files in opengl but i noticed that most example programs just use a precompiled list of normals. something like this..... //table of precalculated normals { -0.525731f 0.000000f 0.850651f } { -0.442863f 0.238856f 0.864188f } { -0.295242f 0.000000f 0.955423f } { -0.309017f 0.500000f 0.809017f } ... ... Ok this may sound abit dumb but i thought each model is made of different triangles how then is it possible that you can use one set of precompiled normals to render all models? It seems abit strange and any ideas will be appreciated. You could use a precompiled table of normals and use a lookup table to select one that is 'good enough' for a particular case. Each triangle is on a distinct plane and it's that plane that has a normal not the triangle itself. For instance lets imagine we have a point. Expand that point into a sphere for the purposes of this discussion makes it a little easier to grasp conceptually. If you draw a perfect circle around that sphere on the y axis then rotate that circle in the x axis 1 degree each time you'll end up with 360 circles. If you take a normal at 1 degree intervals along each of those circles you'll end up with 360 ** 2 points. From there your normal is the vector from the center of the sphere to that point on the sphere and it is a normal for a plane constructed tangential to point on the sphere. What you end up with if you calculate these two for every point on that sphere is a precalculated table of normals which will almost certainly be good enough for most situations. Now you just need to design a lookup scheme for that data (plane -> normal). Do you happen to have any literature i can go through on this? No sorry this is just off the top of my head and my understanding of the math involved. Most higher level math books that cover 3d geometry should probably cover it though. +1. That's what I've heard. The table covers a finite subset of the unit sphere. So instead of storing normals in .md2 you would store indices to save space (That's what vector quantization is about).  It's been answered already but I want to shed some more light on it. The table contains vectors that cover the unit sphere's surface pretty uniformly. It seems the set of 162 vectors are the corners of a subdivided icosahedron. This is done to lossily compress 3D vectors of unit length to an index (8 bits) see vector quantization. For storing an arbitrary normal vector you can search the table for the closest match and store the index of this match instead. With this table of 162 well distributed vectors the angle between the original vector and the approximated one is expected to be below 11å¡ which seems to be good enough for the Quake2 engine.  The MD2 file format specifies that each vertex has a ""normal index"" and this is a lookup into a well-known table of normals. I would assume that these normals are distributed around a sphere. Presumably the tool that built the model chose the most appropriate of these normals for each vertex. With regard to the first answer: if you want a very faceted model (like a cube) then each polygon does indeed have its own normal and each of the vertices that makes up that polygon should use the same normal vector. However if you want smooth shading (such as a torso) it's common for each vertex in a polygon to have a different normal vector. This allows the lighting to vary across the polygon which is useful in both per-vertex and per-pixel lighting scenarios. Though you will use the polygon plane normal for lighting if you are drawing flat-shaded polygons. If you are talking about vertex normals then you still aren't talking about the polygon itself having a normal. You are still talking about a plane having a normal and that plane is usually taken to be a plane constructed perpendicular to the intended curvature of the shape at the point that the vertex occurs. I was just trying to clarify the point that I think you were making in your first paragraph. When it comes to MD2 models I think it's more useful to consider the tangent plane at each vertex rather than the tangent plane for each triangle (which is the way I read your answer at first). From your comment on my answer I think we're on the same page. I've done enough graphics work to be dangerous but not enough to know all that much. I just happen to be strong enough in the maths to figure out what's going on. The first paragraph was an attempt to answer the question as I percieved it was asked. As an aside the normals of the plane the polygon itself is on isn't useless it's just not used in lighting (it might be used ie in a physics engine though or collision detection or...).",c++ c opengl187057,A,"glBlendFunc and alpha blending I want to know how the glBlendFunc works. For example i have 2 gl textures where the alpha is on tex1 i want to have alpha in my final image. Where the color is on tex1 i want the color from tex2 to be. Seconding the shaders. If you can use a shader its much easier to just do what you want with the data rather than messing with arcane blending functions.  glBlendFunc applies only to how the final color fragment gets blended with the frame buffer. I think what you want is multitexturing to combine the two textures by blending the texture stages using glTexEnv or using a fragment shader to combine the the two textures. Does this require use of ARB extension? DavidG: If the technique uses ARB_texture_env_combine or shaders it will. Both are widely supported.  Sadly this is for openGL ES on the iPhone so no shaders but point taken. My problem was a very simplified version of the questions i needed to apply a simple color ( incl alpha ) to a part of a defined texture. As Lee pointed out texture blending is to allow alpha to show up on the framebuffer. The solution was to insist that the artist makes the ""action bit"" of the texture white and then assigning a color to the vertices that i render. Something like this. glTexCoordPointer( 2 GL_FLOAT 0 sprite->GetTexBuffer() ); glVertexPointer( 3 GL_FLOAT 0 sprite->GetVertexBuffer() ); glColorPointer( 4 GL_FLOAT 0 sprite->GetColorBuffer() ); glDrawArrays( GL_TRIANGLES 0 6 ); // Draw 2 triangles Where even tho it has a texture having the color means it adds to the texture's color so where it's an alpha it remains alpha and where it is white ( as i had to make it ) it becomes the color of the color pointer at the point. What does ""action bit"" mean? Does what you describe here solve your problem? yeah that solved my problem. ""action bit"" is the part that i want drawn in a different color. basically if i want a single texture ( maybe like a font set ) where each non transparent bit needs a custom color i used this.  Sorry can't do this with simple blending. We for instance used to do the same thing using frament shaders.",c++ opengl166356,A,"What are some best practices for OpenGL coding (esp. w.r.t. object orientation)? This semester I took a course in computer graphics at my University. At the moment we're starting to get into some of the more advanced stuff like heightmaps averaging normals tesselation etc. I come from an object-oriented background so I'm trying to put everything we do into reusable classes. I've had good success creating a camera class since it depends mostly on the one call to gluLookAt() which is pretty much independent of the rest of the OpenGL state machine. However I'm having some trouble with other aspects. Using objects to represent primitives hasn't really been a success for me. This is because the actual render calls depend on so many external things like the currently bound texture etc. If you suddenly want to change from a surface normal to a vertex normal for a particular class it causes a severe headache. I'm starting to wonder whether OO principles are applicable in OpenGL coding. At the very least I think that I should make my classes less granular. What is the stack overflow community's views on this? What are your best practices for OpenGL coding? if you do want to roll your own the above answers work well enough. A lot of the principles that are mentioned are implemented in most of the open source graphics engines. Scenegraphs are one method to move away from the direct mode opengl drawing. OpenScenegraph is one Open Source app that gives you a large (maybe too large) library of tools for doing OO 3D graphics there are a lot of other out there. This is the approach that Interactive Computer Graphics by Edward Angel takes towards OOP graphics programming. After reading Chapter 10 I'm convinced this is the correct answer. Thanks for the reference I'll definitely check it out later. Unfortunately our lecturers are a little draconian and we can't use 3rd party tools... :( Scenegraphs have their downsides this is highly recommended reading http://home.comcast.net/~tom_forsyth/blog.wiki.html#%5B%5BScene%20Graphs%20-%20just%20say%20no%5D%5D  I usually have a drawOpenGl() function per class that can be rendered that contains it's opengl calls. That function gets called from the renderloop. The class holds all info needed for its opengl function calls eg. about position and orientation so it can do its own transformation. When objects are dependent on eachother eg. they make a part of a bigger object then compose those classes in a other class that represents that object. Which has its own drawOpenGL() function that calls all the drawOpenGL() functions of its children so you can have surrounding position/orientation calls using push- and popmatrix. It has been some time but i guess something similar is possible with textures. If you want to switch between surface normals or vertex normals then let the object remember if its one or the other and have 2 private functions for each occasion that drawOpenGL() calls when needed. There are certainly other more elegant solutions (eg. using the strategy design pattern or something) but this one could work as far as I understand your problem sry for the late answer. yes im thinking: a mesh class consisting of a list of triangle classes and maybe the function for the avarage normals; the triangle class can generate its own normal. The mesh could do the drawing of the triangles. A primitive can be a mesh with a specific form. This is the approach I'm using now (w.r.t. the 2 private functions). It doesn't quite work out for me since each Triangle class depends on 6 other Triangles for normal averaging. Would you also recommend modelling a mesh instead of primitives as my basic class?  The most practical approach seems to be to ignore most of OpenGL functionality that is not directly applicable (or is slow or not hardware accelerated or is a no longer a good match for the hardware). OOP or not to render some scene those are various types and entities that you usually have: Geometry (meshes). Most often this is an array of vertices and array of indices (i.e. three indices per triangle aka ""triangle list""). A vertex can be in some arbitrary format (e.g. only a float3 position; a float3 position + float3 normal; a float3 position + float3 normal + float2 texcoord; and so on and so on). So to define a piece of geometry you need: define it's vertex format (could be a bitmask an enum from a list of formats; ...) have array of vertices with their components interleaved (""interleaved arrays"") have array of triangles. If you're in OOP land you could call this class a Mesh. Materials - things that define how some piece of geometry is rendered. In a simplest case this could be a color of the object for example. Or whether lighting should be applied. Or whether the object should be alpha-blended. Or a texture (or a list of textures) to use. Or a vertex/fragment shader to use. And so on the possibilities are endless. Start by putting things that you need into materials. In OOP land that class could be called (surprise!) a Material. Scene - you have pieces of geometry a collection of materials time to define what is in the scene. In a simple case each object in the scene could be defined by: - What geometry it uses (pointer to Mesh) - How it should be rendered (pointer to Material) - Where it is located. This could be a 4x4 transformation matrix or a 4x3 transformation matrix or a vector (position) quaternion (orientation) and another vector (scale). Let's call this a Node in OOP land. Camera. Well a camera is nothing more than ""where it is placed"" (again a 4x4 or 4x3 matrix or a position and orientation) plus some projection parameters (field of view aspect ratio ...). So basically that's it! You have a scene which is a bunch of Nodes which reference Meshes and Materials and you have a Camera that defines where a viewer is. Now where to put actual OpenGL calls is a design question only. I'd say don't put OpenGL calls into Node or Mesh or Material classes. Instead make something like OpenGLRenderer that can traverse the scene and issue all calls. Or even better make something that traverses the scene independent of OpenGL and put lower level calls into OpenGL dependent class. So yes all of the above is pretty much platform independent. Going this way you'll find that glRotate glTranslate gluLookAt and friends are quite useless. You have all the matrices already just pass them to OpenGL. This is how most of real actual code in real games/applications work anyway. Of course the above can be complicated by more complex requirements. Particularly Materials can be quite complex. Meshes usually need to support lots of different vertex formats (e.g. packed normals for efficiency). Scene Nodes might need to be organized in a hierarchy (this one can be easy - just add parent/children pointers to the node). Skinned meshes and animations in general add complexity. And so on. But the main idea is simple: there is Geometry there are Materials there are objects in the scene. Then some small piece of code is able to render them. In OpenGL case setting up meshes would most likely create/activate/modify VBO objects. Before any node is rendered matrices would need to be set. And setting up Material would touch most of remaining OpenGL state (blending texturing lighting combiners shaders ...). Your idea for the mesh class seems obvious to me now :) What I was trying to do was to use objects for primitives like triangles. Using objects to manage meshes makes a lot more sense as they tend to be pretty self sufficient correct? Also thanks a lot for the insight into the platform independence stuff and the render trees! That helps a lot! I think this is a great answer! ... Anyone starting to write a 3d engine should base it on these principles. (Or even better: use a engine that does all this for you and save valueable time)  Object transformations Avoid depending on OpenGL to do your transformations. Often tutorials teach you how to play with the transformation matrix stack. I would not recommend using this approach since you may need some matrix later that will only be accessible through this stack and using it is very long since the GPU bus is designed to be fast from CPU to GPU but not the other way. Master object A 3D scene is often thought as a tree of objects in order to know object dependencies. There is a debate about what should be at the root of this tree a list of object or a master object. I advice using a master object. While it does not have a graphical representation it will be simpler because you will be able to use recursion more effectively. Decouple scene manager and renderer I disagree with @ejac that you should have a method on each object doing OpenGL calls. Having a separate Renderer class browsing your scene and doing all the OpenGL calls will help you decouple your scene logic and OpenGL code. This is adds some design difficulty but will give you more flexibility if you ever have to change from OpenGL to DirectX or anything else API related. From what I've seen I definitely agree with your last point. I will have to try out the second pattern but my work isn't really complicated enough to warrant platform independence (it's just a couple of throw-away practical excercises).  A standard technique is to insulate the objects' effect on the render state from each other by doing all changes from some default OpenGL state within a glPushAttrib/glPopAttrib scope. In C++ define a class with constructor containing  glPushAttrib(GL_ALL_ATTRIB_BITS); glPushClientAttrib(GL_CLIENT_ALL_ATTRIB_BITS); and destructor containing  glPopClientAttrib(); glPopAttrib(); and use the class RAII-style to wrap any code which messes with the OpenGL state. Provided you follow the pattern each object's render method gets a ""clean slate"" and doesn't need to worry about prodding every possibly modified bit of openGL state to be what it needs. As an optimisation typically you'd set the OpenGL state once at app startup into some state which is as close as possible to what everything wants; this minimisies the number of calls which need to be made within the pushed scopes. The bad news is these aren't cheap calls. I've never really investigated how many per second you can get away with; certainly enough to be useful in complex scenes. The main thing is to try and make the most of states once you've set them. If you've got an army of orcs to render with different shaders textures etc for armour and skin don't iterate over all the orcs rendering armour/skin/armour/skin/...; make sure you set up the state for the armour once and render all the orcs' armour then setup to render all the skin. It's kinda strange to instantiate an object call its render() function and then destroy it just to insulate the state. Am I understanding correctly? Sorry I didn't explain it very well. The object doing push/pop on the GL state is just a convenience helper... nothing to do with the objects you're rendering. Code would look something like: renderable* thing=new...; { gl_pushed_scope p; thing->render(); }",c++ opengl oop1998251,A,"Basic C++ memory question a friend of mine declared a new type using typedef GLfloat vec3_t[3]; and later used vec3_t to allocate memory vertices=new vec3_t[num_xyz* num_frames]; He freed the memory using delete [] vertices; Question: 1. Since vec3_t is an alias for GLfloat[3] does it mean that vec3_t[num_xyz* num_frames] is equivalent to GLfloat[3][num_xyz* num_frames]; 2. If the above is a 2 dimentional array How is it supporsed to be properly deleted from memory? thanks in advance; from deo GLfloat is an array that is ""statically"" allocated and thus that doesn't need to be explicitly deallocated. From a memory point of view this typedef is equivalent to the following structure: typedef struct { GLfloat f1; GLfloat f2; GLfloat f3; } vec3_t; You can then have the following code which is now less confusing: vec3_t* vertices = new vec3_t [num_xyz* num_frames]; [...] delete[] vertices;  It will be deleted the same way it's been allocated - one contiguous piece of memory. See 2D array memory layout this not entirely correct there is a difference between allocation an array and allocating a single object. that's why you have delete vs delete[] The visualization in the first example of your link is misleading - ttt is defined as an array not as a pointer.  1. a two dimensional array can be thoght of as a one dimensional array where each element is an array. using this definition you can see that new vec3_t[num_xyz* num_frames] is equivalent to a two dimensional array. 2. this array is made of num_xyz* num_frames members each taking a space of sizeof (vec3_t) when new is carried out it allocates num_xyz* num_frames memory blokes in the heap it takes note of this number so that when calling delete[] it will know how many blocks of sizeof (vec3_t) it should mark as free in the heap.  You almost got it right vec3_t[num_xyz* num_frames] is equivalent to GLfloat[num_xyz* num_frames][3] Since you allocated with new[] you have to delete with delete[].  I think that the delete is OK but to reduce confusion I tend to do this: struct vec3_t{ GLFloat elems[3]; }; vec3_t* vertices = new vec3_t[num_xyz* num_frames]; Now you can see the type of vertices and: delete [] vertices; is obviously correct.",c++ opengl1940787,A,"Which IDE should I use for this art project? I have an art project that will require processing a live video feed to use as the basis of a particle system which will be rendered using OpenGL and projected on a stage. I have a CUDA enabled graphics card and I was thinking it would be nice to be able to use that for the image and particle system processing. This project only needs to run on my computer. I am normally a C# asp.net Visual Studio kinda guy but for this project I plan on using c++. Should I do the work in Eclipse on Ubuntu or Visual Studio in Windows? I realize this can be fairly arbitrary but I wondering if one IDE/OS might be better suited for this kind of work than the other As far as the CUDA or OpenGL support is concerned you are fine with either of them. The nVidia examples are also multiplatform. The real question is if you plan on using any GUI Toolkit as there are a only a few choices that are really portable. In the end I'd recommend going with what you feel more comfortable with or where you will have the biggest knowledge gain (if learning something is a goal of the project.).  Are you aware of OpenFrameworks? This might just help shortcut to what you need. No I hadnt heard of it but I am gonna check it out. Dude. OpenFrameworks looks awesome! That is definitely in the same vein as this project I am working on. Thanks for the link Given it's a 'C++ toolkit for creative coding' would be good to know what you think.  +1 for Visual Studio. I haven't heard about any IDE especially good for such tasks. If you already know VS I see no reason to learn anything else.  Since you're already familiar with Visual Studio you should probably stick with it. In addition you'll be able to use the Nexus debugger to debug both the OpenGL and CUDA components.  While the CUDA toolkit is cross-platform i recommend Linux in this case: The debugger is based on gdb and the usability of the gcc toolchain is just much better on *nixes. You also don't seem to have any windows specific dependencies. What do you mean about the usability of the gcc toolchain? On windows you either use gcc-tools via a graphical interface i.e. have to figure out their way to pass flags etc. to the gcc tools or use them on the commandline which ain't fun on windows (using cygwin doesn't help with everything). What do you mean by ""the usability of the gcc toolchain is just much better on *nixes""? What is ""*nixes""? I assume linux/unix based OS's Thats what i meant.",c++ visual-studio-2008 eclipse opengl cuda1080635,A,"Other's library #define naming conflict Hard to come up with a proper title for this problem. Anyway... I'm currently working on a GUI for my games in SDL. I've finished the software drawing and was on my way to start on the OpenGL part of it when a weird error came up. I included the ""SDL/SDL_opengl.h"" header and compile. It throws ""error C2039: 'DrawTextW' : is not a member of 'GameLib::FontHandler'"" which is a simple enough error but I don't have anything called DrawTextW only FontHandler::DrawText. I search for DrawTextW and find it in a #define call in the header ""WinUser.h""! //WinUser.h #define DrawText DrawTextW Apparently it replaces my DrawText with DrawTextW! How can I stop it from spilling over into my code like that? It's a minor thing changing my own function's name but naming conflicts like this seem pretty dangerous and I would really like to know how to avoid them all together. Cheers! Just #undef the symbols you don't want. But Make sure that you include windows.h and do this before you include SDL: #include <windows.h> #undef DrawText #include <SDL/SDL_opengl.h>  You have a couple of options all of which suck. Add #undef DrawText in your own code Don't include windows.h. If another library includes it for you don't include that directly. Instead include it in a separate .cpp file which can then expose your own wrapper functions in its header. Rename your own DrawText. When possible I usually go for the middle option. windows.h behaves badly in countless other ways (for example it doesn't actually compile unless you enable Microsoft's proprietary C++ extensions) so I simply avoid it like the plague. It doesn't get included in my files if I can help it. Instead I write a separate .cpp file to contain it and expose the functionality I need. Also feel free to submit it as a bug and/or feedback on connect.microsoft.com. Windows.h is a criminally badly designed header and if people draw Microsoft's attention to it there's a (slim) chance that they might one day fix it. The good news is that windows.h is the only header that behaves this badly. Other headers generally try to prefix their macros with some library-specific name to avoid name collisions they try to avoid creating macros for common names and they try avoid using more macros than necessary. Yeah Windows.h does collide. I guess it is fair to complain it doesn't compile w/o ms extensions enabled but it is also reasonable that an ms-specific header should (require those extensions).  It's an unfortunate side effect of #includeing <windows.h>. Assuming you're not actually using Windows' DrawText() anywhere in your program it's perfectly safe to #undef it immediately after: // wherever you #include <windows.h> or any other windows header #include <windows.h> #undef DrawText It could. But that would require Microsoft to care. Thanks! It compiles fine now. Quite an annoying side effect indeed. Seems as if the SDL_opengl.h includes the windows.h but the #undef does it's job. Could the windows.h have been written differently to avoid this problem?  There is no general way of avoiding this problem - once you #include a header file using the preprocessor it can redefine any name it likes and there is nothing you can do about it. You can #undef the name but that assumes you know the name was #defined in the first place.",c++ opengl sdl1799070,A,"What might cause OpenGL to behave differently under the ""Start Debugging"" versus ""Start without debugging"" options? I have written a 3D-Stereo OpenGL program in C++. I keep track of the position objects in my display should have using timeGetTime after a timeBeginPeriod(1). When I run the program with ""Start Debugging"" my objects move smoothly across the display (as they should). When I run the program with ""Start without debugging"" the objects occationally freeze for several screen refreshes then jump to a new position. Any ideas as to what may be causing this problem and how to fix it? Edit: It seems like the jerkiness can be resolved after a short delay when I run through ""Start without debugging"" if I click the mouse button. My application is a console application (I take in some parameters when the program first starts). Might there be a difference in window focus between these two options? Is there an explicit way to force the focus to the OpenGL window (in full screen through glutFullScreen();) when I'm done taking input from the console window? Thanks. How does your release build perform and what is your overall CPU usage like? Since I work almost entirely in a development enviroment and CPU usage shouldn't be an issue with the simple displays I render I've never built a release build. My recent attempt to do so was thwarted by a screen capture library that I have been using that refuses to link correctly. CPU utilazation on the core running the process is high but seldom pegged. The most common thing that causes any program to behave differently while being debugged and not being debugged is using uninitialized variables and especially reading uninitialized memory. Check that you're not doing that. Something more OpenGL specific - You might have some problems with flushing of commands. Try inserting glFinish() after drawing every frame. It might also be helpful to somehow really make sure that when the freeze occurs there are actually frames being rendered and not that the whole application is frozen. If there are its more likely that you have some bug in the logic since it seems that OpenGL does its job. Thank you for the advice. Any suggestion on how to determine whether frames are still being rendered during the apparent freeze? I had tried to check that before but found that the slow down due to writing a file was obscuring whatever it was that was going on. As this is a stereo display @ 120Hz I have about 8.33ms to get through my entire display loop. Usually a debug build will initialize memory in exactly the same way whether run through a debugger or not. On windows you would still be using the debug runtime libraries. It can make a big difference when moving to a release build.  The timeGetTime API only has a precision of something like 10ms. If the intervals you're measuring are less than 50ms or so you may simply be seeing the effects of the expected variance in the system timer. I have no idea why the debugger would have an effect on this but then the whole workings of the system are a black box. You could use the QueryPerformanceCounter to get higher-resolution timings which may help. Thanks I'll take a look at that. I had thought that timeBeginPeriod(1) was accurately setting the resolution down to 1ms. If the resolution is @ 10ms that could definately cause some problems for my code. @drknexus The documentation for timeGetTime and timeBeginPeriod implies that it's 1ms but it seems to be highly dependent on what hardware is available. I suppose it's possible that ""modern"" hardware has fixed this it's been a few years since I played with it. This is where I remember the 10ms value from but it's also a few years old: http://support.microsoft.com/kb/172338",c++ windows opengl timer stereo-3d1672926,A,"Setting up a camera in OpenGL I've been working on a game engine for awhile. I've started out with 2D Graphics with just SDL but I've slowly been moving towards 3D capabilities by using OpenGL. Most of the documentation I've seen about ""how to get things done"" use GLUT which I am not using. The question is how do I create a ""camera"" in OpenGL that I could move around a 3D environment and properly display 3D models as well as sprites (for example a sprite that has a fixed position and rotation). What functions should I be concerned with in order to setup a camera in OpenGL camera and in what order should they be called in? Here is some background information leading up to why I want an actual camera. To draw a simple sprite I create a GL texture from an SDL surface and I draw it onto the screen at the coordinates of (SpriteX-CameraX) and (SpriteY-CameraY). This works fine but when moving towards actual 3D models it doesn't work quite right. The cameras location is a custom vector class (i.e. not using the standard libraries for it) with X Y Z integer components. I have a 3D cube made up of triangles and by itself I can draw it and rotate it and I can actually move the cube around (although in an awkward way) by passing in the camera location when I draw the model and using that components of the location vector to calculate the models position. Problems become evident with this approach when I go to rotate the model though. The origin of the model isn't the model itself but seems to be the origin of the screen. Some googling tells me I need to save the location of the model rotate it about the origin then restore the model to its origal location. Instead of passing in the location of my camera and calculating where things should be being drawn in the Viewport by calculating new vertices I figured I would create an OpenGL ""camera"" to do this for me so all I would need to do is pass in the coordinates of my Camera object into the OpenGL camera and it would translate the view automatically. This tasks seems to be extremely easy if you use GLUT but I'm not sure how to set up a camera using just OpenGL. EDIT #1 (after some comments): Following some suggestion here is the update method that gets called throughout my program. Its been updated to create perspective and view matrices. All drawing happens before this is called. And a similar set of methods is executed when OpenGL executes (minus the buffer swap). The xyz coordinates are straight an instance of Camera and its location vector. If the camera was at (256 32 0) then 256 32 and 0 would be passed into the Update method. Currently z is set to 0 as there is no way to change that value at the moment. The 3D model being drawn is a set of vertices/triangles + normals at location X=320 Y=240 Z=-128. When the program is run this is what is drawn in FILL mode and then in LINE mode and another one in FILL after movement when I move the camera a little bit to the right. It likes like may Normals may be the cause but I think it has moreso to do with me missing something extremely important or not completely understanding what the NEAR and FAR parameters for glFrustum actually do. Before I implemented these changes I was using glOrtho and the cube rendered correctly. Now if I switch back to glOrtho one face renders (Green) and the rotation is quite weird - probably due to the translation. The cube has 6 different colors one for each side. Red Blue Green Cyan White and Purple. int VideoWindow::Update(double x double y double z) { glMatrixMode( GL_PROJECTION ); glLoadIdentity(); glFrustum(0.0f GetWidth() GetHeight() 0.0f 32.0f 192.0f); glMatrixMode( GL_MODELVIEW ); SDL_GL_SwapBuffers(); glLoadIdentity(); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glRotatef(0 1.0f 0.0f 0.0f); glRotatef(0 0.0f 1.0f 0.0f); glRotatef(0 0.0f 0.0f 1.0f); glTranslated(-x -y 0); return 0; } EDIT FINAL: The problem turned out to be an issue with the Near and Far arguments of glFrustum and the Z value of glTranslated. While change the values has fixed it I'll probably have to learn more about the relationship between the two functions. If you have not checked then may be looking at following project would explain in detail what ""tsalter"" wrote in his post. Camera from OGL SDK (CodeColony) Also look at Red book for explanation on viewing and how does model-view and projection matrix will help you create camera. It starts with good comparison between actual camera and what corresponds to OpenGL API. Chapter 3 - Viewing  You have to do it using the matrix stack as for object hierarchy but the camera is inside the hierarchy so you have to put the inverse transform on the stack before drawing the objects as openGL only uses the matrix from 3D to camera.  Just remember that OpenGl post multiplies.  You need a view matrix and a projection matrix. You can do it one of two ways: Load the matrix yourself using glMatrixMode() and glLoadMatrixf() after you use your own library to calculate the matrices. Use combinations of glMatrixMode(GL_MODELVIEW) and glTranslate() / glRotate() to create your view matrix and glMatrixMode(GL_PROJECTION) with glFrustum() to create your projection matrix. Remember - your view matrix is the negative translation of your camera's position (As it's where you should move the world to relative to the camera origin) as well as any rotations applied (pitch/yaw). Hope this helps if I had more time I'd write you a proper example! Thanks for the push in the right direction. I've made an edit in the post if you could take a look at my code and screenshots. Something is not quite right :) Actually the issue really is with the values I pick for the Near and Far arguments to glFrustum as well as the Z value passed into glTranslated. Thanks.",c++ opengl sdl724060,A,"How do I render text on to a square (4 vertices) in OpenGL? I'm using Linux and GLUT. I have a square as follows: glVertex3f(-1.0 +1.0 0.0); // top left glVertex3f(-1.0 -1.0 0.0); // bottom left glVertex3f(+1.0 -1.0 0.0); // bottom right glVertex3f(+1.0 +1.0 0.0); // top right I guess I can't use glutBitmapCharacter since this is only ideal for 2D ortho. Simple enough I'd like to render ""Hello world!"" anywhere on said square. Should I create a texture and then apply it to the vertices using glTexCoord2f? The simplest way is to load a font map from a image such as those generated by the bitmap font builder (I know it's windows but I can't find one for linux) eg: The example is a 256x256 gif though you may what to convert it to a png/tga/bmp. It's full ASCII mapped gird 16x16 characters. Load the texture and use glTexCoord2f to line it up on your quad and you should be good to go. Here's an example using a bitmap of the above: unsigned texture = 0; void LoadTexture() { // load 24-bit bitmap texture unsigned offset width height size; char *buffer; FILE *file = fopen(""text.bmp"" ""rb""); if (file == NULL) return; fseek(file 10 SEEK_SET); fread(&offset 4 1 file); fseek(file 18 SEEK_SET); fread(&width 1 4 file); fread(&height 1 4 file); size = width * height * 3; buffer = new char[size]; fseek(file offset SEEK_SET); fread(buffer 1 size file); glEnable(GL_TEXTURE_2D); glGenTextures(1 &texture); glBindTexture(GL_TEXTURE_2D texture); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); glTexImage2D(GL_TEXTURE_2D 0 GL_RGB width height 0 GL_RGB GL_UNSIGNED_BYTE buffer); fclose(file); printf(""Loaded\n""); } void DrawCharacter(char c) { int column = c % 16 row = c / 16; float x y inc = 1.f / 16.f; x = column * inc; y = 1 - (row * inc) - inc; glBegin(GL_QUADS); glTexCoord2f( x y); glVertex3f( 0.f 0.f 0.f); glTexCoord2f( x y + inc); glVertex3f( 0.f 1.f 0.f); glTexCoord2f( x + inc y + inc); glVertex3f( 1.f 1.f 0.f); glTexCoord2f( x + inc y); glVertex3f( 1.f 0.f 0.f); glEnd(); } Very helpful thanks.  Indeed rendering to a bitmap is a solution. There's a decent tutorial on how to do it here on GameDev.",c++ opengl textures1726122,A,"SDL_image/C++ OpenGL Program: IMG_Load() produces fuzzy images I'm trying to load an image file and use it as a texture for a cube. I'm using SDL_image to do that. I used this image because I've found it in various file formats (tga tif jpg png bmp) The code : SDL_Surface * texture; //load an image to an SDL surface (i.e. a buffer) texture = IMG_Load(""/Users/Foo/Code/xcode/test/lena.bmp""); if(texture == NULL){ printf(""bad image\n""); exit(1); } //create an OpenGL texture object glGenTextures(1 &textureObjOpenGLlogo); //select the texture object you need glBindTexture(GL_TEXTURE_2D textureObjOpenGLlogo); //define the parameters of that texture object //how the texture should wrap in s direction glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_REPEAT); //how the texture should wrap in t direction glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_REPEAT); //how the texture lookup should be interpolated when the face is smaller than the texture glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); //how the texture lookup should be interpolated when the face is bigger than the texture glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); //send the texture image to the graphic card glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA texture->w texture->h 0 GL_RGB GL_UNSIGNED_BYTE texture-> pixels); //clean the SDL surface SDL_FreeSurface(texture); The code compiles without errors or warnings ! I've tired all the files formats but this always produces that ugly result : I'm using : SDL_image 1.2.9 & SDL 1.2.14 with XCode 3.2 under 10.6.2 Does anyone knows how to fix this ? As a note: The reason you find that image in so many formats is because it's the standard image for image compression tests. ""Lenna"" is the name of the image and the model's name is Lena Soderberg. The history of the image is quite interesting. The reason the image is distorted is because it's not in the RGBA format that you've specified. Check the texture->format to find out the format it's in and select the appropriate GL_ constant that represents the format. (Or transform it yourself to the format of your choice.) +1 beat me to it  I think greyfade has the right answer but another thing you should be aware of is the need to lock surfaces. This is probably not the case since you're working with an in-memory surface but normally you need to lock surfaces before accessing their pixel data with SDL_LockSurface(). For example: bool lock = SDL_MUSTLOCK(texture); if(lock) SDL_LockSurface(texture); // should check that return value == 0 // access pixel data e.g. call glTexImage2D if(lock) SDL_UnlockSUrface(texture);  If you have an alpha chanel every pixel is 4 unsigned bytes if you don't it's 3 unsigned bytes. This image has no transpareny and when I try to save it its a .jpg. change glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA texture->w texture->h 0 GL_RGB GL_UNSIGNED_BYTE texture-> pixels); to glTexImage2D(GL_TEXTURE_2D 0 GL_RGB texture->w texture->h 0 GL_RGB GL_UNSIGNED_BYTE texture-> pixels); That should fix it. For a .png with an alpha channel use glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA texture->w texture->h 0 GL_RGBA GL_UNSIGNED_BYTE texture-> pixels); this is actually wrong. the parameter you're modifying is used to tell GL what format to _store_ the texture as. storing to RGBA when your input is RGB is perfectly valid it sets A=1 for all texels.",c++ opengl sdl sdl-image1558505,A,"C++ and OpenGL Help I am wanting to code something with OpenGL but I don't want to have to go through Windows API is this Possible? If so then some links to tutorials on how to do this would be nice. Just keep in mind that graphics processing is very tied to hardware and platform. Anything you use is going to abstract that away from you (perhaps by going through the Windows API when on Windows or another API on other OS's). I reconmend Freeglut its has some nice revisions and is more up to date in fact someone made some updates to this year.  In order to create a Win32 window for displaying OpenGL content - without going through Win32 API GLUT is the only option that I'm aware of.  If you need a complete visualization framework then VTK is the best FREE choice.  Yes. GLFW Qt + OpenGL GLUT or FreeGLUT Or see my question. I'd also add SFML http://www.sfml-dev.org/ SDL is far too slow. Thanks for your help! :) Should probably add SDL to your list. http://www.libsdl.org/ @Anthoni: That's why I didn't recommend it :) I'm personally not very fond of it. Anywho see the ""my question"" link...that lists all of the one's I'm aware of.",c++ opengl724156,A,Keyboard input hesitation when held down? Does anyone know why there is some hesitation when you hold down a keyboard key and try to process it? I'm calling a function right in my WinProc(...) that will move an image on the screen (OpenGL) when a key is held down. I press it and get a single response then there is about .5 seconds of nothing then it behaves as normal (moves 1 pixel every WinMain loop). I'm wondering if the Windows messages are being delayed somehow because of some feature I need to disable??? Here's my code: int WINAPI WinMain(HINSTANCE hinstance HINSTANCE hprevinstance LPSTR lpcmdline int nshowcmd) { bool quit = false; MSG msg; createWindow(hinstance SCRW SCRH SCRD WINDOWED); // Main loop while (!quit) { if (PeekMessage(&msg NULL NULL NULL PM_REMOVE)) { if (msg.message == WM_QUIT) quit = true; TranslateMessage(&msg); DispatchMessage(&msg); } renderFrame(); // processes graphics SwapBuffers(hdc); } return msg.lParam; } and WinProc (there were more cases but same thing...):  LRESULT CALLBACK WinProc(HWND hwnd UINT msg WPARAM wparam LPARAM lparam) { switch(msg) { case WM_KEYDOWN: switch ( wparam ) { case VK_RIGHT: key_RIGHT(); return 0; } return 0; } } and key_RIGHT: void key_RIGHT() { MoveObjectRight1Pixel(); } It's a rather standard keyboard setting to have a small delay between when the key was pressed and when repeat messages get generated. Instead of processing keyboard input in your Windows message handler you could instead keep an array of 256 bits indicating the current state of the keyboard. When you receive a WM_KEYDOWN or WM_KEYUP you update the bit of the corresponding key. Then in your main loop you check the current state of the key and the previous state of the key (by keeping a second array of 256 bits that you make a copy to every frame). If the key is currently down but was not down in the previous frame then you move your object accordingly. Another alternative is to use the GetAsyncKeyState() function. I was afraid I would have to do some silly workaround... There's no way to force Windows to send me messages at a civilized rate? That's a huge keyboard. :) Most standard PC keyboards are around 105 keys so 128 bits sounds like plenty to me. @unwind: True most keyboards have a sane number of keys but the virtual keycodes (VK_* constants) range from 0 to 255.  Well I think you're dealing with a standard windows feature here. Windows usually has a delay before it fires the repititive keystrokes events when thekey is suppressed. I don't know about other methods but on the first key press you could activate a timer which would check whether the key is suppressed every few milliseconds and then process your code accordingly. I think that would solve the problem you're having.,c++ windows opengl input messages1020924,A,"Build errors w/ GLee (GL Easy Extension Library) Using Code::Blocks w/ mingw and trying to use GLee for some OpenGL on windows. I'm getting the following build errors: GLee.c|60|undefined reference to `_wglGetProcAddress@4' GLee.c|10748|undefined reference to `_wglGetProcAddress@4' GLee.c|10751|undefined reference to `_wglGetCurrentDC@0' GLee.c|10797|undefined reference to `_glGetString@4' GLee.c|10910|undefined reference to `_glGetString@4' GLee.c|10976|undefined reference to `_glGetString@4' And I'm just including GLee likes so (with GLee.c not the .dll): #include ""GLee.h"" According to Ben Woodhouse GLee is ""written in pure ANSI C so any C or C++ compiler should work. You can include the source files in your projects directly or compile them into a library"" so I should be having no problems. Google didn't give me much on this so I'm hoping some OpenGL vets (or anyone familiar with GLee) out there can point me in the right direction. It looks like you need to link your application against the OpenGL libraries (specifically Opengl32.lib) which will provide the functions that you are missing. Perhaps the OpenGL FAQ might be of help in figuring this out.",c++ c opengl536168,A,"Why won't my OpenGL draw anything? I'm working on porting my open source particle engine test from SDL to SDL + OpenGL. I've managed to get it compiling and running but the screen stays black no matter what I do. main.cpp: #include ""glengine.h"" int WINAPI WinMain( HINSTANCE hInstance HINSTANCE hPrevInstance LPSTR lpCmdLine int nCmdShow ) { //Create a glengine instance ultragl::glengine *gle = new ultragl::glengine(); if(gle->init()) gle->run(); else std::cout << ""glengine initializiation failed!"" << std::endl; //If we can't initialize or the lesson has quit we delete the instance delete gle; return 0; }; glengine.h: //we need to include window first because GLee needs to be included before GL.h #include ""window.h"" #include <math.h> // Math Library Header File #include <vector> #include <stdio.h> using namespace std; namespace ultragl { class glengine { protected: window m_Window; ///< The window for this lesson unsigned int m_Keys[SDLK_LAST]; ///< Stores keys that are pressed float piover180; virtual void draw(); virtual void resize(int x int y); virtual bool processEvents(); void controls(); private: /* * We need a structure to store our vertices in otherwise we * just had a huge bunch of floats in the end */ struct Vertex { float x y z; Vertex(){} Vertex(float x float y float z) { this->x = x; this->y = y; this->z = z; } }; struct particle { public : double angle; double speed; Vertex v; int r; int g; int b; int a; particle(double angle double speed Vertex v int r int g int b int a) { this->angle = angle; this->speed = speed; this->v = v; this->r = r; this->g = g; this->b = b; this->a = a; } particle() { } }; particle p[500]; float particlesize; public: glengine(); ~glengine(); virtual void run(); virtual bool init(); void glengine::test2(int num); void glengine::update(); }; }; window.h: #include <string> #include <iostream> #include ""GLee/GLee.h"" #include <SDL/SDL.h> #include <SDL/SDL_opengl.h> #include <GL/glu.h> using namespace std; namespace ultragl { class window { private: int w_height; int w_width; int w_bpp; bool w_fullscreen; string w_title; public: window(); ~window(); bool createWindow(int width int height int bpp bool fullscreen const string& title); void setSize(int width int height); int getHeight(); int getWidth(); }; }; glengine.cpp (the main one to look at): #include ""glengine.h"" namespace ultragl{ glengine::glengine() { piover180 = 0.0174532925f; } glengine::~glengine() { } void glengine::resize(int x int y) { std::cout << ""Resizing Window to "" << x << ""x"" << y << std::endl; if (y <= 0) { y = 1; } glViewport(00xy); glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(45.0f(GLfloat)x/(GLfloat)y1.0f100.0f); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } bool glengine::processEvents() { SDL_Event event; while (SDL_PollEvent(&event))//get all events { switch (event.type) { // Quit event case SDL_QUIT: { // Return false because we are quitting. return false; } case SDL_KEYDOWN: { SDLKey sym = event.key.keysym.sym; if(sym == SDLK_ESCAPE) //Quit if escape was pressed { return false; } m_Keys[sym] = 1; break; } case SDL_KEYUP: { SDLKey sym = event.key.keysym.sym; m_Keys[sym] = 0; break; } case SDL_VIDEORESIZE: { //the window has been resized so we need to set up our viewport and projection according to the new size resize(event.resize.w event.resize.h); break; } // Default case default: { break; } } } return true; } bool glengine::init() { srand( time( NULL ) ); for(int i = 0; i < 500; i++) p[i] = particle(0 0 Vertex(0.0f 0.0f 0.0f) 0 0 0 0); if (!m_Window.createWindow(640 480 32 false ""Paricle Test GL"")) { return false; } particlesize = 0.01; glShadeModel(GL_SMOOTH); // Enable Smooth Shading glClearColor(0.0f 0.0f 0.0f 0.5f); // Black Background glClearDepth(1.0f); // Depth Buffer Setup glEnable(GL_DEPTH_TEST); // Enables Depth Testing glDepthFunc(GL_LEQUAL); // The Type Of Depth Testing To Do glEnable(GL_BLEND); glBlendFunc(GL_ONE  GL_ONE_MINUS_SRC_ALPHA); return true; } void glengine::test2(int num) { glPushMatrix(); glTranslatef(p[num].v.x p[num].v.y p[num].v.z); glBegin(GL_QUADS); glColor4i(p[num].r p[num].g p[num].b p[num].a); // Green for x axis glVertex3f(-particlesize -particlesize particlesize); glVertex3f( particlesize -particlesize particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f(-particlesize particlesize particlesize); glEnd(); glPopMatrix(); } void glengine::draw() { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Clear Screen And Depth Buffer glLoadIdentity(); // Reset The Current Modelview Matrix gluLookAt(0 5 20 0 0 0 0 0 0); for(int i = 0; i < 500; i++) test2(i); } void glengine::update() { for(int i = 0; i < 500; i++) { if(p[i].a <= 0) p[i] = particle(5 + rand() % 360 (rand() % 10) * 0.1 Vertex(0.0f 0.0f 0.0f) 0 255 255 255); else p[i].a -= 1; p[i].v.x += (sin(p[i].angle * (3.14159265/180)) * p[i].speed); p[i].v.y -= (cos(p[i].angle * (3.14159265/180)) * p[i].speed); } } void glengine::run() { while(processEvents()) { update(); draw(); SDL_GL_SwapBuffers(); } } }; And finally window.cpp: #include ""window.h"" namespace ultragl { window::window(): w_width(0) w_height(0) w_bpp(0) w_fullscreen(false) { } window::~window() { SDL_Quit(); } bool window::createWindow(int width int height int bpp bool fullscreen const string& title) { if( SDL_Init( SDL_INIT_VIDEO ) != 0 ) return false; w_height = height; w_width = width; w_title = title; w_fullscreen = fullscreen; w_bpp = bpp; //Set lowest possiable values. SDL_GL_SetAttribute(SDL_GL_RED_SIZE 5); SDL_GL_SetAttribute(SDL_GL_GREEN_SIZE 5); SDL_GL_SetAttribute(SDL_GL_BLUE_SIZE 5); SDL_GL_SetAttribute(SDL_GL_ALPHA_SIZE 5); SDL_GL_SetAttribute(SDL_GL_DEPTH_SIZE 16); SDL_GL_SetAttribute(SDL_GL_DOUBLEBUFFER 1); //Set title. SDL_WM_SetCaption(title.c_str() title.c_str()); // Flags tell SDL about the type of window we are creating. int flags = SDL_OPENGL; if(fullscreen == true) flags |= SDL_FULLSCREEN; // Create window SDL_Surface * screen = SDL_SetVideoMode( width height bpp flags ); if(screen == 0) return false; //SDL doesn't trigger off a ResizeEvent at startup but as we need this for OpenGL we do this ourself SDL_Event resizeEvent; resizeEvent.type = SDL_VIDEORESIZE; resizeEvent.resize.w = width; resizeEvent.resize.h = height; SDL_PushEvent(&resizeEvent); return true; } void window::setSize(int width int height) { w_height = height; w_width = width; } int window::getHeight() { return w_height; } int window::getWidth() { return w_width; } }; Anyway I really need to finish this but I've already tried everything I could think of. I tested the glengine file many different ways to where it looked like this at one point: #include ""glengine.h"" #include ""SOIL/SOIL.h"" #include ""SOIL/stb_image_aug.h"" #include ""SOIL/image_helper.h"" #include ""SOIL/image_DXT.h"" namespace ultragl{ glengine::glengine() { piover180 = 0.0174532925f; } glengine::~glengine() { } void glengine::resize(int x int y) { std::cout << ""Resizing Window to "" << x << ""x"" << y << std::endl; if (y <= 0) { y = 1; } glViewport(00xy); glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(45.0f(GLfloat)x/(GLfloat)y1.0f1000.0f); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } bool glengine::processEvents() { SDL_Event event; while (SDL_PollEvent(&event))//get all events { switch (event.type) { // Quit event case SDL_QUIT: { // Return false because we are quitting. return false; } case SDL_KEYDOWN: { SDLKey sym = event.key.keysym.sym; if(sym == SDLK_ESCAPE) //Quit if escape was pressed { return false; } m_Keys[sym] = 1; break; } case SDL_KEYUP: { SDLKey sym = event.key.keysym.sym; m_Keys[sym] = 0; break; } case SDL_VIDEORESIZE: { //the window has been resized so we need to set up our viewport and projection according to the new size resize(event.resize.w event.resize.h); break; } // Default case default: { break; } } } return true; } bool glengine::init() { srand( time( NULL ) ); for(int i = 0; i < 500; i++) p[i] = particle(0 0 Vertex(0.0f 0.0f 0.0f) 0 0 0 0); if (!m_Window.createWindow(640 480 32 false ""Paricle Test GL"")) { return false; } particlesize = 10.01; glShadeModel(GL_SMOOTH); // Enable Smooth Shading glClearColor(0.0f 0.0f 0.0f 0.5f); // Black Background glClearDepth(1.0f); // Depth Buffer Setup glEnable(GL_DEPTH_TEST); // Enables Depth Testing glDepthFunc(GL_LEQUAL); // The Type Of Depth Testing To Do glEnable(GL_BLEND); glBlendFunc(GL_ONE  GL_ONE_MINUS_SRC_ALPHA); return true; } void glengine::test2(int num) { //glPushMatrix(); //glTranslatef(p[num].v.x p[num].v.y p[num].v.z); glColor4i(255 255 255 255); glBegin(GL_QUADS); glNormal3f( 0.0f 0.0f 1.0f); glVertex3f(-particlesize -particlesize particlesize); glVertex3f( particlesize -particlesize particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f(-particlesize particlesize particlesize); glEnd(); // Back Face glBegin(GL_QUADS); glNormal3f( 0.0f 0.0f-1.0f); glVertex3f(-particlesize -particlesize -particlesize); glVertex3f(-particlesize particlesize -particlesize); glVertex3f( particlesize particlesize -particlesize); glVertex3f( particlesize -particlesize -particlesize); glEnd(); // Top Face glBegin(GL_QUADS); glNormal3f( 0.0f 1.0f 0.0f); glVertex3f(-particlesize particlesize -particlesize); glVertex3f(-particlesize particlesize particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f( particlesize particlesize -particlesize); glEnd(); // Bottom Face glBegin(GL_QUADS); glNormal3f( 0.0f-1.0f 0.0f); glVertex3f(-particlesize -particlesize -particlesize); glVertex3f( particlesize -particlesize -particlesize); glVertex3f( particlesize -particlesize particlesize); glVertex3f(-particlesize -particlesize particlesize); glEnd(); // Right face glBegin(GL_QUADS); glNormal3f( 1.0f 0.0f 0.0f); glVertex3f( particlesize -particlesize -particlesize); glVertex3f( particlesize particlesize -particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f( particlesize -particlesize particlesize); glEnd(); // Left Face glBegin(GL_QUADS); glNormal3f(-1.0f 0.0f 0.0f); glVertex3f(-particlesize -particlesize -particlesize); glVertex3f(-particlesize -particlesize particlesize); glVertex3f(-particlesize particlesize particlesize); glVertex3f(-particlesize particlesize -particlesize); glEnd(); //glPopMatrix(); } void glengine::draw() { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Clear Screen And Depth Buffer glLoadIdentity(); // Reset The Current Modelview Matrix gluLookAt(0 5 20 0 0 0 0 1 0); for(int i = 0; i < 500; i++) test2(i); } void glengine::update() { for(int i = 0; i < 500; i++) { if(p[i].a <= 0) p[i] = particle(5 + rand() % 360 (rand() % 10) * 0.1 Vertex(0.0f 0.0f -5.0f) 0 255 255 255); else p[i].a -= 1; p[i].v.x += (sin(p[i].angle * (3.14159265/180)) * p[i].speed); p[i].v.y -= (cos(p[i].angle * (3.14159265/180)) * p[i].speed); } } void glengine::run() { while(processEvents()) { update(); draw(); SDL_GL_SwapBuffers(); } } }; It still didn't work. I'm really at my wits end on this one. I find you get better responses if you can reduce your source code to the absolute minimum example while still having the same issue. People don't want to do the work looking through 4 pages of source when you could reduce the problem to a 1 page simple example. Reduce your program to as simple a drawing program as possible. Setup your OpenGL window viewport etc and attempt to draw a single line. If it doesn't show up post that code here. Then when people see the issue with that you can apply the fix to your bigger problem. I haven't checked your code but one thing I always do when debugging this kind of problems is to set the clear color to something colorful like (1 0 1) or so. This will help you see if the problem is that your drawn object is completely black or if it's not drawn at all. EDIT: As someone mentioned in the comment: It also shows if you have a correct GL context if the clear operation clears to the right color or if it stays black. And to see if you've created a proper context where at least glClear works.  You're not checking the return values of the SDL-GL-SetAttribute() calls. And is 5/5/5/5 20-bpp color supported by your video card? agreed it's better to avoid being so specific without having a working environment yet!  Okay I managed to fix it using a lot of your suggestions and some other source code I had laying around. Turns out the problem was from 3 different lines. particlesize = 0.01; should have been bigger: particlesize = 1.01; glColor4i(255 255 255 255) was turning the cube the same color as the clear color because I was using it wrong. I couldn't figure out how to use it right so I'm using glColor4f(0.0f1.0f1.0f0.5f) instead and that works. Last of all gluLookAt(0 5 20 0 0 0 0 0 0) needed to be gluLookAt(0 5 20 0 0 0 0 1 0) Thank you all for your help and your time.  Check OpenGL for error states. Use glslDevil glIntercept or gDebugger. Check the glGetError function. Can you test whether SDL actually acquired a device context? Does the Window reflect changes in the glClearColor call? Don't use 0.5 as an alpha value in glClearColor. Try these suggestions and report back with a minimal example as Simucal suggested.",c++ opengl sdl1252680,A,"OpenGL Windowing Library for 2009 Trying to decide on a library for creating a window and capturing user input for my OpenGL app but there are just way too many choices: GLUT (win32) FreeGLUT OpenGLUT SFML GLFW SDL FLTK OGLWFW Clutter Qt Others? GLUT is simply outdated. I liked GLFW but it seems you can't set the window position before displaying it (I wanted it centered is that so much to ask?) so you see it appear and then shift over which bothers me. Plus development seems to have stopped on it too. SFML has some nice features but it uses event polling rather than callbacks which I prefer for decoupling. I don't think I need all the GUI features of FLTK. SDL is slow (doesn't seem to take advantage of the GPU). And the other 3 I don't know much about (FreeGLUT OpenGLUT OGLWFW). So which is the lesser of the evils? Are there others I haven't heard about? I'm just trying to make a simple 2D game. I'm familiar enough with OpenGL that I don't really need drawing routines but I probably wouldn't complain about other functions that might be useful if they are implemented properly. OGLWFW seems to be Win32-only at the moment. The website says that Linux/Mac support is ""planned."" You haven't said whether licensing or cross-platform support is an issue - if it is you might want to make note of that. Oh. I must have glossed over that. Cross-platform is nice... especially since I'm developing on Ubuntu right now. Licensing is only a minor concern as I never seem to complete anything anyways... I never get to the point where I'm capable of selling it. I'd go for Qt. Nice general purpose library + opengl support Got something simple up and running now... starting to really like it :) Not only does it have perty callbacks... they're magically already registered for me. Seems to force you to separate your code which is probably a good thing or I'd probably have a big mess already :D Right...forgot about Qt. Steep learning curve though no? Tried using it once before completely baffled me. Hrm... at least the editor it came with was (confusing). NetBeans 6.8 seems to have some level of support for Qt so getting a simple app up and running wasn't so bad. I use Qt with Visual Studio on Windows and Vim/make on *nix boxes. It has really good documentation and clean API. I think there are tutorials for using it with IDEs like Eclipse/NetBeans/etc.. Just google for them  SDL allows you to create an OpenGL context that is accelerated (depending on drivers / hardware support). I know you tagged as C++ however pygame (python) is a great library for creating 2D games which also supports an OpenGL context. Pygame is built on SDL. Clutter is a new OpenGL based GUI library with bindings for Perl Python C# C++ Vala and Ruby. I haven't used it myself. From the website: Clutter uses OpenGL (and optionally OpenGL ES for use on Mobile and embedded platforms) for rendering but with an API which hides the underlying GL complexity from the developer. The Clutter API is intended to be easy to use efficient and flexible. Clutter seems rather confusing. Do you know any open sourced programs that are written in clutter? @the_drow: I think the most high profile open-source user of Clutter right now is the Intel Moblin netbook OS: moblin.org. SDL is still C-esque (requires cleanup) and uses event polling rather than callbacks and doesn't seem to have very good integration with OpenGL (requires the usage of SDL wrapper functions?). Once you set up the OpenGL context in pygame or SDL you can use any OpenGL you want. I have an OpenGL app that basically used SDL for windowing and event polling and OpenGL for the graphics. Well yeah... I'm not saying it doesn't work just is going to take a lot more convincing to beat out the other libraries.  IF ""learning c++ part of what you're trying to achieve"": then IF ""you only want to learn OpenGL with a fullscreen mode"": USE GLUT //Because it's VERY VERY simple. You can get set up VERY quick ELSE: USE QT //Great library has many many things that will help you. It is portable it has a nice API ENDIF IF ""you don't need C++"": then USE Python //I recommend it it is fast no long link times good api omg I love this language Background: I also tried to make simple 2D games once I started with C++ and NeHe. I knew nothing about OpenGL and C++ (had Java background). The language overrun me so did OpenGL. So it was a very hard learning curve. I don't recommend going that way since you can get faster results by using a dynamic language (such as Python). So I started learning some years later with python. I could get the ""rotating cubes"" working much faster. Well I personally already know both C++ and OpenGL so that's not really a problem. I just needed a cross-platform windowing API. All the other tidbits were a nice touch on Qt ;)  GLUT and the other GLUT alternatives should not be used in any sort of production application. They are good for putting together a quick demo application or to try something out but not for much more than that. If you're trying to make an OpenGL game I'd recommend SDL. It focuses more on gaming needs. It most definitely can be used with OpenGL. A brief google for ""SDL OpenGL"" turned up this link on how to initialize OpenGL with SDL. Enabling OpenGL should also enable hardware rendering with the GPU. Qt is a reasonable alternative but it's better if you want to embed OpenGL within a larger desktop application (think 3D modeling CAD/CAM medical visualization etc) where you need access to standard OS widgets for the UI. Yeah... but Qt also seems so much more modern and well designed! My game is going to have an editor... I *can* make use of those extra widgets :p I'll consider SDL for future projects though.  We have had rather good experiences with ClanLib 0.8 in 2008 and ClanLib 2.1 in 2009 on our C++ course. The productivity of the students (as measured by the quality of their project works) has greatly increased since switching over from SDL. However it needs to be noted that 2.1 is still very incomplete and one will certainly run into features that are simply not implemented yet. A couple of groups used Irrlicht (3D engine) with good results. SFML looks promising but I haven't had a chance to try it yet. As others have stated GLUT is not really suitable for anything serious. The rest of the libraries mentioned are something more of GUI toolkits than game development libraries.  Per recent corespondance with the author development on OGLWFW has stopped. Good to know. They all seem to be dying out... guess you gotta go for a big professionally developed framework these days.",c++ opengl154730,A,"Capturing video out of an OpenGL window in Windows I am supposed to provide my users a really simple way of capturing video clips out of my OpenGL application's main window. I am thinking of adding buttons and/or keyboard shortcuts for starting and stopping the capture; when starting I could ask for a filename and other options if any. It has to run in Windows (XP/Vista) but I also wouldn't like to close the Linux door which I've so far been able to keep open. The application uses OpenGL fragment and shader programs the effects due to which I absolutely need to have in the eventual videos. It looks to me like there might be even several different approaches that could potentially fulfill my requirements (but I don't really know where I should start): An encoding library with functions like startRecording(filename) stopRecording and captureFrame. I could call captureFrame() after every frame rendered (or every second/third/whatever). If doing so makes my program run slower it's not really a problem. A standalone external program that can be programmatically controlled from my application. After all a standalone program that can not be controlled almost does what I need... But as said it should be really simple for the users to operate and I would appreciate seamlessness as well; my application typically runs full-screen. Additionally it should be possible to distribute as part of the installation package for my application which I currently prepare using NSIS. Use the Windows API to capture screenshots frame-by-frame then employ (for example) one of the libraries mentioned here. It seems to be easy enough to find examples of how to capture screenshots in Windows; however I would love a solution which doesn't really force me to get my hands super-dirty on the WinAPI level. Use OpenGL to render into an offscreen target then use a library to produce the video. I don't know if this is even possible and I'm afraid it might not be the path of least pain anyway. In particular I would not like the actual rendering to take a different execution path depending on whether video is being captured or not. Additionally I would avoid anything that might decrease the frame rate in the normal non-capture mode. If the solution were free in either sense of the word then that would be great but it's not really an absolute requirement. In general the less bloat there is the better. On the other hand for reasons beyond this question I cannot link in any GPL-only code unfortunately. Regarding the file format I cannot expect my users to start googling for any codecs but as long as also displaying the videos is easy enough for a basic-level Windows user I don't really care what the format is. However it would be great if it were possible to control the compression quality of the output. Just to clarify: I don't need to capture video from an external device like camcorder nor am I really interested in mouse movements even though getting them does not harm either. There are no requirements regarding audio; the application makes no noise whatsoever. I write C++ using Visual Studio 2008 for this very application also taking benefit of GLUT and GLUI. I have a solid understanding regarding C++ and linking in libraries and that sort of stuff but on the other hand OpenGL is quite new for me: so far I've really only learnt the necessary bits to actually get my job done. I don't need a solution super-urgently so feel free to take your time :) I had to create a demo project of recording an OpenGL rendering into a video. I used glReadPixels to get the pixel data and created the video with OpenCV's cvWriteFrame. OpenCV lets you write in divx or even x264/vp8(with ffmpeg compiled in). I have a more detailed writeup on my blog post along with a sample project. http://tommy.chheng.com/2013/09/09/encode-opengl-to-video-with-opencv/ Hi bluefeet the essential part of the answer is present: ""use glReadPixels to get the pixel data"" and use OpenCV's ""cvWriteFrame"". The blog post goes into more detail but that's the gist of the answer. Thanks for posting your answer! Please note that you should post the essential parts of the answer here on this site or your post risks being deleted [See the FAQ where it mentions answers that are 'barely more than a link'.](http://stackoverflow.com/faq#deletion) You may still include the link if you wish but only as a 'reference'. The answer should stand on its own without needing the link.  The easiest option is going to be saving each rendered frame from within your app and then merging them into an AVI. When you have the AVI there are many libraries available that can convert it into a more optimal format or possibly skip the AVI step altogether. In terms of getting each frame you could accomplish this either by rendering into an offscreen texture as you suggest or using the backbuffer directly as a source if your hardware supports this. Doing either of these (and saving each frame) is going to be difficult without a heavy penalty on framerate. Providing your application is deterministic you could ""record"" the users actions as a series of inputs and then have an export mode that sequentially renders these to an offscreen surface to generate the AVI. The frame rate penalty is quite acceptable as long as it applies only when really capturing a video. The app is deterministic in the sense of not being stochastic but what gets displayed is based in addition to user actions also on results from an external measurement device.  There are two different questions here - how to grab frames from an OpenGL application and how to turn them into a movie file. The first question is easy enough; you just grab each frame with glReadPixels() (via a PBO if you need the performance). The second question is a little harder since the cross-platform solutions (ffmpeg) tend to be GPL'd or LGPL'd. Is LGPL acceptable for your project? The Windows way of doing this (DirectShow) is a bit of a headache to use. Edit: Since LGPL is ok and you can use ffmpeg see here for an example of how to encode video. Yes: LGPL is ok. glReadPixels() indeed seems to be what I need for the first question thanks! @MikeF: that link is bust  This does look pretty relevant for merging into an AVI (as suggested by Andrew) however I was really hoping to avoid the LPBITMAPINFOHEADERs etc. Thanks for the answers I will report on the success if there is going to be any :) In the meantime additional tips for encoding the raw frames from glReadPixels into video clips would be appreciated. Edit: So far ffmpeg suggested by Mike F seems to be the way to go. However I didn't get into the actual implementation yet but hopefully that will change in the near future! Be aware that using VfW and DirectShow can use only whatever codecs are installed on the user's machine so you need to either bundle the codec you plan to use or find out which codecs they're *guaranteed* to already have.",c++ opengl video screenshot video-capture1531023,A,"moving a point in 3d space I have a point at 000 I rotate the point 30 deg around the Y axis then 30 deg around the X axis. I then want to move the point 10 units forward. I know how to work out the new X and Y position MovementX = cos(angle) * MoveDistance; MovementY = sin(angle) * MoveDistance; But then I realised that these values will change because of Z won't they? How do I work out Z and have I worked out X and Y correctly? Thanks! Use rotation matrices. :) They take a considerable investment of effort to learn but once you know how to use them everything becomes much less work. Well a point at (000) rotated at any angle at the xy or z axis will stay a (000). Assuming by ""forward"" you mean the direction pointing along the z-axis the components of the movement vector become: MovementX = 0; MovementY = 0; MovementZ = 10. Seriously try to ask exactly what you want to know. Ok I guess the equivalent in OpenGL would be... glRotatef( 30.0f 0.0f 1.0f 0.0f ); glRotatef( 30.0f 1.0f 0.0f 0.0f ); glTranslate( 0.0f 0.0f 10.0f ); But I want to do it manually so that I know the world coordinates of the point (so that I can compare it to other coordinates) I don't know too much about OpenGL but if your point is starting at (000) then drhirsch is correct and your point won't be moving anywhere through rotations. So the only function that would do anything is the glTranslate. Therefore your new position would be (0010). However I feel like that might not be what you were going for. Are you sure your point doesn't have a direction vector associated with it? You should multiply point coordinates to full rotation matrix which is matRotationTotal = matRotationX * matRotationY * matRotationZ. Check this article for details.",c++ math opengl coordinates1351129,A,Calculating 3D tangent space In order to use normal mapping in GLSL shaders you need to know the normal tangent and bitangent vectors of each vertex. RenderMonkey makes this easy by providing it's own predefined variables (rm_tangent and rm_binormal) for this. I am trying to add this functionality to my own 3d engine. Apparently it is possible to calculate the tangent and bi tangent of each vertex in a triangle using each vertex's xyz coordinates uv texture coordinates and normal vector. After some searching I devised this function to calculate the tangent and bitangent for each vertex in my triangle structure. void CalculateTangentSpace(void) { float x1 = m_vertices[1]->m_pos->Get(0) - m_vertices[0]->m_pos->Get(0); float x2 = m_vertices[2]->m_pos->Get(0) - m_vertices[0]->m_pos->Get(0); float y1 = m_vertices[1]->m_pos->Get(1) - m_vertices[0]->m_pos->Get(1); float y2 = m_vertices[2]->m_pos->Get(1) - m_vertices[0]->m_pos->Get(1); float z1 = m_vertices[1]->m_pos->Get(2) - m_vertices[0]->m_pos->Get(2); float z2 = m_vertices[2]->m_pos->Get(2) - m_vertices[0]->m_pos->Get(2); float u1 = m_vertices[1]->m_texCoords->Get(0) - m_vertices[0]->m_texCoords->Get(0); float u2 = m_vertices[2]->m_texCoords->Get(0) - m_vertices[0]->m_texCoords->Get(0); float v1 = m_vertices[1]->m_texCoords->Get(1) - m_vertices[0]->m_texCoords->Get(1); float v2 = m_vertices[2]->m_texCoords->Get(1) - m_vertices[0]->m_texCoords->Get(1); float r = 1.0f/(u1 * v2 - u2 * v1); Vec3<float> udir((v2 * x1 - v1 * x2) * r (v2 * y1 - v1 * y2) * r (v2 * z1 - v1 * z2) * r); Vec3<float> vdir((u1 * x2 - u2 * x1) * r (u1 * y2 - u2 * y1) * r (u1 * z2 - u2 * z1) * r); Vec3<float> tangent[3]; Vec3<float> tempNormal; tempNormal = *m_vertices[0]->m_normal; tangent[0]=(udir-tempNormal*(Vec3Dot(tempNormal udir))); m_vertices[0]->m_tangent=&(tangent[0].Normalize()); m_vertices[0]->m_bitangent=Vec3Cross(m_vertices[0]->m_normal m_vertices[0]->m_tangent); tempNormal = *m_vertices[1]->m_normal; tangent[1]=(udir-tempNormal*(Vec3Dot(tempNormal udir))); m_vertices[1]->m_tangent=&(tangent[1].Normalize()); m_vertices[1]->m_bitangent=Vec3Cross(m_vertices[1]->m_normal m_vertices[1]->m_tangent); tempNormal = *m_vertices[2]->m_normal; tangent[2]=(udir-tempNormal*(Vec3Dot(tempNormal udir))); m_vertices[2]->m_tangent=&(tangent[2].Normalize()); m_vertices[2]->m_bitangent=Vec3Cross(m_vertices[2]->m_normal m_vertices[2]->m_tangent); } When I use this function and send the calculated values to my shader the models look almost like they do in RenderMonkey but they flicker in a very strange way. I traced the problem to the tangent and bitangent I am sending OpenGL. This leads me to suspect that my code is doing something wrong. Can anyone see any problems or have any suggestions for other methods to try? I should also point out that the above code is very hacky and I have very little understanding of the math behind what is going on. Found the solution. Much simpler (but still a little hacky) code: void CalculateTangentSpace(void) { float x1 = m_vertices[1]->m_pos->Get(0) - m_vertices[0]->m_pos->Get(0); float y1 = m_vertices[1]->m_pos->Get(1) - m_vertices[0]->m_pos->Get(1); float z1 = m_vertices[1]->m_pos->Get(2) - m_vertices[0]->m_pos->Get(2); float u1 = m_vertices[1]->m_texCoords->Get(0) - m_vertices[0]->m_texCoords->Get(0); Vec3<float> tangent(x1/u1 y1/u1 z1/u1); tangent = tangent.Normalize(); m_vertices[0]->m_tangent = new Vec3<float>(tangent); m_vertices[1]->m_tangent = new Vec3<float>(tangent); m_vertices[2]->m_tangent = new Vec3<float>(tangent); m_vertices[0]->m_bitangent=new Vec3<float>(Vec3Cross(m_vertices[0]->m_normal m_vertices[0]->m_tangent)->Normalize()); m_vertices[1]->m_bitangent=new Vec3<float>(Vec3Cross(m_vertices[1]->m_normal m_vertices[1]->m_tangent)->Normalize()); m_vertices[2]->m_bitangent=new Vec3<float>(Vec3Cross(m_vertices[2]->m_normal m_vertices[2]->m_tangent)->Normalize()); } Sorry but that's just wrong. What you do is simply scale the edge v01 by 1/u1. Not only could u1 be zero but it obviously is not correct. For a correct answer refer to http://stackoverflow.com/questions/5255806/how-to-calculate-tangent-and-binormal  You will get zero division in your 'r' calculation for certain values of u1 u2 v1 and v2 resulting in unknown behavior for 'r'. You should guard against this. Figure out what 'r' should be if the denominator is zero and that MIGHT fix your problem. I too have little understanding of the math behind this. Suggested implementation that sets r = 0 if denominator is zero: #include <cmath> ... static float PRECISION = 0.000001f; ... float denominator = (u1 * v2 - u2 * v1); float r = 0.f; if(fabs(denominator) > PRECISION) { r = 1.0f/denominator; } ... Thanks for the reply. Unfortunately your suggestion doesn't help.,c++ opengl shader glsl1715027,A,"DDS DXT1 loading in OpenGL crashes Any idea whats wrong with my code? when i execute glCompressedTexImage2D() the program just crashes (comes the Windows XP crash message thing...) Im trying to load DDS image without mipmaps the image format is DDS DXT1 Am i missing some include file or what did i do wrong? I downloaded the included files from: http://sourceforge.net/projects/glew/files/glew/1.5.1/glew-1.5.1-win32.zip/download I have glew32.dll in the same folder as my .exe is. The code below has only the parts i changed to be able to load DDS images: #pragma comment(lib ""glew32.lib"") #include <GL\glew.h> #include <GL\gl.h> ... typedef struct { GLuint dwSize; GLuint dwFlags; GLuint dwFourCC; GLuint dwRGBBitCount; GLuint dwRBitMask; GLuint dwGBitMask; GLuint dwBBitMask; GLuint dwABitMask; } DDS_PIXELFORMAT; typedef struct { GLuint dwMagic; GLuint dwSize; GLuint dwFlags; GLuint dwHeight; GLuint dwWidth; GLuint dwLinearSize; GLuint dwDepth; GLuint dwMipMapCount; GLuint dwReserved1[11]; DDS_PIXELFORMAT ddpf; GLuint dwCaps; GLuint dwCaps2; GLuint dwCaps3; GLuint dwCaps4; GLuint dwReserved2; } DDS_HEADER; DDS_HEADER DDS_headers; ... FILE *fp = fopen(""test.dds"" ""rb""); fread(&DDS_headers 1 sizeof(DDS_headers) fp); img_width = DDS_headers.dwWidth; img_height = DDS_headers.dwHeight; maxsize = (img_width*img_height)/2; unsigned char *imgdata = (unsigned char *)malloc(maxsize); fread(imgdata 1 maxsize fp); fclose(fp); GLuint texID; glGenTextures(1 &texID); glBindTexture(GL_TEXTURE_2D texID); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glCompressedTexImage2D(GL_TEXTURE_2D 0 GL_PALETTE4_R5_G6_B5_OES img_width img_height 0 maxsize imgdata); // NOTICE: // Ive also tried with function: // glCompressedTexImage2DARB(); // and internalformats: GL_COMPRESSED_RGB_S3TC_DXT1_EXT and all of the possible formats... its not a format error. please if you want DXT1 put DXT1 in the glCompressedTexImage2D call. There's no point in keeping PALETTE in there. Now what at the time of the glCompressedTexImage2D call what are the values of img_width/img_height/maxsize/imgdata ? i think the problem was at initialization/includefiles ive got the DDS loading working now by some other code thanks for help anyways. I remember messing with the DDS loader in glew I don't think the header information there is correct. I never could get it to work correctly. The best way would be to use the header contstucts that are in DDraw.h here's what I was able to use for DXT13and 5 which if I remember correctly are the only ones that can work in OpenGL.  struct DDS_IMAGE_DATA { GLsizei width; GLsizei height; GLint components; GLenum format; int numMipMaps; GLubyte *pixels; }; DDS_IMAGE_DATA* CImage::loadDDSTextureFile( const char *filename ) { DDS_IMAGE_DATA *pDDSImageData; DDSURFACEDESC2 ddsd; char filecode[4]; FILE *pFile; int factor; int bufferSize; // Open the file pFile = fopen( filename ""rb"" ); if( pFile == NULL ) { #if DEBUG char str[255]; printf( str ""loadDDSTextureFile couldn't find or failed to load \""%s\"""" filename ); #endif return NULL; } // Verify the file is a true .dds file fread( filecode 1 4 pFile ); if( strncmp( filecode ""DDS "" 4 ) != 0 ) { #if DEBUG char str[255]; printf( str ""The file \""%s\"" doesn't appear to be a valid .dds file!"" filename ); #endif fclose( pFile ); return NULL; } // Get the surface descriptor fread( &ddsd sizeof(ddsd) 1 pFile ); pDDSImageData = (DDS_IMAGE_DATA*) malloc(sizeof(DDS_IMAGE_DATA)); memset( pDDSImageData 0 sizeof(DDS_IMAGE_DATA) ); // // This .dds loader supports the loading of compressed formats DXT1 DXT3 // and DXT5. // switch( ddsd.ddpfPixelFormat.dwFourCC ) { case FOURCC_DXT1: // DXT1's compression ratio is 8:1 pDDSImageData->format = GL_COMPRESSED_RGBA_S3TC_DXT1_EXT; factor = 2; break; case FOURCC_DXT3: // DXT3's compression ratio is 4:1 pDDSImageData->format = GL_COMPRESSED_RGBA_S3TC_DXT3_EXT; factor = 4; break; case FOURCC_DXT5: // DXT5's compression ratio is 4:1 pDDSImageData->format = GL_COMPRESSED_RGBA_S3TC_DXT5_EXT; factor = 4; break; default: #if DEBUG char str[255]; printf( str ""The file \""%s\"" doesn't appear to be compressed "" ""using DXT1 DXT3 or DXT5!"" filename ); #endif return NULL; } // // How big will the buffer need to be to load all of the pixel data // including mip-maps? // if( ddsd.dwLinearSize == 0 ) { #if DEBUG printf(""dwLinearSize is 0!""); #endif } if( ddsd.dwMipMapCount > 1 ) bufferSize = ddsd.dwLinearSize * factor; else bufferSize = ddsd.dwLinearSize; pDDSImageData->pixels = (unsigned char*)malloc(bufferSize * sizeof(unsigned char)); fread( pDDSImageData->pixels 1 bufferSize pFile ); // Close the file fclose( pFile ); pDDSImageData->width = ddsd.dwWidth; pDDSImageData->height = ddsd.dwHeight; pDDSImageData->numMipMaps = ddsd.dwMipMapCount; if( ddsd.ddpfPixelFormat.dwFourCC == FOURCC_DXT1 ) pDDSImageData->components = 3; else pDDSImageData->components = 4; return pDDSImageData; } void CImage::loadDDS(const char * szFilename tTexture & texData) { DDS_IMAGE_DATA *pDDSImageData = loadDDSTextureFile(szFilename); if( pDDSImageData != NULL ) { texData.m_nHeight = pDDSImageData->height; texData.m_nWidth = pDDSImageData->width; texData.m_nHeight = pDDSImageData->height; texData.m_eFormat = pDDSImageData->format; int nHeight = pDDSImageData->height; int nWidth = pDDSImageData->width; int nNumMipMaps = pDDSImageData->numMipMaps; int nBlockSize; if( pDDSImageData->format == GL_COMPRESSED_RGBA_S3TC_DXT1_EXT ) nBlockSize = 8; else nBlockSize = 16; //glGenTextures( 1 &g_compressedTextureID ); //glBindTexture( GL_TEXTURE_2D g_compressedTextureID ); glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR ); glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR ); int nSize; int nOffset = 0; // Load the mip-map levels for( int i = 0; i < nNumMipMaps; ++i ) { if( nWidth == 0 ) nWidth = 1; if( nHeight == 0 ) nHeight = 1; nSize = ((nWidth+3)/4) * ((nHeight+3)/4) * nBlockSize; glCompressedTexImage2DARB( GL_TEXTURE_2D i pDDSImageData->format nWidth nHeight 0 nSize pDDSImageData->pixels + nOffset ); nOffset += nSize; // Half the image size for the next mip-map level... nWidth = (nWidth / 2); nHeight = (nHeight / 2); } } if( pDDSImageData != NULL ) { if( pDDSImageData->pixels != NULL ) free( pDDSImageData->pixels ); free( pDDSImageData ); } } This particular bit of code makes a few assumptions of the DDS file we are trying to load first that is is a compressed file either DXT13 or 5 and that the DDS file has pre-generated mipmaps saved in it hence we don't have to generate them ourselves. I hope this was able to help you it took me some time to get it working correctly myself. If there's anything in this code snippet that seems unclear let me know and I'll help you further. Thanks for your help but i've tested my headers and they are fine i copied them from MSDN and since i only use width/height in my code there cant be header related errors plus i've tested with static numbers without using headers at all. I noticed your code has RGBA so i tried with it too but that didnt make any difference... I cant test your code because i dont have the needed type definitions: CImage tTexture DDSURFACEDESC2 FOURCC_DXT1 FOURCC_DXT3 FOURCC_DXT5. Anyways cant i use DDS images unless i make miplevels in them?  The format argument that you're passing to glCompressedTexImage2D looks a bit odd: glCompressedTexImage2D(GL_TEXTURE_2D 0 GL_PALETTE4_R5_G6_B5_OES img_width img_height 0 maxsize imgdata); I think this is neither a paletted texture nor an OpenGL ES extension. Perhaps something like GL_COMPRESSED_RGB_S3TC_DXT1_EXT would work better. glewIsSupported(""EXT_texture_compression_dxt1"") returns false :/ what can i do to get this thing working? Woops I forgot the GL_ prefix; try GL_EXT_texture_compression_dxt1. If it still returns false then your platform doesn't support DXT1 compression so you'll need to convert your source data to some other format. yeah still returns false i dont quite understand how this is going to help me to make this DDS thing to work? what code you use to use DDS images in your applications? can you share it? my GFX card surely supports DDS thats a fact. i have tried with that too and all the possible format types you can give there every of them results in a crash. Make sure your OpenGL implementation supports the DXT1 extension. You can do so by calling the glewIsSupported function and passing in the following string: ""EXT_texture_compression_dxt1"".",c++ opengl directdraw dds-format328019,A,"How to load bmp into GLubyte array? All I am trying to load up a bmp file into a GLubyte array (without using aux). It is unbelievable how what I thought would have been a trivial task is sucking up hours of my time. Can't seem to find anything on Google! This is what I hacked together but it's not quite working: // load texture GLubyte *customTexture; string fileName(""C:\\Development\\Visual Studio 2008\\Projects\\OpenGL_Test\\Crate.bmp""); // Use LoadImage() to get the image loaded into a DIBSection HBITMAP hBitmap = (HBITMAP)LoadImage( NULL (LPCTSTR)const_cast<char*>(fileName.c_str()) IMAGE_BITMAP 0 0 LR_CREATEDIBSECTION | LR_DEFAULTSIZE | LR_LOADFROMFILE ); customTexture = new GLubyte[3*256*256]; // size should be the size of the .bmp file GetBitmapBits(hBitmap 3*256*256 (LPVOID) customTexture); GetBitmapDimensionEx(hBitmap&szBitmap); What happens is the call to LoadImage seems to be returning Undefined Value (NULL? I am not able to figure out if it's actually loading the bmp or not - a bit confused). At the moment I am converting bmps to raw then it's all easy. Anyone has any better and cleaner snippet? LoadImage() can only load bitmaps that are embedded into your executable file with the resource compiler - it can't load external bitmaps from the filesystem. Fortunately bitmap files are really simple to read yourself. See Wikipedia for a description of the file format. Just open up the file like you would with any other file (important: open it in binary mode i.e. with the ""rb"" option using fopen or the ios::binary flag using the C++ ifstream) read in the bitmap dimensions and read in the raw pixel data. Thanks for your remarks on LoadImage - I know how to open the file with ifstream - I was kinda hoping I could find some snippet instead of going there and figure out from scratch how to read byte-by-byte (I even think I already did it smt like N years ago) since it sounds like a common task  It is a common task that's why glaux among others gives you functions for it. Reading a bitmap is a trivial matter especially if there is only one depth/bpp to account for. Also see this question. I am not using glaux. Since it's a common task I'd expect someone to show me how to get it done or to point out some useful examples on the web since I couldn't find anything. At the moment I am converting bmp to raw.",c++ opengl glut1645309,A,"Help me evaluate this casting I found this in the PowerVR mesh drawing code and I don't really know how to read it. &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]] What is going on here? Is this a reference to void cast as an unsigned short pointer and then offset by (3*mesh(etc...) + batchNum)? It's breaking my brain. It's found in the context of a glDrawElements call: glDrawElements(GL_TRIANGLES i32Tris * 3 GL_UNSIGNED_SHORT &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]]); It's found in the context of a glDrawElements call: glDrawElements(GL_TRIANGLES i32Tris * 3 GL_UNSIGNED_SHORT &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]]); Its an obfuscated way of computing sizeof(unsigned short) * 3 * mesh.sBoneBatches.pnBatchOffset[batchNum] but since it doesn't actually save any characters its not a very good obfuscation Only one caveat - it returns a pointer to address N not N as a number.  It's computing a byte offset -- 3 * mesh.sBoneBatches.pnBatchOffset[batchNum] is the index. Using 0 as the pointer means that the address will be just the offset value nothing else. In essence it's `3 * mesh.sBoneBatches.pnBatchOffset[batchNum] * sizeof(unsigned short)` (though with a different type) assuming that `unsigned short[]` arrays are aligned naturally with no padding. I dont get it . Shouldn't ""((unsigned short*)0)[...]"" dereference a null pointer thus leading in unexpected behavior ? @TheSamFrom1984: No because dereferencing is not needed to take the address. This is a common trick in C and is theoretically illegal in C++ but generally works anyways.  Let's go from the inside out. (unsigned short*)0 This is casting 0 to an unsigned short pointer. This will be used for computing a memory offset computed in terms of the size of an unsigned short. 3 * mesh.sBoneBatches.pnBatchOffset[batchNum] This is presumably the offset in memory of some batch of triangles. A triangle is composed of 3 shorts so it looks like they are storing an offset in terms of numbers of triangles and then multiplying by 3 to get the number of shorts. ((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]] This is now using that 0 pointer to find the memory location of the given offset. This would normally return the value of that memory location but they want a pointer to pass into glDrawElements so the use the & operator to get a pointer to that memory location: &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]] why isn't the applicaiton crashing at step 3 ? (attempt to get the value of a bad memory location) Because of the & operator the value is never accessed. Just the address of the value. It didn't know about this particularity of the language. Doesn't seem ""natural"" to me. Thanks. That was very helpful thank you. One more question is a 0 pointer the same concept as a void pointer? No but C mandates that the NULL pointer converts to an integral 0 and vice versa and C++ mandates that NULL is represented by 0.  Really this kind of hack is due to OpenGL expressing offsets inside Buffer Objects through the pointer argument of glDrawElements. glDrawElements(mode count type void* indices) indices represents either a client-side memory pointer or a server-side memory offset based on the binding of GL_ELEMENT_ARRAY_BUFFER_ARB It's interesting to dig a bit deeper... From the VBO specification: Is it legal C to use pointers as offsets? We haven't come to any definitive conclusion about this. [...] Varying opinions have been expressed as to whether this is legal although no one could provide an example of a real system where any problems would occur.",c++ opengl1617370,A,"OpenGL alpha transparency I am starting to use opengl and I wanted to try alpha transparency. Here's my code: void display(void); int main(int argc char** argv) { glutInit(&argc argv); glutInitDisplayMode(GLUT_SINGLE|GLUT_RGBA); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glEnable( GL_BLEND ); glutInitWindowSize(600600); glutInitWindowPosition(20050); glutCreateWindow(""glut test""); glutDisplayFunc(display); glutMainLoop(); return 0; } void display() { glClear(GL_COLOR_BUFFER_BIT); glPointSize(8); glBegin(GL_POINTS); glColor4f(.23.78.321.0); glVertex2f(00); glColor4f(.23.78.320.1); glVertex2f(0.10); glEnd(); glFlush(); } The problem is that these two points appear identical (even when I set the alpha to 0). Is there something I missed to enable alpha transparency? Thank you have you glEnable'd alpha blending? And have you set up your blend parameters? You can't just set the alpha you need to setup various other parameters in OpenGL. glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glEnable( GL_BLEND ); OpenGL requires most everything to be enabled. A good place to start looking. (Disabling and enabling different operations at different times can lead to the most interesting effects.) Thanks I added them but I forgot to set the background color This is really helpful for a beginner Thank you~  Just a guess but could it be that you dont have a background color ? So when your rendering the second vertex which has alpha 0.1 there is no background to compute the proper color ? Just a guess been years since i used opengl. Yep this was it. I should've added: `glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glEnable( GL_BLEND ); glClearColor(0.00.00.00.0);` but only after glutCreateWindow()",c++ opengl transparency alpha751840,A,How do I make textures transparent in OpenGL? I've tried to research this on Google but there doesn't appear to me to be any coherent simple answers. Is this because it's not simple or because I'm not using the correct keywords? Nevertheless this is the progress I've made so far. Created 8 vertices to form 2 squares. Created a texture with a 200 bit alpha value (so about 80% transparent). Assigned the same texture to each square which shows correctly. Noticed that when I use a texture with 255 alpha it appears brighter. The init is something like the following: glClearColor(0.0 0.0 0.0 0.0); glShadeModel(GL_FLAT); glEnable(GL_DEPTH_TEST); glEnable(GL_CULL_FACE); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glPixelStorei(GL_UNPACK_ALIGNMENT 1); glGenTextures(1 textureIds); glTexEnvf(GL_TEXTURE_ENV GL_TEXTURE_ENV_MODE GL_REPLACE); int i j; GLubyte pixel; for (i = 0; i < TEXTURE_HEIGHT; i++) { for (j = 0; j < TEXTURE_WIDTH; j++) { pixel = ((((i & 0x8) == 0) ^ ((j & 0x8) == 0)) * 255); texture[i][j][0] = pixel; texture[i][j][1] = pixel; texture[i][j][2] = pixel; texture[i][j][3] = 200; } } glBindTexture(GL_TEXTURE_2D textureIds[0]); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_REPEAT); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_REPEAT); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA TEXTURE_WIDTH TEXTURE_HEIGHT 0 GL_RGBA GL_UNSIGNED_BYTE texture); This is somewhat similar to the code snippet from page 417 in the book OpenGL Programming Guide and creates a check pattern. And then the display function contains... glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glEnable(GL_TEXTURE_2D); // Use model view so that rotation value is literal not added. glMatrixMode(GL_MODELVIEW); glPushMatrix(); // ... translation etc ... glBindTexture(GL_TEXTURE_2D textureIds[0]); glBegin(GL_QUADS); glTexCoord2f(0.0 0.0); glVertex3f(-1.0 +1.0 0.0); // top left glTexCoord2f(0.0 1.0); glVertex3f(-1.0 -1.0 0.0); // bottom left glTexCoord2f(1.0 1.0); glVertex3f(+1.0 -1.0 0.0); // bottom right glTexCoord2f(1.0 0.0); glVertex3f(+1.0 +1.0 0.0); // top right glEnd(); // not neccecary to repeat just good practice glBindTexture(GL_TEXTURE_2D textureIds[0]); glBegin(GL_QUADS); glTexCoord2f(0.0 0.0); glVertex3f(-0.5 +1.0 -1.0); // top left glTexCoord2f(0.0 1.0); glVertex3f(-0.5 -1.0 -1.0); // bottom left glTexCoord2f(1.0 1.0); glVertex3f(+1.5 -1.0 -1.0); // bottom right glTexCoord2f(1.0 0.0); glVertex3f(+1.5 +1.0 -1.0); // top right glEnd(); glFlush(); glDisable(GL_TEXTURE_2D); glPopMatrix(); SwapBuffers(); So this renders a 2nd square in the background; I can see this but it looks like they're being blended with the background (I assume this because they are darker with 200 bit alpha than 255 bit) instead of the texture behind... As you can see no transparency... How can I fix this? You have TEXTURE_HEIGHT twice in that loop instead of TEXTURE_WIDTH Ah thanks but that won't matter as they're both 64. So the other answer which was here but was deleted mentioned this - Generally for alpha blending to work correctly you need to sort the objects from far to near in the coordinate system of the camera. This is why your polygons are blended with the background. You can confirm that this is indeed the problem by disabling the depth test. Without depth test all the fragments are displayed and you'll be able to see the alpha blending. More on this in this page.,c++ opengl textures opacity1478355,A,Programmatically block screen saver in Mac OSX Is it possible to programatically ask Mac OS X not to turn on the screen saver while your application is active? You want to use: UpdateSystemActivity(UsrActivity); Here is Apple's example code. Be aware this is deprecated for 64bit binaries and I have not found a suitable replacement but the struggle continues.,c++ osx opengl766127,A,Obtaining current ModelView matrix In OpenGL how do I read the current x/y translation in the modelview matrix? I know that you have to load the current matrix into an array and read the floats from there but I don't know precisely how to do it. Use glGlet GLfloat matrixf[16]; glGetFloatv(GL_MODELVIEW_MATRIX matrixf); GLdouble matrixd[16]; glGetDoublev(GL_MODELVIEW_MATRIX matrixd); GLint matrixi[16]; glGetIntegerv(GL_MODELVIEW_MATRIX matrixi);  In order to retrieve the current modelview matrix you have to call the glGetFloatv function with GL_MODELVIEW_MATRIX parameter. GLfloat matrix[16]; glGetFloatv (GL_MODELVIEW_MATRIX matrix); From the documentation: GL_MODELVIEW_MATRIX params returns sixteen values: the modelview matrix on the top of the modelview matrix stack. Initially this matrix is the identity matrix. Beat me to it :),c++ opengl matrix translation1614393,A,"SDL GL program terminates immediately I'm using Dev-C++ 4.9.9.2 (don't ask why) and SDL 1.2.8. Next I've created new project: SDL&GL. This project contains already some code: #include <SDL/SDL.h> #include <gl/gl.h> int main(int argc char *argv[]){ SDL_Event event; float theta = 0.0f; SDL_Init(SDL_INIT_VIDEO); SDL_SetVideoMode(600 300 0 SDL_OPENGL | SDL_HWSURFACE | SDL_NOFRAME); glViewport(0 0 600 300); glClearColor(0.0f 0.0f 0.0f 0.0f); glClearDepth(1.0); glDepthFunc(GL_LESS); glEnable(GL_DEPTH_TEST); glShadeModel(GL_SMOOTH); glMatrixMode(GL_PROJECTION); glMatrixMode(GL_MODELVIEW); int done; for(done = 0; !done;){ glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glLoadIdentity(); glTranslatef(0.0f0.0f0.0f); glRotatef(theta 0.0f 0.0f 1.0f); glBegin(GL_TRIANGLES); glColor3f(1.0f 0.0f 0.0f); glVertex2f(0.0f 1.0f); glColor3f(0.0f 1.0f 0.0f); glVertex2f(0.87f -0.5f); glColor3f(0.0f 0.0f 1.0f); glVertex2f(-0.87f -0.5f); glEnd(); theta += .5f; SDL_GL_SwapBuffers(); SDL_PollEvent(&event); if(event.key.keysym.sym == SDLK_ESCAPE) done = 1; } SDL_Quit(); return(0); } Next I compiled project and try to run it. After run the program shows for less than 1 second and immediately terminates. Debugger returns following error: ""An Access Violation (Segmentation Fault) raised in your program"". I'm using Windows 2003 and Radeon x1950 PRO with latest drivers. I've tested program on laptop with Windows XP and it works perfectly. Why this program doesn't work on my computer? I take it checking the error returns on all your SDL and GL calls wasn't informative? Yes it wasn't informative because it returns following error: ""An Access Violation (Segmentation Fault) raised in your program"". It works for me too. I'd try removing SDL_HWSURFACE and add SDL_DOUBLEBUF from the window call. SDL_SetVideoMode(600 300 0 SDL_OPENGL | SDL_NOFRAME | SDL_DOUBLEBUF); while(!done) looks prettier and easier to read. Since it's tagged with C++ why are you not using bools for this? bool done = false; while(!done){ You also want while(SDL_PollEvent(&event)) as there can be more than one event per frame. while(SDL_PollEvent(&event)) { switch(event.type) case SDL_KEYDOWN: if(event.key.keysym.sym == SDLK_ESCAPE) done = true; }  I finally found some time to solve this problem. I have completly uninstalled old card graphic drivers and install 9.8 ATI drivers with Catalyst Control Center. Now everything is working. There where no problem in code itself. The problem was something in my system with graphic drivers. Anyway thanks for your answers!  My guess is it's crashing on SDL_PollEvent(). It returns 1/true if there is an event 0/false if not. When it does return true it will be a certain type of SDL_Event based on event.type. SDL_Event is a union of all SDL events and some info in one event is not guaranteed to be in the same order type etc as another. So you just need to check the type of the event and handle it as necessary.. Check out the docs for more info of course. Something like this: if (SDL_PollEvent(&event)) { switch (event.type) { case SDL_KEYUP: if (event.key.keysym.sym == SDLK_ESCAPE) done = 1; } } That would not _crash_. It will read from uninitialized memory or a previous event value so it could __quit__ prematurely. Good catch though!",c++ opengl sdl1564870,A,"Recommended OpenGL / GLUT Reference What OpenGL / GLUT reference is good for day to day programming as you learn? Ideally I'm looking for something with lots of C++ sample code to help me learn as I develop OpenGL applications as well as details about the APIs similar to what MSDN provides for .net programming. If there isn't a one stop shop then please list the set of references I should use and what the strengths of each one is. The PyOpenGL Documentation is identical to the OpenGL docs but far more readable and user-friendly. Have a look. I also second the OpenGL SuperBible.  I learned OpenGL using the OpenGL Super Bible. It's still the best reference for it that I can find.  The Red Book is the standard book on OpenGL. Don't be discouraged by the fact that the Amazon review for the 7th Edition has only two stars; this is because people are disappointed that there isn't more on the newest OpenGL features in the book. Previous editions got more stars. Another good book is the OpenGL SuperBible. The NeHe Tutorials are one of the most often cited OpenGL tutorials with sample code not only in C++ but in many other programming languages.  I think that by ""Glut"" you mean ""Freeglut"". In this case you should use this specific reference: http://jocelyn.frechot.free.fr/freeglut/freeglut_2.6.0-api_0.3.xhtml It contains latest references for current Freeglut. This way you can use special aptitudes of Freeglut (like controlling your own GL loop with glutMainLoopEvent() which is invaluadble when you're using Freeglut with others libraries.  For all the details about the OpenGL-API there are of course the sdk documentation pages https://www.opengl.org/sdk/docs/ or you could look into the standard specification itself (which I personally avoid most of the time). This most likely only helps if you already have a basic understanding of how to use the GL. The news section on opengl.org also often links to tutorials and books. Just skimmed through it and found this tutorial. As for OpenGL related books I only know the Super Bible which I think is okay to get started. When learning OpenGL without any computer graphics knowledge a book on that topic can be very helpful too. A classic would be Coumputer Graphics Principal and Practice by James D. Foley which is still an excellent read but it doesn't focus much on real time rendering. For that Real-Time Rendering by Akenine-Moller is an excellent choice.",c++ opengl reference glut1105349,A,C++ OpenGL Window and Context creation framework / library I'm searching for an multi platform OpenGL framework that abstracts the creation of windows and gl contexts in C++. I'd like to have an OO representation of Window Context & co where i can instantiate a Window create a Context and maybe later set the window to fullscreen. I'm thinking about implementing this myself for xgl wgl and agl. But before So here comes the Question: Which libraries / frameworks should i check out first before inventing the wheel again? Edit: So far named libraries: glut Qt SDL gtkglext wxwdigets SFML Hmm could be I would be curious if it were that simple. In my case it's just a Win32 reference build for a console game so I hadn't looked into it too deeply. Plus for debugging purposes I actually like/need to see the printfs. I'm using GLUT and wxWidgets successfully in two separate projects. The only downside to GLUT under Win32 is that it opens up a DOS window - good for seeing any printf's but it doesn't look as professional to have an app that opens up that window. @Jim: That's a bit in the executable that can easily be changed with `editbin` and is usually an artefact of compilation flags? wxWidgets is another alternative but may also be too heavyweight.  SMFL is another similar to SDL but takes a more object oriented approach. This one sounds promising ( their site says: SFML is composed of several packages to perfectly suit your needs. You can use SFML as a minimal windowing system to interface with OpenGL or as a fully-featured multimedia library for building games or interactive programs. ). I'm going to implement it myself - but since i think i can reuse some of the SMFL code i will accept this answer SFML is lovely. It doesn't (or maybe didn't) support creating contexts with stencil buffers but hey you can edit the code.  You could look at Glut (C) Qt (C++) SDL (C). I have already worked with glut (it was nice for the little opengl examples back at university but i think it wont fit in a bigger project). The Qt api seems nice - but i don't like the thought to introduce a dependecy to Qt. It seems to heavyweight for my needs. The same goes for SDL. There is also http://GtkGLExt.sf.net @Christoph: I can fully understand what you mean. In an inhouse solution for a former employer I had to solve a similar problem. First attempts were using the platform bindings I mentioned above. After a year or so I switched to your idea of implementing platform specifics myself.  libapril is nice also. It is clean simple and supports newer mobile platforms.,c++ opengl window multiplatform766167,A,What happens to pixels after passing them into glTexImage2D()? If for example I create an array of pixels like so: int *getPixels() { int *pixels = new int[10]; pixels[0] = 1; pixels[1] = 0; pixels[1] = 1; // etc... } glTexImage2D(... getPixels()); Does glTexImage2D use that reference or copy the pixels into it's own memory? If the answer is the former then should I do the following? int *p = getPixels(); glTexImage2D(... p); /* Just changed to delete[] because delete * would only delete the first element! */ delete[] p; I think you would need to call glDeleteTextures() after you have finished drawing your pixels to free the memory.  From this quote in the man page it sounds like glTexImage2D allocates its own memory. This would make sense ideally the OpenGL API would send data to be stored on the graphics card itself (if drivers/implementation/etc permitted). In GL version 1.1 or greater pixels may be a null pointer. In this case texture memory is allocated to accommodate a texture of width width and height height. You can then download subtextures to initialize this texture memory. The image is undefined if the user tries to apply an uninitialized portion of the texture image to a primitive. So yea I'd imagine there is no harm in freeing the memory once you've generated your texture. I'm not sure if that's the case on PC but on consoles texture uploading functions very often simply store pointer to texture and schedule DMA transfer for later time so you can't free memory as soon the function returned.  Yes after the call to geTexImage2D() returns it is safe to discard the data you passed to it. infact if you don't do that you'll have a memory leak like in this code: int *getPixels() { int *pixels = new int[10]; pixels[0] = 1; pixels[1] = 0; pixels[1] = 1; // etc... } glTexImage2D(... getPixels()); You pass the pointer to the buffer to the call but then the pointer is lost and most likely leaks. What you should do is store it and delete it aftet the call retuns: int *pbuf = getPixels(); glTexImage2D(... pbuf); delete[] pbuf; alternativly if the texture is of a constant size you can pass a pointer to an array that is on the stack: { int buf[10]; ... glTexImage2D(... pbuf); } Finally if you don't want to worry about pointers and arrays you can use STL: vector<int> buf; fillPixels(buf); getTexImage2D(... buf.begin()); Oops! My initial snippet used delete instead of delete[] for deleting the pixel buffer. technically buf.begin() might not work but &buf[0] is guaranteed to work. In most STL implementations the vector iterator is a pointer but it doesn't have to be. actually delete[] is what you should be using. my bad fixed now. btw this is exactly why you should use STL :),c++ opengl glteximage2d239917,A,"Getting started with OpenGL As you can see here I'm about to start work on a 3d project for class. Do you have any useful resources/websites/tips/etc. on someone getting started with OpenGL for the first time? The project will be in C++ and accessing OpenGL via GLUT. Processing has also open gl support it's an easy way to get started.. If you don't like the editor which comes with processing you can also use it with a Java IDE such as Eclipse or Intellij.  I agree with jwfearn above. OpenGL SuperBible is the most readable interesting and understandable book for OpenGL to me. Give it a try.  Check this out http://www.videotutorialsrock.com/ with full reference material  video tutorials and sample codes.  OpenGL is extremely easy to start with especially if you're using a toolkit like GLUT or SDL. Starting from man glVertex and man glDrawElements will get you started (look at the references at the bottom for other functions). If you're on a windows system typing those commands into google yields the same results. Further down the road it's probably a good idea to check out the extensions repository. I would advise against books in this case because the (more exact) specifications are published online making that the most accurate and up-to-date resource.  NeHe is way out of date and most books are too. Check out the tutorial here: http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html . I found it really useful and it covers a modern shader-based approach. See also the wiki by madsy in #opengl on freenode: http://www.mechcore.net/w/Main_Page . The descriptions are much less approachable and more technical but it's also a good forward-looking basis and provides a lot of deeper mathematical background than the tutorials linked above.  This link has some answers to a similar question.  The Learning Modern 3D Graphics Programming series of tutorials is an extremely excellent and thorough introduction to key GL concepts and techniques. Additionally the Modern OpenGL sides by Little Grasshopper provide a summary/reference to several of the more recent OpenGL features. If anybody is interested in running the Learning Modern 3D Graphics Programming tutorials on OS X I have an Xcode+GLFW3 port here: https://github.com/rsanchezsaez/gltut-glfw (wip) I'm really impressed the section Following the Data of Learning Modern 3D Graphics Programming!  The OpenGL ""Red Book"" is the best place to start learning. The OpenGL ""Blue Book"" is documentation for the OpenGL API. There are many online resources for opengl and glprogramming.com is one of them. Check out the links section.  Beginning OpenGL Game Programming is a very good introduction even if you don't care about games. The official Programming Guide and Reference Manual are a must too. Might as well pick up a general book on computer graphics. There are plenty of online resources but I really think you should go for books even if it was the freely available old versions of the official books. NeHe's site has some interesting code samples but I don't think it's a good resource to start learning as it glosses over many details and follows a quick and dirty approach to get things running. +1 Thank you for recommending Beginning OpenGL Game Programming. I got it and its excellent.  The Red Book online - http://www.glprogramming.com/red/ - http://www.glprogramming.com/blue/ Hope you manage to go through this.. I didn't... another unfinished dream on my shelf. Keep in mind though neither of these books(especially the blue book) are up to the latest OpenGL versions the programming model with modern OpenGL is quite different from what these books teach you.  The OpenGL 4.0 Shading Language Cookbook is a gem when you start writing shaders. If you're using the OpenGL Superbible I'd recommend skipping past most of the early chapters and jumping ahead to Chapter 8 (Buffer Objects). The first half of the book teaches more about the concept of the rendering pipeline through the use of wrappers than about actual OpenGL.  check glut: http://en.wikipedia.org/wiki/OpenGL_Utility_Toolkit http://www.lighthouse3d.com/opengl/glut/index.php?1  This is the best web page(for me) to learn OpenGL http://nehe.gamedev.net/. And this is the official book http://www.amazon.com/OpenGL-Programming-Guide-Official-Learning/dp/0321335732  If you are serious about learning OpenGL you should check out the OpenGL Bootcamp at Big Nerd Ranch. The instructor has written several award winning games including the ""Big Bang Board Games"" published by Freeverse software.  I highly recommend OpenGL SuperBible Sixth Edition by Graham Sellers Richard S. Wright Jr. and Nicholas Haemel. ISBN-13: 978-0321902948 It's a one-stop-shop with both tutorial and reference. It combines some of the best aspects of both the ""Red"" and ""Blue"" books along with some great graphics background information. If your budget is limited to one book this is a great choice. I agree that was my first GL book and it was much more approachable than the Red and Blue books. Since the Red Book isn't quite up to date anymore the SuperBible is a much better book to start modern OpenGL programming.  The NeHe tutorials will get you going but as luke points out they don't really cover much background of why. Although it is WPF rather than opengl (do you have to use opengl?) Petzolds book on 3d graphics with wpf does a very good job of introducing 3d graphics and some of the maths behind it. I may be in the minority but I dislike NeHe's tutorials. Like many that's where I started but there are many *many* better resources. The learning curve with graphics APIs is steep and NeHe offers very little insight to the whys and wherefores. Please post any other suggestions here or edit my post - that's exactly what SO is for.  A German wiki about very much stuff about OpenGL and related game programming here. And of couse the official documentation for openGL 2 openGL 3 and openGL 4.",c++ opengl 3d glut623127,A,OpenGL coordinate problem I am creating a simple 2D OpenGL application but I seem to be experiencing some camera issues. When I draw a rectangle at (2020) it is drawn at (2520) or so. When I draw it at (100 20) it is drawn at 125 or so. For some reasons everything is being shifted to the right by a few %. I have pasted a trimmed down version here http://pastebin.com/m56491c4c Is there something wrong with the way I am setting up GLUT? I know it's not my objects doing anything weird since the same thing happens when I disable them. Thanks in advance. It depends on what system you are working on but usually most windows coordinate systems start at the bottom left corner and count up to the top and to the right. In this case your gluOrth02D call would be wrong. You Have: gluOrtho2D(0 appWidth appHeight 0); Which has the top of the window mapping to the bottom vice-verse. Most of the time it's: gluOrtho2D(0 appWidth 0 appHeight); As I said it depends on the system platform your working with. I can only speak for most linux implementations. Just something else to check out just in case it affecting your bug.  You seem to be calling your glOrtho2D on your ModelView matrix. I doubt that that's the problem (since I guess in this case your Projection should be the identity) but you should still call it on your Projection matrix instead. You should also print out w and h in your resize call just to make sure that your window size is actually what you think it is (I don't know how glut works but glutInitWindowSize() may include borders which would mess things up). I've already tried setting it up in the order you suggested without any luck. I've messed around with the outputs and am fairly confident that the window size is correct. All out of ideas.  You need to set the projection matrix inside the reshape function (resize()) which also automatically solves the problem of the user resizing the window: void resize(int w int h) { glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } And then in your draw function make sure that the matrix mode is model-view: void draw() { glMatrixMode(GL_MODELVIEW); glLoadIdentity(); ... } Other problems with your code: You probably shouldn't be calling glutPostRedisplay() at the end of draw(). This is going to make your CPU run at 100%. You can instead use glutTimerFunc() to still have updates every some number of milliseconds. In processMouse() you're using wsprintf() on an array of chars: wsprintf() takes an array of wide characters (wchar_t) so you should make the local variable s of type wchar_t[] or use sprintf() and MessageBoxA() instead of wsprintf() and MessageBoxW() (to which MessageBox() expands as a macro when compiling a Unicode application which I'm assuming you're doing). You're also vulnerable to a buffer overflow -- you should use a buffer of at least 12 characters even though realistically you'll never be passed a very large x value. Finally you should also use snprintf()/wsnprintf() instead of sprintf()/wsprintf() to protect against the buffer overflow. Thanks that fixed it! Thanks for your other suggestions. I know it's messy code I just wanted a quick working sample to post. Thanks again!,c++ opengl glut984792,A,Lighting issue in OpenGL I have trouble developing an OpenGL application. The weird thing is that me and a friend of mine are developing a 3d scene with OpenGL under Linux and there is some code on the repository but if we both checkout the same latest version that means the SAME code this happens: On his computer after he compiles he can see the full lighting model whilst on mine I have only the ambient lights activated but not the diffuse or specular ones. Can it be a problem of drivers ?(since he uses an ATi card and I use an nVIDIA one) Or the static libraries ? I repeat it is the same code compiled in different machines.. that's the strange thing it should look the same. Thanks for any help or tip given. This can very easily be a driver problem or one card supporting extensions that the other does not. Try his binaries on your machine. If it continues to fail either your drivers are whack or you're using a command not supported by your card. On the other hand if your screen looks right when using your code compiled on his machine then your static libraries have a problem. Yes you're right it is a drivers issue. I have tried the binaries on my machine and compiled the program in different machines. With the newer nVIDIA cards and Ubuntu it happens the same in all the PCs I tested.,c++ opengl lighting859501,A,"Learning OpenGL in Ubuntu I'm trying to learn OpenGL and improve my C++ skills by going through the Nehe guides but all of the examples are for Windows and I'm currently on Linux. I don't really have any idea how to get things to work under Linux and the code on the site that has been ported for Linux has way more code in it that's not explained (so far the only one I've gotten to work is the SDL example: http://nehe.gamedev.net/data/lessons/linuxsdl/lesson01.tar.gz). Is there any other resource out there that's a bit more specific towards OpenGL under Linux? Maybe you would like to use Qt to draw the windows and widgets. Here's a tutorial based on the Nehe guides to show you how to create OpenGL images with Qt. To learn OpenGL the OpenGL Red Book is a must read. There's an online version. It has very good explanations and examples.  I'll guess that it is the compilation process that is the biggest difference initially. Here is a useful Makefile for compiling simple OpenGL apps on Ubuntu. INCLUDE = -I/usr/X11R6/include/ LIBDIR = -L/usr/X11R6/lib FLAGS = -Wall CC = g++ # change to gcc if using C CFLAGS = $(FLAGS) $(INCLUDE) LIBS = -lglut -lGL -lGLU -lGLEW -lm All: your_app # change your_app. your_app: your_app.o $(CC) $(CFLAGS) -o $@ $(LIBDIR) $< $(LIBS) # The initial white space is a tab Save this too a file called Makefile and should be in the same directory. Compile by writing make from terminal or :make from Vim. Good luck  a little update for the makefile because I found this old answers from @Millthorn and it didn't worked: you dont need to definde the include path since it's in standard lib http://stackoverflow.com/a/2459788/1059828 a minimal makefile to compile open GL could look like this: LDFLAGS=-lglut -lGL -lGLU -lGLEW -lm all: your_app http://surflab.cise.ufl.edu/wiki/Getting_Started_with_OpenGL_in_Ubuntu  The first thing to do is install the OpenGL libraries. I recommend:  freeglut3 freeglut3-dev libglew1.5 libglew1.5-dev libglu1-mesa libglu1-mesa-dev libgl1-mesa-glx libgl1-mesa-dev Once you have them installed link to them when you compile: g++ -lglut -lGL -lGLU -lGLEW example.cpp -o example In example.cpp include the OpenGL libraries like so: #include <GL/glew.h> #include <GL/glut.h> #include <GL/gl.h> #include <GL/glu.h> #include <GL/glext.h> Then to enable the more advanced opengl options like shaders place this after your glutCreateWindow Call: GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr ""Error %s\n"" glewGetErrorString(err)); exit(1); } fprintf(stdout ""Status: Using GLEW %s\n"" glewGetString(GLEW_VERSION)); if (GLEW_ARB_vertex_program) fprintf(stdout ""Status: ARB vertex programs available.\n""); if (glewGetExtension(""GL_ARB_fragment_program"")) fprintf(stdout ""Status: ARB fragment programs available.\n""); if (glewIsSupported(""GL_VERSION_1_4 GL_ARB_point_sprite"")) fprintf(stdout ""Status: ARB point sprites available.\n""); That should enable all OpenGL functionality and if it doesn't then it should tell you the problems.",c++ linux opengl ubuntu1878257,A,How can I draw a cylinder that connects two points in OpenGL i have two point each point has its own X and Y value and they have the same Z value. i want a function to draw Cylinder between these two points. I think you're going to have to be a lot more specific. There are an infinite number of cylinders that contain those two points. Can you describe how you want the cylinder to be oriented with respect to the points? I suspect you want a cylinder of unit- (or user specified-) radius with those two points on its centerline at either end. But you need to be specific... For building a cylinder with two given points you need vector analysis. You are building two perpendicular vectors which are added to each point and scaled with sin/cos multiplied with the radius. It accepts all points (The old code had a bug because it missed the sqrt() for the length). Now it functions correctly and draw the cylinder with gl routines; I tested it in JOGL. For faster drawing move the firstPerpVector/secondPerpVector/points variable to a private final array field and initialize them at the beginning. Java code:  public float[] getFirstPerpVector(float x float y float z) { float[] result = {0.0f0.0f0.0f}; // That's easy. if (x == 0.0f || y == 0.0f || z == 0.0f) { if (x == 0.0f) result[0] = 1.0f; else if (y == 0.0f) result[1] = 1.0f; else result[2] = 1.0f; } else { // If xyz is all set we set the z coordinate as first and second argument . // As the scalar product must be zero we add the negated sum of x and y as third argument result[0] = z; //scalp = z*x result[1] = z; //scalp = z*(x+y) result[2] = -(x+y); //scalp = z*(x+y)-z*(x+y) = 0 // Normalize vector float length = 0.0f; for (float f : result) length += f*f; length = (float) Math.sqrt(length); for (int i=0; i<3; i++) result[i] /= length; } return result; } public void drawCylinder(GL gl float x1 float y1 float z1 float x2 float y2 float z2) { final int X = 0 Y = 1 Z = 2; // Get components of difference vector float x = x1-x2 y = y1-y2 z = z1-z2; float[] firstPerp = getFirstPerpVector(xyz); // Get the second perp vector by cross product float[] secondPerp = new float[3]; secondPerp[X] = y*firstPerp[Z]-z*firstPerp[Y]; secondPerp[Y] = z*firstPerp[X]-x*firstPerp[Z]; secondPerp[Z] = x*firstPerp[Y]-y*firstPerp[X]; // Normalize vector float length = 0.0f; for (float f : secondPerp) length += f*f; length = (float) Math.sqrt(length); for (int i=0; i<3; i++) secondPerp[i] /= length; // Having now our vectors here we go: // First points; you can have a cone if you change the radius R1 final int ANZ = 32; // number of vertices final float FULL = (float) (2.0f*Math.PI) R1 = 4.0f; // radius float[][] points = new float[ANZ+1][3]; for (int i=0; i<ANZ; i++) { float angle = FULL*(i/(float) ANZ); points[i][X] = (float) (R1*(Math.cos(angle)*firstPerp[X]+Math.sin(angle)*secondPerp[X])); points[i][Y] = (float) (R1*(Math.cos(angle)*firstPerp[Y]+Math.sin(angle)*secondPerp[Y])); points[i][Z] = (float) (R1*(Math.cos(angle)*firstPerp[Z]+Math.sin(angle)*secondPerp[Z])); } // Set last to first System.arraycopy(points[0]0points[ANZ]03); gl.glColor3f(1.0f0.0f0.0f); gl.glBegin(GL.GL_TRIANGLE_FAN); gl.glVertex3f(x1y1z1); for (int i=0; i<=ANZ; i++) { gl.glVertex3f(x1+points[i][X] y1+points[i][Y] z1+points[i][Z]); } gl.glEnd(); gl.glBegin(GL.GL_TRIANGLE_FAN); gl.glVertex3f(x2y2z2); for (int i=0; i<=ANZ; i++) { gl.glVertex3f(x2+points[i][X] y2+points[i][Y] z2+points[i][Z]); } gl.glEnd(); gl.glBegin(GL.GL_QUAD_STRIP); for (int i=0; i<=ANZ; i++) { gl.glVertex3f(x1+points[i][X] y1+points[i][Y] z1+points[i][Z]); gl.glVertex3f(x2+points[i][X] y2+points[i][Y] z2+points[i][Z]); } gl.glEnd(); }  If you cant use gluCylinder() from the GLU library (eg if you ae on OpenGL-ES) Or you have to make the sides of the cylinder from small flat segments http://stackoverflow.com/questions/1056504/how-do-you-draw-a-cylinder-with-opengles but i need a function to calculate the X and Y angles between the points and draw the cylinder from the first one to the other  Found bugs using the currently accepted answer. Ended up coming up with my own. You can do it with some simple maths: http://www.thjsmith.com/40/cylinder-between-two-points-opengl-c Put the code in your answer; don't just link to some website.  http://lifeofaprogrammergeek.blogspot.com/2008/07/rendering-cylinder-between-two-points.html Well it was 2009... ;) this uses deprecated OpenGL calls.,c++ opengl 3d155672,A,"Starting Graphics & Games Programming (Java and maybe C++) I've always had an interest in creating my own games and now at university I have the opportunity to create some 2D and 3D games using Java and C++ for those who are that way inclined. I've never really programmed a game before let alone graphics so I'm completely new to the area. After a quick trip to the library today I came across very little information on starting 2D game development or even graphics programming in Java or C++. I can program in Java at a reasonable level but I have never touched C++. Can someone recommend any good books on starting Graphics Programming in Java or C++? I have no C++ experience but I've programmed in C and Java and feel reasonably comfortable in both. Should I take the jump and dive into C++ when important marks are at stake? I hear a lot of good things about C++ as far as Games Programming goes so I'm unsure as to what is the best option for me. I'm looking to write a 2D game in my own time for a bit of fun before I start getting into the heavy work at university. Can anyone recommend some good resources for those interested in writing their own games using OpenGL and Java/C++. For those who have followed my other questions here I am terrible at Maths and hold very little knowledge of even the foundations. Is this going to be a serious problem for me? If I were to need Math knowledge what do you recommend I brush up on? I apologise if my questions seem a bit vague as I'm a complete newbie when it comes to a lot of these topics. Any help you could provide would be much appreciated. EDIT: I've already checked out the question titled ""game programming"" but have found it to not really cater for my specific questions. Both. I've always had an interest in Graphics Programming but now that I have a class in it I'm forced to take action. As far as I am aware at university we'll be using Java and OpenGL (my university is a Java School so we only get support for that) but I am interested in learning C++ as well. Are you doing graphics at uni or just in your spare time? If you're doing it at uni then what do they use? C++ OpenGL and (usually) GLUT seems to be the standard language&APIs I've observed. It's probably useful to follow that path & knowing basics will give you more time to pick up the bonus marks Similar question here that you might find useful: [http://stackoverflow.com/questions/124322/game-programming#124427](http://stackoverflow.com/questions/124322/game-programming#124427) I am currently doing exactly the same thing as you are now at university but I have nearly finished studying that part of my course! How bizzare!?! Mite be able to send you some extra lecture material aswell if it helps? On top of books I think you should check out these sites: NeHe Productions Video Tutorials Rock Opengl Programming Guide: The Official Guide to Learning Opengl  Version 2.1 Addison Wesley 2007 Here are the books that I would also check out: Hearn & Baker Computer Graphics with OpenGL Prentice-Hall 2003. I'd also seriously consider using C++ as your programming language. From what I have been taught and what I have been reading around C/C++ language branch are the most used by the game industry for example I heard STEAM who made the half life series use C++ and some others. Here is a free C++ book on the internet: Thinking in C++ 2nd ed. Volume 1 å©2000 by Bruce Eckel Here is what I learnt from: C++ in 24 Hours Sams Teach Yourself  I am going through a similar process to you at the moment. I found the following site helpful it has a great tutorial (space invaders in Java): http://www.cokeandcode.com/node/6 You can get the source code and mess around with it. You will probably benefit from an IDE if you don't already have a favorite you could try Netbeans (netbeans.org) which is free and I think it's pretty good. As for books this one is OK (it's java-centric): http://www.amazon.com/Killer-Game-Programming-Andrew-Davison/dp/0596007302 I personally decided to use Java for games (for now) because I am very comfortable with Java and far less so with C++. But you will find that most people use C++ for commercial games. Actually I started with pygame (python games framework) which is also nice to get started especially if you know python. That tutorial is great! I've just downloaded the source run through with it and it all looks a lot easier than I had anticipated. Thanks for the answer!  Something to help you get into OpenGL is GLUT - the OpenGL Utility Toolkit. It takes care of a lot of the lower-level setup that's a pain to deal with if you're starting an OpenGL project from scratch. It makes it easier to jump right in and start putting stuff on the screen. GLUT hasn't been updated for awhile but you can also try FreeGLUT which is a newer open source replacement of GLUT and is included by several linux distributions out of the box.  I know that you specifically asked for Java or C++ but I have found that PyGame (based on SDL) has been a really good primer for me on basic game techniques. They have a couple of comprehensive tutorials and examples and python is a very clear and easy to pick up language. I would also suggest the Xbox 360 option as well if you already have one. That is what I will be moving onto next so that I can learn all the 3D stuff that PyGame is not so great at. As for math you could probably get by without real skills for simple 2D games. If you intend on having any kind of physics simulation then are going to have to actaully learn some real math. Even basic 2D games like riding a motorcycle over jumps involve a lot of physics math to make it work/fun  Here are a few books that I used when writing OpenGL code in C++: Fundamentals of Computer Graphics OpenGL SuperBible OpenGL Red Book OpenGL Primer You might check out the MIT OpenCourse on computer graphics too. It might help supplement your development. Best of luck!  Short answer: Don't write in C++ !!! You'll spend more time futzing with the language than actually learning about games physics collisions etc. Get a copy of Python and PyGame. It's easy to get started but you'll actually learn heaps. Having learnt Java you'll be amazed how much simpler Python is for getting the same things done. Once you're comfortable with your skill set then look at Panda3D. It's what's used at Pixar / Disney. If you choose to get your hands dirty with C++ at this stage then diving into Panda3D is going to be good. All the major studios use Python as does Google. If you end up specialising in C++ you'll become an engine-room programmer. Very necessary but not as glamorous. Oh you mentioned grades etc. Unless you need to take a course in C++ programming you'd be wasting precious time and energy on it. I don't know that any major studios use Python for much game code other than EVE Online which uses it on its massive server backend. Certainly for any console platform I've found Python to be unacceptably memory-bulky.  I would recommend starting with C++ and SFML. C++ has the largest available library of existing code and every example you find will probably be C/C++ oriented. SFML is a windowing and graphics library (comparable to SDL if you've heard of that) done using object-oriented design that fits into C++ better. It'll allow you to get up and running with an OpenGL window right off the bat and the documentation is pretty good as well.  I have this book: Beginning C++ Through Game Programming. It might seem trivial at first but it teaches you a-lot as you go through it. There is no GUI based programming in this book just the console. Which is good to an extent if you want to see how an entire ""story"" of a game can come together. You can also check out Gamedev.net they have a vast amount of resources and articles to get you started. Good luck. :) Thank you for the link to the book. I'm not very confident in my programming abilities as of yet and this book seems to be perfect for my needs. I'll be sure to check it out very soon.  Game programming especially where graphics are concerned involves a fair amount of math. You'll at least want to have a basic familiarity with vectors and matrices specifically with regard to representing rotation translation and scaling transformations. To that end I recommend Geometric Tools for Computer Graphics. A course in linear algebra wouldn't hurt as well although that's probably already in your curriculum. As far as game programming in Java I recommend taking a look at jMonkeyEngine an open source game engine with all sorts of fun example code to get you started. It was originally based on the engine presented in the book 3D Game Engine Design (same author as Geometric Tools above) which is another good source of information about game programming in 3D. There's also C++ code and papers on various topics in 3D graphics at that book's official site.  Honnestly go buy a xbox 360 and download the free XNA SDK package to get started. Update your maths skills and modify some indie games in their sample kit. Build a small game invite some friends around for some beers and have FUN! Your going to learn a lot faster than touching C/C++/Java and also too your going to have fun well doing it. Interesting answer. At university I am told to use Java or another language if we're feeling adventurous but the Xbox 360 idea does sound like it'll be good fun when the coursework is out of the way and if I decide to keep making games. Thanks for answering! The reason why i recommend the Xbox 360 approach is because its fast has fast turn around times and is just fun! At this stage you do not want to be bogged down with the problems with large scale C++ development. This is were most young people dont get past. Why is turn around time so important? In our game development studio the turn around time from compile to run is about 5-10 minutes depending on the content. So if you make a mistake and your new its going to take you 5 minutes to find out. This is why turn around time is important at the start.",java c++ opengl graphics1823927,A,"Simulated time in a game loop using c++ I am building a 3d game from scratch in C++ using OpenGL and SDL on linux as a hobby and to learn more about this area of programming. Wondering about the best way to simulate time while the game is running. Obviously I have a loop that looks something like: void main_loop() { while(!quit) { handle_events(); DrawScene(); ... SDL_Delay(time_left()); } } I am using the SDL_Delay and time_left() to maintain a framerate of about 33 fps. I had thought that I just need a few global variables like int current_hour = 0; int current_min = 0; int num_days = 0; Uint32 prev_ticks = 0; Then a function like : void handle_time() { Uint32 current_ticks; Uint32 dticks; current_ticks = SDL_GetTicks(); dticks = current_ticks - prev_ticks; // get difference since last time // if difference is greater than 30000 (half minute) increment game mins if(dticks >= 30000) { prev_ticks = current_ticks; current_mins++; if(current_mins >= 60) { current_mins = 0; current_hour++; } if(current_hour > 23) { current_hour = 0; num_days++; } } } and then call the handle_time() function in the main loop. It compiles and runs (using printf to write the time to the console at the moment) but I am wondering if this is the best way to do it. Is there easier ways or more efficient ways? ""just need a few global variables"" - don't. There are nearly always better alternatives e.g. a `struct` wich contains the information and gets passed around. What exactly are you trying to do? I haven't tried SDL but will the ticks be dependent on the processor rate or is it always locked to milliseconds? You want to have a simulated minute every 30 seconds? SDL_GetTicks -- Gets the number of milliseconds since SDL library initialization. Based on this I am trying to simulate a minute every 30 seconds. Global state like time which is used by everything everywhere really ought to be in a global variable. Otherwise you'd have to pass your ""current time and framecount"" struct to every single function in the entire game. That assumes the entire game needs the current time and framecount which I'm pretty sure it doesn't. In my experience I get the time in the main loop and pass it to the update function that's it. Few places really need this data. I have the opposite experience. We use gpGlobals->currenttime absolutely everywhere: in the particle systems AI behavior speech special effects trigger hysteresis weapon fire rates (and projectile movement) animation scripting on and on. I grepped just now and found 4707 uses in one game DLL alone. Other than the issues already pointed out (you should use a structure for the times and pass it to handle_time() and your minute will get incremented every half minute) your solution is fine for keeping track of time running in the game. However for most game events that need to happen every so often you should probably base them off of the main game loop instead of an actual time so they will happen in the same proportions with a different fps.  I am not a Linux developer but you might want to have a look at using Timers instead of polling for the ticks. http://linux.die.net/man/2/timer_create EDIT: SDL Seem to support Timers: SDL_SetTimer If you look at the handle_time() function it's clear that he's not just counting minutes and hours for the fps ( which is handled by DrawScene()). As far as I understand he wanted to keep a ""clock"" in the game. I accept your comment performance in game development and I won't argue with you here as it's not my field.But as an idea for a different implementation I don't think there was anything fundamentally wrong with my answer While good advice in general this is not a good idea for game development. Not even for displaying the time in the game like he does ? Do you actually use polling instead ? zildjohn01: why are timers are a bad idea for a game? I am assuming they have some sort of performance impact? This is bad because you don't want a game to have a callback that does something every (n) milliseconds -- some frames might finish a little ahead of 33 and some others might lag behind. If you have a frame that takes 37 milliseconds and then set the next frame for 33 after that then you'll be effectively slowing down time as your two frames (representing 66ms of game time) would take place over 70ms of real time. It's much better to have the game loop running at full speed all the time -- never sleeping or pausing -- and use a high-resolution realtime clock to mark constant timesteps. I never suggested handling the frames with a timer. He was asking about Time. That's why he wanted the time: ""I am using the SDL_Delay and time_left() to maintain a framerate of about 33 fps.""  One of Glenn's posts you will really want to read is Fix Your Timestep!. After looking up this link I noticed that Mads directed you to the same general place in his answer.  I've mentioned this before in other game related threads. As always follow the suggestions by Glenn Fiedler in his Game Physics series What you want to do is to use a constant timestep which you get by accumulating time deltas. If you want 33 updates per second then your constant timestep should be 1/33. You could also call this the update frequency. You should also decouple the game logic from the rendering as they don't belong together. You want to be able to use a low update frequency while rendering as fast as the machine allows. Here is some sample code: running = true; unsigned int t_accum=0lt=0ct=0; while(running){ while(SDL_PollEvent(&event)){ switch(event.type){ ... } } ct = SDL_GetTicks(); t_accum += ct - lt; lt = ct; while(t_accum >= timestep){ t += timestep; /* this is our actual time in milliseconds. */ t_accum -= timestep; for(std::vector<Entity>::iterator en = entities.begin(); en != entities.end(); ++en){ integrate(en (float)t * 0.001f timestep); } } /* This should really be in a separate thread synchronized with a mutex */ std::vector<Entity> tmpEntities(entities.size()); for(int i=0; i<entities.size(); ++i){ float alpha = (float)t_accum / (float)timestep; tmpEntities[i] = interpolateState(entities[i].lastState alpha entities[i].currentState 1.0f - alpha); } Render(tmpEntities); } This handles undersampling as well as oversampling. If you use integer arithmetic like done here your game physics should be close to 100% deterministic no matter how slow or fast the machine is. This is the advantage of increasing the time in fixed time intervals. The state used for rendering is calculated by interpolating between the previous and current states where the leftover value inside the time accumulator is used as the interpolation factor. This ensures that the rendering is is smooth no matter how large the timestep is. This is essentially what we do (in our commercial product). We run our game logic at a constant timestep of 10hz but let the rendering thread run as fast as possible and draw frames as quickly as the GPU is ready for them. This means that physics and AI and entity logic isn't impacted by render speed (otherwise the world would go into slow-time when the rendering bogged down). The rendering isn't on a constant timestep it just goes as fast as it can (up to 60hz of course). @Crashworks : Exactly. You don't care about a fixed timestep for the rendering. It would make the rendering choppy. One can make the case that there *should* be a constant rendering timestep too since a variable framerate feels choppier than a consistent one and because the monitor is updating at 30/60hz (or 25/50 for PAL) so any frame that isn't aligned with a vsync interval will ""tear"" in the middle of the screen. It's very subjective though so we made ""wait for vsync"" a customer-settable option. Hah NTSC vs PAL differences was actually the reason why SEGA Genesis games for European consoles ran faster than North American ones. So even game consoles suffered from this issue though less than PC games from the same era. @Mads In which part of the above code (integrate or interpolate) is collision detection done?",c++ opengl sdl1270517,A,"Porting project to my laptop results in a blank screen So I'm making something in openGL using SDL. I'm about to take a long flight and I can't seem to get the project to work on my laptop. I've used SDL on my laptop before so I'm left thinking it is openGL's fault. The laptop is on win xp pro and has an intel 945 graphics ""card."" I've tried updating the drivers but to no avail. The images I'm using can't be the problem because I have it coded so the program closes if it can't locate the file. Also I get no errors at all when compiling it just creates the window and instead of all my images I get white. Just white. Any ideas? Please I don't want to be on this 5 hr flight and go stir crazy =/ Do the SDL NeHe (http://nehe.gamedev.net/lesson.asp?index=02) demos work? You might need to provide a bit more info but at a guess I'd say your textures are invalid. OpenGL draws white when there is a texture problem. Possible reasons are... Image size is bigger than the max texture size of the graphics chip Image isn't power of 2 and the card doesn't support rectangular textures. You have run out of texture memory. Your texture env isn't supported by the graphics chip eg. unsupported format. You aren't drawing what you think you are drawing eg a white quad on a white background in the position you think you are drawing it ie you're looking in the wrong direction. Programmer error. Try drawing a single 128x128 texture on a single quad with a glcolor of purple and a clear colour of orange. This will eliminate most of the above problems and give you something to debug. It's been a while but maybe this can help someone else. I knew about the power of 2 however it slipped my mind that my desktop has a much new/better gfx card than my laptop does. So while my desktop could compensate for the non-power-of-2-sizes my laptop's gfx card could not.",c++ gui opengl sdl721998,A,Why does my colored cube not work with GL_BLEND? My cube isn't rendering as expected when I use GL_BLEND. glEnable(GL_CULL_FACE); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE); I'm also having a similar problem with drawing some semi-opaque vertices in front which could well be related. Related: Why do my semi-opaque vertices make background objects brighter in OpenGL? Here's what it's supposed to look like: And here's what it actually looks like: Please see the code used to create the colored cube and the code used to actually draw the cube. The cube is being drawn like so: glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glPushMatrix(); glLoadIdentity(); // ... do some translation rotation etc ... drawCube(); glPopMatrix(); // ... swap the buffers ... Did you get the first picture from your program? It looks like you have lighting enabled on the second one try with a glShadeModel( GL_FLAT ) before drawing Hmm no this makes the sides of the cube solid colours but only 3 of them...  You could try disabling all lighting before drawing the cube: glDisable(GL_LIGHTING);  This has me stomped. What it looks like is that some vertices have some alpha values that are non-opaque. However the code you posted has all 1. for alpha. So... in order to debug more did you try to change your clear color to something non-black ? Say green ? From the code I doubt lighting is turned on since no normals were specified. Last comment offtopic... You should really not use glBegin/glEnd (2 function calls per vertex + 2 per primitive is really not a good usage of the recent developments in OpenGL). Try glDrawElements with QUAD_LIST or even better TRIANGLE_LIST. You already have the data nicely laid out for that.,c++ opengl blend1191093,A,"I'm seeing artifacts when I attempt to rotate an image This is the before: znd after: EDIT:: Now that I look at imageshack's upload the artifacts are diminished a great deal.. but trust me they are more pronounced than that. I don't understand why this is happening. Imageshack uploads them to jpg but in my program they are in the image folder as .tif (The reason for .tif is because I couldn't get ANY other image to maintain their transparent parts). But anyways these artifacts follow the original top of the image as it rotates anywhere except the original. Here's part of my code that loads the image GLuint texture; GLenum texture_format; GLint nofcolors; GLfloat spin; bool Game::loadImage() { SDL_Surface * surface; // this surface will tell us the details of the image if ( surface = SM.load_image(""Images/tri2.tif"") ) { //get number of channels in the SDL surface nofcolors = surface->format->BytesPerPixel; //contains an alpha channel if ( nofcolors == 4 ) { if ( surface->format->Rmask == 0x000000ff ) texture_format = GL_RGBA; else texture_format = GL_BGRA; } else if ( nofcolors == 3 ) //no alpha channel { if ( surface->format->Rmask == 0x000000ff ) texture_format = GL_RGB; else texture_format = GL_BGR; } // Have OpenGL generate a texture object handle for us glGenTextures( 1 &texture ); // Bind the texture object glBindTexture( GL_TEXTURE_2D texture ); // Set the texture‰Ûªs stretching properties glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR ); glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR ); glTexImage2D( GL_TEXTURE_2D 0 nofcolors surface->w surface->h 0 texture_format GL_UNSIGNED_BYTE surface->pixels ); glEnable(GL_TEXTURE_2D); glEnable(GL_BLEND); glBlendFunc(GL_ONE GL_ONE_MINUS_SRC_ALPHA); } else { SDL_Quit(); return false; } // Free the SDL_Surface only if it was successfully created if ( surface ) { SDL_FreeSurface( surface ); return true; } else return false; } void Game::drawImage() { // Clear the screen before drawing glClear( GL_COLOR_BUFFER_BIT ); glTranslatef( float(S_WIDTH/2) float(S_HEIGHT/2) 0.0f ); glRotatef( spin 0.0 0.0 1.0 ); // Bind the texture to which subsequent calls refer to glBindTexture( GL_TEXTURE_2D texture ); glBegin( GL_QUADS ); { // Top-left vertex (corner) glTexCoord2i( 0 0 ); glVertex3f( -64 0 0 ); // Top-right vertex (corner) glTexCoord2i( 1 0 ); glVertex3f( 64 0 0 ); // Bottom-right vertex (corner) glTexCoord2i( 1 1 ); glVertex3f( 64 128 0 ); // Bottom-left vertex (corner) glTexCoord2i( 0 1 ); glVertex3f( -64 128 0 ); } glEnd(); glLoadIdentity(); SDL_GL_SwapBuffers(); } lol when I loaded this page I thought your artifacts were dust on my screen and I tried wiping it off. Try saving as PNG? PNG format won't load transparency correctly in my program. Nor will anything else except TIF. It is driving me insane.. Looks like the texture is set to GL_WRAP. Try GL_CLAMP_TO_EDGE instead. Yep it's definitely wrapping. I just started OpenGL like a day or two ago I don't see any place where I typed GL_WRAP... so where do I put the clamp to edge part? Insert the following right after you set the min/mag filters: glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_CLAMP_TO_EDGE); Ah that worked thanks a lot. Now I gotta figure out this annoying (lack of) transparency in images.  In Game::loadImage after your glBindTexture call: glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_CLAMP_TO_EDGE); Your current setting is GL_REPEAT which is the OpenGL default.",c++ gui opengl789904,A,"OpenGL Color Matrix How do I get the OpenGL color matrix transforms working? I've modified a sample program that just draws a triangle and added some color matrix code to see if I can change the colors of the triangle but it doesn't seem to work.  static float theta = 0.0f; glClearColor( 1.0f 1.0f 1.0f 1.0f ); glClearDepth(1.0); glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glPushMatrix(); glRotatef( theta 0.0f 0.0f 1.0f ); glMatrixMode(GL_COLOR); GLfloat rgbconversion[16] = { 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f }; glLoadMatrixf(rgbconversion); glMatrixMode(GL_MODELVIEW); glBegin( GL_TRIANGLES ); glColor3f( 1.0f 0.0f 0.0f ); glVertex3f( 0.0f 1.0f  0.5f); glColor3f( 0.0f 1.0f 0.0f ); glVertex3f( 0.87f -0.5f 0.5f ); glColor3f( 0.0f 0.0f 1.0f ); glVertex3f( -0.87f -0.5f 0.5f ); glEnd(); glPopMatrix(); As far as I can tell the color matrix I'm loading should change the triangle to black but it doesn't seem to work. Is there something I'm missing? It looks like you're doing it correctly but your current color matrix sets the triangle's alpha value to 0 as well so while it is being drawn it does not appear on the screen. I added a glEnable(GL_BLEND); but it still doesn't affect the picture. It's still a red/green/blue triangle. It appears that the color matrix is having no effect. Actually the triangle is still drawn in its original red green and blue so I don't believe the color matrix is affecting the alpha component. the alpha component will only affect the final image if transparency is enabled which it apparently is not.  ""Additionally if the ARB_imaging extension is supported GL_COLOR is also accepted."" From the glMatrixMode documentation. Is the extension supported on your machine? As far as I know ARB_imaging is enabled but how do I find out? I'm using VS 2008. I've run the OpenGL Extensions Viewer and it does say that GL_ARB_imaging is a valid extension on my machine.  I have found the possible problem. The color matrix is supported by the ""Image Processing Subset"". In most HW it was supported by driver.(software implementation) Solution: Add this line after glEnd(): glCopyPixels(00 getWidth() getHeight()GL_COLOR); It's very slow....  The color matrix only applies to pixel transfer operations such as glDrawPixels which aren't hardware accelerated on current hardware. However implementing a color matrix using a fragment shader is really easy. You can just pass your matrix as a uniform mat4 then mulitply it with gl_FragColor",c++ opengl colors525227,A,"Console menu updating OpenGL window I am making an application that does some custom image processing. The program will be driven by a simple menu in the console. The user will input the filename of an image and that image will be displayed using openGL in a window. When the user selects some processing to be done to the image the processing is done and the openGL window should redraw the image. My problem is that my image is never drawn to the window instead the window is always black. I think it may have to do with the way I am organizing the threads in my program. The main execution thread handles the menu input/output and the image processing and makes calls to the Display method while a second thread runs the openGL mainloop. Here is my main code: #include <iostream> #include <GL/glut.h> #include ""ImageProcessor.h"" #include ""BitmapImage.h"" using namespace std; DWORD WINAPI openglThread( LPVOID param ); void InitGL(); void Reshape( GLint newWidth GLint newHeight ); void Display( void ); BitmapImage* b; ImageProcessor ip; int main( int argc char *argv[] ) { DWORD threadID; b = new BitmapImage(); CreateThread( 0 0 openglThread NULL 0 &threadID ); while( true ) { char choice; string path = ""TestImages\\""; string filename; cout << ""Enter filename: ""; cin >> filename; path += filename; b = new BitmapImage( path ); Display(); cout << ""1) Invert"" << endl; cout << ""2) Line Thin"" << endl; cout << ""Enter choice: ""; cin >> choice; if( choice == '1' ) { ip.InvertColour( *b ); } else { ip.LineThinning( *b ); } Display(); } return 0; } void InitGL() { int argc = 1; char* argv[1]; argv[0] = new char[20]; strcpy( argv[0] ""main"" ); glutInit( &argc argv ); glutInitDisplayMode( GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH); glutInitWindowPosition( 0 0 ); glutInitWindowSize( 800 600 ); glutCreateWindow( ""ICIP Program - Character recognition using line thinning Hilbert curve and wavelet approximation"" ); glutDisplayFunc( Display ); glutReshapeFunc( Reshape ); glClearColor(0.00.00.01.0); glEnable(GL_DEPTH_TEST); } void Reshape( GLint newWidth GLint newHeight ) { /* Reset viewport and projection parameters */ glViewport( 0 0 newWidth newHeight ); } void Display( void ) { glClear (GL_COLOR_BUFFER_BIT); // Clear display window. b->Draw(); glutSwapBuffers(); } DWORD WINAPI openglThread( LPVOID param ) { InitGL(); glutMainLoop(); return 0; } Here is my draw method for BitmapImage: void BitmapImage::Draw() { cout << ""Drawing"" << endl; if( _loaded ) { glBegin( GL_POINTS ); for( unsigned int i = 0; i < _height * _width; i++ ) { glColor3f( _bitmap_image[i*3] / 255.0 _bitmap_image[i*3+1] / 255.0 _bitmap_image[i*3+2] / 255.0 ); // invert the y-axis while drawing glVertex2i( i % _width _height - (i / _width) ); } glEnd(); } } Any ideas as to the problem? Edit: The problem was technically solved by starting a glutTimer from the openglThread which calls glutPostRedisplay() every 500ms. This is OK for now but I would prefer a solution in which I only have to redisplay every time I make changes to the bitmap (to save on processing time) and one in which I don't have to run another thread (the timer is another thread im assuming). This is mainly because the main processing thread is going to be doing a lot of intensive work and I would like to dedicate most of the resources to this thread rather than anything else. You need to make OpenGL calls on the thread in which context was created (glutInitDisplayMode). Hence glXX calls inside Display method which is on different thread will not be defined. You can see this easily by dumping the function address hopefully it would be undefined or NULL.  It sounds like the 500ms timer is calling Display() regularly after 2 calls it fills the back-buffer and the front-buffer with the same rendering. Display() continues to be called until the user enters something which the OpenGL thread never knows about but since global variable b is now different the thread blindly uses that in Display(). So how about doing what Jesse Beder says and use a global int call it flag to flag when the user entered something. For example: set flag = 1; after you do the b = new BitmapImage( path ); then set flag = 0; after you call Display() from the OpenGL thread. You loop on the timer but now check if flag = 1. You only need call glutPostRedisplay() when flag = 1 i.e. the user entered something. Seems like a good way without using a sleep/wake mechanism. Accessing global variables among more than one thread can also be unsafe. I think the worst that can happen here is the OpenGL thread miss-reads flag = 0 when it should read flag = 1. It should then catch it after no more than a few iterations. If you get strange behavior go to synchronization. With the code you show you call Display() twice in main(). Actually main() doesn't even need to call Display() the OpenGL thread does it.  Your problem may be in Display() at the line b->Draw(); I don't see where b is passed into the scope of Display(). b is declared globally just above main() Yes I see it now in the morning light.  I've had this problem before - it's pretty annoying. The problem is that all of your OpenGL calls must be done in the thread where you started the OpenGL context. So when you want your main (input) thread to change something in the OpenGL thread you need to somehow signal to the thread that it needs to do stuff (set a flag or something). Note: I don't know what your BitmapImage loading function (here your constructor) does but it probably has some OpenGL calls in it. The above applies to that too! So you'll need to signal to the other thread to create a BitmapImage for you or at least to do the OpenGL-related part of creating the bitmap. That's not entirely accurate the requirement is that the OpenGL context can only be _current_ to any single thread. The context can however be created in a different thread than which it is current. This is useful for offscreen rendering for example. Just to clarify the problem in this case is definitely the multithreaded rendering though since the OP is making gl calls from different threads using the same context or rather a non-existent context (in the main thread). Re comment #1 that's true but I thought it was irrelevant (and unnecessary complication) for this particular example.  A few points: Generally if you're going the multithreaded route it's preferable if your main thread is your GUI thread i.e. it does minimal tasks keeping the GUI responsive. In your case I would recommend moving the intensive image processing tasks into a thread and doing the OpenGL rendering in your main thread. For drawing your image you're using vertices instead of a textured quad. Unless you have a very good reason it's much faster to use a single textured quad (the processed image being the texture). Check out glTexImage2D and glTexSubImage2D. Rendering at a framerate of 2fps (500ms as you mentioned) will have negligible impact on resources if you're using an OpenGL implementation that is accelerated which is almost guaranteed on any modern system and if you use a textured quad instead of a vertex per pixel.",c++ multithreading opengl console679113,A,"Trouble porting OpenGL app to Windows I am trying to move an OpenGL app to Windows. It was my understanding that Windows had a decent OpenGL implementation. But I'm starting to think that it doesn't... Specifically I use array buffers and glDrawArrays. When I tried to compile my code in Visual Studio 2008 Pro I received the following errors: vertexbuffers.cpp(31) : error C3861: 'glGenBuffers': identifier not found vertexbuffers.cpp(32) : error C2065: 'GL_ARRAY_BUFFER' : undeclared identifier vertexbuffers.cpp(32) : error C3861: 'glBindBuffer': identifier not found vertexbuffers.cpp(33) : error C2065: 'GL_ARRAY_BUFFER' : undeclared identifier vertexbuffers.cpp(33) : error C2065: 'GL_STATIC_DRAW' : undeclared identifier vertexbuffers.cpp(33) : error C3861: 'glBufferData': identifier not found When I examined <GL\gl.h> (contained in C:\Program Files\Microsoft SDKs\Windows\v6.0A\Include\gl) I saw: /* ClientArrayType */ /* GL_VERTEX_ARRAY */ /* GL_NORMAL_ARRAY */ /* GL_COLOR_ARRAY */ Update but it would seem that those contants get defined elsewhere. How am I supposed to generate buffers if I don't have access to those functions? The documentation doesn't say that those array types are disabled. How do I get access to the real implementation on OpenGL on Windows? Could you past the specific errors? It is possible that those comments are for documentation and that things are properly defined elsewhere. Yes I jumped the gun with those functions. My real errors come from the Buffer generation functions. Sorry for the confusion I have edited the question. Thanks for posting those it helped me understand and fix up my answer. :) The #defines are commented out in the header file whenever they would otherwise be repeated. Look at line 1054 of gl.h: /* vertex_array */ #define GL_VERTEX_ARRAY 0x8074 If this #define is actually missing then you should probably replace the file with a fresh copy. If you look at the documentation for glGenBuffers you will see that it is only available in OpenGL 1.5 and higher. The header file for Windows only comes with OpenGL 1.2 and you should use the extension mechanism to access the newer functionality. If you call wglGetProcAddress with the function name e.g. void (__stdcall *glGenBuffers)(GLsizeiGLuint*) = wglGetProcAddress(""glGenBuffers""); then you have a pointer to the function. You have better eyes than me. Thank you for the reference. Unfortunately I still have errors revolving around glBindBuffer. edited after i looked at the errors you were recieving. unfortunately the header is 1.2 only but there are easy fixes. :)  Microsoft's support for OpenGL stretches only as far as OpenGL-1.1 up to Windows XP and OpenGL-1.4 starting with Vista. Any OpenGL functionality beyond those must be delivered and supported by the installable client driver (ICD) i.e. the GPU driver's OpenGL implmenentation. To access the advanced functionality OpenGL provides the so called Extension System formed by wglGetProcAddress which is kind of like GetProcAddress for DLLs but gives access to functions of the OpenGL implementation (=driver). To make things easier nice wrapper libraries like GLEW have been developed which do all the grunt work initializing all the available OpenGL extensions providing them to the end user.  It would seem that the buffer functions are only available on Windows as extension methods. OpenGL provides glext.h that declares pointers to all of these functions. It is then up to my app to use wglGetProcAddress to get pointers to the functions. For example: PFNGLGENBUFFERSPROC myglBindBuffers = (PFNGLGENBUFFERSPROC)wglGetProcAddress(""glGenBuffersARB""); Thankfully I only have to do it for about 4 functions. Unfortunately I now have to add platform-dependent code to my app. +1 for the sensible way to find these type definitions - thanks!  You might give GLEW a shot: http://glew.sourceforge.net/ I'm pretty sure I used it at some time in the past and makes this sort of thing a little easier and more portable. Great link thank you very much.",c++ windows visual-studio opengl