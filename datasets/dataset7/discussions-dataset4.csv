603114,A,Fonts for Carbon OpenGL app on OS X I'm trying to add text rendering to a Carbon OpenGL app I'm developing for OS X. Since the aglUseFont is now deprecated I'm looking for another way to add text as well as be able to query the glyph properties (i.e. width height spacing etc) So far I've investigated CoreText and ATSUI but both without much luck. Please help me!! Thanks! You could take a look at the FreeType project: it's an open source portable font rendering engine that supports OpenType TrueType Postscript Type 1 and other formats. There are several open source integrations of FreeType with OpenGL; see for example OGLFT. Or you could just roll your own: it's not hard to make FreeType generate bitmaps in some suitable pixel format and then pass these bitmaps to glTexImage2D.  In the end I just went with good old glBitmap for my fonts. Found an apple dev sample that created rendered each character and got its pertinent info (width height offset etc.) However if I get the time to do some more work on it later I plan on using the FreeType project as was suggested above. Thanks!,c++ osx opengl fonts carbon
340185,A,"using GDAL/OGR api to read vector data (shapefile)--How? I am working on an application that involves some gis stuff. There would be some .shp files to be read and plotted onto an opengl screen. The current opengl screen is using the orthographic projection as set from glOrtho() and is already displaying a map using coordinates from a simple text file.. Now the map to be plotted is to be read from a shapefile. I have the following doubts: How to use the WGS84 projection of the .shp file(as read from the .prj file of the shapefileWKT format) into my existing glOrtho projection..is there any conversion that needs to be done? and how is it different from what the glOrtho() sets up?basically how to use this information? My application needs to be setup in such a way that i can know the exact lat/long of a point on the map.for eg. if i am hovering on X cityits correct lat/long could be fetched.I know that this can be done by using opensource utils/apis like GDAL/OGR but i am messed up as the documentation of these apis are not getting into my head. I tried to find some sample c++ progs but couldnt find one. I have already written my own logic to read the coordinates from a shapefile containing either points/polyline/polygon(using C-shapelib) and plotted over my opengl screen.I found a OGR sample code in doc to read a POINTS shapefile but none for POLYGON shapefile.And the problem is that this application has to be so dynamic that upon loading the shapefileit should correctly setup the projection of the opengl screen depending upon the projection of the .shp file being read..eg WGS84LCCEVEREST MODIFIED...etc. how to achieve this from OGR api? Kindly give your inputs on this problem.. I am really keen to make this work but im not getting the right start.. 1.Shapefile rendering is quite straight forward in OpenGL.You may require ""shapelib""a free shapefile parsing library in C(google for it).Use GL_POINTS for point shapefile GL_LINES for line shapefile and GL_LINE_LOOP for polygon shapefile.Set your bounding box coods to the Ortho. 2.What u read from .prj file is projection info.WGS84 gives u lat/long coods(Spherical). But ur display system is 2D(Rectangular).Sou need to convert 3D Spherical coods to 2D Rectangular coods(This is the meaning of Projection).Projection types are numerousdepending on the area of interest on the globe(remeber projection distorts area/shape/size of features).Projection types range from PolyconicModified EverestNADUTM etc. 3.If u simly need WGS84 then read bounding box coods of ur sh file and assign them to glOrtho.If u have any projection(eg:-UTM) then u convert ur bounding box coods into Projection coods and then assign the newly projected coods to glOrtho.For converting lat/long into any Projectionu may require projection libraries like ""Projlib"" or ""GeotransEngine"" and etc. For further clarifications u may contact me on dgplinux@ y a h o o . c o m  Please read the OGR API Tutorial where you can learn how to read vector data from sources like Shapefile. Next check the OGR Projections Tutorial where you can learn about how to use information about projection and spatial reference system read from OGR sources.  GDAL/OGR has everything you need to load a vector file then convert any coordinates. I understand your frustration with GDAL as the documentation is not the greatest. If you want a good intro to using the API look at gdalinfo.c and ogrinfo.cpp in the GDAL subversion tree. Source can be seen at https://svn.osgeo.org/gdal/trunk/gdal. If that does not help I have two basic examples that I use to parse vector info and do coordinate conversion. They are really bad but they may help get the point. Vector Loading Coordinate Conversion Finally if you are not familiar with GIS formats I would consider reading the ArcGIS introduction here under Guide Books/Map Projections. I can compete with experts despite no cartography training due to these guides. Another good source is Wikipedia. When in doubt just pick a UTM grid you want to stick with and use UTM as your coordinate system. It uses X (Easting) Y(Northing) and Z(Altitude). The only key is picking a single UTM Grid and making sure all coordinates use that as a reference. UTM is easy to test code with as there are a lot of guides online. You also can find conversion code using OGR/GDAL or other resources. Other projected coordinate systems are worthwhile and may be better but I would look at that to start with. Finally if all else fails take a look at the NGA GeoTrans. That is a great testing tool.",c++ linux opengl gdal
1009150,A,"C++/OpenGL - Rotating a rectangle For my project i needed to rotate a rectangle. I thought that would be easy but i'm getting an unpredictable behavior when running it.. Here is the code:  glPushMatrix(); glRotatef(30.0f 0.0f 0.0f 1.0f); glTranslatef(vec_vehicle_position_.x vec_vehicle_position_.y 0); glEnable(GL_TEXTURE_2D); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glBegin(GL_QUADS); glTexCoord2f(0.0f 0.0f); glVertex2f(0 0); glTexCoord2f(1.0f 0.0f); glVertex2f(width_sprite_ 0); glTexCoord2f(1.0f 1.0f); glVertex2f(width_sprite_ height_sprite_); glTexCoord2f(0.0f 1.0f); glVertex2f(0 height_sprite_); glEnd(); glDisable(GL_BLEND); glDisable(GL_TEXTURE_2D); glPopMatrix(); The problem with that is that my rectangle is making a translation somewhere in the window while rotating. In other words the rectangle doesn't keep the position : vec_vehicle_position_.x and vec_vehicle_position_.y. What's the problem ? Thanks To elaborate on the previous answers. Transformations in OpenGL are performed via matrix multiplication. In your example you have: M_r_ - the rotation transform M_t_ - the translation transform v - a vertex and you had applied them in the order: M_r_ * M_t_ * v Using parentheses to clarify: ( M_r_ * ( M_t_ * v ) ) We see that the vertex is transformed by the closer matrix first which is in this case the translation. It can be a bit counter intuitive because this requires you to specify the transformations in the order opposite of that which you want them applied in. But if you think of how the transforms are placed on the matrix stack it should hopefully make a bit more sense (the stack is pre-multiplied together). Hence why in order to get your desired result you needed to specify the transforms in the opposite order. Indeed :) Thanks for the update !  Make sure the rotation is applied before the translation.  Inertiatic provided a very good response. From a code perspective your transformations will happen in the reverse order they appear. In other words transforms closer to the actual drawing code will be applied first. For example: glRotate(); glTranslate(); glScale(); drawMyThing(); Will first scale your thing then translate it then rotate it. You effectively need to ""read your code backwards"" to figure out which transforms are being applied in which order. Also keep in mind what the state of these transforms is when you push and pop the model-view stack.  You need to flip the order of your transformations: glRotatef(30.0f 0.0f 0.0f 1.0f); glTranslatef(vec_vehicle_position_.x vec_vehicle_position_.y 0); becomes glTranslatef(vec_vehicle_position_.x vec_vehicle_position_.y 0); glRotatef(30.0f 0.0f 0.0f 1.0f); Thanks a lot it works !",c++ opengl rotation
1272120,A,What are the error messages for breaking the GLSL shader instruction limits? We're a small dev team working with some GLSL that may be too large for older graphics cards to compile. We want to display a sensible error message to the user (rather than just dump the info log or output a generic 'this shader didn't work' type of message) when this happens based on the type of error. The question is ATI and nVidia have different conventions for these error messages and the only way I've found to decide what type of error the shader had is to parse the error string generated by glGetShaderInfoLog. Given that is there a listing somewhere or does anyone know what the error output for both ATI and nVidia cards looks like? Or is there a better way to detect when the instruction limit has been exceeded? Even if you know what the error messages look like now nVidia and ATI are under no obligation to keep them the same in the next version(s) of their drivers. They basically can't be relied on for anything except debugging purposes. I would look and see if the vendor extensions might be able to provide you with more specific diagnostic information. http://petewarden.com/notes/archives/2005/06/fragment_progra_3.html did the trick.,c++ opengl glsl
1218876,A,"Problem with using OpenGL's VBO I just tried to render the first redbook example ( the white Quad ) by using VBOs. It works fine with immediate mode and vertex arrays. But when using VBOs the screen stays black. I think i must have missed something important. init: unsigned int bufIds[2]; glGenBuffers( 2 bufIds ); GLfloat vertices[] = { 0.25 0.25 0.0 0.75 0.25 0.0 0.75 0.75 0.0 0.25 0.75 0.0 }; glBindBuffer( GL_ARRAY_BUFFER bufIds[0] ); glBufferData( GL_ARRAY_BUFFER 12 vertices GL_STATIC_DRAW ); glBindBuffer( GL_ARRAY_BUFFER 0 ); glClearColor( 0 0 0 1 ); glColor3f( 1 1 1 ); glOrtho( 0.0 1.0 0.0 1.0 -1.0 1.0 ); renderloop for VBO (not working): glClear( GL_COLOR_BUFFER_BIT ); glEnableClientState( GL_VERTEX_ARRAY ); glBindBuffer( GL_ARRAY_BUFFER bufIds[0] ); glVertexPointer( 3 GL_FLOAT 0 0 ); glDrawArrays( GL_QUADS 0 12 ); glBindBuffer( GL_ARRAY_BUFFER 0 ); glDisableClientState( GL_VERTEX_ARRAY ); renderloop for vertex arrays (working): glClear( GL_COLOR_BUFFER_BIT ); glEnableClientState( GL_VERTEX_ARRAY ); glBindBuffer( GL_ARRAY_BUFFER 0 ); glVertexPointer( 3 GL_FLOAT 0 vertices ); glDrawArrays( GL_QUADS 0 12 ); glDisableClientState( GL_VERTEX_ARRAY ); argh i just figured it out by trying to read back the contents of the buffer: i need to allocate the buffer with 12 * sizeof( GLfloat ) instead of only 12 glBufferData( GL_ARRAY_BUFFER 12 * sizeof( GLfloat ) vertices GL_STATIC_DRAW ); my read back code GLfloat vertices2[12]; glBindBuffer( GL_ARRAY_BUFFER bufIds[0] ); glGetBufferSubData ( GL_ARRAY_BUFFER 0 12 * sizeof( GLfloat ) vertices2 ); glBindBuffer( GL_ARRAY_BUFFER 0 ); for ( int i = 0; i < 4; i ++ ) { LOG_DEBUG << ""point "" << i << "": "" << vertices2[ i * 3 + 0 ] << "" / "" << vertices2[ i * 3 + 1 ] << "" / "" << vertices2[ i * 3 + 2 ]; } +1 Awesome! I had exactly the same problem thank you so much!",c++ opengl vbo
640874,A,How do draw to a texture in OpenGL Now that my OpenGL application is getting larger and more complex I am noticing that it's also getting a little slow on very low-end systems such as Netbooks. In Java I am able to get around this by drawing to a BufferedImage then drawing that to the screen and updating the cached render every one in a while. How would I go about doing this in OpenGL with C++? I found a few guides but they seem to only work on newer hardware/specific Nvidia cards. Since the cached rendering operations will only be updated every once in a while i can sacrifice speed for compatability. glBegin(GL_QUADS); setColor(DARK_BLUE); glVertex2f(0 0); //TL glVertex2f(appWidth 0); //TR setColor(LIGHT_BLUE); glVertex2f(appWidth appHeight); //BR glVertex2f(0 appHeight); //BR glEnd(); This is something that I am especially concerned about. A gradient that takes up the entire screen is being re-drawn many times per second. How can I cache it to a texture then just draw that texture to increase performance? Also a trick I use in Java is to render it to a 1 X height texture then scale that to width x height to increase the performance and lower memory usage. Is there such a trick with openGL? I sincerely doubt drawing from a texture is less work than drawing a gradient. In drawing a gradient: Color is interpolated at every pixel In drawing a texture: Texture coordinate is interpolated at every pixel Color is still interpolated at every pixel Texture lookup for every pixel Multiply lookup color with current color Not that either of these are slow but drawing untextured polygons is pretty much as fast as it gets.  If you don't want to use Framebuffer Objects for compatibility reasons (but they are pretty widely available) you don't want to use the legacy (and non portable) Pbuffers either. That leaves you with the simple possibility of reading the contents of the framebuffer with glReadPixels and creating a new texture with that data using glTexImage2D. Let me add that I don't really think that in your case you are going to gain much. Drawing a texture onscreen requires at least texel access per pixel that's not really a huge saving if the alternative is just interpolating a color as you are doing now!  Consider using a display list rather than a texture. Texture reads (especially for large ones) are a good deal slower than 8 or 9 function calls. So would a display list that draws a full screen gradient be faster than just caching the result and drawing that over and over? It's something you'll want to benchmark yourself but yes I believe so. See Sorren's answer for the rationale.  Look into FBOs - framebuffer objects. It's an extension that lets you render to arbitrary rendertargets including textures. This extension should be available on most recent hardware. This is a fairly good primer on FBOs: OpenGL Frame Buffer Object 101 The whole point of the question is that he can't use FBOs because he wants the lowest possible hardware denominator.  Before doing any optimization you should make sure you fully understand the bottlenecks. You'll probably be surprised at the result.  Hey there thought I'd give you some insight in to this. There's essentially two ways to do it. Frame Buffer Objects (FBOs) for more modern hardware and the back buffer for a fall back. The article from one of the previous posters is a good article to follow on it and there's plent of tutorials on google for FBOs. In my 2d Engine (Phoenix) we decided we would go with just the back buffer method. Our class was fairly simple and you can view the header and source here: http://code.google.com/p/phoenixgl/source/browse/branches/0.3/libPhoenixGL/PhRenderTexture.h http://code.google.com/p/phoenixgl/source/browse/branches/0.3/libPhoenixGL/PhRenderTexture.cpp Hope that helps! links are dead: the branch 0.3 isn't anymore Here's a link to the equivalent file in the newer version of phoenix: https://github.com/jonparrott/PhoenixCore/blob/master/source/RenderTarget.h,c++ opengl graphics
664698,A,"How to I draw pixels as a texture to a polygon in OpenGL? In C++ OpenGL I want to draw each pixel manually (as a texture I assume) on to a simple square primitive or indeed 2 polygons forming a square. I have no idea where to start or what phrases to look for. Am I looking for texture mapping or creating textures? Most examples are to load from a file but I dont want to do that. I've tried reading my OpenGL Programming Guide book but its just a maze as I'm quite new to OpenGL. Please help. In other words you want to draw a texture without perspective? After you see the right tutorials check my post over here -> http://stackoverflow.com/questions/503816/linux-fastest-way-to-draw/504440#504440 The geometry is to be rotated transformed etc. in a 3D sense. I'm not sure what you mean ""draw without perspective""... Thanks to Reed Copsey for pointing me towards glTexImage2D. Turns out this is very simple; just pass an array of GLubyte to the glTexImage2D function (as well as all the functions needed to bind the texture etc). Haven't tried this exact snippet of code but it should work fine. The array elements represent a serial version of the rows columns and channels. int pixelIndex = 0; GLubyte pixels[400]; for (int x = 0; x < 10; x++) { for (int y = 0; y < 10; x++) { for (int channel = 0; channel < 4; channel++) { // 0 = black 255 = white pixels[pixelIndex++] = 255; } } } glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA SIZE SIZE 0 GL_RGBA GL_UNSIGNED_BYTE pixels); I've read in the OpenGL book you can use a 2D array for monochrome images so I assume you could use a 3D array also.  If the square you are drawing to is 2 dimensional and not rotated you may be looking for glDrawPixels. It allows you to draw a rectangular region of pixels directly to the buffer without using any polygons.  If you are new then I would recommend starting from tutorial one on from NeHe.  Take a close look at glTexImage2D. This is the call that loads the image in OpenGL for a 2D Texture. glTexImage2D actually takes a raw pointer to the pixel data in the image. You can allocate memory yourself and set the image data directly (however you want) then call this function to pass the image to OpenGL. Once you've created the texture you can bind it to the state so that your triangles use that texture. There is nothing in OpenGL that directly requires images used as textures to be loaded from files. A good resource for textures can be found in this GameDev article.  What you are looking for is generally called ""render to texture."" http://developer.nvidia.com/object/gdc_oglrtt.html That tutorial is pretty old. I'd guess that some of those extensions aren't actually extensions in newer versions of OGL. Sorry I can't be more help. I don't work directly with graphics API's much any more and if I did I'd use D3D if I could (to avoid the extension soup and for other reasons I won't bore you with here.)  Do note that while glTexImage2D() takes a pointer to the pixels to load it does not care about any changes to that data after the call. That is the data is really loaded from the memory area you specify and OpenGL creates a copy of its own. This means that if the data changes you will need to either re-do the glTexImage2D() call or use something like glTexSubImage2D() to do a partial update. I would recommend testing and profiling to see if sub-texture updating is quick not sure how actual drivers optimize for that case.",c++ opengl textures
1122203,A,"How do I get all the shader constants (uniforms) from a ID3DXEffect? I'm creating an effect using hr = D3DXCreateEffectFromFile( g_D3D_Device shaderPath.c_str() macros NULL 0 NULL &pEffect &pBufferErrors ); I would like to get all the uniforms that this shader is using. In OpenGL I used glGetActiveUniform and glGetUniformLocation to get constant's size type name etc. Is there a D3DX9 equivalent function? Thanks! D3DXHANDLE handle = m_pEffect->GetParameterByName( NULL ""Uniform Name"" ); if ( handle != NULL ) { D3DXPARAMETER_DESC desc; if ( SUCCEEDED( m_pEffect->GetParameterDesc( handle &desc ) ) ) { // You now have pretty much all the details about the parameter there are in ""desc"". } } You can also iterate through each parameter by doing the following: UINT index = 0; while( 1 ) { D3DXHANDLE handle = m_pEffect->GetParameter( NULL index ); if ( handle == NULL ) break; // Get parameter desc as above. index++; }",c++ opengl graphics 3d directx
1824656,A,Hiding the Cursor I have a windows program with directx/opengl renderers and a custom mouse rendered as a quad. The program currently runs windowed. The problem is the standard windows mouse is overlaid ontop of my custom cursor. How do I hide it when its inside my window? Try ShowCursor(FALSE); when you init your window.,c++ windows opengl directx mouse
1919251,A,Display image in opengl I am fairly new to openGL. I have a 3d game that I have running and it seems to go fairly well. What I would like to do is display an image straight onto the screen and I am not sure the easiest way to do that. My only idea is to draw a rectangle right in front of the screen and use the image as the texture. It seems like there should be an easier way. This is for menu screens and things so if there is a better way to do that as well please let me know. I would recommend setting up OpenGL for 2D rendering via gluOrtho2d(); then load the image into a texture and as you said draw it to the screen by creating a polygon and binding the texture to it. A good example can be found here.  You've got the basic idea. The other obvious alternative is to use glDrawPixels() but I think you'll find the texture method has much better performance. If you're feeling frisky you might also take a look at Pixel Buffer Objects. Good luck!,c++ opengl
181731,A,Texture Sampling in Open GL i need to get the color at a particular coordinate from a texture. There are 2 ways i can do this by getting and looking at the raw png data or by sampling my generated opengl texture. Is it possible to sample an opengl texture to get the color (RGBA) at a given UV or XY coord? If so how? Off the top of my head your options are Fetch the entire texture using glGetTexImage() and check the texel you're interested in. Draw the texel you're interested in (eg. by rendering a GL_POINTS primitive) then grab the pixel where you rendered it from the framebuffer by using glReadPixels. Keep a copy of the texture image handy and leave OpenGL out of it. Options 1 and 2 are horribly inefficient (although you could speed 2 up somewhat by using pixel-buffer-objects and doing the copy asynchronously). So my favourite by FAR is option 3. Edit: If you have the GL_APPLE_client_storage extension (ie. you're on a Mac or iPhone) then that's option 4 which is the winner by a long way. Outside of option 2 you need a method for sampling based on the texcoords though which is the hard part -- this is described in my answer. My understanding was that the poster was asking how to implement the 'getTexel' part of your example not how to replicate the filtering (which depends on what kind of filtering he wants to replicate anyway). I was sort of asking about the getTexel part but he's right that 1 and 2 are too slow esp considering i'm working on a mobile device. Thanks :) Mobile device? Would that be an iPhone by any chance? See updated answer. indeed it would Mike F thanks for the tip... altho in this case it does turn out that the data can be pre processed as raw image data before the creation of the openGL texture.  The most efficient way I've found to do it is to access the texture data (you should have our PNG decoded to make into a texture anyway) and interpolate between the texels yourself. Assuming your texcoords are [01] multiply texwidth*u and texheight*v and then use that to find the position on the texture. If they're whole numbers just use the pixel directly otherwise use the int parts to find the bordering pixels and interpolate between them based on the fractional parts. Here's some HLSL-like psuedocode for it. Should be fairly clear: float3 sample(float2 coord texture tex) { float x = tex.w * coord.x; // Get X coord in texture int ix = (int) x; // Get X coord as whole number float y = tex.h * coord.y; int iy = (int) y; float3 x1 = getTexel(ix iy); // Get top-left pixel float3 x2 = getTexel(ix+1 iy); // Get top-right pixel float3 y1 = getTexel(ix iy+1); // Get bottom-left pixel float3 y2 = getTexel(ix+1 iy+1); // Get bottom-right pixel float3 top = interpolate(x1 x2 frac(x)); // Interpolate between top two pixels based on the fractional part of the X coord float3 bottom = interpolate(y1 y2 frac(x)); // Interpolate between bottom two pixels return interpolate(top bottom frac(y)); // Interpolate between top and bottom based on fractional Y coord }  As others have suggested reading back a texture from VRAM is horribly inefficient and should be avoided like the plague if you're even remotely interested in performance. Two workable solutions as far as I know: Keep a copy of the pixeldata handy (wastes memory though) Do it using a shader,c++ opengl
680125,A,Can I use a grayscale image with the OpenGL glTexImage2D function? I have a texture which has only 1 channel as it's a grayscale image. When I pass the pixels in to glTexImage2D it comes out red (obviously because channel 1 is red; RGB). glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA dicomImage->GetColumns() dicomImage->GetRows() 0 GL_RGBA GL_UNSIGNED_BYTE pixelArrayPtr); Do I change GL_RGBA? If so what to? It appears that I should use GL_LUMINANCE instead of GL_RGBA for the 3rd argument. Edit (in reply to comments): When I set the 7th argument to GL_LUMINANCE (as well as the 3rd) the picture goes completely distorted. With the DICOM pixel format it appears that the 7th argument must be GL_RGBA for some reason. The strange behavior is because I'm using the DICOM standard. The particular DICOM reader I am using outputs integer pixel values (as pixel values may exceed the normal maximum of 255). For some strange reason the combination of telling OpenGL that I am using an RGBA format but passing in integer values rendered a perfect image. Because I was truncating the DICOM > 255 pixel values anyway it seemed logical to copy the values in to a GLbyte array. However after doing so a SIGSEGV (segmentation fault) occurred when calling glTexImage2D. Changing the 7th parameter to GL_LUMINANCE (as is normally required) returned the functionality to normal. Weird eh? So a note to all developers using the DICOM image format: You need to convert the integer array to a char array before passing it to glTexImage2D or just set the 7th argument to GL_RGBA (the later is probably not recommended). My case is unusual see my edit for explanation. It is argument #7 that counts here! Hmm when change #7 it goes red... but #3 has the desired effect. Can you explain this please? As ypnos says arguments 7 and 8 are what you're interested in. They tell OpenGL the format of the data which the pointer points to. The earlier arguments indicate the type of texture as I recall. Nick argument #3 is for internal storage #7 is for input format. GL will convert from #7 to #3. The effect you are getting can have several reasons I can't tell from here. In almost every case however you want both be the same anyway.  Change it to GL_LUMINANCE. See http://www.opengl.org/documentation/specs/man_pages/hardcopy/GL/html/gl/teximage2d.html It seems your link is broken. (It redirects to the home page). [glTextImage2D](http://www.opengl.org/sdk/docs/man/xhtml/glTexImage2D.xml) works but it doesn't mention GL_LUMINANCE anywhere. It seems they obsoleted it... Deprecated in OpenGL 4.0. But OpenGL 4.0 does not have a fixed function pipeline you need to bind your own shaders that's why there is no use in LUMINANCE ALPHA etc. any more. You need to use GL_RED and process the single channel accordingly.,c++ opengl textures
1259461,A,"How to fix weird camera rotation while moving camera with sdl opengl in c++ I have a camera object that I have put together from reading on the net that handles moving forward and backward strafe left and right and even look around with the mouse. But when I move in any direction plus try to look around it jumps all over the place but when I don't move and look around its fine. I'm hoping someone can help me work out why I can move and look around at the same time? main.h #include ""SDL/SDL.h"" #include ""SDL/SDL_opengl.h"" #include <cmath> #define CAMERASPEED 0.03f // The Camera Speed struct tVector3 // Extended 3D Vector Struct { tVector3() {} // Struct Constructor tVector3 (float new_x float new_y float new_z) // Init Constructor { x = new_x; y = new_y; z = new_z; } // overload + operator tVector3 operator+(tVector3 vVector) {return tVector3(vVector.x+x vVector.y+y vVector.z+z);} // overload - operator tVector3 operator-(tVector3 vVector) {return tVector3(x-vVector.x y-vVector.y z-vVector.z);} // overload * operator tVector3 operator*(float number) {return tVector3(x*number y*number z*number);} // overload / operator tVector3 operator/(float number) {return tVector3(x/number y/number z/number);} float x y z; // 3D vector coordinates }; class CCamera { public: tVector3 mPos; tVector3 mView; tVector3 mUp; void Strafe_Camera(float speed); void Move_Camera(float speed); void Rotate_View(float speed); void Position_Camera(float pos_x float pos_yfloat pos_z float view_x float view_y float view_z float up_x float up_y float up_z); }; void Draw_Grid(); camera.cpp #include ""main.h"" void CCamera::Position_Camera(float pos_x float pos_y float pos_z float view_x float view_y float view_z float up_x float up_y float up_z) { mPos = tVector3(pos_x pos_y pos_z); mView = tVector3(view_x view_y view_z); mUp = tVector3(up_x up_y up_z); } void CCamera::Move_Camera(float speed) { tVector3 vVector = mView - mPos; mPos.x = mPos.x + vVector.x * speed; mPos.z = mPos.z + vVector.z * speed; mView.x = mView.x + vVector.x * speed; mView.z = mView.z + vVector.z * speed; } void CCamera::Strafe_Camera(float speed) { tVector3 vVector = mView - mPos; tVector3 vOrthoVector; vOrthoVector.x = -vVector.z; vOrthoVector.z = vVector.x; mPos.x = mPos.x + vOrthoVector.x * speed; mPos.z = mPos.z + vOrthoVector.z * speed; mView.x = mView.x + vOrthoVector.x * speed; mView.z = mView.z + vOrthoVector.z * speed; } void CCamera::Rotate_View(float speed) { tVector3 vVector = mView - mPos; tVector3 vOrthoVector; vOrthoVector.x = -vVector.z; vOrthoVector.z = vVector.x; mView.z = (float)(mPos.z + sin(speed)*vVector.x + cos(speed)*vVector.z); mView.x = (float)(mPos.x + cos(speed)*vVector.x - sin(speed)*vVector.z); } and the mousemotion code void processEvents() { int mid_x = screen_width >> 1; int mid_y = screen_height >> 1; int mpx = event.motion.x; int mpy = event.motion.y; float angle_y = 0.0f; float angle_z = 0.0f; while(SDL_PollEvent(&event)) { switch(event.type) { case SDL_MOUSEMOTION: if( (mpx == mid_x) && (mpy == mid_y) ) return; // Get the direction from the mouse cursor set a resonable maneuvering speed angle_y = (float)( (mid_x - mpx) ) / 1000; //1000 angle_z = (float)( (mid_y - mpy) ) / 1000; //1000 // The higher the value is the faster the camera looks around. objCamera.mView.y += angle_z * 2; // limit the rotation around the x-axis if((objCamera.mView.y - objCamera.mPos.y) > 8) objCamera.mView.y = objCamera.mPos.y + 8; if((objCamera.mView.y - objCamera.mPos.y) <-8) objCamera.mView.y = objCamera.mPos.y - 8; objCamera.Rotate_View(-angle_y); SDL_WarpMouse(mid_x mid_y); break; case SDL_KEYUP: objKeyb.handleKeyboardEvent(eventtrue); break; case SDL_KEYDOWN: objKeyb.handleKeyboardEvent(eventfalse); break; case SDL_QUIT: quit = true; break; case SDL_VIDEORESIZE: screen = SDL_SetVideoMode( event.resize.w event.resize.h screen_bpp SDL_OPENGL | SDL_HWSURFACE | SDL_RESIZABLE | SDL_GL_DOUBLEBUFFER | SDL_HWPALETTE ); screen_width = event.resize.w; screen_height = event.resize.h; init_opengl(); std::cout << ""Resized to width: "" << event.resize.w << "" height: "" << event.resize.h << std::endl; break; default: break; } } } does this happen only when you are looking and moving at the same time? yes individually they work together it goes weird. I agree with Goz. You need to use homegenous 4x4 matrices if you want to represent affine transformations such as rotate + translate Assuming row major representation then if there is no scaling or shearing your 4x4 matrix represents the following: Rows 0 to 2 : The three basis vectors of your local co-ordinate system ( i.e xyz ) Row 3 : the current translation from the origin So to move along your local x vector as Goz says because you can assume it's a unit vector if there is no scale/shear you just multiply it by the move step ( +ve or -ve ) then add the resultant vector onto Row 4 in the matrix So taking a simple example of starting at the origin with your local frame set to world frame then your matrix would look something like this 1 0 0 0 <--- x unit vector 0 1 0 0 <--- y unit vector 0 0 1 0 <--- z unit vector 0 0 0 1 <--- translation vector In terms of a way most game cameras work then the axes map like this: x axis <=> Camera Pan Left/Right y axis <=> Camera Pan Up/Down z axis <=> Camera Zoom In/Out So if I rotate my entire frame of reference to say look at a new point LookAt then as Goz puts in his BaseCamera overloaded constructor code you then construct a new local co-ordinate system and set this into your matrix ( all mCameraMat.Set( vLat vUp vDir vPos ) does typically is set those four rows of the matrix i.e VLat would be row 0 vUp row 1 vDir row 2 and vPos row 3 ) Then to zoom in/out would just become row 3 = row 2 * stepval Again as Goz rightly points out you then need to transform this back into world-space and this is done by multiplying by the inverse of the view matrix Do bear in mind that the above code is for a row major matrix. You will need to transpose the matrix for a column major matrix :) Yep absolutely true  I'm not entirely sure what you are doing above. Personally I would just allow a simple 4x4 matrix. Any implementation will do. To rotate you simply need to rotate using the change of mouse x and y as euler inputs for rotation around the y and x axes. There is lots of code available all over the internet that will do this for you. Some of those matrix libraries won't provide you with a ""MoveForward()"" function. If this is the case its ok moving forward is pretty easy. The third column (or row if you are using row major matrices) is your forward vector. Extract it. Normalise it (It really should be normalised anyway so this step may not be needed). Multiply it by how much you wish to move forward and then add it to the position (the 4th column/row). Now here is the odd part. A view matrix is a special type of matrix. The matrix above defines the view space. If you multiply your current model matrix by this matrix you will not get the answer you expect. Because you wish to transform it such that the camera is at the origin. As such you need to effectively undo the camera transformation to re-orient things to the view defined above. To do this you multiply your model matrix by the inverse of the view matrix. You now have an object defined in the correct view space. This is my very simple camera class. It does not handle the functionality you describe but hopefully will give you a few ideas on how to set up the class (Be warned I use row major ie DirectX style matrices). BaseCamera.h: #ifndef BASE_CAMERA_H_ #define BASE_CAMERA_H_ /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ #include ""Maths/Vector4.h"" #include ""Maths/Matrix4x4.h"" /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ class BaseCamera { protected: bool mDirty; MathsLib::Matrix4x4 mCameraMat; MathsLib::Matrix4x4 mViewMat; public: BaseCamera(); BaseCamera( const BaseCamera& camera ); BaseCamera( const MathsLib::Vector4& vPos const MathsLib::Vector4& vLookAt ); BaseCamera( const MathsLib::Matrix4x4& matCamera ); bool IsDirty() const; void SetDirty(); MathsLib::Matrix4x4& GetOrientationMatrix(); const MathsLib::Matrix4x4& GetOrientationMatrix() const; MathsLib::Matrix4x4& GetViewMatrix(); }; /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline MathsLib::Matrix4x4& BaseCamera::GetOrientationMatrix() { return mCameraMat; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline const MathsLib::Matrix4x4& BaseCamera::GetOrientationMatrix() const { return mCameraMat; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline bool BaseCamera::IsDirty() const { return mDirty; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ inline void BaseCamera::SetDirty() { mDirty = true; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ #endif BaseCamera.cpp: #include ""Render/stdafx.h"" #include ""BaseCamera.h"" /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera() : mDirty( true ) { } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera( const BaseCamera& camera ) : mDirty( camera.mDirty ) mCameraMat( camera.mCameraMat ) mViewMat( camera.mViewMat ) { } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera( const MathsLib::Vector4& vPos const MathsLib::Vector4& vLookAt ) : mDirty( true ) { MathsLib::Vector4 vDir = (vLookAt - vPos).Normalise(); MathsLib::Vector4 vLat = MathsLib::CrossProduct( MathsLib::Vector4( 0.0f 1.0f 0.0f ) vDir ).Normalise(); MathsLib::Vector4 vUp = MathsLib::CrossProduct( vDir vLat );//.Normalise(); mCameraMat.Set( vLat vUp vDir vPos ); } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ BaseCamera::BaseCamera( const MathsLib::Matrix4x4& matCamera ) : mDirty( true ) mCameraMat( matCamera ) { } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/ MathsLib::Matrix4x4& BaseCamera::GetViewMatrix() { if ( IsDirty() ) { mViewMat = mCameraMat.Inverse(); mDirty = false; } return mViewMat; } /*+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+*/",c++ linux opengl sdl
1533380,A,c++ opengl converting model coordinates to world coordinates for collision detection (This is all in ortho mode origin is in the top left corner x is positive to the right y is positive down the y axis) I have a rectangle in world space which can have a rotation m_rotation (in degrees). I can work with the rectangle fine it rotates scales everything you could want it to do. The part that I am getting really confused on is calculating the rectangles world coordinates from its local coordinates. I've been trying to use the formula: x' = x*cos(t) - y*sin(t) y' = x*sin(t) + y*cos(t) where (x y) are the original points (x' y') are the rotated coordinates and t is the angle measured in radians from the x-axis. The rotation is counter-clockwise as written. -credits duffymo I tried implementing the formula like this: //GLfloat Ax = getLocalVertices()[BOTTOM_LEFT].x * cosf(DEG_TO_RAD( m_orientation )) - getLocalVertices()[BOTTOM_LEFT].y * sinf(DEG_TO_RAD( m_orientation )); //GLfloat Ay = getLocalVertices()[BOTTOM_LEFT].x * sinf(DEG_TO_RAD( m_orientation )) + getLocalVertices()[BOTTOM_LEFT].y * cosf(DEG_TO_RAD( m_orientation )); //Vector3D BL = Vector3D(AxAy0); I create a vector to the translated point store it in the rectangles world_vertice member variable. That's fine. However in my main draw loop I draw a line from (000) to the vector BL and it seems as if the line is going in a circle from the point on the rectangle (the rectangles bottom left corner) around the origin of the world coordinates. Basically as m_orientation gets bigger it draws a huge circle around the (000) world coordinate system origin. edit: when m_orientation = 360 it gets set back to 0. I feel like I am doing this part wrong: and t is the angle measured in radians from the x-axis. Possibly I am not supposed to use m_orientation (the rectangles rotation angle) in this formula? Thanks! edit: the reason I am doing this is for collision detection. I need to know where the coordinates of the rectangles (soon to be rigid bodies) lie in the world coordinate place for collision detection. what you do is rotation [ special linear transformation] of a vector with angle Q on 2d.It keeps vector length and change its direction around the origin. [linear transformation : additive L(m + n) = L(m) + L(n) where {m n} € vector  homogeneous L(k.m) = k.L(m) where m € vector and k € scalar ] So: You divide your vector into two pieces. Like m[1 0] + n[0 1] = your vector. Then as you see in the image rotation is made on these two pieces after that your vector take the form: m[cosQ sinQ] + n[-sinQ cosQ] = [m*cosQ - n*sinQ m*sinQ + n*cosQ] you can also look at Wiki Rotation If you try to obtain eye coordinates corresponding to your object coordinates you should multiply your object coordinates by model-view matrix in opengl. For M => model view matrix and transpose of [x y z w] is your object coordinates you do: M[x y z w]T = Eye Coordinate of [x y z w]T Thank you. I will work on this later tonight! :) OpenGL handles graphics not linear algebra operations. If he needs the transformed coordinates for collision detection that's a bad advice in my opinion. But +1 for a good explanation. If you use OpenGL to transform the mesh during the actual rendering remember to rotate about the Z axis.  This seems to be overcomplicating things somewhat: typically you would store an object's world position and orientation separately from its set of own local coordinates. Rotating the object is done in model space and therefore the position is unchanged. The world position of each coordinate is the same whether you do a rotation or not - add the world position to the local position to translate the local coordinates to world space. Any rotation occurs around a specific origin and the typical sin/cos formula presumes (00) is your origin. If the coordinate system in use doesn't currently have (00) as the origin you must translate it to one that does perform the rotation then transform back. Usually model space is defined so that (00) is the origin for the model making this step trivial.,c++ opengl matrix rotation trigonometry
1733488,A,"Terrain minimap in OpenGL? So I have what is essentially a game... There is terrain in this game. I'd like to be able to create a top-down view minimap so that the ""player"" can see where they are going. I'm doing some shading etc on the terrain so I'd like that to show up in the minimap as well. It seems like I just need to create a second camera and somehow get that camera's display to show up in a specific box. I'm also thinking something like a mirror would work. I'm looking for approaches that I could take that would essentially give me the same view I currently have just top down... Does this seem feasible? Feel free to ask questions... Thanks! Well I don't have an answer to your specific question but it's common in games to render the world to an image using an orthogonal perspective from above and use that for the minimap. It would at least be less performance intensive than rendering it on the fly.  One way to do this is to create an FBO (frame buffer object) with a render buffer attached render your minimap to it and then bind the FBO to a texture. You can then map the texture to anything you'd like generally a quad. You can do this for all sorts of HUD objects. This also means that you don't have to redraw the contents of your HUD/menu objects as often as your main view; update the the associated buffer only as often as you require. You will often want to downsample (in the polygon count sense) the objects/scene you are rendering to the FBO for this case. The functions in the API you'll want to check into are: glGenFramebuffersEXT glBindFramebufferEXT glGenRenderbuffersEXT glBindRenderbufferEXT glRenderbufferStorageEXT glFrambufferRenderbufferEXT glFrambufferTexture2DEXT glGenerateMipmapEXT There is a write-up on using FBOs on gamedev.net. Another potential optimization is that if the contents of the minimap are static and you are simply moving a camera over this static view (truly just a map). You can render a portion of the map that is much larger than what you actually want to display to the player and fake a camera by adjusting the texture coordinates of the object it's mapped onto. This only works if your minimap is in orthographic projection. Would using multiple viewports also work? Can I render polygons to the frame buffer? Obviously I'm pretty noob to graphics programming haha. Yes you can draw to an FBO in exactly the same way that you draw normally in your window's frame buffer (the same as rendering to the back buffer before swapping with the front buffer). You can use different gluPerspective/gluLookAt's in each FBO. The benefit is that you don't have to redraw in the FBO if you don't need to for the current frame the FBO can be of any dimensions and it's very fast when used for textures as you don't need to use a glCopyTexImage. You can create many FBOs and switch between them with glBindFramebufferEXT (0 binds the window's framebuffer that your used to).",c++ opengl graphics
662440,A,"Palette Animation in OpenGL I am making an old-school 2d game and I want to animate a specific color in my texture. Only ways I know are: opengl shaders. animating one color channel only. white texture under the color-animated texture. But I don't want to use shaders I want to make this game as simple as possible not many extra openGL functions etc.. And the color channel animating doesnt fit this because I need all color channels in my textures. Currently I am doing it with 2 textures: white texture under the other texture translated the specific pixel color into transparent then I change white texture color with glColor3f() function to what ever I want and I see the ""palet animation"" on that specific color. But that style sounds pretty hacky so I am wondering if there is some better trick to this? If you want your game to work on modern hardware the fragment shaders are pretty much your only option since paletted textures are deprecated and may not be available on newer hardware. The solution you have right now sounds reasonable especially since it will probably work everywhere.  How about just using paletted textures? There are extensions to do just that. If using extensions is out of question you can just make palette handling by your own. Just do your palette tricks there are lot of them and just write RGB texture using palette. Ofcourse this limits number of colors but thats whole point of using palette. Paint programs that are good for palette handling are nowadays rare. That's why I will not remove Deluxe Paint from my drive. ahh Deluxe Paint!  You may also use the stencil buffer to do something equivalent. Write into the stencil buffer at the positions where your animation should occur (= the positions that should have the animated color). Then render the box with the normal texture only on non-set positions of the stencil buffer using the corresponding stencil op. After that render the animated color into the resp. positions using different stencil op. I don't know if this really would be an improvement - depends also on in which way you have the data on your hands - but is probably less ""hackish"".  For a 2D game I think it's better to use ARB_vertex_program extension than GLSL. There are still OpenGL drivers out there that do not support GLSL but support ARB_fragment_program. Support for paletted textures is nowadays non-existent but pre-shader-age video cards may support it. See the links for statistics. The old ARB language is a low-level assembly language but you can use NVidia's high level Cg language to compile shaders to this format. Despite it being a NVidia technology it works for all GPUs. That said if you are programming the game for your own amusement it may not be worth the trouble.  While I am unfamiliar with the palette texture extension I still recommend using a fragment shader for this sort of effect. It is almost trivial to do a color-key replacement with a shader versus the other methods you mentioned and will be way faster than writing the palette functionality yourself. Here's an example GLSL fragment shader that would replace the color white in a texture for whatever color is passed in. uniform vec4 fvReplaceColor; uniform sampler2D baseMap; varying vec2 Texcoord; void main( void ) { vec4 fvBaseColor = texture2D( baseMap Texcoord); if(fvBaseColor == vec4(1.0 1.0 1.0 1.0)) fvBaseColor = fvReplaceColor; gl_FragColor = fvBaseColor; } Yes it does take a little bit extra to set up shader but but what it sounds like you are trying to do I feel it's the best approach.",c++ c opengl
784445,A,"How could simply calling Pitch() and Yaw() cause the camera to eventually Roll()? I'm coding a basic OpenGL game and I've got some code that handles the mouse in terms of moving the camera. I'm using the following method: int windowWidth = 640; int windowHeight = 480; int oldMouseX = -1; int oldMouseY = -1; void mousePassiveHandler(int x int y) { int snapThreshold = 50; if (oldMouseX != -1 && oldMouseY != -1) { cam.yaw((x - oldMouseX)/10.0); cam.pitch((y - oldMouseY)/10.0); oldMouseX = x; oldMouseY = y; if ((fabs(x - (windowWidth / 2)) > snapThreshold) || (fabs(y - (windowHeight / 2)) > snapThreshold)) { oldMouseX = windowWidth / 2; oldMouseY = windowHeight / 2; glutWarpPointer(windowWidth / 2 windowHeight / 2); } } else { oldMouseX = windowWidth / 2; oldMouseY = windowHeight / 2; glutWarpPointer(windowWidth / 2 windowHeight / 2); } glutPostRedisplay(); } However after looking around in circles you'll find the camera starts to ""roll"" (rotate). Since I'm only calling Pitch and Yaw I don't see how this is possible. Here is the code I'm using for my Camera class: http://pastebin.com/m20d2b01e As far as I know my camera ""rolling"" shouldn't happen. It should simply pitch up and down or yaw left and right. NOT roll. What could be causing this? Well if you start off looking forward horizontal to the horizon pitch up 90 degrees then yaw left 90 degrees then pitch down 90 degrees you'd be looking in the same direction as you started but the horizon would be vertical (as if you'd rolled 90 degrees left). Edit: I think the problem is that yaw/pitch/roll would be appropriate if the camera is being treated like an airplane. What you probably want to do is treat it like a point in a sphere keeping track of where on the sphere you are pointing the camera. Instead of yaw/pitch use spherical coordinates keeping track of theta (latitude) and phi (longitude). They may sound similar but consider the extreme case where the camera is pointing directly up. With yaw/pitch you can still freely adjust the yaw and pitch from that straight-up direction. With theta/phi you could only adjust theta downward and no matter how much you adjusted phi decreasing theta would still give you a camera that is parallel to the horizon. This is how FPS camera's work (you can't look so far down that you're looking behind you). Edit 2: Looking at the camera code you linked to you want to be using the rotLati(float angle) and rotLongi(float angle) functions. If I start moving my mouse in tight circles on the screen you'll see a distinct and slow roll. It completely rolls all 360 degrees. It isn't inverting the view. +1 Beat me to it. If you're moving your mouse in tight circles you'll get small amounts of roll. That's just how the combination of movements work. well if you replace the 90's in my answer with 20's the answer would still be the same. moving in circles would do basically what i am describing. Nestor Kip just explained how pitching and yawing can be combined to get the net effect of a roll. His numbers are exaggerated to make the effect easier to see -- hold your hand in front of you and make those motions with it. You're getting smaller motions from your mouse which is why you have to move it around a lot more before you go 360 degrees but it's still the same issue.  You will probably need to use quaternions for composing rotations if you are not doing so already. This avoids the problem of gimbal lock which you can get when orienting a camera by rotation around the 3 axes. Here is how to convert between them. Use quaternions is the answer.  I believe this circumstance is called Gimbal Lock - there's an example of it with illustrations on the wikipedia page.  Mathematically the reason for this is that rotations in 3D space do not commute. What that means is that pitch() followed by yaw() is not the same as yaw() followed by pitch() but a consequence of that fact is that the three kinds of rotations are inextricably linked and you can't perform any two of them without getting some of the third. In other words any sequence of pitch()es and yaw()s will produce a noticeable roll() effect over time unless the second half of the sequence is the exact reverse of the first half. (There's a lot of fairly intricate math involved in this but the details aren't particularly relevant)  Congratulations -- you have discovered Lie group theory! Yes it's possible. The outcome of a series of transformations depends on the order in which they're executed. Doing a pitch followed by a yaw is not the same as a doing a yaw followed by a pitch. In fact in the limit of infinitesimally small yaws and pitches the difference amounts to a pure roll; the general case is a little more complicated. (Physicists call this the ""commutation relationships of the rotational group"".) If you're familiar with rotation matrices you can work it out quite easily. heh looks like we were thinking exactly the same thing ;-)  Pitch/yaw/roll are all relative to your vehicle's orientation. When you pitch up/down you change your axis of yaw. Likewise when you yaw you change your pitch axis. So it's possible to change your orientation in a way similar to a roll maneuver just by a combination and pitch & yaw maneuvers.",c++ opengl camera glut
237899,A,"How to draw a filled envelop like a cone on OpenGL (using GLUT)? I am using freeglut for opengl rendering... I need to draw an envelop looking like a cone (2D) that has to be filled with some color and some transparency applied. Is the freeglut toolkit equipped with such an inbuilt functionality to draw filled geometries(or some trick)? or is there some other api that has an inbuilt support for filled up geometries.. Edit1: just to clarify the 2D cone thing... the envelop is the graphical interpretation of the coverage area of an aircraft during interception(of an enemy aircraft)...that resembles a sector of a circle..i should have mentioned sector instead.. and glutSolidCone doesnot help me as i want to draw a filled sector of a circle...which i have already done...what remains to do is to fill it with some color... how to fill geometries with color in opengl? Edit2: All the answers posted to this questions can work for my problem in a way.. But i would definitely would want to know a way how to fill a geometry with some color. Say if i want to draw an envelop which is a parabola...in that case there would be no default glut function to actually draw a filled parabola(or is there any?).. So to generalise this question...how to draw a custom geometry in some solid color? Edit3: The answer that mstrobl posted works for GL_TRIANGLES but for such a code: glBegin(GL_LINE_STRIP); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glEnd(); which draws a square...only a wired square is drawn...i need to fill it with blue color. anyway to do it? if i put some drawing commands for a closed curve..like a pie..and i need to fill it with a color is there a way to make it possible... i dont know how its possible for GL_TRIANGLES... but how to do it for any closed curve? just to clarify the 2D cone thing... the envelop is the graphical interpretation of the coverage area of an aircraft during interception(of an enemy aircraft)...that resembles a sector of a circle..i should have mentioned sector instead.. Edit yoru comment into question because it's where it belongs. edited my original question..as i am newbie to stackoverflow i thought that followups to orig questions should go into comments.. is this how stackoverflow works? if the original poster feels to add to the conversation..how he should proceed?through comments/answering to own question or editing ques Since you reclarified your question to ask for a pie: there's an easy way to draw that too using opengl primitives: You'd draw a solid sphere using gluSolidSphere(). However since you only want to draw part of it you just clip the unwanted parts away: void glClipPlane(GLenum plane const GLdouble * equation); With plane being GL_CLIPPLANE0 to GL_CLIPPLANEn and equation being a plane equation in normal form (a*x + b*y + c*z + d = 0 would mean equation would hold the values { a b c d }. Please note that those are doubles and not floats.  On the edit on colors: OpenGL is actually a state machine. This means that the current material and/or color position is used when drawing. Since you probably won't be using materials ignore that for now. You want colors. glColor3f(float r float g float b) // draw with r/g/b color and alpha of 1 glColor4f(float r float g float b float alpha) This will affect the colors of any vertices you draw of any geometry you render - be it glu's or your own - after the glColorXX call has been executed. If you draw a face with vertices and change the color inbetween the glVertex3f/glVertex2f calls the colors are interpolated. Try this: glBegin(GL_TRIANGLES); glColor3f(0.0 0.0 1.0); glVertex3f(-3.0 0.0 0.0); glColor3f(0.0 1.0 0.0); glVertex3f(0.0 3.0 0.0); glColor3f(1.0 0.0 0.0); glVertex3f(3.0 0.0 0.0); glEnd(); But I pointed at glColor4f already so I assume you want to set the colors on a per-vertex basis. And you want to render using display lists. Just like you can display lists of vertices you can also make them have a list of colors: all you need to do is enable the color lists and tell opengl where the list resides. Of course they need to have the same outfit as the vertex list (same order). If you had glEnableClientState(GL_VERTEX_ARRAY); glVertexPointer(3 GL_FLOAT 0 vertices_); glDisableClientState(GL_VERTEX_ARRAY); you should add colors this way. They need not be float; in fact you tell it what format it should be. For a color list with 1 byte per channel and 4 channels (R G B and A) use this: glEnableClientState(GL_VERTEX_ARRAY); glEnableClientState(GL_COLOR_ARRAY); glVertexPointer(3 GL_FLOAT 0 vertices_); glColorPointer(4 GL_UNSIGNED_BYTE 0 colors_); glDisableClientState(GL_COLOR_ARRAY); glDisableClientState(GL_VERTEX_ARRAY); EDIT: Forgot to add that you then have to tell OpenGL which elements to draw by calling glDrawElements.  On Edit3: The way I understand your question is that you want to have OpenGL draw borders and anything between them should be filled with colors. The idea you had was right but a line strip is just that - a strip of lines and it does not have any area. You can however have the lines connect to each other to define a polygon. That will fill out the area of the polygon on a per-vertex basis. Adapting your code: glBegin(GL_POLYGON); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 0.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(200.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 200.0 0.0); glColor3f(0.0 0.0 1.0); glVertex3f(0.0 0.0 0.0); glEnd(); Please note however that drawing a polygon this way has two limitations: The polygon must be convex. This is a slow operation. But I assume you just want to get the job done and this will do it. For the future you might consider just triangulating your polygon. Note that you don't need the multiple glColor3f() statements. OpenGL is a state machine and the state is retained. So one glColor3f()-statement will suffice. ThankYou mstroblthis is really what i wanted...ur solution made my day!  I'm not sure what you mean by ""an envelop"" but a cone is a primitive that glut has: glutSolidCone(radius height number_of_slices number_of_stacks) The easiest way to fill it with color is to draw it with color. Since you want to make it somewhat transparent you need an alpha value too: glColor4f(float red float green float blue float alpha) // rgb and alpha run from 0.0f to 1.0f; in the example here alpha of 1.0 will // mean no transparency 0.0 total transparency. Call before drawing. To render translucently blending has to be enabled. And you must set the blending function to use. What you want to do will probably be achieved with the following. If you want to learn more drop me a comment and I will look for some good pointers. But here goes your setup: glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); Call that before doing any drawing operations possibly at program initialization. :) I never remember those blending parameters.  I remember there was a subroutine for that. But it's neither too hard to do by yourself. But I don't understand the 2D -thing. Cone in 2D? Isn't it just a triangle? Anyway here's an algorithm to drawing a cone in opengl First take a circle subdivision it evenly so that you get a nice amount of edges. Now pick the center of the circle make triangles from the edges to the center of the circle. Then select a point over the circle and make triangles from the edges to that point. The size shape and orientation depends about the values you use to generate the circle and two points. Every step is rather simple and shouldn't cause trouble for you. First just subdivision a scalar value. Start from [0-2] -range. Take the midpoint ((start+end)/2) and split the range with it. Store the values as pairs. For instance subdividing once should give you: [(01) (12)] Do this recursively couple of times then calculate what those points are on the circle. Simple trigonometry just remember to multiply the values with π before proceeding. After this you have a certain amount of edges. 2^n where n is the amount of subdivisions. Then you can simply turn them into triangles by giving them one vertex point more. Amount of triangles ends up being therefore: 2^(n+1). (The amounts are useful to know if you are doing it with fixed size arrays. Edit: What you really want is a pie. (Sorry the pun) It's equally simple to render. You can again use just triangles. Just select scalar range [-0.25 - 0.25] subdivide project to circle and generate one set of triangles. The scalar - circle projection is simple as: x=cos(v*pi)r y=sin(vpi)*r where (xy) is the resulting vertex point r is a radius and trigonometric functions work on radiances not degrees. (if they work with degrees replace pi with 180) Use vertex buffers or lists to render it yourself. Edit: About the coloring question. glColor4f if you want some parts of the geometry to be different by its color you can assign a color for each vertex in vertex buffer itself. I don't right now know all the API calls to do it but API reference in opengl is quite understandable.",c++ opengl glut freeglut
640095,A,"When using a GL_RGBA16F_ARB-texture it contains just crap but I get no error message I generate a texture like this: GLuint id; glGenTextures(1 &id); glBindTexture(GL_TEXTURE_2D id); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA16 //GL_RGBA16F_ARB //< Won't work 256 256 0 GL_RGBA GL_FLOAT NULL ); glBindTexture(GL_TEXTURE_2D 0); I attach it to a framebuffer object (FBO) for rendering to. All of this works like a charm when I set the internal format to GL_RGBA16. However I need a higher dynamic range and was thinking that GL_RGBA16F_ARB might do the trick. Unfortunately if I replace GL_RGBA16 with GL_RGBA16F_ARB in the code given above the texture seems to stop working. Nothing I try to render to the FBO/texture sticks and when I use the texture it contains random garbage. (A lot of purple as it turns out) This would not be so frustrating if I had gotten an error message hinting at what might be wrong but I can't seem to find one. In other words glGetError() returns 0 after the glTexImage2D-call and glCheckFramebufferStatusEXT(GL_FRAMEBUFFER_EXT) returns GL_FRAMEBUFFER_COMPLETE_EXT when I have attached the texture I haven't messed with glClampColorARB(...) ... yet :) Have I forgotten to check for errors in a place/way that I haven't thought of? Do GL_RGBA16F_ARB-textures require any special treatment that I haven't given? Is there anything else that might be wrong? I'm stumped since everything works smoothly with GL_RGBA16... :( EDIT: When using GL_RGBA16F_ARB the first frame I try to render to screen doesn't make it. Seems to me that I should be getting an error message somewhere..? EDIT: By inspecting ShadowIce's working code example I figured out that the problems disappeared if I changed the depth buffer on my FBO and give glRenderBufferStorageEXT(...) GL_DEPTH_COMPONENT24 as its second parameter rather than GL_DEPTH_COMPONENT16. I have no idea why this works but apparently it does work. Also ShadowIce's code breaks like mine if I do the opposite substitution there. How are you actually filling the texture? I am attaching it to an FBO and rendering to it. There shouldn't be anything special to do for setting up a framebuffer with float textures. Some things I would check: Is the FBO bound and the draw/read buffer set correctly before you call glCheckFramebufferStatusEXT? Also try testing it right before you draw to it. Does the texture look ok after a simple glClear with a specific clear color? If yes there might be something wrong with your shaders (if you use any) or the way you draw to the FBO. Are your drivers up to date? And does the problem still exist on a PC with different hardware? How about GL_RGBA32F_ARB? Edit: Check the id of your framebuffer and texture also check if the texture id matches the one attached to your fbo (with glGetFramebufferAttachmentParameteriv). Normally I would guess that everything is ok with that if it works with a RGBA texture but random data (especially purple) is a good sign that nothing was written to the texture or it wasn't cleared properly. I have written a small example application that should work maybe that helps. I have only tested it on windows so for linux you might need to change it a bit: link The example you made worked nicely. Now to isolate the differences... I ought to create sockpuppet-accounts so I could thank you properly ;) Hehe thanks. ;) 1. Yes. Verified by testing right before drawing 2. No it does not look OK 3. I get my drivers from the Ubuntu-repository. Should be reasonably up to date (nvidia). I get the same problem om a different machine 4. I get the same result sometimes but now it has apparently locked completely! (4. continued:) When breaking it in a debugger the stack I get to see is only one deep with ""libGL"" as the only element. The program is completely locked and uses 100% CPU :O Also: Thanks! If you have any more ideas I'm all ears :)  GL_HALF_FLOAT_ARB might work as the type instead of GL_FLOAT. Thanks. Unfortunately this gives the same result; It still works with GL_RGBA16 but fails with GL_RGBA*F_ARB :(",c++ c opengl fbo
746034,A,glDrawPixels in grayscale? I have an image that I'm rendering like this: glDrawPixels(image->width image->height GL_BGR GL_UNSIGNED_BYTE image->imageData); Is there anyway I can draw it in grayscale instead (without loading it into a texture first)? I don't care if only say the blue component is used for the gray value rather than the L2 norm or something I just need a quick and dirty output. GL_LUMINANCE would be great except that it won't work on a 3-channel image. @timday: A perverse idea for you to try and I've no idea if it'll work: glPixelZoom(1.0f/3.0f1.0f); glDrawPixels(3*widthheightGL_LUMINANCEGL_UNSIGNED_BYTEdata); ie treat your 3-channel image as being a 1-channel (grayscale) image 3 times as wide and compensate for this by squishing the width using the x zoom factor. I believe GL always does nearest-neighbour sampling for zoomed glDrawPixels so it ought to consistently pick out the same component from each triple of samples as you require. Oh my God..... I can't believe that works! It sort of splits the image in half and one side grainier than the other but it works. Could be something to do with the precision with which 1.0/3.0 is maintained internally (especially if it works OK up to a certain size). I'd suggest adjusting the ratio very slightly or using a 4-bytes-per-pixel format and a zoom factor of 0.25.,c++ opengl
1569829,A,Open Source C++ game engine math libraries? I'm looking for a free to use game engine math library. Specifically I'd like a good matrix and vector implementation. And everything needed to move objects in 3D space. Does anyone know any good ones? I'm targeting OpenGL. I'd like to write them myself but don't have the time. Besides Ogre 3D there's also Crystal Space. Here's an article that compares the two. That article appears to 404 on me. @darthcoder it looks like arcanoria.com did some reorganizing. I've updated the article's URL accordingly  I have good work with Open Dynamics Engine is Full and Stable Physics Engine the ode in a BSD License and have some functions for Matrix Manipulation Quaternion and rotations.  Take a look here https://sourceforge.net/projects/mg3d/ It is an opensource engine that has all former OpenGL transformation routines. The implementation here is very straightforward and clear. And it is very easy to include the module with the routines in your project.  If you want an entire 3D engine (which of course would contain the 3d maths you need) see Ogre 3D (LGPL) Actually MIT license now for current svn trunk and all coming releases.  I'd recommend OpenGL Mathematics (GLM) Though if you want physics with your math you could go with Bullet Physics Library Finally if you want an entire engine i'd go with OGRE I think GLM will work nicely... seems light weight enough and has what I need  You might want to consider Blitz++.,c++ opengl
933020,A,Macro use depending on integer I have to use a macro multiple times inside a function and the macro that needs to be used depends on a number I pass into the function. e.g.  function(int number) { switch(number) { case 0: doStuff(MACRO0); break; case 1: doStuff(MACRO1); break; } } The problem is: I have a lot stuff to do per switch statement with the same macro. Is there are more elegant solution then having all that stuff in the switch statement? Like passing the macro itself to the function? I have read about eval() like methods in C++ but they just don't feel right to me. Another way could be do determine into what the macro expands but I haven't found any info on this. Oh it's openGL actually. This very much depends on what MACRO1 expands to. If it's a constant you can just call the function outside of the switch or do multiple fall-through cases. If it depends on the local context then you will have to evaluate it every time.  I would use a function object struct Method1 { void operator()() { ... } }; template<typename Method> void function(Method m) { ... m(); ... } int main() { function(Method1()); } Why not just use a function pointer? Because function objects using op() are faster and can be inlined. Given that he is doing OpenGL i think speed matters.  In addition to the above suggestions often I find using virtual inheritance can get rid of long conditionals especially switches-over-enums type code. I'm not sure what your particular situation so I'm not sure how applicable his will be but let's say the above was enum StyleID { STYLE0 = 0 STYLE1 STYLE2 /* ... */ }; void drawStyle(StyleID id) { switch(id) { case STYLE1: doDraw(MACROSTYLE1); break; /* ... */ }; } Long blocks of switches could be avoided via virtual inheritance: class StyleInterface { /* some methods */ virtual void doDraw() = 0; }; class Style1 : public StyleInterface { /* concrete impl */ virtual void doDraw() { doDraw(MACROSTYLE1); } }; void drawStyle(StyleInterface* id) { id->doDraw(); },c++ opengl macros
366742,A,Creating an Environment Stack in OpenGL I'd like to create an abstraction in OpenGL of the environment settings(blending stenciling depth etc.) that works like the matrix stack. Push onto the stack make any changes you need draw your objects then pop the stack and go back to the prior settings. For example currently you might have drawing code like this: glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glDisable(GL_DEPTH_TEST); //Draw operations glEnable(GL_DEPTH_TEST); glDisable(GL_BLEND); But with an environment stack it would look like this: glPushEnv(); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glDisable(GL_DEPTH_TEST); //Draw operations glPopEnv(); As I see it there are only 2 ways to do this: Create my own 'flavor' of each environment setting function and call that. It will in turn update the current EnvStack data structure and call the OpenGL environment function. Alter the OpenGL environment functions to point to my environment functions which will again update the current EnvStack data structure and call the original OpenGL environment functions. So option 1 is obviously much simpler. But I run into a problem if I'm using other peoples code in that I don't necessarily know what changes it's making to the environement and therefor my data structure would be out of sync. And since the whole point is to have a simple method of ensuring the environment settings are correct this is not cool. So my question is in this context how do I change the functions that the OpenGL environment functions point to? OpenGL already contains this functionality. You want glPushAttrib(GL_ALL_ATTRIB_BITS); and glPopAttrib();. See http://opengl.org/documentation/specs/man_pages/hardcopy/GL/html/gl/pushattrib.html for more. Wow. I have never come across that. It seemed like something so useful they'd have to have a way to do it. Thanks.,c++ c opengl abstraction
371665,A,"openGL into png I'm trying to convert an openGL [edit: ""card that I drew""(?):) thx unwind]containing a lot of textures (nothing moving) into one PNG file that I can use in another part of the framework I'm working with. Is there a C++ library that does that? thanks! What does ""card that I drew"" mean? it's a static constant scene that looks like a card... What is an ""OpenGL file""? OpenGL is a graphics API it doesn't specify any file formats. Do you mean a DDS file or something?  There are better ways to make a compose texture than drawing them with the graphics card. This is really something you would want to do before hand on the cpu store and then use as and when you need it with opengl Can you give me a clue on how I would do that?  If you simply mean ""take a scene rendered by OpenGL and save it as an image"" then it is fairly straightforward. You need to read the scene with glReadPixels() and then convert that data to an image format such as PNG (http://www.opengl.org/resources/faq/technical/miscellaneous.htm). There are also more efficient ways of achieving this such as using FBOs. Instead of rendering the scene directly into the framebuffer you can render it to a texture via an FBO then render that texture as a full-screen quad. You can then take this texture and save it to a file (using glGetTexImage for example).",c++ opengl png ldf
52431,A,How do I draw text using OpenGL SDL and C++? I heard about SDL_TFF which I read about here but I don't understand how am I supposed to connect the TrueType2 library. Maybe there is something better out there? Here's a good answer if you decide to use ttf: http://stackoverflow.com/questions/5289447/using-sdl-ttf-with-opengl You can try to use FreeGLUT. If you like you can pull the text drawing files from the project.  I came across this great guide on linking in SDL extensions for those new to SDL which you may find useful. That said when I had your problem I eventually went with FTGL as the way SDL-ttf produces an SDL-Surface with its font rendered on it overcomplicated matters in my situation. This may not be the case in your situation though  For OpenGL things I usually use QT. This library is well documented and easy to use.  This article is about SDL and text output. Hope that helps.,c++ opengl sdl
1719325,A,"FLTK in Cygwin using Eclipse (Linking errors) I have this assignment due that requires the usage of FLTK. The code is given to us and it should compile straight off of the bat but I am having linking errors and do not know which other libraries I need to include. I currently have ""opengl32"" ""fltk_gl"" ""glu32"" and ""fltk"" included (-l) each of which seem to reduce the number of errors. I compiled FLTK using make with no specified options. Including all of the produced library files doesn't fix the problem and I'm convinced that it's just some Windows specific problem. Compile log: **** Build of configuration Debug for project CG5 **** make all Building target: CG5.exe Invoking: Cygwin C++ Linker g++ -o""CG5.exe"" ./src/draw_routines.o ./src/gl_window.o ./src/my_shapes.o ./src/shape.o ./src/shapes_ui.o ./src/tesselation.o -lopengl32 -lfltk_z -lfltk_gl -lglu32 -lfltk /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x197): undefined reference to `_SelectPalette@12' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x1a7): undefined reference to `_RealizePalette@4' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x1fe): undefined reference to `_glDrawBuffer@4' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x20d): undefined reference to `_glReadBuffer@4' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x23a): undefined reference to `_glGetIntegerv@8' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x2c3): undefined reference to `_glOrtho@48' /usr/lib/gcc/i686-pc-cygwin/3.4.4/../../../libfltk_gl.a(Fl_Gl_Window.o):Fl_Gl_Window.cxx:(.text+0x2f3): undefined reference to `_SwapBuffers@4' ...and lots more Thanks a ton for the help. EDIT: These first few lines are obviously OpenGL related although I'm still not sure what additional libraries need to be included. its probably not it but try changing the order so that -lglu32 is before the fltk libs. Also are you certain its: -lGLU32 -lOpenGL32 rather than -lGLU -lGL ? Sorry for the lack of closure but I just booted into my Linux netbook and got it working. -lfltk -lfltk_gl -lGLU -lGL -lXext -lX11 -lm  Just a guess: your makefile was written for Linux and on Cygwin some libraries are either missing or in a different place. You're going to have to examine the makefile locate the missing libraries and either move the libs to where the makefile expects them or change the makefile to look in the right place. The libraries it needs are listed on the line starting g++ (prepend 'lib' to the names after the -l flags)",c++ windows opengl linker-error fltk
1167120,A,OpenGL Alpha blending with wrong color I am trying to create a simple ray tracer. I have a perspective view which shows the rays visibly for debugging purposes. In my example screenshot below I have a single white sphere to be raytraced and a green sphere representing the eye. Rays are drawn as lines with glLineWidth(10.0f) If a ray misses the sphere it is given color glColor4ub(100100100100); in my initialization code I have the following:  glEnable(GL_ALPHA_TEST); glAlphaFunc(GL_GREATER 0.0f); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHAGL_SRC_ALPHA); You can see in the screen shot that for some reason the rays passing between the perspective view point and the sphere are being color blended with the axis line behind the sphere rather than with the sphere itself. Here is a screenshot: Can anyone explain what I am doing wrong here? Thanks!! I edited to include your screenshot AlphaTest is only for discarding fragments - not for blending them. Check the spec By using it you are telling OpenGL that you want it to throw away the pixels instead of drawing them so you won't can any transparent blending. The most common blending function is glBlendFunc (GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); You can also check out the OpenGL Transparency FAQ.  You dont need the glAlphaFunc disable it. Light rays should be blended by adding to the buffer: glBlendFunc(GL_ONE GL_ONE) (for premultiplied alpha which you chose. Turn off depth buffer writing (not testing) when rendering the rays: glDepthMask(GL_FALSE) Render the rays last.  Is it a possibility you cast those rays before you draw the sphere? Then if Z-buffer is enabled the sphere's fragments simply won't be rendered as those parts of rays are closer. When you are drawing something semi-transparent (using blending) you should watch the order you draw things carefully. In fact I think you cannot use Z-buffer in any sensible way together with ray-tracing process. You'll have to track Z-order manually. While we are at it OpenGL might not be the best API to visualize ray-tracing process. (It will do so possibly much slower than pure software ray-tracer) @puddlesofjoy: any chance you could edit the question and add the result after this fix? It might be instructive for people to see what your intended result looked like. Not a biggie - just a suggestion. Glad this worked out! Well I'll be... just changing the order of my drawing code is all it took; I had no idea that could effect things so much!! Thank you sir! You are a gentleman and a scholar. Just to clear up my intent for the curious. The actual ray tracing is fully independent of the OpenGL drawing. I have just thrown in a 3d perspective view to help me debug while I get my ray tracer working. It just shows the rays and a copy of the raytraced image textured onto a quad in front of the raytracer eye point.,c++ qt opengl
1191525,A,"I can't get the transparency in my images to work Stemming from this question of mine: http://stackoverflow.com/questions/1191093/im-seeing-artifacts-when-i-attempt-to-rotate-an-image In the source code there I am loading a TIF because I can't for the life of me get any other image format to load the transparency parts correctly. I've tried PNG GIF & TGA. I'd would like to be able to load PNGs. I hope the source code given in the question above will be enough if not then let me know. For a better description of what happens when I attempt to load some other format -- One of the images I was attempting was a 128*128 orange triangle. Depending on the format it would either make the entire 128*128 square orange or make the transparent parts of the image white. OK I'm new at OpenGL + SDL but here is what I have.. Loads all? formats SDL_image supports except I can't get .xcf to work and don't have a .lbm to test with. //called earlier.. glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); //load texture SDL_Surface* tex = IMG_Load(file.c_str()); if (tex == 0) { std::cout << ""Could not load "" << file << std::endl; return false; } glGenTextures(1 &texture); glBindTexture(GL_TEXTURE_2D texture); //nearest works but linear is best when scaled? glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); width = tex->w; height = tex->h; //IMG_is* doesn't seem to work right esp for TGA so use extension instead.. std::string ext = file.substr(file.length() - 4); bool isBMP = (ext.compare("".bmp"") == 0) || (ext.compare("".BMP"") == 0); bool isPNG = (ext.compare("".png"") == 0) || (ext.compare("".PNG"") == 0); bool isTGA = (ext.compare("".tga"") == 0) || (ext.compare("".TGA"") == 0); bool isTIF = ((ext.compare("".tif"") == 0) || (ext.compare("".TIF"") == 0) || (ext.compare(""tiff"") == 0) || (ext.compare(""TIFF"") == 0)); //default is RGBA but bmp and tga use BGR/A GLenum format = GL_RGBA; if(isBMP || isTGA) format = (tex->format->BytesPerPixel == 4 ? GL_BGRA : GL_BGR); //every image except png and bmp need to be converted if (!(isPNG || isBMP || isTGA || isTIF)) { SDL_Surface* fixedSurface = SDL_CreateRGBSurface(SDL_SWSURFACE width height 32 0x000000ff 0x0000ff00 0x00ff0000 0xff000000); SDL_BlitSurface(tex 0 fixedSurface 0); glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA width height 0 format GL_UNSIGNED_BYTE fixedSurface->pixels); SDL_FreeSurface(fixedSurface); } else { glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA width height 0 format GL_UNSIGNED_BYTE tex->pixels); } SDL_FreeSurface(tex); list = glGenLists(1); glNewList(list GL_COMPILE); GLint vertices[] = { 00 00 01 0height 11 widthheight 10 width0 }; glEnableClientState(GL_TEXTURE_COORD_ARRAY); glEnableClientState(GL_VERTEX_ARRAY); glBindTexture(GL_TEXTURE_2D texture); glTexCoordPointer(2 GL_INT 4*sizeof(GLint) &vertices[0]); glVertexPointer(2 GL_INT 4*sizeof(GLint) &vertices[2]); glDrawArrays(GL_POLYGON 0 4); glDisableClientState(GL_VERTEX_ARRAY); glDisableClientState(GL_TEXTURE_COORD_ARRAY); glEndList(); And then to draw I set the color to opaque white (doesn't affect transparency?) then just call the list.. glColor4f(1111); glCallList(list); And of course any help for my code would be much appreciated too! :) I used bits and pieces from your code and it finally worked. Turns out when I was loading the image ( I used my own function instead of IMG_Load() ) it was discarding the alpha layer as it was converting the BPPs. So thanks a lot for your code.  Make sure that you have alpha blending enabled with glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); otherwise primitives will draw solid colors where there should be transparency. You may need a different blendfunc. This is a common setup. I have that code in my previous question linked in the first post. Actually you have: glBlendFunc(GL_ONE GL_ONE_MINUS_SRC_ALPHA); // notice the GL_ONE instead of GL_SRC_ALPHA - this can make a difference depending on what you are seeing Sorry if you had the code already I wasn't able to scroll the code block on an iPhone. As Jim was saying the blendfunc that you have may not be correct. The blendfunc you're using is for blending a texture with pre-multiplied alpha. Ah well I tried your code then and now it just won't draw it at all :( Try to load in an image that has a known amount of transparency then look at the loaded buffer in the debugger to make sure that the alpha value contains the transparency you expect. If you don't see anything at all I suspect that the alpha could be 0.  I'm not familiar with SDL but since it's SDL that loading the image I would look closer at their docs. I use .png in my own work along with OpenGL and transparency works with no problem. (I use a .png parser called LightZPng.) Also I just noticed your linked post has: glBlendFunc(GL_ONE GL_ONE_MINUS_SRC_ALPHA); instead of: glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); This would have the affect of adding the pixels that should be transparent to whatever is in the background (assuming the alpha is 0 in those texels). Changing that makes the image not appear at all. Even with the TIF now Huh could it be that alpha is simply 0 for all your texels? I would look at your image data before passing it to glTexImage2D to confirm what the alpha values are for some texels. If they are simply RGB images (no alpha at all) then be sure to do a glColor4ub(255 255 255 255) before rendering to be sure your alpha is effectively set to fully opaque.",c++ gui opengl sdl
1147249,A,Connecting Catmull-Rom splines together and calculating its length? I'm trying to create a class which takes in any number of points (position and control) and creates a catmull-rom spline based on the information given. What I'm doing - and I'm really unsure if this is the right way to do it - is storing each individual point in a class like so: class Point { public: Vector3 position; Vector3 control; } Where obviously position is the position of the point and control is the control point. My issue is connecting up the splines - obviously given the above class holding a point in the spline array indicates that any given position can only have one control point. So when having three or more points in a catmull rom spline the various individual catmull-rom splines which are being connected share one position and one control with another such spline. Now with position being the same is required - since I want to create splines which are continuous between themselves. However I really wonder should the control points also be the same between the two splines? With a bit of fiddling of the control points I can make it appear to be transitioning smoothly from one spline to another however I must emphasize that I'm not sure if the way they are transitioning is consistent with how catmull-rom splines form their shape. I'd much rather do it correctly than sit on my hands and rationalize that it's good enough. Obviously the second part of my question is self explanatory: Given two control and position points how do I calculate the length of a catmull-rom spline? With regards to measuring the lengths you can do this with calculus since its a polynomial spline. You need to do integrate the distance function across the line. Its described quite well on Wikipedia...  To define the spline between two control points the Catmull-Rom spline needs the control points and the tangent vector at each control point. However the tangent vector at internal (i.e. non-endpoint) control points is defined by the control points on either side of it: T(Pn) = (Pn+1 - Pn-1) / 2. For closed curves the spline is completely defined by the set of control points. For non-closed curves you need to also supply the tangent vector at the first and last control point. This is commonly done: T(P0) = P1 - P0 and T(Pn) = Pn - Pn-1. Thus for closed curves your data structure is just a list of control points. For general splines it's a list of points plus the first and last normal vector. If you want to have a cardinal spline then you can add a weighting factor to the tangent vector calculation as in the Wikipedia article. To calculate the length of such a spline one approach would be to approximate it by evaluating the spline at many points and then calculating the linear distance between each neighboring pair of points.,c++ opengl graphics directx spline
23918,A,"OpenGL Rotation I'm trying to do a simple rotation in OpenGL but must be missing the point. I'm not looking for a specific fix so much as a quick explanation or link that explains OpenGL rotation more generally. At the moment I have code like this: glPushMatrix(); glRotatef(90.0 0.0 1.0 0.0); glBegin(GL_TRIANGLES); glVertex3f( 1.0 1.0 0.0 ); glVertex3f( 3.0 2.0 0.0 ); glVertex3f( 3.0 1.0 0.0 ); glEnd(); glPopMatrix(); But the result is not a triangle rotated 90 degrees. Edit Hmm thanks to Mike Haboustak - it appeared my code was calling a SetCamera function that use glOrtho. I'm too new to OpenGL to have any idea of what this meant but disabling this and rotating in the Z-axis produced the desired result. Thanks! The ""accepted answer"" is not fully correct - rotating around the Z will not help you see this triangle unless you've done some strange things prior to this code. Removing a glOrtho(...) call might have corrected the problem in this case but you still have a couple of other issues. Two major problems with the code as written: Have you positioned the camera previously? In OpenGL the camera is located at the origin looking down the Z axis with positive Y as up. In this case the triangle is being drawn in the same plane as your eye but up and to the right. Unless you have a very strange projection matrix you won't see it. gluLookat() is the easiest command to do this but any command that moves the current matrix (which should be MODELVIEW) can be made to work. You are drawing the triangle in a left handed or clockwise method whereas the default for OpenGL is a right handed or counterclockwise coordinate system. This means that if you are culling backfaces (which you are probably not but will likely move onto as you get more advanced) you would not see the triangle as expected. To see the problem put your right hand in front of your face and imagining it is in the X-Y plane move your fingers in the order you draw the vertices (11) to (32) to (31). When you do this your thumb is facing away from your face meaning you are looking at the back side of the triangle. You need to get into the habit of drawing faces in a right handed method since that is the common way it is done in OpenGL. The best thing I can recommend is to use the NeHe tutorials - http://nehe.gamedev.net/. They begin by showing you how to set up OpenGL in several systems move onto drawing triangles and continue slowly and surely to more advanced topics. They are very easy to follow.  Regarding Projection matrix you can find a good source to start with here: http://msdn.microsoft.com/en-us/library/bb147302(VS.85).aspx It explains a bit about how to construct one type of projection matrix. Orthographic projection is the very basic/primitive form of such a matrix and basically what is does is taking 2 of the 3 axes coordinates and project them to the screen (you can still flip axes and scale them but there is no warp or perspective effect). transformation of matrices is most likely one of the most important things when rendering in 3D and basically involves 3 matrix stages: Transform1 = Object coordinates system to World (for example - object rotation and scale) Transform2 = World coordinates system to Camera (placing the object in the right place) Transform3 = Camera coordinates system to Screen space (projecting to screen) Usually the 3 matrix multiplication result is referred to as the WorldViewProjection matrix (if you ever bump into this term) since it transforms the coordinates from Model space through World then to Camera and finally to the screen representation. Have fun  When I had a first look at OpenGL the NeHe tutorials (see the left menu) were invaluable.  Ensure that you're modifying the modelview matrix by putting the following before the glRotatef call: glMatrixMode(GL_MODELVIEW); Otherwise you may be modifying either the projection or a texture matrix instead.  Do you get a 1 unit straight line? It seems that 90deg rot. around Y is going to have you looking at the side of a triangle with no depth. You should try rotating around the Z axis instead and see if you get something that makes more sense. OpenGL has two matrices related to the display of geometry the ModelView and the Projection. Both are applied to coordinates before the data becomes visible on the screen. First the ModelView matrix is applied transforming the data from model space into view space. Then the Projection matrix is applied with transforms the data from view space for ""projection"" on your 2D monitor. ModelView is used to position multiple objects to their locations in the ""world"" Projection is used to position the objects onto the screen. Your code seems fine so I assume from reading the documentation you know what the nature of functions like glPushMatrix() is. If rotating around Z still doesn't make sense verify that you're editing the ModelView matrix by calling glMatrixMode. IMPORTANT! See also Perry's answer below.  I'd like to recommend a book: 3D Computer Graphics : A Mathematical Introduction with OpenGL by Samuel R. Buss It provides very clear explanations and the mathematics are widely applicable to non-graphics domains. You'll also find a thorough description of orthographic projections vs. perspective transformations.",c++ opengl glut
721802,A,"What disadvantages could I have using OpenGL for GUI design in a desktop application? There are tons of GUI libraries for C/C++ but very few of them are based on the idea that opengl is a rather multiplatform graphics library. Is there any big disadvantage on using this OpenGL for building my own minimal GUI in a portable application? Blender is doing that and it seems that it works well for it. EDIT: The point of my question is not about using an external library or making my own. My main concern is about the use of libraries that use opengl as backend. Agar CEGUI or Blender's GUI for instance. Thanks. Here's an oddball one that bit a large physics experiment I worked on: because an OpenGL GUI bypasses some of the usual graphics abstraction layers it may defeat remote viewing applications. In the particular instance I'm thinking of we wanted to allow remote shift operations over VNC. Everything worked fine except for the one program (which we only needed about once per hour but we really needed) that used an OpenGL interface. We had to delay until a remote version of the OpenGL interface could be prepared. I was not involved in the solution but they arranged to display the OpenGL on the remote host. A little clunky but it meant I could sit shift in my PJs in my usual time-zone so I was happy. Yes this is the kind of answer I was looking for. Thanks dmckee! Good point. Generally anything that uses 3D acceleration will be invisible to apps like VNC. There are some ways to make it work but it causes a lot of performance issues and so isn't a great idea. I think to fix this you have to think an app as GUI-core-decoupled so you don't need VNC to control it. Actually there are several possibilities to use OpenGL enabled applications remotely such as for example xvnc/xf4vnc virtualgl or even just plain X11. The best solution depends on whether you want the OpenGL hardware acceleration to be done on the remote machine or locally on the viewer.  You're losing the native platforms capabilities for accessibility. For example on Windows most controls provide information to screen readers or other tools supporting accessibility impaired users. Basically unless you have a real reason to do this you shouldn't.  Re-inventing the Wheel: Yeah you'd be doing it. But I note OP used the word ""minimal"" in the problem statement so assuming it really doesn't need to scale up to all that it may be a small enough wheel as to not matter. The product I currently work on supports OpenGL on three platforms (Win Mac Linux) and we built all our own widgets (text boxes buttons dialogs). It's a lot of work but now that we've done it we own a huge chunk of our stack and don't have to debug into third party frameworks when things don't work as expected. It's nice having complete control of the experience. There's always something you want to do that a framework doesn't support. Like everything in our business it is a trade-off and you just have to weigh your needs against your need to finish on time. Portability: Yes you will still have to write platform specific code to boot-strap everything. This will be difficult if you've not done it before as it requires you to understand all the target platforms. Windows Drivers: We've found that graphics card manufacturers have much better support for DirectX on Windows than OpenGL since that's what is required to get MSFT certification. Often low- to mid-range cards have bugs missing functionality or outright crashes in their OpenGL support.  With Qt 4.5 you can select if you want to use OpenGL as window renderer. More info: http://blog.qt.digia.com/blog/2008/10/22/so-long-and-thanks-for-the-blit/ Read the comments to know about the problems about this. Your link is dead. Here's the new link: http://blog.qt.digia.com/blog/2008/10/22/so-long-and-thanks-for-the-blit/ link updated :)  The obvious one is that you're basically building the GUI elements yourself instead of having a nice designed like for wxWidgets or Qt.  You cant just use opengl you need a platform specific code to set up a window for opengl. From the freely available opengl red book: OpenGL is designed as a streamlined hardware-independent interface to be implemented on many different hardware platforms. To achieve these qualities no commands for performing windowing tasks or obtaining user input are included in OpenGL; instead you must work through whatever windowing system controls the particular hardware you're using. There are multiplatform solutions for this like glut qt wxWidgets ... And if you have to use them anyway why not use built in GUI elements. They also give you the opportunity to build your own and make use of the framework to handle mouse/keyboard events and stuff. It's very easy to make a multiplatform opengl program using preprococessor conditions and glut. It's far from using a huge library like qt for the whole GUI system. It very similar to make a multiplatform opengl program using qt: derive from QGLWidget; override initialiseGL paintGL resizeGL; assign your new class as mainwidget to a QApplication and thats it. The library is bigger yes. But i dont see the problem with that? Can you please tell me how they give you the opportunity to build your own GUI elements?  If you want a GUI as in windows/button etc. Don't do it yourself. There a lot of free solutions for this wxwidgetsqt or GTK. All have OpenGL support if you want a 3d window",c++ gui opengl
1691538,A,"What 3D graphics framework should I use for a real world game engine? I'm a C++ programmer with very extensive server programming experience. I'm however fairly bored at the moment and I decided to tackle a new area: 3D game programming for learning purposes. Additionally I think this learning project may turn out to be good resume material in the future should I decide to work in this area. Instead of creating a 3D engine from scratch I decided to emulate as exactly as I'm able an existing one: World of Warcraft's. If you are curious as to why (feel free to skip this): It's a real world successful game All the map textures models and what not are already done (I'm not interested in learning how to actually draw a texture with photoshop or whatever) Their file formats have been more or less completely reverse engineered There is already an identical open source project (wowmapview) that I can look at if I'm in trouble. ok that was a long preface.. Now my main question is the following: Should I use DirectX OpenGL wrapper libraries such as sdl or what? What's the most used one in the real world? And something else that perplexes me: World of Warcraft appears to be using both! In fact normally it uses DirectX but you can use opengl by starting it with the ""-opengl"" switch via command line. Is this something common in games? Why did they do it? I imagine it's a lot of work and from my understanding nobody uses OpenGL anyway (very very few people know about the secret switch at all). If it's something usually done do programmers usually create their own 3d engine ""wrapper"" something like SDL made in house and based on switches / #defines / whatnot decide which API function to ultimately call (DirectX or OpenGL)? Or is this functionality already built in in sdl (you can switch between DirectX and OpenGL at will)? And finally do you have any books to suggest? Thanks! ""Why did they do it?"" Because Mac OsX doesn't have DirectX support? SDL is merely a platform abstraction layer it substitutes neither OpenGL nor DirectX. When SDL is used though it is more commonly used with OpenGL since both cross platform libraries. Also worth noting is that SDL doesn't provide a widget toolkit so if that's a requirement (not very common in fullscreen 3D games) something like Qt might be a better choice for a cross platform application. @gf: good point. Makes me wonder now why they don't support linux natively since they went through all the trouble to make OpenGL work for mac.. should only need relatively trivial changes. oh well guess I'll never find out. Blizzard do or at least did at some point have an internal unsupported build for Linux but it's never been released. I'm pretty sure they're using SDL too since Slouken WoW's lead interface programmer is also the author of SDL... take a look at this as well http://gamedev.stackexchange.com/questions/90/why-is-it-so-hard-to-develop-a-mmo this is my favorite post when I think I am boring and decide to do some game for fun especially WOW like . ^_^ You might want to look at some projects that encapsulate the low level 3d api in a higher level interface that is api independent such as Ogre3D. As you are doing this to learn I assume you probably will be more interesting in implementing the low level detail yourself but you could probably learn a lot from such a project. Or irrlicht or openscenegraph Indeed all good.  if you are really only interested in the rendering part i can suggest ogre3d. it is clean c++ tested and cross-platform. i used it in two projects and compared to other experiences (torque3d for example) i liked the good support (tutorials/wiki/forum) and the not so steep learning curve. i think someone can also learn a lot by looking at the sourcecode and the concepts they have used in the design. there is a accompanying book as well which is a bit outdated but it is good for the start the problem with this is you will be thinking inside this engine and soon you will need gameplay-like (timers events) elements for simulating and testing your effects or whatever you want to do. so you will end up working around ogre3ds shortcomings (it is not a game engine) and implement in on your own or use another middleware. so if you really want to touch 3d rendering first i would take some computer graphics books (gpu gems shaderx) and see some tutorials and demos on the web and start building my own basic framework. this is for the experience and i think you will learn the most from this approach. at least i did ...  If you just want it to work on Windows then DirectX is a good choice. Your first decision will be to look at what is available for both OpenGL and DirectX and decide which will give you more of what you want with less work. If you have an old version of DirectX and don't want to upgrade you may want to use OpenGL but the main reason for the switch is for support on non-windows machines. Since you are using C++ you can pick either option. If you use a library or wrapper it will simplify your life but then you will need to design within the limits of the libraries. You may want to use Lua for much of the top-level parts of your application as that is what WoW is using and then where you need you can go lower-level to limit how much low-level work you need to do. I'm already familiar with Lua and I've used it extensively in the non performance critical parts of my server projects I love it! However I wanted to focus on the actual 3d and rendering engine not the interface so I don't think I will be using it for this project. Then just first decide what requirements you have such as OS and peripheral support as support will differ between OpenGL and DirectX. I decided I'll go with DirectX (at least at first) thanks! Nice choice it comes with so much when you have just installed it. Try to avoid making shapes with individual triangles fans will be your friend. :)  I realize you already accepted an answer but I think this deserves some more comments. Sorry to quote you out of order I'm answering by what I think is important. Instead of creating a 3D engine from scratch I decided to emulate as exactly as I'm able an existing one: World of Warcraft's. However I wanted to focus on the actual 3d and rendering engine not the interface so I don't think I will be using it [lua] for this project. From these two snippets I can tell you that you are not trying to emulate the game engine. Just the 3D rendering backend. It's not the same thing and the rendering backend part is very small part compared to the full game engine. This by the way can help answer one of your questions: World of Warcraft appears to be using both! In fact normally it uses DirectX but you can use opengl by starting it with the ""-opengl"" switch via command line. Yep they implemented both. The amount of work to do that is non-negligeable but the rendering back-end in my experience is at most 10% of the total code usually less. So it's not that outraging to implement multiple ones. More to the point the programming part of a game engine today is not the biggest chunk. It's the asset production that is the bulk (and that includes most game programming. Most lua scripts are considered on that side of things e.g.) For WoW OSX support meant OpenGL. So they did it. They wanted to support older hardware too... So they support DX8-level hardware. That's already 3 backends. I'm not privy to how many they actually implement but it all boils down to what customer base they wanted to reach. Multiple back-ends in a game engine is something that is more or less required to scale to all graphics cards/OSs/platforms. I have not seen a single real game engine that did not support multiple backends (even first party titles tend to support an alternate back-end for debugging purposes). ok that was a long preface.. Now my main question is the following: Should I use DirectX OpenGL wrapper libraries such as sdl or what? Well this depends strongly on what you want to get out of it. I might add that your option list is not quite complete: DirectX9 DirectX10 DirectX11 OpenGL < 3.1 (before deprecated API is removed) OpenGL >= 3.1 OpenGL ES 1.1 (only if you need to. It's not programmable) OpenGL ES 2.0 Yep those APIs are different enough that you need to decide which ones you want to handle. If you want to learn the very basics of 3D rendering any of those can work. OpenGL < 3.1 tends to hide a lot of things that ultimately has to happen in user code for the other ones (e.g. Matrix manipulation see this plug). The DX SDKs do come with a lot of samples that help understand the basic concepts but they also tend to use the latest and greatest features of DX when it's not necessarily required when starting (e.g. using Geometry shader to render sprites...) On the other hand most GL tutorials tend to use features that are essentially non-performant on modern hardware (e.g. glBegin/glEnd selection/picking ... see the list of things that got removed from GL 3.1 or this other plug) and tend to seed the wrong concepts for a lot of things. What's the most used one in the real world? For games DirectX9 is the standard today in PC world. By a far margin. However I'm expecting DirectX11 to grab more market share as it allows for some more multithreaded work. It's unfortunately significantly more complicated than DX9. nobody uses OpenGL anyway (very very few people know about the secret switch at all). Ask the Mac owners what they think. Side question do you really think hardware vendors would spend any energy in OpenGL drivers if this was really the case (I realize I generalize your comment sorry)? there are real world usages of it. Not much in games though. And Apple makes OpenGL more relevant through the iphone (well OpenGL ES really). If it's something usually done do programmers usually create their own 3d engine ""wrapper"" It's usually a full part of the engine design. Mind you it's not abstracting the API at the same level it's usually more at a ""draw this with all its bells and whistles over there"". Which rendering algorithm to apply on that draw tends to be back-end specific. This however is very game engine dependent. If you want to understand better you could look at UE3 it just got released free (beer) for non-commercial use (I have not looked at it yet so I don't know if they exposed the backends but it's worth a look). To get back to my comment that game engine does not just mean 3D look at this. +1 for a very thorough and thoughtful answer! Great answer marked this one as accepted. Thanks ! wow I did not even know you could change the accepted answer after the fact.  I think the primary benefit of using OpenGL over DirectX is the portability. DirectX only runs on windows. However this is often not a problem (many games only run on Windows). DirectX also provides other libraries which are useful for games which are unrelated to graphics such as sound and input. I believe there are equivalents which are often used with OpenGL but I don't think they're actually part of OpenGL itself. If you're going to be locking into windows with DirectX and you are willing to/interested in learning C# and managed code I have found XNA to be and extremely easy platform to learn. It allows you to learn most of the concepts without dealing with a lot of the really tricky details of DirectX (or OpenGL). You can still use shader code and have the full power of DirectX but in a much friendlier environment. It would be my recomendation but again you'd have to switch to C# (mind you you can also put that on you're resume). DirectX no longer supports C#: http://social.msdn.microsoft.com/Forums/en-US/gametechnologiesdirectx101/thread/e9f458e0-65c6-4936-9af4-4276fc92f111. The last version I used with support was 9.0c Huge +1 for XNA given the askers background and future applicability of the technology True you cannot program DirectX directly with C# however XNA is a C# library which uses DirectX underneath. It is not the same as DirectX and is very much in use today version 3.0 just came out recently.  I'm doing some OpenGL work right now (on both Windows and Mac). Compared to my midnight game programming using the Unity3D engine usingOpenGL is a bit like having to chop down your own trees to make a house versus buying the materials. Unity3D runs on everything (Mac PC and iPhone Web player etc) and lets you concentrate on the what makes a game a game. To top it off it's faster than anything I could write. You code for it in C# Java or Boo. I just used Unity to mock up a demo for a client who wants something made in OpenGL so it has it's real world uses also. -Chris PS: The Unity Indie version recently became free. A little correction: You can program in Javascript not Java! :-)",c++ opengl directx sdl
944665,A,"Designing a Qt + OpenGL application in Eclipse I'm starting a C++ project using OpenGL and Qt for the UI in eclipse. I would like to create a UI where a portion of the window contains a frame for OpenGL rendering and the rest would contain other Qt widgets such as buttons and so on. I haven't used Qt or the GUI editor in eclipse before and I'm wondering what the best approach would be? Should I create the UI by hand coding or would it be easier to use eclipse's GUI designer - I had a quick look at this and there doesn't seem to be an OpenGL widget built in. Thanks any particular aversion to QtCreator? This might be handy http://doc.trolltech.com/4.5/qtopengl.html If you are using Qt Designer (which I think is available via Eclipse Integration) you can place a base QWidget in the layout and then ""promote"" that widget to a QGLWidget. To do this: Add the QWidget to the desired place in the layout Right-click on the widget Select ""Promote To"" Enter QGLWidget as the class name and as the header Hit Add Select the QGLWidget from the list of promoted widgets at the top of the dialog Hit Promote This way you don't have to go through the placeholder route and create an additional layer. Thanks. That was just what I was after :-)  I had the same problem while using Qt Designer. I used a simple frame as a placeholder for the OpenGL widget then in main window constructor I created OpenGL widget manually and assigned it to the placeholder frame (as a child). The main advantage here is that you see where the OpenGL widget should be while designing your interface. The main disadvantage is that some coding is still required to set up the GUI.  Why won't you use Qt Eclipse Integration? It works flawlessly enables you to edit UIs directly from Eclipse. i am using that...",c++ eclipse gui qt opengl
538810,A,How do I get the current mouse position in C++ / OpenGL? I know that I can use a Mouse callback function for when a user clicks the mouse but what if I want to know the current x/y position without the user clicking? Will I have to use a different callback that gets called on any mouse movement and keep track of the x/y myself or is there a function I can call within GLUT/OpenGL to get it? This is more related to glut than OpenGL. Things like mouse position are usually handled by the OS and have little to do with either OpenGL or C++. Register a glutPassiveMotionFunc callback function See info about callbacks @crashmstr Now? It is working now.  You need to use the glutMotionFunc/glutPassiveMotionFunc callback to track mouse movement independent of mouse clicks. 7.6 glutMotionFunc glutPassiveMotionFunc,c++ opengl mouse glut
679210,A,How can I use a dynamically sized texture array with glTexImage2D? Currently I'm able to load in a static sized texture which I have created. In this case it's 512 x 512. This code is from the header: #define TEXTURE_WIDTH 512 #define TEXTURE_HEIGHT 512 GLubyte textureArray[TEXTURE_HEIGHT][TEXTURE_WIDTH][4]; Here's the usage of glTexImage2D: glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA TEXTURE_WIDTH TEXTURE_HEIGHT 0 GL_RGBA GL_UNSIGNED_BYTE textureArray); And here's how I'm populating the array (rough example not exact copy from my code): for (int i = 0; i < getTexturePixelCount(); i++) { textureArray[column][row][0] = (GLubyte)pixelValue1; textureArray[column][row][1] = (GLubyte)pixelValue2; textureArray[column][row][2] = (GLubyte)pixelValue3; textureArray[column][row][3] = (GLubyte)pixelValue4; } How do I change that so that there's no need for TEXTURE_WIDTH and TEXTURE_HEIGHT? Perhaps I could use a pointer style array and dynamically allocate the memory... Edit: I think I see the problem in C++ it can't really be done. The work around as pointed out by Budric is to use a single dimensional array but use all 3 dimensions multiplied to represent what would be the indexes: GLbyte *array = new GLbyte[xMax * yMax * zMax]; And to access for example x/y/z of 1/2/3 you'd need to do: GLbyte byte = array[1 * 2 * 3]; However the problem is I don't think the glTexImage2D function supports this. Can anyone think of a workaround that would work with this OpenGL function? Edit 2: Attention OpenGL developers this can be overcome by using a single dimensional array of pixels... [0]: column 0 > [1]: row 0 > [2]: channel 0 ... n > [n]: row 1 ... n > [n]: column 1 .. n ... no need to use a 3 dimensional array. In this case I've had to use this work around as 3 dimensional arrays are apparently not strictly possible in C++. You don't simply multiply the indices. It's (rowIndex * numColumns * numColourComponents + columnIndex + colourComponent); Oops. Should be (rowIndex * numColumns * numColourComponents + columnIndex*numColourComponents + colourComponent); You could always wrap it up in a class. If you are loading the image from a file you get the height and width out with the rest of the data (how else could you use the file?) you could store them in a class that wraps the file loading instead of using preprocessor defines. Something like: class ImageLoader { ... ImageLoader(const char* filename ...); ... int GetHeight(); int GetWidth(); void* GetDataPointer(); ... }; Even better you could hide the function calls to glTexImage2d in there with it. class GLImageLoader { ... ImageLoader(const char* filename ...); ... GLuint LoadToTexture2D(); // returns texture id ... };  Ok since this took me ages to figure this out here it is: My task was to implement the example from the OpenGL Red Book (9-1 p373 5th Ed.) with a dynamic texture array. The example uses: static GLubyte checkImage[checkImageHeight][checkImageWidth][4]; Trying to allocate a 3-dimensional array as you would guess won't do the job. Someth. like this does NOT work: GLubyte***checkImage; checkImage = new GLubyte**[HEIGHT]; for (int i = 0; i < HEIGHT; ++i) { checkImage[i] = new GLubyte*[WIDTH]; for (int j = 0; j < WIDTH; ++j) checkImage[i][j] = new GLubyte[DEPTH]; } You have to use a one dimensional array: unsigned int depth = 4; GLubyte *checkImage = new GLubyte[height * width * depth]; You can access the elements using this loops: for(unsigned int ix = 0; ix < height; ++ix) { for(unsigned int iy = 0; iy < width; ++iy) { int c = (((ix&0x8) == 0) ^ ((iy&0x8)) == 0) * 255; checkImage[ix * width * depth + iy * depth + 0] = c; //red checkImage[ix * width * depth + iy * depth + 1] = c; //green checkImage[ix * width * depth + iy * depth + 2] = c; //blue checkImage[ix * width * depth + iy * depth + 3] = 255; //alpha } } Don't forget to delete it properly: delete [] checkImage; Hope this helps... +1 for putting so much effort in to a 3 year old question. Helped me out too ! SO take my vote...This is one of those little intricacies of OpenGL  You can use int width = 1024; int height = 1024; GLubyte * texture = new GLubyte[4*width*height]; ... glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA width height 0 GL_RGBA GL_UNSIGNED_BYTE textureArray); delete [] texture; //remove the un-needed local copy of the texture; However you still need to specify the width and height to OpenGL in glTexImage2D call. This call copies texture data and that data is managed by OpenGL. You can delete resize change your original texture array all you want and it won't make a different to the texture you specified to OpenGL. Edit: C/C++ deals with only 1 dimensional arrays. The fact that you can do texture[a][b] is hidden and converted by the compiler at compile time. The compiler must know the number of columns and will do texture[a*cols + b]. Use a class to hide the allocation access to the texture. For academic purposes if you really want dynamic multi dimensional arrays the following should work: int rows = 16 cols = 16; char * storage = new char[rows * cols]; char ** accessor2D = new char *[rows]; for (int i = 0; i < rows; i++) { accessor2D[i] = storage + i*cols; } accessor2D[5][5] = 2; assert(storage[5*cols + 5] == accessor2D[5][5]); delete [] accessor2D; delete [] storage; Notice that in all the cases I'm using 1D arrays. They are just arrays of pointers and array of pointers to pointers. There's memory overhead to this. Also this is done for 2D array without colour components. For 3D dereferencing this gets really messy. Don't use this in your code. @Budric my array needs to have 3 dimensions not 2. I'm accessing it in the style of texture[column][row][channel] Hmm this would work only when assigning values to the array I get this error: invalid types ‘unsigned char[int]’ for array subscript Are you using texture[i][j]? You can't use that with the code above. texture[row * width + column].,c++ opengl glteximage2d
1569939,A,Rendering different triangle types and triangle fans using vertex buffer objects? (OpenGL) About half of my meshes are using triangles another half using triangle fans. I'd like to offload these into a vertex buffer object but I'm not quite sure how to do this. The triangle fans all have a different number of vertices... for example one might have 5 and another 7. VBO's are fairly straight forward using plain triangles however I'm not sure how to use them with triangle fans or with different triangle types. I'm fairly sure I need an index buffer but I'm not quite sure what I need to do this. I know how many vertices make up each fan during run time... I'm thinking I can use that to call something like glArrayElement Any help here would be much appreciated! VBOs and index buffers are an orthogonal things. If you're not using index buffers yet maybe it is wiser to move one step at a time. So... regarding your question. If you put all your triangle fans in a vbo the only thing you need to draw them is to setup your vbo and pass the index in it for your fan start  glBindBuffer(GL_VERTEX_BUFFER buffer); glVertexPointer(3 GL_FLOAT 0 NULL); // 3 floats per vertex for each i in fans glDrawArrays(GL_TRIANGLE_FAN indef_of_first_vertex_for_fan[i] fan_vertex_count[i]) Edit: I'd have to say that you're probably better off transforming your fans to a regular triangle set and use glDrawArrays(GL_TRIANGLES) for all your triangles. A call per primitive is rarely efficient. Well if you have 200K triangle fans then you have that many calls **per frame**. Think about the cost compared to some load-time (or better off-line process time) that's required to transform the fans to triangles. sun proposed an extension because this was too much of an overhead *10 years ago*. The overhead/intersting work ratio only increased since. http://www.opengl.org/registry/specs/SUN/triangle_list.txt To be complete I should add that [primitive restart](http://www.opengl.org/registry/specs/NV/primitive_restart.txt) is an extension that you could be using as well (but requires indexing of your primitives which you should look at anyways). [glMultiDrawArrays](http://www.opengl.org/sdk/docs/man/xhtml/glMultiDrawArrays.xml) is yet another attempt at reducing the overhead but is rarely actually optimized in the drivers. Bahbar Thanks for the tip! I had read about transforming the fans into triangles... However I might have 200000 triangle fan primitives. Granted I'd only need to do this once so in your opinion is it cheaper to transform them and then do glDrawArrays or should I just do the call for each prim? Thanks Good stuff thanks man. It is so hard to find good examples/documentation of this kind of stuff,c++ opengl graphics 3d vertex-buffer
1977737,A,"OpenGL Rotations around World Origin when they should be around Local Origin I'm implementing a simple camera system in OpenGL. I set up gluPerspective under the projection matrix and then use gluLookAt on the ModelView matrix. After this I have my main render loop which checks for keyboard events and if any of the arrow keys are pressed modifies angular and forward speeds (I only rotate through the y axis and move through the z (forwards)). Then I move the view using the following code (deltaTime is the amount of time since the last frame was rendered in seconds in order to decouple movement from framerate): //place our camera newTime = RunTime(); //get the time since app start deltaTime = newTime - time; //get the time since the last frame was rendered time = newTime; glRotatef(view.angularSpeed*deltaTime010); //rotate glTranslatef(00view.forwardSpeed*deltaTime); //move forwards //draw our vertices draw(); //swap buffers Swap_Buffers(); Then the code loops around again. My draw algorithm begins with a glPushMatrix() and ends in a glPopMatrix(). Each call to glRotatef() and glTranslatef() pushes the view forwards by the forwards speed in the direction of view. However when I run the code my object is drawn in the correct place but when I move the movement is done with the orientation of the world origin (000 - facing along the Z axis) as opposed to the local orientation (where I'm pointing) and when I rotate the rotation is done about (000) and not the position of the camera. I end up with this strange effect of my camera orbiting (000) as opposed to rotating on the spot. I do not call glLoadIdentity() at all anywhere inside the loop and I am sure that the Matrix Mode is set to GL_MODELVIEW for the entire loop. Another odd effect is if I put a glLoadIdentity() call inside the draw() function (between the PushMatrix and PopMatrix calls the screen just goes black and no matter where I look I can't find the object I draw. Does anybody know what I've messed up in order to make this orbit (000) instead of rotate on the spot? Thanks in advance for all of your help Please put code into code tags! glRotate() rotates the ModelView Matrix around the World Origin so to rotate around some arbitrary point you need to translate your matrix to have that point at the origin rotate and then translate back to where you started. I think what you need is this float x y z;//point you want to rotate around glTranslatef(00view.forwardSpeed*deltaTime); //move forwards glTranslatef(xyz); //translate to origin glRotatef(view.angularSpeed*deltaTime010); //rotate glTranslatef(-x-y-z); //translate back //draw our vertices draw(); //swap buffers Swap_Buffers(); Note that the comments on the 2nd and 3rd glTranslatef() calls are incorrect. They should be ""translate to origin"" and ""translate from origin"" respectively. Good catch! Thanks  Swap your rotate and translate calls around :) Since they post-multiply the matrix stack the last the be called is the 'first' to be applied conceptually if you care about that sort of thing. I tried that but it just gives the same effect. Since the rotate and translate calls all have such minute values in them and since they stack on top of each other as the while loop iterates there is little difference in their application order. Oh all your transformations are relative! If you reset the camera position each time (glLoadIdentity()) and do an absolute translation and rotation each time then things should work :)  I think you first have to translate your camera to point (000) then rotate then translate it back.",c++ opengl matrix rotation transform
1816431,A,QT Webkit & OpenGL Rendering Context Would it be possible to create a window with a webpage using a webkit component using QT4 then embed an OpenGL context into the middle in the same way a java applet or a flash applet may appear normally? Sure you can embed any QWidget into a web page shown through a QWebView including a QGLWidget. This would be a starting point in the docs: http://doc.trolltech.com/4.5/qwebpage.html#setPluginFactory . I gogoled using the references you mentioned and it lead me to this: http://daniel-albuschat.blogspot.com/2008/12/embedding-qt-widgets-into-qtwebkit.html ^_^ Since you gave me the lead I needed Ill accept the answer thanks!,c++ qt opengl
1135901,A,"Windows message loop Theres some reason for this code not reach the first else? I got it exactly the same from vairous sources. Than I did my own encapsulation. Everything goes fine. Window is created messages are treated events are generated to keyborad input in the client area the gl canvas works fine (when I force it to draw). The only problem is that message loop never leaves the first if. :/ I'm really stuck. while (!done) { if (::PeekMessage (&msg NULL 0 0 PM_REMOVE)) { if (msg.message == WM_QUIT) { done = TRUE; } else { ::TranslateMessage (&msg); ::DispatchMessage (&msg); } } else { // Code is never reaching this! draw (); ::SwapBuffers(hDC); idle (); } } return msg.wParam; Obviously something it posting new messages into the queue while Translate/Dispatch is done. You should just list all messages retrieved and deduce what message it is and why it appears. Using spy i got a hard flow of WM_PAINT with hdc 0. No idea how this is being generated. In your case the message queue must never be empty - why? Well it depends on what the rest of your program is doing. Some possibilities: Your code is posting new messages to the queue in a manner such that the queue doesn't get empty. I'd suggest logging out the message ids as they are handled. You aren't handling paint messages - from msdn: ""The PeekMessage function normally does not remove WM_PAINT messages from the queue. WM_PAINT messages remain in the queue until they are processed. However if a WM_PAINT message has a NULL update region PeekMessage does remove it from the queue."" Hope this helps. [Edit] To handle WM_PAINT either call BeginPaint and EndPaint or forward to DefWindowProc looks like you can also do it by forwarding wm_paint to DefWindowProc but it probably does the same thing looks like thats the problem. But the my MainWndProc is returning 0 from WM_PAINT. What I know is that every message you treat you return 0. So in theory no message should be left. He's handling WM_PAINT because he's seeing things appearing. Added this code and it start to work as expected. But would like to have some explanation if possible please. case WM_PAINT: BeginPaint(hwnd &ps); EndPaint(hwnd &ps); return 0; WM_PAINT isn't a real message - it's just a flag on the window that says it has invalidated regions (WM_PAINT never handled.) As long as that flag is present WM_PAINT will be generated. there isnt another way to acomplish that? looks like this will just add more processing to my program instead if just discard the message.  PeekMessage will return 0 only if there are no messages in the message queue. Since there are messages to be dispatched in the message queue it is returning a non-zero value and your else condition is never executed. I guess he is asking about this - why doesn't it ever happen that there're no messages left.  Make sure you are processing the WM_PAINT correctly. By this I mean make sure you are calling BeginPaint and EndPaint from inside the WM_PAINT message otherwise you will be confusing Windows into thinking your application still needs to be painted.  May be there is always a message waiting ? If you have a new question please ask it by clicking the [Ask Question](http://stackoverflow.com/questions/ask) button. Include a link to this question if it helps provide context.",c++ winapi opengl
363302,A,openGL textures that are not 2^x in dimention I'm trying to display a picture in an openGL environment. The picture's origninal dimensions are 3648x2432 and I want to display it with a 256x384 image. The problem is 384 is not a power of 2 and when I try to display it it looks stretched. How can I fix that? If GL_EXT_texture_rectangle is true then use GL_TEXTURE_RECTANGLE_EXT for the first param in glEnable() and GLBindTexture() calls.  You can resize your texture so it is a power of two (skew your texture so that when it is mapped onto the object it looks correct). I am under the impression that textures are always a power of two because the hardware would use the same amount of resources anyways. But I could be incorrect. Yes this is incorrect in modern hardware.  There's three ways of doing this that I know of - The one Albert suggested (resize it until it fits). Subdivide the texture into 2**n-sized rectangles and piece them together in some way. See if you can use GL_ARB_texture_non_power_of_two. It's probably best to avoid it though since it looks like it's an Xorg-specific extension. Option 2 is great - prevents loss of fidelity  ARB_texture_rectangle is probably what you're looking for. It lets you bind to GL_TEXTURE_RECTANGLE_ARB instead of GL_TEXTURE_2D and you can load an image with non power-of-2 dimensions. Be aware that your texture coordinates will range from [0..w]x[0..h] instead of [0..1]x[0..1].,c++ opengl ldf
1095378,A,"How do I destruct data associated with an object after the object no longer exists? I'm creating a class (say C) that associates data (say D) with an object (say O). When O is destructed O will notify C that it soon will no longer exist :( ... Later when C feels it is the right time C will let go of what belonged to O namely D. If D can be any type of object what's the best way for C to be able to execute ""delete D;""? And what if D is an array of objects? My solution is to have D derive from a base class that C has knowledge of. When the time comes C calls delete on a pointer to the base class. I've also considered storing void pointers and calling delete but I found out that's undefined behavior and doesn't call D's destructor. I considered that templates could be a novel solution but I couldn't work that idea out. Here's what I have so far for C minus some details:  // This class is C in the above description. There may be many instances of C. class Context { public: // D will inherit from this class class Data { public: virtual ~Data() {} }; Context(); ~Context(); // Associates an owner (O) with its data (D) void add(const void* owner Data* data); // O calls this when he knows its the end (O's destructor). // All instances of C are now aware that O is gone and its time to get rid // of all associated instances of D. static void purge (const void* owner); // This is called periodically in the application. It checks whether // O has called purge and calls ""delete D;"" void refresh(); // Side note: sometimes O needs access to D Data *get (const void *owner); private: // Used for mapping owners (O) to data (D) std::map _data; }; // Here's an example of O class Mesh { public: ~Mesh() { Context::purge(this); } void init(Context& c) const { Data* data = new Data; // GL initialization here c.add(this new Data); } void render(Context& c) const { Data* data = c.get(this); } private: // And here's an example of D struct Data : public Context::Data { ~Data() { glDeleteBuffers(1 &vbo); glDeleteTextures(1 &texture); } GLint vbo; GLint texture; }; }; P.S. If you're familiar with computer graphics and VR I'm creating a class that separates an object's per-context data (e.g. OpenGL VBO IDs) from its per-application data (e.g. an array of vertices) and frees the per-context data at the appropriate time (when the matching rendering context is current). So if I understand correctly you want reference counting? Thanks all. In my program there is an update thread and a render thread. The render thread is responsible for rendering twice once for each eye to create a stereo image. Each 3D object O has eye-specific rendering data D for each context (i.e. for each O there are two copies of D). The problem: O can be created or destroyed in the update thread but O's eye-specific rendering data must be destroyed in the render thread and only when rendering the associated eye. C needs to know if O is destructed so that in the render thread it will know to destroy both instances of D. The question is rather vague on the requirements so it's hard to give a good concrete answer. I hope the following helps. If you want the data to disappear immediately when its owner dies have the owner delete it (and notify C if the C instances need to know). If you want C to do the deletion at its leisure your solution looks fine. Deriving from Data seems to me the right thing to do. (Of course it is crucial that ~Data() be virtual as you have done.) What if D is an array of objects? There are two interpretations of this question. If you mean that D is always an array let it be an array (or vector<>) of pointers to Data. Then in C::purge() walk the vector and delete the objects. If you mean that D could be an array of objects but could also be a single object there are two ways to go. Either decide that it is always an array (possibly of size 1) or that it is a single object (derived from Data) which can be a class wrapping the array of the actual objects (or pointers to them). In the latter case the wrapper class destructor should walk the array and do the deletions. Note that if you want the array (or vector<>) to contains the actual objects not pointers to them (in which case you won't have to walk the array and delete manually) then you'll have the following limitations. 1. All objects in the array will have to be of the same actual type. 2. You will have to declare the array to be of that type. This will lose you all the benfits of polymorphism.  To answer the question ""What if D is an array of objects ?"" I'd suggest a vector<> but you'd have to associate it with D:  struct D_vector :D { vector<whatever> vw; };  What you're looking for is Boost::shared_ptr or some similar smart-pointer system.",c++ opengl graphics
723762,A,"Programs causing static noise in speakers? Does anyone know a reason why my programs could be causing my speakers to output some soft static? The programs themselves don't have a single element that outputs sound to anything yet when I run a few of my programs I can hear a static coming from my speakers. It even gets louder when I run certain programs. Moving the speakers around doesn't help so it must be coming from inside the computer. I'm not sure what other details to put down since this seems very odd. They are OpenGL programs written in C++ with MS Visual C++. Edit: It seems to be that swapping the framebuffers inside an infinite loop is making the noise as when I stop swapping I get silence... I get exactly the same thing with a number of programs - particularly with ObjectDock (a Mac Dock imitation for Windows) - but only when I move my mouse if the cursor is over the dock. Quite strange. I don't know if ObjectDock is supposed to do that - seems weird that it would. Since you say you don't touch sound in your programs I doubt it's your code doing this. Does it occur if you run any other graphics-intensive programs? Also what happens if you mute various channels in the mixer (sndvol32.exe on 32-bit windows)? Not knowing anything else I'd venture a guess that it could be related to the fan on your graphics card. If your programs cause the fan to turn on and it's either close to your sound card or the fan's power line crosses an audio cable it could cause some static. Try moving any audio cables as far as possible from the fan and power cables and see what happens. It could also be picking up static from a number of other sources and I wouldn't say it's necessarily unusual. If non-graphics-intensive programs cause this as well it could be hard-disk access or even certain frequencies of CPU/power usage being picked up on an audio line like an antenna. You can also try to reduce the number of loops in your audio wires and see if it helps but no guarantees. I opened Sim City muted game sounds and I heard the same static. I guess I never noticed because I always play with sound. =/ Turns out it was some power wires hanging over my sound card. I moved them and silence!  Most electronic devices give off some kind of electromagnetic interference. Your speakers or sound hardware may be picking up something as simple as the signaling on your video cable or the graphics card itself. Cheap speakers and poorly-protected audio devices tend to be fairly sensitive to this kind of radiation in my experience.  There is interference on your motherboard that is leaking onto your sound bus. This is usually because of the quality of your motherboard or the age of it. Also the layout of the equipment inside your computer (close together over lapping) often will make interesting EM fields. My old laptop used to do this a lot easier as it got older. So as things are winding up or down you'll hear it. Try to see if it happens on a different computer. Try different computers of different ages and different configurations (external soundcard or a physical sound card etc). Hope that helps.  Computers consume a different amount of power when executing code. This fluctuation of current acts like a RF transmitter and can be picked up by audio equipment and it will be essentially ""decoded"" much like a AM modulated signal. As the execution usually does not produce a recognizable signal it sounds like white noise. A good example of audio equippment picking up a RF signal is if you hold your (GSM) cell phone close to an audio amplifier when receiving a call. You most likely will hear a characteristic pumping buzz from the cell phone's transmitter. Go here to learn more about Electromagnetic compatibility. There are multiple ways a signal can couple into your audio. As you mentioned a power cord to be the source it was most likely magnetic inductive coupling. This is exactly what it is. You either need better sheilding on your cables/speakers or on your computer. If your PC case is plastic it won't block any RF generated from within. If the case is steel it acts like a Faraday Cage. I don't think thats the reason. The current level is too low in the circuits to cause audible sound. Also there is no system in the computer which is built to catch RF frequency. This noise should be generated by actually getting a electric signal.  :) You will be surprised to know that the speaker input is picking up static from the hard disk. When you do something memory/disk intensive (like swapping framebuffers) so that the hard disk has to rotate fast the sound will appear. I had the same problem some years back I solved it too. But I am sorry that I don't remember how I did it. Hope the diagnosis helps in remedying the problem. UPDATE: I remembered. If you are using Windows go to volume control and mute all the external inputs/outputs like CD input etc. Just keep the two basic ones.  Crappy audio hardware on motherboards especially the ones that end up in office PCs. The interior of a PC case is full of electrical noise. If that couples to the audio hardware you'll hear it. Solution: Get a pair of headphones with a volume control on the cord. Turn the volume on the headphones down and turn the volume on the PC up full. This will increase the signal level relative to the noise level in most cases.  tempest dvbt",c++ opengl audio
665688,A,"Is stereoscopy (3D stereo) making a come back? I'm working on a stereoscopy application in C++ and OpenGL (for medical image visualization). From what I understand the technology was quite big news about 10 years ago but it seems to have died down since. Now many companies seem to be investing in the technology... Including nVidia it would seem. Stereoscopy is also known as ""3D Stereo"" primarily by nVidia (I think). Does anyone see stereoscopy as a major technology in terms of how we visualize things? I'm talking in both a recreational and professional capacity. With nVidia's 3D kit you don't need to ""make a stereoscopy application"" drivers and video card take care of that. 10 years ago there was good quality stereoscopy with polarized glasses and extremely expensive monitors and low quality stereoscopy with red/cyan glasses. What you have now is both cheap and good quality. Right now all you need is 120Hz LCD entry level graphics card and $100 shutter glasses. So no doubt about it it will be the next big thing. At least in entertainment. I'm struggling to find a 120Hz LCD... Could someone name a few brands or models? Ones explicitly named by Nvidia: Samsung SyncMaster 2233RZ and ViewSonic FuHzion VX2265wm. I'm sure there are more. It's a bit hard to find them though as refresh Hz it's not something that's important LCD spec so most don't even specify this. Ah interesting. I was under the impression that 120Hz LCDs weren't available - perhaps I've been living in the past with the CRT sitting on my desk. Could you point me in the right direction? For example Nvidia bundles it's kit with Samsung SyncMaster 2233RZ. Sony sales 200Hz LCD Bravia Z since last year. 120Hz LCD is pretty much standard now. http://en.wikipedia.org/wiki/Motion_interpolation BTW. You're serious about having CRT on your desk? Wow! Haha yes I'd read in so many places that LCD doesn't support over 80Hz and I believed it since that appeared to be common knowledge; I guess all the material I read must have been ancient and defunct. As a result of this I bought a CRT. Now I feel a little embarrassed! I bought it about 2 years ago mind. Since when has >80Hz been around? Well it was true about 2 years ago. The thing is that fast LCD are TN which don't give you colors as rich as IPS or VA. BTW. most entry level LCD still are 60Hz although as long as you don't need stereoscopy it doesn't really mater. @Zeus one for each eye perfect for stereoscopy ;-) @vartec - I have 2 crt's on my desk :(  Enthusiasm for stereo display seems to come and go in cycles of hype and disappointment (e.g cinema). I don't expect TV and PCs will be any different. For medical visualisation if it was that useful there would be armies of clinicians sitting in front of expensive displays wearing shutter glasses already. Big hint: there aren't. And that market doesn't need 3D display tech to reach ""impulse purchase"" pricing levels as an enabler. You make a very good point. Actually in the medical field stereo 3D is quickly gaining popularity and is actively being used in multiple practices including robotics and surgery. For example with the DaVinci robot surgeons perform cases every day using a 3D display to operate with and the technicians moving tools in and out of the robot/person's body (using a monitor) don't want to push anything too far into the patient and stab them. Normally the robot would prevent you from doing this but it tends to malfunction and people just turn it off. So the need for depth perception is very important.  One reason why it is probably coming back is due to the fact that we know have screens with high enough refreshrate so that 3D is possible. I think I read that you will need somewhere around 100Hz for 3D-TV. So no need for bulky glasses anymore. Edit: To reiterate: You no longer need glasses in order to have 3D TV. This article was posted in a swedish magazine a few weeks ago: http://www.nyteknik.se/nyheter/it_telekom/tv/article510136.ece. What it says is basically that instead of glasses you use a technique with vertical lenses on the screen. Problem with CRT is that they are not flat. Our more modern flat screens obviously hasn't got this problem. The second problem is that you need high frequency (at least 100 Hz as that makes the eye get 50 frames per second) and a lot of pixels since each eye only gets half the pixels. TV sets that support 3D without glasses have been sold by various companies since 2005. It seems to me that 3D TVs are still quite rare I remember when Sharp tried this but I haven't really seen that product recently (I think they stopped marketing it) - I think Zalman do one right? Who else does them? What's the marketing keyword used most ""3D TV""? Zalman does ones that use polarization the 120Hz thing is based shutter glasses. The 3D-without-glasses displays are technically known as ""autosteroscopic"". I've seen the Philips one and it's quite impressive (as a tradeshow gimmick anyway).",c++ opengl nvidia stereo-3d stereoscopy
327642,A,"OpenGL and monochrome texture Is it possible to pump monochrome (graphical data with 1 bit image depth) texture into OpenGL? I'm currently using this: glTexImage2D( GL_TEXTURE_2D 0 1 game->width game->height 0 GL_LUMINANCE GL_UNSIGNED_BYTE game->culture[game->phase] ); I'm pumping it with square array of 8 bit unsigned integers in GL_LUMINANCE mode (one 8 bit channel represents brightness of all 3 channels and full alpha) but it is IMO vastly ineffective because the onlu values in the array are 0x00 and 0xFF. Can I (and how) use simple one-bit per pixel array of booleans instead somehow? The excessive array size slows down any other operations on the array :( After some research I was able to render the 1-bit per pixel image as a texture with the following code: static GLubyte smiley[] = /* 16x16 smiley face */ { 0x03 0xc0 /* **** */ 0x0f 0xf0 /* ******** */ 0x1e 0x78 /* **** **** */ 0x39 0x9c /* *** ** *** */ 0x77 0xee /* *** ****** *** */ 0x6f 0xf6 /* ** ******** ** */ 0xff 0xff /* **************** */ 0xff 0xff /* **************** */ 0xff 0xff /* **************** */ 0xff 0xff /* **************** */ 0x73 0xce /* *** **** *** */ 0x73 0xce /* *** **** *** */ 0x3f 0xfc /* ************ */ 0x1f 0xf8 /* ********** */ 0x0f 0xf0 /* ******** */ 0x03 0xc0 /* **** */ }; float index[] = {0.0 1.0}; glPixelStorei(GL_UNPACK_ALIGNMENT1); glPixelMapfv(GL_PIXEL_MAP_I_TO_R 2 index); glPixelMapfv(GL_PIXEL_MAP_I_TO_G 2 index); glPixelMapfv(GL_PIXEL_MAP_I_TO_B 2 index); glPixelMapfv(GL_PIXEL_MAP_I_TO_A 2 index); glTexImage2D(GL_TEXTURE_2D0GL_RGBA16160GL_COLOR_INDEXGL_BITMAPsmiley); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); and here is the result: Nice! Have you compared its performance to the 1 byte per pixel variant? I found swapping the internal format to `LUMINANCE` produces the same visuals with a quarter the memory. If there are any formats smaller I would be interested. Idk how to get S3TC formats. I don't think pyOpenGL has those extensions. It does not appear that you can clamp this texture without it changing shape. I was just trying to get rid of the cross border bleeding.  The smallest uncompressed texture-format for luminance images uses 8 bits per pixel. However 1 bit per pixel images can be compressed without loss to the S3TC or DXT format. This will still not be 1 bit per pixel but somewhere between 2 and 3 bits. If you really need 1 bit per pixel you can do so with a little trick. Load 8 1 bit per pixel textures as one 8 bit Alpha-only texture (image 1 gets loaded into bit 1 image 2 into bit 2 and so on). Once you've done that you can ""address"" each of the sub-textures using the alpha-test feature and a bit of texture environment programming to turn alpha into a color. This will of only work if you have 8 1 bit per pixel textures and tricky to get right though. The 8 textures trick sounds interesting but it is not really what I am looking for. I'm using the OpenGL to visualize data from my application so such trick would slow down standard operation on the array (not speaking of programming complications).",c++ opengl textures
791687,A,OpenGL: glTexImage2D conflicts with glGenLists & glCallList? I have a simple OpenGL application where I have 2 objects displayed on screen: 1) particle system where each particle is texture mapped with glTexImage2D() call. In the drawEvent function I draw it as a GL_TRIANGLE_STRIP with 4 glVertex3f. 2) a 3D text loaded from an object file where each point is loaded using glNewList/glGenLists and store each point as glVertex. I draw it by making call to glCallList. The problem is as soon as I call glTexImage2D() to map my particle with a .bmp file the 3D text would not show on the screen. The particle would look fine. (this is not expected) If I dont call glTexImage2D I see both the 3D text and particle system. In this case particle system looks horrible because I didnt texture map with anything. (this is expected) Does anyone know why using call list and glTexImage2D might conflict with each other? EDIT i also forgot to mention: I do call glBindTexture(GL_TEXTURE_2D this->texture); inside the drawLoop before particle system is called. EDIT2 i only call glTexImage2D() once at system start up (when I texture mapped the bitmap) glTexImage2D uploads the texture to the video-ram (simplified said). If OpenGL would allow you to place a glTexImage2D call inside the list it had to store the pixel-data in the list as well. Now what happends if you would execute the list? You would upload the same image data into the same texture all over gain. That makes no sense therefore it's left out. If you want to change textures between draw calls use glBindTexure. That call sets the current texture. It's much faster. Regarding your image-upload via glTexImage2D: Do that only once for each texture. Either at the start of your program (if it's small) or each time you load new content from disk. hi nils..yes i only call glTextImage2D once (at the start of the program) if you do so why do you want to put it into a list? hi..i'm sorry i dont explain clearly. i put all of my 3D fonts into a list. for particle system i use glGenTextures(1 &this->texture); glBindTexture(GL_TEXTURE_2D this->texture); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_WRAP_S GL_REPEAT); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_WRAP_T GL_REPEAT); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_MAG_FILTER GL_LINEAR); glTexParameterf(GL_TEXTURE_2DGL_TEXTURE_MIN_FILTER GL_LINEAR);  I already solve this problem. Before use GL_TEXTURE_2D you need set enable function=> glEnable(GL_TEXTURE_2D); And before use glCallList you also need set disable function => glDisable(GL_TEXTURE_2D);  It might not be a conflict of glTexImage2D and glCallList at all. Is your texture mapping ok ? have you set the texture coordinates propperly? Try checking for a vertex to texture coordinates missmatch. texture mapping looks ok for my particle system. the two when instantiated independently work fine. it's only when i to have them both displayed on the same screen it doesnt work.,c++ c opengl
1626081,A,"OpenGL - Textures loading improperly UPDATE: I've posted the Renderer code below since this code here doesn't seem to be the problem. I'm having a problem with some code of mine where when I try to upload multiple textures to openGL one at a time it fails somewhat spectacularly with the renderer only ending up using a single texture. I've done a bit of debugging to trace the error to this function but I'm having problems figuring out what part of the function is at fault. Are there are particularly obvious screwups I'm making that I'm simply not seeing or is there a more subtle flaw in my code? Here're the structs I use for storing texture information and generally just keeping track of all my pointers typedef struct { float Width; float Height; } texInfo; typedef struct { dshlib::utfstr ResourceName; texInfo * TextureInfo; GLuint TextureNum; SDL_Surface * Image; } texCacheItem; And here's the current WIP graphics loader. Basically it loads a named .png file out of a .zip archive using a prewritten library (incidentally it's being tested with this program). Then it's loaded with libpng and then loaded up as a texture with caching thrown in to speed loading up and avoid loading a single texture more than once. I omitted the #include statements since they were just cruft. texCacheItem * loadGraphics(dshlib::utfstr FileName) { for(int i = 0; i < NumTexCached; i++) { //First see if this texture has already been loaded if(TextureCache[i]->ResourceName == FileName) return TextureCache[i]; } dshlib::utfstr FullFileName = ""Data/Graphics/""; //If not create the full file path in the archive FullFileName += FileName; dshlib::FilePtr file = resourceCtr.OpenFile(FullFileName); //And open the file if (!file->IsOk()) { //If the file failed to load... EngineState = ENGINESTATE_ERR; return NULL; } SDL_Surface * T = loadPNG(file); texCacheItem * Texture = new texCacheItem; Texture->TextureInfo = new texInfo; glGenTextures(1 &Texture->TextureNum); //Allocate one more texture and save the name to the texCacheItem glBindTexture(GL_TEXTURE_2D Texture->TextureNum); //Then create it glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA8 T->w T->h 0 GL_RGBA GL_UNSIGNED_BYTE T->pixels); Texture->TextureInfo->Width = (float)T->w; //Write the useful data Texture->TextureInfo->Height = (float)T->h; Texture->ResourceName = FileName; //And the caching info needed Texture->Image = T; //And save the image for if it's needed later and for deleting if (!TexCacheSize) { //If this is the first load this is 0 so allocate the first 8 Cache slots. TexCacheSize = 8; TextureCache = new texCacheItem*[8]; } if(NumTexCached == TexCacheSize) { //If we're out of cache space if (TexCacheSize == 32768) { //If too many cache items error out EngineState = ENGINESTATE_ERR; return NULL; } TexCacheSize <<= 1; //Double cache size texCacheItem ** NewSet = new texCacheItem*[TexCacheSize]; memcpy(NewSet TextureCache NumTexCached * sizeof(texCacheItem*)); //And copy over the old cache delete TextureCache; //Delete the old cache TextureCache = NewSet; //And assign the pointer to the new one } TextureCache[NumTexCached++] = Texture; //Store the texCacheItem to the Cache file->Close(); //Close the file file = NULL; //And NULL the smart pointer. [NTS: Confirm with Disch this is what won't cause a memory leak] return Texture; //And return the loaded texture in texCacheItem form. } SDL_Surface *loadPNG(dshlib::FilePtr File) { Uint8 *PNGFile = new Uint8[(long)File->GetSize()]; File->GetAr<Uint8>(PNGFile (long)File->GetSize()); return IMG_LoadPNG_RW(SDL_RWFromMem(PNGFile (long)File->GetSize())); } Here's the renderer code file. It's quite messy at the moment apologies for that. level->activeMap basically tells the renderer which ""layer"" of the tilemap (0 being the front 3 the back) to draw the sprites above. #include ""../MegaJul.h"" void render(void) { //Render the current tilemap to the screen glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT); glLoadIdentity(); glTranslatef(0.0f 0.0f -4.0f); if (level) { glBegin(GL_QUADS); float dT = 32.0f / level->dTex; float sX fX fXa sY tX tY sYa sYb sXa tXa tYa; unsigned long m = level->mapDimensions[0] * level->mapDimensions[1]; float ai; long long t; Sint16 * p; glBindTexture(GL_TEXTURE_2D level->tilemap->TextureNum); for (int i = 3; i >= 0; i--) { if (level->layers[i]->mapPosition[0] > 0) level->layers[i]->mapPosition[0] = 0; if (level->layers[i]->mapPosition[0] < 0 - (signed long)((level->mapDimensions[0] - 21) * 32)) level->layers[i]->mapPosition[0] = 0 - (signed long)((level->mapDimensions[0] - 21) * 32); if (level->layers[i]->mapPosition[1] < 0) level->layers[i]->mapPosition[1] = 0; if (level->layers[i]->mapPosition[1] > (signed long)((level->mapDimensions[1] - 16) * 32)) level->layers[i]->mapPosition[1] = (signed long)((level->mapDimensions[1] - 16) * 32); if (i == level->activeMap) { for (int j = 0; j < NumSprites; j++) { glBindTexture(GL_TEXTURE_2D Sprites[j]->Graphics->TextureNum); Sprites[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); } for (int j = 0; j < NumBullets; j++) { glBindTexture(GL_TEXTURE_2D Bullets[j]->Texture->TextureNum); Bullets[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); } } glBindTexture(GL_TEXTURE_2D level->tilemap->TextureNum); t = 0 - ((level->layers[i]->mapPosition[0] - (level->layers[i]->mapPosition[0] % 32)) /32) + (((level->layers[i]->mapPosition[1] - (level->layers[i]->mapPosition[1] % 32)) /32) * level->mapDimensions[0]); ai = (float)(3 - i); //Invert Z-Index sX = (float)((level->layers[i]->mapPosition[0] % 32)); sY = (float)((level->layers[i]->mapPosition[1] % 32)); if (sX > 0) sX -= 32; if (sY < 0) sY += 32; fX = sX /= 32.0f; sY /= 32.0f; fXa = sXa = sX + 1.0f; sYa = sY + 14.0f; sYb = sY + 15.0f; for (int y = 0; y < 16; y++) { for (int x = 0; x < 21; x++) { p = level->tiles[level->layers[i]->map[t]]->position; tX = p[0] / level->dTex; tY = p[1] / level->dTex; tXa = tX + dT; tYa = tY + dT; glTexCoord2f(tX tYa); glVertex3f(fX sYa ai); // Bottom Left Of The Texture and Quad glTexCoord2f(tXatYa); glVertex3f(fXa sYa ai); // Bottom Right Of The Texture and Quad glTexCoord2f(tXatY); glVertex3f(fXa sYb ai); // Top Right Of The Texture and Quad glTexCoord2f(tX tY); glVertex3f(fX sYb ai); // Top Left Of The Texture and Quad fX += 1.0f; fXa += 1.0f; t++; if (t >= m) break; } sYb -= 1.0f; sYa -= 1.0f; fXa = sXa; fX = sX; t += level->mapDimensions[0] - 21; //21 is the number of tiles drawn on a line (20 visible + 1 extra for scrolling) } } glEnd(); } SDL_GL_SwapBuffers(); } Here's the code segments that set the tilemap data for sprites and the level: Level: void loadLevel(dshlib::utfstr FileName) { -snip- texCacheItem * Tex = loadGraphics(FileName); if (!Tex) { //Load the tile graphics for the level unloadLevel(); EngineState = ENGINESTATE_ERR; return; } else { level->dTex = Tex->TextureInfo->Width; level->tilemap = Tex; } -snip- } Sprite: void SpriteBase::created() { this->Graphics = loadGraphics(DefaultGFX()); -snip- } UPDATE 2: Sid Farkus noted one big mistake I made with the renderer so here's an updated renderer.cpp: #include ""../MegaJul.h"" void render(void) { //Render the current tilemap to the screen glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT); glLoadIdentity(); glTranslatef(0.0f 0.0f -4.0f); if (level) { float dT = 32.0f / level->dTex; float sX fX fXa sY tX tY sYa sYb sXa tXa tYa; unsigned long m = level->mapDimensions[0] * level->mapDimensions[1]; float ai; long long t; Sint16 * p; for (int i = 3; i >= 0; i--) { if (level->layers[i]->mapPosition[0] > 0) level->layers[i]->mapPosition[0] = 0; if (level->layers[i]->mapPosition[0] < 0 - (signed long)((level->mapDimensions[0] - 21) * 32)) level->layers[i]->mapPosition[0] = 0 - (signed long)((level->mapDimensions[0] - 21) * 32); if (level->layers[i]->mapPosition[1] < 0) level->layers[i]->mapPosition[1] = 0; if (level->layers[i]->mapPosition[1] > (signed long)((level->mapDimensions[1] - 16) * 32)) level->layers[i]->mapPosition[1] = (signed long)((level->mapDimensions[1] - 16) * 32); if (i == level->activeMap) { for (int j = 0; j < NumSprites; j++) { glBindTexture(GL_TEXTURE_2D Sprites[j]->Graphics->TextureNum); glBegin(GL_QUADS); Sprites[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); glEnd(); } for (int j = 0; j < NumBullets; j++) { glBindTexture(GL_TEXTURE_2D Bullets[j]->Texture->TextureNum); glBegin(GL_QUADS); Bullets[j]->render(level->layers[i]->mapPosition[0] level->layers[i]->mapPosition[1]); glEnd(); } } glBindTexture(GL_TEXTURE_2D level->tilemap->TextureNum); glBegin(GL_QUADS); -snipped out renderer since it was bloat glEnd(); } SDL_GL_SwapBuffers(); } Your code looks fine have you verified your PNG loader is working correctly? Create a 4x4 texture and check the values to make sure you've got the right things out of the loader and that the byte ordering matches what you pass to glTexImage2D. Barring that I'd focus on your rendering code. Btw weird naming convention to start variable names with capital letter. I was confused for good 10 seconds when I first looked at your code. (not to say it's wrong just weird) I'm assuming you can't do any sort of debugging or logging for this? If you could I'd expect this would be trivial to diagnose. The main thing that looks dangerous to me is that you're not checking the return value from loadPNG. I'd put something there as the very first thing I did. I'd consider commenting out the initial check for an already-cached texture too. If things start working at that point you know it's a problem with the resource names or filenames (or the comparison of them). As an aside I'm surprised that you're using classes and smart pointers but rolling your own std::vector with bare pointers and arrays. ;) I have done debugging - and I've had no luck which is why I'm asking here. As far as the intermix goes well I'm sure I'll get around to making it make sense soon enough.  In your renderer are you calling glBindTexture appropriately? It sounds like your renderer is just using whatever the last texture you uploaded was since that was the last time you called glBindTexture. glBindTexture is what tells OpenGL texture to use for your polygons. Nope I'm making sure I always call glBindTexture() in the renderer. Would it help if I posted the relevant parts of it for reference? Yeah definitely since there is nothing obvious wrong in the loading code. (This assumes you are actually loading different textures of course. :) ) Print out the TextureNum before you call bind to be sure you are in fact at least setting a different opengl texture. Also try using GLintercept to see if you can spot anything using that. (I use this all the time to track down my opengl problems.) Running a debugger check on those shows that the sprite has TextureNum = 2 and level has TextureNum = 1. Thanks for the glIntercept tip though! ..And that solved my problem thanks! I should clarify; the GLIntercept program showed me what I did wrong. I had the last glEnd() one curly-brace too far down and wasn't noticing that.  With your rendering code I can see you're calling BindTexture in a glBegin/End block. From the opengl docs: GL_INVALID_OPERATION is generated if glBindTexture is executed between the execution of glBegin and the corresponding execution of glEnd. Move your BindTexture calls outside the glBegin()/glEnd() block and you should be golden. You'll probably have to have multiple blocks to accommodate your rendering style. edit: With the updated code make sure of a couple things; your sprite positions are visible on the screen with the current projection/model view matrix and your sprite texture ids are valid textures. There's nothing technically wrong that jumps out at me now but your values may be off. Hm now I'm getting the same issue but instead the *other* texture is used... Copying in the newer version knowing me I completely goofed it up... Make sure inside your render() calls you don't do anything wierd either. Remember there are a limited subset of functions you're permitted to call between glBegin and glEnd. The only things the sprite's render does is calculate some offsets and then call glTexCoord2f and glVertex3f.",c++ opengl textures
819953,A,"How to start writing a music visualizer in C++? I'm interested in learning to use OpenGL and I had the idea of writing a music visualizer. Can anyone give me some pointers of what elements I'll need and how I should go about learning to do this? with a ""#include"" perhaps? sorry couldn't help myself :) You can find implementation of FFT algorithms and other useful informations in Numerical Recipes in C book. The book is free AFAIK. There is also Numerical Recipes in C++ book.  For the music analysis part you should study the basis of Fourier series then pick a free implementation of a DFFT (digital fast fourier transform) algorithm.  From my point of view...check this site: http://nehe.gamedev.net/ really good Information and Tutorials for using OpenGL edit: http://www.opengl.org/code/  If you use C++/CLI here's an example that uses WPF four (fourier that is;) display. He references this site that has considerable information about what your asking here's anoutline from the specific page; How do we split sound into frequencies? Our ears do it by mechanical means mathematicians do it using Fourier transforms and computers do it using FFT. The Physics of Sound 1.2. Harmonic Oscillator Sampling Sounds Fourier Analysis Complex Numbers Digital Fourier Transform FFT Ahhh I found this a few minutes later it's a native C++ analyzer. Code included that should get you off and running.  My approach for creating BeatHarness (http://www.beatharness.com) : record audio in real time have a thread that runs an FFT on the audio to get the frequency intensities calculate audio-volume for left and right channel filter the frequencies in bands (bass midtones treble) now you have some nice variables to use in your graphics display. For example show a picture where the size is multiplied by the bass - this will give you a picture that'll zoom in on the beat. From there on it's your own imagination ! :)  Are you trying to write your own audio/music player? Perhaps you should try writing a plugin for an existing player so you can focus on graphics rather than the minutia of codecs dsp and audio output devices. I know WinAMP and Foobar have APIs for visualization plugins. I'm sure Windows Media Player and iTunes also have them. Just pick a media player and start reading. Some of them may even have existing OpenGL plugins from which you can start so you can focus on pure OpenGL. I'm not trying to write a player or anything just something that will take in an MP3 and visualize it but I don't know anything about how to connect all those pieces. I'm on Linux so I'd stick to something simple. Haven't really thought about integrating it into anything else. consider using a library like mpg123 libmad or ffmpeg to decode then mp3 into audio samples. From there you'll want to use DFT(FFT) to convert audio to frequency information (see FFTW). At this point you'll have raw frequency data similar to what you see on most visuliazers (winamp/xmms moving lines w/ peaks). After that you need to figure out what to visual based on frequency and changes in frequency.  If you're just after some basic 3D or accelerated 2D then I'd recommend purchasing a copy of Dave Astle's ""Beginning OpenGL Game Programming"" which covers the basics of OpenGL in C++.  You may want to consider using libvisual's FFT/DCT functions over FFTW; they're much simpler to work with and provide data that's similarly easy to work with for the sake of generating visuals. Several media players and visualization plugins use libvisual to some extent for their visuals. Examples: Totem (player) GOOM (plugin for Totem and other players) PsyMP3 2.x (player)",c++ opengl audio visualization
1738636,A,"OpenGL Keyboard Camera Controls I have been following this tutorial series for OpenGL: GLUT: http://www.lighthouse3d.com/opengl/glut/ I have reached the stage of implementing camera controls using the keyboard: http://www.lighthouse3d.com/opengl/glut/index.php?8 When doing the advanced tutorial it stops working. I've pretty much just copied and pasted it. When I run its version of the code it works. Mine just doesn't seem to work. It should rotate the camera view when moving left and right and move forward and backwards when using up and down keys. My code is here broken down: This part of my code renders components in the scene with init() which initilizes values etc.: void display(void) { if (deltaMove) moveMeFlat(deltaMove); if (deltaAngle) { angle += deltaAngle; orientMe(angle); } glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glBindTexture(GL_TEXTURE_2D texture[0]); RenderSkyDome(); glBindTexture(GL_TEXTURE_2D NULL); glutSwapBuffers(); } void init(void) { glClearColor(0.0 0.0 0.0 0.0); glClearDepth(1.0f); glColor3f(0.0 0.0 1.0); glMatrixMode(GL_PROJECTION); glLoadIdentity(); glEnable(GL_TEXTURE_2D); glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glHint(GL_PERSPECTIVE_CORRECTION_HINT GL_NICEST); LoadTextures(""clouds2.bmp"" 0); GenerateDome(600.0f 5.0f 5.0f 1.0f 1.0f); snowman_display_list = createDL(); } This is my main loop function: int main () { glutInitDisplayMode(GLUT_DEPTH | GLUT_DOUBLE | GLUT_RGBA); glutInitWindowSize(800 600); glutInitWindowPosition(0 0); glutCreateWindow(""Captain Ed's Adventures: Great Wall of China""); init(); //Glut Input Commands glutIgnoreKeyRepeat(1); glutSpecialFunc(pressKey); glutSpecialUpFunc(releaseKey); glutKeyboardFunc(processNormalKeys); glutDisplayFunc(display); glutIdleFunc(display); glutReshapeFunc(reshape); // This redraws everything on screen when window size is changed. glutMainLoop(); return 0; } Here are my input functions which are called: void pressKey(int key int x int y) { switch (key) { case GLUT_KEY_LEFT : deltaAngle = -0.10f;break; case GLUT_KEY_RIGHT : deltaAngle = 0.10f;break; case GLUT_KEY_UP : deltaMove = 50;break; case GLUT_KEY_DOWN : deltaMove = -50;break; } } void releaseKey(int key int x int y) { switch (key) { case GLUT_KEY_LEFT : if (deltaAngle < 0.0f) deltaAngle = 0.0f; break; case GLUT_KEY_RIGHT : if (deltaAngle > 0.0f) deltaAngle = 0.0f; break; case GLUT_KEY_UP : if (deltaMove > 0) deltaMove = 0; break; case GLUT_KEY_DOWN : if (deltaMove < 0) deltaMove = 0; break; } } void processNormalKeys(unsigned char key int x int y) { if (key == 27) exit(0); } Variables that get used by the camera and the functions that should change it: static float angle=0.0deltaAngle = 0.0ratio; static float x=0.0fy=1.75fz=5.0f; static float lx=0.0fly=0.0flz=-1.0f; static int deltaMove=0; void orientMe(float ang) { lx = sin(ang); lz = -cos(ang); glLoadIdentity(); gluLookAt(x y z x + lxy + lyz + lz 0.0f1.0f0.0f); } void moveMeFlat(int i) { x = x + i*(lx)*0.1; z = z + i*(lz)*0.1; glLoadIdentity(); gluLookAt(x y z x + lxy + lyz + lz 0.0f1.0f0.0f); } I believe that's pretty much everything I am working with so basically what should happen is that when I press UP key deltaMove should = 50 and when this happens the if statement in void display(void) should do moveMeFlat(deltaMove); I don't know if I am doing this wrong or if there is a better result.... I can move ""moveMeFlat(deltaMove)"" within the relevant switch cases but this does not allow me to have the movement that I want. It seems to work using the source code from the tutorials above with the right functionality but not in my case so my assumption is that it's to do with the if statement in display. The end result I am looking for is to be able to have forwards and backwards working with left and right rotating the camera. I would like to be able to press forward and left key and see the camera swerve left like in a racing game... This question has had many points looked into over at http://www.gamedev.net/community/forums/topic.asp?topic_id=553415 still no solution let me know if you can help thanks. You can download my sourcecode for this @ http://www.anicho.com/downloads/StageTwo.zip Try getting rid of static in front of all of your global variable declarations. The last use of static is as a global variable inside a file of code. In this case the use of static indicates that source code in other files that are part of the project cannot access the variable. Only code inside the single file can see the variable. (It's scope -- or visibility -- is limited to the file.) This technique can be used to simulate object oriented code because it limits visibility of a variable and thus helps avoid naming conflicts. This use of static is a holdover from C. -http://www.cprogramming.com/tutorial/statickeyword.html Also instead of this: if (deltaMove) moveMeFlat(deltaMove); if (deltaAngle) { angle += deltaAngle; orientMe(angle); } try this: if (deltaMove != 0) moveMeFlat(deltaMove); if (deltaAngle != 0.0) { angle += deltaAngle; orientMe(angle); } http://www.gamedev.net/community/forums/topic.asp?topic_id=553415 solution can be found here  Is GLUT idle when a key press is occurring? I think it is not idle. Your display() function is only called when the window is created and when GLUT is idle which doesn't allow your position to be changed. I suggest adding: glutTimerFunc(40 display 1); to the end of your display function. This will cause display to be called every 40 milliseconds. Also don't register display as the idle function or you'll have the idle function starting lots of series of timer functions. P.S. I would call glFlush(); just before swapping the buffers in your display function. http://www.gamedev.net/community/forums/topic.asp?topic_id=553415 thanks for the glFlush tip the solution can be found on this forum post",c++ opengl input keyboard glut
1450023,A,"openGL and STL? I am using openGL and am currently passing it a vertex array. The problem is that I have to create many vertices and add them in between one another (for order). This means that using a regular array is pretty annoying/inefficient. I want to use a data structure from STL so that I can efficiently (and easily) put new vertices at any index. The problem is that openGL expects a regular array. Does anyone know how to go about this? Is there an easy way to convert from an STL vector to an array? I am using openGL 1.1 Thanks vector<int> array; ..... functionThatAcceptsArray(&array[0]); // yes this is standard  It depends on how much control you need over your intermediate data representations and how much you are able to precalculate. For a compromise between freedom and memory usage read about the Winged Edge data structure. The structure allows quick access and traversal between vertices edges and faces and works like a doubled linked-list. If you implement the concept and make an iterator implementation for it you can use std::copy to copy the data into any STL container. As the rest of the folks have already mentioned use std::vector as the final representation when OpenGL needs the data. And lastly: Don't be afraid to have several instances of the same data!  OpenGL requires a contiguous array of elements. For hopefully obvious reasons there is no efficient way to insert a single element into a contiguous array. It is necessarily at least O(N). However you potentially could add N elements in less than the O(N^2) that the vector achieves for N random insertions. For example if you don't actually add new vertices ""at any index"" but always close to the previous one you could add all the elements to a std::list (O(1) per element O(N) total) then copy the std::list to a std::vector. In fact it doesn't have to be the previous element just a previous element so if the order is based on a recursive traversal of some tree then you might still be able to do this. If new vertices are added at an index determined by some linear order then add all the elements to a std::map or std::multi_map (O(log N) per element O(N log N) total) then copy that to a vector. So the lowest-complexity way of doing it depends on how the order is determined. Whether these lower-complexity solutions are actually faster than the vector depends on N. They have much higher overheads (O(N) allocations instead of O(log N) for the vector) so N might have to be pretty big before the asymptotic behaviour kicks in. If you do use either of the solutions I describe then the easy/efficient way to copy either a list or a map to a vector is like this: std::vector<glVertex3f> vec; vec.reserve(listormap.size()); vec.insert(vec.begin() listormap.begin() listormap.end());  You can use a pointer to the first address of the vector as an array pointer. STL vectors are guaranteed to keep their elements in contiguous memory. So you can just do something like: &vertices[0] where vertices is your vector.",c++ opengl stl
205522,A,openGL SubTexturing I have image data and i want to get a sub image of that to use as an opengl texture. glGenTextures(1 &m_name); glGetIntegerv(GL_TEXTURE_BINDING_2D &oldName); glBindTexture(GL_TEXTURE_2D m_name); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA m_width m_height 0 GL_RGBA GL_UNSIGNED_BYTE m_data); How can i get a sub image of that image loaded as a texture. I think it has something to do with using glTexSubImage2D but i have no clue how to use it to create a new texture that i can load. Calling: glTexSubImage2D(GL_TEXTURE_2D 0 xOffset yOffset xWidth yHeight GL_RGBA GL_UNSIGNED_BYTE m_data); doe nothing that i can see and calling glCopyTexSubImage2D just takes part of my framebuffer. Thanks Edit: Use glPixelStorei. You use it to set GL_UNPACK_ROW_LENGTH to the width (in pixels) of the entire image. Then you call glTexImage2D (or whatever) passing it a pointer to the first pixel of the subimage and the width and height of the subimage. Don't forget to restore GL_UNPACK_ROW_LENGTH to 0 when you're finished with it. Ie: glPixelStorei( GL_UNPACK_ROW_LENGTH img_width ); char *subimg = (char*)m_data + (sub_x + sub_y*img_width)*4; glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA sub_width sub_height 0 GL_RGBA GL_UNSIGNED_BYTE subimg ); glPixelStorei( GL_UNPACK_ROW_LENGTH 0 ); Or if you're allergic to pointer maths: glPixelStorei( GL_UNPACK_ROW_LENGTH img_width ); glPixelStorei( GL_UNPACK_SKIP_PIXELS sub_x ); glPixelStorei( GL_UNPACK_SKIP_ROWS sub_y ); glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA sub_width sub_height 0 GL_RGBA GL_UNSIGNED_BYTE m_data ); glPixelStorei( GL_UNPACK_ROW_LENGTH 0 ); glPixelStorei( GL_UNPACK_SKIP_PIXELS 0 ); glPixelStorei( GL_UNPACK_SKIP_ROWS 0 ); Edit2: For the sake of completeness I should point out that if you're using OpenGL-ES then you don't get GL_UNPACK_ROW_LENGTH. In which case you could either (a) extract the subimage into a new buffer yourself or (b)... glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA sub_width sub_height 0 GL_RGBA GL_UNSIGNED_BYTES NULL ); for( int y = 0; y < sub_height; y++ ) { char *row = m_data + ((y + sub_y)*img_width + sub_x) * 4; glTexSubImage2D( GL_TEXTURE_2D 0 0 y sub_width 1 GL_RGBA GL_UNSIGNED_BYTE row ); } Basically i have an image ( as raw data ) and i want to use part of that image as a texture. I know how to load the entire image as a texture but don't know how to use just a bit of it. Poster answered this. If you have an image in memory it is stored somehow. If you used glTexImage2D(...GL_RGBA GL_UNSIGNED_BYTE) to load then there's a byte for R G B and alpha value. The images are stored with pixel (0 0) starts at position 0. Pixel x at row y starts at image[width*y+x][0] since i am using openGL ES i ended up doing the first option u gave that is extracting subimage into a new buffer. sadly on the device it no longer works so i'm going to rework the texture so that i don't need to solve the problem.,c++ opengl
1993431,A,"OpenGL: Rendering more than 8 lights how? How should I implement more than 8 lights in OpenGL? I would like to render unlimited amounts of lights efficiently. So whats the preferred method for doing this? OpenGL lights is a simplistic system that as far as I know is already in the deprecated list. You should handle lights yourself by writing a shader. Take a look here. the tutorial says ""therefore the best approach is to compile different shaders for different numbers of lights."" umm... thats not really what i want to do since i can move in the world and sometimes there might be 100 lights rendered sometimes 10 lights... i wouldnt want to write up to 2000 shaders for every possible combination of visible lights. Or did i understand something horribly wrong :/  Deferred shading. In a nutshell you render your scene without any lights. Instead you store the normals and world positions along with the textured pixels into multiple frame-buffers (so called render targets). You can even do this in a single pass if you use a multiple render-target extension. Once you have your buffers prepared you start to render a bunch of full-screen quads each with a pixel shader program that reads out the normals and positions and computes the light for one or multiple light-sources. Since light is additive you can render as much full-screen quads as you want and accumulate the light for as much light-sources as you want. A final step does a composition between your light and the unlit textured frame-buffer. That's more or less the state-of-the-art way to do it. Getting fog and transparency working with such a system is a challenge though.",c++ opengl lights
765629,A,"Learning OpenGL through Java I'm interested in learning OpenGL and my favorite language at the time is Java. Can I reap its full (or most) benefits using things like JOGL or should I instead focus on getting stronger C++ skills? Btw which is your Java OpenGL wrapper library of choice and why? if you really want to get serious at 3d programming you have to learn C/C++ C++ is the standard for programming 3d games Games aren't the only commercial application of 3d graphics there's plenty of other things kitchen design software for example?  I have done some basic OpenGL development in Delphi and Java as well. I used JOGL as mentioned in others' replies and I must conclude that although there is very little difference in programming OpenGL in Java using JOGL and programming OpenGL in other languages (Delphi C++ etc...) it just doesn't feel right. It was driving me crazy to set it all up correctly and then writing stuff like gl.glBegin (GL.GL_QUADS) there seems to be a lot of superfuous gl GL GLU to be written and it just gets in your way. Also the performance would be I believe much better if you used C++ or similar NOT Java. I am not saying hands off JOGL (Java+OpenGL) it can be done and it really isn't too different but... as I said. Give c++ a try if you can. imho gl GL GLU is not superfluous since they allow to direct translate in Java their C code or tutorial I mean I prefer calling glBegin(GL_QUADS) instead of gl.glBegin(GL.GL_QUADS) :) then use ""import static"" :-) I partially agree. The ""gl"" prefix on the method names is not necessary.  JOGL is a wrapper library that allows OpenGL to be used in the Java programming language. It is currently the reference implementation for JSR-231 (Java Bindings for OpenGL) so it should be your first choice  Do it in C++ the bits you'll be using will basically be the same because there won't be much messing about with pointers so it's not a major leap of intuition. Also the C++ bindings are more reliable and (in my experience) the fastest out of all the langauges available (Python in particular REALLY can't do OpenGL). Also C++ is always a good thing to learn it makes you think about programming concepts in much more detail as opposed to just popping in an ArrayList<> or whatever pre-built class kind-of-serves your purposes =]  If you are also interested in just doing 3D stuff in Java without worrying about all that low-level stuff check out Java3D. Worst case you can look at how they leverage OpenGL for some good learning material.",java c++ opengl
618829,A,openGL glDrawElements with interleaved buffers Thus far i have only used glDrawArrays and would like to move over to using an index buffer and indexed triangles. I am drawing a somewhat complicated object with texture coords normals and vertex coords. All this data is gathered into a single interleaved vertex buffer and drawn using calls similar to ( Assuming all the serup is done correctly ): glVertexPointer( 3 GL_FLOAT 22 (char*)m_vertexData ); glNormalPointer( GL_SHORT 22 (char*)m_vertexData+(12) ); glTexCoordPointer( 2 GL_SHORT 22 (char*)m_vertexData+(18) ); glDrawElements(GL_TRIANGLES m_numTriangles GL_UNSIGNED_SHORT m_indexData ); Does this allow for m_indexData to also be interleaved with the indices of my normals and texture coords as well as the standard position index array? Or does it assume a single linear list of inidices that apply to the entire vertex format ( POS NOR TEX )? If the latter is true how is it possible to render the same vertex with different texture coords or normals? I guess this question could also be rephrased into: if i had 3 seperate indexed lists ( POS NOR TEX ) where the latter 2 cannot be rearranged to share the same index list as the first what is the best way to render that. You cannot have different indexes for the different lists. When you specify glArrayElement(3) then OpenGL is going to take the 3rd element of every list. What you can do is play with the pointer you specify since essentially the place in the list which is eventually accessed is the pointer offset from the start of the list plus the index you specify. This is useful if you have a constant offset between the lists. if the lists are just a random permutation then this kind of play for every vertex is probably going to be as costy as just using plain old glVertex3fv() glNormal3fv() and glTexCoord3fv() Just to confirm the indices used in glDrawElements apply to your entire vertex format every time a vertex occurs on that mesh it must have the same normal and texture coords? i ask because certain 3D apps export models where a given vertex position might have different normal & texture coords If you want the same vertex to appear in a mesh with two different sets normals and texture coords then you'll need to add the vertex twice to arrays. As far as OpenGL is concerned they are two different vertices.  I am having similar trouble attempting to do the same in Direct3D 9.0 For my OpenGL 3 implementation it was rather easy and my source code is available online if it might help you any... https://github.com/RobertBColton/enigma-dev/blob/master/ENIGMAsystem/SHELL/Graphics_Systems/OpenGL3/GL3model.cpp,c++ opengl vertex-buffer
1815513,A,"Problem with a volumetric fog in OpenGL Good day. I am trying to make a volumetric fog in OpenGL using glFogCoordfEXT. Why does a fog affect to all object of my scene even if they're not in fog's volume? And these objects become evenly gray as a fog itself. Here is a pic Code: void CFog::init() { glEnable(GL_FOG); glFogi(GL_FOG_MODE GL_LINEAR); glFogfv(GL_FOG_COLOR this->color); glFogf(GL_FOG_START 0.0f); glFogf(GL_FOG_END 1.0f); glHint(GL_FOG_HINT GL_NICEST); glFogi(GL_FOG_COORDINATE_SOURCE_EXT GL_FOG_COORDINATE_EXT); } void CFog::draw() { glBlendFunc(GL_SRC_ALPHA GL_SRC_ALPHA); glEnable(GL_BLEND); glPushMatrix(); glTranslatef(this->coords[0] this->coords[1] this->coords[2]); if(this->angle[0] != 0.0f) glRotatef(this->angle[0] 1.0f 0.0f 0.0f); if(this->angle[1] != 0.0f) glRotatef(this->angle[1] 0.0f 1.0f 0.0f); if(this->angle[2] != 0.0f) glRotatef(this->angle[2] 0.0f 0.0f 1.0f); glScalef(this->size this->size this->size); GLfloat one = 1.0f; GLfloat zero = 0.0f; glColor4f(0.0 0.0 0.0 0.5); glBegin(GL_QUADS); // Back Wall glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glEnd(); GLenum err; if((err = glGetError()) != GL_NO_ERROR) { char * str = (char *)glGetString(err); } glBegin(GL_QUADS); // Floor glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f-2.5f 15.0f); glEnd(); glBegin(GL_QUADS); // Roof glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f 2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f 2.5f 15.0f); glEnd(); glBegin(GL_QUADS); // Right Wall glFogCoordfEXT( zero); glVertex3f( 2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f 2.5f 15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glEnd(); glBegin(GL_QUADS); // Left Wall glFogCoordfEXT( zero); glVertex3f(-2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f 2.5f 15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glEnd(); glPopMatrix(); glDisable(GL_BLEND); //glDisable(GL_FOG); } Maybe there's no lighting in the scene? If all light sources are disabled all objects are going to appear in a flat ambient color. Check to see what happens if you disable fog altogether. Does this still happen? No lighting is there if I comment fog draw and init in my scene render functon all is fine  You seem to not be clear on how GL_fog_coord_EXT works. You're saying that an object is ""outside the fog volume"" but OpenGL does not have any notion of a fog volume. At any point either Fog is completely off or it's on in which case the fog equation will be applied with a fog coefficient that depends both on the fog mode (LINEAR in your case) and the fog coordinate. Regarding the fog coordinate. when using  glFogi(GL_FOG_COORDINATE_SOURCE_EXT GL_FOG_COORDINATE_EXT); You're telling OpenGL that every time you'll provide a vertex you'll also provide which fog coordinate to use through glFogCoordfEXT So what does it mean in your case ? Assuming you're not calling glFogCoordfEXT in your teapot drawing code you'll end up with the value of your last call to glFogCoordfEXT which looks like a glFogCoordf(one). So everything drawn in that case will be fully in fog which is what you observe. Now I'm not sure exactly what you're trying to achieve so I don't know how to help you solve the issue exactly. However if the goal is to use your quads to mimic fog simply turn fog off when drawing the scene and turn it on only when drawing the cube (I'm pretty sure it won't look like nice fog though). Thank you Bahbar!! this: ""you'll end up with the value of your last call to glFogCoordfEXT which looks like a glFogCoordf(one). "" helped! I just changed a drawing order and everything became ok:))",c++ opengl graphics
648619,A,"Resizing an OpenGL window causes it to fall apart For some reason when I resize my OpenGL windows everything falls apart. The image is distorted the coordinates don't work and everything simply falls apart. I am sing Glut to set it up. //Code to setup glut glutInitWindowSize(appWidth appHeight); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA); glutCreateWindow(""Test Window""); //In drawing function glMatrixMode(GL_MODELVIEW); glLoadIdentity(); glClear(GL_COLOR_BUFFER_BIT); //Resize function void resize(int w int h) { glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } The OpenGL application is strictly 2D. This is how it looks like initially: http://www.picgarage.net/images/Corre_53880_651.jpeg this is how it looks like after resizing: http://www.picgarage.net/images/wrong_53885_268.jpeg The images are no longer available. You should not forget to hook the GLUT 'reshape' event: glutReshapeFunc(resize); And reset your viewport: void resize(int w int h) { glViewport(0 0 width height); //NEW glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } A perspective projection would have to take the new aspect ratio into account: void resizeWindow(int width int height) { double asratio; if (height == 0) height = 1; //to avoid divide-by-zero asratio = width / (double) height; glViewport(0 0 width height); //adjust GL viewport glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(FOV asratio ZMIN ZMAX); //adjust perspective glMatrixMode(GL_MODELVIEW); } And replace the C-style cast with static_cast<> since you're working with C++ :)",c++ opengl graphics glut
1963157,A,OpenGL Video Memory Usage is there an API or profiler application that can track the video memory usage of my application? I am using C++/OpenGL on Windows but I am open to suggestions on other platforms as well. On Mac OS X you have OpenGL Profiler.app which comes with the Developer Tools (on the OS DVD or from http://developer.apple.com) On Windows you could try gDEBugger - a good commercial OpenGL Profiling tool.,c++ memory opengl profiling video-memory
1302025,A,Game engine map editor. SDL->wxWidgets I have been writing an OpenGL game engine for a while which uses SDL for all the window management and for portability. I would like to create a level editor using the full engine. The engine itself isn't at all tied in with SDL except for input. I would like to use wxWidgets for the GUI and I have been looking at some OpenGL samples which are quite simple and easy to understand. Would it be simpler to try and integrate SDL with wxWidgets and use both or switch between them for use in different applications? What would be the best way to switch between the two systems? In all likelihood it would be easier to use one GUI API per application as opposed to merging the two together (i.e. easier to have SDL/OpenGL in your game and wxWidgets/OpenGL in your level editor). Usually these libraries are not built to be merged together or used in conjunction with other libraries so it would be nearly impossible to use them both in one program. For example I don't know much about SDL but a correctly-written wxWidgets program uses internal macros to generate int main() and start up its message pump among other things. If SDL required you to do the same thing (run some special initialization code in your int main() or allow SDL to generate its own int main()) you would be unable to initialize SDL properly without breaking wxWidgets and vice versa. Again I don't know if this particular conflict actually exists between the two libraries but it's just an example of how the two can interact and interfere with each other. That said in a perfect world it would be better to choose one of the libraries and use it both for your engine and level editor (i.e. use SDL/OpenGL for the engine and editor or wxWidgets/OpenGL for the engine and editor) but if you're happy maintaining two different API codebases then if it ain't broke don't fix it. SDL doesn't use a main() macro and can be initialized in parts but I'd guess that the message loops would overlap. (SDL requires you to implement your own loop.) Yeah if not at int main() then at the message loop will there be issues. wxWidgets creates its own native message loop completely hidden from the end-user; modifying it even slightly would easily break the library.,c++ opengl wxwidgets sdl
279358,A,"Invalid lock sequence error in an OpenSceneGraph application I have an application that is built against OpenSceneGraph (2.6.1) and therefore indirectly OpenGL. The application initializes and begins to run but then I get the following exception ""attempt was made to execute an invalid lock sequence"" in OpenGL32.dll. When I re-run it I sometimes get this exception and sometimes an exception about a ""privileged instruction"". The call stack looks like it is corrupted so I can't really tell exactly where the exception is being thrown from. I ran the app quite a bit a couple of days ago and never saw this behavior. Since then I have added an else clause to a couple of ifs and that is all. My app is a console application is built with Visual Studio 2008 and it sets OpenScenGraph to SingleThreaded mode. Anybody seen this before? Any debugging tips? Can you reproduce it with one of the standard examples? Can you create a minimal app that causes this? Do you have a machine with a different brand video card you can test it on (eg Nvidia vs. ATI) there are some issues with openscenegraph and bad OpenGL drivers. Have you tried posting to osg-users@lists.openscenegraph.org  The problem turned out to be our app was picking up an incorrect version of the OpenGL DLL  instead of the one installed in System32.",c++ opengl openscenegraph
1759397,A,"the quake 2 md2 file format (theory) i am trying to load md2 files in opengl but i noticed that most example programs just use a precompiled list of normals. something like this..... //table of precalculated normals { -0.525731f 0.000000f 0.850651f } { -0.442863f 0.238856f 0.864188f } { -0.295242f 0.000000f 0.955423f } { -0.309017f 0.500000f 0.809017f } ... ... Ok this may sound abit dumb but i thought each model is made of different triangles how then is it possible that you can use one set of precompiled normals to render all models? It seems abit strange and any ideas will be appreciated. You could use a precompiled table of normals and use a lookup table to select one that is 'good enough' for a particular case. Each triangle is on a distinct plane and it's that plane that has a normal not the triangle itself. For instance lets imagine we have a point. Expand that point into a sphere for the purposes of this discussion makes it a little easier to grasp conceptually. If you draw a perfect circle around that sphere on the y axis then rotate that circle in the x axis 1 degree each time you'll end up with 360 circles. If you take a normal at 1 degree intervals along each of those circles you'll end up with 360 ** 2 points. From there your normal is the vector from the center of the sphere to that point on the sphere and it is a normal for a plane constructed tangential to point on the sphere. What you end up with if you calculate these two for every point on that sphere is a precalculated table of normals which will almost certainly be good enough for most situations. Now you just need to design a lookup scheme for that data (plane -> normal). Do you happen to have any literature i can go through on this? No sorry this is just off the top of my head and my understanding of the math involved. Most higher level math books that cover 3d geometry should probably cover it though. +1. That's what I've heard. The table covers a finite subset of the unit sphere. So instead of storing normals in .md2 you would store indices to save space (That's what vector quantization is about).  It's been answered already but I want to shed some more light on it. The table contains vectors that cover the unit sphere's surface pretty uniformly. It seems the set of 162 vectors are the corners of a subdivided icosahedron. This is done to lossily compress 3D vectors of unit length to an index (8 bits) see vector quantization. For storing an arbitrary normal vector you can search the table for the closest match and store the index of this match instead. With this table of 162 well distributed vectors the angle between the original vector and the approximated one is expected to be below 11° which seems to be good enough for the Quake2 engine.  The MD2 file format specifies that each vertex has a ""normal index"" and this is a lookup into a well-known table of normals. I would assume that these normals are distributed around a sphere. Presumably the tool that built the model chose the most appropriate of these normals for each vertex. With regard to the first answer: if you want a very faceted model (like a cube) then each polygon does indeed have its own normal and each of the vertices that makes up that polygon should use the same normal vector. However if you want smooth shading (such as a torso) it's common for each vertex in a polygon to have a different normal vector. This allows the lighting to vary across the polygon which is useful in both per-vertex and per-pixel lighting scenarios. Though you will use the polygon plane normal for lighting if you are drawing flat-shaded polygons. If you are talking about vertex normals then you still aren't talking about the polygon itself having a normal. You are still talking about a plane having a normal and that plane is usually taken to be a plane constructed perpendicular to the intended curvature of the shape at the point that the vertex occurs. I was just trying to clarify the point that I think you were making in your first paragraph. When it comes to MD2 models I think it's more useful to consider the tangent plane at each vertex rather than the tangent plane for each triangle (which is the way I read your answer at first). From your comment on my answer I think we're on the same page. I've done enough graphics work to be dangerous but not enough to know all that much. I just happen to be strong enough in the maths to figure out what's going on. The first paragraph was an attempt to answer the question as I percieved it was asked. As an aside the normals of the plane the polygon itself is on isn't useless it's just not used in lighting (it might be used ie in a physics engine though or collision detection or...).",c++ c opengl
187057,A,"glBlendFunc and alpha blending I want to know how the glBlendFunc works. For example i have 2 gl textures where the alpha is on tex1 i want to have alpha in my final image. Where the color is on tex1 i want the color from tex2 to be. Seconding the shaders. If you can use a shader its much easier to just do what you want with the data rather than messing with arcane blending functions.  glBlendFunc applies only to how the final color fragment gets blended with the frame buffer. I think what you want is multitexturing to combine the two textures by blending the texture stages using glTexEnv or using a fragment shader to combine the the two textures. Does this require use of ARB extension? DavidG: If the technique uses ARB_texture_env_combine or shaders it will. Both are widely supported.  Sadly this is for openGL ES on the iPhone so no shaders but point taken. My problem was a very simplified version of the questions i needed to apply a simple color ( incl alpha ) to a part of a defined texture. As Lee pointed out texture blending is to allow alpha to show up on the framebuffer. The solution was to insist that the artist makes the ""action bit"" of the texture white and then assigning a color to the vertices that i render. Something like this. glTexCoordPointer( 2 GL_FLOAT 0 sprite->GetTexBuffer() ); glVertexPointer( 3 GL_FLOAT 0 sprite->GetVertexBuffer() ); glColorPointer( 4 GL_FLOAT 0 sprite->GetColorBuffer() ); glDrawArrays( GL_TRIANGLES 0 6 ); // Draw 2 triangles Where even tho it has a texture having the color means it adds to the texture's color so where it's an alpha it remains alpha and where it is white ( as i had to make it ) it becomes the color of the color pointer at the point. What does ""action bit"" mean? Does what you describe here solve your problem? yeah that solved my problem. ""action bit"" is the part that i want drawn in a different color. basically if i want a single texture ( maybe like a font set ) where each non transparent bit needs a custom color i used this.  Sorry can't do this with simple blending. We for instance used to do the same thing using frament shaders.",c++ opengl
166356,A,"What are some best practices for OpenGL coding (esp. w.r.t. object orientation)? This semester I took a course in computer graphics at my University. At the moment we're starting to get into some of the more advanced stuff like heightmaps averaging normals tesselation etc. I come from an object-oriented background so I'm trying to put everything we do into reusable classes. I've had good success creating a camera class since it depends mostly on the one call to gluLookAt() which is pretty much independent of the rest of the OpenGL state machine. However I'm having some trouble with other aspects. Using objects to represent primitives hasn't really been a success for me. This is because the actual render calls depend on so many external things like the currently bound texture etc. If you suddenly want to change from a surface normal to a vertex normal for a particular class it causes a severe headache. I'm starting to wonder whether OO principles are applicable in OpenGL coding. At the very least I think that I should make my classes less granular. What is the stack overflow community's views on this? What are your best practices for OpenGL coding? if you do want to roll your own the above answers work well enough. A lot of the principles that are mentioned are implemented in most of the open source graphics engines. Scenegraphs are one method to move away from the direct mode opengl drawing. OpenScenegraph is one Open Source app that gives you a large (maybe too large) library of tools for doing OO 3D graphics there are a lot of other out there. This is the approach that Interactive Computer Graphics by Edward Angel takes towards OOP graphics programming. After reading Chapter 10 I'm convinced this is the correct answer. Thanks for the reference I'll definitely check it out later. Unfortunately our lecturers are a little draconian and we can't use 3rd party tools... :( Scenegraphs have their downsides this is highly recommended reading http://home.comcast.net/~tom_forsyth/blog.wiki.html#%5B%5BScene%20Graphs%20-%20just%20say%20no%5D%5D  I usually have a drawOpenGl() function per class that can be rendered that contains it's opengl calls. That function gets called from the renderloop. The class holds all info needed for its opengl function calls eg. about position and orientation so it can do its own transformation. When objects are dependent on eachother eg. they make a part of a bigger object then compose those classes in a other class that represents that object. Which has its own drawOpenGL() function that calls all the drawOpenGL() functions of its children so you can have surrounding position/orientation calls using push- and popmatrix. It has been some time but i guess something similar is possible with textures. If you want to switch between surface normals or vertex normals then let the object remember if its one or the other and have 2 private functions for each occasion that drawOpenGL() calls when needed. There are certainly other more elegant solutions (eg. using the strategy design pattern or something) but this one could work as far as I understand your problem sry for the late answer. yes im thinking: a mesh class consisting of a list of triangle classes and maybe the function for the avarage normals; the triangle class can generate its own normal. The mesh could do the drawing of the triangles. A primitive can be a mesh with a specific form. This is the approach I'm using now (w.r.t. the 2 private functions). It doesn't quite work out for me since each Triangle class depends on 6 other Triangles for normal averaging. Would you also recommend modelling a mesh instead of primitives as my basic class?  The most practical approach seems to be to ignore most of OpenGL functionality that is not directly applicable (or is slow or not hardware accelerated or is a no longer a good match for the hardware). OOP or not to render some scene those are various types and entities that you usually have: Geometry (meshes). Most often this is an array of vertices and array of indices (i.e. three indices per triangle aka ""triangle list""). A vertex can be in some arbitrary format (e.g. only a float3 position; a float3 position + float3 normal; a float3 position + float3 normal + float2 texcoord; and so on and so on). So to define a piece of geometry you need: define it's vertex format (could be a bitmask an enum from a list of formats; ...) have array of vertices with their components interleaved (""interleaved arrays"") have array of triangles. If you're in OOP land you could call this class a Mesh. Materials - things that define how some piece of geometry is rendered. In a simplest case this could be a color of the object for example. Or whether lighting should be applied. Or whether the object should be alpha-blended. Or a texture (or a list of textures) to use. Or a vertex/fragment shader to use. And so on the possibilities are endless. Start by putting things that you need into materials. In OOP land that class could be called (surprise!) a Material. Scene - you have pieces of geometry a collection of materials time to define what is in the scene. In a simple case each object in the scene could be defined by: - What geometry it uses (pointer to Mesh) - How it should be rendered (pointer to Material) - Where it is located. This could be a 4x4 transformation matrix or a 4x3 transformation matrix or a vector (position) quaternion (orientation) and another vector (scale). Let's call this a Node in OOP land. Camera. Well a camera is nothing more than ""where it is placed"" (again a 4x4 or 4x3 matrix or a position and orientation) plus some projection parameters (field of view aspect ratio ...). So basically that's it! You have a scene which is a bunch of Nodes which reference Meshes and Materials and you have a Camera that defines where a viewer is. Now where to put actual OpenGL calls is a design question only. I'd say don't put OpenGL calls into Node or Mesh or Material classes. Instead make something like OpenGLRenderer that can traverse the scene and issue all calls. Or even better make something that traverses the scene independent of OpenGL and put lower level calls into OpenGL dependent class. So yes all of the above is pretty much platform independent. Going this way you'll find that glRotate glTranslate gluLookAt and friends are quite useless. You have all the matrices already just pass them to OpenGL. This is how most of real actual code in real games/applications work anyway. Of course the above can be complicated by more complex requirements. Particularly Materials can be quite complex. Meshes usually need to support lots of different vertex formats (e.g. packed normals for efficiency). Scene Nodes might need to be organized in a hierarchy (this one can be easy - just add parent/children pointers to the node). Skinned meshes and animations in general add complexity. And so on. But the main idea is simple: there is Geometry there are Materials there are objects in the scene. Then some small piece of code is able to render them. In OpenGL case setting up meshes would most likely create/activate/modify VBO objects. Before any node is rendered matrices would need to be set. And setting up Material would touch most of remaining OpenGL state (blending texturing lighting combiners shaders ...). Your idea for the mesh class seems obvious to me now :) What I was trying to do was to use objects for primitives like triangles. Using objects to manage meshes makes a lot more sense as they tend to be pretty self sufficient correct? Also thanks a lot for the insight into the platform independence stuff and the render trees! That helps a lot! I think this is a great answer! ... Anyone starting to write a 3d engine should base it on these principles. (Or even better: use a engine that does all this for you and save valueable time)  Object transformations Avoid depending on OpenGL to do your transformations. Often tutorials teach you how to play with the transformation matrix stack. I would not recommend using this approach since you may need some matrix later that will only be accessible through this stack and using it is very long since the GPU bus is designed to be fast from CPU to GPU but not the other way. Master object A 3D scene is often thought as a tree of objects in order to know object dependencies. There is a debate about what should be at the root of this tree a list of object or a master object. I advice using a master object. While it does not have a graphical representation it will be simpler because you will be able to use recursion more effectively. Decouple scene manager and renderer I disagree with @ejac that you should have a method on each object doing OpenGL calls. Having a separate Renderer class browsing your scene and doing all the OpenGL calls will help you decouple your scene logic and OpenGL code. This is adds some design difficulty but will give you more flexibility if you ever have to change from OpenGL to DirectX or anything else API related. From what I've seen I definitely agree with your last point. I will have to try out the second pattern but my work isn't really complicated enough to warrant platform independence (it's just a couple of throw-away practical excercises).  A standard technique is to insulate the objects' effect on the render state from each other by doing all changes from some default OpenGL state within a glPushAttrib/glPopAttrib scope. In C++ define a class with constructor containing  glPushAttrib(GL_ALL_ATTRIB_BITS); glPushClientAttrib(GL_CLIENT_ALL_ATTRIB_BITS); and destructor containing  glPopClientAttrib(); glPopAttrib(); and use the class RAII-style to wrap any code which messes with the OpenGL state. Provided you follow the pattern each object's render method gets a ""clean slate"" and doesn't need to worry about prodding every possibly modified bit of openGL state to be what it needs. As an optimisation typically you'd set the OpenGL state once at app startup into some state which is as close as possible to what everything wants; this minimisies the number of calls which need to be made within the pushed scopes. The bad news is these aren't cheap calls. I've never really investigated how many per second you can get away with; certainly enough to be useful in complex scenes. The main thing is to try and make the most of states once you've set them. If you've got an army of orcs to render with different shaders textures etc for armour and skin don't iterate over all the orcs rendering armour/skin/armour/skin/...; make sure you set up the state for the armour once and render all the orcs' armour then setup to render all the skin. It's kinda strange to instantiate an object call its render() function and then destroy it just to insulate the state. Am I understanding correctly? Sorry I didn't explain it very well. The object doing push/pop on the GL state is just a convenience helper... nothing to do with the objects you're rendering. Code would look something like: renderable* thing=new...; { gl_pushed_scope p; thing->render(); }",c++ opengl oop
1998251,A,"Basic C++ memory question a friend of mine declared a new type using typedef GLfloat vec3_t[3]; and later used vec3_t to allocate memory vertices=new vec3_t[num_xyz* num_frames]; He freed the memory using delete [] vertices; Question: 1. Since vec3_t is an alias for GLfloat[3] does it mean that vec3_t[num_xyz* num_frames] is equivalent to GLfloat[3][num_xyz* num_frames]; 2. If the above is a 2 dimentional array How is it supporsed to be properly deleted from memory? thanks in advance; from deo GLfloat is an array that is ""statically"" allocated and thus that doesn't need to be explicitly deallocated. From a memory point of view this typedef is equivalent to the following structure: typedef struct { GLfloat f1; GLfloat f2; GLfloat f3; } vec3_t; You can then have the following code which is now less confusing: vec3_t* vertices = new vec3_t [num_xyz* num_frames]; [...] delete[] vertices;  It will be deleted the same way it's been allocated - one contiguous piece of memory. See 2D array memory layout this not entirely correct there is a difference between allocation an array and allocating a single object. that's why you have delete vs delete[] The visualization in the first example of your link is misleading - ttt is defined as an array not as a pointer.  1. a two dimensional array can be thoght of as a one dimensional array where each element is an array. using this definition you can see that new vec3_t[num_xyz* num_frames] is equivalent to a two dimensional array. 2. this array is made of num_xyz* num_frames members each taking a space of sizeof (vec3_t) when new is carried out it allocates num_xyz* num_frames memory blokes in the heap it takes note of this number so that when calling delete[] it will know how many blocks of sizeof (vec3_t) it should mark as free in the heap.  You almost got it right vec3_t[num_xyz* num_frames] is equivalent to GLfloat[num_xyz* num_frames][3] Since you allocated with new[] you have to delete with delete[].  I think that the delete is OK but to reduce confusion I tend to do this: struct vec3_t{ GLFloat elems[3]; }; vec3_t* vertices = new vec3_t[num_xyz* num_frames]; Now you can see the type of vertices and: delete [] vertices; is obviously correct.",c++ opengl
1940787,A,"Which IDE should I use for this art project? I have an art project that will require processing a live video feed to use as the basis of a particle system which will be rendered using OpenGL and projected on a stage. I have a CUDA enabled graphics card and I was thinking it would be nice to be able to use that for the image and particle system processing. This project only needs to run on my computer. I am normally a C# asp.net Visual Studio kinda guy but for this project I plan on using c++. Should I do the work in Eclipse on Ubuntu or Visual Studio in Windows? I realize this can be fairly arbitrary but I wondering if one IDE/OS might be better suited for this kind of work than the other As far as the CUDA or OpenGL support is concerned you are fine with either of them. The nVidia examples are also multiplatform. The real question is if you plan on using any GUI Toolkit as there are a only a few choices that are really portable. In the end I'd recommend going with what you feel more comfortable with or where you will have the biggest knowledge gain (if learning something is a goal of the project.).  Are you aware of OpenFrameworks? This might just help shortcut to what you need. No I hadnt heard of it but I am gonna check it out. Dude. OpenFrameworks looks awesome! That is definitely in the same vein as this project I am working on. Thanks for the link Given it's a 'C++ toolkit for creative coding' would be good to know what you think.  +1 for Visual Studio. I haven't heard about any IDE especially good for such tasks. If you already know VS I see no reason to learn anything else.  Since you're already familiar with Visual Studio you should probably stick with it. In addition you'll be able to use the Nexus debugger to debug both the OpenGL and CUDA components.  While the CUDA toolkit is cross-platform i recommend Linux in this case: The debugger is based on gdb and the usability of the gcc toolchain is just much better on *nixes. You also don't seem to have any windows specific dependencies. What do you mean about the usability of the gcc toolchain? On windows you either use gcc-tools via a graphical interface i.e. have to figure out their way to pass flags etc. to the gcc tools or use them on the commandline which ain't fun on windows (using cygwin doesn't help with everything). What do you mean by ""the usability of the gcc toolchain is just much better on *nixes""? What is ""*nixes""? I assume linux/unix based OS's Thats what i meant.",c++ visual-studio-2008 eclipse opengl cuda
1080635,A,"Other's library #define naming conflict Hard to come up with a proper title for this problem. Anyway... I'm currently working on a GUI for my games in SDL. I've finished the software drawing and was on my way to start on the OpenGL part of it when a weird error came up. I included the ""SDL/SDL_opengl.h"" header and compile. It throws ""error C2039: 'DrawTextW' : is not a member of 'GameLib::FontHandler'"" which is a simple enough error but I don't have anything called DrawTextW only FontHandler::DrawText. I search for DrawTextW and find it in a #define call in the header ""WinUser.h""! //WinUser.h #define DrawText DrawTextW Apparently it replaces my DrawText with DrawTextW! How can I stop it from spilling over into my code like that? It's a minor thing changing my own function's name but naming conflicts like this seem pretty dangerous and I would really like to know how to avoid them all together. Cheers! Just #undef the symbols you don't want. But Make sure that you include windows.h and do this before you include SDL: #include <windows.h> #undef DrawText #include <SDL/SDL_opengl.h>  You have a couple of options all of which suck. Add #undef DrawText in your own code Don't include windows.h. If another library includes it for you don't include that directly. Instead include it in a separate .cpp file which can then expose your own wrapper functions in its header. Rename your own DrawText. When possible I usually go for the middle option. windows.h behaves badly in countless other ways (for example it doesn't actually compile unless you enable Microsoft's proprietary C++ extensions) so I simply avoid it like the plague. It doesn't get included in my files if I can help it. Instead I write a separate .cpp file to contain it and expose the functionality I need. Also feel free to submit it as a bug and/or feedback on connect.microsoft.com. Windows.h is a criminally badly designed header and if people draw Microsoft's attention to it there's a (slim) chance that they might one day fix it. The good news is that windows.h is the only header that behaves this badly. Other headers generally try to prefix their macros with some library-specific name to avoid name collisions they try to avoid creating macros for common names and they try avoid using more macros than necessary. Yeah Windows.h does collide. I guess it is fair to complain it doesn't compile w/o ms extensions enabled but it is also reasonable that an ms-specific header should (require those extensions).  It's an unfortunate side effect of #includeing <windows.h>. Assuming you're not actually using Windows' DrawText() anywhere in your program it's perfectly safe to #undef it immediately after: // wherever you #include <windows.h> or any other windows header #include <windows.h> #undef DrawText It could. But that would require Microsoft to care. Thanks! It compiles fine now. Quite an annoying side effect indeed. Seems as if the SDL_opengl.h includes the windows.h but the #undef does it's job. Could the windows.h have been written differently to avoid this problem?  There is no general way of avoiding this problem - once you #include a header file using the preprocessor it can redefine any name it likes and there is nothing you can do about it. You can #undef the name but that assumes you know the name was #defined in the first place.",c++ opengl sdl
1799070,A,"What might cause OpenGL to behave differently under the ""Start Debugging"" versus ""Start without debugging"" options? I have written a 3D-Stereo OpenGL program in C++. I keep track of the position objects in my display should have using timeGetTime after a timeBeginPeriod(1). When I run the program with ""Start Debugging"" my objects move smoothly across the display (as they should). When I run the program with ""Start without debugging"" the objects occationally freeze for several screen refreshes then jump to a new position. Any ideas as to what may be causing this problem and how to fix it? Edit: It seems like the jerkiness can be resolved after a short delay when I run through ""Start without debugging"" if I click the mouse button. My application is a console application (I take in some parameters when the program first starts). Might there be a difference in window focus between these two options? Is there an explicit way to force the focus to the OpenGL window (in full screen through glutFullScreen();) when I'm done taking input from the console window? Thanks. How does your release build perform and what is your overall CPU usage like? Since I work almost entirely in a development enviroment and CPU usage shouldn't be an issue with the simple displays I render I've never built a release build. My recent attempt to do so was thwarted by a screen capture library that I have been using that refuses to link correctly. CPU utilazation on the core running the process is high but seldom pegged. The most common thing that causes any program to behave differently while being debugged and not being debugged is using uninitialized variables and especially reading uninitialized memory. Check that you're not doing that. Something more OpenGL specific - You might have some problems with flushing of commands. Try inserting glFinish() after drawing every frame. It might also be helpful to somehow really make sure that when the freeze occurs there are actually frames being rendered and not that the whole application is frozen. If there are its more likely that you have some bug in the logic since it seems that OpenGL does its job. Thank you for the advice. Any suggestion on how to determine whether frames are still being rendered during the apparent freeze? I had tried to check that before but found that the slow down due to writing a file was obscuring whatever it was that was going on. As this is a stereo display @ 120Hz I have about 8.33ms to get through my entire display loop. Usually a debug build will initialize memory in exactly the same way whether run through a debugger or not. On windows you would still be using the debug runtime libraries. It can make a big difference when moving to a release build.  The timeGetTime API only has a precision of something like 10ms. If the intervals you're measuring are less than 50ms or so you may simply be seeing the effects of the expected variance in the system timer. I have no idea why the debugger would have an effect on this but then the whole workings of the system are a black box. You could use the QueryPerformanceCounter to get higher-resolution timings which may help. Thanks I'll take a look at that. I had thought that timeBeginPeriod(1) was accurately setting the resolution down to 1ms. If the resolution is @ 10ms that could definately cause some problems for my code. @drknexus The documentation for timeGetTime and timeBeginPeriod implies that it's 1ms but it seems to be highly dependent on what hardware is available. I suppose it's possible that ""modern"" hardware has fixed this it's been a few years since I played with it. This is where I remember the 10ms value from but it's also a few years old: http://support.microsoft.com/kb/172338",c++ windows opengl timer stereo-3d
1672926,A,"Setting up a camera in OpenGL I've been working on a game engine for awhile. I've started out with 2D Graphics with just SDL but I've slowly been moving towards 3D capabilities by using OpenGL. Most of the documentation I've seen about ""how to get things done"" use GLUT which I am not using. The question is how do I create a ""camera"" in OpenGL that I could move around a 3D environment and properly display 3D models as well as sprites (for example a sprite that has a fixed position and rotation). What functions should I be concerned with in order to setup a camera in OpenGL camera and in what order should they be called in? Here is some background information leading up to why I want an actual camera. To draw a simple sprite I create a GL texture from an SDL surface and I draw it onto the screen at the coordinates of (SpriteX-CameraX) and (SpriteY-CameraY). This works fine but when moving towards actual 3D models it doesn't work quite right. The cameras location is a custom vector class (i.e. not using the standard libraries for it) with X Y Z integer components. I have a 3D cube made up of triangles and by itself I can draw it and rotate it and I can actually move the cube around (although in an awkward way) by passing in the camera location when I draw the model and using that components of the location vector to calculate the models position. Problems become evident with this approach when I go to rotate the model though. The origin of the model isn't the model itself but seems to be the origin of the screen. Some googling tells me I need to save the location of the model rotate it about the origin then restore the model to its origal location. Instead of passing in the location of my camera and calculating where things should be being drawn in the Viewport by calculating new vertices I figured I would create an OpenGL ""camera"" to do this for me so all I would need to do is pass in the coordinates of my Camera object into the OpenGL camera and it would translate the view automatically. This tasks seems to be extremely easy if you use GLUT but I'm not sure how to set up a camera using just OpenGL. EDIT #1 (after some comments): Following some suggestion here is the update method that gets called throughout my program. Its been updated to create perspective and view matrices. All drawing happens before this is called. And a similar set of methods is executed when OpenGL executes (minus the buffer swap). The xyz coordinates are straight an instance of Camera and its location vector. If the camera was at (256 32 0) then 256 32 and 0 would be passed into the Update method. Currently z is set to 0 as there is no way to change that value at the moment. The 3D model being drawn is a set of vertices/triangles + normals at location X=320 Y=240 Z=-128. When the program is run this is what is drawn in FILL mode and then in LINE mode and another one in FILL after movement when I move the camera a little bit to the right. It likes like may Normals may be the cause but I think it has moreso to do with me missing something extremely important or not completely understanding what the NEAR and FAR parameters for glFrustum actually do. Before I implemented these changes I was using glOrtho and the cube rendered correctly. Now if I switch back to glOrtho one face renders (Green) and the rotation is quite weird - probably due to the translation. The cube has 6 different colors one for each side. Red Blue Green Cyan White and Purple. int VideoWindow::Update(double x double y double z) { glMatrixMode( GL_PROJECTION ); glLoadIdentity(); glFrustum(0.0f GetWidth() GetHeight() 0.0f 32.0f 192.0f); glMatrixMode( GL_MODELVIEW ); SDL_GL_SwapBuffers(); glLoadIdentity(); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glRotatef(0 1.0f 0.0f 0.0f); glRotatef(0 0.0f 1.0f 0.0f); glRotatef(0 0.0f 0.0f 1.0f); glTranslated(-x -y 0); return 0; } EDIT FINAL: The problem turned out to be an issue with the Near and Far arguments of glFrustum and the Z value of glTranslated. While change the values has fixed it I'll probably have to learn more about the relationship between the two functions. If you have not checked then may be looking at following project would explain in detail what ""tsalter"" wrote in his post. Camera from OGL SDK (CodeColony) Also look at Red book for explanation on viewing and how does model-view and projection matrix will help you create camera. It starts with good comparison between actual camera and what corresponds to OpenGL API. Chapter 3 - Viewing  You have to do it using the matrix stack as for object hierarchy but the camera is inside the hierarchy so you have to put the inverse transform on the stack before drawing the objects as openGL only uses the matrix from 3D to camera.  Just remember that OpenGl post multiplies.  You need a view matrix and a projection matrix. You can do it one of two ways: Load the matrix yourself using glMatrixMode() and glLoadMatrixf() after you use your own library to calculate the matrices. Use combinations of glMatrixMode(GL_MODELVIEW) and glTranslate() / glRotate() to create your view matrix and glMatrixMode(GL_PROJECTION) with glFrustum() to create your projection matrix. Remember - your view matrix is the negative translation of your camera's position (As it's where you should move the world to relative to the camera origin) as well as any rotations applied (pitch/yaw). Hope this helps if I had more time I'd write you a proper example! Thanks for the push in the right direction. I've made an edit in the post if you could take a look at my code and screenshots. Something is not quite right :) Actually the issue really is with the values I pick for the Near and Far arguments to glFrustum as well as the Z value passed into glTranslated. Thanks.",c++ opengl sdl
724060,A,"How do I render text on to a square (4 vertices) in OpenGL? I'm using Linux and GLUT. I have a square as follows: glVertex3f(-1.0 +1.0 0.0); // top left glVertex3f(-1.0 -1.0 0.0); // bottom left glVertex3f(+1.0 -1.0 0.0); // bottom right glVertex3f(+1.0 +1.0 0.0); // top right I guess I can't use glutBitmapCharacter since this is only ideal for 2D ortho. Simple enough I'd like to render ""Hello world!"" anywhere on said square. Should I create a texture and then apply it to the vertices using glTexCoord2f? The simplest way is to load a font map from a image such as those generated by the bitmap font builder (I know it's windows but I can't find one for linux) eg: The example is a 256x256 gif though you may what to convert it to a png/tga/bmp. It's full ASCII mapped gird 16x16 characters. Load the texture and use glTexCoord2f to line it up on your quad and you should be good to go. Here's an example using a bitmap of the above: unsigned texture = 0; void LoadTexture() { // load 24-bit bitmap texture unsigned offset width height size; char *buffer; FILE *file = fopen(""text.bmp"" ""rb""); if (file == NULL) return; fseek(file 10 SEEK_SET); fread(&offset 4 1 file); fseek(file 18 SEEK_SET); fread(&width 1 4 file); fread(&height 1 4 file); size = width * height * 3; buffer = new char[size]; fseek(file offset SEEK_SET); fread(buffer 1 size file); glEnable(GL_TEXTURE_2D); glGenTextures(1 &texture); glBindTexture(GL_TEXTURE_2D texture); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); glTexImage2D(GL_TEXTURE_2D 0 GL_RGB width height 0 GL_RGB GL_UNSIGNED_BYTE buffer); fclose(file); printf(""Loaded\n""); } void DrawCharacter(char c) { int column = c % 16 row = c / 16; float x y inc = 1.f / 16.f; x = column * inc; y = 1 - (row * inc) - inc; glBegin(GL_QUADS); glTexCoord2f( x y); glVertex3f( 0.f 0.f 0.f); glTexCoord2f( x y + inc); glVertex3f( 0.f 1.f 0.f); glTexCoord2f( x + inc y + inc); glVertex3f( 1.f 1.f 0.f); glTexCoord2f( x + inc y); glVertex3f( 1.f 0.f 0.f); glEnd(); } Very helpful thanks.  Indeed rendering to a bitmap is a solution. There's a decent tutorial on how to do it here on GameDev.",c++ opengl textures
1726122,A,"SDL_image/C++ OpenGL Program: IMG_Load() produces fuzzy images I'm trying to load an image file and use it as a texture for a cube. I'm using SDL_image to do that. I used this image because I've found it in various file formats (tga tif jpg png bmp) The code : SDL_Surface * texture; //load an image to an SDL surface (i.e. a buffer) texture = IMG_Load(""/Users/Foo/Code/xcode/test/lena.bmp""); if(texture == NULL){ printf(""bad image\n""); exit(1); } //create an OpenGL texture object glGenTextures(1 &textureObjOpenGLlogo); //select the texture object you need glBindTexture(GL_TEXTURE_2D textureObjOpenGLlogo); //define the parameters of that texture object //how the texture should wrap in s direction glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_REPEAT); //how the texture should wrap in t direction glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_REPEAT); //how the texture lookup should be interpolated when the face is smaller than the texture glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); //how the texture lookup should be interpolated when the face is bigger than the texture glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); //send the texture image to the graphic card glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA texture->w texture->h 0 GL_RGB GL_UNSIGNED_BYTE texture-> pixels); //clean the SDL surface SDL_FreeSurface(texture); The code compiles without errors or warnings ! I've tired all the files formats but this always produces that ugly result : I'm using : SDL_image 1.2.9 & SDL 1.2.14 with XCode 3.2 under 10.6.2 Does anyone knows how to fix this ? As a note: The reason you find that image in so many formats is because it's the standard image for image compression tests. ""Lenna"" is the name of the image and the model's name is Lena Soderberg. The history of the image is quite interesting. The reason the image is distorted is because it's not in the RGBA format that you've specified. Check the texture->format to find out the format it's in and select the appropriate GL_ constant that represents the format. (Or transform it yourself to the format of your choice.) +1 beat me to it  I think greyfade has the right answer but another thing you should be aware of is the need to lock surfaces. This is probably not the case since you're working with an in-memory surface but normally you need to lock surfaces before accessing their pixel data with SDL_LockSurface(). For example: bool lock = SDL_MUSTLOCK(texture); if(lock) SDL_LockSurface(texture); // should check that return value == 0 // access pixel data e.g. call glTexImage2D if(lock) SDL_UnlockSUrface(texture);  If you have an alpha chanel every pixel is 4 unsigned bytes if you don't it's 3 unsigned bytes. This image has no transpareny and when I try to save it its a .jpg. change glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA texture->w texture->h 0 GL_RGB GL_UNSIGNED_BYTE texture-> pixels); to glTexImage2D(GL_TEXTURE_2D 0 GL_RGB texture->w texture->h 0 GL_RGB GL_UNSIGNED_BYTE texture-> pixels); That should fix it. For a .png with an alpha channel use glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA texture->w texture->h 0 GL_RGBA GL_UNSIGNED_BYTE texture-> pixels); this is actually wrong. the parameter you're modifying is used to tell GL what format to _store_ the texture as. storing to RGBA when your input is RGB is perfectly valid it sets A=1 for all texels.",c++ opengl sdl sdl-image
1558505,A,"C++ and OpenGL Help I am wanting to code something with OpenGL but I don't want to have to go through Windows API is this Possible? If so then some links to tutorials on how to do this would be nice. Just keep in mind that graphics processing is very tied to hardware and platform. Anything you use is going to abstract that away from you (perhaps by going through the Windows API when on Windows or another API on other OS's). I reconmend Freeglut its has some nice revisions and is more up to date in fact someone made some updates to this year.  In order to create a Win32 window for displaying OpenGL content - without going through Win32 API GLUT is the only option that I'm aware of.  If you need a complete visualization framework then VTK is the best FREE choice.  Yes. GLFW Qt + OpenGL GLUT or FreeGLUT Or see my question. I'd also add SFML http://www.sfml-dev.org/ SDL is far too slow. Thanks for your help! :) Should probably add SDL to your list. http://www.libsdl.org/ @Anthoni: That's why I didn't recommend it :) I'm personally not very fond of it. Anywho see the ""my question"" link...that lists all of the one's I'm aware of.",c++ opengl
724156,A,Keyboard input hesitation when held down? Does anyone know why there is some hesitation when you hold down a keyboard key and try to process it? I'm calling a function right in my WinProc(...) that will move an image on the screen (OpenGL) when a key is held down. I press it and get a single response then there is about .5 seconds of nothing then it behaves as normal (moves 1 pixel every WinMain loop). I'm wondering if the Windows messages are being delayed somehow because of some feature I need to disable??? Here's my code: int WINAPI WinMain(HINSTANCE hinstance HINSTANCE hprevinstance LPSTR lpcmdline int nshowcmd) { bool quit = false; MSG msg; createWindow(hinstance SCRW SCRH SCRD WINDOWED); // Main loop while (!quit) { if (PeekMessage(&msg NULL NULL NULL PM_REMOVE)) { if (msg.message == WM_QUIT) quit = true; TranslateMessage(&msg); DispatchMessage(&msg); } renderFrame(); // processes graphics SwapBuffers(hdc); } return msg.lParam; } and WinProc (there were more cases but same thing...):  LRESULT CALLBACK WinProc(HWND hwnd UINT msg WPARAM wparam LPARAM lparam) { switch(msg) { case WM_KEYDOWN: switch ( wparam ) { case VK_RIGHT: key_RIGHT(); return 0; } return 0; } } and key_RIGHT: void key_RIGHT() { MoveObjectRight1Pixel(); } It's a rather standard keyboard setting to have a small delay between when the key was pressed and when repeat messages get generated. Instead of processing keyboard input in your Windows message handler you could instead keep an array of 256 bits indicating the current state of the keyboard. When you receive a WM_KEYDOWN or WM_KEYUP you update the bit of the corresponding key. Then in your main loop you check the current state of the key and the previous state of the key (by keeping a second array of 256 bits that you make a copy to every frame). If the key is currently down but was not down in the previous frame then you move your object accordingly. Another alternative is to use the GetAsyncKeyState() function. I was afraid I would have to do some silly workaround... There's no way to force Windows to send me messages at a civilized rate? That's a huge keyboard. :) Most standard PC keyboards are around 105 keys so 128 bits sounds like plenty to me. @unwind: True most keyboards have a sane number of keys but the virtual keycodes (VK_* constants) range from 0 to 255.  Well I think you're dealing with a standard windows feature here. Windows usually has a delay before it fires the repititive keystrokes events when thekey is suppressed. I don't know about other methods but on the first key press you could activate a timer which would check whether the key is suppressed every few milliseconds and then process your code accordingly. I think that would solve the problem you're having.,c++ windows opengl input messages
1020924,A,"Build errors w/ GLee (GL Easy Extension Library) Using Code::Blocks w/ mingw and trying to use GLee for some OpenGL on windows. I'm getting the following build errors: GLee.c|60|undefined reference to `_wglGetProcAddress@4' GLee.c|10748|undefined reference to `_wglGetProcAddress@4' GLee.c|10751|undefined reference to `_wglGetCurrentDC@0' GLee.c|10797|undefined reference to `_glGetString@4' GLee.c|10910|undefined reference to `_glGetString@4' GLee.c|10976|undefined reference to `_glGetString@4' And I'm just including GLee likes so (with GLee.c not the .dll): #include ""GLee.h"" According to Ben Woodhouse GLee is ""written in pure ANSI C so any C or C++ compiler should work. You can include the source files in your projects directly or compile them into a library"" so I should be having no problems. Google didn't give me much on this so I'm hoping some OpenGL vets (or anyone familiar with GLee) out there can point me in the right direction. It looks like you need to link your application against the OpenGL libraries (specifically Opengl32.lib) which will provide the functions that you are missing. Perhaps the OpenGL FAQ might be of help in figuring this out.",c++ c opengl
536168,A,"Why won't my OpenGL draw anything? I'm working on porting my open source particle engine test from SDL to SDL + OpenGL. I've managed to get it compiling and running but the screen stays black no matter what I do. main.cpp: #include ""glengine.h"" int WINAPI WinMain( HINSTANCE hInstance HINSTANCE hPrevInstance LPSTR lpCmdLine int nCmdShow ) { //Create a glengine instance ultragl::glengine *gle = new ultragl::glengine(); if(gle->init()) gle->run(); else std::cout << ""glengine initializiation failed!"" << std::endl; //If we can't initialize or the lesson has quit we delete the instance delete gle; return 0; }; glengine.h: //we need to include window first because GLee needs to be included before GL.h #include ""window.h"" #include <math.h> // Math Library Header File #include <vector> #include <stdio.h> using namespace std; namespace ultragl { class glengine { protected: window m_Window; ///< The window for this lesson unsigned int m_Keys[SDLK_LAST]; ///< Stores keys that are pressed float piover180; virtual void draw(); virtual void resize(int x int y); virtual bool processEvents(); void controls(); private: /* * We need a structure to store our vertices in otherwise we * just had a huge bunch of floats in the end */ struct Vertex { float x y z; Vertex(){} Vertex(float x float y float z) { this->x = x; this->y = y; this->z = z; } }; struct particle { public : double angle; double speed; Vertex v; int r; int g; int b; int a; particle(double angle double speed Vertex v int r int g int b int a) { this->angle = angle; this->speed = speed; this->v = v; this->r = r; this->g = g; this->b = b; this->a = a; } particle() { } }; particle p[500]; float particlesize; public: glengine(); ~glengine(); virtual void run(); virtual bool init(); void glengine::test2(int num); void glengine::update(); }; }; window.h: #include <string> #include <iostream> #include ""GLee/GLee.h"" #include <SDL/SDL.h> #include <SDL/SDL_opengl.h> #include <GL/glu.h> using namespace std; namespace ultragl { class window { private: int w_height; int w_width; int w_bpp; bool w_fullscreen; string w_title; public: window(); ~window(); bool createWindow(int width int height int bpp bool fullscreen const string& title); void setSize(int width int height); int getHeight(); int getWidth(); }; }; glengine.cpp (the main one to look at): #include ""glengine.h"" namespace ultragl{ glengine::glengine() { piover180 = 0.0174532925f; } glengine::~glengine() { } void glengine::resize(int x int y) { std::cout << ""Resizing Window to "" << x << ""x"" << y << std::endl; if (y <= 0) { y = 1; } glViewport(00xy); glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(45.0f(GLfloat)x/(GLfloat)y1.0f100.0f); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } bool glengine::processEvents() { SDL_Event event; while (SDL_PollEvent(&event))//get all events { switch (event.type) { // Quit event case SDL_QUIT: { // Return false because we are quitting. return false; } case SDL_KEYDOWN: { SDLKey sym = event.key.keysym.sym; if(sym == SDLK_ESCAPE) //Quit if escape was pressed { return false; } m_Keys[sym] = 1; break; } case SDL_KEYUP: { SDLKey sym = event.key.keysym.sym; m_Keys[sym] = 0; break; } case SDL_VIDEORESIZE: { //the window has been resized so we need to set up our viewport and projection according to the new size resize(event.resize.w event.resize.h); break; } // Default case default: { break; } } } return true; } bool glengine::init() { srand( time( NULL ) ); for(int i = 0; i < 500; i++) p[i] = particle(0 0 Vertex(0.0f 0.0f 0.0f) 0 0 0 0); if (!m_Window.createWindow(640 480 32 false ""Paricle Test GL"")) { return false; } particlesize = 0.01; glShadeModel(GL_SMOOTH); // Enable Smooth Shading glClearColor(0.0f 0.0f 0.0f 0.5f); // Black Background glClearDepth(1.0f); // Depth Buffer Setup glEnable(GL_DEPTH_TEST); // Enables Depth Testing glDepthFunc(GL_LEQUAL); // The Type Of Depth Testing To Do glEnable(GL_BLEND); glBlendFunc(GL_ONE  GL_ONE_MINUS_SRC_ALPHA); return true; } void glengine::test2(int num) { glPushMatrix(); glTranslatef(p[num].v.x p[num].v.y p[num].v.z); glBegin(GL_QUADS); glColor4i(p[num].r p[num].g p[num].b p[num].a); // Green for x axis glVertex3f(-particlesize -particlesize particlesize); glVertex3f( particlesize -particlesize particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f(-particlesize particlesize particlesize); glEnd(); glPopMatrix(); } void glengine::draw() { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Clear Screen And Depth Buffer glLoadIdentity(); // Reset The Current Modelview Matrix gluLookAt(0 5 20 0 0 0 0 0 0); for(int i = 0; i < 500; i++) test2(i); } void glengine::update() { for(int i = 0; i < 500; i++) { if(p[i].a <= 0) p[i] = particle(5 + rand() % 360 (rand() % 10) * 0.1 Vertex(0.0f 0.0f 0.0f) 0 255 255 255); else p[i].a -= 1; p[i].v.x += (sin(p[i].angle * (3.14159265/180)) * p[i].speed); p[i].v.y -= (cos(p[i].angle * (3.14159265/180)) * p[i].speed); } } void glengine::run() { while(processEvents()) { update(); draw(); SDL_GL_SwapBuffers(); } } }; And finally window.cpp: #include ""window.h"" namespace ultragl { window::window(): w_width(0) w_height(0) w_bpp(0) w_fullscreen(false) { } window::~window() { SDL_Quit(); } bool window::createWindow(int width int height int bpp bool fullscreen const string& title) { if( SDL_Init( SDL_INIT_VIDEO ) != 0 ) return false; w_height = height; w_width = width; w_title = title; w_fullscreen = fullscreen; w_bpp = bpp; //Set lowest possiable values. SDL_GL_SetAttribute(SDL_GL_RED_SIZE 5); SDL_GL_SetAttribute(SDL_GL_GREEN_SIZE 5); SDL_GL_SetAttribute(SDL_GL_BLUE_SIZE 5); SDL_GL_SetAttribute(SDL_GL_ALPHA_SIZE 5); SDL_GL_SetAttribute(SDL_GL_DEPTH_SIZE 16); SDL_GL_SetAttribute(SDL_GL_DOUBLEBUFFER 1); //Set title. SDL_WM_SetCaption(title.c_str() title.c_str()); // Flags tell SDL about the type of window we are creating. int flags = SDL_OPENGL; if(fullscreen == true) flags |= SDL_FULLSCREEN; // Create window SDL_Surface * screen = SDL_SetVideoMode( width height bpp flags ); if(screen == 0) return false; //SDL doesn't trigger off a ResizeEvent at startup but as we need this for OpenGL we do this ourself SDL_Event resizeEvent; resizeEvent.type = SDL_VIDEORESIZE; resizeEvent.resize.w = width; resizeEvent.resize.h = height; SDL_PushEvent(&resizeEvent); return true; } void window::setSize(int width int height) { w_height = height; w_width = width; } int window::getHeight() { return w_height; } int window::getWidth() { return w_width; } }; Anyway I really need to finish this but I've already tried everything I could think of. I tested the glengine file many different ways to where it looked like this at one point: #include ""glengine.h"" #include ""SOIL/SOIL.h"" #include ""SOIL/stb_image_aug.h"" #include ""SOIL/image_helper.h"" #include ""SOIL/image_DXT.h"" namespace ultragl{ glengine::glengine() { piover180 = 0.0174532925f; } glengine::~glengine() { } void glengine::resize(int x int y) { std::cout << ""Resizing Window to "" << x << ""x"" << y << std::endl; if (y <= 0) { y = 1; } glViewport(00xy); glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(45.0f(GLfloat)x/(GLfloat)y1.0f1000.0f); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } bool glengine::processEvents() { SDL_Event event; while (SDL_PollEvent(&event))//get all events { switch (event.type) { // Quit event case SDL_QUIT: { // Return false because we are quitting. return false; } case SDL_KEYDOWN: { SDLKey sym = event.key.keysym.sym; if(sym == SDLK_ESCAPE) //Quit if escape was pressed { return false; } m_Keys[sym] = 1; break; } case SDL_KEYUP: { SDLKey sym = event.key.keysym.sym; m_Keys[sym] = 0; break; } case SDL_VIDEORESIZE: { //the window has been resized so we need to set up our viewport and projection according to the new size resize(event.resize.w event.resize.h); break; } // Default case default: { break; } } } return true; } bool glengine::init() { srand( time( NULL ) ); for(int i = 0; i < 500; i++) p[i] = particle(0 0 Vertex(0.0f 0.0f 0.0f) 0 0 0 0); if (!m_Window.createWindow(640 480 32 false ""Paricle Test GL"")) { return false; } particlesize = 10.01; glShadeModel(GL_SMOOTH); // Enable Smooth Shading glClearColor(0.0f 0.0f 0.0f 0.5f); // Black Background glClearDepth(1.0f); // Depth Buffer Setup glEnable(GL_DEPTH_TEST); // Enables Depth Testing glDepthFunc(GL_LEQUAL); // The Type Of Depth Testing To Do glEnable(GL_BLEND); glBlendFunc(GL_ONE  GL_ONE_MINUS_SRC_ALPHA); return true; } void glengine::test2(int num) { //glPushMatrix(); //glTranslatef(p[num].v.x p[num].v.y p[num].v.z); glColor4i(255 255 255 255); glBegin(GL_QUADS); glNormal3f( 0.0f 0.0f 1.0f); glVertex3f(-particlesize -particlesize particlesize); glVertex3f( particlesize -particlesize particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f(-particlesize particlesize particlesize); glEnd(); // Back Face glBegin(GL_QUADS); glNormal3f( 0.0f 0.0f-1.0f); glVertex3f(-particlesize -particlesize -particlesize); glVertex3f(-particlesize particlesize -particlesize); glVertex3f( particlesize particlesize -particlesize); glVertex3f( particlesize -particlesize -particlesize); glEnd(); // Top Face glBegin(GL_QUADS); glNormal3f( 0.0f 1.0f 0.0f); glVertex3f(-particlesize particlesize -particlesize); glVertex3f(-particlesize particlesize particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f( particlesize particlesize -particlesize); glEnd(); // Bottom Face glBegin(GL_QUADS); glNormal3f( 0.0f-1.0f 0.0f); glVertex3f(-particlesize -particlesize -particlesize); glVertex3f( particlesize -particlesize -particlesize); glVertex3f( particlesize -particlesize particlesize); glVertex3f(-particlesize -particlesize particlesize); glEnd(); // Right face glBegin(GL_QUADS); glNormal3f( 1.0f 0.0f 0.0f); glVertex3f( particlesize -particlesize -particlesize); glVertex3f( particlesize particlesize -particlesize); glVertex3f( particlesize particlesize particlesize); glVertex3f( particlesize -particlesize particlesize); glEnd(); // Left Face glBegin(GL_QUADS); glNormal3f(-1.0f 0.0f 0.0f); glVertex3f(-particlesize -particlesize -particlesize); glVertex3f(-particlesize -particlesize particlesize); glVertex3f(-particlesize particlesize particlesize); glVertex3f(-particlesize particlesize -particlesize); glEnd(); //glPopMatrix(); } void glengine::draw() { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Clear Screen And Depth Buffer glLoadIdentity(); // Reset The Current Modelview Matrix gluLookAt(0 5 20 0 0 0 0 1 0); for(int i = 0; i < 500; i++) test2(i); } void glengine::update() { for(int i = 0; i < 500; i++) { if(p[i].a <= 0) p[i] = particle(5 + rand() % 360 (rand() % 10) * 0.1 Vertex(0.0f 0.0f -5.0f) 0 255 255 255); else p[i].a -= 1; p[i].v.x += (sin(p[i].angle * (3.14159265/180)) * p[i].speed); p[i].v.y -= (cos(p[i].angle * (3.14159265/180)) * p[i].speed); } } void glengine::run() { while(processEvents()) { update(); draw(); SDL_GL_SwapBuffers(); } } }; It still didn't work. I'm really at my wits end on this one. I find you get better responses if you can reduce your source code to the absolute minimum example while still having the same issue. People don't want to do the work looking through 4 pages of source when you could reduce the problem to a 1 page simple example. Reduce your program to as simple a drawing program as possible. Setup your OpenGL window viewport etc and attempt to draw a single line. If it doesn't show up post that code here. Then when people see the issue with that you can apply the fix to your bigger problem. I haven't checked your code but one thing I always do when debugging this kind of problems is to set the clear color to something colorful like (1 0 1) or so. This will help you see if the problem is that your drawn object is completely black or if it's not drawn at all. EDIT: As someone mentioned in the comment: It also shows if you have a correct GL context if the clear operation clears to the right color or if it stays black. And to see if you've created a proper context where at least glClear works.  You're not checking the return values of the SDL-GL-SetAttribute() calls. And is 5/5/5/5 20-bpp color supported by your video card? agreed it's better to avoid being so specific without having a working environment yet!  Okay I managed to fix it using a lot of your suggestions and some other source code I had laying around. Turns out the problem was from 3 different lines. particlesize = 0.01; should have been bigger: particlesize = 1.01; glColor4i(255 255 255 255) was turning the cube the same color as the clear color because I was using it wrong. I couldn't figure out how to use it right so I'm using glColor4f(0.0f1.0f1.0f0.5f) instead and that works. Last of all gluLookAt(0 5 20 0 0 0 0 0 0) needed to be gluLookAt(0 5 20 0 0 0 0 1 0) Thank you all for your help and your time.  Check OpenGL for error states. Use glslDevil glIntercept or gDebugger. Check the glGetError function. Can you test whether SDL actually acquired a device context? Does the Window reflect changes in the glClearColor call? Don't use 0.5 as an alpha value in glClearColor. Try these suggestions and report back with a minimal example as Simucal suggested.",c++ opengl sdl
1252680,A,"OpenGL Windowing Library for 2009 Trying to decide on a library for creating a window and capturing user input for my OpenGL app but there are just way too many choices: GLUT (win32) FreeGLUT OpenGLUT SFML GLFW SDL FLTK OGLWFW Clutter Qt Others? GLUT is simply outdated. I liked GLFW but it seems you can't set the window position before displaying it (I wanted it centered is that so much to ask?) so you see it appear and then shift over which bothers me. Plus development seems to have stopped on it too. SFML has some nice features but it uses event polling rather than callbacks which I prefer for decoupling. I don't think I need all the GUI features of FLTK. SDL is slow (doesn't seem to take advantage of the GPU). And the other 3 I don't know much about (FreeGLUT OpenGLUT OGLWFW). So which is the lesser of the evils? Are there others I haven't heard about? I'm just trying to make a simple 2D game. I'm familiar enough with OpenGL that I don't really need drawing routines but I probably wouldn't complain about other functions that might be useful if they are implemented properly. OGLWFW seems to be Win32-only at the moment. The website says that Linux/Mac support is ""planned."" You haven't said whether licensing or cross-platform support is an issue - if it is you might want to make note of that. Oh. I must have glossed over that. Cross-platform is nice... especially since I'm developing on Ubuntu right now. Licensing is only a minor concern as I never seem to complete anything anyways... I never get to the point where I'm capable of selling it. I'd go for Qt. Nice general purpose library + opengl support Got something simple up and running now... starting to really like it :) Not only does it have perty callbacks... they're magically already registered for me. Seems to force you to separate your code which is probably a good thing or I'd probably have a big mess already :D Right...forgot about Qt. Steep learning curve though no? Tried using it once before completely baffled me. Hrm... at least the editor it came with was (confusing). NetBeans 6.8 seems to have some level of support for Qt so getting a simple app up and running wasn't so bad. I use Qt with Visual Studio on Windows and Vim/make on *nix boxes. It has really good documentation and clean API. I think there are tutorials for using it with IDEs like Eclipse/NetBeans/etc.. Just google for them  SDL allows you to create an OpenGL context that is accelerated (depending on drivers / hardware support). I know you tagged as C++ however pygame (python) is a great library for creating 2D games which also supports an OpenGL context. Pygame is built on SDL. Clutter is a new OpenGL based GUI library with bindings for Perl Python C# C++ Vala and Ruby. I haven't used it myself. From the website: Clutter uses OpenGL (and optionally OpenGL ES for use on Mobile and embedded platforms) for rendering but with an API which hides the underlying GL complexity from the developer. The Clutter API is intended to be easy to use efficient and flexible. Clutter seems rather confusing. Do you know any open sourced programs that are written in clutter? @the_drow: I think the most high profile open-source user of Clutter right now is the Intel Moblin netbook OS: moblin.org. SDL is still C-esque (requires cleanup) and uses event polling rather than callbacks and doesn't seem to have very good integration with OpenGL (requires the usage of SDL wrapper functions?). Once you set up the OpenGL context in pygame or SDL you can use any OpenGL you want. I have an OpenGL app that basically used SDL for windowing and event polling and OpenGL for the graphics. Well yeah... I'm not saying it doesn't work just is going to take a lot more convincing to beat out the other libraries.  IF ""learning c++ part of what you're trying to achieve"": then IF ""you only want to learn OpenGL with a fullscreen mode"": USE GLUT //Because it's VERY VERY simple. You can get set up VERY quick ELSE: USE QT //Great library has many many things that will help you. It is portable it has a nice API ENDIF IF ""you don't need C++"": then USE Python //I recommend it it is fast no long link times good api omg I love this language Background: I also tried to make simple 2D games once I started with C++ and NeHe. I knew nothing about OpenGL and C++ (had Java background). The language overrun me so did OpenGL. So it was a very hard learning curve. I don't recommend going that way since you can get faster results by using a dynamic language (such as Python). So I started learning some years later with python. I could get the ""rotating cubes"" working much faster. Well I personally already know both C++ and OpenGL so that's not really a problem. I just needed a cross-platform windowing API. All the other tidbits were a nice touch on Qt ;)  GLUT and the other GLUT alternatives should not be used in any sort of production application. They are good for putting together a quick demo application or to try something out but not for much more than that. If you're trying to make an OpenGL game I'd recommend SDL. It focuses more on gaming needs. It most definitely can be used with OpenGL. A brief google for ""SDL OpenGL"" turned up this link on how to initialize OpenGL with SDL. Enabling OpenGL should also enable hardware rendering with the GPU. Qt is a reasonable alternative but it's better if you want to embed OpenGL within a larger desktop application (think 3D modeling CAD/CAM medical visualization etc) where you need access to standard OS widgets for the UI. Yeah... but Qt also seems so much more modern and well designed! My game is going to have an editor... I *can* make use of those extra widgets :p I'll consider SDL for future projects though.  We have had rather good experiences with ClanLib 0.8 in 2008 and ClanLib 2.1 in 2009 on our C++ course. The productivity of the students (as measured by the quality of their project works) has greatly increased since switching over from SDL. However it needs to be noted that 2.1 is still very incomplete and one will certainly run into features that are simply not implemented yet. A couple of groups used Irrlicht (3D engine) with good results. SFML looks promising but I haven't had a chance to try it yet. As others have stated GLUT is not really suitable for anything serious. The rest of the libraries mentioned are something more of GUI toolkits than game development libraries.  Per recent corespondance with the author development on OGLWFW has stopped. Good to know. They all seem to be dying out... guess you gotta go for a big professionally developed framework these days.",c++ opengl
154730,A,"Capturing video out of an OpenGL window in Windows I am supposed to provide my users a really simple way of capturing video clips out of my OpenGL application's main window. I am thinking of adding buttons and/or keyboard shortcuts for starting and stopping the capture; when starting I could ask for a filename and other options if any. It has to run in Windows (XP/Vista) but I also wouldn't like to close the Linux door which I've so far been able to keep open. The application uses OpenGL fragment and shader programs the effects due to which I absolutely need to have in the eventual videos. It looks to me like there might be even several different approaches that could potentially fulfill my requirements (but I don't really know where I should start): An encoding library with functions like startRecording(filename) stopRecording and captureFrame. I could call captureFrame() after every frame rendered (or every second/third/whatever). If doing so makes my program run slower it's not really a problem. A standalone external program that can be programmatically controlled from my application. After all a standalone program that can not be controlled almost does what I need... But as said it should be really simple for the users to operate and I would appreciate seamlessness as well; my application typically runs full-screen. Additionally it should be possible to distribute as part of the installation package for my application which I currently prepare using NSIS. Use the Windows API to capture screenshots frame-by-frame then employ (for example) one of the libraries mentioned here. It seems to be easy enough to find examples of how to capture screenshots in Windows; however I would love a solution which doesn't really force me to get my hands super-dirty on the WinAPI level. Use OpenGL to render into an offscreen target then use a library to produce the video. I don't know if this is even possible and I'm afraid it might not be the path of least pain anyway. In particular I would not like the actual rendering to take a different execution path depending on whether video is being captured or not. Additionally I would avoid anything that might decrease the frame rate in the normal non-capture mode. If the solution were free in either sense of the word then that would be great but it's not really an absolute requirement. In general the less bloat there is the better. On the other hand for reasons beyond this question I cannot link in any GPL-only code unfortunately. Regarding the file format I cannot expect my users to start googling for any codecs but as long as also displaying the videos is easy enough for a basic-level Windows user I don't really care what the format is. However it would be great if it were possible to control the compression quality of the output. Just to clarify: I don't need to capture video from an external device like camcorder nor am I really interested in mouse movements even though getting them does not harm either. There are no requirements regarding audio; the application makes no noise whatsoever. I write C++ using Visual Studio 2008 for this very application also taking benefit of GLUT and GLUI. I have a solid understanding regarding C++ and linking in libraries and that sort of stuff but on the other hand OpenGL is quite new for me: so far I've really only learnt the necessary bits to actually get my job done. I don't need a solution super-urgently so feel free to take your time :) I had to create a demo project of recording an OpenGL rendering into a video. I used glReadPixels to get the pixel data and created the video with OpenCV's cvWriteFrame. OpenCV lets you write in divx or even x264/vp8(with ffmpeg compiled in). I have a more detailed writeup on my blog post along with a sample project. http://tommy.chheng.com/2013/09/09/encode-opengl-to-video-with-opencv/ Hi bluefeet the essential part of the answer is present: ""use glReadPixels to get the pixel data"" and use OpenCV's ""cvWriteFrame"". The blog post goes into more detail but that's the gist of the answer. Thanks for posting your answer! Please note that you should post the essential parts of the answer here on this site or your post risks being deleted [See the FAQ where it mentions answers that are 'barely more than a link'.](http://stackoverflow.com/faq#deletion) You may still include the link if you wish but only as a 'reference'. The answer should stand on its own without needing the link.  The easiest option is going to be saving each rendered frame from within your app and then merging them into an AVI. When you have the AVI there are many libraries available that can convert it into a more optimal format or possibly skip the AVI step altogether. In terms of getting each frame you could accomplish this either by rendering into an offscreen texture as you suggest or using the backbuffer directly as a source if your hardware supports this. Doing either of these (and saving each frame) is going to be difficult without a heavy penalty on framerate. Providing your application is deterministic you could ""record"" the users actions as a series of inputs and then have an export mode that sequentially renders these to an offscreen surface to generate the AVI. The frame rate penalty is quite acceptable as long as it applies only when really capturing a video. The app is deterministic in the sense of not being stochastic but what gets displayed is based in addition to user actions also on results from an external measurement device.  There are two different questions here - how to grab frames from an OpenGL application and how to turn them into a movie file. The first question is easy enough; you just grab each frame with glReadPixels() (via a PBO if you need the performance). The second question is a little harder since the cross-platform solutions (ffmpeg) tend to be GPL'd or LGPL'd. Is LGPL acceptable for your project? The Windows way of doing this (DirectShow) is a bit of a headache to use. Edit: Since LGPL is ok and you can use ffmpeg see here for an example of how to encode video. Yes: LGPL is ok. glReadPixels() indeed seems to be what I need for the first question thanks! @MikeF: that link is bust  This does look pretty relevant for merging into an AVI (as suggested by Andrew) however I was really hoping to avoid the LPBITMAPINFOHEADERs etc. Thanks for the answers I will report on the success if there is going to be any :) In the meantime additional tips for encoding the raw frames from glReadPixels into video clips would be appreciated. Edit: So far ffmpeg suggested by Mike F seems to be the way to go. However I didn't get into the actual implementation yet but hopefully that will change in the near future! Be aware that using VfW and DirectShow can use only whatever codecs are installed on the user's machine so you need to either bundle the codec you plan to use or find out which codecs they're *guaranteed* to already have.",c++ opengl video screenshot video-capture
1531023,A,"moving a point in 3d space I have a point at 000 I rotate the point 30 deg around the Y axis then 30 deg around the X axis. I then want to move the point 10 units forward. I know how to work out the new X and Y position MovementX = cos(angle) * MoveDistance; MovementY = sin(angle) * MoveDistance; But then I realised that these values will change because of Z won't they? How do I work out Z and have I worked out X and Y correctly? Thanks! Use rotation matrices. :) They take a considerable investment of effort to learn but once you know how to use them everything becomes much less work. Well a point at (000) rotated at any angle at the xy or z axis will stay a (000). Assuming by ""forward"" you mean the direction pointing along the z-axis the components of the movement vector become: MovementX = 0; MovementY = 0; MovementZ = 10. Seriously try to ask exactly what you want to know. Ok I guess the equivalent in OpenGL would be... glRotatef( 30.0f 0.0f 1.0f 0.0f ); glRotatef( 30.0f 1.0f 0.0f 0.0f ); glTranslate( 0.0f 0.0f 10.0f ); But I want to do it manually so that I know the world coordinates of the point (so that I can compare it to other coordinates) I don't know too much about OpenGL but if your point is starting at (000) then drhirsch is correct and your point won't be moving anywhere through rotations. So the only function that would do anything is the glTranslate. Therefore your new position would be (0010). However I feel like that might not be what you were going for. Are you sure your point doesn't have a direction vector associated with it? You should multiply point coordinates to full rotation matrix which is matRotationTotal = matRotationX * matRotationY * matRotationZ. Check this article for details.",c++ math opengl coordinates
1351129,A,Calculating 3D tangent space In order to use normal mapping in GLSL shaders you need to know the normal tangent and bitangent vectors of each vertex. RenderMonkey makes this easy by providing it's own predefined variables (rm_tangent and rm_binormal) for this. I am trying to add this functionality to my own 3d engine. Apparently it is possible to calculate the tangent and bi tangent of each vertex in a triangle using each vertex's xyz coordinates uv texture coordinates and normal vector. After some searching I devised this function to calculate the tangent and bitangent for each vertex in my triangle structure. void CalculateTangentSpace(void) { float x1 = m_vertices[1]->m_pos->Get(0) - m_vertices[0]->m_pos->Get(0); float x2 = m_vertices[2]->m_pos->Get(0) - m_vertices[0]->m_pos->Get(0); float y1 = m_vertices[1]->m_pos->Get(1) - m_vertices[0]->m_pos->Get(1); float y2 = m_vertices[2]->m_pos->Get(1) - m_vertices[0]->m_pos->Get(1); float z1 = m_vertices[1]->m_pos->Get(2) - m_vertices[0]->m_pos->Get(2); float z2 = m_vertices[2]->m_pos->Get(2) - m_vertices[0]->m_pos->Get(2); float u1 = m_vertices[1]->m_texCoords->Get(0) - m_vertices[0]->m_texCoords->Get(0); float u2 = m_vertices[2]->m_texCoords->Get(0) - m_vertices[0]->m_texCoords->Get(0); float v1 = m_vertices[1]->m_texCoords->Get(1) - m_vertices[0]->m_texCoords->Get(1); float v2 = m_vertices[2]->m_texCoords->Get(1) - m_vertices[0]->m_texCoords->Get(1); float r = 1.0f/(u1 * v2 - u2 * v1); Vec3<float> udir((v2 * x1 - v1 * x2) * r (v2 * y1 - v1 * y2) * r (v2 * z1 - v1 * z2) * r); Vec3<float> vdir((u1 * x2 - u2 * x1) * r (u1 * y2 - u2 * y1) * r (u1 * z2 - u2 * z1) * r); Vec3<float> tangent[3]; Vec3<float> tempNormal; tempNormal = *m_vertices[0]->m_normal; tangent[0]=(udir-tempNormal*(Vec3Dot(tempNormal udir))); m_vertices[0]->m_tangent=&(tangent[0].Normalize()); m_vertices[0]->m_bitangent=Vec3Cross(m_vertices[0]->m_normal m_vertices[0]->m_tangent); tempNormal = *m_vertices[1]->m_normal; tangent[1]=(udir-tempNormal*(Vec3Dot(tempNormal udir))); m_vertices[1]->m_tangent=&(tangent[1].Normalize()); m_vertices[1]->m_bitangent=Vec3Cross(m_vertices[1]->m_normal m_vertices[1]->m_tangent); tempNormal = *m_vertices[2]->m_normal; tangent[2]=(udir-tempNormal*(Vec3Dot(tempNormal udir))); m_vertices[2]->m_tangent=&(tangent[2].Normalize()); m_vertices[2]->m_bitangent=Vec3Cross(m_vertices[2]->m_normal m_vertices[2]->m_tangent); } When I use this function and send the calculated values to my shader the models look almost like they do in RenderMonkey but they flicker in a very strange way. I traced the problem to the tangent and bitangent I am sending OpenGL. This leads me to suspect that my code is doing something wrong. Can anyone see any problems or have any suggestions for other methods to try? I should also point out that the above code is very hacky and I have very little understanding of the math behind what is going on. Found the solution. Much simpler (but still a little hacky) code: void CalculateTangentSpace(void) { float x1 = m_vertices[1]->m_pos->Get(0) - m_vertices[0]->m_pos->Get(0); float y1 = m_vertices[1]->m_pos->Get(1) - m_vertices[0]->m_pos->Get(1); float z1 = m_vertices[1]->m_pos->Get(2) - m_vertices[0]->m_pos->Get(2); float u1 = m_vertices[1]->m_texCoords->Get(0) - m_vertices[0]->m_texCoords->Get(0); Vec3<float> tangent(x1/u1 y1/u1 z1/u1); tangent = tangent.Normalize(); m_vertices[0]->m_tangent = new Vec3<float>(tangent); m_vertices[1]->m_tangent = new Vec3<float>(tangent); m_vertices[2]->m_tangent = new Vec3<float>(tangent); m_vertices[0]->m_bitangent=new Vec3<float>(Vec3Cross(m_vertices[0]->m_normal m_vertices[0]->m_tangent)->Normalize()); m_vertices[1]->m_bitangent=new Vec3<float>(Vec3Cross(m_vertices[1]->m_normal m_vertices[1]->m_tangent)->Normalize()); m_vertices[2]->m_bitangent=new Vec3<float>(Vec3Cross(m_vertices[2]->m_normal m_vertices[2]->m_tangent)->Normalize()); } Sorry but that's just wrong. What you do is simply scale the edge v01 by 1/u1. Not only could u1 be zero but it obviously is not correct. For a correct answer refer to http://stackoverflow.com/questions/5255806/how-to-calculate-tangent-and-binormal  You will get zero division in your 'r' calculation for certain values of u1 u2 v1 and v2 resulting in unknown behavior for 'r'. You should guard against this. Figure out what 'r' should be if the denominator is zero and that MIGHT fix your problem. I too have little understanding of the math behind this. Suggested implementation that sets r = 0 if denominator is zero: #include <cmath> ... static float PRECISION = 0.000001f; ... float denominator = (u1 * v2 - u2 * v1); float r = 0.f; if(fabs(denominator) > PRECISION) { r = 1.0f/denominator; } ... Thanks for the reply. Unfortunately your suggestion doesn't help.,c++ opengl shader glsl
1715027,A,"DDS DXT1 loading in OpenGL crashes Any idea whats wrong with my code? when i execute glCompressedTexImage2D() the program just crashes (comes the Windows XP crash message thing...) Im trying to load DDS image without mipmaps the image format is DDS DXT1 Am i missing some include file or what did i do wrong? I downloaded the included files from: http://sourceforge.net/projects/glew/files/glew/1.5.1/glew-1.5.1-win32.zip/download I have glew32.dll in the same folder as my .exe is. The code below has only the parts i changed to be able to load DDS images: #pragma comment(lib ""glew32.lib"") #include <GL\glew.h> #include <GL\gl.h> ... typedef struct { GLuint dwSize; GLuint dwFlags; GLuint dwFourCC; GLuint dwRGBBitCount; GLuint dwRBitMask; GLuint dwGBitMask; GLuint dwBBitMask; GLuint dwABitMask; } DDS_PIXELFORMAT; typedef struct { GLuint dwMagic; GLuint dwSize; GLuint dwFlags; GLuint dwHeight; GLuint dwWidth; GLuint dwLinearSize; GLuint dwDepth; GLuint dwMipMapCount; GLuint dwReserved1[11]; DDS_PIXELFORMAT ddpf; GLuint dwCaps; GLuint dwCaps2; GLuint dwCaps3; GLuint dwCaps4; GLuint dwReserved2; } DDS_HEADER; DDS_HEADER DDS_headers; ... FILE *fp = fopen(""test.dds"" ""rb""); fread(&DDS_headers 1 sizeof(DDS_headers) fp); img_width = DDS_headers.dwWidth; img_height = DDS_headers.dwHeight; maxsize = (img_width*img_height)/2; unsigned char *imgdata = (unsigned char *)malloc(maxsize); fread(imgdata 1 maxsize fp); fclose(fp); GLuint texID; glGenTextures(1 &texID); glBindTexture(GL_TEXTURE_2D texID); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glCompressedTexImage2D(GL_TEXTURE_2D 0 GL_PALETTE4_R5_G6_B5_OES img_width img_height 0 maxsize imgdata); // NOTICE: // Ive also tried with function: // glCompressedTexImage2DARB(); // and internalformats: GL_COMPRESSED_RGB_S3TC_DXT1_EXT and all of the possible formats... its not a format error. please if you want DXT1 put DXT1 in the glCompressedTexImage2D call. There's no point in keeping PALETTE in there. Now what at the time of the glCompressedTexImage2D call what are the values of img_width/img_height/maxsize/imgdata ? i think the problem was at initialization/includefiles ive got the DDS loading working now by some other code thanks for help anyways. I remember messing with the DDS loader in glew I don't think the header information there is correct. I never could get it to work correctly. The best way would be to use the header contstucts that are in DDraw.h here's what I was able to use for DXT13and 5 which if I remember correctly are the only ones that can work in OpenGL.  struct DDS_IMAGE_DATA { GLsizei width; GLsizei height; GLint components; GLenum format; int numMipMaps; GLubyte *pixels; }; DDS_IMAGE_DATA* CImage::loadDDSTextureFile( const char *filename ) { DDS_IMAGE_DATA *pDDSImageData; DDSURFACEDESC2 ddsd; char filecode[4]; FILE *pFile; int factor; int bufferSize; // Open the file pFile = fopen( filename ""rb"" ); if( pFile == NULL ) { #if DEBUG char str[255]; printf( str ""loadDDSTextureFile couldn't find or failed to load \""%s\"""" filename ); #endif return NULL; } // Verify the file is a true .dds file fread( filecode 1 4 pFile ); if( strncmp( filecode ""DDS "" 4 ) != 0 ) { #if DEBUG char str[255]; printf( str ""The file \""%s\"" doesn't appear to be a valid .dds file!"" filename ); #endif fclose( pFile ); return NULL; } // Get the surface descriptor fread( &ddsd sizeof(ddsd) 1 pFile ); pDDSImageData = (DDS_IMAGE_DATA*) malloc(sizeof(DDS_IMAGE_DATA)); memset( pDDSImageData 0 sizeof(DDS_IMAGE_DATA) ); // // This .dds loader supports the loading of compressed formats DXT1 DXT3 // and DXT5. // switch( ddsd.ddpfPixelFormat.dwFourCC ) { case FOURCC_DXT1: // DXT1's compression ratio is 8:1 pDDSImageData->format = GL_COMPRESSED_RGBA_S3TC_DXT1_EXT; factor = 2; break; case FOURCC_DXT3: // DXT3's compression ratio is 4:1 pDDSImageData->format = GL_COMPRESSED_RGBA_S3TC_DXT3_EXT; factor = 4; break; case FOURCC_DXT5: // DXT5's compression ratio is 4:1 pDDSImageData->format = GL_COMPRESSED_RGBA_S3TC_DXT5_EXT; factor = 4; break; default: #if DEBUG char str[255]; printf( str ""The file \""%s\"" doesn't appear to be compressed "" ""using DXT1 DXT3 or DXT5!"" filename ); #endif return NULL; } // // How big will the buffer need to be to load all of the pixel data // including mip-maps? // if( ddsd.dwLinearSize == 0 ) { #if DEBUG printf(""dwLinearSize is 0!""); #endif } if( ddsd.dwMipMapCount > 1 ) bufferSize = ddsd.dwLinearSize * factor; else bufferSize = ddsd.dwLinearSize; pDDSImageData->pixels = (unsigned char*)malloc(bufferSize * sizeof(unsigned char)); fread( pDDSImageData->pixels 1 bufferSize pFile ); // Close the file fclose( pFile ); pDDSImageData->width = ddsd.dwWidth; pDDSImageData->height = ddsd.dwHeight; pDDSImageData->numMipMaps = ddsd.dwMipMapCount; if( ddsd.ddpfPixelFormat.dwFourCC == FOURCC_DXT1 ) pDDSImageData->components = 3; else pDDSImageData->components = 4; return pDDSImageData; } void CImage::loadDDS(const char * szFilename tTexture & texData) { DDS_IMAGE_DATA *pDDSImageData = loadDDSTextureFile(szFilename); if( pDDSImageData != NULL ) { texData.m_nHeight = pDDSImageData->height; texData.m_nWidth = pDDSImageData->width; texData.m_nHeight = pDDSImageData->height; texData.m_eFormat = pDDSImageData->format; int nHeight = pDDSImageData->height; int nWidth = pDDSImageData->width; int nNumMipMaps = pDDSImageData->numMipMaps; int nBlockSize; if( pDDSImageData->format == GL_COMPRESSED_RGBA_S3TC_DXT1_EXT ) nBlockSize = 8; else nBlockSize = 16; //glGenTextures( 1 &g_compressedTextureID ); //glBindTexture( GL_TEXTURE_2D g_compressedTextureID ); glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR ); glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR ); int nSize; int nOffset = 0; // Load the mip-map levels for( int i = 0; i < nNumMipMaps; ++i ) { if( nWidth == 0 ) nWidth = 1; if( nHeight == 0 ) nHeight = 1; nSize = ((nWidth+3)/4) * ((nHeight+3)/4) * nBlockSize; glCompressedTexImage2DARB( GL_TEXTURE_2D i pDDSImageData->format nWidth nHeight 0 nSize pDDSImageData->pixels + nOffset ); nOffset += nSize; // Half the image size for the next mip-map level... nWidth = (nWidth / 2); nHeight = (nHeight / 2); } } if( pDDSImageData != NULL ) { if( pDDSImageData->pixels != NULL ) free( pDDSImageData->pixels ); free( pDDSImageData ); } } This particular bit of code makes a few assumptions of the DDS file we are trying to load first that is is a compressed file either DXT13 or 5 and that the DDS file has pre-generated mipmaps saved in it hence we don't have to generate them ourselves. I hope this was able to help you it took me some time to get it working correctly myself. If there's anything in this code snippet that seems unclear let me know and I'll help you further. Thanks for your help but i've tested my headers and they are fine i copied them from MSDN and since i only use width/height in my code there cant be header related errors plus i've tested with static numbers without using headers at all. I noticed your code has RGBA so i tried with it too but that didnt make any difference... I cant test your code because i dont have the needed type definitions: CImage tTexture DDSURFACEDESC2 FOURCC_DXT1 FOURCC_DXT3 FOURCC_DXT5. Anyways cant i use DDS images unless i make miplevels in them?  The format argument that you're passing to glCompressedTexImage2D looks a bit odd: glCompressedTexImage2D(GL_TEXTURE_2D 0 GL_PALETTE4_R5_G6_B5_OES img_width img_height 0 maxsize imgdata); I think this is neither a paletted texture nor an OpenGL ES extension. Perhaps something like GL_COMPRESSED_RGB_S3TC_DXT1_EXT would work better. glewIsSupported(""EXT_texture_compression_dxt1"") returns false :/ what can i do to get this thing working? Woops I forgot the GL_ prefix; try GL_EXT_texture_compression_dxt1. If it still returns false then your platform doesn't support DXT1 compression so you'll need to convert your source data to some other format. yeah still returns false i dont quite understand how this is going to help me to make this DDS thing to work? what code you use to use DDS images in your applications? can you share it? my GFX card surely supports DDS thats a fact. i have tried with that too and all the possible format types you can give there every of them results in a crash. Make sure your OpenGL implementation supports the DXT1 extension. You can do so by calling the glewIsSupported function and passing in the following string: ""EXT_texture_compression_dxt1"".",c++ opengl directdraw dds-format
328019,A,"How to load bmp into GLubyte array? All I am trying to load up a bmp file into a GLubyte array (without using aux). It is unbelievable how what I thought would have been a trivial task is sucking up hours of my time. Can't seem to find anything on Google! This is what I hacked together but it's not quite working: // load texture GLubyte *customTexture; string fileName(""C:\\Development\\Visual Studio 2008\\Projects\\OpenGL_Test\\Crate.bmp""); // Use LoadImage() to get the image loaded into a DIBSection HBITMAP hBitmap = (HBITMAP)LoadImage( NULL (LPCTSTR)const_cast<char*>(fileName.c_str()) IMAGE_BITMAP 0 0 LR_CREATEDIBSECTION | LR_DEFAULTSIZE | LR_LOADFROMFILE ); customTexture = new GLubyte[3*256*256]; // size should be the size of the .bmp file GetBitmapBits(hBitmap 3*256*256 (LPVOID) customTexture); GetBitmapDimensionEx(hBitmap&szBitmap); What happens is the call to LoadImage seems to be returning Undefined Value (NULL? I am not able to figure out if it's actually loading the bmp or not - a bit confused). At the moment I am converting bmps to raw then it's all easy. Anyone has any better and cleaner snippet? LoadImage() can only load bitmaps that are embedded into your executable file with the resource compiler - it can't load external bitmaps from the filesystem. Fortunately bitmap files are really simple to read yourself. See Wikipedia for a description of the file format. Just open up the file like you would with any other file (important: open it in binary mode i.e. with the ""rb"" option using fopen or the ios::binary flag using the C++ ifstream) read in the bitmap dimensions and read in the raw pixel data. Thanks for your remarks on LoadImage - I know how to open the file with ifstream - I was kinda hoping I could find some snippet instead of going there and figure out from scratch how to read byte-by-byte (I even think I already did it smt like N years ago) since it sounds like a common task  It is a common task that's why glaux among others gives you functions for it. Reading a bitmap is a trivial matter especially if there is only one depth/bpp to account for. Also see this question. I am not using glaux. Since it's a common task I'd expect someone to show me how to get it done or to point out some useful examples on the web since I couldn't find anything. At the moment I am converting bmp to raw.",c++ opengl glut
1645309,A,"Help me evaluate this casting I found this in the PowerVR mesh drawing code and I don't really know how to read it. &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]] What is going on here? Is this a reference to void cast as an unsigned short pointer and then offset by (3*mesh(etc...) + batchNum)? It's breaking my brain. It's found in the context of a glDrawElements call: glDrawElements(GL_TRIANGLES i32Tris * 3 GL_UNSIGNED_SHORT &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]]); It's found in the context of a glDrawElements call: glDrawElements(GL_TRIANGLES i32Tris * 3 GL_UNSIGNED_SHORT &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]]); Its an obfuscated way of computing sizeof(unsigned short) * 3 * mesh.sBoneBatches.pnBatchOffset[batchNum] but since it doesn't actually save any characters its not a very good obfuscation Only one caveat - it returns a pointer to address N not N as a number.  It's computing a byte offset -- 3 * mesh.sBoneBatches.pnBatchOffset[batchNum] is the index. Using 0 as the pointer means that the address will be just the offset value nothing else. In essence it's `3 * mesh.sBoneBatches.pnBatchOffset[batchNum] * sizeof(unsigned short)` (though with a different type) assuming that `unsigned short[]` arrays are aligned naturally with no padding. I dont get it . Shouldn't ""((unsigned short*)0)[...]"" dereference a null pointer thus leading in unexpected behavior ? @TheSamFrom1984: No because dereferencing is not needed to take the address. This is a common trick in C and is theoretically illegal in C++ but generally works anyways.  Let's go from the inside out. (unsigned short*)0 This is casting 0 to an unsigned short pointer. This will be used for computing a memory offset computed in terms of the size of an unsigned short. 3 * mesh.sBoneBatches.pnBatchOffset[batchNum] This is presumably the offset in memory of some batch of triangles. A triangle is composed of 3 shorts so it looks like they are storing an offset in terms of numbers of triangles and then multiplying by 3 to get the number of shorts. ((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]] This is now using that 0 pointer to find the memory location of the given offset. This would normally return the value of that memory location but they want a pointer to pass into glDrawElements so the use the & operator to get a pointer to that memory location: &((unsigned short*)0)[3 * mesh.sBoneBatches.pnBatchOffset[batchNum]] why isn't the applicaiton crashing at step 3 ? (attempt to get the value of a bad memory location) Because of the & operator the value is never accessed. Just the address of the value. It didn't know about this particularity of the language. Doesn't seem ""natural"" to me. Thanks. That was very helpful thank you. One more question is a 0 pointer the same concept as a void pointer? No but C mandates that the NULL pointer converts to an integral 0 and vice versa and C++ mandates that NULL is represented by 0.  Really this kind of hack is due to OpenGL expressing offsets inside Buffer Objects through the pointer argument of glDrawElements. glDrawElements(mode count type void* indices) indices represents either a client-side memory pointer or a server-side memory offset based on the binding of GL_ELEMENT_ARRAY_BUFFER_ARB It's interesting to dig a bit deeper... From the VBO specification: Is it legal C to use pointers as offsets? We haven't come to any definitive conclusion about this. [...] Varying opinions have been expressed as to whether this is legal although no one could provide an example of a real system where any problems would occur.",c++ opengl
1617370,A,"OpenGL alpha transparency I am starting to use opengl and I wanted to try alpha transparency. Here's my code: void display(void); int main(int argc char** argv) { glutInit(&argc argv); glutInitDisplayMode(GLUT_SINGLE|GLUT_RGBA); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glEnable( GL_BLEND ); glutInitWindowSize(600600); glutInitWindowPosition(20050); glutCreateWindow(""glut test""); glutDisplayFunc(display); glutMainLoop(); return 0; } void display() { glClear(GL_COLOR_BUFFER_BIT); glPointSize(8); glBegin(GL_POINTS); glColor4f(.23.78.321.0); glVertex2f(00); glColor4f(.23.78.320.1); glVertex2f(0.10); glEnd(); glFlush(); } The problem is that these two points appear identical (even when I set the alpha to 0). Is there something I missed to enable alpha transparency? Thank you have you glEnable'd alpha blending? And have you set up your blend parameters? You can't just set the alpha you need to setup various other parameters in OpenGL. glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glEnable( GL_BLEND ); OpenGL requires most everything to be enabled. A good place to start looking. (Disabling and enabling different operations at different times can lead to the most interesting effects.) Thanks I added them but I forgot to set the background color This is really helpful for a beginner Thank you~  Just a guess but could it be that you dont have a background color ? So when your rendering the second vertex which has alpha 0.1 there is no background to compute the proper color ? Just a guess been years since i used opengl. Yep this was it. I should've added: `glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glEnable( GL_BLEND ); glClearColor(0.00.00.00.0);` but only after glutCreateWindow()",c++ opengl transparency alpha
751840,A,How do I make textures transparent in OpenGL? I've tried to research this on Google but there doesn't appear to me to be any coherent simple answers. Is this because it's not simple or because I'm not using the correct keywords? Nevertheless this is the progress I've made so far. Created 8 vertices to form 2 squares. Created a texture with a 200 bit alpha value (so about 80% transparent). Assigned the same texture to each square which shows correctly. Noticed that when I use a texture with 255 alpha it appears brighter. The init is something like the following: glClearColor(0.0 0.0 0.0 0.0); glShadeModel(GL_FLAT); glEnable(GL_DEPTH_TEST); glEnable(GL_CULL_FACE); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); glPixelStorei(GL_UNPACK_ALIGNMENT 1); glGenTextures(1 textureIds); glTexEnvf(GL_TEXTURE_ENV GL_TEXTURE_ENV_MODE GL_REPLACE); int i j; GLubyte pixel; for (i = 0; i < TEXTURE_HEIGHT; i++) { for (j = 0; j < TEXTURE_WIDTH; j++) { pixel = ((((i & 0x8) == 0) ^ ((j & 0x8) == 0)) * 255); texture[i][j][0] = pixel; texture[i][j][1] = pixel; texture[i][j][2] = pixel; texture[i][j][3] = 200; } } glBindTexture(GL_TEXTURE_2D textureIds[0]); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_REPEAT); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_REPEAT); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_NEAREST); glTexParameterf(GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_NEAREST); glTexImage2D( GL_TEXTURE_2D 0 GL_RGBA TEXTURE_WIDTH TEXTURE_HEIGHT 0 GL_RGBA GL_UNSIGNED_BYTE texture); This is somewhat similar to the code snippet from page 417 in the book OpenGL Programming Guide and creates a check pattern. And then the display function contains... glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glEnable(GL_TEXTURE_2D); // Use model view so that rotation value is literal not added. glMatrixMode(GL_MODELVIEW); glPushMatrix(); // ... translation etc ... glBindTexture(GL_TEXTURE_2D textureIds[0]); glBegin(GL_QUADS); glTexCoord2f(0.0 0.0); glVertex3f(-1.0 +1.0 0.0); // top left glTexCoord2f(0.0 1.0); glVertex3f(-1.0 -1.0 0.0); // bottom left glTexCoord2f(1.0 1.0); glVertex3f(+1.0 -1.0 0.0); // bottom right glTexCoord2f(1.0 0.0); glVertex3f(+1.0 +1.0 0.0); // top right glEnd(); // not neccecary to repeat just good practice glBindTexture(GL_TEXTURE_2D textureIds[0]); glBegin(GL_QUADS); glTexCoord2f(0.0 0.0); glVertex3f(-0.5 +1.0 -1.0); // top left glTexCoord2f(0.0 1.0); glVertex3f(-0.5 -1.0 -1.0); // bottom left glTexCoord2f(1.0 1.0); glVertex3f(+1.5 -1.0 -1.0); // bottom right glTexCoord2f(1.0 0.0); glVertex3f(+1.5 +1.0 -1.0); // top right glEnd(); glFlush(); glDisable(GL_TEXTURE_2D); glPopMatrix(); SwapBuffers(); So this renders a 2nd square in the background; I can see this but it looks like they're being blended with the background (I assume this because they are darker with 200 bit alpha than 255 bit) instead of the texture behind... As you can see no transparency... How can I fix this? You have TEXTURE_HEIGHT twice in that loop instead of TEXTURE_WIDTH Ah thanks but that won't matter as they're both 64. So the other answer which was here but was deleted mentioned this - Generally for alpha blending to work correctly you need to sort the objects from far to near in the coordinate system of the camera. This is why your polygons are blended with the background. You can confirm that this is indeed the problem by disabling the depth test. Without depth test all the fragments are displayed and you'll be able to see the alpha blending. More on this in this page.,c++ opengl textures opacity
1478355,A,Programmatically block screen saver in Mac OSX Is it possible to programatically ask Mac OS X not to turn on the screen saver while your application is active? You want to use: UpdateSystemActivity(UsrActivity); Here is Apple's example code. Be aware this is deprecated for 64bit binaries and I have not found a suitable replacement but the struggle continues.,c++ osx opengl
766127,A,Obtaining current ModelView matrix In OpenGL how do I read the current x/y translation in the modelview matrix? I know that you have to load the current matrix into an array and read the floats from there but I don't know precisely how to do it. Use glGlet GLfloat matrixf[16]; glGetFloatv(GL_MODELVIEW_MATRIX matrixf); GLdouble matrixd[16]; glGetDoublev(GL_MODELVIEW_MATRIX matrixd); GLint matrixi[16]; glGetIntegerv(GL_MODELVIEW_MATRIX matrixi);  In order to retrieve the current modelview matrix you have to call the glGetFloatv function with GL_MODELVIEW_MATRIX parameter. GLfloat matrix[16]; glGetFloatv (GL_MODELVIEW_MATRIX matrix); From the documentation: GL_MODELVIEW_MATRIX params returns sixteen values: the modelview matrix on the top of the modelview matrix stack. Initially this matrix is the identity matrix. Beat me to it :),c++ opengl matrix translation
1614393,A,"SDL GL program terminates immediately I'm using Dev-C++ 4.9.9.2 (don't ask why) and SDL 1.2.8. Next I've created new project: SDL&GL. This project contains already some code: #include <SDL/SDL.h> #include <gl/gl.h> int main(int argc char *argv[]){ SDL_Event event; float theta = 0.0f; SDL_Init(SDL_INIT_VIDEO); SDL_SetVideoMode(600 300 0 SDL_OPENGL | SDL_HWSURFACE | SDL_NOFRAME); glViewport(0 0 600 300); glClearColor(0.0f 0.0f 0.0f 0.0f); glClearDepth(1.0); glDepthFunc(GL_LESS); glEnable(GL_DEPTH_TEST); glShadeModel(GL_SMOOTH); glMatrixMode(GL_PROJECTION); glMatrixMode(GL_MODELVIEW); int done; for(done = 0; !done;){ glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glLoadIdentity(); glTranslatef(0.0f0.0f0.0f); glRotatef(theta 0.0f 0.0f 1.0f); glBegin(GL_TRIANGLES); glColor3f(1.0f 0.0f 0.0f); glVertex2f(0.0f 1.0f); glColor3f(0.0f 1.0f 0.0f); glVertex2f(0.87f -0.5f); glColor3f(0.0f 0.0f 1.0f); glVertex2f(-0.87f -0.5f); glEnd(); theta += .5f; SDL_GL_SwapBuffers(); SDL_PollEvent(&event); if(event.key.keysym.sym == SDLK_ESCAPE) done = 1; } SDL_Quit(); return(0); } Next I compiled project and try to run it. After run the program shows for less than 1 second and immediately terminates. Debugger returns following error: ""An Access Violation (Segmentation Fault) raised in your program"". I'm using Windows 2003 and Radeon x1950 PRO with latest drivers. I've tested program on laptop with Windows XP and it works perfectly. Why this program doesn't work on my computer? I take it checking the error returns on all your SDL and GL calls wasn't informative? Yes it wasn't informative because it returns following error: ""An Access Violation (Segmentation Fault) raised in your program"". It works for me too. I'd try removing SDL_HWSURFACE and add SDL_DOUBLEBUF from the window call. SDL_SetVideoMode(600 300 0 SDL_OPENGL | SDL_NOFRAME | SDL_DOUBLEBUF); while(!done) looks prettier and easier to read. Since it's tagged with C++ why are you not using bools for this? bool done = false; while(!done){ You also want while(SDL_PollEvent(&event)) as there can be more than one event per frame. while(SDL_PollEvent(&event)) { switch(event.type) case SDL_KEYDOWN: if(event.key.keysym.sym == SDLK_ESCAPE) done = true; }  I finally found some time to solve this problem. I have completly uninstalled old card graphic drivers and install 9.8 ATI drivers with Catalyst Control Center. Now everything is working. There where no problem in code itself. The problem was something in my system with graphic drivers. Anyway thanks for your answers!  My guess is it's crashing on SDL_PollEvent(). It returns 1/true if there is an event 0/false if not. When it does return true it will be a certain type of SDL_Event based on event.type. SDL_Event is a union of all SDL events and some info in one event is not guaranteed to be in the same order type etc as another. So you just need to check the type of the event and handle it as necessary.. Check out the docs for more info of course. Something like this: if (SDL_PollEvent(&event)) { switch (event.type) { case SDL_KEYUP: if (event.key.keysym.sym == SDLK_ESCAPE) done = 1; } } That would not _crash_. It will read from uninitialized memory or a previous event value so it could __quit__ prematurely. Good catch though!",c++ opengl sdl
1564870,A,"Recommended OpenGL / GLUT Reference What OpenGL / GLUT reference is good for day to day programming as you learn? Ideally I'm looking for something with lots of C++ sample code to help me learn as I develop OpenGL applications as well as details about the APIs similar to what MSDN provides for .net programming. If there isn't a one stop shop then please list the set of references I should use and what the strengths of each one is. The PyOpenGL Documentation is identical to the OpenGL docs but far more readable and user-friendly. Have a look. I also second the OpenGL SuperBible.  I learned OpenGL using the OpenGL Super Bible. It's still the best reference for it that I can find.  The Red Book is the standard book on OpenGL. Don't be discouraged by the fact that the Amazon review for the 7th Edition has only two stars; this is because people are disappointed that there isn't more on the newest OpenGL features in the book. Previous editions got more stars. Another good book is the OpenGL SuperBible. The NeHe Tutorials are one of the most often cited OpenGL tutorials with sample code not only in C++ but in many other programming languages.  I think that by ""Glut"" you mean ""Freeglut"". In this case you should use this specific reference: http://jocelyn.frechot.free.fr/freeglut/freeglut_2.6.0-api_0.3.xhtml It contains latest references for current Freeglut. This way you can use special aptitudes of Freeglut (like controlling your own GL loop with glutMainLoopEvent() which is invaluadble when you're using Freeglut with others libraries.  For all the details about the OpenGL-API there are of course the sdk documentation pages https://www.opengl.org/sdk/docs/ or you could look into the standard specification itself (which I personally avoid most of the time). This most likely only helps if you already have a basic understanding of how to use the GL. The news section on opengl.org also often links to tutorials and books. Just skimmed through it and found this tutorial. As for OpenGL related books I only know the Super Bible which I think is okay to get started. When learning OpenGL without any computer graphics knowledge a book on that topic can be very helpful too. A classic would be Coumputer Graphics Principal and Practice by James D. Foley which is still an excellent read but it doesn't focus much on real time rendering. For that Real-Time Rendering by Akenine-Moller is an excellent choice.",c++ opengl reference glut
1105349,A,C++ OpenGL Window and Context creation framework / library I'm searching for an multi platform OpenGL framework that abstracts the creation of windows and gl contexts in C++. I'd like to have an OO representation of Window Context & co where i can instantiate a Window create a Context and maybe later set the window to fullscreen. I'm thinking about implementing this myself for xgl wgl and agl. But before So here comes the Question: Which libraries / frameworks should i check out first before inventing the wheel again? Edit: So far named libraries: glut Qt SDL gtkglext wxwdigets SFML Hmm could be I would be curious if it were that simple. In my case it's just a Win32 reference build for a console game so I hadn't looked into it too deeply. Plus for debugging purposes I actually like/need to see the printfs. I'm using GLUT and wxWidgets successfully in two separate projects. The only downside to GLUT under Win32 is that it opens up a DOS window - good for seeing any printf's but it doesn't look as professional to have an app that opens up that window. @Jim: That's a bit in the executable that can easily be changed with `editbin` and is usually an artefact of compilation flags? wxWidgets is another alternative but may also be too heavyweight.  SMFL is another similar to SDL but takes a more object oriented approach. This one sounds promising ( their site says: SFML is composed of several packages to perfectly suit your needs. You can use SFML as a minimal windowing system to interface with OpenGL or as a fully-featured multimedia library for building games or interactive programs. ). I'm going to implement it myself - but since i think i can reuse some of the SMFL code i will accept this answer SFML is lovely. It doesn't (or maybe didn't) support creating contexts with stencil buffers but hey you can edit the code.  You could look at Glut (C) Qt (C++) SDL (C). I have already worked with glut (it was nice for the little opengl examples back at university but i think it wont fit in a bigger project). The Qt api seems nice - but i don't like the thought to introduce a dependecy to Qt. It seems to heavyweight for my needs. The same goes for SDL. There is also http://GtkGLExt.sf.net @Christoph: I can fully understand what you mean. In an inhouse solution for a former employer I had to solve a similar problem. First attempts were using the platform bindings I mentioned above. After a year or so I switched to your idea of implementing platform specifics myself.  libapril is nice also. It is clean simple and supports newer mobile platforms.,c++ opengl window multiplatform
766167,A,What happens to pixels after passing them into glTexImage2D()? If for example I create an array of pixels like so: int *getPixels() { int *pixels = new int[10]; pixels[0] = 1; pixels[1] = 0; pixels[1] = 1; // etc... } glTexImage2D(... getPixels()); Does glTexImage2D use that reference or copy the pixels into it's own memory? If the answer is the former then should I do the following? int *p = getPixels(); glTexImage2D(... p); /* Just changed to delete[] because delete * would only delete the first element! */ delete[] p; I think you would need to call glDeleteTextures() after you have finished drawing your pixels to free the memory.  From this quote in the man page it sounds like glTexImage2D allocates its own memory. This would make sense ideally the OpenGL API would send data to be stored on the graphics card itself (if drivers/implementation/etc permitted). In GL version 1.1 or greater pixels may be a null pointer. In this case texture memory is allocated to accommodate a texture of width width and height height. You can then download subtextures to initialize this texture memory. The image is undefined if the user tries to apply an uninitialized portion of the texture image to a primitive. So yea I'd imagine there is no harm in freeing the memory once you've generated your texture. I'm not sure if that's the case on PC but on consoles texture uploading functions very often simply store pointer to texture and schedule DMA transfer for later time so you can't free memory as soon the function returned.  Yes after the call to geTexImage2D() returns it is safe to discard the data you passed to it. infact if you don't do that you'll have a memory leak like in this code: int *getPixels() { int *pixels = new int[10]; pixels[0] = 1; pixels[1] = 0; pixels[1] = 1; // etc... } glTexImage2D(... getPixels()); You pass the pointer to the buffer to the call but then the pointer is lost and most likely leaks. What you should do is store it and delete it aftet the call retuns: int *pbuf = getPixels(); glTexImage2D(... pbuf); delete[] pbuf; alternativly if the texture is of a constant size you can pass a pointer to an array that is on the stack: { int buf[10]; ... glTexImage2D(... pbuf); } Finally if you don't want to worry about pointers and arrays you can use STL: vector<int> buf; fillPixels(buf); getTexImage2D(... buf.begin()); Oops! My initial snippet used delete instead of delete[] for deleting the pixel buffer. technically buf.begin() might not work but &buf[0] is guaranteed to work. In most STL implementations the vector iterator is a pointer but it doesn't have to be. actually delete[] is what you should be using. my bad fixed now. btw this is exactly why you should use STL :),c++ opengl glteximage2d
239917,A,"Getting started with OpenGL As you can see here I'm about to start work on a 3d project for class. Do you have any useful resources/websites/tips/etc. on someone getting started with OpenGL for the first time? The project will be in C++ and accessing OpenGL via GLUT. Processing has also open gl support it's an easy way to get started.. If you don't like the editor which comes with processing you can also use it with a Java IDE such as Eclipse or Intellij.  I agree with jwfearn above. OpenGL SuperBible is the most readable interesting and understandable book for OpenGL to me. Give it a try.  Check this out http://www.videotutorialsrock.com/ with full reference material  video tutorials and sample codes.  OpenGL is extremely easy to start with especially if you're using a toolkit like GLUT or SDL. Starting from man glVertex and man glDrawElements will get you started (look at the references at the bottom for other functions). If you're on a windows system typing those commands into google yields the same results. Further down the road it's probably a good idea to check out the extensions repository. I would advise against books in this case because the (more exact) specifications are published online making that the most accurate and up-to-date resource.  NeHe is way out of date and most books are too. Check out the tutorial here: http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html . I found it really useful and it covers a modern shader-based approach. See also the wiki by madsy in #opengl on freenode: http://www.mechcore.net/w/Main_Page . The descriptions are much less approachable and more technical but it's also a good forward-looking basis and provides a lot of deeper mathematical background than the tutorials linked above.  This link has some answers to a similar question.  The Learning Modern 3D Graphics Programming series of tutorials is an extremely excellent and thorough introduction to key GL concepts and techniques. Additionally the Modern OpenGL sides by Little Grasshopper provide a summary/reference to several of the more recent OpenGL features. If anybody is interested in running the Learning Modern 3D Graphics Programming tutorials on OS X I have an Xcode+GLFW3 port here: https://github.com/rsanchezsaez/gltut-glfw (wip) I'm really impressed the section Following the Data of Learning Modern 3D Graphics Programming!  The OpenGL ""Red Book"" is the best place to start learning. The OpenGL ""Blue Book"" is documentation for the OpenGL API. There are many online resources for opengl and glprogramming.com is one of them. Check out the links section.  Beginning OpenGL Game Programming is a very good introduction even if you don't care about games. The official Programming Guide and Reference Manual are a must too. Might as well pick up a general book on computer graphics. There are plenty of online resources but I really think you should go for books even if it was the freely available old versions of the official books. NeHe's site has some interesting code samples but I don't think it's a good resource to start learning as it glosses over many details and follows a quick and dirty approach to get things running. +1 Thank you for recommending Beginning OpenGL Game Programming. I got it and its excellent.  The Red Book online - http://www.glprogramming.com/red/ - http://www.glprogramming.com/blue/ Hope you manage to go through this.. I didn't... another unfinished dream on my shelf. Keep in mind though neither of these books(especially the blue book) are up to the latest OpenGL versions the programming model with modern OpenGL is quite different from what these books teach you.  The OpenGL 4.0 Shading Language Cookbook is a gem when you start writing shaders. If you're using the OpenGL Superbible I'd recommend skipping past most of the early chapters and jumping ahead to Chapter 8 (Buffer Objects). The first half of the book teaches more about the concept of the rendering pipeline through the use of wrappers than about actual OpenGL.  check glut: http://en.wikipedia.org/wiki/OpenGL_Utility_Toolkit http://www.lighthouse3d.com/opengl/glut/index.php?1  This is the best web page(for me) to learn OpenGL http://nehe.gamedev.net/. And this is the official book http://www.amazon.com/OpenGL-Programming-Guide-Official-Learning/dp/0321335732  If you are serious about learning OpenGL you should check out the OpenGL Bootcamp at Big Nerd Ranch. The instructor has written several award winning games including the ""Big Bang Board Games"" published by Freeverse software.  I highly recommend OpenGL SuperBible Sixth Edition by Graham Sellers Richard S. Wright Jr. and Nicholas Haemel. ISBN-13: 978-0321902948 It's a one-stop-shop with both tutorial and reference. It combines some of the best aspects of both the ""Red"" and ""Blue"" books along with some great graphics background information. If your budget is limited to one book this is a great choice. I agree that was my first GL book and it was much more approachable than the Red and Blue books. Since the Red Book isn't quite up to date anymore the SuperBible is a much better book to start modern OpenGL programming.  The NeHe tutorials will get you going but as luke points out they don't really cover much background of why. Although it is WPF rather than opengl (do you have to use opengl?) Petzolds book on 3d graphics with wpf does a very good job of introducing 3d graphics and some of the maths behind it. I may be in the minority but I dislike NeHe's tutorials. Like many that's where I started but there are many *many* better resources. The learning curve with graphics APIs is steep and NeHe offers very little insight to the whys and wherefores. Please post any other suggestions here or edit my post - that's exactly what SO is for.  A German wiki about very much stuff about OpenGL and related game programming here. And of couse the official documentation for openGL 2 openGL 3 and openGL 4.",c++ opengl 3d glut
623127,A,OpenGL coordinate problem I am creating a simple 2D OpenGL application but I seem to be experiencing some camera issues. When I draw a rectangle at (2020) it is drawn at (2520) or so. When I draw it at (100 20) it is drawn at 125 or so. For some reasons everything is being shifted to the right by a few %. I have pasted a trimmed down version here http://pastebin.com/m56491c4c Is there something wrong with the way I am setting up GLUT? I know it's not my objects doing anything weird since the same thing happens when I disable them. Thanks in advance. It depends on what system you are working on but usually most windows coordinate systems start at the bottom left corner and count up to the top and to the right. In this case your gluOrth02D call would be wrong. You Have: gluOrtho2D(0 appWidth appHeight 0); Which has the top of the window mapping to the bottom vice-verse. Most of the time it's: gluOrtho2D(0 appWidth 0 appHeight); As I said it depends on the system platform your working with. I can only speak for most linux implementations. Just something else to check out just in case it affecting your bug.  You seem to be calling your glOrtho2D on your ModelView matrix. I doubt that that's the problem (since I guess in this case your Projection should be the identity) but you should still call it on your Projection matrix instead. You should also print out w and h in your resize call just to make sure that your window size is actually what you think it is (I don't know how glut works but glutInitWindowSize() may include borders which would mess things up). I've already tried setting it up in the order you suggested without any luck. I've messed around with the outputs and am fairly confident that the window size is correct. All out of ideas.  You need to set the projection matrix inside the reshape function (resize()) which also automatically solves the problem of the user resizing the window: void resize(int w int h) { glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } And then in your draw function make sure that the matrix mode is model-view: void draw() { glMatrixMode(GL_MODELVIEW); glLoadIdentity(); ... } Other problems with your code: You probably shouldn't be calling glutPostRedisplay() at the end of draw(). This is going to make your CPU run at 100%. You can instead use glutTimerFunc() to still have updates every some number of milliseconds. In processMouse() you're using wsprintf() on an array of chars: wsprintf() takes an array of wide characters (wchar_t) so you should make the local variable s of type wchar_t[] or use sprintf() and MessageBoxA() instead of wsprintf() and MessageBoxW() (to which MessageBox() expands as a macro when compiling a Unicode application which I'm assuming you're doing). You're also vulnerable to a buffer overflow -- you should use a buffer of at least 12 characters even though realistically you'll never be passed a very large x value. Finally you should also use snprintf()/wsnprintf() instead of sprintf()/wsprintf() to protect against the buffer overflow. Thanks that fixed it! Thanks for your other suggestions. I know it's messy code I just wanted a quick working sample to post. Thanks again!,c++ opengl glut
984792,A,Lighting issue in OpenGL I have trouble developing an OpenGL application. The weird thing is that me and a friend of mine are developing a 3d scene with OpenGL under Linux and there is some code on the repository but if we both checkout the same latest version that means the SAME code this happens: On his computer after he compiles he can see the full lighting model whilst on mine I have only the ambient lights activated but not the diffuse or specular ones. Can it be a problem of drivers ?(since he uses an ATi card and I use an nVIDIA one) Or the static libraries ? I repeat it is the same code compiled in different machines.. that's the strange thing it should look the same. Thanks for any help or tip given. This can very easily be a driver problem or one card supporting extensions that the other does not. Try his binaries on your machine. If it continues to fail either your drivers are whack or you're using a command not supported by your card. On the other hand if your screen looks right when using your code compiled on his machine then your static libraries have a problem. Yes you're right it is a drivers issue. I have tried the binaries on my machine and compiled the program in different machines. With the newer nVIDIA cards and Ubuntu it happens the same in all the PCs I tested.,c++ opengl lighting
859501,A,"Learning OpenGL in Ubuntu I'm trying to learn OpenGL and improve my C++ skills by going through the Nehe guides but all of the examples are for Windows and I'm currently on Linux. I don't really have any idea how to get things to work under Linux and the code on the site that has been ported for Linux has way more code in it that's not explained (so far the only one I've gotten to work is the SDL example: http://nehe.gamedev.net/data/lessons/linuxsdl/lesson01.tar.gz). Is there any other resource out there that's a bit more specific towards OpenGL under Linux? Maybe you would like to use Qt to draw the windows and widgets. Here's a tutorial based on the Nehe guides to show you how to create OpenGL images with Qt. To learn OpenGL the OpenGL Red Book is a must read. There's an online version. It has very good explanations and examples.  I'll guess that it is the compilation process that is the biggest difference initially. Here is a useful Makefile for compiling simple OpenGL apps on Ubuntu. INCLUDE = -I/usr/X11R6/include/ LIBDIR = -L/usr/X11R6/lib FLAGS = -Wall CC = g++ # change to gcc if using C CFLAGS = $(FLAGS) $(INCLUDE) LIBS = -lglut -lGL -lGLU -lGLEW -lm All: your_app # change your_app. your_app: your_app.o $(CC) $(CFLAGS) -o $@ $(LIBDIR) $< $(LIBS) # The initial white space is a tab Save this too a file called Makefile and should be in the same directory. Compile by writing make from terminal or :make from Vim. Good luck  a little update for the makefile because I found this old answers from @Millthorn and it didn't worked: you dont need to definde the include path since it's in standard lib http://stackoverflow.com/a/2459788/1059828 a minimal makefile to compile open GL could look like this: LDFLAGS=-lglut -lGL -lGLU -lGLEW -lm all: your_app http://surflab.cise.ufl.edu/wiki/Getting_Started_with_OpenGL_in_Ubuntu  The first thing to do is install the OpenGL libraries. I recommend:  freeglut3 freeglut3-dev libglew1.5 libglew1.5-dev libglu1-mesa libglu1-mesa-dev libgl1-mesa-glx libgl1-mesa-dev Once you have them installed link to them when you compile: g++ -lglut -lGL -lGLU -lGLEW example.cpp -o example In example.cpp include the OpenGL libraries like so: #include <GL/glew.h> #include <GL/glut.h> #include <GL/gl.h> #include <GL/glu.h> #include <GL/glext.h> Then to enable the more advanced opengl options like shaders place this after your glutCreateWindow Call: GLenum err = glewInit(); if (GLEW_OK != err) { fprintf(stderr ""Error %s\n"" glewGetErrorString(err)); exit(1); } fprintf(stdout ""Status: Using GLEW %s\n"" glewGetString(GLEW_VERSION)); if (GLEW_ARB_vertex_program) fprintf(stdout ""Status: ARB vertex programs available.\n""); if (glewGetExtension(""GL_ARB_fragment_program"")) fprintf(stdout ""Status: ARB fragment programs available.\n""); if (glewIsSupported(""GL_VERSION_1_4 GL_ARB_point_sprite"")) fprintf(stdout ""Status: ARB point sprites available.\n""); That should enable all OpenGL functionality and if it doesn't then it should tell you the problems.",c++ linux opengl ubuntu
1878257,A,How can I draw a cylinder that connects two points in OpenGL i have two point each point has its own X and Y value and they have the same Z value. i want a function to draw Cylinder between these two points. I think you're going to have to be a lot more specific. There are an infinite number of cylinders that contain those two points. Can you describe how you want the cylinder to be oriented with respect to the points? I suspect you want a cylinder of unit- (or user specified-) radius with those two points on its centerline at either end. But you need to be specific... For building a cylinder with two given points you need vector analysis. You are building two perpendicular vectors which are added to each point and scaled with sin/cos multiplied with the radius. It accepts all points (The old code had a bug because it missed the sqrt() for the length). Now it functions correctly and draw the cylinder with gl routines; I tested it in JOGL. For faster drawing move the firstPerpVector/secondPerpVector/points variable to a private final array field and initialize them at the beginning. Java code:  public float[] getFirstPerpVector(float x float y float z) { float[] result = {0.0f0.0f0.0f}; // That's easy. if (x == 0.0f || y == 0.0f || z == 0.0f) { if (x == 0.0f) result[0] = 1.0f; else if (y == 0.0f) result[1] = 1.0f; else result[2] = 1.0f; } else { // If xyz is all set we set the z coordinate as first and second argument . // As the scalar product must be zero we add the negated sum of x and y as third argument result[0] = z; //scalp = z*x result[1] = z; //scalp = z*(x+y) result[2] = -(x+y); //scalp = z*(x+y)-z*(x+y) = 0 // Normalize vector float length = 0.0f; for (float f : result) length += f*f; length = (float) Math.sqrt(length); for (int i=0; i<3; i++) result[i] /= length; } return result; } public void drawCylinder(GL gl float x1 float y1 float z1 float x2 float y2 float z2) { final int X = 0 Y = 1 Z = 2; // Get components of difference vector float x = x1-x2 y = y1-y2 z = z1-z2; float[] firstPerp = getFirstPerpVector(xyz); // Get the second perp vector by cross product float[] secondPerp = new float[3]; secondPerp[X] = y*firstPerp[Z]-z*firstPerp[Y]; secondPerp[Y] = z*firstPerp[X]-x*firstPerp[Z]; secondPerp[Z] = x*firstPerp[Y]-y*firstPerp[X]; // Normalize vector float length = 0.0f; for (float f : secondPerp) length += f*f; length = (float) Math.sqrt(length); for (int i=0; i<3; i++) secondPerp[i] /= length; // Having now our vectors here we go: // First points; you can have a cone if you change the radius R1 final int ANZ = 32; // number of vertices final float FULL = (float) (2.0f*Math.PI) R1 = 4.0f; // radius float[][] points = new float[ANZ+1][3]; for (int i=0; i<ANZ; i++) { float angle = FULL*(i/(float) ANZ); points[i][X] = (float) (R1*(Math.cos(angle)*firstPerp[X]+Math.sin(angle)*secondPerp[X])); points[i][Y] = (float) (R1*(Math.cos(angle)*firstPerp[Y]+Math.sin(angle)*secondPerp[Y])); points[i][Z] = (float) (R1*(Math.cos(angle)*firstPerp[Z]+Math.sin(angle)*secondPerp[Z])); } // Set last to first System.arraycopy(points[0]0points[ANZ]03); gl.glColor3f(1.0f0.0f0.0f); gl.glBegin(GL.GL_TRIANGLE_FAN); gl.glVertex3f(x1y1z1); for (int i=0; i<=ANZ; i++) { gl.glVertex3f(x1+points[i][X] y1+points[i][Y] z1+points[i][Z]); } gl.glEnd(); gl.glBegin(GL.GL_TRIANGLE_FAN); gl.glVertex3f(x2y2z2); for (int i=0; i<=ANZ; i++) { gl.glVertex3f(x2+points[i][X] y2+points[i][Y] z2+points[i][Z]); } gl.glEnd(); gl.glBegin(GL.GL_QUAD_STRIP); for (int i=0; i<=ANZ; i++) { gl.glVertex3f(x1+points[i][X] y1+points[i][Y] z1+points[i][Z]); gl.glVertex3f(x2+points[i][X] y2+points[i][Y] z2+points[i][Z]); } gl.glEnd(); }  If you cant use gluCylinder() from the GLU library (eg if you ae on OpenGL-ES) Or you have to make the sides of the cylinder from small flat segments http://stackoverflow.com/questions/1056504/how-do-you-draw-a-cylinder-with-opengles but i need a function to calculate the X and Y angles between the points and draw the cylinder from the first one to the other  Found bugs using the currently accepted answer. Ended up coming up with my own. You can do it with some simple maths: http://www.thjsmith.com/40/cylinder-between-two-points-opengl-c Put the code in your answer; don't just link to some website.  http://lifeofaprogrammergeek.blogspot.com/2008/07/rendering-cylinder-between-two-points.html Well it was 2009... ;) this uses deprecated OpenGL calls.,c++ opengl 3d
155672,A,"Starting Graphics & Games Programming (Java and maybe C++) I've always had an interest in creating my own games and now at university I have the opportunity to create some 2D and 3D games using Java and C++ for those who are that way inclined. I've never really programmed a game before let alone graphics so I'm completely new to the area. After a quick trip to the library today I came across very little information on starting 2D game development or even graphics programming in Java or C++. I can program in Java at a reasonable level but I have never touched C++. Can someone recommend any good books on starting Graphics Programming in Java or C++? I have no C++ experience but I've programmed in C and Java and feel reasonably comfortable in both. Should I take the jump and dive into C++ when important marks are at stake? I hear a lot of good things about C++ as far as Games Programming goes so I'm unsure as to what is the best option for me. I'm looking to write a 2D game in my own time for a bit of fun before I start getting into the heavy work at university. Can anyone recommend some good resources for those interested in writing their own games using OpenGL and Java/C++. For those who have followed my other questions here I am terrible at Maths and hold very little knowledge of even the foundations. Is this going to be a serious problem for me? If I were to need Math knowledge what do you recommend I brush up on? I apologise if my questions seem a bit vague as I'm a complete newbie when it comes to a lot of these topics. Any help you could provide would be much appreciated. EDIT: I've already checked out the question titled ""game programming"" but have found it to not really cater for my specific questions. Both. I've always had an interest in Graphics Programming but now that I have a class in it I'm forced to take action. As far as I am aware at university we'll be using Java and OpenGL (my university is a Java School so we only get support for that) but I am interested in learning C++ as well. Are you doing graphics at uni or just in your spare time? If you're doing it at uni then what do they use? C++ OpenGL and (usually) GLUT seems to be the standard language&APIs I've observed. It's probably useful to follow that path & knowing basics will give you more time to pick up the bonus marks Similar question here that you might find useful: [http://stackoverflow.com/questions/124322/game-programming#124427](http://stackoverflow.com/questions/124322/game-programming#124427) I am currently doing exactly the same thing as you are now at university but I have nearly finished studying that part of my course! How bizzare!?! Mite be able to send you some extra lecture material aswell if it helps? On top of books I think you should check out these sites: NeHe Productions Video Tutorials Rock Opengl Programming Guide: The Official Guide to Learning Opengl  Version 2.1 Addison Wesley 2007 Here are the books that I would also check out: Hearn & Baker Computer Graphics with OpenGL Prentice-Hall 2003. I'd also seriously consider using C++ as your programming language. From what I have been taught and what I have been reading around C/C++ language branch are the most used by the game industry for example I heard STEAM who made the half life series use C++ and some others. Here is a free C++ book on the internet: Thinking in C++ 2nd ed. Volume 1 ©2000 by Bruce Eckel Here is what I learnt from: C++ in 24 Hours Sams Teach Yourself  I am going through a similar process to you at the moment. I found the following site helpful it has a great tutorial (space invaders in Java): http://www.cokeandcode.com/node/6 You can get the source code and mess around with it. You will probably benefit from an IDE if you don't already have a favorite you could try Netbeans (netbeans.org) which is free and I think it's pretty good. As for books this one is OK (it's java-centric): http://www.amazon.com/Killer-Game-Programming-Andrew-Davison/dp/0596007302 I personally decided to use Java for games (for now) because I am very comfortable with Java and far less so with C++. But you will find that most people use C++ for commercial games. Actually I started with pygame (python games framework) which is also nice to get started especially if you know python. That tutorial is great! I've just downloaded the source run through with it and it all looks a lot easier than I had anticipated. Thanks for the answer!  Something to help you get into OpenGL is GLUT - the OpenGL Utility Toolkit. It takes care of a lot of the lower-level setup that's a pain to deal with if you're starting an OpenGL project from scratch. It makes it easier to jump right in and start putting stuff on the screen. GLUT hasn't been updated for awhile but you can also try FreeGLUT which is a newer open source replacement of GLUT and is included by several linux distributions out of the box.  I know that you specifically asked for Java or C++ but I have found that PyGame (based on SDL) has been a really good primer for me on basic game techniques. They have a couple of comprehensive tutorials and examples and python is a very clear and easy to pick up language. I would also suggest the Xbox 360 option as well if you already have one. That is what I will be moving onto next so that I can learn all the 3D stuff that PyGame is not so great at. As for math you could probably get by without real skills for simple 2D games. If you intend on having any kind of physics simulation then are going to have to actaully learn some real math. Even basic 2D games like riding a motorcycle over jumps involve a lot of physics math to make it work/fun  Here are a few books that I used when writing OpenGL code in C++: Fundamentals of Computer Graphics OpenGL SuperBible OpenGL Red Book OpenGL Primer You might check out the MIT OpenCourse on computer graphics too. It might help supplement your development. Best of luck!  Short answer: Don't write in C++ !!! You'll spend more time futzing with the language than actually learning about games physics collisions etc. Get a copy of Python and PyGame. It's easy to get started but you'll actually learn heaps. Having learnt Java you'll be amazed how much simpler Python is for getting the same things done. Once you're comfortable with your skill set then look at Panda3D. It's what's used at Pixar / Disney. If you choose to get your hands dirty with C++ at this stage then diving into Panda3D is going to be good. All the major studios use Python as does Google. If you end up specialising in C++ you'll become an engine-room programmer. Very necessary but not as glamorous. Oh you mentioned grades etc. Unless you need to take a course in C++ programming you'd be wasting precious time and energy on it. I don't know that any major studios use Python for much game code other than EVE Online which uses it on its massive server backend. Certainly for any console platform I've found Python to be unacceptably memory-bulky.  I would recommend starting with C++ and SFML. C++ has the largest available library of existing code and every example you find will probably be C/C++ oriented. SFML is a windowing and graphics library (comparable to SDL if you've heard of that) done using object-oriented design that fits into C++ better. It'll allow you to get up and running with an OpenGL window right off the bat and the documentation is pretty good as well.  I have this book: Beginning C++ Through Game Programming. It might seem trivial at first but it teaches you a-lot as you go through it. There is no GUI based programming in this book just the console. Which is good to an extent if you want to see how an entire ""story"" of a game can come together. You can also check out Gamedev.net they have a vast amount of resources and articles to get you started. Good luck. :) Thank you for the link to the book. I'm not very confident in my programming abilities as of yet and this book seems to be perfect for my needs. I'll be sure to check it out very soon.  Game programming especially where graphics are concerned involves a fair amount of math. You'll at least want to have a basic familiarity with vectors and matrices specifically with regard to representing rotation translation and scaling transformations. To that end I recommend Geometric Tools for Computer Graphics. A course in linear algebra wouldn't hurt as well although that's probably already in your curriculum. As far as game programming in Java I recommend taking a look at jMonkeyEngine an open source game engine with all sorts of fun example code to get you started. It was originally based on the engine presented in the book 3D Game Engine Design (same author as Geometric Tools above) which is another good source of information about game programming in 3D. There's also C++ code and papers on various topics in 3D graphics at that book's official site.  Honnestly go buy a xbox 360 and download the free XNA SDK package to get started. Update your maths skills and modify some indie games in their sample kit. Build a small game invite some friends around for some beers and have FUN! Your going to learn a lot faster than touching C/C++/Java and also too your going to have fun well doing it. Interesting answer. At university I am told to use Java or another language if we're feeling adventurous but the Xbox 360 idea does sound like it'll be good fun when the coursework is out of the way and if I decide to keep making games. Thanks for answering! The reason why i recommend the Xbox 360 approach is because its fast has fast turn around times and is just fun! At this stage you do not want to be bogged down with the problems with large scale C++ development. This is were most young people dont get past. Why is turn around time so important? In our game development studio the turn around time from compile to run is about 5-10 minutes depending on the content. So if you make a mistake and your new its going to take you 5 minutes to find out. This is why turn around time is important at the start.",java c++ opengl graphics
1823927,A,"Simulated time in a game loop using c++ I am building a 3d game from scratch in C++ using OpenGL and SDL on linux as a hobby and to learn more about this area of programming. Wondering about the best way to simulate time while the game is running. Obviously I have a loop that looks something like: void main_loop() { while(!quit) { handle_events(); DrawScene(); ... SDL_Delay(time_left()); } } I am using the SDL_Delay and time_left() to maintain a framerate of about 33 fps. I had thought that I just need a few global variables like int current_hour = 0; int current_min = 0; int num_days = 0; Uint32 prev_ticks = 0; Then a function like : void handle_time() { Uint32 current_ticks; Uint32 dticks; current_ticks = SDL_GetTicks(); dticks = current_ticks - prev_ticks; // get difference since last time // if difference is greater than 30000 (half minute) increment game mins if(dticks >= 30000) { prev_ticks = current_ticks; current_mins++; if(current_mins >= 60) { current_mins = 0; current_hour++; } if(current_hour > 23) { current_hour = 0; num_days++; } } } and then call the handle_time() function in the main loop. It compiles and runs (using printf to write the time to the console at the moment) but I am wondering if this is the best way to do it. Is there easier ways or more efficient ways? ""just need a few global variables"" - don't. There are nearly always better alternatives e.g. a `struct` wich contains the information and gets passed around. What exactly are you trying to do? I haven't tried SDL but will the ticks be dependent on the processor rate or is it always locked to milliseconds? You want to have a simulated minute every 30 seconds? SDL_GetTicks -- Gets the number of milliseconds since SDL library initialization. Based on this I am trying to simulate a minute every 30 seconds. Global state like time which is used by everything everywhere really ought to be in a global variable. Otherwise you'd have to pass your ""current time and framecount"" struct to every single function in the entire game. That assumes the entire game needs the current time and framecount which I'm pretty sure it doesn't. In my experience I get the time in the main loop and pass it to the update function that's it. Few places really need this data. I have the opposite experience. We use gpGlobals->currenttime absolutely everywhere: in the particle systems AI behavior speech special effects trigger hysteresis weapon fire rates (and projectile movement) animation scripting on and on. I grepped just now and found 4707 uses in one game DLL alone. Other than the issues already pointed out (you should use a structure for the times and pass it to handle_time() and your minute will get incremented every half minute) your solution is fine for keeping track of time running in the game. However for most game events that need to happen every so often you should probably base them off of the main game loop instead of an actual time so they will happen in the same proportions with a different fps.  I am not a Linux developer but you might want to have a look at using Timers instead of polling for the ticks. http://linux.die.net/man/2/timer_create EDIT: SDL Seem to support Timers: SDL_SetTimer If you look at the handle_time() function it's clear that he's not just counting minutes and hours for the fps ( which is handled by DrawScene()). As far as I understand he wanted to keep a ""clock"" in the game. I accept your comment performance in game development and I won't argue with you here as it's not my field.But as an idea for a different implementation I don't think there was anything fundamentally wrong with my answer While good advice in general this is not a good idea for game development. Not even for displaying the time in the game like he does ? Do you actually use polling instead ? zildjohn01: why are timers are a bad idea for a game? I am assuming they have some sort of performance impact? This is bad because you don't want a game to have a callback that does something every (n) milliseconds -- some frames might finish a little ahead of 33 and some others might lag behind. If you have a frame that takes 37 milliseconds and then set the next frame for 33 after that then you'll be effectively slowing down time as your two frames (representing 66ms of game time) would take place over 70ms of real time. It's much better to have the game loop running at full speed all the time -- never sleeping or pausing -- and use a high-resolution realtime clock to mark constant timesteps. I never suggested handling the frames with a timer. He was asking about Time. That's why he wanted the time: ""I am using the SDL_Delay and time_left() to maintain a framerate of about 33 fps.""  One of Glenn's posts you will really want to read is Fix Your Timestep!. After looking up this link I noticed that Mads directed you to the same general place in his answer.  I've mentioned this before in other game related threads. As always follow the suggestions by Glenn Fiedler in his Game Physics series What you want to do is to use a constant timestep which you get by accumulating time deltas. If you want 33 updates per second then your constant timestep should be 1/33. You could also call this the update frequency. You should also decouple the game logic from the rendering as they don't belong together. You want to be able to use a low update frequency while rendering as fast as the machine allows. Here is some sample code: running = true; unsigned int t_accum=0lt=0ct=0; while(running){ while(SDL_PollEvent(&event)){ switch(event.type){ ... } } ct = SDL_GetTicks(); t_accum += ct - lt; lt = ct; while(t_accum >= timestep){ t += timestep; /* this is our actual time in milliseconds. */ t_accum -= timestep; for(std::vector<Entity>::iterator en = entities.begin(); en != entities.end(); ++en){ integrate(en (float)t * 0.001f timestep); } } /* This should really be in a separate thread synchronized with a mutex */ std::vector<Entity> tmpEntities(entities.size()); for(int i=0; i<entities.size(); ++i){ float alpha = (float)t_accum / (float)timestep; tmpEntities[i] = interpolateState(entities[i].lastState alpha entities[i].currentState 1.0f - alpha); } Render(tmpEntities); } This handles undersampling as well as oversampling. If you use integer arithmetic like done here your game physics should be close to 100% deterministic no matter how slow or fast the machine is. This is the advantage of increasing the time in fixed time intervals. The state used for rendering is calculated by interpolating between the previous and current states where the leftover value inside the time accumulator is used as the interpolation factor. This ensures that the rendering is is smooth no matter how large the timestep is. This is essentially what we do (in our commercial product). We run our game logic at a constant timestep of 10hz but let the rendering thread run as fast as possible and draw frames as quickly as the GPU is ready for them. This means that physics and AI and entity logic isn't impacted by render speed (otherwise the world would go into slow-time when the rendering bogged down). The rendering isn't on a constant timestep it just goes as fast as it can (up to 60hz of course). @Crashworks : Exactly. You don't care about a fixed timestep for the rendering. It would make the rendering choppy. One can make the case that there *should* be a constant rendering timestep too since a variable framerate feels choppier than a consistent one and because the monitor is updating at 30/60hz (or 25/50 for PAL) so any frame that isn't aligned with a vsync interval will ""tear"" in the middle of the screen. It's very subjective though so we made ""wait for vsync"" a customer-settable option. Hah NTSC vs PAL differences was actually the reason why SEGA Genesis games for European consoles ran faster than North American ones. So even game consoles suffered from this issue though less than PC games from the same era. @Mads In which part of the above code (integrate or interpolate) is collision detection done?",c++ opengl sdl
1270517,A,"Porting project to my laptop results in a blank screen So I'm making something in openGL using SDL. I'm about to take a long flight and I can't seem to get the project to work on my laptop. I've used SDL on my laptop before so I'm left thinking it is openGL's fault. The laptop is on win xp pro and has an intel 945 graphics ""card."" I've tried updating the drivers but to no avail. The images I'm using can't be the problem because I have it coded so the program closes if it can't locate the file. Also I get no errors at all when compiling it just creates the window and instead of all my images I get white. Just white. Any ideas? Please I don't want to be on this 5 hr flight and go stir crazy =/ Do the SDL NeHe (http://nehe.gamedev.net/lesson.asp?index=02) demos work? You might need to provide a bit more info but at a guess I'd say your textures are invalid. OpenGL draws white when there is a texture problem. Possible reasons are... Image size is bigger than the max texture size of the graphics chip Image isn't power of 2 and the card doesn't support rectangular textures. You have run out of texture memory. Your texture env isn't supported by the graphics chip eg. unsupported format. You aren't drawing what you think you are drawing eg a white quad on a white background in the position you think you are drawing it ie you're looking in the wrong direction. Programmer error. Try drawing a single 128x128 texture on a single quad with a glcolor of purple and a clear colour of orange. This will eliminate most of the above problems and give you something to debug. It's been a while but maybe this can help someone else. I knew about the power of 2 however it slipped my mind that my desktop has a much new/better gfx card than my laptop does. So while my desktop could compensate for the non-power-of-2-sizes my laptop's gfx card could not.",c++ gui opengl sdl
721998,A,Why does my colored cube not work with GL_BLEND? My cube isn't rendering as expected when I use GL_BLEND. glEnable(GL_CULL_FACE); glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE); I'm also having a similar problem with drawing some semi-opaque vertices in front which could well be related. Related: Why do my semi-opaque vertices make background objects brighter in OpenGL? Here's what it's supposed to look like: And here's what it actually looks like: Please see the code used to create the colored cube and the code used to actually draw the cube. The cube is being drawn like so: glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glPushMatrix(); glLoadIdentity(); // ... do some translation rotation etc ... drawCube(); glPopMatrix(); // ... swap the buffers ... Did you get the first picture from your program? It looks like you have lighting enabled on the second one try with a glShadeModel( GL_FLAT ) before drawing Hmm no this makes the sides of the cube solid colours but only 3 of them...  You could try disabling all lighting before drawing the cube: glDisable(GL_LIGHTING);  This has me stomped. What it looks like is that some vertices have some alpha values that are non-opaque. However the code you posted has all 1. for alpha. So... in order to debug more did you try to change your clear color to something non-black ? Say green ? From the code I doubt lighting is turned on since no normals were specified. Last comment offtopic... You should really not use glBegin/glEnd (2 function calls per vertex + 2 per primitive is really not a good usage of the recent developments in OpenGL). Try glDrawElements with QUAD_LIST or even better TRIANGLE_LIST. You already have the data nicely laid out for that.,c++ opengl blend
1191093,A,"I'm seeing artifacts when I attempt to rotate an image This is the before: znd after: EDIT:: Now that I look at imageshack's upload the artifacts are diminished a great deal.. but trust me they are more pronounced than that. I don't understand why this is happening. Imageshack uploads them to jpg but in my program they are in the image folder as .tif (The reason for .tif is because I couldn't get ANY other image to maintain their transparent parts). But anyways these artifacts follow the original top of the image as it rotates anywhere except the original. Here's part of my code that loads the image GLuint texture; GLenum texture_format; GLint nofcolors; GLfloat spin; bool Game::loadImage() { SDL_Surface * surface; // this surface will tell us the details of the image if ( surface = SM.load_image(""Images/tri2.tif"") ) { //get number of channels in the SDL surface nofcolors = surface->format->BytesPerPixel; //contains an alpha channel if ( nofcolors == 4 ) { if ( surface->format->Rmask == 0x000000ff ) texture_format = GL_RGBA; else texture_format = GL_BGRA; } else if ( nofcolors == 3 ) //no alpha channel { if ( surface->format->Rmask == 0x000000ff ) texture_format = GL_RGB; else texture_format = GL_BGR; } // Have OpenGL generate a texture object handle for us glGenTextures( 1 &texture ); // Bind the texture object glBindTexture( GL_TEXTURE_2D texture ); // Set the texture’s stretching properties glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR ); glTexParameteri( GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR ); glTexImage2D( GL_TEXTURE_2D 0 nofcolors surface->w surface->h 0 texture_format GL_UNSIGNED_BYTE surface->pixels ); glEnable(GL_TEXTURE_2D); glEnable(GL_BLEND); glBlendFunc(GL_ONE GL_ONE_MINUS_SRC_ALPHA); } else { SDL_Quit(); return false; } // Free the SDL_Surface only if it was successfully created if ( surface ) { SDL_FreeSurface( surface ); return true; } else return false; } void Game::drawImage() { // Clear the screen before drawing glClear( GL_COLOR_BUFFER_BIT ); glTranslatef( float(S_WIDTH/2) float(S_HEIGHT/2) 0.0f ); glRotatef( spin 0.0 0.0 1.0 ); // Bind the texture to which subsequent calls refer to glBindTexture( GL_TEXTURE_2D texture ); glBegin( GL_QUADS ); { // Top-left vertex (corner) glTexCoord2i( 0 0 ); glVertex3f( -64 0 0 ); // Top-right vertex (corner) glTexCoord2i( 1 0 ); glVertex3f( 64 0 0 ); // Bottom-right vertex (corner) glTexCoord2i( 1 1 ); glVertex3f( 64 128 0 ); // Bottom-left vertex (corner) glTexCoord2i( 0 1 ); glVertex3f( -64 128 0 ); } glEnd(); glLoadIdentity(); SDL_GL_SwapBuffers(); } lol when I loaded this page I thought your artifacts were dust on my screen and I tried wiping it off. Try saving as PNG? PNG format won't load transparency correctly in my program. Nor will anything else except TIF. It is driving me insane.. Looks like the texture is set to GL_WRAP. Try GL_CLAMP_TO_EDGE instead. Yep it's definitely wrapping. I just started OpenGL like a day or two ago I don't see any place where I typed GL_WRAP... so where do I put the clamp to edge part? Insert the following right after you set the min/mag filters: glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_CLAMP_TO_EDGE); Ah that worked thanks a lot. Now I gotta figure out this annoying (lack of) transparency in images.  In Game::loadImage after your glBindTexture call: glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_CLAMP_TO_EDGE); Your current setting is GL_REPEAT which is the OpenGL default.",c++ gui opengl
789904,A,"OpenGL Color Matrix How do I get the OpenGL color matrix transforms working? I've modified a sample program that just draws a triangle and added some color matrix code to see if I can change the colors of the triangle but it doesn't seem to work.  static float theta = 0.0f; glClearColor( 1.0f 1.0f 1.0f 1.0f ); glClearDepth(1.0); glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glPushMatrix(); glRotatef( theta 0.0f 0.0f 1.0f ); glMatrixMode(GL_COLOR); GLfloat rgbconversion[16] = { 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f 0.0f }; glLoadMatrixf(rgbconversion); glMatrixMode(GL_MODELVIEW); glBegin( GL_TRIANGLES ); glColor3f( 1.0f 0.0f 0.0f ); glVertex3f( 0.0f 1.0f  0.5f); glColor3f( 0.0f 1.0f 0.0f ); glVertex3f( 0.87f -0.5f 0.5f ); glColor3f( 0.0f 0.0f 1.0f ); glVertex3f( -0.87f -0.5f 0.5f ); glEnd(); glPopMatrix(); As far as I can tell the color matrix I'm loading should change the triangle to black but it doesn't seem to work. Is there something I'm missing? It looks like you're doing it correctly but your current color matrix sets the triangle's alpha value to 0 as well so while it is being drawn it does not appear on the screen. I added a glEnable(GL_BLEND); but it still doesn't affect the picture. It's still a red/green/blue triangle. It appears that the color matrix is having no effect. Actually the triangle is still drawn in its original red green and blue so I don't believe the color matrix is affecting the alpha component. the alpha component will only affect the final image if transparency is enabled which it apparently is not.  ""Additionally if the ARB_imaging extension is supported GL_COLOR is also accepted."" From the glMatrixMode documentation. Is the extension supported on your machine? As far as I know ARB_imaging is enabled but how do I find out? I'm using VS 2008. I've run the OpenGL Extensions Viewer and it does say that GL_ARB_imaging is a valid extension on my machine.  I have found the possible problem. The color matrix is supported by the ""Image Processing Subset"". In most HW it was supported by driver.(software implementation) Solution: Add this line after glEnd(): glCopyPixels(00 getWidth() getHeight()GL_COLOR); It's very slow....  The color matrix only applies to pixel transfer operations such as glDrawPixels which aren't hardware accelerated on current hardware. However implementing a color matrix using a fragment shader is really easy. You can just pass your matrix as a uniform mat4 then mulitply it with gl_FragColor",c++ opengl colors
525227,A,"Console menu updating OpenGL window I am making an application that does some custom image processing. The program will be driven by a simple menu in the console. The user will input the filename of an image and that image will be displayed using openGL in a window. When the user selects some processing to be done to the image the processing is done and the openGL window should redraw the image. My problem is that my image is never drawn to the window instead the window is always black. I think it may have to do with the way I am organizing the threads in my program. The main execution thread handles the menu input/output and the image processing and makes calls to the Display method while a second thread runs the openGL mainloop. Here is my main code: #include <iostream> #include <GL/glut.h> #include ""ImageProcessor.h"" #include ""BitmapImage.h"" using namespace std; DWORD WINAPI openglThread( LPVOID param ); void InitGL(); void Reshape( GLint newWidth GLint newHeight ); void Display( void ); BitmapImage* b; ImageProcessor ip; int main( int argc char *argv[] ) { DWORD threadID; b = new BitmapImage(); CreateThread( 0 0 openglThread NULL 0 &threadID ); while( true ) { char choice; string path = ""TestImages\\""; string filename; cout << ""Enter filename: ""; cin >> filename; path += filename; b = new BitmapImage( path ); Display(); cout << ""1) Invert"" << endl; cout << ""2) Line Thin"" << endl; cout << ""Enter choice: ""; cin >> choice; if( choice == '1' ) { ip.InvertColour( *b ); } else { ip.LineThinning( *b ); } Display(); } return 0; } void InitGL() { int argc = 1; char* argv[1]; argv[0] = new char[20]; strcpy( argv[0] ""main"" ); glutInit( &argc argv ); glutInitDisplayMode( GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH); glutInitWindowPosition( 0 0 ); glutInitWindowSize( 800 600 ); glutCreateWindow( ""ICIP Program - Character recognition using line thinning Hilbert curve and wavelet approximation"" ); glutDisplayFunc( Display ); glutReshapeFunc( Reshape ); glClearColor(0.00.00.01.0); glEnable(GL_DEPTH_TEST); } void Reshape( GLint newWidth GLint newHeight ) { /* Reset viewport and projection parameters */ glViewport( 0 0 newWidth newHeight ); } void Display( void ) { glClear (GL_COLOR_BUFFER_BIT); // Clear display window. b->Draw(); glutSwapBuffers(); } DWORD WINAPI openglThread( LPVOID param ) { InitGL(); glutMainLoop(); return 0; } Here is my draw method for BitmapImage: void BitmapImage::Draw() { cout << ""Drawing"" << endl; if( _loaded ) { glBegin( GL_POINTS ); for( unsigned int i = 0; i < _height * _width; i++ ) { glColor3f( _bitmap_image[i*3] / 255.0 _bitmap_image[i*3+1] / 255.0 _bitmap_image[i*3+2] / 255.0 ); // invert the y-axis while drawing glVertex2i( i % _width _height - (i / _width) ); } glEnd(); } } Any ideas as to the problem? Edit: The problem was technically solved by starting a glutTimer from the openglThread which calls glutPostRedisplay() every 500ms. This is OK for now but I would prefer a solution in which I only have to redisplay every time I make changes to the bitmap (to save on processing time) and one in which I don't have to run another thread (the timer is another thread im assuming). This is mainly because the main processing thread is going to be doing a lot of intensive work and I would like to dedicate most of the resources to this thread rather than anything else. You need to make OpenGL calls on the thread in which context was created (glutInitDisplayMode). Hence glXX calls inside Display method which is on different thread will not be defined. You can see this easily by dumping the function address hopefully it would be undefined or NULL.  It sounds like the 500ms timer is calling Display() regularly after 2 calls it fills the back-buffer and the front-buffer with the same rendering. Display() continues to be called until the user enters something which the OpenGL thread never knows about but since global variable b is now different the thread blindly uses that in Display(). So how about doing what Jesse Beder says and use a global int call it flag to flag when the user entered something. For example: set flag = 1; after you do the b = new BitmapImage( path ); then set flag = 0; after you call Display() from the OpenGL thread. You loop on the timer but now check if flag = 1. You only need call glutPostRedisplay() when flag = 1 i.e. the user entered something. Seems like a good way without using a sleep/wake mechanism. Accessing global variables among more than one thread can also be unsafe. I think the worst that can happen here is the OpenGL thread miss-reads flag = 0 when it should read flag = 1. It should then catch it after no more than a few iterations. If you get strange behavior go to synchronization. With the code you show you call Display() twice in main(). Actually main() doesn't even need to call Display() the OpenGL thread does it.  Your problem may be in Display() at the line b->Draw(); I don't see where b is passed into the scope of Display(). b is declared globally just above main() Yes I see it now in the morning light.  I've had this problem before - it's pretty annoying. The problem is that all of your OpenGL calls must be done in the thread where you started the OpenGL context. So when you want your main (input) thread to change something in the OpenGL thread you need to somehow signal to the thread that it needs to do stuff (set a flag or something). Note: I don't know what your BitmapImage loading function (here your constructor) does but it probably has some OpenGL calls in it. The above applies to that too! So you'll need to signal to the other thread to create a BitmapImage for you or at least to do the OpenGL-related part of creating the bitmap. That's not entirely accurate the requirement is that the OpenGL context can only be _current_ to any single thread. The context can however be created in a different thread than which it is current. This is useful for offscreen rendering for example. Just to clarify the problem in this case is definitely the multithreaded rendering though since the OP is making gl calls from different threads using the same context or rather a non-existent context (in the main thread). Re comment #1 that's true but I thought it was irrelevant (and unnecessary complication) for this particular example.  A few points: Generally if you're going the multithreaded route it's preferable if your main thread is your GUI thread i.e. it does minimal tasks keeping the GUI responsive. In your case I would recommend moving the intensive image processing tasks into a thread and doing the OpenGL rendering in your main thread. For drawing your image you're using vertices instead of a textured quad. Unless you have a very good reason it's much faster to use a single textured quad (the processed image being the texture). Check out glTexImage2D and glTexSubImage2D. Rendering at a framerate of 2fps (500ms as you mentioned) will have negligible impact on resources if you're using an OpenGL implementation that is accelerated which is almost guaranteed on any modern system and if you use a textured quad instead of a vertex per pixel.",c++ multithreading opengl console
679113,A,"Trouble porting OpenGL app to Windows I am trying to move an OpenGL app to Windows. It was my understanding that Windows had a decent OpenGL implementation. But I'm starting to think that it doesn't... Specifically I use array buffers and glDrawArrays. When I tried to compile my code in Visual Studio 2008 Pro I received the following errors: vertexbuffers.cpp(31) : error C3861: 'glGenBuffers': identifier not found vertexbuffers.cpp(32) : error C2065: 'GL_ARRAY_BUFFER' : undeclared identifier vertexbuffers.cpp(32) : error C3861: 'glBindBuffer': identifier not found vertexbuffers.cpp(33) : error C2065: 'GL_ARRAY_BUFFER' : undeclared identifier vertexbuffers.cpp(33) : error C2065: 'GL_STATIC_DRAW' : undeclared identifier vertexbuffers.cpp(33) : error C3861: 'glBufferData': identifier not found When I examined <GL\gl.h> (contained in C:\Program Files\Microsoft SDKs\Windows\v6.0A\Include\gl) I saw: /* ClientArrayType */ /* GL_VERTEX_ARRAY */ /* GL_NORMAL_ARRAY */ /* GL_COLOR_ARRAY */ Update but it would seem that those contants get defined elsewhere. How am I supposed to generate buffers if I don't have access to those functions? The documentation doesn't say that those array types are disabled. How do I get access to the real implementation on OpenGL on Windows? Could you past the specific errors? It is possible that those comments are for documentation and that things are properly defined elsewhere. Yes I jumped the gun with those functions. My real errors come from the Buffer generation functions. Sorry for the confusion I have edited the question. Thanks for posting those it helped me understand and fix up my answer. :) The #defines are commented out in the header file whenever they would otherwise be repeated. Look at line 1054 of gl.h: /* vertex_array */ #define GL_VERTEX_ARRAY 0x8074 If this #define is actually missing then you should probably replace the file with a fresh copy. If you look at the documentation for glGenBuffers you will see that it is only available in OpenGL 1.5 and higher. The header file for Windows only comes with OpenGL 1.2 and you should use the extension mechanism to access the newer functionality. If you call wglGetProcAddress with the function name e.g. void (__stdcall *glGenBuffers)(GLsizeiGLuint*) = wglGetProcAddress(""glGenBuffers""); then you have a pointer to the function. You have better eyes than me. Thank you for the reference. Unfortunately I still have errors revolving around glBindBuffer. edited after i looked at the errors you were recieving. unfortunately the header is 1.2 only but there are easy fixes. :)  Microsoft's support for OpenGL stretches only as far as OpenGL-1.1 up to Windows XP and OpenGL-1.4 starting with Vista. Any OpenGL functionality beyond those must be delivered and supported by the installable client driver (ICD) i.e. the GPU driver's OpenGL implmenentation. To access the advanced functionality OpenGL provides the so called Extension System formed by wglGetProcAddress which is kind of like GetProcAddress for DLLs but gives access to functions of the OpenGL implementation (=driver). To make things easier nice wrapper libraries like GLEW have been developed which do all the grunt work initializing all the available OpenGL extensions providing them to the end user.  It would seem that the buffer functions are only available on Windows as extension methods. OpenGL provides glext.h that declares pointers to all of these functions. It is then up to my app to use wglGetProcAddress to get pointers to the functions. For example: PFNGLGENBUFFERSPROC myglBindBuffers = (PFNGLGENBUFFERSPROC)wglGetProcAddress(""glGenBuffersARB""); Thankfully I only have to do it for about 4 functions. Unfortunately I now have to add platform-dependent code to my app. +1 for the sensible way to find these type definitions - thanks!  You might give GLEW a shot: http://glew.sourceforge.net/ I'm pretty sure I used it at some time in the past and makes this sort of thing a little easier and more portable. Great link thank you very much.",c++ windows visual-studio opengl
1854575,A,"glBitmap() without GL_COLOR_INDEX Is it somehow possible to get glBitmap() to draw a GL_RGBA bitmap? glBitmap() is a lot quicker than glDrawPixels() but perhaps that has to do with that the format is GL_COLOR_INDEX instead of GL_RGBA? I'm running my glDrawPixels() in a display list; is there perhaps some smart way to speed it up? From the documentation: ""A bitmap is a binary image"" - Here a ""binary image"" simply means an image in which every pixel has exactly two possible colors which map to ""transparent"" and ""the current raster color"". You can't paint anything else using this function. Some other things you can try to achieve the same effect possibly with better performance: Drawing a screen-aligned quad with a texture Drawing a textured point sprite using texture coordinate replacement",c++ performance opengl
1370683,A,Opengl Selective glClipPlane I have a scene drawn in openGL (openGl 1.1 win32). I use glClipPlane to hide foreground objects to allow the user to see/edit distance parts. The selection is done natively without using openGL. But the glClipPlane applies to all openGL elements - coordinate icons gridlines etc and even elements drawn in gluOrtho2D on top - scale bars selection boxes etc. Is there anyway to selective override the clipplanes to allow these elements to be drawn while clipping the main scene? Isn't surrounding only the objects you want to hide with glEnable(GL_CLIP_PLANE); and glDisable(GL_CLIP_PLANE); enough? Yes it would have been even better if I disabled the plane with the same GL_CLIP_PLANEi that I enabled it with ! It's funny how after starring at it for hours you spot the error just after you explain it to someone else.,c++ opengl graphics
1975778,A,"OpenGL antialiasing isn't working I'm using the following code in order to antialias only the edges of my polygons: glHint(GL_POLYGON_SMOOTH_HINT GL_NICEST); glEnable(GL_POLYGON_SMOOTH); But it doesn't work. I can force enable antialiasing by the nvidia control panel and it does antialias my application polygons. With the code above I even enabled blending but it has no effect. Also the rendering code shouldn't be changed since the nvidia control panel can turn it on and it certainly cant modify my rendering code it must be some on/off flag. What is it? I've heard of ""multisampling"" but I don't need that. Edit: the nvidia control panel setting is ""application controlled"" when it doesn't work. do you create your render context with multisampling? that's what nvidia's control panel changes. It depends on your window system / framework usually there is a 'samples' value you can set to 4 or 8 somewhere. In windows it goes into the pixel format struct. Have you got the Antialiasing Settings"" in the nVidia control panel set to ""Application-Controlled""? Most likely your hardware does not support it. Not all OpenGL implementations support antialiased polygons; see the OpenGL FAQ. I've definitely run into this problem before on a first-generation MacBook -- its GPU the Intel GMA 950 does not support antialiased polygons. geforce 8800GTS should support though  It may be that your glEnable call is after the glHint call. nope i tried both ways. Interesting I'm unable to replicate what you're seeing with any of my code. I've got a hunch the NVidia Control Panel is not being your friend.  You need to ask for a visual/pixelformat with support for multisampling. This is an attribute in the attribute list you pass to glXChooseFBConfig when using GLX/XLib and wglChoosePixelformatARB when using the Win32 API. See my post here: http://stackoverflow.com/questions/1513811/getting-smooth-big-points-in-opengl/1513979#1513979  Try enabling blending glBlendFunc(GL_SRC_ALPHA_SATURATE GL_ONE); glEnable(GL_BLEND); glEnable(GL_POLYGON_SMOOTH); Also following article might help http://www.edm2.com/0603/opengl.html",c++ opengl
536314,A,Texturing Spheres with Cubemaps (not reflection maps) I want to texture a sphere with a cube map. So far my research has thrown up many many results on Google involving making OpenGL auto generate texture coordinates but I want to generate my own coordinates. Given an array of coordinates comprising the vertexes of an imperfect sphere (height mapped but essentially a sphere) centered on 000 how would one generate texture coordinates for a cube map? Are you doing this via GLSL? In that case textureCube accepts a vec3 as texture coordinate which is a unit vector on a sphere. In your case you would take the coordinate of your fragment with respect to the center of the sphere normalize it and pass it as a coordinate. No need to worry about the internal representation as six two-dimensional textures. uniform samplerCube cubemap; varying vec3 pos; // position of the fragment w.r.t. the center of the sphere /* ... */ vec4 color = textureCube(cubemap normalize(pos).stp); It works like that also in fixed-pipeline OpenGL. By the way here is how the coordinates are used internally: the largest coordinate in absolute value is used to select which one of the six textures is read from (the sign selects positive or negative). The other two coordinates are used to lookup the texel in the selected map after being divided by the largest coordinate. *slaps head* thankyou! I'm working in plain C++ atm but eventually moving to GLSL once I know more about it,c++ opengl texture-mapping
259890,A,"OpenGL glDrawPixels on dynamic 3D arrays How do you draw the following dynamic 3D array with OpenGL glDrawPixels()? You can find the documentation here: http://opengl.org/documentation/specs/man_pages/hardcopy/GL/html/gl/drawpixels.html float ***array3d; void InitScreenArray() { int i j; int screenX = scene.camera.vres; int screenY = scene.camera.hres; array3d = (float ***)malloc(sizeof(float **) * screenX); for (i = 0 ; i < screenX; i++) { array3d[i] = (float **)malloc(sizeof(float *) * screenY); for (j = 0; j < screenY; j++) array3d[i][j] = (float *)malloc(sizeof(float) * /*Z_SIZE*/ 3); } } I can use only the following header files: #include <math.h> #include <stdlib.h> #include <windows.h> #include <GL/gl.h> #include <GL/glu.h> #include <GL/glut.h> Is this for homework? Yes but it's just a mall part I can't solve. The whole homework will be a ray tracer and this structure stores the screen pixels. Uh ... Since you're allocating each single pixel with a separate malloc() you will have to draw each pixel with a separate call to glDrawPixels() too. This is (obviously) insane; the idea of bitmapped graphics is that the pixels are stored in an adjacent compact format so that it is quick and fast (O(1)) to move from one pixel to another. This looks very confused to me. A more sensible approach would be to allocate the ""3D array"" (which is often referred to as a 2D array of pixels where each pixel happens to consist of a red green and blue component) with a single call to malloc() like so (in C): float *array3d; array3d = malloc(scene.camera.hres * scene.camera.vres * 3 * sizeof *array3d);  Thanks unwind. I got the same advice on gamedev.net so I have implemented the following algorithm: typedef struct { GLfloat R G B; } color_t; color_t *array1d; void InitScreenArray() { long screenX = scene.camera.vres; long screenY = scene.camera.hres; array1d = (color_t *)malloc(screenX * screenY * sizeof(color_t)); } void SetScreenColor(int x int y float red float green float blue) { int screenX = scene.camera.vres; int screenY = scene.camera.hres; array1d[x + y*screenY].R = red; array1d[x + y*screenY].G = green; array1d[x + y*screenY].B = blue; } void onDisplay( ) { glClearColor(0.1f 0.2f 0.3f 1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glRasterPos2i(00); glDrawPixels(scene.camera.hres scene.camera.vres GL_RGB GL_FLOAT array1d); glFinish(); glutSwapBuffers(); } My application doesn't work yet (nothing appears on screen) but I think it's my fault and this code will work.  wouldn't you want to use glTexImage2D() instead: see here",c++ arrays opengl graphics
331866,A,Events in cpp/opengl Hello I would like to create infrastructure to handle events for my opengl project. It should be similar to what wpf has - 3 types of events - direct tunneling bubbling. I then want to handle events such as mouse up down move etc. How should i approach this problem? Is there any library to handle this. thanks The OpenGL Utility Toolkit (GLUT) provides precisely this - you set up a bunch of event handlers for things like keyboard input mouse input redrawing the display and window resizing call the glutMainLoop() function and you're good to go.,c++ events opengl
793327,A,fixing glCopyTexSubImage2D upside down textures Since I've started learning about rendering to a texture I grew to understand that glCopyTexSubImage2D() will copy the designated portion of the screen upside down. I tried a couple of simple things that came to mind in order to prevent/work around this but couldn't find an elegant solution. there are two problems with doing a ::glScalef(1.0f -1.0f 1.0f) before rendering the texture to the screen: 1 I have to do this every time I'm using the texture. 2 I'm mostly working with 2D graphics and have backface culling turned off for GL_BACKsides. As much as possible I'd love to save switching this on and off. tried switching matrix mode to GL_TEXTURE and doing the ::glScalef(1.0f -1.0f 1.0f) transformation on capturing but the results were the same. (I'm guessing the texture matrix only has an effect on glTexCoord calls?) So how can I fix the up-down directions of textures captured with glCopyTexSubImage2D? If my understanding is correct it depends on where your origin is. That function seems to assume your origin is in the bottom-left whereas most 2D stuff assumes an origin of the top-left which is why it seems upside down. I suppose you could change the origin to the bottom-left do the capture then change the origin back to the top-left and render again before doing the swap. But that's a horrible solution since you're effectively rendering twice but it might be fine if you don't plan to do it every frame.  What are you going to be using the texture images for? Actually trying to render them upside down would usually take more work than moving that code somewhere else. If you're trying to use the image without exporting it just flipping the texture coordinates wherever you're using the result would be the most efficient way. If you're trying to export it then you either want to flip them yourself after rendering. On a related note if you are making a 2D game why is backface culling turned on? Good point. It was a case of guesswork optimization I'm afraid.=) Also I didn't need to flip textures for this game - not until now anyhow. I have still to see how much slower is it going to be but in the meantime turning backface culling off and flipping does seem to be the easiest and most elegant solution. Thank you!  Use glReadPixels() to copy to a buffer flip the image then use glTexImage2D() to write it back.,c++ opengl capture render textures
977629,A,"OpenGL Rotation Matrices And ArcBall I have been tasked with creating a OpenGL scene implementing ideas such as simple movement and an Arcball interface. The problem I am having is dealing with the rotation matrix which NeHe's Arcball class (http://nehe.gamedev.net/data/lessons/lesson.asp?lesson=48) computes. What I have so far is a very simple solar system (just the earth moon and sun) which looks great. What I want is for the camera to follow whichever planet the user selects (by clicking on the one they want) and for them to be able to rotate around the planet at a fixed distance using mouse-drag (arcball). As I said at the beginning NeHe's class is generating a rotation matrix based on the mouse clicking and dragging. What I want to apply the matrix to is the camera position. However when I do that my camera just wobbles without ever rotating around the planet. So I am guessing that I am either missing some step or that I have a horrible understanding of what I am trying to do. Here is some code from my camera class to crunch on: // transform is the matrix from NeHe's arcball interface void camera::update(Matrix4fT transform) { glm::mat4 transform_m = glm::mat4(0.0f); // convert nehe's matrices to GLM matrix for(int i=0; i < 4; i++) for(int j=0; j < 4; j++) transform_m[i][j] = transform.M[i*4+j]; // apply matrix to the position glm::vec4 pos4 = glm::vec4(this->pos 1.0f); pos4 = transform_m * pos4; this->pos = glm::vec3(pos4); } void camera::apply(planet *target) { // called at the beginning of GLPaint gluLookAt(this->pos.xthis->pos.ythis->pos.z // cam->position target->pos.xtarget->pos.ytarget->pos.z // moving this->up.xthis->up.ythis->up.z); // (010) } Other than that NeHe's functions are called in the right places (during click and drag)... So really I have no idea where to go from here. I hope someone can help me with this and if you want to see the whole code base (its programmed in C++ and pushed into a QTPanel) just send me an email. Thanks Carl Sverre (carl at carlsverre dot com) What does the ""transform"" matrix at camera::update represent exactly? As far as I understand the transform matrix should represent the required rotation of a sphere according to the arcball class. Hence if the user clicks one point at xyz on the sphere and moves the mouse to x2y2z2 the matrix will represent the required rotation of the sphere to move xyz to x2y2z2. Hope that makes sense. Well maybe I am wrong but what I think that is happening to you is that you are rotating around the center of coordinates and not around the planet (that it's what you want to do). To correct that what you have to do is: Translate the point you want to rotate around (the center of the planet) to the center of coordinates applying a translation of the negation of its position Rotate as you are doing it Undo the translation previously done. The thing to understand is that rotations are done around the center of coordinates and if you want to rotate around somewhere different you must first move that point to the center of coordinates. Hope that it helps. Well I solved the problem but I went down a different path. I made sure that the sphere I was rotating about was at 000 and rather than rotating the camera I just rotated the scene. Anyways long story short its fixed. I will give this one to you because no one else answered and your answer helped me get to my solution. Cheers Hey thanks for the answer! I tried to implement what you mentioned like so: http://2dsquid.pastebin.com/m510b0130 Unfortunately it didn't work and results in my camera flying away from my planet until the whole scene is out of the frustrum. Maybe I have the order wrong in the multiplication... Any suggestions? So I figured out the zooming out problem (my rotation matrix was computed on a sphere around 000 so I was transforming 1 to many times)... Now the camera sorta rotates around the planet like it should except it rotates at light speed and if I rotate to the left for awhile and then try to rotate to the right it keeps rotating to the left until it seems to ""normalize"" and then start rotating to the right. And any up/down motion messes it all up. Any ideas? If it is rotating too fast you must configure your multiplication parameters to make it rotate slower.. I don't know exactly how are you getting the parameters but it's a matter of ""playing"" with the values to get your desired effect.",c++ qt opengl matrix rotation
1558052,A,"Is it possible to translate twice in one OpenGL Matrix? This is a homework question I am sorry but I am lost. What happens is the following I am asked to do a tiling of hexagons. Like the grid map in many Risk games and Wild Arms XF. I understand the current transformation is the matrix that translates the points I give to OpenGL to screen coordinates and if you apply a transformation it moves the the center point usually (00) to wherever. If you use glPushMatrix and glPopMatrix. What it does is make a replica of the current CT matrix and you do operations on it and when you pop it you return to the matrix without the transformation. The problem is the following I am trying to draw a hexagon Translate(Displace I like it better) draw another Hexagon translate and Draw the last hexagon. What happens is the third hexagon dissapears from the face of the viewport and I am only raising it twice. What is funny is that consecutive rotates and scales give me no problems. so a small sample of the code. #include <windows.h> #include <stdio.h> #include <GL/gl.h> #include <GL/glu.h> #include <GL/glut.h> #include <iostream> #include <ctime> using namespace std; void initCT(void) { glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } void drawHexagon() { glBegin(GL_LINE_STRIP); glVertex2i(00); glVertex2i(20 0); glVertex2i(2510); glVertex2i(2020); glVertex2i(020); glVertex2i(-510); glVertex2i(00); glEnd(); } void myInit(void) { initCT(); glClearColor(1.0f1.0f1.0f0.0f); glColor3f(0.0f 0.0f 0.0f); //set the drawin color glPointSize(1.0); //a 'dot'is 4 by 4 pixel glMatrixMode(GL_MODELVIEW); glLoadIdentity(); gluOrtho2D(-100.0 400.0 -400.0 400.0); glViewport(0 0 640 480); } void myDisplay (void) { glClear (GL_COLOR_BUFFER_BIT); glMatrixMode( GL_MODELVIEW ); drawHexagon(); glTranslated(0.0f20.0f1.0f); drawHexagon(); glTranslated(0.0f20.0f1.0f); drawHexagon(); glTranslated(0.0f20.0f1.0f); drawHexagon(); glFlush(); //send all output to display } void main (int argc char **argv) { glutInit (&argc argv); glutInitDisplayMode (GLUT_SINGLE | GLUT_RGB); glutInitWindowSize (640 480); glutInitWindowPosition (100 150); glutCreateWindow (""Hexagon Tiling""); glutDisplayFunc (myDisplay); myInit(); glutMainLoop(); } I have used this code with the similar result only two hexagons draw the others go into the Hitchhikers guide to the galaxy. I changed the colors of each hexagon and only the first two appear. THANK YOU GUYS YOU ARE THE MEN. I am sorry if giving the answer to one upsets the other but you both are right. Also for people using Google this is from Computer Graphics using Open GL there is an error in the translation function that the author gives you in the translate it should be 0 in the Z axis. Yes you can use a glTranslate multiple times. This will construct a new translate matrix and multiply it onto your current matrix. Some thoughts: Your initCT function looks redundant. gluOrtho2D is usually used to modify the projection matrix. Try something like this: .... glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(-100.0 400.0 -400.0 400.0); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); .... Usually one starts their drawing by resetting their modelview matrix to identity. It's possible that glut does this for you but in case that's not happening try this: .... glClear (GL_COLOR_BUFFER_BIT); glMatrixMode( GL_MODELVIEW ); glLoadIdentity(); drawHexagon(); .... Thank you and Thank You I will take your observations into account and fix the application accordingly. You say that his initCT function is redundant then recommend that he do what's in the initCT? He switches into modelview and loads identity twice. I added a switch to projection and a loadIdentity moved one modelview loadIdentity down and suggested that he remove the other. That's all. Fair enough... That makes sense.  You're translating behind the camera; Each translation goes up in the y axis by 20 and in the z axis by 1. After the second step your hexagons are behind the camera and can't be seen in the viewport. Try glTranslated(0.0f20.0f0.0f); for each translation; that should help.",c++ opengl graphics
784247,A,"How can I make my mouse control the camera like a FPS using OpenGL/SDL? I've created this basic 3D Demo using OpenGL/SDL. I handled the keyboard callback so I can ""strafe"" left and right using 'a' and 's' and move forward and backward using 's' and 'w'. However I would like to now make it so I can control the direction my camera is ""looking"" based off my mouse movements. Just like in a FPS shooter when you move the mouse around it makes the camera look around in various directions. Does anyone have any idea how I could utilize the mouse callbacks to ""points"" the camera class correctly when I move the mouse? #include ""SDL.h"" #include ""Camera.h"" Camera cam; Scene scn; //<<<<<<<<<<<<<<<<<myKeyboard>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> void myKeyboard(unsigned char key int x int y) { switch(key) { case 's': cam.slide(0.0 0.0 0.2); break; case 'w': cam.slide(0.0 0.0 -0.2); break; case 'a': cam.yaw(-1.0); break; case 'd': cam.yaw(1.0); break; case 27: exit(0); } glClear(GL_COLOR_BUFFER_BIT); glutPostRedisplay(); } void displaySDL( void ) { glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT); scn.drawSceneOpenGL(); glFlush(); glutSwapBuffers(); } int main( int argc char* argv[] ) { glutInit(&argc argv); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH); glutInitWindowSize(640 480); glutInitWindowPosition(100 100); glutCreateWindow(""SDL Sence With Camera""); glutKeyboardFunc(myKeyboard); glutDisplayFunc(displaySDL); glShadeModel(GL_SMOOTH); glEnable(GL_DEPTH_TEST); glEnable(GL_NORMALIZE); glViewport(0 0 640 480); scn.read(""fig5_63.dat""); glEnable(GL_LIGHTING); glEnable(GL_LIGHT0); scn.makeLightsOpenGL(); cam.set(2.3 1.3 2.0 0 0.25 0 0 1 0); cam.setShape(30.0f 64.0f/48.0f 0.5f 50.0f); glutMainLoop(); return 0; } This is a tar with my SDL file and the file I pasted above and my Camera class. http://www.filedropper.com/fpsdemotar If someone can give me some tips for what algorithm I should use when processing mouse callbacks in terms of pointing the camera I would appreciate it. Thanks! Mouse moving up/down -> pitch Mouse moving right/left -> yaw. I do not believe having your 'a' and 'd' keys be yaw is accurate. Actually your whole setup is a bit odd to me since from a geometric standpoint I view the coordinate as (x y z). You set s and w to go ""up"" and ""down"" (z) instead of ""forward"" and ""back"" (y). I see it as a xy graph that has been set flat on a table and you are looking at it from above. Moving close to it decreases z which is coming out of the plane. Here is how I would have it setup: w -> slide(0 0.2 0); // y s -> slide(0 -0.2 0); a -> slide(-0.2 0 0); // x d -> slide(0.2 0 0); //The following goes in your mouse event handler or something: pitch(newMouseLocation.y - oldMouseLocation.y); // mouse y is related to pitch yaw(newMouseLocaiton.x - oldMouseLocation.x); // mouse x is related to yaw I realize that you do not need to follow this coordinate convention but it just seems more intuitive for me. I hope this helps. Thank you for your input CookieOfFortune. Some guides I have stumbled on to suggest centering the mouse to the center of the screen. Do you think I should do this or not? One thing I was considering was each mouse callback handling the pitch yaw like you suggest then forcing the mouse to move back to the center so you never reach the ledge. What do you think? Hmmm... does mousemove stop registering a change if it hits the edge? If that is the case (and I do believe that it is) then yes you should center the mouse (But only if you have focus!! don't want someone alt-tabbing and then be unable to move their mouse!).",c++ opengl camera sdl
1003497,A,How to remove black background from textures in OpenGL I'm looking for a way to remove the background of a 24bit bitmap while keeping the main image totally opaque up until now blending has served the purpose but now I need to keep the main bit opaque. I've searched on Google but found nothing helpful I think I'm probably searching for the wrong terms though so any help would be greatly appreciated. Edit: Yes sorry I'm using black right now as the background. I'm assuming by background you are talking about a designated color such as black: R0G0B0? When you are creating the texture for OpenGL you'll want to create the texture as a 32-bit RGBA texture. Create a buffer to hold the contents of this new 32-bit texture and iterate through your 24-bit bitmap. Every time you come across your background color set the alpha to zero in your new 32-bit bitmap. struct Color32; struct Color24; void Load24BitTexture(Color24* ipTex24 int width int height) { Color32* lpTex32 = new Color32[width*height]; for(x = 0; x < width; x++) { for(y = 0; y < height; y++) { int index = y*width+x; lpTex32[index].r = ipTex24[index].r; lpTex32[index].g = ipTex24[index].g; lpTex32[index].b = ipTex24[index].b; if( ipTex24[index] == BackgroundColor) lpTex32[index].a = 0; else lpTex32[index].a = 255; } } glTexImage2D(GL_TEXTURE_2D 0 4 width height 0 GL_RGBA GL_UNSIGNED_BYTE lpTex32); delete [] lpTex32; } Fantastic exactly what I was looking for. I wasn't quite sure how the alpha channel was read in OGL after Googling it and reading your code I think I'm sorted and ready to go. Thank you :)  What you really need is an alpha channel in your image. Since it's 24 bit you don't have one but you could create one on the fly as you load the image by simply setting the alpha to zero for every black (0x000000) pixel This would be probably the easiest solution if he has enough memory to burn. If not it would be a waste to spend a whole 8bit alpha channel on indexed transparency.  You will need to load your textures with GL_RGBA format in your call to glTexImage2D. If you have done this then you just need to enable blend:  /* Set up blend */ glEnable(GL_BLEND); glBlendFunc (GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); I totally misunderstood the question but one ~would~ need to enable blend so I wont delete my answer.  This sounds a bit confusing. 24bit image has no transparency it’s simply an array of RGB values. What do you mean by “keeping the main bit opaque”? Do you want to have all pixels with certain color – for example all black pixels – transparent? If this is what you want you can google opengl transparency mask (see the NeHe #20 for example). I said I originally used the blend functions to remove the black but it left me with a translucent central image instead of opaque. I have used a transparency mask in the past but I had lots of issues with it and didn't like requiring another file per image. Aha I think I understand now. If you have enough memory simply go with an alpha channel in the image as others have pointed out.  Sounds to me like you're looking for the Stencil Buffer which is used for just this sort of task of restricting the drawing area. The buffer stores a value associated to each pixels where it's 1 subsequent drawing will be rendered and where it's 0 it will be masked. Usually stencil buffers are used with multipass rendering techniques to create things like reflections and heads up displays.,c++ c opengl alpha blending
1540859,A,C2664 error in an attempt to do some OpenGl in c++ Here is an abstract of my code. I'm trying to use glutSpecialFunc to tell glut to use my KeyPress function class Car : public WorldObject { public: void KeyPress(int key int x int y) { } Car() { glutSpecialFunc(&Car::KeyPress); // C2664 error } } The error message I get is: Error 1 error C2664: 'glutSpecialFunc' : cannot convert parameter 1 from 'void (__thiscall Car::* )(intintint)' to 'void (__cdecl *)(intintint)' c:\users\thorgeir\desktop\programmingproject1\quickness\quickness\car.cpp 18 Quickness This question should be retagged as Visual Studio or Visual-C++ What I ended up doing was Adding: virtual void KeyPress(int key int x int y) {}; to the WorldObject class And bubble the event to the Car.  void KeyPressed(int key int x int y) { KeyPress(keyxy); list<WorldObject*>::iterator iterator = ChildObjects.begin(); while(iterator != ChildObjects.end()) { (*iterator)->KeyPressed(keyxy); iterator++; } }  Your function is a member of a class. When you do something like Car c; c.drive() that drive() function needs a car to work with. That is the this pointer. So glut can't call that function if it doesn't have a car to work on it's expecting a free function. You could make your function static which would mean the function does not operate on a car. glut will then be able to call it however I assume you want to manipulate a car. The solution is to make the function pass it's call onto an object like this: void key_press(int key int x int y) { activeCar->KeyPress(key x y); } Where activeCar is some globally accessible pointer to car. You can do this with some sort of CarManager singleton. CarManager keeps track of the active car being controlled so when a key is pressed you can pass it on: CarManager::reference().active_car().KeyPress(key x y). A singleton is an object that has only one globally accessible instance. It is outside the scope of the answer but you can Google for various resources on creating one. Look up Meyers Singleton for a simple singleton solution. A different approach is to have a sort of InputManager singleton and this manager will keep track of a list of objects it should notify of key presses. This can be done in a few ways the easiest would be something like this: class InputListener; class InputManager { public: // ... void register_listener(InputListener *listener) { _listeners.push_back(listener); } void unregister_listener(InputListener *listener) { _listeners.erase(std::find(_listeners.begin() _listeners.end() listener)); } // ... private: // types typedef std::vector<InputListener*> container; // global KeyPress function you can register this in the constructor // of InputManager by calling glutSpecialFunc static void KeyPress(int key int x int y) { // singleton method to get a reference to the instance reference().handle_key_press(key x y); } void handle_key_press(int key int x int y) const { for (container::const_iterator iter = _listeners.begin(); iter != _listenders.end() ++iter) { iter->KeyPress(key x y); } } container _listeners; }; class InputListener { public: // creation InputListener(void) { // automatically add to manager InputManager::reference().register_listener(this); } virtual ~InputListener(void) { // unregister InputManager::reference().unregister_listener(this); } // this will be implemented to handle input virtual void KeyPress(int key int x int y) = 0; }; class Car : public InputListener { // implement input handler void KeyPress(int key int x int y) { // ... } }; Of course feel free to ask more questions if this doesn't make sense. Wow great answer and very quick!. Explained everything.,c++ opengl glut c2664
73117,A,"Making a game in C++ using parallel processing I wanted to ""emulate"" a popular flash game Chrontron in C++ and needed some help getting started. (NOTE: Not for release just practicing for myself)  Basics: Player has a time machine. On each iteration of using the time machine a parallel state is created co-existing with a previous state. One of the states must complete all the objectives of the level before ending the stage. In addition all the stages must be able to end the stage normally without causing a state paradox (wherein they should have been able to finish the stage normally but due to the interactions of another state were not). So that sort of explains how the game works. You should play it a bit to really understand what my problem is. I'm thinking a good way to solve this would be to use linked lists to store each state which will probably either be a hash map based on time or a linked list that iterates based on time. I'm still unsure. ACTUAL QUESTION: Now that I have some rough specs I need some help deciding on which data structures to use for this and why. Also I want to know what Graphics API/Layer I should use to do this: SDL OpenGL or DirectX (my current choice is SDL). And how would I go about implementing parallel states? With parallel threads? EDIT (To clarify more): OS -- Windows (since this is a hobby project may do this in Linux later) Graphics -- 2D Language -- C++ (must be C++ -- this is practice for a course next semester) Q-Unanswered: SDL : OpenGL : Direct X Q-Answered: Avoid Parallel Processing Q-Answered: Use STL to implement time-step actions.  So far from what people have said I should: 1. Use STL to store actions. 2. Iterate through actions based on time-step. 3. Forget parallel processing -- period. (But I'd still like some pointers as to how it could be used and in what cases it should be used since this is for practice). Appending to the question I've mostly used C# PHP and Java before so I wouldn't describe myself as a hotshot programmer. What C++ specific knowledge would help make this project easier for me? (ie. Vectors?) This sounds very similar to Braid. You really don't want parallel processing for this - parallel programming is hard and for something like this performance should not be an issue. Since the game state vector will grow very quickly (probably on the order of several kilobytes per second depending on the frame rate and how much data you store) you don't want a linked list which has a lot of overhead in terms of space (and can introduce big performance penalties due to cache misses if it is laid out poorly). For each parallel timeline you want a vector data structure. You can store each parallel timeline in a linked list. Each timeline knows at what time it began. To run the game you iterate through all active timelines and perform one frame's worth of actions from each of them in lockstep. No need for parallel processing.  Parallel processing isn't the answer. You should simply ""record"" the players actions then play them back for the ""previous actions"" So you create a vector (singly linked list) of vectors that holds the actions. Simply store the frame number that the action was taken (or the delta) and complete that action on the ""dummy bot"" that represents the player during that particular instance. You simply loop through the states and trigger them one after another. You get a side effect of easily ""breaking"" the game when a state paradox happens simply because the next action fails.  What you should do is first to read and understand the ""fixed time-step"" game loop (Here's a good explanation: http://www.gaffer.org/game-physics/fix-your-timestep). Then what you do is to keep a list of list of pairs of frame counter and action. STL example: std::list<std::list<std::pair<unsigned long Action> > > state; Or maybe a vector of lists of pairs. To create the state for every action (player interaction) you store the frame number and what action is performed most likely you'd get the best results if action simply was ""key pressed"" or ""key released"": state.back().push_back(std::make_pair(currentFrame VK_LEFT | KEY_PRESSED)); To play back the previous states you'd have to reset the frame counter every time the player activates the time machine and then iterate through the state list for each previous state and see if any matches the current frame. If there is perform the action for that state. To optimize you could keep a list of iterators to where you are in each previous state-list. Here's some pseudo-code for that: typedef std::list<std::pair<unsigned long Action> > StateList; std::list<StateList::iterator> stateIteratorList; // foreach(it in stateIteratorList) { if(it->first == currentFrame) { performAction(it->second); ++it; } } I hope you get the idea... Separate threads would simply complicate the matter greatly this way you get the same result every time which you cannot guarantee by using separate threads (can't really see how that would be implemented) or a non-fixed time-step game loop. When it comes to graphics API I'd go with SDL as it's probably the easiest thing to get you started. You can always use OpenGL from SDL later on if you want to go 3D. Thanks for the great detail on this one. This actually looks like a good solution for this. Any recommendations on the graphics engine to use to go along with it? I'd go with SDL as it's probably the easiest thing to get you started. You can always use OpenGL from SDL later if you want to go 3D. OGRE is a great 2D73D game engine for C++  After briefly glossing over the description I think you have the right idea I would have a state object that holds the state data and place this into a linked list...I don't think you need parallel threads... as far as the graphics API I have only used opengl and can say that it is pretty powerful and has a good C / C++ API opengl would also be more cross platform as you can use the messa library on *Nix computers.  A very interesting game idea. I think you are right that parrellel computing would be benefical to this design but no more then any other high resource program. The question is a bit ambigous. I see that you are going to write this in C++ but what OS are you coding it for? Do you intend on it being cross platform and what kind of graphics would you like ie 3D 2D high end web based. So basically we need a lot more information.  Unless you're desperate to use C++ for your own education you should definitely look at XNA for your game & graphics framework (it uses C#). It's completely free it does a lot of things for you and soon you'll be able to sell your game on Xbox Live. To answer your main question nothing that you can already do in Flash would ever need to use more than one thread. Just store a list of positions in an array and loop through with a different offset for each robot.  I have played this game before. I don't necessarily think parallel processing is the way to go. You have shared objects in the game (levers boxes elevators etc) that will need to be shared between processes possibly with every delta thereby reducing the effectiveness of the parallelism. I would personally just keep a list of actions then for each subsequent iteration start interleaving them together. For example if the list is in the format of <[iteration.action]> then the 3rd time thru would execute actions 1.1 2.1 3.1 1.2 2.2 3.3 etc. And I would store these actions with the time as the key as in an associative array? As long as you have a fixed time frame between steps a simple list should do. No problems with using a key but it just seems simpler.",c++ opengl graphics directx sdl
1704164,A,"OpenGL / C++ / Qt - Advice needed I am writing a program in OpenGL and I need some sort of interfacing toolbar. My initial reactions were to use a GUI then further investigation into C++ I realized that GUI's are dependent on the OS you are using (I am on Windows). Therefore I decided to use QT to help me. My Question is if I am taking the best/appropriate approach to this solution. Am I even able to write my OpenGL program and have the GUI I want to create interface with the C++ code to do what I want it to do. For example If I create a simple ""control panel"" with arrows in each direction. And on screen I have a box object created by glut can I interface the arrows to be clicked on and interact with the openGL program to move the box? Thanks for all of the help Zach Smith Using Qt is coherent for your problem: it provides good integration of OpenGl through the QtOpenGL module. Derive your display classes from QGLWidget and implement virtual methods paintGL() etc. You will have access to the Qt's signal and slot system so that you will be able to catch Gui events and update the OpenGl display. QtOpenGL Module: http://doc.trolltech.com/4.5/qtopengl.html With very little knowledge of Qt I got an openGL window with a decent particle system working in about 2 hours. Its really quite nice.  You can use regular non-OpenGL Qt widgets on top of a QGLWidget so what you describe is do-able. One thing I came across when doing this was that the regular widgets had to be opaque. The moment they were transparent in any way there was all sorts of garbage underneath them. That was a while ago so maybe the latest version addresses this issue. May have been platform-specific too so YMMV. Ah my anonymous downvoting shadow has returned!  Stay in GLUT. There's no need to add Qt for a control panel. You could open a sub window (or second window whichever works better for your program design) and draw the controls into that window and use GLUT to handle the mouse interaction. Furthermore Qt and GLUT each have their own event loops. To use just Qt's event loop you'd have to abandon much of the GLUT structure since the GLUT event loop would not be there to call your callback functions. Qt does have the functionality to have let somebody else event loop call Qt's event processing code but I don't think there's an easy way to make GLUT hand off the event information. Since the author does not say that he is already using GLUT your comment does not apply. Qt is clearly superior to GLUT and allows the application to evolve into either a full-blown application or a reusable widget. Dat Chu the author does say he's already using GLUT. See paragraph three of his post and the tags. You are right that Qt is superior to GLUT in most aspects but mixing Qt and GLUT is very hard.",c++ gui qt opengl glut
1607953,A,c++ OpenGL coordinate transformation I just don't seem to be able to figure this out in my head. I'm trying to move an object in 3D space. If I have a point at 5155 and use opengl functions to change the model view.... glTranslatef( 10.0f 4.0f 4.0f ); glRotatef( 33.0f 1.0f 0.0f 0.0f ); glTranslatef( 10.0f 4.0f 4.0f ); Is there a way I can find out where that point ends up (in world / global coordinates)? Can I do some kind of matrix calculations that will give me back 202623 (or what every the new coordinate position is)? Please help I've been stuck on this for so long! Try the following: 1) Push the current matrix into stack; 2) Load identity and apply your transformations; 3) Get the resulting transformation matrix into some temp variable. glGet or something like that will help; 4) Pop the matrix from the stack; Now you have your transformation matrix. Multiply your point by this matrix to predict the point's coordinates after the transformation. I think I get it now :) something along these lines... http://www.gamedev.net/reference/articles/article877.asp  Definitely: check out http://research.cs.queensu.ca/~jstewart/454/notes/pipeline/ In short all of these calls reduce to a single matrix which is multiplied onto the point. SadSido's method will definitely get you the resultant matrix but it may not hurt to actually understand what's going on behind the scenes. The calculations above will result in a linear algebra equation of the following: pOut = [mTranslate] * [mRotate] * [mTranslate] * pIn where mTranslate = the translation calls (matrix for translation) and mRotate = rotate call (matrix for rotation about an arbitrary axis). Calculate that and you're good to go! Thankyou for your answer when I get a little more comfortable with all of this I think I'll delve deeper into matrix maths,c++ opengl coordinates
1403251,A,Unicode Input Handling in Games I have a game that requires me to allow players to chat with each other via network. All is well except the part where players can type in Unicode input. So the question can be split into two parts: When players type how do I capture input? I have done this before via the game input handling (polling) however it is not as responsive as something like Windows Forms. After I capture input into a string how do I output it using TrueType Fonts? The reason I ask this is because usually I would build bitmap fonts at the start of the game from the all the text used in the game. But with unicode input there are nearly 10k characters that are needed which is quite impossible to build at the start of the game. P.S. My target input languages are more specific to Chinese Korean and Japanese. For Input Use SDL_EnableUNICODE to enable unicode input handling Receive the SDL_KeyboardEvent as usual Use the unicode member of SDL_keysym to get the unicode For Rendering If the needed font size is reasonably small say 16px you actually could just render it all to a single texture you can fit a minimum of 4096 glyphs on 1024x1024 texture at that size a bit more when you pack them tightly (see fontgen for example code). That should be enough for common chat but not enough to fit all the glyphs of a TTF file. If you don't want to use a larger texture size you have to generate the fonts on demand to do that just create the Texture's as usual and then use glTexSubImage2D to upload new glyphs to the texture. Another alternative is to not use textures for glyphs but for the text itself. That way you bypass all the trouble that glyph generation produces. But its probably not such a good idea for non-static editable text. Thank you sooo much! I've been pondering about this for ages. Cheers!  I've done no game development myself so I have just a vague idea of how things work there but here are my 2 cents: Don't cache all the glyphs at the start of the program. Instead when you have to display a chat string render the whole string on-the-fly to some new texture. Keep this texture in memory until a time when it is unlikely that it will be needed again (say after the chat window is closed). Perhaps you can re-render the whole chat window when it gets updated - then you would only have one texture to worry about.  As far as display goes I've had very good luck with the caching system described in this tutorial transliterated to C++. For fonts GNU Unifont has full BMP glyph coverage available in convenient TTF form.  When players type how do I capture input? That depends on what you use I guess. I'm not familiar with SDL. On Linux you can use standard X functions and event loop it works well (used in Quake for example; so it should be reactive enough). After I capture input into a string how do I output it using TrueType Fonts? You should have a look at FreeType2 library. It lets you load TrueType fonts and retrieve the glyph (the image) of any character. But with unicode input there are nearly 10k characters that are needed which is quite impossible to build at the start of the game. I have the same problem. I guess a cache manager with MRU (most recently used) characters would do the trick. I bit more complicated than simple static bitmap though.  Here is some code showing how to capture keyboard input with SDL. First of all you need to query key input from SDL by calling EventPoll. You can do that whenever you are ready to process input or regularly in a fixed interval and store keys and keyboard status in internal tables. void EventPoll (ulong mask) { SDL_Event event; while (SDL_PollEvent (&event)) { switch(event.type) { case SDL_KEYDOWN: KeyHandler (reinterpret_cast<SDL_KeyboardEvent*> (&event)); break; case SDL_KEYUP: KeyHandler (reinterpret_cast<SDL_KeyboardEvent*> (&event)); break; // handle other events } } } void KeyHandler (SDL_KeyboardEvent *event) { SDLKey keySym = event->keysym.sym; wchar_t unicode = event->keysym.unicode; int keyState = (event->state == SDL_PRESSED); // process key info e.g. put key into a buffer and // store keyboard state } Here is a link to a document describing methods to render text with OpenGL: http://www.opengl.org/resources/features/fontsurvey/ What you may want to do is to capture keyboard input and render it on the fly using the proper font(s) you have preloaded.,c++ opengl unicode sdl
1085617,A,Catmull-Rom splines - how do they work? From this site which seems to have the most detailed information about catmull-rom splines: http://www.mvps.org/directx/articles/catmull/ it makes mention of needing four points to create the spline. However it does not mention how the points p0 and p3 affect the values between p1 and p2. Another question I have is how would you create continuous splines? Would it be as easy as defining the points p1 p2 to be continuous with p4 p5 by making p4 = p2 (that is assuming we have p0 p1 p2 p3 p4 p5 p6... pN). A more general question is how would one calculate tangents on catmull rom splines? Would it have to involve taking two points on the spline (say at 0.01 0.011) and getting the tangent based on pythagoras given the position coordinates those input values give? Normal Catmull-Rom is also prone to loops and self-intersection which can be a problem. I strongly recommend using the centripetal parameterization shown here: http://stackoverflow.com/questions/9489736/catmull-rom-curve-with-no-cusps-and-no-self-intersections/19283471#19283471 Take a look at equation 2 -- it describes how the control points affect the line. You can see points p0 and p3 go into the equation for plotting points along the curve from P1 to P2. You'll also see that the equation gives P1 when t == 0 and P2 when t == 1. This example equation can be generalized. If you have points R0 R1 ... RN then you can plot the points between RK and RK+1 by using equation 2 with P0 = RK-1 P1 = RK P2 = RK+1 P3 = RK+2. You can't plot from R0 to R1 or from RN-1 to RN unless you add extra control points to stand in for R-1 and RN+1. The general idea is that you can pick whatever points you want to add to the head and tail of a sequence to give youself all the parameters to calculate the spline. You can join two splines together by dropping one of the control points between them. Say you have R0 R1 ... RN and S0 S1 ... SM they can be joined into R0 R1 ... RN-1 S1 S2 ... SM. To compute the tangent at any point just take the derivative of equation 2.  The Wikipedia article goes into a little bit more depth. The general form of the spline takes as input 2 control points with associated tangent vectors. Additional spline segments can then be added provided that the tangent vectors at the common control points are equal which preserves the C1 continuity. In the specific Catmull-Rom form the tangent vector at intermediate points is determined by the locations of neighboring control points. Thus to create a C1 continuous spline through multiple points it is sufficient to supply the set of control points and the tangent vectors at the first and last control point. I think the standard behavior is to use P1 - P0 for the tangent vector at P0 and PN - PN-1 at PN. According to the Wikipedia article to calculate the tangent at control point Pn you use this equation: T(n) = (P(n - 1) + P(n + 1)) / 2 This also answers your first question. For a set of 4 control points P1 P2 P3 P4 interpolating values between P2 and P3 requires information form all 4 control points. P2 and P3 themselves define the endpoints through which the interpolating segment must pass. P1 and P3 determine the tangent vector the interpolating segment will have at point P2. P4 and P2 determine the tangent vector the segment will have at point P3. The tangent vectors at control points P2 and P3 influence the shape of the interpolating segment between them. I'm sorry if I'm missing something but could you point out where in the article it gives that formula? I see a different one at http://en.wikipedia.org/wiki/Cubic_Hermite_spline#Catmull.E2.80.93Rom_spline. (I'm not all that good at math. I just wanted to know how you got the formula.),c++ opengl graphics directx spline
1874103,A,negative color with glBlendFunc()? i want my lines to be drawn with negative color (taken from the screen under the line) i just didnt understand how the blending works looked at docs etc tested 50 combinations and so on. started to think its not possible at all... could someone just give the two values? You should use logic ops for that purpose. Not blend. So all you have to do is to call: glEnable(GL_COLOR_LOGIC_OP); glLogicOp(GL_INVERT); You can use GL_ XOR too depending what you want to achieve. GL_ XOR is useful if you want to restore the frame buffer exactly in the state that it was before the line draw happened. Just draw a second time the same line with GL_XOR again ((a xor b) xor b == a). It's a common trick in CAD world.  Draw a white line and use glBlendFunc(GL_ONE_MINUS_DST_COLOR GL_ZERO); Don't forget to enable GL_BLEND,c++ opengl
173770,A,What ways are there of drawing 3D trees using Java and OpenGL? I know how to draw basic objects using JOGL or LWJGL to connect to OpenGL. What I would like is something that can generate some kind of geometry for trees similar to what SpeedTree is famous for. Obviously I don't expect the same quality as SpeedTree. I want the trees to not look repetitive. Speed is not a concern I do not expect to need more than 100 trees on screen at one time. Are there free tree-drawing libraries available in Java? Or sample code or demos? Is there anything in other languages which I could port or learn from? A combination of OpenSceneGraph and SpeedTree has worked for me.  There are thousands of methods. A better question would define 'best' in a more confined way. Are we talking 'best' as in speed of drawing (suitable for thousands or millions of trees)? Best as in best-looking? etc.  I know of two libraries that enable the usage of OpenGl with java. LWJGL (Light Weight Java Gaming Library) which imo is the better one due to its simplicity and its similarity to using opengl with c/c++. JOGL If you want to mix swing components with opengl this may be the better choice I've never used it but several years ago it was known to be pretty buggy I don't know if its matured since then. As for drawing trees there are many ways to do so like the other poster said you might want to be more specific. edit: I guess I misunderstood the question a bit oh well : / You can load in a 3d model of a tree and display that.  Here are a couple resources that may be helpful: gamedev thread on the subject - contains some useful advice/suggestions ngPlant - an open-source procedural plant generation tool. It is not written in Java but you may be able to find ideas in its algorithms.  If you're serious about getting good-looking fast trees there's a commercial C++ library SpeedTree. Lots of big-time games use it (e.g. GTA Elder Scrolls). SpeedTree is amazing but I am planning some software that will be free and open source (probably GPL licensed) and so I need to either find a free solution or make my own.  http://www.codeplex.com/LTrees has some source code on that. it's c++ though.  2D or 3D? In 2D a common way is to use L-systems. I also tried an OO approach defining objects for trunk branches leaves all extending an abstract class and implementing a Genotype interface (to vary the kind of trees). Not sure if it is efficient (lot of objects created particularly if I animate the tree) but interesting to do.  http://arbaro.sourceforge.net/ http://www.propro.ru/go/Wshop/povtree/povtree.html Non java: http://www.aust-manufaktur.de/austt.html Wow these look promising thanks. I don't see the javadoc for arbaro but you can always get the source and generate it yourself. Good luck.  If you are using eclipse/SWT try Draw 2D.,java c++ c opengl procedural-generation
1856640,A,"The procedure entry point _ftol2 could not be located in the dynamic link library msvcrt.dll I've recently been tinkering with a little gameproject using VC++ 2008. I'm using SDL OpenGL Boost and Box2D as included libraries. It works fine on my windows 7 machine aswell as a friend's w7 machine. How ever it wont work on my second friend's XP sp3 machine with the vc++ 2008 SP1 redist pack installed. When he starts the .exe he get's the error: ""The procedure entry point _ftol2 could not be located in the dynamic link library msvcrt.dll"" Most forum threads I've read suggests that the msvcrt.dll is corrupt or outdated. My version of the msvcrt.dll is 7.0.7600.1385 and his is 7.0.2600.5512 . Can't find an update for it can't simply replace it because it reverts to the old version on reboot and it doesn't seem to help to simply include my msvcrt.dll in my game's folder. According to this thread on gamedev.net OpenGL32.dll calls the _ftol2. Their conclusion is to install the vc++ 2008 redist pack which I've mentioned is already installed. Any ideas that might shed light on a solution to this error? Edit: Using Dependency Walker I assured that OpenGL32.dll and GLU32.dll does indeed call the _ftol2 in MSVCRT.dll. How do I avert this dependency? You need to determine what is referencing MSVCRT.DLL. Nothing should be - this is the Windows version of the CRT. Your application should be linking against MSVCRT90.DLL. According to the thread I linked to OpenGL32.dll calls MSVCRT.dll. How would I go about to determine what my included libraries reference to and how would I change those references?  I got also similar strange message ""strncpy_s could not be located in the dynamic link library msvcrt.dll"" and found solution. In my case making setup project with VS 2005 MAPI32.dll was added. So installation was made under WIN 7 64 and didn't work under XP 32 (although the right MAPI32.dll used from WIN 7). After it is excluded from project or manually deleted from target folder all works fine. How does one exclude MAPI32.dll from the project? Right click on MAPI32.dll into setup project and choose exclude.  I shouldn't have included the opengl32.dll from my system with my game. The opengl32.dll on XP is an older version and is properly linked with the MSVCRT.dll on XP aswell. When I included the windows 7 opengl32.dll it simply didn't match with the xp dlls. Removing the opengl32.dll and glu32.dll from my game folder solved the problem and the game works fine.",c++ dll opengl visual-c++
1550904,A,"Stuck building a game engine I'm trying to build a (simple) game engine using c++ SDL and OpenGL but I can't seem to figure out the next step. This is what I have so far... An engine object which controls the main game loop A scene renderer which will render the scene A stack of game states that can be pushed and popped Each state has a collection of actors and each actor has a collection of triangles. The scene renderer successfully sets up the view projection matrix I'm not sure if the problem I am having relates to how to store an actors position or how to create a rendering queue. I have read that it is efficient to create a rendering queue that will draw opaque polygons front to back and then draw transparent polygons from back to front. Because of this my actors make calls to the ""queueTriangle"" method of the scene renderer object. The scene renderer object then stores a pointer to each of the actors triangles then sorts them based on their position and then renders them. The problem I am facing is that for this to happen the triangle needs to know its position in world coordinates but if I'm using glTranslatef and glRotatef I don't know these coordinates! Could someone please please please offer me a solution or perhaps link me to a (simple) guide on how to solve this. Thankyou! A 'queueTriangle' call sounds to me to be very inefficient. Modern engines often work with many thousands of triangles at a time so you'd normally hardly ever be working with anything on the level of a single triangle. And if you were changing textures a lot to accomplish this ordering then that is even worse. I'd recommend a simpler approach - draw your opaque polygons in a much less rigorous order by sorting the actor positions in world space rather than the positions of individual triangles and render the actors from front to back an actor at a time. Your transparent/translucent polygons still require the back-to-front approach (providing you're not using premultiplied alpha) but everything else should be simpler and faster. in fact th opaque advice above is what the graphics card manufacturers recommend because switching shader/constant/texture/etc regularly is much more expensive than a polygon failing the early out Z test. Yep. Reducing overdraw is good but not at the expense of numerous state changes.  If you write a camera class and use its functions to move/rotate it in the world you can use the matrix you get from the internal quaternion to transform the vertices giving you the position in camera space so you can sort triangles from back to front.",c++ opengl sdl
1241853,A,"New to SDL OpenGL on Linux whats wrong with this? I have written some code to experiment with opengl programming on Ubuntu its been a little while but I used to have a reasonable understanding of C. Since c++ i'm told is the language of choice for games programming I am trying to develop with it. This is my first real attempt at opengl with sdl and I have gotten to this far it compiles and runs but my camera function doesn't seem to do anything. I know there is probably a lot better ways to do this sort of stuff but I wanted to get the basics before I moved on to more advanced stuff. main.cpp #include <iostream> #include <cmath> #include ""SDL/SDL.h"" #include ""SDL/SDL_opengl.h"" int screen_width = 640; int screen_height = 480; const int screen_bpp = 32; float rotqube = 0.9f; float xpos = 0 ypos = 0 zpos = 0 xrot = 0 yrot = 0 angle=0.0; float lastx lasty; SDL_Surface *screen = NULL; // create a default sdl_surface to render our opengl to void camera (void) { glRotatef(xrot1.00.00.0); // x-axis (left and right) glRotatef(yrot0.01.00.0); // y-axis (up and down) glTranslated(-xpos-ypos-zpos); // translate the screen to the position SDL_GL_SwapBuffers(); } int DrawCube(void) { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glLoadIdentity(); glTranslatef(0.0f 0.0f-7.0f); glRotatef(rotqube0.0f1.0f0.0f); glRotatef(rotqube1.0f1.0f1.0f); glBegin(GL_QUADS); glColor3f(0.0f1.0f0.0f); glVertex3f( 1.0f 1.0f-1.0f); glVertex3f(-1.0f 1.0f-1.0f); glVertex3f(-1.0f 1.0f 1.0f); glVertex3f( 1.0f 1.0f 1.0f); glColor3f(1.0f0.5f0.0f); glVertex3f( 1.0f-1.0f 1.0f); glVertex3f(-1.0f-1.0f 1.0f); glVertex3f(-1.0f-1.0f-1.0f); glVertex3f( 1.0f-1.0f-1.0f); glColor3f(1.0f0.0f0.0f); glVertex3f( 1.0f 1.0f 1.0f); glVertex3f(-1.0f 1.0f 1.0f); glVertex3f(-1.0f-1.0f 1.0f); glVertex3f( 1.0f-1.0f 1.0f); glColor3f(1.0f1.0f0.0f); glVertex3f( 1.0f-1.0f-1.0f); glVertex3f(-1.0f-1.0f-1.0f); glVertex3f(-1.0f 1.0f-1.0f); glVertex3f( 1.0f 1.0f-1.0f); glColor3f(0.0f0.0f1.0f); glVertex3f(-1.0f 1.0f 1.0f); glVertex3f(-1.0f 1.0f-1.0f); glVertex3f(-1.0f-1.0f-1.0f); glVertex3f(-1.0f-1.0f 1.0f); glColor3f(1.0f0.0f1.0f); glVertex3f( 1.0f 1.0f-1.0f); glVertex3f( 1.0f 1.0f 1.0f); glVertex3f( 1.0f-1.0f 1.0f); glVertex3f( 1.0f-1.0f-1.0f); glEnd(); SDL_GL_SwapBuffers(); rotqube +=0.9f; return true; } bool init_sdl(void) { if( SDL_Init( SDL_INIT_EVERYTHING ) != 0 ) { return false; } SDL_GL_SetAttribute( SDL_GL_RED_SIZE 5 ); SDL_GL_SetAttribute( SDL_GL_GREEN_SIZE 5 ); SDL_GL_SetAttribute( SDL_GL_BLUE_SIZE 5 ); SDL_GL_SetAttribute( SDL_GL_DEPTH_SIZE 16 ); SDL_GL_SetAttribute( SDL_GL_DOUBLEBUFFER 1 ); // TODO: Add error check to this screen surface init screen = SDL_SetVideoMode( screen_width screen_height screen_bpp SDL_OPENGL | SDL_HWSURFACE | SDL_RESIZABLE ); return true; } static void init_opengl() { float aspect = (float)screen_width / (float)screen_height; glViewport(0 0 screen_width screen_height); glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(60.0 aspect 0.1 100.0); glMatrixMode(GL_MODELVIEW); glClearColor(0.0 0.0 0.0 0); glEnable(GL_DEPTH_TEST); } void heartbeat() { float xrotrad yrotrad; int diffx diffy; SDL_Event event; while(1) { while(SDL_PollEvent(&event)) { switch(event.type) { case SDL_KEYDOWN: switch(event.key.keysym.sym) { case SDLK_ESCAPE: exit(0); break; case SDLK_w: yrotrad = (yrot / 180 * 3.141592654f); xrotrad = (xrot / 180 * 3.141592654f); xpos += (float)sin(yrotrad); zpos -= (float)cos(yrotrad); ypos -= (float)sin(xrotrad); std::cout << ""w pressed"" << std::endl; break; case SDLK_s: yrotrad = (yrot / 180 * 3.141592654f); xrotrad = (xrot / 180 * 3.141592654f); xpos -= (float)sin(yrotrad); zpos += (float)cos(yrotrad); ypos += (float)sin(xrotrad); break; case SDLK_d: yrotrad = (yrot / 180 * 3.141592654f); xpos += (float)cos(yrotrad) * 0.2; zpos += (float)sin(yrotrad) * 0.2; break; case SDLK_a: yrotrad = (yrot / 180 * 3.141592654f); xpos -= (float)cos(yrotrad) * 0.2; zpos -= (float)sin(yrotrad) * 0.2; break; default: break; } break; case SDL_MOUSEMOTION: diffx=event.motion.x-lastx; //check the difference between the current x and the last x position diffy=event.motion.y-lasty; //check the difference between the current y and the last y position lastx=event.motion.x; //set lastx to the current x position lasty=event.motion.y; //set lasty to the current y position xrot += (float)diffy; //set the xrot to xrot with the addition of the difference in the y position yrot += (float)diffx; //set the xrot to yrot with the addition of the difference in the x position break; case SDL_QUIT: exit(0); break; case SDL_VIDEORESIZE: screen = SDL_SetVideoMode( event.resize.w event.resize.h screen_bpp SDL_OPENGL | SDL_HWSURFACE | SDL_RESIZABLE ); screen_width = event.resize.w; screen_height = event.resize.h; init_opengl(); std::cout << ""Resized to width: "" << event.resize.w << "" height: "" << event.resize.h << std::endl; break; default: break; } } DrawCube(); camera(); SDL_Delay( 50 ); } } int main(int argc char* argv[]) { if( init_sdl() != false ) { std::cout << ""SDL Init Successful"" << std::endl; } init_opengl(); std::cout << ""Hello World"" << std::endl; heartbeat(); // this is essentially the main loop SDL_Quit(); return 0; } Makefile all: g++ -o test main.cpp -lSDL -lGL -lGLU It compiles and runs I guess I just need some help with doing the camera translation. Thanks Try rendering the camera before drawing the cube. Right now the camera translation is working fine but you're drawing the cube in the same position relative to the camera. If you draw the cube first then move the camera you should see the translation you were expecting.  Remove the glLoadIdentity() call from DrawCube(). Replace it with glPushMatrix() at the beginning and glPopMatrix() at the end. Now pressing 'w' does something. (I am not entirely sure what it is supposed to do.) The problem is glLoadIdentity clears all the previous transformations set up with glTranslatef and the like. Detailed description: http://www.opengl.org/documentation/specs/man_pages/hardcopy/GL/html/gl/pushmatrix.html",c++ opengl ubuntu sdl
1102918,A,"Windows not drawing above OpenGL windows I have an application with an OpenGL window as a child window of the main window. When I display a dialog box above the OpenGL window it doesn't get drawn. It's like it's not getting WM_PAINT messages. If I can guess the title bar position of the dialog box I can drag it and it's still responsive. I realise this might be a vague question but I was wondering if anyone else has seen this sort of behaviour before and knew of a solution? I wondered if the Pixel Format Descriptor would make a difference - I had PFD_DRAW_TO_WINDOW but changing to PDF_DRAW_TO_BITMAP didn't make any difference. I'm not sure what else I should be looking at? You may need to switch overlay off. It can be done via forcing back buffer presenting method to copy instead of swap. Use wglChoosePixelFormatARB and one of parameters should be WGL_SWAP_METHOD_ARB with value WGL_SWAP_COPY_ARB  This may seems stupid but are you sure your OpenGL window is not flagged ""topmost"" ? Does the dialog box disappear also behind borders of your window or just behind the OpenGL rendering rectangle ? No it's not topmost and yes it is only the OpenGL rendering rectangle that was the problem not the whole window. (However see my answer for solution!)  Bugger. Should have given all the details. I was running Windows in a virtual machine on Mac OS X using Parallels. I upgrade from Parallels 3 to 4 and now everything is working fine. I suspect a Parallels video driver issue. Thanks to all those who answered with suggestions.  Is your opengl window constantly rendering. It is possible that the 3D hardware is simply rendering into an overlay that is overdrawing your dialog box. If you position the dialog box so it overlaps your main window can you see some of it? Try to pause rendering into the main display to see if it effects the results. You will also need to make sure that your window style ensures the results are clipped... cs.style |= WS_CLIPSIBLINGS | WS_CLIPCHILDREN ; You should check though all the items mentioned in this MSDN article as it covers a lot of the basics for getting opengl rendering in a window correctly. http://msdn.microsoft.com/en-us/library/ms970745.aspx Thanks. A useful link.",c++ windows opengl
732894,A,Access violation with malloc() and glDrawPixels()? Can anyone see what's wrong with this code? SIZE_BG is 6MB as I am trying to draw a large bitmap image (3366x600). I use malloc to prevent my image from overflowing the stack. I get an access violation error on the call to glDrawPixels(). bgPtr seems to point to the correct data as I checked the first few bytes before calling glDrawPixels and they are correct.  bgPtr = (char*)malloc(SIZE_BG); fstream inFile(texFileName ios::in | ios::binary); inFile.read(bgPtr SIZE_BG); inFile.close(); //... other code glDrawPixels(3366 600 GL_BGRA_EXT GL_UNSIGNED_BYTE bgPtr+54); SIZE_BG is 6MB 3366 × 600 is approximately 1.92 million pixels BRGA indicates 4 bytes per pixel so 3366 × 600 × 4 is just over 7.7MB Therefore your buffer is too small... glDrawPixels() will read past the end into unallocated memory. Very true. Thanks! @sharptooth: seems revision comments are busted but i think i see where you were going with that edit now...,c++ opengl malloc filestream
775217,A,"What is a good way to load textures dynamically in OpenGL? Currently I am loading an image in to memory on a 2nd thread and then during the display loop (if there is a texture load required) load the texture. I discovered that I could not load the texture on the 2nd thread because OpenGL didn't like that; perhaps this is possible but I did something wrong - so please correct me if this is actually possible. On the other hand if my failure was valid - how do I load a texture without disrupting the rendering loop? Currently the textures take around 1 second to load from memory and although this isn't a major issue it can be slightly irritating for the user. That seems an exceptionally long time to load a texture into VRAM from system memory. How large is it and are you having OpenGL generate mipmap levels for you? Very large they are medical images. You can certainly load the texture from disk into RAM in any number of threads you like but OpenGL won't upload to VRAM in multiple threads for the reason mentioned in Reed's answer. Given the loading from disk is the slowest part thats the bit you'll probably want to thread. The loading thread(s) build up a queue of textures to be uploaded then this queue is consumed by the thread that owns the GL context (mind your access to that queue by the various threads however). You could also consider a non-threaded approach of uploading N textures per frame where N is a number that doesn't slow the rendering down too much. @Justin thanks for taking the time to answer. Perhaps my question is phrased badly; at present the heavy duty loading is done in the 2nd thread (this takes about 10 or so seconds to load because there's some image processing) then it takes only a brief moment to copy the pixels into VRAM during the GLUT loop; however while brief it does take an ""uncomfortable"" amount of time in terms of user experience. Okey that makes sense - post an answer here when you figure out what works for you!  You can load a texture from disk to memory on any thread you like using any tool you wish for reading the files. However when you bind it to OpenGL it's going to need to be handled on the same thread as the rendering for that OpenGL context. That being said this discussion suggests that using a PBO in a second thread is an option and can speed up the process. First paragraph not so helpful sort of repeating what I said hehe ;) but the 2nd paragraph looks very helpful thanks for the link! I'll certainly check it out :) Just saying that what you were doing there is valid - I wasn't sure from how you wrote it whether that was the case. Good luck! Understood. Thanks! :)",c++ opengl textures
683942,A,"What's wrong with my file dependencies? I have a program using 3 header files and 3 .cpp files in addition to main.cpp. I'm using VC++ 2008. Here's the setup. All three headers are guarded with #ifndef HEADERNAME_H etc. Also all three headers have corresponding .cpp files which #include their respective headers. /* main.cpp */ #include ""mainHeader.h"" /* mainHeader.h */ #include <windows.h> #include <iostream> //Others... #include ""Moo.h"" /* Moo.h */ #include ""mainHeader.h"" #include ""Foo.h"" class Moo { private: int varA; Foo myFoo1; Foo myFoo2; Foo myFoo3; Public: void setVarA(int); // defined in Moo.cpp //etc }; /* Foo.h */ #include ""mainHeader.h"" class Foo { ... }; I'm getting compiler errors for instantiating the three Foo's inside of Moo.h: error C2079: 'Moo::setVarA' uses undefined class 'Foo' I included Foo.h right there so why is it undefined? This is the only file that includes Foo.h. I even tried declaring Foo by placing 'class Foo;' before my declaration of class Moo. I also have #defines in my Foo.h file that are also causing compiler errors when I use them in Moo.h. (undeclared identifier error C2065). It seems like Foo.h isn't getting included because Moo.h can't find anything defined/declared in it. What's going on? My actual code is long but here you go (it's a mario platformer game btw): This would be Foo.h: //************************** // Animation.h //************************** // Header to Animation class #ifndef ANIMATION_H #define ANIMATION_H #include ""../Headers/MarioGame.h"" #define MAX_ANIMATIONS 58 extern char* fileAnimations[MAX_ANIMATIONS]; extern char marioAnims[MAX_ANIMATIONS][3000]; extern char background [3700000]; class Animation { private: int imageCount; public: DWORD lastAnimTick; int lastAnim; int anims[4][2]; DWORD animsTime[4]; // Constructor Animation(); // Mutators void addImage(int left int right DWORD time); void defaultAnimation(); // Accessors int getImage(bool facingLeft); int getImageCount(); int getCurLoc(); }; #endif // ANIMATION_H This would be ""Moo.h."" Note the many private Animation members all causing errors: //************************** // Mario.h //************************** // Header to Mario class #ifndef MARIO_H #define MARIO_H #include ""../Headers/MarioGame.h"" #include ""../Headers/Animation.h"" enum { MARIO_TYPE_SMALL MARIO_TYPE_BIG MARIO_TYPE_LEAF }; class Animation; class Mario { private: #pragma region Variable Declarations float xPos; float yPos; float xVel; float yVel; float lastXVel; //used for determining when walk/run decceleration is complete float xAccel; float yAccel; float xSize; float ySize; float halfW; float halfH; bool grounded; bool walking; bool running; bool pRunning; bool jumping; bool hunching; bool gliding; bool flying; bool decelerating; int facing; DWORD pRunTimer; int pChargeLevel; DWORD jumpStart; DWORD glideStart; int type; bool updateXMovement; bool updateYMovement; bool screenUpLock; char marioAnims[MAX_ANIMATIONS][3000]; Animation smallStand; Animation smallWalk; Animation smallRun; Animation smallPRun; Animation smallJump; Animation smallSkid; Animation bigStand; Animation bigWalk; Animation bigRun; Animation bigPRun; Animation bigJump; Animation bigSkid; Animation bigHunch; Animation leafStand; Animation leafWalk; Animation leafRun; Animation leafPRun; Animation leafJump; Animation leafSkid; Animation leafHunch; Animation leafTailWhack; Animation leafGlide; Animation leafFly; Animation leafFalling; Animation* lastAnim; #pragma endregion public: //Constructor Mario(); //Mutators void setGlideStart( DWORD g ); void setHunching( bool h ); void setGliding( bool g ); void setFlying( bool f ); void setType( int t ); void setPChargeLevel( int c ); void setPRunTimer( DWORD t ); void setScreenUpLock( bool s ); void setUpdateXMovement( bool m ); void setUpdateYMovement( bool m ); void setDecelerating( bool d ); void setPos( float x float y ); void setVel( float x float y ); void setAccel( float x float y ); void setSize( float x float y ); void setJumping( bool j ); void setJumpStart( DWORD s ); void setGrounded( bool g ); void setWalking( bool w ); void setRunning( bool r ); void setPRunning( bool r ); void setLastXVel( float l ); void setFacing( int f ); void defaultAnimations(); //Accessors DWORD getGlideStart(); bool getHunching(); bool getGliding(); bool getFlying(); int getType(); int getPChargeLevel(); DWORD getPRunTimer(); bool getScreenUpLock(); bool getUpdateXMovement(); bool getUpdateYMovement(); bool getDecelerating(); float getXPos(); float getYPos(); float getXVel(); float getYVel(); float getXAccel(); float getYAccel(); bool getJumping(); DWORD getJumpStart(); float getXSize(); float getYSize(); float getHalfW(); float getHalfH(); bool getGrounded(); bool getWalking(); bool getRunning(); bool getPRunning(); float getLastXVel(); int getFacing(); int getAnimation(); }; #endif // MARIO_H This would be ""mainHeader.h"": //************************** // MarioGame.h //************************** // Header to MarioGame functions // Contains Includes Defines Function Declarations Namespaces for program #ifndef MARIOGAME_H #define MARIOGAME_H //*===================== // Defines //*===================== #define WINDOWED 0 // predefined flags for initialization #define FULLSCREEN 1 #define WNDCLASSNAME ""MarioGame"" // window class name #define WNDNAME ""Mario Game"" // string that will appear in the title bar #define NUM_OF_KEYS 5 #define KEY_SPACE 0 #define KEY_UP 1 #define KEY_DOWN 2 #define KEY_RIGHT 3 #define KEY_LEFT 4 #define KEY_CONTROL 5 #define GRIDW 2.0f #define GRIDH 2.0f #define PATHING_SIZE 33 //*===================== // Includes //*===================== #include <windows.h> #include <gl/gl.h> #include <gl/glu.h> #include <iostream> #include <fstream> #include <vector> #include <math.h> #include <WAVEMIX.H> #include ""../Console/guicon.h"" #include ""../Headers/Mario.h"" //*===================== // Function Declarations //*===================== LRESULT CALLBACK WinProc(HWND hwnd UINT msg WPARAM wparam LPARAM lparam); HWND createWindow(HINSTANCE &hinst int width int height int depth); void renderFrame(); void think(); void loadTextures(); void WMInit(HINSTANCE HWND); void resize (int width int height); void shutdown(); void keyLeft(bool); void keyRight(bool); void keySpace(bool); void keyDownArrow(bool); bool checkBoundary(float float); void onPlayerDeath(); class Mario; //*===================== // Namespaces //*===================== using namespace std; //*===================== // Global Variable Declarations //*===================== extern Mario Player; extern HDC hdc; extern HGLRC hglrc; extern HWND hwnd; extern int SCRW; extern int SCRH; extern int SCRD; extern DWORD oldTick; extern DWORD oldTick2; extern DWORD oldPTime; extern float pixelZoom; extern float screenPosX; extern float screenPosY; extern float playerScrollMultiplier; extern float playerTerminalWalkVel; extern float playerWalkAccel; extern float playerRunAccel; extern float playerTerminalRunVel; extern float playerDecel; extern float playerPVel; extern DWORD playerPRunAchieveTime; extern float playerJumpUpVel; extern float playerJumpTime; extern float gravityAccel; extern float playerTerminalFallVel; extern float playerTerminalGlideFallVel; extern bool keyDown[NUM_OF_KEYS]; extern bool lastSpace; extern bool drawPathingMap; extern float pathing [PATHING_SIZE][5][2]; #endif // MARIOGAME_H Here's main.cpp: //************************** // main.cpp //************************** // Primary implementation file; handles Win32 interface #include ""../Headers/MarioGame.h"" //*===================== // WinMain //*===================== int WINAPI WinMain(HINSTANCE hinstance HINSTANCE hprevinstance LPSTR lpcmdline int nshowcmd) { ... } //And other functions.... Please more of the real code rather than pseudo-code that approximates what you think the (faulty) original does. All the subtle errors that real code contain won't be propagated. @Adam: Actually a working testcase is better. I won't read all of his code. You need a forward declaration for class Foo. For more information refer to item 31 of ""Effective C++ Third Edition"". Note: if you forward declare Foo that means your class Moo will only be able to have pointers of type Foo. If something includes Foo.h this is what happens (the arrows show dependency): Foo.h --includes--> mainHeader.h --includes--> Moo.h --includes--> Foo.h Note that when class Moo is specified the second Foo.h is not included due to your guards also class Foo has not been declared yet because that happens after including mainheader.h First is it possible to do this without forward declaration and without pointers but actual classes? Second what about a #define in Foo.h that is not found by Moo.h??? If you are able to ensure that Foo.h is ALWAYS included before Moo.h you shouldn't have a problem. Your example only shows Moo.h including Foo.h but I don't think this is the case. As others have suggested you should post a more complete example that fails. Thank a bunch you saw the problem but it took the preprocessor dump file (thanks Dan Olsen) for me to see it.  The best tool for debugging these sorts of situations is the compiler's ""dump preprocessed output to a file"" option. If you enable this you'll most likely see the problem right away. Check your compiler options for how to enable it. Great tip this helped me see the problem.  Another problem to watch out for is the use of 'using namespace' statements - you should not use these in header files at all. This is to avoid ambiguities for modules that include lots of different header files. So yes without it you will need to do lots of std::vector std::list etc which can be a pain.",c++ visual-c++ opengl dependencies
385629,A,Draw Order in OpenGL I am rendering an OpenGL scene that include some bitmap text. It is my understanding the order I draw things in will determine which items are on top. However my bitmap text even though I draw it last is not on top! For instance I am drawing: 1) Background 2) Buttons 3) Text All at the same z depth. Buttons are above the background but text is invisible. It I change the z depth of the text I can see it but I then have other problems. I am using the bitmap text method from Nehe's Tutorials. How can I make the text visible without changing the z depth? You can also use glDepthFunc (GL_ALWAYS). If you vote this answer down please explain what is wrong about it.  You can simply disable the z-test via  glDisable (GL_DEPTH_TEST); // or something related.. If you do so the Z of your text-primitives will be ignored. Primitives are drawn in the same order as your call the gl-functions. Another way would be to set some constant z-offset via glPolygonOffset (not recommended) or set the depth-compare mode to something like GL_LESS_EQUAL (the EQUAL is the important one). That makes sure that primitives drawn with the same depth are rendered ontop of each other. Hope that helps. Yep turn off z-buffering for UI drawing. Good answer--I was going to suggest all of the same! :),c++ opengl
721705,A,How do I set the opacity of a vertex in OpenGL? The following snippet draws a gray square. glColor3b(50 50 50); glBegin(GL_QUADS); glVertex3f(-1.0 +1.0 0.0); // top left glVertex3f(-1.0 -1.0 0.0); // bottom left glVertex3f(+1.0 -1.0 0.0); // bottom right glVertex3f(+1.0 +1.0 0.0); // top right glEnd(); In my application behind this single square exists a colored cube. What function should I use to make square (and only this square) opaque? Use glColor4 instead of glColor3. For example: glBlendFunc(GL_SRC_ALPHAGL_ONE); glColor4f(1.0f1.0f1.0f0.5f); Thanks. Hmm doesn't seem to be working for me what could I be doing wrong?  glColor4f(float rfloat g float b flaot alpha); (in your case maybe clColor4b) also make sure that blending is enabled. (you have to reset the color to non-alpha afterwads which might involve a glGet* to save the old vertexcolor)  You can set colors per vertex glBegin(GL_QUADS); glColor4f(1.0 0.0 0.0 0.5); // red 50% alpha glVertex3f(-1.0 +1.0 0.0); // top left // Make sure to set the color back since the color state persists glVertex3f(-1.0 -1.0 0.0); // bottom left glVertex3f(+1.0 -1.0 0.0); // bottom right glVertex3f(+1.0 +1.0 0.0); // top right glEnd();  In the init function use these two lines: glEnable(GL_BLEND); glBlendFunc(GL_SRC_ALPHA GL_ONE_MINUS_SRC_ALPHA); And in your render function ensure that glColor4f is used instead of glColor3f and set the 4th argument to the level of opacity required. glColor4f(1.0 1.0 1.0 0.5); glBegin(GL_QUADS); glVertex3f(-1.0 +1.0 0.0); // top left glVertex3f(-1.0 -1.0 0.0); // bottom left glVertex3f(+1.0 -1.0 0.0); // bottom right glVertex3f(+1.0 +1.0 0.0); // top right glEnd();,c++ opengl transparency opacity
532663,A,Why does wgluseFontBitmaps consume too much memory on some computers? I'm creating a game in OpenGL which loads the entire Arial Unicode MS font when it loads. The program uses on avg. 10 megs of memory on my computer (op sys is WinXP SP2) and runs without problems but when I move the program to my laptop (with Vista) the wglUseFontBitmaps hangs and allocates memory fluently and never returns. This problem occured recently and I have no idea why and never had such problem before. Why does wglUseFontBitmaps do this and how to fix it? update: I tried an older version and it runs but eats 400megs of memory (so it is not a new problem) How many glyph display lists are you trying to generate with wglUseFontBitmaps()? Can you show us your invocation? Perhaps Vista is trying to do all 60000-some-odd glyphs in one go and XP is doing some sort of on-demand construction? I've had good luck with FreeType2 and MS Arial Unicode though it does take some time to get up to speed with the API. This tutorial can be C++-ized to great effect. I loaded just the entire font... #define UNICODEFONTSIZE 65535 ... SelectObject(hdchfontArialUnicodeStuff); wglUseFontBitmapsW(hdc 0 UNICODEFONTSIZEListBase); ... Please note this does not cause any problems on XP. I have heard that Vista has some OpenGL issues. This may be one of them. For XP 10MB Mem Usage or Mem Usage + VM Size? overall mem usage Do the machines have the same DPI? Does the machines have different ClearType settings?,c++ c winapi opengl graphics
1370111,A,"Looking for C++ implementation of OpenGL gears example I have often seen the spinning gears OpenGL example ( I think originally done by SGI) but I today I have only been able to find C and Ruby implementations can anyone point me to a c++ implementation? I think that if you compile the C version with a C++ compiler it will work. Why do you want a C++ version? Removed ""gears"" tag which usually refers to Google Gears and may be misleading in this context If you want to mess around with OpenGL i strongly reccomend using OpenSceneGraph (OSG) since you can focus better on computer graphics aspects instead. It's using all the C++ magic and design patterns.  What in particular would you be looking for in a C++ implementation that the C one doesn't provide? OpenGL is a C API and thus a C demonstration is practical. A C++ implementation would call all the same functions in the same order and to the same effect it would likely just wrap the implementation in an object. This doesn't really further one's understanding of the core API and can possibly add a layer of obfuscation to those not familiar with some C++ styles and patterns. If what you are really looking for is an example of initiating OpenGL wrapped in a C++ framework I made a few of those a while back. You can find them here. Please note that I'm no longer actively maintaining the code or page though.",c++ opengl
577125,A,DDS texture loading How would I load a dds texture file into an OpenGL 2dtexture or cube map texture? If the DDS contains a compressed texture then use glCompressedTexImage2DARB() if it contains uncompressed data the usual glTexImage2D procedure applies. Once for each mipmap level if the DDS file contains mipmaps and once for each cubemap face if its a cubemap. For how to go about reading the header and data in a DDS file look up the documentation for it on MSDN or in the DirectX SDK. It's a fairly standard container format there aren't too many surprises. Be aware that DDS uses a top-left image origin whereas OpenGL assumes a bottom-left origin for image data. This means you will probably want to vertically flip a DDS image after loading it in. You can do this without decompressing them if they are in DXT1/3/5 but it's a slightly fiddly process that involves bit manipulation on the contents of each 4x4 compression block.  Depending on your needs the DevIL library can take care of feeding OpenGL with a DDS file content.  I believe you use the glCompressedTexImage2DARB method and its friends. This PDF seems to contain some promising info that may be helpful to you.,c++ opengl textures directdraw
199016,A,"wglCreateContext in C# failing but not in managed C++ I'm trying to use opengl in C#. I have following code which fails with error 2000 ERROR_INVALID_PIXEL_FORMAT First definitions: [DllImport(""user32.dll"" CharSet = CharSet.Auto SetLastError = true ExactSpelling = true)] public static extern IntPtr GetDC(IntPtr hWnd); [StructLayout(LayoutKind.Sequential)] public struct PIXELFORMATDESCRIPTOR { public void Init() { nSize = (ushort) Marshal.SizeOf(typeof (PIXELFORMATDESCRIPTOR)); nVersion = 1; dwFlags = PFD_FLAGS.PFD_DRAW_TO_WINDOW | PFD_FLAGS.PFD_SUPPORT_OPENGL | PFD_FLAGS.PFD_DOUBLEBUFFER | PFD_FLAGS.PFD_SUPPORT_COMPOSITION; iPixelType = PFD_PIXEL_TYPE.PFD_TYPE_RGBA; cColorBits = 24; cRedBits = cRedShift = cGreenBits = cGreenShift = cBlueBits = cBlueShift = 0; cAlphaBits = cAlphaShift = 0; cAccumBits = cAccumRedBits = cAccumGreenBits = cAccumBlueBits = cAccumAlphaBits = 0; cDepthBits = 32; cStencilBits = cAuxBuffers = 0; iLayerType = PFD_LAYER_TYPES.PFD_MAIN_PLANE; bReserved = 0; dwLayerMask = dwVisibleMask = dwDamageMask = 0; } ushort nSize; ushort nVersion; PFD_FLAGS dwFlags; PFD_PIXEL_TYPE iPixelType; byte cColorBits; byte cRedBits; byte cRedShift; byte cGreenBits; byte cGreenShift; byte cBlueBits; byte cBlueShift; byte cAlphaBits; byte cAlphaShift; byte cAccumBits; byte cAccumRedBits; byte cAccumGreenBits; byte cAccumBlueBits; byte cAccumAlphaBits; byte cDepthBits; byte cStencilBits; byte cAuxBuffers; PFD_LAYER_TYPES iLayerType; byte bReserved; uint dwLayerMask; uint dwVisibleMask; uint dwDamageMask; } [Flags] public enum PFD_FLAGS : uint { PFD_DOUBLEBUFFER = 0x00000001 PFD_STEREO = 0x00000002 PFD_DRAW_TO_WINDOW = 0x00000004 PFD_DRAW_TO_BITMAP = 0x00000008 PFD_SUPPORT_GDI = 0x00000010 PFD_SUPPORT_OPENGL = 0x00000020 PFD_GENERIC_FORMAT = 0x00000040 PFD_NEED_PALETTE = 0x00000080 PFD_NEED_SYSTEM_PALETTE = 0x00000100 PFD_SWAP_EXCHANGE = 0x00000200 PFD_SWAP_COPY = 0x00000400 PFD_SWAP_LAYER_BUFFERS = 0x00000800 PFD_GENERIC_ACCELERATED = 0x00001000 PFD_SUPPORT_DIRECTDRAW = 0x00002000 PFD_DIRECT3D_ACCELERATED = 0x00004000 PFD_SUPPORT_COMPOSITION = 0x00008000 PFD_DEPTH_DONTCARE = 0x20000000 PFD_DOUBLEBUFFER_DONTCARE = 0x40000000 PFD_STEREO_DONTCARE = 0x80000000 } public enum PFD_LAYER_TYPES : byte { PFD_MAIN_PLANE = 0 PFD_OVERLAY_PLANE = 1 PFD_UNDERLAY_PLANE = 255 } public enum PFD_PIXEL_TYPE : byte { PFD_TYPE_RGBA = 0 PFD_TYPE_COLORINDEX = 1 } [DllImport(""gdi32.dll"" CharSet = CharSet.Auto SetLastError = true ExactSpelling = true)] public static extern int ChoosePixelFormat(IntPtr hdc [In] ref PIXELFORMATDESCRIPTOR ppfd); [DllImport(""gdi32.dll"" CharSet = CharSet.Auto SetLastError = true ExactSpelling = true)] public static extern bool SetPixelFormat(IntPtr hdc int iPixelFormat ref PIXELFORMATDESCRIPTOR ppfd); [DllImport(""opengl32.dll"" CharSet = CharSet.Auto SetLastError = true ExactSpelling = true)] public static extern IntPtr wglCreateContext(IntPtr hDC); And now the code that fails: IntPtr dc = Win.GetDC(hwnd); var pixelformatdescriptor = new GL.PIXELFORMATDESCRIPTOR(); pixelformatdescriptor.Init(); var pixelFormat = GL.ChoosePixelFormat(dc ref pixelformatdescriptor); if(!GL.SetPixelFormat(dc pixelFormat ref pixelformatdescriptor)) throw new Win32Exception(Marshal.GetLastWin32Error()); IntPtr hglrc; if((hglrc = GL.wglCreateContext(dc)) == IntPtr.Zero) throw new Win32Exception(Marshal.GetLastWin32Error()); //<----- here I have exception the same code in managed C++ is working HDC dc = GetDC(hWnd); PIXELFORMATDESCRIPTOR pf; pf.nSize = sizeof(PIXELFORMATDESCRIPTOR); pf.nVersion = 1; pf.dwFlags = PFD_DRAW_TO_WINDOW | PFD_SUPPORT_OPENGL | PFD_DOUBLEBUFFER | PFD_SUPPORT_COMPOSITION; pf.cColorBits = 24; pf.cRedBits = pf.cRedShift = pf.cGreenBits = pf.cGreenShift = pf.cBlueBits = pf.cBlueShift = 0; pf.cAlphaBits = pf.cAlphaShift = 0; pf.cAccumBits = pf.cAccumRedBits = pf.cAccumGreenBits = pf.cAccumBlueBits = pf.cAccumAlphaBits = 0; pf.cDepthBits = 32; pf.cStencilBits = pf.cAuxBuffers = 0; pf.iLayerType = PFD_MAIN_PLANE; pf.bReserved = 0; pf.dwLayerMask = pf.dwVisibleMask = pf.dwDamageMask = 0; int ipf = ChoosePixelFormat(dc &pf); SetPixelFormat(dc ipf &pf); HGLRC hglrc = wglCreateContext(dc); I've tried it on VIsta 64-bit with ATI graphic card and on Windows XP 32-bit with Nvidia with the same result in both cases. Also I want to mention that I don't want to use any already written framework for it. Can anyone show me where is the bug in C# code that is causing the exception? Calling wglCreateContext twice helps too. if (SetPixelFormat(DC iPixelformat ref pfd) == false) throw new Win32Exception(Marshal.GetLastWin32Error()); RC = wglCreateContext(DC); if (RC == HGLRC.Zero) { if (SetPixelFormat(DC iPixelformat ref pfd) == false) throw new Win32Exception(Marshal.GetLastWin32Error()); RC = wglCreateContext(DC); if (RC == HGLRC.Zero) throw new Win32Exception(Marshal.GetLastWin32Error()); }  Found solution. Problem is very strange ugly and really hard to find. Somwhere on the internet I found that when you are linking opengl32.lib while compiling c++ application it must be placed before gdi32.lib. The reason for this is that (supposedly) opengl32.dll is overwriting ChoosePixelFormat and SetPixelFormat functions (and probably more :-). As I found in my c++ version accidentally it was the case. Heh but how to do it in C# After few days of searching I found that in tao framework they solved it using kernel32.dll LoadLibrary() function and loading opengl32.dll before calling SetPixelFormat public static bool SetPixelFormat(IntPtr deviceContext int pixelFormat ref PIXELFORMATDESCRIPTOR pixelFormatDescriptor) { Kernel.LoadLibrary(""opengl32.dll""); return _SetPixelFormat(deviceContext pixelFormat ref pixelFormatDescriptor); } So we know that opengl32.dll must be loaded before gdi32.dll is there any other way of doing this. After while I thought that we can call some NOP function from opengl32.dll to load it. For example: [DllImport(""opengl32.dll"" EntryPoint = ""glGetString"" CharSet = CharSet.Auto SetLastError = true ExactSpelling = true)] static extern IntPtr _glGetString(StringName name); public static string glGetString(StringName name) { return Marshal.PtrToStringAnsi(_glGetString(name)); } public enum StringName : uint { GL_VENDOR = 0x1F00 GL_RENDERER = 0x1F01 GL_VERSION = 0x1F02 GL_EXTENSIONS = 0x1F03 } and on the start of application before any call to gdi32.dll I use this: GL.glGetString(0); Both ways solves the problem.  I cannot test this right now but my first suspicion would be the structure packing. Have you tried setting the packing to 1 in the StructLayout attribute? For example: [StructLayout(LayoutKind.Sequential Pack=1)] Cheers Brian",c# c++ opengl
734259,A,"With OpenGL how can I use gluOrtho2D properly with default projection? I'm trying to use gluOrtho2D with glutBitmapCharacter so that I can render text on the screen as well as my 3D objects. However when I use glOrtho2D my 3D objects dissapear; I assume this is because I'm not setting the projection back to the OpenGL/GLUT default but I'm not really sure what that is. Anyway this is the function I'm using to render text: void GlutApplication::RenderString(Point2f point void* font string s) { glMatrixMode(GL_PROJECTION); glPushMatrix(); glLoadIdentity(); gluOrtho2D(0.0 WindowWidth 0.0 WindowHeight); glMatrixMode(GL_MODELVIEW); glPushMatrix(); glLoadIdentity(); glDisable(GL_TEXTURE); glDisable(GL_TEXTURE_2D); glRasterPos2f(point.X point.Y); for (string::iterator i = s.begin(); i != s.end(); ++i) { glutBitmapCharacter(font *i); } glEnable(GL_TEXTURE); glEnable(GL_TEXTURE_2D); glMatrixMode(GL_MODELVIEW); glPopMatrix(); glMatrixMode(GL_PROJECTION); glPopMatrix(); } And the render function is similar to this: glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glPushMatrix(); glLoadIdentity(); // Do some translation here. // Draw some 3D objects. glPopMatrix(); // For some reason this stops the above from being rendered // where the camera is facing (I assume they are still being rendered). Point2f statusPoint(10 10); RenderString(statusPoint GLUT_BITMAP_9_BY_15 ""Loading...""); Your code looks okay. Most likely you've just messed up the matrix stack somewhere. I suggest that you check if you forgot a glPopMatrix somewhere. To do so you can get the stack depth via glGet(GL_MODELVIEW_STACK_DEPTH). Getters for the other matrix-stacks are available as well. Also you can take a look at the current matrix. Call glGetFloatv(GL_MODELVIEW_MATRIX Pointer_To_Some_Floats) to get it. You can print out the floats each time you've set up a modelview or projection matrix. That way you should be able to to find out which matrix erratically ends up as the current matrix. That should give you enough clues to find the bug.  When I needed something similar I didn't try to push and pop the matrix states I just set everything back from scratch each time: void set2DMode() { glMatrixMode(GL_PROJECTION); glLoadIdentity(); glOrtho(0 w h 0 -1 1); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } void set3DMode() { glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(50.0 (float) w / h 1 1024); gluLookAt(0 0 400 0 0 0 0.0 1.0 0.0); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } void cb_display(void) { set3DMode(); // draw some stuff set2DMode(); // draw some text // and some more }  I agree with Andrei. Basiaclly what you do is: at second and all next frames just after your glClear work with projection matrix because last operation was glMatrixMode(GL_PROJECTION); at the end of RenderString. (at least looking at the code that you've posted). Try to put: glMatrixMode(GL_MODELVIEW) right after glClear.  You're in projection mode after calling the RenderString() function. Not sure if that would break anything but it stood out to me.",c++ opengl
668325,A,"SwapBuffers crashing my program! I have an OpenGL program that works on all of my computers but one. It's a desktop with Vista 64 and a Radeon HD4850. The problem seems to be in my call to SwapBuffers(hdc). It compiles fine and then gives me an exception: Unhandled exception at 0x00000000 in Program.exe: 0xC0000005: Acces violation. Using VC++ to break before the call to SwapBuffers shows hdc's value to be: 0xfe011734 {unused=???} CXX0030: Error: expression cannot be evaluated Anyone have a clue what could be happening? Is there something about SwapBuffers that would change from one PC to the next? I've gotten it to work on XP32 XP64 and a (different) Vista64. while (!quit) { if (PeekMessage(&msg NULL NULL NULL PM_REMOVE)) { if (msg.message == WM_QUIT) quit = true; TranslateMessage(&msg); DispatchMessage(&msg); } renderFrame(); //draws the scene SwapBuffers(hdc); if (GetAsyncKeyState(VK_ESCAPE)) shutdown(); think(); //calculates object positions etc. } The drivers on the problematic system (HD4850) are up-to-date. I've run and wrote the program on another Vista64 system with a Radeon HD4870 also with up-to-date drivers. As far as I know the drivers for these two cards are nearly identical as both are in the HD48xx series. For that reason it seems odd that the GPU is causing the problem. Anyway am I wrong or is this a memory issue? (Access violation) Also if I remove the call to SwapBuffers(hdc) the program runs seemingly well although nothing is drawn of course because the framebuffers are never swapped. But it is at least stable. Call stack (-> is stack ptr):  ATKOGL32.dll!6aef27bc() opengl32.dll!665edb2d() opengl32.dll!665f80d1() gdi32.dll!75e14104() -> MyProg.exe!WinMain(HINSTANCE__ * hinstance=0x009a0000 HINSTANCE__ * hprevinstance=0x00000000 char * lpcmdline=0x003b4a51 int nshowcmd=1) Line 259 + 0xe bytes MyProg.exe!__tmainCRTStartup() Line 578 + 0x35 bytes MyProg.exe!WinMainCRTStartup() Line 400 kernel32.dll!7641e3f3() ntdll.dll!777dcfed() ntdll.dll!777dd1ff() Heres the assembly (-> is the next instruction to be executed):  SwapBuffers(hdc); 009B1B5C mov esiesp 009B1B5E mov eaxdword ptr [hdc (9BF874h)] 009B1B63 push eax 009B1B64 call dword ptr [__imp__SwapBuffers@4 (0E1040Ch)] -> 009B1B6A cmp esiesp 009B1B6C call @ILT+780(__RTC_CheckEsp) (9B1311h) Whatever hdc is set to it doesn't look to be a proper value. Is the window created before this call? Is there any multithreading involved with this application that could hurt hdc? Try creating a watch on the address of hdc itself and see when the value is changed to be an invalid location that might give you a hint as to where it changes. I'm not doing multithreading. hdc seems to never change before the crash. On my other computers the value of hdc is the same (unused; expression cannot be evaluated). I installed an older display driver and things work until I restart the PC. Then I have to reinstall the driver for it to work. Try swapping in the 4850 into the other working Vista computer - see if it's the drivers or the card itself? Alright wow. My further actions upon discovering that would be to start messing with the Hardware. Swap out the video card see if that helps? I swapped my HD4850 with the HD4870 on my other working PC. No problems. Appears to be a bug in the HD4850?  This is almost definitely a bug in the drivers. The reason why you can't see the value of hdc is because the top stackframe for the crash is actually inside ATKOGL32.dll but since there are no symbols for that the debugger shows you your code. As far as I can tell ATKOGL32.dll is actually an ASUS wrapper for the ATI driver and that's where the crash happens. You might want to install stock ATI drivers from amd.com and see if the crash still persists. While the driver should never crash no matter what series of OpenGL calls you make in my experience usually the crashes are the result of some kind of invalid call that your program makes. Technically this should just be ignored and the error state set but thats not always what happens. You can check for any invalid OpenGL calls easily by using a program like gDebugger. Over two years after I asked this question as well as multiple posts on other tech forums sites I still haven't been able to resolve the problem on this card. I have to reinstall my drivers every time I restart the computer. I'm convinced the card is faulty... I'll accept your answer since you're probably the closest. Try adding ""glFlush();"" right before calling ""SwapBuffers"".  It looks like you could be accessing the HDC after the window has been destroyed does the problem disappear if you break out of the loop as soon as you get WM_QUIT ? Nope. The crash happens immediately when I start the program. I tried putting a break; in after quit = true; and same problem... Can you add what the crashing callstack is?",c++ winapi opengl
818762,A,"Unflip wxImage loading I have the code here working fine except that all the non-power of 2 images are flipped in the y direction. In the wxImageLoader file there is this loop which I believe is the culprit:  for(int y=0; y<newHeight; y++) { for(int x=0; x<newWidth; x++) { if( x<(*imageWidth) && y<(*imageHeight) ){ imageData[(x+y*newWidth)*bytesPerPixel+0]= bitmapData[( x+(rev_val-y)*(*imageWidth))*old_bytesPerPixel + 0]; imageData[(x+y*newWidth)*bytesPerPixel+1]= bitmapData[( x+(rev_val-y)*(*imageWidth))*old_bytesPerPixel + 1]; imageData[(x+y*newWidth)*bytesPerPixel+2]= bitmapData[( x+(rev_val-y)*(*imageWidth))*old_bytesPerPixel + 2]; if(bytesPerPixel==4) imageData[(x+y*newWidth)*bytesPerPixel+3]= alphaData[ x+(rev_val-y)*(*imageWidth) ]; } else { imageData[(x+y*newWidth)*bytesPerPixel+0] = 0; imageData[(x+y*newWidth)*bytesPerPixel+1] = 0; imageData[(x+y*newWidth)*bytesPerPixel+2] = 0; if(bytesPerPixel==4) imageData[(x+y*newWidth)*bytesPerPixel+3] = 0; } }//next }//next But I can't figure out how to un-flip the images. The correct for loop is: for(int y=0; y<newHeight; y++) { for(int x=0; x<newWidth; x++) { if( x<(*imageWidth) && y<(*imageHeight) ){ imageData[(x+y*newWidth)*bytesPerPixel+0]= bitmapData[( x+y*(*imageWidth))*old_bytesPerPixel + 0]; imageData[(x+y*newWidth)*bytesPerPixel+1]= bitmapData[( x+y*(*imageWidth))*old_bytesPerPixel + 1]; imageData[(x+y*newWidth)*bytesPerPixel+2]= bitmapData[( x+y*(*imageWidth))*old_bytesPerPixel + 2]; if(bytesPerPixel==4) imageData[(x+y*newWidth)*bytesPerPixel+3]= alphaData[ x+y*(*imageWidth) ]; } else { imageData[(x+y*newWidth)*bytesPerPixel+0] = 0; imageData[(x+y*newWidth)*bytesPerPixel+1] = 0; imageData[(x+y*newWidth)*bytesPerPixel+2] = 0; if(bytesPerPixel==4) imageData[(x+y*newWidth)*bytesPerPixel+3] = 0; } }//next }//next this solved the flipping problem Did you solve your own problem?? Or do you mean that the above is what you meant to post originally??  In your loop you use (rev_val - y) for the index of the pixels of your ""old"" image. This will cause the image to flip. Try to find an alternative. From your code you posted it is difficult to know what the function of rev_val is. the full code is in the link I posted in the question OK. I missed it. The code flips the image on purpose. But it seems to do the same for power of two images. Try to debug it by replacing rev_val-y with y and see what happens.",c++ opengl wxwidgets
384200,A,"C++ Operator Ambiguity Forgive me for I am fairly new to C++ but I am having some trouble regarding operator ambiguity. I think it is compiler-specific for the code compiled on my desktop. However it fails to compile on my laptop. I think I know what's going wrong but I don't see an elegant way around it. Please let me know if I am making an obvious mistake. Anyhow here's what I'm trying to do: I have made my own vector class called Vector4 which looks something like this: class Vector4 { private: GLfloat vector[4]; ... } Then I have these operators which are causing the problem: operator GLfloat* () { return vector; } operator const GLfloat* () const { return vector; } GLfloat& operator [] (const size_t i) { return vector[i]; } const GLfloat& operator [] (const size_t i) const { return vector[i]; } I have the conversion operator so that I can pass an instance of my Vector4 class to glVertex3fv and I have subscripting for obvious reasons. However calls that involve subscripting the Vector4 become ambiguous to the compiler: enum {x y z w} Vector4 v(1.0 2.0 3.0 4.0); glTranslatef(v[x] v[y] v[z]); Here are the candidates: candidate 1: const GLfloat& Vector4:: operator[](size_t) const candidate 2: operator[](const GLfloat* int) <built-in> Why would it try to convert my Vector4 to a GLfloat* first when the subscript operator is already defined on Vector4? Is there a simple way around this that doesn't involve typecasting? Am I just making a silly mistake? Thanks for any help in advance. The const applied to the size_t arguments is not needed - it is pointless. However it is benign and has no bearing on your problem. You should tell us what compilers you're using.... I am unable to check which compiler I used on my desktop at the moment but the compiler I am using on my laptop (which gives me the error) is g++ 4.3.2. I am using Eclipse and have these flags on by default: -O0 -g3 -Wall -c -fmessage-length=0 Why are you passing a ""const size_t"" to operator[] ? There seems to be a concern with my use of const before the by-value size_t parameter. I know it is not necessary but I think it makes it explicitly clear that the value should not change as we are using it as an index. Can someone explain why this is so undesirable? @Scott Is not undesirable but just unnecessary in many cases as copying of intrinsic types like size_t or int is no-cost operation and usually they are passed by value anyway so no side-effects left outside function - such function argument is local to function. The const size_t is helpful to keep function self-describing and indicate variable never changes its value.  This is explained in the book ""C++ Templates - The Complete Guide"". It's because your operator[] takes size_t but you pass a different type which first has to undergo an implicit conversion to size_t. On the other side the conversion operator can be chosen too and then the returned pointer can be subscript. So there is the ambiguity. Solution is to drop the conversion operator. They should generally be avoided as they just introduce problems as you see. Provide a begin and end member function that returns vector and vector + 4 respectively. Then you can use v.begin() if you want to pass to native openGL functions. There is a bit confusion in the comments. I think i will update this answer now to reflect the most recent concept of this. struct Vector4 { // some of container requirements typedef GLfloat value_type; typedef GLfloat& reference; typedef GLfloat const& const_reference; typedef GLfloat * iterator; typedef GLfloat const * const_iterator; typedef std::ptrdiff_t difference_type; typedef std::size_t size_type; static const size_type static_size = 4; // returns iterators to the begin and end iterator begin() { return vector; } iterator end() { return vector + size(); } const_iterator begin() const { return vector; } const_iterator end() const { return vector + size(); } size_type size() const { return static_size; } size_type max_size() const { return static_size; } void swap(Vector4 & that) { std::swap(*this that); } // some of sequences reference operator[](size_type t) { return vector[t]; } const_reference operator[](size_type t) const { return vector[t]; } // specific for us. returns a pointer to the begin of our buffer. // compatible with std::vector std::array and std::string of c++1x value_type * data() { return vector; } value_type const* data() const { return vector; } // comparison stuff for containers friend bool operator==(Vector4 const&a Vector4 const&b) { return std::equal(a.begin() a.end() b.begin()); } friend bool operator!=(Vector4 const&a Vector4 const&b) { return !(a == b); } friend bool operator<(Vector4 const&a Vector4 const&b) { return std::lexicographical_compare(a.begin() a.end() b.begin() b.end()); } friend bool operator> (Vector4 const&a Vector4 const&b) { return b < a; } friend bool operator<=(Vector4 const&a Vector4 const&b) { return !(b < a); } friend bool operator>=(Vector4 const&a Vector4 const&b) { return !(a < b); } private: GLfloat vector[4]; } Thank you very much! I didn't even consider the size_t conversion! I really like your solution. It fits well with the stl conventions I have seen so far. better you define a data() function that just returns vector too. and then you pass with data(). the reason is begin() returns an iterator that is seen as a black box that just provides the facility to iterate while data() returns a pointer to the actual data. boost::array follows the same design Doh! Yeah I agree. begin() and end() should return an iterator like in the stl. I haven't come across data() yet and only know a little about boost's conventions. Thanks for the tip! I think I need to familiarize myself with more of these conventions and read the arguments for them. A pointer fits all requirement for an iterator and it is at the same time a pointer. I don't see any problem with using begin() to return a pointer. What would be the problems of the initial approach? dribeas of course a pointer is an iterator. i didn't say otherwise. after all that is the reason you can give .begin() to STL algorithms. but the point is an iterator is not always a pointer. so if you say "".begin returns an random access iterator"" you can't pass it to functions that need GLfloat*. it would require that you know implementation details of Vector4 to know it returns a plain pointer instead or you would have to do &*v.begin() . but i think you agree that v.data() is nicer  It's too hard to get rid of the ambiguity. It could easily interpret it as the direct [] access or cast-to-float* followed by array indexing. My advice is to drop the operator GLfloat*. It's just asking for trouble to have implicit casts to float this way. If you must access the floats directly make a get() (or some other name of your choice) method to Vector4 that returns a pointer to the raw floats underneath. Other random advice: rather than reinvent your own vector classes you should use the excellent ones in the ""IlmBase"" package that is part of OpenEXR Thanks! I had no idea that conversion operators were such a bad idea. I guess since I'm pretty new to C++ I haven't gotten burned with them yet. I realized there must be tons of great vector and matrix classes out there but I really decided to make my own as more of an exercise.",c++ opengl operators ambiguity operator-keyword
742342,A,"Simple OpenGL texture map not working? I'm trying to figure out texture mapping in OpenGL and I can't get a simple example to work. The polygon is being drawn though it's not textured but just a solid color. Also the bitmap is being loaded correctly into sprite1[] as I was successfully using glDrawPixels up til now. I use glGenTextures to get my tex name but I notice it doesn't change texName1; this GLuint is whatever I initialize it to even after the call to glGenTextures... I have enabled GL_TEXTURE_2D. Heres the code: GLuint texName1 = 0; glGenTextures(1 &texName1); glBindTexture(GL_TEXTURE_2D texName1); glPixelStorei(GL_UNPACK_ALIGNMENT 1); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_REPEAT); glTexParameteri (GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_REPEAT); glTexParameteri (GL_TEXTURE_2D GL_TEXTURE_MAG_FILTER GL_LINEAR); glTexParameteri (GL_TEXTURE_2D GL_TEXTURE_MIN_FILTER GL_LINEAR); glTexEnvf(GL_TEXTURE_ENV GL_TEXTURE_ENV_MODE GL_MODULATE); glTexImage2D(GL_TEXTURE_2D 0 GL_BGRA_EXT sprite1[18] sprite1[22] 0 GL_BGRA_EXT GL_UNSIGNED_BYTE &sprite1[54]); glColor3f(1 1 0); glBindTexture(GL_TEXTURE_2D texName1); glBegin(GL_QUADS); glTexCoord2f (0.0 0.0); glVertex3f (0.0 0.0 -5.0f); glTexCoord2f (1.0 0.0); glVertex3f (.5 0.0 -5.0f); glTexCoord2f (1.0 1.0); glVertex3f (.5 .5 -5.0f); glTexCoord2f (0.0 1.0); glVertex3f (0.0 .5 -5.0f); glEnd(); UPDATE: I'm at a loss. Here's everything I've tried: Turns out I was initializing my texture before OGL was initialized. The texture is initialized (glGenTextures->glTexImage2D) in a class constructor and drawn (glBegin->glEnd) in a member function that is called every frame. genTextures appears to be working correctly now and I'm getting a name of 1. Every possible combination of GL_RGBA8 GL_BGRA_EXT (GL_BGRA doesn't work on my system; I need the _EXT) and I even removed the alpha channel from the bitmap and tried all combinations of GL_RGB GL_BGR_EXT etc etc. No luck. Tried procedurally creating a bitmap and using that Made sure GL_COLOR_MATERIAL isn't enabled. Changed bitmap size to 32x32. Tried glTexEnvi instead of glTexEnvf. if texName1 is remaining 0 after glGenTextures you should check your error conditions... Is the width & height stored as byte or int in sprite1[]? width and height is stored as byte little endian. I've used sprite1[18] and sprite1[22] successfully before with glDrawPixels; they are correct. The bitmap is only 29x20 pixels so it works. Also how do I check error conditions? From the reference pages I don't see that glGenTextures generates any errors that would be useful to me using glGetError(). Some random ideas: GL_COLOR_MATERIAL might be enabled change ""glTexEnvf"" to ""glTexEnvi"" and see if that helps if texName1 is 0 after glGenTextures you might not have an active OpenGL context For error checking I recommend writing a small function that prints readable output for the most common results from glGetErrors and use that to find the line that creates the error. Another possibility would be to use something like GLIntercept BuGLe or gDEBugger.  My OpenGL is rusty but I remember having same problems with with glTexImage2D . Finally I managed to make it work but I always had more luck with gluBuild2DMipmaps so i ended up with gluBuild2DMipmaps ( GL_TEXTURE_2D type i.width i.height type GL_UNSIGNED_BYTE i.data ); which replaced glTexImage2D ( GL_TEXTURE_2D 0 type i.width i.height 0 type GL_UNSIGNED_BYTE i.data ); That sounds like a forgotten glTexParameter for the MIN filter. The default would be GL_NEAREST_MIPMAP_LINEAR which would only work with mipmapped textures.  I'll put this here as I had the same issue and found another post explaining the issue. The iPhone does support GL_BGRA(GL_EXT_BGRA) but seemingly only as an input format and not as an internal format. So if you change the glTexImage2D call to have an internal format of GL_RGBA then it works. glTexImage2D(GL_TEXTURE_2D 0 GL_RGBA width height 0 GL_BGRA GL_UNSIGNED_BYTE &sprite1[54]); I hope this helps someone else that stumbles upon this post. I'd love to hear more on this as it's been confusing me. It seems putting BGRA as an internal format works but it is actually using RGBA. Or am I mistaken?  First thing I'd check is the colour material setting as mentioned by ShadowIce then check your texture file to ensure it's a reasonable size (i.e. something like 256x256) and an RGB bitmap. If the file has even a slight problem it WILL NOT render correctly no matter how you try. Then I'd stop trying to just debug that code and instead see what you have different to the tutorial on the NeHe website. NeHe is always a good place to check if you're trying to do stuff in OpenGL. Textures are probably the hardest thing to get right and they only get more difficult as the rest of your GL skills increase.  In addition to mentat's note that you might have a problem with non-power-of-two texture dimensions you mention the texture name generation not changing the name. That sounds as if you're calling glGenTextures() too early i.e. before initializing OpenGL. If you're not then I suggest adding code just after the call to glGenTextures() that check the OpenGL error state by calling glGetError(). This worked for me...I was trying to create textures before OpenGL initialization. As I have seen time and time again: order of function calls are so important in OpenGL programming.  Wow... Okay I found the problem. My call to glEnable was glEnable(GL_BLEND | GL_TEXTURE_2D). Using glGetError I saw I was getting a GL_INVALID_ENUM for this call so I moved GL_TEXTURE_2D to its own enable function and bingo. I guess logical OR isn't allowed for glEnable...? Anyway much thanks to everyone for the help... it was very informative for me. AFAIK OpenGL constants aren't just powers of two they're arbitrary values. Which would mean that using logical OR would destroy information. The OR-able constants are these with -BIT in the name like GL_COLOR_BUFFER_BIT. Not a lot of them in GL so don't get used to it.  In your comments you say your bitmap is 29x20 pixels. Afaik to generate a valid texture OpenGL requires that the image size (on each dimension) be a power of 2. It doesn't need to be a square it can be a rectangle though. You can overcome this by using some OpenGL extensions like GL_ARB_texture_rectangle. GL_ARB_texture_rectangle has been incorporated into OpenGL since 2.0 http://www.opengl.org/wiki/NPOT_Texture",c++ opengl textures
1236670,A,"How to make OpenGL apps in 64-bits windows OpenGL experts My project compiles link and run in xp32 then I tried to cross compile it to x64 and I came across a lot of questions. There's no native x64 instalable OpenGL SDK so I link against what? I saw someone saying that x64 apps use 32bits opengl dll. I tryied to run my compiled 64-bits app in a xp64 with drivers to my video card (radeon 4850) the same I use on xp32 and I got that typical error ""bla bla bla maybe reinstalling you application will resolve the problem"" If I use video card drivers how to keep it working with another Cards should I build a version for each? (no sense). Should I load an available library dinamicaly? (same no sense) Which is the reference implementation for x64? where do I find its libs to link against? I'm really lost on that matter. I did a lot of searchs and found nothing that helped me understant till the momment. So what is the path? What I want to know to make native x64 OpenGL apps? Any help is highly appreciated! The 64-bit OpenGL import library is included in the Windows SDK and gets installed to %ProgramFiles%\Microsoft SDKs\Windows\<version>\Lib\x64\OpenGL32.lib. The corresponding DLL is named opengl32.dll and is located in %SystemRoot%\system32. The 32-bit version is also named opengl32.dll and is located in %SystemRoot%\syswow64 on 64-bit Windows. You can't load 32-bit DLLs in a 64-bit process so whatever you read about x64 apps using the 32-bit OpenGL DLL was incorrect. There is definitely a 64-bit OpenGL DLL but it has ""32"" in its name presumably to make porting easier. looks good will try it. much thanks! 64 bit OpenGL is called opengl32.dll??? Thanks microsoft.... Oh I did not know it. That is wierd.",c++ winapi opengl visual-studio-2005 x64
902348,A,"Point-triangle intersection in 3d from mouse coordinates? I know how to test intersection between a point and a triangle. ...But i dont get it how i can move the starting position of the point onto the screen plane precisely by using my mouse coordinates so the point angle should change depending on where mouse cursor is on the screen also this should work perfectly no matter which perspective angle i am using in my OpenGL application so the point angle would be different on different perspective angles... gluPerspective() is the function im talking about. What do you mean ""move the starting position of the point""? A point is a point. Get my point? ;) Can you clarify what your question is? You're asking how you can move a point precisely? Well gonna take a shot and guess what you mean. The guess is that you would like to pick objects with your mouse. Check out: glUnProject. This transforms the screen coordinates back into 3d world coordinates. Google has more information if you run into problems. Cheers !  yes i want to move the point on the screen plane so for example i could render a cube on that point where my mouse is currently by using 3d coordinates and then i shoot a line from that position to the place where my mouse is pointing so it would hit the triangle in my 3d world and that how i could select that object with mouse. sorry for being unclear :/ -- Edit: yay i got it working with that nehe tutorial! thanks i didnt know it would be that easy! This is the code im using now and it works great: void GetOGLPos(int x int y GLdouble &posX GLdouble &posY GLdouble &posZ){ GLint viewport[4]; GLdouble modelview[16]; GLdouble projection[16]; GLfloat winX winY winZ; glGetDoublev(GL_MODELVIEW_MATRIX modelview); glGetDoublev(GL_PROJECTION_MATRIX projection); glGetIntegerv(GL_VIEWPORT viewport); winX = (float)x; winY = (float)viewport[3]-(float)y; glReadPixels(x int(winY) 1 1 GL_DEPTH_COMPONENT GL_FLOAT &winZ); gluUnProject(winX winY winZ modelview projection viewport &posX &posY &posZ); }  You need to generate a ray (line) passing through the mouse location perpendicular to the screen. I would recommend getting some basic information on 3d geometry and 2d projections before you go much further. Check out Wikipedia A book search on Google has come up with quite a few titles. Foley & Van Dam though is the definitive book - here on Amazon.co.uk or here on Amazon.com",c++ opengl 3d mouse intersection
1528144,A,OpenGL 3.2 Programming Guide? Most resources available online are very outdated. Specifically they are all about OpenGL 2 which matches to DirectX 9. The current specification is 3.2 which is equivalent (or well very close) to DirectX 10 (11). But the specification itself is very hard to read. In contrast DirectX SDK is a wonderful piece of documentation samples and tools. Where I can get the programming guide for OpenGL that is not outdated? Where can I get the samples? And so on. The OpenGL Technical Wiki is a starting point. It also contains some OpenGL 3.2 tutorials. Don't expect anything like the DirectX SDK but afaik there's no better resource. OpenGL learning seems to be more like a trial and error process where the developer forum is especially helpful.  Well if you have no real experience of OpenGL stick to OpenGL 2.0. If you want go to 3.2 I recommand reading the specs: http://www.opengl.org/registry/doc/glspec32.core.20090803.pdf and gl-extensions: http://www.opengl.org/registry/#arbextspecs About samples you can check humus website it's really up to date and there's plenty of GL/DX materials: http://www.humus.name/,c++ opengl
1976850,A,3d max integration with c++ Cal3D where to start? okay i'm making a game using c++ (for the engine) and openGL now i've had lots of trouble using cal3d library for importing my 3d max models into my c++ project as a matter of fact i dunno where to even start i can't find any decent guide and their documentation is pure shit really. i've been searching and trying stuff in this for over a month but i don't even understand the file structure it uses so far :S i really need some help r there any other libraries? any decent guide i can use? i'm stuck thnx alot Rather than write your own exporter consider using one of the built-in exporters for FBX COLLADA Crosswalk (.XSI) the Quake/Doom3 .MD3/.MD4 format or even OBJ. It'll be much easier to parse the resulting file format on your end than to write and maintain a brand-new exporter. thnx alot FBX rocks :)  Max is a complete pain for any kind of scripting or plugin. I'd suggest using maya instead if at all possible. You'll get better results for animation and rigging too. I know it's not a direct answer to your question but part of the problem is the info for stuff like this is not easy to come by.,c++ opengl 3d
1185426,A,"I need help on how to rotate an image with OpenGL using SDL I'm making a game and the arm of the character will be constantly rotating since it will be following the mouse cursor. I've never worked with openGL before and I need some help getting started. If anyone knows any good websites to start learning and one that specifically contains rotation please let me know. I've already visited NeHe and went to that rotation tutorial but after attempting to go through the other basic tutorials it still seems very confusing to me so some clarification on the topic would also be appreciated. Also I don't know if this would change things but I need to rotate about the player's ""shoulder"" so I need to know how to make the arm rotate about this point. You could also refer to your previous question: http://stackoverflow.com/questions/1183900/best-way-to-rotate-an-image-using-sdl Use glTranslate to move the origin of your coordinate system and glRotate to rotate around this origin. Anyway you should probably get a book about the basics of computer graphics. If you are serious about this go for 3d computer graphics by Alan Watt. Yeah I'll look into that. I got an image to rotate finally but there seems to be little dots of color around the top edge of the image as I rotate. Why would this be?",c++ gui opengl rotation sdl
1995062,A,Why can't I run my OpenGL program outside of Visual Studio? I have an OpenGL-program using GLSL that I can run just fine with the Play-button in Visual Studio (2008) -- both in the standard Release and Debug configurations. However when I try to run the executable from Explorer all I get is a flashing cmd-prompt with no text in it to indicate any kind of failure loading something. I have tried copying the required DLL-files (glut32.dll glew.dll etc.) to the same folder as the executable is located in but that didn't make a difference. I also tried copying the GLSL-files to the same folder but that didn't help either. What do I need to do to make the program run without Visual Studio? Have you tried checking the paths of any external resources. The run button in Visual Studio will by default run the program with a different working directory than if you use explorer. The other thing you should do is try adding some logging (even just writing text to stderr at critical points). That way you can see for example if you application ever got to the main function. That was it! Thanks! Don't know why I didn't think of that.,c++ visual-studio opengl crash glsl
1059200,A,true isometric projection with opengl I am a newbie in OpenGL programming with C++ and not very good at mathematics. Is there a simple way to have isometric projection? I mean the true isometric projection not the general orthogonal projection. (Isometric projection happens only when projections of unit X Y and Z vectors are equally long and angles between them are exactly 120 degrees.) Code snippets are highly appreciated.. duplicate of http://www.allegro.cc/forums/thread/600737 An isometric projection is just a matter of using an orthographic projection with a specific rotation angle. You should be able to choose any of the 8 potential orientations with a orthographic projection and get a perfect isometric view of your model. Just follow the math in your referenced Wiki article for setting up the view matrix and do an orthographic projection for your projection matrix and you're all set.  Maybe I'm not quite grokking the math correctly but couldn't you just position your camera as it explains in that Wikipedia link and use a standard orthogonal projection? Even if it's not the same the projection stack is entirely up to you. glMatrixMode(GL_PROJECTION); glLoadIdentity(); // your isometric matrix here (see math on Wikipedia) glMatrixMode(GL_MODELVIEW);  Try using gluLookAt glClearColor(0.0 0.0 0.0 1.0); glClear(GL_COLOR_BUFFER_BIT); glMatrixMode(GL_PROJECTION); glLoadIdentity(); /* use this length so that camera is 1 unit away from origin */ double dist = sqrt(1 / 3.0); gluLookAt(dist dist dist /* position of camera */ 0.0 0.0 0.0 /* where camera is pointing at */ 0.0 1.0 0.0); /* which direction is up */ glMatrixMode(GL_MODELVIEW); glBegin(GL_LINES); glColor3d(1.0 0.0 0.0); glVertex3d(0.0 0.0 0.0); glVertex3d(1.0 0.0 0.0); glColor3d(0.0 1.0 0.0); glVertex3d(0.0 0.0 0.0); glVertex3d(0.0 1.0 0.0); glColor3d(0.0 0.0 1.0); glVertex3d(0.0 0.0 0.0); glVertex3d(0.0 0.0 1.0); glEnd(); glFlush(); Results in We can draw a cube to check that parallel lines are indeed parallel glPushMatrix(); glTranslated(0.5 0.5 0.5); glColor3d(0.5 0.5 0.5); glutWireCube(1); glPopMatrix(); This won't give you an isometric projection because parallel lines say along the x and y axes won't appear parallel when they actually should. look at the picture on the wikipedia article that was linked too in the question this looks exactly like that projection. Any parallel lines will appear parallel because this is a Orthographic projection (http://en.wikipedia.org/wiki/Orthographic_projection). The x and y axes are not parallel. I don't mean the x and y axes are parallel but that lines parallel to the x axis should appear parallel in an isometric projection (and I don't think they will in yours) and similarly in the y direction. Whoops my bad. I missed that you were *only* calling gluLookAt. +1 now.,c++ opengl projection isometric
773935,A,"Handle Tab key in GLUT I use OpenGL+GLUT for simple application but I can't handle a ""Tab"" key press. Does anybody knows how to handle pressing of Tab key ? thanx P.S.:Mac OS 10.5.6 GCC 4.0 Solution void processNormalKeys(unsigned char key int x int y){ if ((int)key == 9) { //tab pressed .... } .... } .... int main(int argc char ** argv) { .... glutKeyboardFunc(processNormalKeys); .... } I believe hitting tab triggers the normal keyboard callback with a key value of 9 (ASCII for tab).",c++ opengl glut
459901,A,window handlers for opengl I've been programming opengl using glut as my window handler lately i've been thinking if there are any advantages to switching to an alternate window handler such as wxWidgets or qt. Are there any major differences at all or is it just a matter of taste? Since glut provides some additional functions for opengl-programming beyond the window handling features would there be a point in combining an additional toolkit with glut? do you want game-like windows inside your openGL window or OS level windows around your openGL window? Well - glut is okay to get a prototype going. It depends on the OS but later on you may want to prevent the screen-saver from starting you may want to disable the task switch (only the bad guys do this though) You may want to react to power down events. You may find out that glut can't deal with dead-keys on one or another operation system or or or... There are thousand reasons why you may want to get rid of it. It's a framework designed to make the start easy and do 90% of the usual stuff you need but it can never do 100%. You can always hack yourself into glut or lift the init-code but one day you will find out that it's easier to redesign the init-code from scratch and adjust it to your task. Opening a window and initializing OpenGL is not rocket-science . Use glut as long as it works for you but as soon as it makes problems get rid of it. It'll take you just a hand full of hours so you won't loose much. For a toy project glut is the way to go though.  I can only speak from experiential of using QT: Once you have the basic structure set up then it is a simple case of doing what you have always done: for example the project I am working on at the moment has an open gl widget embedded in the window. This widget has functions such as initializeGL resize...paintGL etc. Advantages include the ability to pass variables to an from the other windows / widgets etc. QT also has additional functions for handelling mouse clicks and stuff (great for 2d stuff 32d stuff requires some more complex maths)  You need to move from glut as soon as you want more complex controls and dialogs etc. QT has an excellent openGL widget there is also an interesting article in the newsletter about drawing controls ontop of GL to give cool WPF style effects. wxWidgets also comes with an opengl example but I don't have much experience of it.  SDL while not just a window handler will make it easier to use OpenGL than raw Win32 code. However my experience with Qt GTK and wxWidgets has not been too bad... certainly not that much better than Win32 its probably a matter of taste in those cases. I'd recommend avoiding widgets and wrappers like GLUT if you want a fine level of control over the window and resources but if you are just looking for speed of development then these tools are ideal.,c++ opengl graphics 3d glut
1027179,A,"Displaying SVG in OpenGL without intermediate raster I have some simple SVG artwork (icon and glyph kind of things) which I want to display in an OpenGL app (developing in C++ on Debian using Qt). The obvious solution is to use the ImageMagick libs to convert the SVGs to raster images and texture map them onto some suitable polygons (or just use good old glDrawPixels). However I'm wondering if there's anything out there which will translate the SVG directly to a sequence of OpenGL calls and render it using OpenGL's lines polygons and the like. Anyone know of anything which can do this ? hmm.. tricky problem when bezier curves are involved. would be nice to see a solution as the two technologies complement each other nicely. This question has been asked again at http://stackoverflow.com/questions/6287650/rendering-svg-with-opengl-and-opengl-es/ It looks like Inkscape has some interesting export options you may find useful. They include DXF PovRay EPS PS (PostScript) XAML Latex and OpenDocument Drawing (ODG). Perhaps there is a converter for one of those and you could use Inkscape as an intermediary. DXF in particular is a likely candidate since it is a common 3D format already. assimp is a good candidate for loading DXF. Inkscape doesn't have yet a full implementation of SVG so I would imagine that its export options are also partial. Well actually all the SVG I'm using is created in Inkscape anyway so as long as it can export all it's own stuff... no problem. You're going to more limited by the output format than SVG anyway. DXF is like a million years old it isn't going to support crop marks or complex gradients.  there's also tkzinc as a possibility  http://lii-enac.fr/~conversy/research/sauvage/index.html This seems the closest to what I originally had in mind. The pythonness of it might not actually be a problem as I'm toying with doing the app front-end using PyQt4 rather than pure C++.  My answer is going to about displaying vector graphics wtih OpenGL in general because all solutions for this problem can support rather trivially SVG in particular although none support animated SVGs (SMIL). Since there was nothing said about animation I assume the question implied static SVGs only. Since 2011 the state of the art is Mark Kilgard's baby NV_path_rendering which is currently only a vendor (Nvidia) extension as you might have guessed already from its name. There are a lot of materials on that: https://developer.nvidia.com/nv-path-rendering Nvidia hub but some material on the landing page is not the most up-to-date http://developer.download.nvidia.com/devzone/devcenter/gamegraphics/files/opengl/gpupathrender.pdf Siggraph 2012 paper http://on-demand.gputechconf.com/gtc/2014/presentations/S4810-accelerating-vector-graphics-mobile-web.pdf GTC 2014 presentation http://www.opengl.org/registry/specs/NV/path_rendering.txt official extension doc You can of course load SVGs and such https://www.youtube.com/watch?v=bCrohG6PJQE. They also support the PostScript syntax for paths. You can also mix path rendering with other OpenGL (3D) stuff as demoed at: https://www.youtube.com/watch?v=FVYl4o1rgIs https://www.youtube.com/watch?v=yZBXGLlmg2U NV_path_rendering is now used by Google's Skia library behind the scenes when available. (Nvidia contributed the code in late 2013 and 2014.) One of the cairo devs (who is an Intel employee as well) seems to like it too http://lists.cairographics.org/archives/cairo/2013-March/024134.html although I'm not [yet] aware of any concrete efforts for cairo to use NV_path_rendering. An upstart having even less (or downright no) vendor support or academic glitz is NanoVG which is currently developed and maintained. (https://github.com/memononen/nanovg) Given the number of 2D libraries over OpenGL that have come and gone over time you're taking a big bet using something not supported by a major vendor in my humble opinion.  Qt can do this. QSvgRenderer can take an SVG and paint it over a QGLWidget Its possibly you'll need to fiddle around with the paintEvent() abit if you want to draw anything else on the QGLWidget other than the SVG. Thanks very interesting (should have noticed that one myself). It's unclear to me from the documentation whether the vector aspect would be preserved completely in QPainter-on-QGLWidget or whether some intermediate rasterization happens. Ought to be easy enough to find out using GLintercept/GLtrace though... QPainter doesn't usually do any rasterization. There's no reason for it to start doing it in the case of SVG. I also note that enabling automatic multisampling in the Nvidia drivers also makes the SVG graphics look very nice and antialiased (which wouldn't happen if Qt was just blitting some software-rendered bitmap). After getting GLTrace to work with Qt4 apps on my platform (simple fix sent to author) I can confirm the SVG is being drawn on the GLWidget using an impressive number of gl*calls. Trolltech provide a nice example of combining 2D & 3D worlds at http://doc.trolltech.com/4.4/opengl-overpainting.html. The only thing I can imagine wanting to do further would be capturing the SVG render calls in a GL display list (although the only reason to do that would be performance and if that became a problem I'd be more likely to cache rendered bitmaps instead). I'd send a feature request for that using the bug report web form anyway.  SVGL appears to address this but has been dormant for several years. Still you might be able to find some code of value there. The ""sauvage"" project mentioned by ""dru"" below appears to be the successor. Same author but python rather than C++.",c++ opengl graphics svg
1807857,A,repeatedly render loop with Qt and OpenGL I've made a project with Qt and OpenGL. In Qt paintGL() was repeatedly call I beleive so I was able to change values outside of that function and call update() so that it would paint a new image. I also believe that it called initializeGL() as soon as you start up the program. Now my question is: I want that same functionality in a different program. I do not need to draw any images etc. I just was wondering if there was a way to make a function like paintGL() that keeps being called so the application never closes. I tried just using a while(true) loop that kept my program running but the GUI was inactive because of the while loop. Any tips other than threading preferably. Thanks. The exact mechanism will depend on which GUI toolkit you are using. In general your app needs to service the run loop constantly for events to be dispatched. That is why your app was unresponsive when you had it running in a while loop. If you need something repainted constantly the easiest way is to create a timer when your window is created and then in the timer even handler or callback you invalidate your window which forces a repaint. Your paint handler can then be called at the frequency of your timer such as 25 times per second. Assuming you keep using QT just create a timer http://doc.trolltech.com/4.5/qtimer.html and connect the timeout signal to a slot that calls invalidate on your window. In your paint handler you do your GL drawing. How may I decide which toolkit to use? Sorry for the noob questions,c++ qt opengl glut
1839128,A,"Why is my glutWireCube not placed in origin? I have the following OpenGL code in the display function: glLoadIdentity(); gluLookAt(eyex eyey eyez atx aty atz upx upy upz); // called as: gluLookAt(20 5 5 -20 5 5 0 1 0); axis(); glutWireCube (1.); glFlush (); axis() draws lines from (000) to (1000) (0100) and (0100) plus a line from (100) to (130). My reshape function contains the following: glViewport (0 0 (GLsizei) w (GLsizei) h); glMatrixMode (GL_PROJECTION); glLoadIdentity (); gluPerspective(45.0 (GLsizei) w / (GLsizei) h 1.0 100.0); glMatrixMode (GL_MODELVIEW); This image shows the result of running the program with 1. as the argument to glutWireCube: As you can see the cube isn't centered around (000) as the documentation says it should be: The cube is centered at the modeling coordinates origin (...) (source) If I run the program with 5. as the argument the cube is displaced even further: Why is that and how do I place the cubes around (000)? FURTHER INFORMATION It doesn't matter if I switch the order of axis() and glutWireCube. Surrounding axis() with glPushMatrix() and glPopMatrix() doesn't fix it either. SOLUTION I modified gluPerspective to start looking further away from the camera and now the Z-buffering works properly so it is clear that the cubes are placed around the origin. Are you sure axis does not mess with the view matrix ? What happens if you call it after the drawing of the cube ? Edit to add: Actually... Looking at the picture closer it looks like it might be centered at the origin. The center of the cube seems to align exactly with the intersection of the 3 axes. The only thing that looks suspicious is that the red line does not write over the white edge. do you have Z-buffering properly set up ? I think I have Z-buffering enabled with `glEnable(GL_DEPTH_TEST)` and `glDepthMask(GL_TRUE)`. The result is the same if the lines are present or not. Actually you are right. If I view the image with `glOrtho` instead of `gluLookAt` and `gluPerspective` the cubes are centered around the origin. How would I enable z-buffering correctly? I increased `znear` in `gluPerspective` and now the Z-buffering works. Now I just have to figure out how to see more of the image :) how about reducing zfar then ? the ratio of the 2 is what really affects your Z-buffer precision Unfortunately that doesn't change the result either. :(  It might be right I think it's hard to determine due to the perspective ... But I guess it isn't from staring a bit more at it. To quickly rule out that axis() isn't modifying the model view matrix surround the call with matrix push/pops: glPushMatrix(); axis(); glPopMatrix(); Things to investigate/check: Is this the entire window? It seems odd that the view is down in one corner. Does it help if you add an increasing rotation before the rendering? That can make it easier to determine the perspective by giving more clues. You can also try moving the ""camera"" around by changing the arguments to gluLookAt() dynamically. > Is this the entire window? It seems odd that the view is down in one corner. Yes it's the entire window. I'm forced to do a front-perspective with a eye-position at (2055). Unfortunately that doesn't change the result.",c++ opengl graphics coordinates
1366095,A,"Game engine development question I am thinking of making a simple game engine for my course final year project. I want it to be modular and expandable so that I can add new parts if I have time. For example I would make a graphics engine that would be completely independent of the other systems once that was finished I could add a physics engine etc. I would also want to make a tool set to go with this engine. For the tools I would like to use C# but I am not sure about the libraries. My question is if I want a C# GUI program can I reference a library written in C++? Also would there be any performance problems etc. if I made some of the libraries in C# but wanted to use them from a C++ game. I would like to avoid C++ as much as possible my experiences have shown that development time can be a lot higher for a project over that of using C# or Java etc. My graphics development would be in OpenGL this is all I have been taught. We have only done this in C++ but I have seen that projects such as SharpGL allow for the development with C#. Is there any performance issues with this. I am not looking for a blindingly fast top graphics game. It will most likely be something simple to show my engine working. My engine probably won't be that great either as I only have a year and am working on my own. Any advice on this would be appreciated. I am still really in the planning stages so it wouldn't be too much work to completely change what I want to do. I would just like to preempt any major problems I might have. Thanks +1 Nice project. Good luck with it. If you can pull this off even under a relaxed set of goals you're set for a great project. First you need to get a grasp of scope: How long do you have to work on the project? How many people are working on the project? If you can only create one ""piece of the pie"" on your own which one would you pick? Use this to establish a working plan to make sure if you don't get as far as you'd like you still have enough to make the work show as a great project. A game engine is a big development task. A game engine with a toolchain is an enormous development task. In a lot of ways choosing a smaller but more challenging task is preferable because it shows higher-level thinking about problem solving which is greatly preferred by academics - double that if you are CS and not [Area of] Engineering. Since you are working in managed languages things you may want to consider are: Expressing gameplay logic (rules of the game) in a clean manner so as to provide an efficient and reliable path from designer->developer->tester. If you want this could absolutely include the manner in which you describe the rules (Custom editor? Code API? DSL?) Game AI has no shortage of extremely challenging problems. Physics and graphics are interesting and I believe managed languages will eventually be used in these areas but you may find yourself a bit more limited in your ability to solve these problems. If I were going to work in this area now I would be trying to answer: ""If I could use a managed language for writing [graphics|physics] code without hurting performance what kinds of [language|runtime] features make the most difference in improving the expressiveness correctness maintainability and reliability of the resulting programs."" This goes way past simply having garbage collection and pointer safety. Great answer! +1  You should look into XNA. It performs quite well and from what I've heard is quite easy to work with. About referencing C++ code from C#: It's perfectly doable though it will take some effort from your side to get it right. C++/CLI can work as an intermediate wrapper or you can use P/Invoke. Just remember that C++ is unmanaged and that you will need to do some manual garbage collection which can be somewhat icky in a managed environment like .Net. Perfomance in the C++ -> C# bridge is okayish but I'm unsure how it will perform if you need do 100.000 calls to a math-library each second. I guess a small test would be good.. I'll see if I get time to do this later today though I somewhat doubt it :) I've used XNA for other project before but don't really want to use it for this. The problem is that anything that XNA handles I won't get any marks for and a lot of what it handles is things I know how to do so I would prefer showing that I can do by doing it myself. For example loading resources such as models/textures from a file. Ah okay.. My bad then :)  When creating a graphics engine (I'll only really speak about graphics as that is my area of expertise) these are a few of the choices you need to make EARLY in the project: 1) Is it an indoor and/or outdoor engine? 2) What sort of visibility system are you going to use? 3) Forward or deferred renderer? 4) How will you have animation hierarchies? 5) Dynamic or static lighting? 6) How are you going to handle transparency? 7) How will you handle a 2D overlay? 8) What mesh formats will you use? Roll your own? Bear in mind you need a GOOD solid vector/matrix library and preferably a full maths library that will help with things from eulers to quaternions to axis-aligned bounding boxes. Do a lot of research as well. Try to find out what the potential problems you are going to face are. Remember things like changing a texture or a shader can have HUGE impacts on your pipelining. You need to minimise these as much as possible. Bear in mind that getting a rotating bump mapped mesh on the screen is simple by comparison to getting an engine running. Don't let this put you off. It should be no problem to write a fairly simple game rendering engine in a matter of months :) Also ... find communities specific to this area. You will get a lot of good (though non-OpenGL-centric) information off DirectXDev. There is a fair bit of general game development algorithm information available at GDAlgorithms. There is also an OpenGL specific mailing list here. Its worth noting that DirectXDev and GDAlgorithms at least (I don't know so much on the GL mailing list) are populated by some VERY experienced 3D engine and game developers. Don't post up lots of ""beginner"" questions as this does tend to breed contempt among the members. Though the odd query or 2 at whatever level (beginner to advanced) will get amazing answers. Good luck! I wish I'd had the chance to do this at University. I might not have gone off and joined the games industry and enjoyed an extra year of sleeping late ;) hehehe  Make sure your scope is feasible. As a teenager I went through the build-my-own-engine phase a few years later I realized I would have gotten a whole lot more done if I had just used pygame and pyopengl and not wasted the effort. Check out the forums at gamedev.net.  Why do you need any C++ code? There are already several wrapper libraries exposing OpenGL or DirectX to .NET such as SlimDX (which as the name implies is a much thinner more light-weight wrapper than something like XNA) If you're more comfortable with C# there's no reason why you couldn't write your entire game in that. Performance generally won't be a problem. In most cases the performance of C# code is comparable to C++. Sometimes it's faster sometimes it's slower. But there are few cases where C# isn't fast enough. (However there is a significant performance cost to interfacing between native and .NET code -- so doing that too often will hurt performance -- so the trick is if you use native code at all to have sufficiently big native operations so the jump to/from .NET isn't done too often) Apart from that a word of advice: Don't bother writing an ""engine"". You'll be wasting your time producing a big monolithic chunk of code which ultimately doesn't work because it was never tested against the requirements of an actual game only what you thought your future game would need. If you want to experiment with game development make a game. And then by all means refactor it and clean it up and try to extract reusable parts of the code. But if the code hasn't already been used in a game you won't be able to use it to build a game in the future either. The engines used in commercial games are just this code extracted from previous games code which has been tested and which works. By contrast hobbyist engines pretty much always end up taking 2+ years of the developer's time without ever offering anything usable. The whole concept of a ""game engine"" is flawed. In every other field of software development you'd frown at the idea of one vaguely-defined component doing basically ""everything I need to make my product"". You'd be especially suspicious of the idea that it is a separate entity that can be developed separate from the actual product it's supposed to support. Only in game development which is by and large stuck in 80's methodologies is it a common approach. Even though it doesn't actually work. If this is a school project I'm sure whoever is supposed to grade it will appreciate it if you apply common good software practices to the field. Just because many newcomers to game development don't do that (and prefer to stick to some kind of myth about ""engines"") there's no reason why you shouldn't do better. Good answer if he wasn't doing this for a school project. Since he is this is much less applicable since the end result is expected to be an engine. True but in a school project you are still expected to produce something that *makes sense*. Any sane definition of a ""game engine"" includes ""must support the development of a game"". And at least in my school projects I've always been expected to make something that *fulfills its purpose*. In other words if you make a game engine which isn't useful in making games who is it going to impress? Whether or not it is actually *used* to make games is less relevant but it should be *capable* of that job. And there is no way it'll achieve that if it is designed as a standalone ""engine"".  Sounds to me like you have a pretty good plan for your project. (I get your point about XNA in your comment to @Meeh) You can interop with C++ via P/Invoke directly or COM you could also I guess come up with some SOA way of doing it but to be honest as yucky as this sounds I would be inclined to target COM as your API lever of choice...why? because then you open up your API to a lot of common client language's not just C# and VB.NET you will also get Delphi VBA Powerbuilder etc. Performance should not be a problem as the API entry points are just to kick of the work and transport data structures the real work is done in your library in native code so don't worry too much about the perf. ATL will be your friend with creating COM Classes that provide entry to your Library.  Whilst it's certainly do-able to bridge between C++ and C# (Via managed C++ is a good way to go if the interface is complex P/Invoke for a very simple one) for tool<->engine communication I might suggest instead a network based interface. This is ideal for a high-ish level interface such as you might want for a level editor/model viewer or such. An actual object modeller is not such a good target. What tools do you envision? If you do it this way you gain the ability to connect to remote instances of the engine or multiple instances or even instances running on different hardware platforms. It'll also teach you a bit about sockets if you don't know them already.",c# c++ performance opengl
1501516,A,"OpenGL / C++ - How do you fill an area surrounding a point on a mouse click? I've got the mouseclick handler correctly set up. I have a drawing with some shapes. Is there any way for me to fill the surrounding part of a point until it hits a polygon boundary. Something like Microsoft Paint's ""fill"" command. Thanks! How are your shapes defined? Triangles? Line segments? Are they filled polygons or just drawn line segments with an empty interior? If they are filled is it a constant color a gradient or a texture? A screenshot would be helpful. Consider using OpenGL selection capabilities with glSelectBuffer. Refer to this chapter of the red book for explanation.  The advice to use glSelectBuffer is pretty good. Once you've read that chapter however look for the ""Now that you know"" chapter and look for a section named ""Object Selection Using the Back Buffer"" -- it's a lot simpler and usually entirely adequate.  What you're looking for is called a Flood Fill and it's a per-pixel algorithm; which means you're going to want to look at either Frame buffer objects with shaders or use the (very slow) glDrawPixels.  Using opengl? I seriously doubt that ... What you can do ist to detect the shape selected by the mouse click and draw this shape in a different color. How do I detect the shape?",c++ opengl
60751,A,c++ Having multiple graphics options Currently my app uses just Direct3D9 for graphics however in the future I' m planning to extend this to D3D10 and possibly OpenGL. The question is how can I do this in a tidy way? At present there are various Render methods in my code void Render(boost::function<void()> &Call) { D3dDevice->BeginScene(); Call(); D3dDevice->EndScene(); D3dDevice->Present(0000); } The function passed then depends on the exact state eg MainMenu->Render Loading->Render etc. These will then oftern call the methods of other objects. void RenderGame() { for(entity::iterator it = entity::instances.begin();it != entity::instance.end(); ++it) (*it)->Render(); UI->Render(); } And a sample class derived from entity::Base class Sprite : public Base { IDirect3DTexture9 *Tex; Point2 Pos; Size2 Size; public: Sprite(IDirect3DTexture9 *Tex const Point2 &Pos const Size2 &Size); virtual void Render(); }; Each method then takes care of how best to render given the more detailed settings (eg are pixel shaders supported or not). The problem is I'm really not sure how to extend this to be able to use one of what may be somewhat diffrent (D3D v OpenGL) render modes... Define an interface that is sufficient for your application's graphic output demands. Then implement this interface for every renderer you want to support. class IRenderer { public: virtual ~IRenderer() {} virtual void RenderModel(CModel* model) = 0; virtual void DrawScreenQuad(int x1 int y1 int x2 int y2) = 0; // ...etc... }; class COpenGLRenderer : public IRenderer { public: virtual void RenderModel(CModel* model) { // render model using OpenGL } virtual void DrawScreenQuad(int x1 int y1 int x2 int y2) { // draw screen aligned quad using OpenGL } }; class CDirect3DRenderer : public IRenderer { // similar but render using Direct3D }; Properly designing and maintaining these interfaces can be very challenging though. In case you also operate with render driver dependent objects like textures you can use a factory pattern to have the separate renderers each create their own implementation of e.g. ITexture using a factory method in IRenderer: class IRenderer { //... virtual ITexture* CreateTexture(const char* filename) = 0; //... }; class COpenGLRenderer : public IRenderer { //... virtual ITexture* CreateTexture(const char* filename) { // COpenGLTexture is the OpenGL specific ITexture implementation return new COpenGLTexture(filename); } //... }; Isn't it an idea to look at existing (3d) engines though? In my experience designing this kind of interfaces really distracts from what you actually want to make :) Because I already have the d3d9 stuff for everything I need I just want the option to extend to other API's. Since I already have classes wrapping most d3d stuff (eg Texture Sprite Mesh) I could just overload those leaving the actaul objects with API independent stuff.  I'd say if you want a really complete answer go look at the source code for Ogre3D. They have both D3D and OpenGL back ends. http://www.ogre3d.org Basically their API kind of forces you into working in a D3D-ish way creating buffer objects and stuffing them with data then issuing draw calls on those buffers. That's the way the hardware likes it anyway so it's not a bad way to go. And then once you see how they do things you might as well just just go ahead and use it and save yourself the trouble of having to re-implement all that it already provides. :-),c++ opengl graphics direct3d
1327231,A,"Is it possible to run C++ binded with SDL+OpenGL code on a web browser? My client wants her website to have an application that renders 3D (light 3D stuff we are drawing only flat squares in 3D world) but web programming is not my thing. So I am looking for something that can run a C++ program from a web browser. But I think if this is the case then the client side must download the program first and that's not what I want. The client should only be able to use this application only on the website. I came across Google Native Client which claims that it can run x86 native code in web applications. I haven't decide whether it is worth it or not and I don't know whether this is what I want or not so I decided to ask experienced people about this. If I want to have something like this is what I said above possible? Or I completely need other languages like Flex because it does not worth the trouble? Or is Google Native Client suitable for doing something like this? You could write a browser plugin in C++? Now it seems possibele: https://developers.google.com/native-client/community/porting/SDLgames Ricardo Cabello has done a Javascript 3d software-rendering engine called three.js. It's a good fit for you because you only require lightweight graphics and Javascript lacks some Flash problems (like focus-stealing slow loading and nasty context menus). Unfortunately there doesn't seem to be any documentation. There are examples and demos though. One of the demos: here  A Java applet may also be an option. Might be easier to convert your code to Java since it has similar syntax to C++ and the Java3D API may prove very useful. Apparently it's quite easy to use although I haven't worked with it myself. The Java3D.org website is a good place to start. Most people have Java and Flash already installed so both are fairly safe. Yea depending on the context. Processing vs. looks. You probably wouldn't use Java if you just wanted a shiny button or a media player. You probably wouldn't use Flash if you want something with more intense code like a 3D simulation or a fractals calculator. One of the more recent things with Java is WebStart which basically allows the Java program to be installed via web onto the host computer (although this is probably beyond your needs). Two major recent Java programs are Geogebra (geometry/drawing popular educational tool) and Centra (web conferencing presentations and classrooms). Java Applet? I'm not sure. I have a feeling that I no longer see Java Applet these days. I see Flash Flex and such. Are Java Applets still around?  No NativeClient is not what you want. It won't let you run SDL+OpenGL -- it may be C++ code but it's run inside a sandbox. Running SDL in a browser is difficult in general. OpenGL somewhat less so but it's no cakewalk either. Any such native code solution is difficult if you want it to work across browsers and platforms -- you'll have to develop NPAPI plugins for multiple platforms (which will all be fairly different) as well as an ActiveX control. You are looking at four separate projects. Almost assuredly the correct answer here is to use Flash in one form or another.  If you decide to go the plugin route FireBreath is a project that lets you create a plugin (that you could do rendering from) that will compile to be both an NPAPI plugin (firefox google chrome apple safari) and an ActiveX control (IE)  Google's NativeClient framework supports what you want to do. It's a plugin that users will have to install but it runs a sandboxed C++ application as if it were a browser plugin. It seems to be exactly what you're looking for. As others have pointed out your other options are a Silverlight applet a Flash applet a Java applet the HTML5 Canvas tag or an actual plugin (ActiveX for IE NPAPI for all other browsers).  You could also take a look at OSAKit. It's a set of browser plugins plus a set of tools to wrap an existing native executable into a package that the plugin can run inside the browser. The wrapping process is really easy can be done in 5 minutes. The whole thing looks a bit unprofessional but actually works. (I'm not sure about security though and this may be a concern for your client.)  The only thing that can ""run"" inside a browser is an ActiveX control. So no matter what way you go (a COM object written in C++ a Silverlight app even a Flash program) they all have to be hosted inside a downloadable ActiveX. So if the requirement is that you must not download anything you're out of luck. Now this being said Flash is pretty much available everywhere and I'm fairly certain it can do what you want and Silverlight while not quite as popular yet is tailor made for this sort of thing and is rapidly gaining acceptance in the web programming world. You could get away with using either of them. but you get the idea you still have to download SOMETHING regardless of what you call it. Firefox Chrome et. al. support the NPAPI framework. Firefox (and other Gecko-based browsers) support XPCOM-based plugins. Only IE supports ActiveX. really? i can run activeX in Firefox and Google chrome? Cause IE sure ain't a browser more like a dedicated virus delivery platform.  Your only stable bet to display C++ in the browser is to make the user download an plug-in. Otherwise you could look at a Javascript solution instead maybe O3D could be what you're looking for? O3D looks tempting. Let me give it a try.",c++ web-applications opengl 3d
764824,A,"Best way to organize entities in a game? Let's say I'm creating an OpenGL game in C++ that will have many objects created (enemies player characters items etc.). I'm wondering the best way to organize these since they will be created and destroyed real-time based on time player position/actions etc. Here's what I've thought of so far: I can have a global array to store pointers to these objects. The textures/context for these objects are loaded in their constructors. These objects will have different types so I can cast the pointers to get them in the array but I want to later have a renderObjects() function that will use a loop to call an ObjectN.render() function for each existing object. I think I've tried this before but I didn't know what type to initialize the array with so I picked an arbitrary object type then cast anything that wasn't of that type. If I remember this didn't work because the compiler didn't want me dereferencing the pointers if it no longer knew their type even if a given member function had the same name: (*Object5).render() <-doesn't work? Is there a better way? How to commercial games like HL2 handle this? I imagine there must be some module etc. that keeps track of all the objects. you may also check how a 3d engine is structured e.g. this one: http://www.ogre3d.org/docs/manual/manual_4.html#SEC4 The way I've been approaching it is to have a display layer that knows nothing about the gameworld itself. its only job is to recieve an ordered list of objects to draw onto the screen that all fit a uniform format for a graphic object. so for instance if it's a 2D game your display layer will receive a list of images along with their scaling factor opacity rotation flip and source texture and whatever other attributes a display object could have. The view may also be responsible for recieving high level mouse interactions with these displayed objects and dispatching them somewhere appropriate. But it's important that the view layer not know anything sementically about what it is that it's displaying. Only that it's some kind of square with a surface area and some attributes. Then the next layer down is a program whose job it is simply to generate a list of these objects in order. It's helpful if each object in the list has some kind of unique ID as it makes certain optimisation strategies possible in the view layer. Generating a list of display objects is a much less daunting sort of task than trying to figure out for each sort of character how its going to physically render itself. Z sorting is simple enough. Your display object generating code just needs to generate the list in the order that you want and you can use whatever means you need to to get there. In our display object list program each character prop and NPC has two parts: A resource database assistant and a character instance. The database assistant presents for each character a simple interface from which each character can pull up any image/statistics/animation/arrangement etc that the character will need. You'll probably want to come up with a fairly uniform interface for fetching the data but it's going to vary a little from object to object. A tree or a rock doesn't need as much stuff as a fully animated NPC for example. Then you need some way of generating an instance for each type of object. You might implement this dichotomy using your language's built in class/instance systems or depending on your needs you may need to work a little beyond that. for example having each resource database be an instance of a resource database class and each character instance being an instance of a ""character"" class. This saves you from writing a chunk of code for every single little object in the system. This way you only need to write code for broad categories of objects and only change little things like which row of a database to fetch images from. Then don't forget to have an internal object representing your camera. Then it's your camera's job to query each character about where they are in relation to the camera. It is basically going around each character instance and asking for its display object. ""What do you look like and where are you?"" Each character instance in turn has its own little resourcey databasey assistant thing to query. So each character instance has available to it all the information it needs to tell the camera what it needs to know. This leaves you with a set of character instances in a world that's more or less oblivious to the nitty gritty of how they are to be displayed on a physical screen and more or less oblivious to the nitty gritty of how to fetch image data from the hard drive. This is good- it leaves you with as clean a slate as possible for a sort of platonically ""pure"" world of characters in which you can implement your game logic without worrying about things like falling off the edge of the screen. Think of what sort of interface you would like if you were to put a scripting language into your game engine. Simple as possible right? As grounded in a simulated world as possible without worrying about little technical implementation details right? That's what this strategy lets you do. Additionally the separation of concerns lets you swap out the display layer with whatever technology you like: Open GL DirectX software rendering Adobe Flash Nintendo DS whatever- Without having to fuss around too much with the other layers. In addition you can actually swap out the database layer to do things like reskin all the characters- Or depending on how you built it swap in a completely new game with new content that reuses the bulk of the character interactions/ collision detection/ path finder code that you wrote in the middle layer. This is a great answer. My game is 2D and fairly simple (I'll only have 30-40 objects in existence at any time. I've often imagined how I would implement an engine that actually needs a ""database"" to keep loads of info about objects and you present a good solution. Thanks!  For my soon-to-be personal game project I use a component-based entity system. You can read more about it by searching ""component based game development"". A famous article is Evolve Your Hierarchy from the Cowboy programming blog. In my system entities are just ids - unsigned long a bit like in a relational database. All the data and the logic associated to my entities are written into Components. I have Systems that link entity ids with their respective components. Something like that: typedef unsigned long EntityId; class Component { Component(EntityId id) : owner(id) {} EntityId owner; }; template <typename C> class System { std::map<EntityId C * > components; }; Then for each kind of functionality I write a special component. All entities don't have the same components. For example you could have a static rock object that has the WorldPositionComponent and the ShapeComponent and a moving enemy that has the same components plus the VelocityComponent. Here's an example: class WorldPositionComponent : public Component { float x y z; WorldPositionComponent(EntityId id) : Component(id) {} }; class RenderComponent : public Component { WorldPositionComponent * position; 3DModel * model; RenderComponent(EntityId id System<WorldPositionComponent> & wpSys) : Component(id) position(wpSys.components[owner]) {} void render() { model->draw(position); } }; class Game { System<WorldPositionComponent> wpSys; System<RenderComponent> rSys; void init() { EntityId visibleObject = 1; // Watch out for memory leaks. wpSys.components[visibleObject] = new WorldPositionComponent(visibleObject); rSys.components[visibleObject] = new RenderComponent(visibleObject wpSys); EntityId invisibleObject = 2; wpSys.components[invisibleObject] = new WorldPositionComponent(invisibleObject); // No RenderComponent for invisibleObject. } void gameLoop() { std::map<EntityId RenderComponent *>::iterator it; for (it = rSys.components.iterator(); it != rSys.components.end(); ++it) { (*it).second->render(); } } }; Here you have 2 components WorldPosition and Render. The Game class holds the 2 systems. The Render component has an access to the position of the object. If the entity doesn't have a WorldPosition component you can choose default values or ignore the entity. The Game::gameLoop() method will only render visibleObject. There is no waste of processing for non-renderable components. You can also split my Game class into two or three to separate display and input systems from the logic. Something like Model View and Controller. I find it neat to define my game logic in term of components and to have entities that only have the functionality that they need - no more empty render() or useless collision detection checks. Thanks this is quite elegant. One think though when I tried to compile it the last line of code needed to be: `(*it).second->render();`. Right thanks for your fix. Good elegant work! I'm curious: are you using an EventManager to manage communication? And how to safely manage dependencies between components such as the RenderComponent? Interesting way to avoid a message bus i was also interested on you managed to create components at runtime while satisfying depends between components such as order of creation and that: thank you for responding on this now dated answer btw. Well for events I put delegates in the relevant components. I use FastDelegate (http://www.codeproject.com/KB/cpp/FastDelegate.aspx). As for dependencies I'm not sure what you're talking about. Here RenderComponent depends on WorldPositionComponent which System is passed into RenderComponent's constructor. The dependency is enforced by the compiler. The definition and building of my Systems are not data-driven that's not a feature I need.  Is there a better way? How to commercial games like HL2 handle this? I imagine there must be some module etc that keeps track of all the objects. Commercial 3D games use a variation on the Scene Graph. An object hierarchy like the one Adam describes is placed in what is usually a tree structure. To render or manipulate objects you simply walk the tree. Several books discuss this and the best I've found are 3D Game Engine Design and Architecture both by David Eberly.  I'm not sure I fully understand the question but I think you are wanting to create a collection of polymorphic objects. When accessing a polymorphic object you must always refer to it by a pointer. Here is an example. First you need to set up a base class to derive your objects from: class BaseObject { public: virtual void Render() = 0; }; Then create the array of pointers. I use an STL set because that makes it easy to add and remove members at random: #include <set> typedef std::set<BaseObject *> GAMEOBJECTS; GAMEOBJECTS g_gameObjects; To add an object create a derived class and instantiate it: class Enemy : public BaseObject { public: Enemy() { } virtual void Render() { // Rendering code goes here... } }; g_gameObjects.insert(new Enemy()); Then to access objects just iterate through them: for(GAMEOBJECTS::iterator it = g_gameObjects.begin(); it != g_gameObjects.end(); it++) { (*it)->Render(); } To create different types of object just derive more classes from class BaseObject. Don't forget to delete the objects when you remove them from the collection. Great. I hadn't thought of using inheritance. Also don't forget of aggregation!  You should make a superclass of all your objects that has a generic render() method. declare this method virtual and have each subclass implement it in its own way.",c++ opengl
1792776,A,Shader limitations I've been tuning my game's renderer for my laptop which has a Radeon HD 3850. This chip has a decent amount of processing power but rather limited memory bandwidth so I've been trying to move more shader work into fewer passes. Previously I was using a simple multipass model: Bind and clear FP16 blend buffer (with depth buffer) Depth-only pass For each light do an additive light pass Bind backbuffer use blend buffer as a texture Tone mapping pass In an attempt to improve the performance of this method I wrote a new rendering path that counts the number and type of lights to dynamically build custom GLSL shaders. These shaders accept all light parameters as uniforms and do all lighting in a single pass. I was expecting to run into some kind of limit so I tested it first with one light. Then three. Then twenty-one with no errors or artifacts and with great performance. This leads me to my actual questions: Is the maximum number of uniforms retrievable? Is this method viable on older hardware or are uniforms much more limited? If I push it too far at what point will I get an error? Shader compilation? Program linking? Using the program? I guess the maximum number of uniforms is determined by the amount of video memory as it's just a variable. Normal varaibles on the cpu are limited by your RAM too right? Uniforms have a strict limit set by the hardware. Modern hardware has very high limits but they still exist. Older hardware (2+ years) may have much lower limits.  For how to get the max number supported by your OpenGL implementation see moonshadow's answer. For an idea of where the limit actually is for arbitrary GPUs I'd recommend looking at which DX version that GPU supports. DX9 level hardware: vs2_0 supports 256 vec4. ps2_0 supports 32 vec4. vs3_0 is 256 vec4 ps3_0 is 224 vec4. DX10 level hardware: vs4_0/ps4_0 is a minumum of 4096 constants per constant buffer - and you can have 16 of them. In short It's unlikely you'll run out with anything that is DX10 based.  See 4.3.5 Uniform of The OpenGL® Shading Language specs: There is an implementation dependent limit on the amount of storage for uniforms that can be used for each type of shader and if this is exceeded it will cause a compile-time or link-time error. Uniform variables that are declared but not used do not count against this limit. It will fail at link or compile-time but not using the program.  Shader uniforms are typically implemented by the hardware as registers (or sometimes by patching the values into shader microcode directly e.g. nVidia fragment shaders). The limit is therefore highly implementation dependent. You can retrieve the maximums by querying GL_MAX_VERTEX_UNIFORM_COMPONENTS_ARB and GL_MAX_FRAGMENT_UNIFORM_COMPONENTS_ARB for vertex and fragment shaders respectively. @mvanbem: One uniform takes up at least one 4D vector. So if you are really tight on uniform storage consider packing multiple scalars into one 4D vector. Note that for GL2.0 you're guaranteed to have at least 512 vertex uniforms and 64 fragment uniforms. Oh failure to RTFM on my part. More questions come to mind: Do floats and ints each consume one component? Are there any restrictions on packing? Four floats versus one vec4? One float and one vec3?,c++ opengl glsl shader
365316,A,3D Engine Comparison I am currently investigating several free/open source OpenGL based 3D engines and was wondering if you guys could provide some feedback on these engines and how they are to work with in a real world project. The engines being compared are (in no particular order): Crystal Space Panda3D Irrlicht These are the main ones i know that are cross-platform any there any others that i should be looking at? @Atmospherian could you add links to the engines you are considering? From what I've seen is that: Cystal Space is a bit bloated. Ogre3d is slow. Well slower than Irrlicht (very fast) and Panda3d. Don't forget OSG - very well done. Terathon makes an engine called C4. It is a game engine with a very mature set of features that is impressive for a $350 engine that includes full source code. The engine is available and being used on the PS3 as well so this is a very solid platform. Not sure if you're considering just 3D graphics engines or full game engines but thought I'd mention it. Definitely not free or open source.  You may also want to look at Ogre 3D: http://www.ogre3d.org/ Unfortunately I don't have any experience developing with Ogre or any of those you mentioned.  More focused on large terrains than games (think GIS or flight simulators) there is also openscenegraph  If you just want a graphics engine I recommend Ogre3d. It is very powerful in that regard. If you want the beginnings of a game engine - i.e. something that is easy to plug networking game entities physics etc into then I recommend Crystalspace 3d. I've used cs3d (Crystalspace 3d) for making games and it was easy to get it up and running quickly - but Ogre3d seems to have more to offer in how you can tweak your graphics but it isn't as easy to hook in all the other game stuff as a package like Crystalspace. Btw there are tons of comparisons of these engines if you just google it.  You can find a lot of informations on lot of engines on this database. CrystalSpace is a full engine so it's a monolithic bloc that you have to customize for your needs. Irrlicht too but it's made do do things easy. The counter effect is that it's hard to do specific things. Now i think Ogre might be the most general purpose hardware accelerated 3D rendering engine around here. Maybe Horde3D is better suited for specific high quality rendering but nothing that cannot be done with Ogre too.,c++ open-source opengl 3d cross-platform
641542,A,"Why doesn't glCopyTexSubImage2D copy my square correctly? here is the output: http://i43.tinypic.com/9a5zyx.png if things were working the way i wanted the colors in the left square would match the colors in the right square. thanks for any help regarding the subject #include <gl/glfw.h> const char* title=""test""; GLuint img; unsigned int w=64h=64; int screenwidthscreenheight; void enable2d() { glMatrixMode(GL_PROJECTION); glPushMatrix(); glLoadIdentity(); glViewport(00screenwidthscreenheight); glOrtho(0screenwidthscreenheight0-11); glMatrixMode(GL_MODELVIEW); glPushMatrix(); glLoadIdentity(); glPushAttrib(GL_DEPTH_BUFFER_BIT|GL_LIGHTING_BIT); glDisable(GL_DEPTH_TEST); glDisable(GL_LIGHTING); glClearColor(0.0f 0.0f 0.0f 0.5f); } void drawmytex() { glEnable(GL_TEXTURE_2D); glBindTexture(GL_TEXTURE_2Dimg); glBegin(GL_QUADS); glTexCoord2i(00); glVertex2i(00); glTexCoord2i(10); glVertex2i(w0); glTexCoord2i(11); glVertex2i(wh); glTexCoord2i(01); glVertex2i(0h); glEnd(); glDisable(GL_TEXTURE_2D); } void drawquad(int xint y) { glBegin(GL_QUADS); glColor3f(0.0f1.0f0.0f); glVertex2i(xy); glColor3f(1.0f0.0f1.0f); glVertex2i(x+wy); glColor3f(0.0f1.0f1.0f); glVertex2i(x+wy+h); glColor3f(0.0f0.0f1.0f); glVertex2i(xy+h); glEnd(); } void texcopy() { if (!glIsTexture(img)) glDeleteTextures(1&img); glGenTextures(1&img); glBindTexture(GL_TEXTURE_2Dimg); glTexParameteri(GL_TEXTURE_2DGL_TEXTURE_MAG_FILTERGL_LINEAR); glTexParameteri(GL_TEXTURE_2DGL_TEXTURE_MIN_FILTERGL_LINEAR); glTexImage2D(GL_TEXTURE_2D0GL_RGBAwh0GL_RGBAGL_UNSIGNED_BYTE0); glMatrixMode(GL_PROJECTION); glLoadIdentity(); glOrtho(0wh0-11); glViewport(00wh); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT); drawquad(00); glBindTexture(GL_TEXTURE_2Dimg); glTexParameteri(GL_TEXTURE_2DGL_TEXTURE_MAG_FILTERGL_LINEAR); glTexParameteri(GL_TEXTURE_2DGL_TEXTURE_MIN_FILTERGL_LINEAR); //glCopyTexImage2D(GL_TEXTURE_2D0GL_RGBA00wh0); glCopyTexSubImage2D(GL_TEXTURE_2D00000wh); glMatrixMode(GL_PROJECTION); glLoadIdentity(); glOrtho(0screenwidthscreenheight0-11); glViewport(00screenwidthscreenheight); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } int main() { int running; glfwInit(); running=glfwOpenWindow(640480000000GLFW_WINDOW); if (!running) { glfwTerminate(); return 0; } glfwSetWindowTitle(title); glfwEnable(GLFW_STICKY_KEYS); glfwGetWindowSize(&screenwidth&screenheight); enable2d(); texcopy(); do { glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT); glLoadIdentity(); drawquad(640); drawmytex(); glfwSwapBuffers(); running=!glfwGetKey(GLFW_KEY_ESC)&&glfwGetWindowParam(GLFW_OPENED); GLenum error=glGetError(); if (error!=GL_NO_ERROR)running=error; glfwSleep(.017); } while (running==1); glDeleteTextures(1&img); glfwTerminate(); return running; } Try adding 'glColor3f(111);' in your 'drawmytex' function. I suspect that your texture is modulated (multiplied) with the current color if so the problem is not the texture copy but the way you display it.",c++ opengl 2d
589836,A,"Problem regarding OpenGL and the process of converting C to C++ Alright. So I wanted to use a file written in c in c++. I ran the code in c and had absolutely no problems. Since I don't know c I worked with some conversion software for a while but it wasn't effective (guessing the coding format wasn't in the style it needed). I decided to try it out myself and it looked like all I had to do was change a few malloc statements to new statments and replace free with delete. So I ran the program and it appears most of the functions work fine but this one gives me trouble. When I'm trying to update the display of a new image (I'm reading tiff image data and displaying it on the screen) I run into problems in these particular statements: glutReshapeWindow(ImageWidthImageLength); glClear(GL_COLOR_BUFFER_BIT);//these glRasterPos2i(0 0);//three glDrawPixels(ImageWidth ImageLength GL_RGB GL_UNSIGNED_BYTE ImageData);//statements are the ones giving me the error I included the first statement in the codeblock to show that it doesn't immediately blow up any any sort of OpenGl call. I keep getting errors along the lines of Cannot access memory at address 0xfbad248c Cannot access memory at address 0xfbad248c Cannot access memory at address 0xaabbeeaa I know this might be sort of vague but what could be some common sources of this error? It worked in c what may have caused this error in the quick switch to c++? Thanks and let me know if you need any more information. Alright so I'm not really using the standard OpenGL library I believe (this is for a class and I was given the library files by an instructor). I have the the code #include <GL/glut.h> as my #include. These libraries are exactly the same as the one found in the c file as I merely copied and pasted them into my c++ project. (I've also worked with them in a limited manner in another c++ project). Just in case I checked my file system for the files ""OPENGL32.DLL"" and ""GLU32.DLL"" and did not find them. I should probably include these additional details: A init() method is called at the beginning of the program before the actual input loop is called: void init(void) { glClearColor (0.0 0.0 0.0 0.0); glShadeModel(GL_FLAT); makeCheckImage(); glPixelStorei(GL_UNPACK_ALIGNMENT 1); } This method seems to work fine (a simple checkerboard pattern is created shortly after this function call). I'm not sure which sort of OpenGL files these pertain to (""OPENGL32.DLL"" or ""GLU32.DLL"") but hopefully this can narrow the problem down a little bit. As for the ImageData line I don't believe the ImageData object is corrupted or something of the sort because of some of reasons you were talking about and the fact that the code in the very first code block I posted (the source of errors) is within an else block as such: if (ImageData == NULL) {/*code*/} else{/*this is where the error code is*/} I can post info on the makefiles if you think it would be helpful (just don't want to clutter up this thread with the stuff if it's not). The first statement that doesn't give an error is an command from GLU32.DLL the others should be OPENGL32.DLL commands. Check if you linked the libraries correctly and try to copy the DLLs in the program directory. The last line could also be some error with your ImageData but since you said it worked in C and you only replaced malloc/free with new/delete it's unlikely that something is wrong there anyway you should check this too. A possibility would be to read the whole ImageData array without an OpenGL call involved. By the way I hope that ImageLength isn't the length of the complete image file but the height of the image. EDIT: OK so it seems to be a bit different. The init function you posted seems to access other gl... functions so your libraries somehow work. By the way it's interesting that you couldn't find the DLLs. They are usually stored in the Windows\System32 path. ""if (ImageData == NULL)"" is a first step but you should also try to read all ImageWidth*ImageLength entries for debug purposes perhaps you allocated the wrong number of bytes. Another reason for the crash could be that the header files you use aren't up-to-date and not compatible with your calls or the DLLs. If all this doesn't solve your problem posting the makefile would indeed be useful. I responded with an edit. ""wrong number of bytes"" That's exactly right. Quite embarassing. I appreciate the help greatly though; I would have been running in circles for god knows how long looking through archived OpenGL errors.  I don't know how experienced you are with the differences between C and C++ so I don't want to patronize you but its worth a shot: Have you used unsigned char* data = new unsigned char[size]; //correct //.... delete[] data; (array allocation) or unsigned char* data new unsigned char(size); //wrong //.... delete data; (allocate a single char with value size) for your image data?",c++ opengl
1252976,A,"How to handle multiple keypresses at once with SDL? been getting myself familiar with OpenGL programming using SDL on Ubuntu using c++. After some looking around and experimenting I am starting to understand. I need advice on keyboard event handling with SDL. I have a 1st person camera and can walk fwd back strafe left and right and use the mouse to look around which is great. Here is my processEvents function: void processEvents() { int mid_x = screen_width >> 1; int mid_y = screen_height >> 1; int mpx = event.motion.x; int mpy = event.motion.y; float angle_y = 0.0f; float angle_z = 0.0f; while(SDL_PollEvent(&event)) { switch(event.type) { case SDL_KEYDOWN: switch(event.key.keysym.sym) { case SDLK_ESCAPE: quit = true; break; case SDLK_w: objCamera.Move_Camera( CAMERASPEED); break; case SDLK_s: objCamera.Move_Camera(-CAMERASPEED); break; case SDLK_d: objCamera.Strafe_Camera( CAMERASPEED); break; case SDLK_a: objCamera.Strafe_Camera(-CAMERASPEED); break; default: break; } break; case SDL_MOUSEMOTION: if( (mpx == mid_x) && (mpy == mid_y) ) return; SDL_WarpMouse(mid_x mid_y); // Get the direction from the mouse cursor set a resonable maneuvering speed angle_y = (float)( (mid_x - mpx) ) / 1000; angle_z = (float)( (mid_y - mpy) ) / 1000; // The higher the value is the faster the camera looks around. objCamera.mView.y += angle_z * 2; // limit the rotation around the x-axis if((objCamera.mView.y - objCamera.mPos.y) > 8) objCamera.mView.y = objCamera.mPos.y + 8; if((objCamera.mView.y - objCamera.mPos.y) <-8) objCamera.mView.y = objCamera.mPos.y - 8; objCamera.Rotate_View(-angle_y); break; case SDL_QUIT: quit = true; break; case SDL_VIDEORESIZE: screen = SDL_SetVideoMode( event.resize.w event.resize.h screen_bpp SDL_OPENGL | SDL_HWSURFACE | SDL_RESIZABLE | SDL_GL_DOUBLEBUFFER | SDL_HWPALETTE ); screen_width = event.resize.w; screen_height = event.resize.h; init_opengl(); std::cout << ""Resized to width: "" << event.resize.w << "" height: "" << event.resize.h << std::endl; break; default: break; } } } now while this is working it has some limitations. The biggest one and the purpose of my question is that it seems to only process the latest key that was pressed. So if I am holding 's' to walk backwards and I press 'd' to strafe right I end up strafing right but not going backwards. Can someone point me in the right direction for better keyboard handling with SDL support for multiple keypresses at once etc? Thanks SDL keeps track of the current state of all keys. You can access this state via: SDL_GetKeyState() So each iteration you can update the movements based on the key state. To make the movement smooth you should update the movement magnitude based on the time elapsed between updates.  Instead of only looking at keydown events any solution which is going to be caring about multiple keys at once is going to have to be looking at both keydown and keyup events and keeping track of the state of the keys in question. So instead of (pseudocode): on keydown: case left_key: object.setMovement(left) case forward_key: object.setMovement(forward) instead you'd have something more like (again pseudocode): on keydown: case left_key: keystates[left] = true object.updateMovement(keystates) case forward_key: keystates[forward] = true object.updateMovement(keystates) on keydown: case left_key: keystates[left] = false object.updateMovement(keystates) case forward_key: keystates[forward] = false object.updateMovement(keystates) Then the updateMovement routine would look at keystates and figure out a composite movement based on the states of all movement keys together.  A good approach will be to write a keyboard (""input"") handler that will process input events and keep the event's state in some sort of a structure (associative array sounds good - key[keyCode]). Every time the keyboard handler receives a 'key pressed' event it sets the key as enabled (true) and when it gets a key down event it sets it as disabled (false). Then you can check multiple keys at once without pulling events directly and you will be able to re-use the keyboard across the entire frame without passing it around to subroutines. Some fast pseudo code: class KeyboardHandler { handleKeyboardEvent(SDL Event) { keyState[event.code] = event.state; } bool isPressed(keyCode) { return (keyState[keyCode] == PRESSED); } bool isReleased(keyCode) { return (keyState[keyCode] == RELEASED); } keyState[]; } ... while(SDL Pull events) { switch(event.type) { case SDL_KEYDOWN: case SDL_KEYUP: keyHandler.handleKeyboardEvent(event); break; case SDL_ANOTHER_EVENT: ... break; } } // When you need to use it: if(keyHandler.isPressed(SOME_KEY) && keyHandler.isPressed(SOME_OTHER_KEY)) doStuff(TM); Really a nice solutionthis can be used in almost all your games  use SDL_GetKeyState to get the keyboard state",c++ linux opengl ubuntu sdl
