0,A,"Testing CustomObject marshallers in grails? I'm trying to create custom object marshallers in grails and a tutorial I was following indicated that the marshaller should be setup in BootStrap.groovy in the init closure. However when I call myObject as JSON in tests the marshaller doesn't get used. What do I need to do to use custom marshallers in tests? This has been an issue in Grails since 1.2.4 at least. There is an open JIRA ticket out there with no work being done. Is this the ticket you're talking about: http://jira.grails.org/browse/GRAILS-6899  You should be able to register the object marshaller any time before you use it. So to use it in a test just add it to the setUp method. Odd. I created a ""No Class name marshaller"" and registered it with priority 0 1 and 1000 and it just doesn't seem to be being hit. Also this is a system-wide marshaller so I need to have this in a more general place."
1,A,"Conditionally ignoring tests in JUnit 4 OK so the @Ignore annotation is good for marking that a test case shouldn't be run. However sometimes I want to ignore a test based on runtime information. An example might be if I have a concurrency test that needs to be run on a machine with a certain number of cores. If this test were run on a uniprocessor machine I don't think it would be correct to just pass the test (since it hasn't been run) and it certainly wouldn't be right to fail the test and break the build. So I want to be able to ignore tests at runtime as this seems like the right outcome (since the test framework will allow the build to pass but record that the tests weren't run). I'm fairly sure that the annotation won't give me this flexibility and suspect that I'll need to manually create the test suite for the class in question. However the documentation doesn't mention anything about this and looking through the API it's also not clear how this would be done programmatically (i.e. how do I programatically create an instance of Test or similar that is equivalent to that created by the @Ignore annotation?). If anyone has done something similar in the past or has a bright idea of how else I could go about this I'd be happy to hear about it. You should checkout Junit-ext project. They have RunIf annotation that performs conditional tests like: @Test @RunIf(DatabaseIsConnected.class) public void calculateTotalSalary() { //your code there } class DatabaseIsConnected implements Checker { public boolean satisify() { return Database.connect() != null } } [Code sample taken from their tutorial] Thanks for this answer - an interesting alternative syntax for the functionality though I'll be going with `Assume` directly so as not to introduce another dependency. I personally prefer this solution. If you have many tests that should be run based on the same conditions this would be far more ideal than having to use Assume in every test. Also if this can be used on a class level rather than the method level it will be even more ideal. junit-ext is not available on Maven Central :-( I would prefer it 'cuz this helps to run the test conditionally at run time. It suits where a number of unit test are going to run and the requirement is to run the unit tests on particular checker. I really amazed to see that junit-ext is not available on maven repository. How would we get avail this in maven project.  The JUnit way is to do this at runtime is org.junit.Assume.  @Before public void beforeMethod() { org.junit.Assume.assumeTrue(someCondition()); // rest of setup. } You can do it in a @Before method or in the test itself but not in an @After method. If you do it in the test itself your @Before method will get run. An assumption failure causes the test to be ignored. Edit: To compare with the @RunIf annotation from junit-ext their sample code would look like this: @Test public void calculateTotalSalary() { assumeThat(Database.connect() is(notNull())); //test code below. } Not to mention that it is much easier to capture and and use the connection from the Database.connect() method this way. @notnoop that isn't my observation at all. They are ignored. The IDEA test runner reports them that way and a look at the JUnit source code shows that it reports the test as ignored. To quote: ""In the future this may change and a failed assumption may lead to the test being ignored."" It in fact changed as of 4.5 I believe. The current javadoc says: ""The default JUnit runner treats tests with failing assumptions as ignored. Custom runners may behave differently."" http://github.com/KentBeck/junit/blob/7aac4b19d359285041ccb51d575235339a1a8be0/src/main/java/org/junit/Assume.java Thanks! I didn't catch that in the 4.5 release notes! Thanks! Thanks that's exactly what I'm looking for. The semantics of `Assume` sound like exactly what I'm trying to do here so even if a runner doesn't treat it as `@Ignore` I trust it will be handled appropriately. Eclipse 3.6 with Junit 4.8.1 reports false Assumptions as a passing test. Same with ant 1.8.1. @fijaaron Regarding ANT this is a known limitation of its XML format and since so many tools rely on this format changing it is a big deal so it won't happen so fast. Regarding eclipse since IDEA does it I have no idea why eclipse hasn't caught up. Regarding the ANT functionality see here: https://issues.apache.org/bugzilla/show_bug.cgi?id=43969 Note that if you are using maven's surefire plugin with JUnit in this fashion you need to use surefire 2.7.2 or greater to get tests halted because of Assume to show up as Skipped rather than success. That Eclipse reports failed assumptions as passing is a bug: https://bugs.eclipse.org/bugs/show_bug.cgi?id=359944 If you have an After method though it appears the After method still runs but it doesnt look like Assume can be used in an After method. Is there anyway to prevent both the before and after methods from running? @Yishai I set up a simple test where I put Assume.assumeTrue(false) in the before method but the after method still runs. It sounds like that is the opposite of what you expected... @JeffStorey the after method only runs if the assume is in the method itself not in before. That makes sense - if there was a before there would be an after. You can gain more fine grained control using a TestRule. http://kentbeck.github.com/junit/javadoc/latest/org/junit/rules/TestRule.html Thanks I never really thought about using Assume in a beforeClass method. I'll give that a shot. And I'll look into ClassRule as well. Thanks. @JeffStorey then you are looking for a couple of things. One is the `@BeforeClass` annotation where you can have your assumption fail there which will skip the whole class. Another is `@ClassRule` (for the fine grained control but over the whole class one time). @JeffStorey I set up a test as well and it worked as I described or at least I thought it did. I tried again and you are correct. I guess TestRule is the way to go then. @Yishai thanks. I'm not sure if TestRule will work in this case though. Trying to find a way to basically ignore the whole test class if the assumption is false without annotating every method. I could always write a custom runner if needed to process some annotation. That is AWESOME! I'm going to use this method to separate my smoke tests with both eclipse and maven. Sweet."
2,A,"How do i write Junit4 tests without Spring transactional test support? I would like to test transaction rollbacks in my application service. As a result i do not want to use spring's AbstractTransactionalJUnit4SpringContextTests with the @Transactional annotation as that wraps my test method in a transaction. I know spring also offers AbstractJUnit4SpringContextTests for tests without transactions. However i need to have some transactions to save data into my database and also query data for assertions after running the service under test. How can i write a Junit 4 spring transactional test without the default transaction test management? The spring docs should cover this in their testing chapter What you might want is to configure your test without the default TransactionalTestExecutionListener like this @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration @TestExecutionListeners({DependencyInjectionTestExecutionListener.classDirtiesContextTestExecutionListener.class}) public class SimpleTest { @Test public void testMethod() { // execute test logic... } } I'm sorry I didn't quite read your original question completely. So you actually need a Transaction in the test. In that case creating it programatically would probably work I think you need the JPATransactionManager for that. Then again maybe you could do what you need to do declaratively by controlling the propagation behaviour. Maybe if you could provide an example test of what you need to do... Paul would i then use the EntityManager to create transactions programatically when i need to use them?  Here is the answer to your question ; @ContextConfiguration(value = ""/applicationContext.xml"") public class JPASpringTest extends AbstractJUnit4SpringContextTests { @PersistenceContext(unitName=""jpadenemelocal"") EntityManager entityManager; @Autowired protected PlatformTransactionManager transactionManager; @Test public void testInsertRolManyToMany() { TransactionStatus status = transactionManager.getTransaction(null); // your code transactionManager.commit(status); } }"
3,A,"Show grails test results in Netbeans IDE does anybody know a possibility to show the Unit-Test results of a grails project in the ""Test Result""-Window of the netbeans IDE? The test results are saved in the project folder as JUnit XML Files. It is in the target/test-reports/html/index.html from project window. Just right click on it and choose view. I know that :-) But I'm looking for the direct integration like the one for Java-Projects where you can browse the results in the Testresult window. Seems that there is no other solution for the current version of netbeans  It does not look like there is any such integration like there is for e.g. Ruby. For Java projects the IDE does not wait for XML files to be written to a build directory; it listens for special events in the Ant <junit> task and reports them immediately. I do not think much work is being done on the Groovy support generally but perhaps someone could write a plugin which just waits for TEST-*.xml to appear in some build directory and displays them in Test Results. This would require using the org.netbeans.modules.gsf.testrunner API (currently not published for public use). Such a plugin could be handy not just for Grails users. nice idea. Dont know if i'm able to do this but maybe it worth a try"
4,A,How can I run all JUnit tests in one package @ NetBeans? I have like trillion test packages with bazillion tests and I want to run just some of packages. Now I must run whole project (some tests takes long to complete) or I need to run every single file manually. How is possible to run just some packages in NetBeans ? I can't find this option ... If you use JUnit 4 then try ClasspathSuite and its regex filters.  It's probably not what you want but the NetBeans help topic Running a JUnit Test says: If you want to run a subset of the project's tests or run the tests in a specific order you can create test suites that specify the tests to run as part of that suite. After creating a test suite you run the suite in the same way you run a single test class. Creating a test suite is covered in the topic Creating a JUnit Test. yep I knew that only solution so far is writing suite but I am actually pretty surprised (or I don't understand how is that not possible) that such an great IDE doesn't offer this feature :/ @Xorty: Yes although you can run individual tests in NetBeans I don't see any other way to run a subset. It is a little easier to run selected tests in Eclipse. I use tons of Maven and I am not giving up NetBeans there ... (nothing again Eclipse it's brilliant)
5,A,"How do I configure JUnit's Source in Eclipse? I am using Eclipse Galileo for Java EE and I want to configure JUnit to show me the source code when I try to navigate to its methods. I've tried attaching source to the JUnit library but the library definition is not editable. I cannot even find where to configure the JUnit library in the preferences. When I open the Add Library window and choose JUnit I see a dialog where I can choose the JUnit version but it shows that Source Location is ""not found"". How can I configure Eclipse to find JUnit's source? I downloaded the Eclipse SDK and checked the differences and I finally got it to work. Download this JAR into your eclipse/plugins directory. Edit the file source.info in your eclipse/configuration/org.eclipse.equinox.source directory and add the following line: org.junit4.source4.5.0.v20090423plugins/org.junit4.source_4.5.0.v20090423.jar-1false Open the file artifacts.xml in your eclipse directory and add the following fragment: <artifact classifier='osgi.bundle' id='org.junit4.source' version='4.5.0.v20090423'> <properties size='2'> <property name='artifact.size' value='128389'/> <property name='download.size' value='128389'/> </properties> </artifact> If Eclipse is already open you'll need to restart it for the changes to be detected. Note: For Eclipse 3.6 (Helios) you should use the updated JAR(s). See the comments by @Supressingfire and @Milo. Note: on Eclipse 3.6(Helios) step 3 (artifacts.xml) is not necessary. Tested on Ubuntu Eclipse 3.6: Version: Helios Service Release 2 Build id: 20110218-0911 Thank you for this feedback very interesting. +1 You could select your own answer as the official one if you want (no reputation gain in this gain though) Nice answer. Can you update it for the Helios/3.6 release? I believe the new URL is ftp://ftp.osuosl.org/pub/eclipse/eclipse/updates/3.6/R-3.6-201006080911/plugins/org.junit.source_4.8.1.v4_8_1_v20100427-1100.jar and the version would presumably be 4.8.1.v4_8_1_v20100427-1100 I haven't downloaded Eclipse 3.6 yet so I just referenced your comment for now. I might update it later if I have time. Just want to confirm that this solution works on 3.6 service release 1. I don't know whether it was necessary but I also added the the jar for 3.8.2 source code as I use Junit 3 launcher. Used the following two jars: ftp://ftp.osuosl.org/pub/eclipse/eclipse/updates/3.6/R-3.6.1-201009090800/plugins/org.junit.source_3.8.2.v3_8_2_v20100427-1100.jar ftp://ftp.osuosl.org/pub/eclipse/eclipse/updates/3.6/R-3.6.1-201009090800/plugins/org.junit.source_4.8.1.v4_8_1_v20100427-1100.jar  As mentioned in this thread if you have downloaded the SDK version of Galileo yu have the sources of the main plugins. For JUnit this would be:  <pathTo>\eclipse-SDK-3.5-win32-x86_64\eclipse\plugins\org.junit4.source_4.5.0.v20090423.jar You can try to add that as a source file in the Source tab of a JUnit launcher configuration and see if that solves your issue when you are debugging your JUnit test and are stepping through JUnit native methods. (Note: I have the x64 version of eclipse Galileo but I believe the file is the same for the x32 or for other platforms) Thanks for the reply but I don't have the Eclipse SDK. However I have found a way to do it without downloading the whole SDK. Please check my solution.  @Hosam Aly answer also works in Eclipse 4.3.1: The jar to download is here The text to append to eclipse\configuration\org.eclipse.equinox.source is org.junit.source4.11.0.v201303080030plugins/org.junit.source_4.11.0.v201303080030.jar-1false I did not need to change artifacts.xml"
6,A,"Our project contains 2600 class files - where and how should we start writing junit tests? Our project contains 2600 class files and we have decided to start using automated tests. We know we have should have started this 2599 class files ago but how and where should large projects start to write tests? Pick a random class and just go? What's important to know? Are there any good tools to use? Actually you should have started 2600 classes ago. Write the unit tests first. That's only for TDD and not everyone wants to do that. Write a unit test before you change something and for every bug you encounter. In other words test the functionality you are currently working on. Otherwise it is going to take a lot of time and effort to write tests for all classes. +1 for _every bug you encounter_ Write a test case that demonstrates the bug then do the fix then see that the test case passes.  Don't try unit tests first. Do system tests (end-to-end-tests) that cover large areas of code. Write unit tests for all new code. This way you stabilize the old code with your system regression tests. As more and more new code comes in the fraction of code without unit tests begin to fade away. Writing unit tests for old code without the system tests in place will likly break the code and will be to much work to be justified as the code is not written with testability in mind. The other possibility when retrofitting old code with unit tests is that you write the unit tests to pass (since the system already ""works"") and end up validating incorrect behaviour. @Mark Peters: IMHO that's fine in a huge system there are always somebody relying on the incorrect behavior. By writing the unittest so it passes the incorrect behavior when you correct the incorrect behavior the unittest might warn you that the change may affect someone else's code (and then it's design decision whether to break the backward compatibility or not). That's a good point Lie. You can defer figuring out what's ""right"" and what's ""wrong"" until a change needs to be made. Best to flag those types of tests though since unit tests are generally supposed to *specify* correct behaviour not just protect against functional changes. +1 for mentioning the agony of _the code is not written with testability in mind_ Whether one TDDs or not the tests really need to be written more or less in parallel with the code.  Oh and one more thing - having an insufficient number of unit tests is far better than having none. Add a few at a time if that's all you can do. Don't give up.  Other answers have given useful advice but I miss a clear articulation of the underlying principle: strive to maximize the benefit from your efforts. Covering a large legacy codebase with unit tests to a significant extent takes a lot of time and effort. You want to maximize the outcome of your effort from the start. This not only gives valuable feedback early on but helps convincing / keeping up the support of both management and fellow developers that the effort is worth it. So start with the easiest way to test the broadest functionality which is typically system/integration tests identify the critical core functionality of the system and focus on this identify the fastest changing/most unstable part(s) of the system and focus on these. +1 for _strive..._ Pragmatism is good.  You may find Michael Feathers' book Working Effectively with Legacy Code useful. yes this is a great book  You might find this book relevant and interesting. The author explains how to do exactly what you ask for here. http://my.safaribooksonline.com/0131177052  Start writing tests for each bug that is filed (Write the test watch it fail fix the bug test again). Also test new features first (they are more likely to have errors). It will be slow in the beginning but as your test infrastructure grows it will become easier. If you are using java 5 or higher use junit 4. Learn about the difference of unit tests integration tests and acceptance tests. Also have a look at mocking.  You're fairly dorked now but write tests that bolster the most critical code you have. For example if you have code that allows functionality based upon users' rights then that's a biggy - test that. The routine that camelcases a name and writes it to a log file? Not so much. ""If this code broke how much would it suck"" is a good litmus test. ""Our internal maintenance screens would look bad on IE6"" is one answer. ""We'd send 10000000 emails to each of our customers"" is another answer. Which classes would you test first hehe."
7,A,"How can i use JUnit in Servlet and JSP? I am writing a web application which should be tested with JUnit framework. So please suggest me how we can use JUnit in Jsp and servlet and also how to generate test case reports using Ant?? Thanks in advance Why cant we use Cactus? I have heard about that and what it differs from other test cases? Duplication of: http://stackoverflow.com/questions/1398423/how-to-unit-test-logic-in-jsp/1398460 ? Yes cactus is an good option.. it is mainly for Integration testing and can also satisfy the Unit testing. For details u just try this site http://jakarta.apache.org/cactus Cactus is retired is there an alternative?  For servlets I use the spring framework mock classes - there are mock request response servlet context etc. You do not need to use the spring framework in your application to use them. Regarding your second question i think what you are looking for is the junitreport Ant task. Here is a sample (taken from here): <target name=""junit"" description=""Runs the unit tests"" depends=""jar""> <delete dir=""${junit.out.dir.xml}""/> <mkdir dir=""${junit.out.dir.xml}""/> <junit printsummary=""yes"" haltonfailure=""no""> <classpath refid=""classpath.test""/> <formatter type=""xml""/> <batchtest fork=""yes"" todir=""${junit.out.dir.xml}""> <fileset dir=""${src.dir}"" includes=""**/*Test.java""/> </batchtest> </junit> </target> <target name=""junitreport"" description=""Create a report for the rest result""> <mkdir dir=""${junit.out.dir.html}""/> <junitreport todir=""${junit.out.dir.html}""> <fileset dir=""${junit.out.dir.xml}""> <include name=""*.xml""/> </fileset> <report format=""frames"" todir=""${junit.out.dir.html}""/> </junitreport> </target>"
8,A,Can JUnit simulate OutOfMemoryErrors? I have a method which tries to call an in-memory image converter and if that fails then tries to do the image conversion on disk. (The in-memory image converter will try to allocate a second copy of the image so if the original is very large we might not have sufficient memory for it.) public BufferedImage convert(BufferedImage img int type) { try { return memory_converter.convert(type); } catch (OutOfMemoryError e) { // This is ok we just don't have enough free heap for the conversion. } // Try converting on disk instead. return file_converter.convert(img type); } I'd like to write unit tests for JUnit which exercise each code path but it's inconvenient to run JUnit with little enough heap to force an OutOfMemoryError. Is there some way to simulate an OutOfMemoryError within JUnit? It has occurred to me that I could make a fake subclass of BufferedImage that throws an OutOfMemoryError the first time a method called by the in-memory converter is called but then behaves normally on subsequent calls. This seems like a hack though. You should be mocking out your converter rather than using a real one. Once you do that you simply have your mocking library throw a new OOME when the convert() method is called. For example with JMock you would do this: allowing(mockConverter).convert(with(any(int.class))); will(throwException(new OutOfMemoryError()));  The handling of an OutOfMemoryError and the testing of it is very difficult. You can not test it with a mock. Depending on which place the OutOfMemoryError exception occur the effect can be very different. The problem is that the OutOfMemoryError can not be pass through to your calling code. If you want a real test then you need to allocate in a second thread memory and produce a an OutOfMemoryError. This should you repeat multiple time to see different effects. If you want only test your catch block then you can mock it. Because a OutOfMemoryError can produce a fatal error on any other thread in your application you should prevent it. That I think a better solution is to calculate the free memory. The needed memory size and call the on disk conversion before the exception occur. The problem I see with checking the amount of free heap is that finding that it's too little is no guarantee that that the allocation will fail---attempting the allocation could cause the garbage collector to free enough heap for it to succeed.  Inject a stub or mock memory_converter whose convert() method throws OutOfMemryError. It has occurred to me that I could make fake subclass of BufferedImage that throws an OutOfMemoryError the first time a method called by the in-memory converter is called but then behaves normally on subsequent calls. This seems like a hack though. Mocking frameworks are generally very powerful and should let you specify this behavior. For example JMock: http://www.jmock.org/returning.html Returning Different Values on Consecutive Calls There are two ways to return different values on different calls. The first is to define multiple expectations and return a different value from each: oneOf (anObject).doSomething(); will(returnValue(10)); oneOf (anObject).doSomething(); will(returnValue(20)); oneOf (anObject).doSomething(); will(returnValue(30)); The first invocation of doSomething will return 10 the second 20 and the third 30.  It has occurred to me that I could make fake subclass of BufferedImage that throws an OutOfMemoryError the first time a method called by the in-memory converter is called but then behaves normally on subsequent calls. This seems like a hack though. You are headed down the right path here. Mocking out objects is exactly what these types of things are beneficial for. You don't really care if the OutOfMemory error is legit you just want to make sure it is thrown/caught and the other path is executed. Mock it out and you will be good to go.
9,A,"Why can classes being unit tested with JUnit not have a main? My lecturer mentioned this before but I don't really understand why this is the case. Would anyone be able to explain ? We are writing a program to compute an array list of prime numbers and we have to use JUnit to ensure all members of this arraylist are prime. Why can I not use a main in testing this class ? Thank you very much :) Because JUnit tests are run by a framework not as a standard console application. The JUnit test runner finds the tests by reflection. See the documentation here.  Because JUnit is providing a main that calls the functions that you provide in your classes. You can still have your own main functions; they just won't get used when you run JUnit. You can use main functions to test your own classes individually but using JUnit has some advantages as described in org.life.java's answer. +1 - there is a `main` but the test runner is the running application the test classes and the classes under test are a kind of ""payload"".  You can it just wouldn't be recommended. If you write a unit test for testing it then you can use the junit test runner to run the test and to produce a report indicating whether it passed or failed. If you don't do this then you'll need to code your own report mechanism. Unit tests have the following structure normally: Create test infrastructure Execute test Validate passed Your situation has something similar and is thus a good candidate for using junit. The unit testing API's available provide you with useful utilities that you would ordinarily have to code yourself. Why don't you try both approaches and see for yourself.  See: org.junit.runner.JUnitCore.main(String...) something like that is underlying.  In unit testing you are not testing anything as a whole. A unit test must test a UNIT normally a method. So you should write the method that computes your array and use Junit to just test the method. The main method is just an entrypoint and it ""defines"" the flow of the procedure. In unit testing we don't worry on flow. We just forcus on the unit. The program flow is verified using the System/Component test not by the unit tests.  Ok these answers are for the most part too complex. I think your question is more fundamental. ANd its a very good one The answer is when you become a java developer and start writing large amount of code that get updated/fixed over time then it helps to have a separate test plug-in that will automatically run tests on your code from outside the code to check if it’s still working in the way you would expect. This means you can fix/debug different aspect of the code for whatever reason and afterwards your boss walks over and asks does the code still do what the client wanted it to do since your fix? Without complication You can answer him without complex in-main error statements which are mixed up with the normal program output (and slow down the code in non test conditions) but with a pretty green junit bar that says it all still works. You won’t see the value of this until you develop large projects and you have hundreds of tests to do. In addition junit has a number of other tricks up its sleeves..."
10,A,"Selenium typeKeys strips out dot from the String being typed The following instruction Selenium.typeKeys(""location"" ""gmail.com""); types the string gmailcom instead of gmail.com. What's happening there? From the comments: I am trying to simulate autofill and the only way to do it currently on selenium is to combine type and typeKeys. eg: selenium.type(""assigned_to"" split[0]+""@""); selenium.typeKeys(""assigned_to"" ""gmail.com""); Now my question is why typeKeys doesn't type the 'dot' in between gmail.com? Did you solve this? We had similar problems using typekeys in selenium python. One workaround we figured to resolve this issue is to use the combination of 'type' and 'type_keys'. As you might be aware type does not have such issues. We did this in our selenium python script and it works just fine. For example: If there's an email address to be entered in a text box: test.me@test.me.uk Then do type(locator’test.me@test.me.’) type_keys(locator’uk’) Maybe a very crude way to do but it did the job. Hope this helps someone else with a similar problem.  Have you tried using the Native key functions and javascript char codes? I couldn't get a 'period' character to work using them (char 190) but I got the decimal (char 110) to work just fine and the text box shouldn't have a problem with either. selenium.Focus(""assigned_to""); selenium.Type(""assigned_to"" split[0]+""@""); selenium.TypeKeys(""assigned_to"" ""gmail""); selenium.KeyPressNative(""110""); selenium.TypeKeys(""assigned_to"" ""com""); I couldn't find any function in selenium that types a period (ASCII 46).  I got the same behaviour but fixed it by passing a variable to the Type command instead of a string. string email = @""name@gmail.com""; selenium.Type(inputfield email); It works like a charm! Thats because you are using the Type method the problem in the question is with the TypeKeys method.  I'm also seeing this behaviour when using Selenium RC (C#) and with different characters ('y' for example which also seems to remove the character follow it from the typing action..) For some situations it is entirely possible to get around the issue by typing out the keys with the TypeKeys or KeyPress functions but I haven't managed to find a solution that works when you want to type text in a field enter control characters ([Enter] or a newline for example) and then type more text.. (using the 'Type' function in this case simply overwrites the field contents...). If anyone manages to find a reasonable solution to this issue please add it to this question as it's starting to come up in google now and would probably help alot of people out.. (I'd start a bounty if I could..)  Use the type method. From the javadoc for typekeys: this command may or may not have any visible effect even in cases where typing keys would normally have a visible effect ... In some cases you may need to use the simple ""type"" command to set the value of the field and then the ""typeKeys"" command to send the keystroke events corresponding to what you just typed. Believe me i read that doc. I am trying to simulate autofil and the only way to do it currently on selenium is to combine type and typeKeys. eg: selenium.type(""assigned_to"" split[0]+""@""); selenium.typeKeys(""assigned_to"" ""gmail.com""); Now my question is why typeKeys doesnt type the 'dot' in between gmail.com? I added your comment to your question. Comments aren't read by everyone.  Suppose the string to be typed using typeKeys is ""abc.xyz.efg"". Then we can use type and typeKeys commands to write the given string. type(locator""abc.xyz."") typeKeys(locator""efg"") The above two steps are useful whenever you want to select an element in drop down box and the drop down pops down only if we use typeKeys command.  Also try to set focus on element before write on it. selenium.focus(locator); selenium.typeKeys(locator value); it did function in my case handling a input type=password.  This is an open bug in Selenium (bug SEL-519). After some fiddling around with it I finally managed to enter '.' into a textarea by using JavaScript. Execute something like window.document.getElementById('assigned_to').value += '.' via storeEval or the like."
11,A,TestNG Ant tasks vs Surefire I was wondering how different surefire is when executing TestNG than TestNG ant tasks? The reason is that I am seeing consistent difference in behavior when trying to run a TestNG test that extends a JUnit test base (this is a workaround to run JBehave tests in TestNG described here: http://jbehave.org/documentation/faq/). Surefire detects my test as a JUnit test incorrectly (probably because its base is TestCase) while the Ant tasks run perfectly. Can anyone provide an insight into how TestNG handle both cases? The test looks as follows: public class YourScenario extends JUnitScenario { @org.testng.annotations.Test public void runScenario() throws Throwable { super.runScenario(); } } The short answer is that the ant task is part of the TestNG distribution so it's part of our tests and I always make sure that it remains up to date with TestNG. Surefire is developed as part of the Maven project and as such it sometimes lags behind (and just like you I have sometimes encountered bugs when running my tests with Surefire that didn't happen when running from the command line/ant/Eclipse). I'll bring this question to the Maven team's attention maybe they'll have more to say. Thanks Cedric..  This looks to be a known bug: http://jira.codehaus.org/browse/SUREFIRE-575. Have you tried using a TestNG XML suite definition instead of Surefire's automatic test case detection? That may work but unfortunately will not help me since I am passing the filter for the tests to run  e.g. (**/*Test) from an external script. So sticking with the ant-tasks is more straightforward than executing some script to modify that xml with filter input.
12,A,"Unit testing both the presence/absence of a library The question: Is there a way I can use a ClassLoader to test both the presence and absence of the library I'm checking for? I have a some code which will use a particular library if available or fall back to some embedded code if not. It works fine but I'm not sure how to unit test both cases in the same run or if it's even possible. At the moment my unit tests only check one case because the library is either in the main classpath or it's not. The code I use to check for the library availability is basically: boolean available; try { Class.forName(""net.sf.ehcache.CacheManager""); available = true; } catch (ClassNotFoundException e) { available = false; } I could potentially alter the way this is done if it would make unit testing easier. You are trying to do two things at once: test the different implementations of your code and test that the correct implementation is selected based on the circumstances (or more generally that your app works regardless of whether or not the library in question is present). Separating the two tasks into distinct tests simplifies the problem. Thus I would implement the two versions of your code as two Strategies and put the code which checks the classpath and creates the necessary strategy into a Factory (Method). Then both strategies can be unit tested independent of classloader and classpath settings: interface MyStrategy { public void doStuff(); } class MyLibraryUsingStrategy implements MyStrategy { public void doStuff() { // call the library } } class MyEmbeddedStrategy implements MyStrategy { public void doStuff() { // embedded code } } In unit tests you can simply create and test either of the concrete strategies: @Test void testEmbeddedStrategy() { MyStrategy strategy = new MyEmbeddedStrategy(); strategy.doStuff(); // assert results } A simple factory method to create the appropriate strategy: MyStrategy createMyStrategy() { MyStrategy strategy; try { Class.forName(""net.sf.ehcache.CacheManager""); strategy = new MyLibraryUsingStrategy(); } catch (ClassNotFoundException e) { strategy = new MyEmbeddedStrategy(); } return strategy; } Since the factory code is fairly trivial you may even decide not to write automated tests for it. But at any rate testing the factory is more (part) of an integration test than a unit test; you can simply put together different setups of your app - one with and the other without the library in question - and see that both work properly. Marking this as the answer as it's probably the closest I'm going to get. In my case the choice only determines which implementation of an interface to create so the Strategy pattern isn't really needed on top of this. But both of the implementations are fully unit tested and you are right that the remaining code is trivial enough that it may not warrant a cumbersome setup of running tests with different classpaths. @Nick if I understand you correctly you have already implemented Strategy :-) Possibly - I'd just call it regular polymorphism! :)"
13,A,"Where does LogCat's Log.x() output go when running Android JUnit tests? I noticed that when testing plain Java classes via test classes derived from TestCase and AndroidTestCase LogCat output disappears. Is it possible to still capture the output of these messages? or my only recourse is to use the much more sluggish ActivityInstrumentationTestCase2<> as a base class? I had similar issue... The point here is that the logcat view available in Eclipse does not show anything when running the project in Android Junit mode. At least in the Android 2.1 that I was using that's the behavior. You can workaround this issue by checking the logcat from command line (terminal window): # check the device name you are using # It gives something like this: $ ./adb devices List of devices attached emulator-5554 device # open logcat of the device $ ./adb -s emulator-5554 logcat D/AndroidRuntime( 943): D/AndroidRuntime( 943): >>>>>> AndroidRuntime START com.android.internal.os.RuntimeInit <<<<<< D/AndroidRuntime( 943): CheckJNI is ON . . .  Simply using Log.v(""MyIdentifier""""MyMessage"") statements seems to log everything for me both from the Unit test classes themselves and from my Android application under test. This info may help someone using Android jUnit for the first time: Android jUnit testing will only actually start an activity when getActivity() is called from within a test class. If get Activity is not called you will not see the results of any of the logging calls you have written in for example onCreate or onResume. There seems to be an exception to this rule though when the Activity marked as ""Main"" and ""Launcher"" in the Android Manifest is under test.  This will also happen if compiling on a device and then running JUnit test with the simulator set as the target. The reason for this is the active device in your DDMS perspective will no longer be selected and the Logcat view will be filtering accordingly. This is why ""adb logcat"" will still work (No filter on it). Make sure you select the same device in the DDMS perspective as the target for the JUnit test. Hope this helps someone because it drove me nuts for a bit.  Both of these statements produce log in logcat:  android.util.Log.d(TAG ""This is Log.d""); System.out.println(""This is System.out.println""); I meant exactly that. This is how you log from your tests. Are you using android test runner or something else ? This could be your problem I guess. Yes I know but that is not what I asked. My problem is that I would like to see the same LogCat output while running JUnit."
14,A,CalledFromWrongThreadException exercising JUnit tests on Android I am new to JUnit and Android and good test documentation for working with Android is hard to find. I have a test project with classes that extend ActivityInstrumentationTestCase2. Simple tests to examine the state of the GUI (what's enabled relative positions etc) work as expected. However when I attempt to perform button click actions the wrong thread exception is thrown. Anyone know how to get around this issue? As a follow-on does anybody have any good suggestions for free resources on test or TDD for Android? I am using Eclipse/MotoDev. Thanks I can get different failure traces depending on how I invoke each button but including one here for reference: android.view.ViewRoot$CalledFromWrongThreadException: Only the original thread that created a view hierarchy can touch its views. at android.view.ViewRoot.checkThread(ViewRoot.java:2683) at android.view.ViewRoot.playSoundEffect(ViewRoot.java:2472) at android.view.View.playSoundEffect(View.java:8307) at android.view.View.performClick(View.java:2363) at com.android.tigerslair.demo1.test.GoTest.setUp(GoTest.java:49) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:169) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:154) at android.test.InstrumentationTestRunner.onStart(InstrumentationTestRunner.java:430) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1447) Here is the simple setup() routine: @Override protected void setUp() throws Exception { super.setUp(); TigersLair activity=getActivity(); mGoBtn = (Button) activity.findViewById(R.id.go); mGoBtn.performClick(); } It doesn't matter if I perform the click in setUp() or the actual test. You need to execute all clicks in the UIThread. This can be done by the following two examples. @UiThreadTest public void testApp() { TestApp activity = getActivity(); Button mGoBtn = (Button) activity.findViewById(R.id.testbutton); mGoBtn.performClick(); } or public void testApp2() throws Throwable { TestApp activity = getActivity(); final Button mGoBtn = (Button) activity.findViewById(R.id.testbutton); runTestOnUiThread(new Runnable() { @Override public void run() { mGoBtn.performClick(); } }); }
15,A,"writing test case for a DAO on a J2ee Application I am trying to write some test cases for my DAO classes in a J2EE applications. Methods in my DAO classes try to get connection to the Database based on a JDBC URL (which is on the app server). So from the front end if I click bunch of stuff and make the DAO trigger it runs fine. However when I write tests cases for the DAO and the DAO object calls the method then it is not able to get the connection to the database. I think since the JDBC resource is on the App server that is why it is not working from the test class. because of this when I run my tests instead of pass or fail..it returns bunch of errors. Has someone encountered this issue? what can I do to overcome this? Example: public class DBConnectionManager { public static final String DB_URL = ""jdbc/RSRC/my/connection/mydb"" public Connection getconnection () { DataSource ds = ServiceLocator.getInstance().getDataSource(DB_URL); return ds.getconnection(); } } public class MyDAO extends DBConnectionManager { publci SomeBean getContents (String id) { Connection con = getConnection(); CallableStatement cs = con.prepareCall(""{call myStorProc(?)}""); cs.setString(1 id); ... //code to call resultset and retrieve SomeBean goes here .. return SomeBean; } } public class MyTests extends TestCase { public testGetcontents () { MyDAO myd = new MyDAO (); SomeBean smb = myd.getContents(""someparm""); assertEquals (5 smb.getSomeVal()); } } Should I be doing something extra in my testcase...? if so what? EDIT: error I get is: java.lang.NoClassDefFoundError: com/iplanet/ias/admin/common/ASException at java.lang.ClassLoader.defineClass1(Native Method) Your DAO has a JNDI lookup string hard wired into it. Unless you have a JNDI lookup service available it won't be able to get a connection. I don't think a DAO should be responsible for acquiring a database connection. This design won't allow you to set transactions for a unit of work because a DAO can't know if it's part of a larger unit of work. I'd recommend passing the connection into the DAO perhaps into its constructor. That way a service layer can establish appropriate transaction boundaries if there's more than one DAO in a single unit of work. This design will have the added benefit of making it possible for your application to use its JNDI resource appropriately and your test to get its connection from a DriverManager without having to use a JNDI lookup. You have two different sources for acquiring the DataSource or Connection - one for the app and another for the test. UPDATE: Here's what I mean expressed in your code: public class DBConnectionManager { public static final String DB_URL = ""jdbc/RSRC/my/connection/mydb"" public Connection getConnection (String jndiLookup) { DataSource ds = ServiceLocator.getInstance().getDataSource(jndiLookup); return ds.getconnection(); } public Connection getConnection(String driver String url String username String password) throws ClassNotFoundException SQLException { Class.forName(driver); return DriverManager.getConnection(url username password); } } public class MyDAO { private Connection connection; public MyDao(Connection connection) { this.connection = connection; } public SomeBean getContents (String id) { CallableStatement cs = this.connection.prepareCall(""{call myStorProc(?)}""); this.connection.setString(1 id); //code to call resultset and retrieve SomeBean goes here return someBean; } } You show nothing about closing resources properly or transactions. Judging by this code you'll be in trouble on both counts. I'd think carefully about your implementation. I'll recommend Spring JDBC to you. You can write your DAOs in Spring without rewriting your whole app. I'll also point out that you might also be looking at generics: Don't Repeat The DAO. can believe I overlooked that. so based on your example it would work like DBConnectionManager dbm = new DBConnectionManager(); MyDAO myd = new MyDAO (dbm.getConnection(""some/jndi"")); That's one way. Another is MyDAO myDao = new MyDAO(dbm.getConnection(""driver"" ""url"" ""username"" ""password""); See how your test class can work now? yup definitely notice that now. since you know my history and what I'm trying to achieve (compare results of two DAO's one old {in this question} with one new). Could you please see if you can give me some help on this question: http://stackoverflow.com/questions/1711566/is-this-possible-to-do-with-ibatis-spring based on the code you provided...connection variable inside MyDAO class would never get populated (unless i'm reading it wrong?). Lets say I initiate MyDAO myd = new MyDAO(); then where is the call to getConnection (in DBConnectionManager) gets made? Yes you are reading it wrong. There IS no default constructor only one that takes a Connection passed in as parameter. You cannot create a DAO without a Connection the way I wrote it. That's the way Java works - you only get a default constructor from the compiler if you don't write a constructor. thanks I like the design where connection is passed to the DAO. But then lets say you have some DAO helper class which actually calls the DAO...will that helper class pass connection to the DAO? I dont think right now I can change the design...guess I will have to drop the awesome testing ideas >_<. Using your design though when i made object of the DAO...in the constructor I could pass connection acquired from DriverManager... can you suggest a way so that getContents() method in my DAO becomes aware whether it got called from the JUnit method. If it is...then it will take connection from driver manager... No your DAO should not know whether it's being tested or used in the app. Pass in the connection from somewhere else.  Where I work our DAOs have an injectable connection (via constructor injection) and we unit test against a mock connection. To test the code in the DAO we pass in a mocked (usually using Mockito) connection and set up expectations in our unit tests as to what methods will be called. This makes for somewhat noisy tests as the tests look very similar to the code being developed but it works for us. What's the point of a mock connection? If you're testing a DAO why would you not want to connect to a real database? This is one use of mocks that has never made sense to me. Once it's working then mock the DAO for its clients.  It could be a permissions issue on the database you're trying to access. What errors are you getting? One useful way for testing database access is to create a clean local ""test"" version of your database as part of your test harness. Before you run your tests use scripts to create a local copy of the database with all the pertinent data then run your tests against that rather than the remote server. People may argue that testing against a database in a unit test is not truly a unit test since it has an external dependency. If you're able to refactor your DAO classes you can make it so the actual data source is injectable through some interfaces. In your test code you'd inject a ""mock"" data source which provides your test data in some in memory format then in production you'd use/inject the actual database source classes. If you can hide the external (non-business code related) parts of your DAO behind interfaces you can use mocks in your unit tests to test more of your functionality rather than the actual data access. actually point of me writing this test case is other than just testing for data access. I asked this question a week back. this might tell you what i'm really trying to do http://stackoverflow.com/questions/1690401/need-suggestions-on-getting-started-with-junit. Furthermore the errors I am getting are like '../logs/myapp.log' the system cannot find the path specified. All these errors are coming because i am not coming directly from web tier. is there a way to 'mock' the test such a way so it seems like they are coming from web tier... looks like it IS security problems. java.lang.NoClassDefFoundError: com/iplanet/ias/admin/common/ASException  Test well your ServiceLocator first. As you mentioned the problem is probably because the datasource is declared on the server. Here the ""bunch of errors"" should be helpful as of whether the problem is in acquiring the DataSource or the Connectiion itself. What database are you using? Can you logon to it from your machine from console? If not - configure it so that your host is allowed."
16,A,What is the correct terminology & best practice for common fixtures between test classes? I understand in Junit 4 @Before can be used to setup test fixtures for the test class's multiple test methods. However in my system there are common test objects i would like to have available for all tests. What is the most appropriate name for these objects and what is a good best practice way to store them? Have a separate project(I guess package in your case) for fakes/stubs. Let them be static There is definitely a use case for having some shared data between test cases. JUnit provides the @BeforeClass annotation for this. Should help you I hope... http://junit.sourceforge.net/javadoc/org/junit/BeforeClass.html Note that any BeforeClass methods have to be static. This may not be an issue but it may cause you to have a mix of static and non-static members in the class. Not a huge deal but maybe not the cleanest code.  The preferred names for your common test objects will depend on what these objects really are (I can't help unless you provide more detail). If you want to share an object between all the tests then it must be static in your test class (JUnit will recreate the test class before every test).  The best practice is to create them in the fixture so as to keep the tests isolated unless their state is not changed by the tests (such as a logger). Otherwise one may have side-effects between the tests: one test failing because of another one or the opposite.  Why not have an abstract BaseTest superclass that holds the common objects and initializes them either in the constructor or a @BeforeClass method?
17,A,"JUnit Theories: Why can't I use Lists (instead of arrays) as DataPoints? I've started using the new(ish) JUnit Theories feature for parameterizing tests. If your Theory is set up to take for example an Integer argument the Theories test runner picks up any Integers marked with @DataPoint: @DataPoint public static Integer number = 0; as well as any Integers in arrays: @DataPoints public static Integer[] numbers = {1 2 3}; or even methods that return arrays like: @DataPoints public static Integer[] moreNumbers() { return new Integer[] {4 5 6}; }; but not in Lists. The following does not work: @DataPoints public static List<Integer> numberList = Arrays.asList(7 8 9); Edit: It looks like other collections are not supported either as this does not work. @DataPoints public static Collection<Integer> numberList = new HashSet<Integer>() {{ add(7); add(8); add(9); }}; Am I doing something wrong or do Lists Sets etc. really not work? Was it a conscious design choice not to allow the use of Collections as data points or is that just a feature that hasn't been implemented yet? Are there plans to implement it in a future version of JUnit? (I'm currently using version 4.8.1 whereas the newest version is 4.8.2 but it looks like this is not something that was added in 4.8.2) +1 Seems quite odd. Did you try any other Collection types? Just tried using a `Set` and updated the question. That doesn't work either whether you declare it as `Set` or `Collection`. I've added this as an issue: http://github.com/KentBeck/junit/issues/issue/110 I'll keep an eye on that issue and see if anyone posts a comment with some more information about this. As a side-note I don't see a tag for this ""Theories"" feature. I assume the [`theory`] tag is for theoretical questions about things like big-O complexity and such. Please feel free to retag. I've looked at the issue and it seems there is now a pending commit for it. The reason that it wasn't in there seems to be simply that nobody asked for it and it's quite complex to do (as you've proven in your patch) Thanks for the follow-up. I should have added a comment to this SO answer saying that I did manage to figure out a solution (though it's certainly not perfect) and then sent a pull request on github. I was not sure if putting a comment in 3 years old post is a good thing. But I just wish to know whats the update on this issue. This issue is still not resolved in latest Mockito version. Also link to issue mentioend above does not exists anymore."
18,A,"Junit GWT error - Cannot find function attachEvent in object [object Window] I try to create junit test for my GWT application. The application uses MathJax javascript library. I encounter an error that seems to be connected with javascript code. The listing of the full error message is here. I think that the core (main cause) of the error could be the following: TypeError: Cannot find function attachEvent in object [object Window]. (http://192.168.1.10:50987/com.qtitools.player.Player.JUnit/MathJax/MathJax.js#987) (http://192.168.1.10:50987/com.qtitools.player.Player.JUnit/com.qtitools.player.Player.JUnit.nocache.js#530) What could be the reason for not finding the attachEvent function? Thanks for any hints on how to fix this issue. The problem is the HtmlUnit library does not support attachEvent function properly while running test in FF emulator. The solution is to run tests only in IE7 emulator. To achieve that open in eclipse add -Dgwt.args=""-runStyle HtmlUnit:IE7"" to VM arguments. I actually get this error when I include CHROME or CHROME_16. It runs perfectly fine with FIREFOX_10 and FIREFOX_17 whatsoever."
19,A,Running a JUnit test from Groovy Console How can I use the Groovy Console to kick off junit tests? (Currently using Groovy 1.6.0) I currently use: junit.textui.TestRunner.run(MyTest) (Where my junit test class is MyTest) I've tried running: MyTest within the Groovy Console but that just returns the class instance. Is there an easier way? I've done it this way too; combined with log4j config to dump out a junit run @saua - I was hoping to be able to just highlight the classname and run that What's wrong with the way you currently use?
20,A,"The ""is"" in JUnit 4 assertions Is there any semantic difference between writing assertThat(object1 is(equalTo(object2))); and writing assertThat(object1 equalTo(object2))); ? If not I would prefer the first version because it reads better. Are there any other considerations here? They are equivalent as far as I'm aware. The ""Is"" matcher just passes through to the contained matcher. It seems it's there to add readability and perhaps backwards compatibility.  Documentation says it all: Decorates another Matcher retaining the behavior but allowing tests to be slightly more expressive. eg. assertThat(cheese equalTo(smelly)) vs assertThat(cheese is(equalTo(smelly))) http://www.junit.org/apidocs/org/hamcrest/core/Is.html In other words you're on the right track. Facepalm - I shoulda read that."
21,A,"Logging in grails integration test? I'm using grails 1.3.6 and no matter what I do I can't get log.* statements output to the console. Test: class MyObjectTests extends GroovyTestCase { MyObjectTests myObjectTests public void testSomething(){ myObjectTests.something() } } Class: class MyObject { void something(){ log.info( ""Hello."" ) } } I've tried using GrailsUnitTestCase and mockLogging but that doesn't work either. Only println sends anything to the console. What gives? This is http://jira.codehaus.org/browse/GRAILS-6858 and it's fixed in 1.3.7. You should upgrade if possible but there are workarounds in the JIRA comments. :/. I can't upgrade to 1.3.7 due to another regression. That's unfortunate but since it's testing-related one of the workarounds in the comments will work since it doesn't involve anything that needs to end up in the deployed war."
22,A,Eclipse - BEA Workshop issues Help! My eclipse workshop keep terminating on me while in the middle of test run or just navigation. Below is the error property i keep getting. Can anyone help stop this nightmare. Thanks ===== JVM terminated. Exit code=1 C:/bea/jdk150_11/jre/bin/javaw.exe -Xms384m -Xmx768m -XX:MaxPermSize=256m -Dweblogic.home=C:/bea/wlserver_10.0 -Dosgi.install.area=C:\bea\tools\eclipse32\eclipse -Dosgi.instance.area.default=C:/bea/user_projects/w4WP_workspaces/Untitled -Dosgi.configuration.area=C:/bea/workshop_10.0/workshop4WP/eclipse/configuration -Declipse.product=com.bea.workshop.product.wl.workshop -Dosgi.splashPath=file: C:/bea/workshop_10.0/workshop4WP/eclipse/plugins/com.bea.workshop.product.wl_1.0.0 -DprodMode=preProduction -jar C:\bea\workshop_10.0\workshop4WP\startup.jar -0s win32 -ws win32 -arch x86 -launcher C:\bea\workshop_10.0\workshop4WP\workshop4WP.exe -name Workshop4WP -showsplash 600 -exitdata 154_64 -vm C:/bea/jdk150_11/jre/bin/javaw.exe -vmargs -Xms384m -Xmx768m -XX:MaxPermSize=256m -Dweblogic.home=C:/bea/wlserver_10.0 -Dosgi.install.area=C:\bea\tools\eclipse32\eclipse -Dosgi.instance.area.default=C:/bea/user_projects/w4WP_workspaces/Untitled -Dosgi.configuration.area=C:/bea/workshop_10.0/workshop4WP/eclipse/configuration -Declipse.product=com.bea.workshop.product.wl.workshop -Dosgi.splashPath=file: C:/bea/workshop_10.0/workshop4WP/eclipse/plugins/com.bea.workshop.product.wl_1.0.0 -DprodMode=preProduction -jar C:\bea\workshop_10.0\workshop4WP\startup.jars The question is answered at oracle http://forums.oracle.com/forums/thread.jspa?messageID=4467747#4467747 Change the JVM setting it looks like JVM is getting crashed out here. you can use default Jrockit jre for this or and change the heap size for min and max as 1024m. if you want to use sun JDK then change the JVM setting as Max heap as 1024m and min Heap size as 1024 m permisze as 256 M. This will resolve your problem. the ini file can be found: %BEA_Home%/ workSpaceStudio_1.1\workSpaceStudio\ There is you will see workSpaceStudio.ini you can change that one. if it is not a workspace stuido then you will find your ini file under following directory. %Bea_HOme% / Worksop10 / \workshop4WP/ There you will get the .ini file.  I wonder if this can help you. If memory is your problem maybe you can attach the jvisualvm tool that ships with the JDK so you can see what's going on in memory as the server runs. Start your server and note its PID. Go to your Java /bin find jvisualvm.exe and start it up. You'll have to give jvisualvm the PID of the server process so it can attach to it. You'll get a nice visual of memory as it's running. Maybe that will tell you something about what happens when it crashes. that didnt help me much. My problem is not that i cant open eclipse. My problem is that eclipse terminates on intermittent basis. Sometimes when am just navigating other times in the middle of a junit test run. I am thinking its a memory allocation problem but dont know how to go about solving it.
23,A,"Method naming in TDD or BDD when doing the simplest thing that can possibly work For example I have to create Vector class that can return vectors length. First I add test: @Test public void shouldReturnLengthOfVector() { Vector3D vector = new Vector3D(4d 2d -4d); assertThat(vector.length() is(6d)); } While writing test create class Vector3D and add method stub. public double length() { // TODO Auto-generated method stub return 0; } Test do not pass. What is simples thing to pass the test? Hard coded value: public double length() { return 6d; } Test passes. Now I add some method that checks ""cornercase"": @Test public void someCornercaseShouldReturnLengthOfVector() { Vector3D vector = new Vector3D(1d -2d -2d); assertThat(vector.length() is(3d)); } Of-course that does not pass. I change my implementation: public double length() { return Math.sqrt(i * i + j * j + k * k); } Everything is green! How to pick names for methods when I follow ""simplest thing that can possibly work"" principle? In this example method someCornercaseShouldReturnLengthOfVector and that is't good name. What's the point of the step where you hard code a value to return in `length()`? You are just going to replace this in a few steps later on. Why make tests pass just for the sake of passing? @matt b I am following 3 laws of TDD. Expectation in test is still valid and gives me a starting point - after it is green it should stay so or something is wrong. I follow the following pattern when naming my methods: WhatImTesting_WhatAreTheImputs_ExpectedResult Just because it is a simple and clear case to you doesn't mean it is to everyone else. Although my method naming can get wordy it is normally very clear what they are testing. To use your example I would name my method something like Length_ValidConstructorImputs_CorrectLength() this way just by reading the method name I know that we are testing the length method with valid inputs (from the constructor) and the correct length should be returned. For another example suppose I wanted to test the following constructor: public class House { Door theDoor; public House(Door aDoor) { if(aDoor == null) { throw new IllegalArgumentException(); } theDoor = aDoor; } } I would probably have two methods to test this: Constructor_ValidDoor_ObjectCreated(): This would check that the House object is created when the constructor parameter is valid. Constructor_NullDoor_ExceptionThrown(): This would check that an exception is thrown when a null parameter is passed into the constructor. I agree. The method names can be long and a bit wordy. I'm ok with this in tests though since I want to be able to read the method name and know exactly what is tested and if one fails I've got a pretty good idea of where to start looking. It is tricky because if I have negative values method name gets really ugly.  You can refactor your test code too. someCornercaseShouldReturnLengthOfVector is a perfectly valid name while you're at the ""simplest thing that can possibly work"" phase. Refactor the name to reflect what the code actually tests once you're further along in your testing. You can also comment your test code to make its intent more clear. ;) It is. So I am struggling with coming up with some good name. @brainimus answer was my first though but it is not working well enough if input is complex. ""... the only truly good comment is the comment you found a way not to write."" -- Rober. C. Martin. So I would like to avoid them. After more functionality add this method still would be valid so refactoring would affect only its name. @janis: Isn't its name exactly what your question is about?  There is no unit test for; Is the meaning of this method clear. You have to use your judgement. The simplest name which would work is just l ;) You should also think about clarity when determining what you can work with. i.e. you have to consider the developer(s) as well as the what the compiler will let you get away with.  I think in the case here I would either have followed Anon's advice and put a second assertion into the same test neatly dodging the naming question or if religious about the ""one assertion per test"" rule have named the next test shouldReturnLengthOfAnotherVector.  The simplest thing that could possibly work is to add another assertion to shouldReturnLengthOfVector: @Test public void shouldReturnLengthOfVector() { Vector3D v1= new Vector3D(4d 2d -4d); assertThat(v1.length() is(6d)); Vector3D v2 = new Vector3D(1d -2d -2d); assertThat(v2.length() is(3d)); } I realize that some people believe that each test should have a single assertion. However I think it is more useful as documentation of behavior if you give several examples of ""normal"" operation and reserve separate test cases for true corner cases (eg passing NaN). Another approach is to be more specific with your test naming. In this case you wouldn't create a test ""should return length of vector."" Instead you'd create a bunch of methods like ""identical values are zero length"" ""vectors that have length 1"" (which could have lots of assertions varying the different parameters) ""vectors with negative values"" and so on. Enlightenment! I have got several assumptions wrong: 1) I wasn't doing simplest thing (it would be Vector3D(000)) 2) example doesn't illustrate the problem because there is no real corner case 3) my test method naming was to broad - it must be more specific to illustrate behavior being tested. I tend to agree that doggedly following the ""one assertion per test"" rule can result in some ugly and unwieldy test suites."
24,A,"How can I use jUnit to test a class that uses a singleton? I am developing a simulator for a machine. It reads a file in stores it in a string (as a binary number) to an array that is supposed to simulate the memory of a computer. Then it gets passed to an interpreter that takes gets the first element of the array and based on the first 4 characters it gets sent to another class. An example of one of those classes is below: public class Add { private String cell; Add() { cell = null; } /** * Method that performs the operation add. */ public void addition() { // TODO Figure out 2's complement. Loader instance = Loader.Instance(); cell = instance.getCell(); Integer DR1num = Integer.parseInt(cell.substring(4 6) 2); Integer SR1num = Integer.parseInt(cell.substring(7 9) 2); if (cell.charAt(10) == '0') { Integer SR2num = Integer.parseInt(cell.substring(13 15) 2); Integer addedval = instance.getRegister(SR1num) + instance.getRegister(SR2num); instance.setCCR(addedval); instance.setRegister(DR1num addedval); } else if (cell.charAt(10) == '1') { Integer immediate = Integer.parseInt(cell.substring(11 15) 2); Integer addedval = instance.getRegister(SR1num) + immediate; instance.setCCR(addedval); instance.setRegister(DR1num addedval); } else { System.err.println(""Invalid Char at Position 10. Expected 0 or 1""); } instance.incrementPC(); } public String getcell() { return cell; } } Variable Descriptions: instance is the singleton cell is the binary string that is stored in the memory array DR1num is Destination Register. (aka R0 R1 ... R7) SR1num is Source Register What this operation does is takes either two Source Registers and adds them together (then sets the Condition Code Register if it is positive negative or zero) or takes a Source Register and an immediate value and adds those two together. The result is stored in the Destination Register. I chose to use a singleton class because I need the Memory General Purpose Registers Condition Code Register and Program Counter to be in sync throughout the entire program. The problem that this presents is that I have no idea how to test my program (preferably with jUnit). The only way I can think of testing the program is by writing a bunch of individual files for the program to read in. Except this means that instead of testing only one part of my program at a minimum it tests three classes. The class that reads in the program and contains all the methods for the singleton the class that determines which class to call and finally the class that actually executes that portion (such as Add). Does anyone have any suggestions for a better way to test? I think you can test it if the loader is an interface and then by using mocking frameowork like EasyMock you can insert the mocked class instance instead of the actual instance. This will work if the Loader is an interface  The way to avoid this is traditionally to use inversion of control (IoC) and inject an instance of your singleton class into the client class. Your client class is provided with all the dependent classes it requires to function and not concerned with the lifecycle of the dependent classes. Spring is the common Java framework used to do this but you can inject simply by constructing your class with a reference to the singleton (it's that simple). If the singleton class implements an interface then testing is made easier since you can inject mock variants rather than the real implementation and build your mock to provide suitable test data for your tests. @Matt - nice links. Thx Obligatory: ""Singletons are pathological liars"" http://googletesting.blogspot.com/2008/08/by-miko-hevery-so-you-join-new-project.html and ""Where have all the Singletons gone?"" http://misko.hevery.com/2008/08/21/where-have-all-the-singletons-gone/ Guice is also a great DI library and focused on DI only."
25,A,"Spring context tests can't find config locations I have a large application spread across multiple Spring bean definition xml files. In my test suite I manually load up the XML files I need using a FileSystemXmlApplicationContext to perform the tests I want to run. This reduces test set up time and allows me to use the same exact configuration files that are used in production. Now I'm trying to use Spring's transactional test base classes which take the config locations and load the context for me. For some reason when the application context is created Spring cannot find any of the config files. This is confusing because I run the test from the same working directory as when I load the config myself using FileSystemXmlApplicationContext. If I prepend all my config locations with ""file:"" the paths I specify in my test are found but any files that are imported or referenced by beans defined in the config (e.g. properties files) cannot be found. What's the deal? Can I get tests that extend the spring context test classes to work the same as the ones where I create the context myself? For example creating the context like this works fine: ApplicationContext ctx = new FileSystemXmlApplicationContext(new String[] { ""WEB-INF/services-context.xml""}) If I extend AbstractTransactionalDataSourceSpringContextTests the following does not find services-context.xml: @Override protected String[] getConfigLocations() { return new String[] { ""WEB-INF/services-context.xml""}; } This finds services-context but the PropertyPlaceholderConfigurer defined in there fails to find it's properties files.  @Override protected String[] getConfigLocations() { return new String[] { ""file:WEB-INF/services-context.xml""}; } Adam I have explained the solution in http://www.skill-guru.com/blog/2009/12/16/aspect-oriented-programmingaop-with-spring/#comments  Another possible solution is to duplicate the services-config.xml and rename as services-config-test.xml and then put under classpath. The same thing goes for properties file.  Can't you use classpath XML factories like ClassPathXmlApplicationContext? thx - just what I needed!  We put all of our Spring config and properties files in the classpath which keeps things simple - we can just extend our test classes from a base class like: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={ ""/spring/*.xml"" ""/testSpring/*.xml"" }) public abstract class AbstractIntegrationTest { Here the paths are all paths in classpath. If you don't want to do that have you checked how you are referencing the properties files in your services-context.xml? I suspect that if you add file: to your context configuration then you'll also need to add this to your property file reference. You could perhaps just use a separate test Spring config file to change the definition of your property placeholder and place this at the end of your list of context files - its definitions will then override those defined in earlier files.  In addition to overriding getConfigLocations I also overrode loadContext and used a trusty fileSystemXmlApplicationContext in there.  @Override protected String[] getConfigLocations() { return new String[] { ""WEB-INF/services-config.xml"" }; } @Override protected ConfigurableApplicationContext loadContext(String[] locations) throws Exception { return new FileSystemXmlApplicationContext(locations); }  ApplicationContext ctx = new FileSystemXmlApplicationContext(new String[] { ""WebRoot/WEB-INF/services-context.xml""})  Your config locations are relative URIs and will be interpreted as such by the base test class with the URI being resolved relative to the location of the test class itself. Try using fully qualified URIs or use relative URI taking into account where the test class is. Prepending with file: forces the use of qualified uri's but then that causes the problem where files referenced in the config file cannot be found."
26,A,"JUnit test cases- setting up data I have a simple CRUD operations that needs to be unit tested These test cases are for DAO layer- so all tests are against the database and hence cant be mocked. So I have one test cases for create another for update and another for read. 1) Should I hard code the data in the JUnit class or do you externalize it ? 2) The Read TestCase would obviously need data in the database. Should I depend upon the Create Test cases to set up the data or use a SQL statement for that ? what is the best-practice on this? If you can point me to an internet resource which discusses this- that would be great. Thanks !!! I'd recommend using DBUnit. Allows you to setup a database to a known state using files and has nice comparison with expected results functionality. Getting started guide is here. It's also good practice not perform any commits as part of the testcase and then you can rollback on tearDown().  Spring has excellent support for this sort of thing - unit tests that you want to operate against a ""test"" database which can be scripted to be re-created on each individual unit test. The latter part of that last sentence is the key to developing re-usable and extensible unit tests - a unit test against a database should not be forced to rely on the data optimistically being in a certain state or rely on a previous unit test to run first - you'll want to re-create the database for each unit test so that each test case gets a ""clean"" version of the data. The step-by-step tutorial on setting up Spring MVC actually has a section on setting up unit tests for database classes which I think would be a valuable reference even when you are not using Spring MVC - you can use this as a reference on how to set up the test database to be created/initialized from the build script using the Spring container to re-load the data on each test run etc."
27,A,"log4j vs. System.out.println - logger advantages? I'm using log4j for the first time in a project. A fellow programmer told me that using System.out.println is considered a bad style and that log4j is something like standard for logging matters nowadays. We do lots of JUnit testing - System.out stuff turns out to be harder to test. Therefore I began utilizing log4j for a Console controller class that's just handling command-line parameters. // log4j logger config org.apache.log4j.BasicConfigurator.configure(); Logger logger = LoggerFactory.getLogger(Console.class); Category cat = Category.getRoot(); Seems to work: logger.debug(""String""); Produces: 1 [main] DEBUG project.prototype.controller.Console - String I got two questions regarding this: From my basic understanding using this logger should provide me comfortable options to write a logfile with timestamps - instead of spamming the console - if debug mode is enabled at the logger? Why is System.out.println harder to test? I searched stackoverflow and found a testing recipe. So I wonder what kind of advantage I really get by using log4j. Anything that you print to System.out will go to ""standard out"" and while you can redirect standard out to a file and compare it what have you that is very inflexible. Additionally you cannot filter what goes to standard out if you use System.out... everything will be printed. With log4j you can set different logging levels so that logging messages that are below a certain severity/importance threshold are not printed (e.g. if you change the logging level to WARN then DEBUG and INFO messages will not be displayed anymore). Additionally log4j allows logging to be controlled on a class-by-class basis whereas System.out can only be controlled at the granularity of the entire application (if you redirect System.out you redirect it for the entire program). By contrast each logger in log4j can be given a different appender. In addition you can give a log4j logger multiple appenders (so that it goes the system logger and over the network for example). You can even have a log4j logger append to a StringBuilder so that you can easily read what was written. And while System.out can be redirected this redirection tends to be fairly limited; System.out can be redirected to a file or to a pipe (to another program) but you wouldn't be able to redirect it to a URL for example; by contrast it would be very easy to create an appender that transmits logging messages using HTTP POST.  The logger gives to ability to define different levels of importance of the logged messages and the ability to use different sink for the output - the console a file etc. Also it's easy to enable or disable only some type of message when using a logger - for example you don't want to see every debug message in production. I don't think that using loggers offers any significant advantages in unit tests but I'd prefer it even there anyways. In unit tests asserts are usually my primary concern. Btw you should really consider using something like Commons Logging or SLF4J as a log framework facade - it's bad style to tie your code to a specific logging framework. Common Logging and SLF4J make it easy to switch logging frameworks if you choose to. +1 excellent advice with regards to Commons Logging  Use e.g. org.apache.log4j.BasicConfigurator.configure(new FileAppender( new PatternLayout(""%d{ISO8601} %-5p %t: %m%n"") // see e.g. http://en.wikipedia.org/wiki/Log4j#TTCC ""log/mainWhatever.log"")); Using logger.setLevel(...) you can easily choose whether to display logger.debug(..) messages e.g. set it to level warn and any trace debug and info statements will not be printed. This saves you the time of having to comment out only occasionally needed debug statements. Also have a look at Wikipedia."
28,A,Logging level under maven surefire I'm unable to adjust java logging's logging level. I'm using maven surefire (mvn test) and trying to adjust from the default INFO to e.g. FINEST. I have logging.properties file under src/test/resources/logging.properties after compile i see under target/test-classes i see a logging.properties file with the intended config: java.util.logging.ConsoleHandler.level=FINEST javax.enterprise.system.container.ejb.level=FINE ... however the console output from glassfish only have INFO / SEVERE level messages. Where did I go wrong? or is this another pain in the butt thing with maven? i'm debugging a unit test that uses the Embedded container in glassfish v3. according to: http://forums.java.net/jive/thread.jspa?messageID=395759 under reply by 'Marina Vatkina' thats the key to output FINE messages for the EJB container... I guess my question is is my setup correct for java util logging? Or is there extra configuration needed? - not just for this particular instance but lets say I have a class com.something.Main and i want to output FINEST messages for it... I'm a bit confused here. How does glassfish fit into this? I think it's likely you are getting confused between glassfish's logging and the maven unit test logging. What framework is the maven build using for logging? You need to specifiy your handlers in the logging file handlers = java.util.logging.ConsoleHandler java.util.logging.FileHandler then it should work awesome its working now thank you. How did I miss that?????  I was looking at this exact issue but did not want a project configuration (pom.xml) file change for every time I need specific logging on a test. The -D property works from maven command line. Thus you can select the logging configuration file from the command line: mvn -Djava.util.logging.config.file=`enter filename here` test If you are using the generic level denominator .level=FINEST be aware that 3rd party logging will also appear at that level. To disable or set the maven or 3rd party logging to a specific level use explicit log level selection for those classes in the selected log configuration file. I have a lot of log lines from com.google.inject..... aug 08 2014 12:14:33 PM com.google.inject.internal.util.$Stopwatch resetAndLog FINE: Instance member validation: 3ms aug 08 2014 12:14:33 PM com.google.inject.internal.util.$Stopwatch resetAndLog FINE: Provider verification: 1ms aug 08 2014 12:14:33 PM com.google.inject.internal.util.$Stopwatch resetAndLog FINE: Static member injection: 1ms So I add: com.google.inject.level=INFO to the file. Remember that the level setting is recursive to all subclasses. Thus com.level=NONE will disable all logging for all loggers from the com domain. Combining this with the test select feature -Dtest=... in the surefire plugin described here is very good for isolating bugs and errors.  Looks like its not possible yet under maven?: http://jira.codehaus.org/browse/MNG-2570 Also when I setup the pom to use a custom java.util.logging properties file as follows: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.4.2</version> <configuration> <systemProperties> <property> <name>java.util.logging.config.file</name> <value>${project.build.directory}/test-classes/logging.properties</value> </property> </systemProperties> ... logging disappears...  try ${build.testOutputDirectory}/logging.properties Also I specify this stuff on the command line with surfire-args. <argLine>${surefire.argLine} ${argLine} -Djava.util.logging.config.file=${build.testOutputDirectory}/logging.properties</argLine>  I tried setting java.util.logging.config.file in the MAVEN_OPTS environment variable which does not work but finally got it working by putting that system property in the pom.xml (and of course creating an appropriate logging.properties in src/test/resources):  <plugins> <plugin> <artifactId>maven-surefire-plugin</artifactId> <configuration> <systemProperties> <property> <name>java.util.logging.config.file</name> <value>src/test/resources/logging.properties</value> </property> </systemProperties> </configuration> </plugin> </plugins>
29,A,"Proper way to autowire a Hibernate Session in a Spring Transaction JUnit test This question is similar to a previous one. I am trying to @Autowire a Hibernate Session in one of my Spring-JUnit-Transactional tests but I am getting this exception: java.lang.IllegalStateException: No Hibernate Session bound to thread and configuration does not allow creation of non-transactional ... Here is my JUnit class: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""/applicationContext.xml""}) @TransactionConfiguration(transactionManager=""transactionManager"") @Transactional public class MyTest { @Qualifier(""session"") @Autowired private Session session; @Test public void testSomething() { session.get(User.class ""me@here.com""); } } Every works fine if I @Autowire a SessionFactory and get my Session programmatically (instead of defining it in the Spring XML) like so: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""/applicationContext.xml""}) @TransactionConfiguration(transactionManager=""transactionManager"") @Transactional public class MyTest{ @Qualifier(""sessionFactory"") @Autowired private SessionFactory sessionFactory; @Test public void testSomething() { Session session = SessionFactoryUtils.getSession(sessionFactory false); session.get(User.class ""me@here.com""); } } I can however get my original example to work if I define my Session in my Spring XML with <aop:scoped-proxy /> like so: <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:tx=""http://www.springframework.org/schema/tx"" xmlns:aop=""http://www.springframework.org/schema/aop"" xsi:schemaLocation="" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd ""> <bean id=""dataSource"" class=""com.mchange.v2.c3p0.ComboPooledDataSource"" destroy-method=""close""> ... </bean> <bean id=""sessionFactory"" class=""org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean""> <property name=""dataSource"" ref=""dataSource"" /> <property name=""configLocation""><value>classpath:/hibernate.cfg.xml</value></property> <property name=""configurationClass""> <value>org.hibernate.cfg.AnnotationConfiguration</value> </property> </bean> <bean id=""transactionManager"" class=""org.springframework.orm.hibernate3.HibernateTransactionManager""> <property name=""sessionFactory"" ref=""sessionFactory""/> <property name=""dataSource"" ref=""dataSource"" /> </bean> <tx:annotation-driven transaction-manager=""transactionManager""/> <bean id=""session"" class=""org.springframework.orm.hibernate3.SessionFactoryUtils"" factory-method=""getSession"" scope=""prototype""> <constructor-arg ref=""sessionFactory"" /> <constructor-arg value=""false"" /> <!-- This is seems to be needed to get rid of the 'No Hibernate Session' error' --> <aop:scoped-proxy /> </bean> </beans> My question is: Why is <aop:scoped-proxy /> needed given that there should only one thread-bounded transaction context in my unit test? What is the proper way to define my Hibernate Session bean? I think the ""proper"" way is the injection of the SessionFactory and programmatically fetching the Session from it. The reason that you're getting the exception is down to the documented behaviour of SessionFactoryUtils.getSession(): Get a Hibernate Session for the given SessionFactory. Is aware of and will return any existing corresponding Session bound to the current thread for example when using HibernateTransactionManager. Will create a new Session otherwise if ""allowCreate"" is true. Since nothing has bound a session to the current transaction it fails. My suggestion would be to use HibernateTemplate - define one in your context and autowire that into your test. HibernateTemplate has most of the same operations as a war Session but does the session handling bit for you. You should just be able to do: hibernateTemplate.get(User.class ""me@here.com""); Thanks for the response. If I set ""allowCreate"" to true Spring appears to create a second non-transactional database session i.e. the @Transactional annotation does not rollback my changes during test. The problem with autowiring a HibernateTemplate is that I have DAO-level classes that depend on Session. I guess I could have them dependent on HibernateTemplate and then do a get(User.class ...) as you suggested. However I feel that I am violating the Law of Demeter given that the true dependency of DAO class is Session and NOT HibernateTemplate. Are your DAOs injected with a Session or with a SessionFactory? If you're injecting a Session you probably want to rethink that it's probably not a good idea. DAOs are injected with Session. Can you explain why that's not a good idea? Thanks. Because the Session is a short-lived object and you generally don't inject short-lived objects like that. Thanks for the reply again. Yes that is true but I have my Session objects scoped as ""request"" which Spring should terminate the Session at the end of each HTTP request.  SessionFactoryUtils.getSession() is as good as any other way of getting the Session. It does the same thing HibernateDaoSupport.getSession() would do. The reason you need scoped-proxy is because of timing. Without the scoped-proxy it seems that it is injecting the Session before the test begins and thus before the transaction begins and so you get the errors. By adding the scoped-proxy it proxies the Session and injects that so it does not inject the actual session upfront (before the transaction starts) but only fetches it and makes calls on it once the test is running when it actually needs to make a call against it."
30,A,"Custom JUnit Report? I am using the ant tasks 'junit' and 'junitreport' to run my JUnit Tests and generate a report at the end (=> ""Unit Test Results""). Is it there some easy way to extend this output somehow to get more information displayed in the report? For example to add an additional column which contains link to a screenshot taken by the test. I've seen that one could write an own ant junit test runner like the EclipseTestRunner but this is quite some effort. Is there no API to access the elements of a unit report? what kind of a screen shot are you looking for? you don't just want the stack trace of the error you're looking for console out or is it a graphical representation? Hi I don't want to know how to get a screenshot. I already achieved this. What I need to know is how can I extend the JUnit report so I can include a link to my screenshot. Thanks. Nothing new with latest jUnit versions? The junitreport task uses XSLT to produce the report from the XML files generated by the junittask. You can customize the output by specifying your own XSLT using the styledir attribute of the nested report element: <!-- use reportstyle/junit-frames.xsl to produce the report --> <report styledir=""reportstyle"" format=""frames"" todir=""testreport""/> For customizing the the output one option would be to make a copy of the default XSLT and modify that. Or you could look for an alternative XSLT which is more easy to customize for your purposes. For small changes it might be easiest to just import the default XSLT and override whatever templates you need to customize. For example to add a column for each test you would need to override the template which produces the table header and the template which produces a table row. Below I have just copied those templates and modified them a bit to add one column (look for two additions marked with <!-- ADDED -->). <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""> <!-- import the default stylesheet --> <xsl:import href=""jar:file:lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl""/> <!-- override the template producing the test table header --> <xsl:template name=""testcase.test.header""> <xsl:param name=""show.class"" select=""''""/> <tr valign=""top""> <xsl:if test=""boolean($show.class)""> <th>Class</th> </xsl:if> <th>Name</th> <th>Status</th> <th width=""80%"">Type</th> <th nowrap=""nowrap"">Time(s)</th> <!-- ADDED --> <th>Screenshot</th> </tr> </xsl:template> <!-- override the template producing a test table row --> <xsl:template match=""testcase"" mode=""print.test""> <xsl:param name=""show.class"" select=""''""/> <tr valign=""top""> <xsl:attribute name=""class""> <xsl:choose> <xsl:when test=""error"">Error</xsl:when> <xsl:when test=""failure"">Failure</xsl:when> <xsl:otherwise>TableRowColor</xsl:otherwise> </xsl:choose> </xsl:attribute> <xsl:variable name=""class.href""> <xsl:value-of select=""concat(translate(../@package'.''/') '/' ../@id '_' ../@name '.html')""/> </xsl:variable> <xsl:if test=""boolean($show.class)""> <td><a href=""{$class.href}""><xsl:value-of select=""../@name""/></a></td> </xsl:if> <td> <a name=""{@name}""/> <xsl:choose> <xsl:when test=""boolean($show.class)""> <a href=""{concat($class.href '#' @name)}""><xsl:value-of select=""@name""/></a> </xsl:when> <xsl:otherwise> <xsl:value-of select=""@name""/> </xsl:otherwise> </xsl:choose> </td> <xsl:choose> <xsl:when test=""failure""> <td>Failure</td> <td><xsl:apply-templates select=""failure""/></td> </xsl:when> <xsl:when test=""error""> <td>Error</td> <td><xsl:apply-templates select=""error""/></td> </xsl:when> <xsl:otherwise> <td>Success</td> <td></td> </xsl:otherwise> </xsl:choose> <td> <xsl:call-template name=""display-time""> <xsl:with-param name=""value"" select=""@time""/> </xsl:call-template> </td> <!-- ADDED --> <td> <a href=""link/to/screenshot/for/test/{@name}"">screenshot</a> </td> </tr> </xsl:template> </xsl:stylesheet> Here's how the result looks like: Hi I don't want to apply a different style to my report. I need to extend the report by additional (custom) information. So I have to be able to somehow modify the data of the report not its style. Thanks. Accessing the file system from XSLT is not really feasible without using extensions. About the only format that is accessible from XSLT 1 is (not surprisingly) XML. Three options that come to my mind at the moment ranging from the one requiring the least programming to the one requiring the most: ... ... (1) Put the screenshots into separate directories per test. Then instead of linking directly to the screenshot link to the directory. ... or (2) in your Ant build file generate an XML file listing the screenshots from the file system and access that XML file from XSLT using the `document()` function. Thanks for your proposition! That's a good approach as start. It's just that I have not only 1 single screenshot per test method but several screenshots depending on what is tested. So the screenshots are numbered and named e.g. TestClass-testMethod-01.png. Is there a way with XSLT to ""count"" the number of screenshots on the file system which ""belong"" to the test method?. ... or (3) Using the `classname` attribute of the `formatter` of the `junit` task specify your own formatter which includes the information about screenshots in the generated XML. Suppose that your unit tests output the names of the screenshots it creates. I guess it might be possible to extend the default `XMLJUnitResultFormatter` to inspect the output from each test and write out additional XML tags for each screenshot detected from the output. http://svn.apache.org/viewvc/ant/core/tags/ANT_171/src/main/org/apache/tools/ant/taskdefs/optional/junit/XMLJUnitResultFormatter.java?view=markup The source data for the report produced by the `junitreport` task comes from the XML files generated by the `junit` task. However based on that data you can add some some extra information to the report. -- For example if your unit tests produce screenshots in some known location with names that can be derived from the unit test names it is possible to create links to them by customizing the report style as in the example above. (I edited the answer to add a screenshot of the end result.) Thanks a lot @Jukka Matilainen..Your answer really helped me a lot...:)"
31,A,"How to best test Java code? I have been working on a comparatively large system on my own and it's my first time working on a large system(dealing with 200+ channels of information simultaneously). I know how to use Junit to test every method and how to test boundary conditions. But still for system test I need to test all the interfacing and probably so some stress test as well (maybe there are other things to do but I don't know what they are). I am totally new to the world of testing and please give me some suggestions or point me to some info on how a good code tester would do system testing. PS: 2 specific questions I have are: how to test private functions? how to testing interfaces and avoid side effects? Here are two web sites that might help: The first is a list of open source Java tools. Many of the tools are addons to JUnit that allow either easier testing or testing at a higher integration level. Depending on your system sometimes JUnit will work for system tests but the structure of the test can be different. As for private methods check this question (and the question it references). You cannot test interfaces (as there is no behavior) but you can create an abstract base test classes for testing that implementations of an interface follow its contract. EDIT: Also if you don't already have unit tests check out Working Effectivly with Legacy Code; it is a must for testing code that is not set up well for testing. I ordered the book just now. Thanks. +1 for the Feathers book reference.  Private functions will be tested when the public functions that call them. Your testing of the public function only cares that the result returned is correct. When dealing with API (to other packages or URLS or even to file/network/database) you should mock them. A good unit test should run in a few milliseconds not in seconds. Mocking is the only way to do that. It means that bugs between packages can be dealt with a lot easier than logical bugs at the functional level. For Java easymock is a very good mocking framework.  Mocking is a good way to be able to simulate system tests in unit testing; by replacing (mocking) the resources upon which the other component depends you can perform unit testing in a ""system-like"" environment without needing to have the entire system constructed to do it. As to your specific questions: generally you shouldn't be using unit testing to test private functions; if they're private they're private to the class. If you need to test something test a public method which uses that private method to do something. Avoiding side effects that can be potentially problematic is best done using either a complete test environment (which can easily be wiped back to a ""virgin"" state) or using mocking as described above. And testing interfaces is done by well testing the interface methods.  The lists of tools given before are useful. From personal experience these are the tools I find useful: Mocking - Mockito is an excellent implementation and has clever techniques to ensure you only have to mock the methods you really care about. Database testing - DBunit is indespensible for setting up test data and verifying database interactions. Stress testing - Jmeter - once you see passed the slightly clunky gui this is a very robust tool for setting up scenarios and running stress tests. As for general approach start by trying to get tests running for the usual ""happy paths"" through your application these can form a basis for regression testing and performance testing. Once this is complete you can start looking at edge cases and error scenarios. Although this level of testing should be secondary to good unit testing. Good luck!  Firstly if you already have a large system that doesn't have any unit tests and you're planning on adding some then allow me to offer some general advice. From maintaining the system and working with it you'll probably already know the areas of the system which tend to be buggiest which tend to change often and which tend not to change very much. If you don't you can always look through the source control logs (you are using source control right?) to find out where most of the bug fixes and changes are concentrated. Focus your testing efforts on these classes and methods. There's a general rule called the 80/20 rule which is applicable to a whole range of things this being one of them. It says that roughly on average you should be able to cover 80 percent of the offending cases by doing just 20% of the work. That is by writing tests for just 20% of the code you can probably catch 80% of the bugs and regressions. That's because most of the fragile code commonly changed code and worst offending code makes up just 20% of the codebase. In fact it may be even less. You should use junit to do this and you should use something like JMock or some other mocking library to ensure you're testing in isolation. For system testing/integration testing that is testing things while they're working together I can recommend FitNesse. I've had good experience with it in the past. It allows you to write your test in a web browser using simple table-like layouts where you can easily define your inputs and expected outputs. All you have to do is write a small backing class called a Fixture which handles the creation of the components.  You may have a look on this list : http://stackoverflow.com/questions/1105620/tools-for-regression-testing-test-automation-of-database-centric-java-applicati for a list of interesting tools. As you seem to already use Junit extensively it means that you're already ""test infected"" that is a good point... In my personal experience the most difficult thing to manage is data. I mean controlling very acutely the data agaisnt which the tests are runned."
32,A,"JUnit parallel to Rails Fixtures? My team has a set of POJO's that we build up to pass into our code and test various functions. For example we have an Address class which contains address information. It doesn't make sense to constantly rebuild this class every time we have to whack an address onto an object to test something. I am thinking that something like Rails' fixtures would be good but simply having some sane package and class in the test tree to store all these would be nice. Any ideas? Does JUnit have any built in tools to help with this? I found the xUnit Patterns book very helpful for this kind of stuff. You might find the Creation Method pattern useful. Set up the test fixture by calling methods that hide the mechanics of building ready-to-use objects behind Intent Revealing Names. Follow the link for lots of detail about implementation and design choices. If that doesn't specifically help check out some of the other fixture setup patterns.  JUnit's parameterised tests may be what you want. You can set up a collection of data (in your case your POJOs) and run your tests using these as parameters. So you don't need to rewrite tests when you add new example data. Link to ""parameterised tests"" is broken."
33,A,"EasyMock returns Null for Expected Method I have am having a problem with EasyMock returning null for an expected (defined) method call. Creation of the mocked Object mock = EasyMock.createMock(DAO.class); Mock Set up in unit test. expect(mock.update(myObj).andReturn(myObjUpdated).once(); replayAll(); service.setDao(mock); service.processData(myObj); verifyAll(); processData method simply calls MyObject objUpdated = dao.update(myObj); here is the interface that the mock is being built from. public interface DAO { public <ENTITY> ENTITY update(ENTITY entity); } I am pretty confused by what might be causing the problem. I have confirmed that 'obj' is the same object as I defined in the unit test. I have also not experienced this problem (that I am aware of) with any other methods that mocked. Could the problem possibly be with the Object that is being passed in? Thanks in advance. I am really not sure what other information might be helpful to you here. edit: this is the test class (and as it turns out where my misunderstanding began) public class TestMyService extends EasyMockHelper {...} So it turns out that my main problem isn't with the expectations or even with the creation of the mock object. I had a fundamental misunderstanding about how the EasyMockSupport class which my test is extending functions. This isn't covered very well in the documentation but if you exam the examples a bit more closely my error became obvious. The EasyMockSupport class gives my test class access to methods such as replayAll() verifyAll() and resetAll(). what these do is allow me to now worry about manually controlling each created mock object. However what the documentation failed to mention was that you have to create you Mock object USING the methods provided by the EasyMockSupport class so that it can properly register the controls. ((this makes total sense btw I simply wasn't reading it anywhere)). The EasyMockSupport class if you look into the API provides the child class with all the methods that it would normally use statically from the EasyMock class such as createMock(Class class). So as for the updated code public class TestMyService extends EasyMockSupport { private MyService service; private MyDao dao; private MyObject myObj; @Before public void setUp() { service = new MyService(); // THIS IS THE KEY mock = createMock(IDao.class); //CORRECT // mock = EasyMock.createMock(IDao.class); //WRONG service.setDao(mock); myObj = new MyObject(""expectedData""); } @After public void tearDown() { verifyAll(); } @Test public void testMyService() { expect(mock.update(myObj)).andReturn(myObj); replayAll(); service.myService(myObj); } } public class MyService() { private IDao dao; public void setDao(IDao dao) {this.dao = dao; } public MyObject myService(MyObject myObj) { return dao.update(myObj); } } The other 'key' is to extend your test case class with EasyMockSupport."
34,A,"Setting up test inputs in eclipse I have some methods that would require to execute over a java class. For example my method receives as argument a class file something like: Information info = grabInformation(""class_to_execute""); This method would run the ""class_to_execute""and capture its output. And I would like to later assert its output with a given expected value. My question is: how could I set up eclipse so that my test cases would find the classes that it will execute? Is adding the classes to the build path enough? Are there some variables I could set? I don't think the CLASSPATH has anything to do with it. I'm afraid you'll have to elaborate on that. Did you check your classpath? If ""class_to_execute"" is in another project or JAR then add it to your Build Path under Libraries. Do you have any reason to believe that's not enough? Build path == CLASSPATH for most purposes. If you're having Build Path or CLASSPATH problems it might be easier to debug if you do this: Information info = grabInformation(class_to_execute.class); If it can't find the class then put your cursor on the error and type Control+1. Eclipse might be able to help you fix the Build Path automatically."
35,A,"Is there a tool for Java which finds which lines of code are tested by specific JUnit tests? Is there a tool for Java which given a set of JUnit tests and a class to test will tell you which lines of the class are tested by the tests? ie. required to be present for the tests to run successfully. I don't mean ""code coverage"" which only tells you whether a line is executed but something stronger than that: Is the line required for the test to pass? I often comment out a line of code and run a test to see if the test really is testing that line of code. I reckon this could be done automatically by a semi-smart tool (eg. something like an IDE that can work out what can be removed from a method whilst keeping it compilable). As a side note I believe pure TDD would eliminate the need for such a tool however it would come very handy indeed when writing unit tests for existing (legacy) code. @Péter Only true if you can start from scratch but theres always legacy code around ;) @Péter @fielding: Even for green-field stuff code coverage tools are a big help. @fielding if you have the code already (but no unit tests) by definition you can't do TDD. Of course this is the common case (alas) but that was not the point here. @Nathan it never hurts to use a coverage tool however IMHO TDD (when done well) ensures that you only write code for which you have a(t least one) unit test. @Péter: agreed i just like the explicit feedback i get from the cobertura reports it helps me confirm the test did what I thought it would. I use emma for most of my projects. i included it in my ant build file and it generates html files for the reports two other coverage projects i read about but haven't tried yet are clover or cobertura  What you are looking for might be referred to as mutation testing. While mutation testing won't tell you which lines of code are required to pass per se. What mutation testing does is modify your source code looking for changes it can make to your code but your test still passes. E.g. changing if (a < b) to if (a >= b) and seeing if the test still passes. This will highlight weaknesses in your test. Another java library for mutation testing is jumble.  There's an open source mutation-testing tool called Jester that changes the lines of your source code then runs your tests and reports whether your tests passed anyway. Sounds closer to what you're looking for than code coverage tools. Jester is a test tester for testing your java JUnit tests (Pester is for Python PyUnit tests). It modifies your source code runs the tests and reports if the tests pass despite the changes to the code. This can indicate missing tests or redundant code. WRT the discussion about whether these tools are needed in a pure TDD project there is a link on the Jester project webpage to a posting about the benefits of using Jester on code written during a TDD session (Uncle Bob's infamous bowling TDD example).  I love cobertura because the generated reports are IMHO the most beautiful. And it has its own ant target! In comparison to emma it has also branch coverage not only line coverage which is misleading very often. +1 IMO cobertura is the best solution to this problem Cobertura is great for code coverage but I think the OP is looking for assertion coverage. E.g. you can have 100% code coverage but assert nothing. The OP is looking for a tool to help identify where assertions are lacking."
36,A,"JUnit 4's @Suite passing data @Rule and deprecating @Before and @After I'm working with some legacy test code which makes use of a TestSetup class to setup and teardown a server around a test suite of classes containing tests. I've converted the classes to use junit annotations and have attempted to use the @Suite and @Suite.Classes annotations to define the suite. But I've hit a wall. The old version of the tests extend TestSetup to loop through all the instantiated test classes and inject various references to server objects into them. An example being a reference to the Spring framework web context. My problem is that using the annotations I cannot see how to pass fixture data to the instantiated test classes before the tests are executed. I've looked into the new @Rule and MethodRule techniques but (frankly) they just appear to be a complex but limited solution to a problem that's not really clear. Anyway I could not see how they could solve my problem. An additional concern being that it would appear that the JUnit authors are intent on eventually removing the @Before/@After concept from JUnit. So does anyone know how to pass data from a class annotated with @Suite to each test that the Suite class runs? What is ""pass fixture data to the instantiated test classes"" exactly? My problem is that using the annotations I cannot see how to pass fixture data to the instantiated test classes before the tests are executed. JUnit4-style test suites are known to be more limited than JUnit3-style test suites. I've heard complaints about this but am not aware of an official solution. I think there might be some third-party solutions that try to tackle this problem. Regarding your particular problem I don't think you can get access to the test instances from a JUnit4-style suite because JUnit4 instantiates a test right before the method gets run. The simplest way out would be to continue using the JUnit3-style suites. They should run fine with JUnit4. One drawback of managing shared resources with a suite is that you can't run the test classes separately anymore. On the other hand if the test classes need to be run together anyway (maybe in a particular order) it may be a valid solution. Another approach is to tackle the shared resources problem inside-out: In your case test classes would declare with a @Rule that they need the server to be present. The first time the rule gets executed it starts the server keeps relevant information in static field(s) and does whatever is necessary to prepare the test. For example it could inject some of the test's fields or it could just allow the test to use the rule's API to get at the information it needs. The next time the rule gets executed it skips the ""start server"" step. Yes static state is evil but it's sometimes the only way to get around JUnit's limitations. (I won't touch the subject of using other testing frameworks here.) The big advantage of the inside-out approach is that it allows you to run test classes directly (without going through the suite) and independently from each other. The drawback is that it can be more difficult to implement and that it makes it hard to tear down non-memory resources (in your case: shutting down the server). To accomplish the latter you will have to use a JVM shutdown hook because JUnit doesn't provide an adequate callback. Although this will probably work (because build tools and IDEs typically use a separate JVM for each test run) it is nevertheless ugly. A third approach is to let your Ant/Maven/Gradle/whatever build set up/tear down common resources before/after running the tests. Again this is less flexible and convenient than the inside-out approach and brings up the question of how to pass information from the build to the JVM that's running the tests (if necessary). The Maven Cargo plugin is a typical example for this approach. Last but not least you could try to be pragmatic and start/stop the server once per test class. This is very simple to implement but may or may not be adequate in your situation (e.g. it might be too time-consuming). I've looked into the new @Rule and MethodRule techniques but (frankly) they just appear to be a complex but limited solution to a problem that's not really clear Rules are a way to write modular and reusable JUnit extensions. Much better than using base classes where you will soon hit the single inheritance problem. This is even more relevant for libraries shipping JUnit extensions. An additional concern being that it would appear that the JUnit authors are intent on eventually removing the @Before/@After concept from JUnit. Where did you hear that? Aren't you confusing this with xUnit.net which strongly discourages the use of a setup/teardown method? Anyway performing some action before or after a test is one of the most important use cases for rules so I don't think you should be concerned. I'm not sure where I read they where going away but I might have been confused by the comments on the BlockJUnit4ClassRunner class. I've read so many blogs and tutorials trying to figure this out it's easy to get confused. :-) THanks for the posting thats massive.  JUnit 4.9 is cooking a better solution for this in the form of a builder but I think before then the cleanest (although a bit difficult to implement) way to address this is to make a custom Runner implementation (extend one of the existing ones). @Peter is quite right that Suite creation has weak points over JUnit 3.8 although it also has some strong points."
37,A,"How to mix Android JUnit classes with plain JUnit classes in same project Is it possible to have both Android JUnit classes (i.e. extends ActivityInstrumentationTestCase2<>) with plain JUnit classes (i.e. extends TestCase) in the same project? Some of the classes in my app are ""generic"" (i.e. do not require an activity or the emulator to be tested) and I would like to created test cases for them but without having to create a separate test project. Is this possible? Yes you can. There should be no problem in having different super classes in the same test project.  See this http://marakana.com/tutorials/android/junit-test-example.html This is the best tutorial I have found on the subject."
38,A,"Change working directory in ant junit task I have a ant file that runs JUnits tests. These tests rely on a relative path to certain configuration files. I've tried to set the working directory for the batch test but fail. I want the working directory to be ${plugins.dir}/${name} The JUnit part of the ant script: <junit haltonfailure=""no"" printsummary=""on"" fork=""true"" showoutput=""true"" dir=""${plugins.dir}/${name}""> <jvmarg value=""-Duser.dir=${plugins.dir}/${name}""/> <classpath> <path refid=""project.classpath""/> <pathelement location=""${plugins.dir}/${dependency}/@dot/""/> <pathelement location=""${plugins.dir}/${name}/"" /> </classpath> <formatter type=""xml"" /> <sysproperty key=""basedir"" value=""${plugins.dir}/${name}""/> <sysproperty key=""dir"" value=""${plugins.dir}/${name}""/> <batchtest todir=""${junit.output}""> <fileset dir=""${dir}""> <include name=""**\*AllTests.class"" /> </fileset> </batchtest> </junit> I've googled and searched but the workarounds I've found have been to set the ""dir"" ""sysproperty"" or ""jvmarg"". As you can see I've tried them all :) Is there a way to print the current dir in the tag? It doesnt support . That would allow me to verify if the dir is actually changed and to what. One wildcard in this equation is that this is being run in Hudson that starts upp an eclipse process that starts antrunner. This is so we can run both junit and eclipse plugin junit tests. Shouldn't have anything to do with the problem I think. Is it not possible to simply fix your tests such that they are self-contained? E.g. if the resources you need are put somewhere in the classpath instead you could load them through `getClass().getResource(String)` instead. @Mark peters Yes that is possible. But that involves updating some 400-500 tests so I'd prefer to consider that plan B. I think you are right with setting the basedir property (see projects attributes). However since it is a property of ANT (and not of the JVM) it is READ ONLY! Does it effect other target if you set the basedir when calling your ant task? See Command Line reference. ant -Dbasedir=somedir Alternatively span a new ant process to call your junit target. See the AntCall task or Ant task. Following examples assumes that the junit target contains your junit task. I used this task for other properties (never needed the basedir property so far. <antcall target=""junit""> <param name=""basedir"" value=""${plugins.dir}/${name}""/> </antcall> <ant dir=""${plugins.dir}/${name}"" target=""junit"" /> Thanks. I haven't has time to try this out yet but it seems like it will solve atleast some of my problems so I'm marking this as the answer.  I had the same scenario and in my case I saw that dir=""...."" is ignored if run in same jvm so I simply added fork='true' and it worked. Quote from Apache's documentation site ""dir - The directory in which to invoke the VM. Ignored if fork is disabled."". More here. and dont forget to use fork=""true"" on the batchtest part too as it overwrites the junits fork property. That did the trick for me.  I'm using NetBeans. When I add to ant properties (from Tools Options Java Ant): work.dir=C:/MyWorkingDir/ it execute ant with the following command and change the working dir to C:\MyWorkingDir ant -f D:\workspace\lib\project -Dfork=true -Djavac.includes=com/myapp/MyTest.java -Dtest.includes=com/myapp/MyTest.java ""-Dwork.dir=C:/MyWorkingDir/"" test-single"
39,A,"JUnit for Spring beans using autowire? Here is the code: public class Customer { @Autowired private Person person; //some business logic using person object } Now I need to write the jUnit test case for the Customer class how to go about it? Shall i use Mockito to mock the person object and then execute the business logic and if yes the how to set the mocked person object as the property of the Customer without any setter/getter? Thanks! Sounds to me like you're overthinking it. I would not mock a business object model; that's for interface-based classes when you don't want an integration test. I would not inject a Person into a JUnit test; I'd simply call ""new"" instantiate what was appropriate for my test and get on with it. If your Customer has to have an instance of Person I'd recommend constructor injection. You can pass a mock Person that way if you must. Unless Person is just a struct - and perhaps even if it is - mocking is a good idea. It's a unit test for Customer right? Why would you not try to factor out the behavior of other significant classes? I'm thinking that mocking a business object like a Person wouldn't buy me anything over and above just calling new and instantiating one. one more thing I would like to ask is that if DAO call is taking place from an Action object then while write junit for the action should have mock the dao call as we are writing unit testing 'cos if we don't mock DAO it would be categorized as integration test? Yes. Test the DAO using a database connection - it makes no sense otherwise. But other classes that use the DAO can benefit from a mock once you're sure the DAO is sound."
40,A,Netbeans: Is it possible to use newer version of JUnit than the bundeled one? Has anyone tried to use a newer version of JUnit in Netbeans? Netbeans currently has 4.5 bundled. Newest is 4.8 or so. Could a newer version of JUnit break compatibility with Netbeans' unit test integration? Or do something else negative? Yes you would add the new Junit JAR as a library and then add it to the project's Test Libraries folder in the Projects window. The Test Libraries folder is below the one shown here. Thanks! I will try! It worked! Tried JUnit 4.8.1.
41,A,"How to make HtmlUnit's WebClient accelerate execution of javascript to be triggered by window.setTimeout? I am using the Java library HtmlUnit to create a regression test suite for a web app. I have an ""onload"" handler hooked in the body of pages of the app to redirect to a timeout page after the session would have expired. The handler is some JavaScript of the form: window.setTimeout( function() { window.location = 'timout.html'; } 3600000); I would like to test that the redirect will eventually be fired when the time arrives but the closest thing I can find is to actually wait the entire duration of the time (say an hour in the example above) as suggested by the Java sample below: WebClient webClient = new WebClient(); ... webClient.waitForBackgroundJavaScript( 3600000); I would like to know if it is possible to trick the script execution engine into behaving as if that much time has passed without actually having to wait minutes or hours ""real time"" for a test suite to be run. Ideally one could tell the engine/client/interpreter that X milliseconds had passed (to emulate the wait) or perhaps to set some kind of ""time dilation"" factor and poll the page to see how it is being updated. I don't think you can do that easily. The only way I can see it possible would be by mocking the Rhino method that handles the settimeout in you tests... Perhaps with a framework like Mockito... But that's not what you want as if you want to test the effectiveness of the redirection of the page you test the amount of time it takes to be redirected cheers grooveek Just because I don't like your answer doesn't mean it's not true I guess. Point!"
42,A,"Does reflection breaks the idea of private methods because private methods can be access outside of the class? Does reflection break the idea of private methods? Because private methods can be accessed from outside of the class? (Maybe I don't understand the meaning of reflection or miss something else please tell me) http://en.wikipedia.org/wiki/Reflection_%28computer_science%29 Edit: If relection breaks the idea of private methods - do we use private methods only for program logic and not for program security? Thanks Reflection doesnt break encapsulation. Developers do ;) Likewise casting breaks the idea of static typing right? @Ken: Only if it's C-style casting without limits. What about downcasting? It forces the runtime to do a type check which defeats the purpose of static typing i.e. compile-time type checking. @Steven Sudit how can it? since then C-style casting converts object to another type in some reasonable way? @Ken: Using C-style casting lets you cast anything to anything at great peril. If you feel like it you can take the address and cast it to a `char*` allowing you to modify the values byte by byte. @valya: The whole point is that C-style casting is not obligated to be reasonable. In contrast a C++ `dynamic_cast<>` does do a runtime type check (using RTTI) so long as certain constraints are met (such as the existence of a vtable). @Steven Sudit I mean that if you do something like (int)""123"" it's not going to work so C-style casting break type system like Hulk breaks your relationship - together with your house @valya: If you're saying that C-style casting breaks whatever it wants to break (much like ""Hulk smash"") then yes. .NET specific [why-can-reflection-access-protected-private-member-of-class-in-c](http://stackoverflow.com/questions/2084353/why-can-reflection-access-protected-private-member-of-class-in-c?rq=1) Yes reflection breaks this idea. Native languages also have some tricks to break OOP rules for example in C++ it is possible to change private class members using pointer tricks. However by using these tricks we get the code which can be incompatible with future class versions - this is the price we pay for breaking OOP rules. @Steven Sudit: so what? it all boils down to the same thing. Somebody else subverting the original class designer's intentions. Arguably even if we didn't have reflection we could use an `unsafe` block with a raw pointer to mess with the internals of a class. It may be worth noting that Code Access Security can be used to mitigate such issues. Also reflection is permissions based by the CLR so in a partial trust environment reflection might throw a security exception. @JeremyP: It's true that the effect is similar. It's also true that this removes ""private"" entirely rather than stepping around it on a case-by-case basis so there really is a meaningful distinction. @Josh correct but CAS is now deprecated. @Henk: I missed the memo. When did CAS get deprecated? As of Fx 4 it is turned off by default. I'm not sure if the new claims system can control reflection. Here's a link: http://www.simple-talk.com/dotnet/.net-framework/whats-new-in-code-access-security-in-.net-framework-4.0---part-i/ Actually in C++ you can access private members by putting the following line in the header file `#define private public`. @JeremyP: That's not accessing private members that's using a preprocessor to make them public. @Henk: Thank you for the interesting link. It appears that CAS isn't going away but the CAS *Policy* model is being replaced with something a little easier to use. Good to know.  Yes Reflection could be used to violate encapsulation and even cause incorrect behavior. Keep in mind that the assembly needs to be trusted to perform reflection so there are still some protections in place. This answer appears to be correct so why was it downvoted?  Yes. Reflection breaks encapsulation principle. That's not only to get access to private members but rather expose whole structure of a class.  access control through private/protected/package/public is not primarily meant for security. it helps good guys to do the right thing but doesn't prevent bad guys from doing wrong things. generally we assume others are good guys and we include their code into our application without though. if you can't trust the guy of a library you are including you are screwed.  I think this is a matter of opinion but if you are using reflection to get around the encapsulation put in place by a developer on a class then you are defeating the purpose. So to answer your question it breaks the idea of encapsulation (or information hiding) which simply states that private properties/methods are private so they cant be mucked with outside the class.  do we use private methods only for program logic and not for program security? It is not clear what you mean by ""program security"". Security cannot be discussed in a vacuum; what resources are you thinking of protecting against what threats? The CLR code access security system is intended to protect resources of user data from the threat of hostile partially trusted code running on the user's machine. The relationship between reflection access control and security in the CLR is therefore complicated. Briefly and not entirely accurately the rules are these: full trust means full trust. Fully trusted code can access every single bit of memory in the process. That includes private fields. The ability to reflect on privates in partial trust is controlled by a permission; if it is not granted then partial trust code may not do reflection on privates. See http://blogs.msdn.com/b/shawnfa/archive/2006/09/29/777047.aspx for details. The desktop CLR supports a mode called ""restricted skip visibility"" in which the rules for how reflection and the security system interact are slightly different. Basically partially trusted code that has the right to use private reflection may access a private field via reflection if the partially trusted code is accessing a private field from a type that comes from an assembly with equal or less trust. See http://blogs.msdn.com/b/shawnfa/archive/2006/10/05/using-lightweight-codegen-from-partial-trust.aspx for details The executive summary is: you can lock partially trusted code down sufficiently that it is not able to use reflection to look at private stuff. You cannot lock down full trust code; that's why it's called ""full trust"". If you want to restrict it then don't trust it. So: does making a field private protect it from the threat of low trust code attempting to read it and thereby steal user's data? Yes. Does it protect it from the threat of high trust code reading it? No. If the code is both trusted by the user and hostile to the user then the user has a big problem. They should not have trusted that code. Note that for example making a field private does not protect a secret in your code from a user who has your code and is hostile to you. The security system protects good users from evil code. It doesn't protect good code from evil users. If you want to make something private to keep it from a user then you are on a fool's errand. If you want to make it private to keep a secret from evil hackers who have lured the user into running hostile low-trust code then that is a good technique.  No reflection doesn't break the idea of private methods. At least not per se. There is nothing that says that reflection can't obey access restrictions. Badly designed reflection breaks the idea of private methods but that doesn't have anything to do with reflection per se: anything which is badly designed can break the idea of private methods. In particular a bad design of private methods can also obviously break the idea of private methods. What do I mean by badly designed? Well as I said above there is nothing stopping you from having a language in which reflection obeys access restrictions. The problem with this is that e.g. debuggers profilers coverage tools IntelliSense IDEs tools in general need to be able to violate access restrictions. Since there is no way to present different different versions of reflection to different clients most languages opt for tools over safety. (E is the counterexample which has absolutely no reflective capabilities whatsoever as a conscious design choice.) But who says that you cannot present different versions of reflection to different clients? Well the problem is simply that in the classical implementation of reflection all objects are reponsible for reflecting on themselves and since there is only one of every object there can be only version of reflection. So where does the idea of bad design come in? Well note the word ""responsible"" in the above paragraph. Every object is responsible for reflecting on itself. Also every object is responsible for whatever it is that it was written for in the first place. In other words: every object has at least two responsibilities. This violates one of the basic principles of object-oriented design: the Single Responsibility Principle. The solution is rather simple: break up the object. The original object is simply responsible for whatever it was originally written for. And there is another object (called a Mirror because it is an object that reflects other objects) which is responsible for reflection. And now that the responsibility for reflection is broken out into a separate object what's stopping us from having not one but two three many Mirror Objects? One that respects access restrictions one that only allows an object to reflect upon itself but not any other objects one that only allows introspection (i.e. is read-only) one that only allows to reflect on read-only callsite information (i.e. for a profiler) one that gives full access to the entire system including violating access restrictions (for a debugger) one that only gives read-only access to the method names and signatures and respects access restrictions (for IntelliSense) and so on … As a nice bonus this means that Mirrors are essentially Capabilities (in the capability-security sense of the word) for reflection. IOW: Mirrors are the Holy Grail on the decade-long quest to reconcile security and runtime dynamic metaprogramming. The concept of Mirrors was originally invented in Self from where it carried over into Animorphic Smalltalk/Strongtalk and then Newspeak. Interestingly the Java Debugging Interface is based on Mirrors so the designers of Java (or rather the JVM) clearly knew about them but Java's reflection is broken.  Reflection does provide a way to circumvent Java's Access Protection Modifiers and therefore violates strict encapsulation as it realised in C++ and Java. However this does not matter as much as you might think. Access Protection Modifiers are intended to assist programmers to develop modular well factored systems not to be uncompromising gate keepers. There are sometimes very good reasons to break strict encapsulation such as Unit Testing and framework development. While it may initially be difficult to stomach the idea that Access Protection Modifiers are easily circumventable try to remember that there are many languages (Python Ruby etc.) that do not have them at all. These languages are used to build large and complex systems just like languages which do provide access protection. There is some debate on whether Access Protection Modifiers are a help or a hindrance. Even if you do value access protection treat it like a helping hand not the making or breaking of your project. The granddaddy of object-oriented programming languages Smalltalk doesn't have them either. In fact the first language I remember seeing Access Protection Modifiers in was C++. While I might agree that ""encapsulation"" is an OO concept I don't think that the C++-style ""public/private/protected"" system is. You can do encapsulation without that. @Ken: Point taken. Reflection does provide a way to circumvent Java's Access Protection Modifiers and therefore rapes encapsulation @Ken: Technically encapsulation just requires combining the data and the functions that act upon it (now called methods) into a single entity (called a class). Having said that being able to remove some of the contents of that entity from public view aids encapsulation so much that it's generally accepted as a basic requirement. rubi+python+php have private methods Steven: Granted. I'm just pointing out that the C++ approach isn't the only way to hide internal values. SICP (see section 3.1.1) for example implements local state using environments (`let`). Scheme doesn't have `private` but that doesn't mean you can't have private local state.  Reflection makes it possible for any CLR class to examine and manipulate properties and fields of other CLR classes but not necessarily to do so sensibly. It's possible for a class to obscure the meaning of properties and fields or protect them against tampering by having them depend in non-obvious fashion upon each other static fields underlying OS info etc. For example a class could keep in some variable an encrypted version of the OS handle for its main window. Using reflection another class could see that variable but without knowing the encryption method it could not identify the window to which it belonged or make the variable refer to another window. I've seen classes that claim to act as ""universal serializers""; they can be very useful if applied to something like a data-storage-container class which is missing a ""serializable"" attribute but is otherwise entirely straightforward. They will produce gobbledygook if applied to any class whose creator has endeavored to obscure things.  It does as other already stated. However I remember that in Java there can be a security manager active that could prevent you from accessing any private methods even with reflection if you don't have the rights to do so. If you run your local JVM such a manager is usually not active.  Yes it does break encapsulation. But there are many good reasons to use it in some situations. For example: I use MSCaptcha in some websites but it renders a < div> around the < img > tag that messes with my HTML. Then i can use a standard < img> tag and use reflection to get the value of the captcha's image id to construct a URL. The image id is a private Property but using reflection i can get that value. Is it bad to break encapsulation??? Actually it solved my problem. you bring good example not to use reflaction because in your case reflaction breaks the encapsulation.Only when she not breaks like unit test etc.. its proper to use it.  Yes but it is not a problem. Encapsulation is not about security or secrets just about organizing things. Reflection is not part of 'normal' programming. If you want to use it to break encapsulation you accept the risks (versioning problems etc) Reflection should only be used when there are no better (less invasive) ways to accomplish something. Reflection is for system-level 'tooling' like persistence mapping and should be tucked away in well tested libraries. I would find any use of reflection in normal application code suspect. I started with ""it is not a problem"". I meant: as long as you use reflection as intended. And careful. Encapsulation is about hiding away details of implementation so that you can maintain state in such a way as to avoid having it become invalid. Reflection whether you consider it ""normal"" does violate this in a *potentially* problematic way as it can bypass all checks on internal consistency. +1 for 'Encapsulation is not about security"" @Steven: Yes but that is why you reserve Reflection for special problems. Hmm. Does deserialization qualify as a special problem? @Steven: Yes. _____ Interesting. I suspect that by your standards I do a great deal of ""special"" programming. What about compilation of dynamic languages (a la IronPython)?  It's like your house. Locks only keep out honest people or people who aren't willing to pick your lock. Data is data if someone is determined enough they can do anything with your code. Literally anything. So yes reflection will allow people to do things you don't want them to do with your code for example access private fields and methods. However the important thing is that people will not accidentally do this. If they're using reflection they know they're doing something they probably aren't intended to do just like no one accidentally picks the lock on your front door.  Yes it breaks the encapsulation if you want it to. However it can be put to good use - like writing unit tests for private methods or sometimes - as I have learned from my own experience - getting around bugs in third party APIs :) Note that encapsulation != security. Encapsulation is an object oriented design concept and is only meant for improving the design. For security there is SecurityManager in java."
43,A,"JUnit in android I am familiar with JUnit testing in android.. My Question is if we are using calculator and we want to test addition operation..To test the addition operation if we are using more number of test cases(for example 30). instead of rewriting the test cases for 30 times is there any generic way to do this or is there any way to take the test cases form excel sheet or xml file..? Please let me know is there any better way ... Thanks in advace You could reduce duplicate code by making a function and calling it in each JUnit test. E.g.: public void test_add_zeroPlusZero() { Assert.assertTrue(""Failed on 0+0."" testAdd(new int[] {00})0); } private boolean testAdd(int[] values int expectedValue) { // Try to add. // If adding fails return false. // If adding succeeds return true. } As for reading in huge numbers of test values couldn't you just read in multiple rows of integers (representing the values to add and the expected result each value separated by a space or something) from file and then put them through the testAdd function (or equivalent) shown above? In pseudocode this might look like: public void test_add_from_file() File testValueFile = get file with test values() while ((nextLine = testValueFile.readLine) != END_OF_FILE) int[] values = parse values from nextLine int expectedValue = parse expected value from nextLine Assert.assertTrue(""Couldn't add the following values: "" + values testAdd(values expectedValue))"
44,A,"Test class with a new() call in it with Mockito A have a legacy class that contains a new() call to instantiate a LoginContext(): public class TestedClass { public LoginContext login(String user String password) { LoginContext lc = new LoginContext(""login"" callbackHandler); } } I want to test this class using Mockito to mock the LoginContext as it requires that the JAAS security stuff be set up before instantiating but I'm not sure how to do that without changing the login() method to externalise the LoginContext. Is it possible using Mockito to mock the LoginContext class? For the future I would recommend Eran Harel answer (refactoring moving new to factory that can be mocked). But if you don't want to change the original source code use very handy and unique feature: spies. From the documentation: You can create spies of real objects. When you use the spy then the real methods are called (unless a method was stubbed). Real spies should be used carefully and occasionally for example when dealing with legacy code. In your case you should write: TestedClass tc = spy(new TestedClass()); LoginContext lcMock = mock(LoginContext.class); when(tc.login(anyString() anyString())).thenReturn(lcMock); This was exactly what I was looking for thanks Tomasz. The extra part at the bottom of the doco about doReturn was especially helpful.  Not that I know of but what about doing something like this when you create an instance of TestedClass that you want to test: TestedClass toTest = new TestedClass() { public LoginContext login(String user String password) { //return mocked LoginContext } }; Another option would be to use Mockito to create an instance of TestedClass and let the mocked instance return a LoginContext.  You can use a factory to create the login context. Then you can mock the factory and return whatever you want for your test. public class TestedClass { private final LoginContextFactory loginContextFactory; public TestedClass(final LoginContextFactory loginContextFactory) { this.loginContextFactory = loginContextFactory; } public LoginContext login(String user String password) { LoginContext lc = loginContextFactory.createLoginContext(); } } public interface LoginContextFactory { public LoginContext createLoginContext(); }"
45,A,"Grails: How do you unit test a command object with a service injected into it I am trying to test a Controller that has a Command object with data binding. The Command Object has a Service injected into it. But When I try test the command object the injected service method is never found as it is never ""injected"" Is there a way to mock a service inside a command object? Test method void testLoginPasswordInvalid() { mockRequest.method = 'POST' mockDomain(User [new User(login:""freddy"" password:""realpassword"")]) mockLogging(UserService) // userService mocked MockUtils.prepareForConstraintsTests(LoginCommand) def userService = new UserService() def user = userService.getUser(""freddy"")//Gets called and returns the mockDomain assert userService.getUser(""freddy"")//Passes def cmd = new LoginCommand(login:""freddy"" password:""letmein"") cmd.validate() // Fails (userService is nevr injected) controller.login(cmd) assertTrue cmd.hasErrors() assertEquals ""user.password.invalid"" cmd.errors.password assertEquals ""/store/index"" renderArgs.view } The getUser() method of the userService isn't found Cannot invoke method getUser() on null object java.lang.NullPointerException: Cannot invoke method getUser() on null object Code The login method of the controller being called def login = { LoginCommand cmd -> if(request.method == 'POST') { if(!cmd.hasErrors()){ session.user = cmd.getUser() redirect(controller:'store') } else{ render(view:'/store/index' model:[loginCmd:cmd]) } }else{ render(view:'/store/index') } } The Command Object has a ""userService"" injected into it. The validator calls this userService to find a user  class LoginCommand { def userService String login String password static constraints = { login blank:false validator:{ val cmd -> if(!cmd.userService.getUser()){ return ""user.not.found"" } } } The userService.getUser() looks like this. class UserService { boolean transactional = true User getUser(String login) { return User.findByLogin(login) } } Service injection is done using Spring autowire-by-name. (Grep the Grails source tree for autowire to find a nice code fragment you can use to get it to autowire your controllers for you in integration tests.) This only functions in integration tests where there's a Spring application context around that has the beans that can be injected. In unit tests you have to do this yourself since there's no Spring-land surrounding your stuff. This can be a pain but gives you some benefits: 1) It's easy to inject mock versions of services - for example using an Expando - in order to more closely specify the behavior of your controller's collaborating services and to allow you to test only the controller logic rather than the controller and service together. (You can certainly do the latter in a unit test as well but you have the choice of how to wire it up.) 2) It forces you to be explicit about the dependencies of your controller - if you depend on it your tests will show it. This makes them a better specification for the behavior of your controller. 3) You can mock only the pieces of external collaborators your controller depends on. This helps your tests be less fragile - less likely to need to change when things change. Short answer: your test method needs a cmd.userService = userService line. Thanks this worked a treat and now I understand more  What John says is on the mark. One example might be: def mockUsers = [new User(login:""freddy"" password:""realpassword"")] mockDomain(User mockUsers) def userService = [getUser:{String login -> mockUsers[0]}] as UserService def cmd = new LoginCommand (/*arguments*/) cmd.userService = userService You can lookup other ways to mock objects at http://groovy.codehaus.org/Groovy+Mocks"
46,A,"Using AbstractTransactionalJUnit4SpringContextTests with Oracle Procs which COMMIT I'm using AbstractTransactionalJUnit4SpringContextTests to run a simple test: public class MyTests extends AbstractTransactionalJUnit4SpringContextTests { @Test public void simpleProcTest() { simpleJdbcTemplate.update(""BEGIN p_dummy_commit; END;""); } } The proc mentioned does a simple table update then a COMMIT. Problem is the change made by this dummy proc doesn't get rolled back as I'd hoped. Background: we have a large app which has lots of procs which do this I am trying to find out how to test these in a rolled-back manner despite them containing explicit COMMIT statements. I'm hoping to take advantage of a transaction-nesting capability which I'd thought I might be getting for ""free"" with this Spring superclass. It seems I don't though. Anyone know how to achieve this? I've confirmed the rollback succeeds as expected when I do direct SQL UPDATEs outside of a proc so my Spring datasource and transactionManager look to be behaving OK. I've been advised offline that because our procs are littered with explicit COMMIT statements we just aren't going to be able to easily perform 100% clean rollbacks. Bah humbug.  Nested Transactions are probably the way to go. Apparently (according to DataSourceTransactionManager nested transactions require the use of the SavePoint mechanism. And if I understand the Oracle Docs right you can probably roll back the outer transaction if your stored procedure sets a save point. But how you can get Spring to know your SavePoint is beyond my grasp. Probably you have to call it explicitly in your SQL. I'm starting to think that my app procs' many embedded COMMIT statements are going to make any kind of easy auto-rollback mechanism impossible. As you mention it does seem like Oracle uses SAVEPOINT for this very purpose but I don't know if replacing all these explicit commits is going to be an option unfortunately."
47,A,"Running junit tests in parallel in a Maven build? I'm using JUnit 4.4 and Maven and I have a large number of long-running integration tests. When it comes to parallelizing test suites there are a few solutions that allow me to run each test method in a single test-class in parallel. But all of these require that I change the tests in one way or another. I really think it would be a much cleaner solution to run X different test classes in X threads in parallel. I have hundreds of tests so I don't really care about threading individual test-classes. Is there any way to do this? Thanks for the information looking forward to getting support for this in surefire will help no end in larger projects. You can change your test to be TestNg test in a minute (you just need to change imports) TestNG is the best in parallel testing.  You could try Gridgain that lets you run distribute your tests across a compute grid. I've tried out the GridGain solution and have had two major problems. Firstly you have to tell GridGain to exclude from your grid task's classpath anything that GridGain also uses e.g. Spring and lots of Apache Commons stuff. Secondly the network classloading while a brilliant idea doesn't work for libraries that want to search the classpath e.g. Spring Does your comment still hold true today?  Inspired by JUnit's experimental ParallelComputer runner I've built my own ParallelSuite and ParallelParameterized runners. Using these runners one can easily parallelize test suites and parameterized tests. ParallelSuite.java public class ParallelSuite extends Suite { public ParallelSuite(Class<?> klass RunnerBuilder builder) throws InitializationError { super(klass builder); setScheduler(new RunnerScheduler() { private final ExecutorService service = Executors.newFixedThreadPool(4); public void schedule(Runnable childStatement) { service.submit(childStatement); } public void finished() { try { service.shutdown(); service.awaitTermination(Long.MAX_VALUE TimeUnit.NANOSECONDS); } catch (InterruptedException e) { e.printStackTrace(System.err); } } }); } } ParallelParameterized.java public class ParallelParameterized extends Parameterized { public ParallelParameterized(Class<?> arg0) throws Throwable { super(arg0); setScheduler(new RunnerScheduler() { private final ExecutorService service = Executors.newFixedThreadPool(8); public void schedule(Runnable childStatement) { service.submit(childStatement); } public void finished() { try { service.shutdown(); service.awaitTermination(Long.MAX_VALUE TimeUnit.NANOSECONDS); } catch (InterruptedException e) { e.printStackTrace(System.err); } } }); } } Usage is simple. Just change @RunWith annotations value to one of these Parallel* classes. @RunWith(ParallelSuite.class) @SuiteClasses({ATest.class BTest.class CTest.class}) public class ABCSuite {}  You can check out the open source library - Test Load Balancer. It does exactly what you ask for - run different test classes in parallel. This integrates at the ant-junit level so that you do not have to change your tests in anyway. I am one of the authors of the library. Also think about not running them in threads as you may need a process level sandbox. For example if you are hitting a DB in your integration tests you do not want one test to fail because another test added some data in a different thread. Most of the times tests are not written with this in mind. Finally how have solved this problem till now?  TestNG can do that (this was my first reflex - then I saw you're already having a lot of testcases). For JUnit look at parallel-junit. Unfortunately this is not the answer to the question I am asking. parallel-junit only runs within a single test class. TestNG also only runs within a single class and my tests are not TestNG tests. @PlatinumAzure: I updated the link. I do not know how this project is maintained. Another question was asked recently to [distribute junit tests execution on several machines](http://stackoverflow.com/questions/10690587/running-parallel-junit-tests-on-multiple-remote-machines). @philant: The link to parallel-junit is broken not to mention it has probably been rendered obsolete.  tempus-fugit offers something similar check the docs for details. It relies on JUnit 4.7 and you just mark your test to @RunWith(ConcurrentTestRunner). Cheers Nice project you have there ;) I'll check it out  Use maven plugin: <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.7.1</version> <configuration> <parallel>classes</parallel> <threadCount>5</threadCount> </configuration> </plugin> </plugins> </build> plus 1 for mvn. also mvn test -t 4 will use 4 threads for example. [](http://maven.apache.org/surefire/maven-surefire-plugin/test-mojo.html#parallel) can only be used with TestNG the OP is using JUnit. In the case of JUnit use `perthread` is actually supported by surefire if you're using Junit 4.7 or later. [surefire guide](http://maven.apache.org/surefire/maven-surefire-plugin/examples/junit.html)  From junit 4.7 it's now possible to run tests in parallel without using TestNG. Actually it has been possible since 4.6 but there are a number of fixes being made in 4.7 that will make it a viable option. You may also run parallel tests with spring which you can read about here The linked page says ""for most dual-core solutions running with parallel threads is currently never any faster than running non-threaded"". Is that still the case? I would think that if your tests do any IO they would still benefit. For example if your unit tests are more like integration tests and hit the database running in parallel should speed them up. @Raedwald Dont expect *too* much for the short non-io-bound unit tests is what I'm trying to say. Newer versions of surefire are also better/more efficient than 2.5 described in the post so you may get slightly better results.  You can run the tests in parallel using ParallelComputer provided by Junit itself. Here's a small snippet to get you started. Class[] cls = { TestCase1.class TestCase2.class }; Result result = JUnitCore.runClasses(ParallelComputer.classes() cls); List<Failure> failures = result.getFailures(); This will help when you need to run tests from code as it has no dependencies on Maven or any other build management tools. Please note that this will run all test cases in parallel if you have any dependencies between different test cases it might result in false positives. You SHOULD NOT have interdependent tests anyway.  This solution is very convenient: While this may theoretically answer the question [it would be preferable](http://meta.stackexchange.com/q/8259) to include the essential parts of the answer here and provide the link for reference.  Actually after reading all these answers I decided to write my own solution. It seemed much easier for me. While this may theoretically answer the question [it would be preferable](http://meta.stackexchange.com/q/8259) to include the essential parts of the answer here and provide the link for reference."
48,A,"Test data directory with jUnit I'm writing some jUnit tests that depend on data files. Where should those data files go? And how would I (in the jUnit tests) get the location of that directory? In Python I would use something similar to: datadir = os.dirname(__file__) + ""/data/"" Kind of depends on what you're using the data files for but in general just create a package and make sure it's on your classpath. To load a properties file from the ""data"" package add a ""MyData.props"" file and you can use load a properties file like: this.getClass().getClassLoader().getResourceAsStream(""/data/MyData.props""); Again not exactly sure if this answers your question since I'm not 100% sure what you're trying to do but I hope it helps a little.  Keep your test data close to your test classes (same package). As todd.run suggested use getResourceAsStream() to access your data files."
49,A,"Spring Dependency Injection with TestNG Spring support JUnit quite well on that: With the RunWith and ContextConfiguration annotation things look very intuitive @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = ""classpath:dao-context.xml"") This test will be able to run both in Eclipse & Maven in correctly. I wonder if there is similar stuff for TestNG. I'm considering moving to this ""Next Generation"" Framework but I didn't find a match for testing with Spring. It works with TestNG as well you need to subclass org.springframework.test.context.testng.AbstractTestNGSpringContextTests or org.springframework.test.context.testng.AbstractTransactionalTestNGSpringContextTests. Thanks. That's exactly what I'm looking for.  Spring and TestNG work well together but there are some things to be aware of. Aside from subclassing AbstractTestNGSpringContextTests you need to be aware of how it interacts with standard TestNG setup/teardown annotations. TestNG has three levels of setup BeforeSuite BeforeClass BeforeMethod which occur exactly as you would expect (great example of self-documenting APIs). These all have an optional value called ""dependsOnMethods"" which can take a String or String[] which is the name or name(s) of the methods at the same level. The AbstractTestNGSpringContextTests class has a BeforeClass annotated method called springTestContextPrepareTestInstance which you must set your setup method to depend on if you are using an autowired class in it. For methods you don't have to worry about the autowiring since it occurs when the test class is setup in that before class method. This just leaves the question of how you might use an autowired class in a method annotated with BeforeSuite. You can do this by manually calling springTestContextPrepareTestInstance - while its not setup by default to do this I've done it several times successfully. So to illustrate a modified version of Arup's example: import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.testng.AbstractTestNGSpringContextTests; import org.testng.annotations.Test; @Test @ContextConfiguration(locations = {""classpath:applicationContext.xml""}) public class TestValidation extends AbstractTestNGSpringContextTests { @Autowired private IAutowiredService autowiredService; @BeforeClass(dependsOnMethods={""springTestContextPrepareTestInstance""}) public void setupParamValidation(){ // Test class setup code with autowired classes goes here } @Test public void testNullParamValidation() { // Testing code goes here! } } The method org.springframework.test.context.testng.AbstractTestNGSpringContextTests#springTestContextPrepareTestInstance already has the annotation @BeforeClass so this solution appear to me as redundant. This solution allows you to add autowire-field dependent code to your test. ""springTestContextPrepareTestInstance"" being a ""before class"" method doesn't guarantee that it will run before the ""before class"" of a sub-class - you need to explicitly set the dependsOnMethods field  Here is an example that worked for me: import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.testng.AbstractTestNGSpringContextTests; import org.testng.annotations.Test; @Test @ContextConfiguration(locations = {""classpath:applicationContext.xml""}) public class TestValidation extends AbstractTestNGSpringContextTests { public void testNullParamValidation() { // Testing code goes here! } }"
50,A,"JUnit @Test expected annotation not working I've got the following test: @Test(expected = IllegalStateException.class) public void testKey() { int key = 1; this.finder(key); } But JUnit reports that the test fails although it throws — as expected — an IllegalStateException. Do I have to configure something else to make this run? I run the test now with @RunWith(Suite.class) @SuiteClasses(Test.class) public class TestSuite { } like in this question but am still not getting the desired result. And when I remove the test prefix I'm still getting an error. I gotta say that I run these tests with Eclipse but it's configured to use the JUnit 4 Runner. When the test's name doesn't start with `test` I get the message `junit.framework.AssertionFailedError: No tests found in MyClass` And yes the class extends `TestCase`. Well then it's clear - you are mixing JUnit 3 and JUnit 4 features which doesn't work. Either write your test in JUnit 3 style or in JUnit 4 style. Maybe you are inadvertently running the test with JUnit 3. Does the test class extend TestCase? What happens when you change the name of the test method so that it no longer starts with `test`? The problem was that the class in which the test was nested was an extension of TestCase. Since this is JUnit 3 style the annotation didn't work. Now my test class is a class on its own. Sorry for the typo. ;-)  Just tested this under JUnit4: this DO work test completes successfully. Look if it is a IllegalSelectorException or such.  i tried this one and work perfectly as expected. public class SampleClassTest { @Test(expected = ArithmeticException.class ) public void lost(){ this.lost(0); } private void lost(int i) throws ArithmeticException { System.out.println(3/i); } } also ensure that junit4 is added as dependancy @ (annotations) are new feature in junit 4.  This looks correct to me. Check your assumptions. Are you sure it throws the exception? If what you say is true removing the expected from the annotation should make it fail. I'd be stepping through the code with a debugger to see what's going on. I'll assume you have an IDE that will do so like IntelliJ.  No this JUnit test should work as it is - there is nothing more needed on this side. What makes you sure that the test throws an IllegalStateException? Is it possible that it gets wrapped into another exception of different type? Please post the exact failure message from JUnit. As @duffymo suggested it is easy to verify what (if any) exception the test really throws. JUnit's message: `java.lang.IllegalStateException: ... MyErrorMessage ..` Yes it's possible for the exception to be wrapped into another one but in that case you should assert the _wrapper_ one and never the other.  I faced same issue solution is simple ""Don't extends TestCase class"""
51,A,"How To Run integrational Tests In our project we have a plenty of Unit Tests. They help to keep project rather well-tested. Besides them we have a set of tests which are unit tests but depends on some kind of external resource. We call them external tests. They can access web-service sometimes or similar. While unit tests is easy to run the integrational tests couldn't pass sometimes - for example due to timeout error. Also these tests can take too much time to run. Currently we keep integration/external unit tests just to run them when developing corresponding functionality. For plain unit tests we use TeamCIty for continuous integration. How do you run the integration unit tests and when do you run them? Please make this a community wiki. I don't think there's a ""answer"" just discussion and advice. not sure about that. @Preet Sangha: ""not sure about that""? Not sure about what? Are you saying this should not be CW? If so why not? What kind of ""correct"" answer is possible? This isn't like a syntax error is it? How is this question like a syntax error where there's an ""answer""? As probably of this discussion I lookto how to categorize Integration Tests. And now what I can add by myself: 1. Test that checks that our software communicates as expected with remote software (remote ws mock-ups can be used for that as noted by S. Lott). 2. Test that checks that remote service works as expected by us. @slott. I was thinking about the nature of the question. How do you run ... and when.... Yes there are many possible answers that doesn't mean they aren't all correct. There's nothing I see about SO only being about 'an' answer. The idea is to let the community decide which is (a) best and also to judge the correctness. We run all the tests in one huge suite. It takes 7 minutes to run. Our integration tests create mock servers. They never time out -- except when the test requires the server to time out. So we have the following kinds of things. (The code sample is Python) class SomeIntegrationTest( unittest.TestCase ): def setUp( self ): testclient.StartVendorMockServer( 18000 ) # port number self.connection = applicationLibrary.connect( 'localhost' 18000 ) def test_should_do_this( self ): self.connection.this() self.assert... def tearDown( self ): testClient.KillVendorMockServer( 18000 ) This has some limitations -- it's always forking the client mock server for each test. Sometimes that's okay and sometimes that's too much starting and stopping. We also have the following kinds of things class SomeIntegrationTest( unittest.TestCase ): def setUp( self ): self.connection = applicationLibrary.connect( 'localhost' 18000 ) def test_should_do_this( self ): self.connection.this() self.assert... if __name__ == ""__main__"": testclient.StartVendorMockServer( 18000 ) # port number result= unittest.TextTestRunner().run() testclient.KillVendorMockServer( 18000 ) system.exit( result.failures + result.errors ) To support this testing we have a number of mocked-up servers for various kinds of integration tests. @S.Lott I believe they are tests too. ""first time develop"" is a kind of Learning Tests (I believe you;ve heard about them). And if you develop with rather small ws which is quite stable in it's API it's ok but in live world we are working with rather live major ws which updates API rather often and we even find bugs sometimes in their ws. The proof of that is that today we found a bug which reflects that we don't know how remote ws should communicate with our software :-) To clarify that we should ask the support of remote ws and writing a good test would be nice too. @Vladimir: ""Learning Tests"" is a contradiction. You can only ""test"" when you have **Expected Results** and your software is shown to produce the **Expected Results**. If you're exploring or learning something new that does not fit the definition of ""test"". That's ""Exploring"" and ""Learning"" not ""Testing"". I realize that many times you must explore web services to understand a change. I understand that when you learn something you find that software is wrong. None of that is ""testing"". That's learning. Do you require developers to run the tests before a check-in? That's good approach to mock-up remote services. I think we don't do that because sometimes it's hard to do mock-up - or it can take some time. Also sometimes I don't even know how remote ws works. Third thing is that sometimes it's helpful to test that remote service works fine as expected. We had a cases when major service provider had issues that could potentially found by some external tests. @Vladimir: ""sometimes I don't even know how remote ws works"". False. You know what your application sends and receives. That's all you have to handle in the Mockup. Nothing more just enough to make the test pass. @matt b: Of course. Yes it takes time to run the complete test suite. Software is complex and difficult and it takes real work to test. @S.Lott I agree that we should mockups which are proof that remote ws works as we expect. But there is another stage - when you first time develop with certain remote ws. Or when ws changed the protocol. Do you have such tests which checks that ws work as you expect in your mockups? @Vladimir: Those aren't ""tests"". ""first time develop"" and ""when ws changed the protocol"" aren't things you find by testing. That's ""design"" and before that ""requirements"". Nothing to do with testing except you may write some small software (we call them ""spike solutions"") to determine what the protocol *really* is. But this is not ""testing"": this is finding out what requirements you have and creating a designing.  We're using Maven2: maven-surefire-plugin to run unit tests (in the test phase) and maven-failsafe-plugin for integration tests (integration-test phase). By default all tests run when the project is built however integration tests can be turned off using profiles. In many cases integration tests are the part of the module n some cases there are also dedicated modules which only do integration tests. One of the teams also uses Fitnesse for acceptance testing. These tests are also in dedicated modules. We're using Hudson for CI. Thanks for mentioning Fitnesse - I've heard of it before. And now just watched the presentation - it's from Robert Martin. Looks good. I will try to think how we can integrate it for our purposes! Do you have a lot of acceptance tests in Fitnesse? My team - not the other team has really a lot of things done as acceptance tests.  In our project we have separate suite for regular/plain unit tests and separate suite for integration tests. The are two reasons for that: performance: integration tests are much slower test fragility: integration tests fail more often due to environment-related conditions (give false positives). We use TeamCity as our main Continuous Integration server and Maven as build system. We use the following algorithm to run the tests: We run unit tests at within Eclipse IDE and before every commit. We run unit tests automatically after each commit on TeamCity agents using Maven's mvn clean install We run integration tests automatically on TeamCity agent after ""main"" build is completed. The way we trigger integration tests execution is by configuring TeamCity's integration.tests task to be dependent on ""main"" continous.build task see here for details: http://confluence.jetbrains.net/display/TCD4/Dependencies+Triggers We run only integration tests (excluding unit tests) by: using separate directory named ""src/it/java"" to keep integration tests excluding by default this source folder from maven-surefire-plugin configuration (configuration/excludes element) using Maven profile called ""integration"" to exclude regular unit tests and include tests from ""src/it/java"" (this profile is configured by passing -Pintegration in integration.tests task). This is the most close pick for me except that we would replace some integration tests to unit tests with mockups."
52,A,"Writing test case for Junit testing As a developer I'm a newbie to Unit testing and have a requirement to write a test case to unit test the following code. Could somebody help me here and also give me some pointers on how to write unit tests in eclipse. private void handle(final DocumentEvent e) { Document doc = e.getDocument(); try { String text = e.getDocument().getText(0 doc.getLength()); if (text.length() >= maxMessageSize) { try { component.getHighlighter() .addHighlight(maxMessageSize text.length() + 1 painter); } catch (BadLocationException ex) { System.out.println(ex.getMessage()); } } else { component.getHighlighter().removeAllHighlights(); } } catch (BadLocationException e1) { System.out.println(e1.getMessage()); } } Thanks Update For some reason when I running the test case I'm not getting any coverage at all. Am I doing something wrong here?? Further researching suggests that I need to use test.perform() method to call the method I want to test.. Is that correct?? Can you please suggest something?? Here is the code: public class TestMaxLength { static final int maxMessageSize = 125; JTextPane textPane = new JTextPane(); //***EasyMock varibles**** private JTextComponent mockComponent; private MaxLength classUnderTest; private DocumentEvent mockEvent; private Document mockDocument; private Highlighter mockHighlighter; @Before public void setUp() { mockComponent = EasyMock.createMock(JTextComponent.class); mockEvent = EasyMock.createMock(DocumentEvent.class); mockDocument = EasyMock.createMock(Document.class); EasyMock.expect(mockEvent.getDocument()).andStubReturn(mockDocument); EasyMock.expect(mockDocument.getLength()).andReturn(256); mockHighlighter = EasyMock.createMock(Highlighter.class); EasyMock.expect(mockComponent.getHighlighter()).andReturn(mockHighlighter); } @Test public void testSetLength() { MaxLength maxListener = new MaxLength(125); maxListener.decorate(textPane); } @Test public void testEmptyText() { EasyMock.expect(mockDocument.getText(0 1)).andStubReturn(""""); mockHighlighter.removeAllHighlights(); EasyMock.replay(mockComponent mockEvent mockDocument mockHighlighter); classUnderTest.handle(mockEvent); EasyMock.verify(mockComponent mockEvent mockDocument mockHighlighter); } } The decorate(JtextComponent jComponent) method is present in the class to be tested (MaxLength) and is defined as : public final void decorate(final JTextComponent c) { //TODO throw exception if already decorating this.component = c; component.getDocument().addDocumentListener(this); } # UPDATE: @Peter: Managed to find out that it is not the Component class that is the problem but instead I needed asm (http://forge.ow2.org/projects/asm). I've also change the code to combine the 2 methods into 1 method: public void testEmptyText() { maxSizeListener.decorate(mockComponent); //mockHighlighter.removeAllHighlights(); EasyMock.replay(mockComponent mockEvent mockDocument mockHighlighter); maxSizeListener.handle(mockEvent); EasyMock.verify(mockComponent mockEvent mockDocument mockHighlighter); } But now I'm having a different error on verify: java.lang.AssertionError: Expectation failure on verify: getHighlighter(): expected: 1 actual: 0 at org.easymock.internal.MocksControl.verify(MocksControl.java:184) at org.easymock.EasyMock.verify(EasyMock.java:2038) at net.TestMaxLength.testEmptyText(TestMaxLength.java:98) This is caused when executing EasyMock.verify() statement on mockComponent. Could you format the code so it's readable (and remove the line numbers)? It must be a trick question because you shouldn't unit test private methods. When you write a unit test you try to test if (in this case) the method does what it is supposed to do. You should not look at the implementation and write your test from that. Instead you should think about what inputs the method should be able to handle and what should be the result (returned value and/or side effects) after the method has been called. Then you should write one or more tests that calls the method with valid and and invalid inputs and make the test confirm that the results matched what you thought would happen. This was a short and incomplete description read more at Wikipedia and junit.org. Here is an old (2005) but working guide to JUnit in Eclipse. Thanks for this. I'm using Cobertura tool to calculate the line and branch coverage and the line numbers were the indications where the code is was not tested. Also the private accessor/visibility for the method was a mistaken and its been changed to public now. Could somebody give me a sample test case for this piece of code. Would be much appreciated. Thanks Do you mean some skeleton code where you can fill in the specifics? Because as I said you can not write a test if you don't know what behavior it should test. I strongly disagree. You describe black box testing while unit testing is white-box testing - you _do_ see the code to be tested as it is (ideally) your own code! It is another matter that you should think about what contract the method is supposed to fulfill and write test cases to verify that. And let me add this: don't write a test *just* to increase your coverage if the test doesn't really test your code. That would be false test coverage indeed. @Péter Török: If you wrote the test yourself that is true of course. But I am quite sure we at SO didn't write the code above and hence we could only write tests that confirm that the code is working if we get no indication of its purpose. And consider test driven development where unit testing is at the core. There you don't even have the code when you write the test. @Péter Török - I know where you're coming from but you should definitely *start* black-box. The goal with a unit test after all is often not so much to ensure that the code works **right now** but rather to have a test in place to check that future modified versions of the code will *continue* working. If you use your white-box view to say ""oh I know this can never be a problem because of the implementation details here so I won't test it"" then what happens when someone refactors the class? @Andrzej @Peter: I see your point and I agree that one should not test _only_ what the code explicitly does right now. That's why I mentioned testing against contract above. And it is true that in pure TDD one writes test first then the code. However in testing legacy code (as I believe is the case in this question) you can't avoid the fact that the code is already written and often lacking any useful documentation you must deduce the supposed ""contract"" from the code itself.  I recommend using a mocking framework such as EasyMock. Mocks allow you to configure dependencies with the desired behaviour for your tests. In your case you need a mock DocumentEvent and ideally another one for component which I guess is a class member. The two aspects to unit testing how to test i.;e. the technical details of assembling the right set of objects in the right state required for the test to run properly (aka the _test fixture) and what to test i.e. the scenarios to validate. How to test Eclipse supports JUnit out of the box so you may quickly generate new JUnit testcases (in Project Explorer context menu: New -> (Other ->) JUnit -> JUnit Test Case) then run it by clicking on the Run button. Setting up the test fixture in your case would look something like this using EasyMock (and assuming you can pass the component as a constructor parameter to your tested class): private Component mockComponent; private ClassUnderTest classUnderTest; private DocumentEvent mockEvent; private Document mockDocument; private Highlighter mockHighlighter; @Before public void setUp() { mockComponent = createMock(Component.class); classUnderTest = new ClassUnderTest(mockComponent); mockEvent = createMock(DocumentEvent.class); mockDocument = createMock(Document.class); expect(mockEvent.getDocument()).andStubReturn(mockDocument); expect(mockDocument.getLength()).andReturn(1); mockHighlighter = createMock(Highlighter.class); expect(mockComponent.getHighlighter()).andReturn(mockHighlighter); } @Test public void testEmptyText() { expect(mockDocument.getText(0 1)).andStubReturn(""""); mockHighlighter.removeAllHighlights(); replay(mockComponent mockEvent mockDocument mockHighlighter); classUnderTest.handle(mockEvent); verify(mockComponent mockEvent mockDocument mockHighlighter); } This test assumes that maxMessageSize is at least 1 by default - setting maxMessageSize up for the test is left to you as an exercise as the code snippet you published gives no clue for that. What to test The method you show gets text from the document associated with the event then based on its length it does different things. I would write at least the following unit tests for this: empty document text with maxMessageSize == 0 empty document text with maxMessageSize > 0 nonempty document text with maxMessageSize == text.length() nonempty document text with maxMessageSize > text.length() nonempty document text with maxMessageSize < text.length() and addHighlight() throwing BadLocationException Notes sensing the BadLocationException is a bit tricky since all it produces is an output to stdout; luckily you can easily reassign stdout via System.setOut. However you may want to consider improving exception handling at least by using a logging framework instead of printing to stdout. from the code it seems that other methods (such as removeAllHighlights() and/or getText()) may also throw BadLocationException however the try-catch blocks are not well organized. I would consider adding more unit tests where those methods throw and after that refactoring the exception handling code. Update I thought there was something wrong that I was doing...Can you please provide me with the modified/corrected code please??? Your testSetLength method is not really testing anything - you need assert statements (and/or EasyMock verification) in order for your unit tests to actually verify some behaviour. However it provides the missing clue for setting up the tested class. So I try to unify your two test methods to create one which is hopefully working (I am writing from memory so I can't guarantee it will all compile and run perfectly at first try) :  @Test public void testEmptyText() { // set up the test class with a specific max length classUnderTest = new MaxLength(125); // this shall be called from inside decorate() mockDocument.addDocumentListener(classUnderTest); // the mock document shall always return an empty text EasyMock.expect(mockDocument.getText(0 1)).andStubReturn(""""); // we expect this to be called from inside handle() mockHighlighter.removeAllHighlights(); // start replay mode EasyMock.replay(mockComponent mockEvent mockDocument mockHighlighter); // inject mock component into tested object maxListener.decorate(mockComponent); // call the tested method classUnderTest.handle(mockEvent); // verify that all expected calls to the mocks have been made EasyMock.verify(mockComponent mockEvent mockDocument mockHighlighter); } @Global glad to hear :-) `expect(mockDocument.getLength()).andReturn(1)` makes the mock document expect a single call to `getLength()` and return 1 from that call. `andStubReturn()` makes the mock return the specified value from any number of subsequent calls to that method. See the section _Specifying Return Values_ in [the EasyMock documentation](http://easymock.org/EasyMock3_0_Documentation.html) for more details. @Global I added the content of your comments as an update to your original post - please do so in the future as code is very badly readable in comments :-) Do I understand correctly that your unit tests actually run and you get the green bar meaning all tests pass but you get no coverage output? @Global I think the coverage configuration is distinct from the unit test configuration but I have never used coverage tools in Eclipse so I can't give you much concrete help with this. Have you rebuilt your project after switching Cobertura on? IMHO the coverage tool (if configured properly) should work fine with vanilla JUnit test methods. I have never heard about that `test.perform()` method and I doubt it would have anything to do with your coverage setup - could you give a reference to the source of this information? @Global the unit test `testSetLength` does not seem to test the method `handle` you showed in your original post. So the mock setup is irrelevant for this test. And even if it were you don't call `EasyMock.replay` so the mocks are still in recording mode during the whole test. @Peter I thought there was something wrong that I was doing...Can you please provide me with the modified/corrected code please??? @Peter I forgot to mention that I separated the test methods into 2 methods one that uses the mock objects and other without any mock objects. I have now added both the methods in the code above..Cld you have a look and let me know what am I doing wrong.. @Global the error message `Expectation failure on verify: getHighlighter(): expected: 1 actual: 0` means that the method `getHighlighter()` was expected to be called once (as was configured in the last statement of your `setUp()` method) but it was not called. Which is strange because in the `handle` method you showed above `component.getHighlighter()` is supposed to be called always (except if an exception was thrown within the method). I suggest you debug the test to figure out what is actually happening inside `handle`. @Global hope that helps. Note that I won't be present on SO very often in the coming weeks so don't be disappointed if you don't get quick answers from me... @Global see my update. @Peter: Could you please advise what could I do here o fix this problem...Thanks Hi Peter thanks for this. But I'm getting a java.lang.NoClassDefFoundError when trying to create a mock Component class. mockComponent = EasyMock.createMock(Component.class); Rest all the createMock statements are working fine. What do you suggest could be causing this!!! Thanks @Global your code above contains `mockComponent = EasyMock.createMock(JTextComponent.class)` isn't it working? In my first example `Component` was just a placeholder name for a class whose real name I didn't know - sorry for not making this clear. I assume from your second code sample that the real type is `JTextComponent`. @Peter: Yes thats right that I'm using JTextComponent but am not able to create the mock object of it.. Its giving me java.lang.NoClassDefFoundError when running the Junit test. Could you please help!!! The decorate(JtextComponent jComponent) method is present in the class to be tested (MaxLength) and is defines as : public final void decorate(final JTextComponent c) { //TODO throw exception if already decorating this.component = c; component.getDocument().addDocumentListener(this); } @Peter: For some reason when I running the test case I'm not getting any coverage at all. Am I doing something wrong here?? Further researching suggests that I need to use test.perform() method to call the method I want to test.. Is that correct?? Can you please suggest soemthing?? Here is the code: public class TestMaxLength { static final int maxMessageSize = 125; JTextPane textPane = new JTextPane(); //***EasyMock varibles**** private JTextComponent mockComponent; private MaxLength classUnderTest; private DocumentEvent mockEvent; private Document mockDocument; private Highlighter mockHighlighter; @Before public void setUp() { mockComponent = EasyMock.createMock(JTextComponent.class); mockEvent = EasyMock.createMock(DocumentEvent.class); mockDocument = EasyMock.createMock(Document.class); EasyMock.expect(mockEvent.getDocument()).andStubReturn(mockDocument); EasyMock.expect(mockDocument.getLength()).andReturn(256); mockHighlighter = EasyMock.createMock(Highlighter.class); EasyMock.expect(mockComponent.getHighlighter()).andReturn(mockHighlighter); } @Test public void testSetLength() { MaxLength maxListener = new MaxLength(125); maxListener.decorate(textPane); } } @ Peter: Pleas see the update in my original question. Wld appreciate your comments.. @Global strange. For us it worked. What do you precisely mean by ""mvn instal didnt work""? Hi Péter Török Could you please give me some instructions to install EasyMock manually. I tried adding this dependency in the POM file as instructed at the Easymock documentation page and then tried running mvn instal but it didnt work. Was I doing something wrong there?? In which case could you let me know how to get this dependecny from teh maven repo.. Also I've downloaded the jar file and was wondering how to install Easymock that way. Thanks. I added the following code to the project POM file at top level: org.easymock easymock 3.0 test There were no further instructions on the EasyMock documentation hence I tried downloading the dependency from the Maven repo by running this command.. I've just started using Maven couple of weeks ago so didnt know what else I needed to do. Thx. @Global by ""top level"" do you mean that it is not inside the `` section? If so move it there and retry `mvn install`. @Global sounds like a classpath problem. Make sure that your test classes also ""see"" the jar containing JTextComponent.class at both compile and runtime. @Péter No I did include the easymock in the section. How can we check that this dependency is actually present and available to us for download? Also I'm trying to imported the EasyMock by having this statement import org.easymock.EasyMock.*; but when using your method (createMock()) is throwing an error.. @Global You can `import static org.easymock.EasyMock.*` to make the code work as it is or you can add the `EasyMock.` qualifier to the individual calls. Sorry I should have mentioned that in the answer :-( @Global for verifying the dependency use the [maven-dependency-plugin](http://maven.apache.org/plugins/maven-dependency-plugin/). I don't have much experience with it but try `mvn dependency:resolve -Dsilent=true`. @Peter do you mean that if I add this import statement I dont have to add EasyMock in the POM file. I'm sorry but i'm really confused and totally lost as to what needs to be done to get this to work. Also what are the prerequisites to make this import statement to work? Do I need to play with the jars or something?? Please advise... @Peter: How can I achieve to make the test classes see the JTextComponent class?? I thought that all Swing text components inherit from the same superclass JTextComponentwhich is made available as part of the java.Swing package.... Please advise.. Finally got EasyMock working. Just used EasyMock.createMock() with the import statement at the top. Looking at your code could you please briefly explain what each statement in the code is doing. I know little about what expect() Verify() replay() methods do but what about andStubReturn() and return() method?? Thanks Thanks all working fine now. @Global it's already there in the `setUp` method :-) @Peter: It was really helpful. Appreciate that!! One thing that I'm not sure is that if mockComponent.getHighlighter() is called in handle() then so I need to add expect for method getHighlighter() on mockComponent too???"
53,A,Passing a service to non-service in grails? I have a non-service class which is defined as such: class A{ B b A( B b ){ this.b = b } } where B is a grails service. In my unit tests I tried this: A a = new A( new B() ); For some reason however b never gets set and the variable b [local the argument to the mehod] isn't even visible in Intelli-J's debugger when running the test. That is I can rename the argument to service and the debugger shows it as undefined. When I try to start a server I get Initialization of bean failed; nested exception is java.lang.reflect.MalformedParameterizedTypeException so I'm assuming this is related. What's going on here? Does this help? http://stackoverflow.com/questions/1703952/grails-how-do-you-unit-test-a-command-object-with-a-service-injected-into-it/ That seems like a strange scenario. What is A doing? Is there any chance that A is really a service? I'd suggest making A a service in which case you can inject the B service into it as per normal usage. I don't think that doing 'new B()' would really be valid unless B has no dependencies on anything (autowired fields). Perhaps injecting B into the test would be better? You're probably right. It looks like A *should* be a service.
54,A,"better jUnit test coverage needed Good evening everyone Suppose your code has the following statement: if (string ends with char 'y') { if ((char before last is not 'e') || (char before last is not 'a')) { return (something awesome); } } So that's simple enough i thought ... Test1: input = ""xy"" (last char is y char before last is not e or a) Result - partial coverage ... What other tests i am missing? If this was and && instead of || i suppose test would be far easier but || confused me a bit. Can you suggest Test23n? Thank you You want to test for the expected behavior with inputs ""ey"" and ""ay"". You may find that your method is not doing quite what you think it is. I think || has indeed confused you a bit.  There's different types of coverage: Method level coverage (what C++ compilers used to do) Line level coverage Symbol level coverage (most C# coverage tools provide this) Branch level coverage (many Java coverage tools provide this) I think your confusion is coming in to play because you are thinking in terms of symbol coverage while your tool is giving you branch coverage. Here's the distinction: Symbol coverage will measure whether you have reached each symbol (i.e. bunch of code up to a ';' character). The below line contains two symbols: int i = 0; int j = 3; Branch coverage measures each condition in both the true and false values. In your example you had 4 different conditions listed each of which have a true branch and a false branch. In order to fully test each branch you need a test for each of the following conditions: string ends with char y char before last is e string ends with char y char before last is a string ends with char y char before last is neither a nor e string does not end with char y With the code you wrote you will probably experience something unexpected. You will get something_awesome no matter what if the string ends in y. If the string ends with 'ey' then it is not ending with 'ay'. If either of those conditions are true you get something awesome. Write the tests and see for yourself.  You can also write tests to make sure you don't return something awesome Test2: input = ""ez"" does not return something awesome Test3: input = ""af"" does not return something awesome The tests should prove your intended behaviour. What about various lengths of strings? (empty '1' '2 ' '3 ') You may want to turn this into a method or function and name it something appropriate. Then write the tests for this simple method (in TDD you would write them first)."
55,A,JUnit testcases for PDF File My java program writes information on PDF form. Does anyone know a good way to write test cases using JUnit to make sure it has printed the required information on PDF form. Use the same library you are using to write to the PDF form to look for the expected results in the pdf form  Do you create the PDF with code you've written or do you use a third-party PDF library? If the latter you shouldn't be testing this. You should assume the library works and just test whether you make the right calls to it by mocking/stubbing it out. I use third-party PDF library. ... if you're writing a unit test. If you're doing integration testing the OP's use case is valid
56,A,Java get thread created by calling main I have a client server application and for testing purposes I need to start the client in a test method by calling Client.main(); That creates some new thread. Now I need to wait until that thread is completed before performing assertions in my test. How do I know when this happens? Alternatively how do I know which thread was started by the call (the client can create other threads too). Hard to answer without knowing what your `Client` does. You can use join() for waiting for other thread to finish execution. The problem here is: you call `join()` on a thread instance and the OP problem is *getting* those instances.  Calling main programmatically won't start a new thread on its own. That would only happen if the main method has code to explicitly start a new thread. If that's the case you should change Client to provide access to this thread in some form so you can call join() on it. (Calling Thread.join() is the standard way of waiting for a thread to finish.) Does Client.main() perform any tasks other than starting a new thread? If not it would probably be simpler for your tests to just call whatever run() method the new thread will end up running and making the test single-threaded as far as possible. Thread.getCurrentThread().join(); @Suresh: Um no. Bad idea. Just to complete the information: Yes the `Client` is extending `Thread`. And yes the `main()` does other things too. Anyway you have been helpful (+1).  Never tried it but it could work in your special case. Thread has a static metod to get stacktraces for all live threads and with this method you get a Set of thread objects. Calling it before and after calling main should allow you to get references to all threads that have been created/started while the main method executed:  Set<Thread> before = Thread.getAllStackTraces().keySet(); Client.main(); Set<Thread> after = Thread.getAllStackTraces().keySet(); Of course identifying your threads is even easier (and more reliable) if your threads are created with unique IDs/names. Now you can calculate the difference and call join on all those threads. It may have a hell of sideeffects but as mentioned above may help in your specific test case. I thought you wanted to wait for *all* those threads to complete?? So it's just one that you're interested in? Give it name get the collection of all threads as mentioned above and look for that (named) thread in that collection. join it and you're done. Yeah I thought of that too. The problem is there can be created more threads in the main() so it will most likely not work. Thanks anyway.
57,A,"Reporting on test scenarios for a non-technical audience? I have (jUnit) unit tests. My project manager would like a human-friendly list of test cases and scenarios (think: a spreadsheet or report for general distribution to a less technical audience). What is an effective way to bridge this gap? Unit tests are for testing software components. A less technical audience is (in my opinion) more interested in functionality than in implemention in features than components. That's the gap you need to bridge: the gap between a component and a feature. The audience may be more interested in system test results than in unit test results; see also Should one test internal implementation or only test public behaviour? People might be interested in lists of tests if the tests a) Test something interesting and b) Have a name which reflects what it's testing. For example there's a list of my test case names here and if you're interested in that functionality I think you can tell from the names of tests approximately what's being tested (and conversely what functionality has been broken if a regression test ever failed). Good food for thought. Perhaps I need to think about the code tests vs. project-level ""test scenarios"" in more detail. Thanks for taking the time to respond. Also ask your boss: what do they want to know when they read the report ... why are they reading the report what are they hoping to get out of it (or why is he giving it to them)? Do they want to know what's broken? Do they want to know what's being worked on? Do they want to see ""velocity""? Do they want to know how long everything will take to fix?  You might want to check out StoryTeller. I haven't used it but as I understand it it's a way to write acceptance tests in non-technical language or at least a domain-specific language (DSL). This interview with Jeremy Miller the creator gives a pretty good description. I'm not sure this is the ""answer"" but it's a lead in the right direction so I'll take it. Thanks."
58,A,"How can I wipe data from my HSQLDB after every test? I had some JUnit tests already written in my project which used to populate data in the setup method. Now I have added maven to my project and I want to execute all test cases form maven i.e. using mvn test. The problem now is that my data base is not cleared after every test class has run. I need to clear the HSQLDB after test cases of each class have run. What we do in all our tests is that we rollback the transaction at the very end of execution (after all assertions are through). We use Spring and by-default tests don't commit at the very end. This ensures that you always return to the starting state of the database (after initial creation of entity tables and running of import.sql). Even if you don't use Spring you can probably roll your own try {} finally {} block to rollback a started transaction for each test. This is a good idea if you are only testing a single transaction. It is possible that the test is encompassing multiple transactions.  Following fredt's advice TRUNCATE SCHEMA PUBLIC RESTART IDENTITY AND COMMIT NO CHECK worked for me. Relevant part of code in the JUnit test for the DAO. @After public void tearDown() { try { clearDatabase(); } catch (Exception e) { fail(e.getMessage()); } } public void clearDatabase() throws Exception { DataSource ds = (DataSource) SpringApplicationContext.getBean(""mydataSource""); Connection connection = null; try { connection = ds.getConnection(); try { Statement stmt = connection.createStatement(); try { stmt.execute(""TRUNCATE SCHEMA PUBLIC RESTART IDENTITY AND COMMIT NO CHECK""); connection.commit(); } finally { stmt.close(); } } catch (SQLException e) { connection.rollback(); throw new Exception(e); } } catch (SQLException e) { throw new Exception(e); } finally { if (connection != null) { connection.close(); } } } According to documentation at http://hsqldb.org/doc/2.0/guide/dataaccess-chapt.html#dac_truncate_statement If RESTART IDENTITY is specified all table IDENTITY sequences and all SEQUENCE objects in the schema are reset to their start values  Another solution is listed in ""Clearing the database between tests"" http://www.objectpartners.com/2010/11/09/unit-testing-your-persistence-tier-code/ that article has a lot of great solutions. thanks  You can clear the data by dropping the schema. The default schema is called PUBLIC. If you execute the SQL satement below it will clear all data and drop all tables. DROP SCHEMA PUBLIC CASCADE Alternatively if you need the table and schema object definitions you can create a file: database containing the objects but no data and add the property below to the .properties file. Using this type of database for tests the changes to data are not persisted files_read_only=true The latest alternative available in HSQLDB 2.2.6 and later allows you to clear all the data in a schema while keeping the tables. In the example below the PUBLIC schema is cleared. TRUNCATE SCHEMA public AND COMMIT This statement has been enhanced in the latest versions of HSQLDB. See http://hsqldb.org/doc/2.0/guide/dataaccess-chapt.html#dac_truncate_statement under Truncate Statement This will delete all the tables in the schema. However my requirement is that I need the tables but no data in them Added alternative approach. #3 works great for me. For various reasons I can't rollback between test methods so I made an @@After @@Transactional method that takes the entity manager and executes this.  I had a simple SQL script that was run before each test with the following statement at the beginning: TRUNCATE SCHEMA public AND COMMIT; but I have run into lock problems between tests and adding this worked for me like a charm: @After public void after() throws Exception { if (entityManager.getTransaction().isActive()) { entityManager.getTransaction().rollback(); } }"
59,A,"Junit and Selenium and tearDown Such a junit : @Test public void testA {...} @Test pulic void testB {...} @After public void closeBrowsers() Exception { selenium.stop(); } Here is the question : closeBrowsers() method called after every test method; in that case it is called twice and i got ""Wrong test finished."" from JUnit. I need a junit method/annotation which will be called after all tests finised (just called once after all tests finished) is it possible ? Also i tried to check if selenium is up or not in closeBrowsers() but no way i couldn't find any solution. P.S : I 've read this one : http://stackoverflow.com/questions/1317844/how-to-close-a-browser-on-a-selenium-rc-server-which-lost-its-client but i couldn't understand the solution and also currently http://www.santiycr.com.ar/djangosite/blog/posts/2009/aug/25/close-remaining-browsers-from-selenium-rc blog side is down You can make your selenium variable static @AfterClass needs static method. Look at that please : public class xxxTest extends SeleneseTestCase { @Test public void testaaa() throws Exception { try { selenium.setSpeed(""100""); ..... } } } If I use @AfterClass then testaaa() needs to be static and in that case i can't use selenium variable... Any solution for this ? Thanks. Use the @AfterClass annotation. http://junit.sourceforge.net/doc/faq/faq.htm#organize_3  You can make your selenium variable static initialize it in @BeforeClass static method and cleanup in @AfterClass: public class ...... { private static Selenium selenium; @BeforeClass public static void initSelenium() { selenium = new DefaultSelenium(...); // or initialize it in any other way } @Test public void testA {...} @Test pulic void testB {...} @AfterClass public static void closeBrowsers() throws Exception { selenium.stop(); } } Thanks......... While that works if all your tests are contained in one class that wont work if you have tests in separate class files (and not using a Suite thereby accepting the default behavior of running all tests in project). For that we would need a AfterSuite annotation or a custom hack to do similar. The question was exactly about using browser in one class if you need to reuse the same browser for multiple tests of course you will need something else - base class for all tests inject browser instance etc - there are so many options"
60,A,Hudson + JUnit + embedded GlassFish how to provide domain configuration? I'm using NetBeans and GlassFish 3.0.1 to create an EJB3 application. I have written a few Unit Tests which get run via JUnit and make use of the embedded GlassFish. Whenever I run these tests on my development machine (so from within NetBeans) it's all good. Now I would like to let Hudson do those tests. At the moment it is failing with lookup failure on a resource (in this case the datasource to a JPA persistance unit): [junit] SEVERE: Exception while invoking class org.glassfish.persistence.jpa.JPADeployer prepare method [junit] java.lang.RuntimeException: javax.naming.NamingException: Lookup failed for 'mvs_devel' in SerialContext After searching around and trying to learn about this I believe it is related to the embedded GlassFish not having been configured with resources. In other words it's missing a domain.xml file. Right? Two questions: Why does it work with NetBeans on my dev box? What magic does NetBeans do in the background? How should I provide the file? Where does the embedded GlassFish on the Hudson-box expect it? Hudson is using the same Ant build-scripts (created by NetBeans). I've read this post about instanceRoot and the EmbeddedFileSystemBuilder but I don't understand enough of that. Is this needed for every TestCase (Emb. GF gets started/stopped for each bean-under-test)? Is this part of EJBContainer.createEJBContainer()? Again why is it not necessary to do this when running tests on NetBeans? Update Following Peter's advice I can confirm: when running ant on a freshly checked out copy of the code with the same properties as hudson is configured the tests get executed! 10-1 it is a classpath issue as IDE's tend to swap paths in and out depending if you run normally or unittests. Try running the tests on a commandline from a freshly checked out version from your SCM. Chances are you'll have the same error. Debugging on your local machine is a lot easier than on a remote machine. When it builds reliably on the command line (in a separate directory) then it is time to move to hudson. thanks for the suggestion! When I run 'ant test' the tests actually work and get executed. Any other idea? @Hank: Did you run it on the Hudson server from the command line? If yes it might be a permission issue. Run your test with the same credential as Hudson is running? @2nd Peter: You da man! Thanks mate that was it. ${GLASSFISH_HOME}/glassfish/domains/domain1/config was not readable to the hudson user. All good now!
61,A,"taking output on console when running junit through ant script i am using ant script and running a junit the problem i am facing is that i am unable to get the output on console rather i get the output in a log file. how can i achieve that.. i am using the following script. <target name=""validate_mapping"" description=""Testing the Hibernate ""> <path id=""validator.classpath""> <fileset dir=""${basedir}\lib""> <include name=""Junit-Temp-Test.jar"" /> </fileset> </path> <junit printsummary=""yes""> <formatter type=""plain""/> <test name=""com.ofss.fc.junit.test.SampleTest"" /> <classpath> <path refid=""validator.classpath"" /> <path refid=""lib.ext.classpath"" /> </classpath> </junit> </target> Please help me. I think you need to specify ""usefile"" attribute <formatter usefile=""false"" type=""plain""/>"
62,A,"What is the recommended way to log data that caused errors in JUnit? I'm relatively new to JUnit and I was writing a few of my first tests today. For some particular method I wanted to pass random values (all of which are within the correct range). If the method fails for any reason I want to know which value caused it to fail. So what's the recommended way of doing this? (Or is it bad to use random values in JUnit tests?) You can try to use: http://www.openfuture.de/Log4Unit/ for logging but I would recommend against random values for unit tests as they are supposed to be repeated. If you want to test a lot of values just use a for loop and some modifications to the index value which is easily repeated. If you think about it there is really no situation where it would be more beneficial to use random values than ""hard coded"" ones. If you want a good spread over a value range you can use a function or use random values with a fixed seed (to get the same numbers). If a test fails you want to be able to fix it and run the test again. That's the problem with random numbers in unit tests. +1. Tests should be ""repeatable""... use a diverse set of values testing boundary conditions as well.. a RowTest should fit your need. Thank you. But testing using random values isn't that bad IMHO as long as it is put to use correctly. After checking for boundary and some middle values a random test can be good at revealing other special cases that I didn't think of when creating the test. ... By logging the seed value the error can be reproduced and thus fixed. Otherwise the error may slip for a very long time. (Which reminds me of the binary search implementation bug :) ). What do you think? It sounds cumbersome. And by your logic you want to find errors my ""accident"". If you use a fixed random value and test 10000 values but the same ones each time it will be as good and repeatable without having to look in log files. Log files in unit tests are more meant to log things that work if you want some additional information as JUnit doesn't give you anything more than passed in those cases. Why is it cubersome? Yes I want to find errors by accident if I couldn't think of tests that would reveal them. Actually I don't intend to use a fixed random value. I've currently implemented it as using System.nanotime() for the seed and showing it in the assert message if it fails. Good that you saw what I meant above (it should have been fixed random seed value[but seed was missing]). I still argue that using the same 10000 values is better than 10000 random as the fixed values are easier to repeat. I think using both in combination may be a good thing but the fixed ones are the most important ones by my logic. Showing info if the assertion fails is certainly the way to go. Those who use JUnit logging are looking for the ability to get more information from the tests that passes something I've never had a need to. Also a note on the above. If I have 10000 random values but the same random values each time. Then it will cover lots of cases where you didn't think of those tests and got them ""by accident"" but it will be the same accident each time. Random and changing values will cover more ground over time sure. I just argue that if you have A: The test values you can think of yourself. B: Random but fixed values and C: Random changing values. Then I think A+B will cover 99.999% of the cases and at the same time be easily repeatable. Of course you could do A+B+C but I don't think the C is worth the extra effort. In either case doing just A+C is something I would never do. As I argue that A+B will cover almost everything and be easier to manage. I think I've explained my opinion now but it's not a science. Good look with your approach! :-)  If your unit test fails for any reason you will see a red traffic light in your test runner. The test runner will also show you which test method failed and the test runner's log reports more detail (the dumped stack trace for example). Investigate that error correct it and your test should never fail again unless you break the code. Thus I don't see the necessity for logging exceptions. You should fix any error (red traffic light) immediately. Using random values can be quite dangerous if you cannot guarantee that your generation of these values is free of errors. Checking the boundary conditions might be more useful. Thank you but I'm not actually trying to log exceptions as much as I am trying to create reproducible pseudo-random test cases. Check my comment on @Kent's answer or the link to Fuzzy Testing in @ShuggyCoUk's answer.  Just report the actual and expected values in the ""diagnostic message"" parameter of your assertions. That's the common practice for JUnit tests and the ""helper"" assert methods tend to do this by default e.g. assertEquals will say something like ""expected 6 and got 7"". Random values are OK as long as you can ensure that the random range constitutes an equivalence class for the code under test. Thank you. This is what I was looking for.  You can have repeatable random values by providing a constant seed to the random number generator. In Java create a java.util.Random with a fixed seen and pass to the class as a constructor parameter (dependency injection). Like this: new ClassUnderTest(new Random(123L)); Depending on what you are testing you might also separate the generating of the random values from their use. If you have class X which takes random values from range 1 to 10 then you can test it by passing it the edge values 1 and 10 and some value from the middle such as 4. Then you need another test for the producer of those random values. For example give it a java.util.Random with a fixed seed and generate 100 values and check that all of them are within the allowed range.  I would propose parameterized test cases. so you can use random values (in the Data method) and it's ""logged"" in your runner if any method will fail.  If you really want to use random values then simply place the value in use in the textual part of the assert methods. Then if an assert is blown the input value will be present and you can investigate why it had a problem. This is Fuzz Testing and is a powerful technique but is most useful when you don't have the available source code or when testing a system with complex internal state and multiple interactions. A more useful type of testing for you may be white box testing where test inputs are deliberately selected to cover the various classes of input you might get. JTest appears to be an automated tool for this in java. MS Research supplies PEX for c#). Simply using a coverage tool and validating that you cover the relevant paths is normally sufficient if doing it by hand though the boundary cases supplied by the automated tools are often instructive. Thanks a lot. This is very insightful information.  Have you tried using the 'assertThat' methods and Hamcrest Matchers that are part of JUnit 4.4+? Check out the README [1] and search for 'assertThat'. I've become quite fond of the much more semantic look of the code and the failure messages are much more informative. [1] http://junit.sourceforge.net/README.html Thank you. This is indeed useful information. I didn't try these methods because I was using JUnit 4.3.1 (which comes bundled with Eclipse Ganymede). I'll definitely try the new version."
63,A,"assertEquals doesn't work without second parameter casting Folks why I'm getting the ""The method assertEquals(String Object Object) is ambiguous for the type DictionaryTest"" error for this JUnit test? @Test public void testEditCard() { Integer a = 10; Integer b = 12; Integer c = 2; assertEquals(""test"" a-b c); } Adding casting assertEquals(""test"" (Integer)(a-b) c); resolves the problem. Because of the wonders of autoboxing and -unboxing: assertEquals(""test"" /* this is an int */ a-b /* this is an Integer */ c); Can be evaluated as assertEquals(String long long); // in this case the second parameter is unboxed // (and the first one silently casted) or as assertEquals(String Object Object); // in this case the first parameter is boxed If you declare all variables as int (not Integer) there should be no ambiguity. I have the same problem and I need Integer(not int). Which assertequals should I use??  It's because the compiler can't tell if you want to call assertEquals(String Object Object) or assertEquals(String long long). Since a-b and c can be automatically coerced to long the compiler sees an ambiguity. Your explicit cast tells the compiler that you want the Object version. Note that in this case you could use int rather than Integer variables which would also fix the ambiguity. Looking at the JUnit API it seems there is no `assertEquals(String int int)` which seems odd...have I missed something? Did the JUnit people really decide to force all `int` checks to be performed using `long`s? Or have I totally misread the API on http://www.junit.org/apidocs/org/junit/Assert.html? ints can be substituted for longs just not the other way round. Yes I know it just seems like a strange design decision. Integer arithmetic is often faster than long arithmetic although I guess that will become less relevant as 32-bit systems become less common. I suppose for unit testing efficiency is not so much of an issue."
64,A,"Why is Eclipse using JUnit 3 when I have junit-4.3.1.jar in my build path? I'm using Ganymede on Ubuntu Linux and I have junit-4.3.1.jar in my build path. I click File > New > Java > ""JUnit 4 Test Case"" when I create a new test but when it's run Eclipse appears to be using JUnit 3. I believe this is the case because it's ignoring my annotations. When I remove all test* methods JUnit complains ""No tests found"". If I do the same with another test I already created JUnit runs fine and finds my test named createTask(). So it appears to be using JUnit 3 at times and JUnit 4 at others. How do I force Eclipse Ganymede to use JUnit 4? I have 2 suggestions that might help. First in the Run/Debug configuration Make sure the JUnit 4 test runner is selected. Next if this is a plugin project add the junit4 runtime to the project as a dependency (in the dependencies tab). JUnit 4 is test runner that is selected. And this is a standard Java project; builds to a JAR. How are you launching? Are you creating a run or debug configuration and clicking on it each time or right-click and run/debug as on the test? right-click either on the file in Project Explorer or in the editing pane. Try setting up a debug or run configuration that you can reuse. Use the down arrow by the debug or run buttons and do a new Junit configuration. Then pick JUnit 4. When you want to re-run it select the configuration from the drop down.  I figured out the answer via this link: Spring is apparently incompatible with JUnit 4.3.1 when extending AbstractTransactionalSpringDataSourceTest (or whatever it is). So I upgraded to JUnit 4.6 (b/c 4.5 has issues) and used the annotation-based configuration and voila all is well. Thank you."
65,A,"Drools testing with junit What is the best practice to test drools rules with junit? Until now we used junit with dbunit to test rules. We had sample data that was put to hsqldb. We had couple of rule packages and by the end of the project it is very hard to make a good test input to test certain rules and not fire others. So the exact question is that how can I limit tests in junit to one or more certain rule(s) for testing? Thanks for help Hubidubi I created simple library that helps to write unit tests for Drools. One of the features is exactly what you need: declare particular drl files you want to use for your unit test: @RunWith(DroolsJUnitRunner.class) @DroolsFiles(value = ""helloworld.drl"" location = ""/drl/"") public class AppTest { @DroolsSession StatefulSession session; @Test public void should_set_discount() { Purchase purchase = new Purchase(new Customer(17)); session.insert(purchase); session.fireAllRules(); assertTrue(purchase.getTicket().hasDiscount()); } } For more details have a look on blog post: http://maciejwalkowiak.pl/blog/2013/11/24/jboss-drools-unit-testing-with-junit-drools/  Personally I use unit tests to test isolated rules. I don't think there is anything too wrong with it as long as you don't fall into a false sense of security that your knowledge base is working because isolated rules are working. Testing the entire knowledge base is more important. You can write the isolating tests with AgendaFilter and StatelessSession StatelessSession session = ruleBase.newStatelessSesssion(); session.setAgendaFilter( new RuleNameMatches(""<regexp to your rule name here>"") ); List data = new ArrayList(); ... // create your test data here (probably built from some external file) StatelessSessionResult result == session.executeWithResults( data ); // check your results here. Code source: http://blog.athico.com/2007/07/my-rules-dont-work-as-expected-what-can.html  Do not attempt to limit rule execution to a single rule for a test. Unlike OO classes single rules are not independent of other rules so it does not make sense to test a rule in isolation in the same way that you would test a single class using a unit test. In other words to test a single rule test that it has the right effect in combination with the other rules. Instead run tests with a small amount of data on all of your rules i.e. with a minimal number of facts in the rule session and test the results and perhaps that a particular rule was fired. The result is not actually that much different from what you have in mind because a minimal set of test data might only activate one or two rules. As for the sample data I prefer to use static data and define minimal test data for each test. There are various ways of doing this but programmatically creating fact objects in Java might be good enough. yes I know how rule execution works. This is the way we do it now. My problem is with this approach that it is very hard to make enough and appropriate test data. Because we don't limit the runnable rules any other rules can run and change the final result. So it's hard to predict the final result for asserts. That was the reason why I thought that it would be better to test rules izolated. I guess I was trying to say that the fact that 'any other rules can run and change the final result' is exactly why testing a rule in isolation is less meaningful.  A unit test with DBUnit doesn't really work. An integration test with DBUnit does. Here's why: - A unit test should be fast. -- A DBUnit database restore is slow. Takes 30 seconds easily. -- A real-world application has many not null columns. So data isolated for a single feature still easily uses half the tables of the database. - A unit test should be isolated. -- Restoring the dbunit database for every test to keep them isolated has drawbacks: --- Running all tests takes hours (especially as the application grows) so no one runs them so they constantly break so they are disabled so there is no testing so you application is full of bugs. --- Creating half a database for every unit test is a lot of creation work a lot of maintenance work can easily become invalid (with regards to validation which database schema's don't support see Hibernate Validator) and ussually does a bad job of representing reality. Instead write integration tests with DBunit: - One DBunit the same for all tests. Load it only once (even if you run 500 tests). -- Wrap each test in a transaction and rollback the database after every test. Most methods use propagation required anyway. Set the testdata dirty only (to reset it in the next test if there is a next test) only when propagation is requires_new. - Fill that database with corner cases. Don't add more common cases than are strictly needed to test your business rules so ussually only 2 common cases (to be able to test ""one to many""). - Write future-proof tests: -- Don't test the number of activated rules or the number of inserted facts. -- Instead test if a certain inserted fact is present in the result. Filter the result on a certain property set to X (different from the common value of that property) and test the number of inserted facts with that property set to X."
66,A,Detecting the running of all tests versus one test or a class from eventTestPhaseStart? I want this: eventTestPhaseStart = { name -> ant.sequential { clean() } } to only run when I run grails test-app to test all tests at once and NOT before each test is run individuall or as a class (like from an IDE). Is there an easy way to modify this to detect this scenario? This currently isn't possible.  When looking at $GRAILS_HOME/scripts/_GrailsTest.groovy the event to hook onto should be TestPhasesStart instead of TestPhaseStart (not the plural form). This still seems to run when running individual tests through IntelliJ.
67,A,"C# testing framework that works like JUnit in Eclipse? I come from a Java/Eclipse background and I fear that I am spoiled by how easy it is to get JUnit and JMock running in Eclipse and have that GUI with the bar and pass/fail information pop up. It just works with no hassle. I see a lot of great options for testing in C# with Visual Studio. NUnit looks really nice because it contains unit and mock testing all in one. The trouble is I can't figure out how to get the IDE display my results. The NUnit documentation seems to show that it doesn't automatically show results through the VS IDE. I found http://testdriven.net/ which seems to trumpet that is makes VS display these stats and work with multiple frameworks but it isn't open source. Is there anyway to get unit and mock testing working with the VS IDE like it does in Java with Eclipse? The ""personal"" edition is free. If you are looking for something like Eclipse/JUnit you shouldn't have tried Microsoft product line. But the good news is that SharpDevelop has such nice integration with NUnit and it is open source. However it aims as an alternative to VS not an addon for VS.  On installing NUnit you get an NUnit.exe - use this to open and run your tests. It has an UI and shows pass/fails and shows output. You can add a build action in Visual Studio that on a specific testing configuration will build then immediately invoke NUnit on that dll. EDIT: (more details) In test project: Project Properties -> Debug (set a build configuration - I use ""NUnitDebug"") Start Action -> ""Start external program"": C:\Program Files\NUnit 2.5.3\bin\net-2.0\nunit.exe (use your own path) Start Options -> Command line arguments: MyTestProject.dll (replace with the name of your DLL) EDIT2: As brendan said Moq is a good mock framework that can be used. Glad you got it don't forget to mark as answer :) I got it to work thanks a ton I would have been lost forever without these instructions :)  Resharper will let you do this and has a nice UI. I believe the core of it is NUnit. For the mock stuff you'll want to use Moq. Resharper is not free/open source but is so worth the price. Sorry dude could've sworn I typed an 'e'. Disconnect between fingers and brain there.  You could read ASP.NET MVC Test Framework Integration Walkthrough and run your tests from the VS test runner.  Have you tried using the Testing projects in Visual studio? They're practically identical to nUnit and can be run simply by hitting F5. For mocking chose whichever suits you We're looking at Moq for Silverlight support. MSTest is not available in all VS editions."
68,A,"Is it possible to programmatically generate JUnit test cases and suites? I have to write a very large test suite for a complex set of business rules that are currently captured in several tabular forms (e.g. if parameters X Y Z are such and such the value should be between V1 and V2). Each rule has a name and its own semantics. My end goal is to have a test suite organized into sub test suites with a test case to each rule. One option is to actually hard code all these rules as tests. That is ugly time consuming and inflexible. Another is to write a Python script that would read the rule files and generate Java classes with the unit tests. I'd rather avoid this if I can. Another variation would be to use Jython. Ideally however I would like to have a test suite that would read the files and would then define sub-suites and tests within them. Each of these tests might be initialized with certain values taken from the table files run fixed entry points in our system and then call some validator function on the results based on the expected value. Is there a reasonable way to pull this off using only Java? Update: I may have somewhat simplified our kind of rules. Some of them are indeed tabular (excel style) others are more fuzzy. The general question though remains as I'm probably not the first person to have this problem. Very interested to find out about this; I need to learn more about test cases. We tried FIT and decided to go with Concordion. The main advantages of this library are: the tests can be checked in alongside the code base (into a Subversion repository for example) they are executed by a standard JUnit runner  I wrote something very similar using JUnit. I had a large number of test cases (30 pages) in an XML file. Instead of trying to generate different tests I did it all in a single test which worked just fine. My test looked something like this: void setup() { cases = read in the xml file } void test_fn_works() { for case in cases { assert(case.expected_result fn(case.inputs) 'Case ' + case.inputs + ' should yield ' + case.expected_result); } } With Ruby I did exactly what you are saying-- generating tests on the fly. Doing this in Java though is complex and I don't think it is worth it since there is another quite reasonable approach. Hope this helps.  Within JUnit 4 you will want to look at the Parameterized runner. It was created for the purpose you describe (data driven tests). It won't organize them into suites however. In Junit 3 you can create TestSuites and Tests programatically. The answer is in Junit Recipes which I can expand if you need it (remember that JUnit 4 can run Junit 3 tests).  Have you considered using FIT for that? You seem to have the tables already ready and ""business rules"" sounds like ""business people write them using excel"". FIT is a system for checking tests based on tables with input->expected output mappings and a open source java library for running those tests is available. Also there are Java drivers for it. Thats what i was going to say; your case sounds exactly like the kind of thing fit was meant to solve; i recommend fitnesse (http://fitnesse.org) as that is the most updated and current implementation i can think of. Thank you. I'm familiar with FIT but wasn't familiar with the new implementations; I'll check them out. Still I wonder if there's a programattic way to stay within JUnit."
69,A,"Is there a way to ""fail fast"" for junit with the maven surefire plugin? I'm currently working on a java project using maven. We use the maven surefire plugin to run our junit suite as part of the build process. Our test suite is rapidly growing in both coverage and execution time. The execution time is very frustrating and time consuming when you end up waiting ten minutes to find out that a test failed in the first minute of testing. I would like to find a way to make the build process fail upon the first error/failure in the test suite. I know that this is possible for other build tools but I have been unable to find a way to do this with maven surefire. I know that there is an unresolved ticket for this functionality in the surefire jira but I'm hoping that there is an existing solution out there for this. I'm afraid that the answer is probably ""No there isn't."" Its seems to be the default behaviour (fail fast) with Maven 3. I've just encountered this today by luck. Or am i wrong ?  A few ways to speed it up if not exactly what you need: If it's a multi module build add --fail-fast to the command line to drop out after the first module. Look into failsafe to move long running integration tests onto a different phase of the lifecycle. Look into a profile based solution for fast and slow tests - Is there a way to tell surefire to skip tests in a certain package?.  As far as I know no and this really requires the resolution of SUREFIRE-580. If you want to make this happen faster you should at least vote for the issue and optionally submit a patch ;)  There may be a suitable workaround but it depends on your requirements and you need to use a CI server which can handle jvm process return codes. The basic idea is to stop Maven's JVM process altogether and let the OS know that the process has stopped unexpectedly. Then a continuous integration server like Jenkins/Hudson should be able to check for a non-zero exit code and let you know that a test has failed. The first step is to make surefire exit the JVM at the first test failure. You can do that with JUnit 4.7 or higher by using a custom RunListener (put it in src/test/java): package org.example import org.junit.runner.notification.Failure; import org.junit.runner.notification.RunListener; public class FailFastListener extends RunListener { public void testFailure(Failure failure) throws Exception { System.err.println(""FAILURE: "" + failure); System.exit(-1); } } Then you need to configure that class so surefire will register it with the JUnit 4 Runner. Edit your pom.xml and add the listener configuration property to the maven-surefire-plugin. You will also need to configure surefire to not fork a new JVM process for executing the tests. Otherwise it will just go on with the next test cases. <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.10</version> <configuration> <forkMode>never</forkMode> <properties> <property> <name>listener</name> <value>org.example.FailFastListener</value> </property> </properties> </configuration> </plugin> If this does not help I'd try to fork the maven surefire junit provider plugin. Btw unit tests by definition should run faster than 0.1 seconds. If your build really takes so long due to the unit tests you will have to make them run faster in the future. excellent solution. Btw it looks like the never restriction can be relaxed by running maven with the -ff option. After System.exit() surefire will immediately abort the build with a message ""goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test failed: The forked VM terminated without saying properly goodbye. VM crash or System.exit called ?""  This doesn't solve the question exactly but the solution that my workplace ultimately came up with was to use Atlassian's Clover to run specialized builds of just tests pertaining to changed code. We have a Clover build that runs the tests for the changed codes and in turn kicks off the full test build. This has proven to be a satisfactory solution."
70,A,"How to make log4j error() calls throw an exception in jUnit tests? I have a Java project being tested with JUnit (mix of Junit 3 and 4 style) where the classes under test might log a log4j error. I would like to make a unit test fail if such an error is logged. Is there a generic way to configure either log4j or the unit test infrastructure to make any call to a log4j error() method in the code under test throw a runtime exception and therefore fail the test? AOP might be one way but I'm interested in other possibilities too. The intention here is to weed out places in code where log4j error() is being used incorrectly. That is when an error is logged but no exception or error handling has occurred either it's not really an error or it is and should be raised. eg: public class MyTest extends TestCase { public void testLogAnError() { // Want to make this fail new MyClass().logAnError(); } } public class MyClass() { static Logger logger = Logger.getLogger(""foo""); public void logAnError() { // I'm logging an error but not doing anything about it logger.error(""Something bad or is it?""); // TODO throw an exception or don't log an error if there isn't one } } Update: This is what the solution I'm using currently looks like. It is based on sbridges' answer (added to test class): private static final Appender crashAndBurnAppender = new NullAppender () { public void doAppend(LoggingEvent event) { if(event.getLevel() == Level.ERROR) { throw new AssertionError(""logged at error:"" + event.getMessage()); } } }; Then in setUp: Logger.getRootLogger().addAppender(crashAndBurnAppender); And tearDown: Logger.getRootLogger().removeAppender(crashAndBurnAppender); You can create a new Appender that throws an AssertionError when you log at error level. Something like class TestAppender extends AppenderSkeleton { public void doAppend(LoggingEvent event) { if(event.getLevel() == Level.Error) { throw new AssertionError(""logged at error:"" + event.getMessage()); } } } In your test do Logger.getRootLogger().addAppender(new TestAppender()); Edit : as Ralph pointed out remove the TestAppender after you finish the test. Brilliant! I've got something like this up and running... You need to undo `Logger.getRootLogger().addAppender(new TestAppender());` after the test! Because the Logger is static so this appender will also be presend in the next test if it is not removed! - You must do this in a `@After` Method or a finally statement because the ""normal"" code in your test will not be executed after the exception."
71,A,"Comparing unicode characters in Junit Hi I had problems in some flow with unicode chars in some of my flows. So i fixed the flow and added a test. assertEquals(""Björk"" buyingOption.getArtist()); the buyingOption.getArtist() will return the same name that is on here is a snippet : but junit will fail with the message : junit.framework.ComparisonFailure: null Expected :Bj?rk Actual :Bj?rk at com.delver.update.system.AECSystemTest.basicOperationtsTest1(AECSystemTest.java:40) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) I agree with Grodriguez but would like to suggest you to change your default encoding to UTF-8 and forget about this kind of problems. How to do this? It depends on your IDE. For example in Eclipse go to Window/Preferences then type ""encoding"" choose Workspace and change encoding to UTf-8 McDowell: Changing your default encoding is fine as long as you work alone or at least in a ""controlled"" environment. However things get more difficult if you are part of a team as everyone might be working on a different environment. Using Unicode escapes in your source files is fool-proof -- file encodings do not matter any more regardless of what editor / compiler / setup you use. Ok I guess then there is some configuration for maven junit  or the compiler plugin. @Roman - in your Maven pom.xml you need to specify the encoding of your source files to match your editor. This will ensure consistent compilation in any environment. http://docs.codehaus.org/display/MAVENUSER/POM+Element+for+Source+File+Encoding This is one of those fundamental Java design flaws: there is no place to specify the (ASCII-compatible) encoding in the source file itself. It is insane that we are expected to stick to 7 bits when the language has 16 bit chars plus a nasty hack for the rest of them.  I found the solution was to change the default encoding before running mvn test My fix to this issue was to set the ENV var JAVA_TOOL_OPTIONS before running export JAVA_TOOL_OPTIONS=""$JAVA_TOOL_OPTIONS -Dfile.encoding=UTF8"" mvn test  This is probably due to the default encoding used for your Java source files. The ö in the string literal in the JUnit source code is probably being converted to something else when the test is compiled. To avoid this use Unicode escapes (\uxxxx) in the string literals in your JUnit source code: assertEquals(""Bj\u00F6rk"" buyingOption.getArtist());"
72,A,Selenium fails to kill browser when test fails How do I setUp selenium to kill the test browser page on occasions where test fails. Currently when running selenium test cases and a test fails the browser page stays open and that causes problems when large number of tests is failing. Interestingly enough it isnt the case when the test passes. Any suggestion?? when you start the test does it open a new browser or does it use one that's already open? If the latter you may not have the browser configured correctly. I think you have to configure the browser to open new pages in a new window. This might affect the close too. Thanks for the quick response. Do you mean when i start the test or when each test case runs? Each test case currently opens a new browser and closes at the end of the test. Currently as mentioned above it only closes when tests pass but stays open when it fails. You should call selenium.stop() of course :) It sounds like you need a try/finally block duh! thanks a bunch  Its not quite that simple in my experience anyway (I'm new to selenium and JUnit). It depends how the failure occurs and what you do with it. JUnit should automatically call tearDown() when a test fails and selenium.stop() should be in this. However sometimes tearDown() isn't called for instance when the error occurs in setUp() or if you're doing something sill when a failure occurs.
73,A,"Parameterized test case classes in JUnit 3.x I have a JUnit 3.x TestCase which I would like to be able to parameterize. I'd like to parametrize the entire TestCase (including the fixture). However the TestSuite.addTestSuite() method does not allow be to pass a TestCase object just a class:  TestSuite suite = new TestSuite(""suite""); suite.addTestSuite(MyTestCase.class); I would like to be able to pass a parameter (a string) to the MyTestCase instance which is created when the test runs. As it is now I have to have a separate class for each parameter value. I tried passing it an anynomous subclass:  MyTestCase testCase = new MyTestCase() { String getOption() { return ""some value""; } } suite.addTestSuite(testCase.getClass()); However this fails with the assertion:  ... MyTestSuite$1 has no public constructor TestCase(String name) or TestCase()` Any ideas? Am I attacking the problem the wrong way? To answer ""Am I attacking the problem the wrong way?"" it would be helpful to know why you'd like to have a parameterized test case ... perhaps there's another way to go about solving your problem. I'm testing a API layer which can work against multiple backend implementations. I want to run the same test against all defined backends (currently only two but that will change). If this is Java 5 or higher you might want to consider switching to JUnit 4 which has support for parameterized test cases built in. Yes I know. Unfortunately the Ant Build File export does not support JUnit 4.x so I have to choose automated builds and JUnit 4.x :-( I'm talking about Eclipse I forgot to say. Ah OK. We don't use Ant here but we do use Eclipse and JUnit 4.x. I'm in the same situation except with Android. Android only supports JUnit 3.  Rather than create a parameterized test case for the multiple/different backends you want to test against I would look into making my test cases abstract. Each new implementation of your API would need to supply an implementing TestCase class. If you currently have a test method that looks something like public void testSomething() { API myAPI = new BlahAPI(); assertNotNull(myAPI.something()); } just add an abstract method to the TestCase that returns the specific API object to use. public abstract class AbstractTestCase extends TestCase { public abstract API getAPIToTest(); public void testSomething() { API myAPI = getAPIToTest(); assertNotNull(myAPI.something()); } public void testSomethingElse() { API myAPI = getAPIToTest(); assertNotNull(myAPI.somethingElse()); } } Then the TestCase for the new implementation you want to test only has to implement your AbstractTestCase and supply the concrete implementation of the API class: public class ImplementationXTestCase extends AbstractTestCase{ public API getAPIToTest() { return new ImplementationX(); } } Then all of the test methods that test the API in the abstract class are run automatically. This is actually what I'm doing right now and it's ok as long as I only have a one or two testcases but for each new test case I need one class per backend which doesn't scale (eventually the number of backends will be like 15-20). Is it possible to have the N test cases all refer to the same method for returning the concrete implementation? Perhaps then each implementation could still just implement that one method. But I guess you might be testing other classes in the API as well... which makes it more complicated  a few details are not perfect such as the names of the tests in the IDE being the same across parameter sets (JUnit 4.x appends [0] [1] ...). To solve this you just need to overwrite getName() and change the constructor in your test case class:  private String displayName; public ParameterizedTest(final int value) { this.value = value; this.displayName = Integer.toString(value); } @Override public String getName() { return super.getName() + ""["" + displayName + ""]""; }  Ok here is a quick mock-up of how JUnit 4 runs parameterized tests but done in JUnit 3.8.2. Basically I'm subclassing and badly hijacking the TestSuite class to populate the list of tests according to the cross-product of testMethods and parameters. Unfortunately I've had to copy a couple of helper methods from TestSuite itself and a few details are not perfect such as the names of the tests in the IDE being the same across parameter sets (JUnit 4.x appends [0] [1] ...). Nevertheless this seems to run fine in the text and AWT TestRunners that ship with JUnit as well as in Eclipse. Here is the ParameterizedTestSuite and further down a (silly) example of a parameterized test using it. (final note : I've written this with Java 5 in mind it should be trivial to adapt to 1.4 if needed) ParameterizedTestSuite.java: package junit.parameterized; import java.lang.reflect.Constructor; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; import java.lang.reflect.Modifier; import java.util.ArrayList; import java.util.Collection; import junit.framework.Test; import junit.framework.TestCase; import junit.framework.TestSuite; public class ParameterizedTestSuite extends TestSuite { public ParameterizedTestSuite( final Class<? extends TestCase> testCaseClass final Collection<Object[]> parameters) { setName(testCaseClass.getName()); final Constructor<?>[] constructors = testCaseClass.getConstructors(); if (constructors.length != 1) { addTest(warning(testCaseClass.getName() + "" must have a single public constructor."")); return; } final Collection<String> names = getTestMethods(testCaseClass); final Constructor<?> constructor = constructors[0]; final Collection<TestCase> testCaseInstances = new ArrayList<TestCase>(); try { for (final Object[] objects : parameters) { for (final String name : names) { TestCase testCase = (TestCase) constructor.newInstance(objects); testCase.setName(name); testCaseInstances.add(testCase); } } } catch (IllegalArgumentException e) { addConstructionException(e); return; } catch (InstantiationException e) { addConstructionException(e); return; } catch (IllegalAccessException e) { addConstructionException(e); return; } catch (InvocationTargetException e) { addConstructionException(e); return; } for (final TestCase testCase : testCaseInstances) { addTest(testCase); } } private Collection<String> getTestMethods( final Class<? extends TestCase> testCaseClass) { Class<?> superClass= testCaseClass; final Collection<String> names= new ArrayList<String>(); while (Test.class.isAssignableFrom(superClass)) { Method[] methods= superClass.getDeclaredMethods(); for (int i= 0; i < methods.length; i++) { addTestMethod(methods[i] names testCaseClass); } superClass = superClass.getSuperclass(); } return names; } private void addTestMethod(Method m Collection<String> names Class<?> theClass) { String name= m.getName(); if (names.contains(name)) return; if (! isPublicTestMethod(m)) { if (isTestMethod(m)) addTest(warning(""Test method isn't public: ""+m.getName())); return; } names.add(name); } private boolean isPublicTestMethod(Method m) { return isTestMethod(m) && Modifier.isPublic(m.getModifiers()); } private boolean isTestMethod(Method m) { String name= m.getName(); Class<?>[] parameters= m.getParameterTypes(); Class<?> returnType= m.getReturnType(); return parameters.length == 0 && name.startsWith(""test"") && returnType.equals(Void.TYPE); } private void addConstructionException(Exception e) { addTest(warning(""Instantiation of a testCase failed "" + e.getClass().getName() + "" "" + e.getMessage())); } } ParameterizedTest.java: package junit.parameterized; import java.util.Arrays; import java.util.Collection; import junit.framework.Test; import junit.framework.TestCase; import junit.parameterized.ParameterizedTestSuite; public class ParameterizedTest extends TestCase { private final int value; private int evilState; public static Collection<Object[]> parameters() { return Arrays.asList( new Object[] { 1 } new Object[] { 2 } new Object[] { -2 } ); } public ParameterizedTest(final int value) { this.value = value; } public void testMathPow() { final int square = value * value; final int powSquare = (int) Math.pow(value 2) + evilState; assertEquals(square powSquare); evilState++; } public void testIntDiv() { final int div = value / value; assertEquals(1 div); } public static Test suite() { return new ParameterizedTestSuite(ParameterizedTest.class parameters()); } } Note: the evilState variable is just here to show that all test instances are different as they should be and that there is no shared state between them."
74,A,"How do you unit test Java EE code? I want to ask for your prefered way to test Java EE code? I found only three project that are trying to help to code unit tests in Java EE environment: http://jakarta.apache.org/cactus/ : Last Published: 2009-01-18 http://www.junitee.org/ : Last Release: 2004-12-11 http://ejb3unit.sourceforge.net/ : Last Release: 2008-05-17 So I wonder is there any framework helping to write (j) unit test for Java EE code? do you use embedded Java EE servers like jboss or glassfish v3? do you mockup and inject by yourself? Thanks a lot... See http://stackoverflow.com/questions/1733805/where-can-i-find-good-unit-testing-resources-for-ejb-and-j2ee Antoher out of date project: http://sourceforge.net/projects/mockejb/ Last Release: 2004-09-04 There's [this article](http://www.oracle.com/technetwork/articles/java/unittesting-455385.html) by Adam Bien about unit testing for JavaEE from 2011. I've been facing the same problem of running integration tests based on JUnit in a Java EE 6 container (Glassfish v3 to be precise) and after a lot of browsing and searching I could not find a solution that really suited me needs so I wrote my own now published as jeeunit on Google Code. I wouldn't call it a test framework it is really just a handful of classes providing the glue between JUnit and Embedded Glassfish. The general idea is similar to Cactus your tests run in the container and get triggered by a servlet from outside. jeeunit supports JUnit 4 Glassfish v3 CDI and generates the standard XML JUnit reports just like Ant or Maven Surefire (in fact I reused some code from Ant for generating the reports).  I had a requirement to test a CDI application and wrote a custom JUnit runner that runs everything outside of the web container. http://jglue.org/cdi-unit/ It is suitable for Java SE and also supports dummy Request Session and Conversation scopes for testing web apps. It's small and fast which is great when you have lots of unit tests.  As you are interested in unit testing I recommend JUnit. You can unit test the methods in the core classes. If you have difficulty in writing unit test cases using JUnit then probably the design is not modular and it is highly coupled. First focus on your core functionality and test it using JUnit.  If by Unit Testing you mean... unit testing (testing a unit in isolation) then you actually don't need any particular framework since EJB3.0 are nothing more than annotated POJOs and thus can be relatively easily tested without any special fixture. Now if you mean something else - like Integration Testing or Functional Testing - then yes tools can help and simplify things (but you should really start to use the right terminology :) I'll assume that this is what you have in mind. First JUnitEE seems dead and obsolete and I'm not even sure it has anything for EJB3.x. Second I'm not impressed by the Java EE 5 support of Cactus and having to deploy Cactus tests is painful (I think that Cactus was nice for J2EE 1.4 but is a bit outdated now). So this leaves us with Ejb3Unit which is in my opinion the best option especially if you want to run out of container tests i.e. without really deploying the application (much faster). If you want to run in container tests then you could indeed use an embedded container and my current preference goes to GlassFish v3 even for Java EE 5 (I may be wrong but I'm pretty disappointed by the starting time of the latest JBoss releases so it isn't getting much of my attention). See the post GlassFish Embedded Reloaded an appserver in your pocket for sample code (that you could use from your tests) or Using maven plugin for v3 embedded glassfish (if you are using maven). Another option would be to package and deploy your application with Cargo and then run some tests against the deployed application (with Selenium or a BDD tool for example). This could be useful if you want to run end-to-end tests with a container that doesn't provide any embedded API. So to answer your last question I would indeed use available tools maybe a combination of them for tests that are not unit tests and wouldn't mock/inject stuff myself except if they don't cover some needs that I can't think of right now. Thanks a lot. I think I will create cunstructors for all my EJB pojos to allow injection without any reflection. Cargo ist intresting for integration test. Thanks! Is there any easy Mocking/Simulating for JMS TimerService and SessionContext? I heard Cactus project is back to life with new lead developer Petar Tahchiev. JSFUnit is based on Cactus. @cetnar Oh good to know! Actually I was maybe a bit too much focused on EJB3 and totally forgot JSF. Thank you very much for this information. ""and thus can be relatively easily tested without any special fixture"": but not EJBs that have a `@PersistenceContext` `EntityManager`? @PascalThivent: ""2011/08/05 - Jakarta Cactus has been retired."""
75,A,In-MemoryDB: create schema in 'setUp()' of Unit Testing: Netbeans (6.5.1) Hibernate (3) Junit(3) HSQL (1.8) What are the steps needed to setup an in-memory DB build the schema automatically with Hibernate's 'hbm2ddl' tool within a Junit (3) 'setUp()' using Netbeans 6.5.1 ? I'm not using Hibernate annotations - just a mapping file. For the actual code I want to use an on-disk database of course. [that is the Junits live a separate 'test' package] So I think this is getting there: Create a standard Java Project in Netbeans 6.5.1  add in the Hiberate library. Create the POJOs hibernate.cfg and hibernate mapping file. Copy the cfg and mapping file to the test package. The setup method looks like this:  protected void setUp() throws Exception { Configuration config = new Configuration(); config.configure(); SchemaExport exporter; exporter=new SchemaExport(config); exporter.create(true true); } Create a standard Java Project in Netbeans 6.5.1  add in the Hiberate library. Create the POJOs hibernate.cfg and hibernate mapping file. Copy the cfg and mapping file to the test package. The outline of the test case looks like this: import org.hibernate.Session; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import org.hibernate.tool.hbm2ddl.SchemaExport; ... public class DatabaseTest extends TestCase { private static Configuration config; private static SessionFactory sessionFactory; private static Session session; ... @Override protected void setUp() throws Exception { config = new Configuration(); config.configure(); SchemaExport exporter; exporter=new SchemaExport(config); exporter.create(true true); sessionFactory = config.buildSessionFactory(); session=sessionFactory.openSession(); } ... @Override protected void tearDown() throws Exception { session.close(); }
76,A,Writing test code for verifying database entries when testing an API I'm writing test code to test a client-server application. The application under test consists of an application that runs on Tomcat or another Java EE application server and client jars that expose an API. I'm basically writing test code that uses this client API to connect to the server. In addition to extensively testing the API operations my superiors have advised me to connect to the database on the server and verify that fields are being populated properly. I have done that for some of my test cases but it hasn't really caught any bugs during regression. Bugs are caught when a particular functionality fails but that anyway gets revealed in the code that tests the API itself. It seems that DB data verification is not really useful especially considering the extra effort required to write and maintain all that code. My question is: Is there any real benefit to write test code for connecting to the DB and verifying entries in this manner? Do the benefits pay off for the costs incurred in writing such code? With such kind of tests you are verifying the content of the database. The other tests only verify the API operations but not the results into the DB. The point is that after I send some data using the API I use the API calls themselves to retrieve most of the data and then I verify what I get. Thanks for your answer though. But this is testing multiple levels in once and not level by level. Starting with API putting something into the API and checking the result in the DB. Then fetching something from the DB and giving it back via the API. These are at least two levels you can test and in my opinion you should test. That could be a good point...but isn't it really covered by the API testing? If the steps outlined by jmz are followed the API will eventually give the same data back. To me the two levels of testing feels like performing the same test in two different ways with one way (the API way) being a superset of the other.  Looking at the database may find bugs but it's not very likely so I would keep the checking of the database to a minimum. As long as the API saves and restores data correctly you don't really care how it has been stored. You can check the database but personally I would not do this in a systematic way. You're asking the right questions the purpose of testing is to find bugs. How many bugs have you found with this testing? Automated regression testing is sometimes looked upon as a no-cost thing but if you're continually having to update the tests then it isn't. If you're unhappy at having to maintain these tests I would record how much time you're spending doing it and then you can argue that you could be doing more productive work instead doing other forms of testing or development. Thanks for your answer MatthieuF. Unfortunately I can choose only one answer so I'm choosing the answer that came first.  Reading the database is unnecessary for such tests. You can achieve better results by testing that: Save requests return a success status. Get requests return the saved data. The saved data is returned by a get request even after you reset client state ie. take client-side caching into consideration. If you test the database contents your test cannot be used to see if a change in the database works because the tests expect a certain state in the database. If this is changed your tests fail even if the system works. Very true regarding the database state changes. I have had to spend time correcting the test cases because of changes in the database schema. Testing the database like this is testing for side-effects. You should investigate why your superiors think that a working API needs side-effect testing? and explain that such testing does not increase reliability.
77,A,"junit assert in thread throws exception What am I doing wrong that an exception is thrown instead of showing a failure or should I not have assertions inside threads?  @Test public void testComplex() throws InterruptedException { int loops = 10; for (int i = 0; i < loops; i++) { final int j = i; new Thread() { @Override public void run() { ApiProxy.setEnvironmentForCurrentThread(env);//ignore this new CounterFactory().getCounter(""test"").increment();//ignore this too int count2 = new CounterFactory().getCounter(""test"").getCount();//ignore assertEquals(j count2);//here be exceptions thrown. this is line 75 } }.start(); } Thread.sleep(5 * 1000); assertEquals(loops new CounterFactory().getCounter(""test"").getCount()); } StackTrace Exception in thread ""Thread-26"" junit.framework.AssertionFailedError: expected:<5> but was:<6> at junit.framework.Assert.fail(Assert.java:47) at junit.framework.Assert.failNotEquals(Assert.java:277) at junit.framework.Assert.assertEquals(Assert.java:64) at junit.framework.Assert.assertEquals(Assert.java:195) at junit.framework.Assert.assertEquals(Assert.java:201) at com.bitdual.server.dao.ShardedCounterTest$3.run(ShardedCounterTest.java:77) Whats your stacktrace? @Frederik added stacktrace Why are you creating a new Thread in this test? I mean why the h@$! would you want to create Threads in a unit test? @Cem I have a set of (initial) tests I'm developing off of and one of them (attempts) to detect a race condition(the 3 lines I say to ignore become relevant for this discussion). Is there a better way to do race condition testing? Do I need to move to another tool for this kind of test? You can't really test for race conditions with unit tests especially by creating threads to simulate situations. Even on this example you gave you're checking that the counter better be 2 when the 2nd thread is running. Even though you create the threads in order they're not necessarily going to run at the same order. Also the threads can get preempted between the time you call increment and get so there is already a race condition in your test. Every once in a blue moon it will pass or fail. Unit tests should be more deterministic that this. Granted the specific assertions were a little naive but the unit tests are incredibly effective in catching a significant portion of race/contention issues for my specific situation. Where multiple worker threads are concerned such as in the original question simply joining one of them is not sufficient. Ideally you'll want to wait for all worker threads to complete while still reporting assertion failures back to the main thread such as in Eyal's answer. Here's a simple example of how to do this using ConcurrentUnit: public class MyTest extends ConcurrentTestCase { @Test public void testComplex() throws Throwable { int loops = 10; for (int i = 0; i < loops; i++) { new Thread(new Runnable() { public void run() { threadAssertEquals(1 1); resume(); } }).start(); } threadWait(100 loops); // Wait for 10 resume calls } }  I ended up using this pattern it work with both Runnables and Threads. It is largely inspired from the answer of @Eyal Schneider: private final class ThreadUnderTestWrapper extends ThreadUnderTest { private Exception ex; @Override public void run() { try { super.run(); } catch ( Exception ex ) { this.ex = ex; } } public Exception getException() throws InterruptedException { super.join(); // use runner.join here if you use a runnable. return ex; } }  The JUnit framework captures only assertion errors in the main thread running the test. It is not aware of exceptions from within new spawn threads. In order to do it right you should communicate the thread's termination state to the main thread. You should synchronize the threads correctly and use some kind of shared variable to indicate the nested thread's outcome. EDIT: Here is a generic solution that can help: class AsynchTester{ private Thread thread; private volatile AssertionError exc; public AsynchTester(final Runnable runnable){ thread = new Thread(new Runnable(){ public void run(){ try{ runnable.run(); }catch(AssertionError e){ exc = e; } } }); } public void start(){ thread.start(); } public void test() throws InterruptedException{ thread.join(); if (exc != null) throw exc; } } You should pass it the runnable in the constructor and then you simply call start() to activate and test() to validate. The test method will wait if necessary and will throw the assertion error in the main thread's context. *""You should synchronize the threads correctly ...""* in this example the simple way is for the main thread to call `join()` on the child thread ... and get rid of the `sleep(5000)` call. The sleep call smelled a little but I didn't dwell on it since it was unit test code but I will most certainly use the correct way now that I know."
78,A,"Class teardown in junit 3? We have a lot of integration tests written using JUnit 3 though we're now running them with 4.4. A few of these have a need for a tearDown method that runs after all the tests in the class are complete (to free some common resources). I see that this can be done in junit 4 with @AfterClass (org.junit). However mixing this into existing junit 3 tests that extend TestCase (junit.framework.*) doesn't seem to work. [BTW is there a migration tool yet? Question 264680 indicated there wasn't one a year ago.] I've seen mention of using junit.extensions.TestSetup for this kind of thing. My brief testing of this didn't seem work. Any examples? Assume we don't have time currently to manually convert and verify all the tests (and train all involved developers on using the junit 4 annotations) but just want to address the few classes that require an AfterClass equivalent in junit 3. Found the answer to my own question :) I had tried this briefly before posting my question but it wasn't working for me. But I realize now that it's because our test framework is invoking junit differently than the default so it wasn't calling the ""suite"" method that is needed in the following solution. In Eclipse if I use the bundled junit runner to directly execute one Test/TestCase it runs fine. A way to do this setup/teardown once per class in junit 3 is to use TestSuite. Here's an example on junit.org: Is there a way to make setUp() run only once? public static Test suite() { return new TestSetup(new TestSuite(YourTestClass.class)) { protected void setUp() throws Exception { System.out.println("" Global setUp ""); } protected void tearDown() throws Exception { System.out.println("" Global tearDown ""); } }; } I could execute my test with this code now but it seems the suite() method is not called. What did you do to make such call to this method? I though it would be called automatically did you run this on Android? I have the exactly same code but I'm getting a ClassNotFound exception. Have you already faced this?does YourTestClass extends TestCase?  In JUnit 4 your test class doesn't need to extend TestCase. Instead you must ensure that the name of your test class matches *Test and each test method is annotated with @Test. Have you tried making these changes then adding the @AfterClass annotation to the relevant method? There are annotations to replace most/all functionality you may currently be using from TestCase. In parallel to trying to get a class-wide teardown I'm also trying to just convert one class to junit 4 annotations. Not having much luck yet as Eclipse/command-line then thinks I don't have any tests to be run. I've updated my answer the advice I gave previously was completely wrong and I deserve to have been downvoted to hell. Please follow the instructions above and try again."
79,A,"Can you help me avoid a kludging JUnit in a master/slave Ant build setup? We're using the task within our master build to invoke targets in separate ant builds for each of our sub-projects. So far so good we get things built we can even run JUnit tasks within each one and everybody is happy. However... We want to take it to the next level we would like to have a single JUnit report generated from the JUnit test result XML for all of our sub-projects but if we terminate the build whenever any of the sub-projects has a unit test failure we never get to the point where we can generate a unified report. So that suggests that we would somehow note that unit tests failed within one or more of the sub-projects and not fail immediately but wait until the end of the master build to fail. What mechanism exists for that communication from the child builds up to the master build though? Properties are immutable and everything else we think of (properties files we update files we touch etc.) seem like horrible kludges. Is there a way to do this that fits in nicely with Ant and doesn't seem like something horrible we grafted on? The junit task supports the haltonerror and haltonfailure attributes which if set to false will cause the ant script to continue to run even if a test fails. There is also an errorproperty and failureproperty you can set instead. You can then copy your junit reports to a master directory (with all success and failures) and use the fail task to fail if either of those properties have been set. Something along the lines of: <target name=""run-tests"" > <junit printsummary=""on"" fork=""yes"" haltonfailure=""false"" haltonerror=""false"" errorproperty=""test.failed"" failureproperty=""tests.failed"" showoutput=""true"" maxmemory=""512m""> <classpath refid=""classpath"" /> <formatter type=""xml"" /> <batchtest todir=""test/reports""> <fileset dir=""${build.classes.dir}""> <patternset refid=""testfiles"" /> </fileset> </batchtest> </junit> <copy todir=""test/master/reports"" dir=""test/reports"" /> <fail if=""tests.failed""/> </target> Sorry... having major issues with the SO XML formatter  OK I never got an answer on this one that I liked that much but we did end up finding a good solution we like a lot. We switched from using Anthill OS as our build server to Hudson and after we did that we were able to take advantage of a Hudson feature where it will aggregate JUnit results from any number of locations to produce a single report that goes with each build (whether successful or not). So in short use Hudson. It rocks!"
80,A,"JUnit - stop it from exiting on finish? Quick JUnit question. I'm running some unit tests that involve starting up the GUI and doing a load of stuff. I would like to see the results after the test to confirm it visually. However it gets to the end of the code and exits as it should. If I want to override this I put a breakpoint on the last line of the test. This is pretty awkward though. Is there some option to stop it from exiting? involve starting up the GUI and doing a load of stuff - not a unit test for UI related tests I'd suggest Selenium - http://seleniumhq.org/ @Samuel: Perhaps my definitions are a bit off. But I need to start the GUI to perform the tests on different components. yeah that's great you're testing UI (it's much harder). I hope my selenium suggestion also helps =) Due to the fact you require a GUI and user interaction during the execution of the test this is a ""functional"" test rather than a ""unit"" test. You could write the results to a file at the end this would have the added benefit that you could assert that the output is correct/present programatically at the end. If you really want to keep the test running then you could insert an infinite loop at the end of your test: JUnit 3: public void tearDown() { while (true) { Thread.sleep(2000); }; } JUnit 4: @After public void tearDown() { while (true) { Thread.sleep(2000); }; } This will keep the JUnit thread running but you will need to ensure that your GUI events are being handled in another thread.  How about using a countdownlatch which you count down. private static CountDownLatch countdown = new CountDownLatch(1); @AfterClass public static void tearDownClass() throws Exception { countdown.await(); } Then somewhere else in your code you count down latch on event. countdown.countDown(); When countdown.countDown() is called then countdown.await() will continue.  One possibility is that your JUnit test is executing a tearDown() method perhaps in a base class which shuts down the GUI. If so you can override the tearDown() method in your JUnit test to prevent this behaviour. E.g. protected void tearDown() { //do nothing } Good idea but GUI still exits. I tried making the main class a field object just in case it was a garbage collector destroying the object. No luck there either  In Eclipse: Run configurations... > Test > Keep JUnit running... Tried this it doesn't work the GUI still exits This was the answer to the title of your question. What is the nature of the GUI? For Swing there's http://www.jdemo.de/ which might be helpful."
81,A,"Spurious ^G (BELL) characters in JUnit log making CruiseControl barf I have a CruiseControl build server running a large number of projects. On one of them I have recently noticed that only one of the two test suites are present in the build report (but failures in the other one still cause the build to fail). Further investigation showed that the XML output file of JUnit generated by ant's xmlformatter (which CruiseControl parses to produce build reports) contains occasional instances of the ASCII code 7 (BELL) character inside a CDATA section containing the system-out of a test case. Cruiscontrol apparently cannot deal with this and xmllint also considers these characts illegal within a CDATA section. Unfortunately I can't find anything that would write these characters; they appear at the beginning of a particular line of log output but not always (though the logging code always prints the same string literal). And shouldn't the xmlformatter produce valid XML no matter what a test case writes to its standard output? Has anyone had similar problems? This is how the relevant sections of the XML logfile looks like (anonymized since this is a corporate app):  <testcase classname=""Testclass"" name=""testMethod"" time=""0.0020""></testcase> <system-out><![CDATA[15.10.09 16:49:41.161 (MainUIClass): Starte UI initialize ... ^G15.10.09 16:49:58.881 (SubUiClass): Starte UI initialize 15.10.09 16:49:58.881 (SubUiClass): UI initialize beendet ^G15.10.09 16:49:59.264 (SubUiClass): Starte UI initialize 15.10.09 16:49:59.264 (SubUiClass): UI initialize beendet This is the code producing that log output: SystemProperties.getLogger().logInfo(getClass() ""Starte UI initialize""); ... SystemProperties.getLogger().logInfo(getClass() ""UI initialize beendet""); It seems to be always between those two lines (but not everytime they occur). However those occur much more often than anything else in the sysout stream. In some cases there are multiple BELL characters at the beginning of a line. And I've checked the source files - nothing fishy in there. Does it always occur on ""Starte UI initialize""? Or better: Does it always occur _after_ ""UI initialize beendet""? Btw greetings from Germany ;) Could you please change the encoding of the Java file? May be there invisible characters. Have you tried rewriting the log lines? Very interesting problem! Could you please post some of the infected XML? I think it would be helpfull thanks! I have now found out that the problematic characters are part of the tests's stdout stream and are produced by the beep() method of sun.awt.HeadlessToolkit i.e. the Toolkit implementation that is used when as is the case on the build server the java.awt.headless system property set to true. It seems clear to me that this is first and foremost a bug in ant's xmlformatter for JUnit logs which (I'd say) should produce valid XML output no matter what is in the stdout stream. Edit I was using an old version of ant; the current version (1.7.1) does not have this problem. That's definitely a WTF... Show me the bug entry and I'll vote for it."
82,A,"Installed Eclipse PDT I do not have ""Run As -> JUnit"" I installed Eclipse PDT and now I want to run a JUnit test in a Java project. The Java perspective is used but there are no options displayed in the Run As menu item. The ""Run Configurations"" item has the ""launch new configuration"" disabled. The tests are JUnit4 (the project compiles with junit4.jar but not with junit3.jar). How can I get the ""Run As -> JUnit"" option? If you take a look at the Galileo Packages Comparison page you will see the Eclipse Php distro (the one with PDT features) does not include the JDT (Java Development Tools) features. That means the ""nature"" of the projects you will create will not include ""java"" and any ""Run as JUnit"". You would need to load another distro (like the J2EE one) and then include the PDT plugin on top of it in order to have both. Thanks. I thought it might work the other way around - install a JDT plugin into the PHP distro. J2EE distro now works with Run As Junit."
83,A,"Display my tests using Eclipse JUnit plugin? I have written a test framework for a company-specific language. This test framework is able to output JUnit-like XML. As an example I can save it to file and open it using Eclipse JUnit view or disp^lay it in my Hudson server. I want now to have it directly integrated in my Eclipse in order for these tests to be run on Eclipse ""save"" action and for their results to be displayed in a JUnit view. What is the best way to do that ? Save them to file and open that file in Eclipse (and if so how to do it ?) Directly populate a JUnit view with content of XML ? (and if so how to do it ?) Create my own view (which I would like to do as I'm a complete beginner in SWT) I haven't tried this myself but I imagine that with a little bit of trickery it would be possible to automatically open a properly formatted file in the JUNit view. I think a call to org.eclipse.ui.ide.IDE.openEditor(IWorkbenchPage URI String boolean) should work. This is because JUnit-formatted xml files are set to be opened with the JUnit view. Before you get to this point you need to create an Eclipse plugin. This will be a simple plugin that will have one class that will listen for the completion of runs for your testing framework and another class that will know where to find the resulting xml files and will call openEditor on it.  In my opinion the best way is create a simple Eclipse plugin that shows your view. Eclipse has a pretty good API to do that and there is a lot of documentation. From my experience implementing Eclipse plugins you don't need to be a good GUI designer since most controls are written and you can reuse it. If you decide to take this way here are some resources to start: Creating a minimal Eclipse plugin Implementing a simple view with context menu. Understanding Eclipse viewers Hope it helps."
84,A,Eclipse function/plugin that finds corresponding junit class? I've been searching high and low for an Eclipse feature that lets you right-click on a main source class and find the corresponding JUnit class(es) without me having to navigate through the test classes of my project. I am using Mylyn which helps reduce the clutter but it would be much easier if there was a feature that performs a find automagically. I am following the Maven standard directory layout (/src/main/java /src/test/java etc.). And all of my test classes are named *Test. I'd imagine this can be feasibly implemented and probably already exists. Is there a function or plugin in Eclipse that finds the corresponding JUnit test classes for a given main class? The moreUnit plugin probably works for you. Capabilities (from its site): Decorate classes which have a testcase. Mark methods in the editor which are under test. Jump to a testcase/testmethod in the editor via the menu or a shortcut. Rename classes/methods and moreUnit will rename the corresponding testcode too. Move classes and moreUnit will move the corresponding tests. Generate a testmethod stub for the method under cursor-position in the editor via the menu or a shortcut. This is exactly what I needed. Thanks! +1 looks very interesting  Infinitest plugin runs your JUnits for those classes that you're changing as you're changing them. There is no need to right-click on the updated class to find the relevant JUnit class and then to run it - it will get run automatically. Test errors (if any) will show up the same way as Eclipse shows syntax errors.  As a partial answer to your question there is no requirement that tests have a one to one correspondence with main classes or any standard naming convention (even with maven). What you would want is a plugin that (for example based on a regex) matches source classNames to dest ClassNames and then loads that. Such a plugin would allow you to do what you want (and also for other uses not related to junit) but I'm not aware of one.  This plugin claims to be able to do this as well as other stuff. An useful feature of this plugin is the ability to jump between similar class e.g FooDAO to FooService FooService to FooAction etc. To use this feature one needs to configure this first. To configure please go to Windows -> Preferences ->Fast Code Preferences -> Mapping Btween Similar Classes. This is very similar to the configuration for create similar classes.
85,A,Looking for free and easy that (auto) generates JUnit tests Can I get recommendations of free and easy tool(s) that can generate JUnit tests based on existing code? Either as a stand-alone application or pref. an Eclipse-plugin. I suppose guys who would make that tool could earn a lot of money having a tool which writes code. Surely not that far-fetched? All I can do it to strongly advise you to back off from this approach. There are tools out there that auto-generate JUnit tests (most are not free) but it comes with the cost of unmaintainable test code. There are a whole bunch of code generators at http://www.junit.org/taxonomy/term/7 - but why would it be bad to use them? I want to like to rightclick on a method and create some basic junit-code nothing fancy One of the problem with these tools is that it generates tests based on the actual behavior of the code - the tests are not on the desired contract. That is if you have a bogus code the generated test will reflect the implemented bogus behavior. Then when you change (fix) the code the tests fail. These tools are basically only good for legacy code test coverage and even this is controversial. Well depends on what do you need your tests for... If just to have tests coverage and basic behavior tested - then yes use can use test generators. If you are serious about the project and plan to expend it see the future of the project - then it would be really better to create good unit tests in traditional way.
86,A,"Is this Spring training useful? We have a Spring + Ibatis based J2EE app. I planned to wrap around our DAO's (which call iBatis templates ...by getting spring beans) with test cases. I don't have much experience with JUnit so I thought that simply making an object of my DAO and then calling one of the methods will work. But wrong I was turns out that the whole J2EE app runs on appserver (container) but obviously the JUnit test cases are outside the container. So in my test case when I make object of the dao and call a method...it fails on a line like this which is in my DAO method ApplicationInitializer.getApplicationContext().getBean(""myMapclientBean""); So I went on a Google hunt...came across some posts and following the tubes I ended up on Spring's 4 day training course. Wanted to get your guys opinion on what do you think about this course? Is it valuable for the price? And can a person learn this stuff on their own too? By following couple of books? (Maybe not in 4 days but say over a month). oh and I'm still not able to Unit tests these DAO's...>_< If you touch a database in your tests then it's not a unit test; it's an integration test. Now it's perfectly OK to use JUnit to also do integration testing but you need to be aware of the difference. Spring provides integration testing support for JUnit and TestNG - check the reference manual. thanks. knowing that difference will help me in google searches as well. You may want to open a new question to deal with the unit test question. You might get better responses that way since this one really is more about the Core Spring course. I took the Core Spring course about a year and half ago. The syllabus apparently changed a little bit since then though it's still very similar. The instructor was very competent. I was working with Spring prior to taking this course but in the class I feel like I learned how to do things even better. I think you can probably get all of the raw information out of books Spring's online documentation and the source code itself but what the class does is connect everything together and teach best practices. That is not just ""how"" but also ""why"" and ""when"" you should use such-and-such feature. This is why I felt like my skills improved. The instructor was happy to answer questions about specific real-world issues beyond the classwork. So it would be a good idea to come armed with questions. The course isn't cheap. Whether it's worth it depends on whether you do well with classrooms versus self-teaching and utilizing online communities and peers to get that feel of best practices that books just don't offer. Also keep in mind that Spring is constantly evolving. Although you can get a good foundation from the class you'll still have to adapt to new features down the road (or take another class). Also took the course and I agree with everything @Jeff said. IMO it was well worth it. thanks guys. thats helpful. I'll ask for my small company and see if they are willing....or i'll start lurking around jobs.SO :). Since you guys took training can you tell me how Testcases will work if the J2EE app works inside a container (app server). I mean all the datasources and stuff are on the app server...so how will jUnit work when your DAO's are getting datasources from the appserver? Your description sounds more like an integration test than a unit test... but that seems to always be the case when testing DAOs and databases. You may want to look at the AbstractTransactionalDataSourceSpringContextTests class. This might also come in handy for you: http://iamjosh.wordpress.com/2007/12/11/unit-testing-dao-classes-with-junit-spring/  To solve the problem at hand: When using Spring your beans shouldn't really go out to the ApplicationContext and ask for the beans they need - they should provide setters (or constructors) so that the dependencies can be injected. It sounds like whomever designed these classes did the opposite of dependency injection. under the hood...the context is coming like this springContext = WebApplicationContextUtils .getWebApplicationContext(event.getServletContext()); could you please provide a little sample of how you think they should be so that Junit test cases would be able to use these beans outside of the container? The Spring MVC tutorial even though it's specifically for the MVC framework is an excellent intro to Spring IMO. http://static.springframework.org/docs/Spring-MVC-step-by-step/ Dependency Injection means that a class is *given* it's dependencies not that it goes out and finds them.  In answer to the second part of the question (I have not had experience of the spring course so can't comment). You can make use of a defined application context from within Junit tests and then instantiate the DAO's as you would from within the server container either through having your test case extend AbstractTransactionalDataSourceSpringContextTests if you are using Junit 3.8 or older. If you are using later Junit versions (4+) you can use Spring TestContext framework and annotations. Both explained far better than i can here from Spring and on this annotations RefCard -1 - AbstractTransactionalDataSourceSpringContextTests is an abortion. So are much of the Spring provided test case extensions. AbstractSingleSpringContextTests is where you should stop using them - they get more and more awful after that point. Its sure an opinion. Just showing the options the guy asking the question can then find his own way to get things done. I've used the AbstractTransactionalDataSourceSpringContextTests route. works for me. I've instantiated junits for the remoting project we have at work and this forms the basis of our junit tests. via public class TestBase extends AbstractDependencyInjectionSpringContextTests  I've used the AbstractTransactionalDataSourceSpringContextTests route. works for me. I've instantiated junits for the remoting project we have at work and this forms the basis of our junit tests. via import org.springframework.test.AbstractDependencyInjectionSpringContextTests; public class TestBase extends AbstractDependencyInjectionSpringContextTests { protected String[] getConfigLocations() { return new String[] { ""classpath:conf/dataAccessContext.xml"" ""classpath:conf/applicationContext-domain.xml"" ""classpath:conf/applicationContext-service.xml"" ""classpath:conf/applicationContext-dao.xml"" }; } protected Object getBeanToTest(String beanName) { return applicationContext.getBean(beanName); } protected Object getBeanToTest(Class clazz) { return applicationContext.getBean(clazz.getName()); } }"
87,A,Assigning static final int in a JUnit (4.8.1) test suite I have a JUnit test class in which I have several static final ints that can be redefined at the top of the tester code to allow some variation in the test values. I have logic in my @BeforeClass method to ensure that the developer has entered values that won't break my tests. I would like to improve variation further by allowing these ints to be set to (sensible) random values in the @BeforeClass method if the developer sets a boolean useRandomValues = true;. I could simply remove the final keyword to allow the random values to overwrite the initialisation values but I have final there to ensure that these values are not inadvertently changed as some tests rely on the consistency of these values. Can I use a constructor in a JUnit test class? Eclipse starts putting red underlines everywhere if I try to make my @BeforeClass into a constructor for the test class and making a separate constructor doesn't seem to allow assignment to these variables (even if I leave them unassigned at their declaration); Is there another way to ensure that any attempt to change these variables after the @BeforeClass method will result in a compile-time error? Can I make something final after it has been initialised? Why are you using statics for this? It'd be considerably easier with normal fields. It is typically not a very good idea to use random values in tests because random test output can be bad. When you run your tests once and they pass and then run them again and they fail it can be hard to track down the reason for failure. If I were you I would be sure to have plenty of red flags outputted when random values are enabled and make sure to record these random values so that if one set of values fails the test you can go back and retest with those values to isolate the underlying problem. That's an excellent point and I hadn't thought about it from that perspective. In this case I could have the values output to the console when the test suite is run or I could have the relevant values included in the fail message for each assert or have all the values logged to a file somehow. In this case I don't think I get sufficient value from the random numbers for it to be worth leaving them in though so I'm going to take them out (I had left this functionality disabled by default so it should make no difference to the normal use anyway). You can use a static initializer: final static boolean useRandomValues = true; final static int valueA; final static int valueB; static { if(!useRandomValues) { valueA = 42; valueB = 1337; } else { Random rnd = new Random(); valueA = rnd.nextInt(100); valueB = rnd.nextInt(100); } }  You can do this with a static constructor: import java.util.Random; public class StaticRandom { private static final boolean useRandomValues = true; private static final Random r = new Random(); private static final int value1; private static final int value2; private static final int value3; static { if(useRandomValues) { value1 = r.nextInt(); value2 = r.nextInt(); value3 = r.nextInt(); } else { value1 = 0; value2 = 0; value3 = 0; } } } Perfect that's exactly what I was after. Are static constructors as simple a concept as they appear? When will a static constructor be processed? My assumption would be that it is processed at compile time and the above code would just end up like value1 = 23140542; //Note: I just mashed the numpad here value2 = 20424040; value3 = 87618735; Then the compiler would probably replace those variables with the int literals wherever they show up in the code. Dr. Monkey static blocks contain ordinary code that is processed when the class is initialized (i.e. first loaded into the JVM at runtime) not at compile time. The rules are not that complex. A couple caveats are that code is run in textual order (which means you may not be able to refer to variables in the same scope declared later) and static blocks can not throw checked excptions. For info you can start with http://java.sun.com/docs/books/jls/second_edition/html/classes.doc.html#39245 and http://java.sun.com/docs/books/tutorial/java/javaOO/initial.html .
88,A,Javadoc in Junit test classes? Is it a best practice to put Javadoc comments in junit test classes and methods? Or is the idea that they should be so easy to read and simple that it is unnecessary to provide a narrative of the test intent? I personally use javadoc comments sparingly as I find they increase the on-screen clutter. If I can name a class function or variable in a more self-descriptive way then I will in preference to a comment. An excellent book to read on this topic is Clean Code by Robert C. Martin (a.k.a Uncle Bob). My personal preference is to make both the class and methods self descriptive i.e. class ANewEventManager { @Test public void shouldAllowClassesToSubscribeToEvents() { /* Test logic here */ } } One advantage of this approach is that it is easy to see in the junit output what is failing before browsing the code. Reading clean code right now. Just finished Unit Testing by Roy Ohserov who really stressed the human readability of unit and integration tests.  I use Javadoc in my testing a lot. But it only gets really useful when you add your own tag to your javadoc. The main objective here is to make the test understandable for other developers contributing to your project. And for that we don't even need to generate the actual javadoc. /** * Create a valid account. * @result Account will be persisted without any errors * and Account.getId() will no longer be <code>null</code> */ @Test public void createValidAccount() { accountService.create(account); assertNotNull(account.getId()); } Next we'll need to notify our Javadoc plugin in maven that we added a new tag. <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-javadoc-plugin</artifactId> <version>2.8</version> <configuration> <tags> <tag> <name>result</name> <placement>a</placement> <head>Test assertion :</head> </tag> </tags> </configuration> </plugin> </plugins> </build> And now all that is left to do is call our maven plugin. javadoc:test-javadoc (or javadoc:test-aggregate for multi-module projects) This is a fairly easy example but when running more complex tests it is impossible to describe the tests by simply using a self-descriptive method name.
89,A,"Support for POSTing XML in HttpUnit? Does HttpUnit support getting a response from an HTTP POST with an xml argument? Edit If you want to send a post request you might instantiate a PostMethodWebRequest object. WebRequest request = new PostMethodWebRequest(""http://example.com/thing/create""); And if you want to set parameters for that request I think what you would do is this: request.setParameter(""attribute"" ""value""); But what I am looking for is how to make the body of the post an XML document that holds the data for all the attributes I need to create a new Thing. Does anyone know the best way to accomplish that? Can you be more clear? Whats do you mean by xml argument? Sorry to be vague. I had to navigate through more of the HttpUnit documentation before I could explain myself. I have posted an edit of my question with some clarification. This seems to do the trick: InputStream body = new FileInputStream(""create.xml""); WebRequest request = new PostMethodWebRequest(""http://example.com/thing/create"" body ""text/xml"");"
90,A,"Java Project: Failed to load ApplicationContext I have a Java Project in which I am writing a simple JUNIT test case. I have copied the applicatinoContext.xml file into the root java source directory. I've tried it with some of the recommended settings I have read of here on StackOverflow but still get the same error. Is this error happening due to my project being a java project and NOT a web project or does that even matter? I'm not sure where im going wrong. import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; import org.springframework.transaction.annotation.Transactional; @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""C:/projs/sortation/src/main/java/applicationContext.xml""}) // Also tried these settings but they also didnt work //@ContextConfiguration(locations={""classpath:applicationContext.xml""}) //@ContextConfiguration(""classpath:applicationContext.xml"") @Transactional public class TestSS { @Autowired private EmsDao dao; @Test public void getSites() { List<String> batchid = dao.getList(); for (String s : batchid) { System.out.println(s); } } } Does the error say specifically it can't find the file (`FileNotFoundException`)? Could you paste the stack trace? Looks like you are using maven (src/main/java). In this case put the applicationContext.xml file in the src/main/resources directory. It will be copied in the classpath directory and you should be able to access it with @ContextConfiguration(""/applicationContext.xml"") From the Spring-Documentation: A plain path for example ""context.xml"" will be treated as a classpath resource from the same package in which the test class is defined. A path starting with a slash is treated as a fully qualified classpath location for example ""/org/example/config.xml"". So it's important that you add the slash when referencing the file in the root directory of the classpath. If you work with the absolute file path you have to use 'file:C:...' (if I understand the documentation correctly). Thanks your suggestions worked. I moved the applicationContext.xml to the resources directory and added the slash to the context configuration. Thanks. Funny that Spring MVC runs just fine with it in the WEB-INF/ folder but Junit is left scratching it's head.  I had the same problem and I was using the following plugin for tests: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.9</version> <configuration> <useFile>true</useFile> <includes> <include>**/*Tests.java</include> <include>**/*Test.java</include> </includes> <excludes> <exclude>**/Abstract*.java</exclude> </excludes> <junitArtifactName>junit:junit</junitArtifactName> <parallel>methods</parallel> <threadCount>10</threadCount> </configuration> </plugin> The test were running fine in the IDE (eclipse sts) but failed when using command mvn test. After a lot of trial and error I figured the solution was to remove parallel testing the following two lines from the plugin configuration above:  <parallel>methods</parallel> <threadCount>10</threadCount> Hope that this helps someone out!"
91,A,"Need help with writing test I'm trying to write a test for this class its called Receiver : public void get(People person) { if(null != person) { LOG.info(""Person with ID "" + person.getId() + "" received""); processor.process(person); }else{ LOG.info(""Person not received abort!""); } } Here is the test : @Test public void testReceivePerson(){ context.checking(new Expectations() {{ receiver.get(person); atLeast(1).of(person).getId(); will(returnValue(String.class)); }}); } Note: receiver is the instance of Receiver class(real not mock) processor is the instance of Processor class(real not mock) which processes the person(mock object of People class). GetId is a String not int method that is not mistake. Test fails : unexpected invocation of person.getId() I'm using jMock any help would be appreciated. As I understood when I call this get method to execute it properly I need to mock person.getId()  and I've been sniping around in circles for a while now any help would be appreciated. Also you should use allowing() instead of atLeast(1) since you're stubbing the person object here. Finally if Person is just a value type it might be better to just use the class.  If I understand correctly you have to move the line receiver.get(person); after the scope of context.checking - because this is the execution of your test and not setting expectation. So give this one a go: @Test public void testReceivePerson(){ context.checking(new Expectations() {{ atLeast(1).of(person).getId(); will(returnValue(String.class)); }}); receiver.get(person); } thank you same deal with this as well . I actually solved a problem I'll post answer now I made several mistakes."
92,A,"Why selenium locating element by Xpath failed in IE but it works wll in firefox? I have my selenium automation test scripts run in firefox and it works well. Now I want to run the scripts in IE it seems it failed on the step of locating an element in GUI. It seems selenium can't find the element in GUI while firefox can easily find it. String servicenameidtext = ""//table[@id='release_hostsProcesses_list']/tbody/tr[2]/td[1]/td""; Selenium.getText(servicenameidtext ); In fact the scripts is recorded by selenium-IDE and I have check about the Xpath I think there is no problem of the element. But why selenium failed on this step in IE? Could anyone tell me the reason? And how to solve the problem when running test in IE? Thanks a lot! I want to locate the element ""alarmlm"" from the GUI by String servicenameidtext = ""//table[@id='release_hostsProcesses_list']/tbody/tr[2]/td[1]/td""; P.S: The html of the part: <tbody><tr class=""form-title""><td colspan=""3""><a href=""javascript:void(0);"" onclick=""javascript:ChangeVisibility( 'release_hostsSoftwareInformation' ); return false;""><img src=""img/minus.png"" title=""-"" alt=""-""></a>&nbsp;<b>Software Information</b> </td></tr><tr><td><div id=""release_hostsSoftwareInformation_internal""> <table id=""release_hostsProcesses_list"" border=""0"" width=""100%"" class=""Processes_list""><tbody> <tr class=""title-column""><td><b>Service Name</b></td><td><b>Summary</b></td><td><b>Description</b></td><td><b> Version </b></td><td><b>Release</b></td><td colspan=""2""><b>Installation Date</b></td></tr><tr><td>alarmlm</td><td>mpm</td><td>MultiPlatformManagement tools</td><td>4.0.12</td><td>5</td><td>Wed 10 Nov 2010 04:23:52 AM CST</td></tr><tr><td>annlabclient</td><td>Annlab Client</td><td>Annlab Client</td><td>5.0.13</td><td>23</td><td>Wed 10 Nov 2010 04:23:57 AM CST</td></tr><tr><td>annlabserver</td><td>Annlab Server</td><td>Annlab Server</td><td>5.0.13</td><td>23</td><td>Wed 10 Nov 2010 04:23:57 AM CST</td></tr><tr><td>recoveryalarmlm</td><td>mpm</td><td>MultiPlatformManagement tools</td><td>4.0.12</td><td>5</td><td>Wed 10 Nov 2010 04:23:52 AM CST</td></tr><tr><td>recoverystatlm</td><td>mpm</td><td>MultiPlatformManagement tools</td><td>4.0.12</td><td>5</td><td>Wed 10 Nov 2010 04:23:52 AM CST</td></tr><tr><td>mrfctrl</td><td>MRF Controller</td><td>mrfctrl - including sipproxySubsystem</td><td>1.4.0.5</td><td>1</td><td>Wed 10 Nov 2010 04:24:00 AM CST</td></tr></tbody></table></div></td></tr></tbody> Thanks to all the problem is solved. The cause is: the html in firefox and IE are not the same. Strange. In this Case we can do 1 thing To Inspect Elements for Internet Explorer IE Developer Tools is available and for FireFox Firebug We can compare both Inspections Derive the XPath/CSS from IE Developer Tools for the Object. Use and Check is it working in both IE and FF If it works Fine Otherwise Conditionally we can use the corresponding XPath According to Browsers.  you could try to check out what the text value is directly in IE by appending /text() to the xpath expression. Maybe it finds the empty string? String servicenameidtext = ""//table[@id='release_hostsProcesses_list']/tbody/tr[2]/td[1]/td/text()""; alert(""found:"" + servicenameidtext); My two cents knb I have tried your method but it doesn't work. Thanks all the same:-) knb thanks for your method at least it shows me a way. Thanks!  Try to use all lowercase characters in id field: String servicenameidtext = ""//table[@id='release_hostsprocesses_list']/tbody/tr[2]/td[1]/td""; Selenium.getText(servicenameidtext ); Should work in both FF and IE ZloiAdun I have tried your method but it still doesn't work. I have attached my html could you please have a look? Thanks!  Maybe due to a default namespace? The error explanation is in the data you haven't shown us -- what about providing the (x)Html text? UPDATE: Now the OP has provided the source (x)Html document. The provided XPath expression: //table[@id='release_hostsProcesses_list']/tbody/tr[2]/td[1]/td doesn't select the required element simply because a td is never a child of another td but this is exactly what this XPath expression is looking for -- it ends with: td[1]/td @zhaojing: Thanks I have now found the problem with your XPath expression. See my updated answer. Dimitre thanks for your suggestion I have added my html in the problem. Could you please help me to have a look? Thanks! thanks for your suggestion I have added my html in the problem. Could you please help me to have a look? Thanks!"
93,A,Multiple correct results with Hamcrest (is there a or-matcher?) I am relatively new to matchers. I am toying around with hamcrest in combination with JUnit and I kinda like it. Is there a way to state that one of multiple choices is correct? Something like assertThat( result is( either( 1 or( 2 or( 3 ) ) ) ) ) //does not work in hamcrest The method I am testing returns one element of a collection. The list may contain multiple candidates. My current implementation returns the first hit but that is not a requirement. I would like my testcase to succeed if any of the possible candidates is returned. How would you express this in Java? (I am open to hamcrest-alternatives) marcos is right but you have a couple other options as well. First of all there is an either/or: assertThat(result either(is(1)).or(is(2))); but if you have more than two items it would probably get unwieldy. Plus the typechecker gets weird on stuff like that sometimes. For your case you could do: assertThat(result isOneOf(1 2 3)) or if you already have your options in an array/Collection: assertThat(result isIn(theCollection)) Hmmm... for some inexplicable reason my Eclipse environment (which is only about 6 months old) has an ancient Hamcrest library and I don't get these goodies. Thank you for mentioning isOneOf() . Didn't know about that.  assertThat (result anyOf(equalTo(1) equalTo(2) equalTo(3))) From Hamcrest tutorial: anyOf - matches if any matchers match short circuits (like Java ||) Moreover you could write your own Matcher what is quite easy to do. Thanks dude. Saved my life.
94,A,"Mark unit test as an expected failure in JUnit4 Is there an extension for JUnit4 which allows for marking some tests as ""expected to fail""? I would like to mark the test for current features under development with some tag for instance @wip. For these tests I would like to ensure that they are failing. My acceptance criteria: Scenario: A successful test tagged @wip is recorded as failure Given a successful test marked @wip When the test is executed Then the test is recorded as failure. Scenario: A failing test tagged @wip is recorded as fine Given a failing test tagged @wip When the test is executed Then the test is recorded as fine. Scenario: A successful test not tagged @wip is recorded as fine Given a successful test not tagged @wip When the test is executed Then the test is recorded as successful. Scenario: A failing test not tagged with @wip is recorded as failure Given a failing test not tagged with @wip When the test is executed Then the test is recorded as failure. possible duplicate of [Mark unit test as an expected failure in JUnit](http://stackoverflow.com/questions/4055022/mark-unit-test-as-an-expected-failure-in-junit) The @Ignore annotation says not to bother with the result. oh yes but I would like to bother about the result. In particular a test tagged as @wip should be executed and it must fail. Okay define fail? Do you mean you expect an exception? What you describe in the description is exactly the reverse of a normal test. Why not reverse all assertions in your test then you have the desired effect. @zenog I'm not sure if that comment was directed at me? I never really got the question I think. @Wes I meant Alex.  Short answer no extension will do that as far as I know and in my opinion it would defeat the whole purpose of JUnit if it would exist. Longer answer red/green is kind of sacred and circumventing it shouldn't become a habit. What if you accidentally forgot to remove the circumvention and assume that all tests passed? You could make it expect an AssertionError or Exception. @wip @Test(expected=AssertionError.class) public void wipTest() { fail(""work in progress""); } Making a shortcut in your IDE for that shouldn't be too hard. Of course I was assuming you tag the test with an annotation in the source code. In my opinion what you are asking is against JUnit's purpose but I do understand the use for it. An alternative would be to implement a WIPRunner with the WIP annotation and somehow make it accept failures of tests with the WIP annotation. If you are integrating with a BDD framework I would suggest a way to let it run the unit tests you marked @wip seperately and decide within your BDD methods if the result is ok."
95,A,"jUnit same exception in different cases I'm writing a jUnit test for a constructor that parses a String and then check numerous things. When there's wrong data for every thing some IllegalArgumentException with different message is thrown. So I would like to write tests for it but how can i recognize what error was thrown? This is how can I do it: @Test(expected=IllegalArgumentException.class) public void testRodneCisloRok(){ new RodneCislo(""891415""dopocitej(""891415"")); } and this is how I would like to be but I don't know if it is possible to write it somehow: @Test(expected=IllegalArgumentException.class(""error1"")) public void testRodneCisloRok(){ new RodneCislo(""891415""dopocitej(""891415"")); } You'll need to do it the old fashioned way: @Test public void testRodneCisloRok() { try { new RodneCislo(""891415""dopocitej(""891415"")); fail(""expected an exception""); } catch (IllegalArgumentException ex) { assertEquals(""error1"" ex.getMessage()); } } The @Test(expected=...) syntax is a handy convenience but it's too simple in many cases. If it is important to distinguish between exception conditions you might want to consider developing a class hierarchy of exceptions that can be caught specifically. In this case a subclass of IllegalArgumentException might be a good idea. It's arguably better design and your test can catch that specific exception type.  If you have JUnit 4.7 or above you can use this (elegant) way: @Rule public ExpectedException exception = ExpectedException.none(); @Test public void testRodneCisloRok(){ exception.expect(IllegalArgumentException.class); exception.expectMessage(""error1""); new RodneCislo(""891415""dopocitej(""891415"")); }"
96,A,"Using JMockit to mock autowired interface implementations We are writing JUnit tests for a class that uses Spring autowiring to inject a dependency which is some instance of an interface. Since the class under test never explicitly instantiates the dependency or has it passed in a constructor it appears that JMockit doesn't feel obligated to instantiate it either. Up until now we have been using SpringRunner to have Spring load mock dependencies for us which works. Two things we don't like about this are 1) the Spring framework has to be loaded and initialized each time running the tests which is not exactly speedy and 2) we are forced to explicitly create all mock dependencies as real classes something which JMockit helps eliminate. Here's a simplified example of what we're testing: public class UnitUnderTest { @Autowired ISomeInterface someInterface; public void callInterfaceMethod() { System.out.println( ""UnitUnderTest.callInterfaceMethod calling someInterface.doSomething""); someInterface.doSomething(); } } So the question is is there a way to have JMockit create a mock someInterface? see also: http://stackoverflow.com/questions/1638911/mock-object-and-spring-annotations JMockit will always instantiate a mocked interface (except in the case of a final mock field) but that only occurs in test code. It will not automatically inject the instance into code under test. You would have to manually inject the mock instance. For example: public class SomeTest { @Autowired UnitUnderTest unitUnderTest; @Mocked ISomeInterface theMock; // created and assigned automatically @Test public void testSomeMethod() { Deencapsulation.setField(unitUnderTest theMock); //proceed with unit test here } } mockit.Deencapsulation is a Reflection-based utility class that lets you invoke private methods get/set fields etc. Just a note: since this answer was posted JMockit has added support for automatic injection of mock objects into tested classes. In this example replace `@Autowired` with `@Tested` and `@Mocked` with `@Injectable`. Wow @Rogério! that's nice! JMockit is so powerful... I'm loving it!!  You can use org.springframework.test.util.ReflectionTestUtils to explicitly inject your mocked ISomeInterface in your test case. See documentation (+1) that's one good class I didn't know of :) I love finding these little gems in the Spring API... +1 This is a useful class to be sure. Still having to explicitly create the mock defeats one of the main reasons for using JMockit in the first place. This is super useful. I create mocks by hand and had an autowired property in the class under test. This allowed me to set the property in the test to the mock without editing the code under test! Excellent.  With the hints kindly provided above here's what I found most useful as someone pretty new to JMockit: JMockit provides the Deencapsulation class to allow you to set the values of private dependent fields (no need to drag the Spring libraries in) and the MockUp class that allows you to explicitly create an implementation of an interface and mock one or more methods of the interface. Here's how I ended up solving this particular case: @Before public void setUp() { IMarketMakerDal theMock = new MockUp <IMarketMakerDal>() { @Mock MarketMakerDcl findByMarketMakerGuid( String marketMakerGuid ) { MarketMakerDcl marketMakerDcl = new MarketMakerDcl(); marketMakerDcl.setBaseCurrencyCode( CURRENCY_CODE_US_DOLLAR ); return marketMakerDcl; } }.getMockInstance(); setField( unitUnderTest theMock ); } Thanks everyone for the help."
97,A,"Level of detail of your unit tests I wanted to start a discussion about the details that you cover in your unit tests. Do you test major functionalities that consist of several methods doing one task at once with one test? or maybe you even test automatic properties? Because for example I see little value in writing a test that would test only this:  public Email { set { if(Regex.Match(/*....*/)) email = value; } get { return email; } } As it's really clear and it's just a waste of time. Usually when I do unit tests I test a whole task - like on this example - a whole registration procedure. I'm asking this because currently I'm reading the book ""Applying Domain Driven Design and Patterns"" authored by Jimmy Nilsson and there he points out that he's testing even such small details with a dedicated test. Isn't such level of coverage an overuse? How Much Unit Test Coverage Do You Need? http://www.artima.com/weblogs/viewpost.jsp?thread=204677 Don't think you need to test an email class? Go on show me your email regex and I will show you a valid address that breaks it. I try to test whatever could throw an exception or may fail in any other way. For example in your example what if someone enters something that is not an email address do you just quietly fail? More important I called Email with a valid email address so email has a value. Then it gets called with an invalid email address. What should be the expected behavior? Is the original email address still there (I think so) and is that the correct behavior? If there is a specification on how it should behave then that sets the grounds for a unit test IMO. If you test this function at a higher level then you just need to make certain you are testing all possible situations for this setter but testing the getter part is pointless. In my example even if you throw an exception on invalid email the behavior is still very predictable that I doubt the sense of testing such a small piece of code. If you'd take a look at pages 80-84 of the ADDD&P you could notice that even such details are tested - like writing a whole test method to test an addition of two integers. +1 for the tests sanity checking the API design @Karim - I tend to test anything that may have an error or unexpected response. For example if you always assume that adding two ints is positive then you should properly handle if it is negative so then a test is needed. But if you don't care positive negative overflow then no need to test. @Karim - If an exception is thrown in your example then the value isn't changed so you still have a valid email address in your property unless you set 'email=""""' before using the regular expression. But regardless there should be some document explaining what the state should be in the event of an invalid email address. That would be the test case.  There are a number of reasons why you should test this particular property. Here are the different tests and the reasoning behind those. Test all the possible correct values that the property could possibly get invoked with. This is primarily to build up a complete picture of what values this property expects. This will become invaluable for later refactoring (i.e. regression). Test that the property returns the value that was previously successfully set. Because this is not straigh forward get/set property you want to make sure that it returns the right value. Test that it throws exception whenever an invalid value was passed in. Since this particular bit of code really should handle invalid input you will need to throw an exception if the value passed in is not valid. Testing this particular proeprty is certainly not a waste of time since email validation is not all that straight forward  you should really test the bollocks out of this one.  An important attribute of unit tests is that they show clearly what is wrong when they fail. If they test very small functionality this is possible. If you test for example a whole registration process you can't be sure what line went wrong. And as cletus said testing a process would generally be considered systems or integration testing. Integration tests aren't expected to point out exactly what is wrong but more that something is wrong.  Tests are not just there to test that what you wrote works. Tests are also there to test that what you wrote STILL works. Like many years and many developers later. What you think is a dead simple class might in the future become more complicated. For example lets say it started with no filter. Just a simple get/set. Why test that? Then later some other developer adds in the regex filter and its faulty. Then suddenly the class is broken but its untested so nobody knows. This reveals itself as mysterious failures further up the call stack which now take more time to debug. Probably more than it would have taken to write a few tests. And then in the future somebody's going to try and get clever and optimize your code. Or reformat it regexes have a tendency to be written unreadable and can often do with some cleanup. This is ripe for subtle breakage. Small targeted unit tests will catch this. In your example above the regex is presumably there to filter for things that look like email addresses. That necessitates checking that regex is working else your Email class stops accepting emails. Or it starts taking gibberish. Maybe your regex doesn't cover a valid email address that's worth testing once you discover it. Eventually somebody is going to replace your regex with an actual parser. Maybe it works maybe it doesn't. A good test will have a simple list of valid and invalid email addresses which you can easily add to as corner cases are discovered. Tests also allow you to exercise your interface and discover holes. What happens when you put in something that isn't an email address? That's right nothing. Email.set silently throws away the input. Garbage in nothing out isn't very polite. Maybe it should throw an exception. This is something that would quickly become clear as you're trying to test it because it would be necessary to test whether the set worked. Testing can also reveal inflexibilities and things which cannot be overriden or customized. In your example it would be handy to test the regex filter directly rather than having to instantiate an object each time. This is because the filter is the most complicated bit and its easier to test and debug while going through as few layers as possible. By putting it into Email.is_email_address you can now test it directly. As a side-effect it can also be overriden in a subclass. This is handy because most people get email validation WRONG because EMAIL HATES THE LIVING! Finally you want your tests to be decoupled. Test one thing without it being effected by additional complexities so you can clearly see the root of the problem. Your Email class is an excellent candidate for a simple decoupled unit test. +1. Good advice  One could argue that testing the whole registration process is too high-level. Small pieces of functionality tend to be far easier to write tests for whereas more complicated tests often require a fair amount of scaffolding. The strict approach would be to aim for 100% (or close to it) code coverage on your unit tests (many IDEs either have the capability to measure this directly or via some kind of plug-in). In practice you may not want to be so strict but instead pick key classes that you need to work because so much else depends on them. One of the advantages of dependency injection (""DI"") or inversion of control (""IoC"") is that it encourages you to break your code up into pluggable pieces which not only makes your code easier to test (because you can easily plug-in mocks) but also tends to make those pieces smaller. Writing tests for many small pieces tends to be much quicker than for fewer large pieces. Testing a whole registration process is more about systems or integration testing.  I would certainly test the code snippet you showed. It may just be a single if statement but Regular Expressions are extremely easy to get wrong. I'd use a data-driven test to verify the regex is correct. Even if it was just a case of if(some simple condition) I would advocate writing a test to make sure it is correct. After all if the logic is simple you might switch off your brain while writing it. Similarly it's easy to get a ! in the wrong place use && instead of || add parenthesis in the wrong place or just make an elementary mistake. If you add a test to cover the new code before you write it (the new code) the test will often catch the mistake before you go any further. Moreover writing the test using the AAA pattern (arrange act assert) helps you sanity check your API because your test is using it! I've often tested simple-looking parts of my code and then realised from another perspective (a user's) the API could be clearer. I personally wouldn't test auto properties with dedicated tests unless I knew for a fact the implementation is likely to change. That time is probably better spent adding more tests covering potential edge cases or more tests for the highest traffic areas of your code.  Tests of simple functions are themselves simple. So they are easy to write and easy to run. They take very little time to create and are a place holder for when the module gets more complicated later. So yes test even the simple functions. Now the function you typed above contains a regular expression which you helpfully commented out. Regular expressions are notorious hard to predict. The regular expression to match a valid email address is probably very complicated. So I would be testing the hell out of it. I'd have unit test after unit test plying that regular expression with all the different email address variants that I could think of and all the corner and negative cases as well. The decision about whether to write a test or not is always a decision about short- and long-term benefits. Tests are always beneficial in the long term because they anchor the behavior of the code and detect side effects and unintended consequences. The act of deferring a test is always made for short term gain so that you don't ""waste time"". The problem is that ever test you don't write is a hole in the system in which side effects and unintended consequences can accumulate undetected. Since tests for simple modules are quick and easy to write and since Murphy will ensure that nasty bugs will hide in any hole you provide for them it seems unlikely that those simple tests are truly a waste of time."
98,A,"Writing Unittest for generic classes... best approach? I'm supposed to write unit tests for a project whose code another colleague has written. He created a generic list class. Now i'm thinking about the best way to create the unittests for the class and how to avoid much repetitive code. The List class can be instaniated with String Integer and some other Types. How can i avoid to write a testclass for each of these types and instead use one testclass for every datatype? Thanks Patrick Ok I'll bite... why did your colleague reinvent the List wheel? I'm just curious. More importantly if unit tests are required why hadn't the colleague code them right off the bat? JUnit4 has a parametrized runner which will let you write one test for all the different types. This assumes that there is no substantial behavior difference between the different types so the test can exercise the class the same way. That being said I have to echo the comments that the underlying circumstances of the question is very bizarre. The JDK has plenty of list implementations and it is very strange to be writing a class for this purpose. The List has some functionality to undo changes. He needed that for the project. Why do i write the tests for it? My colleagues never have worked with it (they despise such witchcraft ;-)) and i (new guy in the company) have researched to topic a little bit more. So my Boss has told me to support my colleague in his project by writing the unittests. It's also supposed to show them the benefits of using a framework like Junit.  I agree with Max A. that you need to test the expected behavior. If the generic class behaves differently for different parameterized types then you need to test that. Otherwise testing with one type should be adequate. Also if you want to assert that the generic class can be instantiated for any type you may want to write a test that instantiates the class for Object. This would be a somewhat odd test in that if it compiles it ""passes"" but if you keep your tests as a regression of the systems expected behavior (and you should) then this test will guard against someone changing the generic class to be parameterized with <T extends YYY>. Thanks for the usefull hint! :-)  Does the class you're testing treat its contained elements in any special way based on their type? If it treats instances of String and Integer the same then you can simply test it once with String and assume that Integer will work just fine. Unit testing depends heavily on the expected behavior."
99,A,How to cover methods of inner classes? I'm faced with a problem on how to cover methods of my innner classes. I have a small GUI (swing) application with several buttons and defined action listeners in form of inner classes. Now I want to write tests for each of the action listener methods defined in these inner classes but when I'm creating a new JUnit using Eclipse I don't have a option to cover methods of inner classes but the only public methods of the Class where those inners are defined. Please advice. An instance of an inner class can only be created by its outer class. You have to write wrapper functions in the outer class or to make the inner class a normal one. I would recommend against writing wrapper methods to pass through to the inner class (for no other purpose than testing). At that point you could just declare the inner class to be public static and have done with it. It would dirty up the class's implementation immensely and would lead to having a good portion of methods related explicitly to testing and serving no other purpose. Rather your inner method should be tested through the tests on your outer methods (i.e. the inner class is correct if the outer method is tested and shown to be correct because the inner method is part of the outer)  This means your code is not testable. Move your logic in separate classes and unit-test them. (They can be package-private for example so that they are still not accessible to the outside world) If your inner classes are logically inner - i.e. they represent some internal functioning of your class - don't unit test them unit-test the parent class' methods.  AFAIK GUI components are hard to unit test. If you really want to test them maybe you could pull out the logic code into methods in the outer class and perform a method call in the inner class for the action. If your inner class actions are GUI updates I'm not sure how they can be effectively tested.  Conceptually inner methods are not generally tested with unit tests because they are considered an implementation detail. You should be testing the behavior of your class by testing its public methods. That said... http://www.artima.com/suiterunner/private.html I'm not sure why you need inner classes...They're not used anywhere else? Why not just make them public classes outside of your main class? Thanks for the link to the article. The reason I'm using inner classes is because their methods dealing with private methods of parent class. It's possible to move them outside of my main class but then I would need to define many accessors for private fields of the main class. (See also: How do I test my `private` methods?)
100,A,JUnit Tests - Is a call to JUnitCore.main() necessary? Good afternoon I am running some JUnit tests on my application using ant. In doing so I am following the instructions in the step-by-step Spring-MVC tutorial. [*] The instructions never mention a call to org.junit.runner.JUnitCore.main() in running a test. My question is is it necessary to call org.junit.runner.JUnitCore.main to run a test if you are running the tests through command-line ant (as opposed to an IDE)? Or is ant smart enough to locate all the methods in a TestCase subclass and run all of them without an explicit call to JUnitCore.main()? [*] http://static.springsource.org/docs/Spring-MVC-step-by-step/part3.html Thanks ktm Ant knows what to do. As long as you're using the right ant-task for that (like jUnit task: http://ant.apache.org/manual/Tasks/junit.html). So does it run every method in that class in the order that it appears in the class? I ask this because the tutorial gives a method called setUp which initializes an object which is the first method in the class. Following this is the method that performs the test. If I were to switch the order of the two methods would it not work? I am about to figure this out for myself but I'd just like to confirm that the order of methods is indeed significant. well the order in which the methods were declared is irrelevant as long as they are called setUp and test***. If you use jUnit 4 you can just annotate the methods as @BeforeTest @BeforeClass @Test I see thanks! x
101,A,Correct location (in Gradle) to put code that should be run in _all_ test cases? Just wondering... I have some code (disabling certain logging output) that I'd like to be run before all tests... I do not see any such examples for JUnit/Groovy testing in the samples directory... is there a good/correct place to put such code? Thank you! Misha p.s. I am using the 0.9 preview 3 version. According to the Test source code you should be able to set a beforeSuite/beforeTest Closure on the test Task. I haven't tried it but I imagine it works something like this: test { beforeSuite{ //run custom setup code here } }
102,A,"How to start and stop an Tomcat container with Java? I have a Maven project that starts a tomcat container for pre-integration-tests (jUnit Tests). Most of my tests require that the web-application under tests is restarted. So I'd like to restart the Tomcat container before each jUnit test is executed. As for now I use the cargo-maven2-plugin to configure the tomcat container. So is it possible to start and stop the container with a java statement? are you sure you need to to restart the container and not just reload the webapp? Yes it is. But I am afriad how much time will your tests take. You must fix your tests to not depend on server startup and shutdown. Its fine to run tests inside container but container should be started once before tests. Well to answer your question you can execute the relevant .sh or .bat files from Java using system calls. Something like below Runtime r = Runtime.getRuntime(); Process p = r.exec(""start.sh""); p.waitFor();  bootstrap.jar and commons-logging-api-1.1.1 from tomcat\bin to your classpath and the following snippet may help Bootstrap bootstrap=new Bootstrap(); bootstrap.setCatalinaHome(""D:/apache-tomcat-5.5.28""); bootstrap.start(); Thread.sleep(60000); bootstrap.stop();  I'm using follow method: private static final String TOMCAT = ""D:/test_tomcat""; @BeforeClass public static void runTomcat() { bootstrap = new Bootstrap(); bootstrap.setCatalinaHome(TOMCAT); try { bootstrap.start(); } catch (Exception e) { e.printStackTrace(); fail(""Не удалось запустить Tomcat""); } } @AfterClass public static void stopTomcat() { try { bootstrap.stop(); } catch (Exception e) { e.printStackTrace(); } } But is method have a one problem - I need copy WAR file in 'webapps' directory every time have change source code. I thing what copy my class from classpath in web-inf and copy 'jar' from classpath in web-inf/lib.  So is it possible to start and stop the container with a java statement? Your use case looks extremely weird (having to restart the container between tests) but let's not discuss this. To answer your question yes it is possible and this can be done using Cargo's Java API. To start a Tomcat container and deploy your war you can do something like this in the setUp() method: // (1) Optional step to install the container from a URL pointing to its distribution Installer installer = new ZipURLInstaller(new URL(""http://www.apache.org/dist/tomcat/tomcat-6/v6.0.20/bin/apache-tomcat-6.0.20.zip"")); installer.install(); // (2) Create the Cargo Container instance wrapping our physical container LocalConfiguration configuration = (LocalConfiguration) new DefaultConfigurationFactory() .createConfiguration(""tomcat6x"") ContainerType.INSTALLED ConfigurationType.STANDALONE); container = (InstalledLocalContainer) new DefaultContainerFactory() .createContainer(""tomcat6x"" ContainerType.INSTALLED configuration); container.setHome(installer.getHome()); // (3) Statically deploy some WAR (optional) WAR deployable = new WAR(""./webapp-testing-webapp/target/webapp-testing-webapp-1.0.war""); deployable.setContext(""ROOT""); configuration.addDeployable(deployable); // (4) Start the container container.start(); And stop it in the tearDown() method. // (6) Stop the container container.stop(); @Vinegar It's more usual to use Cargo through plugins (at least I think so) but Cargo *is* actually a Java API before all. And this can be handy (more and more containers do offer an embedded API but Cargo's API is ""unified""). Yes I tend to agree. Thanks. Thanks didn't think of Cargo for this."
103,A,"How to run test cases on a list of slightly different jars? I have a list of jar files all containing slightly different versions of the same system. I want to execute a set of test cases on each jar file while being able to obtain the results for each jar (via RunListener or something like that). I am a bit confused with the class loading problems that are implied. How can I nicely do this ? If the jars maintain the same interface then all you have to do is run each test with a slighly different classpath - each time with jars from another version. If you're using Eclipse JUnit integration just create N run configurations in each one in the classpath tab specify required jars. If you want to do it programmatically then I suggest you start with empty classpath and use URLClassLoader giving it a different set of jars each time. Something like this: URLClassloader ucl = new URLClassLoader( list of jars from version1 ); TestCase tc = ucl.loadClass(""Your Test Case"").newInstance(); tc.runTest(); ucl = new URLClassLoader( list of jars from version2 ); TestCase tc = ucl.loadClass(""Your Test Case"").newInstance(); tc.runTest();  If you are using ant the JUnit task takes a classpath. <target name=""test""> <junit ...""> <classpath> <pathelement path=""${test.jar.path}"" /> </classpath> ... </junit> </target> You can use the antcall task to call your test target repeatedly with a differed value for the test.jar.path property."
104,A,Unit Testing for user inputs - Java I am trying to write unit tests for a game i have created. The game is a tic tac toe implementation which interacts with the user and takes input from the command line. I am not sure how i can go about writing a test which will take and check user input. For example: When run: the program asks user for entering name and his symbol ('x' or '0') and then keep accepting inputs of position. Do i create a @Before method and write input statements inside this method? or is there a better way to do this? Writing everything in my @Before method will make it extremely big. Any suggestions?? Please help. Thank you You say you want to write a test which will take and check user input You shouldn't have one class that does both take and check user input that's poor cohesion. Instead have two classes a Validator (or checker or what ever you want to call it) and an input processor that takes the user input and builds some object that the Validator understands. Once you have separated the concerns writing a junit test case for your Validator should be quite straightforward. As for the user input class have a look at this question about how to unit test console based app thank you. i created a separate class for both. i will create a mock object to test it. many thanks :)  Ideally you'd split your input class into an interface and an implementation and then create a MockInput for testing purposes. interface IUserInput { public String getUserName(); public XO_Enum getUserSymbol(); } class MockUserInput implements IUserInput { private XO_Enum xo; private String name; public MockUserInput(String name_ XO_Enum xo_) { xo = xo_; name = name_; } public String getUserName() { return name; } public XO_Enum getUserSymbol() { return xo; } } When some test needed a fake user input you just whip up a mock one for the occasion and keep going. The symbol and name could be hard-coded but this way gives you some flexibility. Given hhafez's answer one of us is clearly answering the wrong question... and I'm not so sure it's him. Ah well this'll be helpful either way. thanks. both helped. :)
105,A,"Java Junit testing problem I am using Junit 4. My whole program is working fine. I am trying to write a test case. But there is one error... here is very basic sample test public class di extends TestCase{ private static Records testRec; public void testAbc() { Assert.assertTrue( ""There should be some thing."" di.testRec.getEmployee() > 0); } } and when i run this it give me error that fName can not be null if i use super and do like this public TestA() { super(""testAbc""); } it work all fine. It wasn't this before with JUnit 3.X am I doing wrong or they changed it :( Sorry if I am not clear Is there any way to executre test without super? or calling functions etc. ? what is TestAgnes? It isn't mentioned in the first code snippet. Please clear your question. In JUnit 4 you need not extend TestCase instead use the @Test annotation to mark your test methods: public class MyTest { private static Records testRec; @Test public void testAbc() { Assert.assertTrue( ""There should be some thing."" MyTest.testRec.getEmployee() > 0); } } As a side note testing a static member in your class may make your unit tests dependent on each other which is not a good thing. Unless you have a very good reason for this I would recommend removing the static qualifier. thanks for your reply. When I tried this it gave me error ""Type mismatch: cannot convert from Test to Annotation"" what should I to avoid this error? It gives me if I used @Test Sounds like you imported some other Test class which is not an annotation. Make sure you import `org.junit.Test` that should resolve the issue. thanks it worked ! ! ! I want to make an additional note if you are getting the ""Type mismatch"" error above you might be doing what I did and named your test class ""Test"" which conflicts with the import."
106,A,"Does JUnit 3 have something analogous to @Ignore I'm forced to use JUnit 3. If I were using JUnit 4 I would occasionally use @Ignore since several of my tests take a bit of time. Is there anything analogous in JUnit 4? Commenting out tests is sloppy and changing the name (from testXxx()) could lead to forgotten tests. @Ignore is great because it always reminds you which tests were not run. Does anyone have a best practice for running some of a test classes methods in JUnit 3? I don't know any other solution apart from commenting out tests or renaming them. I would go for the renaming option and use my own convention. For example all of them will start with ignoreXXX(). Then you can do one find/replace with your editor and you are ready. Yes. JUnit 3 reflectively looks for methods starting with ""test"" to execute.  In order to conditionally ignore tests in Robotium / JUnit 3 I override runTest() like @Override protected void runTest() throws Throwable { // Do nothing if the precondition does not hold. if (precondition) { super.runTest(); } } Tests which are ignored this way will show up as ""Success"" in Eclipse but as there is no ""Ignored"" state with JUnit 3 this is the best I was able to get.  you can prepend the method with failing so all methods like failingtest***() will be ignored during the junit run. Please keep an eye out for when posts have been written and whether they are still in need of an answer!"
107,A,"JUnit: checking if a void method gets called I have a very simple filewatcher class which checks every 2 seconds if a file has changed and if so the onChange method (void) is called. Is there an easy way to check ik the onChange method is getting called in a unit test? code: public class PropertyFileWatcher extends TimerTask { private long timeStamp; private File file; public PropertyFileWatcher(File file) { this.file = file; this.timeStamp = file.lastModified(); } public final void run() { long timeStamp = file.lastModified(); if (this.timeStamp != timeStamp) { this.timeStamp = timeStamp; onChange(file); } } protected void onChange(File file) { System.out.println(""Property file has changed""); } } @Test public void testPropertyFileWatcher() throws Exception { File file = new File(""testfile""); file.createNewFile(); PropertyFileWatcher propertyFileWatcher = new PropertyFileWatcher(file); Timer timer = new Timer(); timer.schedule(propertyFileWatcher 2000); FileWriter fw = new FileWriter(file); fw.write(""blah""); fw.close(); Thread.sleep(8000); // check if propertyFileWatcher.onChange was called file.delete(); } As I understand your PropertyFileWatcher is meant to be subclassed. So why not subclass it like this: class TestPropertyFileWatcher extends PropertyFileWatcher { boolean called = false; protected void onChange(File file) { called = true; } } ... TestPropertyFileWatcher watcher = new TestPropertyFileWatcher ... assertTrue(watcher.called);  Here is a simple modification for your test. @Test public void testPropertyFileWatcher() throws Exception { final File file = new File(""testfile""); file.createNewFile(); final AtomicBoolean hasCalled = new AtomicBoolean( ); PropertyFileWatcher propertyFileWatcher = new PropertyFileWatcher(file) { protected void onChange ( final File localFile ) { hasCalled.set( true ); assertEquals( file localFile ); } } Timer timer = new Timer(); timer.schedule(propertyFileWatcher 2000); FileWriter fw = new FileWriter(file); fw.write(""blah""); fw.close(); Thread.sleep(8000); // check if propertyFileWatcher.onChange was called assertTrue( hasCalled.get() ); file.delete(); } @nkr1pt. Definitely go with a reputable mocking framework. If you are not confined to 1.4 version of JDK take a look at jMock. I really like this solution because it doesn't add a dependency to a mocking framework; however mocking frameworks are a necessity for unit testing; that's why I'm accepting the mocking suggestion as the accepted answer to my question.  With Mockito you can verify whether a method is called at least once/never. See point 4 in this page: http://mockito.googlecode.com/svn/branches/1.5/javadoc/org/mockito/Mockito.html You can use any mocking framework and not just mockito. Take a look at EasyMock or jMock and pick what you like. The rule of thumb of writing unit tests is that you should only mock the objects that you can control. In other words the mock objects should be made available to the class under test using constructor arguments/setters or parameters to your method under test. By this logic you cannot mock static invocations final or private or ""new"" objects created inside the method of the class under test. Any idea how you can do it in EasyMock? I find the documentation lacking on this. When I create a mock for PropertyFileWatcher like this: PropertyFileWatcher propertyFileWatcher = createMockBuilder(PropertyFileWatcher.class).withConstructor(file).createMock(); and record the expected call to onChange and replay: propertyFileWatcher.onChange(file); replay(propertyFileWatcher); the onChnage method is called right away and info is printed to sysout but I would just like to verify IF that method was called or not Don't use mockito being able to do the AtomicBoolean solution. The second link is not up to date. you can see this page : [Mockito documentation](http://docs.mockito.googlecode.com/hg/latest/org/mockito/Mockito.html)"
108,A,"Unit Testing gwt-dispatch I'm trying to write some unit tests for a gwt-dispatch service with JUnit. I'm getting the following error when stepping through the test with my debugger: Error in custom provider com.google.inject.OutOfScopeException: Cannot access scoped object. Either we are not currently inside an HTTP Servlet request or you may have forgotten to apply com.google.inject.servlet.GuiceFilter as a servlet filter for this request. I'm going to simplify the code a bit here -- hopefully I'm not stripping out anything necessary. import junit.framework.TestCase; import net.customware.gwt.dispatch.client.standard.StandardDispatchService; import com.google.inject.Guice; import com.google.inject.Injector; import com.google.inject.servlet.ServletModule; ... public class LoggedInServiceTest extends TestCase { Injector i; StandardDispatchService service; protected com.google.inject.Injector getInjector() { return Guice.createInjector(new ServletModule() new TestServletModule() new ActionsHandlerModule() new TestDispatchModule() new OpenIdGuiceModule()); } public void setUp() throws Exception { i = getInjector(); service = i.getInstance(StandardDispatchService.class); } public void testNotLoggedIn() { try { GetProjectsResult result = (GetProjectsResult) service.execute(new GetProjectsAction()); result.getSizeOfResult(); } catch (Exception e) { fail(); } } } The service request is indeed supposed to be going through a GuiceFilter and it looks like that filter is not being set. Any ideas on what other setup needs to be done to register the filter? Some stylistic suggestions in your test. 1. setUp should be protected and call `super.setUp()` before doing anything (`TestCase.setUp()` does nothing but if you change the base class you could have problems). 2. Instead of catching Exception in your test and calling `fail()` change your test method to declare that it throws `Exception`. If a test throws an exception it automatically fails. Thanks. The test is actually supposed to verify that the proper exception is thrown -- but you're right that the simplified code I posted isn't exactly pretty. Write an AppSession interface and two implementations: HttpAppSession and MockAppSession. Make your server-side handlers depend on AppSession and not on HttpSession directly. Use Guice to inject HttpSession into HttpAppSession. That's the one you'll use in production and for actually running your app. within a real servlet container. The MockAppSession should not depend on HttpSession nor HttpServletRequest nor any other Guice Http scope. That's the one you'll use during testing. Now your Guice module should inject an AppSession implementation as follows: bind(AppSession.class).to(MockAppSession.class) bind(MockAppSession.class).in(Singleton.class) That'll sort you out.  The problem is just what it states. You are trying to access a scoped object but you are not currently in the scope. Most likely your test is asking the injector for a RequestScoped object or an object that has a RequestScoped object in the injection dependency tree but the test didn't do anything to enter the scope. Binding the GuiceFilter in the test doesn't help because your test isn't trying to send an HttpServletRequest through GuiceFilter to a servlet. The best option would be to unit test your code. Create your classes in isolation injecting mocks. Assuming you want to do some kind of integration test you have three options: Have your test install a test module that called bindScope(RequestScoped.class new FakeScope). The FakeScope class would implement Scope and have methods to enter and exit the scope. You may have to ""seed"" the scope with fake implementations of objects you depend on. See the Guice CustomScopes wiki page. This is the best option for integration tests IMHO Use ServletScopes.scopeRequest (Javadoc) to run part of the test code inside of a simulated request scope. This gets a bit ugly since you need to pass a Callable. Do a full end-to-end test. Start your server and send it requests using Selenium. It's really hard to get good coverage this way so I would leave this to things that you really need a browser to test. Things might get a bit messy if the class you are testing depends indirectly on HttpServletRequest or HttpServletResponse. These classes can be challenging to setup correctly. Most of your classes should not depend on the servlet classes directly or indirectly. If that is not the case you are either doing something wrong or you need to find a good action framework that allows you have most of your code not depend on these classes. Here's an example of approach 1 using SimpleScope from the Guice CustomScopes wiki page: public class LoggedInServiceTest extends TestCase { private final Provider<StandardDispatchService> serviceProvider; private final SimpleScope fakeRequestScope = new SimpleScope(); private final HttpServletRequest request = new FakeHttpServletRequest(); protected Injector createInjector() { return Guice.createInjector(new FakeRequestScopeModule() new LoggedInServiceModule(); } @Override protected void setUp() throws Exception { super.setUp(); Injector injector = createInjector(); scope.enter(); serviceProvider = injector.getProvider(StandardDispatchService.class); } @Override protected void tearDown() throws Exception { fakeRequestScope.exit() super.tearDown(); } public void testNotLoggedIn() { fakeRequestScope.enter(); // fill in values of request fakeRequestScope.seed(FakeHttpServletRequest.class request); StandardDispatchService service = serviceProvider.get(); GetProjectsAction action = new GetProjectsAction(); try { service.execute(action); fail(); } catch (NotLoggedInException expected) { } } private class FakeRequestScopeModule extends AbstractModule() { @Override protected void configure() { bind(RequestScoped.class fakeRequestScope); bind(HttpServletRequest.class) .to(FakeHttpServletRequest.class) .in(RequestScoped.class) } } } The class I'm ultimately testing depends on HttpServletRequest/HttpSession via @Inject Provider. Is there any way to get around the NullPointerException it throws when calling .get() on the injected Provider? Good luck! If you study the above code it becomes clear that modules are another possible injection point. If the class you are testing does not directly depend on `HttpServletRequest` then perhaps you setup a test module that lets you fake out the direct or indirect dependencies of your class. You can use `Modules.override()` to replace bindings in a production module with bindings in a test module. You have to seed the HttpServletRequest into the scope. I added a code example. Which version of Guice is this written against? I'm noticing that methods like the bind() in FakeRequestScopeModule seem to be moved to bindScope() etc. Even after those changes I see errors like: No implementation for javax.servlet.http.HttpServletRequest was bound. I used the latest Guice API docs but these methods have been around for as long as I've used Guice. `bindScope()` is for binding a `Scope` object to a binding annotation while `bind()` is for binding classes and interfaces to implementations. It would help if you would update your question with the complete stacktrace. I'll update the answer a bit more. Since `HttpServletRequest` is an interface some module needs to provide a binding for that interface. Since `HttpServetRequest` is bound as `RequestScoped` some module needs associate that annotation to a `Scope` object so Guice knows when to create a request and when to use a cached request. I believe in the live application `ServletModule` does both. I updated my code so `FakeRequestScopeModule` does both. You will need to provide a `FakeHttpServletRequest` implementation. As I said it's better to have your objects not depend on the servlet code so you don't have to do all this work. Thanks you very much for your help! I'm going to mark your answer as accepted."
109,A,"ClassNotFoundException on a JUNIT ant script I have written few test scripts using JUnit 4 and Selenium. I have added the jar files for JUnit and Selenium to eclipse and if I run my tests through eclipse IDE everything is working as expected. I am now trying to run these tests through the ant script below: <project name=""JUnit"" default=""test""> <property name=""src"" value=""./src"" /> <property name=""classes"" value=""./classes"" /> <property name=""test.class.name"" value=""AllTests"" /> <path id=""test.classpath""> <pathelement location=""${classes}"" /> <pathelement location=""C:/Program Files/eclipse 3.5/plugins/org.junit4_4.5.0.v20090824/junit.jar"" /> <pathelement location=""C:/selenium/selenium-server-standalone-2.0b2.jar"" /> <pathelement location=""C:/Program Files/eclipse 3.5/plugins/org.hamcrest.core_1.1.0.v20090501071000.jar"" /> </path> <target name=""test""> <junit fork=""yes"" haltonfailure=""yes""> <test name=""${test.class.name}"" /> <formatter type=""plain"" usefile=""false"" /> <classpath refid=""test.classpath"" /> </junit> </target> </project> The problem is that when I run this ant script I am getting the following Exception: [junit] java.lang.ClassNotFoundException: AllTests [junit] at java.net.URLClassLoader$1.run(Unknown Source) [junit] at java.security.AccessController.doPrivileged(Native Method) [junit] at java.net.URLClassLoader.findClass(Unknown Source) [junit] at java.lang.ClassLoader.loadClass(Unknown Source) [junit] at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) [junit] at java.lang.ClassLoader.loadClass(Unknown Source) [junit] at java.lang.Class.forName0(Native Method) [junit] at java.lang.Class.forName(Unknown Source) I have added all the jar files that I see in eclipse .classpath file so a bit confused of which files I am missing. Found the problem stupid one but I guess it was lack of concentration... My .class files are in the ./bin folder and the ant script was pointing to ./classes. I copied it from an old ant script I've been using and didn't take notice of the properties configured."
110,A,"Spring / JTA / JPA unit test : Rollback not working I am trying to test an entity EJB3 with Spring. The EJB itself does not uses Spring and I would like to keep duplications of the production JPA configuration minimal (ie not duplicating persistence.xml for exemple). My unit tests seems to work but even though my unit tests should be transactionnal data is persisted between the various test methods ... Here is my entity : package sample; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.Id; @Entity public class Ejb3Entity { public Ejb3Entity(String data) { super(); this.data = data; } private Long id; private String data; @Id @GeneratedValue public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getData() { return data; } public void setData(String data) { this.data = data; } } My unit test : package sample; import static org.junit.Assert.*; import javax.persistence.EntityManager; import javax.persistence.PersistenceContext; import org.junit.Before; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; import org.springframework.transaction.annotation.Transactional; @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""/appContext.xml""}) @Transactional public class Ejb3EntityTest { @PersistenceContext EntityManager em; @Before public void setUp() throws Exception { Ejb3Entity one = new Ejb3Entity(""Test data""); em.persist(one); } @Test public void test1() throws Exception { Long count = (Long) em.createQuery(""select count(*) from Ejb3Entity"").getSingleResult(); assertEquals(Long.valueOf(1l) count); } @Test public void test2() throws Exception { Long count = (Long) em.createQuery(""select count(*) from Ejb3Entity"").getSingleResult(); assertEquals(Long.valueOf(1l) count); } } and my appContext.xml : <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:tx=""http://www.springframework.org/schema/tx"" xmlns:context=""http://www.springframework.org/schema/context"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd""> <bean id=""jotm"" class=""org.springframework.transaction.jta.JotmFactoryBean"" /> <bean id=""transactionManager"" class=""org.springframework.transaction.jta.JtaTransactionManager""> <property name=""userTransaction"" ref=""jotm"" /> <property name=""allowCustomIsolationLevels"" value=""true"" /> </bean> <bean id=""dataSource"" class=""org.enhydra.jdbc.standard.StandardXADataSource""> <property name=""driverName"" value=""org.h2.Driver"" /> <property name=""url"" value=""jdbc:h2:mem:unittest;DB_CLOSE_DELAY=-1"" /> <property name=""user"" value="""" /> <property name=""password"" value="""" /> <property name=""transactionManager"" ref=""jotm"" /> </bean> <bean id=""emf"" class=""org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean""> <property name=""persistenceUnitPostProcessors""> <bean class=""sample.JtaDataSourcePersistenceUnitPostProcessor""> <property name=""jtaDataSource"" ref=""dataSource"" /> </bean> </property> <property name=""jpaVendorAdapter""> <bean class=""org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter""> <property name=""showSql"" value=""false"" /> <property name=""generateDdl"" value=""true"" /> <property name=""database"" value=""H2"" /> <property name=""databasePlatform"" value=""org.hibernate.dialect.H2Dialect"" /> </bean> </property> <property name=""jpaPropertyMap""> <map> <entry key=""hibernate.transaction.manager_lookup_class"" value=""org.hibernate.transaction.JOTMTransactionManagerLookup"" /> <entry key=""hibernate.transaction.auto_close_session"" value=""false"" /> <entry key=""hibernate.current_session_context_class"" value=""jta"" /> </map> </property> </bean> </beans> When I run my test test2 fails because it finds 2 entity where I expected only one (because the first one should have been rollbacked ...) I have tried a lot of different configurations and this one seems to be the most comprehensive I can get ... I have no other ideas. Do you ? Why do you think that the first entity should have been rolled back? Because I'm using the @Transactional annotation that makes each test run use its own transaction that is automatically rollbacked by Spring. When I was trying to integrate JOTM and Hibernate I eventually ended up having to code my implementation of ConnectionProvider. Here is what it looks like right now: http://pastebin.com/f78c66e9c Then you specify your implementation as the connection privider in hibernate properties and transactions magically start to work. The thing is that the default connection provider calls getConnection() on the datasource. In you own implementation you call getXAConnection().getConnection(). This makes the difference Sorry I ended using BTM instead of JOTM and I did not had the chance to do what you suggest.  Edit: (Sorry seems I was only half awake when I wrote this paragraph. Of course you're right everything should be rolled back by default.) You could check what the transaction manager is really doing for example by enabling debug output for it. Assuming log4j: log4j.logger.org.springframework.transaction=DEBUG The transaction manager gives you very nice log output about created and joined transactions and also about commits and rollbacks. That should help you find out what isn't working with your setup. Thanks for the suggestion. Having more logs helped a lot.  I managed to make it work using Bitronix instead of JOTM. Bitronix provides a LrcXADataSource that allows a non XA database to participate in the JTA transaction. I think the issues were that H2 is not XA compliant and the enhydra StandardXADataSource does not make it magically so (I also ended using HSQLDB but that is unrelated to the issue). Here is my spring context that works : <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:tx=""http://www.springframework.org/schema/tx"" xmlns:context=""http://www.springframework.org/schema/context"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd""> <context:annotation-config /> <tx:annotation-driven transaction-manager=""transactionManager"" /> <!-- Bitronix Transaction Manager embedded configuration --> <bean id=""btmConfig"" factory-method=""getConfiguration"" class=""bitronix.tm.TransactionManagerServices""> <property name=""serverId"" value=""spring-btm"" /> <property name=""journal"" value=""null"" /> </bean> <!-- create BTM transaction manager --> <bean id=""BitronixTransactionManager"" factory-method=""getTransactionManager"" class=""bitronix.tm.TransactionManagerServices"" depends-on=""btmConfigdataSource"" destroy-method=""shutdown"" /> <bean id=""transactionManager"" class=""org.springframework.transaction.jta.JtaTransactionManager""> <property name=""transactionManager"" ref=""BitronixTransactionManager"" /> <property name=""userTransaction"" ref=""BitronixTransactionManager"" /> <property name=""allowCustomIsolationLevels"" value=""true"" /> </bean> <!-- DataSource definition --> <bean id=""dataSource"" class=""bitronix.tm.resource.jdbc.PoolingDataSource"" init-method=""init"" destroy-method=""close""> <property name=""className"" value=""bitronix.tm.resource.jdbc.lrc.LrcXADataSource"" /> <property name=""uniqueName"" value=""unittestdb"" /> <property name=""minPoolSize"" value=""1"" /> <property name=""maxPoolSize"" value=""3"" /> <property name=""allowLocalTransactions"" value=""true"" /> <property name=""driverProperties""> <props> <prop key=""driverClassName"">org.hsqldb.jdbcDriver</prop> <prop key=""url"">jdbc:hsqldb:mem:unittestdb</prop> <prop key=""user"">sa</prop> <prop key=""password""></prop> </props> </property> </bean> <!-- Entity Manager Factory --> <bean id=""emf"" class=""org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean""> <property name=""dataSource"" ref=""dataSource"" /> <property name=""jpaVendorAdapter""> <bean class=""org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter""> <property name=""showSql"" value=""true"" /> <property name=""generateDdl"" value=""true"" /> <property name=""database"" value=""HSQL"" /> </bean> </property> <property name=""jpaPropertyMap""> <map> <entry key=""hibernate.transaction.manager_lookup_class"" value=""org.hibernate.transaction.BTMTransactionManagerLookup"" /> <entry key=""hibernate.transaction.auto_close_session"" value=""false"" /> <entry key=""hibernate.current_session_context_class"" value=""jta"" /> </map> </property> </bean> Just a week after posting here I ran into the same problem. I also couldn't get JOTM to rollback properly; it always said it was rolling back but the changes from the transaction still hit the database. BTM does the job just fine both with a MySQL and with a H2 backend. Weird."
111,A,"Small java project growing (maven and junit) I'm working on a growing java project and I'm probably going to cooperate with somebody else to improve some features. I'd like to use some tools to improve the quality of my work keeping in mind that: I don't have too much time to spend on this project it's a small project but it's really important for me I don't want to buy software/hardware for it I'm already using SVN what do you think about maven and junit? is it worth spending time for them? Do you know any other good tool? Maven and JUnit are good for enforcing good habits (unit testing uniform structure) and together with good SCM habits I would say those are amongst the most important things for collaborative development.  I've used maven on one project and miss it. There is a fairly large upfront investment though in getting it setup and configured. The XML documentation was out of date when i was working with it (perhaps this has improved). Once you get past this initial setup though it's a wonderful time saver. As for JUnit it's great. Use it. Both of these tools should be treated as an investment. At first it may seem like a lot of unrelated stuff but the project will grow more predictably with less problems over the long haul.  We use Maven and JUnit on a fairly large project and find it very helpful. For project planning I highly recommend FogBugz. It's the best issue tracking system I have seen to date with good support for project management as well and free for teams of up to 2 people. ""free for teams for up to 2 people"" sounds dangerous for a small project with growth potential. You don't want to be locked out of your issue tracker (or deny access to it for some people) if your group grows. It's far superior to any of the open source alternatives I have tried. If a commercial project has >2 people it's certainly worth considering a purchase. I'm not associated with the company at all other than being a customer (it's made by the makers of Stack Overflow by the way).  JUnit is good for helping verify your code on any project. Maven has a learning curve that can be hard to get over. If you have one module and a relatively simple set of build steps you may find it simpler to use Ant. On the other hand with a Maven build you can simply add additional reports to your code to check various parameters on your code and it is much harder to migrate to Maven than if you've conformed to its conventions from the start. Examples of Maven plugins that can help check your code: Findbugs (static analysis of possible bugs) Checkstyle (enforce coding standards) PMD (more static analysis) PMD CPD (copy paste detection) JDepend (cyclic dependency checking and package coupling) Cobertura (code coverage) If you're interested in the code quality plugins also consider Sonar it wraps these plugins up and gives you some funky reports. If you're interested in best practice also consider a Continuous Integration server Hudson is free and integrates well with Maven. +1 for findbugs  Since you are not using JUnit my guess is you don't have any unit tests yet. This would seem to me the most important step for you to take if other people will start working on your code. Without unit tests someone can easily break functionality without knowing it. Create a suite of unit tests that cover at least 80% of the code. You can use Cobertura to measure code coverage. This might seem like a lot of work (it is) but will save you far more time in the future. Maven is the de-facto standard for building and deployment at the moment but it has its drawbacks too. If you have a well documented build procedure in place (either using Ant or custom scripts) I would suggest it is less important to introduce Maven than to add unit tests. I like Maven too - but in reality it's probably going to take more than a few days for him to 1.) learn enough about Maven to see how his existing project would fit into Maven. 2.) change his project structure to match what Maven expects (depending on how big it is and how much it differs). adding tests can take months but adding maven can take days.  If you use Eclipse and SVN I would recommend you to take a look at Mylyn. Its underlying concepts are very simple but it helps a lot when working on a team. In my modest opinion Maven is too annoying for the real benefit of it. Maybe ant is just enough for your deployment tasks."
112,A,"Do any tools use the hamcrest Factory annotation? I sat down to write a matcher today and decided to take a quick look at the jmock documentation to refresh my memory on the process and noticed a reference to the org.hamcrest.Factory annotation. The documentation for the annotation states. Marks a Hamcrest static factory method so tools recognise them. A factory method is an equivalent to a named constructor. Do any tools actually use this annotation? As explained in the Hamcrest tutorial the Factory annotation is used by a Hamcrest code generator org.hamcrest.generator.config.XmlConfigurator. It generates a Java source file that contains all factories from a configured set of classes so that you can statically import all of them by using a single static import. I have not used this feature yet because I manually collect my self-written matchers in a factory class as soon as I write the matcher and on usage I statically import each factory method by itself (using the Eclipse ""Favorites"" feature for auto-import)."
113,A,"Is it required to write custom matchers for this Mockito situation? I have the following situation... I want to throw an exception whenever a particular method is called. I don't care what is passed in to this method I just want an exception to be called. The catch is that this method takes custom classes as parameters. The only way I have found to do this is with the following: // Matches any Foo TypeSafeMatcher<Foo> fooMatcher = new TypeSafeMatcher<Foo>() { @Override public Boolean matchesSafely(Foo foo) { return true; } // more overrides } doThrow(new RuntimeException(""dummy exception"")).when(mockObj).someMethod(fooMatcher); I tried doing the following but I'm getting a message similar to ""can't convert Object to Foo""... which makes sense: doThrow(new RuntimeException(""dummy exception"")).when(mockObj).someMethod(anyObject()); I'm wondering if there is an easier way to do this without having to create a custom matcher for each custom class? Thanks have you tried the isA matcher? doThrow(new RuntimeException(""dummy exception"")).when(mockObj).someMethod(isA(Foo.class));  A cast may suffice: doThrow(new RuntimeException(""dummy exception"")).when(mockObj).someMethod((Foo)anyObject());  There is also doAnswer() if you want to do something different than raising an exception doAnswer(new Answer() { public Object answer(InvocationOnMock invocation) { Object[] args = invocation.getArguments(); Mock mock = invocation.getMock(); return null; }}) .when(mock).someMethod();"
114,A,What is the best way to write a JUnit test against a method signature that calls a Web service? The method I want to test has a local variable that references an object returned from a Web service call. This Web service returns information specific to a particular user based upon input from that user on a Web page. It is like a question/answer where the answer given in a form text field must match the answer provided by the user at an earlier date and is now tied from their account. There are certain things that happen based upon whether or not they got the answer correct. Should I mock out the service as a field on the class instead where I can set it to return a dummy service response or is there a better way to test it? Thanks! I think your intuition is good on this one to mock out the service. I agree. It's a unit test. Isolate the subject under test from other dependencies. Exactly. You can test the other dependencies independently. JMock might be helpful for this.
115,A,"Spring 3 replacement for isDisabledInThisEnvironment I'm currently converting a test class which extended the Spring class AbstractTransactionalSpringContextTests to Spring 3. This abstract class is now deprecated and I should use AbstractJUnit38SpringContextTests. The test class had this method in it: @Override protected boolean isDisabledInThisEnvironment(String testMethodName) { // Test is only needed for bugfixing and development. Do not check in with this flag on false. return true; } What is the replacement for this isDisabledInThisEnvironment method? This is a feature provided by `ConditionalTestCase` to get around the fact that JUnit doesn't have the concept of ""skip test"" like TestNG does. I don't think there *is* a replacement for this is was likely overlooked during the transition to JUnit4. This concept is implemented with @IfProfileValue. Works with both JUnit and TestNG. Note: Spring `@IfProfileValue` only **skips/ignores** the test if the name and value doesn't match and it doesn't **not-run** it. If you want to implement JUnit Categories type of implementation with this you may need to end up extending `SpringJUnit4ClassRunner#runChild` to _not-run_ instead of _ignoring_. This mainly matters if its results are in jenkins."
116,A,"unit testing with junit4 how do i create a test suite in junit4 ?? possible duplicate of [Junit4 Test Suites](http://stackoverflow.com/questions/457276/junit4-test-suites) Many IDEs have built in JUnit functionality which supply an easy GUI method of setting up the basic code. Which IDE do you use? Please read the documentation - http://junit.sourceforge.net/ Unit Testing is really easy and best explained on a simple example. We'll have the following class calculating the average of an array: package com.stackoverflow.junit; public class Average { public static double avg(double[] avg) { double sum = 0; // sum all values for(double num : avg) { sum += num; } return sum / avg.length; } } Our JUnit test will now test some basic operations of this method: package com.stackoverflow.junit; import junit.framework.TestCase; public class AverageTest extends TestCase { public void testOneValueAverage() { // we expect the average of one element (with value 5) to be 5 the 0.01 is a delta because of imprecise floating-point operations double avg1 = Average.avg(new double[]{5}); assertEquals(5 avg1 0.01); double avg2 = Average.avg(new double[]{3}); assertEquals(3 avg2 0.01); } public void testTwoValueAverage() { double avg1 = Average.avg(new double[]{5 3}); assertEquals(4 avg1 0.01); double avg2 = Average.avg(new double[]{7 2}); assertEquals(4.5 avg2 0.01); } public void testZeroValueAverage() { double avg = Average.avg(new double[]{}); assertEquals(0 avg 0.01); } } The first two test cases will show that we implemented the method correct but the last test case will fail. But why? The length of the array is zero and we are diving by zero. A floating point number divided by zero is not a number (NaN) not zero. You should have continued reading the question after the title ;-) i have read your post ;) But at the beginning a short self-compiling example is worth more than some basic explaniton or class ""wrapper"" showing how to define a test suite but not any test case It's not my post the question is about a **test suite** (i.e. a grouping of test cases) and about JUnit 4 (your example shows a JUnit 3-style test). oh your're right. he has written test case in the title and therefore i have read test case in his ""description"" again my fault.  Here is one example: @RunWith(Suite.class) @Suite.SuiteClasses( { TestClass1.class TestClass2.class }) public class DummyTestSuite { }"
117,A,"Changing names of parameterized tests Is there a way to set my own custom test case names when using parameterized tests in JUnit4? I'd like to change the default — [Test class].runTest[n] — to something meaningful. Could you please accept the new answer from rescdsk as it's the correct answer? yup just did exactly that. You can create a method like @Test public void name() { Assert.assertEquals("""" inboundFileName); } While I wouldn't use it all the time it would be useful to figure out exactly which test number 143 is.  You may also want to try JUnitParams: http://code.google.com/p/junitparams/  A workaround would be to catch and nest all Throwables into a new Throwable with a custom message that contains all information about the parameters. The message would appear in the stack trace. This works whenever a test fails for all assertions errors and exceptions as they are all subclasses of Throwable. My code looks like this: @RunWith(Parameterized.class) public class ParameterizedTest { int parameter; public ParameterizedTest(int parameter) { super(); this.parameter = parameter; } @Parameters public static Collection<Object[]> data() { return Arrays.asList(new Object[][] { {1} {2} }); } @Test public void test() throws Throwable { try { assertTrue(parameter%2==0); } catch(Throwable thrown) { throw new Throwable(""parameter=""+parameter thrown); } } } The stack trace of the failed test is: java.lang.Throwable: parameter=1 at sample.ParameterizedTest.test(ParameterizedTest.java:34) Caused by: java.lang.AssertionError at org.junit.Assert.fail(Assert.java:92) at org.junit.Assert.assertTrue(Assert.java:43) at org.junit.Assert.assertTrue(Assert.java:54) at sample.ParameterizedTest.test(ParameterizedTest.java:31) ... 31 more  With Parameterized as a model I wrote my own custom test runner / suite -- only took about half an hour. It's slightly different from darrenp's LabelledParameterized in that it lets you specify a name explicitly rather than relying on the first parameter's toString(). It also doesn't use arrays because I hate arrays. :) public class PolySuite extends Suite { // ////////////////////////////// // Public helper interfaces /** * Annotation for a method which returns a {@link Configuration} * to be injected into the test class constructor */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public static @interface Config { } public static interface Configuration { int size(); Object getTestValue(int index); String getTestName(int index); } // ////////////////////////////// // Fields private final List<Runner> runners; // ////////////////////////////// // Constructor /** * Only called reflectively. Do not use programmatically. * @param c the test class * @throws Throwable if something bad happens */ public PolySuite(Class<?> c) throws Throwable { super(c Collections.<Runner>emptyList()); TestClass testClass = getTestClass(); Class<?> jTestClass = testClass.getJavaClass(); Configuration configuration = getConfiguration(testClass); List<Runner> runners = new ArrayList<Runner>(); for (int i = 0 size = configuration.size(); i < size; i++) { SingleRunner runner = new SingleRunner(jTestClass configuration.getTestValue(i) configuration.getTestName(i)); runners.add(runner); } this.runners = runners; } // ////////////////////////////// // Overrides @Override protected List<Runner> getChildren() { return runners; } // ////////////////////////////// // Private private Configuration getConfiguration(TestClass testClass) throws Throwable { return (Configuration) getConfigMethod(testClass).invokeExplosively(null); } private FrameworkMethod getConfigMethod(TestClass testClass) { List<FrameworkMethod> methods = testClass.getAnnotatedMethods(Config.class); if (methods.isEmpty()) { throw new IllegalStateException(""@"" + Config.class.getSimpleName() + "" method not found""); } if (methods.size() > 1) { throw new IllegalStateException(""Too many @"" + Config.class.getSimpleName() + "" methods""); } FrameworkMethod method = methods.get(0); int modifiers = method.getMethod().getModifiers(); if (!(Modifier.isStatic(modifiers) && Modifier.isPublic(modifiers))) { throw new IllegalStateException(""@"" + Config.class.getSimpleName() + "" method \"""" + method.getName() + ""\"" must be public static""); } return method; } // ////////////////////////////// // Helper classes private static class SingleRunner extends BlockJUnit4ClassRunner { private final Object testVal; private final String testName; SingleRunner(Class<?> testClass Object testVal String testName) throws InitializationError { super(testClass); this.testVal = testVal; this.testName = testName; } @Override protected Object createTest() throws Exception { return getTestClass().getOnlyConstructor().newInstance(testVal); } @Override protected String getName() { return testName; } @Override protected String testName(FrameworkMethod method) { return testName + "": "" + method.getName(); } @Override protected void validateConstructor(List<Throwable> errors) { validateOnlyOneConstructor(errors); } @Override protected Statement classBlock(RunNotifier notifier) { return childrenInvoker(notifier); } } } And an example: @RunWith(PolySuite.class) public class PolySuiteExample { // ////////////////////////////// // Fixture @Config public static Configuration getConfig() { return new Configuration() { @Override public int size() { return 10; } @Override public Integer getTestValue(int index) { return index * 2; } @Override public String getTestName(int index) { return ""test"" + index; } }; } // ////////////////////////////// // Fields private final int testVal; // ////////////////////////////// // Constructor public PolySuiteExample(int testVal) { this.testVal = testVal; } // ////////////////////////////// // Test @Ignore @Test public void odd() { assertFalse(testVal % 2 == 0); } @Test public void even() { assertTrue(testVal % 2 == 0); } } Wow !Nice Job ...  None of it was working for me so I got the source for Parameterized and modified it create a a new test runner. I didn't have to change much but IT WORKS!!! import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; import java.lang.reflect.Constructor; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; import java.lang.reflect.Modifier; import java.util.ArrayList; import java.util.Arrays; import java.util.Collection; import java.util.List; import org.junit.Assert; import org.junit.internal.runners.ClassRoadie; import org.junit.internal.runners.CompositeRunner; import org.junit.internal.runners.InitializationError; import org.junit.internal.runners.JUnit4ClassRunner; import org.junit.internal.runners.MethodValidator; import org.junit.internal.runners.TestClass; import org.junit.runner.notification.RunNotifier; public class LabelledParameterized extends CompositeRunner { static class TestClassRunnerForParameters extends JUnit4ClassRunner { private final Object[] fParameters; private final String fParameterFirstValue; private final Constructor<?> fConstructor; TestClassRunnerForParameters(TestClass testClass Object[] parameters int i) throws InitializationError { super(testClass.getJavaClass()); // todo fParameters = parameters; if (parameters != null) { fParameterFirstValue = Arrays.asList(parameters).toString(); } else { fParameterFirstValue = String.valueOf(i); } fConstructor = getOnlyConstructor(); } @Override protected Object createTest() throws Exception { return fConstructor.newInstance(fParameters); } @Override protected String getName() { return String.format(""%s"" fParameterFirstValue); } @Override protected String testName(final Method method) { return String.format(""%s%s"" method.getName() fParameterFirstValue); } private Constructor<?> getOnlyConstructor() { Constructor<?>[] constructors = getTestClass().getJavaClass().getConstructors(); Assert.assertEquals(1 constructors.length); return constructors[0]; } @Override protected void validate() throws InitializationError { // do nothing: validated before. } @Override public void run(RunNotifier notifier) { runMethods(notifier); } } @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public static @interface Parameters { } private final TestClass fTestClass; public LabelledParameterized(Class<?> klass) throws Exception { super(klass.getName()); fTestClass = new TestClass(klass); MethodValidator methodValidator = new MethodValidator(fTestClass); methodValidator.validateStaticMethods(); methodValidator.validateInstanceMethods(); methodValidator.assertValid(); int i = 0; for (final Object each : getParametersList()) { if (each instanceof Object[]) add(new TestClassRunnerForParameters(fTestClass (Object[]) each i++)); else throw new Exception(String.format(""%s.%s() must return a Collection of arrays."" fTestClass.getName() getParametersMethod().getName())); } } @Override public void run(final RunNotifier notifier) { new ClassRoadie(notifier fTestClass getDescription() new Runnable() { public void run() { runChildren(notifier); } }).runProtected(); } private Collection<?> getParametersList() throws IllegalAccessException InvocationTargetException Exception { return (Collection<?>) getParametersMethod().invoke(null); } private Method getParametersMethod() throws Exception { List<Method> methods = fTestClass.getAnnotatedMethods(Parameters.class); for (Method each : methods) { int modifiers = each.getModifiers(); if (Modifier.isStatic(modifiers) && Modifier.isPublic(modifiers)) return each; } throw new Exception(""No public static parameters method on class "" + getName()); } public static Collection<Object[]> eachOne(Object... params) { List<Object[]> results = new ArrayList<Object[]>(); for (Object param : params) results.add(new Object[] { param }); return results; } }  This feature has made it into JUnit 4.11. To use change the name of parameterized tests you say: @Parameters(name=""namestring"") namestring is a string which can have the following special placeholders: {index} - the index of this set of arguments. The default namestring is {index}. {0} - the first parameter value from this invocation of the test. {1} - the second parameter value and so on For example (from the unit test for the Parameterized annotation) @RunWith(Parameterized.class) static public class FibonacciTest { @Parameters(name= ""{index}: fib({0})={1}"") public static Iterable<Object[]> data() { return Arrays.asList(new Object[][] { { 0 0 } { 1 1 } { 2 1 } { 3 2 } { 4 3 } { 5 5 } { 6 8 } }); } private final int fInput; private final int fExpected; public FibonacciTest(int input int expected) { fInput= input; fExpected= expected; } @Test public void testFib() { assertEquals(fExpected fib(fInput)); } private int fib(int x) { // TODO: actually calculate Fibonacci numbers return 0; } } will give names like testFib[1: fib(1)=1] and testFib[4: fib(4)=3]. (The testFib part of the name is the method name of the @Test). There is no reason it would not be in 4.11 it's in master. Now when 4.11 will be available that is a good question :-) 4.11 is now in beta and can be downloaded from the same link as above :-) Yes but there is a bug. If you put a parenthesis in the parameter ""name"" value like you are doing in this posting it breaks the display of the unit test name in Eclipse. @djangofan are you saying that's an Eclipse bug? I'm not working with Eclipse on a daily basis anymore so I can't confirm. Or is it a jUnit bug? @rescdsk - im not sure. the parameter name that is displayed in the JUNit tab in eclipse gets munged (when parenthesis exists in the string). great but what if `{0}` and `{1}` are arrays? JUnit should ideally call `Arrays.toString({0})` not `{0}.toString()`. For example my `data()` method returns `Arrays.asList(new Object[][] {{ new int[] { 1 3 2 } new int[] { 1 2 3 } }});`. @Pool - yes its a related bug but I doubt its the same. @djangofan This is an 8 year old Eclipse bug: https://bugs.eclipse.org/bugs/show_bug.cgi?id=102512 @Pool - that is not the same bug. the bug I speak of is for JUnit version 4.11 only. @djangofan - I believe it will only show up with JUnit version 4.11 due to the possibility to add brackets. You wrote `the parameter name that is displayed in the JUNit tab in eclipse gets munged (when parenthesis exists in the string)`. The bug report details that Eclipse removes detail in brackets and that it is done deliberately. Isn't this what you meant by munged? @djangofan - OK so what happens for the name for you? The characters are corrupted and display garbled? That sounds a more serious issue and it would be worth reporting the bug if it is still occurring.  I recently came across the same problem when using JUnit 4.3.1. I implemented a new class which extends Parameterized called LabelledParameterized. It has been tested using JUnit 4.3.1 4.4 and 4.5. It reconstructs the Description instance using the String representation of the first argument of each parameter array from the @Parameters method. You can see the code for this at: http://code.google.com/p/migen/source/browse/trunk/java/src/.../LabelledParameterized.java?r=3789 and an example of its use at: http://code.google.com/p/migen/source/browse/trunk/java/src/.../ServerBuilderTest.java?r=3789 The test description formats nicely in Eclipse which is what I wanted since this makes failed tests a lot easier to find! I will probably further refine and document the classes over the next few days/weeks. Drop the '?' part of the URLs if you want the bleeding edge. :-) To use it all you have to do is copy that class (GPL v3) and change @RunWith(Parameterized.class) to @RunWith(LabelledParameterized.class) assuming the first element of your parameter list is a sensible label. I don't know if any later releases of JUnit address this issue but even if they did I can't update JUnit since all my co-developers would have to update too and we have higher priorities than re-tooling. Hence the work in the class to be compilable by multiple versions of JUnit. Note: there is some reflection jiggery-pokery so that it runs across the different JUnit versions as listed above. The version specifically for JUnit 4.3.1 can be found here and for JUnit 4.4 and 4.5 here. nice! I'll have to take a look at that... :-) One of my co-developers today had a problem with it since the version I point to in the above message uses JUnit 4.3.1 (not 4.4 as I originally said). He is using JUnit 4.5.0 and it caused problems. I'll be addressing these today. That works real nice thank you! I took some time to understand that you need to _pass_ the test name in the constructor but not _memorize_ it. Thanks for the code! Works great as long as I run the tests from Eclipse. Does anyone have experience with making it work with the JUnit Ant Task though? The test reports are named `execute[0] execute[1] ... execute[n]` in the test reports generated. Very nice. Works like a charm. Would be nice if you could add the info that it is required to add ""String label ..."" as first parameter to the invoked @Test-method.  Looking at JUnit 4.5 its runner clearly doesn't support that as that logic is buried inside a private class inside the Parameterized class. You could not use the JUnit Parameterized runner and create your own instead which would understand the concept of names (which leads to the question of how you might set a name ...). From a JUnit perspective it would be nice if instead of (or in addition to) just passing an increment they would pass the comma delimited arguments. TestNG does this. If the feature is important to you you can comment on the yahoo mailing list referenced at www.junit.org. that's a bummer! I would highly appreciate if there's a improvement for this in JUnit! Instead of passing an argument perhaps just using the toString() on the test class. I imagine adding this feature to JUnit would be pretty straightforward. Just checked there is an outstanding feature request for this at: http://github.com/KentBeck/junit/issues#issue/44 Please vote it up. Looks like it made it in and will be in the next release of JUnit :-) https://github.com/KentBeck/junit/commit/3a5c9f2731462e36dd1c173ea8840d7b9b34b0ab @Frank I think that the release that addresses this issue is not yet released. It will be in JUnit 4.11. At that time (assuming the design remains the same) it will be about a textual way of specifying how you name the test including taking parameters as names. Pretty nice actually. Whats the current answer regarding this issue? JUnit 4.11 has now been released :-) Here is the updated link to the original issue https://github.com/junit-team/junit/issues/44 for future reference  I make extensive use of static import for Assert and friends so it is easy for me to redefine assertion: private <T> void assertThat(final T actual final Matcher<T> expected) { Assert.assertThat(editThisToDisplaySomethingForYourDatum actual expected); } For example you could add a ""name"" field to your test class initialized in the constructor and display that on test failure. Just pass it in as the first elements of your parameters array for each test. This also helps label the data: public ExampleTest(final String testLabel final int one final int two) { this.testLabel = testLabel; // ... } @Parameters public static Collection<Object[]> data() { return asList(new Object[][]{ {""first test"" 3 4} {""second test"" 5 6} }); } This is fine if the test fails an assert but there are other cases such as if an exception is thrown that fails the test or if the test is expecting an exception to be thrown that make thinking of the name overhead that should be handled by the framework.  Check out JUnitParams as dsaff mentioned works using ant to build parameterized test method descriptions in the html report. This was after trying LabelledParameterized and finding that it although it works with eclipse it does not work with ant as far as the html report is concerned. Cheers  from junit4.8.2 you can create your own MyParameterized class by simply copy Parameterized class. change the getName() and testName() methods in TestClassRunnerForParameters. I tried this but doesn't help. While creating new class getParametersMethod fails."
118,A,"Junit: splitting integration test and Unit tests I've inherited a load of Junit test but these tests (apart from most not working) are a mixture of actual unit test and integration tests (requiring external systems db etc). So I'm trying to think of a way to actually separate them out so that I can run the unit test nice and quickly and the integration tests after that. The options are.. Split them into separate directories. Move to Junit4 (from v3) and annotate the classes to separate them. Use a file naming convention to tell what a class is  i.e. AdapterATest and AdapterAIntergrationTest. 3 has the issue that Eclipse has the option to ""Run all tests in the selected project/package or folder"". So it would make it very hard to just run the integration tests. 2: runs the risk that developers might start writing integration tests in unit test classes and it just gets messy. 1: Seems like the neatest solution but my gut says there must be a better solution out there. So that is my question how do you lot break apart integration tests and proper unit tests? I would just like to thank you all for you input I know this is a subjective question and doesn't one correct answer. But you have helped me realise that there isn't any other options than the ones I've listed. I think I'm going to go with the directory stucture for the moment and move to JUnit4 although not use annotations for splitting them up just yet. I would move up to Junit4 just for having it :) You could separate them into different test suites. I don't know how they are organised in Junit3 but it should be easy in Junit4 just to build up test suites and put all the real unit tests in one of them and then use a second suite for the integration tests. Now define a run configuration for both suites in eclipse and you can easily run a single suite. These suites also could be launched from an automated process allowing you to run the unit tests every time the source changes and maybe the integration tests (if they are really large) only once a day or once an hour.  There's not one right answer. As you've explained there are several ways to do it which will work. I've done both the file naming scheme and splitting things into different directories. It sounds like splitting thing up into different directories might work better for you and that seems a little clearer to me so I'd lean towards that. I don't think I would try annotations because that seems more fine-grained to me. Do you really want these two types of tests mixed together in the same file? I wouldn't.  You can split them very easily using JUnit categories and Maven. This is shown very very briefly below by splitting unit and integration tests. Define A Marker Interface The first step in grouping a test using categories is to create a marker interface. This interface will be used to mark all of the tests that you want to be run as integration tests. public interface IntegrationTest {} Mark your test classes Add the category annotation to the top of your test class. It takes the name of your new interface. import org.junit.experimental.categories.Category; @Category(IntegrationTest.class) public class ExampleIntegrationTest{ @Test public void longRunningServiceTest() throws Exception { } } Configure Maven Unit Tests The beauty of this solution is that nothing really changes for the unit test side of things. We simply add some configuration to the maven surefire plugin to make it to ignore any integration tests. <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.11</version> <dependencies> <dependency> <groupId>org.apache.maven.surefire</groupId> <artifactId>surefire-junit47</artifactId> <version>2.12</version> </dependency> </dependencies> <configuration> <includes> <include>**/*.class</include> </includes> <excludedGroups>com.test.annotation.type.IntegrationTest</excludedGroups> </configuration> </plugin> When you do a mvn clean test only your unmarked unit tests will run. Configure Maven Integration Tests Again the configuration for this is very simple. To run only the integration tests use this: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.11</version> <dependencies> <dependency> <groupId>org.apache.maven.surefire</groupId> <artifactId>surefire-junit47</artifactId> <version>2.12</version> </dependency> </dependencies> <configuration> <groups>com.test.annotation.type.IntegrationTest</groups> </configuration> </plugin> If you wrap this in a profile with id IT you can run only the fast tests using mvn clean install. To run just the integration/slow tests use mvn clean install -P IT. But most often you will want to run the fast tests by default and all tests with -P IT. If that's the case then you have to use a trick: <profiles> <profile> <id>IT</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <excludedGroups>java.io.Serializable</excludedGroups> <!-- An empty element doesn't overwrite so I'm using an interface here which no one will ever use --> </configuration> </plugin> </plugins> </build> </profile> </profiles> As you can see I'm excluding tests that are annotated with java.io.Serializable. This is necessary because the profile will inherit the default config of the Surefire plugin so even if you say <excludedGroups/> or <excludedGroups></excludedGroups> the value com.test.annotation.type.IntegrationTest will be used. You also can't use none since it has to be an interface on the classpath (Maven will check this). Notes: The dependency to surefire-junit47 is only necessary when Maven doesn't switch to the JUnit 4 runner automatically. Using the groups or excludedGroups element should trigger the switch. See here. Most of the code above was taken from the documentation for the Maven Failsafe plugin. See the section ""Using JUnit Categories"" on this page. During my tests I found that this even works when you use @RunWith() annotations to run suites or Spring-based tests. Thanks for doing that! I think there's an error in your last pom.xml fragment. you pasted the same snippet as for ""test"" phase. it still excludes the integration tests and also is not bound to any maven phase. Indeed the last pom fragment is a copy&paste mistake. It should show the maven-failsafe-plugin. So what should be the second xml then? :O ill upvote as soon as the second pom is fixed  We use Maven Surefire Plugin to run unit tests and Maven Failsafe Plugin to run integration tests. Unit tests follow the **/Test*.java **/*Test.java **/*TestCase.java naming conventions integration tests - **/IT*.java **/*IT.java **/*ITCase.java. So it's actually your option number three. In a couple of projects we use TestNG and define different test groups for integration/unit tests but this is probably not suitable for you. +1 for the maven + surefire + failsafe + junit combo. I didn't realize failsafe would run ""IT*"" automagically. Sweet.  I currently use separate directories due to organisational policy (and Junit 3 legacy) but I'm looking to transition to annotations myself now I'm on Junit 4. I wouldn't be overly concerned about developers putting integration tests in your unit test classes - add a rule in your coding standards if necessary. I'm interested to know what sort of other solutions there might be apart from annotations or physically separating the classes.."
119,A,"Using @Override in JUnit4 I am using Selenium RC and JUnit to write tests. If a @Test fails I would like to take a screen shot but I want to use a specific screenshot name which is passed from the Test class. Currently I have : @Rule public ScreenshotRule email = new ScreenshotRule(); @Test public void testLogin() throws InterruptedException { MyTest someTest = new MyTest(selenium); someTest.someMethod (); // if anything assert fails in this method i want to take a shot and call it a name which i will pull from the MyTest class. } Here is the Screenshot class : public class ScreenshotRule extends TestWatchman { @Override public void failed(Throwable e FrameworkMethod method) { System.out.println(method.getName() + "" failed""); // At this point I wish to take a screenshot and call } } So how do I pass a String arg from the MyTest class to the Screenshot class? The TestWatchman signature does not permit you to pass any additional information so you'll have to work with what you have. For example the FrameworkMethod class has methods to fetch the unit test's java.lang.reflect.Method object so you could just use the name of that Method object for your screenshot. It's not what you wanted but it's better than nothing. You could also call the getDeclaringClass() on the Method to fetch the Class representing the test class but you won't be able to fetch the instance of that class which would be useful. P.S. I'm not sure what @Override has to do with your question. How is it relevant? Thanks I used the getName () method in the end to give some names to my screenshot."
120,A,"Issue selenium code maintenance I want to group the common methods in one file and use it. For example login to a page using selenium may be used in multiple times. Define that in class A and call it in class B. However it throws null pointer exception. class A has public void test_Login() throws Exception { try{ selenium.setTimeout(""60000""); selenium.open(""http://localhost""); selenium.windowFocus(); selenium.windowMaximize(); selenium.windowFocus(); selenium.type(""userName"" ""admin""); selenium.type(""password"" ""admin""); Result=selenium.isElementPresent(""//input[@type='image']""); selenium.click(""//input[@type='image']""); selenium.waitForPageToLoad(Timeout); } catch(Exception ex) { System.out.println(ex); ex.printStackTrace(); } } with all other java syntax in class B public void test_kk() throws Exception { try { a.test_Login(); } catch(Exception ex) { System.out.println(ex); ex.printStackTrace(); } } with all syntax. When I execute class B I got this error java.lang.NullPointerException at A.test_Login(A.java:32) at B.test_kk(savefile.java:58) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at junit.framework.TestCase.runTest(TestCase.java:168) at junit.framework.TestCase.runBare(TestCase.java:134) at com.thoughtworks.selenium.SeleneseTestCase.runBare(SeleneseTestCase.j ava:212) at junit.framework.TestResult$1.protect(TestResult.java:110) at junit.framework.TestResult.runProtected(TestResult.java:128) at junit.framework.TestResult.run(TestResult.java:113) at junit.framework.TestCase.run(TestCase.java:124) at junit.framework.TestSuite.runTest(TestSuite.java:232) at junit.framework.TestSuite.run(TestSuite.java:227) at junit.textui.TestRunner.doRun(TestRunner.java:116) at junit.textui.TestRunner.doRun(TestRunner.java:109) at junit.textui.TestRunner.run(TestRunner.java:77) at B.main(B.java:77) I hope someone must have tried this before. I may miss something here. I've done that. It opens the browser but fail to put the address there. If some sequences of actions needs to done repeatedly in various places I want to have that actions under a method call that method wherever needed. for example take the login method below. In every suite instead writing the entire code I want to have that in a class file say A and call that in class B class C etc. login() { open webpage enter username enter password click submit } This is the common way programmers used to maintain code. I am just trying that with Selenese. Can you add how your do the setup to this question? Make sure that the selenium server is started and that you start selenium browser before running this code How is the selenium object initialized in class A? Do you remember to pass it in from where it gets created in class B? If that is the way it works that is it's hard to see when that part of the code is not included... I dont want to initialize anything in class A. Just the part of code is there to call from class B. Is that incorrect? I don't know I can't see the scopes in the code you posted but it seems that if you intialize the selenium object in class B then you have to pass it to class A somehow (like through the constructor maybe). Sorry maybe that was unclear - I mean apparently some magic happens the initializes the selenium property of an object that extends SeleneseTestCase when the test is run. When you try to call another class from inside the test that property is not initialized and therefore you have to pass in the value of the selenium property from the outer class. So add an argument to your class A constructor an use that to initalize the selenium property in A. From B you would then create a new A by doing new A(selenium) or something. If this is the problem at all :-) I will check this initialization thing.  The way we do it is we have helper classes with static methods on them. In the actual test cases we set up our selenium object and pass the object into the static method so it can operate on it. public BaseHelper { public static login( final String username final String password final DefaultSelenium selenium ) { selenium.type(""userName"" username); selenium.type(""password"" password); selenium.click(""loginbutton""); } } public LoginTest { DefaultSelenium selenium; onSetup() { selenium = new DefaultSelenium(...); } public testLogin() { BaseHelper.login( ""admin"" ""admin"" selenium); // assert that this passed BaseHelper.login( ""foo"" ""bar"" selenium); // assert this failed because no user 'foo' BaseHelper.login( ""admin"" ""bar"" selenium); // assert this failed because admin's password was incorrect } } Hope this illustrates the point. Besides the better readability and easier maintenance you are also able to test multiuser behavior by creating two (or more) selenium objects and pass those around in the tests."
121,A,"How do I perform a Unit Test using threads? Executive Summary: When assertion errors are thrown in the threads the unit test doesn't die. This makes sense since one thread shouldn't be allowed to crash another thread. The question is how do I either 1) make the whole test fail when the first of the helper threads crashes or 2) loop through and determine the state of each thread after they have all completed (see code below). One way of doing the latter is by having a per thread status variable e.g. ""boolean[] statuses"" and have ""statuses[i] == false"" mean that the thread failed (this could be extended to capture more information). However that is not what I want: I want it to fail just like any other unit test when the assertion errors are thrown. Is this even possible? Is it desirable? I got bored and I decided to spawn a bunch of threads in my unit test and then have them call a service method just for the heck of it. The code looks approximately like: Thread[] threads = new Thread[MAX_THREADS]; for( int i = 0; i < threads.length; i++ ) { threads[i] = new Thread( new Runnable() { private final int ID = threadIdSequenceNumber++; public void run() { try { resultRefs[ID] = runTest( Integer.toString( ID ) ); // returns an object } catch( Throwable t ) { // this code is EVIL - it catches even // Errors - don't copy it - more on this below final String message = ""error testing thread with id => "" + ID; logger.debug( message t ); throw new IllegalStateException( message t ); // need to wrap throwable in a // run time exception so it will compile } } } ); } After this we will loop through the array of threads and start each one. After that we will wait for them all to finish. Finally we will perform some checks on the result references. for( Thread thread : threads ) thread.start(); logger.debug( ""waiting for threads to finish ..."" ); boolean done = false; while( !done ) { done = true; for( Thread thread : threads ) if( thread.isAlive() ) done = false; } for( int i = 0; i < resultRefs.length; i++ ) { assertTrue( ""you've got the world messed dawg!"" myCondition(resultRefs[i]) ); Here's the problem. Did you notice that nasty try-catch-throwable block? I just added that as a temporary hack so I could see what was going on. In runTest( String ) a few assertions are made e.g. assertNotNull( null ) but since it is in a different thread it doesn't cause the unit test to fail!!!! My guess is that we will need to somehow iterate over the threads array check the status of each and manually cause an assertion error if the thread terminated in a nasty way. What's the name of the method that gives this information (the stack trace of the dead thread). Concurrency is one of those things that are very difficult to unit test. If you are just trying to test that the code inside each thread is doing what it is supposed to test may be you should just test this code isolated of the context. If in this example the threads collaborate to reach a result may be you can test that collaboration without using threads. That would be done by executing all the collaborative parts sequentially. If you want to test for race conditions and these kind of things unit testing is not the best way. You will get tests that sometimes fail and sometimes don´t fail. To summarize I think that may be your problem is that you are unit testing in a level too high. Hope this helps good point. i was mainly interesting in writing the test out of curiosity. i see that it's not a trivial thing to do though. :( It is interesting that programming is pushing towards more multithreading and more unit testing at the same time. The two seem like incompatible goals unless we move towards pure functional languages where concurrency problems are diminished..  Implement a UncaughtExceptionHandler that sets some flags (which the Threads peridocially check) and set it on each Thread.  The Google Testing Blog had an excellent article on this subject that's well worth reading: http://googletesting.blogspot.com/2008/08/tott-sleeping-synchronization.html It's written in Python but I think the principles are directly transferable to Java.  It is possible making the unit test to fail by using a special synchronization object. Take a look at the following article: Sprinkler - Advanced synchronization object I'll try to explain the main points here. You want to be able to externalize internal threads failures to the main thread which in your case is the test. So you have to use a shared object/lock that both the internal thread and the test will use to sync each other. See the following test - it creates a thread which simulates a thrown exception by calling a shared object named Sprinkler. The main thread (the test) is blocked on Sprinkler.getInstance().await(CONTEXT 10000) which by the time release is called - will be free and catch the thrown exception. In the catch block you can write the assert which fails the test.  @Test public void testAwait_InnerThreadExternalizeException() { final int CONTEXT = 1; final String EXCEPTION_MESSAGE = ""test inner thread exception message""; // release will occur sometime in the future - simulate exception in the releaser thread ExecutorServiceFactory.getCachedThreadPoolExecutor().submit(new Callable<void>() { @Override public Void call() throws Exception { Sprinkler.getInstance().release(CONTEXT new RuntimeException(EXCEPTION_MESSAGE)); return null; } }); Throwable thrown = null; try { Sprinkler.getInstance().await(CONTEXT 10000); } catch (Throwable t) { // if the releaser thread delivers exception it will be externelized to this thread thrown = t; } Assert.assertTrue(thrown instanceof SprinklerException); Assert.assertEquals(EXCEPTION_MESSAGE thrown.getCause().getMessage()); }  Unit testing in a multithreaded environment is tough... so some adjustments need to be made. Unit tests must be repeatable.. deterministic. As a result anything with multiple threads fails this criteria. Tests with multiple threads also tend to be slow. I'd either try to see if I can get by with testing on a single thread.. does the logic under test really need multiple threads. If that doesn't work go with the member variable approach that you can check against an expected value at the end of the test when all the threads have finished running. Hey seems like there's another question just like this. Check my post for a link to a longer discussion at the tdd yahoogroup http://stackoverflow.com/questions/111676/unit-testing-a-multithreaded-application  Your runnable wrapper should be passing the exception object back to your test class and then you can store them in a collection. When all the tests are finish you can test the collection. If it isn't empty iterate over each of the exceptions and .printStackTrace() then fail.  Another popular option for Junit concurrent thread testing is Matthieu Carbou's method using a custom JunitRunner and a simple annotation. See the full documentation"
122,A,"How to force the static block running in each test method? I found static block is running only once when I execute multiple JUnit tests. How can I force it to run for each test method? I am using latest JUnit 4.8.2 Also according to xUnit design principle every method should be totally independent on others. Why static block only be executed once? @Test TestMethod1 () { Accounts ac = new Accounts(); ac.method1(); //kill the thread inside } @Test TestMethod2 () { Accounts ac = new Accounts(); ac.method2(); // the thread is no longer available!! } class Accounts { static { // initalize one thread to monitor something } } This even happens when TestMethod1 and TestMethod2 are in the different Test Classes. pls show some code explaining how you run it if you want something initialized before every @Test is run do it in a @Before annotated method static blocks are only executed on class loading because that is what they are: class initializers. To have a static block run multiple times would require you to unload the class (not an easy thing to do...). If you need to use static blocks you can come up with ways to test them. Why not unwrap the block into a public (static) method? All you have to do in that world is test the method:  static { staticInitMethod(); } public static void staticInitMethod(){ //insert initialization code here } you also might be able to get away with just an ordinary initializer  {//not static //insert initialization code here } Although the truth is most code doesn't need to use initializers like this at all. Edit: Turns out Oracle likes the static method approach http://download.oracle.com/javase/tutorial/java/javaOO/initial.html  Is the static code for the tests for the class being tested? If the code is static so the tests can share then you need to move the code into its own class. Then either have the test class constructor instantiate a static instance or create a test suite that does the same thing. If you want each test to stand alone then move what you are doing in your static block into the setup()/teardown() methods it's what they are there for.  Um... make it non-static? You can have instance initializer blocks too (same as static blocks just without the static keyword). But test setup code should actually go into an explicit setUp() or @Before method. I think the point here is to *test* static initialization via JUnit not to redesign the class being tested. So prove that each static method when called first (and therefore loading the class and causing the static init) works correctly. Could be wrong though. I need to have a static block to make sure every objects share some common things. @user - you don't **have to**. There are other ways to share common things.  Why static block only be executed once? Because that is the whole point of static initializer blocks! Or to put it another way if you want some initialization code to execute multiple times put it in a regular constructor or method or (in a tiny number of cases) a non-static initializer block. In the context of JUnit the normal way to implement test startup and shutdown code using setUp() and tearDown() methods. If you are trying to unit test the execution of static initialization in your own code you are in for a rough road I think. But then unit testing of code with static state (e.g. singletons) is always difficult ... and that's one of the reasons that people think that static state is a bad idea. Consider using a Dependency Injection (aka Inversion of Control) framework instead of singletons. Alternatively consider modifying your singletons / static initialization code to make it easier to test. For instance add a static method that allows a test to re-execute the initialization. (And before you say that this breaks the singleton pattern: yes I know. You need to choose between design / implementation ""purity"" and ease of testing.) But in Junit framework every test method should be totally independent of another? Each JUnit test class can have its own `startup()` and `shutdown()`. If you want the test methods to be totally independent you could put each one in its own class. @Stephen singleton pattern is useful from time to time. Yes ... but it also presents serious problems for unit testing. Many people think that Dependency Injection offers a cleaner alternative to singletons. Its your choice but testability is one of the things that is affected by that choice. You can't have it both ways ... @user: If you are dealing with a singleton then you should have said so. @user705414: Correct. However such independence is only possible to achieve if the code is designed for it. And code that depends on static initializer blocks does not play well with independent unit tests. Possible solution: Move the code from the static initializer to a static method and call this method from the initializer and from the tests. @user: It's nice when you can have your tests be all totally independent of each other – good for tracking down problems – but irritatingly expensive too. If you're using singletons quite a bit you might be better off using an IoC framework (e.g. Spring) to handle configuration and lifetime management and that will help with the testing too."
123,A,"Is there a way of having something like jUnit Assert message argument in Mockito's verify method? Let's assume a snippet of testing code: Observable model = Class.forName(fullyQualifiedMethodName).newInstance(); Observer view = Mockito.mock(Observer.class); model.addObserver(view); for (Method method : Class.forName(fullyQualifiedMethodName).getDeclaredMethods()) { method.invoke(model composeParams(method)); model.notifyObservers(); Mockito.verify( view Mockito.atLeastOnce() ).update(Mockito.<Observable>any() Mockito.<Object>any()); } Mockito.verify method throws an exception if a method in a model hasn't invoked Observable.setChanged() method. Problem: without adding loggers/System.print.out I can't realize what's the current method that has failed the test. Is there a way of having something similar to jUnit Assert methods: Assert.assertEquals( String.format(""instances %s %s should be equal"" inst1 inst2) inst1.getParam() inst2.getParam() ); SOLUTION: verify(observer new VerificationMode() { @Override public void verify(VerificationData data) { assertTrue( format( ""method %s doesn't call Observable#setChanged() after changing the state of the model"" method.toString() ) data.getAllInvocations().size() > 0); } }).update(Mockito.<Observable>any() Mockito.<Object>any()); This does the trick (simple and clear): try { verify(myMockedObject times(1)).doSomthing(); } catch (MockitoAssertionError e) { throw new MockitoAssertionError(""Was expecting a call to myMockedObject.doSomthing but got ""+ e.getMessage()); } Passing the cause as second argument is usally better than concatenating the message. `throw new MockitoAssertionError(""message"" e)`  There isn't a direct API call that allows a message on verify. But I think if you change your verify signature to use the method object rather than Mockito.any() the toString() on the Method class will kick in and give you what you want. Something like this. import static org.mockito.Matchers.anyObject; import static org.mockito.Mockito.atLeastOnce; import static org.mockito.Mockito.verify; import static org.mockito.Matchers.eq; ... Observable model = Class.forName(""class name"").newInstance(); verify(view times(1)).update(eq(model) anyObject()); for (Method method : Class.forName(""class name"").getDeclaredMethods()) { method.invoke(model composeParams(method)); model.notifyObservers(); verify(view atLeastOnce()).update(eq(method) anyObject()); } Nice try but it's not going to work. First method of the Observer#update(Object source Object arg) is the model that is broadcasting the change not the method that has changed its state. Just add the expectation you want to see. If that includes another type of observable then so be it. You probably need to add more code that explains your class structure better.  You cannot do in mockito. Mockito syntax makes very easy to test expected behaviour but it has no concept of test state. What you're trying to do is to have some information that are not in the mocked object when the mocks fails expectations. If you really want to do I see 2 general ways: either you create your own verificationMode implementing the interface org.mockito.verification; public static interface VerificationMode and adding a method like atLeastOnceMsd(String msg) that will show the message in case of failing or adding the current tested method in the model to the view object for example with a similar line in the inner loop.  view.setName(""now we are testing "" + method.getName());  You could create matcher to print information on the current method. It's gonna be a bit clunky but it will work print the method name when the verification fails."
124,A,"Unit testing a Java multi-threaded network app I am writing a Java multi-threaded network application and having real difficulty coming up with a way to unit test the object which sends and receives communication from network clients. The object sends out a message to a number of clients and then waits for responses from the clients. As each client responds a dashboard-style GUI is updated. In more detail... A Message object represents a text message to be sent and contains an array of Clients which should receive the message. The Message object is responsible for dispatching itself to all the appropriate clients. When the dispatch() method is invoked on a Message object the object spawns a new thread (MessageDispatcher) for each client in the Client array. Each MessageDispatcher: opens a new TCP socket (Socket) to the client delivers the message to its client... PrintWriter out.println(msg text) creates a 'Status' object which is passed to a Queue in the Message object and then on to the GUI. Each Status object represents ONE of the following events: Message passed to Socket (via Printwriter out.println() ) Display receipt received from client (via BufferedReader/InputStreamReader in.readline()... blocks until network input is received ) User acknowledge receipt received from client (via same method as above) So.. I want to unit test the Message object. (using JUnit) The unit test is called MessageTest.java (included below). My first step has been to set up a Message object with a single recipient. I then used JMockit to create a mock Socket object which can supply a mock OutputStream object (I am using ByteArrayOutputStream which extends OutputStream) to PrintWriter. Then when the MessageDispatcher calls (PrintWriter object).out the message text will be ideally passed to my mock Socket object (via the mock OutputStream) which can check that the message text is OK. And the sample principle for the InputStreamReader.... The mock Socket object also supplies a mock InputStreamReader object which supplies a mock BufferedReader which is called by the MessageDispatcher (as mentioned previously MessageDispatcher blocks on in.readLine() ). At this point the mock BufferedReader should supply a fake confirmation to the MessageDispatcher... // mock Socket Mockit.redefineMethods(Socket.class new Object() { ByteArrayOutputStream output = new ByteArrayOutputStream(); ByteArrayInputStream input = new ByteArrayInputStream(); public OutputStream getOutputStream() { return output; } public InputStream getInputStream() { return input; } }); If this wasn't multi-threaded this should all work OK. However I have no idea how to do this with multiple threads. Can anyone give me any advice or tips? Also if you have any input on the design (eg. Message object responsible for its own delivery rather than a separate delivery object.. ""dependency injection""-style / separate thread for each client delivery) then I would be interested to hear that too. UPDATE: here is the code: Message.java public class Message { Client[] to; String contents; String status; StatusListener listener; BlockingQueue<Status> statusQ; public Message(Client[] to String contents StatusListener listener) { this.to = to; this.contents = contents; this.listener = listener; } public void dispatch() { try { // open a new thread for each client // keep a linked list of socket references so that all threads can be closed List<Socket> sockets = Collections.synchronizedList(new ArrayList<Socket>()); // initialise the statusQ for threads to report message status statusQ = new ArrayBlockingQueue<Status>(to.length*3); // max 3 status objects per thread // dispatch to each client individually and wait for confirmation for (int i=0; i < to.length; i++) { System.out.println(""Started new thread""); (new Thread(new MessageDispatcher(to[i] contents sockets statusQ))).start(); } // now monitor queue and empty the queue as it fills up.. (consumer) while (true) { listener.updateStatus(statusQ.take()); } } catch (Exception e) { e.printStackTrace(); } } // one MessageDispatcher per client private class MessageDispatcher implements Runnable { private Client client; private String contents; private List<Socket> sockets; private BlockingQueue<Status> statusQ; public MessageDispatcher(Client client String contents List<Socket> sockets BlockingQueue<Status> statusQ) { this.contents = contents; this.client = client; this.sockets = sockets; this.statusQ = statusQ; } public void run() { try { // open socket to client Socket sk = new Socket(client.getAddress() CLIENTPORT); // add reference to socket to list synchronized(sockets) { sockets.add(sk); } PrintWriter out = new PrintWriter(sk.getOutputStream() true); BufferedReader in = new BufferedReader(new InputStreamReader(sk.getInputStream())); // send message out.println(contents); // confirm dispatch statusQ.add(new Status(client ""DISPATCHED"")); // wait for display receipt in.readLine(); statusQ.add(new Status(client ""DISPLAYED"")); // wait for read receipt in.readLine(); statusQ.add(new Status(client ""READ"")); } catch (Exception e) { e.printStackTrace(); } } } } .... and the corresponding unit test: MessageTest.java public class MessageTest extends TestCase { Message msg; static final String testContents = ""hello there""; public void setUp() { // mock Socket Mockit.redefineMethods(Socket.class new Object() { ByteArrayOutputStream output = new ByteArrayOutputStream(); ByteArrayInputStream input = new ByteArrayInputStream(); public OutputStream getOutputStream() { return output; } public InputStream getInputStream() { return input; } }); // NB // some code removed here for simplicity // which uses JMockit to overrides the Client object and give it a fake hostname and address Client[] testClient = { new Client() }; msg = new Message(testClient testContents this); } public void tearDown() { } public void testDispatch() { // dispatch to client msg.dispatch(); } } can you post some code that you are working with and tell us why that code isn't working? just put up the 2 main objects perhaps instead of redefining the methods getOutputStream and getInputStream you can instead use an AbstractFactory in your Message class which creates output and input streams. In normal operation the factory will use a Socket to do that. However for testing give it a factory which gives it streams of your choosing. That way you have more control over exactly what is happening. +1 abstract factory pattern ftw! This is my thinking... I would rather use JMockit that pass a factory to Message.... having said that I am open to both ways and have refactored a copy of Message to this end (and so far it works). However I am searching for a less-involved method.. You're right some kind of factory could be used in the Message class but at the cost of making it more complex. Since the Message class can be unit tested cleanly with a mocking tool I would rather do that than add extra complexity to the production code.  Notice that the sending of multiple messages (multicast) can be achieved in a single blocking method through the NIO API (java.nio) as well without the creation of new threads. NIO is quite complex though. I would start by writing the tests first with a test-defined StatusListener implementation which stores all update events in a list. When the dispatch() method returns the test can execute asserts on the state of the event list. Using threads or NIO is an implementation detail for the Message class. So unless you don't mind coupling the tests to this implementation detail I would recommend introducing a helper class that would be responsible for sending multiple asynchronous messages and notifying the Message object upon any asynchronous replies. Then you can mock the helper class in the unit tests without coupling them to either threads or NIO. I successfully implemented a test for the case of sending a message to one client. I also made some changes to the original production code as follows: public class Message { private static final int CLIENT_PORT = 8000; // Externally provided: private final Client[] to; private final String contents; private final StatusListener listener; // Internal state: private final List<Socket> clientConnections; private final BlockingQueue<Status> statusQueue; public Message(Client[] to String contents StatusListener listener) { this.to = to; this.contents = contents; this.listener = listener; // Keep a list of socket references so that all threads can be closed: clientConnections = Collections.synchronizedList(new ArrayList<Socket>()); // Initialise the statusQ for threads to report message status: statusQueue = new ArrayBlockingQueue<Status>(to.length * 3); } public void dispatch() { // Dispatch to each client individually and wait for confirmation: sendContentsToEachClientAsynchronously(); Status statusChangeReceived; do { try { // Now monitor queue and empty the queue as it fills up (consumer): statusChangeReceived = statusQueue.take(); } catch (InterruptedException ignore) { break; } } while (listener.updateStatus(statusChangeReceived)); closeRemainingClientConnections(); } private void closeRemainingClientConnections() { for (Socket connection : clientConnections) { try { connection.close(); } catch (IOException ignore) { // OK } } clientConnections.clear(); } private void sendContentsToEachClientAsynchronously() { for (Client client : to) { System.out.println(""Started new thread""); new Thread(new MessageDispatcher(client)).start(); } } // One MessageDispatcher per client. private final class MessageDispatcher implements Runnable { private final Client client; MessageDispatcher(Client client) { this.client = client; } public void run() { try { communicateWithClient(); } catch (IOException e) { throw new RuntimeException(e); } } private void communicateWithClient() throws IOException { // Open connection to client: Socket connection = new Socket(client.getAddress() CLIENT_PORT); try { // Add client connection to synchronized list: clientConnections.add(connection); sendMessage(connection.getOutputStream()); readRequiredReceipts(connection.getInputStream()); } finally { connection.close(); } } // Send message and confirm dispatch. private void sendMessage(OutputStream output) { PrintWriter out = new PrintWriter(output true); out.println(contents); statusQueue.add(new Status(client ""DISPATCHED"")); } private void readRequiredReceipts(InputStream input) throws IOException { BufferedReader in = new BufferedReader(new InputStreamReader(input)); // Wait for display receipt: in.readLine(); statusQueue.add(new Status(client ""DISPLAYED"")); // Wait for read receipt: in.readLine(); statusQueue.add(new Status(client ""READ"")); } } } public final class MessageTest extends JMockitTest { static final String testContents = ""hello there""; static final String[] expectedEvents = {""DISPATCHED"" ""DISPLAYED"" ""READ""}; @Test public void testSendMessageToSingleClient() { final Client theClient = new Client(""client1""); Client[] testClient = {theClient}; new MockUp<Socket>() { @Mock(invocations = 1) void $init(String host int port) { assertEquals(theClient.getAddress() host); assertTrue(port > 0); } @Mock(invocations = 1) public OutputStream getOutputStream() { return new ByteArrayOutputStream(); } @Mock(invocations = 1) public InputStream getInputStream() { return new ByteArrayInputStream(""reply1\nreply2\n"".getBytes()); } @Mock(minInvocations = 1) void close() {} }; StatusListener listener = new MockUp<StatusListener>() { int eventIndex; @Mock(invocations = 3) boolean updateStatus(Status status) { assertSame(theClient status.getClient()); assertEquals(expectedEvents[eventIndex++] status.getEvent()); return eventIndex < expectedEvents.length; } }.getMockInstance(); new Message(testClient testContents listener).dispatch(); } } The JMockit test above uses the new MockUp class not yet available in the latest release. It can be replaced with Mockit.setUpMock(Socket.class new Object() { ... }) though. just to clarify... do you mean creating a class in between the threads/NIO and the message object? (ie. removing the resposibility of the 'implementation details' as you say from the message object)... If so isn't this against the principle of encapsulation (putting all the methods to deal with an objects data inside the object) and dependency injection? (ie. not very good from an OO design point of view) Yes IF you consider important to encapsulate those implementation details; it isn't necessary of course. Notice that creating such a helper class would be following the OO principle of ""encapsulating what varies""; in this case the ""thing that varies"" is the way in which the asynchronous I/O is dealt with (ie with blocking or non-blocking I/O operations). Dependency injection is not relevant here the way I see it; why should it be given that there are no externally configurable services involved?"
125,A,"Does a Unittest has to have an Assertion like ""assertEquals(..)"" I have a little JUnit-Test that export an Object to the FileSystem. In the first place my test looked like this public void exportTest { //...creating a list with some objects to export... JAXBService service = new JAXBService(); service.exportList(list ""output.xml""); } Usually my test contain a assertion like assertEquals(...). So I changed the code to the following public void exportCustomerListTest() throws Exception { // delete the old resulting file so we can test for a new one at the end File file = new File(""output.xml""); file.delete(); //...creating a list with some objects to export... JAXBService service = new JAXBService(); service.exportCustomers(list ""output.xml""); // Test if a file has been created and if it contains some bytes FileReader fis = new FileReader(""output.xml""); int firstByte = fis.read(); assertTrue(firstByte != -1 ); } Do I need this or was the first approach enough? I am asking because the first one is actually just ""testing"" that the code runs but not testing any results. Or can I rely on the ""contract"" that if the export-method runs without an exception the test passes? Thanks you could use assertTrue(new File(""output.xml"")).exist()); if you notice problems during the generation of the file you can unit test the generation process (and not the fact that the file was correctly written and reloaded from the filesystem) You can either go with the ""gold file"" approach (testing that two files are 1 to 1 identical) or test various outputs of your generator (I imagine that the generation of the content is separated from the saving into the file)  Sometimes a test only needs to demonstrate that no exceptions were thrown. In that case relying that an exception will fail the test is good enough. There is certainly nothing gained in JUnit by calling the assertEquals method. A test passes when it doesn't throw an AssertionException not because that method is called. Consider a method that allows null input you might write a test like this:  @Test public void testNullAllowed() { new CustomObject().methodThatAllowsNull(null); } That might be enough of a test right there (leave to a separate test or perhaps there is nothing interesting to test about what it does with a null value) although it prudent to leave a comment that you didn't forget the assert you left it out on purpose. In your case however you haven't tested very much. Sure it didn't blow up but an empty method wouldn't blow up either. Your second test is better at least you demonstrate a non-empty file was created. But you can do better than that and check that at least some reasonable result was created.  Well you're testing that your code runs to completion without any exceptions - but you're not testing anything about the output. Why not keep a file with the expected output and compare that with the actual output? Note that this would probably be easier if you had an overload of expertCustomers which took a Writer - then you could pass in a StringWriter and only write to memory. You could test that in several ways with just a single test of the overload which takes a filename as that would just create a FileOutputStream wrapped in an OutputStreamWriter and then call the more thoroughly tested method. You'd really just need to check that the right file existed probably.  I agree with the other posts. I will also add that your first test won't tell a test suite or test runner that this particular test has failed."
126,A,"Unit Testing AssertError in JUnit I'm trying to ensure that a parameter can't be null by adding an assert statement at the top of the method. When unit testing I'm trying to declare that the AssertError is expected but it still gets recognized as a failed test even though it's behaviour is correct (AssertError is getting thrown). class ExampleTest { @Test(expected=AssertError.class) public void testAssertFails() { assert 0 == 1; } } Why bother using the `assert` keyword in a unit test when JUnit has a whole bunch of `assertXYZ` methods for this very task? @skaffman: I think it's intended that the code to be tested will use `assert` the unit test in the question is just for demonstration. @Allain: although the listed code doesn't appear to have the same problem this may be of help: http://stackoverflow.com/questions/395317/cause-of-an-unexpected-behaviour-using-junit-4s-expected-exception-mechanism The problem is the exception class is not called AssertError but AssertionError check out the Java API Javadoc.  You probably need to enable assertions with the -ea JVM argument since they're off by default. When the assertions are disabled then the assert won't throw an exception if it fails. If you're running this in Eclipse you can edit your Installed JRE in preferences to add this as an argument or you add it to the run configuration for your tests. Also the exception thrown is AssertionError not AssertError.  Probably not a good idea to mix JVM-level assertions with JUnit assertions. That said here's how it was done before we had annotations. You might consider doing it this way simply for clarity/documentation: public class Foo { public void someMethod(String someArg) { if (s == null) throw new NullPointerException(""someArg cannot be null""); } } public class FooTest { public void testSomeMethodNullArg() { try { foo.someMethod(null); fail(""someMethod failed to throw NullPointerException for null arg""); } catch (NullPointerException expected) { // expected exception } }"
127,A,Unit Testing Composite Service Methods I am writing (junit) unit tests for a class which implements an exposed interface with methods like: public Set<Setting> getUserSettings(); public Set<Setting> getOrganizationSettings(); public Set<Setting> getDefaults(); public Set<Setting> getAllSettings(); The methods for getting Settings from a specific layer do IO from various places for retrieving their results. getAllSettings() Returns a single set of all the Settings at all levels with the 'uppermost' level having preference (i.e. if a setting exists in the default and user level the setting in the user-level will be used. I've already written the unit tests for getUserSettings() getOrganizationSettings() getDefaults() mocking out the IO operations with Mocked objects. The implementation for the getAllSettings() looks something like public Set<Setting> getAllSettings(){ Set<Setting> defaults = getUserSettings(); Set<Setting> custom = getOrganizationSettings(); Set<Setting> undefined = getDefaults(); //perform some sorting and business logic //return fully sorted set } My question lies in how to unit test the getAllSettings() method. Do I use mocks (using easymock/powermock) for all the downstream resource calls that the user/organization/defaultSettings methods use? It seems like there would be a cleaner/better/easier way to do it. You could write a test in the following form @Test public void testGetAllSettings() { Foo fixture = new Foo() { public Set<Setting> getUserSettings() { // canned impl } public Set<Setting> getOrganizationSettings() { // canned impl } public Set<Setting> getDefaults() { // canned impl } } Assert.assertEquals(whatEverItShouldEqual fixture.getAllSettings()); } This would allow you to test the logic of get all settings independent of the other methods. Another alternative would be to mock the IO of these methods. If you have a layer that does the logic of IO then that could be mocked. As you mention this can be a pain if you have lots of dependencies. Perhaps a sign that you need less dependencies? (maybe the class should be broken up into smaller units for example?) I didn't even think of using that. That is pretty much exactly what I was looking for.
128,A,"Failed web service unit tests behind a proxy in Eclipse I have a problem executing unit tests in Eclipse behind a proxy as the classes used by the unit tests call some web services which are behind a proxy. In Tomcat I can of course specify a proxy to be used but when I run unit tests directly from Eclipse the proxy configuration of Eclipse seems to be ignored. How can I configure my unit tests or Eclipse to use a proxy in this case? I found the solution: apparently one can configure the proxy in run/debug settings in Eclipse. To do this: right click on the unit test class debug as - debug configurations go to tab ""Arguments"" add proxy configuration to ""VM arguments"": -Dhttp.proxyHost=proxy.host.com -Dhttp.proxyPort=8080 -Dhttp.nonProxyHosts=[list of hosts separated with pipe character] If you are behind a password protected proxy you have to add: -Dhttp.proxyUser=someUserName -Dhttp.proxyPassword=somePassword Edited answer accordingly thanks for pointing that out (the dashes were in my original answer but were apparently edited out when improving the formatting). Maybe obvious to veteran Java developers but you need to put a dash (-) in front of each argument when you add it to the VM arguments section in Eclipse (or at least I did). Ex: -Dhttp.proxyHost=abcd.com -Dhttp.proxyPort=8888. You should also note that the proxy configuration for HTTPS uses a slightly different property eg. -Dhttps.proxyHost=proxy.host.com. I was wondering why this wasn't working for me at first and it was because my code uses HTTPS."
129,A,JUnit - assertSame Can someone tell me why assertSame() do fail when I use values > 127?  import static org.junit.Assert.*; ... @Test public void StationTest1() { .. assertSame(4 4); // OK assertSame(10 10); // OK assertSame(100 100); // OK assertSame(127 127); // OK assertSame(128 128); // raises an junit.framework.AssertionFailedError! assertSame(((int) 128)((int) 128)); // also junit.framework.AssertionFailedError! } I'm using JUnit 4.8.1. You should use `assertSame` only for reference equality checks - for example `a == b`. For equality you should use `assertEquals`. assertSame takes two Object arguments and so the compiler has to autobox your int literals into Integer. This is equivalent to assertSame(Integer.valueOf(128) Integer.valueOf(128)); Now for values between -128 and 127 the JVM will cache the results of Integer.valueOf so you get the same Integer object back each time. For values outside of that range you get new objects back. So for assertSame(127 127) JUnit is comparing the same objects hence it works. For assertSame(128 128) you get different objects so it fails. Just another reason to be careful with autoboxing.  The reason is the autoboxing of Java. You use the method: public static void assertSame(Object expected Object actual) It only works with Objects. When you pass ints to this method Java automatically calls Integer.valueOf( int i ) with these values. So the cast to int has no effect. For values less than 128 Java has a cache so assertSame() compares the Integer object with itself. For values greater than 127 Java creates new instances so assertSame() compares an Integer object with another. Because they are not the same instance the assertSame() method returns false. You should use the method: public static void assertEquals(Object expected Object actual) instead. This method uses the equals() method from Object.
130,A,"How can I run (or change the order of) specific test-methods in a JUnit test class? I am fairly new to Java. I have constructed a single JUnit test class and inside this file are a number of test methods. When I run this class (in NetBeans) it runs each test method in the class in order. Question 1: How can I run only a specific sub-set of the test methods in this class? (Potential answer: Write @Ignore above @Test for the tests I wish to ignore. However if I want to indicate which test methods I want to run rather than those I want to ignore is there a more convenient way of doing this?) Question 2: Is there an easy way to change the order in which the various test methods are run? Thanks. When you say methods are you talking about test methods? Why does the order matter? Each unit test should be able to run in isolation. It is generally a test smell to depend on the order of execution tests should be stateless. Yes by methods I should have written test methods. @dom farr: Perhaps I am doing something wrong but I wanted to test whether changing the order of the test method calls resulted in any problems/bugs. @K McCoy. Your tests are cause bugs? I would suggest creating a set of tests that describe the behaviour your want and then complete the code so you only get passing tests. This answer tells you how to do it. Randomizing the order the tests run is a good idea! Like the comment from dom farr states each unit test should be able to run in isolation. There should be no residuals and no given requirements after or before a test run. All your unit tests should pass run in any order or any subset. It's not a terrible idea to have or generate a map of Test Case --> List of Test and then randomly execute all the tests.  I've not been using Java that long either but as far as I've seen there isn't a convenient method of marking methods to execute rather than ignore. Instead I think this could be achieved using the IDE. When I want to do that in Eclipse I can use the junit view to run individual tests by clicking on them. I imagine there is something similar in Netbeans. I don't know of an easy way to reorder the test execution. Eclipse has a button to rerun tests with the failing tests first but it sounds like you want something more versatile.  There are a number of approaches to this but it depends on your specific needs. For example you could split up each of your test methods into separate test classes and then arrange them in different test suites (which would allow for overlap of methods in the suites if desired). Or a simpler solution would be to make your test methods normal class methods with one test method in your class that calls them in your specific order. Do you want them to be dynamcially called?  You should read about TestSuite's. They allow to group & order your unit test methods. Here's an extract form this article ""JUnit test classes can be rolled up to run in a specific order by creating a Test Suite. EDIT: Here's an example showing how simple it is:  public static Test suite() { TestSuite suite = new TestSuite(""Sample Tests""); suite.addTest(new SampleTest(""testmethod3"")); suite.addTest(new SampleTest(""testmethod5"")); return suite; } Your example would run all the test methods in SampleTest.class. However what if in SampleTest.class I have various test methods [testmethod1() testmethod2()...] and I only with to run testmethod3() and testmethod5(); would this be possible? @K McCoy: I've edited my example to match your requirements:) I have tested the above code and noted two things: (1) The first line should read: public static TestSuite suite() { (2) Inside the class SampleTest one needs to define the constructor public SampleTest (String name) { super(name); } Otherwise this did the trick for me. Thanks! I have tried to execute the above code but I get ""The constructor SampleTest(String) is undefined"". Is there something I'm missing here? I also read that [running test methods of the same test class in a sequential order was not possible](http://stackoverflow.com/questions/8518314/selenium-junit-tests-how-do-i-run-tests-within-a-test-in-sequential-order) so I'd be very happy if you could help me out on that"
131,A,"Unit testing CRUD operations when data source is configured in application server properties Please tell me: How can I write unit test (e.g. with JUnit) for operation ""insert into some table"" when many properties for normal application work are saved in config files for application server? Thanks!a You can either: Change your class to require its configuration settings (like the data source) in its constructor. Make callers read the configuration settings and dictate them to your class. When you do this your unit tests can specify the settings when they instantiate test instances. Or Use a mock object instead of the normal mechanism to obtain configuration settings. The second option won't be possible if you use a shared singleton ConfigurationSettings (or something similar) to get your configuration data. If that's the case use the first method - which is generally better anyway. (As your question demonstrates different callers use different configuration techniques.)"
132,A,"Could not find class: org.junit.tests.AllTests - OSX I'm having a little trouble getting started with JUnit tests. This should be pretty basic. I downloaded junit-4.8.2.jar from JUnit.org and placed it in my home directory. If I try to test my setup in a TCSH terminal with the following command: java -cp ~/junit-4.8.2.jar org.junit.runner.JUnitCore org.junit.tests.AllTests I receive the following output JUnit version 4.8.2 Could not find class: org.junit.tests.AllTests Time: 0.003 OK (0 tests) Any help would be greatly appreciated. Thanks. Does the documentation say you need to pass exactly ""org.junit.tests.AllTests"" as the second argument? This was posted by unixking - Get the latest version of JUnit from http://sourceforge.net/projects/junit/files/junit/4.10/ Unzip junit4.10.zip at C:\MyDev\lib so we have C:\MyDev\lib\junit4.10 In the command prompt run the following commands  : the home is where i work best set MY_HOME=C:/MyDev : place to find the junit jar files set JUNIT_HOME=%MY_HOME%/lib/junit4.10 : find the junit jar files as well as other classes that one intends to use set CLASSPATH=%CLASSPATH%;%JUNIT_HOME%;%JUNIT_HOME%/junit-4.10.jar : this is where my jdk is set JAVA_HOME=%MY_HOME%/tool/jdk1.6.0_24 : run sample tests (sitting in any folder) java org.junit.runner.JUnitCore org.junit.tests.AllTests  Check out: the junit faq specifically this item bolding mine: Test the installation by running the sample tests distributed with JUnit. Note that the sample tests are located in the installation directory directly not the junit.jar file. Therefore make sure that the JUnit installation directory is on your CLASSPATH. Then simply type: java org.junit.runner.JUnitCore org.junit.tests.AllTests If you only downloaded the jar you can't run the sample tests. You need to get the src package and follow the instructions for that."
133,A,"Run Jettys ServletTester within JUnit test I'm trying to run Jettys ServletTester in my JUnit test. I created a simple HelloServlet first to test the setup but I get an IllegalAccessException when I try to request the servlet. Here is what I have so far: My unit test @Before public void setUp() throws Exception { tester = new ServletTester(); tester.setContextPath(""/context""); tester.addServlet(HelloServlet.class ""/hello/*""); tester.start(); } @After public void tearDown() throws Exception { tester.stop(); } @Test public void testDefaultServlet() throws Exception { HttpTester request = new HttpTester(); request.setMethod(""GET""); request.setHeader(""Host""""127.0.0.1""); request.setURI(""/context/hello/info""); request.setVersion(""HTTP/1.0""); HttpTester response = new HttpTester(); response.parse(tester.getResponses(request.generate())); assertNull(response.getMethod()); assertEquals(200response.getStatus()); assertEquals(""<h1>Hello Servlet</h1>""response.getContent()); } My HelloServlet This servlet is defined in the same file as the unit test because I want it to be there for the initial setup of jetty. After everything is running I'll remove it (or maybe keep it but it will stay within the unit test then). Update This servlet was defined inside the unit test itself because it was meant only as a configuration test for the jetty server itself. But jetty wasn't able to access it and after moving it into a public class and a file for itself everything worked like expected. See the comment. class HelloServlet extends HttpServlet { @Override public void doGet(HttpServletRequest req HttpServletResponse resp) throws ServletException IOException { PrintWriter out = resp.getWriter(); out.println(""Hello World!""); out.flush(); } } My Exception... 2009-10-20 09:36:28.973::INFO: Logging to STDERR via org.mortbay.log.StdErrLog 2009-10-20 09:36:28.989::INFO: jetty-6.1.21 2009-10-20 09:36:29.098::INFO: Started LocalConnector@0.0.0.0:1 2009-10-20 09:36:29.161:/context:WARN: unavailable java.lang.IllegalAccessException: Class org.mortbay.jetty.servlet.Holder can not access a member of class my.package.HelloServlet with modifiers """" at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65) at java.lang.Class.newInstance0(Class.java:349) at java.lang.Class.newInstance(Class.java:308) at org.mortbay.jetty.servlet.Holder.newInstance(Holder.java:153) at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:428) at org.mortbay.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:339) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:487) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:390) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:326) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:536) at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:915) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:405) at org.mortbay.jetty.LocalConnector.accept(LocalConnector.java:186) at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:707) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) Strange enough because I got this example almost straight from http://docs.codehaus.org/display/JETTY/ServletTester. Any thoughts or maybe a working example of a embedded jetty servlet container in a junit test? Your HelloServlet must be implemented as a public class: public class HelloServlet extends HttpServlet { ... } Sorry I forgot to mention that my HelloServlet is defined in the same file as the unit test. Oh crap. It has to be defined outside the test to be public so that the ServletHolder can access it... thanks a lot. :-)"
134,A,"Spring - applicationContext.xml cannot be opened because it does not exist I have a Spring MVC application and a problem with JUnit tests combined with the file applicationContext.xml. In my JUnit test class I write: final ApplicationContext context = new ClassPathXmlApplicationContext(""applicationContext.xml""); service = (TestServiceImpl) context.getBean(""testServiceImpl""); The error I get is that aplicationContect.xml can not be found: org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from class path resource [applicationContext.xml]; nested exception is java.io.FileNotFoundException: class path resource [applicationContext.xml] cannot be opened because it does not exist But it exits in the WEB-INF folder. Here you can see the tree of my project with the applicationContext.xml file: http://img28.imageshack.us/img28/2576/treexp.png So what's wrong here? Why does the file not exist for the JUnit test? Thank you in advance & Best Regards. I placed the applicationContext.xml in the src/main/java folder and it worked  You should keep your Spring files in another folder marked as ""source"" (just like ""src"" or ""resources""). WEB-INF is not a source folder therefore it will not be included in the classpath (i.e. JUnit will not look for anything there).  If you use maven create a directory called 'resources' in the main directory. and then copy your applicationContext.xml into it. from your java code call: ApplicationContext appCtx = new ClassPathXmlApplicationContext(""applicationContext.xml""); END  The ClassPathXmlApplicationContext isn't going to find the applicationContext.xml in your WEB-INF folder it's not on the classpath. You could copy the application context into your classpath (could put it under src/test/resources and let Maven copy it over) when running the tests.  I got the same error. I solved it moving the file applicationContext.xmlin a sub-folder of the srcfolder. e.g: context = newClassPathXmlApplicationContext(""/com/ejemplo/dao/applicationContext.xml""); I've tried this but it doesn't help."
135,A,"Java Unit Test: Replace a private method under test Is there any way of replacing the logic within a private method when running a JUnit test? A bit of background: we have some private methods which interact with bundles within an OSGi container. This is not available in the unit test therefore the methods will fail. We have looked at JMockIt but the method replace functionality seems to want to force you to replace all the methods in the class which call one another. The implementation would be something like this: public final doSomething() { firstThing(); secondThing(); } private firstThing() { // normal code } private secondThing() { // code which is unavailable in a unit test } And the unit test would specify the new implementation of secondThing(): // replace secondThing() in impl with this secondThing() private secondThing() { // dummy code } // run tests I also think dependency injection would solve this problem. If you don't want another framework in your project and this is the only place which makes trouble you could define an interface for secondThing and have 2 implementations for that one for the original code and an empty one for the unittest.  My advice - redesign your application. If you want to change the behaviour of a private method: make it protected / public and override it in a mock-like object move the functionality out of the method into a helper class which is injectable (via dependency injection). Then mock that helper an inject the mock into the class-under-test instead of the original heloper. A workaround may be some byte-code manipulation technique but I don't recommend such. So not even `java.lang.reflect.Proxy` should be used? It does manipulate bytecode you know... no it works 'by interface' only  You certainly can solve this situation with JMockit. One way would be to define a ""mock-up"" class for example: public class MyTest { @Test public void testDoSomething() { new MockUp<ClassWhichDependsOnOtherBundles>() { @Mock void secondThing() { // do anything here } }; new ClassWhichDependsOnOtherBundles().doSomething(); } } Only the secondThing() method in the mocked class will be replaced by JMockit. The JMockit Expectations API could also be used with partial mocking. this looks interesting. in some cases i missed this in EasyMock where afaik only a complete class can be mocked. Yes the ""JMockit Annotations"" API always does partial mocking based on the `@Mock` methods specified by the user. The EasyMock API is similar to the ""JMockit Expectations"" API; both of them also support partial mocking although EasyMock requires the names of methods to be mocked in strings (for *partial* mocking that is). I think this most completely answers the question as asked although I appreciate the suggested de-coupling solutions and may re-design  You are coupling your implementation to the creation of the osgi object (doing it inside secondThing() or the class itself). If you passed the implementation into your class from the outside you could use a stub/mock when testing instead. I also think this is the right fix. In my opinion any component which interacts with the outside world should be an object provided via a constructor/factory parameter (or otherwise specifiable) and not hardcoded into the class. (Obviously you have to have “leaves” to bottom out on which do inherently refer to the outside world (e.g. server names) but those you test differently.) Then any subsection of your program other than those leaves may be tested inside a completely controlled universe.  there is a nice stub-pattern ProductionClass.java:   public class ProductionClass { ... //default visibility to make it available for stubbing void firstThing(){...}  ... }  BlaTest.java (same package as production class):   public class BlaTest {  @Test void test_xx(){ //use stubbed impl ProductionClass sut = new ProductionClassStubbed(); sut.doSomething(); } } class ProductionClassStubbed extends ProductionClass{ @Override void firstThing(){ //stub out fill in what you want (like dummy behaviour) } }  One different thing. I saw a final modifier in your sample-code. Beware of using final modifier. They are evil for testability. Only use if really really necessary. The `final` keyword is an important mechanism for OO design in Java. For JMockit it makes no difference so use it as much as you want. yes 'final' is a java language feature which has its uses. but for me it is a anti-pattern if people are using it as a default modifier. i had often cases where i had to get tests in place depending on legacy code. final was used and i was doomed for stubbing. in my cases mock-frameworks weren't the right tool (-> for certain reasons i had to build own hand-coded stubs)."
136,A,"How can a test 'dirty' a spring application context? The spring framework documentation states: In the unlikely case that a test may 'dirty' the application context requiring reloading - for example by changing a bean definition or the state of an application object - Spring's testing support provides mechanisms to cause the test fixture to reload the configurations and rebuild the application context before executing the next test. Can someone elaborate this? I am just not getting it. Examples would be nice. Each JUnit test method is assumed to be isolated that is does not have any side effects that could cause another test method to behave differently. This can be achieved by modifying the state of beans that are managed by spring. For example say you have a bean managed by spring of class MySpringBean which has a string property with a value of ""string"". The following test method testBeanString will have a different result depending if it is called before or after the method testModify. @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""/base-context.xml""}) public class SpringTests { @Autowired private MySpringBean bean; @Test public void testModify() { // dirties the state of a managed bean bean.setString(""newSring""); } @Test pubic void testBeanString() { assertEquals(""string"" bean.getString()); } } use the @DirtiesContext annotation to indicate that the test method may change the state of spring managed beans. Thanks I think i get it now. Hi do you know if there's a way to manually call dirties context at all? I can't use the annotation or extend the class unfortunately.."
137,A,"How do you set a custom session when unit testing with wicket? I'm trying to run some unit tests on a wicket page that only allows access after you've logged in. In my JUnit test I cannot start the page or render it without setting the session. How do you set the session? I'm having problems finding any documentation on how to do this.  WicketTester tester = new WicketTester(new MyApp()); ((MyCustomSession)tester.getWicketSession()).setItem(MyFactory.getItem(""abc"")); //Fails to start below no session seems to be set tester.startPage(General.class); tester.assertRenderedPage(General.class); Using Wicket 1.4 I use my normal WebApplication and WebSession implementations called NewtEditor and NewtSession in my app. I override newSession where I do the same than in the regular app code except that I sign in right away. I also override newSessionStore for performance reasons I copied this trick from WicketTesters code. tester = new WicketTester(new NewtEditor() { @Override public Session newSession(Request request Response response) { NewtSession session = new NewtSession(request); session.signIn(getTestDao()); return session; } @Override protected ISessionStore newSessionStore() { // Copied from WicketTester: Don't use a filestore or we spawn lots of threads // which makes things slow. return new HttpSessionStore(this); } });  What I frequently do is to provide a fake WebApplication with overrides for things that I want to mock or stub. Among the things I override is the method  public abstract Session newSession(Request request Response response); which allows you to return a fake session setup with anything you want. This is in Wicket 1.3 - if you're using 1.4 some of this may have changed and as noted in another response it may be related to a wicket bug. But assuming the interface hasn't changed too much overriding this method may also be another way of working around the issue in WICKET-1215. I have used this means before as well to provide a custom Session during testing.  You may be running into WICKET-1215. Otherwise what you're doing looks fine. For example I have a Junit4 setup method that looks like: @Before public void createTester() { tester = new WicketTester( new MyApp() ); // see http://issues.apache.org/jira/browse/WICKET-1215 tester.setupRequestAndResponse(); MyAppSession session = (MyAppSession) tester.getWicketSession(); session.setLocale(Locale.CANADA); session.setUser(...); }"
138,A,"JUnit4 fail() is here but where is pass()? There is a fail() method in JUnit4 library. I like it but experiencing a lack of pass() method which is not present in the library. Why is it so? I've found out that I can use assertTrue(true) instead but still looks unlogical. @Test public void testSetterForeignWord(){ try { card.setForeignWord(""""); fail(); } catch (IncorrectArgumentForSetter ex){ } // assertTrue(true); } Just use return statement - in most cases that will pass as pass(). @topchef that single comment hit the hammer on the head while everybody else debates about which is acceptable and which isn't. Call return statement anytime your test is finished and passed. +1 must have been correct answer (in most cases with exception of expected timeouts etc.)  There is no need for the pass method because when no AssertionFailedException is thrown from the test code the unit test case will pass. The fail() method actually throws an AssertionFailedException to fail the testCase if control comes to that point. I think it's actually a junit.framework.AssertionFailedError. What about the errors cases. say iam getting element not visible exception from webdriver.Should i catch exception and return.  As long as the test doesn't throw an exception it passes unless your @Test annotation specifies an expected exception. I suppose a pass() could throw a special exception that JUnit always interprets as passing so as to short circuit the test but that would go against the usual design of tests (i.e. assume success and only fail if an assertion fails) and if people got the idea that it was preferable to use pass() it would significantly slow down a large suite of passing tests (due to the overhead of exception creation). Failing tests should not be the norm so it's not a big deal if they have that overhead. Note that your example could be rewritten like this: @Test(expected=IncorrectArgumentForSetter.class) public void testSetterForeignWord("""") throws Exception { card.setForeignWord(""""); } Also you should favor the use of standard Java exceptions. Your IncorrectArgumentForSetter should probably be an IllegalArgumentException. -1 - pass() method is not doing nothing - it's quitting test immediately without exceptions. That would be effectively equivalent to return statement in most cases (except for expected exceptions timeouts etc.). @grigory: You're right that a `pass()` method couldn't just do nothing lest it be possible for a test to fail after calling it. So I removed that sentence. The fail() method and assertX() methods really just throw an AssertionError which causes the test method to exit uncleanly. That's why a successful return indicates success..."
139,A,"Handling unit tests with a condition on the current time I'm taking a stab at setting up unit tests for some utility classes in a project I'm working on and one of the classes (contains licensing info) has a method that does some determination based on the current time. i.e. the license contains an expiry date and the license string validates that date but the actual logic to see if the license is expired is based on the current time. public boolean isValid() { return isLicenseStringValid() && !isExpired(); } public boolean isExpired() { Date expiry = getExpiryDate(); if( expiry == null ) { return false; } Date now = new Date(); return now.after( expiry ); } So I'm not sure what to do since the 'new Date()' thing isn't a static criterion. Should I not bother to test 'isValid' and just test 'isLicenseStringValid()' and the 'getExpiryDate()' function separately? Do I just use a license key in the test with a crazy long expiry such that I'll have switched jobs by the time it expires? Do I try to mock out 'new Date()' to some 'getCurrentTime()' method such that I can fake what time it is now? What do others normally do with tests that are time-conditional? BTW I see a problem in your code other than date logic: if( expiry == null ) { return false; } I don't think you should return `false` on `isExpired()` method if the expiry date is null. For more information read - https://www.owasp.org/index.php/Fail_securely It's a license object that has two possible modes: 1. perpetual with no expiry date (hence the above) 2. temporary with an expiry date so it is correct that if there is no expiry date set it isn't expired. Use dependency injection and inject a TimeProvider that provides a getExpiryDate() method.  If you feel the TimeProvider/Clock abstraction is too overboard perfectionist (which may very well be the case) consider this instead Make getCurrentType protected virtual then create a TestingProductionType decendant of the ProductionType that contains the code you posted. In that type override the getCurrentType() method to return some deterministic result. In your unit test create an instance of this TestingProductionType instead. Viola the dependency of current time is now removed from your unit tests. The only production code that is now not unit tested is a method with a single line returning new Date(). I could live with that.  All three approaches are possible: don't test: lazy man's way use a license that won't expire for ages until you've left the job: cover my ass way use a mock for the current date such as a TimeProvider: the perfectionist way I'd go for a comprimise: I'd add the current date as a parameter to the isExpired method and the isValid method. For your live production code add a simple isValid() no-arg override that calls isValid(new Date()). Your test code uses the version that takes the current date as the parameter. This option I like best. If the language supports default args it makes it a bit easier.  If you can check out Mole at http://research.microsoft.com/en-us/projects/pex/ Moles allows to replace any .NET method with a delegate Just use it to replace Date and have it return what ever you need. Then you don't need to do anything crazy. -Raul  I usually inject a date provider into the tested code. This also helps if you need to switch conventions or otherwise ""fix"" the time testing code.  Definitely mock out new Date(). Create a Clock interface with a getCurrentTime() method or something similar. That way you can have a FakeClock for testing and a SystemClock which uses System.currentTimeMillis() or whatever. I've done this a number of times - it's worked extremely well. It's logical too - effectively you require a ""current time service"" so that should be injected like any other dependency. Thanks it feels a little odd using DI for things that feel like part of the language itself but it sounds like this'll work."
140,A,"How to prepare state for several JUnit tests only once I need to test a program that first preprocesses some data and then computes several different results using this preprocessed data -- it makes sense to write separate test for each computation. Official JUnit policy seems to be that I should run preprocessing before each computation test. How can I set up my test so that I could run the preparation only once (it's quite slow) before running remaining tests? Use the annotation @BeforeClass to annotate the method which will be run once before all test methods. Thanks! I think I should start reading manuals after all :p. JUnit main ""tutorial"" gave me impression that this kind of preparation is not directly supported. @BeforeClass requires a static method requiring static fields. This is not necessarily evil but it could affect your tests."
141,A,"Get result from an activity after finish(); in an Android unit test I'm currently writing some Android unit tests and while I've gotten most things to work the way I want one thing has left me kind of stumped. I have the following code in my activity under test: Intent result = new Intent(); result.putExtra(""test"" testinput.getText().toString()); setResult(Activity.RESULT_OK result); finish(); I'm trying to figure out how to use Instrumentation (or whatever) to be able to read the result of the activity or get at the intent after the activity is finished. Can anyone help? http://stackoverflow.com/questions/4160755/testing-that-an-activity-called-setresult Yup similar question. Saw it before asking but there was no good answer. Started a new one with some code... You can do this by writing a special activity whose only purpose is to start the activity you are testing for result and save the result for you to assert correctness on. For example you could create an activity named ResultReceiverActivity. Give it three methods: getResultCode getResultData and getReceivedRequestCode which can be used to verify that the tested activity returned the right values. You would create a test case that extends ActivityInstrumentationTestCase2 and the generic parameter would be ResultReceiverActivity. Calling getActivity will get you the activity instance. public class ReturnedResultTest extends ActivityInstrumentationTestCase2<ResultReceiverActivity> { public void testReturnedResult() { ResultReceiverActivity a = getActivity(); assertEquals(Activity.RESULT_OK a.getResultCode()); assertEquals(""myResult"" a.getResultData().getStringExtra(""test"")); assertEquals(0x9999 a.getReceivedRequestCode()); } } ResultReceiverActivity needs to override onActivityResult of course and should just store the values of that methods parameter in its fields like so: @Override protected void onActivityResult(int requestCode int resultCode Intent data) { super.onActivityResult(requestCode resultCode data); this.receivedRequestCode = requestCode; this.resultCode = resultCode; this.resultData = data; } Of course you may want to customize the activity that ResultReceiverActivity starts and you can easily do that by using getIntent in its onCreate method. In your test case call setActivityIntent before calling getActivity to set which Intent is used to start the activity. Thanks I'll test this! However I've realized that my question should have been along the lines of: unit testing the result of an activity which takes user input. (I.e. the activity takes input from the screen.) Easy enough to test by itself might be harder to unit test. Don't know if all of that could be called a ""unit test"" really...  I'm not sure if it is different for unit tests but you should be able to use onActivityResult as seen here: StartingActivities. You just start the Activity with startActivityForResult(intent requestCode) and then use @Override protected void onActivityResult(int requestCode int resultCode Intent data) back in the activity that used startActivityForResult. Yes that's true and thanks. However I'm asking specifically about the unit testing case. Can you add a little bit more detail to the question about the specific issue you are seeing?  Or you could also use Robolectric and shadow the Activity under test. Then ShadowActivity provides you with methods to easily know if an Activity is finishing and for retrieving its result code. As an example one of my tests looks like this: @Test public void testPressingFinishButtonFinishesActivity() { mActivity.onCreate(null); ShadowActivity shadowActivity = Robolectric.shadowOf(mActivity); Button finishButton = (Button) mActivity.findViewById(R.id.finish_button); finishButton.performClick(); assertEquals(DummyActivity.RESULT_CUSTOM shadowActivity.getResultCode()); assertTrue(shadowActivity.isFinishing()); }  You can use reflection and grab the values directly from the Activity. protected Intent assertFinishCalledWithResult(int resultCode) { assertThat(isFinishCalled() is(true)); try { Field f = Activity.class.getDeclaredField(""mResultCode""); f.setAccessible(true); int actualResultCode = (Integer)f.get(getActivity()); assertThat(actualResultCode is(resultCode)); f = Activity.class.getDeclaredField(""mResultData""); f.setAccessible(true); return (Intent)f.get(getActivity()); } catch (NoSuchFieldException e) { throw new RuntimeException(""Looks like the Android Activity class has changed it's private fields for mResultCode or mResultData. Time to update the reflection code."" e); } catch (Exception e) { throw new RuntimeException(e); } } Wow. Fantastic this is exactly what I was looking for. Works perfectly! Thanks. Great! I am using this instead of getActivity.getResultCode() inside a unit test and this works perfect This has helped me greatly THANKYOU!"
142,A,Running a JUnit4 test - from a java program I was wondering how to run some JUnit4 test inside a java program. Basically - depending on some conditions during runtime I need to decide which test runner to use. Using Junit3 I could override runTest method from TestCase class - but in JUnit4 tests do not extend TestCase class so I have nothing to override... Is there maybe some method that I need to implement... or sth else... The problem I have is that I have to run a method (which cannot be static) before each test case If this non-static method is something related to pre-conditioning the test case you could achieve this by annotating a method using @Before in you test class. Take a look at the Junit4 docs for the behavior of @Before. Otherwise to simply trigger a Junit test class you can use the following code: Runner r = try { r = new BlockJUnit4ClassRunner(Class.forName(testClass)); } catch (ClassNotFoundException | InitializationError e) { // FIX if necessary: JDK 7 syntax // handle } JUnitCore c = new JUnitCore(); c.run(Request.runner(r)); Hope that helps.  Here's some sample code for you: public void checkTests() throws org.junit.runners.model.InitializationError { checkTestClass((Class<?>) MyTestClss.class); } private void checkTestClass(Class<?> theClass) throws InitializationError { final RunNotifier notifier = new RunNotifier(); notifier.addListener(new RunListener() { @Override public void testFailure(Failure failure) throws Exception { Assert.fail(failure.getMessage()); } }); final Runner runner = new BlockJUnit4ClassRunner(theClass); runner.run(notifier); } This looks promising. The problem I have is that I have to run a method (which cannot be static) before each test case... For example using JUnitCore runner. What is the best way to achieve this?
143,A,"How do I Dynamically create a Test Suite in JUnit 4? I would like to create a junit test suite using JUnit 4 where the names of the test classes to be included are not known until the test suite is run. In JUnit 3 I could do this: public final class MasterTester extends TestCase { /** * Used by junit to specify what TestCases to run. * * @return a suite containing what TestCases to run */ public static TestSuite suite() { TestSuite suite = new TestSuite(); for(Class<?> klass : gatherTestClasses()) { suite.addTestSuite(klass); } return suite; } } and let the gatherTestClasses() method deal with figuring out what test classes to run. In JUnit 4 the documentation says to use an annotation: @SuiteClasses({TestClass1.class TestClass2.class...}) to build up my test suite. There are numerous SO answers showing how to do this. Unfortunately the examples I see do not seem to allow for passing a dynamically generated list of TestClasses. This SO answer suggested I would have to subclass BlockJUnit4ClassRunner which I do not want to do. Dynamically specified test suites seem like something that must be in JUnit 4 somewhere. Does anyone know where? I've tried this using JUnit 4.8 and it works: @RunWith(AllTests.class) public class SomeTests { public static TestSuite suite() { TestSuite suite = new TestSuite(); suite.addTest(new JUnit4TestAdapter(Test1.class)); suite.addTest(new JUnit4TestAdapter(Test2.class)); return suite; } } What do I do with this method? I want the suite be become a child of my parent suite http://stackoverflow.com/questions/18834908 I tried that but when I add `@BeforeClass` annotation it the code inside `@BeforeClass` doesn't work. Any idea?  I found Classpath suite quite useful when used with a naming convention on my test classes. http://johanneslink.net/projects/cpsuite.jsp Here is an example: import org.junit.extensions.cpsuite.ClasspathSuite; import org.junit.runner.RunWith; @RunWith(ClasspathSuite.class) @ClassnameFilters({"".*UnitTest""}) public class MySuite { } Awesome! Thank you! Andrejs's answer makes Classpath redundant at least for the task of creating test suites dynamically. It helped me too.thanks.  I'm not sure what gatherTestClasses() does but let's say it returns some tests when the OS is Linux and different tests when the OS is Windows. You can replicate that in JUnit 4.4 with assumptions: @Test public void onlyOnLinux() { assumeThat(getOS() is(OperatingSystem.LINUX)); // rest of test } @Test public void onlyOnWindows() { assumeThat(getOS() is(OperatingSystem.WINDOWS)); // rest of test } @Test public void anyOperatingSystem() { // just don't call assumeThat(..) } The implementation of getOS() and OperatingSystem being your custom code. Perfect just what I was looking for. If you're using apache commons-lang you can use ""assumeTrue(SystemUtils.IS_OS_WINDOWS);""  To create a dynamic test suite you need to use the @RunWith annotation. There are two common ways to use it: @RunWith(Suite.class) This allows you to specify which classes compose the test suite in question. This is equivalent to the JUnit 3 style: import junit.framework.TestSuite; import junit.framework.TestCase; public final class MasterTester extends TestCase { public static TestSuite suite() { TestSuite suite = new TestSuite(); suite.addTestSuite(TestClass1.class); suite.addTestSuite(TestClass2.class); // etc... return suite; } } The equivalent JUnit 4 class will be: import org.junit.runners.Suite; @RunWith(Suite.class) @SuiteClasses({TestClass1.class TestClass2.class}) public final class MasterTester { } @RunWith(AllTests.class) This allows you to dynamically specify the tests which compose the test suite. If your tests are not known until runtime you cannot specify them in the annotations. You can use this construction instead. So if the JUnit 3 code is: import junit.framework.TestCase; import junit.framework.TestSuite; import junit.framework.Test; public final class MasterTester extends TestCase { public static TestSuite suite() { TestSuite suite = new TestSuite(); for (Test test : findAllTestCasesRuntime()) { suite.addTest(test); } return suite; } } The equivalent JUnit 4 code will be: import org.junit.runners.AllTests; import junit.framework.TestSuite; import junit.framework.Test; @RunWith(AllTests.class) public final class MasterTester { public static TestSuite suite() { TestSuite suite = new TestSuite(); for (Test test : findAllTestCasesRuntime()) { suite.addTest(test); } return suite; } } The action inside this for loop seems wrong. Do you mean to do suite.addTest(test); ? Thanks for pointing out. I've updated the sources to use addTest() and addTestSuite() why are you making a new TestCase()? shouldn't you add the test that you found? Of course :) Corrected. And what would a `Test` class be? What class should I extend? @Peterdk The same you'll extend in JUnit 3 style test case: junit.framework.TestCase or some specification of it which gives you additional features. @Danail Nachev: Sorry but this is all non-sense (not your fault). JUnit4 proudly replaced inheritance by annotations so I don't have any `Test` just plain old classes. So your solution doesn't work for me unless I edit them all. @maaartinus Fair point."
144,A,"Junit test case for wrapper class I have a Java class that simply extends a library class and calls a method of its parent with a default parameter. How do I write a Junit test for that? A MockObjectTestCase is good too. Here is an example of what I'm talking about: public class ResourceBundleMessageSource { public String getMessage(String key Object[] objects Locale locale) { //Spring library method } } public class MessageResource extends ResourceBundleMessageSource { public String getMessage(String key) { return (getMessage(key null Locale.getDefault()); } } I know the wrapper method isn't even necessary but makes frequent calls to it easier. Note the class works fine I'm only interested in how the unit test is written. For this particular example I probalby would not bother to test it. If you do need to test it try something like: @Test public void getDefaultMessage() { ResourceBundleMessageSource origSource = <create source> MessageResource subSource = <create with same criteria as origSource> String key = <some key that is locale-specific> assertEquals(origSource.getMessage(key null Locale.getDefault()) subSource.getMessage(key)); } If the first two lines are hard to write then it makes even more sense not to test it. If you have several tests like this move the first two lines into a setup fixture.  I don't think it's even worth writing a unit test for that. If there's already a test for ResourceBundleMessageSource.getMessage() then that should be good enough.  If you would be willing to refactor your class slightly I would recommend MessageResource delegate to a MessageSource instance rather than extend ResourceBundleMessageSource. Then I'd use mocks in my unit test. Something like this: public class MessageResource implements MessageSource { private final MessageSource delegate; public MessageResource(MessageSource delegate) { this.delegate = delegate; } public String getMessage(String key) { return delegate.getMessage(key null Locale.getDefault()); } // need to implement three other MessageSource methods // simple pass-throughs to delegate } and unit test public class MessageResourceTest { private MessageSource mockDelegate; private MessageResource messageResource; @Before public void setUp() throws Exception { mockDelegate = //mock with your favorite framework or by hand messageResource = new MessageResource(mockDelegate); } @Test public void testGetMessage() { String key = ""foo""; String actualMessage = messageResource.getMessage(key); assertEquals(key /* get key passed to mock delegate */ ); assertSame(Locale.getDefault() /* get Locale passed to mock delegate */); assertEquals(/*expected message from mock*/ actualMessage); } }"
145,A,"How can I run a custom JUnit4 Runner on JUnit3 test classes with Ant? We have test classes which are built on Spring 2.0.8's AbstractTransactionalDataSourceSpringContextTests. There are a huge number of these all of which are written in JUnit3 style. In order to use JUnit 4 filtering we have concocted a replacement JUnit38Runner which allows us to match these tests to a specific application environment and filter them out accordingly. The whole test suite runs fine outside of Ant by using the @RunWith annotation on our custom JUnit38Runner. When we try to run in Ant however it forces individual tests to run either as junit.framework.TestSuite or wrapped in a JUnit4TestAdapter both of which ignore @RunWith annotations under JUnit4. To make matters worse our existing suites are explicitly overridden by Ant which calls the suite() methods directly rather than delegating to JUnit. I have attempted to extend from the Ant JUnitTestRunner and simply override the run() method however the class is simply not written for extension. Aside from copying the whole of the JUnitTestRunner and hacking it (which will open us up to brittle code issues) has anyone had any luck with other approaches to solving this problem? It seems like there are a couple of code-smells here. I know it's probably not the answer you are looking for but it may be time to refactor your test cases. Refactoring won't fix the problems with Ant (wrapping JUnit 4 tests in Junit 3). With numerous workarounds it's stumbling along but I think I'm going to have to bite the bullet and write a JUnit4 Ant task one of these days. Maybe there is an answer in [Upgrading to JUnit4 and keeping legacy JUnit 3 tests and test suites by running them together][1] ? [1]:http://stackoverflow.com/questions/1861875/upgrading-to-junit4-and-keeping-legacy-junit-3-tests-and-test-suites-by-running @TheNail This is the unfortunate workaround we've had to use - a custom Suite runner and removing all of our old test suites. Can't you just port them instead of removing? We had a similar problem and although it's not as clean as running the junit task it's not terribly difficult to solve. We created a class with a main() that simply invokes the Junit4Runner. It adds a RunListener that attempts to write out the junit report output in XML. The idea was that the dataformat is much less likely to change than the runner so it's less brittle. I've stripped out a fair amount of environment-specific code but this is the basic idea. Our test target in ant looks like this:  <java failonerror=""yes"" fork=""true"" classname=""com.mycompany.test.Junit4Runner""> <classpath> <pathelement location=""${basedir}/bin"" /> <pathelement path=""${ProjectTest.classpath}"" /> <!-- above classpath includes junit-4.8.1.jar --> </classpath> <arg value=""${test.class}"" /> </java> You can view the code for the runner class here. It doesn't depend on anything outside Java 6 SE and Junit 4.8 and it may be compatible with Java 5 SE. That looks like it might work although I'd get you to change the p_ and m_ before committing in my shop ;-). Unfortunately don't have time to test it right now but I'll mark that as solved for now. The p_ and m_ is a style choice in ours - I guess I've grown used to it :)"
146,A,"hibernate - junit noob - class variable value lost between 2 test methods ... I got a simple integration test going and all test methods run fine ... BUT... I set up a class var int tempId; for use by the following methods. testSaveTag() sets the value when successfully executed ( for now everything's auto committed) and the testUpdateTag() updates the freshly created tag. @Test public void testSaveTag() { Tag tag = new Tag(); tag.setDescription(""Test Tag""); tempId = instance.saveTag(tag); } @Test public void testUpdateTag() { Tag tag = instance.getTag(tempId ); tag.setDescription(""updated tag description!""); instance.updateTag(tag); } The value of tempID gets lost between the the 2 methods. So I'm thinking ""What's the proper way to do this"" ... and ""why is the value lost?"" Thanks in advance JUnit test methods should never depend on running in a certain order and should only share class variables that are not changed by the tests. In testUpdateTag() you might have to create and save a new tag just to get an ID. Or is there a way to retrieve a tag that you can then update? BTW I hope you are adding assertions at some point... ;-) That's the ticket! Ehm of course yes :-) Duh I can combine those 2 test!  Sorry I am not the junit expert how ever in most unit test frame works there is a setup and tear down of the system of test which happens between each of your unit test methods. So your tempId is probably not surviving this process. ... hm that makes sense. Thanks."
147,A,"How to setup JUnit tests for Glassfish Embeddable EJBContainer + EclipseLink JPA? I'm trying to use EJB 3.1 Embeddable EJBContainer on Glassfish 3.1 for integration testing my EJB's. There's a classloading issue I can't figure out. My ejbs are build into dum-ejb.jar. They use EclipseLink JPA. I also create EJB client jar dum-ejb-client.jar while attempting to fight the classloading issues. Client jar contains the EJB interfaces and Entity classes (which are usually parameters or returns values). Client jar also contains a lot of unneeded classes that could be dropped (but I don't see how it would solve the problem). The problem is that since EclipseLink does bytecode weaving to the Entity classes the Entity classes must not be in the classpath when the junit tests are run: http://www.java.net/forum/topic/glassfish/glassfish/embedded-glassfish-and-weaving I can do that and configure classpath so that dum-ejb.jar is not included. If I use EJBContainer so that I look up my service as a java.lang.Object and call it's methods via reflection the test works. But of course that's not how I want to write my tests. Typical test would be like: @Test public void testInEJBContainer() throws Exception { File ejbJarFile = new File(""target/dum/dum-ejb.jar""); Map props = new HashMap(); props.put(""org.glassfish.ejb.embedded.glassfish.instance.root"" ""target/classes/instance-root""); props.put(EJBContainer.MODULES new File[]{ejbJarFile}); EJBContainer container = EJBContainer.createEJBContainer(props); CompanyService = (CompanyService) container.getContext().lookup(""java:global/dum/CompanyServiceImpl""); log.info(""result of findAll() "" + service.findAll(false)); } How could I run the test if CompanyService interface and returned Company Entity classes can not be in the classpath? Even if dum-ejb.jar is not on classpath and dum-ejb-client.jar is EclipseLink weaving gets broken. Isn't this exactly the typical use case for EJBContainer shouldn't there be a simple solution to this? Turns out I ran into classloading problems since I was running the EJBContainer from maven ear project. When I run it from the maven ejb project itself there's no such issues and EJBContainer is easy to use."
148,A,"@parameters in Junit 4 Can I have more than one method with @Parameters in junit test class which is running with Parameterized class ? @RunWith(value = Parameterized.class) public class JunitTest6 { private String str; public JunitTest6(String region String coverageKind String majorClass Integer vehicleAge BigDecimal factor) { this.str = region; } @Parameters public static Collection<Object[]> data1() { Object[][] data = {{some data}} return Arrays.asList(data); } @Test public void pushTest() { System.out.println(""Parameterized str is : "" + str); str = null; } @Parameters public static Collection<Object[]> data() { Object[][] data = {{some other data}} return Arrays.asList(data); } @Test public void pullTest() { System.out.println(""Parameterized new str is : "" + str); str = null; } } You can create inner classes for each set of methods that operate on the same parameters. For example: public class JunitTest6 { @RunWith(value = Parameterized.class) public static class PushTest{ private String str; public PushTest(String region) { this.str = region; } @Parameters public static Collection<Object[]> data() { Object[][] data = {{some data}} return Arrays.asList(data); } @Test public void pushTest() { System.out.println(""Parameterized str is : "" + str); str = null; } } @RunWith(value = Parameterized.class) public static class PullTest{ private String str; public PullTest(String region) { this.str = region; } @Parameters public static Collection<Object[]> data() { Object[][] data = {{some other data}} return Arrays.asList(data); } @Test public void pullTest() { System.out.println(""Parameterized new str is : "" + str); str = null; } } } This didn't work for me none of the tests got run.  You can use the Theories runner (search for the word theories at that link) to pass different parameters to different methods. yes this could be the solution Thank you Yishai. @dm76 your link seems to do what the questioner asked as well (multiple data points take the place of multiple parameter methods) so I'm nut understanding your comment? multiple data points yes but they will be all used everytime. once cannot easily set which data would be used for a specific test. That's how I interpreted the question (which also seems to be the most useful situation) According to http://blogs.oracle.com/jacobc/entry/junit_theories you cannot...  Probably the data1 method but no guarantee of that it'll use whichever one the JVM gives junit4 first. Here's the relevant code from junit: private FrameworkMethod getParametersMethod(TestClass testClass) throws Exception { List<FrameworkMethod> methods= testClass.getAnnotatedMethods(Parameters.class); for (FrameworkMethod each : methods) { int modifiers= each.getMethod().getModifiers(); if (Modifier.isStatic(modifiers) && Modifier.isPublic(modifiers)) return each; } throw new Exception(""No public static parameters method on class "" + testClass.getName()); } So the first public static annotated method that it finds will be used but it may find them in any order. Why do you have uour test written that way? You should only have one @Parameters-annotated method. Thank you skaffman. Actually I wanted test two different behavior valid one and invalid one in same test class with different methods. Ah OK. If you're keen then you could write your own Test runner implementation (copy the source for the Parameterized and modify it to make your own) to do that but the built-in Parameterized runner is quite crude. hmm thanks anyways.  It's not designated to have more than one data method. You can see it in skaffman's answer. Why it's not provided to implement two data methods? The answer could be: Coupling. Is it too complex to split this test up into two testcases? You would be able to introduce a small inheritance and share common methods. With two testcases you could provide two separated data methods and test your stuff very well. I hope it helps. An upvote would comfort me ;) that's right it can easily achieved using to separate test cases. Thank you guerda for your response. Actually I was looking for the same way which TestNG uses providing data providers for each method."
149,A,"How can I test final and static methods of a utility project? I'm trying to implement unit testing for aproject it uses a legacy ""utility"" project that is littered with static methods and many of the classes are final or their methods are final. I'm not able to update the legacy project at all. JMock and EasyMock both choke on final methods and I don't see a nice way to test the static calls. What techniques are there to test these? Is there any reason why you need to use a Mock framework? Yes the method calls use JNDI properties to connect to the database and JMS I don't want to implement all the pieces for my test. Are you able to refactor any of the legacy code to assist you? No updated the question to reflect that As already pointed out JMockit can be used. An example: @Test public void mockStaticAndFinalMethods(final LegacyService mock) { new NonStrictExpectations() { { LegacyService.staticMethod(""hello""); result = ""Hello altered World""; } }; String actual = LegacyService.staticMethod(""hello""); new LegacyService().finalMethod(123 ""test""); assertEquals(""Hello altered World"" actual); new Verifications() { { mock.finalMethod(123 ""test""); // verify this call occurred at least once } }; } The JMockit project home page contains a comparison with PowerMock for those interested.  JMock together with JDave can mock final methods and classes if you need to. Here are instructions. That being said I would treat this legacy code (as others have suggested already) as an external dependency and build interfaces and mock those. It is another layer of indirection but since you can't change that legacy code it seems to be a reasonable one.  How about a level of indirection / Dependency Injection? Since the legacy utility project is your dependency create an interface to separate it out from your code. Now your real/production implementation of this interface delegates to the legacy utility methods. public LegacyActions : ILegacyActions { public void SomeMethod() { // delegates to final/static legacy utility method } } For your tests you can create a mock of this interface and avoid interacting with the legacy utility thingie.  JMockit allows you to mock static methods and final classes. I assume it uses some classloadin-fu although I haven't really looked into it. JMockit Expectations API allows expectations to be set on any kind of method invocation (on interfaces abstract classes concrete final or non final classes and on static methods) as well as on class instantiation through any constructors.  If your non-refactorable method uses something like JNDI to connect to another service I'd consider starting a JDNI service and populating it with stubs which you control. It's a pain but relatively straightforward. It may mean setting up a database or JMS listener or whatever but there should be a lightweight java implementation you can drop into the tests. That's ok for integration tests but unit tests shouldn't need a database @Rich Seller: I agree but if you had some eggs you could have bacon and eggs if you had some bacon. By that I mean that the original question stated that he couldn't refactor the code and wanted to add tests. Sometimes you have to work with what you have not what you want. You may be able to mock the database and inject that mock in the JNDI.  If you're able to refactor your code you can wrap your calls to the final/static methods in simple instance methods for example: protected Foo doBar(String name) { return Utility.doBar(name); } This allows you to override your wrapper method in the unit test to return a mock instance of Foo. Alternatively you can use Powermock which extends Easymock (and Mockito) to allow mocking of final and static methods: PowerMock is a framework that extend other mock libraries such as EasyMock with more powerful capabilities. PowerMock uses a custom classloader and bytecode manipulation to enable mocking of static methods constructors final classes and methods private methods removal of static initializers and more. Here's an example test mocking a static final method the example shows how to mock some other types too: @Test public void testMockStaticFinal() throws Exception { mockStatic(StaticService.class); String expected = ""Hello altered World""; expect(StaticService.sayFinal(""hello"")).andReturn(""Hello altered World""); replay(StaticService.class); String actual = StaticService.sayFinal(""hello""); verify(StaticService.class); assertEquals(""Expected and actual did not match"" expected actual); // Singleton still be mocked by now. try { StaticService.sayFinal(""world""); fail(""Should throw AssertionError!""); } catch (AssertionError e) { assertEquals(""\n Unexpected method call sayFinal(\""world\""):"" e.getMessage()); } } +1 for PowerMock since refactoring is out of the question. this looks like what I need fo you know if it works in a Maven build? I've not tried it yet but it appears so: http://code.google.com/p/powermock/wiki/EasyMock_maven thanks I'll try that +1 for introducing a testing tool that I didn't know about. :)"
150,A,"Bad form for JUnit test to throw exception? I'm pretty new to JUnit and I don't really know what best practices are for exceptions and exception handling. For example let's say I'm writing tests for an IPAddress class. It has a constructor IPAddress(String addr) that will throw an InvalidIPAddressException if addr is null. As far as I can tell from googling around the test for the null parameter will look like this. @Test public void testNullParameter() { try { IPAddress addr = new IPAddress(null); assertTrue(addr.getOctets() == null); } catch(InvalidIPAddressException e) { return; } fail(""InvalidIPAddressException not thrown.""); } In this case try/catch makes sense because I know the exception is coming. But now if I want to write testValidIPAddress() there's a couple of ways to do it: Way #1: @Test public void testValidIPAddress() throws InvalidIPAddressException { IPAddress addr = new IPAddress(""127.0.0.1""); byte[] octets = addr.getOctets(); assertTrue(octets[0] == 127); assertTrue(octets[1] == 0); assertTrue(octets[2] == 0); assertTrue(octets[3] == 1); } Way #2: @Test public void testValidIPAddress() { try { IPAddress addr = new IPAddress(""127.0.0.1""); byte[] octets = addr.getOctets(); assertTrue(octets[0] == 127); assertTrue(octets[1] == 0); assertTrue(octets[2] == 0); assertTrue(octets[3] == 1); } catch (InvalidIPAddressException e) { fail(""InvalidIPAddressException: "" + e.getMessage()); } } Is is standard practice to throw unexpected exceptions to JUnit or just deal with them yourself? Thanks for the help. if i understand your question the answer is either - personal preference. personally i throw my exceptions in tests. in my opinion a test failing by assertion is equivalent to a test failing by an uncaught exception. both show something that needs to be fixed. the important thing to remember in testing is code coverage.  In general way #1 is the way to go there is no reason to call out a failure over an error - either way the test essentially failed. The only time way #2 makes sense if you need a good message of what went wrong and just an exception won't give that to you. Then catching and failing can make sense to better announce the reason of the failure.  Since JUnit 4.7 you have the possibility to use an ExpectedException rule and you should use it. The rule gives you the possibility to define exactly the called method where the exception should be thrown in your test code. Moreover you can easily match a string against the error message of the exception. In your case the code looks like this:  @Rule public ExpectedException expectedException = ExpectedException.none(); @Test public void test() { //working code here... expectedException.expect(InvalidIPAddressException.class); IPAddress addr = new IPAddress(null); } UPDATE: In his book Practical Unit Testing with JUnit and MockitoTomek Kaczanowski argues against the use of ExpecetedExcpetion because the rule ""breaks the arrange/act/assert [...] flow"" of a Unit test (he suggests to use Catch Exception Library instead). Although I can understand his argument I think using the rule is fine if you do not want to introduce another 3rd-party library (using the rule is better than catching the exception ""manually"" anyway). This rule asserts that an exception of a certain type is thrown. Going beyond that by also matching a string against the error message of your exception is an entirely different type of test and really falls in the domain of UI or client interface testing. For UI/client interface related testing of exception messages you may also need to consider that the string could be localized in which case you'll need the UI locale information etc. Also the exception can just be instantiated explicitly and its message(s) tested directly. mrjmh is right that matching the message could be used for UI testing (fyi the `ExpectedException` does not offer an `expectLocalizedMessage` or a similar method in the current 4.11 release). Besides UI testing the problem with matching the exception message is that it often leads to overspecified tests.  Reg: Testing for Exceptions I agree with ""Pascal Thivent"" ie use @Test (expected=InvalidIPAddressException.class) Reg: Testing for testValidIPAddress IPAddress addr = new IPAddress(""127.0.0.1""); byte[] octets = addr.getOctets(); I would write a test like class IPAddressTests { [Test] public void getOctets_ForAValidIPAddress_ShouldReturnCorrectOctect() { // Test code here } } The point is when testinput is VALID ipAddress The test must be on the public methods/capabilities on the class asserting that they are working as excepted  Actually the old style of exception testing is to wrap a try block around the code that throws the exception and then add a fail() statement at the end of the try block. Something like this: public void testNullParameter() { try { IPAddress addr = new IPAddress(null); fail(""InvalidIPAddressException not thrown.""); } catch(InvalidIPAddressException e) { assertNotNull(e.getMessage()); } } This isn't much different from what you wrote but: Your assertTrue(addr.getOctets() == null); is useless. The intend and the syntax are clearer IMO and thus easier to read. Still this is a bit ugly. But this is where JUnit 4 comes to the rescue as exception testing is one of the biggest improvements in JUnit 4. With JUnit 4 you can now write your test like this: @Test (expected=InvalidIPAddressException.class) public void testNullParameter() throws InvalidIPAddressException { IPAddress addr = new IPAddress(null); } Nice isn't it? Now regarding the real question if I don't expect an exception to be thrown I'd definitely go for way #1 (because it's less verbose) and let JUnit handle the exception and fail the test as expected. +1 - Pascal your answers have been so impressive. Nice work. @duffymo Thank you your comment is very much appreciated. I actually tried @Test(expected=InvalidIPAddressException.class) for testNullParameter() and the tests returned errors in eclipse. Have you ever run into that? Hmm I just tried with JUnit 4 and Eclipse and the test just runs fine. However I forgot the throws clause in my answer which is required if `InvalidIPAddressException` is a checked exception. I've modified my answer accordingly. Does that help? No. I'll probably post a different question about this though. thanks for the help. The @Test(expected=...) form can actually be worse than an explicit try/catch for instance if the ""setup"" part of the test can throw the expected exception. You lose error localization and your test can stay green even if the exercised method stops throwing the correct exception. The try-catch method will allow you to test the characteristics of the thrown exception if that's an interesting thing to do in context.  For the null test you can simply do this: public void testNullParameter() { try { IPAddress addr = new IPAddress(null); fail(""InvalidIPAddressException not thrown.""); } catch(InvalidIPAddressException e) { } } If the exception has a message you could also check that message in the catch if you wish. E.g. String actual = e.getMessage(); assertEquals(""Null is not a valid IP Address"" actual); For the valid test you don't need to catch the exception. A test will automatically fail if an exception is thrown and not caught. So way #1 would be all you need as it will fail and the stack trace will be available to you anyway for your viewing pleasure.  IMO it is better to handle the exception and show appropriate messaging from the test than throwing it from a test. This solution adds more boilerplate to a test with little to no benefit. Let JUnit handle unexpected exceptions by marking the offending test as an error; if a test's successful outcome depends on the test subject raising a specific exception use the ""expected"" attribute of the Test annotation or use the ExpectedException JUnit Rule. Tests should be designed to be run many times. Most advantage of tests are when they are automated For eg: Every time you check in your code to source control the test can run. So there is very little use of tests that output meaningful texts for humans (cos there is little human intervention) Hence by principle have a test that tests just one aspect and if it fails you must be able to tell without doubt the reason for failure.  For tests where I don't expect an exception I don't bother to catch it. I let JUnit catch the exception (it does this reliably) and don't cater for it at all beyond declaring the throws cause (if required). I note re. your first example that you're not making use of the @expected annotation viz. @Test (expected=IndexOutOfBoundsException.class) public void elementAt() { int[] intArray = new int[10]; int i = intArray[20]; // Should throw IndexOutOfBoundsException } I use this for all tests that I'm testing for throwing exceptions. It's briefer than the equivalent catch/fail pattern that I had to use with Junit3."
151,A,"JUnit Test if XML Document is sorted on arbitrary column Given an xml document like <root> <row> <a>2</a> <b>1</b> <c>8</c> </row> <row> <b>5</b> <a>2</a> <c>8</c> </row> <row> <a>2</a> <c>8</c> <b>6</b> </row> </root> Is there an easy way to assert that the XML document is sorted on element B in XMLUnit Edit: I have an odd problem with a piece of software that I can not change where the value of the XML tags in any given node needs to be in a specific order. I'd like it for my test harness to enforce this set of rules before any other validation. I can't think of any (but then again I don't know XmlUnit et al). Why do you want to do this? Is this the output of some method to be tested? Can you refactor the code to produce output in a more testable format? Good Question (+1). See my answer for a very simple solution. :) @Peter I'm dealing with some legacy software where the XML needs to be in a quirky format. I'd like to make sure my xml output always obeys those quirky rules. Use the value of the following simple XPath 1.0 expression: not(//b[. > following::b]) This is almost what I wanted. I got my solution after looking at this. Thanks! Hey sal what IS your solution? @belwood: @sal says that he put this XPath expression in his code -- probably the real column names were different. If you have the same problem this answer should get you going.  If you can execute XQuery then the XQuery assertion: every $i in (1 to count ($x/row) - 1) satisfies let $j := $i + 1 return number($x/row[$i]/b) <= number($x/row[$j]/b) where $x is the document is true if the rows are in ascending order of b false otherwise I'm too junior to comment on Dimitre's neat answer but shouldn't it be not($x//b[number(.) > following::b/number(.)]) for this numeric data and isnt this O N^2 since every b is compared with all the following b's? Chris you're right about number()  You can build an assertion around an XPath selection. Probably the simplest way would be to use the XPath expression /root/row/b request the NODESET result type then copy the text from each result node into a List structure. You can assert that the text in that list is equal to an ""expected"" list created with the Arrays.asList() method (I'm assuming you're using Java). This was what I was doing and I wasn't happy with it. For starters there was too much code that was specific to a particular test and more code that might have to change as my code changed."
152,A,"Customized new JUnit templates in Eclipse? I've spent a few hours searching for this and can't find it so I'm asking. Why here? How can I customize Eclipse so that when I create a new JUnit test class or method it includes different items? For other source I'd just go into Window->Preferences and change the templates. But for JUnit that doesn't seem to be possible. I've recently installed MoreUnit and that doesn't seem to help either. Anyone have a pointer? What specifically do you mean by ""it includes different items""? Do you want particular methods to show up by default? Or do you want specify settings in the Run Configuration by default? Or something else? Specifically I have some ""test helper"" methods I like to have available in my junit test classes. I therefore would like to make my new test class extend a utility class. Sadly it appears that this is not possible. It does not seem to be directly available. The idea has been raised to save a file (like a JUnit empty file) as a template to reuse it at future (JUnit for instance) creation time. This is bug 37440... and it is not very popular (since 2003!). The closest would be Window > Preferences > Java > Editor - Templates and look under ""test method (JUnit4)"" However as mentioned in this thread: but that will not affect the auto-generated JUnit test method stub templates. In other words in the New JUnit Test Case dialog one can select the methods of the class under test and test methods stubs are auto-generated. To modify the template that creates these JUnit test method stubs... seems not possible at the moment. Bug 71783 is somewhat related to this issue regarding the comment templates of those methods. Note: the generation of the JUnit class itself seems entirely hard-coded in the class org.eclipse.jdt.junit.wizards.NewTestCaseWizardPageOne Speaking of template you also needed to to provide VM arguments to all JUnit tests you run manually copying them for every new configuration. Bug 37862 has been set to address that and allow to create a Installed JRE for use with a JUnit testing configuring it with the correct set of VM arguments. +1 very thorough Excellent though disappointing answer. Thanks. Disappointing no I'd say insane. Eclipse was supposed to be an ecosystem with plugins and infinite customizability. This is ludicrous. (BTW easily done in Xcode from those bad closed guys..)"
153,A,"Suggestion for JUnit testing All While writing a test method for method A (which has many internal conditions) should I focus on testing a single condition each time? I am finding it difficult to structure a test method for method A that would cover all the code path in method A. Can anyone please suggest me how to go about writing a test method.? you should write a test for every execution path. For example if you have if (cond1 || cond2) {....} you should test for cond1 and cond2. Separate test methods are fine and encouraged if it makes sense. Its ok to have testMyMethodCond1(){...} and testMyMethodCond2(){...} and whatever else you need. Also when you say your method has 'many internal conditions' maybe you want to refactor your code so some of those conditions are handled in other smaller methods that are easier to test.  My preference is to have one failure point per JUnit test method. This means that I will have many JUnit test methods per target class method that I'm testing. In your example I may have testA1() testA2() testA3() all testing the same method (A). Each of these would test a different success or failure condition of method A. If there are 8 paths through method A then you need at least 8 test methods calling it and maybe some for error handling conditions. Thanks. Additionally if the method I am testing is throwing an IOException then should my test method throw the same exception? i.e. `public void testA1() throws IOException` ? Catch the exceptions in your JUnit test methods. You should include JUnit test methods that force the exception to be thrown (when possible). Fail or pass your JUnit test based on expected results of the call into your method. A JUnit test method is a success if it expects exception xxx to be thrown and the method being tested throws exception xxx. If the method you are testing declares that it throws IOException but the particular test is not supposed to cause that exception to be thrown then yes your test method should just declare that it throws IOException (I usually just say ""throws Exception""). You don't want to catch the exception in the test unless you are trying to test that a method given the parameters and the state of the object should throw an exception.  Do not feel the need to have one-test-per-method. Keep your unit tests fine-grained descriptive and easy to understand. If that means multiple similar tests all calling the same target method then do it. For that matter try and avoid the habit of systematically writing a unit test for each method. Unit testing should be well thought-out not habitual. They should describe the behaviour of classes rather than of individual methods. +1. I often separate tests that are designed to throw exceptions (e.g. @Test(expected=SomeException.class)) from those that satisfy various assertions. Complex methods may have a dozen test cases while simpler methods may be sufficiently tested with one.  Tests should be simpler preferably much simpler than the thing tested. Otherwise the error is more likely to be in your test. So it's much better to have a lot of small simple test methods that execute small parts of your method than one big clunky one. (You can use a code coverage tool like Cobertura to verify that you're covering all paths in your method.)  First of all you should consider breaking your method into several methods if it has more than one responsibility. Secondly I would advice you to write multiple tests for each method. Each test should cover a specific path through the method under testing. Each test should also (of course) give the expected outcome depending on your test data."
154,A,Source code is in JDK 1.4 and JUnit test cases in JDK 1.5 I am working on a project that is using JDK 1.4 and I am planning to write JUnit test cases in JDK 1.5 for several reasons like: I wanna explore JDK 1.5 to use Mocking frameworks more efficiently. Is it a good idea to do so?? (I've created two projects in Eclipse and am trying this) Yes go ahead. But you might not able to use few things for example generics. I mean if the original method is returing say Map you can't say in your test something like Map<String Integer> map = someOriginalMehtod(); I suppose. you can do that will just get a some unchecked warnings  No need for Java 5 as it seems James Carr backported Mockito to Java 4 see http://blog.james-carr.org/2009/10/01/using-mockito-with-junit3. I never used that backport personally though.  Yeah is perfect. As long as you run it in Java 5 of course ;) Java 1.4 is a subset of Java 1.5
155,A,"Java testing a method with JUnit (void) I have the following void method and i have no idea how to test it with a JUnit test. public void removeFriend(String rf) { boolean found = false; int i = 0; while(i < friendList.size() && !found) { if(friendList.get(i).equals(rf)) { found = true; } else{ i++; } } friendList.remove(i); } What's it removing a friend from? What's the rest of the class look like? You might find studying the ""programming by contract"" style of class design helpful. It works well with the ""test driven development"" (TDD) style. Briefly you describe and document what a mutator method is meant to do only in terms of values returned by its accessible (public and protected) getter methods.  Add some friends to whatever this object is assert that they're there. Call removeFriend() assert that the one you removed isn't there.  I would assume your class has more than one method. You need to use these other methods to test the behaviour before and after calling this method. BTW: Your method appears to remove the first entry if no match is found. Is this intended? would blow up if there were no entries. would blow up if any entries in friendList is null otherwise does the same thing as friendList.remove(rf); I would write a test for all these conditions. The important thing to look for is all the conditions where your method might break rather than just a situation where it might work.  presumably friendList is a field on the class where the method was defined. So to test: Create an instance of the class and populate friendList Call removeFriend with a String that should match. assert that the friendList no longer contains the friend. You probably also want to test by passing in a String for a friend that isn't in the list to assert that the list is unchanged. Edit: its an interesting question you raised in the comments. I usually am very explicit with these sorts of things so I would do List<String> friendList = new ArrayList<String>(); friendList.add(""friend1""); friendList.add(""friend2""); // assert before actually calling the method under test to make sure my setup is ok assertEquals(2 friendList.size()); // theObj is an instance of the class you are testing theObj.friendList = friendList; // call the method under test theObj.removeFriend(""friend1""); // be explicit the list should now have size1 and NOT contain the removed friend assertEquals(1 friendList.size()); assertFalse(friendList.contains(""friend1""); Note that I have left some stuff out but that is a general outline. Don't forget you can test the case where you remove a String that is NOT in the list. You might also want to test the case where the same friend is in the list two times. Thanks is this required to do i with assertEquals? not really sure how to populate the assert. @Mael i edited my answer.  First of all are you aware that you can replace the entire method with friendList.remove(rf);? It will do exactly what the method name claims (and is guaranteed to be already thoroughly tested). As for testing even the most dogmatic proponents of ""pure"" unit tests don't claim that each method of a class in an OO language has to be testable in complete isolation. Otherwise you couldn't test OO code at all and Unit testing would be restricted to functional languages. So what you do in the test is create an object of the class that contains the method use its constructor or setters to put it into a specific state the method can operate on then call the method and then test that the object is in the expected state afterwards. More concretely: Create an object of the class put some entries into friendList call removeFriend() with one of the entries test that this entry is not anymore present"
156,A,"Ensure that all getter methods were called in a JUnit test I have a class that uses XStream and is used as a transfer format for my application. I am writing tests for other classes that map this transfer format to and from a different messaging standard. I would like to ensure that all getters on my class are called within a test to ensure that if a new field is added my test properly checks for it. A rough outline of the XStream class @XStreamAlias(""thing"") public class Thing implements Serializable { private int id; private int someField; public int getId(){ ... } public int someField() { ... } } So now if I update that class to be: @XStreamAlias(""thing"") public class Thing implements Serializable { private int id; private int someField; private String newField; public int getId(){ ... } public int getSomeField() { ... } public String getNewField(){ ... } } I would want my test to fail because the old tests are not calling getNewField(). The goal is to ensure that if new getters are added that we have some way of ensuring that the tests check them. Ideally this would be contained entirely in the test and not require modifying the underlying Thing class. Any ideas? Thanks for looking! Easy. Don't test getting methods. They are ""too simple"" for testing. You can use Sonar. I find it very useful and easy to set up. Also easy to integrate with hudson. They also have an eclipse plugin.  I have written such thing you can find it here. It makes proxy of the object so that all calls are intercepted and later can be checked.  It sounds like you are talking about a meta test - a test of a test. IMHO it doesn't make a lot of sense. The test is the spec. So the developer of the test case should be updating the test when they add new fields. That said here's a ghetto way to call all the getter methods of a Java bean. public class GhettoBeanTest { private static class Subclass extends SomeObject { @Override public String toString() { return org.apache.commons.lang.builder.ToStringBuilder.reflectionToString(this); } } @Test public void ensureToStringMethodExecutes() { String value = new Subclass().toString(); // teset passed - all getter methods were executed by the toString method above } }  Consider Java Bean Tester: http://sourceforge.net/projects/javabeantester/ We use it very often. And we like it.  If you would like to test your tests automatically than JUnit is not a good option. How you would like to ensure that your ""special test"" will be invoked always as last after all other tests? (I don't say it is not possible but it is against JUnit model/philosophy saying test cases should be independent of each other). If by any chance you will decide to do this one of the possibilities is to generate Cobertura code coverage report and parse it to find if all getters were covered during the tests. Possibly there is also a way to intercat with Cobertura through some API instead of depending on output report files. Example algorithm: run regular test suite run Cobertura run special test case  May be code coverage tools is what you need. If you have 100% code coverage all get methods have been called. If you are using eclipse check EclEmma plugin. Crap. I need the ""stupid-helmet"" badge from StackOverflow. I have Emma installed and am using it in other projects. Thank you for the reminder Ha."
157,A,"How to create structured automatic testing with selenium (junit or testng)? What is your best practice for this? I have written tests that are using selenium for functional testing. I'm using Java. I would like to be able to create structure between tests. Is it possible to do it with jUnit or TestNG? Would like to be able to create test structure that will produce report like this: Top test FAIL - Module1 test PASS -- Module1Class1 test PASS -- Module1Class2 test PASS --- Module1Class3Method1 test PASS --- Module1Class3Method2 test PASS - Module2 test FAIL -- Module2Class1 test FAIL --- Module2Class1Method1 test PASS --- Module2Class1Method2 test FAIL --- Module2Class1Method3 test PASS I have tests that are doing functional testing using Selenium. I generated those with Selenium IDE or manually. I exported them to java. I am using Selenium RC. I can run them as Java application or as jUnit tests to have reports from it. The problem is that I have many classes and a lot of tests in it. I would like to run those module by module having not only results (pass/fail) from module level test but also from all tests in classes below methods below. AFAIK jUnit allows only flat test structure. So I wonder about testNG. Does that make it more clear? In TestNG you can define suites via the configuration file (testng.xml) which should match your needs. You could structure it to have a test group for every Module. A failure in a test group renders the whole test as a failure. But i think you could also do that in JUnit which i am not too familiar with. What is nice in TestNG is that you can define dependencies. These will enforce a certain logical order for test execution and will skip tests that depend on failing tests instead of letting them fail as well. Makes analysis a lot more easier and tests end earlier because all those tests that are potentially doomed to fail will be left aside. But like i said earlier i think you can do that in JUnit as well. It's more a matter of taste. And if you decide otherwise it is not a big undertaking to convert from JUnit to TestNG or visa versa.  You cannot because this is not the format that Selenium interprets tests. Selenium supports several different language syntaxes the easiest of which are JS and HTML table rows. You could use the record feature of the Selenium IDE from Firefox if you do not want to hand write your tests to fit a certain language syntax. Hi. Thanks for answer. But I'm not asking about Selenium syntax. This is question for junit/testng tests results. Maybe I should remove information about Selenium from description not to confuse anyone ;) Unit tests are tests from within code to analyze if bits of code do as they are intended. That is not what Selenium tests. Selenium tests for what the browser sees post render which is not what parts of interactive code are written for but rather what an application is written for in full.  The best pattern I have seen for organizing the code behind selenium tests is the page object pattern: http://blog.m.artins.net/acceptance-tests-with-jbehave-selenium-page-objects/ Here's a Java helper library: http://code.google.com/p/webdriver/wiki/PageFactory Interesting thanks :)  I would recommend looking at the JUnit or TestNG XML reports. You should be able to transform them with a bit of XSLT to provide a new HTML report with your required format.  I am not complete sure if i am right here..but i think this can help you: http://www.jamesnetherton.com/blog/2007/07/02/Creating-a-Selenium-test-suite/ You can group your tests with it in an very easy structered way. Thanks for the idea. My question is more like one comment for this blog post on your site: Posted by Madhavi | Tuesday 22 January 9:01 AM ""(...) Do you know any way to inbed one test suite into another. I need to create smaller test suites for each functionality and include those test suites into the master test suite."" Thanks :) The idea behind is to have not only small tests for each functionality but being able to run all of those small grouped on level1 then a few of those grouped on level2 and so on. If the group on level2 (as all in them) fails - then having possibility to dig down to group on level1 and then to the one small test that failed. Isn't it usual practice for writing functional tests? You have a neat idea there...isn't the same procedure that a lot of people use to learn foreign languages? Talking about Unit testing..i don't know if anybody developed something for this issues. I'm sorry...i can't answer your question. from my point of knowledge it is not possible but also not needed. You have to design your code around those test classes not the other way. Sorry I think your approach is maybe to complex. But an solution would be to design special test suites and place them into packages. So you can test software components via packages. Hope this helps... It looks like digging into testng can help with it. See my last link.  did you consider using Selenium Remote Control? http://seleniumhq.org/projects/remote-control/ I'm using Selenium RC  I just have found the solution for this question so I'm putting here link so that others can benefit from it. beust.com I haven't try that yet though. Update: After trying it I can generate results like this: Module1.Class3.Method1 PASS Module1.Class3.Method2 PASS Module2.Class1.Method1 PASS Module2.Class1.Method2 FAIL Module2.Class1.Method3 PASS The fail method name is ""Method2"" it is located in class ""Class1"" and it is package ""Module2"". I have used all the standard possibilities of TestNG (a lot of it) + I have overwritten the TestListenerAdapter using ITestResult methods: getName() and getTestClass().getName() It's not really the structure I was looking for but little bit parsing can tell me where the fail was. And as a plus I don't have to name methods with class and package name in it. +1 looks nice. Thank you for that link"
158,A,"Android AsyncTask testing with Android Test Framework I have a very simple AsyncTask implementation example and have problem to test it using Android JUnit framework. It works just fine when I instantiate and execute it in normal application. However when it's executed from any of Android Testing framework classes (i.e. AndroidTestCase ActivityUnitTestCase ActivityInstrumentationTestCase2 etc) it behaves strangely: - It executes doInBackground() method correctly - However it doesn't invokes any of its notification methods (onPostExecute() onProgressUpdate() etc) -- just silently ignores them without showing any errors. This is very simple AsyncTask example package kroz.andcookbook.threads.asynctask; import android.os.AsyncTask; import android.util.Log; import android.widget.ProgressBar; import android.widget.Toast; public class AsyncTaskDemo extends AsyncTask<Integer Integer String> { AsyncTaskDemoActivity _parentActivity; int _counter; int _maxCount; public AsyncTaskDemo(AsyncTaskDemoActivity asyncTaskDemoActivity) { _parentActivity = asyncTaskDemoActivity; } @Override protected void onPreExecute() { super.onPreExecute(); _parentActivity._progressBar.setVisibility(ProgressBar.VISIBLE); _parentActivity._progressBar.invalidate(); } @Override protected String doInBackground(Integer... params) { _maxCount = params[0]; for (_counter = 0; _counter <= _maxCount; _counter++) { try { Thread.sleep(1000); publishProgress(_counter); } catch (InterruptedException e) { // Ignore } } } @Override protected void onProgressUpdate(Integer... values) { super.onProgressUpdate(values); int progress = values[0]; String progressStr = ""Counting "" + progress + "" out of "" + _maxCount; _parentActivity._textView.setText(progressStr); _parentActivity._textView.invalidate(); } @Override protected void onPostExecute(String result) { super.onPostExecute(result); _parentActivity._progressBar.setVisibility(ProgressBar.INVISIBLE); _parentActivity._progressBar.invalidate(); } @Override protected void onCancelled() { super.onCancelled(); _parentActivity._textView.setText(""Request to cancel AsyncTask""); } } This is a test case. Here AsyncTaskDemoActivity is a very simple Activity providing UI for testing AsyncTask in mode: package kroz.andcookbook.test.threads.asynctask; import java.util.concurrent.ExecutionException; import kroz.andcookbook.R; import kroz.andcookbook.threads.asynctask.AsyncTaskDemo; import kroz.andcookbook.threads.asynctask.AsyncTaskDemoActivity; import android.content.Intent; import android.test.ActivityUnitTestCase; import android.widget.Button; public class AsyncTaskDemoTest2 extends ActivityUnitTestCase<AsyncTaskDemoActivity> { AsyncTaskDemo _atask; private Intent _startIntent; public AsyncTaskDemoTest2() { super(AsyncTaskDemoActivity.class); } protected void setUp() throws Exception { super.setUp(); _startIntent = new Intent(Intent.ACTION_MAIN); } protected void tearDown() throws Exception { super.tearDown(); } public final void testExecute() { startActivity(_startIntent null null); Button btnStart = (Button) getActivity().findViewById(R.id.Button01); btnStart.performClick(); assertNotNull(getActivity()); } } All this code is working just fine except the fact that AsyncTask doesn't invoke it's notification methods when executed by within Android Testing Framework. Any ideas? The way to deal with this is to run any code that invokes an AsyncTask in runTestOnUiThread(): public final void testExecute() { startActivity(_startIntent null null); runTestOnUiThread(new Runnable() { public void run() { Button btnStart = (Button) getActivity().findViewById(R.id.Button01); btnStart.performClick(); } }); assertNotNull(getActivity()); // To wait for the AsyncTask to complete you can safely call get() from the test thread getActivity()._myAsyncTask.get(); assertTrue(asyncTaskRanCorrectly()); } By default junit runs tests in a separate thread than the main application UI. AsyncTask's documentation says that the task instance and the call to execute() must be on the main UI thread; this is because AsyncTask depends on the main thread's Looper and MessageQueue for its internal handler to work properly. NOTE: I previously recommended using @UiThreadTest as a decorator on the test method to force the test to run on the main thread but this isn't quite right for testing an AsyncTask because while your test method is running on the main thread no messages are processed on the main MessageQueue — including the messages the AsyncTask sends about its progress causing your test to hang. @Matthieu Were you using `runTestOnUiThread()` from a test method with the `@UiThreadTest` decorator? That won't work. If a test method does not have `@UiThreadTest` it should run on its own non-main thread by default. Oh that could me. I'll double check that. Nope did not work. The postExecute method of the ASyncTask is never getting called... Anyway got something working (even if it's not very pretty) thanks to you. That saved me... although I had to call ""runTestOnUiThread"" from yet another thread otherwise I would get ""This method can not be called from the main application thread"" That answer is pure jewel. It should be reworked to emphasize on update if you really want to keep initial answer put it as some background explanation and a common pitfall. @Snicolas good point. I've rewritten my answer to emphasize the ""update"" and gotten rid of the old incorrect code.  I met a similar problem while implementing some unit-test. I had to test some service which worked with Executors and I needed to have my service callbacks sync-ed with the test methods from my ApplicationTestCase classes. Usually the test method itself finished before the callback would be accessed so the data sent via the callbacks would not be tested. Tried applying the @UiThreadTest bust still didn't work. I found the following method which worked and I still use it. I simply use CountDownLatch signal objects to implement the wait-notify (you can use synchronized(lock){... lock.notify();} however this results in ugly code) mechanism. public void testSomething(){ final CountDownLatch signal = new CountDownLatch(1); Service.doSomething(new Callback() { @Override public void onResponse(){ // test response data // assertEquals(.. // assertTrue(.. // etc signal.countDown();// notify the count down latch } }); signal.await();// wait for callback } What is `Service.doSomething()`? I'm testing an AsynchTask. Did this and well the background task is seems never be called and singnal waits forever :( @Ixx did you call `task.execute(Param...)` before `await()` and put `countDown()` in `onPostExecute(Result)`? (see http://stackoverflow.com/a/5722193/253468) Also @PeterAjtai `Service.doSomething` is an async call like `task.execute`.  I found a lot of close answers but none of them put all the parts together correctly. So this is one correct implementation when using an android.os.AsyncTask in your JUnit tests cases.  /** * This demonstrates how to test AsyncTasks in android JUnit. Below I used * an in line implementation of a asyncTask but in real life you would want * to replace that with some task in your application. * @throws Throwable */ public void testSomeAsynTask () throws Throwable { // create a signal to let us know when our task is done. final CountDownLatch signal = new CountDownLatch(1); /* Just create an in line implementation of an asynctask. Note this * would normally not be done and is just here for completeness. * You would just use the task you want to unit test in your project. */ final AsyncTask<String Void String> myTask = new AsyncTask<String Void String>() { @Override protected String doInBackground(String... arg0) { //Do something meaningful. return ""something happened!""; } @Override protected void onPostExecute(String result) { super.onPostExecute(result); /* This is the key normally you would use some type of listener * to notify your activity that the async call was finished. * * In your test method you would subscribe to that and signal * from there instead. */ signal.countDown(); } }; // Execute the async task on the UI thread! THIS IS KEY! runTestOnUiThread(new Runnable() { @Override public void run() { myTask.execute(""Do something""); } }); /* The testing thread will wait here until the UI thread releases it * above with the countDown() or 30 seconds passes and it times out. */ signal.await(30 TimeUnit.SECONDS); // The task is done and now you can assert some things! assertTrue(""Happiness"" true); } thank for writing up a complete example... I was having a lot of small problems implementing this. Just over 1 year later and you saved me. Thank you Billy Brackeen! This was working perfectly for me and then out of the blue all of a sudden test failures even if I run the code included in the sample above with the final assert validating the countdown is 0 very strange. If you want a timeout to count as a test failure you can do: `assertTrue(signal.await(...));`  If you don't mind executing the AsyncTask in the caller thread (should be fine in case of Unit testing) you can use an Executor in the current thread as described in https://stackoverflow.com/a/6583868/1266123 public class CurrentThreadExecutor implements Executor { public void execute(Runnable r) { r.run(); } } And then you run your AsyncTask in your unit test like this myAsyncTask.executeOnExecutor(new CurrentThreadExecutor() testParam); This is only working for HoneyComb and higher.  I wrote enough unitests for Android and just want to share how to do that. First off here is helper class that responsible to wait and release waiter. Nothing special: SyncronizeTalker public class SyncronizeTalker { public void doWait(long l){ synchronized(this){ try { this.wait(l); } catch(InterruptedException e) { } } } public void doNotify() { synchronized(this) { this.notify(); } } public void doWait() { synchronized(this){ try { this.wait(); } catch(InterruptedException e) { } } } } Next lets create interface with one method that should be called from AsyncTask when work is done. Sure we also want to test our results: TestTaskItf public interface TestTaskItf { public void onDone(ArrayList<Integer> list); // dummy data } Next lets create some skeleton of our Task that we gonna test: public class SomeTask extends AsyncTask<Void Void SomeItem> { private ArrayList<Integer> data = new ArrayList<Integer>(); private WmTestTaskItf mInter = null;// for tests only public WmBuildGroupsTask(Context context WmTestTaskItf inter) { super(); this.mContext = context; this.mInter = inter; } @Override protected SomeItem doInBackground(Void... params) { /* .... job ... */} @Override protected void onPostExecute(SomeItem item) { // .... if(this.mInter != null){ // aka test mode this.mInter.onDone(data); // tell to unitest that we finished } } } At last - our unitest class: TestBuildGroupTask public class TestBuildGroupTask extends AndroidTestCase implements WmTestTaskItf{ private SyncronizeTalker async = null; public void setUP() throws Exception{ super.setUp(); } public void tearDown() throws Exception{ super.tearDown(); } public void test____Run(){ mContext = getContext(); assertNotNull(mContext); async = new SyncronizeTalker(); WmTestTaskItf me = this; SomeTask task = new SomeTask(mContext me); task.execute(); async.doWait(); // <--- wait till ""async.doNotify()"" is called } @Override public void onDone(ArrayList<Integer> list) { assertNotNull(list); // run other validations here async.doNotify(); // release ""async.doWait()"" (on this step the unitest is finished) } } That's all. Hope it will help to someone. Thanks good job :)"
159,A,"can't find run as junit test in eclipse I created a test class in Eclipse like this @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:applicationContext.xml"" }) @TransactionConfiguration @Transactional public class TeamTest extends AbstractTransactionalJUnit4SpringContextTests { @Test public void testCreate() { assert (true); }} However when I click right click on the file I don't see option to run as JUnit! What is wrong? I am using Eclipse 3.6 What import is specified to load the Test annotation? i.e. make sure it is 'import org.junit.Test' @Pace: ""extends AbstractTransactionalJUnit4SpringContextTests"" is enough to testable in Eclipse. Good to know thanks! Make sure you are using Junit as your testing framwork and not something like TestNg which cover most of Junit's purpose.  Make sure your eclipse environment is using JUnit 4. JUnit 3 doesn't make use of annotations (it uses the old extends TestCase style) There are few things to double check: Window > Preferences > Java > JUnit Are you seeing junit4 or junit3 imports? If that looks good make sure the project itself is using JUnit4 instead of JUnit3. Right Click on project > Properties > Java Build Path > Libraries Is JUnit4 included there? Is anything JUnit related there? If JUnit3 is in there click on it and click Remove. Then click Add Library... and follow the prompts from there to add JUnit again. Out of curiosity are the JUnits run outside of eclipse? Like with a mvn install or whatever build target you have for Ant that'll run JUnits Even if you are on JUnit 4+  if your test ""extends testCase"" it wont find the @Test annotations - remove the exdends. This answer is good and helped me.  Write a simple test case to see if Eclipse works correctly or not. If simple test case can be run check your testcase especially import classes. Or make a try in ""Run"" -> ""Run Configurations""fill the ""Test class"" as ""TeamTest""(full class name). Then click ""Run"" see what will happen..."
160,A,Run IntelliJ JUnit Tests in Serial I'm using the JUnit runner in IntelliJ to run all tests in a package. When I run my tests in isolation they pass when I run the entire package some fail suggesting interplay between the tests. I'd like to force the tests to be run in serial rather than parallel to prove this - can anyone tell me how I can configure this to happen? Thanks Tests don't run in parallel at least not in IDEA 9.x. Unless you are using IDEA 10 EAP your problem is different and you need to provide more details to get a good answer.
161,A,Using assertArrayEquals in unit tests My intention is to use assertArrayEquals(int[] int[]) JUnit method described in the API for verification of one method in my class. But Eclipse shows me the error message that it can't recognize such a method. Those two imports are in place: import java.util.Arrays; import junit.framework.TestCase; Did I miss something? Thank you! What version of JUnit you're using? Have you used JUnit assertions before? Are you sure you want unit testing and not e.g. `java.util.Arrays.equals/deepEquals` methods? Frankly I don't know what version of JUnit I'm using but I know I've downloaded the Eclipse Version: 3.5.1 and JUnit was included. I haven't been using JUnit assertions before I'm just learning. And yeah I want unit testing. I found out - it is JUnit3. Yes and the referenced JavaDoc is for JUnit 4. Javadoc for 3.8.1 can be found [here](http://www.junit.org/junit/javadoc/3.8.1/index.html) This should work with JUnit 4: import static org.junit.Assert.*; import org.junit.Test; public class JUnitTest { /** Have JUnit run this test() method. */ @Test public void test() throws Exception { assertArrayEquals(new int[]{123}new int[]{123}); } } (answer is based on this wiki article) And this is the same for the old JUnit framework (JUnit 3): import junit.framework.TestCase; public class JUnitTest extends TestCase { public void test() { assertArrayEquals(new int[]{123}new int[]{123}); } } Note the difference: no Annotations and the test class is a subclass of TestCase (which implements the static assert methods). It is JUnit3 but I'm getting the message: Description Resource Path Location Type The method assertArrayEquals(int[] int[]) is undefined for the type DeckTest are you extending TestCase? Please post sample code that shows the class in question. @AndoidNoob - Assert@assertArrayEquals has been introduced with JUnit 4. So you either have to switch to JUnit 4 (always recommended) or verify the equality of arrays with several Java statements (loop through the array after making sure they're of the same size) @Andreas_D your answer implies that `assertArrayEquals` can be used with JUnit 3. It is in contradiction with your own comment and with siik's tests (as well as my tests). `assertArrayEquals` is simply incompatible with JUnit 3.  If you are writing JUnit 3.x style tests which extend TestCase then you don't need to use the Assert qualifier - TestCase extends Assert itself and so these methods are available without the qualifier. If you use JUnit 4 annotations avoiding the TestCase base class then the Assert qualifier is needed as well as the import org.junit.Assert. You can use a static import to avoid the qualifier in these cases but these are considered poor style by some.  Try adding import static org.junit.Assert.*; assertArrayEquals is a static method.  This could be useful if you want to use just assertEquals without depending on your Junit version assertTrue(Arrays.equals(expected actual));
162,A,"Unable to test exception with junit I have a method which contains a try-catch block and I don't know how to make my test pass... Here is my code: public class ClassToTest { public void loadFileContent() { try { InputStream fileStream = fileLoader.loadFileContent(); fileContentReader = new BufferedReader(new InputStreamReader(fileStream)); } catch(CustomException ce) { logger.log(Level.SEVERE ""Error message"") } } } public class TestClassToTest { @Test (expected = CustomException.class) public void testCustomException() throws Exception { loadFileContent(); } } The only thing I want to do when the exception is thrown is to log it and that all works great however how can I create a test that shows that it works? I tried to capture console output and assertTrue(consoleOutput.contains(""logMessgae"") and that worked as long as I ran only that specific test case but it did not work when I ran it in my test suite. I think the right way to do this is pass your logger into your class and if you're using a mocking framework like Mockito you can verify your logger with verify: ClassToTest myInstance = new ClassToTest(myLogger); // then in your unit test verify(myLogger).log(""my log message"") the key here is passing in your dependencies so you can then verify interactions with them.  If you're trying to test that CustomException is thrown under certain conditions then you should be testing the code that throws the exception rather than the code that catches it. In your example I'm guessing that your undefined ""fileLoader"" is what you're expecting to throw the exception? If you're trying to test that the exception is caught and things continue to run smoothly then you need to redesign ClassToTest so that a mock implementation of the potentially exception throwing class can be injected. That mock implementation would always throw the exception. Then you can run through ClassToTest in your test and assert whatever condition indicates that it handled the exception properly.  The contract of the method is to catch CustomException and log ""Error Message"" to an external component : the logger. You should thus create a mock logger give it to the ClassToTest instance test the method and verify that the mock logger has been called with the aright arguments. I personnally use EasyMock (and its ""classextension"" extension) to mock concrete classes but there are other mock frameworks. Read http://easymock.org/EasyMock3_0_Documentation.html for information about EasyMock and mocking in general. I started with EasyMock but these days strongly prefer Mockito which is based on EasyMock but extended to have greater coverage & easier to use. Check out http://mockito.org Thanks for the fast response! It solved my problem! @John you should upvote all answers that you find useful. (Including ones that you choose to accept.)  Your code is untestable without having a mock file in place. You can make this code more testable by mocking out external dependencies and using dependency injection (i.e. via guice)."
163,A,"How to JUnit tests a @PreAuthorize annotation and its spring EL specified by a spring MVC Controller? I've defined this method in my Spring MVC Controller : @RequestMapping(value = ""{id}/content"" method=RequestMethod.POST) @PreAuthorize(""principal.user.userAccount instanceof T(com.anonym.model.identity.PedagoAccount) AND principal.user.userAccount.userId == #object.pedago.userId AND #form.id == #object.id"") public String modifyContent(@PathVariable(""id"") Project object @Valid @ModelAttribute(""form"") ProjectContentForm form) { .... } Then in my JUnit test I'd like to call this method and ensure that the PreAuthorize condition is verified. But when I set the user principal in my JUnit test with a bad account there is no error and the method completes. It seems the annotation is bypassed. But when I call this method in a normal way (not testing) the PreAuthorize is verified. If it's possible how to test this annotation in a junit test and how to catch the exception if it throws one ? Thanks Nicolas Just a note for others using ""principal"" in PreAuthorize this will fail with a nullpointerexcpetion and not trigger a reauthorization unless you either include something to the effect of isAuthenticated() either in the PreAuthorize annotation or in the security.xml Since you want to test features implemented via Spring AOP you need to use Spring TestContext framework to run tests against application context. Then you create a base test with minimal security configuration: abstract-security-test.xml: <security:authentication-manager alias=""authenticationManager""> <security:authentication-provider user-service-ref = ""userService"" /> </security:authentication-manager> <security:global-method-security pre-post-annotations=""enabled"" /> <bean id = ""userService"" class = ""..."" /> AbstractSecurityTest.java: @ContextConfiguration(""abstract-security-test.xml"") abstract public class AbstractSecurityTest { @Autowired private AuthenticationManager am; @After public void clear() { SecurityContextHolder.clearContext(); } protected void login(String name String password) { Authentication auth = new UsernamePasswordAuthenticationToken(name password); SecurityContextHolder.getContext().setAuthentication(am.authenticate(auth)); } } Now you can use it in your tests: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(...) public class CreatePostControllerSecurityTest extends AbstractSecurityTest { ... @Test @ExpectedException(AuthenticationCredentialsNotFoundException.class) public void testNoAuth() { controller.modifyContent(...); } @Test @ExpectedException(AccessDeniedException.class) public void testAccessDenied() { login(""userWithoutAccessRight"" ""...""); controller.modifyContent(...); } @Test public void testAuthOK() { login(""userWithAccessRight"" ""...""); controller.modifyContent(...); } } Thank you it works fine for authentication but if want to test the whole Spring EL defined in the PreAuthorize annotation how do I do ? Thank you it works fine for authentication but if I want to test the whole Spring EL defined in the PreAuthorize annotation how do I do ? I mean how to verify principal.user.userAccount.userId == #object.pedago.userId AND #form.id == #object.id where #object and #form designate variables of the modifyContent method. Is there any exception to catch for an unverified security Spring EL ? @Nico: Updated. The @ExpectedException annotation is deprecated; use JUnit4's @Test(expected=…) instead. Terrific advice. This works very well for me. One thing I did a bit different is that I separated out my existing `security-context.xml` from my `webapp-context.xml` so that I can use the security context configuration in my tests without including the web app stuff. That way I can reuse the existing security config in my tests."
164,A,"Spring Transactional Unit Tests - Hibernate Queries Not Rolling Back I am using Spring 3 and Hibernate 3.5 I am not getting my transactions to rollback in the test environment which has me worried they would not be rolled back in production either. Test Class: @ContextConfiguration(loader = MyConfigurationLoader.class) @TransactionConfiguration(transactionManager = ""transactionManager"" defaultRollback = true) public class DashTemplateRepositoryTest extends AbstractMulitpleDataSourceSpringContextTests { @Autowired DashTemplateRepository dashTemplateRepository; @Test public void testSaveCategory() { int initialCount = getCategoryCount(); Category c = new Category(); c.setName(""mynewcategory""); dashTemplateRepository.save(c); assertEquals(initialCount + 1 getCategoryCount()); } } Which extends a custom class : public abstract class AbstractTransactionalTemplateTests extends AbstractTransactionalJUnit4SpringContextTests { protected SimpleJdbcTemplate simpleJdbcTemplate; @Autowired DashTemplateRepository dashTemplateRepository; @Resource(name = ""dashDataSource"") public void setDataSource(final DataSource dataSource) { this.simpleJdbcTemplate = new SimpleJdbcTemplate(dataSource); } ..snip... UPDATE: I needed to do the simpleJdbcTemplate insertion because I have multiple dataSources and by default this test class can't handle that I wasn't able to find a spring supported solution but on the spring forums a contributor posted this workaround. The rollback problem existed before I extracted this super class. I'm assuming the problem is more basic than that hibernate doesn't seem to be aware of the transaction manager is there any way I can prove that? Datasource Bean: <bean id=""dashDataSource"" class=""org.springframework.jdbc.datasource.DriverManagerDataSource""> <property name=""driverClassName"" value=""com.mysql.jdbc.Driver""/> <property name=""url"" value=""jdbc:mysql://localhost:3306/dashtemplate""/> <property name=""username"" .../> <property name=""password"" .../> </bean> My Context-text.xml (simplified left out some of it) <!-- Hibernate --> <bean id=""sessionFactory"" class=""org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean""> <property name=""dataSource"" ref=""dashDataSource"" /> <property name=""annotatedClasses""> <list> <value>com.dash.Category</value> </list> </property> <property name=""hibernateProperties""> <props> <prop key=""hibernate.show_sql"">true</prop> <prop key=""hibernate.dialect"">org.hibernate.dialect.MySQLDialect</prop> </props> </property> </bean> <bean id=""dashTemplateRepository"" class=""com.wdp.DashTemplateRepositoryHibernateTemplateImpl""> <property name=""sessionFactory"" ref=""sessionFactory""/> </bean> <tx:annotation-driven transaction-manager=""transactionManager""/> <bean id=""transactionManager"" class=""org.springframework.orm.hibernate3.HibernateTransactionManager""> <property name=""sessionFactory"" ref=""sessionFactory""/> </bean> The test runs fine my object is persisted but it is never rolled back there doesn't appear to be any errors either What does `DashTemplateRepositoryHibernateTemplateImpl` look like? @Bozho: I believe the `defaultRollback` attribute of `@TransactionConfiguration` is supposed to indicate that why should it be rolled-back? so a rollback should happen even for a successful transaction? Sounds strange :) Furthermore `true` seems to be the default value. @bozho that's the default behavior of the spring transactional test suite. that way you can run integration tests on real databases without polluting them `AbstractMulitpleDataSourceSpringContextTests` seems to be a custom class at least I can't find it. What does it extend? @seanizer - thanks for the clarification @seanizer - yes it's a custom class I put in the relevant portion of that class it extends springs tests. This is what my base test class looks like and works fine for me. @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""classpath*:/context.xml""}) @TransactionConfiguration(transactionManager = ""transactionManager"" defaultRollback = true) @Transactional public class BaseTest extends AbstractTransactionalJUnit4SpringContextTests { //.... } this worked for me many months later in a different project - didn't solve the original issue though  The problem was that MySQL was using MyISAM instead of InnoDB - rollbacks are not supported by MyISAM"
165,A,What testing framework for c# is the most similar to Junit 4? Mainly from feature-set standing point as well as similarity of method names/working logic/verbosity ? NUnit is a port of JUnit to .Net.
166,A,Running JUnit test classes from another JUnit test class I have two classes that I am testing (let's call them ClassA and ClassB). Each has its own JUnit test class (testClassA and testClassB respectively). ClassA relies on ClassB for its normal functioning so I want to make sure ClassB passes its tests before running testClassA (otherwise the results from testClassA would be meaningless). What is the best way to do this? In this case it is for an assignment so I need to keep it to the two specified test classes if possible. Can/should I throw an exception from testClassA if testClassB's tests aren't all passed? This would require testClassB to run invisibly and just report its success/failure to testClassA rather than to the GUI (via JUnit). I am using Eclipse and JUnit 4.8.1 Update: So far the best I've managed is a separate file with a test suite as shown below. This is still not quite what I'm after as it still allows ClassB tests to be run if ClassA fails some tests. import org.junit.runner.RunWith; import org.junit.runners.Suite; @RunWith(Suite.class) @Suite.SuiteClasses({testClassA.class testClassBCode.class}) public class testClassB { } You should try testng framework which allow this kind of dependency testing. As in earlier comment: this is for an assignment and JUnit tests are a requirement.  Your unit tests should not depend on other tests. One solution for your problem would be to mock ClassB in your ClassA's tests in order to test its behaviour independently of the results of ClassB's tests. I recommend you give mockito a try! In this case as it's an assignment I can't rely on my lecturers having mockito - essentially they're just going to copy my Java package into their project run their tests on my classes and run my tests on their classes. (for future reference obviously I'd like to know the best way to approach such things). Although it may not be ideal at the moment I'm just looking for a way (if one exists) to say `//don't run this test unless ClassB has passed its tests` without having to rely on the tester seeing my comment. I don't think you can do it in JUnit. You can specify test order by using OrderedSuite add-on for JUnit TestNG or other testing frameworks.
167,A,"How to unit test a method that runs into an infinite loop for some input? This question just occurred to my mind and I want to ask this here. The case is intentional I just write a loop which runs infinitely. How do I go about unit testing it? I ask this because this situation may occur anywhere in the code. Say my method delegates to several other methods and I want to know How it ran into an infinite loop What set of input caused it Call to which method (from this method) has caused this I have no code written for this. The question is purely for knowledge sake as to what to do if this situation arises in future. Please respond. Is the infinite loop intentional? Also google ""halting problem"". May I know the reason from the one who has asked this question to be closed as to why to close it? @dlev yup it is intentional. This just came up to my mind when I was thinking of a situation where this might happen @Shankar: when is it supposed to retrieve result from that method? @zerkms I edited my question. Please reply @Shankar: can you think of any example? Purpose of unitesting is to test each most simple unit in your program. Most simple unit is usually a function which does a single mission so in order to unitest your infinite Loop you will have to extract each single mission this Loop could do in a seperated function which can be called alone once done this you will be able to call these functions given them all possible parameter in order to test different szenarios your function should be able to handle. The infinite Loop as a function doesnt have to be unitested but the smaller missions inside of it.  Have the function that loops delegate the loop check to an injected dependency: interface IShouldLoop { bool ShouldLoop(); } class ClassToTest { private final IShouldLoop shouldLoop; public ClassToTest(IShouldLoop shouldLoop) { this.shouldLoop = shouldLoop; } public void MethodToTest() { while(shouldLoop.ShouldLoop()) { // do whatever } } } You can test that it delegates the loop check to the IShouldLoop dependency every time and you can control the looping in your test so that it only loops the number of times you want. In the production environment instantiate it with an instance of IShouldLoop that always returns true. +1. Very nice approach.  I hope you mean some kind of message-pump/event-handling loop. (Most infinite loops are bad). I'd ensure that the loop delegates to some class which processes the input. Test this class thoroughly. The probability of a loop construct failing to work is minimal.. So I'd test this out via an acceptance test or manually. This is somewhat similar to testing the Main function of an executable. The trick here is also to ensure that Main delegates to a testable class.  How to unit test a method that runs into an infinite loop for some input? You can test the nearly opposite: ""How to unit test a method so that the method will not run longer than Xxxx milliseconds for some input"". If this test fails you may have found a candidate with an infinite loop. NUnit 2.5 has a TimeoutAttribute that makes a test fail if the test takes longer than the given amount of milliseconds."
168,A,"Best Practice: Initialize JUnit class fields in setUp() or at declaration? Should I initialize class fields at declaration like this? public class SomeTest extends TestCase { private final List list = new ArrayList(); public void testPopulateList() { // Add stuff to the list // Assert the list contains what I expect } } Or in setUp() like this? public class SomeTest extends TestCase { private List list; @Override protected void setUp() throws Exception { super.setUp(); this.list = new ArrayList(); } public void testPopulateList() { // Add stuff to the list // Assert the list contains what I expect } } I tend to use the first form because it's more concise and allows me to use final fields. If I don't need to use the setUp() method for set-up should I still use it and why? Clarification: JUnit will instantiate the test class once per test method. That means list will be created once per test regardless of where I declare it. It also means there are no temporal dependencies between the tests. So it seems like there are no advantages to using setUp(). However the JUnit FAQ has many examples that initialize an empty collection in setUp() so I figure there must be a reason. Since each test is executed independently with a fresh instance of the object there's not much point to the Test object having any internal state except that shared between setUp() and an individual test and tearDown(). This is one reason (in addition to the reasons others gave) that it's good to use the setUp() method. Note: It's a bad idea for a JUnit test object to maintain static state! If you make use of static variable in your tests for anything other than tracking or diagnostic purposes you are invalidating part of the purpose of JUnit which is that the tests can (an may) be run in any order each test running with a fresh clean state. The advantages to using setUp() is that you don't have to cut-and-paste initialization code in every test method and that you don't have test setup code in the constructor. In your case there is little difference. Just creating an empty list can be done safely as you show it or in the constructor as it's a trivial initialization. However as you and others have pointed out anything that can possibly throw an Exception should be done in setUp() so you get the diagnostic stack dump if it fails. In your case where you are just creating an empty list I would do the same way you are suggesting: Assign the new list at the point of declaration. Especially because this way you have the option of marking it final if this makes sense for your test class. +1 because you're the first person to actually support initializing the list during object construction for marking it final. The stuff about static variables is off-topic to the question though. @Motlin: true the stuff about static variables is a little off-topic. I'm not sure why I added that but it seemed appropriate at the time an extension of what I was saying in the first paragraph.  The simple best case is this: your test case class tests a single class. Obviously each test method will need an instance of that class. But no test should have to handle the mingled object from previous tests. So one can either 'new' it up in each test or use 'setUp()' (or a @Before annotated method) to keep it DRY. I see no reason to give with the former. Edit: Even though there might not be any shared state - it's still DRYer! No reason to 'new' it up again and again... There's no shared state please see the comments to the other answers. @Motlin - See edit @abyx Similar to Bill the Lizard's answer. But the list is a field not a local variable. So there's no repetition.  Your field initializers will be run once per test method before any tests are run. As long as your field values are small in memory take little set up time and do not affect global state using field initializers is technically fine. However if those do not hold you may end up consuming a lot of memory or time setting up your fields before the first test is run and possibly even running out of memory. For this reason many developers always set field values in the setUp() method where it's always safe even when it's not strictly necessary. Note that in JUnit 4 test object initialization happens right before test running and so using field initializers is safer and recommended style. Interesting. So the behavior you described at first only applies to JUnit 3? Yes indeed Craig.  In your case (creating a list) there is no difference in practice. But generally it is better to use setUp() because that will help Junit to report Exceptions correctly. If an exception occurs in constructor/initializer of a Test that is a test failure. However if an exception occurs during setup it is natural to think of it as some issue in setting up the test and junit reports it appropriately. @Olaf Thanks for the info about the coding standard I hadn't thought about that. I tend to agree with Moss Collum's idea of a coding standard more though. well said. Just get used to always instantiate in setUp() and you have one question less to worry about - e.g. where should I instantiate my fooBar where my collection. It's a kind of coding standard that you just need to adhere to. Benefits you not with lists but with other instantiations.  If you're wondering specifically about the examples in the JUnit FAQ such as the basic test template I think the best practice being shown off there is that the class under test should be instantiated in your setUp method (or in a test method). When the JUnit examples create an ArrayList in the setUp method they all go on to test the behavior of that ArrayList with cases like testIndexOutOfBoundException testEmptyCollection and the like. The perspective there is of someone writing a class and making sure it works right. You should probably do the same when testing your own classes: create your object in setUp or in a test method so that you'll be able to get reasonable output if you break it later. On the other hand if you use a Java collection class (or other library class for that matter) in your test code it's probably not because you want to test it--it's just part of the test fixture. In this case you can safely assume it works as intended so initializing it in the declaration won't be a problem. For what it's worth I work on a reasonably large several-year-old TDD-developed code base. We habitually initialize things in their declarations in test code and in the year and a half that I've been on this project it has never caused a problem. So there's at least some anecdotal evidence that it's a reasonable thing to do.  I started digging myself and I found one potential advantage of using setUp(). If any exceptions are thrown during the execution of setUp() JUnit will print a very helpful stack trace. On the other hand if an exception is thrown during object construction the error message simply says JUnit was unable to instantiate the test case and you don't see the line number where the failure occurred probably because JUnit uses reflection to instantiate the test classes. None of this applies to the example of creating an empty collection since that will never throw but it is an advantage of the setUp() method.  In addition to Alex B's answer. It is even required to use the setUp method to instantiate resources in a certain state. Doing this in the constructor is not only a matter of timings but because of the way JUnit runs the tests each test state would be erased after running one. JUnit first creates instances of the testClass for each test method and starts running the tests after each instance is created. Before running the test method its setup method is ran in which some state can be prepared. If the database state would be created in the constructor all instances would instantiate the db state right after each other before running each tests. As of the second test tests would run with a dirty state. JUnits lifecycle: Create a different testclass instance for each test method Repeat for each testclass instance: call setup + call the testmethod With some loggings in a test with two test methods you get: (number is the hashcode) Creating new instance: 5718203 Creating new instance: 5947506 Setup: 5718203 TestOne: 5718203 Setup: 5947506 TestTwo: 5947506 Correct but off topic. The database is essentially global state. This is not a problem I face. I'm merely concerned with execution speed of properly independent tests.  I prefer readability first which most often does not use the setup method. I make an exception when a basic setup operation takes a long time and is repeated within each test. At that point I move that functionality into a setup method using the @BeforeClass annotation (optimize later). Example of optimization using the @BeforeClass setup method: I use dbunit for some database functional tests. The setup method is responsible for putting the database in a known state (very slow... 30 seconds - 2 minutes depending on amount of data). I load this data in the setup method annotated with @BeforeClass and then run 10-20 tests against the same set of data as opposed to re-loading/initializing the database inside each test. Using Junit 3.8 (extending TestCase as shown in your example) requires writing a little more code than just adding an annotation but the ""run once before class setup"" is still possible. @BeforeClass changes the semantics of the test - it changes the state. I really need an empty list for each test. So this no longer answers the question. @Motlin I added the dbunit example to clarify how you can optimize with setup. +1 because I also prefer readability. However I'm not convinced that the second way is an optimization at all. The database is essentially global state. So moving the db setup to setUp() is not an optimization it's necessary for the tests to complete properly. @Alex B: As Motlin said this is not an optimization. You are just changing where in the code the initialization is done but not how many times nor how quickly. I intended to imply use of the ""@BeforeClass"" annotation. Editing the example to clarify."
169,A,"How to load DBUnit test data once per case with Spring Test Spring Test helpfully rolls back any changes made to the database within a test method. This means that it is not necessary to take the time to delete/reload the test data before each test method. But if you use the @BeforeClass Junit annotation then that forces the data loader to be static. A question that is explored here: http://stackoverflow.com/questions/1052577/why-must-junits-fixturesetup-be-static If the data initialization method is static so must the data connection methods and the data source..and on and on...forcing everything to be static...which won't work. At which point I ask - what good is Spring Test's ability to rollback changes when you have to delete/reload the test data anyway for every test??!?! Spring Test and DbUnit is two excellent frameworks. But it doesn't make sense to combine them. Since Spring Test execute a rollback on the connection it cleans up afterwards while DbUnit cleans up and insert test data in the @Before method. Use Spring if you're not dependent on any dynamic data and dbUnit otherwise. I find DBUnit's ability to load a database table from XML to be convenient. Does Spring Test have a capability like this? Yeah I think Mats is wrong here. Loading test data from an XML file is a nice benefit to db testing with Spring.  We use DBUnit in conjunction with Spring Test extensively. But we do not use the DBUnit functionality to delete data at the end of the test. We put a bunch of DBUnit inserts for our test data in the @Before method to initialise the test. Then when the test is complete we let the spring rollback functionality bring the database back to its original state. The biggest problem we have with this is that the DBUnit data has to be loaded before each test which can be a major performance hit. Most of our tests using DBUnit are read only testing the behaviour of the application based on certain predefined behaviour. So we have a habit of creating master tests that then run all the fine grain tests in a batch within the same transaction. Well that's bad practice. What do you think of the checked solution?  One approach that works is to create a ""data initialiser"" class add it to a test Spring application context that also has your data source and wire this application context into your tests. This relies on the fact that Spring caches the application context between test invocations. For example a test superclass: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""classpath:test-application-context.xml""}) @Transactional public abstract class DataLoadingTest { @Autowired protected DatabaseInitialiser databaseInitialiser; } With test-application-context.xml: <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd""> <bean id=""dataSource"" .../> <bean class=""DatabaseInitialiser""> <property name=""dataSource"" ref=""dataSource""/> </bean> </beans> And public class DatabaseInitialiser extends JdbcDaoSupport { @PostConstruct public void load() { // Initialise your database here: create schema use DBUnit to load data etc. } } In this example: all tests that rely on the database extend DataLoadingTest; Spring initialises the application context upon first test invocation; this calls DatabaseInitialiser.load() via the @PostConstruct annotation; Spring keeps the application context in a cache; further test invocations wire in the DatabaseInitialiser from the application context which is already cached; tests are transactional and roll back at the end to the initial data set. Likewise DatabaseInitialiser can have a method annotated @PostDestroy to perform any rollback necessary at the end of the whole test run. This is in fact exactly how I solved the problem I just forgot to put my code/answer back into SO. Thanks for taking the time to do this for the next guy.  Methods annotated with @BeforeTransaction run like its name suggests before the transaction of each test is started. If in such method you can detect if the test data is loaded then one can load the data when needed. Beware though that the data is left in your (in-memory) database for all subsequent tests. We use this to load ""static"" data that would in a production environment also be bootstrapped into our database when starting it. This way we actually use exactly the same code and data for our tests rather than relying on (DbUnit) exports that might become outdated."
170,A,"Testing with UI elements in Android view I am attempting to test simple UI with the following test case The main idea is to set in the test some of the UI text (to mimic user input) and then actively click an event. public class StackTestCase extends ActivityInstrumentationTestCase2<Stack> { private StackDemo mActivity; private EditText eaten; public StuckTestCase() { super(""com.crocodil.software.stack"" Stack.class); } public StuckTestCase(Class<Stack> activityClass) { super(""com.crocodil.software.stack"" activityClass); } protected void setUp() throws Exception { super.setUp(); mActivity = this.getActivity(); mCount = (Button) mActivity.findViewById(com.crocodil.software.stack.R.id.action); eaten = (EditText) mActivity.findViewById(com.crocodil.software.stack.R.id.eaten); } public void testPreconditions() { assertNotNull(mStatus); } public void testSimpleDefaults(){ double status = Double.valueOf(mStatus.getText().toString()); eaten.setText(2); mCount.performClick(); assertEquals((status-2)Double.valueOf(mStatus.getText().toString())); } } the running result is the exception - android.view.ViewRoot$CalledFromWrongThreadException: Only the original thread that created a view hierarchy can touch its views. at android.view.ViewRoot.checkThread(ViewRoot.java:2802) at android.view.ViewRoot.playSoundEffect(ViewRoot.java:2581) at android.view.View.playSoundEffect(View.java:8516) at android.view.View.performClick(View.java:2407) at com.crocodil.software.stack.test.StackTestCase.testSimpleDefaults(StackTestCase.java:46) at java.lang.reflect.Method.invokeNative(Native Method) at android.test.InstrumentationTestCase.runMethod(InstrumentationTestCase.java:204) at android.test.InstrumentationTestCase.runTest(InstrumentationTestCase.java:194) at android.test.ActivityInstrumentationTestCase2.runTest(ActivityInstrumentationTestCase2.java:186) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:169) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:154) at android.test.InstrumentationTestRunner.onStart(InstrumentationTestRunner.java:520) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1447) This happens on each access to the UI elements and i was unable to avoid it by using handles or async task ? any suggestions? This is an old question but I'm giving you an answer anyway in case someone stumbles upon it. You are not allowed to change states of UI widgets from anywhere but the main thread (UI thread). Your performClick must be done like this: mActivity.runOnUiThread(new Runnable() { @Override public void run() { mCount.performClick(); } }); But that is not all you will also need to sync your instrumentation test with the ui by adding the following line: getInstrumentation().waitForIdleSync(); The sync line is usually placed immediately after the runOnUiThread() code. I'm facing an issue such that 'waitForIdleSync();' call itself hangs in infinite loop. Debugging the issue found that 'emptyRunnable()' message posted through the handler is not getting caught in 'MessageQueue'. Seems some timing issue is present. But not sure how to locate the issue.  The issue is that you can only touch / change views from the UI thread. You can use a runnable as Rober outlined or an annotation. You can run all methods inside a test on the UI thread using the @UiThreadTest annotation:  @UiThreadTest public void testSimpleDefaults(){ double status = Double.valueOf(mStatus.getText().toString()); eaten.setText(2); mCount.performClick(); assertEquals((status-2)Double.valueOf(mStatus.getText().toString())); }"
171,A,Upgrade eclipse 3.5(Galileo) JUnit I would like to upgrade the JUnit version of my Eclipse Galileo (on windows) to JUnit 4.8.1 instead of the included JUnit 4.5. There does not appear to be a plugin update to use the new version or a way to change the location of the JUnit 4 jars. How can I update to the new version and/or use a JUnit jar not under the eclipse install directory? Note: Eclipse Galileo 3.5.2 (just released today) and Eclipse Helios 3.6M5 (to be released) have JUnit4.8 in them. See this Planning Meeting Notes from last January 27th. Darn... As The OP Andrew comments the latest Eclipse 3.5.2 still got a JUnit4.5. As stevendick adds: it shouldn't make it in the 3.5 release cycle but should probably make it in the last 3.6Mx releases. I have updated previously today and am running 3.5.2r352 of org.eclipse.jdt my junit.jar in plugins is still 4.5.0 I'd be surprised that a bug-fix release of Eclipse (3.5.2) contained such a major upgrade to junit - I'm doing an upgrade to check this now. You could call those planning meeting notes ambiguous on the item about JUnit 4.8.1 - I suspect it applies only to the 3.6 milestone. @stevendick: agreed. I just updated the answer to reflect just that.  For plain java projects You could add the junit.jar you want to use as a file in your project add it to the build path and remove the eclipse defined junit library from the build path (assuming it was there). For plugin projects You can do as above or create a new plugin from the junit 4.8 jar and add it to your test plugin's dependencies. This is a plain java project. Will adding the updated JUnit jar as a Refrenced Library cause Eclipse to run JUnit tests with the runners and features from the updated JUnit? I don't think it will. No I don't think it will affect the runner that the IDE uses but I would expect that your test code would see the junit code from the reference library at run time. I guess it depends on what new features of JUnit you want to take advantage of. Well maybe its not that simple. Im having a similar problem trying to override the Eclipse Ant plugin by adding a newer ant to the top of my build path and it didn't help. So theres probably more to this answer.
172,A,"Junit and Java classpath woes - OS X I'm trying to run the sample tests that come with junit4.7 and having some difficulty. java is respecting my CLASSPATH: me@dinosaurhunter ~/Desktop> export CLASSPATH= me@dinosaurhunter ~/Desktop> echo $CLASSPATH me@dinosaurhunter ~/Desktop> java junit.textui.TestRunner junit.samples.AllTests Exception in thread ""main"" java.lang.NoClassDefFoundError: junit/textui/TestRunner me@dinosaurhunter ~/Desktop> source /etc/profile me@dinosaurhunter ~/Desktop> echo $CLASSPATH :/Library/Java/Extensions/junit/:/Library/Java/Extensions/junit/junit.jar me@dinosaurhunter ~/Desktop> java junit.textui.TestRunner junit.samples.AllTests Exception in thread ""main"" java.lang.NoClassDefFoundError: junit/framework/Test at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:164) at junit.runner.BaseTestRunner.loadSuiteClass(BaseTestRunner.java:207) at junit.runner.BaseTestRunner.getTest(BaseTestRunner.java:100) at junit.textui.TestRunner.start(TestRunner.java:179) at junit.textui.TestRunner.main(TestRunner.java:138) but as you can see it's unable to find junit/framework/Test... I looked in the /Library/Java/Extensions/junit/junit.jar and it is included though. /Library/Java/Extensions/junit/junit.jar is a symlink. Is that okay? with -cp and an unset $CLASSPATH I get the same results even without using the symlink. what happens if you add a -classpath var to the commandline and specify the jar directly? Did you build the JAR yourself? This looks like the JAR is built without annotation. If you compile the JUnit with  javac -proc none ... You will get this error.  Ok I just downloaded JUnit 4.7 unpacked the zip file changed directory into the folder and ran the following command successfully: $ java -cp .:junit-4.7.jar junit.textui.TestRunner junit.samples.AllTests ......................................... ......................................... ......................................... ....... Time: 3.255 OK (130 tests) That was on OSX. I think in your example your classpath is a little messed up. Try this: .:/Library/Java/Extensions/junit:/Library/Java/Extensions/junit/junit.jar See the differences? I added the . (current directory) and I removed the trailing slash from the junit directory. UPDATE: I just tested with a symlink and that appears to work too: $ ln -s junit-4.7.jar junit.jar $ java -cp .:junit.jar junit.textui.TestRunner junit.samples.AllTests ......................................... ......................................... ......................................... ....... Time: 2.569 OK (130 tests)  Some things to investigate: Is the junit.jar file in the location you specify and is it readable? Is CLASSPATH exported by /etc/profile? Does it work when you set ""-cp $CLASSPATH"" instead on the command line? Try removing the leading colon in the classpath -- should not be there. Yes the junit.jar exists there with permissions 755. CLASSPATH is exported by /etc/profile. Unsetting CLASSPATH and using -cp gives me the same results. The leading colon doesn't seem to affect it either.  Try putting the junit JAR file directly into the Extensions directory instead of creating a subdirectory for it. I just copied the junit-4.6.jar into the /Library/Java/Extenstions directory and executed the TestRunner class with no issues % java junit.textui.TestRunner Usage: TestRunner [-wait] testCaseName where name is the name of the TestCase class Deleting the library from the Extensions directory results in the expected exception  Exception in thread ""main"" java.lang.NoClassDefFoundError: junit/textui/TestRunner  It is possible that using a symlink is causing your problem. Trying adding the jar that symlink is pointing to and try again. However looking at the java man page for mac os x there doesn't seem to be such a restriction. I did it without the symlink and I'm still getting the exact same issue."
173,A,"Test driven development: Void methods I have been learning about TDD (using JUnit) and I have a doubt about how to go about testing void methods in which case I can't directly use something like an assertTrue() on the return value of a method.. For example say I have a simple console based application and a part of it prints a menu on screen say using this method: public void printMenu() { System.out.println(""Menu:""); System.out.println(""1. Option ONE""); System.out.println(""2. Option TWO""); System.out.println(""3. Exit""); } My question is do I actually have to test this method?? And if so how should I do it? capture the console output and compare to expectations  You can not unit test this method. This method does no logic or processing that you need to unit test. If you need to unit test the Print Menu you can consider outputing the result to a text file. Then read the text file and compare the menu texts..  First: testing UI is hard. Some people don't bother testing things like this because it is very difficult to write meaningful tests that aren't fragile to the point of uselessness. I wouldn't bother testing this method. But: If you want to test menu generation because your menu code is complex and you need ways to ensure that it works you have a couple choices. Refactor your method so that it accepts the output stream as a parameter then pass in an output stream whose contents you can inspect. You might also be able to redirect System.out to achieve this. Refactor your method so that the menu is generated as a bunch of objects then printed separately. You can inspect and verify those objects.  It is difficult to unit test a method which relies on static method calls. It is not a matter of returning something or void. What you could do is abstract the printing into an interface and have your class depend on this interface (using constructor injection for example): private SomePrinterInterface _printer; public void printMenu() { _printer.println(""Menu:""); _printer.println(""1. Option ONE""); _printer.println(""2. Option TWO""); _printer.println(""3. Exit""); } In your unit test you could mock the interface and verify if correct methods have been called on it. This way you could test the printMenu in independently."
174,A,"JUnit - Testing a method that in turn invokes a few more methods This is my doubt on what we regard as a ""unit"" while unit-testing. say I have a method like this public String myBigMethod() { String resultOne = moduleOneObject.someOperation(); String resultTwo = moduleTwoObject.someOtherOperation(resultOne); return resultTwo; } ( I have unit-tests written for someOperation() and someOtherOperation() seperately ) and this myBigMethod() kinda integrates ModuleOne and ModuleTwo by using them as above then is the method ""myBigMethod()"" still considered as a ""unit"" ? Should I be writing a test for this ""myBigMethod()"" ? Say I have written a test for myBigMethod()... If testSomeOperation() fails it would also result in testMyBigMethod() to fail... Now testMyBigMethod()'s failure might show a not-so-correct-location of the bug. One-Cause causing two tests to fail doesn't look so good to me. But donno if there's any better way...? Is there ? Thanks ! You're not combining the results in the code you've shown. I have used the result of someOperationOne() as an argument for someOtherOperation() Oh sorry I didn't catch that. The test for myBigMethod() should test the combination of the results of the other two methods called. So yes it should fail if either of the methods it depends on fails but it should be testing more. There should be some case where someOperation() and someOtherOperation() work correctly but myBigMethod() can still fail. If that's not possible then there's no need to test myBigMethod(). thanks ! that clarifies it for me..! :)  You want to test the logic of myBigMethod without testing the dependencies. It looks like the specification of myBigMethod is: Call moduleOneObject.someOperation Pass the result into moduleTwoObject.someOtherOperation Return the result The key to testing just this behavior is to break the dependencies on moduleOneObject and moduleTwoObject. Typically this is done by passing the dependencies into the class under test in the constructor (constructor injection) or setting them via properties (setter injection). The question isn't just academic because in practice moduleOneObject and moduleTwoObject could go out and hit external systems such as a database. A true unit test doesn't hit external systems as that would make it an ""integration test""."
175,A,"How to use @BeforeClass and @AfterClass in JunitPerf? I want to do some actions before the whole test suite (also after the suite). So I wrote like: public class PerformanceTest extends TestCase { @BeforeClass public static void suiteSetup() throws Exception { //do something } @AfterClass public static void suiteSetup() throws Exception { //do something } @Before public void setUp() throws Exception { //do something } @After public void tearDown() throws Exception { //do something } public PerformanceTest(String testName){ super(testName); } public static Test suite() { TestSuite suite = new TestSuite(); Test testcase1 = new PerformanceTest(""DoTest1""); Test loadTest1 = new LoadTest(testcase1 n); Test testcase2 = new PerformanceTest(""DoTest2""); Test loadTest2 = new LoadTest(testcase2 n); return suite; } public void DoTest1 throws Throwable{ //do something } public void DoTest2 throws Throwable{ //do something } } But I found that it never reach the code in @BeforeClass and @AfterClass. So how could I do to solve this problem? Or is there other way to realize this? Thank you for your help. You cannot extend TestCase and use annotations at the same time. TestCase is a JUnit 3 concept and annotations are a JUnit 4 concept. So how can I do in this case to execute the initial function before the test suite programmatically or using annotations? In fact what I have accomplished was I put the suite() function in another class called PerformanceTestSuite{} then I integrated the code of the initial actions in the suite() function. That worked but...not good as I sense"
176,A,"Unit testing synchronization Consider the following method: /** * Set whether messages are printed to System.out. * * @param printOutput True to print false for silent logging */ public void setPrintOutput(boolean printOutput) { // Synchronize to messages because this field is used when a message is received synchronized (messages) { this.printOutput = printOutput; } } This method is part of a set of several methods that involve messages so I want to write a test that checks that this method is synchronized on messages. Does anyone know how I would do this? The code ""synchronized(messages)"" causes this to be synchronized on the variable ""messages"". This is part of the language and does not need to be unit tested. I think this is beyond unit testing because the whole point of synchronization is to provide a certain type of connection between two distant pieces of code. You can test the way it behaves with messages being null and not null and that gives you full coverage. It won't say anything about whether your synchronization is semantically correct. So in that case this really devolves to a trivial method then? @Michael Pretty much so. Unit testing has its limitations this is one of them. Proving the correctness of a synchronization scheme isn't impossible but is beyond unit testing.  If you really want to unit test it then expose the message to the test code get it's lock in one thread and check that you cannot aquire it from another thread (with timeout preventing deadlock). As @biziclop notices you should not test that. You can measure throughput responsiveness but testing that some particular intrinsic lock is taken is too detailed."
177,A,"Testing a dataset returned from a database If I am testing a database rowset what would be the criteria to test against? Should I test that a row exists with a name of A etc etc (What's in the database)? Is this a bad idea? This sort of test is fragile as the data can change but the query can be correct. With that in mind would it be better to check the column names I have returned in the dataset? Like so (http://stackoverflow.com/questions/696782/retrieve-column-names-from-java-sql-resultset). Thanks I am really confused what condition are you testing for? Testing what's in the database is pointless unless it is data static reference data tables (and you can often make these read only so again pointless). Testing columns names? Why would these change? Hopefully developers are not making changes against the production server? What you probably want is as part of your CI build process to fully deploy your databases (applying all incremental change scripts [you have those right?]) to a test server.  Don't test what's in the database. Just provide some examples of how your system delivers value. If the value is genuinely in the database - for instance another service uses that data - then describe the things your system provides in terms of that value. For instance: My system can save a user with his family of four children and a spouse in another country. My system can handle Japanese addresses. My system can provide historical price data. If you really have to you can then associate big chunks of data - rather than individual rows - with the domain concepts to which they match. Removing some of the duplication - putting it in custom test queries for instance - will help to keep this less brittle. If you can think of examples of the things your system provides then you can probably cover all the data it provides too. Another way to make the tests still less brittle is to use your persistence layer or the persistence layer of the system you're serving (Hibernate for instance) to translate the data back into domain objects which make more sense for these examples. This will also help you check that the data you're providing is actually valuable rather than simply there. If it's your own system you're providing value to you can do this and retain complete freedom where the format used to store the data is concerned. You can also do things like: My system does not allow me to store children without a parent. My system does not allow addresses with more than 150 characters in a line. and check for database exceptions - also allowing you to verify constraints. What would be a way to test a select query which returns a set of rows each representing a PC? Depends who uses the data. If it's another team / system find out what they find valuable about the data then show an example of that. What's particular about the PCs? Do they look to see what software is installed? Who's using them? What the admin password is? Then you can write a bit of JUnit which runs the query called `shouldBeAbleToFindAdminPasswordsForAllPCs` and shows an example of a couple of things you expect. I see. So the standout attribute is the test and what you're looking for. So if I test to see that password has a value of ""passwords"" it will fail when the table changes so this is a good thing to test frequently. Do yiu normally test the value more than the column name returned? Always. Nobody cares about the column name beyond using it to access the information. If you can express the thing that's valuable you get to change the rest without changing your tests rather than pinning things down and making it brittle. Other developers also get to understand what's valuable and how it's used so they can change it safely. If you never change anything you can manually test everything once. Automated tests and manual test plans are only there to help make things easy to change.  You don't need to test anything about the database except that the database and the code work. So then using a database that you have created in your test environment connect the code that uses the database to it and test that it does whatever it is supposed to. Remember that in a test it is very rare that you need to check how something is done but rather you should be testing that dome thing does in fact get done. For example if you are using hibernate to persist your entities you might have a test called canRoundTripAFooBar that saves a foobar loads it back again and compares each field to make sure the one you loaded and the one that you saved are the same (except for those bit that should not be the same like Ids or something)"
178,A,"Where do you keep your Stubs? One of the JUnit best practices is : same package separate directories. I am wondering what is the equivalent for Mock classes ? Do you keep them in the same package as classes they are supposed to mock but in the test directory ? or elsewhere ? You're talking about stubs I suspect rather than mocks... http://martinfowler.com/articles/mocksArentStubs.html Please edit your question if you are talking about stubs Like many things in programming ""it depends."" Here are some rules of thumb I use: If I have a stub that is only used by one test and is small - create an inner class If I have a stub that is only used by one test and is large - put in same package/folder as test If I have a stub that is used by multiple tests in the same package - put in same package/folder as test If I have a stub that is used in many places in the same application - put in a test.util package If I have a stub that is used across applications put it in a jar. I have instances of all of these in my code."
179,A,"JUnit4 + Eclipse ""An internal error occured during Launching"" I'm trying to run JUnit4 test cases on Eclipse 3.4.2 but it's not even starting for me. I am sure that I properly have junit-4.7.jar in my build path and the test application. Here is a simple example that illustrates my problem package test; import org.junit.Before; import org.junit.Test; public class UTest { @Test public void test() { } @Before public void setUp() throws Exception { } } This compiles fine Then I do ""Run JUnit Test case"" from Eclipse and I get an error dialog with this message ""Launching UTest' has encountered a problem An internal error occurred during: ""Launching UTest"". java.lang.NullPointerException I'm not sure how to figure out what exactly generating this NullPointerException. Some pointers would be appreciated I encountered a similar problem but I am using Python. This is what I did to solve/avoid it: Removed my .project file and the project from Eclipse. Created the project again. Everything was working. The problem seemed to be in the .project file where there were some references to CDT Builder and were not there in the new .project file. .project location is in the project_workspace/project_folder  This worked for me: create another copy of the test class (CopyOfUTest.java) run the copy to make sure it passes go into Run > Run Configurations under JUnit find the run configurations for the original class and the copied class right click and delete the configuration of the original class rename the configuration of the copied class to the original configuration name delete the copied class from the project This didn't work for me; the scope of the problem seems beyond just the single test class.  Have you looked in the Eclipse error log? You can see it by opening the ""Error Log"" view. http://help.eclipse.org/help32/topic/org.eclipse.pde.doc.user/guide/tools/views/error%5Flog.htm Couldn't find that view. Problems view is unrelated this is an eclipse INTERNAL error problems view just shows errors in the project. I said the Errors view ... not the Problems view. Sorry you're right. I don't have an Errors view though :/ +1 Great error log show the detail information of the error. You can fix it according to the error log. *""I don't have an Errors view""* - you will find it in Window > Show View > General > Error Log.  Thanks that solved my problem too. The problem started when i removed an old simulator and created a new one. Fix: Like the OP says remove the workspace make sure to keep the projects inside it :) then import them back to eclipse ""Sound like a lot of work"" ? Took me less than half a minute !!!  Your code works fine for me. Eclipse Version: 3.4.1 Build id: M20080911-1700 I right click on the .java file RunAs JUnit Test  None of the given answers here worked for me so I ended up just installing and using InfiniTest instead. It doesn't have this problem and it also runs the tests automatically so I can focus on my work. You rock! This is the only thing that worked for me using ADT 23+!  I was able to fix this just by deleting the workspace and the Eclipse directory and starting over. I love computers Did you re-import the project or re-build in by manually dragging files? Sound like a lot of work that just avoids the problems doesn't solve it. Yes you have to import to import all the files manually but your eclipse editor setting will be change."
180,A,"Print grails test-app failures to console There have been several questions about this on SO already but none of the answers was satisfactory. I am doing test-driven grails development. However because STS keeps crashing I can't use it to run the unit tests and run them using ""grails test-app"" on the command line instead. This is fine but when a test fails it just won't print the REASON for the failure to the console. Using -echoOut and -echoErr as suggested in other threads does not help this will only print stuff that was logged or printed during the test but not the JUnit or Spock reason of failure. So currently I always have to wait for the HTML to be generated and then find the failure in my browser and this reallly slows me down. I will accept any solution no matter how hacky (i.e. hacking it into the grails source and recompiling it would be fine with me!). You can do something like this... grails test-app -echoOut -echoErr Something.testMain ; cat ./target/test-reports/plain/TEST-integration-integration-SomethingTests.txt In Unix `tail -f ./target/test-reports/plain/TEST-integration-integration-SomethingTests.txt & && grails test-app -echoOut -echoErr Something.testMain` should work. Oh yeah forgot I had been toying around with the plain text files already :( That's only a little bit better because I still have to wait for grails to shut down completely which is a couple of seconds after the actual failure. That's not a bad idea although still cumbersome. This might be solved in 2.0: ""* Unit test scaffolding and enhanced unit test output in the console "" http://grails.1312388.n4.nabble.com/Grails-1-4-is-now-Grails-2-0-td3627931.html"
181,A,"Should JUnit tests be javadocced? I have a number of JUnit test cases that are currently not documented with Javadoc comments. The rest of my code is documented but I'm wondering if it's even worth the effort to document these tests. I don't find any value in javadocing the test cases. I just make the method name descriptive enough to know the purpose of the test. In Ruby I know there are tools to create a document from the name of the tests but I haven't seen one of these in Java.  Is it heresy to suggest that code comments are an anti-pattern? I agree with the above answers ideally your code would be descriptive enough to rely on without comments. This is especially true if you are in an (enterprise) environment where people tend to update the code without updating comments so comments become misleading.  I don't see why you should treat test cases any differently than your production code with respect to comments. but as Kaleb mentioned above if the names are verbose enough wouldn't that just be redundant? Properly named identifiers is no substitute for comments.  Maybe you could get someone who is not entirely familiar with your code to give you some quick feedback as to whether your tests are easily understood as they are. At the company I work for we try to give our tests descriptive names and document complexity but it's often hard to get this ""right"" with the first draft because things that are obvious to the developer aren't always obvious to others. Tests are treated like code and are submitted as part of a peer review process so our (small) team can comment as to whether a test is easily understood or not. If a test is a little confusing we can update the name or documentation accordingly and we come away with a better gauge of what works and what doesn't.  If the purpose of the test is obvious I don't bother documenting it. If it's non-obvious because it deals with some obscure situation - or if I want to refer to a specific bug for example - in that case I'll add documentation. I don't document exceptions throw etc though - just a quick summary of the method. This happens relatively rarely. I'm more likely to add documentation for helper methods used within multiple tests. I agree with you. I hate to see unit tests that are no clear what they are testing. Code changes over time and the original reason can be less than obvious. I'd much rather see an explanatory string in each assertFoo statement.  Whether javadoc or not I think that unit tests should definitely be documented. In cases where no formal specification exists the unit tests are what comes closest to defining the expected behaviour of the code. By documenting the test cases you will make it very clear to the reader what the test is testing and why it's testing it.  When thinking about the tests as a documentation it does not make much sense to ""document the documentation"". The name of each test should already in itself describe that what is the purpose of the tests - what is the behaviour being specified by that test. Use long descriptive names for the tests and keep the test code as readable as possible. For example have a look at the test cases in this project. Does any of them do something which is not blatantly obvious by looking at the name of the tests and the test code? It's only in some rare cases when the test code is obscure that comments are needed in tests. For example if you are testing multi-threaded code and the test code does weird things in order to make sure that the test code runs in the correct order. But even in those cases a comment is an apology for not writing cleaner test code.  hell yea. even if its just... create order edit order save load & check it. if its a really simple test then maybe not. I find that as code changes sometimes the reason for the test is not a obvious as it once was.  I think it's valuable to javadoc the conditions under which the tests pass."
182,A,JUnit setUp gets invoked TWO times with one test and messing up Powermock expectNew Very strange behaviour in my test.  public class MyTestclass { @Before void setUp(){ //do some setup but hu i get called twice //here i do some try catch thing to get the stacktrace... } void testOnlyOneTest(){ //make the testing i get called only once } @After void tearDown(){ //do some destroy things... i get called twice too } } StackTrace:  1) MyTestClassTest.setUp() line: 85 NativeMethodAccessorImpl.invoke0(Method Object Object[]) line: not available [native method] NativeMethodAccessorImpl.invoke(Object Object[]) line: 39 DelegatingMethodAccessorImpl.invoke(Object Object[]) line: 25 Method.invoke(Object Object...) line: 597 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner(MethodRoadie).runBefores() line: 129 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner(MethodRoadie).runBeforesThenTestThenAfters(Runnable) line: 93 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(Method Object Runnable) line: 294 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(Runnable) line: 282 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner(MethodRoadie).runTest() line: 84 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner(MethodRoadie).run() line: 49 PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(Method RunNotifier) line: 207 PowerMockJUnit44RunnerDelegateImpl.runMethods(RunNotifier) line: 146 PowerMockJUnit44RunnerDelegateImpl$1.run() line: 120 ClassRoadie.runUnprotected() line: 34 ClassRoadie.runProtected() line: 44 PowerMockJUnit44RunnerDelegateImpl.run(RunNotifier) line: 118 JUnit4TestSuiteChunkerImpl.run(RunNotifier) line: 102 PowerMockRunner(AbstractCommonPowerMockRunner).run(RunNotifier) line: 53 JUnit4TestClassReference(JUnit4TestReference).run(TestExecution) line: 46 TestExecution.run(ITestReference[]) line: 38 RemoteTestRunner.runTests(String[] String TestExecution) line: 467 RemoteTestRunner.runTests(TestExecution) line: 683 RemoteTestRunner.run() line: 390 RemoteTestRunner.main(String[]) line: 197 2) MyTestClassTest.setUp() line: 85 NativeMethodAccessorImpl.invoke0(Method Object Object[]) line: not available [native method] NativeMethodAccessorImpl.invoke(Object Object[]) line: 39 DelegatingMethodAccessorImpl.invoke(Object Object[]) line: 25 Method.invoke(Object Object...) line: 597 WhiteboxImpl.performMethodInvocation(Object Method Object...) line: 2014 WhiteboxImpl.doInvokeMethod(Object Class<?> String Object...) line: 885 WhiteboxImpl.invokeMethod(Object String Object...) line: 713 Whitebox.invokeMethod(Object String Object...) line: 401 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod() line: 305 MethodRoadie$2.run() line: 86 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner(MethodRoadie).runBeforesThenTestThenAfters(Runnable) line: 94 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(Method Object Runnable) line: 294 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(Runnable) line: 282 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner(MethodRoadie).runTest() line: 84 PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner(MethodRoadie).run() line: 49 PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(Method RunNotifier) line: 207 PowerMockJUnit44RunnerDelegateImpl.runMethods(RunNotifier) line: 146 PowerMockJUnit44RunnerDelegateImpl$1.run() line: 120 ClassRoadie.runUnprotected() line: 34 ClassRoadie.runProtected() line: 44 PowerMockJUnit44RunnerDelegateImpl.run(RunNotifier) line: 118 JUnit4TestSuiteChunkerImpl.run(RunNotifier) line: 102 PowerMockRunner(AbstractCommonPowerMockRunner).run(RunNotifier) line: 53 JUnit4TestClassReference(JUnit4TestReference).run(TestExecution) line: 46 TestExecution.run(ITestReference[]) line: 38 RemoteTestRunner.runTests(String[] String TestExecution) line: 467 RemoteTestRunner.runTests(TestExecution) line: 683 RemoteTestRunner.run() line: 390 RemoteTestRunner.main(String[]) line: 197 Any ideas why my setUp gets called twice? I do some mocking in the setUp and after a verify i get the expect 2 times called 1 times thing so my tests fail. i am using powermock 1.4.8 junit 4.4 and the easymock framework Try to rename the setup and teardown methods and see if it makes a difference. @Peter ty but i tried that already. Does your class extend TestCase? It doesn't appear to in the example but I'm just trying to think what could be causing this. If it did then it would ignore the annotations and consider every method that starts with the word test as a test case causing you to have possibly more tests than you think. damn you are good ;) yes you are right my testclass extends from my test super class in my project and that one extends from TestCase... and that beast executed it test**** methodes and the powermock executed @Test annotated tests.... damn you are good! ty :D  I think you want to use @BeforeClass instead of @Before. @Before gets run before each method annotated with @Test whereas @BeforeClass only runs once before all @Test methods. That may work but I think the concern is the fact that there is only one method annotated with @Test. Now that you mention that how are any tests getting run since there are no `@Test` annotated methods? A quick test shows that JUnit4 only runs methods with that annotation so maybe PowerMock is doing it? I wonder if that could be the root of the problem here. @Pace yes that is exactly my problem i think powermock is doing sth. but i can't figure out what if you look into the stacktraces most of them are the same but in the whiteboximpl is makeing me crazy i can't find the problem there...
183,A,"JUnit two variables iterating respective value sets create test functions Checking function takes two arguments I want to have a testcase for each element of the cartesian product of the respective domains but without manually writing all the test cases like in void check(Integer a Integer b) { ... } @Test public void test_1_2() { check(1 2); } ... In python I would go with class Tests(unittest.TestCase): def check(self i j): self.assertNotEquals(0 i-j) for i in xrange(1 4): for j in xrange(2 6): def ch(i j): return lambda self: self.check(i j) setattr(Tests ""test_%r_%r"" % (i j) ch(i j)) How could I do this with Java and JUnit? I think your looking for Parameterized Tests in JUnit 4. http://stackoverflow.com/questions/358802/junit-test-with-dynamic-number-of-tests  I would look at JUnit 4's parameterised tests and generate the array of test data dynamically. Now for the question that is always asked with JUnit 4 parametrised tests - how do you set the test case's names?  with theories you can write something like: @RunWith(Theories.class) public class MyTheoryOnUniverseTest { @DataPoint public static int a1 = 1; @DataPoint public static int a2 = 2; @DataPoint public static int a3 = 3; @DataPoint public static int a4 = 4; @DataPoint public static int b2 = 2; @DataPoint public static int b3 = 3; @DataPoint public static int b4 = 4; @DataPoint public static int b5 = 5; @DataPoint public static int b6 = 6; @Theory public void checkWith(int a int b) { System.out.format(""%d %d\n"" a b); } } (tested with JUnit 4.5) EDIT Theories also produces nice error messages: Testcase: checkWith(MyTheoryOnUniverseTest): Caused an ERROR checkWith(a1 b2) // <--- test failed like your test_1_2 so you don't need to name your test in order to easly identify failures. EDIT2 alternatively you can use DataPoint**s** annotation: @DataPoints public static int as[] = { 1 2 3 4} ; @DataPoints public static int bs[] = { 2 3 4 5 6}; @Theory public void checkWith(int a int b) { System.out.format(""%d %d\n"" a b); }"
184,A,"Comparing entities while unit testing with Hibernate I am running JUnit tests using in memory HSQLDB. Let's say I have a method that inserts some values to the DB and I am checking if the method inserted the values correctly. Note that order of the insertion is not important. @Test public void should_insert_correctly() { MyEntity[] expectedEntities = new MyEntity[2]; // init expected entities Inserter out = new Inserter(session); // out: object under test out.insert(); List list = session.createCriteria(MyEntity.class).list(); assertTrue(list.contains(expectedEntities[0])); assertTrue(list.contains(expectedEntities[1])); } The problem is I cannot compare expected entities to actual ones because the expected's id and the actual's id are different. Since setId() of MyEntity is private (to prevent setting id explicitly) I cannot set all of the entities' id to 0 and compare like that. How can I compare two result set regardless of their ids? Could you please provide some more information about the values your MyEntity class contains? Is there any member that may give you an primary key? A stateful entity should not override equals -- that is entities should be compared for equality by reference identity -- so List.contains will not work as you want. What I do is use reflection to compare the fields of the original and reloaded entities. The function that walks over the fields of the objects ignores transient fields and those annotated as @Transient. I don't find I need to ignore the id. When the object is first flushed to the database Hibernate allocates it an id. When it is reloaded the object will have the same id. The flaw in your test is that you have not set transaction boundaries. You need to save the objects in one transaction. When you commit that transaction Hibernate will flush the objects to the database and allocate their ids. Then in another transaction load the entities back from the database. You will get another set of objects that should have the same ids and persistent (i.e. non-transient) state. Why should stateful entities do not override equals? Don't know it. Note that expectedEntities' and the actual insereted entities' ids are different. Because I am not saving expectedEntities and I don't need to. Then ignore the IDs when reflecting on the persistent fields of the objects.  I would try to implement Object.equals(Object) method in your MyEntity class. List.contains(Object) uses Object.equals(Object) (Source: Java 6 API) to determine if an Object is in this list. The method session.createCriteria(MyEntity.class).list(); returns a list of new instances with the values you inserted (hopefully). So you need to compare the values. This is easily done via the implementation of Object.equals(Object). Clarification edit: You could ignore the ids in your equals method so that the comparison only cares about ""real values"". YAE (Yet Another Edit): I recommend reading this article about the equals() method: Angelika Langer: Secrets Of Equal. It explains all background information very well. I did not downvote I don't find ignoring ids in equals() a good solution. What happens if I need to compare ids too in the future You can just change the equals() method and integrate comparison of the ids. Then the tests which you wrote today will fail! By the way: Why the downvote? I didn't say that I would like to ask the downvoter to say why he didn't like my answer.  I found this more practical. Instead of fetching all results at once I am fetching results according to the criterias and asserting they are not null. public void should_insert_correctly() { Inserter out = new Inserter(session); // out: object under test out.insert(); Criteria criteria; criteria = getCriteria(session 0); assertNotNull(criteria.uniqueResult()); criteria = getCriteria(session 1); assertNotNull(criteria.uniqueResult()); } private Criteria getCriteria(Session session int i) { Criteria criteria = session.createCriteria(MyEntity.class); criteria.add(Restrictions.eq(""x"" expectedX[i])); criteria.add(Restrictions.eq(""y"" expectedY[i])); return criteria; }"
185,A,"How to create TestContext for Spring Test? Newcomer to Spring here so pardon me if this is a stupid question. I have a relatively small Java library that implements a few dozen beans (no database or GUI). I have created a Spring Bean configuration file that other Java projects use to inject my beans into their stuff. I am now for the first time trying to use Spring Test to inject some of these beans into my junit test classes (rather than simply instantiating them). I am doing this partly to learn Spring Test and partly to force the tests to use the same bean configuration file I provide for others. In the Spring documentation is says I need to create an application context using the ""TestContext"" class that comes with Spring. I believe this should be done in a spring XML file that I reference via the @ContextConfiguration annotation on my test class. @ContextConfiguration({""/test-applicationContext.xml""}) However there is no hint as to what to put in the file! When I go to run my tests from within Eclipse it errors out saying ""failed to load Application Context""....of course. Update: Here is test-applicationContext.xml: <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:context=""http://www.springframework.org/schema/context"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd""> <description>Holds application context for testing of the domain module.</description> <!-- Imports the uuid generator bean definitions --> <import resource=""resources/domain-uuid.xml""/> </beans> My project directory is like this: domain/ src/ main/ java/ resources/ test/ java/ resources/ (location of test-applicationContext.xml) Just for fun I also tried to build from the mvn command line via ""mvn clean test"" and I got the following errors which may be my real problem: package org.springframework.test.context does not exist package org.springframework.test.context.junit4 does not exist cannot find symbol symbol: class ContextConfiguration @ContextConfiguration({""/resources/test-applicationContext.xml""}) cannot find symbol symbol: class SpringJUnit4ClassRunner @RunWith(SpringJUnit4ClassRunner.class) when you say the path is ""/test-applicationContext.xml"" it means at the root of the class path. where did u put the file? I put it under ""src/test/resources"" but other than the XML schema declarations and a single import -- its empty!  What to put in the app context file. The way the TestContext Framework works is that it allows you to reuse app wiring in the context of your integration tests. So for the most part there isn't anything special to tests you'd put inside your app context config files. If your controller has a service bean dependency in your app then it will have that in your integration test too. If your DAO has a SessionFactory in your app then same for your integration test. That way you don't have to wire all that stuff up all over again when you write integration tests. Very cool. I said for the most part above because there's at least one exception that comes to mind. Normally your app will use JNDI to locate a DataSource but in an integration test (at least an out-of-container integration test) you won't normally have a JNDI environment available. So you should typically isolate the DataSource bean creation to a separate file and use a JNDI version for your live app and a non-JNDI version (e.g. just create a straight BasicDataSource say) for your integration test. Here's an example of the former: <jee:jndi-lookup id=""dataSource"" jndi-name=""jdbc/myStoreDS"" resource-ref=""true""/> and here's an example of the latter: <bean id=""dataSource"" class=""org.apache.commons.dbcp.BasicDataSource"" destroy-method=""close"" p:driverClassName=""${dataSource.driverClassName}"" p:url=""${dataSource.url}"" p:username=""${dataSource.username}"" p:password=""${dataSource.password}"" /> These would go in separate files. The first might go in beans-datasource.xml for normal app use and the second in beans-datasource-it.xml for integration tests. The configuration that's common to normal app use and integration tests (i.e. the vast majority of your bean config in most cases) should be in a common config file or files. Also Spring 3 introduces a new jdbc namespace that allows you to create an embedded database like an HSQLDB database or a Derby database etc. It looks like this: <jdbc:embedded-database id=""dataSource""> <jdbc:script location=""classpath:hsql/schema.sql"" /> <jdbc:script location=""classpath:hsql/test-data.sql"" /> </jdbc:embedded-database> That would replace the BasicDataSource config described above if you want to use this. Why the error is happening. The error you are seeing is happening because your @ContextConfiguration value is implicitly indicating that the app context file should be on the classpath. IMPORTANT: Remove the /resources piece. That is Maven innards; when it builds your JAR or WAR it copies the contents of the resources directory into your classpath not resources itself. That should help. EDIT: To address the ""no symbol found"" errors you will need to add your test dependencies to your Maven POM. This will be JUnit and the Spring Test module as well both with <scope>test</scope>. In addition if you are using a mock framework like Mockito you will need to add that dependency (with test scope) to your POM as well. Try that and please report back on what happens. Sorry -- I'm not using JNDI or DAOs. I just don't know where to create the textContext! You say ""app context file"" -- what is that? The ""app context file"" is just your Spring configuration XML file. The one that has a top-level element. You can put it directly under WEB-INF or you can put it on the classpath like under src/test/resource like you said in one of your other comments. It might help for you to add your directory structure (the relevant parts) to your original post and also show what your test-applicationContext.xml file contains. OK -- I think I get it. I don't have to specify TestContext anywhere. Spring just ""knows"" somehow (how???). The problem is that Spring can't find my configuration file. Correct Spring will take care of the TestContext part. The developer shouldn't normally have to mess around with the TestContext at all. It is just what the test framework uses to manage contextual information about the test case (as implemented by your test class) and the test currently being run (as implemented by your test method). So if you have the @DirtiesContext annotation on a test method for instance then the framework would mark the application context (that is your bean container) as being dirty so it can be reloaded after running the test. I just added an edit to this comment; see above.  To find it directly under src/test/resources change it to: @ContextConfiguration({""classpath:/test-applicationContext.xml""}) When you're not specifying anything then Spring search in the same package as the test class."
186,A,"Cannot run a JUnit test case containing threads from Eclipse I am running JUnit test case from Eclipse 3.4.1 . This test case creates a class which starts a thread to do some stuff. When the test method ends it seems that Eclipse is forcibly shutting down the thread. If I run the same test from the command line then the thread runs properly. Somehow I do not remember running into such problems with Eclipse before. Is this something that was always present in Eclipse or did they add it in 3.4.x ? Here is an example: When I run this test from Eclipse I get a few printts of the cnt (till about 1800) and then the test case is terminated utomatically. However if I run the main method which starts JUnit's TestRunner then the thread counts indefinetely. import junit.framework.TestCase; import junit.textui.TestRunner; /** * This class shows that Eclipses JUnit test case runner will forcibly * terminate all running threads * * @author pshah * */ public class ThreadTest extends TestCase { static Runnable run = new Runnable() { public void run() { int cnt = 0; while(true) System.out.println(cnt++); } }; public void testThread() { Thread t = new Thread(run); t.start(); } public static void main(String args[]) { TestRunner runner = new TestRunner(); runner.run(ThreadTest.class); } } Are you getting the responce from the thread.if not please let me know.because Junit does not have the capability to get the responce from the application threads. Are you correctly join()-ing your thread within the test? Could you please provide some example code? Look at: http://today.java.net/pub/a/today/2003/08/06/multithreadedTests.html and http://groboutils.sourceforge.net/testing-junit/using_mtt.html I believe this modification will yield the desired result for unit testing various thread scenarios. (sorry if the formatting is wonky) public class ThreadTest { static Runnable run = new Runnable() { public void run() { int cnt = 0; while (true) System.out.println(cnt++); } }; @Test public void threadRun() { Thread t = new Thread(run); t.start(); //Run the thread t for 30 seconds total. //Assert the thread's state is RUNNABLE once per second for(int i=0;i<30;i++){ assertEquals(""RUNNABLE"" t.getState().toString()); try { Thread.sleep(1000);//1 second sleep } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(""Done with my thread unit test.""); } }  This works but you have to name your thread or find another way to refer to it. protected boolean monitorSecondaryThread(String threadName StringBuilder errorMessage boolean ignoreFailSafe) { int NUM_THREADS_BESIDES_SECONDARY_THREAD = 2; int MAX_WAIT_TIME = 10000; MyUncaughtExceptionHandler meh = new MyUncaughtExceptionHandler(); Set<Thread> threadSet = Thread.getAllStackTraces().keySet(); for (Thread t : threadSet) { t.setUncaughtExceptionHandler(meh); } Date start = Calendar.getInstance().getTime(); boolean stillAlive = true; while (stillAlive) { for (Thread t : threadSet) { if (t.getName().equalsIgnoreCase(threadName) && !t.isAlive()) { stillAlive = false; } } Date end = Calendar.getInstance().getTime(); if (!ignoreFailSafe && (end.getTime() - start.getTime() > MAX_WAIT_TIME || Thread.activeCount() <= NUM_THREADS_BESIDES_SECONDARY_THREAD)) { System.out.println(""Oops flawed thread monitor.""); stillAlive = false; } } if (meh.errorCount > 0) { System.out.println(meh.error); errorMessage.append(meh.error); return false; } return true; } private class MyUncaughtExceptionHandler implements UncaughtExceptionHandler { public int errorCount = 0; public String error = """"; @Override public void uncaughtException(Thread t Throwable e) { ByteArrayOutputStream bs = new ByteArrayOutputStream(); PrintStream ps = new PrintStream(bs); e.printStackTrace(ps); error = bs.toString(); errorCount++; } }  I adapted your code to JUnit NG and it's the same result: The thread is killed. public class ThreadTest { static Runnable run = new Runnable() { public void run() { int cnt = 0; while (true) System.out.println(cnt++); } }; @Test public void threadRun() { Thread t = new Thread(run); t.start(); assertEquals(""RUNNABLE"" t.getState().toString()); } } If I use the JUnit jar (4.3.1 in my case) from the Eclipe plugin folder to execute the tests via the command line it has the same behavior like executing it in Eclipse (It's logical :) ). I tested JUnit 4.6 (just downloaded) in the commandline and it also stops after a short time! It's exactly the same behavior like in Eclipse I found out that it is killed if the last instruction is done. It's logical if you consider how JUnit works: For each test a new object is created. If the test is over it's killed. Everything belonging to this test is killed. That means that every thread must be stopped. JUnit deals correctly with this situation. Unit test must be isolated and easy to execute. So it has to end all threads if the end of the test is reached. You may wait till the test is finished and then execute your assertXXX instruction. This would be the right way to test threads. But be carefull: It may kill your execution times!"
187,A,"I am new to automated testing in Java. Which tool should I prefer? JUnit or TestNG? I have read some comparisons of JUnit and TestNG and it looks like TestNG has more configuration options. On the other hand JUnit is more supported by IDEs building tools has more plugins. I have no experience in writing unit tests. Which tool should I prefer? P.S. I think my question is more like: Should I give TestNG a try or just stick with JUnit as everybody else? P.S. We develop web applications so I think the choice should also consider that we will use Selenium later for functional testing. `I have no experience in writing unit tests` Then just pick one and start writing tests IMO. The question is which one? possible duplicate of [Which UnitTest framework to learn for Java now?](http://stackoverflow.com/questions/3411768/which-unittest-framework-to-learn-for-java-now) JUnit and TestNG are both quite similar and once you know one of them it is easy to pick up the other. The main reason I use TestNG is for test groups and test dependencies. You can assign tests to different groups and then easily run all the tests in a group or exclude certain tests from a group. For example if I want to isolate a couple of tests I can simply add them to a test group ""fahd"" and then run this group only. Test dependencies allow you to skip tests when a dependent test fails. You may think these features are not very important but once you use them you'll wonder how you ever lived without them. JUnit 4.8 added `@Category` which allows you to assign tests to a category and run all of the tests in a category. See http://stackoverflow.com/questions/2176570/how-to-run-all-tests-belonging-to-a-certain-category-in-junit-4/2176791#2176791 JUnit does not have a concept of dependent tests.  I always vote for 'as everybody else' because: this experience in most cases is more valuable because there is a lot of info and for jUnit specifically it is ported to many languages so this experience will be 'cross-platform'.  I work as JavaEE devel. We massively use JUnit for testing our business logic modules.It's very powerful flexible and we use his html report in official release docs as a proof of quality.  If this is your first time I'd recommend JUnit. It was the first most popular. It's well documented has great tool support and is the metaphor that is translated-- at least initially-- to all the different languages (see xUnit implementations). It should work fine for most any project and it's a good tool to know. It will be a good baseline for you as a programmer. There are other features of the ""alternatives""-- that's why there are alternatives-- but often it's a matter of style more than anything else. TestNG may have a few different features but JUnit has also evolved with features like annotations alternative matchers etc. Yes JUnit will work fine with Selenium when the time comes. There is still a lot of legacy examples and tutorials for JUnit 3 around. It would confuse beginner tremendously so make sure you are using JUnit 4 resources.  Combination of TestNg and Unitils is unbeatable..It would suffice all your requirements.. Not sure I agree. Doing very similar things in two different ways strikes me as a bad idea. Consistency is important.  TestNG was written to overcome some perceived limitations of JUnit 3. Check out Cedric's blog post or other articles on the TestNG site to see his thinking. Note that some of the limitations of JUnit 3 were by design and TestNG deliberately allows you to do things that the designers of JUnit expressly prevented. The biggest advantage TestNG had over JUnit 3 was that it allowed you to use annotations to define your tests rather than forcing you to extend the JUnit base classes. JUnit 4 can also use annotations in the same way now narrowing the gap. The biggest remaining difference between TestNG and JUnit is that TestNG allows your tests to be dependent on one another and allows you to express those dependencies through the test annotations. JUnit instead forces all your tests to be independent of one another. There are other differences too of course – TestNG defaults to assertEquals(actual expected message) whereas JUnit is assertEquals(message expected actual) – but this is the biggest. I'd say: pick one and try it out. Write some tests. Both work fine with Selenium. Both work fine with ant. Both work fine with CruiseControl or Hudson. Learning to write good unit tests is far more important than learning a particular set of commands (they are pretty similar anyway). TestNG has more power and fewer restrictions IMO but that gives you more opportunities to get things wrong too of course. really good detailed answer +1.  Junit is most popular it supports plugins so better to use it"
188,A,"How do you unit test Scala in Eclipse? I am learning Scala and would like to set up integrated unit testing in Eclipse. As far as I can tell from googling ScalaTest is the way to go possibly in combination with JUnit. What are your experiences with unit testing Scala in Eclipse? Should I use the JUnit runner or something else? @Matt Thanks this works - you should add it as an answer actually. I haven't tried it but there's some documentation here: https://www.assembla.com/wiki/show/scala-ide/Using_Unit_Testing_Frameworks I couldn't get unit tests running from Eclipse either (admittedly I didn't try too hard) but if you're willing to take a look at IntelliJ ScalaTest works from within the IDE with no problems. It turned out that the JUnit runner worked for me out of the box in Eclipse. I just had to import the right package and write @test before the test methods just like in Java. Haven't tried ScalaTest yet though. I highly recommend using IntelliJ when I started out unit testing with scala (http://www.persistentpanda.com/2011/03/unit-testing-with-scala.html) the only thing Eclipse gave me was a headache. Especially with Maven projects.  I've spent the past few days trying to find a combination of unit tests that work with scala for my project and this is what I've found. I am using the release candidates for Scala 2.8 because it fixes a number of bugs that were blocking me in 2.7.2. I initially tried to get Scalatest to work with 2.8 but was unable to find a stable solution in the future that may be a better way to go but currently there appears to be too much in flux around the 2.8 release. The configuration I have working is to use JUnit4 annotations in my scala test code like so:  import org.junit._ import Assert._ class TestSuite{ @Test def testSimple(){ asserEquals(""Equal""11) } } Then I am using a java JUnit test suite to run all my tests together like so:  import junit.framework.Test; import junit.framework.TestSuite; import org.junit.runner.RunWith; import org.junit.runners.Suite; @RunWith(Suite.class) @Suite.SuiteClasses( { TestSuite.class }) public class AllTests { public static Test suite() { TestSuite suite = new TestSuite(""Test Suite""); return suite; } } The only additional trick is to add the output directory as a source directory in Eclipse so that the JUnit runner can find your class files. This is a bit of a hack but it seems to work. Project->Properties->Java Build Path->Source->Add Folder and then click on classes (or bin wherever you are compiling your class files to). This runs fine in Eclipse by right clicking on AllTests.java and selecting RunAs->Junit I found out that I could just right-click on a scala file containing annotated test code and pick RunAs->Junit provided that I had added the library reference for JUnit. Yeah that seems to work for me too though for my project running a full suite of tests is more useful. It did not work when I tried to create a .scala file with the JUnit4 annotations to create a suite of my tests.  I was unable to run the ScalaTest specs with JUnit runner inside eclipse. I think this is because of the absence of the @Test annotated methods. However if your project has Maven support you can run your specs from command line using mvn test. For Surefire plugin to detect your specs use the following configuration in your pom file. <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.5</version> <configuration> <includes> <include>**/*Spec.class</include> </includes> </configuration> </plugin> Check out these example for reference. This has been improved - see @Eric's answer for a pointer to the website: http://www.assembla.com/wiki/show/scala-ide/Using_Unit_Testing_Frameworks  There is a wiki page on the ScalaIDE website on how to run Scala unit tests in Eclipse. If you have specific issues with running specs unit tests I encourage you to post messages on the specs-users mailing list. Eric. If mailing lists were sufficient for everybody then the world would have no need of Stack Overflow I agree not a very helpful answer by myself :-)  Just to keep the answers up to date: ScalaTest now comes with an eclipse plugin which should handle running tests from Eclipse out-of-the-box."
189,A,"fail hudson build on single unit test failure Is there a way to cause hudson to report a build as failed rather than unstable if only a single unit test fails? thanks. It's actually not a good idea to fail the build if tests failed when using hudson. Problem is hudson will not report the state of test pass/fail if the build fails. If the build fails hudson deems it to not have completed properly and thus does not act on the result.  There are two properties to the junit task errorProperty=""maven.test.error"" failureProperty=""maven.test.failure"" After the junit tag you should be able to do something like this <fail message=""Test failed!!!"" if=""maven.test.error"" /> <fail message=""Test failed!!!"" if=""maven.test.failure"" /> But don't nail me on this Nice idea if it works!  If you're using Ant to drive the build you can configure the JUnit task to halt on failure. Is that what you mean? I'm actually using maven... But I don't want to stop the build on a test failure. Hudson just reports the build as unstable but I want it to report as failed if 1 or more unit tests fail.  Look through your job configuration I believe there is a property (check box) that says fail on test failure or something of the sort. We use this on some of our projects at my work. Otherwise if you want to use the Ant method as suggested maven can run ant tasks...  Hudson actually enables the ignoring of test failures. It just needs to be put as a property in hudson. -Dmaven.test.failure.ignore=false Yep that's actually what I did. You can add it as part of MAVEN_OPTS too :)"
190,A,"Advice on unit testing class to test has dependency on Java serial port I am working on a large legacy java application. It already has an extensive existing framework in place for handling device drivers. I need to create a new device driver for a device that connects to a serial port via JavaComm. The existing drivers simply create the new serial port inside their configure() methods then create new input and output streams from the serial port object then use those input and output streams for the device comms but with no unit tests. I want my new class to be unit testable however but not sure how I can do that if it's going to fit in with this existing framework that will expect the serial port input and output streams to be set up in the configure() method. Any ideas?   public void configure() { super.configure(); if (isEmulated()) { return; } if (isFaulty()) { if (isOutOfService()) { ZBopNotificationManager.getInstance().add( new SystemNotice(SeverityCode.LEVEL3 getName() getErrorMessage())); } return; } // ZP: String portName = getSerialPort(); // String portName = ""COM1""; try { CommPortIdentifier id = CommPortIdentifier.getPortIdentifier(portName); Trace.log(this Trace.D ""Port name = "" + id.getName()); port = (SerialPort) id.open(getName() 1000); } catch (NoSuchPortException nspe) { report(SeverityCode.LEVEL3 getName() ""Bar Code Scanner is not connected to "" + portName + "" port or the port does not exist.""); return; } catch (PortInUseException piue) { report(SeverityCode.LEVEL3 getName() portName + "" port is already in-use by some other device. Reason: "" + piue.getMessage()); return; } try { port.setSerialPortParams(9600 SerialPort.DATABITS_8 SerialPort.STOPBITS_1 SerialPort.PARITY_NONE); } catch (UnsupportedCommOperationException e) { // this should not happen port.close(); report(SeverityCode.LEVEL2 getName() portName + "" port configuration failed: "" + e); return; } try { in = port.getInputStream(); out = port.getOutputStream(); } catch (IOException ioe) { port.close(); report(SeverityCode.LEVEL3 getName() portName + "" port configuration failed: "" + ioe.getMessage()); return; } meout(30); // ... other init code omitted }  By the looks of things the javax.comm API doesn't make life easy for unit tests - it's big on classes and light on interfaces. My suggest would be to create interfaces and adaptor classes for each javax.comm class that you need to use in your driver. Your driver code would then talk to those interfaces rather than directly to javax.comm. You probably only need a subset of the API anyway and defining these interfaces should help you clarify your design. This will allow you to use mock implementations of those interfaces in your unit tests (e.g. JMock Mockito etc). Your unit test can inject these mocks into the driver object. When used for real the driver's configure() method can instantiate the adaptor classes rather than the mocks. I almost understand. configure() still needs to create the adapter that creates the real object to use and my unit test needs to call configure() to simulate the bring up of the device. So I'm sill then back at my original problem of having the dependancy on the real serial port? That part I do not understand how to unit test configure() and have it still work with the real port in production code. @fred: Your unit test would *not* call `configure()` it would inject the mocks directly into the object (e.g. using setter methods). Only the runtime framework would call `configure()` and that would create the adaptor objects not mocks.  If i understood you correctly you want to test devicedriver and not a module that uses the device driver. Is it ok to have an integrationtest instead of a unittest? If you connect the serial port-s rxdata with the txdatapin and the rts with the cts pin then the integration test can check that every thing send into outputstream should be received by the inputstream. In my case I need a unit test. Integration test is a good suggestion however."
191,A,"JUnit4 in Eclipse I'm trying to run some JUnit test units in eclipse 3.5 but with no luck at all. JUnit3 works fine. When I create the JUnit4 Test unit eclipse offers to add the JUnit library to the class-path. I accept but when i check to see if it was added in the project's properties panel I can see JUnit4 was added but no JARS where included. If I choose edit the combo says ""JUnit 4"" and just below ""Current location: Not Found"". When I launch a JUnit4 test I get an error saying and internal error occurred NullPointerException. I've read for about two days now and all references say eclipse INCLUDES JUnit4 but somehow there seems to be something i'm missing. I've tried re-creating my projects and creating test in brand new ones with no luck. package test; import static org.junit.Assert.*; import org.junit.Test; public class AuthServiceTest { @Test public final void testValidateCredentials() { fail(""Not yet implemented""); // TODO } } [Edit] I've added junit-4.8.1.jar to the project's classpath and eclipe's classpath but still the same problem. [Edit2] I also added junit-dep-4.8.1.jar since i'm not sure if these dependencies are necessary but no change. See also http://www.vogella.de/articles/JUnit/article.html In my Eclipse installation JUnit 4 is provided it's in plugins\org.junit4_4.3.1\junit.jar If you can't find it then I guess that you may need to download it. You can associate your JUnit with the Eclipse JUnit settings in Windows->Preferences->Java->Build Path->User Libraries Select JUnit there and you can add and edit JARs.  Right click on the project name . Built Path--> Add Libraries Select the appropriate library from the list (JUnit 4 in this instance) .  It appears that the Eclipse 3.5 from Fedora's repository doesn't include JUnit and installing the appropriate package didn't include it either. Another Debian PC presented the same issue. A clean download from eclipse.org solved the problem. I will report this bug at some point :)"
192,A,"How to pre-define the running sequences of junit test cases？ I have a test class in java and there are several methods annotated by @Test in it somehow i want to Junit run method A before method B when i run the whole tests. Is it possible or necessary? The problem I have with this is reporting. If you WANT/NEED to see if each test method fails or passes then you're SCREWED. I understand that you don't want one test to build upon previous tests But regardless of that there may be situations that you need it to do this (or you'll increase the complexity of the test by an order of magnitude). Should the flow of tests in the code be up to the developer of the tests or the developer of the framework ? Show JUnit test code to 10 java developers and I'll be willing to bet most will assume that the tests (regardless of anything external) will be run in the order they appear in the test class. Shouldn't THAT be the default behaviour of JUnit ? (Give me the option of telling it the order instead of JUnit figuring it out"" on its own.)  This sort of dependency on test methods is bad design and should be avoided. If there is initialization code in one test method that needs to be done for the next it should be factored out into a setUp method. Great advice thanks.  Tests should have independent order but some times we have not what we want. If you have a large legacy project with thousands of tests and they depends on their execution order you will have many problems when for example you will try to migrate on java 7 because it will shuffle all tests. You can read more about this problem here: junit test ordering and java 7  use the following to setup thing before and after tests @Before public void setUp() throws MWException { } @After public void tearDown() { }  Check out this Stack Overflow discussion for some best practices.. Here are some other best practices on JUnit JUnit Best Practices Part 2 -- Keep 'em Separated JUnit Best Practices Part 4 -- Don't Be Too Assertive Hope that helps.  If it's only two methods then you'd need to wrap it in a single unit test that truly isn't order-dependent. @Test public void testInOrder() throws Exception { testA(); testB(); } I already trid this but not satisfied i think there might have other way to do this.Thanks anyway."
193,A,"Junit Test Not Deploying from Ant Script I have an ant script that I am trying to run my junit test from. It runs all but the junit portion of the ant with no errors. Anyone know why it's not hitting this portion of my script? <?xml version=""1.0""?> <property file=""build.properties"" /> <property name=""test.class.name"" value=""edu.psu.ist.probability.TestProbability"" /> <path id=""test.classpath""> <pathelement location=""${classes}"" /> <pathelement location=""/lib/junit.jar"" /> <fileset dir=""${lib}""> <include name=""**/*.jar""/> </fileset> </path> <taskdef resource=""checkstyletask.properties"" classpath=""${checkstyle}"" /> <taskdef name=""findbugs"" classname=""edu.umd.cs.findbugs.anttask.FindBugsTask""> <classpath> <pathelement path=""${findbugs.jar}"" /> </classpath> </taskdef> <!-- Encapsulates the process of starting up program. --> <target name=""run"" depends=""build""> <echo>-----------RUN-----------</echo> <java classname=""${main}"" fork=""true""> <!-- fork needed for running on OS X --> <classpath> <pathelement location=""${jar}"" /> <fileset dir=""lib""> <include name=""**/*.jar"" /> </fileset> </classpath> </java> </target> <!-- Encapsulates the process of packaging (jarring) program. --> <target name=""build"" depends=""document""> <echo>---------BUILD--------</echo> <jar destfile=""${jar}"" basedir=""${classes}"" /> </target> <!-- Encapsulates the process of generating javadoc for program. --> <target name=""document"" depends=""findbugs""> <echo>-------DOC--------</echo> <javadoc sourcepath=""${src}"" defaultexcludes=""yes"" destdir=""${docs}"" executable=""C:\Program Files (x86)\Java\jdk1.6.0_20\bin\javadoc.exe"" author=""true"" version=""true"" use=""true"" windowtitle=""Decisions API""> </javadoc> </target> <!-- Encapsulates the process of executing findbugs against program. --> <target name=""findbugs"" depends=""style""> <echo>----------FINDBUGS----------</echo> <findbugs home=""${findbugs}"" output=""html"" outputFile=""${findbugs.output}"" failOnError=""true"" stylesheet=""fancy.xsl""> <auxClasspath> <fileset dir=""${lib}""> <include name=""**/*.jar"" /> </fileset> </auxClasspath> <sourcePath path=""${src}""/> <class location=""${classes}"" /> </findbugs> </target> <!-- Encapsulates the process of style-checking program. --> <target name=""style"" depends=""compile""> <echo>---STYLE---</echo> <checkstyle config=""${checkstyleconfig}"" classpath=""${classes}""> <fileset dir=""${src}"" includes=""**/*.java"" excludes=""**/DecisionsProgram.java **/*Test.java"" /> </checkstyle> </target> <!-- Encapsulates the process of compiling program. --> <target name=""compile"" depends=""clean""> <echo>------------COMPILE--------</echo> <javac destdir=""${classes}""> <src path=""${src}"" /> <classpath> <fileset dir=""lib""> <include name=""**/*.jar"" /> </fileset> </classpath> </javac> </target> <!-- Encapsulates the process of cleaning up program directory structure. --> <target name=""clean""> <!-- make sure dirs exist first to prevent error --> <mkdir dir=""${classes}"" /> <mkdir dir=""${dist}"" /> <mkdir dir=""${docs}"" /> <mkdir dir=""${reports}"" /> <!-- now delete them --> <delete dir=""${classes}"" /> <delete dir=""${dist}"" /> <delete dir=""${docs}"" /> <delete dir=""${reports}"" /> <!-- now recreate them so they can be used --> <mkdir dir=""${classes}"" /> <mkdir dir=""${dist}"" /> <mkdir dir=""${docs}"" /> <mkdir dir=""${reports}"" /> </target> <target name=""test"" depends=""compile""> <echo>-------JUNIT----------</echo> <junit fork=""yes"" haltonfailure=""yes""> <test name=""${test.class.name}"" /> <formatter type=""plain"" usefile=""false"" /> <classpath refid=""test.classpath"" /> </junit> </target> can you show here an output ? how you run the test and what it prints out ? do you run it like > ""ant test"" you need to have also some junit type test implemented if it doesn't find any assert or test methods is going to not consider it junit test. There is no target which is invoking ""test"". One possibility is to replace <target name=""style"" depends=""compile""> with <target name=""style"" depends=""test""> You're absolutely right! I figured this out last night. Thanks!"
194,A,Testing package's classes with JUnit I'm developing an Android application. I have a package to access SQLite3. It isn't a ContentProvider. How can I test the package's classes using JUnit? Thanks. hello VansFannel You can refer this link for JUnit Testing Here Is Link  There is a fairly extensive set of documentation online for the Android test framework. In addition to testing your activities and such using dedicated test classes you are also welcome to have simple TestCase classes that exercise other classes in your Android app.
195,A,How should I unit test Spring Hibernate and Struts using junit What is the best practice? How should I implement it so that the database isn't polluted? this is about integration testing not unit testing. See http://en.wikipedia.org/wiki/Software_testing#Testing_Levels For Hibernate unit testing you can use HSQL DB : Unit-testing should have as few barriers as possible. For relational databases those barriers range from external dependencies (is the database running?) to speed to keeping the relational schema synchronized with your object model. For these reasons it is vital to keep database access code away from the core object model and to test as much as possible without touching a real database. http://www.theserverside.com/tt/articles/article.tss?l=UnitTesting  If you want to perform integration tests on code that accesses database you may benefit from using DBUnit. DBUnit can load test data into the database before each test. This can help you compare the result of the test case against an expected set of values.  This blogpost of mine describes a process of unit-testing using spring hibernate and maven. The maven part can be substituted with your particular build facilities. Тesting Struts (not Structs I guess) depends on your struts/spring integration. Anyway take a look at StrutsTestCase and this article about StrutsUT Thanks for the link but I find your blog too general. it is quite concrete I think. These are the exact steps I take when testing a spring app.
196,A,"What is the right java unit test library for http? I am developing a servlet application I was using JWebUnit to check all the basic responses but now I have to start using other HTTP methods different to GET(POSTPUT and DELETE). So I found HttpUnit it looks that have support for what I need POST and PUT methods are available there and I guess I can walk around the DELETE method through the HEAD which is supported by HttpUnit. However I found there is no recent activity in this project; the last release was about two years ago. so my question is if there is any other library that people is using for it or HttpUnit is the right for my case? Thanks. it's likely that you already use HtmlUnit when using JWebUnit as it relies upon this implementation. Agreed with @Sasi. It should also be noted that there are occasional checkins to the project's SVN repository. The most recent were a couple of months ago to apply a contributed patch for a bug. So there are still developers taking care of the code. That's good to know! I didn't noticed thanks for the info!  You can try out REST Assured which makes it very simple to make http requests and validate the response in Java.  I recommend HtmlUnit instead. No disrespect intended towards HttpUnit: I used it a few years ago and it worked well. However it died as a project for a number of years and more particularly it had little support for Javascript so I switched to HtmlUnit HtmlUnit has been steadily developed since its inception and has exceptional Javascript support. I have used it to test sites containing a lot of Ajax and only once I think did it not properly execute the JS code (turned out to be a wee HtmlUnit bug which I fixed). There are a lot of hooks available to help you test what's going on. As well as the site says ""HtmlUnit is used as the underlying 'browser' by different Open Source tools like Canoo WebTest JWebUnit WebDriver JSFUnit Celerity ..."" I haven't used those as I prefer expressing my tests in Java but I understand HtmlUnit works well for them. Glad to be of help. And yay my first accepted answer! *happy dance* But does HTMLUnit provides a way to query a URL using the http methods PUT and DELETE?? Yup it just takes an extra step of creating a ""WebRequestSettings"" first: http://htmlunit.sourceforge.net/apidocs/com/gargoylesoftware/htmlunit/WebClient.html#getPage%28com.gargoylesoftware.htmlunit.WebWindow%20com.gargoylesoftware.htmlunit.WebRequestSettings%29 Cool! I marked this as the answer because of 2 reasons: firstly HTMLUnit does support the DELETE method in contrast of HTTPUnit where I was looking for a work-around way. And secondly as @gregory commented I was already including it whilst working with JWebUnit so I wont require to add anything else to my require libraries.  Httpunit is a mature library. It works just fine for what you are trying to do."
197,A,Running JUnit test in NetBeans gives NoSuchMethodError error on a method whose return type was changed I have a project in NetBeans with JUnit tests. I made a change to a method return type in a dependent project and the app runs fine. But when I run the JUnit test from inside NetBeans I get a NoSuchMethodError. I made sure to clean and build. Did you try a restat of netbeans? Shouldn't be necessary but you never know ^^ I didn't try restarting NetBeans but that may have worked. If you read my answer you'll see I suspect it's a NetBeans caching bug. The solution was to make a change (any change) to the class which was making the call to the changed method. I then saved the class then undid the change and saved the class. Then running the JUnit test inside NetBeans worked. It seems like a bug with NetBeans caching. I've had strange issues like this before and I've had to close NetBeans delete the NetBeans cache folder and restart NetBeans. But it seems like making a change to that class (and then undoing it) was enough to fix the NetBeans cache.
198,A,"How can I run all JUnit unit tests except those ending in ""IntegrationTest"" in my IntelliJ IDEA project using the integrated test runner? I basically want to run all JUnit unit tests in my IntelliJ IDEA project (excluding JUnit integration tests) using the static suite() method of JUnit. Why use the static suite() method? Because I can then use IntelliJ IDEA's JUnit test runner to run all unit tests in my application (and easily exclude all integration tests by naming convention). The code so far looks like this: package com.acme; import junit.framework.Test; import junit.framework.TestCase; import junit.framework.TestSuite; import java.util.ArrayList; import java.util.Iterator; import java.util.List; public class AllUnitTests extends TestCase { public static Test suite() { List classes = getUnitTestClasses(); return createTestSuite(classes); } private static List getUnitTestClasses() { List classes = new ArrayList(); classes.add(CalculatorTest.class); return classes; } private static TestSuite createTestSuite(List allClasses) { TestSuite suite = new TestSuite(""All Unit Tests""); for (Iterator i = allClasses.iterator(); i.hasNext();) { suite.addTestSuite((Class<? extends TestCase>) i.next()); } return suite; } } The method getUnitTestClasses() should be rewritten to add all project classes extending TestCase except if the class name ends in ""IntegrationTest"". I know I can do this easily in Maven for example but I need to do it in IntelliJ IDEA so I can use the integrated test runner - I like the green bar :) Tagged as Java so more people will see it (don't know if it's really Java-specific but it does use Java). As a side-note: consider using for-each. Your for-loop would look like: for(Class testClass : allClasses) { suite.addTestSuite(testClass); } What about using JUnit4 and the Suite-Runner? Example: @RunWith(Suite.class) @Suite.SuiteClasses({ UserUnitTest.class AnotherUnitTest.class }) public class UnitTestSuite {} I made a small Shell-Script to find all Unit-Tests and another one to find my Integration-Tests. Have a look at my blog entry: http://blog.timomeinen.de/2010/02/find-all-junit-tests-in-a-project/ If you use Spring TestContext you can use the @IfProfile Annotation to declare different tests. Kind regards Timo Meinen  I've written some code to do most of the work. It works only if your files are on the local disk instead of in a JAR. All you need is one class in the package. You could for this purpose create a Locator.java class just to be able to find the package. public class ClassEnumerator { public static void main(String[] args) throws ClassNotFoundException { List<Class<?>> list = listClassesInSamePackage(Locator.class true); System.out.println(list); } private static List<Class<?>> listClassesInSamePackage(Class<?> locator boolean includeLocator) throws ClassNotFoundException { File packageFile = getPackageFile(locator); String ignore = includeLocator ? null : locator.getSimpleName() + "".class""; return toClassList(locator.getPackage().getName() listClassNames(packageFile ignore)); } private static File getPackageFile(Class<?> locator) { URL url = locator.getClassLoader().getResource(locator.getName().replace(""."" ""/"") + "".class""); if (url == null) { throw new RuntimeException(""Cannot locate "" + Locator.class.getName()); } try { return new File(url.toURI()).getParentFile(); } catch (URISyntaxException e) { throw new RuntimeException(e); } } private static String[] listClassNames(File packageFile final String ignore) { return packageFile.list(new FilenameFilter(){ @Override public boolean accept(File dir String name) { if (name.equals(ignore)) { return false; } return name.endsWith("".class""); } }); } private static List<Class<?>> toClassList(String packageName String[] classNames) throws ClassNotFoundException { List<Class<?>> result = new ArrayList<Class<?>>(classNames.length); for (String className : classNames) { // Strip the .class String simpleName = className.substring(0 className.length() - 6); result.add(Class.forName(packageName + ""."" + simpleName)); } return result; } } With some modifications mainly to make it recursive this has worked well for me. Thanks!  Spring has implemented an excellent classpath search function in the PathMatchingResourcePatternResolver. If you use the classpath*: prefix you can find all the resources including classes in a given hierarchy and even filter them if you want. Then you can use the children of AbstractTypeHierarchyTraversingFilter AnnotationTypeFilter and AssignableTypeFilter to filter those resources either on class level annotations or on interfaces they implement. http://static.springsource.org/spring/docs/2.0.x/api/org/springframework/core/io/support/PathMatchingResourcePatternResolver.html http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/core/type/filter/AbstractTypeHierarchyTraversingFilter.html  How about putting each major group of junit tests into their own root package. I use this package structure in my project: test. quick. com.acme slow. com.acme Without any coding you can set up IntelliJ to run all tests just the quick ones or just the slow ones. hanks. Most of the tests need to be in the same package than the class they test since they access non-public members. Whether or not that's a good practice it can't be changed right now. So I need to keep the test and the class in the same package. This answer would still work. test/quick and test/slow are test source roots and tests within either would still be in the com.acme package. This is a great option. It's much easier than scraping the filesystem. It also seems to be much faster than running the tests through ant since IntelliJ only rebuilds the files that have changed. Reply to CNelson: I created more test source roots but the trouble then is that IntelliJ only let's you group and run tests specified by package or class."
199,A,JUnit : Is there a way to skip a test belonging to a Test class's parent? I have two classes: public abstract class AbstractFoobar { ... } and public class ConcreteFoobar extends AbstractFoobar { ... } I have corresponding test classes for these two classes: public class AbstractFoobarTest { ... } and public class ConcreteFoobarTest extends AbstractFoobarTest { ... } When I run ConcreteFoobarTest (in JUnit) the annotated @Test methods in AbstractFoobarTest get run along with those declared directly on ConcreteFoobarTest because they are inherited. Is there anyway to skip them? Using process tags would allow you to run into specific bodies of code Don't post two answers edit your existing one.  Update: Misunderstood the Question Make AbstractFoobarTest abstract. That way the test methods in AbstractFoobarTest are only run once when ConcreteFoobarTest is executed. You are going to need a concrete subclass to test the methods in AbstractFoobar anyway. Or just remove the inheritance between AbstractFoobartest and ConcreteFoobarTest. You don't want to make use of it anyway. Thanks making the parent test class abstract did the trick! (your earlier @ignore tip worked as well but I like this one better.) @Jon so do I :-)  You really don't need AbstractFoobarTest in the first place because after all you can't create an instance of the abstract class so you will still rely on your concrete class to test your abstract class. That means you will end up using your ConcreteFoobarTest will test the APIs from the abstract class. Thus you will have just this:- public class ConcreteFoobarTest { ... @Test public void testOne() { ConcreteFoobar cf = new ConcreteFoobar(); // assert cf.conreteAPI(); } @Test public void testTwo() { ConcreteFoobar cf = new ConcreteFoobar(); // assert cf.abstractClassAPI(); } }
200,A,Data driven integration test tools for java I'm facing what I would think is a common problem but I haven't found much discussion or existing tools to address it. I am trying to set up an integration test system (already having a strong suite of unit tests) which supports QA submitting input data and expected results (in the form of flat files ideally) which can be run from a standardized junit class. I have a plan to roll my own here using the junit @Paramaterized annotation. I'm imagining each set of data and results having a control file and all control files being placed in one directory. The parameter generator method would scan this directory and return a list of these files. The test constructor would then read this file and set up the input data and expected results. Before I start writing this I wanted to see if there already existed tools to do this type of thing and if not get some opinions on the solution I am proposing. The application itself is a purely back end ETL type tool which takes data from several input files in various formats and produces results in a database. I already have some static integration tests which use a Derby in memory database to check the results for trivial input data and I plan to use a similar system to confirm the data here. Expected results would be in the form of (key_value column expected value) triples. I want to use junit in order to tie into the rest of our testing and result reporting infrastructure. Have a look at Spock a Groovy-based testing framework with strong support for data-driven testing. Spock is designed for testing Java (and Groovy) code and is fully compatible with JUnit (in fact Spock tests are run with JUnit). See here for a simple example test that pulls data from a database. In general your plan sounds reasonable to me and can also be realized with plain JUnit and @Parameterized. Spock and Groovy can make your life easier though. For example working with text files and databases is much easier in Groovy than in Java. Disclaimer: I'm the creator of Spock.
201,A,"Should I unit test methods which are inherited from super class? I'm currently writing an implementation of a JDBC driver (yes you read that correctly) in a TDD manner and while I have only finished class stubs at this point and only some minor functionality it just occured to me that since Statement is a superclass for PreparedStatement which is a superclass for CallableStatement what should I do when I really start to write tests for my implementations of those classes which one of these should I do: Create a test suite for Statement and then extend that suite for additional tests for PreparedStatement and then do the same for CallableStatement. Test each implementation individually ignoring the methods inherited from superclass(es). Rigorously test every single method individually for each implementation class; It is possible that some inherited methods work differently depending on implementation after all. Mild variation of this would be that I'd test all those inherited methods the implementation uses. Number two feels the most natural but due to the reason I put to the third one I'm not that sure if it'd be wise to do so. So what do you think I should do? ""test every single method individually for each implementation class"" In particular failure to override a superclass method properly is a common bug. The author of the subclass makes assumptions about the superclass. The superclass changes and the subclass is now broken. This would be the proper way of doing it. You need to ensure that all methods still work.  I would specifically never do alternative 1 (letting the test-class hierarchy be the same as the actual class hierarchy) if this means you will be running the same tests repeatedly for each test subclass. I am also generally sceptical of subclassing test classes other than the general utility base class. I normally make 1 test for each class in a hierarchy abstract or not. So the base class has a separate test (usually with a test-local private subclass that is used for testing it specifically) and I use my knowledge of the subclasses to write proper tests for each subclass. I can see in coverage runs what is missing tests so I'm usually not too formalized up-front.  Provide enough tests so that you feel comfortable - based on your knowledge of the implementation. I don't treat unit testing as completely black-box testing. If you know that the base class never calls any virtual methods (or at least none that are overridden) then note that fact but don't effectively duplicate the unit tests that you've already got. Unit testing can certainly be taken to extremes - it's always worth balancing the value you're getting from it with the effort it's costing you.  With TDD you should not aim at testing methods but behavior or capabilities of your code. Therefore when implementing a subclass you can restrict to testing only the behaviors that are different from the base class. When in doubt write a new test."
202,A,"Mock File class and NullPointerException I'm creating a File mock object with Mockito that will be used as the directory to store a new File. Folder folder = Mockito.mock(File.class); File file = new Agent().createNewFile(folder ""fileName""); and inside my Agent class: public File createNewFile(File folder String filename){ return new File(folder ""testfile""); } But I'm getting a NullPointerException at the initialization block of File when creating the new file inside createNewFile method: java.lang.NullPointerException at java.io.File.<init>(File.java:308) I think it happens because File doesn't have any empty constructor so when mocking the object some internal state remains null. Am I taking the wrong approach mocking the File folder object? My goal is to check some constraints before creating the new file but I don't want to depend on an existing real folder on the file system. Thank you. ""I think it happens because File doesn't have any empty constructor"" I'm not familiar with Mockito but that is most likely the case. The absence of a constructor with parameters sort of makes sense as the javadoc states that File is ""An abstract representation of file and directory pathnames"". As such File would need to point a given file on the host system but doesn't represent it ""physically"" and simply allows to carry out file I/O operations. http://download.oracle.com/javase/1.4.2/docs/api/java/io/File.html You need to define the behavior for getPath() for folder as it gets called internally in File class. You can do it as: File folder = Mockito.mock(File.class); when(folder.getPath()).thenReturn(""C:\temp\""); File file = new Agent().createNewFile(folder ""fileName""); It will work only till you don't really create a new file but only calling new File. I do not believe this solves the problem at least not for me the code in File(File parent String child) constructor blows up because the path attribute is null ""if (parent.path.equals("""")) {""... I too don't understand how this would solve the problem. After all you don't get further then line 1 in the code you post as an example. The nullpointer is thrown right there. Agree this does not answer the question for JDK 7u40 and up due to https://bugs.openjdk.java.net/browse/JDK-8003992 which modified FileInputStream to call File.isInvalid() in its ctor and File.isInvalid() accesses the File.path member variable directly rather than File.getPath()."
203,A,"CollectionAssert in jUnit? Is there a jUnit parallel to NUnit's CollectionAssert? Not directly no. I suggest the use of Hamcrest which provides a rich set of matching rules which integrates nicely with jUnit (and other testing frameworks) This does not compile for some reason (see http://stackoverflow.com/questions/1092981/hamcrests-hasitems): ArrayList actual = new ArrayList(); ArrayList expected = new ArrayList(); actual.add(1); expected.add(2); assertThat(actual hasItems(expected));  Using JUnit 4.4 you can use assertThat() together with the Hamcrest code (don't worry it's shipped with JUnit no need for an extra .jar) to produce complex self-describing asserts including ones that operate on collections: import static org.junit.Assert.assertThat; import static org.junit.matchers.JUnitMatchers.*; import static org.hamcrest.CoreMatchers.*; List<String> l = Arrays.asList(""foo"" ""bar""); assertThat(l hasItems(""foo"" ""bar"")); assertThat(l not(hasItem((String) null))); assertThat(l not(hasItems(""bar"" ""quux""))); // check if two objects are equal with assertThat() // the following three lines of code check the same thing. // the first one is the ""traditional"" approach // the second one is the succinct version and the third one the verbose one assertEquals(l Arrays.asList(""foo"" ""bar""))); assertThat(l is(Arrays.asList(""foo"" ""bar""))); assertThat(l is(equalTo(Arrays.asList(""foo"" ""bar"")))); Using this approach you will automagically get a good description of the assert when it fails. Ooh I hadn't realised hamcrest had made it into the junit distro. Go Nat! If I want to assert l is composed of items (""foo"" ""bar"") but no other items exists - is there some easy syntax for that? Use the above code snippet and throw in an additional assertTrue(l.size() == 2) Meh ugly. In NUnit that's CollectionAssert.AreEqual( Collection expected Collection actual ); Isn't that just assertTrue(collection1.equals(collection2)); ? You are free to write your own CollectionAssert.AreEqual(Collection col1 Collection col2) using example above. You'll needed about 5 minutes to made this. Google have found another Stackoverflow answer that I was looking for! ""the second one is the succinct version"" - it's 1 character less succint than the 'traditional' approach!  Take a look at FEST Fluent Assertions. IMHO they are more convenient to use than Hamcrest (and equally powerful extensible etc) and have better IDE support thanks to fluent interface. See https://github.com/alexruiz/fest-assert-2.x/wiki/Using-fest-assertions"
204,A,"Spring / Hibernate / JUnit - No Hibernate Session bound to Thread I'm trying to access the current hibernate session in a test case and getting the following error: org.hibernate.HibernateException: No Hibernate Session bound to thread and configuration does not allow creation of non-transactional one here at org.springframework.orm.hibernate3.SpringSessionContext.currentSession(SpringSessionContext.java:63) at org.hibernate.impl.SessionFactoryImpl.getCurrentSession(SessionFactoryImpl.java:574) I've clearly missed some sort of setup but not sure what. Any help would be greatly appreciated. This is my first crack at Hibernate / Spring etc and the learning curve is certainly steep! Code follows: The offending class: public class DbUnitUtil extends BaseDALTest { @Test public void exportDtd() throws Exception { Session session = sessionFactory.getCurrentSession(); session.beginTransaction(); Connection hsqldbConnection = session.connection(); IDatabaseConnection connection = new DatabaseConnection(hsqldbConnection); // write DTD file FlatDtdDataSet.write(connection.createDataSet() new FileOutputStream(""test.dtd"")); } } Base class: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""classpath:applicationContext.xml""}) public class BaseDALTest extends AbstractJUnit4SpringContextTests { public BaseDALTest() { super(); } @Resource protected SessionFactory sessionFactory; } applicationContext.xml: <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd""> <bean id=""dataSource"" class=""org.springframework.jdbc.datasource.DriverManagerDataSource""> <property name=""driverClassName""> <value>org.hsqldb.jdbcDriver</value> </property> <property name=""url""> <value>jdbc:hsqldb:mem:sample</value> </property> <property name=""username""> <value>sa</value> </property> <property name=""password""> <value></value> </property> </bean> <bean id=""sessionFactory"" class=""com.foo.spring.AutoAnnotationSessionFactoryBean""> <property name=""dataSource"" ref=""dataSource"" /> <property name=""entityPackages""> <list> <value>com.sample.model</value> </list> </property> <property name=""schemaUpdate""> <value>true</value> </property> <property name=""hibernateProperties""> <props> <prop key=""hibernate.dialect"">org.hibernate.dialect.HSQLDialect </prop> <prop key=""hibernate.show_sql"">true</prop> </props> </property> </bean> </beans> With the above Spring configuration it should be sufficient to code Session session = sessionFactory.getCurrentSession(); in your method and class to test. Session management is done by the Hibernate / Spring /JUnit test configuration as later is done in the Hibernate / Spring configuration in the real application. This is how it worked for my tests. In the final web application there will automatically be a Hibernate session associated with the current web request and therefore in testing there should be no sessionFactory.openSession() call.  Wrong that will just fill your code with session management code. First add a transaction management bean in your context:  <bean id=""transactionManager"" class=""org.springframework.orm.hibernate3.HibernateTransactionManager""> <property name=""sessionFactory"" ref=""sessionFactory""/> </bean> The second thing extend AbstractTransactionalJUnit4SpringContextTests  public class BaseDALTest extends AbstractTransactionalJUnit4SpringContextTests{ Third thing annotate you test class with  @TransactionConfiguration @Transactional If your transaction demarcation is correct(surrounding your dao or service) you should be done. It's not nice to sprinkle session and transaction handling code all around your code (even inside your tests). Ideally you should have @Transactional(propagation = Propagation.REQUIRED) if you are testing some dao/repository tests which are themselves not annotated with transactions since they depend upon the class which initiated to be in transaction. On @TransactionConfiguration tell it about your transaction manager @TransactionConfiguration(transactionManager = ""transactionManager"" defaultRollback = true) I just read your post and I managed to overcome the problem for me. Now I don't see new lines in tables though while no error is thrown. I assume I have some how to stop rollback?  Duh. Session session = sessionFactory.openSession(); Session session = sessionFactory.getCurrentSession(); Oops. (Edited since this was wrong and getting upvoted). actually this is totally wrong. You want to call getCurrentSession() otherwise you will be responsible for closing it and your transactions won't work with the annotation driven model. @Justin - Thanks the post has been updated based on this feedback. @ Downvoter - care to comment why? (This post has been updated for over a year!)  Please refer to the Spring documentation. There is a whole chapter on testing and a section on transaction management: http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/testing.html#testing-tx I've had success extending AbstractTransactionalJUnit4SpringContextTests but there's a workaround: TransactionTemplate transactionTemplate = new TransactionTemplate(transactionManager); transactionTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus status) { // DAO has access to the session through sessionFactory.getCurrentSession() myDao.findSomething(id); } }); I found the dependency on AbstractTransactionalJUnit4SpringContextTests quite restrictive so I really your TransactionTemplate suggestion - it works!  Spring ignores hibernate.current_session_context_class=thread property (which you don't use) and wraps Hibernate’s SessionFactory in it's own transactional version as explained here The solution to this is to set the property <property name=""exposeTransactionAwareSessionFactory""><value>false</value></property> in session factory bean configuration"
205,A,jUnit testing with Google Web Toolkit I would like to be able to run a set of unit tests by linking to them in my application (e.g. I want to be able to click on a link and have it run a set of jUnit tests). The problem is that GWT and jUnit don't seem to be designed for this capability -- only at build time can you run the tests it seems. I would like to be able to include my test code in my application and from onModuleLoad for example run a set of tests. I tried to just instantiate a test object: StockWatcherTest tester = new StockWatcherTest(); tester.testSimple(); but I get: No source code is available for type com.google.StockWatcher.client.StockWatcherTest; even though I include the module specifically. Would anyone know a way to do this? I just want to be able to display the test results within the browser. If you are trying to test UI elements in GWT using JUnit unfortunately you may not do so. JUnit testing is limited to RPC and non-UI client-side testing. See this thread for a great discussion on what you can and cannot do with GWT jUnit testing. If you are not trying to test UI elements but are instead trying to inject your RPC code or client-side logic with test values (hence why you want to be able to click on a link and run a set of JUnit tests) then you should follow the following guide from testearly.com: Testing GWT with JUnit. In short you should make sure that the method you are testing does not include any UI elements and if the method you are testing is asynchronous in nature you must add a timer. In 2.0 HTMLUnit was added. You may wish to use this instead of firing up a browser each time you wish to test.
206,A,"Unit test for Lucene indices I'm working on legacy code that builds an index of popular terms in another index. There are no unit tests in place and the indexing process is a pain to wait for because the first index takes so long to build. I want to structure the second (popular term) index differently. Is there a best practice for testing to see if a Lucene index is being created properly? EDIT>> Per @Pascal's advice I'm using a RAMDirectory then to test the index I just wrote I set up an indexReader and iterate through the term results printing out each term to make sure the data looks alright. Code: IndexReader reader = IndexReader.open(dir2); TermEnum terms = reader.terms(); System.out.println(""Here come the terms!""); while (terms.next()){ if (terms.term().field().equals(""FULLTEXT"")){ System.out.println(terms.term()); } } int numDocs = reader.maxDoc(); System.out.println(""Number of Docs: "" + numDocs); If the index is really large I let it run for a bit then just stop it midway through. Also Luke is a great tool for inspecting the index if you want to be more thorough... I'm just looking for something fast. Any other ideas are welcome! When unit-testing Lucene index I often use the RAMDirectory as it is quick to build. This works really well thanks Pascal! Good idea that way it also doesn't persist right? Yes it won't persist to disk but it will stay in memory for the time of the test."
207,A,"Using different classloaders for different JUnit tests? I have a Singleton/Factory object that I'd like to write a JUnit test for. The Factory method decides which implementing class to instantiate based upon a classname in a properties file on the classpath. If no properties file is found or the properties file does not contain the classname key then the class will instantiate a default implementing class. Since the factory keeps a static instance of the Singleton to use once it has been instantiated to be able to test the ""failover"" logic in the Factory method I would need to run each test method in a different classloader. Is there any way with JUnit (or with another unit testing package) to do this? edit: here is some of the Factory code that is in use: private static MyClass myClassImpl = instantiateMyClass(); private static MyClass instantiateMyClass() { MyClass newMyClass = null; String className = null; try { Properties props = getProperties(); className = props.getProperty(PROPERTY_CLASSNAME_KEY); if (className == null) { log.warn(""instantiateMyClass: Property ["" + PROPERTY_CLASSNAME_KEY + ""] not found in properties using default MyClass class ["" + DEFAULT_CLASSNAME + ""]""); className = DEFAULT_CLASSNAME; } Class MyClassClass = Class.forName(className); Object MyClassObj = MyClassClass.newInstance(); if (MyClassObj instanceof MyClass) { newMyClass = (MyClass) MyClassObj; } } catch (...) { ... } return newMyClass; } private static Properties getProperties() throws IOException { Properties props = new Properties(); InputStream stream = Thread.currentThread().getContextClassLoader().getResourceAsStream(PROPERTIES_FILENAME); if (stream != null) { props.load(stream); } else { log.error(""getProperties: could not load properties file ["" + PROPERTIES_FILENAME + ""] from classpath file not found""); } return props; } This question might be old but since this was the nearest answer I found when I had this problem I though I'd describe my solution. Using Junit4 Split your tests up so that there is one test method per class (this solution only changes classloaders between classes not between methods as the parent runner gathers all the methods once per class) Add the @RunWith(SeparateClassloaderTestRunner.class) annotation to your test classes. Create the SeparateClassloaderTestRunner to look like this: public class SeparateClassloaderTestRunner extends BlockJUnit4ClassRunner { public SeparateClassloaderTestRunner(Class<?> clazz) throws InitializationError { super(getFromTestClassloader(clazz)); } private static Class<?> getFromTestClassloader(Class<?> clazz) throws InitializationError { try { ClassLoader testClassLoader = new TestClassLoader(); return Class.forName(clazz.getName() true testClassLoader); } catch (ClassNotFoundException e) { throw new InitializationError(e); } } public static class TestClassLoader extends URLClassLoader { public TestClassLoader() { super(((URLClassLoader)getSystemClassLoader()).getURLs()); } @Override public Class<?> loadClass(String name) throws ClassNotFoundException { if (name.startsWith(""org.mypackages."")) { return super.findClass(name); } return super.loadClass(name); } } } Note I had to do this to test code running in a legacy framework which I couldn't change. Given the choice I'd reduce the use of statics and/or put test hooks in to allow the system to be reset. It may not be pretty but it allows me to test an awful lot of code that would be difficult otherwise. Also this solution breaks anything else that relies on classloading tricks such as mokito. Instead of looking for ""org.mypackages."" in loadClass() you can also do something like this: return name.startsWith(""java"") || name.startsWith(""org.junit"") ? super.loadClass(name) : super.findClass(name);  Singletons lead to a whole world of hurt. Avoid singletons and your code becomes much easier to test and just all-round nicer. Why negative its a good suggestion. +1 I believe the negative is because this isn't necessarily an answer to the question. There are situations where a singleton might make sense and hence the requirement still stands. We can't control/trust 3rd-party libraries. In light of that I'm voting -1. Sorry. -1 from me too. The question is still useful if it wasn't about singletons but about generally creating a dedicated class loading scope with JUnit. So this answer doesn't help This is of course the first correct answer and I found this Q literally in search of this A along with the myriad of more complex answers. In a big picture context this answer makes a significant point about testability juxtaposed against that which lay below. Suggestions should be comments to the question. This is not an answer to the asked question. No matter how useful the suggestion is it is not an answer and thus deserves the downvotes.  If executing Junit via the Ant task you can set fork=true to execute every class of tests in it's own JVM. Also put each test method in its own class and they will each load and initialise their own version of MyClass. It's extreme but very effective.  You can use Reflection to set myClassImpl by calling instantiateMyClass() again. Take a look at this answer to see example patterns for playing around with private methods and variables.  When I run into these sort of situations I prefer to use what is a bit of a hack. I might instead expose a protected method such as reinitialize() then invoke this from the test to effectively set the factory back to its initial state. This method only exists for the test cases and I document it as such. It is a bit of a hack but it's a lot easier than other options and you won't need a 3rd party lib to do it (though if you prefer a cleaner solution there probably are some kind of 3rd party tools out there you could use)."
208,A,"Running JUnit tests with Maven under Eclipse I just installed the plugin m2e for the first time on Eclipse. I wrote a simple JUnit (version 4) test. I can run it from Eclipse but not from the pom.xml (alt-click Run as Maven Test). I suppose I need to tell Maven to search for that class but I just don't know how. Also I couldn't find JUnit 4 in the groupId ""junit"": only the version 3.8.1 is available. Do I really need to write tests for version 3.x and not version 4+? How to fix this? Think of me as a newbie with Maven: that's exactly what I am. So please don't speak about artifact technobabble unless describing exactly what I need to do. I could barely install Guava as dependency and I'm completely lost right now with these concepts. Thanks! Note that Maven has two independent class locations and only looks for tests in one and Eclipse merges the two. This mean that it is possible to write tests that pass when run inside Eclipse and fail from the command line. you can run Junit 4 with maven. You just need the Junit 4 dependency in your pom. You also need the surefire plugin to execute the tests. See: http://maven.apache.org/plugins/maven-surefire-plugin/ Hint: surefire looks for files with *Test.java nameing to finde tests. Works like a charm. Thanks for the URL and the *Test convention. :) Test*.java also works"
209,A,Should @Entity Pojos be tested? I don't know if I should test my @Entity-annotated Pojos. After all there are mainly just generated getters/setters. Should I test them? When it comes to testing DAOs I'm using all those entities - so they are already propely tested I guess? Thanks for your thoughts. Matt The only reason I could think of the write tests would be to test the @Entity annotation itself. Testing the storage and retrieval of values seems like one is doubting a fundamental ability of our programming environment :)  Can your code contain any bugs? If not what's the point in testing it? In fact trying to test it would just introduce new bugs (because your tests could be wrong). So the conclusion is: You should not test getters and setters without code (i.e. those which just assign or read a field without any additional code). The exception is: When you manually write those getters/setters because you could have made a typo. But even then some code will use these and there should be a test for that code which in turn tests whether the getters/setters behave correctly. Thanks for your answer Aaron!
210,A,"How to unit test synchronized code I am new to Java and junit. I have the following peice of code that I want to test. Would appreciate if you could send your ideas about what's the best way to go about testing it. Basically the following code is about electing a leader form a Cluster. The leader holds a lock on the shared cache and services of the leader get resumed and disposed if it somehow looses the lock on the cache. How can i make sure that a leader/thread still holds the lock on the cache and that another thread cannot get its services resumed while the first is in execution? public interface ContinuousService { public void resume(); public void pause(); } public abstract class ClusterServiceManager { private volatile boolean leader = false; private volatile boolean electable = true; private List<ContinuousService> services; protected synchronized void onElected() { if (!leader) { for (ContinuousService service : services) { service.resume(); } leader = true; } } protected synchronized void onDeposed() { if (leader) { for (ContinuousService service : services) { service.pause(); } leader = false; } } public void setServices(List<ContinuousService> services) { this.services = services; } @ManagedAttribute public boolean isElectable() { return electable; } @ManagedAttribute public boolean isLeader() { return leader; } public class TangosolLeaderElector extends ClusterServiceManager implements Runnable { private static final Logger log = LoggerFactory.getLogger(TangosolLeaderElector.class); private String election; private long electionWaitTime= 5000L; private NamedCache cache; public void start() { log.info(""Starting LeaderElector ({})""election); Thread t = new Thread(this ""LeaderElector (""+election+"")""); t.setDaemon(true); t.start(); } public void run() { // Give the connection a chance to start itself up try { Thread.sleep(1000); } catch (InterruptedException e) {} boolean wasElectable = !isElectable(); while (true) { if (isElectable()) { if (!wasElectable) { log.info(""Leadership requested on election: {}""election); wasElectable = isElectable(); } boolean elected = false; try { // Try and get the lock on the LeaderElectorCache for the current election if (!cache.lock(election electionWaitTime)) { // We didn't get the lock. cycle round again. // This code to ensure we check the electable flag every now & then continue; } elected = true; log.info(""Leadership taken on election: {}""election); onElected(); // Wait here until the services fail in some way. while (true) { try { Thread.sleep(electionWaitTime); } catch (InterruptedException e) {} if (!cache.lock(election 0)) { log.warn(""Cache lock no longer held for election: {}"" election); break; } else if (!isElectable()) { log.warn(""Node is no longer electable for election: {}"" election); break; } // We're fine - loop round and go back to sleep. } } catch (Exception e) { if (log.isErrorEnabled()) { log.error(""Leadership election "" + election + "" failed (try bfmq logs for details)"" e); } } finally { if (elected) { cache.unlock(election); log.info(""Leadership resigned on election: {}""election); onDeposed(); } // On deposition do not try and get re-elected for at least the standard wait time. try { Thread.sleep(electionWaitTime); } catch (InterruptedException e) {} } } else { // Not electable - wait a bit and check again. if (wasElectable) { log.info(""Leadership NOT requested on election ({}) - node not electable""election); wasElectable = isElectable(); } try { Thread.sleep(electionWaitTime); } catch (InterruptedException e) {} } } } public void setElection(String election) { this.election = election; } @ManagedAttribute public String getElection() { return election; } public void setNamedCache(NamedCache nc) { this.cache = nc; } http://today.java.net/article/2003/07/12/multithreaded-tests-junit http://www.junit.org/node/54 As an alternative to testing frameworks (or to extra extensions on top of JUnit if you are using that) is just some plain old code: Create several threads and apply them to this routine. Loop through the threads and test each one until you find out who is the leader. Apply different environmental changes (including the passage of time) to your program's state and redo the test. Is the leader still the leader? Now force the leader to abdicate (kill that thread or something). Did another thread take over?  For JUnit there's the MultithreadingTester from junit-toolbox.  If you are not too particular about using JUnit then you can use TestNG framework. They have multi threaded support."
211,A,"What do you need to do Unit testing in Java with Eclipse? I've recently been lifted out of the .Net world into the Java world and I miss my unit tests. Using Visual Studio I used NUnit and TestDriven.net to run my unit tests. What is a comparable system for Java Using Eclipse? I'm looking specifically for the plugins that will get me going or a guide on how to do it. I'm aware that JUnit is what NUnit was initially based on but I want to know the best way to integrate it into Eclipse as there seem to be a few plugins that do this and I don't have the time to play around with them all. UPDATE Okay I didn't know that JUnit was built into the IDE. Are there any plugins that make using JUnit any easier? I've used the testNG witch has a plug in for eclipse.  fit (http://www.theserverside.com/news/thread.tss?thread_id=39417) dbunit (http://www.dbunit.org/) many others in eclipse you can right click a package and select run as a junit test. be careful of http://xunitpatterns.com/test%20fixture%20-%20ambiguous.html. iirc this boils down to junit creating an instance of each test case before calling setup and nunit just creating one instance.  What do you mean with ""make using JUnit any easier""? I use Ant for runnings tests as a task. Output will be stored into a flat file or a html file. It depends on the TestRunner. Please specify your question and you'll get answers! :)  JUnit 4 is actually really easy to use as long as you're using a project that targets Java 5 or newer and have added it to the project. I mean how much easier can you get than @Test public myTest() { // Test code here } There are also @Before @After @BeforeClass @AfterClass and @Ignore. The *Class methods need to be static methods. @Before runs before each test @BeforeClass runs before the first test... but keep in mind that JUnit tests can run in any order. If you're doing database tests you may want to look into the DBUnit addon although you need to take some special steps to use it with JUnit 4's native mode.  Which version of Eclipse are you using? For as long as I remember (I've been using Eclipse since early 3.xs) Eclipse supports JUnit out of the box. You just: Right-click on a project -> Run As -> JUnit Test Does this not work for you? I'm using Europa. And I didn't know that it was built into the IDE thats pretty cool. Unit Testing and refactoring IDE-support evolved like Smalltalk > Java > .Net n the rest now  Using JUnit with eclipse is actually very easy. Just go to File->New... and select JUnit Test Case. Eclipse will handle adding the JUnit library and all of the imports.  I've been using moreUnit for a few years and can't live without its Ctrl+J shortcut to switch between the class and its test case. I've also found EclEmma useful for finding untested code.  Easier than ""Right-click on a project -> Run As -> JUnit Test""? Like you want it bound to a keypress (because it probably is). Lemme check--Yeah alt-shift-X then ""T"". Easy enough? There is also window/show view/other/java/JUnit that will give you a junit run bar in a window. You can then just hit the run tests button and it will run all the tests in your project/section. Ctrl-shift-L is great for figuring out keybindings if you are getting to know eclipse. Also get VERY familiar wtih ctrl-space just press it whenever you're in the middle of typing something (seriously try it with everything!) Also type ""sysout[ctrl-space]"" @BillK: save two more keypresses ""syso[CTRL + SPACE]"" :)"
212,A,"Designing a robust unit test - testing the same logic in several different ways? In unit test design it is very easy to fall into the trap of actually just calling your implementation logic. For example if testing an array of ints which should all be two higher than the other (2 4 6 8 etc) is it really enough to get the return value from the method and assert that this pattern is the case? Am I missing something? It does seem like a single unit test method needs to be made more robust by testing the same expectation in several ways. So the above expectation can be asserted by checking the increase of two is happening but also the next number is divisible by 2. Or is this just redundant logic? So in short should a unit test test the one expectation in several ways? For example if I wanted to test that my trousers fit me I would/could measure the length put it next to my leg and see the comparison etc. Is this the sort of logic needed for unit testing? Thanks _So the above expectation can be asserted by checking the increase of two is happening but also the next number is divisible by 2. Or is this just redundant logic?_ That's redundant. And possibly wrong - if the spec says ""two more"" then 5 7 is right. But 7 isn't divisible by two (evenly yadda) I'd recommend multiple tests. If you ever need to change the behaviour you'd like to have as few tests to change as possible. This also makes it easier to find what the problem is. If your really blow the implementation and get [1345] your one test will fail but you'll only get one failure for the first thing you test when there are actually two different problems. Try naming your tests. If you can't say in one clear method name what you're testing break up the test. testEntriesStepByTwo testEntriesAllEven Also don't forget the edge cases. The empty list will likely pass the 'each entry is 2 more than the previous' one and 'all entries are even' tests but should it?  For example if testing an array of ints which should all be two higher than the other (2 4 6 8 etc) is it really enough to get the return value from the method and assert that this pattern is the case? Perhaps you need to think a little more about how the function would be used. Will it be use with very large numbers? If so the you may want to try some tests with very large numbers. Will it be used with negative numbers? Am I missing something? It does seem like a single unit test method needs to be made more robust by testing the same expectation in several ways. So the above expectation can be asserted by checking the increase of two is happening but also the next number is divisible by 2. Or is this just redundant logic?redundant logic? Hmm... well 1359 would pass the assertEachValueIncrementsByTwo test but it would not pass the assertValuesDivisibleByTwo test. Does it matter that they are divisible by 2? If so then you really should test that. If not then it's a pointless redundant test. You should try to find more than 1 test for your methods but redundant tests for the sake of more testing is not going to help you. Adding the assertValuesDivisibleByTwo test when that is not really required will just confuse later developers who are trying to modify your code. If you can't think of any more tests try writing a random input function that will generate 100 random test arrays each time you run your tests. You'd be surprised how many bugs escape under the radar when you only check one or two input sets.  If you assert that your array contains 2468 -- then your testing logic might be flawed because your test would pass if you just returned an array with those elements but not with say 681012. You need to test that calculation is correct. So you need to test it with multiple arrays in this particular case. I find that making sure the test fails then making the test pass in the true spirit of TDD helps flush out what the correct test is...  Your unit tests should check all of your assumptions. Whether you do that in 1 test or multiple tests is a personal preference. In the example you stated above you had two different assumptions: (1) Each value should increment by 2. (2) All values should be even. Should (-8-6-4-2) pass/fail? Remember ensuring your code fails when it's supposed to is just as important if not more important then making sure it passes when it's supposed to. _Whether you do that in 1 test or multiple tests is a personal preference._ I disagree. Tests should be as small as possible as simple as possible be named specifically and test exactly one thing. @Tony Ennis: I agree with you but wasn't going to start a holy war :-) heh no war intended ;-)  The array you are testing must be generated in some sort of logic. Isn't it better to test this logic to ensure that the resulting array always meets your requirements?"
213,A,"Failed to load applicationContext: NoSuchBeanDefinitionException when running JUnit test in Spring MVC app When I run: package se.hsr.web; import java.util.List; import junit.framework.Assert; import se.hsr.web.UserDao; import se.hsr.web.User; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration( locations = { ""classpath:HSR-servlet.xml"" }) public class UserTest { private UserDao dao; @Autowired public void setDao(UserDao dao) { this.dao = dao; } @Test public void testCreateData() { int expectedResult = 1; User user = new User(); user.setAge(23); user.setUserName(""Adit""); user.setRegistered(true); dao.saveUser(user); Assert.assertEquals(expectedResult dao.getAllUser(new User()).size()); } @Test public void testRetrieveData() { List<User> userList = dao.getAllUser(new User()); Assert.assertEquals(1 userList.size()); User userExpected = userList.get(0); User userResult = dao.selectUserById(userExpected.getUserId()); Assert.assertEquals(userExpected.getUserId() userResult.getUserId()); } @Test public void testUpdateData() { List<User> userList = dao.getAllUser(new User()); Assert.assertEquals(1 userList.size()); User userExpected = userList.get(0); userExpected.setUserName(""Singgih""); dao.saveUser(userExpected); User userResult = dao.selectUserById(userExpected.getUserId()); Assert.assertEquals(userExpected.getUserName() userResult .getUserName()); } @Test public void testDeleteData() { List<User> userList = dao.getAllUser(new User()); Assert.assertEquals(1 userList.size()); User userExpected = userList.get(0); dao.deleteUser(userExpected); User userResult = dao.selectUserById(userExpected.getUserId()); Assert.assertEquals(userResult null); } } The console output is: ... 24 more Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire method: public void se.hsr.web.UserDaoImpl.setSessionFactory(org.hibernate.SessionFactory); nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No matching bean of type [org.hibernate.SessionFactory] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {} at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:589) at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:84) at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:282) ... 38 more Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No matching bean of type [org.hibernate.SessionFactory] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {} at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:920) at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:789) at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:703) at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:547) ... 40 more My HSR-servlet.xml db-config.xml and configuration.properties all rest in the src/main/resources folder. HSR-servlet.xml: <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:p=""http://www.springframework.org/schema/p"" xmlns:context=""http://www.springframework.org/schema/context"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd""> <context:component-scan base-package=""se.hsr.web""/> <bean id=""viewResolver"" class=""org.springframework.web.servlet.view.UrlBasedViewResolver""> <property name=""viewClass"" value=""org.springframework.web.servlet.view.JstlView"" /> <property name=""prefix"" value=""/WEB-INF/jsp/"" /> <property name=""suffix"" value="".jsp"" /> </bean> <bean id=""propertyConfigurer"" class=""org.springframework.beans.factory.config.PropertyPlaceholderConfigurer""> <property name=""locations""> <list> <value>/configuration.properties</value> </list> </property> </bean> </beans> db-config.xml: <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:p=""http://www.springframework.org/schema/p"" xmlns:context=""http://www.springframework.org/schema/context"" xmlns:tx=""http://www.springframework.org/schema/tx"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd ""> <bean id=""dataSource"" class=""com.mchange.v2.c3p0.ComboPooledDataSource"" destroy-method=""close""> <property name=""driverClass""> <value>${jdbc.driver.className}</value> </property> <property name=""jdbcUrl""> <value>${jdbc.url}</value> </property> <property name=""user""> <value>${jdbc.username}</value> </property> <property name=""password""> <value>${jdbc.password}</value> </property> </bean> <bean id=""sessionFactory"" class=""org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean""> <property name=""dataSource""> <ref bean=""dataSource"" /> </property> <property name=""packagesToScan"" value=""se.hsr.web"" /> <property name=""hibernateProperties""> <props> <prop key=""hibernate.dialect"">${jdbc.hibernate.dialect}</prop> <prop key=""hibernate.hbm2ddl.auto"">create</prop> <!-- uncomment this for first time run--> <prop key=""hibernate.hbm2ddl.auto"">create</prop> <prop key=""hibernate.show_sql"">false</prop> </props> </property> </bean> <bean id=""transactionManager"" class=""org.springframework.orm.hibernate3.HibernateTransactionManager""> <property name=""sessionFactory""> <ref bean=""sessionFactory"" /> </property> </bean> <tx:annotation-driven /> </beans> What could the problem be? Thankful for any help. UPDATE: new errormessage after changing the code after the tip from the first commenter. Caused by: org.springframework.beans.factory.CannotLoadBeanClassException: Cannot find class [com.mchange.v2.c3p0.ComboPooledDataSource] for bean with name 'dataSource' defined in class path resource [db-config.xml]; nested exception is java.lang.ClassNotFoundException: com.mchange.v2.c3p0.ComboPooledDataSource at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1250) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:433) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190) at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:322) ... 80 more Caused by: java.lang.ClassNotFoundException: com.mchange.v2.c3p0.ComboPooledDataSource at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) at org.springframework.util.ClassUtils.forName(ClassUtils.java:257) at org.springframework.beans.factory.support.AbstractBeanDefinition.resolveBeanClass(AbstractBeanDefinition.java:408) at org.springframework.beans.factory.support.AbstractBeanFactory.doResolveBeanClass(AbstractBeanFactory.java:1271) at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1242) ... 86 more I heard that it might be a Junit v > 4.8 specific issue. Anyone can confirm this? Try adding db-config.xml to your context configuration locations : @ContextConfiguration( locations = { ""classpath:HSR-servlet.xml"" ""classpath:db-confix.xml }) Also I assume this configuration works when you load it up in a servlet container? It's just the unit tests that aren't work? Thanks. I inserted that as you said and a new errormessage was displayed see original post for it. As for the second error looks like c3p0 isn't on the classpath. How are you running these unit tests?"
214,A,How can I make Eclipse detect Scala JUnit tests where package doesn't match folder? In Scala packages doesn't necessarily need to match folders. And if tooling wasn't concerned I would rather leave out the ever so redundant organisation prefix of the package to allow for shallower paths. My problem is that the Eclipse JUnit plugin seems to be working with folders rather than classpath. When I place my Scala test classes in folders matching the package everything works fine. If I however put them the way I would like I get a ClassNotFoundException. Say my package prefix is org.myorganisation.myproduct for all classes in a project. Then I would like to have folders like src/test/scala/domainpackage1/ instead of src/test/scala/org/myorganisation/myproduct/domainpackage1 but if I put a test class looking like: package org.myorganisation.myproduct; package domainpackage1 ... @RunWith(classOf[JUnitRunner]) class DomainClass1Spec extends FeatureSpec with GivenWhenThen { ... } in the folder src/test/scala/domainpackage1 I get a java.lang.ClassNotFoundException: domainpackage1.DomainClass1Spec So it seems the JUnit plugin is looking at the source location and looks for a class file matching that rather than finding the class in a more stable way. I mean it should be possible to find the output (.class) from the current position in the compilation unit (.scala) right? It seems worse than this. It doesn't seem to be possible for Eclipse to find the source when debugging if it is not placed in folders matching packages. So it's maybe not only the JUnit plugin that is the problem but Eclipse as a whole. This is a known issue in Eclipse: https://bugs.eclipse.org/bugs/show_bug.cgi?id=16209 It's marked as WONTFIX. But perhaps you should comment on it and point out the issue for Scala...
215,A,"How to run external testcase(Classjunit) in java program? How to run external testcase(Classjunit) in java program? What do you mean by external testcase? If you want to run JUnit tests through a Java program you could use the JUnitCore class JUnitCore is a facade for running tests. It supports running JUnit 4 tests JUnit 3.8.x tests and mixtures. To run tests from the command line run: (windows) java -cp /path/to/junit.jar;/path/to/yourTextClasses org.junit.runner.JUnitCore TestClass1 TestClass2 .... (Unix) java -cp /path/to/junit.jar:/path/to/yourTextClasses org.junit.runner.JUnitCore TestClass1 TestClass2 .... For one-shot test runs use the static method runClasses(Class[]). Make sure you have junit.jar in your classpath and the jar or classes of your external tests also in the the classpath. That way you can execute them from the command-line (which may not be what you are after) or directly within your java program. JUnitCore.runClasses(TestClass1TestClass2...) classpath parameter should be -> -cp ""1;2"" or -cp ""1:2"" @h3xStream: you are right. I have edited and fixed the answer."
216,A,"How to rollback a database transaction when testing services with Spring in JUnit? I have no problem testing my DAO and services but when I test INSERTs or UPDATEs I want to rollback the transaction and not effect my database. I'm using @Transactional inside my services to manage transactions. I want to know is it possible to know if a transaction will be fine but rollback it to prevent altering database? This is my Test: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = ""classpath:/META-INF/spring.cfg.xml"") @TransactionConfiguration(defaultRollback=true) public class MyServiceTest extends AbstractJUnit38SpringContextTests { @Autowired private MyService myService; @BeforeClass public static void setUpClass() throws Exception { } @AfterClass public static void tearDownClass() throws Exception { } @Test public void testInsert(){ long id = myService.addPerson( ""JUNIT"" ); assertNotNull( id ); if( id < 1 ){ fail(); } } } The problem is that this test will fail because transaction was rollbacked but the insert is OK! If I remove @TransactionConfiguration(defaultRollback=true) then the test pass but a new record will be inserted into database. @Test @Transactional @Rollback(true) public void testInsert(){ long id = myService.addPerson( ""JUNIT"" ); assertNotNull(id); if( id < 1 ){ fail(); } } Now can test pass correctly but rollback is ignored and the record is inserted into the database. I have annotated the method addPerson() inside myService with @Transactional obviously. Why is the rollback being ignored? good question +1 :-) If your myService.addPerson(""JUNIT""); method is annotated like @Transactional you will be getting some different kind or errors trying to fix this. So you better just test DAO methods.  Use Following annotation before class : @TransactionConfiguration(transactionManager = ""txManager""defaultRollback = true) @Transactional here txManager is application context's Transaction Manager. Here txManager is an instance or bean id of Transaction manager from application context. <!-- Transaction Manager --> <bean id=""txManager"" class=""org.springframework.orm.hibernate3.HibernateTransactionManager""> <property name=""sessionFactory"" ref=""sessionFactory"" /> </bean> <tx:annotation-driven transaction-manager=""txManager"" /> Add your code inside setUp() method this will execute in start of the test and the last wrap up code should be put in teatDown() method that will executed at last. or you can also use @Before and @After annotation instead of it.  You need to extend transaction boundaries to the boundaries of your test method. You can do it by annotating your test method (or the whole test class) as @Transactional: @Test @Transactional public void testInsert(){ long id=myService.addPerson(""JUNIT""); assertNotNull(id); if(id<1){ fail(); } } You can also use this approach to ensure that data was correctly written before rollback: @Autowired SessionFactory sf; @Test @Transactional public void testInsert(){ myService.addPerson(""JUNIT""); sf.getCurrentSession().flush(); sf.getCurrentSession().doWork( ... check database state ... ); } hi now test pass but rollback was ignored. I have @Transactional over ""testAddPerson"" and over ""addPerson"". If i remove @Transactional from myService there is not an active transaction for the test so i think @Transactional over ""testAddPerson"" doesn't work... @blow: I just noticed that you have `extends AbstractJUnit38SpringContextTests`. It's not needed since you have a JUnit 4 test with `@RunWith`. you are right. Without AbstractJUnit38SpringContextTests and AbstractTransactionalJUnit38SpringContextTests all works fine. Thank you. i also had to add @TestExecutionListeners({TransactionalTestExecutionListener.class})  check out http://static.springsource.org/spring/docs/2.5.x/reference/testing.html Section 8.3.4 in particular Spring has some classes for testing that will wrap each test in a transaction so the DB is not changed. You can change that functionality if you want too. Edit -- based on your more infos you might want to look at AbstractTransactionalJUnit38SpringContextTests at http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/test/context/junit38/AbstractTransactionalJUnit38SpringContextTests.html hi i had more infos. @blow -- i updated my answer. thank you this is useful i found a little problem in my implementation. Now i have another problem rollback is ignored"
217,A,How do I create an ant builder file (build.xml) for an existing Java project? I am working on an Java assignment for a software design course in my university. It's not really complicated and it includes some classes interfaces and jUnit test cases. We were now told we should supply a build.xml file as an input for an ant builder. I have never heard of or used ant before. I also saw Eclipse supports it. My questions are - What does build.xml do? How does Eclipse builds my project and why not do the same instead of using ant? And most important - how to create this file with Eclipse? Thanks. Would http://stackoverflow.com/questions/206473/build-eclipse-java-project-from-command-line/206587#206587 help? Ant is a build tool based on Java. It is very powerful and integrates with various other tools you might need to build any imaginable Java application. It is very useful when deploying a Continunous Integration environment with a Continuous Build server. You are building within Eclipse and your build process is bound to your IDE - which is perfectly fine for a one-man project but might prove unefficient for a team project. Also with Ant you can have a complex build including multiple components and intricate dependencies done in a single step including unit tests. You can either create your Ant build from scratch (which is the best way but it takes quite a while). Otherwise you can create it with Eclipse using the File/Export menu (under General select Ant buildfiles).  You should definitely know how to crate a build.xml file from scratch just so you know what Eclipse does behind the scenes. I taught a Java class a while ago where I explained the basics of ant.
218,A,"Can't run jUnit with Eclipse I use new Eclipse. Create demo test with jUnit (I added default jUnit library built-in Eclipse). Then I write this code: import junit.framework.*; import org.junit.Test; public class SimpleTest extends TestCase { public SimpleTest(String name) { super(name); } public final void main(String method){ } @Test public final void testSimpleTest() { int answer = 2; assertEquals((1+1) answer); } } But it doesn't run. In the Debug tab: org.eclipse.jdt.internal.junit.runner.RemoteTestRunner at localhost:52754 Thread [main] (Suspended (exception ClassNotFoundException)) URLClassLoader$1.run() line: not available [local variables unavailable] AccessController.doPrivileged(PrivilegedExceptionAction<T> AccessControlContext) line: not available [native method] Launcher$AppClassLoader(URLClassLoader).findClass(String) line: not available Launcher$AppClassLoader(ClassLoader).loadClass(String boolean) line: not available Launcher$AppClassLoader.loadClass(String boolean) line: not available Launcher$AppClassLoader(ClassLoader).loadClass(String) line: not available How can I solve this? You as many people have confused the JUnit 3 and JUnit 4. If you are using JUnit 3 name your tests ""test*"" and inherit from TestCase. If you are using JUnit 4 use annotations. Nice to know the difference between the two versions. How to do that? I write new code: import static org.junit.Assert.*; import org.junit.Test; public class SimpleTest { @Test public final void abcTest() { int answer = 2; assertEquals((1+1) answer); } } But it's still error I used default jUnit4 in Eclipse. When I press Alt+Shift+XT. It fired error: Could not find the main class: org.eclipse.jdt.internal.junit.runner.RemoteTestRunner. Program will exit @KimKha seems like something with classpath/classloader definitely not my thing... @KimKha that should run properly (tried it) can you give the exception? Make sure you have JUnit 4 in dependencies. @KimKha I also tried your original example and it works (although it is a v3 and v4 mix). I tried fixing it (removed TestCase supr class and super constructor call) and I only got problem with having a constructor but after deletion worked like a charm. Another thing which comes to my mind is if you are running it like a JUnit Test or Java application. In Eclipse you should run the test using Alt+Shift+XT . While this is true this is not the *root cause* of this problem. Copypaste yourself put a breakpoint on `ClassNotFoundException` and run in debug. You see exactly the same problem until breakpoint is removed.  Remove the breakpoints on Exceptions when running in debug mode or just run in non-debug mode. In the debug view on the right top box click the Breakpoints tab and uncheck any breakpoint on an Exception e.g. ClassNotFoundException and rerun the test. I have no breakpoint. And when I run in non-debug mode it says Could not find the main class: org.eclipse.jdt.internal.junit.runner.RemoteTestRunner. Program will exit. @BalusC Thanks (again). While it may not have helped KimKha that solved the debug-mode-only problem I was having."
219,A,How to tell a Mockito mock object to return something different the next time it is called? So I'm creating a mock object as a static variable on the class level like so... In one test I want Foo.someMethod() to return a certain value while in another test I want it to return a different value. The problem I'm having is that it seems I need to rebuild the mocks to get this to work correctly. I'd like to avoid rebuilding the mocks and just use the same objects in each test. class TestClass { private static Foo mockFoo; @BeforeClass public static void setUp() { mockFoo = mock(Foo.class); } @Test public void test1() { when(mockFoo.someMethod()).thenReturn(0); TestObject testObj = new TestObject(mockFoo); testObj.bar(); // calls mockFoo.someMethod() receiving 0 as the value } @Test public void test2() { when(mockFoo.someMethod()).thenReturn(1); TestObject testObj = new TestObject(mockFoo); testObj.bar(); // calls mockFoo.someMethod() STILL receiving 0 as the value instead of expected 1. } } In the second test I'm still receiving 0 as the value when testObj.bar() is called... What is the best way to resolve this? Note that I know I could use a different mock of Foo in each test however I have to chain multiple requests off of mockFoo meaning I'd have to do the chaining in each test. You could also Stub Consecutive Calls (#10 in 1.8.5 api). In this case you would use multiple ThenReturn statements. import static org.junit.Assert.assertEquals; import static org.mockito.Mockito.mock; import static org.mockito.Mockito.when; import org.junit.Before; import org.junit.Test; public class TestClass { private Foo mockFoo; @Before public void setup() { setupFoo(); } @Test public void testFoo() { TestObject testObj = new TestObject(mockFoo); assertEquals(0 testObj.bar()); assertEquals(1 testObj.bar()); assertEquals(-1 testObj.bar()); assertEquals(-1 testObj.bar()); } private void setupFoo() { mockFoo = mock(Foo.class); when(mockFoo.someMethod()) .thenReturn(0) .thenReturn(1) .thenReturn(-1); //any subsequent call will return -1 } } Nice. Didn't know that. This solved my question thanks Tony! I think you can also take advantage of the fact that .thenReturn() takes varargs so the code can be shortened to: when(mockFoo.someMethod()).thenReturn(0 1 -1); Thanks Justin! That will work as well.  First of all don't make the mock static. Make it a private field. Just put your setUp class in the @Before not @BeforeClass. It might be run a bunch but it's cheap. Secondly the way you have it right now is the correct way to get a mock to return something different depending on the test.
220,A,"How to unit test an SQL query? I have a class DBHandler which takes a query runs it through the SQL server checks for errors and returns the result. How can I unit test this class? Edit: I'll try to be more precise: DBHandler is in charge of passing the query to the server. In order to test that it actually does that throws the correct exceptions etc. I want to connect it to a mock DB which I will populate. My question is - how to do that? How can I create a mock ""server"" that handles calls? Can you be more specific? @duffymo this is a plan I haven't implemented it yet. My question is more precisely - how do I simulate an SQL server to return Queries etc.? Why simulate a SQL Server? You're testing a DBHandler - send the requests to the real database. No need to mock such a thing. @duffymo first of all I don't want to contaminate my database. Secondly I want to know exactly what information exists in the test. Contaminate your database? How does testing do that? testing of adding a record to the DB. You'll want to either use an in-memory test database that you create and populate on set-up or make all your tests transactional so they don't alter your test database. You'll have to worry about the presence of data. If you're using Spring they have support for transactional unit tests. It's not clear what you're asking. You already know about JUnit. What do you think you're missing?  Just pass a SQL query and compare the returned result to expected result. Simple. JUnit is a unit test framework you can utilise that. For sophisticated database unit testing look at DBUnit. Hi Adeel ok agreed so it might be a good idea to separate suite(s) that are real unit tests (no db tests) from functional tests (db test included). @Stivlo: A very valid idea indeed. Not simple the expected result will probably depend on the database contents. If you're really taking this path you should have a development only database and put it in a known state before starting the tests. The problem with this approach is that the tests are going to take a long time for any non trivial problem. @stivlo: Usually we don't take a development database but we have a test database. Yes running a full DBUnit suite might take a long time but that is acceptable. Running a couple of tests during coding don't take much time. Thanks for mentioning DBUnit. It can't solve my problem but I think it can be useful in future projects)  I'd use dependency injection to pass in the database connection or something similar so that the whole thing can be mocked out in the tests. Then you can write tests where the mock query throws exceptions returns various errors or valid results. Then your tests are just checking that DBHandler performs correctly.  As Adeel said use DBUnit for running your tests. Work against HSQLDB during testing.  A quick solution for a mock db works like this: Setup an HSQLDB test server independent from your app. Populate with test data. Use conditional code where you connect to your real database to connect to the test server. A property in your application can control this. Test the application"
221,A,How do I make a WAR archive produced with maven-assembly-plugin available for other projects' unit tests? My problem consists of two maven projects a server project A and a client project B. A uses maven-assembly-plugin to produce several variants of A where one variant is a WAR archive. The problem I am facing relates to the test-driven development of B; how can I make the WAR archive produced in project A accessible/addressable from unit tests in project B? My idea is to construct test cases in project B where the WAR archive is deployed in an embedded Jetty server through the WebApppContext's setWar(String path) function. You can declare an artifact from the other submodule as a test dependency e.g.: <dependencies> <dependency> <groupId>${project.groupId}</groupId> <artifactId>ModuleA</artifactId> <version>${project.version}</version> <type>test-jar</type> <scope>test</scope> </dependency> </dependencies> In this way you can surely use jars from the other module. I am not sure if it works for a WAR file though - but it may be worth a try. Note that tests run against a WAR deployed in an embedded web container would hardly count as unit tests rather integration tests. The approach that works in out project is: build the web app (in our case an EAR) and deploy it (we are using the JBoss Maven plugin but it could be e.g. Cargo or others in your case) run a separate build (with a specific timeout to allow the server to start up) which executes the integration tests against the deployed web app Thank you for your answer I managed to get it working by using the -tag in the dependency declaration and adjusting the maven-dependency-plugin a bit.
222,A,"How to test whether a char is NOT in a string? (java junit) As title says im having trouble with my junit tests passing for checking if a character is not in a string and how to check if an empty string doesnt have a character. here is the method i have:  public static boolean isThere(String s char value){ for(int x = 0; x <= s.length(); x++){ if(s.charAt(x) == value){ return true; } else if(s.length() == 0){ return false; } } return false; And here is the junit test:  public void testIsThere() { { String sVal = ""Jeff George""; boolean hasA = StringMethods.isThere(sVal'e'); assertTrue(hasA); boolean hasE = StringMethods.isThere(sVal 'o'); assertTrue(hasE); boolean notIn = StringMethods.isThere(sVal'b'); assertTrue(notIn); } { String sVal = """"; boolean nothingIn = StringMethods.isThere(sVal'a'); assertFalse(nothingIn); boolean notIn = StringMethods.isThere(sVal'b'); assertFalse(notIn); } } Thank you very much appreciated a better name this isThere() would be containsChar() Some better formatted code and a clearer explanation of what exactly it is that isn't working would help us help you. I think you simply want to `assertFalse(notIn)` What exactly are you trying to achieve here? You are just duplicating functionality that already exists. it should be assertFalse you are correct that is a typing error on my part thank you Some of the questions on SO these days make me want to cry :'( With Java 6 you can just do final String s = ""This is a test""; s.contains(""x""); // False s.contains(""t""); // True Two flaws: 1) It's actually Java 1.5. 2) It doesn't compile. It takes a `CharSequence`: http://java.sun.com/javase/6/docs/api/java/lang/String.html#contains%28java.lang.CharSequence%29  Use String.indexOf() instead: public static boolean contains(String s char value){ return s != null && s.indexOf(value) > -1; } String sVal = ""Jeff George""; assertTrue(contains(sVal 'e')); sVal = null; assertFalse(contains(sVal 'e')); this worked brilliantly there is no way i could have seen that. return if s doesnt equal null AND that the index of the value is greater than -1 meaning that it is there and the string isnt empty. thank you very much!  What problem are you encountering ? Firstly  for(int x = 0; x <= s.length(); x++){ doesn't look right. x is going to run off the end of your string (use x < s.length() instead if you want to iterate through a string). But higher level functions are available for doing what you want (see the other answers here).  Why are you doing this? Your function is already implemented as a method on String. Use String.indexOf instead: s.indexOf('a') == -1 I think Carl Manaster was right in the comments about your specific problem - you need to use assertFalse not assertTrue here: String sVal = ""Jeff George""; boolean notIn = StringMethods.isThere(sVal 'b'); assertFalse(notIn); // not assertTrue As an aside notIn is a terrible name for that variable - it means exactly the opposite of what it says. Maybe that is why you got confused. it should be assertFalse you are correct that is a typing error on my part thank you  Or try StringUtils.contains() from apache commons - that will handle the null String case for you as well. http://commons.apache.org/lang/api/org/apache/commons/lang/StringUtils.html#contains%28java.lang.String%20char%29 apache commons is a poorly designed and implemented code cancer Besides using a 3rd party library for doing such elementary things is IMO sheer madness. I mean if you're going to stuff a jar in your app just to compare a String against `null` and do a `contains(...)` on it I wonder how many 3rd party jars will be there when the app gets shipped! :) +1 because a down-vote seems awful harsh. Apache commons is a widely accepted and used library. Is it way-overkill for just this one problem? Certainly. Has Java 1.5 and 1.6 made many of the functions in commons redundant? Yup. But sheesh...  If String.indexOf(char) returns -1 hasA is false. Otherwise it's true."
223,A,"How do I run JUnit from NetBeans? I've been trying to understand how to start writing and running JUnit tests. When I'm reading this article: http://junit.sourceforge.net/doc/testinfected/testing.htm I get the the middle of the page and they write ""JUnit comes with a graphical interface to run tests. Type the name of your test class in the field at the top of the window. Press the Run button."" I don't know how to launch this program. I don't even know which package it is in or how you run a library class from an IDE. Being stuck I tried this NetBeans tutorial: http://www.netbeans.org/kb/docs/java/junit-intro.html It seemed to be going OK but then I noticed that the menu options for this tutorial for testing a Java Class Library are different from those for a regular Java application or for a Java Web App. So the instructions in this tutorial don't apply generally. I'm using NetBeans 6.7 and I've imported JUnit 4.5 into the libraries folder. What would be the normal way to run JUnit after having written the tests? The JUnit FAQ describes the process from the Console and I'm willing to do that if that is what is typical but given all that I can do inside netbeans it seems hard to believe that there isn't an easier way. Thanks much. EDIT: If I right-click on the project and select ""Test"" the output is: init: deps-jar: compile: compile-test: test-report: test: BUILD SUCCESSFUL (total time: 0 seconds) This doesn't strike me as the desired output of a test especially since this doesn't change whether the test condition is true or not. Any ideas? I had the same issue after imported a eclipse project into NetBeans. To resolve it I followed the above steps outlined by alangalloway but instead of creating a new folder I just pointed to the imported test folder. Maybe in future release NetBeans can automatically recognize imported test cases. Thanks.  Had a similar issue. In Netbeans 7.0.1 what worked for me was to locate the project.xml file (i.e. {project}/nbproject/project.xml) and change:  <test-roots/> to:  <test-roots> <root id=""src.dir""/> </test-roots> (in my case the test files are in the same dir as the source dir)  Re-importing does not appear to be necessary. I had the same issue (imported project right clicking did not bring up any JUnit test options). I took these steps which resolved it using NetBeans 6.8: Add a folder called ""tests"" to your project. Right-click your project and select Properties. Select Sources. Under Test Package Folders click the Add Folder button and select the ""tests"" folder. Right clicking a file + Tools > Create JUnit Tests. Once a test is created right-clicking a file + Test File runs the test.  Even though I've accepted an answer I thought I should mention my difficulty as someone else may encounter it. When importing a project from existing sources into NetBeans if you do not specify a folder for test packages then NetBeans will not offer the JUnit options on the tools menu. The only solution I found was to re-import the project. While primitive it worked.  One way is to right click on your project in the Projects pane and select ""Tests"". That will run the JUnit tests. You can also right click on the test file and select ""Run Test"" and that single file will be ran. The keyboard shortcuts depends on how you have your keymapping set but you'll see them in the context menus. You can also have NetBeans autogenerate tests for you by right clicking your source file and then ""Tools > Create JUnit Tests"". Hmm . . . Something is wrong. If I right click on a source file and select tools there is no JUnit option. If I right click on a test file ""Run Test"" is not an option. If I right click on the project and select ""Test"" it indicates that it is running a test but gives no meaningful information. I'll edit the question to specify this behavior. +1 for the comment about being able to right click on a test file and run it that way. When I'm writing new test cases I don't want to re-run my whole test suite every time (e.g. for the unrelated classes I'm not changing)"
224,A,"Different JUnit-result starting from Eclipse vs. mvn test I've got a JUnit-test which is successfull when starting with mvn test but fails when starting from Eclipse (see stack trace below). What I'm trying to do is inserting new elements into the database with entityManager.persist() and entityManager.flush() (when calling flush I get the error). My config.properties looks the following: db.url=jdbc:derby:target/testdb;create=true;territory=en_US;collation=TERRITORY_BASED db.username= db.password= # Hibernate hibernate.show_sql=true hibernate.hbm2ddl.auto=create With that configuration my test fails in Eclipse but runs with ""mvn test""; if I remove the line ""hibernate.hbm2ddl.auto=create"" the test is successful from both ""mvn test"" and Eclipse. When running the application it works; it really only fails when running the JUnit-Test with ""hibernate.hbm2ddl.auto=create"" enabled. In my Test-class there's a setup-Method looking the following:  @Before @Transactional(propagation = Propagation.REQUIRED readOnly = false) public void setUpDatabase() { final Resource deleteScript = applicationContext.getResource(""delete.sql""); final Resource insertScript = applicationContext.getResource(""insert.sql""); SimpleJdbcTestUtils.executeSqlScript(simpleJdbcTemplate deleteScript true); SimpleJdbcTestUtils.executeSqlScript(simpleJdbcTemplate insertScript false); } delete.sql contains ""delete from"" statements insert.sql inserts the statments again. Java version 1.6.0_16 maven 2.1.0. Any ideas? Thanks a lot Stefan javax.persistence.PersistenceException: org.hibernate.exception.ConstraintViolationException: could not insert: [ch.netcetera.gisab.masterdata.model.security.RuleTarget] at org.hibernate.ejb.AbstractEntityManagerImpl.throwPersistenceException(AbstractEntityManagerImpl.java:614) at org.hibernate.ejb.AbstractEntityManagerImpl.flush(AbstractEntityManagerImpl.java:307) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.springframework.orm.jpa.SharedEntityManagerCreator$SharedEntityManagerInvocationHandler.invoke(SharedEntityManagerCreator.java:198) at $Proxy47.flush(Unknown Source) at ch.netcetera.gisab.masterdata.dao.RuleDAOImpl.updateRuleTargetsOfRule(RuleDAOImpl.java:84) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:307) at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149) at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171) at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204) at $Proxy72.updateRuleTargetsOfRule(Unknown Source) at ch.netcetera.gisab.masterdata.services.RoleServiceImpl.updateUserRoleTO(RoleServiceImpl.java:145) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:307) at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149) at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171) at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204) at $Proxy74.updateUserRoleTO(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:307) at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149) at org.springframework.aop.interceptor.CustomizableTraceInterceptor.invokeUnderTrace(CustomizableTraceInterceptor.java:255) at org.springframework.aop.interceptor.AbstractTraceInterceptor.invoke(AbstractTraceInterceptor.java:110) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171) at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204) at $Proxy74.updateUserRoleTO(Unknown Source) at ch.netcetera.gisab.masterdata.services.RoleServiceTest.testUpdateUserRoleTO(RoleServiceTest.java:140) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.springframework.test.context.junit4.SpringTestMethod.invoke(SpringTestMethod.java:160) at org.springframework.test.context.junit4.SpringMethodRoadie.runTestMethod(SpringMethodRoadie.java:233) at org.springframework.test.context.junit4.SpringMethodRoadie$RunBeforesThenTestThenAfters.run(SpringMethodRoadie.java:333) at org.springframework.test.context.junit4.SpringMethodRoadie.runWithRepetitions(SpringMethodRoadie.java:217) at org.springframework.test.context.junit4.SpringMethodRoadie.runTest(SpringMethodRoadie.java:197) at org.springframework.test.context.junit4.SpringMethodRoadie.run(SpringMethodRoadie.java:143) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.invokeTestMethod(SpringJUnit4ClassRunner.java:160) at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51) at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44) at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27) at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37) at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:97) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: org.hibernate.exception.ConstraintViolationException: could not insert: [ch.netcetera.gisab.masterdata.model.security.RuleTarget] at org.hibernate.exception.SQLStateConverter.convert(SQLStateConverter.java:94) at org.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:66) at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:2295) at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:2688) at org.hibernate.action.EntityInsertAction.execute(EntityInsertAction.java:79) at org.hibernate.engine.ActionQueue.execute(ActionQueue.java:279) at org.hibernate.engine.ActionQueue.executeActions(ActionQueue.java:263) at org.hibernate.engine.ActionQueue.executeActions(ActionQueue.java:167) at org.hibernate.event.def.AbstractFlushingEventListener.performExecutions(AbstractFlushingEventListener.java:321) at org.hibernate.event.def.DefaultFlushEventListener.onFlush(DefaultFlushEventListener.java:50) at org.hibernate.impl.SessionImpl.flush(SessionImpl.java:1027) at org.hibernate.ejb.AbstractEntityManagerImpl.flush(AbstractEntityManagerImpl.java:304) ... 66 more Caused by: java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL091001075326220' defined on 'RULETARGET'. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source) at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source) at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source) at org.apache.commons.dbcp.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:102) at org.hibernate.jdbc.NonBatchingBatcher.addToBatch(NonBatchingBatcher.java:46) at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:2275) ... 75 more Caused by: java.sql.SQLException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL091001075326220' defined on 'RULETARGET'. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 87 more Caused by: ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL091001075326220' defined on 'RULETARGET'. at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source) at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source) at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source) at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source) at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source) at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source) at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source) at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ... 81 more Edit: I forgot: no error during startup but during the failing test method I get the following warning in the log: 2009-10-01 10:16:38924 | main | WARN | JDBCExceptionReporter | SQL Warning: 10000 SQLState: 01J01 2009-10-01 10:16:38924 | main | WARN | JDBCExceptionReporter | Database 'target/testdb' not created connection made to existing database instead. Hibernate: insert into RuleTarget (ruleid targetid type version id) values (? ? ? ? ?) 2009-10-01 10:16:38939 | main | WARN | JDBCExceptionReporter | SQL Error: 20000 SQLState: 23505 2009-10-01 10:16:38939 | main | ERROR | JDBCExceptionReporter | The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL091001101631840' defined on 'RULETARGET'. 2009-10-01 10:16:38939 | main | ERROR | AbstractFlushingEventListener | Could not synchronize database state with session org.hibernate.exception.ConstraintViolationException: could not insert: [ch.netcetera.gisab.masterdata.model.security.RuleTarget] Does it crash while executing the scripts during startup? no... but there's a warning I forgot to mention... (I added it to the original posting) Based on the SQLIntegrityConstraintViolationException it looks like your delete statements are not being executed do you perhaps have two delete.sql files or not have src/test/resources configured as a source location in Eclipse? The applicationContext.getResource(""delete.sql"") call will expect to find delete.sql in the output directory. In a Maven build the contents of src/test/resources will be copied to target/test-classes this will only be done in an Eclipse build if the source location is defined. The true at the end of the first executeSqlScript call means the process will continue without throwing an exception in the event of an error so it may be that the resource is not found and the delete statements not executed. If you run the build with the option set to false (and existing content) does it fail? Note you can configure the Eclipse .classpath file to output test content to target/test-classes to avoid conflicts. The steps to do this are: Open the project properties (alt-enter) Select**Java Build Path** Select the Source tab Click on Allow output folders for source folders. This adds a new entry to the source folder tree. Expand the source folder tree. Double-click on the Output folder: entry. A dialog asks you if you want to use the project's default output folder or a specific output folder. Choose the second option and click Browse... Select the folder you want and click OK and then Finish. You should end up with the following entries in the .classpath file: <classpathentry kind=""src"" output=""target/test-classes"" path=""src/test/java""/> <classpathentry kind=""src"" output=""target/test-classes"" path=""src/test/resources""/> Thanks a lot for that information! I checked - there's only one delete.sql-file and while debugging it becomes clear that it's really found. Dude! Thanks very much for this!  What I've noticed that sometimes my unit test outputs change depending on the level of sandboxing done by the used testing tool. For example Eclipse seems to sandbox current selection(class package project) to a single setup-test-teardown cycle in same context while our CI seems to sandox every single unit test as setup-test-teardown cycle. What this means is that if your test1 has a side effect which is then used in test2 it may work in Eclipse but it also may fail elsewhere. If this is the case you need to rewrite your unit tests at least partly to decouple them from each other. Thanks a lot for your answer... I thought about something like that; but if I run only that specific method out of Eclipse it doesn't work either...  My remarks/question: when you remove line ""hibernate.hbm2ddl.auto"" - the default action is none what base test class you use? Try extends class org.springframework.test.jpa.AbstractJpaTests - it does rollback after each method from exception stack trace seems to tables aren't clean before inserts where your database (file) is stored? Maybe maven build clean database file. do you use any maven plugins that are run in maven and not in eclipse (like hibernate3-maven-plugin)? EDIT: In my opinion test should be run in clean database and leave database clean so each test/method should rollback after execution. Look at my qestion and responses. I think that using hibernate3-maven-plugin is optional. - my Test class extends AbstractTransactionalJUnit4SpringContextTests - database file is stored locally; maven clean didn't help... - most interesting: yes hibernate3-maven is used!"
225,A,"verify object type with easymock i've just come into the world of easymock.i'll like to ask if easymock only does mock object for interfaces? So in my effort to understand i wrote a class to generate unique voucher in java.i obviously can't know which value it will generate to use in the assert stuff.So how to make sure the generated voucher is of the type long? here is the function  public static Long generateID(int length) { logger.info(""Calling generateID with specify length""); Long result = null; if (length > 0) { StringBuffer id = new StringBuffer(length); for (int i = 0; i < length; i++) { id.append(NUMS[(int)Math.floor(Math.random() * 20)]); } result = Long.parseLong(id.toString()); } return result; } here is the test class @Before public void setUp() { mockgenerator = createMock(VGenerator.class); } /** * Test of generateID method of class VGenerator. */ @Test public void testGenerateID() { Long exp = (long)1; int length = 15; expect(mockgenerator.generateID(length)).equals(Long.class); replay(mockgenerator); long res = mockgenerator.generatedID(length); assertEquals(exp.class res.class); } well this might look terrific to you but i'm still confused about how to do this thanks for helping If generateID() attempted to return something that wasn't a long the class wouldn't even compile. EasyMock Class extension can mock classes. It is an extension to EasyMock. It still can mock interface so it's pretty much a replacement to EasyMock. However in your case you are trying to mock a static method. Static method can't be mocked since they can't be overloaded. You need class instrumentation to do so which EasyMock doesn't do.  If it is absolutely crucial that the return type is long and you want to make sure that future changes don't inadvertently change this then you don't need easymock. Just do this: @Test public void TestGenerateIDReturnsLong() { Method method = VGenerator.class.getDeclaredMethod(""generateID"" new Class[0]); Assert.Equals(long.Class method.GetReturnType()); } Currently you are generating a mock implementation of VGenerator and then you test the mock. This is not useful. The point of unit testing is to test a real implementation. So now you might be wondering what mocks are good for? As an example imagine that VGenerator needs to use a random number generator internally and you provide this in the constructor (which is called ""dependency injection""): public VGenerator { private final RandomNumberGenerator rng; // constructor public VGenerator(RandomNumberGenerator rng) { this.rng = rng; } public long generateID(length) { double randomNumber = this.rng.getRandomNumber(); // ... use random number in calculation somehow ... return id; } } When implementing VGenerator you are not really interested in testing the random number generator. What you are interested in is how VGenerator calls the random number generator and how it uses the results to produce output. What you want is to take full control of the random number generator for the purpose of testing so you create a mock of it : @Test public void TestGenerateId() { RandomNumberGenerator mockRNG = createMock(RandomNumberGenerator.class); expect(mockRNG.getRandomNumber()).andReturn(0.123); replay(mockRNG); VGenerator vgenerator = new VGenerator(mockRNG); long id = vgenerator.generateID(); Assert.Equals(5id); // e.g. given random number .123 result should be 5 verify(mockRNG); } i appreciate the explanation.thanks  I believe you misunderstand how easymock is used Calling expect tells the mock object that when you are replaying it this call should be called. Appending .andReturn() Tells the mock object to return whatever you put in there in my example a long value of 1. The point of easymock is that you do not need to implement the mocked interface to test the classses that use it. By mocking you can isolate a class from the classes it depends on and only test the contained code of the class your are currently testing. interface VGenerator { public Long generateID(int in); } @Before public void setUp() { mockgenerator = createMock(VGenerator.class); } @Test public void testGenerateID() { int length = 15; expect(mockgenerator.generateID(length)).andReturn(new Long(1)); replay(mockgenerator); myMethodToBeTested(); verify(mockgenerator); } public void myMethodToBeTested(){ //do stuff long res = mockgenerator.generatedID(length); //do stuff } If I misunderstood your question and it was really does easymock only mock interfaces? then the answer is Yes Easymock only mocks interfaces. Read the documentation for more help Easymock very good explanation.I like it.thanks"
226,A,"HowTo Unit Test Client Server Code I'm currently writing a Java Client Server Application. So i want to implement two Libraries one for the Client and one for the Server. The Client Server Communication has a very strict protocol that I wan't to test with JUnit. As build tool im using Maven and a Husdon Server for continues Integration. Actually I do not have any good Idea how to test these Client / Server Libraries. I got following Approaches: Just write a Dummy Client for testing the server and write a Dummy Server to test the Client. Disadvantages: Unfortunately this will result in many extra work. I could not be 100% sure that client and Server could work together because I'm not sure that the Tests are completely identical. Write an separate Test Project that Tests the Client and the Server together. Disadvantages: The Unit Tests does not belong to the Project it self so Hudson will not run them automatically. Everyone who changes anything at one of these Libraries will have to run the Tests manually to ensure everything is correct. Also i will not receive any Code Coverage Report. Are there any better approaches to test codes like that? Maybe test a Maven Multi Module Project or something like that. I hope any one got a good solution for that Issue. Thanks. You may use any mock object framework to create mock objects - Try jmockit.  Think of all your code as ""transforms input to output"": X -> [A] -> Y X is the data that goes in [A] is the transformer Y is the output. In your case you have this setup: [Client] -> X -> [Server] -> Y -> [Client] So the unit tests work like this: You need a test that runs the client code to generate X. Verify that the code actually produces X with an assert. X should be a final static String in the code. Use the constant X in a second test to call the server code which transforms it into Y (another constant). A third test makes sure that the client code can parse the input Y This way you can keep the tests independent and still make sure that the important parts work: The interface between the components. Hmm two projects because the Client has to go to Low resource Devices. And the Test Project is the second approach i already listed ahead. And i also written the disadvantages of this approach. You can configure Hudson to run the test project every time the client or the server project is built (see ""Build after other projects are built"" in the ""Build Triggers"" section). As for two projects: Put the client and server into two maven modules and the tests in a third. That way you can build everything from the root folder. Hmm this seams to be exactly the way to implement a separate unit test for the Server and the Client. In fact this will result in Copy paste the Constants into both projects and so implementing a ""Dummy Client"" / ""Dummy Server"". That is the first approach i listed above. Why do you put client and server into different projects? *puzzled* Anyway if you really want to do that then create a third unit test project which imports the other two so you need the constants only once. @AmoghTalpallikar: No. You always test the input of something and the output together. It doesn't make sense to test only input - what do you do with the output? Ignore it? And how do you expect to get output when you don't give input? As for anonymous functions: You change the code until you can test it. @AaronDigulla: So Ideally code that processes server output and code that sends input to server component should be in separate methods so that it can be tested ? in languages where we pass anonymous functions as completion handlers how do we test that code ?  So finally the resolution was to build a Multi Module Project with a separate Test Module that includes the Server and the Client Module Works great in Husdon. And even better in the Eclipse IDE. Thanks @ Aaron for the hint  My suggestion would be to use two levels of testing: For your client/server project include some mocking in your unit tests to ensure the object interfaces are working as expected. Following the build have a more extensive integration test run with automation to install the compiled client and server on one or more test systems. Then you can ensure that all the particulars of the protocol are tested thoroughly. Have this integration test project triggered on each successful build of the client/server project. You can use JUnit for this and still receive the conventional report from Hudson."
227,A,"JUnit Testing with Object Fixture Hey guys I'm trying to write a JUnit test to test something but it's just not clicking. I'm supposed to design a jUnit tester that is testing a class but uses another class that implements the original class as a test fixture. I'm supposed to make a generic circular array that has some methods for certain things such as adding to the front of the array and back etc. I'm not quite sure I've implemented the Array casting correctly as you can't cast a generic type array but we've been asked to use a bounded wildcard so I think the way I implemented it is okay....here it is sans comments public class Array12<E> implements LimCapList<E> { private int maxSize; private int first; private int last; private int size; private E[] A12; @SuppressWarnings(""rawtypes"") public Array12(Class <? extends E> clazz int capacity) { this.maxSize = capacity; this.size = 0; this.first = 0; this.last = 0; @SuppressWarnings({ ""unchecked"" ""unused"" }) Array12 A12 = new Array12(clazz capacity); } Now what I want the function to do is to create a circular Array of type clazz with size capacity. Did I implement it correctly? The reason why I ask is because when I try to create the jUnit tester I run into a wall and am pretty stuck on what I need to do to get it going. Here is what I've got so far for the jUnit tester.... public class LimCapListTester extends junit.framework.TestCase { private Array12 array12; protected void setUP() { array12 = new Array12(Class<String> 0); } protected void tearDown() { array12 = null; } Problem is array12 = new Array12(Class 0); Doesn't seem to be working correctly. And I'm not sure if I'm just using the wrong syntax in the jUnit tester or if I wrote my Array12 incorrectly. Any hints on how to fix it? DISCLAIMER This is for a homework assignment so I'm not looking for a solution but rather a hint on where I made a coding error and maybe some more insight on how to write jUnit testers with a different test fixture as all I've had experience with so far is writing a jUnit test for a specific class. For example I wrote a List12.java that implements LinkedLists and wrote a List12Tester.java that worked fine. However in this assignment I need to write a SomeTester.java that tests SomeCode.class but uses Array12 which implements SomeCode.class as a test fixture. I hope I've explained it as best as I can as I'm really confused and I do plan to ask my TAs for help but I figure maybe someone could help me out so I don't look TOO stupid when asking my TA in case the answer is really obvious. :) Thanks guys! Hint: check the signatures of the methods of junit.framework.TestCase. When overriding a method in a super class it's best to use @Override @Override protected void tearDown() { array12 = null; } If tearDown() wasn't a method in a super class then the compiler would complain."
228,A,"JUnit Testing for Automated Testing Scripts? I recently joined this organisation that I am currently working at that has asked me to manage a project to re factor extend and maintain an existing Automated Testing Framework written in java which uses a keyword driven framework and RFT. I have been a developer at heart all my life even though I have moved to Agile management. By habit I write unit tests to test for behavior before writing source code. This framework does not have one unit test. My first instinct was ""where are the unit tests?"" I know I can write unit tests for the testing framework classes. During a discussion here it was brought up that writing unit tests for testing frameworks or scripts may be a waste of time. I diplomatically disagreed. Question 1: Could my instinct be wrong? Do you have any suggestions which can be helpful to fight my case. Question 2: And could this get recursive? Writing tests for tests and tests and so on. Is there rule for when to stop writing unit tests? Is there a concept of testing the tester recursion? Again I am all for unit tests but never been in this kind of situation before. I could not find much on this topic from my research. EDIT Thank you all for your interesting responses! The unit tests will definitely be written without doubt! Highest priority will be given to our self written framework classes and methods that are used most often and would have high ROI and high Penalty for failure. The plan is to gradually and incrementally achieve a high level of code coverage for the whole project (java) If your organization wrote this testing framework then you should unit test it. If you use an existing testing framework (JUnit for example) then I wouldn't unit test that. Leave that testing to the creator of the testing framework. Bottom line: you wrote it you test it. That is a good and simple way to look at it it helped me explain my point. Thanks!  There was a very good interview with Kent Beck on Software Engineering radio where he explained his philosophy on when to write tests when to do TDD etc. He said that when he writes a exploratory bit of code code that won't be shared or code that won't last a spike solution he doesn't write tests. Pretty much the rest of the time he writes tests. To answer your question ask yourself a question. Would it help you to refactor extend maintain this framework if you had tests? If yes write the tests. 1) Usually good practice recommends that you write tests before refactoring (to make sure that behaviour doesn't change) or before doing new code (standard TDD). 2) Yes this can get recursive but you only have so much time in the day so you have to think about the effort you are expending for the extra value you're bringing to the project. Writing unit tests can help you understand the existing code better as well. Personally I would be writing tests. Interesting point about recursion. Can you elaborate how it can get recursive? I see some have disagreed. Thanks  Another thing to consider is that the automated tests themselves are testing the automated testing framework. That is if you break the framework the tests themselves ought to fail. In light of that it may not make sense to invest in writing automated unit tests of the framework itself.  To test something you need a reference something to compare the results to. For the framework or the script the production code can be used as reference - unless the versions are not compatible: if all tests pass with framework N and with framework N+1 then there is no [visible] regression will all due restrictions (provided enough coverage...). That's where writing unit tests for testing frameworks or scripts may be considered a waste of time. The existing frameworks probably works in most cases so spending time to put it under unit tests can be a waste. As with any piece of software writing unit tests when you add new features or when you rework some parts of the code will be helpful. I usually don't write unit tests for my test programs or only for specific parts where automated tests are valuable. I grow them along with the production code using each one as a scaffolding for the other.  Question 1: Could my instinct be wrong? Do you have any suggestions which can be helpful to fight my case. Your instincts are not wrong. If your testing framework has a bug it might miss errors by skipping tests for example. Question 2: And could this get recursive? Writing tests for tests and tests and so on. Is there rule for when to stop writing unit tests? Is there a concept of testing the tester recursion? No. Because test cases are supposed to be so simple that bugs can't survive scrutiny. Every test should be trivial or the class being tested needs refactoring. Obviously sometimes this is easier said than done. I'd examine the test framework and add tests wherever a failure would really hurt. Agreed. Thanks for a comprehensive answer. The places where a failure would really hurt would be the highest priority places to refactor and write tests but eventually it is a good idea I think to hit a high level of overall code coverage."
229,A,"Junit 4 test suite and individual test classes I have a JUnit 4 test suite with BeforeClass and AfterClass methods that make a setup/teardown for the following test classes. What I need is to run the test classes also by them selves but for that I need a setup/teardown scenario (BeforeClass and AfterClass or something like that) for each test class. The thing is that when I run the suite I do not want to execute the setup/teardown before and after each test class I only want to execute the setup/teardown from the test suite (once). Is it possible ? Thanks in advance. If you have jUnit 4.7+ I recommend looking into the new feature called Rules (which are explained in this blog post). They might not be exactly what you want but they are probably the best you get with jUnit. Supposedly TestNG has better test grouping possibilities but I haven't really looked into it myself yet.  I don't know of any standard way to do this with JUnit. The reason for it as you probably already know is that your test cases should run independently of each other. This concerns the ""normal"" setup/teardown methods which run before and after each test method. Class setup and teardown is a bit different though - although I would still prefer running my tests independently and staying out of the trouble zone. However if you really are convinced of what you are doing you could use a global flag to signal whether or not the class setup/teardown is to run and to check for its state in the class setup/teardown methods. In your test suite you could include a special class as the very first one which does nothing more than execute the setup and set the global flag to indicate to the real test cases that their class setup/teardown methods must not be run. Similarly a special last class in the suite can execute the teardown code. The caveat is that I am afraid JUnit does not guarantee the order of execution of test classes inside a suite although most probably it does execute them in the specified order - but this is just an implementation detail. Try it out it may work for you - but there is no guarantee it will always do what you expect. Thanks for the answer i actually thought of that but i thought maybe there was a better solution. The reason for What i want to do is because the setup cleans the environment and resets the database so it takes some time and that is why i don't want to run the setup for each test class but i need to when i run an individual test.  No there's no standard way to do this in JUnit though you could hack something up as Péter Török suggested. Note however that you are more or less abusing JUnit in doing this. The whole point of unit tests it that they are independent of each other. This is because dependencies between tests create a total maintenance nightmare (tests failing because the run in the wrong order). So I'd advise you to strongly consider if it's not better to just always run the setup... Thanks id did consider that but the setup has a much longer duration than the hole test with all the test classes :) I feel your pain :-(. If actually had to do this as well for tests with complicated setup (though it was only per class). Some kind of ""I'm already initialized"" flag is probably your best bet."
230,A,"How can I have the Ant JUnit task run all tests and then stop the rest of the build if any test has failed I'm running JUnit via Ant using a target something like this: <target name=""junit"" depends=""compile""> <mkdir dir=""${report.dir}""/> <junit printsummary=""yes"" haltonfailure=""yes"" showoutput=""yes"" > <classpath> <path refid=""classpath""/> <path location=""${classes.dir}""/> </classpath> <formatter type=""brief"" usefile=""false""/> <formatter type=""xml""/> <batchtest fork=""yes"" todir=""${report.dir}""> <fileset dir=""${src.dir}"" includes=""**/*Tests.java"" /> </batchtest> </junit> </target> I have a class like this: public class UserTests extends TestCase { public void testWillAlwaysFail() { fail(""An error message""); } public void testWillAlwaysFail2() { fail(""An error message2""); } } haltonfailure=""yes"" seems to cause the build to halt as soon as any single test has failed logging only the first failed test. Setting it to ""off"" causes the entire build to succeed (even though test failure messages are written to the output). What I want it for all tests to be run (even if one has failed) and then the build to be stopped if any tests have failed. Is it possible to do this? Another option would be to setup a CI environment like the trusty Jenkins and create jobs for the targets and connect the jobs by down-streaming. If one of the jobs fails the downstream doesn't happen and the whole operation is stopped  You can set the failureproperty attribute of the junit task then test the property afterwards with the fail task: <junit haltonfailure=""no"" failureproperty=""test.failed"" ... > ... </junit> <fail message=""Test failure detected check test results."" if=""test.failed"" /> That makes sense. Thanks."
231,A,m2eclipse filtering test resources I am using m2eclipse and I want to right click and run tests from inside eclipse while the test resources get filtered from Maven. How can I do this? From eclipse when I right click on a test I do not get any m2eclipse options Julia. Similar to: http://stackoverflow.com/questions/2855706/debugging-maven-junit-tests-with-filtered-resources/2856104#2856104 Setup filtering in your pom.xml (and you'll get filtering under Eclipse with m2eclipse m2eclipse runs process-resources after a change of any resource). Run your test as any other test (right-click then Run As > JUnit Test). After a first run you can run an individual test method (right-click on the method in the JUnit view). I have set filtering as true for both process resources as well as process test resources the problem is the values for the placeholders are in profiles.xml which m2eclipse is not able to pick up but when I run it from command line it works. @Julia Oh you're using a `profiles.xml` file. I don't recommend it and this `profiles.xml` is actually deprecated. Move the profiles into your pom.xml (or settings.xml). Ok will do. Thanks. Just wondering if there is any workaround?  Right click on the project -> Run As -> Maven test I want to run a specific method in a TestCase
232,A,"java.lang.ClassCastException: $Proxy96 cannot be cast to ticket.app.DatesFacade I am entering some code into a Hudson Server run on a Ubuntu box in order to run some Code Metrics(Sonar) and Cobertura on the code. The project runs off of Glassfish3.1 uses Maven3 written in Java JSF 2.0 and uses OracleXE Database(irrelevant). The error is being thrown when trying to create an instance of an Facade inside of my JUnit tests. The tests run fine when I am running them from Netbeans but when Hudson does the automatic build on it I get this error:  java.lang.ClassCastException: $Proxy96 cannot be cast to ticket.app.DatesFacade at ticket.app.EventsControllerTest.setUp(EventsControllerTest.java:60) at junit.framework.TestCase.runBare(TestCase.java:128) at junit.framework.TestResult$1.protect(TestResult.java:106) at junit.framework.TestResult.runProtected(TestResult.java:124) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:120) at junit.framework.TestSuite.runTest(TestSuite.java:230) at junit.framework.TestSuite.run(TestSuite.java:225) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:616) at org.apache.maven.surefire.junit.JUnitTestSet.execute(JUnitTestSet.java:207) at org.apache.maven.surefire.junit.JUnit3Provider.executeTestSet(JUnit3Provider.java:107) at org.apache.maven.surefire.junit.JUnit3Provider.invoke(JUnit3Provider.java:79) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:616) at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:103) at $Proxy0.invoke(Unknown Source) at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:145) at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:87) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:69) The Hudson server is not run off of JBoss it is run off of Tomcat. I add this detail because when I Google this error I have found nothing but JBoss errors explaining that the facade is referenced in both the WAR and EAR files. EDIT: This is how I am currently creating instances of the Facade (Which works fine when running the tests through Netbeans GF3.1):  Map properties = new HashMap(); properties.put(EJBContainer.MODULES new File(""target/classes"")); properties.put(""org.glassfish.ejb.embedded.glassfish.configuration.file"" ""server/config/domain.xml""); properties.put(""oracle.jdbc.OracleDriver"" ""server/lib/ojdbc14.jar""); ejbContainer = EJBContainer.createEJBContainer(properties); ctx = ejbContainer.getContext(); EventsFacade instance = (EventsFacade)ctx.lookup(""java:global/classes/EventsFacade""); EDIT: My EventsFacade: @Stateful public class EventsFacade extends AbstractFacade<Events> { @PersistenceContext(unitName = ""tickets_AppTicket_war_1.0-SNAPSHOTPU"") private EntityManager em; protected EntityManager getEntityManager() { return em; } public EventsFacade() { super(Events.class); } } EDIT: And.. my AbstractFacade in full: public abstract class AbstractFacade<T> { private Class<T> entityClass; public AbstractFacade(Class<T> entityClass) { this.entityClass = entityClass; } protected abstract EntityManager getEntityManager(); public void create(T entity) { getEntityManager().persist(entity); } public void edit(T entity) { getEntityManager().merge(entity); } public void remove(T entity) { getEntityManager().remove(getEntityManager().merge(entity)); } public T find(Object id) { return getEntityManager().find(entityClass id); } public List<T> findAll() { javax.persistence.criteria.CriteriaQuery cq = getEntityManager().getCriteriaBuilder().createQuery(); cq.select(cq.from(entityClass)); return getEntityManager().createQuery(cq).getResultList(); } public List<T> findRange(int[] range) { javax.persistence.criteria.CriteriaQuery cq = getEntityManager().getCriteriaBuilder().createQuery(); cq.select(cq.from(entityClass)); javax.persistence.Query q = getEntityManager().createQuery(cq); q.setMaxResults(range[1] - range[0]); q.setFirstResult(range[0]); return q.getResultList(); } public int count() { javax.persistence.criteria.CriteriaQuery cq = getEntityManager().getCriteriaBuilder().createQuery(); javax.persistence.criteria.Root<T> rt = cq.from(entityClass); cq.select(getEntityManager().getCriteriaBuilder().count(rt)); javax.persistence.Query q = getEntityManager().createQuery(cq); return ((Long) q.getSingleResult()).intValue(); } } EDIT: Created new Interface:  @Local public interface EventsInterface<Events> { int count(); void create(Events entity); void edit(Events entity); Events find(Object id); List<Events> findAll(); List<Events> findRange(int[] range); void remove(Events entity); } CHANGED: EventsFacade delaration: public class EventsFacade extends AbstractFacade<Events> implements EventsInterface<Events> { EDIT: After getting the proxy error in my IDE after creating the new interface the output began to show: Field Name == intfClass Field == interface ticket.app.EventsInterface Field Name == containerId Field == 85313541807800321 Field Name == delegate Field == com.sun.ejb.containers.EJBLocalObjectInvocationHandler@72b0f2b2 Field Name == isOptionalLocalBusinessView Field == false and now the `AbstractFacade` definition :) @Bozho - Just posted lol This usually means that you are referencing an object by its class rather than by its interface when there is a proxy around the object (and the proxy is based on the interface). The solution is to refer to the object by its interface. I would guess that EventsFacade has a Local and/or Remote interface. Try using that. If it doesn't have an interface - make one it is a good practice. Define all the public methods in the interface. Update: as it seems this might be a problem with the embedded glassfish you are using. As you can't debug that with a debugger here's what you can do to trace the problem: Use java.lang.reflect.Proxy.getInvocationHandler(object). And then list all the fields of the returned object. (You would have to drop the cast for now and get it from the context simply as Object). Something like: Object ic = Proxy.getInvocationHandler(facade); Field[] fields = ic.getClass().getDeclaredFields(); for (Field field : fields) { field.setAccessible(true); System.out.println(field.getName() + ""="" + field.get(ic); } @Bozho - Should I now change my lookups to search for the `Interface`? @ninn - no I don't think so. @Bozho - I am now getting the Proxy error inside of my IDE and Hudson is saying that the lookup for my EventsFacade has failed. @Bozho - The IDE is now giving an output of: `intfClass=interface ticket.app.EventsInterface containerId=85313464911659009 delegate=com.sun.ejb.containers.EJBLocalObjectInvocationHandler@48586403 isOptionalLocalBusinessView=false` For where we collected the class and it's fields. To make the output a little more clear I changed the format and posted it as an edit on my org. post. Entity Facades implement the oracle.jbo.Entity interface right? My call for the facade is like this: EventsFacade instance = (EventsFacade)ctx.lookup(""java:global/classes/EventsFacade""); If I am to reference it by the interface how would I make the call? @ninn - well I can't know what are your classes. But if the interface is `Entity` then you should use that. Or find a mechanism to get the original object from the proxy. Would the call then be something like: `EventsFacade instance = (EventsFacade)ctx.lookup(oracle.jbo.Entity);` Where ctx is the Context of the EJBContainer? perhaps show me the current code in `EventsControllerTest` @ninn - add this to your original question. It is not quite readable here. @Bozho - It is added. @ninn - well if you are running it on tomcat this means you don't have an EJB provider at all? Can you try using the embedded glassfish: http://embedded-glassfish.java.net/ @Bozho - I have a dependency for the: `glasshfish-embedded-all version 3.1` in my POM already. Wouldn't this do the same? @ninn yeah it should. Could you inspect (With a debugger) what is the actual invocation handler of the proxy you are given. @Bozho - I don't really believe I can. The error only occurs when running off of the Hudson(ubuntu) server. I don't receive any proxy errors while running locally. Or am I mis-understanding your question? @ninn you are understanding it right yeah. I forgot about that point. In that case - could you add some reflection to obtain the invocation handler programtically? See my update how. @Bozho - Then simply do a `System.out.prinln(java.lang.reflectProxy.getInvocationHandler(object)` ? @ninn - not exactly. Inspect the object with reflection (object.getClass().getFields()`) and print all of them together with their values. @Bozho - I just made an edit to my original post I've never used reflectProxy before so I don't really know how it works. Please see my edit for my next question. @ninn - I had a typo. see updated. @Bozho - Now we're talking.. I'm uploading the changes to Hudson now. Give me about 5-8 minutes for the build to complete then I'll get back to you with the results. @ninn - by the way does this facade have a remote or local interface (as it is an EJB I guess) ? @Bozho - As I am sure you can tell I'm fairly new to Enterprise Java Beans.. If my ideal of local vs remote is correct. Local runs the view off of the same server? -So it would be local. @ninn - show me the definition of `EventsFacade` @Bozho - Just posted. @Bozho - Hudson doesn't seem to want to give me the output.. Either that or there are no fields to display. @ninn strange. see my update from a few minutes ago advising you to create an interface and try again. @Bozho - I will attempt to create an interface but I've never done so before. If you have any good tutorials or examples I would appreciate it much. @ninn - your IDE should have an option to extract an interface it is that easy (Then just annotate it with `@Local`) @Bozho - Since the facade `extends AbstractFacade implements EventsInterface` should the `EventsInterface` contain all of `AbstractFacade`'s public methods also? @ninn - it can. or it can extend from another Base interface that has the abstract class methods. @Bozho - I apologize for my ignorance. Do I `Extract Interface` from the `Events Entity Class` or from the `EventsFacade`? When I attempt to extract from the facade it says there is nothing to extract. @ninn - then just take all public methods and make an interface. For the sake of testing the above code. Later you can fix your hierarchies @Bozho - My Abstract Facade holds the methods should I create an interface for it then just have all of my Facades implement that interface? yes try that.. @Bozho - Just made some edits to my org. post.. Do you want me to try running the Hudson Build again? yes run it again."
233,A,"How to mock a web server for unit testing in Java? I would like to create a unit test using a mock web server. Is there a web server written in Java which can be easily started and stopped from a JUnit test case? Try Simple(Maven) its very easy to embed in a unit test. Take the RoundTripTest and examples such as the PostTest written with Simple. Provides an example of how to embed the server into your test case. Also Simple is much lighter and faster than Jetty with no dependencies. So you won't have to add several jars on to your classpath. Nor will you have to be concerned with WEB-INF/web.xml or any other artifacts. While it is probably what the user wanted this isn't a ""mock"" web server it is an actual web server that is launched within the unit test. For example if the port was taken it would fail so not a true mock (ie does have an external dependency from the system). According to Martin Fowler's nomenclature fore [""Test Doubles""](http://www.martinfowler.com/bliki/TestDouble.html) this is a ""fake"". That said it was **exactly** what I was looking for.  Try using the Jetty web server.  Are you trying to use a mock or an embedded web server? For a mock web server try using Mockito or something similar and just mock the HttpServletRequest and HttpServletResponse objects like: MyServlet servlet = new MyServlet(); HttpServletRequest mockRequest = mock(HttpServletRequest.class); HttpServletResponse mockResponse = mock(HttpServletResponse.class); StringWriter out = new StringWriter(); PrintWriter printOut = new PrintWriter(out); when(mockResponse.getWriter()).thenReturn(printOut); servlet.doGet(mockRequest mockResponse); verify(mockResponse).setStatus(200); assertEquals(""my content"" out.toString()); For an embedded web server you could use Jetty which you can use in tests.  Have a look at https://github.com/oharsta/priproba"
234,A,"Time dependent unit tests I need to test a function that whose result will depend on current time (using Joda time's isBeforeNow()).  public boolean isAvailable() { return (this.someDate.isBeforeNow()); } Is it possible to stub/mock out the system time with Mockito so that I can reliably test the function? I use an approach similar to Jon's but instead of creating a specialized interface just for the current time (say Clock) I usually create a special testing interface (say MockupFactory). I put there all the methods that I need to test the code. For example in one of my projects I have four methods there: one that returns a mock-up database client; one that creates a mock-up notifier object that notifies the code about changes in the database; one that creates a mock-up java.util.Timer that runs the tasks when I want it to; one that returns the current time. The class being tested has a constructor that accepts this interface among other arguments. The one without this argument just creates a default instance of this interface that works ""in real life"". Both the interface and the constructor are package private so the testing API doesn't leak outside of the package. If I need more imitated objects I just add a method to that interface and implement it in both testing and real implementations. This way I design code suitable for testing in the first place without imposing too much on the code itself. In fact the code becomes even cleaner this way since much factory code is gathered in one place. For example if I need to switch to another database client implementation in real code I only have to modify just one line instead of searching around for references to the constructor. Of course just as in the case with Jon's approach it won't work with 3rd party code that you are unable or not allowed to modify. This sounds like you're going to create a new interface to encapsulate the dependencies of each class under test? Which will then require at least one implementation. So now for every class under test you have the class at least one test class the dependency grouping interface and an implementation of that interface? I really don't like that. And I feel like adding that additional level of indirection between your code and its actual dependencies (i.e. after looking at your class I have to then look at the dependency interface as well) actually makes the code harder to understand. @Dathan no not each class. Only those that have dependencies that has to be emulated during testing. In my application there happen to be just one such class. Also the number of classes doesn't mean anything. If the implementation is just `class DefaultMockupFactory implements MockupFactory {Timer createTimer() {return new Timer();}}` it isn't that much of a complexity is it? And having `factory.createTimer()` somewhere in the code doesn't make the code harder to understand either. But I agree that in some cases it may not be the best way to do it. @Dathan the interface is nested (with package-private visibility) so it's all still inside the class. And anyway the difference between my and Jon's approach is that I use a single interface (`MockupFactory`) for all dependencies that have to be emulated while Jon proposes to have a separate interface for each dependency (`TimerFactory` and so on). How would you test the code without any interface at all is a mystery to me. One way or another the additional complexity is needed. No I don't think that it adds too much complexity -- just unnecessary complexity. I feel like the additional level of indirection injected by having this facade interface may inhibit the readability of the code. For instance if I had your code sample but `MockupFactory` had a few other methods on it and I wanted to find all the places in the code where `createTimer()` was used I have to navigate from the class under test to the interface and THEN search for uses of the method instead of just searching in the class. I agree *some* interface needs to be added. But I'd much rather follow interface segregation -- so in this case I'd strongly prefer Jon's approach. Part of this is that many interfaces are reused around the system -- it's likely that `TimerFactory` will be reused and therefore you can wire it up in a DI container once and have the same one used everywhere while a `MockupFactory` for a particular class is unlikely to be used more places -- that means more configuration required compared to well-segregated interfaces. @Dathan oh now I see the point. In my case there was only one class that required any kind of special dependency for testing so it was obviously much better to hide the interface inside the class. If there is more than one class I'd probably follow Jon's approach too.  The best way (IMO) of making your code testable is to extract the dependency of ""what's the current time"" into its own interface with an implementation which uses the current system time (used normally) and an implementation which lets you set the time advance it as you want etc. I've used this approach in various situations and it's worked well. It's easy to set up - just create an interface (e.g. Clock) which has a single method to give you the current instant in whatever format you want (e.g. using Joda Time or possibly a Date). Joda time has builtin support for that abstraction (see my answer) so you don't have to introduce it in your code. @Laurent: I don't think that's actually nearly as elegant. Fundamentally I think a service such as ""getting the current time"" *is* a dependency (just as I view random number generation as a dependency) so I think it's good to make that explicit. This means you can parallelize tests etc too. Point taken. Don't get me wrong: I'm usually in favor of abstraction. However the notion of current time can be difficult to abstract in a real-life project especially when third-party libraries are used that don't abstract this notion. @Laurent: Oh yes when you're using third-party libraries it's tricky... but then you'd have the same problem with setCurrentMillisFixed unless your third party library happened to use Joda Time :( @Jon: No I wouldn't since Joda Time already provides ways to control the time for testing purposes as pointed out in other answers/comments. I disagree there is a design issue in this case though: many dependencies are inherently *private* and should not be exposed in the public interface of the class as doing so violates the principle of information hiding while tending to make code more complex than is called for. @Rogerio: And as has been pointed out elsewhere that's still got all the problems of changing a static data source. I don't see that ""I depend on the current time"" ought to be a ""private"" dependency. And heck not all constructors need to be public... As I said before I suspect we'll have to agree to disagree - but I've used the `Clock` interface approach many times and always found it to leave my code feeling cleaner and more explicit. @Jon: Information hiding one of the most fundamental software design principles is all about hiding the ""secrets"" of a module from its clients when said clients have no good reason to know them. Reading the current time usually is such a secret just like sending an email accessing a database etc. In most cases a client module A has no business knowing about the internal dependencies of another module B. Therefore such details should remain hidden inside B's implementation. This of course does not preclude the ability to externally configure those dependencies or to write unit tests. @Rogerio: Normally in Guice I have a public module which is then responsible for providing the relevant services. So that can use a package-private constructor which is still easy to test and still makes the dependencies clear. No information hiding problems there IMO. That code already is testable; no need to over-engineer it by introducing an unnecessary interface not to mention the extra ""wiring"" configuration that would be needed. @Jon: Yes it wouldn't be necessary to expose the `Clock` interface in the public interface of whatever class happens to use it internally. Doing so would not expose the dependency on current system time to client classes but it would still unnecessarily expose it to some separate entity such as the DI container and (more importantly) it would add extra complexity to the system. It's this extra complexity which I object to. To me any additional abstraction adds cost which should be justified through real benefits. @Rogerio: I think we'll have to agree to disagree. I view making the dependency explicit as a wholly good thing allowing parallelization of testing and more comprehensive testing of everything time-related. @Rogerio: That's the thing: I think of it as *inherent* complexity which is merely being made explicit. I suspect that if it were *always* provided as a dependency rather than as a static method you wouldn't be suggesting creating a static method instead and that you'd actually consider that ugly. But as I've said several times I think we're going to have to agree to disagree - feel free to reply if you wish but I'm not going to add any more comments to this already-long thread. @Rogerio: Having looked at your profile you'd presumably suggest using JMockit to test the static method. Personally I regard that as a sticking-plaster over the failure to specify a dependency (a source of the current time). I'd rather fix the design issue by making the dependency explicit rather than working round it with a particular testing framework. @Jon: Well when I find production code using something like a `Clock` interface which only seems to exist to ""make it explicit"" I usually choose to [""make it simple""](http://en.wikipedia.org/wiki/KISS_principle) by eliminating it. So far that has worked well (ie in keeping with Einstein's famous saying).  Joda time supports setting a ""fake"" current time through the setCurrentMillisFixed and setCurrentMillisOffset methods of the DateTimeUtils class. See http://joda-time.sourceforge.net/api-release/org/joda/time/DateTimeUtils.html But these are static methods - you will introduce dependencies between the unittests that way. Thus I'd prefer Jon Skeets solution. hstoerr: I don't see how there would be dependencies between tests unless they were to be executed in different threads (which is probably not the case here). But even then Joda Time provides the `DateTimeUtils.setCurrentMillisProvider(DateTimeUtils.MillisProvider)` method which would certainly allow a thread-bound implementation."
235,A,How is Jmock used with HttpSession and HttpServletRequest I am new to jmock and trying to mock an HttpSession. I am getting: java.lang.AssertionError: unexpected invocation: httpServletRequest.getSession() no expectations specified: did you... - forget to start an expectation with a cardinality clause? - call a mocked method to specify the parameter of an expectation? the test method: @Test public void testDoAuthorization(){ final HttpServletRequest request = context.mock(HttpServletRequest.class); final HttpSession session = request.getSession(); context.checking(new Expectations(){{ one(request).getSession(true); will(returnValue(session)); }}); assertTrue(dwnLoadCel.doAuthorization(session)); } I have done a bit of searching and it isn't clear to me still how this is done. Feels like I am missing some small piece. Anyone with experience in this can just point me in the right direction. thanks You don't need to mock the request object. Since the method you're testing (dwnLoadCel.doAuthorization()) only depends on an HttpSession object that is what you should mock. So your code would look like this: public void testDoAuthorization(){ final HttpSession session = context.mock(HttpSession.class); context.checking(new Expectations(){{ // ??? }}); assertTrue(dwnLoadCel.doAuthorization(session)); } The question becomes: what do you expect the SUT to actually do with the session object? You need to express in your expectations the calls to session and their corresponding return values that are supposed to result in doAuthorization returning true. this helped me in the right direction. I needed to just use the httpssession and upgrade my version of hamcrest.  I think you need to tell the JMock context how many times you expect the method to be called before you actually go ahead and call it. final HttpServletRequest request = context.mock(HttpServletRequest.class); context.checking(new Expectations(){{ one(request).getSession(true); will(returnValue(session)); }}); final HttpSession session = request.getSession(); I'm not super familiar with JMock but do you actually care in your dwnLoadCel unit test how many times certain methods in the mocked object are called? Or are you just trying to test your class that depends on a HttpSession without an actual Session? If it's the latter than I think that JMock is overkill for you. You might want to look into either creating a class that implements the HttpSession interface yourself for the purposes of unit testing only (a stub) and running your tests off of that or you should take a look at dwnLoadCel and determine if it really needs to have a reference to the HttpSession or if it just needs some properties within the HttpSession. Refactor dwnLoadCel to only depend on what it actually needs (a Map or a certain parameter value within the Session object) - this will make your unit test easier (the dependency on the servlet container goes bye-bye). I think that you have some level of dependency injection in your class being tested already but you might be dependent on too broad of an object. The Google Test Blog has had a lot of excellent articles on DI lately that you might find useful (I sure have). good answer. After reviewing the problem it seems reasonable to take a look at the class that dwnLoadCel uses for doAuthorization. Maybe refactoring that is an option. For the record there's the 'allowing()' clause in jMock which effectively stubs a call. It allows the call to be made as many times as you like. We usually 'allow' queries which do not change the state of the collaborator
236,A,"Using PowerMock or How much do you let your tests affect your design? I've been a fan of EasyMock for many years now and thanks to SO I came across references to PowerMock and it's ability to mock Constructors and static methods both of which cause problems when retrofitting tests to a legacy codebase. Obviously one of the huge benefits of unit testing (and TDD) is the way it leads to (forces?) a much cleaner design and it seems to me that the introduction of PowerMock may detract from that. I would see this mostly manifesting itself as: Going back to initialising collaborators rather than injecting them Using statics rather than making the method be owned by a collaborator In addition to this something doesn't quite sit right with me about my code being bytecode manipulated for the test. I can't really give a concrete reason for this just that it makes me feel a little uneasy as it's just for the test and not for production. At my current gig we're really pushing for the unit tests as a way for people to improve their coding practices and it feels like introducing PowerMock into the equation may let people skip that step somewhat and so I'm loathe to start using it. Having said that I can really see where making use of it can cut down on the amount of refactoring that needs to be done to start testing a class. I guess my question is what are peoples experiences of using PowerMock (or any other similar library) for these features would you make use of them and how much overall do you want your tests influencing your design? We've had many of the same questions arise in the .NET arena regarding Typemock Isolator. See This blog post I think that when people start to realize that Testability is not an end goal and that design is learned in other ways then we will stop letting our fear dictate which tools we use or not use a more advanced technology when and if it becomes relevant. Also it makes sense to be able to choose the way you design based on the application needs. don't let a tool tell you how to design - it will leave you no choice. (I work at Typemock but was once against it)  I totally agree that Testability is not an end goal this has been one of the things I have realized when developing PowerMock. I also agree that writing unit tests is one way of getting good design. Using PowerMock should probably be an exception rather than a rule at least features such as expectations on constructors and static mocking. The main motivation we have for using PowerMock is when using third party code that prevents your code from being testable. A good alternative is using an anti-corruption-layer that abstracts the third party code and makes it testable. However sometimes I think the code is cleaner just using the standard APIs. A good example of this is the Java ME API. This is full of static method calls that prevent unit testing. The same problem can occur with legacy code. Some organizations are extremely afraid of modifying their existing code and in this case PowerMock can be used to introduce unit testing in the parts you are writing at the moment without forcing big refactorings. Our problem is specifying a set of best practice rules when to use PowerMock or not that a rookie developer can follow. Creating good design is really hard and since PowerMock gives you more options maybe it just gets harder for a beginner? I think a more experienced developer appreciates having more choices. (founder of PowerMock) Hi Jan thanks for your response and I agree with what you say totally and that's where my concern comes from. The experienced guys will know to use it sparingly it's the novices that worry me and that they'd just start using it where refactoring the code would be a better idea.  I have to strongly disagree with this question. There is no justification for a mocking tool that limits design choices. It's not just static methods that are ruled out by EasyMock EasyMock Class Extension jMock Mockito and others. These tools also prevent you from declaring classes and methods final and that alone is a very bad thing. (If you need one authoritative source that defends the use of final for classes and methods see the ""Effective Java"" book or watch this presentation from the author.) And ""initialising collaborators rather than injecting them"" often is the best design in my experience. If you decompose a class that solves some complex problem by creating helper classes that are instantiated from that class you can take advantage of the ability to safely pass specific data to those child objects while at the same time hiding them from client code (which provided the full data used in the high-level operation). Exposing such helper classes in the public API violates the principle of information hiding breaking encapsulation and increasing the complexity of client code. The abuse of DI leads to stateless objects which really should be stateful because they will almost always operate on data that is specific to the business operation. This is not only true for non-public helper classes but also for public ""business service"" classes called from UI/presentation objects. Such service classes are usually internal code (to a single business application) that is inherently not reusable and have only a few clients (often only one) because such code is by nature domain/use-case specific. In such a case (a very common one by the way) it makes much more sense to have the UI class directly instantiate the business service class passing data provided by the user through a constructor. Being able to easily write unit tests for code like this is precisely what led me to create the JMockit toolkit. I wasn't thinking about legacy code but about simplicity and economy of design. The results I achieved so far convinced me that testability really is a function of two variables: the maintainability of production code and the limitations of the mocking tool used to test that code. So if you remove all limitations from the mocking tool what do you get? Disagree: 1) The mocks frameworks don't force you to inject collaborators. 2) To prevent public visibility you could also use the package visibility (you put the tests in the same package in a different folder). @gontard Yes they do force you to inject collaborators (except for those that support mocking of ""new-ed"" objects of course). Please note that ""injecting collaborators"" is used here (in the question and in my answer) in the general sense and as opposed to ""initialising collaborators""; that is they are created in test code and *passed* somehow to code under test rather than being created & initialized directly in the CUT. Ok i misunderstand what you write. How JMockit solves this ?  I think you're right - if you need PowerMock you probably have smelly code. Get rid of those statics. However I think you're wrong about bytecode instrumentation. I mock out concrete classes all the time using mockito - it keeps me from having to write an interface for every. single. class. That is much cleaner. Only you can prevent code smells. I do use the EasyMock class extension to mock concrete classes but I only do that for existing classes that would be too much work to refactor but the fact that they're mocks BEFORE going into my class seems ok to me  I think you're right to be concerned. Refactoring legacy code to be testable isn't that hard in most cases once you've learned how. Better to go a bit slower and have a supportive environment for learning than take a short cut and learn bad habits. (And I just read this and feel like it is relevant.)"
237,A,"Is there a Windowlicker for wxpython? Having recently read ""Growing OO systems guided by tests"" I am very impressed with the windowlicker testing utility for java/junit. Basically it wraps the GUI and GUI-interaction with drivers and gestures so your integration/end-to-end tests can be written neatly like: //setup ui.enterUserDetailsFor(newUser) ui.sendForm() //assert ui.showsWelcomeMessage() All swing-gui-thread synchronization and finding widgets etc is nicely isolated in the framework. Exactly this sort of higher level testing is something I really miss in my current wxPython project. Is there anything at all similar for wxgtk/wxpython? What I can tell from the windowlicker source it builds on java.awt.robot if there isn't a windowlicker is there anything like robot? The only real Windowlicker available is Aphex Twin's :) I haven't heard of anything specifically for wxPython. You can use the Widget Inspection Tool for some of the stuff you are talking about though. See here: http://wiki.wxpython.org/Widget%20Inspection%20Tool There's also the Sikuli project which is GUI-agnostic: http://groups.csail.mit.edu/uid/sikuli/ Hopefully one of those will help you out. Nice! I'll have to troll around in that inpsection tool's source maybe there's a python windowlicker in the making there! :) Meanwhile sikuli was really cool"
238,A,"Unit testing statics and factories I am implementing a model in Java which requires iterating over a collection and going through a number of identification stages it involves for loops while loops etc. It is the sort of thing I want to test at a fine-grained level so that I have confidence it has been implemented properly. I have used it as an opportunity to start unit testing as it is something I recognise as being beneficial to my code. I have since been reading a forest of books to get up to speed with JUnit and unit testing. Basically my question comes down to two conflicting pieces of advice I have received: 1) Statics are evil. Do not touch statics. Do not test privates either you probably want a class there instead. 2) Use factories for creation to allow dependency injection using parameters - potentially allowing use of mocks and stubs for isolation. In my example I am looking to perform an operation along the lines of: double height = 223.42; // this was set iterating over a collection of doubles //blah HeightBounds b = HeightBounds.getHeightBounds(height); //more blah I have done this in order to avoid building up what would become a very long and complicated block of code which I can only test in its entirety. This way I have public accessible objects that I can test to ensure the system components all perform correctly. Common sense says to me there is nothing wrong with static factories and that they are easily tested but am I missing something blindingly obvious given I am learning test-driven design? Thank you You should unit test every method that it makes sense to test this includes static methods. It is possible that ""statics are evil"" primarily is directed at static _variables_ since they keep their values between tests and might therefore cause dependencies between tests if they are not properly reset. Static _methods_ that do not touch static variables are not problematic in my opinion. the problem is that I could make my bounds checks within the class as a static method call but it would be very procedural and I don't want to list it in my public interface. One way I could test static methods without changing visibility would be to extend the class for testing and make methods to call them but all the same everything I read people are saying ""static methods are a code smell"" you should be using classes. Given I am running in an embedded environment though I prefer the idea of avoid object creation as opposed to perfect OO design... still deliberating! A problem with static factories is that you are not able to replace the factories (and sometime the objects created by the factories) by mocks. - That is one reason why IOC Containers are so usefull. Thanks IoC Containers was a very good keyword for google I have since been reading this: http://martinfowler.com/articles/injection.html  The static factory class introduces coupling between your class and the HeightBounds class. this might make your class difficult to test if for example HeightBounds goes off and looks in DB for information or reads from a web service etc etc. If you instead injected an IHeightBounds implementation into your class then you could mock that out so you could test what happens when the dependencies of you class do certain things. For example what if HeightBounds throws an exception? or returns null? Or you want to test when a particular HeightBound is returned? With an interface it is easy to mock this behaviour with a static factory it is more difficult as you have manufacture the data to create the desired results in the class. You could still only have a single implementation of HeightBounds and would be able to test that in isolation but you would be able to test your method above without even having an real implementation. I would probably have an IHeightBoundFactory interface and inject an implementation into the class. As for testing the privates generally you don't want to. You want to be testing one of 2 things either that the results are what you expected or that the interactions are what you expected. If you have a method called Add and a method called GetAll then you might want to test that when you call Add and then call GetAll you get back the one you added. you don't care how this is implemented just that it works. this is testing the results. Generally in this situation you want to create mock objects that return data. This seems to be your situation. If when you call Add you expect that what is being added is logged then you want to test the interactions with the logging dependency so you inject a mock dependency and verify that the interaction with that class happened when you called Add. Generally in this situation you want to create mock objects that have expectations set and you verify that those expectations have been met. It doesn't look this is necessary in the situation you described above. you're welcome! glad I was able to help. You did the right thing in waiting to see what other answers/feedback might be recieved before accepting an answer. When a question is a technical one which has a correct answer then accepting answers immediately can be appropriate when they are a bit more open ended like this it often pays to wait a little while before accepting an answer. I am very good at open-ended questions :) I hit analysis paralysis quite often I am marking this as the answer as you were able to spot what I was trying to achieve and the concept I had misunderstood then giving examples to fill in the gaps. Thanks Not that I want to do myself out of the rep but you might in general want to wait before accepting an answer as more people may have a chance to give their opinion or shoot my argument down :) Ok thanks will do as you suggest and wait for further feedback and finish addressing the other answers (which also cover lots of good content). This answer is strongly based on the *assumption* that a `static` method cannot be easily mocked... It's a bit strange that people usually take it for granted even though the assumption is false.  Static methods are not evil per se. Look in JDK how many of them are there. Static methods also need to be unit tested so if you have any go on and test them. So it's not true that static methods kill unit testing. But if you make statics only to write less code in unit tests then it is a wrong path since as others have stated you should better use your objects using your usual API - since this is what you want to test. And full ACK for writing tests first - it will also help you design better API cut you classes right. The reason people say static methods kill unit testing is not because you can't test the static methods but that the code that calls the static methods is difficult to test in isolation. In case of Java you'd have to use a framework that supports bytecode instrumentation (like JMockit) to mock/fake/stub/w/e the method. As I understand it's not ideal (although I have only recently learnt about this possibility). I assumed that your statement ""So it's not true that static methods kill unit testing."" was a response to the opposite. The reason I commented is because I felt your answer says ""static methods are OK to use because you can also test them"" and I wanted to add what I understand to be the downsides: Mocking them is difficult because it's more complex and also slower than normal mocking. In general you want your unittests to be simple (so you can maintain them) and fast (so you can run them often). Well actually I personally do not know anybody saying that so can't comment on ""people say"". For me ""kill unit testing"" sounds like ""now that you have static methods you're in real trouble and can't unit test"" which is plain not true (as I stated). True is that is makes testing more difficult yet not impossible. And there are times where static methods are beneficial. And once you have them you should test them. Like you stated is mocking one of the possibilities pretty good I'd say. Other is to prepare the state so that tests work - I did it more than once it is definitely possible.  Statics are evil. Do not touch statics. ""static"" here probably means Singletons i.e. global state. That is indeed difficult to use in unit tests and can introduce lots of subtle issues so it is better to avoid it in general. However static members (fields methods or inner classes) in general are not necessarily a problem per se. Do not test privates either you probably want a class there instead. This is true in general if you feel the need to test a private method it is a sign that your API is not fine grained enough and the enclosing class may be trying to do too much. Often you may identify some coherent group of functionality which is better to extract into a separate class. This improves your design and in turn makes unit testing easier.  Static methods basically kill unit testing. One of the basic ideas when you want to unit test your application is the ability to isolate different portions of your code. You usually do this by wiring mock objects for your environment. You can't wire anything if you are using static methods. Moreover using static methods hides dependencies between objects. In your last paragraph you say you want to learn test-driven design. If you are writing your tests after you've written your code your tests are not actually driving anything. Write your tests first. Regarding private methods you would usually have full test coverage of the public methods that use those private methods so you are going to cover them anyway. If you have a really complex private method that you want to test (you should not) just use reflection to make it accessible. I'd rather break encapsulation than having untested code. Misko Hevery has a very nice post on this topic. DistanceBounds itself does not exist yet - I am still fleshing what objects are needed and the process while keeping in mind how I will test them. Maybe TDD isn't what I meant but basically rather than waterfall-style defining all my tests/code up front I am looking to write them as I come to write the code method at a time ... certainly the tests are driving my design but yes maybe this isn't TDD or any formal process."
239,A,"Naming convention JUnit suffix or prefix Test Class under test MyClass.java JUnit test case name alternatives: TestMyClass.java MyClassTest.java http://moreunit.sourceforge.net seems to use ""Test"" as prefix default but I have seen both uses. Both seems to be recognized when running the entire project as unit test in eclipse as it is the annotation inside classes that are parsed for @Test. I guess maven does the same thing. Which is preferred? Not to offend anybody but I think it is fair to say that ""moreunit"" is much less known than JUnit which is pretty much ubiquitous and established the convention of suffixing test classes ""Test"". Although JUnit4 did away with the necessity of following both class and method naming conventions (resp. ""postfix Test"" and ""prefix test"") I think both are still useful for clarity. Imagine the horror of having src/test/java/.../MyClass.myMethod() tested by src/main/java/.../MyClass.myMethod()... Sometimes it is useful to diverge from the JUnit3 conventions - I find that naming setup methods after what they do (""createTestFactory()"") and annotating them ""@Before"" is much clearer than the generic ""setUp()"". This is particularly useful when several unrelated setup actions need to be performed - they can be in separate methods each tagged @Before. This communicates the independence of the actions very nicely. +1 for inspirational. A great idea from BDD (http://blog.dannorth.net/introducing-bdd): rather than naming test methods ""@Test testFailOnNull() {...}"" use the verb ""should"": ""@Test shouldFailOnNull() {...}"". I find this conveys a lot of information concisely. It avoids repeating ""test"" and reads better than ""@Test failOnNull() {...}"".  I think it is important you feel comfortable with your tests if you are working alone. But if you're in a group you better sit down and get something fixed. I personally tend to use suffix for classes and prefix for methods and try to have my groups adapt to this convention.  Prior to JUnit 4 it was common to name your test classes SomethingTest and then run JUnit across all classes matching *Test.java. These days annotation driven JUnit 4 you just need to annotate your test methods with @Test and be done with it. Your test classes are probably going to be under a different directory structure than your actual source (source in src/ test classes in test/) so these days prefixes/suffixes are largely irrelevant. They are not. Assume we have a model for a connector and this class is named `Connector` than the model for a test to test a connector instance would be a `ConnectorTest`. To me that's pretty obvious. You're right from a pure technical perspective but naming should be related to design. Enlightening to know history. +1 @Andreas_D that is true. Usually a unit test class will centre on testing one java class so the unit tests for `Connector` will be in `ConnectorTest` so in hindsight of my answer prefix/suffixes still have some value.  I prefer to use the suffix - it means that looking down the list of files in a directory is simpler: you don't have to mentally ignore the first four letters to get to something meaningful. (I'm assuming you have the tests in a different directory to the production code already.) It also means that when you use Open Type (Ctrl-T) in Eclipse you end up seeing both the production code and its test at the same time... which is also a reminder if you don't see a test class :) Very informative from a practical point of view +1 for the convenience it brings. I agree I want to see the Thing and it's test ThingTest together in lists of projects. I'm with Jon suffix. +1 for the Open Type point  I prefer using the TestClassName syntax. When using the other syntax I have trouble identifying which is the test and which is the actual class in editors when I have both open. Having to look for the Last four letters in the name is tiresome and also these letters are not always displayed. For me the other syntax leads to several wrong swapping´s between files every day and that is time consuming.  i prefer the suffix: TestCase. this is consistant with: http://xunitpatterns.com/Testcase%20Class.html  Another argument for suffix - at least in english language: A class usually represents a noun it is a model of a concept. An instance of one of your tests would be a 'MyClass test'. In contrast a method would model some kind of action like 'test [the] calculate [method]'. Because of this I'd always use the 'suffix' for test classes and the prefix for test methods: the MyClass test --> MyClassTest test the calculate method --> testCalculate() How to do this from a design point of view was what I was looking for.  I also use MyClassTest_XXX when I want to split my test into multiple classes. This is useful when testing a big class and I want the tests logically grouped. (Can't control legacy code so this scenario does come up.) Then I have something like KitchenSinkTest_ForArray KitchSinkTest_ForCollection etc."
240,A,"Configuring ant to run unit tests. Where should libraries be? How should classpath be configured? avoiding ZipException I'm trying to run my junit tests using ant. The tests are kicked off using a JUnit 4 test suite. If I run this direct from Eclipse the tests complete without error. However if I run it from ant then many of the tests fail with this error repeated over and over until the junit task crashes.  [junit] java.util.zip.ZipException: error in opening zip file [junit] at java.util.zip.ZipFile.open(Native Method) [junit] at java.util.zip.ZipFile.(ZipFile.java:114) [junit] at java.util.zip.ZipFile.(ZipFile.java:131) [junit] at org.apache.tools.ant.AntClassLoader.getResourceURL(AntClassLoader.java:1028) [junit] at org.apache.tools.ant.AntClassLoader$ResourceEnumeration.findNextResource(AntClassLoader.java:147) [junit] at org.apache.tools.ant.AntClassLoader$ResourceEnumeration.nextElement(AntClassLoader.java:130) [junit] at org.apache.tools.ant.util.CollectionUtils$CompoundEnumeration.nextElement(CollectionUtils.java:198) [junit] at sun.misc.CompoundEnumeration.nextElement(CompoundEnumeration.java:43) [junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.checkForkedPath(JUnitTask.java:1128) [junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeAsForked(JUnitTask.java:1013) [junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:834) [junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1785) [junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:785) [junit] at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:288) [junit] at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) [junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) [junit] at java.lang.reflect.Method.invoke(Method.java:597) [junit] at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) [junit] at org.apache.tools.ant.Task.perform(Task.java:348) [junit] at org.apache.tools.ant.Target.execute(Target.java:357) [junit] at org.apache.tools.ant.Target.performTasks(Target.java:385) [junit] at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1337) [junit] at org.apache.tools.ant.Project.executeTarget(Project.java:1306) [junit] at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41) [junit] at org.apache.tools.ant.Project.executeTargets(Project.java:1189) [junit] at org.apache.tools.ant.Main.runBuild(Main.java:758) [junit] at org.apache.tools.ant.Main.startAnt(Main.java:217) [junit] at org.apache.tools.ant.launch.Launcher.run(Launcher.java:257) [junit] at org.apache.tools.ant.launch.Launcher.main(Launcher.java:104) my test running task is as follows:  <target name=""run-junit-tests"" depends=""compile-testsclean-results""> <mkdir dir=""${test.results.dir}""/> <junit failureproperty=""tests.failed"" fork=""true"" showoutput=""yes"" includeantruntime=""false""> <classpath refid=""test.run.path"" /> <formatter type=""xml"" /> <test name=""project.AllTests"" todir=""${basedir}/test-results"" /> </junit> <fail if=""tests.failed"" message=""Unit tests failed""/> </target> I've verified that the classpath contains the following as well as all of the program code and libraries:  ant-junit.jar ant-launcher.jar ant.jar easymock.jar easymockclassextension.jar junit-4.4.jar I've tried debugging to find out which ZipFile it is trying to open with no luck I've tried toggling includeantruntime and fork and i've tried running ant with ant -lib test/libs where test/libs contains the ant and junit libraries. Any info about what causes this exception or how you've configured ant to successfully run unit tests is gratefully received. ant 1.7.1 (ubuntu) java 1.6.0_10 junit 4.4 Thanks. Update - Fixed Found my problem. I had included my classes directory in my path using a fileset as opposed to a pathelement this was causing .class files to be opened as ZipFiles which of course threw an exception. Thanks guys for this information. I just want to add some tip from my experience. I have the same problem with junit as you when junit tryes to open license*.txt files in lib folder where *.jar resides.(Ivy resolve process puts them here) So <path id=""lib.path.id""> <fileset dir=""${lib.dir}"" includes=""**.jar""/> </path> helps too.  If you are using Ubuntu or Debian this will make JUnit (and some other libs) always available for Ant: sudo apt-get install ant-optional  Yes this was very helpful: this: <pathelement location=""${classes.dir}""/> <fileset dir=""${classes.dir}""> <include name=""**.class""/> </fileset> not this: <pathelement location=""${classes.dir}""/> <fileset dir=""${classes.dir}""> <include name=""**.*""/> </fileset>  Found my problem. I had included my classes directory in my path using a fileset as opposed to a pathelement this was causing .class files to be opened as ZipFiles which of course threw an exception. Please provide this information in your QUESTION.  This error is caused specifically because the class path contains explicit references to one or more [files] that are not JAR's. The reference to ""error in opening zip file"" is of course that a JAR is in effect a ZIP file where as other files [JUNIT] has found like class files are not and as such do not have a zip format. So the class path should contain only explicit references to JAR [files] and/or the names of the [directories] where other resources like class files are to be found. So when building up your class path (in ANT) use: <path id=""proj.class.path""> <pathelement location=""c:/my/project/root"" /> :one for each of the [directories] where class files log4j property files and other resources etc are to be found <fileset refid=""my.file.set""> :to describe all the explicit JAR [files] that need to be on the class path. </path> where <fileset id=""my.file.set"" dir=""c:/where/i/keep/my/jars""> <filename name=""myjar1.jar"" /> <filename name=""myjar2.jar"" /> <filename name=""myjar3.jar"" /> </fileset> or NOTE: When using wild cards like [**/*] in you need to make sure the wild card is not matching files that are not JAR files <fileset id=""my.file.set"" dir=""c:/where/i/keep/my/jars""> <include name=""**/*.jar"" /> </fileset>  It sounds like there is an issue with paths. Check following error source: classpath: print out the classpath variable in a junit test run it from eclipse and ant so you can compare them Check your project for absolute paths. Probably ant uses other path prefixes than eclipse. Some more information would help to help :) Good luck!"
241,A,Is there a jUnit for Perl? I've created some business classes using OO Perl and I want to make sure I don't break them when I change the code. Sounds like unit testing is the way to go. Is there anything like jUnit for Perl? Feel free to elaborate on how you've implemented unit testing in Perl projects. Holy sh*t! I was edited by THE brian d foy! I learned Perl from your book! Don't feel too special Brian goes around capitalizing perl/Perl and fixing minor syntax issues. The best is when he answers your question. Test::Class Test::Class is a xUnit/jUnit style framework for testing. Note that it only provides the framework but it builds off of Test::Builder the common perl testing backend. This means all of Perl's extensive other test frameworks (which lie in the Test::* namespace) are available to you as well. Test::Simple and Test::More can get you started but there are many many other testing libraries available for you based on your app.  The ultimate list (for all languages) is at: http://www.xprogramming.com/software.htm (wow they really need to update their stylesheet). It appears that there are several for Perl.  You may also be interested in the responses to this question: How can I do unit testing in Perl? D'oh! Thanks for not flaming me.  There sure is : http://perlunit.sourceforge.net/ a quick Google search found this I don't do any perl development. perlunit was a comprehensive port of jUnit to Perl. In this context it may still be useful however I believe it is considered dead (http://use.perl.org/~Ovid/journal/37463) and the Perl community recommends using Test::Class etc (see Robert P's answer) a quick Google search led me to this question!
242,A,calling xml file in selenium rc test cases how can I integrate xml file into my selenium rc test cases? I want to grab information from the file and insert it into the web page and test it. thanks Nitin most probably who will have to make use of a xml parser library like JBix as if you were developing a common java application To do that you can use org.w3c.dom.Document to read XML data into a Parameterized JUnit test. You MUST use JUnit 4.11 or higher. Here is some sample code to get you started on reading the xml file. Here is an example of creating a Parameterized data provider. To get that working you will need to merge these 2 ideas. I can't write your code for you unfortunately.
243,A,"JUnit Testing -- What makes it so useful over manual testing? I understand the importance of testing and unit testing in general but is JUnit used ubiquitously in the real world and what are the advantages of using it over a ""manual"" test method? That is to say why use a JUnit Test: public class MyTest extends TestCase { public void testSomething() { assertTrue(someCondition); assertTrue(manyOtherConditions); } } versus some kind of lightweight custom-coded unit debugging tool public class MyTest { public static void testSomething() { MyDebugUtility.println(someCondition); MyDebugUtility.println(expectedCondition); } } and checking the return values yourself? Either way you'll have to make a comparison between conditions and with Unit Tests it seems VERY easy to make a slip in your test code (as I've done) and wonder why your class isn't working when your real error was accidentally typing assertTrue rather than assertFalse. Even if you wanted the easy boolean functionality what makes JUnit superior to public class MyTest { public static void testSomething() { if(condition1) MyDebugUtility.println(""Passed condition1""); else MyDebugUtility.doError(); //handle however you want } } Are there more powerful features of JUnit that I'm missing? Most of the unit tests I've written just seemed like cumbersome versions of some simple printing and boolean checking. @Martin: that's uncalled for. Remember: not everyone is an experienced developer. Testing framework like JUnit NUnit etc. give you a lot of functionality for free that you don't have to implement yourself. For example JUnit will automatically invoke test methods for you setUp() tearDown() etc. It also has lots of helper assertion methods. Another plus is JUnit is typically integrated in IDE's. None of this is rocket science and you could implement this yourself. I've had to roll my own JUnit-like functionality before. But unless you have a special need why bother?  JUnit can be integrated into continuos integration process with practically no effort so that if somebody broke the build all the team knows about this in a matter of minutes. This feature alone is worth using it. Of course there are other useful features noted in other answer. JUnit is a wheel you really don't want to reinvent. If you have something against JUnit try TestNG -- it may work for you better.  To answer the first question - yes in practice just about any serious software company will use some form of automated unit testing. Just a few of the many reasons to use an off-the-shelf unit testing framework: Do you really want to hand-check 1000 print statements? Wouldn't you rather have the computer do it for you and just tell you ""yep everything's all right"" so you can get back to coding? They can be run by a script as part of a build or on every commit. Used in this way nobody has to remember to run them (and being human we'll often forget). The test framework has already been written and debugged. If you're worried about introducing bugs in the course of building your tests I'd worry far more about making a mistake in the construction of an entire testing framework from scratch than about making a simple logical error in a few tests. As seand points out xUnit-type frameworks are so common that they're usually integrated into other types of tools - IDEs will run them and continuous integration tools like Hudson can parse their logs and track their success/failure rate over time or notify you via email when they fail. This is all functionality you'd have to build yourself - and although you may not need/want it now if you ever do in the future it'll be available.  In short it's a standardised approach to unit testing. His increases developer familiarity and improves integration with tooling (build tools CI code analysis etc)."
244,A,JUnit Exception Catching I am writing a few test cases that depend on the database being available so on the @Before and @After I open and close the session (Hibernate) as well as start and finish the transaction. Now in the process of development sometimes I get exceptions in the test cases so the @After is never called and I can't clean up (or rollback the transaction which is what I'd like to do). I've (briefly) checked the documentation and could not find how to catch these unexpected exceptions so I can rollback and let the rest of the tests work ok. Any pointers ? According to this resource: http://junit.org/apidocs/org/junit/After.html The @After method is guaranteed to run even if an exception is thrown. You're not calling that method explicitly are you? Duh .... The super class has a @After that was still trying to access the session ....
245,A,Spawn JUnit 4 Tests programmatically I have a JUnit 4 test which creates test data and asserts the test condition for every single data. If everything is correct I get a green test. If one data fails the test the execution of the whole test is interrupted. I would like to have a single JUnit test for each data. Is it possible to spawn JUnit tests programmatically so that I get a lot of tests in my IDE? The reason for this approach is to get a quicker overview which test fails and to continue the remaining tests if one data fails. It sounds like you'd want to write a parameterized test (that does the exact same checks on different sets of data). There's the Parameterized class for this. This example shows how it can be used: @RunWith(Parameterized.class) public class FibonacciTest { @Parameters public static Collection<Object[]> data() { return Arrays.asList(new Object[][] {{ 0 0 } { 1 1 } { 2 1 } { 3 2 } { 4 3 } { 5 5 } { 6 8 } }); } private final int input; private final int expected; public FibonacciTest(final int input final int expected) { this.input = input; this. expected = expected; } @Test public void test() { assertEquals(expected Fibonacci.compute(input)); } } Note that the data() method returns the data to be used by the test() method. That method could take the data from anywhere (say a data file stored with your test sources). Also there's nothing that stops you from having more than one @Test method in this class. This provides an easy way to execute different tests on the same set of parameters. Hi Joachim thank you very much for this very quick response. Sounds like it is exactly the solution to my problem. @Timo: if it was helpful then you should generally hint at that by voting my answer up (or even accepting it). Of course! I just wanted to check the solution for my problem. So: It works great! Thank you!
246,A,Code Coverage in Intellij 10 CE Is there a community plugin or EMMA tool for Code Coverage in Intellij 10 CE? I know it's available as part of the paid version but the bigwigs won't spring for it. Any alternatives? No that's in the enterprise edition. (Works great for both Java and web projects.) I ante up for a personal license every year. It's not too much money and it's worth having. If the bigwigs won't spring for it put your money where your mouth is and buy it yourself. hmmm... how about any external code coverage tools.
247,A,"JUnit Tests - what do i test? If I have a basic accessor method that returns an ArrayList What exactly would i test for it? I am very inexperienced when it comes to testing. then read some articles and watch some presentations ;) Just a related comment it is better to return an iterator to underlying collection rather than providing accessor methods to return your collection instance. Returning an iterator limits mutability of the class that contains the collection instance. @sateesh - even better: `return Collections.unmodifiableList(mylist);`. Some iterators implement a `delete()` method... This question is to vague. I suggest you do some research yourself first so you at least know what to ask. Typically writing explicit Junit tests for accessors is usually a little overkill (what are you testing? return foo;). Using a code coverage tool such as clover can help you target your testing efforts at your most complicated code first.  That depends on how you expect the method to behave. For example: If someone has called the method and changed the list that was retrieved do you want those changes to show up the next time the getter is called? Either way test that behaviour. What does the getter return when the list would be empty? Null or an empty list? This should also be tested.  Interessting question for you: How much should a unit test ""test"" How to use Junit and Hibernate usefully What should not be unit tested Edit: Added some of my favorite Questions here on Stackoverflow regarding JUnit and Unit Testing. what's Hibernate for here? i added it...cause some people tend to forget that Junit can work with a lot of different components. Its a great point of starting for a beginner...but thats just my 2 cents.  Depends on your requirements. You might test: If the return value is not null If the returned collection is not empty If the returned collection is modifiable/unmodifiable If the returned collection is sorted If the returned collection contains all expected values If the accessor method does not throw a runtime exception But as I said it depends on the requirements it depends on the 'kind' of collection you expect when you call the accessor. Maybe you allow setting the list to null but create an empty list. The the test could make sure that you really get an empty list when you set the list to null. Hope it helps to give you an idea!  In general unit tests should test that your method does what it states it should do. If your method returns an arraylist your basic test is to assert that an arraylist is indeed returned when it is called. The next level of detail in the test is to check is the arraylist constructed correctly? Have the values you expect in it been filled correctly? If it's supposed to be an empty list is that the case? Now you have your ""sunny day"" case (i.e the method works under normal conditions) you should add some negative (or ""rainy day"") conditions if appropriate. If the method accepts a length for the array what if you pass in a negative number or a int.Max etc. As stated in another answer this is probably overkill for a simple accessor but the principles apply to any unit tests you need to write.  Always keeps in mind that test code is also code and for every 1000 lines of code you produce at least 4 bugs. So test what doesn't work and don't write tests for something that can't possibly break (like code generated by your IDE). If it does break write a test :)"
248,A,"DBUnit: How to refresh only on failure? I am using DBUnit to test a Spring/Hibernate persistence. I created an abstract test: public abstract class AbstractTestCase extends AbstractTransactionalDataSourceSpringContextTests { @Override protected String[] getConfigLocations() { return new String[] { ""classpath:/applicationContext.xml"" ""classpath:/testDataSource.xml"" }; } @Override protected void onSetUpInTransaction() throws Exception { DataSource dataSource = jdbcTemplate.getDataSource(); Connection con = DataSourceUtils.getConnection(dataSource); boolean validateSchemaExists = true; IDatabaseConnection dbUnitCon = new DatabaseConnection(con ""MHADB_TEST"" validateSchemaExists); DatabaseConfig dbUnitConConfig = dbUnitCon.getConfig(); dbUnitConConfig.setProperty(DatabaseConfig.PROPERTY_DATATYPE_FACTORY new OracleDataTypeFactory()); boolean enableColumnSensing = true; boolean enableDTDMetadata = false; IDataSet dataSet = new FlatXmlDataSet(new File( ""./src/test/resources/mhadb-dataset.xml"") enableDTDMetadata enableColumnSensing); try { DatabaseOperation.CLEAN_INSERT.execute(dbUnitCon dataSet); } finally { DataSourceUtils.releaseConnection(con dataSource); } } } But whenever I run a test it refresh the DB agains the dataSet before EVERY test method which is quite very long and very not nice I think. How could I reload/refresh only on failure? P.S.: How much does the code suck? I found a way to speed up the tests even thought the behaviour is still not as expected (refresh on fail):  dbUnitConConfig.setFeature(DatabaseConfig.FEATURE_BATCHED_STATEMENTS true);"
249,A,Method specific logging Environment: JUnit 4 JDK 6 I would like to log all test names (annotated with @Test) and figure out the amount of time taken to execute each tests executed by JUnit in a standard J2SE environment. Should I rely on in-built JDK logging Logger.entering / exiting with System.currentTimeInMillis or is there a better way to do this. What about the XML files JUnit produces? They contain the tests run (classname & method name) and the time they took to run. Example XML document
250,A,"Mockito Testing an object that relies on injected dependencies (Spring)? I'm new to using Mockito and am trying to understand a way to make a unit test of a class that relies on injected dependencies. What I want to do is to create mock objects of the dependencies and make the class that I am testing use those instead of the regular injected dependencies that would be injected by Spring. I have been reading tutorials but am a bit confused on how to do this. I have one the class I want to test like this: package org.rd.server.beans; import org.springframework.beans.factory.annotation.Autowired; public class TestBean1 { @Autowired private SubBean1 subBean1; private String helloString; public String testReturn () { subBean1.setSomething(""its working""); String something = subBean1.getSomething(); helloString = ""Hello...... "" + something; return helloString; } Then I have the class that I want to use as a mock object (rather than the regular SubBean1 class like below: package org.rd.server.beans.mock; public class SubBean1Mock { private String something; public String getSomething() { return something; } public void setSomething(String something) { this.something = something; } } } I just want to try running a simple test like this: package test.rd.beans; import org.rd.server.beans.TestBean1; import junit.framework.*; public class TestBean1Test extends TestCase { private TestBean1 testBean1; public TestBean1Test(String name) { super(name); } public void setUp() { testBean1 = new TestBean1(); // Somehow inject the mock dependency SubBean1Mock ??? } public void test1() { assertEquals(testBean1.testReturn()""working""); } } I figure there must be some fairly simple way to do this but I can't seem to understand the tutorials as I don't have the context yet to understand everything they are doing / explaining. If anyone could shed some light on this I would appreciate it. In Mockito you aren't really going to create new ""mock"" implementations but rather you are going to mock out the methods on the interface of the injected dependency by telling Mockito what to return when the method is called. I wrote a test of a Spring MVC Controller using Mockito and treated it just like any other java class. I was able to mock out the various other Spring beans I had and inject those using Spring's ReflectionTestUtils to pass in the Mockito based values. I wrote about it in my blog back in February. It has the full source for the test class and most of the source from the controller so it's probably too long to put the contents here. http://digitaljoel.nerd-herders.com/2011/02/05/mock-testing-spring-mvc-controller/ Thanks your answer helps me a lot to understand it in plain English terms :) I think I am on the right track now I'm fairly new to mocking but it makes otherwise complex tests really nice once you get your head around how it works. I've really liked Mockito and found it very friendly to work with. Good luck with your testing! thanks great article I had some issue with getting it to work though but I understand it better now from that I'm sure its something I did wrong from being new to it but I was able to get Ed's way to work.. Mock testing is definitely something I should be doing more of  If you're using Mockito you create mocks by calling Mockito's static mock method. You can then just pass in the mock to the class you're trying to test. Your setup method would look something like this: testBean1 = new TestBean1(); SubBean1 subBeanMock = mock(SubBean1.class); testBean1.setSubBean(subBeanMock); You can then add the appropriate behavior to your mock objects for whatever you're trying to test with Mockito's static when method for example: when(subBeanMock.getSomething()).thenReturn(""its working"");"
251,A,"TestNG: Eclipse confused by JUnit I'm setting up tests with TestNG in an Eclipse project but I get a strange error: When I try to generate a test class from a business class I get a popup with a message saying ""Grid not editable"" (title) ""The compilation unit is not compilable or is not a sublcass of junit.framework.TestCase. Fix and tyr again"" (body). Somehow Eclipse seems to think I want to create JUnit classes and I can't get why. Any clue? Configuration: Eclipse 3.6.0 TestNG 5.14.10 So you do have the TestNG plugin for Eclipse installed? Yes I installed it. You're right I should have mentioned it. Ok I got it. It's Google CodePro. For some reason it thinks that every class which name ends with ""Test"" is a JUnit test. So when I generate tests with the TestNG plugin Eclipse open the test class with the CodePro Test Editor by default and of course it's not a valid JUnit test. I have to find a way to desactivate this. How lame... I found a solution there: http://forums.instantiations.com/viewtopic.php?f=7&t=5535 Google CodePro Analytix the newly open-sourced static analysis tool is the cause of the problem. It creates a new default editor filter which tries to open each class which name ends with ""Test"" with its specific JUnit editor. The TestNG plugin for Eclipse on the other side generates test classes using the tested class name with ""Test"" added at the end. So when you create a test class with the TestNG plugin it's opened by default with CodePro Junit editor which dispays an error popup saying the class is not a valid JUnit test class. Solution is to remove the editor filter in Eclipse Preferences: Window -> Preferences ; General -> Editors -> File Associations Select the *.java file type pick Java Editor and click on the ""Default"" button. Thanks for the help after 367 days !! Thanks for the answer. I just want to add this; If you keep getting the same message after doing the above just close the file and reopen it."
252,A,What is the JUnit default configuration for Logging with log4j and how to change it using eclipse? is there a standard configuration regarding logging with log4j in JUnit using Eclipse? When I create a new JUnit-Test testing a class which uses a Logger I don't have to change the Run Configuration to enable logging. Magically the Junit Test is redirecting the log messages to standard out. But in my case only messages with importance INFO get printed. Does anybody know how to change this behaviour and where to find the configuration of the JUnit logger. I could change the Run configurations for every JUnit test but I am more interested in the default behaviour and the logic behind. This has nothing to do with JUnit it's just log4j behavior. Sounds like there is a log4j.xml or log4j.properties somewhere on your classpath that is getting picked up (as I believe log4j will log nothing by default and instead print warnings to standard error about not being configured correctly) - add -Dlog4j.debug to your system VM args to have log4j print out some diagnostic information about where it is loading the configuration from. Thanks for the tip of using -Dlog4j.debug. Junit finds a log4j.xml in my target folder which it uses for automatic log4j configuration. The behaviour using JUnit is different than running the programm directly becuase then I get the warnings that no logger is configured.
253,A,"jUnit fail() conventions I am wondering by convention when a test fails is it appropriate to: Say why it failed (business logic) Say why the message is seen (exception should have been thrown and it's not) For instance fail(""Accessed the element which does not exist""); or fail(""ArrayIndexOutOfBoundException was expected but something bad happened""); Which one is generally preferred/ accepted? Given that this is testing code and shouldn't make it into the final build I'd say the more verbose the better. As such I'd go with the first one - it's much clearer what the root cause of the problem is which makes it easier to correct. I'd avoid the second one under all circumstances because it's not very helpful for a tester and if it does somehow make it into the final build its even more cryptic than the first to an end user - it simply is no help for testers or users alike. The one other option that I'd consider is rather than indicating that an exception occurred instead give details of the actual exception - that's why God/Jon Skeet (I'm not sure which) invented stack traces. That would convey more detail than either method listed. PS: I like your username... i like your username too :)  If there were a convention about this I would ignore it. You should say whatever will best communicate to someone seeing this message the nature of the problem in such a way that it can be resolved as easily as possible. Too often adhering to a convention fails in this regard.  First if you expect the API you test to throw an exception instead of doing a try-catch with a fail()... @Test public void testMe() { try { api.testThis(); fail(""should not reach here""); } catch(MyException e) {} } ... you should do this:- @Test(expected=MyException.class) public void testMe() { api.testThis(); } That said I rarely use fail(). If I need to perform certain validation and the condition fails I will most likely use assertions than using fail()... for example:- ... instead of... @Test public void testMe() { boolean bool = api.testThis(); if (!bool) { fail(""should be true because bla bla bla""); } } ... do this:- @Test public void testMe() { boolean bool = api.testThis(); assertTrue(""bla bla bla""bool); } However if you really need to use fail() be verbose and explain why it fails rather than ""should not reach here"" or ""should fail here"" because that doesn't help folks who read the testcases. Granted these are just simple examples.... but I think I get my points across. :)"
254,A,"Cleanest way to get a File in a JUnit test case from Maven My test code in Maven project B (which is a child of project A) has something like String filePath = ""src/main/webapp""; //do something with the filePath The above test case runs fine when I run the project from child (i.e level B) but when I run from Parent project A (i.e doing a mvn install at parent level) this fails because obviously there is no folder called ""src/main/webapp"" under parent level (It is however available in child level). I know I could do some coding do check if a test case is running from parent/child module but apparently I want to know what others have done when they have had this problem ? And no I cant use the classpath instead (for various boring reasons). I also tried relative path but then the test case sort of starts to know too much. Is there actually a solution at all for this ? UPDATE (12/Feb) - I have a web.xml under webapp folder and create a jetty server in a test case using that web.xml. Ideally src/main/webapp is not placed in the classpath. It is used by the WAR plugin to package the WAR. Now I tried an alternative where I put my web.xml in the src/main/resource/console-webapp/WEB-INF/web.xml directory and altered the webXml attribute in the maven war plugin. This seems to solve my problem. However in the output WAR I have a web.xml and another redundant web.xml (which was copied because it is in the classpath). I have tried ""packageExcludes"" and ""webResources/excludes"" and got the war plugin to omit the second web.xml but still the directory ""console-webapp"" gets copied (although is empty). Is there any way to tell maven war plugin to ignore a directory completely (i.e what is the ant pattern for that?) Maven 2 tests are run in the module directory in a separate process. This means that ""src/main/webapp"" should work perfectly well no matter from where it was invoked. Are you testing with surefire plugin? JUnit / TestNG? Did you disable forking for surefire? The Maven 2 conventions state that all test resources including files such as the one you are trying to use in your test must be stored in the src/test/resources folder. If you respect this convention you will be able to get your file from the classpath directly. These resources will not be included in the final packaging (JAR WAR EAR...). Of course you can change the directory and then specify the new test resources directory in your pom.xml: <build> <testResources> <testResource> <directory>...</directory> </testResource> </testResources> ... In your question you specified that you can't use the classpath. Why so? The above is true but the OP is very likely not trying to load a test resource but a file from the webapp. +1 for mentioning that by convention all test resources should go into src/test/resources. Pascal you are right. I have updated the question with some details from my findings. Thanks :)  Is the file you are trying to access on the test classpath? If so how about: public class IOUtils { public URL getResourceAsURL(String resource) { ClassLoader cl = getClass().getClassLoader(); return cl.getResource(resource); } public static InputStream getResourceAsStream(String resource) throws IOException { ClassLoader cl = getClass().getClassLoader(); InputStream in = cl.getResourceAsStream(resource); if (in == null) throw new IOException(""resource \"""" + resource + ""\"" not found""); return in; } }"
255,A,"Running Nightly Builds as XP Scheduled Task - Classpath Issue I'm running JUnit nightly builds on my computer as a scheduled task in Windows XP. My application uses Jaxb therefore some of the tests need to reference xsd schemas using the system classpath. When the nightly build runs while I'm not on the computer I get an error like this: java.lang.Exception: Unable to load schema mySchema.xsd from classpath However when I run the build manually I get no such error. The environment variable for the location of my schemas is a System variable not a User variable so I don't know what could be causing this error. What could be cause of this error? EDIT: Alternatives like Hudson is not what I'm asking for. I'm asking for a solution for the technology described above (batch file running as a scheduled task). hudson is another continuous integration engine that is very very simple to setup: Hudson offers the following features: Easy installation: Just java -jar hudson.war or deploy it in a servlet container. No additional install no database. Easy configuration: Hudson can be configured entirely from its friendly web GUI with extensive on-the-fly error checks and inline help. There's no need to tweak XML manually anymore although if you'd like to do so you can do that too. Change set support: Hudson can generate a list of changes made into the build from CVS/Subversion/Mercurial. This is also done in a fairly efficient fashion to reduce the load of the repository. Permanent links: Hudson gives you clean readable URLs for most of its pages including some permalinks link ""latest build""/""latest successful build"" so that they can be easily linked from elsewhere. RSS/E-mail/IM Integration: Monitor build results by RSS or e-mail to get real-time notifications on failures. After-the-fact tagging: Builds can be tagged long after builds are completed JUnit/TestNG test reporting: JUnit test reports can be tabulated summarized and displayed with history information such as when it started breaking etc. History trend is plotted into a graph. Distributed builds: Hudson can distribute build/test loads to multiple computers. This lets you get the most out of those idle workstations sitting beneath developers' desks. File fingerprinting: Hudson can keep track of which build produced which jars and which build is using which version of jars and so on. This works even for jars that are produced outside Hudson and is ideal for projects to track dependency. Plugin Support: Hudson can be extended via 3rd party plugins. You can write plugins to make Hudson support tools/processes that your team uses.  You should check out Hudson or Cruise Control. Both are easy to setup and will take care of this type of problem for you with more reporting control and improved usability. You'll spend less time investigating build problems. haskell? really? do you mean ""hudson""? Oops! The link was right but my text was wrong. :)  My suspicion is that your working directory is different from what you expect (rather than the classpath variable specifically). It's not immediately obvious if you are running this scheduled task as a user or as a system task. In either case it's worth finding out exactly where the task is running. That said while I used to set up automatic builds using Windows XP scheduled tasks (exactly as you are doing now) we currently use CruiseControl. In the abstract this isn't that much different from what you're doing (running a build regularly) but it increases your flexibility. Around here the most valuable ability of CruiseControl is to detect that changes were committed and to only run the build then. We find out about broken builds broken unit tests etc. within 10 minutes of a commit now."
256,A,JSON schema validation using java I'm writing some acceptance tests for a java webapp that returns JSON objects. I would like to verify that the JSON returned validates against a schema. Can anyone suggest any tools for this? A schema for JSON? Does such a thing exist? [Yes](http://json-schema.org/) There is a draft at IETF: http://tools.ietf.org/html/draft-zyp-json-schema-04 and implementations for Java see https://github.com/fge/json-schema-validator The json-schema-validator (currently in version 0.0.1 so it's in a pre-alpha state) worked pretty well for me. Be aware that it is not 100% feature complete but it could still correctly identify a lot of errors in my json content. The project behind is hosted on http://gitorious.org/json-schema-validation-in-java and still quite active  @b.long I came across this post looking for a very particular solution to easily verify if a String's content has a JSON (object/array). I couldn't find any library that would suit my needs. JSON Tools project or json-schema-validator though offer a lot are too big for my needs. Thus I did implement my own solution that is build on top of JSON.org reference implementation of JSON in Java. I have already been using the JSON.org's code and technically all this functionality was already there thus I added this very simple functionality reusing its code. I use it to easily test if the String returned by methods querying my database are formatted in proper JSON which otherwise if badly formatted and posted to a client might cause it to stop working. Hope this is of use to you as it is to me.  The JSON Tools project (Programmer's Guide) includes a tool to validate the contents of a JSON file using a JSON schema. An alternative could be to validate running the (JavaScript) JSON Schema Validator using Rhino. I'm betting a lot has changed on this subject since March 2010. What are your thoughts now? Are you using anything for JSON validation? Also @Fredrik  how about yourself?
257,A,"Netbeans: ""Run -> Test Project"" doesn't do anything I have many JUnit tests which are all created by Netbeans' assistant (so nothing customized). I can run every test manually by doing ""Test File"" (Ctrl+F6). But when I use ""Run -> Test Project"" the message ""No Tests executed"" is displayed. Do I have to register every JUnit test somewhere? Or what could be the problem here? Before that following appears in the output window: init: Deleting: /MY-WORK/my.data.adv/build/built-jar.properties deps-jar: Updating property file: /MY-WORK/my.data.adv/build/built-jar.properties my.commons.init: my.commons.deps-jar: Updating property file: /MY-WORK/my.data.adv/build/built-jar.properties my.commons.compile: Copy libraries to /MY-WORK/my.commons/dist/lib. my.commons.jar: my.data.init: my.data.deps-jar: Updating property file: /MY-WORK/my.data.adv/build/built-jar.properties my.data.compile: Copy libraries to /MY-WORK/my.data/dist/lib. my.data.jar: compile: compile-test: test-report: test: BUILD SUCCESSFUL (total time: 0 seconds) EDIT The project type is ""class library"" no custom configurations in build.xml are used. Perhaps it is relevant to mention that the project is old (created with some Netbeans version prior to 6.7). What type of project exactly? (Desktop Web other) how did you name the class files and where did you put the files? Everything is **classic**: JUnit tests are in package folder ""test"" (label ""Test Packages""). JUnit tests are in package which has same name as that of the classes under test. when I have run into this with NB 6.8it was due to the following reason... The JUnit tests are not in a file named *Test.java. @vkraemer: You're right! Please write this as an answer so I can mark it correct! I have found the following in the `build-impl.xml`: If you are using maven you might want to check your surefire plugin on pom.xml ... <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.11</version> <configuration> <includes> <include>**/*Test.java</include> </includes> <systemPropertyVariables> <java.util.logging.config.file>src/test/resources/logging.properties</java.util.logging.config.file> </systemPropertyVariables> </configuration> </plugin> ...  My tests were working then they stopped just displaying: No tests executed.(0.0 s) The problem was an extra space in the VM Options: after the -D. This caused problems: -D mysetting.home=something and this fixed it: -Dmysetting.home=something  Since I submitted the correct clue necessary to generate an answer I figure that I should add a bit of value to it... If you create a Java->Class Library project with NetBeans you can create a unit test associated with each of the classes in your project's Source Packages. You just need to right-click on the class in the Projects explorer. If you are creating the 'first test' for a project the IDE lets you choose between JUnit 3 and JUnit 4. When you create a test for a.b.c.NewClass NetBeans will allow you to name the test anything you want and put the test in any package you want... most of the time you do not want to change the defaults that appear in the dialog (a.b.c.NewClassTest). In NetBeans 6.9 builds a warning will appear if the name of the test you are about to create does not have ""Test"" as its suffix. If you create test class names that do not end with ""Test"" you can still get them to run when you use the Test action on a project. You just have to trigger them from a 'normal' test class.  Thanks to the user vkraemer! The solution is: Only JUnit tests are run when their name ends with Test. Section from build-impl.xml proves it: <target depends=""initcompile-test-pre-test-run"" if=""have.tests"" name=""-do-test-run""> <j2seproject3:junit testincludes=""**/*Test.java""/> </target>"
258,A,"How to unit test this? I've been playing around with .NET SpeechSynthesizer lately. and I've got a method that takes in a string and creates a .wav file out of that string. but how do I unit test this method ? seems to me like this situation is one in which unit testing cannot help you. am I right ? You got access to any speech recognition software ? You perhaps could try and train it to listen to the generated speech....? Or you could at least check the format is correct and not a zero-byte file I guess... You also might be able to get hold of (or generate them youself) statistical information about the expected length (within a certain error range) of the output file given the input sentence the speed of speech etc.  What do you want to test here? Does the .NET SpeechSynthesizer API write the wave file or does it output something to your code which writes the wave file? Testing this is the same as testing any other dependency. First isolate it (thin & mockable wrapper arround the NET SpeechSynthesizer API). Then validate your code gives the wrapper what you think it should be given and acts on any returned data the way it should act. Leave testing the .NET SpeechSynthesizer API to someone else (the creators of the API). Testing the wrapper is an intergration test (don't let the name stop you from doing it in a TDD manor if thats your thing) and you will likely have to be quite loose on what you validate (""Speak(string) causes the output file to grow""). But again you don't need to validate the API works.  When code has side effects like that its not a clean layer test but you can definitely test the side effects. Write the result to your temp dir. Verify the file is actually written to. Verify the format by loading it as a wave file. To verify whats actually in the WAV file well you probably don't want to analyze the wave file directly. In that case your test needs to monitor what parameters are sent to the WAV generator to verify they are as expected."
259,A,"Mark unit test as an expected failure in JUnit How can I mark a test as an expected failure in JUnit 4? In this case I want to continue to run this test until something is patched upstream. Ignoring the test goes a little too far as then I might forget about it. I may be able to add an @expected annotation and catch the exception thrown by assertThat but that also seems to lie about the expected behavior. Here's what my current test looks like: @Test public void unmarshalledDocumentHasExpectedValue() { doc = unmarshaller.unmarshal(getResourceAsStream(""mydoc.xml"")); final ST title = doc.getTitle(); assertThat(doc.getTitle().toStringContent() equalTo(""Expected"")); } That assert should succeed but because of an upstream bug it doesn't. Yet that test is correct; it should succeed. Virtually all the alternatives that I've found are misleading. Right now I think @Ignore(""This test should pass once fixed upstream"") is my best bet but I still have to remember to come back to it. I'd prefer that the test run. In Python I can use the expectedFailure decorator: class ExpectedFailureTestCase(unittest.TestCase): @unittest.expectedFailure def test_fail(self): self.assertEqual(1 0 ""broken"") With Qt's QTestLib in C++ you can use QEXPECT_FAIL: QEXPECT_FAIL("""" ""Will be fixed next version"" Continue); QCOMPARE(i 42); In both cases above the unit test runs which is what I'm hoping to have happen. Am I missing something in JUnit? I've taken Matthew's answer a step further and actually implemented an @Optional annotation you could use instead of the @Deprecated marker annotation he mentions in his answer. Although simple I'll share the code with you maybe it's of help for someone: @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface Optional { /** * Specify a Throwable to cause a test method to succeed even if an exception * of the specified class is thrown by the method. */ Class<? extends Throwable>[] exception(); } With a simple alteration of Matt's ExpectedFailure class: public class ExpectedFailure implements TestRule { @Override public Statement apply(final Statement base final Description description) { return statement(base description); } private Statement statement(final Statement base final Description description) { return new Statement() { @Override public void evaluate() throws Throwable { try { base.evaluate(); } catch (Throwable e) { // check for certain exception types Optional annon = description.getAnnotation(Optional.class); if (annon != null && ArrayUtils.contains(annon.exception() e.getClass())) { // ok } else { throw e; } } } }; } } You can now annotate your test method with @Optional and it will not fail even if the given type of exception is raised (provide one or more types you would like the test method to pass): public class ExpectedFailureTest { @Rule public ExpectedFailure expectedFailure = new ExpectedFailure(); // actually fails but we catch the exception and make the test pass. @Optional(exception = NullPointerException.class) @Test public void testExpectedFailure() { Object o = null; o.equals(""foo""); } }  Use mocked upstream class if possible. Stub it with correct result. Optionally replace mock with real object after bug is fixed. After you do that you may not come back to real object anyway :-)  I'm not quite getting the specifics of your scenario but here's how I generally test for expected failure: The slick new way: @Test(expected=NullPointerException.class) public void expectedFailure() { Object o = null; o.toString(); } for older versions of JUnit: public void testExpectedFailure() { try { Object o = null; o.toString(); fail(""shouldn't get here""); } catch (NullPointerException e) { // expected } } If you have a bunch of things that you want to ensure throw an exception you may also want to use this second technique inside a loop rather than creating a separate test method for each case. If you were just to loop through a bunch of cases in a single method using expected the first one to throw an exception would end the test and the subsequent cases wouldn't get checked. That is better handled by: `@Test(expected=NullPointerException.class)`. Then you can remove the try/catch block and the fail statement as JUnit will tell you it expected an exception and didn't receive one if one was not thrown. Indeed it is. I'd forgotten they'd added that.  What about explicitly expecting an AssertionError? @Test(expected = AssertionError.class) public void unmarshalledDocumentHasExpectedValue() { // ... } If you're reasonably confident that only the JUnit machinery within the test would raise AssertionError this seems as self-documenting as anything. You'd still run the risk of forgetting about such a test. I wouldn't let such tests into version control for long if ever. That might be the best option... but it feels dirty that when the assertion succeeds the test will ""fail"". Good idea! I'll accept in a few days unless a better answer comes along. (If I haven't accepted an answer by then ping me because I forgot). @SamuelEdwinWard Agreed that it feels dirty...however I would use this only for short-lived failures. If they last longer than a couple of SCM checkin cycles the test is probably stale and so I'd remove it.  I'm assuming here that you want the test to pass if your assert fails but if the assert succeeds then the test should pass as well. The easiest way to do this is to use a TestRule. TestRule gives the opportunity to execute code before and after a test method is run. Here is an example: public class ExpectedFailureTest { public class ExpectedFailure implements TestRule { public Statement apply(Statement base Description description) { return statement(base description); } private Statement statement(final Statement base final Description description) { return new Statement() { @Override public void evaluate() throws Throwable { try { base.evaluate(); } catch (Throwable e) { if (description.getAnnotation(Deprecated.class) != null) { // you can do whatever you like here. System.err.println(""test failed but that's ok:""); } else { throw e; } } } }; } } @Rule public ExpectedFailure expectedFailure = new ExpectedFailure(); // actually fails but we catch the exception and make the test pass. @Deprecated @Test public void testExpectedFailure() { Object o = null; o.equals(""foo""); } // fails @Test public void testExpectedFailure2() { Object o = null; o.equals(""foo""); } } First note that the first method is marked as @Deprecated. I'm using this as a marker for the method for which I want to ignore any assertion failures. You can do whatever you like to identify the methods this is just an example. Next in the ExpectedFailure#apply() when I do the base.evaluate() I'm catching any Throwable (which includes AssertionError) and if the method is marked with the annotation @Deprecated I ignore the error. You can perform whatever logic you like to decide whether you should ignore the error or not based on version number some text etc. You can also pass a dynamically determined flag into ExpectedFailure to allow it to fail for certain version numbers: public void unmarshalledDocumentHasExpectedValue() { doc = unmarshaller.unmarshal(getResourceAsStream(""mydoc.xml"")); expectedFailure.setExpectedFailure(doc.getVersionNumber() < 3000); final ST title = doc.getTitle(); assertThat(doc.getTitle().toStringContent() equalTo(""Expected"")); } For further examples see ExternalResource and ExpectedException Ignoring an expected failure test rather than passing it If you want to mark you tests as Ignored rather than Success it becomes a bit more complex because tests are ignored before they are executed so you have to retrospectively mark a test as ignored which would involve constructing your own Runner. To give you a start see my answer to How to define JUnit method rule in a suite?. Or ask another question. Brilliant! Just create an annotation like `ExpectedFailure(behavior=SucceedWhenTestFails)` and use that instead of `Deprecated` and the test would be perfectly readable and well documented.  One option is mark the test as @Ignore and put text in there that is a bug perhaps and awaiting a fix. That way it won't run. It will then become skipped. You could also make use of the extensions to suit your need in a potentially different way."
260,A,differences between 2 JUnit Assert classes I've noticed that the JUnit framework contains 2 Assert classes (in different packages obviously) and the methods on each appear to be very similar. Can anybody explain why this is? The classes I'm referring to are: junit.framework.Assert and org.junit.Assert. Cheers Don There is in fact a functional change: org.junit.Assert will complain if you use the two-argument assertEquals() with float or double while junit.framework.Assert will silently autobox it.  I believe they are refactoring from junit.framework to org.junit and junit.framework.Assert is maintained for backwards compatibility.  I did a rough source code compare and there are no serious changes. Lot of comments were added in org.junit.Assert and some refactorings are done. The only change is the comparison with Arrays. There are some code clean ups but there's (imho) no functional change. I believe @David Moles is correct and this would be a functional change.  JUnit 3.X: junit.framework.Assert JUnit 4.X: org.junit.Assert Prefer the newest one especially when running JDK5 and higher with annotation support.  The old method (of Junit 3) was to mark the test-classes by extending junit.framework.TestCase. That inherited junit.framework.Assert itself and your test-class gained the ability to call the assert-methods this way. Since version 4 of junit the framework uses Annotations for marking tests. So you no longer need to extend TestCase. But that means the assert-methods aren't available. But you can make a static import of the new Assert-class. That's why all the assert-methods in the new class are static methods. So you can import it this way: import static org.junit.Assert.*; After this static import you can use this methods without prefix. At the redesign they also moved to the new package org.junit that follows better the normal conventions for package-naming.
261,A,"Testing approach - DB Junit i would like to ask you about writting tests which are connected with data from database. In my opinion the best way is to have a diffrent DB schema with correct data only for unit testing. In test code i can load the object on the base of ID. The second possiblity is to putting data into database during the unit test. I dont like it. What do you think. How do you do this? Kind regards Sebastian If you really want to write tests that interact with a database - integration tests - then you will have to put your database in a known state before each test execution. One way to do this would be to load custom dataset before each test for example using DbUnit. Here each test is responsible of his own data there is no cleanup required and you can query the database after a failed test to understand the problem. Another way would be to use a ""live"" database and to run tests inside a transaction and to rollback the changes at the end of the test. Spring and Unitils have support for this. This works well too but it's not always easy to diagnose a failed test when using this approach. Note that the second approach doesn't really exclude the first one you can use custom dataset inside a transaction. Also note that you can use a database containing ""reference data"" (i.e. read-only data like countries etc) and only load ""dynamic data"" to speed up things when using the first approach. Personally I don't really see what's wrong with the former approach (except maybe that tests are a bit slower) tools like DbUnit make it quite easy. And as I said I find tests using DbUnit easier to diagnose. But the later approach definitely works too. In both case you should of course use a dedicated schema1. 1 Actually using one schema per developer is definitely a best practice. Great!!! Thank you very much. Pascal nice detailed answer. You mentioned it in there but I think it's important to stress that tests that interact with a live database are generally integration tests. Using a live a database for a unit test would be overkill and mocking it would be appropriate.  For unit tests you should mock or stub the functionality of the database. Since you are testing the object that calls the database and not the database itself there is no need to use a real database. EasyMock or JMock are good mocking libraries to look into. Thanks. What if i have a big object graph? Then i need to write much code to mock every reference to other object. Im using mocking in Service layer to mock the DAO layer because the DAO layer was arleady tested. And im also use mocking in GUI layer to mock the Service layer because the Service layer was already tested. Dont you think that mocking database to test the DAO layer is not so good idea? We want to test it but in this case we will be testing mock objects. What do you think? Thanks for answer. If there are too many objects to mock it could mean your design is too complicated and one class maybe has too much responsibility. Without seeing the actual code it's tough to tell but see if you can simplify it a bit. As for the database if you're mocking the database you're not testing a mock object if your test is for the DAO. You're testing that the DAO sends the correct commands to the database. For a DAO test it is safe to assume the database will behave as expected (and you can always mock it to do unexpected things to test error cases). Thank you very much.  Three comments: Unit tests are typically small scale fast tests to cover a little piece of code. Tests using a DB are not really unit tests (even if you can run them in e.g. JUnit) but rather system / integration tests. Both are useful for different purposes. Even if your object graph is big I don't see why you would need to test it all at once in a unit test. You test one DAO with the necessary mock objects and assert that the relevant part of the object graph is handled correctly. Then write more tests for the next DAO etc. IMO DAOs typically should not do very complex things (unless part of the business logic is put in there but that's bad design - should be refactored instead of trying to cover it with contorted unit tests). Thank you very much."
262,A,"Jmock Mockery mocking a file system object I want to be able to mock the File object in java using Mockery. I seems like we may not be able to create an interface for the File in java. Is this possible? EDIT: I need to test the indexDoc function in Indexer Class. @Test public void testindexDocs() { final File f = mockFile.mock(File.class); File file = new File(""test""); mockFile.setImposteriser(ClassImposteriser.INSTANCE); final String[] files = { ""C:\\test\\"" ""C:\\test\\test1.html"" ""C:\\test\\test2"" ""C:\\test\\test3.html""}; mockFile.checking(new Expectations(){ { one(f).list();will(returnValue(files)); } }); //TODO test if list() how many time i have called //Document doc = HTMLDocument.Document(file); in function indexDocs } Index Docs function in Indexer class private static void indexDocs(File file) throws Exception{ //Check for file to be a directory or file to be indexed look for html files and add to document if(file.isDirectory()){ String[] files = file.list(); Arrays.sort(files); for (int i = 0; i < files.length; i++) // recursively index them indexDocs(new File(file files[i])); } else if(file.getPath().endsWith("".html"") || file.getPath().endsWith(""htm"")){ // Get the document from HTMLDocument class which takes care of stripping of HTML tag get the path // of HTML file and title of HTML document. Document doc = HTMLDocument.Document(file); // TODO Get the book of HTML it can be a part of HTML document class. writer.addDocument(doc); } } Thanks Sharma The problems your having are the exact reason one should use abstractions rather than concrete classes.  Jmock can mock concrete classes. Just do Mockery context = new Mockery(); context.setImposteriser(ClassImposteriser.INSTANCE); I have editted the question..so that you understand my concern a little better  Don't mock the file system. We tried to do this in the early days and it diverted us from using tests to guide the design. From a quick look at your code there are two things going on one is file navigation the other is html stripping. Perhaps one option would be to introduce a html stripping object (passed in as a collaborator) and mock that then write tests against examples in a real file system."
263,A,Cobertura coverage for single file with maven How do you invoke the cobertura maven plugin when running unit tests for a single file so that you can get coverage reports for just that file I would suggest exlcuding all other files from instrumentation. I'm not really the maven expert :) But the cobertura:cobertura target seems to generate a report (http://mojo.codehaus.org/cobertura-maven-plugin/cobertura-mojo.html). Basically you always need to have some kind of report because the cobertura.ser file is not really human.readable. Good idea. Thanks for that. is it possible to view the report without using the maven site plugin to generate a site?
264,A,"How can i inject servletcontext in spring I need to write a junit test for a rather complex application which runs in a tomcat. I wrote a class which builds up my spring context. private static ApplicationContext springContext = null; springContext = new ClassPathXmlApplicationContext( new String[] {""beans""....}); In the application there is a call: public class myClass implements ServletContextAware { .... final String folder = servletContext.getRealPath(""/example""); ... } which breaks everything because the ServletContext is null. I have started to build a mock object: static ServletConfig servletConfigMock = createMock(ServletConfig.class); static ServletContext servletContextMock = createMock(ServletContext.class); @BeforeClass public static void setUpBeforeClass() throws Exception { expect(servletConfigMock.getServletContext()).andReturn(servletContextMock).anyTimes(); expect(servletContextMock.getRealPath(""/example"")).andReturn(""...fulllpath"").anyTimes(); replay(servletConfigMock); replay(servletContextMock); } Is there a simple methode to inject the ServletContext or to start the tomcat with a deployment descriptor at the runtime of the junit test? I am using: spring maven tomcat 6 and easymock for the mock objects. What you actually want is to test the web-layer. There are a couple of ways to do it: use MockServletContext provided by spring. This is the best way - check the linked documentation for how to do it. use ServletUnit from HtmlUnit use the cactus framework use selenium to test the web layer functionally make a runtime mock as you did And whenever you want to inject something in a test use ReflectionTestUtils The complication comes from the fact that the web-layer is not quite suitable for unit-testing. It is more a subject of functional testing. If there are methods that seem suitable for unit-testing they perhaps belong to the service layer. And they should not have a dependency on the ServletContext The mail goal is to save time: after each change I would have to run maven and then start tomcat. It would have been nice if I could use the junit test to check - in my case a webservice which has many dependencies - the functionality of the methode... basicly I want to wait less for each functiontest. I will look into ReflectionTestUtils. It seems promising. Thx for the hint."
265,A,"How does Java generate signatures for Methods? I have an Java class with a static final method getAll: public static final Vector<Category> getAll(Context context ContentValues where) { ArrayList<Integer> IDs = null; if(where != null && where.containsKey(DatabaseAdapter.KEY_PRODUCT)) { IDs = OvertureItem.getAll(context DatabaseAdapter.TABLE_PRODUCT_CATEGORY new String[] { DatabaseAdapter.KEY_CATEGORY } where); } else { IDs = OvertureItem.getAll(context DatabaseAdapter.TABLE_CATEGORIES where); } Vector<Category> categories = new Vector<Category>(); for(Integer id: IDs) { categories.add(Category.get(context id)); } return categories; } Now I want to hand in null as a value for the where statemant so that it will just be ignored later on in the code. Anyway in the testcase for this method I have: Vector<Category> categories = Category.getAll(context null); Which then in turn gives me a NoSuchMethodError. I don't know exactly why it does that. The only thing I could imagine is that the null I hand in would not match the signature of the above method. But how can I overcome this? I already thought of overloading. But this would just end in rewriting most of the code. At least when I do it how I think. Any suggestions on that? Phil P.S. This is the stack trace I get: java.lang.NoSuchMethodError: com.sap.catalogue.model.Category.getAll at com.sap.overture.test.model.CategoryTest.testGetAll(CategoryTest.java:59) at java.lang.reflect.Method.invokeNative(Native Method) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:169) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:154) at android.test.InstrumentationTestRunner.onStart(InstrumentationTestRunner.java:430) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1447) Look at the logcat output. Was the exception caused by some other failure e.g. a verification error? The real answer So I now finally figured it out and it wasn't as obvious as I expected. I started wondering when every new test case for any new method I wrote would give me the NoSuchMethodError. So I digged a little bit deeper and then suddenly it came to my mind: ""I changed the package name of the android application"". I thought this would not make any difference to the test project as long as I kept the properties right in the AndroidManifest.xml but I was wrong! In fact when your application package is named com.foo.bar.app the package for your tests has to be named com.foo.bar.app.test! What happened was that with my old configuration somehow the classes that sat in the bin/ folder were used. I thought that they should have been deleted when I cleaned the project but they weren't. This way all of the older test cases would still pass and only the new ones would give me the NoSuchMethodError. After I deleted the bin/ folder manually I got a whole bunch of errors. I then renamed the package holding the test cases and did a full clean/ rebuild on the project et voilá everything is back to normal again. Thanks for all the tips! I really appreciate your help that just kept me digging to the bottom of the problem. Hope this here will help anybody with the same problem in the future. Phil  If the method did not exist at compile-time then the code would not compile. If you get NoSuchMethodError at run-time then this suggests that the version of the Category class you are running against is different than the version of the Category class you are compiling against. What is your setup like - is this class in the same project? Are you copying in JARs from another project? +1 - but it doesn't just ""suggest"" this. AFAIK there is no other plausible explanation. The Category class is in the main project (android application). The tests sit in the test project that is directly related to the main application. It is somehow strange because all of the other tests pass without problems. Plus I don't do anything like changing the class at run-time. It is a plain model class. Holding data and the logic to manage the data. @Stephen I don't like to speak in terms of absolutes in the 0.1% chance I didn't think of something or that philgiese has some awkward setup :) @philgiese have you tried cleaning both projects and re-compiling? Also is the method overloaded by any chance? I have done a full clean on both projects without any change. The getAll() method is indeed overloaded. There is also a getAll() Method in the superclass OvertureItem. But the method in the superclass takes 3 parameters and not two. I added the stack trace I get to my original questions. Still don't have a clue why this happens."
266,A,"Java - How do I verify alt text appears when mouse hovers over a button? I am testing a site written in Javascript by writing (very simple) Java tests in Eclipse and running them as JUnit tests. As I am almost completely new to Java I am encountering problems all over the place. For example: on the webpage there are a few buttons that have alt-text that appears if the mouse is hovered over the buttons for half a second. I use Selenium IDE 1.0.10 to get the scLocator IDs but it does not pick up the ID for the alt-text pop-up. If in the Selenium IDE I use ""mouseOver"" or ""mouseOverAndWait"" nothing happens. The test does not fail as the element is present yet the alt-text does not appear. What java command can I use for mouseOver? I guess I can use ""assertScElementPresent('' 10*SECONDS);"" once I find the element but what command can I use in java to simulate a mouse hover over a button? I hope the question is clear and makes sense. Yes sorry I meant that alternative text that appears for img elements. The button image has alternative text when hovering the mouse over it. I think I am using the wrong name for it. Either way the command for hovering is as I found it `scElement(""elementID"").hover();` That means you are using some tool tip library. You'd probably have to say which one. You are probably right but I have no idea which one is used. I guess I'll have to ask the programmers. Either way thank you for all your help Sir. What exactly to you mean with ""alt text""? Do you mean the HTML `alt` attribute? Buttons don't have `alt` attributes only `img` elements do. Also there is no requirement for browsers to show them as tool tips (even if many do) because that is not their purpose. There is a `title` attribute that is more likely (but still not mandatorily) displayed as a tool tip and it applies to buttons. And it's unlikely that it's possible to check these tool tips with Selenium because they are usually shown as OS based tool tips without any JavaScript interface that could control them in any way. @RoToRa Actually the INPUT element can take an ALT attribute if its type is set to `image`. Thus: ``. Did you mean to say ""title"" attribute instead of alternate text? Hovering over a button displays the title. The img alt attribute (alt attribute) is used if the browser cannot find the image resource--it then displays the alt text. I don't know what your ids are so I'm assuming you are using a LinkButton control with an Image control inside of it given your details. I have two solutions below. If you're using this for using the variable later then use the first answer by using an Accessor and an Action. If you are just validating the UI then use the second answer which would be an Assert (validation command) alone. ASP.NET code: <asp:LinkButton ID=""LinkButton1"" runat=""server""> <asp:Image ID=""Image1"" runat=""server"" AlternateText=""img text"" ToolTip=""button"" ImageUrl=""http://www.google.com/intl/en_com/images/srpr/logo1w.png"" /> </asp:LinkButton> Rendered HTML code: <a id=""LinkButton1"" href=""javascript:__doPostBack('LinkButton1''')""><img id=""Image1"" title=""button"" src=""http://www.google.com/intl/en_com/images/srpr/logo1w.png"" alt=""img text"" /></a> <input name=""TextBox1"" type=""text"" id=""TextBox1"" /> #1 (Accessor & Action): Command: storeText Target: xpath=//img[@id='Image1']/@title Value: var_img_title Command: type Target: TextBox1 Value: ${var_img_title} #2 (Assert): Command: assertAttribute Target: xpath=//img[@id='Image1']/@title Value: button"
267,A,"Using Junit 4's Timeout @Rule with Spring's AbstractTransactionalJUnit4SpringContextTests When i use Junit's org.junit.rules.Timeout with spring's base class AbstractTransactionalJUnit4SpringContextTests i get this exception: org.springframework.dao.InvalidDataAccessApiUsageException: no transaction is in progress; nested exception is javax.persistence.TransactionRequiredException: no transaction is in progress The log output shows: 2010-07-20 09:20:16 INFO [TransactionalTestExecutionListener.startNewTransaction] Began transaction (1): transaction manager [org.springframework.orm.jpa.JpaTransactionManager@6a1fbe]; rollback [true] 2010-07-20 09:20:16 INFO [TransactionalTestExecutionListener.endTransaction] Rolled back transaction after test execution for test context [[TestContext@17b60b6 testClass = MyIntegrationTest locations = array<String>['classpath:/context.xml' 'classpath:/junit-context.xml'] testInstance = MyIntegrationTest@10a4d7c testMethod = myTest@MyIntegrationTest testException = org.springframework.dao.InvalidDataAccessApiUsageException: no transaction is in progress; nested exception is javax.persistence.TransactionRequiredException: no transaction is in progress]] Here is my test: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""classpath:/context.xml"" ""classpath:/junit-context.xml""}) @TransactionConfiguration(transactionManager = ""transactionManager"" defaultRollback = true) @Transactional public class MyIntegrationTest extends AbstractTransactionalJUnit4SpringContextTests{ @Rule public Timeout globalTimeout = new Timeout(30000); @Test public void myTest() { // transactional code here saving to the database... } } However whenever i comment out the rule it all works fine. How can i marry these two together to work correctly? The reason why the log shows a transaction being setup and shutdown is that it is living on the original thread which actually the timer is living on. And probably because the transaction uses some sort of ThreadLocal instance my code does not get wrapped in a transaction. I've figured out that the Timeout class starts a new thread and runs my test in it. That's why my transaction gets created but my code is not wrapped in it. Now to figure out how to get it to work how i want... Ahh i worked it out. The way i solved it was to setup the transaction programatically. @Autowired TransactionManager transactionManager; @Test public void test() { TransactionTemplate transactionTemplate = new TransactionTemplate(transactionManager); transactionTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus status) { status.setRollbackOnly(); // DO YOUR TEST LOGIC HERE } }); } Hope it helps.  LOL. You can simply also annotate your test method with @Transactional(timeout = 30) for a 30 second timeout. Which is a lot simpler. Minor note that timeout is in milliseconds not seconds To annotate a single test might not be really helpful if the timeout shall be applied to a lot of tests. Think of more tests in subclasses of MyIntegrationTest. The variable name globalTimeout suggest that the timeout shall be applied to more than one test."
268,A,"JUnit: same tests for different implementaions and fine grained result states we have two different implementations of the same interface. Think of it as a reference and a production implementation. The two implementations are implemented by different teams and the goal is to get the same results from both implementations. The team creating the reference implementation has created a large Junit based amount of test cases (right now ~700 test cases) and those unit tests are run frequently during development. We can run the same set of test cases against the production implementation. Functionality of the production implementation is tested via regression testing. However beeing able to run the unit tests against the production implementation gives us a quick feedback whether something got seriously broken each time we get a new release of the production code. But since certain functionality int the production release is missing or results differ because of known bugs not all tests pass with this implementation. This makes it hard to spot regressions early. There are several categories here: (A) test cases that are only meaningful for the reference implementation and will never be important for the production implementation (B) test cases where only certain assertions have to be omitted when testing the production implementation (ie. additional values reported in reference implementaion) (C) test cases that are known not to work in the production implementation because development of certain features lags behind but should be included later So far we have these options: Cluttering our code with if-statements surrounding assertions that only work in reference implementation. This solves (B) but is hard to maintain. Using assumeTrue. This is OK for (A) but gives the false impression everything is OK in (B). What I'd like to have is Being able to skip certain tests based on a runtime condition like with assumeTrue but these should be reported as skipped rather than successful for (C) Having more result states that take into account whether a test case is known to have worked before that gives Success for a test case that was known to have worked before Fixed for a test case that was known not to have worked before Failure for a test case that was known not to have worked before Regression for a test case that was known to have worked before Skipped Has anyone done something like that before or is it even possible with JUnit (and preferably in conjunction with using the eclipse JUnit plugin)? I don't know about the first part of your question but as far as the multiple result states are concerned you're probably going to need some sort of continuous integration. As far as I know there is no way for JUnit to ""know"" whether your test cases have succeeded or failed in the past so you're going to need to get this information elsewhere. The support I mentioned is provided by the eclipse plugin not JUnit itself. While having to annotate the test cases manually is more work it has the advantage of being able to mark a test case that should be working even if in the last version it failed. Thinking about it it would be a nice feature to have this set at runtime via a condition like `@ExpectSuccess(condition=""version>=123"")`. I was thinking of providing Junit with this information either as Code or Annotation. In fact the eclipse plugin does have some kind of support for retrieving results from a previous run but no comparison support and results are stored on the local machine only. The CI part is already on my list for later this year. To tell the truth I'm not aware of JUnit offering any support for passing to it previous test results. Even if there is a way to do that it will proabably have to involve reading an external file since editing your code/annotations every time a test result chenges is not very practical. You could always retrieve the results and make the comparison yourself as MatthieuF suggests but that seems to me like reinventing the wheel.  To skip a test with a runtime condition you can use Filter and you can choose to ignore a test or not depending upon a runtime condition based upon an aspect of the test (a name better an Annotation @Development() or @Version() on the test method). To use this to solve (B) you would need different test methods for each version one for 3.1 and one for 3.2 etc. This may seem like it clutters your unit tests but actually it makes your job easier to pick out the tests that apply to 3.1. For the 'time machine' part of your question it's very hard for junit to know whether a test has passed before. You would need to record the old results somewhere. To analyse which tests have changed status (from passed to failed) get your junit tests run in a systematic way by a CI system for instance and then save the results somewhere where they can be post-processed to give you regressions. For instance surefire xml reports are fairly easy to parse.  Could you create a base class that contains all test cases known to work in both cases and extend that with two other classes that contain the test cases that are particular to the production and reference implementations? If it's more granular than that ( for example a test is quite long and 90% of it works in both prod and reference ) then you could take the same approach but put the assertions that differ in overridden methods in the subclasses. ( you'd had to create a number of abstract methods in the base class to support this )"
269,A,"How to use JMock to test mocked methods inside a mocked method I'm having trouble determining how to mock a particular piece of code. Here is my method: public void sendNotifications(NotificationType type Person person) { List<Notification> notifications = findNotifications(type); for(Notification notification : notifications) { notification.send(person); } } I would like to be able to use JMock to test that findNotifications is called and that and returns the expected value and that send() is called. findNotifications calls my Dao which is mocked. Notification is an abstract class. I have a unit test like this but it's obviously not working. It satisfies the first two expectations but no more.  @Test public void testSendNotifications2() { final Person person = new Person(); notificationEntries.add(createEntry1); Mockery context = new JUnit4Mockery() {{ setImposteriser(ClassImposteriser.INSTANCE); }}; final NotificationDao dao = context.mock(NotificationDao.class ""notificationDao""); final Notification mockedNotification = context.mock(V2ADTNotification.class ""mockNotification""); notifications.add(mockedNotification); final NotifierServiceImpl mockedService = context.mock(NotifierServiceImpl.class ""mockedService""); //NotifierService service = new NotifierServiceImpl(dao); context.checking(new Expectations() { { one(mockedService).setDao(dao); one(mockedService).sendNotifications(NotificationType.CREATE person); one(mockedService).findNotifications(NotificationType.CREATE); one(dao).getByNotificationType(NotificationType.CREATE); will(returnValue(notificationEntries)); will(returnValue(notifications)); one(mockedNotification).send(person); } }); mockedService.setDao(dao); mockedService.sendNotifications(NotificationType.CREATE person); context.assertIsSatisfied(); } mockedService.sendNotifications(NotificationType.CREATE person); context.assertIsSatisfied(); } How could I get this to work as I want it to? Another way I tried. It satisfies the first two expectations but not the send one. @Test public void testSendNotifications() { final Person person = new Person(); notificationEntries.add(createEntry1); Mockery context = new JUnit4Mockery() {{ setImposteriser(ClassImposteriser.INSTANCE); }}; final NotificationDao dao = context.mock(NotificationDao.class ""notificationDao""); final Notification mockedNotification = context.mock(V2ADTNotification.class ""mockNotification""); NotifierService service = new NotifierServiceImpl(dao); context.checking(new Expectations() { { one(dao).getByNotificationType(NotificationType.CREATE); will(returnValue(notificationEntries)); one(mockedNotification).send(person); } }); service.sendNotifications(NotificationType.CREATE person); context.assertIsSatisfied(); } I'm pretty new to using Jmock so I apologize if it doesn't look like I have much of a clue as to what I'm doing (I don't). test mocked methods inside a mocked method. You really just can't do that. The calls in the real method aren't being made because you're calling the mock instead of the real method. Your first attempt fails the last two expectations because the calls that would make them pass are made only in the real implementation of sendNotifications and not in the mock that you made of it. The second is closer to workable but you need to add your mockedNotification to the notificationEntries list that you've set up to be return by the mock of getByNotificationType. I figured as much but didn't know for sure if I was missing something. I took a different route. Inside the findNotifications method I was using a static factory. I've decided to make the factory an injectable class instead of a static one so now I'm mocking it and now I can test for the behavior I want (otherwise I never would have been able to get the mocked Notification objects instead of just the mocked NotificationEntry objects -- probably should have included more code). I'll mark your answer as correct since it does address my problem and I've explained my solution in this comment."
270,A,Testing a final class in JUnit All I have a class file that is used to store all the constants that would be used throughout my application. The class file contains only constants and methods JUnit test case for such a class? If yes how should I test such a class file? It doesn't sound like a great idea to keep all the constants for the whole application in a single class to start with. Why separate them from the class which is most closely related to the value? For example if you have a default timeout for authentication requests keep it with your authentication service (or whatever). If you really want to have a class with just constants I don't think it needs any tests. Tests should exercise logic - constants don't have any logic.
271,A,"How to develop a Nightly Builder I was told to create a tool like a Nightly Builder for a JUnit project. It's a client-server project with oracle database.The tests are based on QTP. Also there is a test interface written on C#. The tester can click on the interface to choose which tests to run and get a report from each test. So I have to make this procedure automated. So what tools should I use? Thanks in advance Best regards Why not use one of the excellent existing systems that do everything already? You should use Quartz. In the quart scheduling xml file you can specify it to build your project. In your project you should have junit test cases execute whenever a build happens. This way you can achieve a daily build process. If your project already utilizes Spring framework then you can use the spring job scheduler helper library too (it's a wrapper around quartz). Ideally you should use hudson to manage the daily builds but I am not sure if your organization utilizes it or not. Hope this helps.  Have you considered using CruiseControl or like tool? We use this at my work and it was easy to get up and running with Junit and/or TestNG. Other tools to consider are buildbot continuum hudson etc. (Go to google and type ""cruisecontrol vs"" and see a bunch of other auto builder tools.) Then see how they handle nightly builds.... here's a reference for CruiseControl. Do you know how to use CruiseControl with QTP? If so please answer this question http://stackoverflow.com/questions/2601325/how-to-implement-cruise-control-with-qtp"
272,A,"Running Junit Email Tests Without Emails Actually Going Out I want to run unit tests (Junit) on some model classes that typically will send emails confirming that things happened. Is there a mock email server that you can use with unit tests that will let you confirmation that your run tried to send an email without actually sending the emails out? This seems like it would be a nice to have just not sure I want to write my own. The app stack for the emailing aspect is Velocity+Spring so it would be preferable if the test server can be pointed at by simply changing the applicationContext.xml file. Check my answer http://stackoverflow.com/questions/8599791/a-simple-local-smtp-server/22043597#22043597 Alternative answer: Dumbster is a fake SMTP server designed for testing against. It's written in Java. +1 for this approach although SubEthaSMTP might be easier to work with than Dumbster. This does exactly what I asked for. I am going to use it for now I'll probably switch to using a mock object at some point to speed the test up. Just a note - running with Dumbster my unit test is taking about 500ms to startup the server send an email and test that the email was sent. This is significantly better than my Jersey framework tests which take ages.  I assume you're using Javamail and the problem is that javax.mail.Session is final and therefore can't be mocked. Seems like others have suggested you simply define your own 'mail session' interface and create an implementation that uses Javamail. In your tests you then simply inject a mock while in 'real-world-mode' you inject the Javamail implementation. Both JMock and EasyMock will support all the assertions you might want to make on the message you are sending and you testing is complete. As an aside I generally try to avoid any out-of-process calls from within unit tests - it kills you when you're running the test suite frequently which normally translates into it being run less and that's code base issues start to occur. That's really a good comment and made me think about my approach some more. I was thinking to spawn a process but more likely I should be instantiating a bean in my spring config xml that mocks an email server. Hi Steve - that's exactly the point. If you've got another process running as your end-point all of a sudden the asserts become hard so your test ends up simply making sure you don't get errors from the other process which probably isn't the same one that will be used in anger so is really a mute test - after all I think it's safe to assume Javamail works ;-) The mocking approach allows you to validate the content of the message which I think is the real test....  My solution was to wrap the mail server in a class which takes all the config options and has a send() method. In my tests I'd mock this class and override send() with something that saves the current parameters for the assert. To test that the mail service itself works send yourself a mail locally. Try hMail if you're on Windows. I develop on Windows but I run the continuous integration server on Ubuntu so that won't work. Sure; use the address ""localhost"" or ""127.0.0.1"" and add the same account on Ubunutu for the mail as on Windows. I'm not sure what Ubuntu uses but it already has a mail server installed. Probably qmail or postfix.  You can try JavaMail Mock2 https://github.com/salyh/javamail-mock2 Its primarily focused on IMAP/POP3 but SMTP Mock is also available. Its available in maven central. Features Support imap imaps pop3 pop3s smtp smtps Supported for POP3: cast to POP3Folder Folder.getUID(Message msg) Supported for IMAP: cast to IMAPFolder cast to UIDFolder Subfolders -Folder.getMessagesByUID(...) delete/rename folders append messages Support for SMTP: Mock Transport.send() Unsupported for the moment: IMAP extensions like IDLE CONDSTORE ... and casts to POP3Message/IMAPMessage store listeners  I think the Mock JavaMail project is what you want. Absolutely - never mock out a 3rd party interface yourself it's rarely worth the time. (if you DID want to - use PowerMock). But in this case just drop Mock JavaMail in place and magically all your messages will be held in a mock Mailbox for you to examine and assert over. Simple and effective (just keep it out of your production classpath !) there's a quickstart: http://ollivander.franzoni.eu/2011/08/30/mock-javamail-primer/  Phil Haack has a blog post about unit testing email sending with a solution he coded around a freeware mail server."
273,A,"Ending a JUnit Test I have a ""black box"" of code that contains many threads. There is no method to ""kill"" the black box threads that I have found. I am passing data into the ""black box"" through junit and making sure that I am getting the expected outputs. Is there a way to gracefully exit the test case (It keeps it open in eclipse because of the threads in the black box)? In very short no. A JUnit way to handle this is to abstract the thread creation and pass in a fake one under test. JUnit is really about white-box testing not black box testing of multiple threads. It could be done of course but issues like this are going to be painful. Fortunately there are libraries out there that can help with this problem. One such library is ConcoJUnit."
274,A,Applying one test to two separate classes I have two different classes that share a common interface. Although the functionality is the same they work very differently internally. So naturally I want to test them both. The best example I can come up with; I serialize something to a file one class serialize it to plaintext the other to xml. The data (should) look the same before and after the serialization regardless of method used. What is the best approach to test both classes the same way? The tests only differs in the way that they instantiate different classes. I dont want to copy the entire test rename it and change one line. The tests are currently in JUnit but Im going to port them to NUnit anyway so code doesnt really matter. Im more looking for a design pattern to apply to this test suite. I tend to avoid any relations between test classes. I like to keep testcases (or classes) as atomic as possible. The benefit of using inheritance here doesn't outweight the strong coupling you get by it. I guess it would be helpful if you could share the validation of the result of the two classes (Assuming blackbox tests). If both classes are able to let you set an outputstream you might validate that while the classes itself write to PrintWriter or FileWriter (or whatever you need in your cases). Furthermore I would avoid to create files during unit-tests because it might take too much time (+ it might not work on the build machine) and therefore delay your build. Very valid points indeed but keeping them totally separate would mean something like 1000 lines of (near) identical code. Changing the interface or how it works seems a bit like a can of worms when two tests should be updated. But maybe youre right maybe thats something you get when you write two classes with to similar logic. And you wrote the classes *before* you wrote the test Maybe that teaches you a lesson. ;-----)  In C# I'd use a generic helper method to test both cases something like: internal static void SerializationTestHelper<T>() where T : IMySerialize { T serialize = new T(); // do some testing } [TestMethod] public void XmlTest() { SerializationTestHelper<XmlSerialize>(); } [TestMethod] public void PlainTextTest() { SerializationTestHelper<PlainTextSerialize>(); }  I'd reuse the code either with inheritance or aggregation. To have the shortest code I'd move a tested instance creation to a factory method in say XmlImplementationTest class and inherit a TextImplementationTest from it: XmlImplementationTest extends TestCase { Interface tested = null Interface createTested() { return new XmlImplementation() } ... void setUp() { tested = createTested(); } } TextImplementationTest extends XmlImplementationTest { override Interface createTested() { return new TextImplementation() } } This is not completely correct OO design as it's TextImplementationTest is NOT a XmlImplementationTest. But usually you don't need to care about it. Or readdress the test method calls to some common utility class. This would involve more code and not show proper test class in test reports but might be easier to debug.  Create a common abstract base test class for the test. abstract class BaseTest{ @Test public void featureX(){ Type t = createInstance(); // do something with t } abstract Type createInstance(); } ConcreteTest extends BaseTest{ Type createInstace(){ return //instantiate concrete type here. } } Good option but you'll also need to mark `BaseTest` ignored in order for xUnit not to try to run it. Depends on the runner. There are more or less intelligent. The former see that the class is abstract and the later can be configured to ignore the class depending on patterns.
275,A,"Fixtures in JUnit and file structure I'm building a card game which is only relevant because of the examples given below. I consider myself a fairly experienced C++ programmer with considerable TDD experience in that language; most recently using UnitTest++. I am new to Java and intend to target Android once I get my Java legs. What I want to do is something akin to this UnitTest++ setup: class DeckTest { public: Deck deck; }; class EmptyDeck : public DeckTest { // doesn't need to do anything for this simplified example }; TEST_FIXTURE(EmptyDeck HasNoCards) { CHECK_EQUAL(0 deck.GetNumCards()); } TEST_FIXTURE(EmptyDeck ThrowsOnCardAccess) { CHECK_THROWS(InvalidCardIndexException deck.GetCard(0)); } class DeckWithCards : public DeckTest { void setUp() { // load deck with a bunch of cards } }; TEST_FIXTURE(DeckWithCards FirstCardIsAccessible) { // ...etc. Now in C++ I'd just throw this all into a DeckTest.cpp file and be done; multiple fixtures all testing one client class. Makes sense to me. However in Java I feel like I want to do something similar: class DeckTester { Deck deck = new Deck(); } class EmptyDeck extends DeckTester { @Test public void EmptyDeckHasNoCards() { assertThat(deck.GetNumCards() equalTo(0)); } @Test(expected=Deck.EmptyDeckException.class) public void EmptyDeckThrowsWhenGettingCard() throws Deck.EmptyDeckException { Card card = deck.GetCard(0); } } class DeckWithCards extends DeckTester { @Before public void AddCards() { Card card = new Card(Card.Type.k_missed); deck.AddCard(card); // ...or similar... } } public class DeckTests { // What goes here? } ...since I can only have one public class per module I figured I'd try to build a suite or something but I can't figure out the way to do it. I'm using the various AllTests.java in the junit-4.8.2 distro to guide me but the one that looked most promising (org.junit.tests.AllTests) gives me compile errors when I try to mimic it. I figured the preferred way would be to have internal classes but junit doesn't pick those up either. It feels yucky to me to have to split these out into different files but maybe that's just the Java way? Tips most appreciated thanks! Edit: Largely I'm curious about file structure. Given the lack of response I'm starting to think that it's not particularly feasible to have all of these things in one file. So when developing tests like this how do people structure them in Java? Would I create a test.deck package with a number of test classes containing only a small number of tests or do folks just go ahead and embrace the duplication and jam them all into one test class? (Am I giving up too easily by feeling spoiled by the ease of use of UnitTest++ where I include one file to get all of its features and where I can have a bunch of classes that test a feature in one file?) Java has a limitation where only one public top level class can be in a file so the type of style you are looking for is not quite the ""java"" way of doing it but if you want that style it is more natural in TestNG so I suggest you also consider that framework. In JUnit what you do is have an outer class that is a holder of all of these various classes: @RunWith(Suite.class) @Suite.SuiteClasses({EmptyDeck.class DeckWithCards.class}) public class AllTests { public static class DeckTester { ///etc. } public static class EmptyDeck.class extends DeckTester { ///etc. Or you can look the Enclosed.class as an alternative runner but you would run into issues with the DeckTester since it has no tests itself. In general prefer composition over inheritance so the pattern of inheriting the fixture rather than composing it gives me pause. It may be ok but it may be too limiting in Java. Thanks. For whatever reason this does not work in junit 4.8.2 but I will take a look at TestNG. I realized not long after posting my query that the inheritance was silly for such a trivial operation (creating the deck) so now I would have several fixtures that construct the deck differently but the one-class-per-test-file apparent limitation is really dragging me down emotionally. :) I suppose I can put these test fixtures in different files but I'm spoiled by UnitTest++ where I write a test and it runs; in JUnit it seems I have to write the test then mention it to some other system... @dash-tom-bang it should work but I have seen some funky stuff with enclosed classes. I agree that the suite running in JUnit needs some work. They have some initial stuff to address that in 4.9 (not released yet). I think you will be happier with TestNG if you get past the XML requirements ... @dash-tom-bang regarding the fully qualified name you can get around that with a static import. @dash-tom-bang An internal class without a static modifier is tied to an instance of the outer class. See here: http://download.oracle.com/javase/tutorial/java/javaOO/nested.html @dash-tom-bang in JUnit 3.x suites required a static method I think that is what it is referring to. What you get with TestNG is much more flexibility about organizing your tests. It doesn't have rules like required constructors and stuff like that - leaving it to annotations to express structure more naturally. It isn't better than JUnit in every respect (in fact I prefer JUnit I just roll with its organizational requirements). Thanks- still haven't gotten around to TestNG but I'll check it out right now... I'm not a huge fan of XML but I can suck it up for now... Huh wow. Ok the articles that the TestNG author wrote pretty much expose a view on testing diametrically opposed to my own. I am not a huge fan of ""all tests in a suite implicitly depend on each other"" and his arguments against TestSuite are both addressed and completely ridiculous (TestSuite requires static methods? Orly?) Ok jeez I can't believe I failed basic reading comprehension. The 'static' modifier on the internal classes was escaping my vision until today when I went to make my first Matcher. The Matcher is still evading me but I did eventually figure out what was causing my factory to fail it was the missing static. Coming from C++ the 'static' label on an internal class still eludes me (specifically what does a non-static class do?) but I'm sure a bit of time with google will help me work that out. Oh- one note; the SuiteClasses needs to have the wrapper class's name in there too so it's really a bit silly: `({ AllTests.EmptyDeck.class AllTests.DeckWithCards.class })` but it'll do for now. I still hate that I have to add that duplication but it's better than having multiple test ""scenarios"" (or fixtures in the UnitTest++ vernacular) in one class. Thanks. Hadn't thought to static import the module I was in but will try it out.  We can create an empty class which has the runnable classes in annotation: @RunWith(Suite.class) @Suite.SuiteClasses({My1Test.class My2Test.class My2Test.class}) public class AllTests { } Yeah that's what I thought too. Every test gives me an error though. ""Test class should have exactly one public constructor"" (shouldn't the default ctor work?) and ""Class My1Test should be public"" (umm the suite is public shouldn't that be enough?). Five failures for three test methods one for each class about the ctor and one per test for the class being public. No it isn't enough being the suite public. Test class should be also public and if you do that .ctor problem will also disappear. Just try it."
276,A,"Looking for a tutorial on using JUnit with Intellij IDEA 9.x I need an absolute beginners guide to using JUnit and Intellij IDEA 9.x together. I'm running JDK 1.6.0_22 on WinXP. I'm looking for answers to the following questions: Do I need to install JUnit or is it already integrated into Intellij? If I need to set it up how? Assuming I have the interface and impl for a class I want to test how do I set up a JUnit project? Is there a way to autogen a test skeleton based on the class interface? I've got experience with other Unit testing frameworks like PHPUnit and Boost.Test so I'm primarily concerned with the mechanics of getting all this set up and running in Intellij. Edit I'm a complete newb using Intellij. I'm coming from a command-line background i.e. C++ dev using vim and handwritten make files on Linux. I've managed to compile and run some JUnit tests via command line ( downloaded JUnit 4.8.2 and used the -cp swith) but I'm having a heck of a time getting anything set up under Intellij. I tried looking at the online Intellij docs but haven't found those to be very useful. I looked in Intellij's lib directory and it includes Junit-4.7.jar. I really need some kind of quick start guide with step by step instructions from initial project creation through successfully running the first unit test. Do you still need this? Why was this closed? Looks like a real question to me. Should be opened... IDEA comes with JUnit support but bear in mind that you need to invoke tests on your project - and hence your project will need to have JUnit on its classpath. Hence you don't need to ""install"" JUnit but you'll need to make its JAR available to your project as you would with any other third party library (e.g. reference it as a Maven dependency or drop the JAR in lib/). This is where the IDEA support kicks in - I'm 99% sure you don't need to do anything special. Since using JUnit in general simply involves annotating your test methods with @org.junit.Test as per this quick tutorial IDEA does not require anything further. For any class with this annotation on at least one of its methods IDEA will give you the option to execute that class as a JUnit test case. (Note: this may only be the case for files in a directory that's marked as ""Test Sources"" in the project structure but I haven't tested this. It's a good idea to set your project up like this anyway.) Not by default as part of the JUnit support. There exist other plugins to do this but in my personal opinion this is not as useful as it might sound. IDEA does have integrated code coverage (with the full edition) which (again IMHO) is a better way to ensure that your tests cases cover the full functionality of your class/interface. +1 Andrzej. One can use JUnitGenerator plugin to generate test cases from class. @Andrzej Doyle: Could you elaborate a bit on point 2. I'm completely new to JUnit. I've explained a little more specifically about JUnit tests in the second point and included a link to a short tutorial. I didn't realise you were completely new to JUnit; let me know if there's anything that's still unclear and not explained in the link. @Peter Knego: I tried JUnitGenerator but it doesn't seem to do anything really useful. It only generated tests for the set / get methods and seems to have ignored all the other public methods in the interface. Was this JUnitGenerator or JUnitGenerator2? @Peter Knego: It's version 1.1.7 @Andrzej Doyle: I was able to run JUnit tests via command line but can't get anything working via the IDE. See the edit to my OP.  Well I realize that this is much too late for you but for the sake of others reading this I've written just such a tutorial albeit using IntelliJ idea 12. It should work on most recent versions that support Maven. http://www.particlewave.com/2013/05/11/a-simple-junit-tutorial-using-maven-and-intellij-idea/ Great I'm sure others might find it helpful although right after you posted it some ""community moderator"" decided to close the question as ""not a real question"". I don't really get that...  I guess you need to download junit library jar and add it to module settings of your project which you can get pressing Shift+Ctrl+Alt+S In the module settings itself you need to select your test folders. Select the test folders and click on the test sources button on the top To create a test for a corresponding class press Shift+Ctrl+T. I dont think there are any live templates you may need to create it. Press Ctrl+Alt+S and go to live templates and create one looking into other templates for examples.  For a class named Foo: public class Foo { public static int add(int x int y) { return x+y; } } write a JUnit test like this: public class FooTest: { @Test public void testAdd() { int expected = 5; int actual = Foo.add(2 3); Assert.assertEquals(expected actual); } } I create /src and /test folders in my IntelliJ project and mirror the package structure under each one. You can have IntelliJ run all the tests in your project by right clicking on the /test folder and selecting ""Run all tests"". If you edit configurations on your ""all tests"" task you'll see a checkbox to ask IntelliJ to check code coverage for you. It'll refresh the values after every test run. Will this also work with an external build tool like ant? You'll build and run test with either IntelliJ or Ant. Your Ant task will use the Ant tasks. The two are orthogonal. Sorry but I'm a complete newb using Intellij. I'm coming from a command-line background i.e. C++ dev using vim and handwritten make files on Linux. I've managed to compile and run some JUnit tests via command line but I'm having a heck of a time getting anything set up under Intellij. I tried looking at the online Intellij docs ( http://www.jetbrains.com/idea/webhelp/index.jsp ) but haven't found those to be very useful. I really need some kind of quick start guide with step by step instructions from initial project creation through successfully running the first test. No worries Robert we were all first-time users once. We're all ignorant just about different things. I love IntelliJ but I recognize the value of an experienced guide. I don't have time now - work calls - but I'll see if I can do something later tonight."
277,A,"How can I improve my junit tests Right my junit tests look like a long story: I create 4 users I delete 1 user I try to login with the deleted user and make sure it fails I login with one of the 3 remaining user and verify I can login I send a message from one user to the other and verify that it appears in the outbox of the sender and in the inbox of the receiver. I delete the message ... ... Advantages: The tests are quite effective (are very good at detecting bugs) and are very stable becuase they only use the API if I refactor the code then the tests are refactored too. As I don't use ""dirty tricks"" such as saving and reloading the db in a given state my tests are oblivious to schema changes and implementation changes. Disadvantages: The tests are getting difficult to maintain any change in a test affects other tests. The tests run 8-9 min which is great for continuous integration but is a bit frustrating for developers. Tests cannot be run isolated the best you can do is to stop after the test you are interested in has run - but you absolutely must run all the tests that come before. How would you go about improving my tests? Read some code from other people to see how they do it. unit tests should - ideally - be independent and able to run in any order. So I would suggest that you: break up your tests to be independent consider using an in-memory database as the backend for your tests consider wrapping each test or suite in a transaction that is rolled back at the end profile the unit tests to see where the time is going and concentrate on that if it takes 8 minutes to create a few users and send a few messages the performance problem may not be in the tests rather this may be a symptom of performance problems with the system itself - only your profiler knows for sure! [caveat: i do NOT consider these kinds of tests to be 'integration tests' though i may be in the minority; i consider these kinds of tests to be unit tests of features a la TDD] you said you don't consider andy's existing tests integration tests but then propose changes that make them unit tests I think that can misguide people that said the downvote wasn't me as I do consider your answer to provide good extra info Regarding TDD please check my second update to my answer. We are not saying completely different things just adding different pieces of information. @[Freddy Rios]: ah now it makes sense thanks! Steven so you are basically saying to change the whole approach of the tests and yet you claim existing ones aren't integration tests? - they are testing integration with external resources (i.e. db) and are a interrelated story that affects several parts of the system. Instead your modified version promotes testing more specific features avoid hitting the external system and has no side effects to avoid interrelation with other tests (rollback). You just had in mind your version of the tests when you said you don't consider them integration tests ;) @[Freddy Rios]: Sorry I don't understand your comments. Esko's proposed tests would be more in line with TDD.  Reduce dependencies between tests. This can be done by using Mocks. Martin Fowler speaks about it in Mocks aren't stubs especially why mocking reduces dependencies between tests.  First understand the tests you have are integration tests (probably access external systems and hit a wide range of classes). Unit tests should be a lot more specific which is a challenge on an already built system. The main issue achieving that is usually the way the code is structured: i.e. class tightly coupled to external systems (or to other classes that are). To be able to do so you need to build the classes in such a way that you can actually avoid hitting external systems during the unit tests. Update 1: Read the following and consider that the resulting design will allow you to actually test the encryption logic without hitting files/databases - http://www.lostechies.com/blogs/gabrielschenker/archive/2009/01/30/the-dependency-inversion-principle.aspx (not in java but ilustrates the issue very well) ... also note that you can do a really focused integration tests for the readers/writers instead of having to test it all together. I suggest: Gradually include real unit tests on your system. You can do this when doing changes and developing new features refactoring appropriately. When doing the previous include focused integration tests where appropriate. Make sure you are able to run the unit tests separated from the integration tests. Consider your tests are close to testing the system as a whole thus are different from automated acceptance tests only in that they operate on the border of the API. Given this think about factors related to the importance of the API for the product (like if it will be used externally) and whether you have good coverage with automated acceptance tests. This can help you understand what is the value of having these on your system and also why they naturally take so long. Take a decision on whether you will be testing the system as a whole on the interface level or both the interface+api level. Update 2: Based on other answers I want to clear something regarding doing TDD. Lets say you have to check whether some given logic sends an email logs the info on a file saves data on the database and calls a web service (not all at once I know but you start adding tests for each of those). On each test you don't want to hit the external systems what you really want to test is if the logic will make the calls to those systems that you are expecting it to do. So when you write a test that checks that an email is sent when you create an user what you test is if the logic calls the dependency that does that. Notice that you can write these tests and the related logic without actually having to implement the code that sends the email (and then having to access the external system to know what was sent ...). This will help you focus on the task at hand and help you get a decoupled system. It will also make it simple to test what is being sent to those systems. These definitely aren't unit tests if they take 8+ minutes to run - in my current project the ~500 unit tests run in < 10 seconds  You can use JExample an extension of JUnit that allows test methods to have return values that are reused by other tests. JExample tests run with the normal JUnit plugin in Eclipse and also work side by side with normal JUnit tests. Thus migration should be no problem. JExample is used as follows @RunWith(JExample.class) public class MyTest { @Test public Object a() { return new Object(); } @Test @Given(""#a"") public Object b(Object object) { // do something with object return object; } @Test @Given(""#b"") public void c(Object object) { // do some more things with object } } Disclaimer I am among the JExample developers.  If you use TestNG you can annotate tests in a variety of ways. For example you can annotate your tests above as long-running. Then you can configure your automated-build/continuous integration server to run these but the standard ""interactive"" developer build would not (unless they explicitly choose to). This approach depends on developers checking into your continuous build on a regular basis so that the tests do get run! Some tests will inevitably take a long time to run. The comments in this thread re. performance are all valid. However if your tests do take a long time the pragmatic solution is to run them but not let their time-consuming nature impact the developers to the point that they avoid running them. Note: you can do something similar with JUnit by (say) naming tests in different fashions and getting your continuous build to run a particular subset of test classes.  Since everyone else is talking about structure I'll pick different points. This sounds like a good opportunity to profile the code to find bottleknecks and to run it through code coverage to see if you are missing anything (given the time it takes to run it the results could be interesting). I personally use the Netbeans profiler but there are ones in other IDEs and stand alone ones as well. For code coverage I use Cobertura but EMMA works too (EMMA had an annoyance that Cobertura didn't have... I forget what it was and it may not be an issue anymore). Those two are free there are paid ones as well that are nice.  Now you are testing many things in one method (a violation of One Assertion Per Test). This is a bad thing because when any of those things changes the whole test fails. This leads it to not being immediately obvious why a test failed and what needs to be fixed. Also when you intentionally change the behaviour of the system you need to change more tests to correspond the changed behaviour (i.e. the tests are fragile). To know what kind of tests are good it helps to read more on BDD: http://dannorth.net/introducing-bdd http://techblog.daveastels.com/2005/07/05/a-new-look-at-test-driven-development/ http://jonkruger.com/blog/2008/07/25/why-behavior-driven-development-is-good/ To improve the test that you mentioned I would split it into the following three test classes with these context and test method names: Creating user accounts Before a user is created the user does not exist When a user is created the user exists When a user is deleted the user does not exist anymore Logging in When a user exists the user can login with the right password the user can not login with a wrong password When a user does not exist the user can not login Sending messages When a user sends a message the message appears in the sender's outbox the message appears in the reciever's inbox the message does not appear in any other message boxes When a message is deleted the message does not anymore exist You also need to improve the speed of the tests. You should have a unit test suite with good coverage which can run in a couple of seconds. If it takes longer than 10-20 seconds to run the tests then you will hesitate to run them after every change and you lose some of quick feedback that running the tests gives you. (If it talks to the database it's not a unit test but a system or integration test which have their uses but are not fast enough to be executed continually.) You need to break the dependencies of the classes under test by mocking or stubbing them. Also from your description it appears that your tests are not isolated but instead the tests depend on the side-effects caused by previous tests - this is a no-no. Good tests are FIRST.  By testing stories like you describe you have very brittle tests. If only one tiny bit of functionality is changing your whole test might be messed up. Then you will likely to change all tests which are affected by that change. In fact the tests you are describing are more like functional tests or component tests than unit tests. So you are using a unit testing framework (junit) for non-unit tests. In my point of view there is nothing wrong to use a unit testing framework to do non-unit tests if (and only if) you are aware of it. So there are following options: Choose another testing framework which supports a ""story telling""-style of testing much better like other user already have suggested. You have to evaluate and find a suitable testing framework. Make your tests more “unit test”-like. Therefore you will need to break up your tests and maybe change your current production code. Why? Because unit testing aims on testing small units of code (unit testing purists suggest only one class at once). By doing this your unit tests become more independent. If you change the behavior of one class you just need to change a relatively small amount of unit test code. This makes your unit test more robust. During that process you might see that your current code does not support unit testing very well -- mostly because of dependencies between classes. This is the reason that you will also need to modify your production code. If you are in a project and running out of time both options might not help you any further. Then you will have to live with those tests but you can try to ease your pain: Remove code duplication in your tests: Like in production code eliminate code duplication and put the code into helper methods or helper classes. If something changes you might only need to change the helper method or class. This way you will converge to the next suggestion. Add another layer of indirection to your tests: Produce helper methods and helper classes which operate on a higher level of abstraction. They should act as API for your tests. These helpers are calling you production code. Your story tests should only call those helpers. If something changes you need to change only one place in your API and don't need to touch all your tests. Example signatures for your API: createUserAndDelete(string[] usersForCreation string[] userForDeletion); logonWithUser(string user); sendAndCheckMessageBoxes(string fromUser string toUser); For general unit testing I suggest to have a look into XUnit Test Patterns from Gerard Meszaros. For breaking dependencies in your production tests have a look into Working Effectively with Legacy Code from Michael Feathers  In addition to the above pick up a good book on TDD (I can recommend ""TDD and Acceptance TDD for Java Developers""). Even though it will approach from a TDD point of view there is alot of helpful information about writing the right kind of unit tests. Find someone who has alot of knowledge in the area and use them to figure out how you can improve your tests. Join a mailing list to ask questions and just read the traffic coming through. The JUnit list at yahoo (something like groups.yahoo.com/junit). Some of the movers and shakers in the JUnit world are on that list and actively participate. Get a list of the golden rules of unit tests and stick them on your (and others) cubicle wall something like: Thou shalt never access an external system Thou shalt only test the code under test Thou shalt only test one thing at once etc."
278,A,"How do I get JUnit to be consistent when timing out? JUnit @Tests have the useful ability to specify a timeout argument so that a poorly written program gets killed automatically if it takes longer than timeout seconds. The problem is this uses clock time instead of CPU time - so a test will actually run for different amounts of time between runs. For example a test might run for 1.901 or 1.894 seconds for a 2 second timeout depending on what other jobs are running on the CPU at the same time. Can I specify a timeout or similar that would be consistent across runs? (Extensions to this question include: consistent across machines etc) On different machines i think not. Your code will defenetly need a different amout of time depending on cpu+ram etc. Thats more or less what I thought. But what about consistent across runs on the same machine? @Ishtar I have no clue what you just said. What exactly do you want to test? It should take exactly 1.8942451 seconds to execute this code would be hm useless? ""Can I specify a timeout or similar that would be consistent across runs?"" Why would you need that? What are you testing? You're right about `join` but why would you care? I'm missing something here... Could you explain why you need a CPU time clock? What is the real problem here? Why do you care about 0.007 of a second? Both the values specified above are under the 2s time out and if 2s is not enough give the test an extra second or rewrite it. Or is it the timeout that is failing early? Give it three seconds and/or look at the test in question. No the point is that `otherThread.join(long millis)` is not deterministic: 1) the current thread isn't guaranteed to sleep for `millis` time and 2) the `otherThread` will actually run for different amounts of (CPU) time depending on other processes running on the machine. I want timeouts to be consistent across runs because thats useful for unit testing? I don't care how but I assume that CPU time clock will probably have to be used. I think your best approach is to set the JUnit timeout to ~2-3x your required timeout and do your own benchmarking using a ThreadMXBean to measure CPU time if available. You can then fail if you've exceeded your timeout. I tried that and it almost works. Except: 1) the 2-3x is still just a guess and 2) `Thread.sleep()` doesn't increase CPU time but does make it take more time ( thus a program that sleeped for 1.5 seconds would pass a unit test that was supposed to timeout after 1 second ). ou could implement your own junit timeout based on CPU time drawn from the mbean... oooh I might do that :D"
279,A,"Is there a repository of pre-built JUnit tests for common Java Interfaces? I'm teaching a data structures class this semester and we build gobs of JUnit tests given component interfaces. We then use setUp to plug in a specific concrete implementation. In most cases we are implementing java.util interfaces. Is there a common set of rigorous JUnit tests for common Java components or some sort of framework that makes designing the test cases less painful? You could look at how Apache Harmony and GNU Classpath unit test their respective clean-room implementations of the java.util classes.  For these purposes mocking may be the easiest way of having ""concrete"" implementations used by unit tests. One of the best is Mockito.  Hamcrest provides several matchers that are specific to Maps and Collections. You can find them in the tutorial. (JUnit 4.x contains Hamcrest) Sample code: import static org.junit.Assert.*; import static org.junit.matchers.JUnitMatchers.*; import java.util.Arrays; import java.util.Collection; import org.junit.Test; public class DummyTest{ @Test public void someTest(){ final Collection<Integer> coll = Arrays.asList(1 2 3 4 5); assertThat(coll hasItems(5 4 3 2 1)); assertThat(coll not(hasItems(6))); assertThat(coll both( is(List.class) ).and( equalTo( Arrays.asList( 0x10x20x30x40x5 ) ) )); } } Your main starting point would be these convenience classes: CoreMatchers JUnitMatchers Am I the only one that find that kind of ""syntax"" awful?"
280,A,"Junit before class ( non static ) Are there any best practices to get Junit execute a function once in a test file  and it should also not be static. like @BeforeClass on non static function? Here is an ugly solution : @Before void init(){ if (init.get() == false){ init.set(true); // do once block } } well this is something i dont want to do  and i am looking for an integrated junit solution. Out of interest why are you trying to do that? Well  i have a quite a big hierarchy of test files  and base test files  i need the possibility to override this action in the child test classes. i had the same problem in which only the first of many parametrized tests should perform a login. See my answer in another SO question http://stackoverflow.com/questions/6715281/how-to-avoid-inheritance-in-junit-test-cases/19686592#19686592 Note that the ""ugly"" solution the one that works with plain JUnit don't take tearing tests down into account. To use an empty constructor is the easiest solution. You can still override the constructor in the extended class. But it's not optimal with all the inheritance. That's why JUnit 4 uses annotations instead. Another option is to create a helper method in a factory/util class and let that method do the work. If you're using Spring you should consider using the @TestExecutionListeners annotation. Something like this test: @RunWith(SpringJUnit4ClassRunner.class) @TestExecutionListeners({CustomTestExecutionListener.class DependencyInjectionTestExecutionListener.class}) @ContextConfiguration(""test-config.xml"") public class DemoTest { Spring's AbstractTestExecutionListener contains for example this empty method that you can override: public void beforeTestClass(TestContext testContext) throws Exception { /* no-op */ } +1 This technique solved my problem when wanting to use DbUnit and only load dataset once per class +1 This is perfect... for people who are not tied to an ancient version of Spring. :(  If you don't want to set up static initializers for one time initialization and are not particular about using JUnit take a look at TestNG. TestNG supports non-static one-time initialization with a variety of configuration options all using annotations. In TestNG this would be equivalent to: @org.testng.annotations.BeforeClass public void setUpOnce() { // One time initialization. } For teardown @org.testng.annotations.AfterClass public void tearDownOnce() { // One time tear down. } For the TestNG equivalent of JUnit 4's @Before and @After you can use @BeforeMethod and @AfterMethod respectively.  A simple if statement seems to work pretty well too: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""classpath:test-context.xml""}) public class myTest { public static boolean dbInit = false; @Autowired DbUtils dbUtils; @Before public void setUp(){ if(!dbInit){ dbUtils.dropTables(); dbUtils.createTables(); dbInit = true; } } ...  I've never tried but maybe you can create a no-argument constructor and call you function from there? This would work  the problem is that I need the possibility to override this action in the classes that extend this base test class @Roman: oh now I see. Add this to your post this comment makes things much clearer. Constructor will be called as many times the test cases are there. For each test method new Test class object will be created. So using constructor is not a solution here Also this wont work with dependency injection that relies on the object already constructed.  Another option worth considering if using dependency injection (e.g. Spring) is @PostConstruct. This will guarantee dependency injection is complete which wouldn't be the case in a constructor: @PostConstruct public void init() { // One-time initialization... } +1: That's a nice trick!"
281,A,"CruiseControl JUnit test results show all zeroes I've got JUnit hooked up with CruiseControl and unit tests are being executed & logs imported but CruiseControl shows the following output: nz.co.picksend.core.tests.nz.co.picksend.core.dal.FilterTests Tests: 0 Failures: 0 Errors: 0 Duration: 0.0 nz.co.picksend.core.tests.nz.co.picksend.core.dal.ModelObjectRegistryTests Tests: 0 Failures: 0 Errors: 0 Duration: 0.0 nz.co.picksend.core.tests.nz.co.picksend.core.dal.ModelObjectSetTests Tests: 0 Failures: 0 Errors: 0 Duration: 0.0 nz.co.picksend.core.tests.nz.co.picksend.core.humanresources.AllHumanResourcesTests Tests: 0 Failures: 0 Errors: 0 Duration: 0.0 (Note all zeroes) However the log files that are being merged look like this: <?xml version=""1.0"" encoding=""UTF-8"" ?> <testsuite errors=""1"" failures=""0"" hostname=""maudslay"" name=""nz.co.picksend.core.tests.nz.co.picksend.core.rules.MockEvalRule"" tests=""1"" time=""0.0"" timestamp=""2009-11-04T23:05:19""> <properties> ... </properties> <error message=""nz.co.picksend.core.tests.nz.co.picksend.core.rules.MockEvalRule"" type=""java.lang.ClassNotFoundException""> java.lang.ClassNotFoundException: nz.co.picksend.core.tests.nz.co.picksend.core.rules.MockEvalRule at java.net.URLClassLoader$1.run(URLClassLoader.java:200) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:188) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at java.lang.ClassLoader.loadClass(ClassLoader.java:252) at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:169) </error> <system-out><![CDATA[]]></system-out> <system-err><![CDATA[]]></system-err> </testsuite> Here's the snippet from ant's build.xml: <target name=""test""> <junit errorProperty=""test.failed"" failureProperty=""test.failed""> <formatter type=""brief"" usefile=""false"" /> <formatter type=""xml"" /> <batchtest todir=""${buildresults.dir}""> <fileset dir=""${basedir}""> <include name=""nz.co*/**/tests/**/*.java"" /> <include name=""nz.co*/**/test/**/*.java"" /> </fileset> </batchtest> </junit> <fail message=""Tests failed: check test reports."" if=""test.failed"" /> </target> and finally here's the project's snippet from cruisecontrol's config.xml: <project requiremodification=""false"" name=""Pick and Send""> <modificationset QuietPeriod=""30""> .... </modificationset> <schedule Interval=""300"" ShowProgress=""true""> <ant AntWorkingDir=""E:\Build\PickSend2"" BuildFile=""E:\Build\PickSend2\build.xml"" Time=""0400"" Target=""bat"" anthome=""apache-ant-1.7.0"" /> </schedule> <log> <merge Dir=""E:\Build\PickSend2\buildresults"" /> </log> <bootstrappers> .... </bootstrappers> </project> Has anyone had this situation before? Thanks for the comment - this is actually intentional. nz.co.picksend.core is both the project and package base name so the source tree looks like /nz.co.picksend.core/tests/nz/co/picksend/core/... That could be the part of the problem too! Your source tree is ""/nz.co.picksend.core/tests/nz/co/picksend/core/..""; Junit is looking for ""nz.co.picksend.core.tests.nz.co.picksend.core.rules.MockEvalRule""; which it expects to find in ""nz/co/picksend/core/tests/nz/co/picksend/core/rules/MockEvalRule.class"" (just replace "".""s with ""/""s). I would consider refactoring the package name to exclude "".""s and see how that goes. @bguiz you should post as an answer. I think you are correct. Just to back up the previous statements. I doubt you can use dots (full stops) in your package name. JLS (http://java.sun.com/docs/books/jls/third_edition/html/lexical.html#3.8) defines the characters that can be used as part of Java identifiers. And a dot is not one of them. You can check it by running `Character.isJavaIdentifierPart('.')`. Your output contains packages like: ""nz.co.picksend.core.tests.nz.co.picksend.core.rules"". I notice that ""nz.co.picksend.core"" appears twice in the package name. I cannot tell if this is (un)intentional? Possibly the problem. This happened because of the unit test throwing an error not in the test itself but in the test's setup. This for some reason isn't captured by cruisecontrol."
282,A,"Why does a junit test fail in eclipse but pass from ant? I have a JUnit test that is checking to make sure that a customized xml serialization is working properly. The customized xml serialization is just a few custom converters for Xstream. The deserializers work but for some reason in Eclipse 3.x JUnit fails the serialization. In Ant on the command line it works just fine. It also works just fine in Eclipse if I debug and step through the test case but if put a break point after the failing testcase executes it fails still. What gives? Am I having a class path issue? Some additional information: Expected: <site> <name>origin</name> <version>0.6.0</version> <description>Stuff</description> <source>./fake-file.xml</source> <location> <latitude deg=""44"" min=""26"" sec=""37.640""/> <longitude deg=""-57"" min=""-38"" sec=""-6.877""/> <ellipsoid-height value=""-79.256"" units=""meters""/> <geoid-height value=""0.000"" units=""meters""/> </location> </site> Actual: <site> <name>origin</name> <version>0.6.0</version> <description>Stuff</description> <source>./fake-file.xml</source> <location> <latitude deg=""44"" min=""26"" sec=""37.640""/> <longitude deg=""-57"" min=""-38"" sec=""-6.877""/> <ellipsoid-height value=""-79.256"" units=""meters""/> <geoid-height value=""-79.256"" units=""meters""/> </location> </site> The code that writes the location fields: public void marshal(Object source HierarchicalStreamWriter writer MarshallingContext context) { ILatLonEllipsoidHeightPoint aLoc = (ILatLonEllipsoidHeightPoint) source; synchronized(aLoc) { writer.startNode(LATITUDE); writer.addAttribute(DEGREES Integer.toString( PointUnitConversions.getLatitudeHours(aLoc.getLatitude()))); writer.addAttribute(MINUTES Integer.toString( PointUnitConversions.getLatitudeMinutes(aLoc.getLatitude()))); writer.addAttribute(SECONDS String.format(""%.3f"" PointUnitConversions.getLatitudeSeconds(aLoc.getLatitude()))); writer.endNode(); writer.startNode(LONGITUDE); writer.addAttribute(DEGREES Integer.toString( PointUnitConversions.getLongitudeHours(aLoc.getLongitude()))); writer.addAttribute(MINUTES Integer.toString( PointUnitConversions.getLongitudeMinutes(aLoc.getLongitude()))); writer.addAttribute(SECONDS String.format(""%.3f"" PointUnitConversions.getLongitudeSeconds(aLoc.getLongitude()))); writer.endNode(); writer.startNode(ELLIPSOID_HEIGHT); writer.addAttribute(VALUE String.format(""%.3f"" aLoc.getEllipsoidHeight())); writer.addAttribute(UNITS METERS); writer.endNode(); writer.startNode(GEOID_HEIGHT); writer.addAttribute(VALUE String.format(""%.3f"" aLoc.getGeoidHeight())); writer.addAttribute(UNITS METERS); writer.endNode(); } } The PointUnitConversions calls do the obvious math to take a decimal degrees and convert to corresponding integer or double values for the component parts. Its just that last attribute of location that is causing the failure. It works fine if you debug it? Sounds like a timing issue; and debugging it slows it down enough to work. Can you post some code? Edit: Thanks for adding the info. It seems that if its NOT failing when you step through it but it fails if you just run it normally I would bet its a timing/Threading issues. Are you doing this in a Thread somewhere? Is there some contention? Or a race condition? There absolutely is a race condition. The call to aLoc.getGeoidHeight() does much more than I realized. The issue is caused by the order of tests executing. Eclipse is executing the entire test suite in apparently random order but in Ant order is defined. Glad I was able to help. You should look into using the JUnit setup() and teardown() to try and reset the initial conditions before each test is run. That would make sure every test is independent of previous conditions Thanks!. I'm on my way through all of the tests case to find the one causing the problem. The issue is in the setup() things are changed on static members and then not reset in the teardown() methods.  Probably not the same XML lib used. Find out which is used using for example this small code piece: import org.w3c.dom.Text; public class TextTest { public static void main(String[] args) { System.out.println(Text.class.getResource(""Text.class"")); } }  Something's different. Can't tell based on what you posted but my guesses might be: Different JVM used by Ant command shell and Eclipse Different JARs in the CLASSPATH for Ant and Eclipse Different XML encodings."
283,A,"Format an int with zero padding in Java How do I get the following assertion to succeed? { int i = 5; assertEquals(""005"" String.format(""%1??s"" i)); } Problem: I need to format an int as a string of equal length. How about: assertEquals(""005"" String.format(""%03d"" i)); Adding a leading zero to the width of the format says that you want the field left-padded with zeroes. From the docs under 'Number Localization Algorithm' pt. 4.: If the '0' flag is given then the locale-specific zero digits are inserted after the sign character if any and before the first non-zero digit until the length of the string is equal to the requested field width. Thanks alot works great"
284,A,"How to test an Android Library Project I am writing an Android Library Project basing on Android Bitmap class (call it AndroindLib) which contains only utility class (no activity). I tried to test it using Android JUnit but it keeps complaining that can't find the AnroidLib.apk What's the right way to Unit test Android Library Project? Or (**shameless plug alert**) you could use a project I created: Android Library Test Harness (what I like to call ""Alt H""). It's available as a git repo: https://gitorious.org/alth The project is still in development and the documentation is a little sparse so feel free to hit me up if you have any questions. Quoting the documentation: ""There are two recommended ways of setting up testing on code and resources in a library project: You can set up a test project that instruments an application project that depends on the library project. You can then add tests to the project for library-specific features. You can set up a standard application project that depends on the library and put the instrumentation in that project. This lets you create a self-contained project that contains both the tests/instrumentations and the code to test."" so basically all you have to do is add the library to your test project and test the test project. @AbdullahJibaly: Thanks -- I fixed the answer The link is no longer valid any chance you can point us to the current one? Looks like: http://developer.android.com/guide/developing/projects/index.html#testing When I try the first recommendation of creating a simple app project that includes my library and then creating a test project for the simple app project my tests cannot resolve the types from the library project namespaces. *sigh* Got the second (and preferred way working). It's very important that you get the uses-library and instrumentation elements in the correct location in your manifest. The instrumentation element is a child of the root manifest element and the uses-library element is a child of the application element. I was putting the uses-library under the manifest element and ending up with the error message above when trying to run the tests. Not sure if anyone else has had success with these recommendations but I haven't. Tried the second solution first and when I try to run in Eclipse I get the following: does not specify a android.test.InstrumentationTestRunner instrumentation or does not declare uses-library android.test.runner in its AndroidManifest.xml. This is despite the fact that I added both the instrumentation and uses-library elements to my Manifest. There is an important requirement for the 2nd scenario to work fine. Both the library and the test project should share same package name. It will still run but you can stuck into subtle file permission problems as I did.  In your test project simply change the package name so that it's the same as your library's package. For example you have a library whose package is ""com.example.lib"". Create a test project targeting your library. In the manifest file you'll see package=""com.example.lib.test"" and targetPackage=""com.example.lib"". Just change the package from ""com.example.lib.test"" to ""com.example.lib"" (targetPackage leave as is). Also make sure that the library is referenced to your test project NOT in Java build path but as a usual Android library : in Eclipse it must be shown as library in Project->Properties->Android tab but not in Project->Properties->Java Build Path tab. Then run you tests. Your ideas made sense to me so that the test targetted itself (as the app) and the app is linked with the library....but I couldn't get it to work via ant build nor IntelliJ....as the .apk (expected for the app under test) didn't exist when it tried to install it as it is being compiled to ""test/bin/.-debug.apk"" I got it to work by just changing the package name of test project in it's manifest (and the package under test - which should be the same) to NOT coincide with the package name of the library project plus removing the reference to the target test project in the ant.properties file. I think this is a better answer than the accepted one that just echos the documentation - IMHO.  NOTE: This solution is based on using Eclipse Indigo (3.8.2) and might have to be implemented slightly differently for another IDE although the basic principles will be the same. I had similar issues and I found that do the following always works: (NOTE: These instructions are for building a new project group from scratch. If you have already built parts of the project group then you may have to modify your projects so that they connect in the same way.) Create a new Android Library project by checking the ""Is Library"" checkbox during creation. (for example an Android project named ""RemingtonAndroidTools""). Build the Android Library project and verify that it created a jar file in the bin folder. (for example a jar file named ""RemingtonAndroidTools.jar"".) Create an empty Android Project for testing the Android app that will serve as an Android Test App. (For example an Android project named ""RemingtonAndroidToolsTestApp""). You will not need to modify the source code or resources of the Android Test App project unless you have something that must be added for testing. Many things can be tested without any modifications to the Android Test App Project. The Android Test App project is a bridge between your Android Library project and the Android Junit project that makes testing of the Android Library project via Android Junit possible. Go the Library tab of Java Build Path for the Android Test App project (""RemingtonAndroidToolsTestApp"" in this example). Add the jar file (""RemingtonAndroidTools.jar"" in this example) of the Android Library Project (""RemingtonAndroidTools"" in this example) via the ""Add Jars..."" button. Create a new Android Test project (for example ""RemingtonAndroidToolsTester"") that will serve as an Android Library Tester and select the Android Test App project (""RemingtonAndroidToolsTestApp"" in this example) as the target. Go the Library tab of Java Build Path for the Android Library Tester project (""RemingtonAndroidToolsTester"" in this example). Add the jar file (""RemingtonAndroidTools.jar"" in this example) of the Android Library Project (""RemingtonAndroidTools"" in this example) via the ""Add Jars..."" button. Find the last folder of your Android package in the Android Library Tester project (""danny.remington.remington_android_tools_test_app.test"" for example) and add a test class (""MainActivityTest"" for example) that inherits from ActivityInstrumentationTestCase2. Edit the test class (""TestActivityTest"" in this example) to use the activity (for example ""TestActivity"") of the Android Test App (""RemingtonAndroidToolsTestApp"" in this example) as the parameter for ActivityInstrumentationTestCase2. Edit the test class (""TestActivityTest"" in this example) and create a default constructor that makes a call to super(Class) and passing in the class of the Android Test App (""TestActivity.class"" for example). You should end up with three projects (Android Library Android Test App Android Library Tester) that look similar to this: You should end up with a class for testing your Android Library that looks similar to this: package danny.remington.remington_android_tools_test_app.test; import android.test.ActivityInstrumentationTestCase2; import danny.remington.remington_android_tools_test_app.TestActivity; /** * */ public class TestActivityTest extends ActivityInstrumentationTestCase2<TestActivity> { public TestActivityTest() { super(TestActivity.class); } } You can then add any test that you want. You will not need to reference the Android Test App (""RemingtonAndroidToolsTestApp"" in this example) further to run your tests unless they require access to an Android specific component (like the Assets folder for example). If you need to access any Android specific components you can do so by modifying the Android Test App (""RemingtonAndroidToolsTestApp"" in this example) and then referencing it via the instrumentation provided by the standard Android Junit API. (You can read more about that here: http://developer.android.com/tools/testing/testing_android.html)  If your ulitiy classes do not depend on any android specific code you can just use standard JUnit unit tests. No need to use the Android versions. i am depending on Android Bitmap class... This doesn't work because standard JUnit is compile to the standard JVM while Android java files are compiled to the Dalvik VM.  http://www.paulbutcher.com/2010/09/android-library-project-with-tests-step-by-step/ helps describe the process needed to implement the second suggestion in CommonsWare's answer"
285,A,"Tests and inheritance issue Imagine you have an application and you want to make unit tests and functionnal tests over it (not quite hard to imagine). You might have an abstract class let's call it AbstractTestClass from which all your unit tests extends. AbstractTestClass would look something like this (using JUnit 4) : class AbstractTestClass { boolean setupDone = false; @Before public void before() { if(!setupDone) { // insert data in db setupDone = true; } } } Here is what I'm struggling with. I'm having another abstract class which test the web interfaces : class AbstractWebTestClass extends WebTestCase { boolean setupDone = false; @Before public void before() { if(!setupDone) { // here make a call to AbstractTestClass.before() // init the interfaces setupDone = true; } // do some more thing } } It's pretty much the same class except that it extends WebTestCase. This design could give me the possibility to have the same data while unit testing than when testing the interface. Usually when dealing with such issue you should favor composition over inheritance or use a strategy pattern. Unfortunately I don't quite like the idea to favor composition over inheritance in this particular scenario and I don't see how I could use a strategy pattern there is probably a design flaw and I can't quite see the solution. How could I design this architecture in order to achieve my goal. do you mean webtestcase extends abstracttestcase? Can you clearly state what the problem is Im having trouble understanding... Yes. This would be desirable but since you can't do multiple inheritance in Java I'm stuck with this :) ...ah I see now. I would implement this in the following way: class Example { class LazyInitStrategy implements Runnable { private final Runnable operation; private boolean done = false; LazyInitStrategy(Runnable operation) { this.operation = operation; } @Override public void run() { if (!done) { operation.run(); done = true; } } } private final class AbstractInit implements Runnable { public void run() { // insert data in db } } private final class AbstractWebInit implements Runnable { public void run() { // here make a call to AbstractTestClass.before() init the interfaces } } class AbstractTestClass { final LazyInitStrategy setup = new LazyInitStrategy(new AbstractInit()); @Before public void before() { setup.run(); // do some more thing } } class AbstractWebTestClass extends WebTestCase { final LazyInitStrategy setupInfo = new LazyInitStrategy(new AbstractWebInit()); @Before public void before() { setupInfo.run(); // do some more thing } } } Sure this is very simplistic solution but it should eliminate if/else logic duplication for checking if setup was done. Using Runnable is optional I did this just for demo purposes in read world you probably will use another interface. Looks like a strategy pattern doesn't it? I updated sample to look more like strategy pattern :-). Usually in code you do not have 'pure' patterns implementations (and you don't have to! Patterns are just guidelines). Pretty much used your solution. Thanks.  The important thing to accomplish is not to duplicate code. In this situation I would create a MyScenarioTestUtil class that has a bunch of static methods on it that sets up the data as you need to. You would invoke the utility methods from the setup. That way you keep all the code in one place. Its really just a semantics difference from using composition...  I think that the design is wrong in general. You shouldn't use inheritance in unit tests at all. Tests should be isolated and really plain. Very often like in your case it's necessary to prepare some supplementary objects that will help test methods to do their job. In such a case you should define builders of such objects and place them somewhere outside of test cases. For example: public void testMethodThatNeedsSomePreparedObjects() { Foo foo = new FooBuilder() .withFile(""some-text.txt"") .withNumber(123) .build(); // now we are testing class Bar using object of class Foo Bar bar = new Bar(foo); } Thus you need FooBuilder to be defined somewhere else and this class will do all the work you're now trying to do using stategy pattern or inheritance. Both approached are wrong when dealing with unit tests."
286,A,How do I get my ActivityUnitTestCases to sync with the MessageQueue thread and call my Handler? I'm writing unit tests for a ListActivity in Android that uses a handler to update a ListAdapter. While my activity works in the Android emulator running the same code in a unit test doesn't update my adapter: calls to sendEmptyMessage do not call handleMessage in my activity's Handler. How do I get my ActivityUnitTestCase to sync with the MessageQueue thread and call my Handler? The code for the Activity is somewhat like this: public class SampleActivity extends ListActivity implements SampleListener { List samples = new ArrayList(); public void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.sample_list); listView.setEmptyView(findViewById(R.id.empty)); } private final Handler handler = new Handler() { @Override public void handleMessage(Message msg) { // unit test never reaches here sharesAdapter = new SampleAdapter(SampleActivity.this samples); setListAdapter(sharesAdapter); } }; public void handleSampleUpdate(SampleEvent event) { samples.add(event.getSample()); handler.sendEmptyMessage(0); } } The code for my unit test is somewhat like this: public class SampleActivityTest extends ActivityUnitTestCase<SampleActivity> { public SampleActivityTest() { super(SampleActivity.class); } @MediumTest public void test() throws Exception { final SampleActivity activity = startActivity(new Intent(Intent.ACTION_MAIN) null null); final ListView listView = (ListView) activity.findViewById(android.R.id.list); activity.handleSampleUpdate(new SampleEvent(this)); // unit test assert fails on this line: assertTrue(listView.getCount() == 1); } } I suspect but I am not certain that this is because of the use of the instrumentation thread and UIThread not syncing up. So your handler is bound to the thread that creates it. Your handler is created at construction time so it's whatever thread calls the constructor. I don't have the source in front of me but my guess is that in most cases when the app is running I am betting it's the UIThead (main) both constructs and calls onCreate() on your activity. But when the unit test is running it is being bound to the Instrumentation Thread which probably constructs your activity and then onCreate is called on the UIThread. Now you are saying in the instrumentation thread send a message and your thread has no looper associated with it to do anything with it. So it just sits in the queue. Try moving your handler creation to the onCreate(). This should bind your Handler to the main looper and then your async events will be fired from another thread (the instrumentation thread) and the UIThread will gladly reap them from the queue. While I haven't tested it this seems like the issue at first glance.
287,A,Wrapping JUnit Tests (in Eclipse) All of my tests for my Groovy code look like this public void testButtons() { try { page.getButtons(); } catch (Exception e) { throw org.codehaus.groovy.runtime.StackTraceUtils.sanitize(e); } } because I need to sanitize any possible StackTrace that appears (otherwise it's very hard to read since it's got all the Groovy meta-code). Is there any way to specify that all JUnit tests get wrapped in particular way (like an error handler)? Note: I am running these in Eclipse but if there's a way to do this in IntelliJ or Netbeans that would be good to know. Yes use a Rule. Basically you have to have a class which implements the MethodRule interface that handles the exception handling in the apply method by substituting its own Statement implementation that has the try/catch in it. To use a rule you define a field in the test class like so:  @Rule public MethodRule exceptionCleanser = new ExceptionCleanser(); A first cut implementation would probably look something like this:  public class ExceptionCleanser implements MethodRule { public Statement apply(final Statement base FrameworkMethod method Object target) { return new Statement() { public void evaluate() throws Throwable { try { base.evaluate(); } catch (Exception e) { throw org.codehaus.groovy.runtime.StackTraceUtils.sanitize(e); } } }; } } The above is totally untested but you should be able to get the idea. The @Rule annotation was introduced in JUnit 4.7 so you may need to update to use it. Thanks @Yishai. I'll check back here when I get to the real code. Just making some notes :) `println(junit.runner.Version.id());` Hmmm... the syntax seems to all be fine but the `apply` method is never being run. Still figuring this thing out... @yar make sure your class does *not* extend TestCase otherwise the JUnit runner will run it as a 3.8 test. thanks for all your help. It turns out I had to actually learn some stuff to make JUnit 4 work but your code is perfect. Thanks again.
288,A,"Delete or comment out non-working JUnit tests? I'm currently building a CI build script for a legacy application. There are sporadic JUnit tests available and I will be integrating a JUnit execution of all tests into the CI build. However I'm wondering what to do with the 100'ish failures I'm encountering in the non-maintained JUnit tests. Do I: 1) Comment them out as they appear to have reasonable if unmaintained business logic in them in the hopes that someone eventually uncomments them and fixes them 2) Delete them as its unlikely that anyone will fix them and the commented out code will only be ignored or be clutter for evermore 3) Track down those who have left this mess in my hands and whack them over the heads with the printouts of the code (which due to long-method smell will be sufficently suited to the task) while preaching the benefits of a well maintained and unit tested code base You would like to beat up the people that gave you the code or the people that wrote unit tests? P.S. Bad programmers can only understand long methods. You should definitely disable them in some way for now. Whether that's done by commenting deleting (assuming you can get them back from source control) or some other means is up to you. You do not want these failing tests to be an obstacle for people trying to submit new changes. If there are few enough that you feel you can fix them yourself great -- do it. If there are too many of them then I'd be inclined to use a ""crowdsourcing"" approach. File a bug for each failing test. Try to assign these bugs to the actual owners/authors of the tests/tested code if possible but if that's too hard to determine then randomly selecting is fine as long as you tell people to reassign the bugs that were mis-assigned to them. Then encourage people to fix these bugs either by giving them a deadline or by periodically notifying everyone of the progress and encouraging them to fix all of the bugs.  Follow the no broken window principle and take some action towards a solution of the problem. If you can't fix the tests at least: Ignore them from the unit tests (there are different ways to do this). Enter as many issue as necessary and assign people to fix the tests. Then to prevent such situation from happening in the future install a plug in similar to Hudson Game Plugin. People gets assigned points during continuous integration e.g. -10 break the build <-- the worse -1 break a test +1 fix a test etc. Really cool tool to create a sense of responsibility about unit tests within a team. The Hudson Game plugin link is momentary down. Here is another link http://clintshank.javadevelopersjournal.com/ci_build_game.htm  The failing JUnit tests indicate that either The source code under test has been worked on without the tests being maintained. In this case option 3 is definitely worth considering or You have a genuine failure. Either way you need to fix/review the tests/source. Since it sounds like your job is to create the CI system and not to fix the tests in your position i would leave a time-bomb in the tests. You can get very fancy with annotated methods with JUnit 4 (something like @IgnoreUntil(date=""2010/09/16"")) and a custom runner so or you can simply add an an if statement to the first line of each test:  if (isBeforeTimeBomb()) { return; } Where isBeforeTimeBomb() can simply check the current date against a future date of your choosing. Then you follow the advice given by others here and notify your development team that the build is green now but is likely to explode in X days unless the timebombed tests are fixed. Loving the timebomb concept!  I don't know your position in the company but if it's possible leave them in and file the problems as errors in your ticket system. Leave it up to the developers to either fix them or remove the tests. If that doesn't work remove them (you have version control right?) and close the ticket with a comment like 'removed failing junit tests which apparently won't be fixed' or something a bit more polite. The point is junit tests are application code and as such should work. That's what developers get paid for. If a test isn't appropriate anymore (something that doesn't exist anymore got tested) developers should signal that and remove the test.  A CI system that is steady red is pretty worthless. The main benefit is to maintain a quality bar and that's made much more difficult if there's no transition to mark a quality drop. So the immediate effort should be to disable the failing tests and create a tracking ticket/work item for each. Each of those is resolved however you do triage - if nobody cares about the test get rid of it. If the failure represents a problem that needs to be addressed before ship then leave the test disabled. Once you are in this state you can now rely on the CI system to tell you that urgent action is required - roll back the last change or immediately put a team on fixing the problem or whatever.  If they compile but fail: leave them in. That will get you a good history of test improvements over time when using CI. If the tests do not compile but break the build comment them out and poke the developers to fix them. This obviously does not preclude using option 3 (hitting them over the head) you should do that anyway regardless of what you do with the tests.  If you use Junit 4 you can annotate that tests with @Ignore annotation. If you use JUnit 3 you can just rename tests so they don't start with test. Also try to fix tests for functionality you are modifying in order to not make code mess larger. We use JUnit 3 and I had considered renaming the tests to brokenSuchAndSuch but to me this requires more work by future maintainers to understand what I did and why I did it. With commenting out or deletion its much more in your face. The @Ignore annotation is interesting. As for the removal of test from the method name you'd then have to be careful and add a comment to explain that this is not a forgotten/unused method (otherwise somebody could delete it) but a test method that should be fixed. You can rename method to something like fixItLaterTestBlahBlah isn't it? If you comment out tests then it may be possible that they will be uncompilable when you uncomment them because of API changes. If they will be always uncommentable you will fix compilation errors when your API changes.  Comment them out so that they can be fixed later. Generate test coverage reports (with Cobertura for example). The methods that were supposed to be covered by the tests that you commented out will then be indicated as not covered by tests. No need to comment out things that's what revision control is for. delete them or @Ignore them. Commenting them out just leaves more cruft."
289,A,"Best design to organize HttpUnit+junit as standalone app I have a task to write a set of http unit tests which should be deployed and started as single threaded application on external server. After some investigation and reading of documentation I came to following application structure: public static void main(String... args) throws Exception { Class[] testCases = { LoginTest.class Test2.class Test3.class }; TestSuite suite = new TestSuite(testCases); TestResult result = new TestResult(); suite.run(result); displayResults(result); } And testcase looks like: public class LoginPageTest extends TestCase { public void testLogin() throws IOException SAXException { WebConversation wc = new WebConversation(); //Some HttpUnit init code here loginForm.setParameter(""j_username"" login); loginForm.setParameter(""j_password"" pass); loginForm.submit(); String expected = String.format(""/%s/action/logon.do"" endpoint); assertEquals(wc.getCurrentPage().getURL().getPath() expected); } } Has anyone made similar tasks ? Have you some advices than can improve this structure? How can I implement dependency between testcases (e.g. almost everything needs that user should be authenticated -> loginTestCase must be called) ? Any advice greatly appreciated ! Thanks in advance. Currently I've found one appropriate way to design my testing application: TestCase class (extending jUnit): public class LogoutTest extends TestCase { public void testLogout() throws IOException SAXException { new Config().initApplication() .doLogin(""user"" ""pass"") // returns WelcomePage class .goToFilterPage() //Returns another page .doLogout(); //returns LoginPage class } } After some time I've decided that it will be good to make for each page specific class which will have it's specific control methods e.g. ""go to next page"" ""logout"" ""click some button"" etc... public class WelcomePage extends AbstractPage { protected WelcomePage(AbstractPage other) { super(other); String expected = String.format(""/%s/welcome.do"" Config.endpoint); assertEquals(expected webConversation.getCurrentPage().getURL().getPath()); logger.info(String.format(""Current page: %s"" webConversation.getCurrentPage().getURL().getPath() )); } public FilterPage goToFilterPage() throws SAXException IOException { WebLink link = webConversation.getCurrentPage().getLinkWithID(""downloadLink""); assertNotNull(""Check if link exist on page"" link); link.click(); return new FilterPage(this); } } AbstractPage class (implements some common logic for all pages): public abstract class AbstractPage { protected WebConversation webConversation; protected Logger logger; protected AbstractPage(AbstractPage other) { this.webConversation = other.getWebConversation(); this.logger = Logger.getLogger(this.getClass()); } protected AbstractPage(WebConversation webConversation) { this.webConversation = webConversation; this.logger = Logger.getLogger(this.getClass()); } public WebConversation getWebConversation() { return webConversation; } public void doLogout() throws SAXException IOException { WebLink logoutLink = webConversation.getCurrentPage().getLinkWithID(""logoutLink""); assertNotNull(logoutLink); logoutLink.click(); } }"
290,A,"How to get full stack traces logged when a JUnit test running in ant fails? When a JUnit test throws a runtime exception while running in Eclipse you can see the entire stack trace. Our build server uses ant and runs JUnit but the printout on failure only provides the exception's message not the entire ""printStackTrace"". Is there a convenient way to get this functionality? I've posted this answer to this question before realizing that it was a duplicate of yours. Here is my junit tag that does produce the exception trace. <junit showoutput=""yes"" errorProperty=""test.failed"" failureProperty=""test.failed"" haltOnFailure=""${test.halt-on-failure}"" fork=""yes"" forkmode=""${junit.forkmode}"" > <classpath> <pathelement location=""${classes.dir}""/> <pathelement location=""${classes-under-test.classes.dir}"" /> </classpath> <!-- #Formatters for capture and display --> <formatter type=""brief"" usefile=""false"" /> <formatter type=""brief"" /> <formatter type=""xml"" if=""test.generate.xml.output"" /> <!-- #Test case isolation technique --> <test name=""${testcase}"" if=""testcase"" /> <batchtest todir=""${test.data.dir}"" unless=""testcase"" > <fileset dir=""${classes.dir}""> <include name=""**/Test*.class"" /> <exclude name=""**/Test*$*.class"" /> </fileset> </batchtest> </junit> I think the one nested element that will do it for you is  <formatter type=""brief"" usefile=""false"" />"
291,A,"JUnit 4 @BeforeClass & @AfterClass when using Suites When using this approach below by setting up the jUnit with Suites. We got the problem when all @BeforeClass in every Testclass will be executed before any tests starts to execute. (For each n TestClass file the @BeforeClass runs then after they have execute it started to execute the first MyTest.class files @Test) This will cause that we allocate up much resources and memory. My thoughts was that it must be wrong shouldn't each @BeforeClass run only before the actual testclass is executed not when the Suite is started? @RunWith(Suite.class) @Suite.SuiteClasses({ MyTests.class Mytests2.class n1 n2 n }) public class AllTests { // empty } public class MyTests { // no extends here @BeforeClass public static void setUpOnce() throws InterruptedException { ... @Test ... public class MyTests2 { // no extends here @BeforeClass public static void setUpOnce() throws InterruptedException { ... @Test ... Are they executed before each classes' tests or are all executed only before the first (but then the second is run without running all @BeforeClass again)? The latter does seem OK as the @BeforeClass is run before the @Test methods in that test. The amount of memory would not change I suppose unless you clean up after each class' tests (and these are also only happening after the entire suite is completed). What I get right now is that every @BeforeClass is run first. @BeforeClass (Mytests) @BeforeClass (Mytests2) @Test (MyTests) @Test (MyTests2) At my point of view this is not correct. Correct me if I'm wrong but something must be setup wrong to cause this issue. I'm not too familiar with @RunWith in JUnit so I may have done something wrong but I can't seem to replicate the behaviour you describe. With the class: @RunWith(Suite.class) @Suite.SuiteClasses( { FirstTest.class SecondTest.class ThirdTest.class }) public class AllTests { // empty } And FirstTest.java looking like this: public class FirstTest { @BeforeClass public static void doBeforeClass() { System.out.println(""Running @BeforeClass for FirstTest""); } @Test public void doTest() { System.out.println(""Running @Test in "" + getClass().getName()); } } ... with SecondTest.java and ThirdTest.java pretty much the same. I get the test output: Running @BeforeClass for FirstTest Running @Test in FirstTest Running @BeforeClass for SecondTest Running @Test in SecondTest Running @BeforeClass for ThirdTest Running @Test in ThirdTest This is with JUnit 4.5.0 (default JUnit in Eclipse 3.5.1) on Sun's JDK 1.6.0_12. Can you spot any difference in my example from yours? Perhaps a different JDK/JVM? I don't know enough about the internals of JUnit to know if these can be a factor. As you describe is what I was supposed to get also. But I get a bunch of @BeforeClass first then I see the first @Test @Test and so on. Using Sun's JDK 1.6.0_17 Eclipse 3.5 but then our Function tests are executed we are using ant running maven. Could some of this affect the results? If I set it up in Eclipse and run it it works it seems that this issue is somewhat in ant or maven. Sorry really can't help you with running JUnit in ant or maven. Though I did export build.xml for the example I used and ran it with the tests target and got the behaviour you're looking for so I doubt it's anything wrong with the JUnit ant task. *puzzled shrug*  I think @BeforeClass executes at instanciation. It doesn't. ""Annotating a public static void no-arg method with @BeforeClass causes it to be run once before any of the test methods in the class. The @BeforeClass methods of superclasses will be run before those the current class."" (Source: JUnit documentation) Just will add some additional information for those having problem with the memory. JUnit will save all states when running and will therefor also save all the static variables until all the JUnit tests has executed. This was also a major problem since we are running between 1000-6000 JUnit tests. This because it need it to display the result at the end.  Write a @BeforeClass method in AllTests class which will be executed when the suite is started. public class MyTests1 { @BeforeClass public static void beforeClass() { System.out.println(""MyTests1.beforeClass""); } @Before public void before() { System.out.println(""MyTests1.before""); } @AfterClass public static void afterClass() { System.out.println(""MyTests1.AfterClass""); } @After public void after() { System.out.println(""MyTests1.after""); } @Test public void test1() { System.out.println(""MyTests1.test1""); } @Test public void test2() { System.out.println(""MyTests1.test2""); } } public class MyTests2 { @BeforeClass public static void beforeClass() { System.out.println(""MyTests2.beforeClass""); } @Before public void before() { System.out.println(""MyTests2.before""); } @AfterClass public static void afterClass() { System.out.println(""MyTests2.AfterClass""); } @After public void after() { System.out.println(""MyTests2.after""); } @Test public void test1() { System.out.println(""MyTests2.test1""); } @Test public void test2() { System.out.println(""MyTests2.test2""); } } @RunWith(Suite.class) @Suite.SuiteClasses( { MyTests1.class MyTests2.class }) public class AllTests { @BeforeClass public static void beforeClass() { System.out.println(""AllTests.beforeClass""); } @Before public void before() { System.out.println(""AllTests.before""); } @AfterClass public static void afterClass() { System.out.println(""AllTests.AfterClass""); } @After public void after() { System.out.println(""AllTests.after""); } @Test public void test1() { System.out.println(""AllTests.test1""); } @Test public void test2() { System.out.println(""AllTests.test2""); } } OUTPUT AllTests.beforeClass MyTests1.beforeClass MyTests1.before MyTests1.test1 MyTests1.after MyTests1.before MyTests1.test2 MyTests1.after MyTests1.AfterClass MyTests2.beforeClass MyTests2.before MyTests2.test1 MyTests2.after MyTests2.before MyTests2.test2 MyTests2.after MyTests2.AfterClass AllTests.AfterClass hth `AllTests.test1()` and `""AllTests.test2()` are never executed?"
292,A,Debugging surefire/junit's taste in test cases Puzzled: I added a new test case function to a junit test. I run the entire class from either Eclipse or from maven and the old case (there was only one before) runs and the new one does not. It doesn't fail. A breakpoint in it is not hit. The new function has an @Test annotation just like the old one. Junit version is 4.5. Is there a way to get junit to log or trace its thought process in selecting functions to run? I guess you still ran old class file as new Java file was not be compiled successfully. You could modify an old test method to see if the class is really modified: to let successful method to fail.
293,A,"How can I automate a test that sends multiple mock intents to an Android activity? I am trying to send mock intents to an Android activity via the Android instrumentation tools and Android JUnit in Eclipse. I am able to successfully create a test that sends one Intent to an Activity but I want to automate this and send several consecutive Intents so I can test the Activity with many pieces of data put in as an ""extra"" in the Intent. My code (which works for a single Intent) is as follows: public class SearchTest extends ActivityInstrumentationTestCase2<SearchResults> { private ListActivity mActivity; private ArrayList<String> testManifest = new ArrayList<String>(); TextView tv; public SearchTest() { super(""org.fdroid.fdroid"" SearchResults.class); }//SearchTest @Override protected void setUp() throws Exception{ setUpTestManifest(); super.setUp(); setActivityInitialTouchMode(false); Intent i = new Intent(Intent.ACTION_SEARCH); i.setClassName(""org.fdroid.fdroid"" ""org.fdroid.fdroid.SearchResults""); i.putExtra(SearchManager.QUERY testManifest.get(0)); setActivityIntent(i); mActivity = getActivity(); tv = (TextView) mActivity.findViewById(R.id.description); }//setUp public void testSearchResult(){ assertTrue(mActivity.getListView().getCount() > 0); }//testSearchResult public void setUpTestManifest(){ //populate the test manifest testManifest.add(""Sample Key Word 1""); testManifest.add(""Sample Key Word 2""); testManifest.add(""Sample Key Word 3""); }//setupManifest }//SearchTest How can I make this work where I can have hundreds of items in the testManifest and create an Intent and test for each of those items? Would be nice if you let us know if bellow answer is what you been looking for... Have you tried pulling out the launching of the activity out of the setUp code and into a loop within your test method? Example protected void setUp() { setUpTestManifest(); super.setUp(); } public void testSearchResult(){ for (String query : testManifest) { setActivityInitialTouchMode(false); Intent i = new Intent(Intent.ACTION_SEARCH); i.setClassName(""org.fdroid.fdroid"" ""org.fdroid.fdroid.SearchResults""); i.putExtra(SearchManager.QUERY query); setActivityIntent(i); mActivity = getActivity(); tv = (TextView) mActivity.findViewById(R.id.description); assertTrue(mActivity.getListView().getCount() > 0); mActivity.finish(); // close the activity setActivity(null); // forces next call of getActivity to re-open the activity } } -Dan This worked great...thanks! This is cool!!!"
294,A,"How to use JUnit tests with Spring Roo? (Problems with EntityManager) I'm trying to write a JUnit test for a Spring Roo project. If my test requires use of the entity classes I get the following Exception: java.lang.IllegalStateException: Entity manager has not been injected (is the Spring Aspects JAR configured as an AJC/AJDT aspects library?) The Spring Aspects JAR looks to be configured correctly. In particular I have the following in the pom.xml file: <dependency> <groupId>org.springframework</groupId> <artifactId>spring-aspects</artifactId> <version>${spring.version}</version> </dependency> and <plugin> <configuration> <outxml>true</outxml> <aspectLibraries> <aspectLibrary> <groupId>org.springframework</groupId> <artifactId>spring-aspects</artifactId> </aspectLibrary> </aspectLibraries> <source>1.6</source> <target>1.6</target> </configuration> </plugin> and the classes that use the entity classes work fine when not called from a JUnit test. Any idea how I can set things up so that the Entity manager is injected from a JUnit test? Here is my Test class (more or less): public class ServiceExampleTest { @Test public void testFoo() { FooService fs = new FooServiceImpl(); Set<Foo> foos = fs.getFoos(); } } This is enough to throw the exception. The FooServiceImpl class returns a Set of Foo where Foo is an entity class. The getFoos() method works when the application is run in the usual way. The problem only comes in the context of unit tests. Could you post your test class as well? I've never used Spring Roo but with normal Spring tests you usually have to extend AbstractSpringJUnit4Test (or something) or use a custom Spring runner via annotation for the tests. I was also running into the same exception and everything was configured correctly. I removed the project and reimported it again in STS (SpringSource Tool Suite) and this problem went away. Not sure why this fixed it but this issue could have been caused by use of Eclipse to manage the Roo generated project before switching to STS in my case.  This is an incredibly annoying problem with Spring Roo and I have not figured out the official solution for. But ... here are two workarounds: Copy the spring-aspects jar to your project then add it to your Projects AspectJ Aspect Path Use Maven to run your unit tests (and miss the green bar :( ) For option one Right click on your project select Properties-> AspectJ Build -> Aspect Path Tab. Groovy/Grails. I love it. @FarmBoy You can't say that with out saying what you are using now :) I'll accept this answer but I'm not in a position to easily verify it as I no longer use Roo. (I'm at a different job.) Note you can find the spring-aspects jar under your Maven Dependencies in Eclipse right click and add to inpath. That's what worked for me anyway. I'm guessing the accepted answer accomplishes the same thing.  Long time after the question but I have a working solution when trying to run Spring Roo unit tests from within Eclipse... Have the project open in Eclipse In Eclipse Project > Clean > Rebuild (Automatic or Manual doesn't matter) Once the re-build is complete in a console window have Maven clean and re-package (Clean is required): mvn clean package or if your unit tests are failing in maven (and you need Eclipse for debugging your tests)  mvn clean package -Dmaven.test.skip=true 4. Once the package is successful then refresh back in Eclipse. You should be able to run unit tests successfully back in Eclipse now. I found editing entities caused the greatest frequency of the entity manager error. When I stayed clear of editing them I could edit other classes and unit tests would continue to run successfully.  Your unit test class should have @MockStaticEntityMethods annotation.  Your unit test class should have @MockStaticEntityMethods annotation. Just wanted to add more detail to the above answer by @migue as it took me a while to figure out how to get it to work. The site http://java.dzone.com/articles/mock-static-methods-using-spring-aspects really helped me to derive the answer below. Here is what I did to inject entity manager via test class. Firstly annotate your test class with @MockStaticEntityMethods and create MockEntityManager class (which is a class that just implements EntityManager interface). Then you can do the following in your ServiceExampleTest test class: @Test public void testFoo() { // call the static method that gets called by the method being tested in order to // ""record"" it and then set the expected response when it is replayed during the test Foo.entityManager(); MockEntityManager expectedEntityManager = new MockEntityManager() { // TODO override what method you need to return whatever object you test needs }; AnnotationDrivenStaticEntityMockingControl.expectReturn(expectedEntityManager); FooService fs = new FooServiceImpl(); Set<Foo> foos = fs.getFoos(); } This means when you called fs.getFoos() the AnnotationDrivenStaticEntityMockingControl will have injected your mock entity manager as Foo.entityManager() is a static method. Also note that if fs.getFoos() calls other static methods on Entity classes like Foo and Bar they must also be specified as part of this test case. So say for example Foo had a static find method called ""getAllBars(Long fooId)"" which gets called when fs.getFoos() get called then you would need to do the following in order to make AnnotationDrivenStaticEntityMockingControl work. @Test public void testFoo() { // call the static method that gets called by the method being tested in order to // ""record"" it and then set the expected response when it is replayed during the test Foo.entityManager(); MockEntityManager expectedEntityManager = new MockEntityManager() { // TODO override what method you need to return whatever object you test needs }; AnnotationDrivenStaticEntityMockingControl.expectReturn(expectedEntityManager); // call the static method that gets called by the method being tested in order to // ""record"" it and then set the expected response when it is replayed during the test Long fooId = 1L; Foo.findAllBars(fooId); List<Bars> expectedBars = new ArrayList<Bar>(); expectedBars.add(new Bar(1)); expectedBars.add(new Bar(2)); AnnotationDrivenStaticEntityMockingControl.expectReturn(expectedBars); FooService fs = new FooServiceImpl(); Set<Foo> foos = fs.getFoos(); } Remember the AnnotationDrivenStaticEntityMockingControl must be in the same order that fs.getFoos() calls its static methods.  ponzao is correct. I am able to have all the spring injection magic by having my test class extend AbstractJunit4SpringContextTests. e.g. @ContextConfiguration(locations = { ""/META-INF/spring/applicationContext.xml"" }) public class SelfRegistrationTest extends AbstractJUnit4SpringContextTests { Seems nasty but seems to work for me."
295,A,"How should I test private methods in Java? Possible Duplicate: What’s the best way of unit testing private methods? I am a beginner programmer and I don't know how to write an application that will be well structured for unit testing. I want to write applications with the ability to afterwards add effective unit tests. The problem is with private methods - they can't be testing with outside of their classes. Should I solve this problem by changing all methods that are private to protected and let the test class extend source class? Or is there a better solution? My solution (private splitLetters => protected splitLetters) would work like this: Source class: class MyClass{ protected splitLetters(int num){ return num+2; } } Test class: class Test_MyClass extend MyClass{ public splitLettersTest(){ for(int i=0;i<100;i++){ System.println(parent.splitLetters(i)); } } } Solutions: Not testing private methods - Sometimes a private method is doing very complicated tasks that should be tested very well and we don't want that user will have access to this methods. Soon the solution is changing private methods to protected. 2.Nested class way to test - problematic because QA make changes in source code 3.Reflection - If this makes it possible to call private methods it looks like a great solution http://www.artima.com/suiterunner/private3.html (I should learn more to understand reflection. I don't understand how reflections do not break all the idea of having public and private methods if we can call private methods from another class.) Not define private methods (as I showed in my solution) - problematic because sometimes we have to define a private method. Many would argue that private methods don't need to be tested only public interfaces. Duplicate of http://stackoverflow.com/questions/34571/whats-the-best-way-of-unit-testing-private-methods This question is already answered [here](http://stackoverflow.com/questions/34571/whats-the-best-way-of-unit-testing-private-methods). You shouldn't need to test the private methods. When you test your public methods that should in theory also test your private methods.  My personal view is that you should (wherever possible) only test behaviour that is exposed to the end user of that piece of functionality and therefore that you should not test private methods: The test doesn't prove anything except to show that a piece of internal functionality is ""working"" according to something that makes no sense to the people actually using your software. If you change / re-factor your internal implementation you could find that your unit tests start failing when in fact the external functionality exposed has not changed at all! Of course you may choose to subdivide large projects up into smaller chunks of functionality in which case you might choose to unit test the interfaces between the interfaces (for example you might choose to unit test your data access layer despite the fact that the DAL implementation doesn't directly impact the end user).  You should not need to test private methods. A private method is specifically part of the implementation. You should not test the implemenation but the functionality. If you test the functionality a class exposes you can change the implementation while depending on the unit test. If you feel the need to test a private method this is a good sign that you should move the private method to another class and make the method public. By doing this you get smaller classes and you can test the methods easily. If you do not want to expose this new class you can make it package-private (the default access modifier). +1 for a good explanation of why this is a design problem and how to solve it.  Testing private methods implies testing implementation rather than functionality. Think very carefully about why you want to test private methods and you may find you dont need to test them at all.  Look at this article about testing private methods with JUnit. This answer points to a good resource on how to test private methods but make very sure you read the other answers which all pretty much say that with properly designed code you shouldn't need to (and give good reasons why)  In my opinion private methods should not be tested. Tests are for interfaces (in the broad meaning of this word)."
296,A,"How do I activate JButton ActionListener inside code (unit testing purposes)? I need to activate a JButton ActionListener within a JDialog so I can do some unit testing using JUnit. Basically I have this:  public class MyDialog extends JDialog { public static int APPLY_OPTION= 1; protected int buttonpressed; protected JButton okButton; public MyDialog(Frame f) { super(f); okButton.addActionListener(new ActionListener() { public void actionPerformed(ActionEvent e) { buttonpressed= APPLY_OPTION; } } ); public int getButtonPressed() { return buttonpressed; } } then I have my JUnit file: public class testMyDialog { @Test public void testGetButtonPressed() { MyDialog fc= new MyDialog(null); fc.okButton.???????? //how do I activate the ActionListener? assertEquals(MyDialog.APPLY_OPTION fc.getButtonPressed()); } } This may sound redundant to do in a unit test but the actual class is a lot more complicated than that... AbstractButton.doClick Your tests might run faster if you use the form that takes an argument and give it a shorter delay. The call blocks for the delay. Thanks I can never find the methods that I need... It's not an obvious method.  If you have non-trivial code directly in your event handler that needs unit testing you might want to consider adopting the MVC pattern and moving the code to the controller. Then you can unit test the code using a mock View and you never need to programmatically press the button at all. The problem is that I'm extending a class (not actually JDialog) that I did not made the maker of that class didn't use the MVC pattern. Thanks for the info anyway.  You can use reflection to get the button at runtime and fire the event. JButton button = (JButton)PrivateAccessor.get(MyDialog  ""okButton""); Thread t = new Thread(new Runnable() { public void run() { // What ever you want }; }); t.start(); button.doClick(); t.join();"
297,A,"Unable to run simple JUnit TestCase on old version of JUnit I'm attempting to run a simple JUnit test case on version 3.7 of JUnit (I'm not able to upgrade this to the latest version) Running IntelliJ I get the following exception when I attempt to run my JUnit testcase : Exception in thread ""main"" java.lang.NoClassDefFoundError: junit/textui/ResultPrinter at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:169) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:108) Caused by: java.lang.ClassNotFoundException: junit.textui.ResultPrinter at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) ... 3 more Process finished with exit code 1 I have IntelliJ version 10.0.2 JUnit 3.7 on my classpath and the IntelliJ plugin which states it supports 3.x and 4.x I can see IntelliJ executing the following (added breaks to make it more readable) ""C:\Program Files\Java\jdk1.6.0_20\bin\java"" -Didea.launcher.port=7532 ""-Didea.launcher.bin.path=C:\Program Files (x86)\JetBrains\IntelliJ IDEA Community Edition 10.0\bin"" -Dfile.encoding=windows-1252 -classpath ""C:\Program Files (x86)\JetBrains\IntelliJ IDEA Community Edition 10.0\lib\idea_rt.jar; C:\Program Files (x86)\JetBrains\IntelliJ IDEA Community Edition 10.0\plugins\junit\lib\junit-rt.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\alt-rt.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\charsets.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\deploy.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\javaws.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\jce.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\jsse.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\management-agent.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\plugin.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\resources.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\rt.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\ext\dnsns.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\ext\localedata.jar; C:\Program Files\Java\jdk1.6.0_20\jre\lib\ext\sunjce_provider.jar; D:\source\APPS\MyApplication\env\lib\activation.jar; D:\source\APPS\MyApplication\env\lib\com.ibm.mq.jar; D:\source\APPS\MyApplication\env\lib\cryptix-jce-api.jar; D:\source\APPS\MyApplication\env\lib\cryptix-jce-provider.jar; D:\source\APPS\MyApplication\env\lib\gnu-regexp-1.1.4.jar; D:\source\APPS\MyApplication\env\lib\j2ee.jar; D:\source\APPS\MyApplication\env\lib\jdom.jar; D:\source\APPS\MyApplication\env\lib\jndi.jar; D:\source\APPS\MyApplication\env\lib\jnet.jar; D:\source\APPS\MyApplication\env\lib\jSNMP.jar; D:\source\APPS\MyApplication\env\lib\jsse.jar; D:\source\APPS\MyApplication\env\lib\junit.jar; D:\source\APPS\MyApplication\env\lib\log4j-1.2.8.jar; D:\source\APPS\MyApplication\env\lib\mail.jar; D:\source\APPS\MyApplication\env\lib\ojdbc14_10_2.jar; D:\source\APPS\MyApplication\env\lib\xalan.jar; D:\source\APPS\MyApplication\env\lib\xerces.jar"" com.intellij.rt.execution.application.AppMain com.intellij.rt.execution.junit.JUnitStarter -ideVersion5 tests.ValidationUtilsTest If I open up my Junit jar file that I'm using under junit/textui I only have a class called TestRunner. This makes me think IntelliJ is trying to use the wrong version (i.e. not 3.7) What can I try? I've Googled this but haven't had much luck Did you try to locate JAR files containing the class `junit/textui/ResultPrinter` in your workspace? You can use a tool such as [ClassFinder](http://www.adarshr.com/papers/classfinder) for this. IntelliJ IDEA doesn't support JUnit 3.7. The minimum supported version is 3.8. Update your JUnit jar in the classpath and everything will be fine. Well isn't that something... thanks! Many verions of Android don't have support JUnit 3.8 is there any way to use 3.7 in IntelliJ IDEA ? You can add 3.8 in the classpath so that it appears before 3.7 provided by Android. I'm using IDEA 11 where this problem occurs for android development for me too. I downloaded junit 3.8.1 and putted it in my test project as dependency. moved it up before android as recommended. But now I'm getting 'JUnit version 3.8 or later expected: java.lang.RuntimeException: Stub!' There is a question here on SO for that but the answer doesn't help too: http://stackoverflow.com/a/2431427/194609 Any pointers? Create a new question with the sample project and the steps to reproduce or contact support for help. details on how to add junit.jar in the classpath? There's a lot of config dialogs and gradle files..."
298,A,"How to unittest a class using RestTemplate offline? I have a class which has direct dependency on the RestTemplate. I wish I have a JUnit test of it offline. How could I mock a RestTemplate in my unittest? Check out my answer here http://stackoverflow.com/a/22334918/1077708 You can use the Mock classes in package org.springframework.mock.web. Usually you will need MockHttpServletRequest and MockHttpServletResponse but if you need more control you may also need others e.g. MockRequestDispatcher. Both of these implement the corresponding Servlet interfaces but add convenience methods for testing (and most importantly: they work without a real HTTP connection). You can find the Mock classes in the spring-test jar (accessible through Maven) Update: it seems that the above classes are no great help for RestTemplate after all. What you will need is to create a mock ClientHttpRequestFactory and I'm surprised to see that there isn't one in the above package. Here is some code to get you started (haven't tested it): public class MockClientHttpRequestFactory implements ClientHttpRequestFactory{ // overwrite this if you want protected MockClientHttpResponse createResponse(){ return new MockClientHttpResponse(); } // or this protected HttpStatus getHttpStatusCode(){ return HttpStatus.OK; } // or even this @Override public ClientHttpRequest createRequest(final URI uri final HttpMethod httpMethod) throws IOException{ return new MockClientHttpRequest(uri httpMethod); } public class MockClientHttpResponse implements ClientHttpResponse{ private final byte[] data = new byte[10000]; private final InputStream body = new ByteArrayInputStream(data); private final HttpHeaders headers = new HttpHeaders(); private HttpStatus status; @Override public InputStream getBody() throws IOException{ return body; } @Override public HttpHeaders getHeaders(){ return headers; } @Override public HttpStatus getStatusCode() throws IOException{ return getHttpStatusCode(); } @Override public String getStatusText() throws IOException{ return status.name(); } @Override public void close(){ try{ body.close(); } catch(final IOException e){ throw new IllegalStateException(e); } } } class MockClientHttpRequest implements ClientHttpRequest{ private final HttpHeaders headers = new HttpHeaders(); private final HttpMethod method; private final URI uri; private final OutputStream body = new ByteArrayOutputStream(); MockClientHttpRequest(final URI uri final HttpMethod httpMethod){ this.uri = uri; method = httpMethod; } @Override public OutputStream getBody() throws IOException{ return body; } @Override public HttpHeaders getHeaders(){ return headers; } @Override public HttpMethod getMethod(){ return method; } @Override public URI getURI(){ return uri; } @Override public ClientHttpResponse execute() throws IOException{ return createResponse(); } } } You did not answer my question. Your solution do help on the test of the server side servlet MVC framework. I am going to test the client side who use the MVC framework. @Dennis okay I updated my answer This is just what I was after for one of my REST projects. Thanks very much. (+1) the idea works. But I have the feeling that the two methods `getStatusCode` and `getStatusText` in `MockClientHttpResponse` suffering a bit from Schizophrenia. May `getStatusText(){return this.getStatusCode().name();}` would heal them. ;-) @Ralph yup sounds ok @SeanPatrickFloyd I also have similar question [here](https://stackoverflow.com/questions/25698072/simpleclienthttprequestfactory-vs-httpcomponentsclienthttprequestfactory-for-htt) which uses RestTemplate see if you can help me out if possible. Any help will be appreciated.  Sping 3.0 introduced RestTemplate. Since version 3.2 the Spring MVC test framework has provided the class MockRestServiceServer for unit testing client REST code. Is it available on Android? @Piotr This is working for me with gradle: androidTestCompile(""org.springframework:spring-test:3.2.8.RELEASE"") { exclude module: ""spring-core"" }  spring-social-test contains mockup classes that help write tests for RestTemplate. There are also some examples on how to use it within the git repository (e.g. OAuth1TemplateTest). Please keep in mind that there's currently a Spring feature request (#SPR-7951) to move these classes to spring-web. This is ***very*** useful for testing rest clients. they moved this into [spring-test-mvc](http://tinyurl.com/csmuvdz)  I suggest refactoring your client code to remove the direct dependency on RestTemplate and replace it with references to RestOperations which is the interface implemented by RestTemplate. and the one you should be coding to. You can then inject a stub or mock of RestOperations into your code for unit testing and inject a RestTemplate when using it for real."
299,A,"JUnit Test method with randomized nature I'm working on a small project for myself at the moment and I'm using it as an opportunity to get acquainted with unit testing and maintaining proper documentation. I have a Deck class with represents a deck of cards (it's very simple and to be honest I can be sure that it works without a unit test but like I said I'm getting used to using unit tests) and it has a shuffle() method which changes the order of the cards in the deck. The implementation is very simple and will certainly work: public void shuffle() { Collections.shuffle(this.cards); } But how could I implement a unit test for this method. My first thought was to check if the top card of the deck was different after calling shuffle() but there is of course the possibility that it would be the same. My second thought was to check if the entire order of cards has changed but again they could possibly be in the same order. So how could I write a test that ensures this method works in all cases? And in general how can you unit test methods for which the outcome depends on some randomness? Cheers Pete I think that it does make sense for the deck to be considered shuffled if the cards remain in the same order and some re-ordering has occurred although unlikely it is possible to shuffle a real deck of cards back into the same order they were originally in. From a logical perspective would your cards be considered shuffled if the order remained the same? For your last question see http://stackoverflow.com/questions/122741/testing-for-random-value-thoughts-on-this-approach. I also think D. Knuth has a whole chapter about it. If your code uses random number generator set its seed to a fixed value in your unit tests. This way your deck will always shuffle the same way in your tests. This may require an API change to enable your code to publicly expose a method to set the rand seed which can be set in JUnit tests Possible duplicate of http://stackoverflow.com/questions/56411/how-to-test-randomness-case-in-point-shuffling Interesting question. In my opinion the best way would be to store each ""shuffle"" in a collection then compare after each shuffle if your deck matches any of the previous ""decks"" in the collection. Depending on the ammount of ""Randomness"" you require you will increase the ammount of shuffled decks you store in that unit test i.e. after 50 shuffles you would have a collection of 50 ""decks""  Most people seem to be of the opinion that you should test what you're testing. By that I mean what you're building (or integrating when you're making sure a third-party library actually does what it says it does). But you should not test the Java language itself. There should be some testing principle like ""Don't Test PlusEquals"".  You're actually delegating away all the hard work to the java.util.Collections class. This is a central class in Java's collection API and you should just assume that it works like you probably do with the java.lang.String class. I would rather recommend to code against interfaces and mock/stub away your implementation class with the shuffle() method. Then you can just assert that your calls on the shuffle() method are actually called from your test instead of testing exactly the same as the Sun/Oracle guys have tested thorough before. This enables you to focus more on testing your own code where 99.9% of all the bugs probably are located. And if you for example replace java.util.Collections.shuffle() method with one from another framework or your own implementation your integration test will still work! I understand that you're doing this because you want to learn and I believe the knowledge about stubbing/mocking away logic from other frameworks are very useful as part of your testing knowledge.  I suppose you have 52 cards in your deck. The possibility of getting the same order in two subsequent calls is extremely low so I wouldn't bother about it too much. But if you do start getting similar decks multiple times I think it's safe to say you have some problems with your random number generator. So the answer: check that the order is different for the whole deck. Also I think that you can safely make it a requirement for your shuffle() method not to return the cards in the same order twice in a row. And if you want to absolutely make sure to follow that requirement you can check for similarity in the method implementation. Also check that the set of cards represented by each deck is the same (i.e. same number of cards same members once order is ignored).  Asserting whether your shuffle method actually shuffles the cards is very hard if not impossible. Default random number generators are only random to a certain degree. It's impossible to test whether you're satisfied with this degree of randomness because it would take too much time. What you're actually testing is the random number generator itself which doesn't make much sense. What you can test however are the invariants of this method. If you exit the method there should be the exact same number of cards in the deck as when you entered it. The shuffle method should not introduce duplicates. You can of course create a test that checks that in a sequence of n shuffles there are no duplicate decks returned. But once in a while this test may fail (however unlikely as already stated in other answers). Something else to take into account is the random number generator itself. If this is just a toy project java.util.Random is sufficient. If you intend to create some online card game consider using java.security.SecureRandom.  I've worked on random numbers in a modeling and simulations framework and stood before a similar problem: How can I actually unit-test our PRNG implementations. In the end I actually didn't do it. What I did instead was to perform a few sanity checks. For example our PRNGs all advertise how many bits they generate so I checked whether those bits actually did change (with 10k iterations or so) and all other bits were 0. I checked for proper behavior concerning seeds (initializing the PRNG with the same seed must produce the same sequence of numbers) etc. I then decided to put the actual randomness tests into an interactive UI so they can be tested whenever desired but for unit tests a non-deterministic outcome isn't that nice I thought.  You could shuffle repeatedly keeping track of how many times the Ace of spades (or some other card or all other cards) ends up as the first card in the deck. Theoretically the card should end up on top about 1 out of 52 shuffles. After all the data has been gathered compare the actual frequency to the number 1/52 and check if the difference (absolute value) is lower than some chosen epsilon value. The more you shuffle the smaller your epsilon value can be. If your shuffle() method puts the card on the top within your epsilon threshold you can be sure it is randomizing the cards as you would like. And you don't have to stop just at the top card. You can test if each location in the deck gives the same results. Do it with one card do it will all cards it probably doesn't matter. It might be overkill but it would guarantee your shuffle() is working correctly.  Another approach would be to use the shuffle(List<?> list Random random) method and to inject a Random instance seeded with a constant. That way your JUnit test can run a series of calls and check the output to be the expected output. The normal implementation of your class would create a Random instance which is unseeded.  Firstly let's think about the probabilities involved: You can't guarantee that the shuffle won't place the cards in exact order. However the probability of doing this with a 52-card deck is 1 / 52! (i.e. it's minimal and probably not worth worrying about.) You definitely will need to check the whole deck though because the probability of the top card being the same as it was before the shuffle is 1 / 52. For the general case and assuming you're using the java.util.Random number generator just initialise it with the same seed. Then the output for a pre-determined input should then be repeatable. However specifically for this case assuming you haven't implemented your own List I don't really see the point in testing Collections.shuffle(List<?> list) or Collections.shuffle(List<?> list Random rnd) (API link) as these are just part of the Java API."
300,A,"in phpunit is there a method similar to onconsecutivecalls for use inside the ""with"" method? Using PHPUnit I'm mocking the pdo but I'm trying to find a way to prepare more than one database query statement. $pdo = $this->getPdoMock(); $stmt = $this->getPdoStatementMock($pdo); $pdo->expects($this->any()) ->method('prepare') ->with($this->equalTo($title_query)) ->will($this->returnValue($stmt)); $title_stmt = $pdo->prepare($title_query); $desc_stmt = $pdo->prepare($desc_query); I want to pass something similar to onConsecutiveCalls for the ""with"" method so I can prepare multiple statements as seen above. How would you go about doing this? PHPUnit 4.1 got a new method withConsecutive(). From the Test Double Chapter: class FooTest extends PHPUnit_Framework_TestCase { public function testFunctionCalledTwoTimesWithSpecificArguments() { $mock = $this->getMock('stdClass' array('set')); $mock->expects($this->exactly(2)) ->method('set') ->withConsecutive( array($this->equalTo('foo') $this->greaterThan(0)) array($this->equalTo('bar') $this->greaterThan(0)) ); $mock->set('foo' 21); $mock->set('bar' 48); } } Each argument of withConsecutive() is for one call to the specified method.  A couple of folks have noted that at($index) can be used for specific instances of calls to a method. David H. and Vika clarified that $index counts ALL calls to ALL mocked methods of the object. In addition it may be worth noting that the Test Doubles Chapter of the PHPunit documentation has a warning note about this. It points out that using at() should be done with caution since it can lead to brittle tests that depend too much on the specific implementation.  The only thing I have found resembling what you ask is using the 'at': $mock->expects($this->at(0))->method // etc $mock->expects($this->at(1))->method // etc So you set expectations for the first time it is called (at 0) the second time and so on. Perfect! That's exactly what I needed. Thanks!  You can match consecutive invocations of the same method by writing separate expectations with $this->at() instead of $this->any(): $pdo->expects($this->at(0)) ->method('prepare') ->with($this->equalTo($title_query)) ->will($this->returnValue($stmt)); $pdo->expects($this->at(1)) ->method('prepare') ->with($this->equalTo($desc_query)) ->will($this->returnValue($stmt)); $title_stmt = $pdo->prepare($title_query); $desc_stmt = $pdo->prepare($desc_query); Perfect! That's what I was looking for. Working great now. Thanks! Yes good point. And only methods called on Tuesdays. ;) Note that the counter is per-mock across *all* method calls received to it. Thus if there are going to be two intervening calls to `$pdo` you would use 0 and 3. @David yes you're right. Still worth mentioning is that only calls to methods that were `actually replaced` with mock implementation count. I believe you can also combine `at()` with `any()`. I wanted the first call to a mock method to return false and all subsequent calls to return true. So I used `at(0) ... will($this->returnValue(false)` and `any() ... will($this->returnValue(true)`."
301,A,Junit with Embedded Glassfish fails - JMS Resource Adapter should be EMBEDDED I'm trying to test a session bean (NetBeans 6.8 Glassfish V3). Unfortunately the embedded glassfish is unable to start properly as it tries to connect to a remote JMS Provider (at localhost:7676): $ ant test ... [junit] Mar 23 2010 12:13:51 PM com.sun.messaging.jms.ra.ResourceAdapter start [junit] INFO: MQJMSRA_RA1101: SJSMQ JMS Resource Adapter starting: REMOTE [junit] Mar 23 2010 12:13:51 PM com.sun.messaging.jmq.jmsclient.ExceptionHandler throwConnectionException [junit] WARNING: [C4003]: Error occurred on connection creation [localhost:7676]. - cause: java.net.ConnectException: Connection refused The error is in itself correct as no (other) JMS provider is running. I was expecting the embedded glassfish to start the JMS provider in EMBEDDED mode. My test uses javax.ejb.embeddable.EJBContainer : @BeforeClass public static void initContainer() throws Exception { ec = EJBContainer.createEJBContainer(); ctx = ec.getContext(); } When I start glassfish normally it's fine: $ bin/asadmin get server.jms-service.type server.jms-service.type=EMBEDDED How can I get my junit tests to use an embedded glassfish with an EMBEDDED JMS Provider? Ok I downloaded an official glassfish build (v3.0.1) from here and now it's starting the JMS Provider EMBEDDED as expected.
302,A,How was the first release of JDK Unit Tested? Can someone please explain the following How was the first JDK release unit Tested? Since Junit came after Java how did they do it? Are the current releases using Junit to test the JDK API? Regards The JDK is tested  at least now using jtreg . I am not aware of any usage of JUnit to test the JDK.  Unit testing is a concept and a practice. It is not a package or specific implementation thereof (although it was admittedly popularised by JUnit). I don't know what's the Sun Oracle :-) standard is for testing. I'm sure it's pretty thorough though. Bear in mind there is more than on JDK publisher. Unit testing in Java may (or may not) have been popularized by JUnit but it's been around in other languages for a very long time. The tens of thousands of unit tests for the Tcl core for example have long used the builtin Tcltest package.  I imagine that the very first JDKs (pre Java 1.0) were tested using a harness implemented using other technologies; e.g. C shell scripting and so forth. Certainly Java unit testing was done in Java well before JUnit came along. I remember using a framework developed in-house at DSTC in the late 1990s and the GNU Classpath project used a framework called Mauve. Java test frameworks are not rocket science.
303,A,Unit Testing XML independent of physical XML file My question is: In JUnit How do I setup xml data for my System Under Test(SUT) without making the SUT read from an XML file physically stored on the file system Background: I am given a XML file which contains rules for creation of an invoice. My job is to convert these rules from XMl to Java Objects e.g. If there is a tag as below in my XML file which indicates that after a period of 30 days the transaction cannot be invoiced <ExpirationDay>30</ExpirationDay> this converts to a Java class  say ExpirationDateInvoicingRule I have a class InvoiceConfiguration which should take the XML file and create the *InvoicingRule objects. I am thinking of using StAX to parse the XML document within InvoiceConfiguration Problem: I want to unit test InvoiceConfiguration. But I dont want InvoiceConfiguration to read from an xml file physically on the file system . I want my unit test to be independent of any physical stored xml file. I want to create a xml representation in memory. But a StAX parser only takes FileReader( or I can play with the File Object) Thanks guys for your help. Another good source is this link: http://marc.info/?l=xerces-j-dev&m=86952145010437&w=2 This is the answer from Baq Haidri -----Original Message----- From: Baq Haidri [mailto:bhaidri@....] I found this in a book called 'Professional XML' by Wrox publishing: > >public void parseString(String s) throws SAXException IOException >{ > StringReader reader = new SringReader(s); > InputSource source = new InputSource(reader); > parser.parse(source); >}  Refactor your code to take a Reader instead of opening a File. The File opening can be done separately (and is largely untestable if you want to stick to the rule of not accessing files in unit tests - but that's OK because it's just one line and you can't really get it wrong!)  Here is an example of quite a few tests that use both XMLUnit and a custom framework for processing XML. The framework uses StAX to map XML to Java POJOs. https://simple.svn.sourceforge.net/svnroot/simple/trunk/download/stream/src/test/java/org/simpleframework/xml/core/ There are no external dependencies and test coverage is approx 90% http://simple.sourceforge.net/download/stream/report/cobertura/
304,A,"How to test content providers on Android I am trying to test my DB using ProviderTestCase2<T>. I can see the test DB being created. As such I suppose the tested content provider should use the test DB. But as soon as I try any calls against the MockContentResolver (or the one created with newResolverWithContentProviderFromSql) I get an UnsupportedOperationException. This is documented for the MockContentResolver as normal behavior. As such I am a bit unsure on the purpose of the ProviderTestCase2. How do you test your content providers? Thanks I add this entry as I think it can help programmers that want to test their Content Provider. Imagine your Content Provider is called MyProvider and that you have a contract class called MyProviderContract defining some constants. First of all you'll write a test class called MyProviderTestCase that inherits from ProviderTestCase2<MyProvider>. You'll have to define a void constructor which will call the super constructor: public void MyProviderTestCase() { super(MyProvider.class MyProviderContract.AUTHORITY); } Then instead of using directly your provider (avoid using getProvider() as users of your content provider won't access it directly) use the getMockContentResolver() to obtain a reference to a content resolver and then call the methods of this content resolver (query insert etc.). In the following code I show how to test the insert method. public void testInsert() { Uri uri = MyProviderContract.CONTENT_URI; ContentValues values = new ContentValues(); values.put(MyProviderContract.FIELD1 ""value 1""); values.put(MyProviderContract.FIELD2 ""value 2""); Uri resultingUri = getMockContentResolver().insert(uri values); // Then you can test the correct execution of your insert: assertNotNull(resultingUri); long id = ContentUris.parseId(resultingUri); assertTrue(id > 0); } Then you can add as many test methods as you want using a content resolver instead of your content provider directly as would do users of your content provider.  Extend ProviderTestCase2 to override getMockContentResolver() and return your own class derived from MockContentResolver. public class MyProviderTestCase2 extends ProviderTestCase2 { @Override public MockContentResolver getMockContentResolver () { return new MyMockContentResolver(); } } MyMockContentResolver will need to override any methods you want to test in your ContentProvider. Then you should be able to run any tests you want on your content provider while it's isolated by ProviderTestCase2  I have been rewriting bits and bobs on the testprovider: https://github.com/novoda/RESTProvider/blob/master/RESTProviderTest/src/android/test/ProviderTestCase3.java I found this to be the best approach. .../ProviderTestCase3.java - 404 You can find it here: https://github.com/novoda/RESTProvider/blob/525ab182322558416b36898f9a766d2e8a93c108/RESTProviderTest/src/android/test/ProviderTestCase3.java  As far as I found setting up the mock content resolver is not explicitly necessary - I might oversee cases where it is(maybe correct resolving of the provider via URI hings that need corect getType() working) but for me it was enough to do something like this: package org.droidcon.apps.template.provider.test; import org.droidcon.apps.template.provider.ProfileContract; import org.droidcon.apps.template.provider.ProfileProvider; import android.content.ContentProvider; import android.content.ContentValues; import android.database.Cursor; import android.net.Uri; import android.test.ProviderTestCase2; public class ProfileProviderTest extends ProviderTestCase2<ProfileProvider> { public ProfileProviderTest() { super(ProfileProvider.class ProfileProvider.class.getName()); } protected void setUp() throws Exception { super.setUp(); } /** * Very basic query test. * * Prerequisites: * <ul> * <li>A provider set up by the test framework * </ul> * * Expectations: * <ul> * <li> a simple query without any parameters before any inserts returns a * non-null cursor * <li> a wrong uri results in {@link IllegalArgumentException} * </ul> */ public void testQuery(){ ContentProvider provider = getProvider(); Uri uri = ProfileContract.CONTENT_URI; Cursor cursor = provider.query(uri null null null null); assertNotNull(cursor); cursor = null; try { cursor = provider.query(Uri.parse(""definitelywrong"") null null null null); // we're wrong if we get until here! fail(); } catch (IllegalArgumentException e) { assertTrue(true); } } } There is not much sample code on the ProviderTestCase2 class on the internet. This is very helpful. In [Android documentation](http://developer.android.com/tools/testing/contentprovider_testing.html#WhatToTest) they recommend not to use directly your content provider and pass through a content resolver instead. Just read my post to see a more adequate example of how to test a content provider."
305,A,"Model parameter in Junit test method I'm new here and I'm learning Spring-MVC and Junit. I'm trying to implement the test methods with Junit for this basic controller method: public String home(Model model) { model.addAttribute(new Contact()); logger.info(""Welcome home!""); return ""home""; } And the test method is the following: public void testHome() throws Exception{ ContactsController contactsController=new ContactsController(); Assert.assertEquals(""home""contactsController.home(new Model())); } The problem is since Model is an interface I'm obviously not able to create a new Model object to pass it as a parameter to contactsController.home(...). What options would you give me? I have no idea what to do. Thanks You need to pass an implementation of Model such as ExtendedModelMap: public void testHome() throws Exception{ ContactsController contactsController=new ContactsController(); Assert.assertEquals(""home""contactsController.home(new ExtendedModelMap())); } yeah but then the types won't match. So does that mean that I have to change the parameter type as well in the controller method implementation?? @Neets: Types would match since `ExtendedModelMap` implements `Model`. thanks!! I made that observation because I had already tried that with ModelMap and it didn't work but it does work with ExtendedModelMap. So thanks for your help =)"
306,A,"How can I specify JUnit test dependencies? Our toolkit has over 15000 JUnit tests and many tests are known to fail if some other test fails. For example if the method X.foo() uses functionality from Y.bar() and YTest.testBar() fails then XTest.testFoo() will fail too. Obviously XTest.testFoo() can also fail because of problems specific to X.foo(). While this is fine and I still want both tests run it would be nice if one could annotate a test dependency with XTest.testFoo() pointing to YTest.testBar(). This way one could immediately see what functionality used by X.foo() is also failing and what not. Is there such annotation available in JUnit or elsewhere? Something like: public XTest { @Test @DependsOn(method=org.example.tests.YTest#testBar) public void testFoo() { // Assert.something(); } } JExample and TestNG have something like that. I don't know how useful it is but if you try it please come back to tell us whether it was useful. Yeah it seems like an excellent start: http://chem-bla-ics.blogspot.com/2010/08/specifying-unit-test-dependencies-with.html @Cherry: Sounds like an excuse for not writing [fast tests](http://agileinaflash.blogspot.fi/2009/02/first.html). Some options for improving the design: (1) make methodB fast (2) make methodA not depend on methodB (3) use a test double for ClassB when testing ClassA. It useful in situation when ClassA.methodA() invokes ClassB.methodB() and both takes a lot of time. In this case you can specify that testMethodA() depends on testMethodB() that is if testMethodB() fails testMethodA() also fails **without execution**.  There really isn't something like this that I'm aware of. (Edit: you learn something new every day :)) In my opinion this isn't that bad of a thing (though I can see it being useful especially when JUnit it being used for other forms of automated tests - e.g. integration tests). Your tests IMO aren't in the strictest sense of the word ""unit tests"" (at least not the test for X#foo()). Tests for X#foo() should succeed or fail depending only on the implementation of X#foo(). It should not be dependent on Y#foo(). What I'd do in your position is to mock out Y and implement something like MockY#foo() with very simple controlled behavior and use it in X#foo()'s tests. That said with 15000 tests I can see how this would be a pain to refactor. :) I don't see why unit tests cannot have dependencies... our toolkit defines data objects and algorithms... if a data object fails how do you expect the algorithm to automagically correct for that? Perhaps the naming of both methods foo() was misleading... Only strict mock tests would not exhibit any dependencies on each other (and still they may...). In general such dependencies are always possible and have to reflect dependencies in OO design. I believe that maintaining such annotation across all tests would add to complexity without significant benefit but having such annotation in a few core cases may benefit.  You can declare test dependencies in TestNG the syntax is almost the same as in your example. I don't think JUnit offers something similar. https://github.com/junit-team/junit.contrib/tree/master/assumes  In behavior driven design library jBehave there's a keyword GivenScenarios which imports a list of scenarios that are run before the main scenario. This gives an opportunity to define dependencies and have one point of failure. jBehave's logging will tell you if test fails in dependencies or main body section.  There's a contribution to JUnit that address this: https://github.com/junit-team/junit.contrib/tree/master/assumes Good good! I hope it gets merged in soon :)"
307,A,JUnit dynamic method dispatch? Here is the problem I am facing. I have been tasked with testing the query parsing engine of a piece of software through negative testing. That is I must write a large number of queries that will fail and test that they do indeed fail as well as having the expected error message for the particular error in the query. These are defined in an XML file. I've written a simple wrapper around the parsing of the XML document and struct-like classes for these test cases. Now given that I am using JUnit as a testing framework I'm running into this issue - the act of running through all of these externally defined tests lives in a single method. If a single test fails then no more will be run. Is there any way to dynamically dispatch a method to handle each of the tests as I encounter them? This way if a test fails we can still run the remaining ones while getting a report on what did and did not fail. The other alternative is of course writing all of the JUnit tests. I'd like to avoid this for many reasons one of which is that the number of tests to be run is extremely large and a test case is 99% boilerplate code. Thanks. generate code ;) hehe You should look into JUnit's Parameterized annotation. Unless I am misunderstanding it I'd still have to write a method for every single test though. Is that not the case? My post was too hasty - I did misunderstand @Paramaterized. I think this would allow me to accomplish what I am trying to. Thanks for the help!  If I understand correctly the input data and expected results are all defined in XML so you don't need specific code to handle each test case? If you use JUnit4 you could write your own Runner implementation. You could either implement Runner directly or extend ParentRunner. All you need to implement is one method that returns a description of the tests and another method that runs the tests.
308,A,Junit4 Test Suites How do I create test suites with Junit4? All the documentation I've seen doesn't seem to be working for me? And if I use the eclipse wizard it doesn't give me an option to select any of the test classes I created? An up-to-date easy to digest documentation/tutorial link would be most welcome :) With Eclipse 3.7 Indigo the test suite wizard now supports JUnit 4 Of the top of my head create a TestSuite and the invoke addTests. If you want somesource to look at try any opensource lib like hibernate or something from apache and take a look under the test directory of the source for a Tests suite ...  I think TestSuite has fallen out of favor. That might have been the style before 4.x but it's not now as far as I know. I just annotate the tests I want and then run the class. All the annotated tests are run. I might use Ant but most of the time I have IntelliJ run them for me. I could be wrong but i think TestSuite is still good when we need to specify the order of tests especially in automated integration tests where testing smaller scenarios should come before more complex scenarios. @duffymo I find your comment interesting as I have so far been coached in the pre-4.x line of thought regarding organizing testcases into testsuites If you have larger numbers of tests different functional areas/modules a distinction between 'core' and auxiliary/ slower/ expensive non-core tests -- for any of these reasons -- you would structure tests into a TestSuite. You can use the `@SuiteClass` annotation in multiple classes and you can nest Suites within Suites. The line of thought for organizing them in 3.x is still totally valid (and ought to be used). You can structure everything the exact same way as you did before; you just use the annotation syntax instead of `TestSuite.suite()`. There's really no benefit to using `TestSuite` over the annotation and you can take advantage of all the JUnit 4 enhancements if you use the organizationally equivalent annotation.  import org.junit.runners.Suite; import org.junit.runner.RunWith; @RunWith(Suite.class) @Suite.SuiteClasses({TestClass1.class TestClass2.class}) public class TestSuite { //nothing }  You can create a suite like so. For example an AllTest suite would look something like this. package my.package.tests; @RunWith(Suite.class) @SuiteClasses({ testMyService.class testMyBackend.class ... }) public class AllTests {} Now you can run this in a couple different ways: right-click and run in Eclipse as Junit test create a runable Java Application; Main class='org.junit.runner.JUnitCore' and Args='my.package.tests.AllTests' run from the command line: $ java -cp build/classes/:/usr/share/java/junit4.jar:/usr/share/java/hamcrest-core.jar org.junit.runner.JUnitCore my.package.tests.AllTests Would add even more VoteUps for the CMDLine :)
309,A,No tests found using JMock having problems trying to run by unit test with Ant my test class uses Jmock; @RunWith(JMock.class) and annotations to identify each test method. When i attempt to build with ant (1.7.1) i get a [junit] No tests found in MyTestClass [junit] junit.framework.AssertionFailedError: No tests found Any suggestions? can you show us the whole class? or at least the imports and one method? If you're using JUnit 3 you should inherit from MockObjectTestCase instead of using the Runner  What version of JUnit are you using? It looks like 3.8 to me (although I could be mistaken) but the package structure for JUnit 4.x is: org.junit and not junit.framework The @RunWith runner as defined by the JMock guys is only valid with JUnit 4.x
310,A,"Run Current Junit Test in GWTTestCase I have a JUnit test that I run on one class but I recently wrote an emulated version for GWT. Since the specification is the same I would like to use the same test case but I want it to run in the GWT environment which would typically be accomplished by extending GWTTestCase. I really want to avoid any copy/paste nonsense because there are likely to be added tests in the future which I should not be burdened with copying later. How can I import/inherit my standard unit test to be run as either a regular test case or a GWT test case? I think there is no easy way .. But you can extract an interface of your junit test gwt test case and junit test case implements this interface. You can create a third class for implementation all test call methods of gwt test case and junit test are delegated to this implementation class. public interface IRegularTest { public void testSomething(); public void testSomething2(); } public class RegularTestImpl implements IRegularTest { public void testSomething(){ // actual test code } public void testSomething2(){ // actual test code } } public class RegularTest extends TestCase implements IRegularTest { IRegularTest impl = new RegularTestImpl(); public void testSomething(){ impl.testSomething } public void testSomething2(){ } } public class GwtTest extends TestCase implements IRegularTest { IRegularTest impl = new RegularTestImpl(); public void testSomething(){ impl.testSomething } public void testSomething2(){ } } Thanks but this was the type of thing I'd like to avoid. If I have to do this it's pretty likely I won't want to add additional tests when I know I need to. I think you could not find a better solution than this one. If you could I would be the first one who will upvote. Found out if you extend GWTTestCase and override getModuleName to return null it runs as a pure java test.  I have found the solution to this problem. If you extend the original test with GWTTestCase you can override getModuleName to return null. This tells GWTTestCase to run as a normal pure java test (no translation at all). You can then extend this test case with one that overrides getModuleName to return a module name and the same tests will be run with translation. Basically: public class RegularTest extends GWTTestCase { @Override public String getModuleName() { return null; } public void testIt() {...} } ...and the GWT version... public class GwtTest extends RegularTest { @Override public String getModuleName() { return ""some.module""; } } The downside to this is that it forces you to use JUnit3 style tests which I find a little annoying but it beats the alternative."
311,A,"JUnit assertEquals(double expected double actual double epsilon) Possible Duplicate: JUnit: assertEquals for double values Apparently the assertEquals(double expected double actual) has been deprecated. The javadocs for JUnit are surprisingly lacking considerings its wide use. Can you show me how to use the new assertEquals(double expected double actual double epsilon)? Yup its a duplicate. I voted to close. Thanks all! See [the following question with answers](http://stackoverflow.com/q/5686755/680503). Yea just realized this is a duplicate. If it's a duplicate then post a comment as such rather just passing off a link as a new answer. Epsilon is your ""fuzz factor"" since doubles may not be exactly equal. Epsilon lets you describe how close they have to be. If you were expecting 3.14159 but would take anywhere from 3.14059 to 3.14259 (that is within 0.001) then you should write something like double myPi = 22.0d / 7.0d; //Don't use this in real life! assertEquals(3.14159 myPi 0.001); (By the way 22/7 comes out to 3.1428+ and would fail the assertion. This is a good thing.)"
312,A,"What is the recommended way to integrate Hamcrest into Eclipse's JUnit? Is there a recommended way to integrate Hamcrest into the JUnit configuration in Eclipse? Currently Eclipse's JUnit comes with Hamcrest-core only. I want to edit that configuration to include Hamcrest-all. How should I do this? I had problems using hamcrest-all and junit-dep - you then need jMock an easymock and ant.tasks If you just want more matchers add hamcrest-library (along with the junit and hamcrest core you get from Eclipse) Shouldn't this question have the ""hamcrest"" tag? After I posted this I ran into a problem with this approach. I get a SecurityException. Apparently this is because the Eclipse hamcrest-core is signed but the hamcrest-library I added isn't. Supposedly the solution is to put the unsigned one first in the build path but that didn't fix it for me. The tag didn't exist when I created the question IIRC. I've added it now. Thanks for the suggestion.  There's nothing to stop you adding hamcrest as a JAR to your eclipse project's build path. If there's one packaged with eclipse (and I didn't realise there was but fair enough) then it's just there as a convenience. Yes I could always add it as a library but I was hoping to find a way to embed it into JUnit's configuration so that it's automatically added to all (existing and new) projects that use JUnit. This doesn't actually work. Hamcrest adds some of the same classes that JUnit is providing thus leading to SecurityExceptions.  JUnit goes in two distributions: with Hamcrest (junit-4.6.jar) and without Hamcrest (junit-dep-4.6.jar). If I understand your question correctly you need to specify junit-4.6.jar in JUnit configuration. I think it is possible in Eclipse (I use another IDE). It seems to me that JUnit only includes hamcrest-core while I am trying to integrate hamcrest-all into Eclipse. You're right. Probably you can build your own junit.jar with embedded classes from hamcrest-all.jar and replace junit.jar in JUnit eclipse plugin with it. I'm not sure it is a good idea but you can try. Yes I believe this would work but I don't think it's the ""recommended"" way of doing it. :)"
313,A,"JUnit runner gets NullPointerException from java.io.Writer I'm getting a NullPointerException from Maven Surefire plugin. It occurs only on a test that is using DBUnit. The Surefire report file is empty. ------------------------------------------------------- T E S T S ------------------------------------------------------- org.apache.maven.surefire.booter.SurefireExecutionException: null; nested exception is java.lang.NullPointerException: null java.lang.NullPointerException at java.io.Writer.write(Writer.java:140) at java.io.PrintWriter.newLine(PrintWriter.java:436) at java.io.PrintWriter.println(PrintWriter.java:585) at java.io.PrintWriter.println(PrintWriter.java:696) at org.apache.maven.surefire.report.AbstractFileReporter.testSetStarting(AbstractFileReporter.java:59) at org.apache.maven.surefire.report.ReporterManager.testSetStarting(ReporterManager.java:219) at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:138) at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:127) at org.apache.maven.surefire.Surefire.run(Surefire.java:177) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:338) at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:997) [INFO] ------------------------------------------------------------------------ [ERROR] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] There are test failures. When I try to run the same test from NetBeans IDE using a JUnit runner I get the same exception: Exception in thread ""main"" java.lang.NullPointerException at java.io.Writer.write(Writer.java:140) at org.apache.tools.ant.util.DOMElementWriter.write(DOMElementWriter.java:212) at org.apache.tools.ant.taskdefs.optional.junit.XMLJUnitResultFormatter.endTestSuite(XMLJUnitResultFormatter.java:171) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.fireEndTestSuite(JUnitTestRunner.java:714) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:547) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1031) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:888) Test football.dao.jpa.SimpleJPATest FAILED (crashed) The write method from java.io.Writer class has only one String parameter. That means the runner must be passing null as an argument. But how does this happen? public void write(String str) throws IOException { write(str 0 str.length()); } What youre actually seeing is the line.separator System's property is null. I cannot explain why based on what I see but if you can test it try doing: java.security.AccessController.doPrivileged( new sun.security.action.GetPropertyAction(""line.separator"")); Awesome! will memorize this little evil trick +1 just beat me to it. For reference this can be derived from the `PrintWriter.newLine` call which simply defers to `out.write(lineSeparator)`. Thank you! The line.separator property indeed was null. I caused it by calling System.setProperties( props ) with Properties object that contained only 4 properties for DBUnit. I wasn't aware that this method replaces all the properties. I should have read the javadoc more carefully. Again thanks to both of you! Super awesome. Just spent hours trying to debug into the cause of this. Same as you someone had used the System.setProperties(props) method."
314,A,Bulk data set up for Unit Test cases I have developed an application with Spring MVC that deals with bulk data insert/update. For ex: There are some use cases that insert a record with 100-125 attributes. For bulk data insert I'm hard coding the values to be inserted in my Unit test class. I have Transfer Object to carry the data so i'm populating these TOs in my Unit test cases and then calling the desired operation to be tested. For every DAO at least 4 test cases are needed for unit testing CRUD operations. I'm finding it very hard to populate the TOs by hard coding the values in my test case source file. Imaging writing 125 setters for every unit test case. I want to populate my TOs dynamically by reading the data from an XML file or any kind of media so that I need not change the hard coded data for test cases everytime. Setting up the data in an XML file is much easier than hard coding the values in JUNIT source files. I could think of some innovative solutions like setting up data in XML file and then using any JAXB implementation to read the same and populate the TOs.. but i believe there much be some easy and better way to handle this kind of requirement. Need some help on this. Imaging writing 125 setters for every unit test case. You need test helper method(s) that take care of test data. Then you call appropriate method that populates a TO: it could be as little as 0 parameters (completely random/fixed not test driven) or as many as 125 (completely controlled by the test) (and anything in between). Either way no setters in tests anymore.  I am not sure about the Java unit testing frameworks. But in DotNet there is a library called NBuilder which helps you to create test data very easily and quckly. Here is a blog I had written to demonstrate its capabilities. May be you can try and see if there exists an Java alternative to NBuilder or there is a port of the same available in Java. Most of the DotNet libraries I find are ported from Java world like NHibernate Spring.Net etc. So my guess is that you might have a Java equivalent of NBuilder as well.  Maybe you could use this framework: http://jtestcase.sourceforge.net
315,A,"JUnit extend base class and have tests in that class being run I am using JUnit 3 and have a situation where often I have to test that an object is created correctly. My idea was to write a class MyTestBase as shown below and then extend from that for the situation specific unit tests. However in the example I've given MyTests does not run the tests in MyTestBase. public class MyTestBase extends TestCase { protected String foo; public void testFooNotNull() { assertNotNull(foo); } public void testFooValue() { assertEquals(""bar"" foo); } } public class MyTests extends MyTestBase { public void setUp() { this.foo = ""bar""; } public void testSomethingElse() { assertTrue(true); } } What am I doing wrong? Update Apologies. Stupid error. The tests in my base class weren't named correctly. why not using junit 4? Because the system is written in Java 1.4.2 I've voted to close the question because it is just wrong. It doesn't ask a real question. I don't know what exactly you want to do but usually it is not a very good idea to too much common parts in test because when the common part fails you will have a large number of tests that fail even tough you probably have just one small bug in your software. I suggest you to use a Factory or a Builder to create the complex object and then test the Factory (or Builder) for creating the object correctly.  You have said ""MyTests does not run the tests in MyTestBase."". I tried it and all tests were called including the ones in MyTestBase. did you try it with JUnit 3? Yes I tried it from Eclipse with JUnit 3. I ran the MyTests class as a JUnit test and all tests passed successfully. nice. then the question is irrelevant :)  What you are trying to do is not the most appropriate way to achieve your goal: If you want to have common functionality that makes some checks define it in a utility class in static methods define it in the superclass and call it from each test method  Well you could make MyTestBase abstract so that it didn't try to run tests in the base class. A better solution would be to have setUp in the base class and make it call abstract methods (e.g. getFoo()) to initialize the variables it will require later on. In fact if you have those abstract methods you may find you don't even need a set-up phase in the first place - you could call the abstract methods where you need the value instead of using an instance variable. Obviously it will depend on the exact situation but in many cases this could be a lot cleaner."
316,A,"Where do I put properties files for running JUnit tests on Maven? I'm using Maven 3.0.3. I'm running the ""mvn test"" command in which my test files are in the standard place (src/test/java). Where do I put properties files so that they get picked up by Java's ""getResourceAsStream"" method? I tried placing my properties files in both src/main/resources and src/test/resources but my JUnit test isn't finding them. Here's how I want to load the tests ... final InputStream in = getClass().getResourceAsStream(""my.properties""); but this returns null. I'm using JUnit 4.8. Any ideas? Thanks - Dave It works for me if I use getClass().getResourceAsStream(""/my.properties""). Without the / prefix the path is relative to your class's package so you'd have to put the properties file in a path like src/test/resources/com/mycompany/mypackage/my.properties."
317,A,Is it possible to run incremental/automated JUnit testing in Eclipse? Eclipse support incremental compiling. If I save a source file then it will compile the modified files. Is it possible after such incremental compile also to run the JUnit tests of the same package and show the fail in the error view. Then I can see the JUnit test failing and compiling errors in the same view without extra action. Are there any plugins that can do it? I think that you would quickly find that such a feature was annoying and / or slowed you down. It should only run the test of the saved class. If the unit tests run longer as the compiler then it are not unit tests else functional tests. You should split unit and functional tests. You can run all tests in a project using Alt+Shift+XT. I think that making it any more automated than this could take a serious performance toll. Incremental compilation is compiling at most 1 file at a time but you're talking about running potentially hundreds of tests. I does not want run all tests of the project. I want run only the test related to the saved class. We have approx. 100 thousand JUnit tests. This will run to long.  You have to look at these plugins: JUnit Max: Not free developed by Kent Benk (one of the men behind the TDD practice); MoreUnit: Free but essentially dedicated to help you write the tests; Infinitest: Now free this plugin is dedicated to run the tests related to the files you have just modified. So regarding your needs I suggest that you install MoreUnit and Infinitest plugins. Last time I checked Infinitest was free but only for noncommercial use. Looks like now it's just 100% totally free. Good to know thanks! @MatrixFrog Yes indeed. Some of my ex-colleagues (who now works for algodeal - https://beta.algodeal.com/) are now working on this plugin and make it free hosted on GitHub.  Use ExternalToolBuilder. It can be triggered by source modify. There’s Eclipse customized feature(integrate external tool builder) which may meet your need. But it needs extra effort to write the scripts I never used. Automatic test cases is not a convenient way at least single click to see green bar in Eclipse is enough for me:) We have approx. 100 thousand JUnit tests. I want run only the subset that are related to may current class. This run should be approx one second or fewer. Manual selecting the test and set the needed command line parameter for mocking will consume many time and will forget in many cases. JUnit4 provides rich choice to run subset of all test cases. Use: @RunWith(Suite.class) @Suite.SuiteClasses({ A.class B.class ... })
318,A,"Spring & Struts2 REST - junit tests I have three hierarchical layers injected in Spring - rest business logic and database operations. Junit tests for BL and DAO are working OK when rest can inject only business logic ioc layer. My supper class for junit tests: import org.springframework.test.AbstractTransactionalSpringContextTests; public class AbstractTest extends AbstractTransactionalSpringContextTests { protected static final String path = ""config/spring/applicationContext.xml""; /** * Disabled autowire by type * Disabled dependency check */ public AbstractTest() { super(); this.setAutowireMode(AUTOWIRE_BY_NAME); this.setDependencyCheck(false); } @Override protected String[] getConfigLocations() { return new String[] { path }; } } So - rest calls business logic and this calls database operations. Nullpointer exception falls in business logic for database calls. More info with example: REST: getUser(id) calls BL: getUserBO(id) calls DAO: getUserDAO(id) Nullpointer is thrown on getUserDAO in getUserBO method. This only happens with junit tests it is working deployed. I have edited the question. Which method is throwing the NullPointerException? do you have a data source bean defined? without more details (such as stack trace application context files etc.) that's the best guess i have ... I have edited the question. The stack trace and appContext is (i guess) meaningless? can you add some logging statements to the setter method for the userDao (or which ever object is null) to verify that it is initialized by spring during the tests? before calling the dao assert that it is not null as follows: UserDao dao = getUserDao(); assert( dao != null ) : ""UserDao is null: BUG!!!""; This does not provide an answer to the question. To critique or request clarification from an author leave a comment below their post.  I found that it is a problem of struts2 rest class mapping. So Spring couldn't inject..."
319,A,"assertArrayEquals(Object[] o1 Object[] o2) cannot be found My setup:  Netbeans 6.7 Java6 JUnit 4.5 added as the Test libraries When I try to pass in two class arrays (cast as Object[]) I get the error ""cannot find symbol"" and my test case will not compile. I do not have an issue with the other assert statements and as I said I am using the JUnit 4.5 libraries. Does anyone have a clue on how to fix this or observed this quirky behavior? Netbeans is able to find this function declaration through its autocomplete but it is unable to find where it is located at or can navigate to the sources. Sample code: CustomObject[] coa = { new CustomObject() ....} CustomObject[] expected = { new CustomObject() ... } assertArrayEquals((Object[])coa (Object[])expected); Please post the test code. Well Assert.assertArrayEquals is a static method as you can see from your code which is working:  org.junit.Assert.assertArrayEquals(....) But in the code you were giving you were trying to use it as an instance method:  assertArrayEquals((Object[])coa (Object[])expected); That would only work if you'd statically imported Assert.* or Assert.assertArrayEquals. Now if your other assertions are working my guess is that you're still deriving from TestCase (i.e. the ""old"" way of writing JUnit tests) and that your assertions are calling TestCase.assertEquals etc. If you could give a short but complete example of a unit test where one assertion works but assertArrayEquals doesn't we could probably work out what's going on. All of the assertArrayEquals were failing. This explanation makes the most sense.  I like SingleShot's answer except his two arrays actually contain the same objects. What if the objects aren't the same actual objects (different objects same values but should be equal). So I thought I would enhance his answer to show how to do this. @Test public void myArraysShouldBeIdentical() { CustomObject one1 = new CustomObject(""one""); CustomObject two1 = new CustomObject(""two""); CustomObject three1 = new CustomObject(""three""); CustomObject one2 = new CustomObject(""one""); CustomObject two2 = new CustomObject(""two""); CustomObject three2 = new CustomObject(""three""); CustomObject[] expecteds = { one1 two1 three1 }; CustomObject[] actuals = { one2 two2 three2 }; assertArrayEquals(expecteds actuals); } private static class CustomObject { public String value; CustomObject(String inValue) { value = inValue; } @Override public int hashCode() { return value.hashCode(); } @Override public boolean equals(Object obj) { if (obj == null) return false; if (obj == this) return true; if (!(obj instanceof CustomObject)) return false; CustomObject rhs = (CustomObject) obj; return value == rhs.value; } }  You don't need to fully qualify the assertion or cast your arrays to object arrays. Just import the proper parts of JUnit and pass in your arrays directly. You should reverse the parameter order from your example though - what you expect comes first (""expecteds"") what you actually get from the test comes second (""actuals""). This works fine: import org.junit.*; import static org.junit.Assert.*; public class TestJUnitActuallyWorks { @Test public void myArraysShouldBeIdentical() { CustomObject one = new CustomObject(); CustomObject two = new CustomObject(); CustomObject three = new CustomObject(); CustomObject[] expecteds = { one two three }; CustomObject[] actuals = { one two three }; assertArrayEquals(expecteds actuals); } private static class CustomObject {} } That's what I was doing and the compile just didn't seem to want to point to the implementation in the org.junit.Assert.*; class. Third time is a charm: please post your test code........  The issue was that the compiler was refusing to look into the actual class.. but it would with an abosulte path: org.junit.Assert.assertArrayEquals(.... Rather annoying I may add. That doesn't make any sense. Please post your code. It doesn't make any sense. I do not have any overloaded methods that match assertArrayEquals(Object[]Object[]) within the reach of the method. The only place that the definition exists is within org.junit.Assert."
320,A,"Variables in IntelliJ's Run/Debug Configurations I am wanting to set up a Run/Debug Configuration in IntelliJ for JUnit. I know how to set it up to run a specific package a specific class or a specific method. I was wondering if there was a way to set up the configurations so it will run the currently opened Class file or the currently selected test method. I would imagine I would put something like this in the Class field of the Run/Debug Configurations dialog box: my.package.tests.${currentTest} or something like that. Is there a way to accomplish this so I don't have to make a Run/Debug Configuration for each test class I want to run? if you right click on test method there is an option to run only that method. @Joel : please change your question title since answer you accepted wasn't about ""Variables in IntelliJ's Run/Debug Configurations"". You can either use the right-click > ""run testYourtest()"" or the shortcut define in the keymap as ""Run context configuration"". Usually it's ctrl + shift + F10. so I don't have to add a new configuration in the Run/Debug Configuration? No you don't have to  If you need specific settings for all you tests you can change the defaults e.g. add -ea. This way you don't need a configuration for each test. BTW: YoU can run all the tests in a package by selecting the package first. Sometimes it is better to run a group of tests to check you haven't changed something unintended. +1 for the reference to change in defaults"
321,A,"hsqldb not showing reflection of insert query while runing with JUnit test i am using hsqldb as database. i create LmexPostParamDao which has a method insertLmexPostParam(NameValuePostParamVO nameValuePostParamVO) which will insert data in databse. for testing this method i used JUnit test to insert some data in hsqldb. my JUnit test method is as below:  @Test public void testInsertLmexPostParam(){ String lmexPostParamId = UUID.randomUUID().toString(); NameValuePostParamVO nameValuePostParamVO = new NameValuePostParamVO(); nameValuePostParamVO.setLmexPostParamId(lmexPostParamId); nameValuePostParamVO.setParamName(""adapter_id""); nameValuePostParamVO.setParamValue(""7""); lmexPostParamDao.insertLmexPostParam(nameValuePostParamVO); } my insert method is as below:  @Override public void insertLmexPostParam(NameValuePostParamVO nameValuePostParamVO) { String insertQuery = ""insert into LMEX_POST_PARAM(lmex_post_param_id param_name param_value) values (???)""; String[] paramArr = { nameValuePostParamVO.getLmexPostParamId() nameValuePostParamVO.getParamName() nameValuePostParamVO.getParamValue()}; int update = adapterJdbcTemplate.update(insertQuery paramArr); System.out.println(update); } when i run my test case it's returning me 1 as output which is result of adapterJdbcTemplate. which means data inserted sucessfully. but when i see my database it is not showing me a that row which inserted. and when i debug my testcase method with same values it's give a exception : Data integrity violation exception. and after this exception when i see my database it's showing that row in my database. what will be the problem. i do not. when i see code it's look like everything is fine. help me to resolve this. Thank you Check your CREATE TABLE statement for LMEX_POST_PARAM. The CHAR and VARCHAR types must be defined as CHAR(N) and VARCHAR(N) with N large enough to accept the values that you insert. For example VARCHAR(100). If your problem is the database is not showing the successful insert then you should create your database with WRITE_DELAY = 0 or false. See the HSQLDB documentation for the version you are using. thanks fredt it works. right now i set this property with help of hsqldb databasemanagerswing tool. but can you tell me how can i set the same property in spring-config.xml i have give a size varchar(40) which can hold a value of UUID. i generated my primary key with java UUID.random().toString() function. i don't think it is a issue of size. because as i has told in my question that the same method when i debug with same value it's giving me a exception integrity violation exception. and after the exception when i run my get method for the same record it's return me a value. so any other suggestion"
322,A,"Eclipse Junit '-ea' VM Option In eclipse preferences Windows > Preferences > Java > Junit What effect does the ""Add '-ea' to VM args ...."" checkbox option actually have on the new junit launch config? It enables assertions. +1 note: the Java-in-built assertations are not the junit ones ... I do not recommend to use those in-built assert statements. instead do unit testing"
323,A,"JUnitCore.runClasses doesn't print anything I have a test class that I'm trying to run from a main method with the folowing code : Result r = org.junit.runner.JUnitCore.runClasses(TestReader.class); when I examine the Result object I can see that 5 tests have been run but nothing is printed on the screen. Should I do something else to get an output ? There isn't a very clean way of doing this since it isn't a common thing to do. You only need to print the output if you are creating a program that can be used to run tests on the command line and JUnitCore itself does that. All of the options involve using classes in an internal package. Result r = JUnitCore.runMain(new RealSystem() TestReader.class.getName()) If you want to print to something other than System.out you can create your own subclass of org.junit.internal.JUnitSystem and use that instead of RealSystem You can also use org.junit.internal.TextListener. See runMain(JUnitSystem system String... args) in the JUnitCore source for an example. Can you example why ""it isn't a common thing to do"". Calling runClasses is described in the cookbook. If there is another way to do it what is it? The only time you call `runClasses()` is if you are writing your own code to run tests. Usually you have JUnitCore be your main class run the tests form your IDE or use something that came built in your build system to run the tests. If you need to write a custom class to run your tests the usual way is to use org.junit.runner.notification.RunListener`. Personally I've been annoyed at how many times I need to do something in JUnit4 and find internal classes to be the only solution.  Yes you should register TextListener like this:  JUnitCore junit = new JUnitCore(); junit.addListener(new TextListener(System.out)); junit.run(TestReader.class); JUnit 4.11 doesn't come with TextListener in the archive. What do I do in that case? @djangofan: it's org.junit.internal.TextListener in 4.11."
324,A,How to output exception track traces to the console of a failed test in maven I'm running a single test from Maven: mvn test -Dtest=TestCircle If the test fails Maven outputs the exceptions in /target/... folder. Is there a way to tell Maven that this exception should be printed directly on the console so I don't have to look in /target/surefire-reports/...? You can configure the surefire plugin to generate a file test report or just output the test report to the console using the useFile optional parameter:  <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.6</version> <configuration> <useFile>false</useFile> </configuration> </plugin> Or from the command line with the corresponding expression: $ mvn test -Dsurefire.useFile=false
325,A,"How to run all JUnit test cases from NetBeans? I have created several files with unit tests on existing classes in NetBeans project. I used menu ""File/New file/JUnit/Test for Existing Class"" to create test files. I can run one file with unit tests by right click on it and select ""Run File"". But I want to run all files with unit tests. How can I achieve this in the simplest way? I am using NetBeans 6.5. In NetBeans 7.1 you can create a test suite to which you can add as many of your JUnit test files as you want. When you run the suite you run all the files in the suite. To create the test suite create a file like any other: In the main class add your test classes like this: @RunWith(Suite.class) @Suite.SuiteClasses( { com.somewhere.myProject.MyFirstTestClass.class com.somewhere.myProject.MySecondTestClass.class com.somewhere.myProject.MyThirdTestClass.class }) To run right click this file in the Projects windows and select Test File.  Menu ""Run/Test Project"". What's going on here? Looks like rep gaining if you answer yourself the question after 3 minutes. It is the right answer. Yes this is really looks like rep gaining but it is not the case. I posted the question and found the answer immediately after this. As you can see this is not hard. Just I think that this can be useful for somebody else. As you can see somebody voted up my response. More over it is not prohibited by stackoverflow although initially i was not going to answer my own question :)"
326,A,"How to test the textbox value in the Android JUnit test i have just created login screen project.I wrote the testcase for that.I dont know how to enter the text in textbox in JUnit test case. i just did like this  public void testlogin() { final String n1 = ""adithi""; final String p1 = ""adithi""; String name pass; editUname.clearComposingText(); editPswd.clearComposingText(); TouchUtils.tapView(this editUname); sendKeys(""adithi""); TouchUtils.tapView(this editPswd); sendKeys(""adithi""); activity.runOnUiThread(new Runnable() { public void run() { signinbtn.performClick(); } }); name = editUname.getText().toString(); pass = editPswd.getText().toString(); Log.e(""name""name); Log.e(""Password""pass); assertEquals(n1 name); assertEquals(p1 pass); } the testcase result is  junit.framework.ComparisonFailure: expected:<adithi> but was:<> at com.firstpageTest.Test.testlogin(Test.java:126) at java.lang.reflect.Method.invokeNative(Native Method) at android.test.InstrumentationTestCase.runMethod(InstrumentationTestCase.java:204) at android.test.InstrumentationTestCase.runTest(InstrumentationTestCase.java:194) atandroid.test.ActivityInstrumentationTestCase2.runTest (ActivityInstrumentationTestCase2.ja atandroid.test.ActivityInstrumentationTestCase2.runTest (ActivityInstrumentationTestCase2.java:186) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:169) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:154) at android.test.InstrumentationTestRunner.onStart(InstrumentationTestRunner.java:520) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1447) please guide me. jeez! You've edited your question and changed your code! I can sorta confirm UI text is going null somewhere after you `performClick()` Note. I'm not a Java person but.. Your test is failing because well its failing your code is not running as you expected.It says expected:<Karthika> but was:<> So the string from your UI is probably null. You're putting the string Karthika in the UI then your test expects both strings to be adithi but it fails saying it was null. Somewhere the text in your UI goes null. I think it should be assertEquals(actualvar whatyouexpect); like assertEquals(name n1); i checked itit is assertEquals(String expectedString actual); Its going null or blank somewhere then. When/How is `testlogin` running? And what does `performclick()` do? i think sendkeys("""") is not the method to enter the textbox value.but i cant guess the method to give that.  Its a simple one and I have done a small mistake in giving input to JUnit.we must give as  sendKeys(""A D I T H I""); It works now."
327,A,"testing hibernate parent/child relationship i use spring and hibernate for my data access layer i'll like to have some guidance about how to construct my unit testing to test if hibernate is effectively insert in the child table(parent hibernate mapping has the cascade all on the set). For what i know i shouldn't mix dao's unit testing.So supposing the i'm testing the Parent DAO methods saveWithChild: public void testSaveWithChild() { Child c1 = new Child(""prop1"" ""prop2"" prop3); Child c2 = new Child(""prop4"" ""prop4"" prop3); Parent p = new Parent(""prop6""""prop7""); p.addChild(c1); p.addChild(c2); Session session = MysessionImplementation.getSession(); Transaction tx = session.begingTransaction(); ParentDAO.saveWithChild(p); tx.commit(); Session session1 = MysessionImplementation.getSession(); //now it is right to call child table in here? Child c1fromdb = (Child)session1.get(ChildClass.classc1.getID()); Child c2fromdb = (Child)session1.get(ChildClass.classc2.getID()); //parent asserts goes here //children asserts goes here. } I don't know but i don't feel confortable doing this.Isn't there any better way? How will you check those things? thanks for reading. ;) You could instead do: public void testSaveWithChild() { Child c1 = new Child(""prop1"" ""prop2"" prop3); Child c2 = new Child(""prop4"" ""prop4"" prop3); Parent p = new Parent(""prop6""""prop7""); p.addChild(c1); p.addChild(c2); Session session = MysessionImplementation.getSession(); Transaction tx = session.begingTransaction(); ParentDAO.saveWithChild(p); tx.commit(); Session session1 = MysessionImplementation.getSession(); Parent p2 = session1.get(ParentClass.classp.getID()); // children from db should be in p2.getChildren() } This way at least you are not mixing DAOs. thanks will try it  First of all you should definitely close the session after you've called tx.commit(). If MysessionImplementation.getSession() returns active session (similar to SessionFactory.getCurrentSession()) then your test is not even going to hit the database as session1 would be the same as session and both children would still be bound to it. If MysessionImplementation.getSession() returns a new session every time you're leaking resources. Secondly are children in your example TRUE children (is their lifecycle bound to parent)? If that's the case you should not have a ChildDAO at all (and maybe you don't) and you may or may not have a getChildInstance(id) method (whatever's it's called) in your ParentDAO. It's perfectly fine to call such method (or if you don't have it use session.load()) in the ParentDAOTest because you're testing ParentDao's functionality. Finally keep in mind that just testing that children were inserted is not enough. You also need to test that they were inserted with correct parent (if your parent-child relationship is bidirectional you can do it via child.getParent() method or whatever it's called in your case). You should also test child deletion if that's supported by your dao. very good insight.thanks for the answer. MysessionImplementation return a new session everytime.So how to prevent ressource leak? Close the session. Use try / finally: Session session = MysessionImplementation.getSession(); try { do stuff} finally { if (session!=null) session.close()}; thanks dude.really appreciate it Also consider using Hibernate's built-in session management via sessionFactory.getCurrentSession() - details are here: https://www.hibernate.org/42.html Or better yet take a look at Spring: http://static.springsource.org/spring/docs/2.5.x/reference/orm.html#orm-hibernate Spring's HibernateTemplate and HibernateDaoSupport help with a lot of common DAO stuff"
328,A,"GWT test case fail to run: JUnitFatalLaunchException I am trying to run GWT test from eclipse 3.6 but have stuck with this strange error. The test class 'com.company.demo.smartgwt.RequestBuilderTest' was not found in module 'com.company.demo.smartgwt.module'; no compilation unit for that type was seen Have tried adding source folder into Run Dialog--> classpath and source tabs as mentioned here. No luck yet and running out of options.. Any suggestions folks? Full error stack:  com.google.gwt.junit.JUnitFatalLaunchException: The test class 'com.company.demo.smartgwt.RequestBuilderTest' was not found in module 'com.company.demo.smartgwt.module'; no compilation unit for that type was seen at com.google.gwt.junit.JUnitShell.checkTestClassInCurrentModule(JUnitShell.java:718) at com.google.gwt.junit.JUnitShell.runTestImpl(JUnitShell.java:1317) at com.google.gwt.junit.JUnitShell.runTestImpl(JUnitShell.java:1280) at com.google.gwt.junit.JUnitShell.runTest(JUnitShell.java:625) at com.google.gwt.junit.client.GWTTestCase.runTest(GWTTestCase.java:456) at junit.framework.TestCase.runBare(TestCase.java:127) at junit.framework.TestResult$1.protect(TestResult.java:106) at junit.framework.TestResult.runProtected(TestResult.java:124) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:118) at com.google.gwt.junit.client.GWTTestCase.run(GWTTestCase.java:311) at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Some interesting hints can be found at http://raibledesigns.com/rd/entry/testing_gwt_applications also. The issue is probably that you have run the test as a JUnitTest rather than a GWT Junit Testcase. In Eclipse delete your run configuration for the test right click the class press ""Run As"" and select ""GWT Test Case"". Worked for me. I will agree with other answers that GWT test are too slow - but unfortunatrely  you may need them occassionally. If the ""GWT Test Case"" doesn't happen to show up ensure that the project has ""Use Google Web Toolkit"" checkbox checked in ""Project Properties->Google->Web Toolkit"".  Look at your GWTTestCase.getModuleName() method and make sure it's returning the right module.  The GWT test was not in the same package as the class under test.. moving it into the same package resolved this issue. Strange though The GWT tests are too slow to run you can try your hands on using Junit and Mockito to speed up the tests of interaction between widgets A useful link on GWT unit testing http://blog.hivedevelopment.co.uk/2009/10/introduction-to-mvp-unit-testing-part.html"
329,A,"How to test an anonymous inner class that calls a private method We have a bunch of classes that listen for events from the server and then respond to them. For example: class EventManager { private Set<Event> cache = new HashSet<Event>(); private EventListener eventListener = new EventListener() { void onEvent(Event e) { if (e instanceof MyEvent || e instanceof YourEvent) { handleEvent(e); } } } public EventManager(ServerCommunication serverComm) { serverComm.addListener(eventListener); } private handleEvent(Event e) { // handle the event... // ... cache.add(cache); // ... } } Here's a made-up example of the kind of thing we are doing. Here are the problems I see: I'd like to test handleEvent to make sure it's doing what it is supposed to but I can't because it's private. I'd also like to check that something got added to the cache too but that also seems difficult since cache is a private member and I don't want to add a needless getter method. I'd also like to test the code inside the anonymous class's onEvent method. For now what I did was move all logic from the anonymous class to the handleEvent method and I made handleEvent package private (my unit test is in the same package). I'm not checking the contents of the cache although I want to. Does anyone have any suggestion for a better design that is more testable? Here's my solution. ServerCommunication is mocked using Mockito. I use an ArgumentCaptor to grab the listener then I call it's onEvent method. I initially didn't like this solution because Mockito says that ArgumentCaptors should only be used for verification not for stubbing however I don't agree with their reasoning for this (they say ""it makes the tests less readable."") I don't bother testing the cache directly. I do something else after that depends on something being in the cache. the second method on EventManager should be public EventManager(ServerCom....)? it already is public...I've changed the name though I had named the constructor EventListener(..) by accident. Use this opportunity to break the code up into smaller more focussed (default access) classes. A test is just another client for the code. Note that the anonymous inner class' onEvent method is actually accessible so calling it should not be a problem.  I would probably extract a EventCache component. You can replace this for your test with an implementation that counts the cached events or records whatever is of interest. I probably would not change the visibility of handleEvent. You could implement a ServerCommunication that just raises the event from the test case.  Sometimes when coming across private methods that I want to test... they are simply screaming to be public methods on another object. If you believe that HandleEvent is worth testing in isolation (and not through onEvent processing) one approach would be to expose HandleEvent as a public method on new/different object.  Well there are two approaches here: black box and white box. Black box testing suggests you should only test the publicly visible changes. Does this method have any observable effect? (Some things don't - caches being an obvious example where they improve performance but may otherwise be invisible.) If so test that. If not test that it isn't having a negative effect - this may well just be a case of beefing up other tests. White box testing suggests that maybe you could add a package-level method for the sake of testing e.g. Cache getCacheForTesting() By putting ""for testing"" in the name you're making it obvious to everyone that they shouldn't call this from production code. You could use an annotation to indicate the same thing and perhaps even have some build rules to make sure that nothing from production does call such a method. This ends up being more brittle - more tied to the implementation - but it does make it easier to test the code thoroughly IMO. Personally I err on the side of white box testing for unit tests whereas integration tests should definitely be more black box. Others are rather more dogmatic about only testing the public API.  I assume your EventManager is a singleton or you have access to the particular instance of the class you're testing. 1 - I suppose you can send events to your class. Your method is private and nobody else can call it then sending an event should be enough. 2 - You can access that through reflection if you really need to. Your test would depend on a particular implementation. 3 - What would you like to test actually? If you want to be sure that this method is called you can replace the EventListener with another EventListener object through reflection (and eventually call the onEvent method of the first listener from your new listener). But your question seems to be more about code coverage than actual unit-testing."
330,A,Mockito: How to test my Service with mocking? I'm new to mock testing. I want to test my Service method CorrectionService.CorrectPerson(Long personId). The implementation is not yet written but this it what it will do: CorrectionService will call a method of AddressDAO that will remove some of the Adress that a Person has. One Person has Many Addresses I'm not sure what the basic structure must be of my CorrectionServiceTest.testCorrectPerson. Also please do/not confirm that in this test i do not need to test if the adresses are actually deleted (should be done in a AddressDaoTest) Only that the DAO method was being called. Thank you A simplified version of the CorrectionService class (visibility modifiers removed for simplicity). class CorrectionService { AddressDao addressDao; CorrectionService(AddressDao addressDao) { this.addressDao; } void correctPerson(Long personId) { //Do some stuff with the addressDao here... } } In your test: import static org.mockito.Mockito.*; public class CorrectionServiceTest { @Before public void setUp() { addressDao = mock(AddressDao.class); correctionService = new CorrectionService(addressDao); } @Test public void shouldCallDeleteAddress() { correctionService.correct(VALID_ID); verify(addressDao).deleteAddress(VALID_ID); } }  Cleaner version: @RunWith(MockitoJUnitRunner.class) public class CorrectionServiceTest { private static final Long VALID_ID = 123L; @Mock AddressDao addressDao; @InjectMocks private CorrectionService correctionService; @Test public void shouldCallDeleteAddress() { //given //when correctionService.correct(VALID_ID); //then verify(addressDao).deleteAddress(VALID_ID); } }
331,A,"running a subset of JUnit @Test methods We're using JUnit 4 to test: we have classes that don't are a subclass of TestCase and they have public methods annotated with @Test. We have one file with many @Test methods. It would be nice to be able to run a subset of them via Ant from the command line in the style of this recipe for JUnit 3: ant runtest -Dtest=MyTest -Dtests=testFootestBar http://today.java.net/pub/a/today/2003/09/12/individual-test-cases.html I've been trying to think of ways to achieve this with Java reflection etc. Since there doesn't seem to be any way to ""hide"" @Test methods or remove their annotations at runtime the only option seems to be using the ClassLoader's defineClass method which seems quite difficult. P.S. The Right Thing in this situation would be to split up the file but are there alternatives? Thanks for your time. Create your own TestClassMethodsRunner (it's not documentated or I don't find it now). A TestClassMethodsRunner executes all TestCases and you can set up a filtered TestClassMethodsRunner. All you have to do is override the TestMethodRunner createMethodRunner(Object Method RunNotifier) method. This is a simple an hacky solution: public class FilteredTestRunner extends TestClassMethodsRunner { public FilteredTestRunner(Class<?> aClass) { super(aClass); } @Override protected TestMethodRunner createMethodRunner(Object aTest Method aMethod RunNotifier aNotifier) { if (aTest.getClass().getName().contains(""NOT"")) { return new TestMethodRunner(aTest aMethod aNotifier null) { @Override public void run() { //do nothing with this test. } }; } else { return super.createMethodRunner(aTest aMethod aNotifier); } } } With this TestRunner you execute all Tests that don't contain the string ""NOT"". Others will be ignored :) Just add the @RunWith annotation with your TestRunner class. @RunWith(FilteredTestRunner.class) public class ThisTestsWillBeExecuted { //No test is executed. } @RunWith(FilteredTestRunner.class) public class ThisTestsWillBeExecuted { //All tests are executed. } In the createMethodRunner method you can check the current test against a list of tests that must be executed or introduce new criterias. Good luck with this! Hints for a nicer solution are appreciated!  guerda's solution is good. Here's what I ended up doing (it's a mix of Luke Francl's recipe which I linked before and some other stuff I saw on the net): import org.junit.runner.manipulation.Filter; import org.junit.runner.Description; public final class AntCLFilter extends Filter { private static final String TEST_CASES = ""tests""; private static final String ANT_PROPERTY = ""${tests}""; private static final String DELIMITER = ""\\""; private String[] testCaseNames; public AntCLFilter() { super(); if (hasTestCases()) testCaseNames = getTestCaseNames(); } public String describe() { return ""Filters out all tests not explicitly named in a comma-delimited list in the system property 'tests'.""; } public boolean shouldRun(Description d) { String displayName = d.getDisplayName(); // cut off the method name: String testName = displayName.substring(0 displayName.indexOf('(')); if (testCaseNames == null) return true; for (int i = 0; i < testCaseNames.length; i++) if (testName.equals(testCaseNames[i])) return true; return false; } /** * Check to see if the test cases property is set. Ignores Ant's * default setting for the property (or null to be on the safe side). **/ public static boolean hasTestCases() { return System.getProperty( TEST_CASES ) == null || System.getProperty( TEST_CASES ).equals( ANT_PROPERTY ) ? false : true; } /** * Create a List of String names of test cases specified in the * JVM property in comma-separated format. * * @return a List of String test case names * * @throws NullPointerException if the TEST_CASES property * isn't set **/ private static String[] getTestCaseNames() { if ( System.getProperty( TEST_CASES ) == null ) { throw new NullPointerException( ""Test case property is not set"" ); } String testCases = System.getProperty( TEST_CASES ); String[] cases = testCases.split(DELIMITER); return cases; } } import org.junit.internal.runners.*; import org.junit.runner.manipulation.Filter; import org.junit.runner.manipulation.NoTestsRemainException; public class FilteredRunner extends TestClassRunner { public FilteredRunner(Class<?> clazz) throws InitializationError { super(clazz); Filter f = new AntCLFilter(); try { f.apply(this); } catch (NoTestsRemainException ex) { throw new RuntimeException(ex); } } } Then I annotated my test class with: @RunWith(FilteredRunner.class) public class MyTest { and put the following in my ant buildfile: <target name=""runtest"" description=""Runs the test you specify on the command line with -Dtest="" depends=""compile ensure-test-name""> <junit printsummary=""withOutAndErr"" fork=""yes""> <sysproperty key=""tests"" value=""${tests}"" /> <classpath refid=""classpath"" /> <formatter type=""plain"" usefile=""false"" /> <batchtest> <fileset dir=""${src}""> <include name=""**/${test}.java"" /> </fileset> </batchtest> </junit> </target> the key line there being the sysproperty tag. And now I can run ant runtest -Dtest=MyTest -Dtests=testFootestBar as desired. This works with JUnit 4.1 --- in 4.4 subclass from JUnit4ClassRunner and in 4.5 and later subclass from BlockJUnit4ClassRunner. OK that's much more elegant than my solution :) I've been struggling with this same (or at least very similar) problem for a few days and there's one thing that keeps bothering me. What if you want to use your FilteredRunner with Powermock which also requires it's own @RunWith(PowermockRunner.class) annotation? Great answer but outdated now. I think BlockJUnit4ClassRunner needs to be used instead of TestClassRunner  Since JUnit 4.8 we have @Category annotations to solve just that problem. Here's an intro. @Category is pretty cool but only if you already build suites. As the article mentions at the end that doesn't fit into everyone's testing environment. They're definitely worth pointing out though as for some lucky folks this will completely solve their problem. And since Surefire 2.11 (or so) we can use -Dgroups to select tests by category without building suites. I was happy to delete a lot of our test suites last week thanks to this improvement! (The parameter of Maven Surefire plugin is only documented for TestNG but since 2.11 also works for JUnit.)  There is a simpler way for the common case where you need to run only one test method without having to go through the hassle of creating a custom Runner or Filter: public class MyTestClass { public static void main(final String[] args) throws Exception { final JUnitCore junit = new JUnitCore(); final String singleTest = // Get the name of test from somewhere (environment system property whatever you want). final Request req; if (singleTest != null) { req = Request.method(MyTestClass.class singleTest); } else { req = Request.aClass(MyTestClass.class); } final Result result = junit.run(req); // Check result.getFailures etc. if (!result.wasSuccessful()) { System.exit(1); } } // Your @Test methods here. }"
332,A,"Attempt to stub android Activity class using PowerMockito throws RuntimeException ""Stub!"" I found this example where they used PowerMock and EasyMock to stub/mock the Menu and MenuItem classes for android. I have been trying to do something similar with PowerMock and Mockito with the Activity class. I understand that a lot of the methods are final and that in the Android.jar they all just throw RuntimeException(""Stub!""). I also understand that this test isn't complete but I just wanting to see if it is possible to mock the android Activity class. But given that PowerMock allows you to mock classes with final methods shouldn't this code work? @RunWith(PowerMockRunner.class) @PrepareForTest(Activity.class) public class MyTestCase extends TestCase { public void testPlease_JustWork() throws Exception { Activity mockActivity = PowerMockito.mock(Activity.class); PowerMockito.when(mockActivity.getTitle()).thenReturn(""Title""); } } I would think that the RuntimeException would no longer occur and ""Title"" would be returned but it still throws the exception. I have tried all sorts of different things like doReturn(""Title"").when(mockActivity).getTitle(); and suppress(constructor(Activity.class)); Am I doing something wrong or is this just not possible? I think the answer is that the order of your referenced libraries matters because the android.jar includes some stubs for junit. You have to ensure that on your test project if you go to 'properties' and then 'Java Build Path' that the junit jar associated with the full version of powermock you downloaded show up on the 'Order and Export' tab above the android.jar. If it does not the system resolves the junit.framework and junit.runner packages from android.jar before it resolves them from the junit.jar included in powermock. I realize this question is old but I think this is the proper answer so i wanted to make sure to document it on the question.  I just tried your code sample and it works here strange. I downloaded PowerMock 1.4.5 with Mockito and JUnit including dependencies and used the android.jar from the sdk (2.2). It only fails with the exception if i remove the @PrepareForTest. EDIT You could use the android.jar with the removed exception code provided in the article you referenced. I will give it a try... No dice can you show me what your import lines look like? i've uploaded the whole eclipse project to http://maik.0x2a.at/PowerMockTests.zip . You may need to adapt the path to the android.jar Well... I was using the separate junit library that I had downloaded so once I just got the whole package from power mock that included everything and got rid of the refrences to the other junit library created a new test class I got a green bar. So Thanks! I also had some problems in case anyone else is trying this. I noticed that the order of the Java build path is important. If you go to the order and export tab in Eclipse make sure your src folder is at the top then JavaSE-1.6 then the PowerMock jars then finally android.jar.  Probably not a direct answer to your question but you could try Roboelectric in order to test parts of your Android app on your PC. Roboelectric avoids the Stub! exception and gives you some minimal Android classes implementation"
333,A,"need suggestions on getting started with Junit I have not used Junit before and have not done unit testing automatically. Scenario: We are changing our backend DAO's from Sql Server to Oracle. So on the DB side all the stored procedures were converted to oracle. Now when our code calls these thew Oracle Stored Procedures we want to make sure that the data returned is same as compared to sql server stored procedures. So for example I have the following method in a DAO:  //this is old method. gets data from sql server public IdentifierBean getHeadIdentifiers_old(String head){ HashMap parmMap = new HashMap(); parmMap.put(""head"" head); List result = getSqlMapClientTemplate().queryForList(""Income.getIdentifiers"" parmMap); return (IdentifierBean)result.get(0); } //this is new method. gets data from Oracle public IdentifierBean getHeadIdentifiers(String head){ HashMap parmMap = new HashMap(); parmMap.put(""head"" head); getSqlMapClientTemplate().queryForObject(""Income.getIdentifiers"" parmMap); return (IdentifierBean)((List)parmMap.get(""Result0"")).get(0); } now I want to write a Junit test method that would first call getHeadIdentifiers_old and then getHeadIdentifiers and would compare the Object returned (will have to over-write equals and hash in IdentifierBean). Test would pass only when both objects are same. In the tester method I will have to provide a parameter (head in this case) for the two methods..this will be done manually for now. Yeah from the front end parameters could be different and SPs might not return exact results for those parameters. But I think having these test cases will give us some relief that they return same data... My questions are: Is this a good approach? I will have multiple DAO's. Do I write the test methods inside the DAO itself or for each DAO I should have a seperate JUnit Test Class? (might be n00b question) will all the test cases be ran automatically? I do not want to go to the front end click bunch of stuff so that call to the DAO gets triggered. when tests are ran will I find out which methods failed? and for the ones failed will it tell me the test method that failed? lastly any good starting points? any tutorials articles that show working with Junit Here's a quick yet fairly thorough intro to JUnit.  You'll write a test class. public class OracleMatchesSqlServer extends TestCase { public void testHeadIdentifiersShouldBeEqual() throws Exception { String head = ""whatever your head should be""; IdentifierBean originalBean = YourClass.getHeadIdentifiers_old(head); IdentifierBean oracleBean = YourClass.getHeadIdentifiers(head); assertEquals(originalBean oracleBean); } } You might find you need to parameterize this on head; that's straightforward. Update: It looks like this: public class OracleMatchesSqlServer extends TestCase { public void testHeadIdentifiersShouldBeEqual() throws Exception { compareIdentifiersWithHead(""head1""); compareIdentifiersWithHead(""head2""); compareIdentifiersWithHead(""etc""); } private static void compareIdentifiersWithHead(String head) { IdentifierBean originalBean = YourClass.getHeadIdentifiers_old(head); IdentifierBean oracleBean = YourClass.getHeadIdentifiers(head); assertEquals(originalBean oracleBean); } } * Is this a good approach? Sure. * I will have multiple DAOs. Do I write the test methods inside the DAO itself or for each DAO I should have a separate JUnit Test Class? Try it with a separate test class for each DAO; if that gets too tedious try it the other way and see what you like best. It's probably more helpful to have the fine-grainedness of separate test classes but your mileage may vary. * (might be n00b question) will all the test cases be run automatically? I do not want to go to the front end click bunch of stuff so that call to the DAO gets triggered. Depending on your environment there will be ways to run all the tests automatically. * when tests are ran will I find out which methods failed? and for the ones failed will it tell me the test method that failed? Yes and yes. * lastly any good starting points? any tutorials articles that show working with Junit I really like Dave Astels' book. thanks. what do you mean by this - ""You might find you need to parameterize this on head; that's straightforward."" If you want to test for multiple values of the ""head"" variable then you can pass it into a function... I'll update the answer to demonstrate.  Another useful introduction in writing and maintaining large unit test suites might be this book (which is partially available online): XUnit Test Patterns Refactoring Test Code by Gerard Meszaros The book is organized in 3 major parts. Part I consists of a series of introductory narratives that describe some aspect of test automation using xUnit. Part II describes a number of ""test smells"" that are symptoms of problems with how we are automating our tests. Part III contains descriptions of the patterns.  Okay lets see what can be done... Is this a good approach? Not really. Since instead of having one obsolete code path with somewhat known functionality you now have two code paths with unequal and unpredictable functionality. Usually one would go with creating thorough unit tests for legacy code first and then refactor the original method to avoid incredibly large amounts of refactoring - what if some part of your jungle of codes forming the huge application keeps calling the other method while other parts call the new one? However working with legacy code is never optimal so what you're thinking may be the best solution. I will have multiple DAO's. Do I write the test methods inside the DAO itself or for each DAO I should have a seperate JUnit Test Class? Assuming you've gone properly OO with your program structure where each class does one thing and one thing only yes you should make another class containing the test cases for that individual class. What you're looking for here is mock objects (search for it at SO and Google in general lots of info available) which help you decouple your class under test from other classes. Interestingly high amount of mocks in unit tests usually mean that your class could use some heavy refactoring. (might be n00b question) will all the test cases be ran automatically? I do not want to go to the front end click bunch of stuff so that call to the DAO gets triggered. All IDE:s allow you to run all the JUnit test at the same time for example in Eclipse just click the source folder/top package and choose Run -> Junit test. Also when running individual class all the unit tests contained within are run in proper JUnit flow (setup() -> testX() -> tearDown()). when tests are ran will I find out which methods failed? and for the ones failed will it tell me the test method that failed? Yes part of Test Driven Development is the mantra Red-Green-Refactor which refers to the colored bar shown by IDE:s for unit tests. Basically if any of the tests in test suite fails the bar is red if all pass it's green. Additionally for JUnit there's also blue for individual tests to show assertion errors. lastly any good starting points? any tutorials articles that show working with Junit I'm quite sure there's going to be multiple of these in the answers soon just hang on :) Thanks. after all the tests pass. I was planning to remove the methods with suffix '_old'. and will wait for resources. excited about finally getting to unit test :)"
334,A,"starting and stopping hsqldb from unit tests I'm trying to create integration tests using hsqldb in an in memory mode. At the moment I have to start the hsqldb server from the command line before running the unit tests. I would like to be able to control the hsqldb server from my integration tests. I can't seem to get this to all work out though from code. Update: This appears to work along with having a hibernate.cfg.xml file in the classpath: org.hsqldb.Server.main(new String[]{}); and in my hibernate.cfg.xml file: <property name=""connection.driver_class"">org.hsqldb.jdbcDriver</property> <property name=""connection.url"">jdbc:hsqldb:mem:ww</property> <property name=""connection.username"">sa</property> <property name=""connection.password""></property> <property name=""connection.pool_size"">1</property> <property name=""dialect"">org.hibernate.dialect.HSQLDialect</property> <property name=""current_session_context_class"">thread</property> <property name=""cache.provider_class"">org.hibernate.cache.NoCacheProvider</property> <property name=""hbm2ddl.auto"">update</property> Update It appears that this is only a problem when running the unit tests from within Eclipse using jUnit and the built in test runner. If I run  mvn test they are executed correctly and there is no exception. Am I missing something as far as a dependency goes? I have generated the eclipse project using mvn eclipse:eclipse and my pom is: <?xml version=""1.0"" encoding=""UTF-8""?> http://maven.apache.org/xsd/maven-4.0.0.xsd"" xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""> 4.0.0 <groupId>com.myproject</groupId> <artifactId>myproject</artifactId> <version>1.0-SNAPSHOT</version> <packaging>war</packaging> <name>myproject</name> <dependencies> <dependency> <groupId>jstl</groupId> <artifactId>jstl</artifactId> <version>1.1.2</version> </dependency> <dependency> <groupId>taglibs</groupId> <artifactId>standard</artifactId> <version>1.1.2</version> </dependency> <dependency> <groupId>org.hibernate</groupId> <artifactId>hibernate-core</artifactId> <version>3.3.2.GA</version> </dependency> <dependency> <groupId>com.oracle</groupId> <artifactId>ojdbc14</artifactId> <version>10.1.0.4.0</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>1.6.0</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <version>1.6.0</version> </dependency> <dependency> <groupId>log4j</groupId> <artifactId>log4j</artifactId> <version>1.2.14</version> </dependency> <dependency> <groupId>javassist</groupId> <artifactId>javassist</artifactId> <version>3.8.0.GA</version> </dependency> <dependency> <groupId>org.hibernate</groupId> <artifactId>hibernate-annotations</artifactId> <version>3.4.0.GA</version> </dependency> <!-- Test Dependencies --> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.1</version> <scope>test</scope> </dependency> <dependency> <groupId>org.easymock</groupId> <artifactId>easymock</artifactId> <version>3.0</version> <scope>test</scope> </dependency> <dependency> <groupId>hsqldb</groupId> <artifactId>hsqldb</artifactId> <version>1.8.0.1</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mortbay.jetty</groupId> <artifactId>jetty-servlet-tester</artifactId> <version>6.1.24</version> <scope>test</scope> </dependency> <!-- Provided --> <dependency> <groupId>org.apache.tomcat</groupId> <artifactId>servlet-api</artifactId> <version>6.0.26</version> <scope>provided</scope> </dependency> <dependency> <groupId>org.apache.tomcat</groupId> <artifactId>jsp-api</artifactId> <version>6.0.26</version> </dependency> </dependencies> <build> <finalName>ww_main</finalName> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>1.6</source> <target>1.6</target> </configuration> </plugin> <plugin> <groupId>org.mortbay.jetty</groupId> <artifactId>maven-jetty-plugin</artifactId> </plugin> </plugins> </build> <properties> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> Update Ok well not sure what exactly was going wrong here but I seemed to have fixed it. I deleted all the files that HSQLDB created as well as all of the created files in my Maven target folder did a clean recreated my eclipse .project using maven and refreshed the project in eclipse. I think I may have had something left over from a previous configuration that was throwing it off. Thanks for everyone's help! In your shutdown method just do Statement st = conn.createStatement();` st.execute(""SHUTDOWN"");` conn.close();  Try appending this to the jdbc url: ;ifexists=true;shutdown=true;  check my hsqldb maven plugin : https://github.com/avianey/hsqldb-maven-plugin You can just start/stop it like jetty-maven-plugin or tomee-maven-plugin for your tests : <plugin> <!-- current version --> <groupId>fr.avianey.mojo</groupId> <artifactId>hsqldb-maven-plugin</artifactId> <version>1.0.0</version> <!-- default value for in memory jdbc:hsqldb:hsql://localhost/xdb override only values you want to change --> <configuration> <driver>org.hsqldb.jdbcDriver</driver> <path>mem:test</path> <address>localhost</address> <name>xdb</name> <username>sa</username> <password></password> <validationQuery>SELECT 1 FROM INFORMATION_SCHEMA.SYSTEM_USERS</validationQuery> </configuration> <!-- call start and stop --> <executions> <execution> <id>start-hsqldb</id> <phase>pre-integration-test</phase> <goals> <goal>start</goal> </goals> </execution> <execution> <id>stop-hsqldb</id> <phase>post-integration-test</phase> <goals> <goal>stop</goal> </goals> </execution> </executions> </plugin>  What about starting server through Runtime.getRuntime().exec(""shell command here"")? You have to do it only once for all tests so it won't add too big lag. Update Ok looks like you've solved it yourself :) Update 2 To execute some code once before (or after) unit tests you can static class TestWrapper extends TestSetup { TestWrapper(TestSuite suite) { super(suite); } protected void setUp() throws Exception { // start db } protected void tearDown() throws Exception { // kill db } } Then just wrap your test set in it: new TestWrapper(suite) this hasn't quite solved it I don't think. I'm working on moving it into a test suite where it gets executed only once but there seems to be something wrong with it. I can't quite put my finger on it yet though. Updated. I did this stuff a lot so just copypaste from my project :)  I use the following configuration (directly inspired by the Hibernate tutorial) without any problem: <hibernate-configuration> <session-factory> <!-- Database connection settings --> <property name=""hibernate.connection.driver_class"" value=""org.hsqldb.jdbcDriver""/> <property name=""hibernate.connection.url"" value=""jdbc:hsqldb:mem:foobar""/> <property name=""hibernate.connection.username"" value=""sa""/> <property name=""hibernate.connection.password"" value=""""/> <!-- JDBC connection pool (use the built-in) --> <property name=""connection.pool_size"">1</property> <!-- SQL dialect --> <property name=""hibernate.dialect"" value=""org.hibernate.dialect.HSQLDialect""/> <!-- Enable Hibernate's automatic session context management --> <property name=""current_session_context_class"">thread</property> <!-- Disable the second-level cache --> <property name=""cache.provider_class"">org.hibernate.cache.NoCacheProvider</property> <!-- Echo all executed SQL to stdout --> <property name=""show_sql"">true</property> <!-- Drop and re-create the database schema on startup --> <property name=""hibernate.hbm2ddl.auto"" value=""update""/> <mapping resource=""...""/> </session-factory> </hibernate-configuration> When using an in-memory HSQLDB there is no need to start anything explicitly. Just use the mem: protocol and the in-memory database will get started from JDBC. See also Unit-Testing Hibernate With HSQLDB This is exactly what I'm doing but when I run the unit test I get a java.sql.SQLException: socket creation error. I also get a [main] WARN org.hibernate.cfg.SettingsFactory - Could not obtain connection to query metadata from hibernate. I've noticed that it also is always creating files in my classpath that I would expect to be created if I was using a file based DB and not an in memory one. I'm giving you the answer for confirming that my configuration was correct leading me to look at other reasons this was failing. Thanks! @Casey Glad it's solved and thanks. I was actually about to answer: double check your config you must have a conflict somewhere :)"
335,A,"How can I Unit Test Gestures in android? I want to unit test a swipe left/right action on the screen in Android but I haven't been able to find any documentation on it. Can anyone lend a hand? Can it even be done? ""Can anyone lend an hand?"" You want someone to do the swipes for you? Generally the touch will perform an action and you will be testing the action not the touch itself. Is that the only way? I thought I'd be able to test to see if the swipe events actually worked instead of just testing the methods they are supposed to call. I can simulate button clicks and such. Kinda a bummer I can't do the same. Thanks anyways. Why would you want to check the swipe worked - did you write the swipe routines? Yup I did. It is a necessity for the unit test to include them if possible. I don't understand. Are you saying I should do the action and have the test listen for when it has been performed. Let's say for the sake of argument AndroidOS calls OnSwipeEvent when someone swipes. In the stub of OnSwipeEvent you call MySwipeHandler. You need to test MySwipeHandler."
336,A,"Testing graphics generation with JUnit I'm using Java's Graphics2D to generate a graphical representation of a graph. I'm also using ImageIO to write a PNG file. (ImageIO.write(image ""png"" out);) I'm wondering how should I write JUnit tests to test whether the generated graphics is what is expected. I could pre-generate the PNG files but what if the font is a bit different on a different machine? Well what would you like to be certain of? Good question :-) Now as I think of it maybe it is not that important to test whether the image is exactly as expected but whether some expected features are present. You could read all the RGB values of the generated images into an array and compare that 2D-array against one representing a pre-generated image if you are really specific about the complete image. If you wish to ignore the fonts you could do the same for the same for regions of the image that do not contain any variable data depending on the environment where the images are generated. Building in correction and normalization routines for unit tests would be a waste of time unless the application is expected to generate images of such high accuracy as warranted.  You could try testing for specific known features of the output e.g.: It there a white pixel at (100100)? It the border completely black? Is the image the expected size? And/or you could write tests for some ""aggregate properties"" that allow for some fuzziness in the results: Does the image at least 90% match the reference image? (to allow for different fonts antialiasing differences etc.) Is the most common colour in the image equal to the background colour? Thanks. It looks like a good idea to test some features instead of bitmap pixels. As Vineet noticed there could be also a difference in antialiasing.  For me this concrete implementation seems to work: private void compareRasterImages(BufferedImage expectedPngIo BufferedImage actualPngIo) throws AssertionError { int minX = expectedPngIo.getMinX(); int minY = expectedPngIo.getMinY(); int maxX = expectedPngIo.getMinX() + expectedPngIo.getWidth(); int maxY = expectedPngIo.getMinY()+ expectedPngIo.getHeight(); assertEquals(minX actualPngIo.getMinX()); assertEquals(minY actualPngIo.getMinY()); assertEquals(expectedPngIo.getHeight() actualPngIo.getHeight()); assertEquals(expectedPngIo.getWidth() actualPngIo.getWidth()); for (int x_i = minX; x_i < maxX; x_i++){ for (int y_i = minY; y_i < maxY; y_i++) { assertEquals(expectedPngIo.getRGB(x_i y_i) actualPngIo.getRGB(x_i y_i)); } } } I retrieve the BufferedImage from my PNG (as byte[]) using ImageIO: BufferedImage expectedPngIo = ImageIO.read(new ByteArrayInputStream(expectedPng)); enter code here"
337,A,"how to create ant listener for specific task We have around 80 jars in our applications. All are created using javac task and jar task in ant. I would like to introduce findbug checks. One option was to create single findbug check ant project. This has all jars  all source paths defined in it. This works -- require lot of space. Analysis of result too not very straight forward. There are thousands of warnings to start with. One option I am considering is to run ant with special listener on javac task ant  extract source and class location call findbug task with source and class file information. Any other way introduce findbug to a large project. I have to edit 80 odd build.xml file to introduce findbug. What I did was to create a listener for javac and use values from it to create findbug task and run the tools. Works fine. We run the findbugs taskdef classname=""edu.umd.cs.findbugs.anttask.FindBugsTask"" on all the code in our project. You can set reportLevel to high if you want only the major bugs first. Wont that work? tweaked taskFinished()... Fine for my usage. public class JavacListener implements BuildListener public void taskFinished(BuildEvent be) { if ( be.getTask() instanceof UnknownElement ) { UnknownElement ue= (UnknownElement) be.getTask(); ue.maybeConfigure(); if ( ue.getTask() instanceof Javac ) { Javac task = (Javac)ue.getTask(); final Path sourcepath = task.getSrcdir(); FindBugsTask fbtask = new FindBugsTask(); System.out.println (""Trying FindBugs""); fbtask.setSourcePath(sourcepath); fbtask.setAuxClasspath(task.getClasspath()); Path destPath = new Path( task.getProject() ); destPath.setPath(task.getDestdir().getAbsolutePath()); fbtask.setAuxAnalyzepath(destPath); fbtask.setOutputFile(getFileName(task.getProject())); fbtask.setProject(task.getProject()); fbtask.setHome(new File(""C:\\apps\\findbugs-1.3.0"")); fbtask.execute(); } } else { System.out.println(be.getTask().getClass().getName()); System.out.println(be.getTask().getTaskName()); } } .."
338,A,"Java Reflection: Getting fields and methods in declaration order Is there any way to get a classes declared fields (and methods) in the order of declaration using reflection? According to the documentation the ordering of Methods and Fields returned by getFields() getDeclaredFields() etc. is undefined. Specifying something like an index would be possible using annotation as suggested in Java reflection: Is the order of class fields and methods standardized? Are there any better options i.e. not having to specify the index manually? Now before you ask what I need this for: we have a method that takes a quite big data structure as input and performs a lengthy calculation on it. To create unit tests we made a method that takes an input object and an output instance and creates the Java source code (setting up input invoking the calculation method and asserting the correct results afterwards) as output. This code is much more readable when fields are written in declaration order. I'm afraid it's impossible without modifying the compilation process. Normally the field get written into the classfile in any order and the information about the declaration order gets lost. Most probably you could use an annotation processor to write the order in an auxiliary file. It should be quite easy. Look e.g. at interfacegen for an example how an annotation processor can work. You may want to put the information in the same file but this is much harder.  You won't be able to get the information from the class file. As Adam said in an answer to the refrenced other question: The elements in the array returned are not sorted and are not in any particular order. And ""no order"" includes ""no declaration order"". I once used a Java source file parser to get input data for code generators. And this way you'll have fields and methods in declaration order.  With jdk 6 the reflected fields are in deed in their declaration order. In early jdk that wasn't the case. Apparently enough people have nagged. Although not guaranteed by javadoc I would still take this order as granted and I assume the order will be kept in future jdks too. In your app like in most apps the dependency on the declaration order is mostly vanity - your app won't fail if the order screws up it just become a little uglier. I choose this answer. Even if not guaranteed by the documentation it works for me. @irreputable could you provide a link to this ? or was this observation based on your experience only ? Running on Oracle JVM version 1.7 I found the ordering did not match the declaration order.  You may think about using Javadoc with a custom Doclet but this requires the source to be available. There still is no guarantee about the order in the API (methods fields but every javadoc output I've ever seen has them in the right order so I suppose the doclets get them in declaration order.  No not possible with reflection. You could however solve it using a ProcessBuilder and the javap command: Given a Test.java: public abstract class Test { public void method1() { } public void method2() { } public static void main(String[] args) { } public String method3() { return ""hello""; } abstract void method4(); final int method5() { return 0; } } The command javap Test prints: ... public Test(); public void method1(); public void method2(); public static void main(java.lang.String[]); public java.lang.String method3(); abstract void method4(); final int method5(); ..."
339,A,How do I run code in the UI thread in an Android JUnit test without an Activity? I have a database component that relies on AsyncTask to retrieve data. Once used in the application it will always be called from the UI thread but how do I do that in the JUnit tests without instantiating an Activity (i just don't have an activity nor should I have to create one just for testing purposes)? runOnUiThread() is an activity method so it's not an option. Is there a way to simulate the UI thread eiether by getting one from the Android test library or by implementing a MockUiThread? What's the base class of your test ? `AndroidTestCase` Android.OS.Handler should be able to update the ui. To use a handler you have to subclass it and overide handleMessage() to process messages Update: I am using monodroid so I dont know if this is completely translatable But You may be able to use new Handler(context.getMainLooper()).Post(runnable); Thanks it works in regular Android too. :) In another thread I read that all I really have to do is instantiate one AsyncTask in the UI thread before I create any others and the following ones will automatically know the UI Thread it seems to work great! Thanks. This is good info. I used Send to get the messages across.
340,A,"what is the Best approach for writting automation tests for JAva web applications I have found out that for java automation testing better approach would be to use Cruise Control(java) JUNIT (java testing framework) along with Watij. Any further suggestions please. Any one who has succesfully integrated these tools and what limitations are found for this. regards We generally try to test application on 3 levels: Unit tests with JUnit. For this purpose we follow Dependency Injection patter and use mocked objects (based on JMock framework) when necessary. Integration tests. These are also based on JUnit but do not use mocked objects and test different layers of application writing to database for example. For the purpose of basic data we follow the Object Mother pattern. Acceptance tests for this purpose we use Selenum. For test automation and continuous integration our projects are built with Maven and Hudson. One of the improvements we are looking into at the moment is using Groovy instead of Java for the tests. Some of the issues we had to resolve: creation of basic data in test case setup (based on Object Mother pattern) organization of test suits since all tests can take a lot of time to run dealing with duplication in test code and a lot of fiddling with Maven and Selenuim but is definitely worth it in a long run. Sounds interesting. Will take a look... Tellurium looks like Groovy on top of Selenium. Might be worth a look.  One of my co-workers is a big fan of Watir which I'm guessing is similar to Watij. I like Selenium and have been considering playing with Tellurium lately. In any case tying any of those together with a Continuous Integration engine like CruiseControl or Hudson (my favorite) is a great way to handle functional testing for web applications. And JUnit is great for unit tests and even integration tests. Limitations to functional web application testing: While you can tell a user to click on ""the first item in the menu on the left-hand side"" telling the computer to do the same isn't so easy. Web application testing frameworks can't look at the screen and readily discern what elements on the page constitute a menu. They need more specifics on what element to interact with and that in turn requires some knowledge of how the HTML page is constructed. And as the page changes so can the tests. When every little change in one part of the page breaks the tests in other unrelated parts of page those tests are called ""brittle"". Making sure the tests are more stable takes some experience just as it does in regular unit testing. For example use element ID or name to refer to the elements on a page not their full XPath (could be long and unreadable as well as brittle) or their text content (if it might change). Shameless plug: Helium (http://heliumhq.com) can do things similar to ""the first item in the menu on the left-hand side"" and does not require HTML IDs or XPaths. [As indicated by ""Shameless plug"" I am affiliated with this project.]  We like to use WATIR for functional testing of web applications. It's a browser automation package for Ruby. Easy to learn and we haven't run into any roadblocks after writing many test cases. Watij is also a used for functional testing of java web applications. I want to know the best appraoch for java. any idea Understandable. I like to write my code in java (compiled type safe etc) but write the test cases in ruby (less overhead easier to read the tests domain-specific language etc). We were happy with WATIR so it WATIJ is the same quality it would be worth checking out. But I have no experience with it."
341,A,"Set System Property With Spring Configuration File Configuration: Spring 2.5 Junit 4 Log4j The log4j file location is specified from a system property ${log.location} At runtime system property set with -D java option. All is well. Problem / What I Need: At unit test time system property not set and file location not resolved. App uses Spring would like to simply configure Spring to set the system property. More Info: Requirement is for configuration only. Can't introduce new Java code or entries into IDE. Ideally one of Spring's property configuration implementations could handle this--I just haven't been able to find the right combination. This idea is close but needs to add Java code: Spring SystemPropertyInitializingBean Any help out there? Any ideas are appreciated. Spring Batch has a SystemPropertyInitializer class which can be used to set a system property slightly more concisely e.g. to force JBoss logging to use slf4j (with Spring JPA): <bean id=""setupJBossLoggingProperty"" class=""org.springframework.batch.support.SystemPropertyInitializer"" p:keyName=""org.jboss.logging.provider"" p:defaultValue=""slf4j""/> <bean id=""entityManagerFactory"" class=""org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean"" depends-on=""setupJBossLoggingProperty"" Remember to add the ""depends-on"" attribute to force the system property to be set first.  There was a request in the comments for a Spring 3 example on how to do this. <bean id=""systemPrereqs"" class=""org.springframework.beans.factory.config.MethodInvokingFactoryBean""> <property name=""targetObject"" value=""#{@systemProperties}"" /> <property name=""targetMethod"" value=""putAll"" /> <property name=""arguments""> <!-- The new Properties --> <util:properties> <prop key=""java.security.auth.login.config"">/super/secret/jaas.conf</prop> </util:properties> </property> </bean> Excellent. Thanks. Nice thank you +1 Excellent. When I think of how much code would need to be written to do this. Sometimes Spring is magic. :-) Fantastic. Thanks Patrick(s) :-) How would you the load this? Do i just declare an @Autowired variable of type MethodInvokingFactoryBean @ziggy If I have a bean `exampleBean` that requires those system properties to be set then I do `` to make sure they are set before exampleBean is created. Excellent Thanks. @Patrick Still not set in the log4j system prop example. Works if done with -Dcatalina.base=target anything special to make this init before log4j tooling when running with SpringJUnit4ClassRunner? Thx. @JosephLust To process this file Spring must start and it logs things as it starts which initializes log4j so your settings are never seen by log4j. You can try to re-initialize log4j as Spring processes the file - possibly with a call to `PropertyConfigurator.configure(URL configURL)`  You can achieve that with the combination of two MethodInvokingFactoryBeans Create an inner bean that accesses System.getProperties and an outer bean that invokes putAll on the properties acquired by the inner bean: <bean class=""org.springframework.beans.factory.config.MethodInvokingFactoryBean""> <property name=""targetObject""> <!-- System.getProperties() --> <bean class=""org.springframework.beans.factory.config.MethodInvokingFactoryBean""> <property name=""targetClass"" value=""java.lang.System"" /> <property name=""targetMethod"" value=""getProperties"" /> </bean> </property> <property name=""targetMethod"" value=""putAll"" /> <property name=""arguments""> <!-- The new Properties --> <util:properties> <prop key=""my.key"">myvalue</prop> <prop key=""my.key2"">myvalue2</prop> <prop key=""my.key3"">myvalue3</prop> </util:properties> </property> </bean> (You could of course use just one bean and target System.setProperties() but then you'd be replacing existing properties which is not a good idea. Anyway here's my little test method: public static void main(final String[] args) { new ClassPathXmlApplicationContext(""classpath:beans.xml""); System.out.println(""my.key: ""+System.getProperty(""my.key"")); System.out.println(""my.key2: ""+System.getProperty(""my.key2"")); System.out.println(""my.key3: ""+System.getProperty(""my.key3"")); // to test that we're not overwriting existing properties System.out.println(""java.io.tmpdir: ""+System.getProperty(""java.io.tmpdir"")); } And here's the output: my.key: myvalue my.key2: myvalue2 my.key3: myvalue3 java.io.tmpdir: C:\DOKUME~1\SEANFL~1\LOKALE~1\Temp\ Thank you. This works. Remaining problem: Seems log4j is initialized inside another bean that loads before these new system properties are initialized. Now I get to figure out how to force an order to the bean initialization. Use Spring's depends-on attribute. Many thanks for this answer...it is solid gold... Spring 3 simplifies this by eliminating the need for the 2nd MethodInvokingFactoryBean. You use SpEL and the target object line becomes @Patrick true thanks @Tom - I've posted a Spring 3 example as an additional answer. @Patrick - I'd very much appreciate an example for Spring 3. I've never quite figured out from the example above and your comment how to get this working."
342,A,Parameter passed to java command while running JUnit test case I would like to understand the meaning of the following commandwith regards to JUnit: java junit.swingui.TestRunner MoneyTest The above command opens up the result of the testcase in JUnit Swing GUI. Whats the meaning of passing junit.swingui.TestRunner to the java command as a parameter ? Thank You You're instructing the JRE to run the junit.swingui.TestRunner class and pass it the argument MoneyTest Thanks Kevin :)
343,A,"android JUnit Tests not running The problem is not that the tests are failing but that they simply do not run at all. That is to say the console tells me they run but I see absolutely no results from them at all. Note that I 'have' remembered to annotate methods with @Test Here's the code for the test class: package module.jakway.JournalEntry.test; import module.jakway.JournalEntry.Module_JournalEntry; import org.junit.After; import org.junit.AfterClass; import org.junit.Before; import org.junit.BeforeClass; import org.junit.Test; public class MainTest extends android.test.ActivityInstrumentationTestCase2<Module_JournalEntry> { public MainTest(Class activityClass) { super(""module.jakway.JournalEntry"" Module_JournalEntry.class); // TODO Auto-generated constructor stub } @BeforeClass public static void setUpBeforeClass() throws Exception { } @AfterClass public static void tearDownAfterClass() throws Exception { } @Before public void setUp() throws Exception { super.setUp(); Module_JournalEntry journalentry = getActivity(); assertTrue(true); } @After public void tearDown() throws Exception { } @Test public void myTestCase() { assertTrue(false); } } The project I'm testing is named Module_JournalEntry with package module.jakway.JournalEntry and Activity Module_JournalEntry.java Here's the console output: [2011-02-04 20:37:10 - Module_JournalEntryTest] Performing android.test.InstrumentationTestRunner JUnit launch [2011-02-04 20:37:10 - Module_JournalEntryTest] Automatic Target Mode: using existing emulator 'emulator-5554' running compatible AVD 'my2.3Emulator' [2011-02-04 20:37:12 - Module_JournalEntryTest] Application already deployed. No need to reinstall. [2011-02-04 20:37:12 - Module_JournalEntryTest] Project dependency found installing: Module_JournalEntry [2011-02-04 20:37:14 - Module_JournalEntry] Application already deployed. No need to reinstall. [2011-02-04 20:37:14 - Module_JournalEntryTest] Launching instrumentation android.test.InstrumentationTestRunner on device emulator-5554 [2011-02-04 20:37:14 - Module_JournalEntryTest] Collecting test information [2011-02-04 20:37:17 - Module_JournalEntryTest] Sending test information to Eclipse [2011-02-04 20:37:17 - Module_JournalEntryTest] Running tests... [2011-02-04 20:37:19 - Module_JournalEntryTest] Test run finished and the logcat output: 02-04 20:37:10.266: DEBUG/AndroidRuntime(524): >>>>>> AndroidRuntime START com.android.internal.os.RuntimeInit <<<<<< 02-04 20:37:10.266: DEBUG/AndroidRuntime(524): CheckJNI is ON 02-04 20:37:11.236: DEBUG/AndroidRuntime(524): Calling main entry com.android.commands.pm.Pm 02-04 20:37:11.316: DEBUG/AndroidRuntime(524): Shutting down VM 02-04 20:37:11.336: INFO/AndroidRuntime(524): NOTE: attach of thread 'Binder Thread #3' failed 02-04 20:37:11.346: DEBUG/dalvikvm(524): GC_CONCURRENT freed 102K 71% free 297K/1024K external 0K/0K paused 3ms+8ms 02-04 20:37:11.346: DEBUG/jdwp(524): Got wake-up signal bailing out of select 02-04 20:37:11.346: DEBUG/dalvikvm(524): Debugger has detached; object registry had 1 entries 02-04 20:37:12.316: DEBUG/AndroidRuntime(534): >>>>>> AndroidRuntime START com.android.internal.os.RuntimeInit <<<<<< 02-04 20:37:12.316: DEBUG/AndroidRuntime(534): CheckJNI is ON 02-04 20:37:13.136: DEBUG/AndroidRuntime(534): Calling main entry com.android.commands.pm.Pm 02-04 20:37:13.186: DEBUG/AndroidRuntime(534): Shutting down VM 02-04 20:37:13.216: INFO/AndroidRuntime(534): NOTE: attach of thread 'Binder Thread #3' failed 02-04 20:37:13.216: DEBUG/dalvikvm(534): GC_CONCURRENT freed 102K 71% free 297K/1024K external 0K/0K paused 1ms+1ms 02-04 20:37:13.216: DEBUG/dalvikvm(534): Debugger has detached; object registry had 1 entries 02-04 20:37:14.256: DEBUG/AndroidRuntime(544): >>>>>> AndroidRuntime START com.android.internal.os.RuntimeInit <<<<<< 02-04 20:37:14.256: DEBUG/AndroidRuntime(544): CheckJNI is ON 02-04 20:37:15.126: DEBUG/AndroidRuntime(544): Calling main entry com.android.commands.am.Am 02-04 20:37:15.176: INFO/ActivityManager(75): Force stopping package module.jakway.JournalEntry uid=10035 02-04 20:37:15.206: INFO/ActivityManager(75): Start proc module.jakway.JournalEntry for added application module.jakway.JournalEntry: pid=552 uid=10035 gids={1015} 02-04 20:37:15.876: WARN/TestGrouping(552): Invalid Package: '' could not be found or has no tests 02-04 20:37:15.976: INFO/ActivityManager(75): Force stopping package module.jakway.JournalEntry uid=10035 02-04 20:37:15.976: INFO/Process(75): Sending signal. PID: 552 SIG: 9 02-04 20:37:15.986: DEBUG/AndroidRuntime(544): Shutting down VM 02-04 20:37:16.016: DEBUG/dalvikvm(544): GC_CONCURRENT freed 103K 71% free 299K/1024K external 0K/0K paused 1ms+4ms 02-04 20:37:16.036: INFO/AndroidRuntime(544): NOTE: attach of thread 'Binder Thread #3' failed 02-04 20:37:16.046: DEBUG/jdwp(544): Got wake-up signal bailing out of select 02-04 20:37:16.046: DEBUG/dalvikvm(544): Debugger has detached; object registry had 1 entries 02-04 20:37:16.656: DEBUG/AndroidRuntime(563): >>>>>> AndroidRuntime START com.android.internal.os.RuntimeInit <<<<<< 02-04 20:37:16.665: DEBUG/AndroidRuntime(563): CheckJNI is ON 02-04 20:37:17.646: DEBUG/AndroidRuntime(563): Calling main entry com.android.commands.am.Am 02-04 20:37:17.716: INFO/ActivityManager(75): Force stopping package module.jakway.JournalEntry uid=10035 02-04 20:37:17.746: INFO/ActivityManager(75): Start proc module.jakway.JournalEntry for added application module.jakway.JournalEntry: pid=572 uid=10035 gids={1015} 02-04 20:37:18.606: WARN/TestGrouping(572): Invalid Package: '' could not be found or has no tests 02-04 20:37:18.826: INFO/ActivityManager(75): Force stopping package module.jakway.JournalEntry uid=10035 02-04 20:37:18.826: INFO/Process(75): Sending signal. PID: 572 SIG: 9 02-04 20:37:18.846: DEBUG/AndroidRuntime(563): Shutting down VM 02-04 20:37:18.906: INFO/AndroidRuntime(563): NOTE: attach of thread 'Binder Thread #4' failed 02-04 20:37:18.916: DEBUG/dalvikvm(563): GC_CONCURRENT freed 103K 71% free 298K/1024K external 0K/0K paused 2ms+28ms 02-04 20:37:18.916: DEBUG/jdwp(563): Got wake-up signal bailing out of select 02-04 20:37:18.926: DEBUG/dalvikvm(563): Debugger has detached; object registry had 1 entries Thanks very much! Try changing this: public MainTest(Class activityClass) { super(""module.jakway.JournalEntry"" Module_JournalEntry.class); // TODO Auto-generated constructor stub } To this: public MainTest() { super(""module.jakway.JournalEntry"" Module_JournalEntry.class); // TODO Auto-generated constructor stub } The constructor should have no parameters in this case. It's written in the Hello Testing tutorial example under the Adding the test case constructor section. This did it for me. Thanks!  Android bases its testing framework on JUnit 3 which doesn't use annotations but reflection and test method names should include the test prefix. Move to JUnit 3 and run your tests from Eclipse as Run As -> Android JUnit Test or using am instrument from command line. thanks! helped me alot  ""The Android testing API supports JUnit 3 code style but not JUnit 4"" - http://developer.android.com/tools/testing/testing_android.html 1.Update your test to follow JUnit 3 standards  //has to start with the word ""test"" for JUnit 3 to pick it up. public void testMyCase() { assertTrue(false); } 2.Remove any JUnit 4 references from the test project (right click project -> properties -> java build path -> Libraries (tab)"
344,A,"JUnit tests for POJOs I work on a project where we have to create unit tests for all of our simple beans (POJOs). Is there any point to creating a unit test for POJOs if all they consist of is getters and setters? Is it a safe assumption to assume POJOs will work about 100% of the time? Duplicate of - Should @Entity Pojos be tested? See also Is it bad practice to run tests on a DB instead of on fake repositories? Is there a Java unit-test framework that auto-tests getters and setters? Duplicate of http://stackoverflow.com/questions/337241/should-entity-pojos-be-tested In my experience creating unit tests for POJOs with only getters and setters is just overkill. There are some exceptions of course if there is additional logic in the getter/setter like checking for null and doing something special than I would create a unit test for that. Also if there's a bug in the POJO I'd create a unit test for it so we can prevent it from happening again.  I just started a project to do this. its in pre-alpha stage right now. It is an Eclipse plugin. While I agree that typically a POJO setter/getter doesn't necessarily need a test I believe it is nice to have the simple test there because as things change over time it will make it easier for you to add more tests for the POJO's. The Unit test is set up the methods are there to test the setter/getter all you have to do is handle the new complexity. This also helps with the code-coverage reports which helps keep managers happy. (My company uses Emma). https://sourceforge.net/projects/unittestgplugin/  Unit tests not only validate that code works properly but they document expected behavior. I don't know how many times I've seen things break because some time down the road someone changes the behavior of a getter or setter in a POJO and then it unexpectedly breaks something else (for example someone adds logic to the setter that if someone sets a null value on a string the setter changes it to an empty string so that NPEs don't happen). I don't look at unit tests on data store POJOs as a waste of time I see them as a safety net for future maintenance. That being said if you are manually writing tests to validate these objects you are doing it the hard way. Take a look at something like http://gtcgroup.com/testutil.html  I think if the getters and setters have been created using an IDE then it should be fine. We have other things to put our code into. Obviously you would test the POJO's for serialization/de-serialization. Why? Are we testing the serialization mechanism? Unless you have customized the serialization this would be a waste of effort.  I don't think there's a point to testing simple property getters and setters. The point of unit-testing is not to verify that your compiler works. However as soon as you add a conditional null-check or other non-trivial behavior to your getters and setters (or other methods) I think it's appropriate to add unit tests.  Unit Test code you want to know works (for the situations tested of course). Don't unit test code you only want to be kind of sure works. I can't think of much I only want to be kind of sure about.  I once spent two hours because of something like this: int getX() { return (x); } int getY() { return (x); // oops } Since it takes almost no time to write the tests for simple getters I do it now out of habit.  My answer is that trivial getters and setters do not merit their own tests. If I add any code other than simple reads or writes then I add tests. Of course this is all boilerplate so you could easily write a script that generates unit tests for your getters and setters if you think there's any value there. Certain IDEs may allow you to define a template that creates test cases with test methods filled in for this boilerplate code (I'm thinking of IntelliJ here but Eclipse can probably handle it too although I haven't done anything like this in either IDE).  The rule in TDD is ""Test everything that could possibly break"" Can a getter break? Generally not so I don't bother to test it. Besides the code I do test will certainly call the getter so it will be tested. My personal rule is that I'll write a test for any function that makes a decision or makes more than a trivial calculation. I won't write a test for i+1 but I probably will for if (i<0)... and definitely will for (-b + Math.sqrt(b*b - 4*a*c))/(2*a). BTW the emphasis on POJO has a different reason behind it. We want the vast quantity of our code written into POJOs that don't depend on the environment they run in. For example it's hard to test servlets because they depend upon executing within a container. So we want the servlets to call POJOs that don't depend on their environment and are therefore easy to test. If you're at a company that insists on a code coverage percentage you could probably create a generic getter/setter test with BeanUtils from Apache commons. Just pass it the class get all the getters and setter methods use a little method name comparison to match them up.  I'm experimenting with cobatura for code coverage and just came across the same issue. It would be nice to have an annotation to add to the class which said don't include this class in code coverage. However it is possible to put your pojo's in a separate package and then exclude that package from the analysis. See the documentation depending on your tool it works with Ant Maven or command line.  POJOs may also contain other functions such as equals() hashCode() compareTo() and various other functions. It may be useful to know that those functions are working correctly. These are also things I tend to test with POJOs. Not the accessors but equals and hashCode and how the behave in different contexts e.g. do they still work if the argument to equals is a Hibernate proxy etc.  It's probably worth a simple test to make sure you've not written public void setX(int x) { x = x; } Although you should be coding to avoid that (by putting a final modifier on the method parameter or similar). It also depends how you're generating your accessors and if they could suffer from copy/paste errors etc (this will occur even in environments that try to enforce IDE usage - it just will). My main concern with classes containing lots of setters/getters however is what is the class doing ? Objects should do stuff for you rather than just hold and return data. If these are data entity objects then the setter/getter pattern may be correct. However the better pattern is to set the data in the object and ask the object to do stuff with it (calculate your bank balance launch the rocket etc.) rather than return the data and let you do it yourself!  I use IntelliJ as an IDE and it pretty much writes trivial POJO's for me - certainly the constructor and getters/settors. I don't see any point in unit testing this since any bugs will more than likely be in the test code I write."
345,A,"VerifyError When Running jUnit Test on Android 1.6 Here's what I'm trying to run on Android 1.6: package com.healthlogger.test; public class AllTests extends TestSuite { public static Test suite() { return new TestSuiteBuilder(AllTests.class).includeAllPackagesUnderHere().build(); } } and: package com.healthlogger.test; public class RecordTest extends AndroidTestCase { /** * Ensures that the constructor will not take a null data tag. */ @Test(expected=AssertionFailedError.class) public void testNullDataTagInConstructor() { Record r = new Record(null Calendar.getInstance() ""Data""); fail(""Failed to catch null data tag.""); } } The main project is HealthLogger. These are run from a separate test project (HealthLoggerTest). HealthLogger and jUnit4 are in HealthLoggerTest's build path. jUnit4 is also in HealthLogger's build path. The class ""Record"" is located in com.healthlogger. Commenting out the ""@Test..."" and ""Record r..."" lines allows this test to run. When they are uncommented I get a VerifyError exception. I am severely blocked by this; why is it happening? EDIT: some info from logcat after the crash:  E/AndroidRuntime( 3723): Uncaught handler: thread main exiting due to uncaught exception E/AndroidRuntime( 3723): java.lang.VerifyError: com.healthlogger.test.RecordTest E/AndroidRuntime( 3723): at java.lang.Class.getDeclaredConstructors(Native Method) E/AndroidRuntime( 3723): at java.lang.Class.getConstructors(Class.java:507) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestGrouping$TestCasePredicate.hasValidConstructor(TestGrouping.java:226) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestGrouping$TestCasePredicate.apply(TestGrouping.java:215) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestGrouping$TestCasePredicate.apply(TestGrouping.java:211) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestGrouping.select(TestGrouping.java:170) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestGrouping.selectTestClasses(TestGrouping.java:160) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestGrouping.testCaseClassesInPackage(TestGrouping.java:154) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestGrouping.addPackagesRecursive(TestGrouping.java:115) E/AndroidRuntime( 3723): at android.test.suitebuilder.TestSuiteBuilder.includePackages(TestSuiteBuilder.java:103) E/AndroidRuntime( 3723): at android.test.InstrumentationTestRunner.onCreate(InstrumentationTestRunner.java:321) E/AndroidRuntime( 3723): at android.app.ActivityThread.handleBindApplication(ActivityThread.java:3848) E/AndroidRuntime( 3723): at android.app.ActivityThread.access$2800(ActivityThread.java:116) E/AndroidRuntime( 3723): at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1831) E/AndroidRuntime( 3723): at android.os.Handler.dispatchMessage(Handler.java:99) E/AndroidRuntime( 3723): at android.os.Looper.loop(Looper.java:123) E/AndroidRuntime( 3723): at android.app.ActivityThread.main(ActivityThread.java:4203) E/AndroidRuntime( 3723): at java.lang.reflect.Method.invokeNative(Native Method) E/AndroidRuntime( 3723): at java.lang.reflect.Method.invoke(Method.java:521) E/AndroidRuntime( 3723): at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:791) E/AndroidRuntime( 3723): at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:549) E/AndroidRuntime( 3723): at dalvik.system.NativeStart.main(Native Method) possible duplicate of [Android JUnit Tests failing with java.lang.VerifyError](http://stackoverflow.com/questions/5437595/android-junit-tests-failing-with-java-lang-verifyerror) You might be correct. Unfortunately I can no longer try to verify a solution to this problem - I am without a 1.6 device. If you feel that the two are similar enough please feel free to delete / close this question. Does it occur in the emulator? If so you can try it on the 1.6 emulator. This can occur when you forget to export all dependencies for a test project. In Eclipse go to the Java Build Path properties for your projects (both the main project and the test project). Go to the Organize/Export tab and make sure you check all dependencies (select all except for any JRE or Classpath Container that might be listed)."
346,A,"ParameterizedTest with a name in Eclipse Testrunner When you run a JUnit 4 ParameterizedTest with the eclipse testrunner the graphical representation is rather dumb: for each test you have a node called [0] [1] etc. Is it possible give the tests [0] [1] etc. explicit names? Implementing a toString method for the tests does not seem to help. (This is a follow-up question to JUnit test with dynamic number of tests.) I think there's nothing built in in jUnit 4 to do this. I've implemented a solution. I've built my own Parameterized class based on the existing one: public class MyParameterized extends TestClassRunner { @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public static @interface Parameters { } @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public static @interface Name { } public static Collection<Object[]> eachOne(Object... params) { List<Object[]> results = new ArrayList<Object[]>(); for (Object param : params) results.add(new Object[] { param }); return results; } // TODO: single-class this extension private static class TestClassRunnerForParameters extends TestClassMethodsRunner { private final Object[] fParameters; private final Class<?> fTestClass; private Object instance; private final int fParameterSetNumber; private final Constructor<?> fConstructor; private TestClassRunnerForParameters(Class<?> klass Object[] parameters int i) throws Exception { super(klass); fTestClass = klass; fParameters = parameters; fParameterSetNumber = i; fConstructor = getOnlyConstructor(); instance = fConstructor.newInstance(fParameters); } @Override protected Object createTest() throws Exception { return instance; } @Override protected String getName() { String name = null; try { Method m = getNameMethod(); if (m != null) name = (String) m.invoke(instance); } catch (Exception e) { } return String.format(""[%s]"" (name == null ? fParameterSetNumber : name)); } @Override protected String testName(final Method method) { String name = null; try { Method m = getNameMethod(); if (m != null) name = (String) m.invoke(instance); } catch (Exception e) { } return String.format(""%s[%s]"" method.getName() (name == null ? fParameterSetNumber : name)); } private Constructor<?> getOnlyConstructor() { Constructor<?>[] constructors = getTestClass().getConstructors(); assertEquals(1 constructors.length); return constructors[0]; } private Method getNameMethod() throws Exception { for (Method each : fTestClass.getMethods()) { if (Modifier.isPublic((each.getModifiers()))) { Annotation[] annotations = each.getAnnotations(); for (Annotation annotation : annotations) { if (annotation.annotationType() == Name.class) { if (each.getReturnType().equals(String.class)) return each; else throw new Exception(""Name annotated method doesn't return an object of type String.""); } } } } return null; } } // TODO: I think this now eagerly reads parameters which was never the // point. public static class RunAllParameterMethods extends CompositeRunner { private final Class<?> fKlass; public RunAllParameterMethods(Class<?> klass) throws Exception { super(klass.getName()); fKlass = klass; int i = 0; for (final Object each : getParametersList()) { if (each instanceof Object[]) super.add(new TestClassRunnerForParameters(klass (Object[]) each i++)); else throw new Exception(String.format(""%s.%s() must return a Collection of arrays."" fKlass.getName() getParametersMethod().getName())); } } private Collection<?> getParametersList() throws IllegalAccessException InvocationTargetException Exception { return (Collection<?>) getParametersMethod().invoke(null); } private Method getParametersMethod() throws Exception { for (Method each : fKlass.getMethods()) { if (Modifier.isStatic(each.getModifiers())) { Annotation[] annotations = each.getAnnotations(); for (Annotation annotation : annotations) { if (annotation.annotationType() == Parameters.class) return each; } } } throw new Exception(""No public static parameters method on class "" + getName()); } } public MyParameterized(final Class<?> klass) throws Exception { super(klass new RunAllParameterMethods(klass)); } @Override protected void validate(MethodValidator methodValidator) { methodValidator.validateStaticMethods(); methodValidator.validateInstanceMethods(); } } To be used like: @RunWith(MyParameterized.class) public class ParameterizedTest { private File file; public ParameterizedTest(File file) { this.file = file; } @Test public void test1() throws Exception {} @Test public void test2() throws Exception {} @Name public String getName() { return ""coolFile:"" + file.getName(); } @Parameters public static Collection<Object[]> data() { // load the files as you want Object[] fileArg1 = new Object[] { new File(""path1"") }; Object[] fileArg2 = new Object[] { new File(""path2"") }; Collection<Object[]> data = new ArrayList<Object[]>(); data.add(fileArg1); data.add(fileArg2); return data; } } This implies that I instantiate the test class earlier. I hope this won't cause any errors ... I guess I should test the tests :) FYI: This answer is still correct for JUnit 4.0 but fortunately adding customized names is possible in JUnit 4.11. See http://stackoverflow.com/a/10143872/103814  JUnit4 now allows specifying a name attribute to the Parameterized annotation such that you can specify a naming pattern from the index and toString methods of the arguments. E.g.: @Parameters(name = ""{index}: fib({0})={1}"") public static Iterable<Object[]> data() { return Arrays.asList(new Object[][] { { 0 0 } { 1 1 } { 2 1 } { 3 2 } { 4 3 } { 5 5 } { 6 8 } }); }  A code-less though not that comfortable solution is to pass enough context information to identify the test in assert messages. You will still see just testXY[0] failed but the detailed message tells you which one was that. assertEquals(""Not the expected decission for the senator "" + this.currentSenatorName + "" and the law "" + this.votedLaw expectedVote actualVote);  There's no hint that this feature is or will be implemented. I would request this feature because it's nice to have. Not true anymore."
347,A,Hamcrest equal collections Is there a matcher in Hamcrest to compare collections for equality? There is contains and containsInAnyOrder but I need equals not bound to concrete collection type. E.g. I cannot compare Arrays.asList and Map.values with Hamcrest equals. Thanks in advance! `Arrays.asList().equals(new HashMap<>().values()) == false` `List`s and `Set`s are never equal to each other. See also http://stackoverflow.com/q/2509293/21499 I cannot compare Arrays.asList and Map.values with Hamcrest equals. This is because of hamcrest's over-zealous type signatures. You can do this equality comparison but you need to cast the List object to Collection before it'll compile. I often have to do casting with Hamcrest which feels wrong but it's the only way to get it to compile sometimes.  If you have trouble with the equals method of the collections implementation you might also first copy the collections: assertThat( new ArrayList<Whatever>(actual) equalTo( new ArrayList<Whatever>(expected) );  Casting to a Collection may work but it makes some assumptions about the underlying Collection implementations (e.g. order?). A more general approach would be to write your own matcher. Here's a nearly complete matcher implementation that does what you want (you'll need to fill in the imports and describeTo method). Note that this implementation requires all elements of two collections to be equal but not necessarily in the same order. public class IsCollectionOf<T> extends TypeSafeMatcher<Collection<T>> { private final Collection<T> expected; public IsCollectionOf(Collection<T> expected) { this.expected = expected; } public boolean matchesSafely(Collection<T> given) { List<T> tmp = new ArrayList<T>(expected); for (T t : given) { if (!tmp.remove(t)) { return false; } return tmp.isEmpty(); } // describeTo here public static <T> Matcher<Collection<T>> ofItems(T... items) { return new IsCollectionOf<T>(Arrays.asList(items)); } }
348,A,Self-contained test library project cannot find the library classes According to this SDK guide unit-testing a Library project can be achieved by creating a standard application project reference the Library project and then instrument the application for unit testing. However when I do this and launch the test application I get the message No tests found with test runner 'JUnit 3'. I'm using Eclipse and the Android ADT plugin all latest versions. Note: the projects compile just fine. The test project also installs fine to the emulator. But in the console I can see that it looks for <library>.apk which of course doesn't exist since I'm compiling this as a library into the test project. Anyone got this to work? And if so what is the trickery here? Update: after discovering and fixing a problem which was actually including the test classes (!) the test runner now can find all tests. But all the tests fail with the following exceptions: java.lang.NoClassDefFoundError: <nameOfClassInLibraryProject> nameOfClassInLibraryProject are classes defined in the library project. These classes should be compiled into the test project and indeed everything compiles just fine. But when running the test project the runtime doesn't seem to find the library classes. After much fiddling and wasted time in Eclipse I have managed to get Android Library projects to work. According to the Working with Library Projects article: Instead you must compile the library indirectly by referencing the library from a dependent application's build path then building that application. The problem was that I interpreted this to mean that the library project should be added to the Projects tab in Java Build Path. Doing this makes the test project compile since the library code is obviously available to the compiler. But since the library is not compiled into a .jar or .apk in itself the library classes are never deployed to the device. The solution is to not add the library project to Projects rather on the Source tab add the library /src folder using the Link Source... button. And yes it is the library src folder not the library project root that must be linked into the test project. I had the same problem (NoClassDefFoundError) though not with a self-contained test lib project but in a test project for an application project that referenced an android library project. Your solution worked none the less thank you very much for figuring it out. Thanks for this. Only when I removed the project reference from my project's properties and added the library as a Library project it worked. Its easier to add the library in your project properties in the Android section as a library as explained here: http://stackoverflow.com/questions/8248196/how-to-add-a-library-project-to-a-android-project +1 for _a_ solution to the problem however I prefer the solution provided by @theV0ID as it better reflects how the library project is intended to be used... @dbm I agree. Though I think the whole Android/Library/Project thing is quite messed up and Eclipse doesn't help. I moved to IntelliJ a long time ago which improves quite a bit. But I still think the cleanness of .NET projects and assemblies is miles ahead. One assembly references another assembly boom. done. :)
349,A,"java testing: accelerate time to test timeouts? I have an app that manages turns in a game it's fairly complex and it has a lot of timers that generate timeouts.. since they interoperate a lot it's difficult to be sure everything's working right (and keeps working right). I'd like to test it but certain timeouts are of a few minutes to test it completely it'd take at least one hour!! Is there any way to fake an accelerated time for the timers? Or should I just scale all the timeouts proportionally down test them and scale them up again each time? thanks! what happens when something times out & how do you detect it? the answer to your Q could be quite different if for example you say ""it polls to see if timer has expired"" or ""a timer expires and generates an event that is passed to interested listeners"" timers generate visible consequences no polling is needed by the tests actually no timers are referenced in the tests.. Just a note: For such long running tests you might want to look into a Continuous Integration system such as Jenkins to run your tests in the background after each commit. That way you can run a small set of tests just for the changes you made on your desktop and still fall back on your full set of tests for thorough testing with CI. You'll be notified when tests fail in CI but won't have to run the full suit of tests yourself each and every time. A way to do this would be to make your own interface that provides a thin wrapper around Timer. You then program towards the interface everywhere in your code. After that you make two implementations of the interface. The first is the expected implementation that connects to a real Timer object like you have currently. The other is one that you can use for testing. In this implementation you mock the functionality of a Timer but you have full control of how the events are triggered and how long they take. You could scale the duration as suggested by @aioobe or you could make a backing queue that could rapidly fire events so that there is no wasted time. The point is that you don't have to make changes to the real code and use Dependency Injection to make changes needed for testing. nice approach.. the mock timer should be places in the tests right? So that it doesn't mess with the code.. Dependency injection is the ally of test driven development! @luca: Correct. As an example here is a basic sample using [Google Guice](http://code.google.com/p/google-guice/wiki/Motivation?tm=6#Dependency_Injection_with_Guice). You'll see how in the last code block it knows the interface it is requesting but the returned implementation depends on the model used to create the injector.  There is to my knowledge no way to do a global scaling of the speed of the Java Timers. (Assuming they rely on the System.currentTimeMillis method you could perhaps try to add your own bootclasspath and override this implementation with something different but this is tricky business for sure (if it's even possible).) I suggest you add a TIME_SCALING factor in front of periods and frequencies used for debugging purposes. // ... long period = (long) (someValue / TIME_SCALING); // ... // ... double frequency = someValue * TIME_SCALING; // ...  relying on timer ticks to happen on the right time is prone to difficult to trace and rare bugs I'd suggest using locks barriers semaphores ect. to guarantee happens-before relations on everything; this requires more interaction between classes but is more bullet proof (and allows the program to run faster in testing with only one timer needed if it goes way too fast)  I thought of a simple way.. not nice maybe but still.. I can do as aioobe said (the TIME_SCALING) put keep it private in the class then access it from the tests using reflection.. this way at least it can't be seen or used by other classes..  I was doing something along the same lines but thinking about slowing down the JVM by messing with the internals of Hotspot. It's not that hard but it can introduce subtle bugs if not done right. Anyway I posted a question here asking something along those lines and someone came up with the idea of running the virtual machine in VirtualBox and slowing down the time (in my case to gain extra CPU time). Perhaps you could speed up the time in your case reducing CPU time but making tests finish earlier. Or maybe there's a precision limit to the timer or VirtualBox can only slow down time.. hmm.  Maybe you can just simulate the effects of a timer going off and checking the state of the application based on that. It wouldn't help test the timers but it would be a way to test their effects."
350,A,Test problem using JUnitPerf I'm writing a JUnit test using JUnitPerf. Here I want to generate some entries and use them to update a database. To test the capacity of the database I want several test to run simultaneously or randomly so I use for example: Test loadTest = new LoadTest(testCase n); But still I have to insure that in each test a different update source will be used so that a different entry in the database will be updated. So my question is how can I realize this? Thanks a lot Allen You can add a static AtomicInteger as a seed to your test case and increment this as part of the test Use the seed as the basis for generating your test entry for that test. In the simplest case the seed is used as an index to look up the test data in an array. E.g. class MyTestCase extends TestCase { static AtomicInteger seed = new AtomicInteger(); public void testUpdateEntry() { int seedValue = seed.getAndIncrement(); // generate entries from seed // write entries to db } } Thanks that works
351,A,"printing junit result to file I want to print the results of my junit tests to a txt file. this is my code try { //Creates html header String breaks = ""<html><center><p><h2>""+""Test Started on: ""+df.format(date)+""</h2></p></center>""; //Creating two files for passing and failing a test File pass = new File(""Result_Passed-""+df.format(date)+ "".HTML""); File failed = new File(""Result_Failed-""+df.format(date)+ "".HTML""); OutputStream fstreamF = new FileOutputStream(failed true); OutputStream fstream = new FileOutputStream(pass true); PrintStream p = new PrintStream(fstream); PrintStream f= new PrintStream(fstreamF); //appending the html code to the two files p.append(breaks); f.append(breaks); } catch (FileNotFoundException e) { // TODO Auto-generated catch block e.printStackTrace(); } } here my example testcase public void test_001_AccountWorld1() { // Open the MS CRM form to be tested. driver.get(crmServerUrl + ""account""); nameOfIFRAME = ""IFRAME_CapCRM""; PerformCRM_World1(""address1_name"" ""address1_name"" ""address1_line1"" ""address1_postalcode"" true); assertEquals(firstLineFromForm.toString() """"); assertEquals(secondLineFromForm.toString() ""Donaustadtstrasse Bürohaus 1/2 . St""); assertEquals(postcodeFromForm.toString() ""1220""); } I've tried p.append() but doesnt work :( help please Kind regards Ergun P You're probably re-inventing the wheel here. ANT Maven X build tool or your CI server should be doing this for you. Thanks Martijn Verburg I've managed to find a ton of information on on these tools and will be using them to help me. The thing is im going to have the junit project a runnable jar @Limpep Can you elaborate? double click on the junitTest.jar file and it will open and run the tests @Limpep Why? That's what build/CI tools do for you. Probably should of stated im still new to java and junit testing. @Limpep That's cool :). I've amended my answer so that the tools are linked. I _strongly_ recommend that you use those tools instead of trying to re-invent the wheel. These tools are considered industry std so its good to learn them now. @Limpep You're welcome feel free to pop me an email if you get stuck (assuming the fine folks here at SO can't help)!  I believe this functionality already exists. Read this part of JUnit's FAQ."
352,A,"Starting Selenium with custom Firefox profile from Eclipse I'm running Selenium tests from within Eclipse but I can't load a custom Firefox profile. Most sources suggest I need to launch the Selenium Server like this: java -jar selenium-server.jar -firefoxProfileTemplate </path/to/template/> But when launching my test from within Eclipse it doesn't use that - the tests will run if the Selenium Server isn't running. This thread suggests that I can set the profile in the DefaultSelenium constructor: http://stackoverflow.com/questions/1126282/selenium-rc-disabling-browser-cookie But the code generated for me by Selenium IDE (Firefox plugin) doesn't use that constructor: package com.example.tests; import com.thoughtworks.selenium.*; import java.util.regex.Pattern; public class Example extends SeleneseTestCase { public void setUp() throws Exception { setUp(""http://www.example.com/"" ""*firefox""); } public void testExample() throws Exception { selenium.open(""/""); selenium.click(""//body""); } } Where should I set the DefaultSelenium configuration options? Or is there some other method I can use to load my custom Firefox template? Thanks! Stu I made a SeleniumTestCase that starts/stops the server before/after each test class and starts/stops the Selenium instance before/after each test: public class SeleniumTestCase { protected static Selenium selenium; protected static AppNavUtils appNavUtils; @BeforeClass public static void setUpBeforeClass() throws Exception { SeleniumServerControl.getInstance().startSeleniumServer(); } @AfterClass public static void tearDownAfterClass() throws Exception { SeleniumServerControl.getInstance().stopSeleniumServer(); } @Before public void setUp() throws Exception { // Replace ""*chrome"" with ""*firefox"" for Selenium > 1.0 selenium = new DefaultSelenium(""localhost"" 5444 ""*chrome"" ""http://localhost:8080/""); selenium.start(); appNavUtils = new AppNavUtils(selenium); } @After public void tearDown() throws Exception { selenium.stop(); } } The SeleniumServerControl starts and stops the server: public class SeleniumServerControl { private static final SeleniumServerControl instance = new SeleniumServerControl(); public static SeleniumServerControl getInstance() { return instance; } private SeleniumServer server = null; protected SeleniumServerControl(){} public void startSeleniumServer() { if (server == null) { RemoteControlConfiguration rcc = new RemoteControlConfiguration(); rcc.setPort(5444); //rcc.setFirefoxProfileTemplate(newFirefoxProfileTemplate) server = new SeleniumServer(rcc); } server.start(); } public void stopSeleniumServer() { if (server != null) { server.stop(); server = null; } } }  the version of code you have above assumes that you are running your tests against localhost on port 4444 thats why it is has 2 parameters in the setup. To set up eclipse to run it you will need to update the run configuration. That is under Run > Run Configurations Have a look for the item that has selenium in it and add the config above so that when it runs it will pick it up and run. I personally just fire up the server when I start working by running a batch file and kill it at the end of the day. Great - that got it working. Thanks! You can also instantiate and start an instance of org.openqa.selenium.server.SeleniumServer if you want to do it all within the code. See .setFirefoxProfileTemplate()."
353,A,Unit-testing Spring applications using Scala's Specs We have a large infrastructure that's highly dependent on Spring Framework. Recently I began writing code in Scala and test it using Specs. This is all great but at some point I need to use Spring-dependent features (Such as a HibernateDaoSupport-based DAO). Has anyone managed to use the SpringJUnit4ClassRunner class to run Specs tests? Does anyone have a different direction as to how to achieve this goal? Thanks I've struggled with the class runner in a similarly awkward scenario once and then I created a MethodRule implementation called TemporarySpringContext that could also solve your problem I think. Not *exactly* the solution I was looking for but good enough I suppose
354,A,"Can I run JUnit 4 to test Scala code from the command line? If so how? I haven't come across the proper incantation yet. If not what's the best approach to unit-testing Scala code from the command line? (I'm a troglodyte; I use IDEs when I have to but I prefer to play around using Emacs and command-line tools.) Since compiled Scala is just Java bytecode (OK with a lot more $ characters in class names) it would be exactly as for running JUnit 4 tests against Java code i.e. from the command line by passing the test classes as arguments to org.junit.runner.JUnitCore. As JUnit 4 out of the box only has command line support you don't even have to worry about suppressing GUI based test runners. That said the specific Scala test frameworks (ScalaTest ScalaCheck) do provide a more idiomatic set approach to testing code written in this more functional language.  The suggestion of ScalaTest -- or any of the other Scala-specific frameworks for that matter is very good. I'd like to point to something else though. SBT. SBT is a build tool like Ant Maven or Make. One interesting aspect of it which will matter to us is that it is Scala-based. I don't mean it has special capabilities to handle Scala code or that it is written in Scala though both these things are true. I mean it uses Scala code instead of XML like Maven and Ant as the configuration source. That in itself is interesting. Just today I saw a wonderful example of separating test sources from program sources which I post here just because its so cool. // on this project we keep all sources whether they be Scala or Java and whether they be // regular classes or test classes in a single src tree. override def mainScalaSourcePath = ""src"" override def mainJavaSourcePath = ""src"" override def testScalaSourcePath = ""src"" override def testJavaSourcePath = ""src"" override def mainResourcesPath = ""resources"" // distinguish main sources from test sources def testSourceFilter = ""Test*.scala"" | ""Test*.java"" | ""AbstractTest*.scala"" | ""AbstractTest*.java"" | ""ScalaTestRunner.scala"" def mainSourceFilter = (""*.scala"" | ""*.java"") - testSourceFilter override def mainSources = descendents(mainSourceRoots mainSourceFilter) override def testSources = descendents(testSourceRoots testSourceFilter) But what makes it even more interesting is that SBT works like a console. You run ""sbt"" and you get dropped into a console-like interface from which you can type commands like for instance ""test"" and have your tests run.  You may be interested in ScalaTest. ScalaTest is a free open-source testing tool for Scala and Java programmers. It is written in Scala and enables you to write tests in Scala to test either Scala or Java code. It is released under the Apache 2.0 open source license. Because different developers take different approaches to creating software no single approach to testing is a good fit for everyone. In light of this reality ScalaTest is designed to facilitate different styles of testing. See the Runner documentation for how to run tests from the command line."
355,A,using a PLUG-IN to exisitng testing framework or using FULL FRAMEWORK - which do you recommend? Say you have a huge automation project and are using a certain testing framework say -JUnit for most of the automation. However you typically get stuck with automating the flash part of the the application . Now you can usually select several tools for that issue only. on the one hand tehre are tools that are standalone and include all framework elements (such as If database queries multiple test runs etc) and others that are plugins that solve the specific flash problem but require to use the framework (e.g.JUnit) for creating s full blown test (mainly data queries and loops). Which would you recommend to use? what are the pros and cons from your experience in such situations? Personally I like using the same framework as the one I use for the unit tests (with extra components added into the mix to be able to do tests like the above). I find it easier for the team to switch collaborate and contribute at different levels. Test automation involves developer skills imho keeping it in the same platform makes your life easier.
356,A,"Maven/JUnit - Get older static suite junit technique to work in Maven I am switching a project from Ant to Maven. I have my unit tests in the correct location but for whatever reason the maven surefire plugin only sees about 136 (out of 1136 total) of the tests. When using Ant we used the older test suite technique of: public class GatherTests{ public static Test suite(){ ... add tests ... return testSuite; } } as our runner. I haven't been able to determine how to get this to work using Surefire. I am using junit 3.8.1 with Maven 2.0. Does anyone know if this is possible? One thing I tried is: -Dtest=GatherTests with no luck. Many of our tests exist in classes that do not immediately inherit from junit.framework.TestCase so that may be why surefire is having trouble gathering them. I will go through the logs now to see what it is missing. Thanks Tim I looked into it more and it is skipping tests that extend TestCase directly. It always skips the same ones but they seem no different than others that are being included. I did notice that if a package includes multiple unit test classes surefire is only running one of them. However other packages that only include one class are being skipped. Of course I figured it out shortly after posting this. Maven Surefire Plugin Page On there it says: By default the Surefire Plugin will automatically include all test classes with the following wildcard patterns: ""*/Test.java"" - includes all of its subdirectories and all java filenames that start with ""Test"". ""**/*Test.java"" - includes all of its subdirectories and all java filenames that end with ""Test"". ""**/*TestCase.java"" - includes all of its subdirectories and all java filenames that end with ""TestCase"". The tests that were not being run matched the pattern **/*Tests.java. I added an inclusion rule and they are all running now."
357,A,Making ehcache read-write for test code and read-only for production code I would like to annotate many of my Hibernate entities that contain reference data and/or configuration data with @Cache(usage = CacheConcurrencyStrategy.READ_ONLY) However my JUnit tests are setting up and tearing down some of this reference/configuration data using the Hibernate entities. Is there a recommended way of having entities be read-write during test setup and teardown but read-only for production code? Two of my immediate thoughts for non-ideal workarounds are: Using NONSTRICT_READ_WRITE but I am not sure what the hidden downsides are. Creating subclassed entities in my test code to override the read-only cache annotation. Any recommendations on the cleanest way to handle this? (Note: Project uses maven.) Answering my own question: Using NON_STRICT_READ_WRITE is a reasonable solution that has most of the benefits of READ_ONLY but allows your test code to insert and update entities. Remember to evict any cached items during test setup to ensure you aren't reading stale test data. (e.g. evictQueries()).
358,A,"where to put maven parametrized junit input xml files I have parametrized junit test that reads from several XML input files. In the code I have it like this: @Parameters public static Collection<Object[]> getFiles() { Collection<Object[]> params = new ArrayList<Object[]>(); for (File f : new File(""."").listFiles(new someInputFileFilter())) { Object[] arr = new Object[] { f }; params.add(arr); } return params; } What is the recommended way to include possibly hundreds of these XML input files under Maven? Where do I put it? And what changes I need to make to POM and source code above to read these XML files? Place the files somewhere in src/test/resources say src/test/resources/xmlfiles; this will cause Maven to automatically mirror these into target/test-classes. Then you can consume them as described by James Lorenzen: URL path = this.getClass().getResource(""/xmlfiles""); File xmlfileDir = new File(url.getFile()); for (File f : xmlfileDir.listFiles(new ...Filter())) { .... } No POM changes should be necessary. is there alternative to this.getClass().getResource(""/xmlfiles""); ? I'm using parameterized junit with static method. @bob try `MyTestClass.class.getResource(""/xmlfiles"")`"
359,A,"Cannot capture exception details in @Override in JUnit4 @Rule public MethodRule watchman = new TestWatchman() { @Override public void failed(Throwable e FrameworkMethod method) { try { System.out.println(""Exception ="" + e.getClass().getSimpleName()); } catch (IOException e1) { e1.printStackTrace(); } assertTrue(false); } }; The code that causes the failure is : assertTrue(""Missing first picture""selenium.isElementPresent(""//div/a/img[2]"")); My question is how do I pass the error message ""Missing first picture"" to the @Override so that it prints out that message. I'm a little confused as to why your MethodRule.failed method contains the statement assertTrue(false). For the sake of this discussion I will assume that you added that statement for testing and then did not remove it for this post. The Assert.assertTrue method when fails causes a java.lang.AssertionError to be thrown with it's message value being set to the text you supplied as the first parameter to the assertTrue method. In order to retrieve that value you simply invoke the following: e.getMessage(). Here is an example of a Method rule that prints out the success and failure results of the tests being monitored to System.err (used as example only): @Rule public MethodRule watchman = new TestWatchman() { @Override public void failed(Throwable e FrameworkMethod method) { System.err.println(method.getName() + ""() failed with error: "" + e.getMessage()); } @Override public void succeeded(FrameworkMethod method) { System.err.println(method.getName() + ""() succeeded""); } }; If you use this example with the following tests: @Test public void test1() { assertTrue(""will not happen"" true); } @Test public void test2() { assertTrue(""My error message"" false); } You will see the following: test1() succeeded test2() failed with error: My error message Hi Kris your code works thanks. My error is actually triggered in another class that I call from my JUnit test. @Test public void test5OrderPage() throws InterruptedException IOException { OrderPageCheck affOrderPageCheck = new OrderPageCheck(selenium); affOrderPageCheck.check(); } It is the check() method that has the assertTrue statement. But the code above does not capture the error message. Are you saying that the test is in another class or that the `assertTrue` statement you mentioned in you question is in the `affOrderPageCheck.check` method invocation? Also what result are you currently getting? Is the failed method not executing at all? Is the test being marked as successful or as a failure? The assertTrue is in the affOrderPageCheck.check method and so marks the test as a failure. It goes to the @Override but the getMessage() method just prints a blank instead of the error message. Interesting. A few more questions: 1) is the `assertTrue` method being invoked by the external class' method JUnit's `Assert.assertTrue` method? 2) does the check method perform any exception handling that could be converting the AssertionError to a different Throwable instance possibly losing the error message? 3) what version of JUnit are you using? 1) Yes it is org.junit.Assert.assertTrue 2) public void check () throws IOException { 3) JUnit4 If you are receiving an `AssertionError' in the fail method with a blank message there are only a few causes: a) an assert method was invoked without a message or a null/empty message text b) the original AssertionError was caught and a new error was thrown in its place. You may want to verify that the check method is not performing other assert logic that is different than the one you mention in your question."
360,A,"jUnit - How to assert that inherited methods are invoked? Let's say you have some 3rd-party library class that you want to extend simply to add convenience methods to it (so you can call an inherited method with default parameters for example). Using jUnit/jMock is it possible to write an assertion / mock expection that tests that the correct inherited method is called? For example something like this: class SomeClass extends SomeLibraryClass { public String method(String code) { return method(code null Locale.default()); } } How can I assert that method is being called? Do you mean to test if the right message signature was called or are you trying to test message lookup per se? I'd like to test that the right signature was called. I can assume (hopefully) that the library works. Use the @Override annotation and let the compiler figure out if you actually did override the method as intended? @killdash10 it's not an override since the method has a different signature... You can make a further subclass inside your unit test that actually tells you: public class MyTest { boolean methodCalled = false; @Test public void testMySubclass(){ TestSomeClass testSomeClass = new TestSomeClass(); // Invoke method on testSomeclass ... assertTrue( methodCalled); } class TestSomeClass extends SomeClass{ public String method(String code){ methodCalled = true; } } } Won't this just tell you if your test subclass overwrote the method not if your ""real"" subclass overwrote the method? This test will pass even if your ""real"" subclass didn't overwrite the method.  it's hard to tell without a more concrete example but I'd guess that this ought to be an integration test--test the whole package together--rather than a unit test. Sometimes one can be too fine-grained with unit testing.  It may indeed be better to only write integration tests in this case but if you really want a unit test you can have it just as easily as in any other case: public class SomeClassTest { @Test public void testMethod() { final String code = ""test""; new Expectations() { SomeLibraryClass mock; { mock.method(code null (Locale) any); } }; new SomeClass().method(code); } } This test uses the JMockit mocking API. This is good idea. Method from descendant class was invoked sometimes = method from supe was not invoked.  Unit testing is more useful to verify the functionality of given methods not to assert coverage. Unit tests that care more about what method got called know way more about the classes they are testing than they probably should not to mention will be confusing to the reader. Coverage tools like Cobertura or EMMA will tell you whether you properly covered your code. I understand your point the problem is that this method has no functionality except to call the parent method with the correct signature. If the method has no functionality why is it there? The unit test should assert what a user would expect to happen when the method is called. I see from your above comment and re-reading your initial post that this is wrapping a 3rd party library? My bad for missing that that is somewhat more unique in scope."
361,A,"Checking that a List is not empty in Hamcrest I was wondering if anyone knew of a way to check if a List is empty using assertThat() and Matchers? Best way I could see just use JUnit: assertFalse(list.isEmpty()); But I was hoping that there was some way to do this in Hamcrest. For a better solution vote for: http://code.google.com/p/hamcrest/issues/detail?id=97 @FabricioLemos issue#97 seems to be resolved and merget to master git branch. Lets hope it will be soon in next hamcrest release. @rafalmag Good spot. Will be good to fix all my not-so-readable assertions when v1.3 is released Well there's always assertThat(list.isEmpty() is(false)); ... but I'm guessing that's not quite what you meant :) Alternatively: assertThat((Collection)list is(not(empty()))); empty() is a static in the Matchers class. Note the need to cast the list to Collection thanks to Hamcrest 1.2's wonky generics. I find that Hamcrest code looks much nicer if you change your syntax highlighting to make the parenthesis invisible... Your second assertThat() gives me the following error: The method assertThat(T Matcher) in the type Assert is not applicable for the arguments (List Matcher>) @tkeE2036: That's Hamcrest's broken generics at work. Sometimes you need to cast to make it compile e.g. `assertThat((Collection)list is(not(empty())));` could you please give a link to `Matchers` class where this `empty()` method is defined? `org.hamcrest:hamcrest-all:1.1` doesn't have it. @yegor256: Hamcrest is at v1.2 now. @skaffman : in this solution you still get compile warnings because you do not use generics (and in fact you cant...) This is fixed in 1.3 Is this really easier to read than simply `assertFalse(list.isEmpty)`? I always believed DSL should remove boilerplate code instead of introducing it. @dzieciou it gives you a better error message when the test fails. So instead of `expected true but got false` you get something like `expected empty but got [1 2 3]` If you prefer no unchecked conversion and are willing to give up the static import then you can add the generics to the method like: `assertThat(list Matchers.empty())` (assuming list is a collection of `String`s) Note that with the second `assertThat` the test will pass if the list is `null` so the test will assert the list is not empty when it actually is (well actually it's null. There could be a long metaphysical discussion about if a null list can be considered empty though)  If you're after readable fail messages you can do without hamcrest by using the usual assertEquals with an empty list: assertEquals(new ArrayList<>(0) yourList); E.g. if you run assertEquals(new ArrayList<>(0) Arrays.asList(""foo"" ""bar""); you get java.lang.AssertionError Expected :[] Actual :[foo bar]  This is fixed in Hamcrest 1.3. But if you have to use older version - instead of bugged empty() you could use: not(hasSize(0)) (import static org.hamcrest.collection.IsCollectionWithSize.hasSize; or import static org.hamcrest.Matchers.hasSize;). hasSize(greaterThan(0)) (import static org.hamcrest.number.OrderingComparison.greaterThan; or import static org.hamcrest.Matchers.greaterThan;) Example: // given List<String> list = new ArrayList<String>(); // then assertThat(list not(hasSize(0))); // or assertThat(list hasSize(greaterThan(0))); The most important thing about above solutions is that it does not generate any warnings. The second solution is even more useful if you would like to estimate minimum result size. It's really hard to read. @palacsint I hope that my second example is easier to read."
362,A,"Failing Android JUnit tests not breaking my Ant script like I expect? Failing JUnit tests not breaking my Ant script like I expect? My continuous integration server runs an Ant script which calls something like: /tests/ant run-tests My JUnit tests run but with errors: run-tests: [echo] run-tests-helper. [echo] Running tests ... [exec] [exec] com.zedray.stuff.FooBarTest:.... [exec] com.zedray.stuff.FooBarTest:.....INSTRUMENTATION_RESULT: shortMsg=Some error in your code. [exec] INSTRUMENTATION_RESULT: longMsg=java.security.InvalidParameterException: Some error in your code [exec] INSTRUMENTATION_CODE: 0 The errors are OK but my build script keeps going (eventually publishing my broken app to my testers - bad!). What I would expect is for the instrimentaiton to throw a build error so my continuous integration server (TeamCity in this case) realises that something has gone wrong and reports a broken build. The ""failonerror"" is already set in the relevant macrodef so I'm not sure what else I can do? /tests/build.xml Running tests ... Any ideas/suggestions on how to fix this? Regards Mark Mark did you find what you needed here? Like does that work? [There's a filed bug about this](http://code.google.com/p/android/issues/detail?id=14241). It was reported in January. Unfortunately the Android team doesn't read their bug DB. When I had this problem I came up with this solution to solve it. https://github.com/curiousminds/teamCityAntTestRunner another option would of course be to ditch Ant in favor of Maven or Gradle. Both have Android plug-ins that properly fail the build when there are test failures. Maven: http://code.google.com/p/maven-android-plugin/ Gradle: http://code.google.com/p/gradle-android-plugin/ running instrumentation tests has just been added to the Gradle Android plug-in and is waiting to be merged back into the master repository so there should be another release soon.  Also was looking for some kind of standard solution for this. I wonder how do android guys develop or they dont use teamcity and continuous integration? heard hudson has some plugin for android but I dont like hudson. anyway here is quick and dirty solution replace contents in android-sdk-windows\tools\ant\test_rules.xml with:  <attribute name=""emma.enabled"" default=""false"" /> <element name=""extra-instrument-args"" optional=""yes"" /> <sequential> <echo>Running tests ...</echo> <exec executable=""${adb}"" failonerror=""true"" outputproperty=""tests.output""> <arg line=""${adb.device.arg}"" /> <arg value=""shell"" /> <arg value=""am"" /> <arg value=""instrument"" /> <arg value=""-w"" /> <arg value=""-e"" /> <arg value=""coverage"" /> <arg value=""@{emma.enabled}"" /> <extra-instrument-args /> <arg value=""${manifest.package}/${test.runner}"" /> </exec> <echo message=""${tests.output}""/> <fail message=""Tests failed!!!""> <condition> <contains string=""${tests.output}"" substring=""FAILURES"" /> </condition> </fail> </sequential> there are two drawbacks 1) you dont see test output while tests are running until they failed (and the output is crippled somewhow) 2) its better to override this macro in your project these days (as of SDK r15) the ant target is called ""run-tests-helper"" but the above workaround still works fine. And yes put the modified macro into your build.xml rather than altering the original in the android sdk.  I did it another way because I am using the ant test target that is in the Android build.xml file. This target prints to the standard out so I captured stndout into a file then queried the file using this result to fail my task.  <target name=""run-acceptance-tests"" depends=""clean debug install"" > <property name=""log.file"" value=""acceptance_tests_standard_out.txt"" /> <!-- because we don't have control over the 'test' target (to check for passes an fails) this prints to standard out we capture standard out into a file and query this to see if we have any test failures using this to pass/fail our task --> <record name=""${log.file}"" action=""start"" /> <antcall target=""test"" /> <record name=""${log.file}"" action=""stop"" /> <!-- do other stuff --> <loadfile property=""tests.output"" srcFile=""${log.file}"" /> <echo>Checking for failures</echo> <fail message=""acceptance tests failed!"" > <condition> <contains string=""${tests.output}"" substring=""FAILURES"" /> </condition> </fail> <echo>acceptance tests passed!</echo> </target> Quite useful. :) And to help out others if they aren't super-familiar with ant: You just paste this into your build.xml file and then run it using the command 'ant run-acceptance-tests'. Simple and works thanks! This was not working when i ran from a directory above the build.xml (record filename was relative to the current directory but loadfile was looking relative to the build.xml). Replacing '' fixed this (as did using a full instead of relative path).  The ant JUnit task defaults to running all the tests. There are two solutions to this. Easiest solution is to set the haltonerror property to true and the build will fail at the first test failure. Slightly more involved (and my preference) is to set the failureProperty so that all the tests run. This lets you know how many tests fail instead of only the first test that fails. This requires more ant work because you need to add a line after your junit tests like this: <fail message=""tests failed"" if=""failureProperty""/> how do you make this work? do you have to change android own build target? This is not helpful in this case as Androids ant test target actually uses the exec task to run the junit tests on a emulator/device using the adb tool.  I had the same problem and I ened up customize the ""run-tests"" target in my own build.xml like this and there is no need to change the original android sdk test_rules.xml <target name=""run-tests"" depends=""-install-tested-project install"" description=""Runs tests from the package defined in test.package property""> <echo>Running tests ...</echo> <exec executable=""${adb}"" failonerror=""true"" outputproperty=""tests.output""> <arg value=""shell"" /> <arg value=""am"" /> <arg value=""instrument"" /> <arg value=""-w"" /> <arg value=""-e"" /> <arg value=""coverage"" /> <arg value=""@{emma.enabled}"" /> <arg value=""${manifest.package}/${test.runner}"" /> </exec> <echo message=""${tests.output}""/> <fail message=""Tests failed!!!""> <condition> <contains string=""${tests.output}"" substring=""FAILURES"" /> </condition> </fail> </target> Thats better then my solution I didnt like to amend the sdk xml this was long time ago and yes I think I added that to build.xml. Could you please give more details? You appended that in the build.xml of the test project?"
363,A,"Java/ JUnit - AssertTrue vs AssertFalse I'm pretty new to Java and am following the Eclipse Total Beginner's Tutorials. They are all very helpful but in Lesson 12 he uses assertTrue for one test case and assertFalse for another. Here's the code: // Check the book out to p1 (Thomas) // Check to see that the book was successfully checked out to p1 (Thomas) assertTrue(""Book did not check out correctly"" ml.checkOut(b1 p1)); // If checkOut fails display message assertEquals(""Thomas"" b1.getPerson().getName()); assertFalse(""Book was already checked out"" ml.checkOut(b1p2)); // If checkOut fails display message assertEquals(""Book was already checked out"" m1.checkOut(b1p2)); I have searched for good documentation on these methods but haven't found anything. If my understanding is correct assertTrue as well as assertFalse display the string when the second parameter evaluates to false. If so what is the point of having both of them? Thanks for any help! Thomas Edit: I think I see what was confusing me. The author may have put both of them in just to show their functionality (it IS a tutorial after all). And he set up one which would fail so that the message would print out and tell me WHY it failed. Starting to make more sense...I think that's the explanation but I'm not sure. Talking it out to myself like this helps so don't mind me lol. Wow...I just realized that if I hover over a method in Eclipse it will give me info on it. Thanks everyone for your answers! your understanding is incorrect in cases like these always consult the JavaDoc. assertFalse public static void assertFalse(java.lang.String message boolean condition) Asserts that a condition is false. If it isn't it throws an AssertionError with the given message. Parameters: message - the identifying message for the AssertionError (null okay) condition - condition to be checked  The point is semantics. In assertTrue you are asserting that the expression is true. If it is not then it will display the message and the assertion will fail. In assertFalse you are asserting that an expression evaluates to false. If it is not then the message is displayed and the assertion fails. assertTrue (message value == false) == assertFalse (message value); These are functionally the same but if you are expecting a value to be false then use assertFalse. If you are expecting a value to be true then use assertTrue. Thanks. I think I get it. In the assertFalse should it say ""value==true""?  I think it's just for your convenience (and the readers of your code) Your code and your unit tests should be ideally self documenting which this API helps with Think abt what is more clear to read: AssertTrue(!(a > 3)); or AssertFalse(a > 3); When you open your tests after xx months when your tests suddenly fail it would take you much less time to understand what went wrong in the second case (my opinion). If you disagree you can always stick with AssertTrue for all cases :) Thanks! That makes a lot of sense. This is exactly what I was looking for! Thanks!!  The course contains a logical error:  assertTrue(""Book check in failed"" ml.checkIn(b1)); assertFalse(""Book was aleready checked in"" ml.checkIn(b1)); In the first assert we expect the checkIn to return True (because checkin is succesful). If this would fail we would print a message like ""book check in failed. Now in the second assert we expect the checkIn to fail because the book was checked in already in the first line. So we expect a checkIn to return a False. If for some reason checkin returns a True (which we don't expect) than the message should never be ""Book was already checked in"" because the checkin was succesful.  assertTrue will fail if the checked value is false and assertFalse will do the opposite: fail if the checked value is true. Another thing your last assertEquals will very likely fail as it will compare the ""Book was already checked out"" string with the output of m1.checkOut(b1p2). It needs a third parameter (the second value to check for equality). Thanks! I typed that last assertEquals statement by accident. Should not have been there  assertTrue will fail if the second parameter evaluates to false (in other words it ensures that the value is true). assertFalse does the opposite. assertTrue(""This will succeed."" true); assertTrue(""This will fail!"" false); assertFalse(""This will succeed."" false); assertFalse(""This will fail!"" true); As with many other things the best way to become familiar with these methods is to just experiment :-). Thanks! This was very helpful +1 for the neat and precise explanation !  Your first reaction to these methods is quite interesting to me. I will use it in future arguments that both assertTrue and assertFalse are not the most friendly tools. If you would use assertThat(thisOrThat is(false)); it is much more readable and it prints a better error message too. Thanks for the advice! I'm just following a tutorial though lol."
364,A,"issue with aspectj maven plugin currupting my groovy generated classes from gmaven I am tryin to run Grovvy and Aspectj on a module but when I add aspectj the classes seem to return very diferent values than I expected such aspectj junit.framework.ComparisonFailure: null expected:<2011-04-03> but was:<null> testGetUnixDayFromDate(com.baselogic.chapter05.utils.DateUtilitiesTest) Time elapsed: 0.016 sec <<< FAILURE! junit.framework.AssertionFailedError: expected:<0> but was:<15037> testGetDateDifferenceInDays(com.baselogic.chapter05.utils.DateUtilitiesTest) Time elapsed: 0 sec <<< FAILURE! junit.framework.AssertionFailedError: expected:<0> but was:<7> Which is not what I expected and when I turn the aspectj off this all runs fine.  ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.baselogic.chapter05.utils.DateUtilitiesJavaTest Tests run: 1 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.422 sec Running com.baselogic.chapter05.utils.DateUtilitiesTest 2011-04-03 Tests run: 3 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.062 sec Running com.baselogic.chapter05.utils.StringUtilitiesTest Tests run: 26 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.078 sec Results : Tests run: 30 Failures: 0 Errors: 0 Skipped: 0 ... Here is my aspectj plugin that seems to be causing the issue:  <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>aspectj-maven-plugin</artifactId> <version>1.3.1</version> <executions> <execution> <goals> <goal>compile</goal> <goal>test-compile</goal> </goals> </execution> </executions> <configuration> <complianceLevel>1.6</complianceLevel> </configuration> </plugin> But when I enable my aspectj-maven-plugin everything starts failing:  [INFO] ------------------------------------------------------------------------ [INFO] Building Chapter 05: Extending 1.0.2 [INFO] ------------------------------------------------------------------------ [INFO] [INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ ch05 --- [INFO] Deleting C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target [INFO] [INFO] --- gmaven-plugin:1.3:generateStubs (default) @ ch05 --- [INFO] Generated 2 Java stubs [INFO] [INFO] --- maven-resources-plugin:2.4.3:resources (default-resources) @ ch05 --- [INFO] Using 'UTF-8' encoding to copy filtered resources. [INFO] Copying 0 resource [INFO] [INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch05 --- [INFO] Compiling 2 source files to C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\classes [INFO] [INFO] --- maven-scala-plugin:2.15.2:compile (default) @ ch05 --- [INFO] Checking for multiple versions of scala [INFO] includes = [**/*.scala**/*.java] [INFO] excludes = [] [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\src\main\java:-1: info: compiling [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\generated-sources\groovy-stubs\main:-1: info: compiling [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\src\main\groovy:-1: info: compiling [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\src\main\scala:-1: info: compiling [INFO] Compiling 3 source files to C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\classes at 1301955015855 [INFO] prepare-compile in 0 s [INFO] compile in 4 s [INFO] [INFO] --- gmaven-plugin:1.3:compile (default) @ ch05 --- [INFO] Compiled 3 Groovy classes [INFO] [INFO] --- clojure-maven-plugin:1.3.7:compile (compile-clojure) @ ch05 --- [INFO] [INFO] --- aspectj-maven-plugin:1.3.1:compile (default) @ ch05 --- [INFO] [INFO] --- gmaven-plugin:1.3:generateTestStubs (default) @ ch05 --- [INFO] Generated 2 Java stubs [INFO] [INFO] --- maven-resources-plugin:2.4.3:testResources (default-testResources) @ ch05 --- [INFO] Using 'UTF-8' encoding to copy filtered resources. [INFO] Copying 0 resource [INFO] [INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch05 --- [INFO] Compiling 3 source files to C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\test-classes [INFO] [INFO] --- maven-scala-plugin:2.15.2:testCompile (default) @ ch05 --- [INFO] Checking for multiple versions of scala [INFO] includes = [**/*.scala**/*.java] [INFO] excludes = [] [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\src\test\java:-1: info: compiling [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\generated-sources\groovy-stubs\test:-1: info: compiling [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\src\test\groovy:-1: info: compiling [INFO] C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\src\test\scala:-1: info: compiling [INFO] Compiling 3 source files to C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\test-classes at 1301955027323 [INFO] prepare-compile in 0 s [INFO] compile in 3 s [INFO] [INFO] --- gmaven-plugin:1.3:testCompile (default) @ ch05 --- [INFO] Compiled 2 Groovy classes [INFO] [INFO] --- aspectj-maven-plugin:1.3.1:test-compile (default) @ ch05 --- [WARNING] No sources found skipping aspectJ compile [INFO] [INFO] --- maven-surefire-plugin:2.7.1:test (default-test) @ ch05 --- [INFO] Surefire report directory: C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\surefire-reports ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.baselogic.chapter05.utils.DateUtilitiesJavaTest Apr 4 2011 6:10:31 PM com.baselogic.chapter05.utils.DateUtilities getYesterdayDate_aroundBody11$advice INFO: -------------------- Apr 4 2011 6:10:31 PM com.baselogic.chapter05.utils.DateUtilities getYesterdayDate_aroundBody11$advice ... Tests run: 26 Failures: 21 Errors: 0 Skipped: 0 Time elapsed: 0.093 sec <<< FAILURE! Results : Failed tests: testGetYesterdayDate(com.baselogic.chapter05.utils.DateUtilitiesJavaTest) ... Tests run: 31 Failures: 26 Errors: 0 Skipped: 0 [ERROR] There are test failures. Please refer to C:\usr\SYNCH\PACKT\3166\Chapters_Code\ch05\target\surefire-reports for the individual test results. Here is the groovy test:  package com.baselogic.chapter05.utils import groovy.util.GroovyTestCase import java.util.Calendar import java.text.SimpleDateFormat class DateUtilitiesTest extends GroovyTestCase { protected void setUp() { super.setUp() } protected void tearDown() { super.tearDown() } public void testGetYesterdayDate() { Calendar calendar = Calendar.getInstance() calendar.add(Calendar.DATE -1) SimpleDateFormat format = new SimpleDateFormat(""yyyy-MM-dd"") String expected = format.format(calendar.getTime()); String result = DateUtilities.getYesterdayDate() println(result) assertEquals(expected result) } void testGetUnixDayFromDate() { java.text.SimpleDateFormat formatter = new java.text.SimpleDateFormat(""yyyy-MM-dd HH:mm:ss""); java.util.Date date = (java.util.Date) formatter.parse(""2011-03-03 23:59:59""); assertEquals(DateUtilities.getUnixDayFromDate(date) 15037); } void testGetDateDifferenceInDays() { java.text.SimpleDateFormat formatter = new java.text.SimpleDateFormat(""yyyy-MM-dd HH:mm:ss""); java.util.Date date1 = (java.util.Date) formatter.parse(""2011-03-03 23:59:59""); java.util.Date date2 = (java.util.Date) formatter.parse(""2011-03-10 23:59:59""); assertEquals(DateUtilities.getDateDifferenceInDays(date1 date2) 7); } } Then I created a Java unit test case as well:  package com.baselogic.chapter05.utils; import org.junit.Test; import static org.junit.Assert.assertEquals; import java.text.SimpleDateFormat; import java.util.Calendar; public class DateUtilitiesJavaTest { @Test public void testGetYesterdayDate() { Calendar calendar = Calendar.getInstance(); calendar.add(Calendar.DATE -1); SimpleDateFormat format = new SimpleDateFormat(""yyyy-MM-dd""); String expected = format.format(calendar.getTime()); String result = DateUtilities.getYesterdayDate(); // Aspect DontWriteToTheConsole will complain: // System.out.println(result); assertEquals(expected result); } @Test public void testGetYesterdayDateScala() { Calendar calendar = Calendar.getInstance(); calendar.add(Calendar.DATE -1); SimpleDateFormat format = new SimpleDateFormat(""yyyy-MM-dd""); String expected = format.format(calendar.getTime()); String result = ScalaDateUtilities.getYesterdayDate(); assertEquals(expected result); } } Can anyone help me understand why the aspectj-maven-plugin seems to be causing compilation errors with my final bytecode? Updated with Groovy and Java test cases. not much point in showing the output of the tests if we can't see the test code My experience with the GMaven has been poor. Once I switched to Gradle much of the 'weirdness' went away. This sounds like that kind of weirdness you don't want to waste time with GMaven."
365,A,"How to run all JUnit tests of a given package? I use JUnit 4 in eclipse. I have some test classes in my package and want to run them all. How? Right click on the package and choose ""Run as Test"" from the ""Run as"" submenu.  I used to declare a AllTests class so that I would also be able to run all tests from the command line: public final class AllTests { /** * Returns a <code>TestSuite</code> instance that contains all the declared * <code>TestCase</code> to run. * * @return a <code>TestSuite</code> instance. */ public static Test suite() { final TestSuite suite = new TestSuite(""All Tests""); suite.addTest(Test1.suite()); suite.addTest(Test2.suite()); suite.addTest(Test3.suite()); return suite; } /** * Launches all the tests with a text mode test runner. * * @param args ignored */ public static final void main(String[] args) { junit.textui.TestRunner.run(AllTests.suite()); } } // AllTests Where each test class defines  /** * Returns a <code>TestSuite</code> instance that contains all * the declared <code>TestCase</code> to run. * * @return a <code>TestSuite</code> instance. */ public static final Test suite() { return new TestSuite(Test1.class); // change the class accordingly } I also use this variant because it can run automatically after nightly builds I've seen this suggested in a number of different places on the web but to me it seems like there is a major drawback that each time a class is added or removed this file needs to be updated. Ideally it should all be automated right?  with JUnit 4 I like to use an annotated AllTests class: @RunWith(Suite.class) @Suite.SuiteClasses({ // package1 Class1Test.class Class2test.class ... // package2 Class3Test.class Class4test.class ... }) public class AllTests { // Junit tests } and to be sure that we don't forget to add a TestCase to it I have a coverage Test (also checks if every public method is being tested).  Right-click on the package in the package explorer and select 'Run as' and 'Unit-Test'. Is there a simple way to make this include child packages as well?  In the package explorer you can use the context menu of the package and choose run as junit test."
366,A,How to isolate other methods in the same class using EasyMock I have a method that invokes other method in the same class such as: Class MyClass{ public int methodA(){ ... methodB() ... } public int methodB(){ ... } } And I just wanna test methodA() so how can I isolate methodB() using EasyMock. My damn way is to create a fake instance of MyClass and inject it into the methodA() like this: public int methodA(MyClass fake){ ... fake.methodB(); ... } and expect it in my testcase: MyClass fake = EasyMock.createMock(MyClass.class); EasyMock.expect(fake.methodB()).andReturn(...); Is there any better solution for this situation? I think you should consider refactoring MyClass. If the both methods use the same logic in methodB() then extracting the logic to a private method will enable you to test for the isolated scenario of calling each of these methods.  well...it is so simple :-) YourClass instance = mock(YourClass.class); when(instance.methodB(args)).thenReturn(aFakeAnswer); when(instance.methodA(args)).thenCallRealMethod(); I'm sorry for not exact code but in general it does it work like that and you can unit test abstract classes as well using this.  Here's a surprisingly simple solution using EasyMock. It looks daunting but is actually a very simple concept. The trick is to create in your test class both a regular MyClass object and a mocked MyClass object. Then when you instantiate the regular MyClass override the method that you want to mock out and have it call the same method in the mocked MyClass. Given this class: Class MyClass{ public int methodA(){ ... methodB() ... } public int methodB(){ ... } } In your test write: Class MyClassTest { private MyClass myClass; private MyClass mockMyClass; ... @Test public void testMethodA() { // Create mock of MyClass mockMyClass = createMock(MyClass.class); // Instantiate MyClass overriding desired methods with mocked methods myClass = new MyClass() { @Override public int methodB() { return mockMyClass.methodB(); } }; // Set expectations expect(mockMyClass.methodB()).andReturn(...); replay(mockMyClass); // Call test method - will in turn call overridden mocked methodB() myClass.methodA(); verify(mockMyClass); } } You may want to have helper methods that instantiate myClass differently depending on what method you are testing.  Yes: Don't use EasyMock but an anonymous local class. Example: public void testXyz throws Exception() { MyClass fake = new MyClass() { public int methodB(){ return 12; } }; fake.methodA(); } that's what I've expected!
367,A,"How can I reduce build time using a parallel build / parallelizing the tests? Simple question :) How can I reduce build time using a parallel build / parallelizing the tests? We're using TeamCity JUnit Fit Selenium full build is like 30 minutes I think. We don't run it all before committing just the fast tests. How can you know your question is simple when you yourself can not answer it? Since when does a simple question imply knowing the answer? ""Simple"" he says. The real answer depends on what continuous integration build and test automation tools you are using. TeamCity JUnit Fit Selenium how long do your tests take to run..? Use distcc for C/C++ to distribute the build across several computers. Use ccache to cache compiler work that's been done before. Good luck. +1 for “good luck.” :) A real-life build process rarely consists of running the C/C++ compiler only. But these two tools are good starting point for certain scenarios."
368,A,Setting up Mobile JUnit tests to run under JUnit I'm using Mobile JUnit released by Sony Ericsson for unit testing for my J2ME project. I read in the documentation that one can run the tests under regular junit with the help of a few wrapper classes. The documentation in fact recommends that you do this if you want to generate reports for CI builds etc. which is exactly what I want. Unfortunately the documentation is a little terse on how to do this. Has anyone had any luck with this aspect of Mobile JUnit? Actually I gave up. I now have an ant build script that compiles to J2SE to run the junit tests and compiles to run it as J2ME MIDlet. I use the microemulator to get the J2ME classes to compile on J2SE What I did (and I'm not saying this is necessarily a good idea in general but it worked in my specific case) was to separate the the code base into a library and the UI. The library didn't include an J2ME specific stuff in it and it didn't include anything that J2SE had the J2ME didn't. As a result you could check it out in NetBeans as a J2SE project and it would compile. Then I wrote JUnit tests in the J2SE context. This doesn't help test the UI but it made it very easy to test the library. It sounds like you found a more complete solution that tests both. Can you elaborate on what microemulator you used? I used the one at http://www.microemu.org/. The docs aren't the greatest in the world and you can't emulate everything; I had to change the way I used RecordStore to get the tests to work. But if you put that jar in your classpath when you run the tests it works quite effectively.
369,A,"Junit import using * wildcard I've noticed that when importing JUnit the * wildcard doesn't always work. e.g. for the annotation @Test you must import org.junit.Test since org.junit.* doesn't recognize the annotation. Is there a reason for this is it something that needs setting? or just a quirk in the way somethings like JUnit are. FYI I am using: Junit 4.6 Intelli-J 8.1.3. Attach a screenshot otherwise people here won't believe you :) Based on your comment above: I've copy-pasted it and got ""annontation type expected"". it sounds to me like it could be a name collision. Are you importing a class or interface named Test from somewhere else? Is there a class named Test in the same package as the one where you're having the problem? It could be that Java is seeing one of these instead of the annotation. ""Test"" is a *very* generic name and I suspect a name collision as well.  I had a similar problem today in Eclipse. I made a static import to org.junit.Assert.assertEquals but a static import of org.junit.Assert.assertThat fails! And they are in the same class! I'll bet it's an Eclipse bug. I'm using junit 4.4 and eclipse 3.5 I use Intelli-J (as noted in the tags of this question) so it's not just Eclipse.  Works fine for me in IntelliJ. Something screwed up in your settings? Nothing that I changed any idea what could it be? glitch happens. why not try removing and adding back the junit jar? restarting the program? rebooting the machine? etc etc. Have tried all of the above still problem persist.  I don't do it but using import org.junit.*; works fine here the following test turns on a green light: import static junit.framework.Assert.*; import org.junit.*; public class AppTest { @Test public void testApp() { assertTrue(true); } } Tested with Java 6u16 on the command line under Eclipse 3.5 under IntelliJ IDEA 9.0 BETA CE. Works everywhere as expected. In which IDEA does it work for you? I've copy-pasted it and got ""annontation type expected"".  I'm reading something at http://www.velocityreviews.com/forums/t369296-p2-disadvantage-of-using-wildcards-in-import-statement.html that suggests that there's an ""optimize imports"" setting in IntelliJ that might relate to this.  There's no reason I know of why importing org.junit.* wouldn't give you access to org.junit.Test. In fact I just tried it in Eclipse and it works there. Perhaps it's a problem with your IDEA workspace? Would suggest that there's a setting for it or a larger issue that is my problem. I find it hard to believe that Intelli-J is simply broken in that respect."
370,A,"Hudson CI project doesn't run NetBeans JUnit tests of dependent projects I have a set of NetBeans java projects with dependencies between them. I added the project at the top of the dependency tree to Hudson for continuous integration. Everything works fine except that the unit tests of dependent projects don't get run by Hudson. This is because the ant scripts that NetBeans creates has dependent projects setup to run the ""jar"" target and not a target that also runs the unit tests. I could add ant build steps for each dependent project in Hudson to run the unit tests but I was hoping there's a simpler solution. It seems to me like there is no simple solution for this as far as I can tell. My solution will be to use Maven for managing dependencies. This way at I least won't have to manage dependencies redundantly. Both NetBeans and Hudson integrate well with Maven.  I presuming you have your main job and all dependent jobs set up separately in Hudson calling the Ant scripts. Hudson must know the dependencies between each individual job. This would be the items under ""Build Trigger""->""Build after other projects are built"" and ""Post-build Actions""->""Build other projects"". Each of those jobs would need to run JUnit tests independently and generate the JUnit XML output to a directory. You would turn on ""Post-build Actions""->""Publish JUnit test result report"" in all of your jobs (Main and all dependent ones). Then in your parent main job you would enable ""Post-build Actions""->""Aggregate downstream test results"" to have it coalesce the JUnit output into a result to display for the parent job. This should give your main job a presentation of all of the test results for the main and any dependent projects. As mentioned in my question I only added the project at the top of the dependency tree to Hudson. There are no dependent jobs in Hudson. I explicitly said I didn't want to do this because I didn't wanted to have redundant project dependency information in my Hudson configuration. See my answer for my solution."
371,A,Testing Methods in JUnit Prevents Email Sending I'm developing in Java and using JUnit to test some of my methods. Some of my methods send emails. The emails are actually sent by a separate thread that is spawned. The problem is that whenever I test these methods everything goes fine except that the emails are not actually sent. I've followed the execution through the whole stack and every method related to sending the emails is being called and executed correctly. So I tested the methods outside of JUnit and the emails send fine. This isn't a direct answer to your question but I think actually sending emails is more of an integration test than a unit test. You might consider Kohsuke Kawaguchi's excellent Mock JavaMail provider: http://weblogs.java.net/blog/kohsuke/archive/2007/04/introducing_moc.html It allows the test to verify that the mail was sent correctly instead of simply completing without an exception. Could also use Wiser / SubEthaSmtp for similar purposes: http://code.google.com/p/subethasmtp/wiki/Wiser I agree it is an integration test not a unit test. Is the email thread a background (daemon) thread? Perhaps that thread is terminating due to your tests finishing before the emails are sent? If you put some logging in directly before and after the email is meant to be sent do those log entries appear?  Do you have anti-virus turned on? I've wasted a couple of hours troubleshooting a similar problem with a silent anti-virus in the background. +1 for the nice idea! I had such a problem once because the port for SMTP was blocked by default for everything but the email client  If it is a background thread it is probably being killed silently when the test method finishes. This won't show in the debugger because you are stepping through. You might need to write some convenience method in your JUnit test harness that waits at the end of the test method until the mail is actually send. For a quick test you might put a Thread.sleep() at the end of your test method to see if this helps. If this is the cause the test is working—that is it found a bug in the code that is sending an email. It may be that is code that should be altered to provide the means to check for pending work in background threads.  I appreciate all the helpful responses. I tried most of those things. Turns out it just started working. Maybe it was just something with our stack or email server. Kinda lame.
372,A,"To find the number of test methods in a JUnit TestCase Is there a way to know the number of test methods in a test case? What I want to do is have a test case which tests several scenarios and for all these i would be doing the data setUp() only once. Similarly I would like to do the cleanup (tearDown()) once at the end of all the test methods. The current approach i am using is to maintain a counter for the number of test methods that are present in the file and decrement them in the tearDown method and do the cleanup when the count reaches 0. But this counter needs to be taken care of whenever new test methods are added. Regarding finding the number of test methods in a junit TestCase -- you can do this via reflection. Short example for counting tests with @BeforeClass @AfterClass and @Before. public class CountTest { static int count; @BeforeClass public static void beforeClass() { count = 0; } @Before public void countUp() { count++; } @AfterClass public static void printCount() { System.out.println(count + "" tests.""); } @Test public void test1() { assertTrue(true); } // some more tests Output will be e.g.: 5 tests.  Instead of using setup/teardown you should probably use methods annotated with @BeforeClass and @AfterClass instead. As he did not provide neither his Java nor his JUnit version I would like you to add the information that it won't work with java < 5 or junit < 4.x. yes you're right  If you are using Junit4 and the suggestion given by others is the correct one. But if you using earlier version then use this technique to achieve what you want - You can define a suite for all those tests for which you want to setup and teardown only once. Take a look at junit.extensions.TestSetup class. Instead of executing your test classes you need to then execute these suites.  You can do this through @BeforeClass and @AfterClass in JUnit4: http://junit.org/apidocs/org/junit/BeforeClass.html Volker  A solution for junit 3 is to call a special setup method in every test which checks a static flag. if the flag is not set run the global setup. If it is skip the setup. Make sure the global setup is properly synchronized if you want to run tests in parallel."
373,A,"Spring/Hibernate/Junit example of testing DAO against HSQLDB I'm working on trying to implement a JUnit test to check the functionality of a DAO. (The DAO will create/read a basic object/table relationship). The trouble I'm having is the persistence of the DAO (for the non-test code) is being completed through an in-house solution using Spring/Hibernate which eliminates the usual *.hbm.xml templates that most examples I have found contain. Because of this I'm having some trouble understanding how to setup a JUnit test to implement the DAO to create/read (just very basic functionality) to an in-memory HSQLDB. I have found a few examples but the usage of the in-house persistence means I can't extend some of the classes the examples show (I can't seem to get the application-context.xml setup properly). Can anyone suggest any projects/examples I could take a look at (or any documentation) to further my understanding of the best way to implement this test functionality? I feel like this should be really simple but I keep running into problems implementing the examples I have found. edit: Here's my solution for better readability for anyone who needs a hand getting things going: My Class: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = ""classpath:applicationContextTest-Example.xml"") @Transactional public class ExampleDaoTest extends AbstractTransactionalJUnit4SpringContextTests { @Resource(name = ""sessionFactory"") private SessionFactory exampleSessionFactory; @Resource(name = ""exampleDao"") private ExampleDao exampleDao; My applicationContext file: <!-- List of Daos to be tested --> <bean id=""exampleDao"" class=""org.myExample.ExampleDao""/> <!-- Datasource --> <bean id=""example_dataSource"" class=""org.apache.commons.dbcp.BasicDataSource""> <property name=""driverClassName"" value=""org.hsqldb.jdbcDriver""> </property> <property name=""url"" value=""jdbc:hsqldb:mem:ExampleTest""> </property> <property name=""username"" value=""sa""> </property> <property name=""password"" value=""""> </property> </bean> <!-- Session Factory --> <bean id=""sessionFactory"" class=""org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean""> <property name=""dataSource"" ref=""example_dataSource""/> <property name=""annotatedClasses""> <list> <value>org.myExample.ExampleClass</value> </list> </property> <property name=""hibernateProperties""> .... left to user to choose properties </property> </bean> Some additional info: I'm not too experienced with the configuration of Spring/Hibernate I've had them already configured in previous projects until now. My current project is a mishmash of configurations I can't figure out just how the ""in-house"" class I referred to obtains its session factory. All the DAOs extend it (an abstract class) and within is an abstract declaration: public abstract SessionFactory getSessionFactory(); I can't figure out where the ""getSessionFactory()"" method comes from. I think it must be injected by Spring somehow but I can't find any config files doing so. Thanks guys you've all been helpful. For those in the future: I included into my unit test (via @ContextConfiguration as Willie specifies) my applicationContext-Test.xml file in which I defined an HSQLDB datasource session factory transaction manager and the Daos as beans . My Test class is annotated with: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = ""your appContext.xml"") @Transactional and my session factory & dao are annotated as resources (from .xml defs): @Resource(name = ""sessionFactory"") Spring 3 offers a new jdbc namespace that includes support for embedded databases including HSQLDB. So that takes care of that part. I'm wondering what the ""in-house solution"" could be. You can use annotations (either JPA or Hibernate annotations) to ORM your domain objects so why do you need an ""in-house solution""? E.g.: <bean id=""sessionFactory"" class=""org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean"" p:dataSource-ref=""dataSource"" p:packagesToScan=""myapp.model"" /> As far as implementing a test goes use Spring's TestContext Framework. A test can look like this (again I'm assuming Spring 3 below though it should work in Spring 2.5 simply by changing @Inject to @Autowired): @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration({ ""/beans-datasource-it.xml"" ""/beans-dao.xml"" ""/beans-service.xml"" ""/beans-web.xml"" }) @Transactional public class ContactControllerIT { @Inject private ContactController controller; ... setUp() and tearDown() ... @Test public void testGetContact() { String viewName = controller.getContact(request 1L model); ... assertions ... } } You'd put the embedded database inside beans-datasource-it.xml for example. ('it' here stands for integration test and the files are on the classpath.) The controller in this example lives in beans-web.xml and will be autowired into the ContactController field. That's just an outline of what to do but hopefully it's enough to get you started.  The bottom line with hibernate is the SessionFactory - your in-house solution will most likely be creating one of these somehow. Find out how and then add a bean to create one in your test app context in the same way (or if possible using your in-house code that is used at runtime.). You may need to create your own FactoryBean to do the instantiation. (Use AbstractFactoryBean as your base class.) Once this is in place most of the examples using LocalSessionFactoryBean can be migrated to your situation - instead of using LocalsessionFactoryBean use your custom factory bean. (If you've not done so already look at the Testing section in the spring reference - it makes testing with Spring and injecting tests with beans from the context a breeze.)  My application context looks a bit different <beans:bean class=""org.apache.commons.dbcp.BasicDataSource"" id=""HSQL_DS""> <beans:property name=""driverClassName"" value=""org.hsqldb.jdbcDriver""/> <beans:property name=""url"" value=""jdbc:hsqldb:mem:Test""/> <beans:property name=""username"" value=""sa""/> <beans:property name=""password"" value=""""/> </beans:bean> <jdbc:embedded-database id=""HSQL_DS""> <jdbc:script location=""classpath:schema.sql""/> <jdbc:script location=""classpath:data.sql""/> </jdbc:embedded-database> and my test class looks like this: public class Tester { private EmbeddedDatabase db; @Before public void setUp(){ db = new EmbeddedDatabaseBuilder().addDefaultScripts().build(); } @Test public void TestMe(){ System.out.println(""Testing""); } @After public void tearDown(){ db.shutdown(); } }  See here. It assumes maven2 as build tool but you can easily use anything. @downvoter - was it because I linked to my blog?  I have recently implemented a similar solution in some of my code using Hibernate Spring and HSQLDB. Its is worth noting that AbstractTransactionalJUnit4SpringContextTests has now be deprecated - but it is still pretty straight forward to test - I cover most the details here: http://automateddeveloper.blogspot.com/2011/05/hibernate-spring-testing-dao-layer-with.html"
374,A,"Why is my Mockito-based unit test unable to run in Eclipse? I have a Maven-managed project that uses Mockito mocking in its unit tests. I can run all the tests within a Maven build and they run without error (and pass!). However if I right-click a single test function and choose ""Run As -> JUnit Test"" I get an exception java.lang.NoSuchMethodError: org.mockito.Mockito.doAnswer(Lorg/mockito/stubbing/Answer;)Lorg/mockito/stubbing/Stubber;. Of course the ""missing"" method is there if I look at the sources and like I said the tests compile and run from the command line. Best I can think of is if Eclipse is trying to ""help"" me by providing an outdated Mockito artifact (I'm using 1.8.5 in my Maven dependencies) for the JUnit plugin akin to how the Maven plugin can stick you with an oddball version of the Maven runtime for certain tasks. Is this the problem? Is it something else? Can I fix this? ETA: Apparently this may relate to a known issue. There's a good chance that it stems from having multiple versions of Mockito in my classpath (thanks Maven :-/ ). I seem to have my house in order -- Eclipse can run the tests now -- but unfortunately the bug has bitten my Hudson. I have to track down how to remove the old JAR from the classpath there as well. Make sure the unit-test classpath has the correct mockito. You can check this from the run dialog. Btw Eclipse does not ship with mockito so perhaps you are having two versions of it. Take a look at your maven dependency graph and search for duplicates. Nice call on the multiple versions; I've been burned by this often enough I have no idea why it's not one of the first things I check. I'm rebuilding now; will accept if this fixes it :D This worked for me. Thanks!"
375,A,Maven - Selenium - Possible to run only one test We are using JUnit - Selenium for our web tests. We use Maven to start them and build a surefire report. The test suite is pretty large and takes a while to run and sometimes single tests fail because the browser won't start. I want to be able run a SINGLE test using maven so I retest the tests that fail and update the report. I can use mvn test -Dtest=TESTCLASSNAME to run all the tests in one test class but this is not good enough since it takes about 10 minutes to run all the tests in our most complicated test classes and it's very likely that some other test will fail (because the browser wont start) and this will mess up my report. I know I can run one test from Eclipse but that is not what I am looking for. Any help on this would be very appriciated You can have a parent class with common setup and a child class for each testcase that way you can use mvn test -Dtest=TESTCLASSNAME to run a single test. If you are using junit4 you can annotate the methods you wish to ignore for the moment with @Ignore. If you do not have onSetup() and onTeardown() that needs to run for each testcase then you can make your test methods private and have only one testcase that call the others. This way it is easy to comment them out as needed.  c_maker's answer describes the main points - you really should consider breaking up the large test cases into multiple ones. I recommend TestNG or JUnit4 for Selenium tests so that you can easily manage the set up before the whole suite any test dependencies and so on. In TestNG you can also use a group to classify tests to run selectively so that you don't need to set them to ignore when you want to run a particular class of tests.
376,A,"How to add test cases to a suite using jUnit? I have 2 test classes both extend TestCase. Each class contains a bunch of individual tests which run against my program. How can I execute both classes (and all tests they have) as part of the same suite? I am using jUnit 4.8. you are extending TestCase in JUnit 4? Why are you not using annotations instead? Create TestClass and override suite() method and run newly created TestClass.  public static Test suite() { TestSuite suite = new TestSuite(""Test ExpenseTest""); suite.add(TestCase1.class); suite.add(TestCase2.class); return suite; } Is this how jUnit 4 does it? Maybe this JavaDoc will help http://kentbeck.github.com/junit/javadoc/4.8/org/junit/runners/Suite.html. I have never used it JavaDoc have enough information My test set is invoked as part of @Suite.SuiteClasses({MyTests.class}). Where should I add your code to produce a subsuit? Who consumes the suite produced?  In jUnit4 you have something like this: @RunWith(Suite.class) @SuiteClasses({ SomeTest.class SomeOtherTest.class ... }) public class AllTests {} If you want the Eclipse GUI suite builder (New > JUnit Test suite) you have to add public static junit.framework.Test suite() { return new JUnit4TestAdapter(SomeTest.class); } to each of your test classes s.t. the GUI test suite builder recognizes your test."
377,A,"Testing for Exceptions using JUnit. Test fails even if the Exception is caught I am new to testing with JUnit and I need a hint on testing Exceptions. I have a simple method that throws an exception if it gets an empty input string: public SumarniVzorec( String sumarniVzorec) throws IOException { if (sumarniVzorec == """") { IOException emptyString = new IOException(""The input string is empty""); throw emptyString; } I want to test that the exception is actually thrown if the argument is an empty string. For that I use following code:  @Test(expected=IOException.class) public void testEmptyString() { try { SumarniVzorec test = new SumarniVzorec( """"); } catch (IOException e) { // Error e.printStackTrace(); } The result is that the exception is thrown but the test fails. What am I missing? Thank you Tomas maybe sumarniVzorec.equals("""") instead of sumarniVzorec == """" Thank you I repaired that but it didn't solve the problem mentioned above.  Remove try-catch block. JUnit will receive exception and handle it appropriately (consider test successful according to your annotation). And if you supress exception there's no way of knowing for JUnit if it was thrown. @Test(expected=IOException.class) public void testEmptyString() throws IOException { new SumarniVzorec( """"); } Also dr jerry rightfully points out that you can't compare strings with == operator. Use equals method (or string.length == 0) http://junit.sourceforge.net/doc/cookbook/cookbook.htm (see 'Expected Exceptions' part) Thank you but I tried that already and it gives an error: unhandled Exception type IOError You still need to declare the method as `throws IOException` @Tomas where do you get IOError from? Can you post whole error message (with stacktrace)? @developmentalinsanity Thanks my bad. But it wouldn't compile otherwise so it can't really solve IOError. @developmentalinsanity Thank you works perfectly.  how about : @Test public void testEmptyString() { try { SumarniVzorec test = new SumarniVzorec( """"); org.junit.Assert.fail(); } catch (IOException e) { // Error e.printStackTrace(); }  Another way to do this is : public void testEmptyString() { try { SumarniVzorec test = new SumarniVzorec( """"); assertTrue(false); } catch (IOException e) { assertTrue(true); }"
378,A,"How to include performance goals in Java test suite? Is there an easy way to automatically enforce goals like ""This service must support 1000 transactions per minute"" in daily build tests for Java? Is this ever done in JUnit or are there caveats to it? You can't do exactly what you are looking for but you can do the below using JUnitPerf. JUnitPerf tests are intended to be used specifically in situations where you have quantitative performance and/or scalability requirements that you'd like to keep in check while refactoring code. For example you might write a JUnitPerf test to ensure that refactoring an algorithm didn't incur undesirable performance overhead in a performance-critical code section. You might also write a JUnitPerf test to ensure that refactoring a resource pool didn't adversely affect the scalability of the pool under load.  That doesn't feel like a unit test to me. It could be a long-running transaction one that will delay the completion of your build. You might reconsider if the total running time is longer than your build frequency. It's not even clear to me how meaningful the question is because it doesn't take into account simultaneous users. You can easily do a ""poor man's"" multi-threaded load test like this with TestNG. It's not possible in JUnit 4.4 but it might be in a later version. Yeah I agree. Definitely not a Unit test. Just wondering if JUnit was capable of it. It would really easily integrate with the other tests that way. Could be possible with JUnit 4.9 - I haven't done it but I have with TestNG right out of the box. http://junit.org/node/589 and http://testng.org/doc/index.html"
379,A,"Java: Is assertEquals(String String) reliable? I know that == has some issues when comparing two Strings. It seems that String.equals() is a better approach. Well I'm doing JUnit testing and my inclination is to use assertEquals(str1 str2). Is this a reliable way to assert two Strings contain the same content? I would use assertTrue(str1.equals(str2)) but then you don't get the benefit of seeing what the expected and actual values are on failure. On a related note does anyone have a link to a page or thread that plainly explains the problems with str1 == str2? Thanks! If you are unsure you can read the code or the Javadoc. BTW if you want to test they are the same object you can use assertSame. If str1 and str2 are null assertEquals() is true but assertTrue(str1.equals(str2)) throws an exception. The first example will also print a useful error message such as the contents of str1 and str2 the second does not. In a nutshell - you can have two String objects that contain the same characters but are different objects (in different memory locations). The == operator checks to see that two references are pointing to the same object (memory location) but the equals() method checks if the characters are the same. Usually you are interested in checking if two Strings contain the same characters not whether they point to the same memory location.  public class StringEqualityTest extends TestCase { public void testEquality() throws Exception { String a = ""abcde""; String b = new String(a); assertTrue(a.equals(b)); assertFalse(a == b); assertEquals(a b); } }  assertEquals uses the equals method for comparison. There is a different assert assertSame which uses the == operator. To understand why == shouldn't be used with strings you need to understand what == does: it does an identity check. That is a == b checks to see if a and b are the same object. It is built into the language and its behavior cannot be changed by different classes. The equals method on the other hand can be overridden by classes. Its default behavior (in the Object class) is to do an identity check just like the == operator but many classes including String override it to instead do an equivalence check. Analogy time: imagine that each String object is a piece of paper with something written on it. Let's say I have two pieces of paper with ""Foo"" written on them and another with ""Bar"" written on it. If I take the first two pieces of paper and use == to compare them it will return false because it's essentially asking ""are these the same piece of paper?"". It doesn't need to even look at what's written on the paper. The fact that I'm giving it two pieces of paper (rather than the same one twice) means it will return false. If I use equals however the equals method will read the two pieces of paper ad see that they say the same thing (""Foo"") and so it'll return true. The bit that gets confusing with Strings is that the Java has a concept of ""interning"" Strings and this is (effectively) automatically performed on any string literals in your code. This means that if you have two equivalent string literals in your code (even if they're in different classes) they'll actually both refer to the same String object. This makes the == operator return true more often than one might expect.  You should always use .equals() when comparing Strings in Java. JUnit calls the .equals() method to determine equality in the method assertEquals(Object o1 Object o2). So you are definitely safe using assertEquals(string1 string2). (Because Strings are Objects) Here is a link to a great Stackoverflow question regarding some of the differences between == and .equals(). IIRC assertEquals() succeeds if both strings are null. If this is not what you want then call assertNotNull() as well. @finnw checking the source you are indeed correct. additionally if you want to test for == you can call assertSame() I wouldn't say *always*; sometimes reference equality is desired even for strings.  Yes it is used all the time for testing. It is very likely that the testing framework uses .equals() for comparisons such as these. Below is a link explaining the ""string equality mistake"". Essentially strings in Java are objects and when you compare object equality typically they are compared based on memory address and not by content. Because of this two strings won't occupy the same address even if their content is identical so they won't match correctly even though they look the same when printed. http://blog.enrii.com/2006/03/15/java-string-equality-common-mistake/  ""The == operator checks to see if two Objects are exactly the same Object."" http://leepoint.net/notes-java/data/strings/12stringcomparison.html String is an Object in java so it falls into that category of comparison rules.  The JUnit assertEquals(obj1 obj2) does indeed call obj1.equals(obj2). There's also assertSame(obj1 obj2) which does obj1 == obj2 (i.e. verifies that obj1 and obj2 are referencing the same instance) which is what you're trying to avoid. So you're fine."
380,A,"Which Continuous Integration Plugin for Eclipse I do a lot of TDD and am thinking of installing a Continuous Integration Plugin for Eclipse. The two most popular seem to be JUnit Max and Infinitest. As both are ""payed for"" I'd like some opinions on the pros/cons/otherwise of each. Maybe there is a ""free"" alternative that I've missed? Note: as mentioned by Kent Beck himself the JUnit Max project is no longer actively developed. This blog post summarizes it (July 2009): I wrote about JUnit Max in a previous post. In that post I commented that I was not sure if people were willing to pay $2/month for it. It turned out that I was right. Kent Beck just announced that he has deadpooled JUnit Max. Ken adds: The conundrum I faced was how to market without any cash. I do have my reputation–people will (briefly) listen to what I say. That’s why I used the media I used. Actually if I had to do it over again I would attach my name less prominently to the product. Some people bought Max because it was a tool I produced not because it was a tool they really thought they needed and that delayed clear feedback. The signal that clinched the decision to deadpool Max was the lack of word-of-mouth. Subscribers were telling their friends but their friends weren’t buying. That being said he is planning to get JUnit Max back as he said himself in a vlog interview last week (July 2d 2010): (new release mid or end July?). So right now for large workspaces with many large projects Infinitest might scale better. (I have no direct experience with it). It isn’t open source but for personal use it should be possible get a free-of-charge individual license. (see the dual-licensing model for Infinitest). Oh well that answers that question! thanks Note to self: [twitter evaluation for ""JUnit Max""](http://twitter.com/elefevre/status/19728852247): ""JUnit Max is currently lacking a bit behind @infinitest. But with @kentbeck's name attached to it it could sell to big accounts."" JUnit Max is officially alive again"
381,A,"""Forked Java VM exited abnormally"" error from junit tests I have a java junit test that passes when run alone on a development machine. We also have a hudson job which runs all the tests invoked via ant on a Mac OS X 10.4 node with Java 1.5. The test was passing in the hudson build until recently but now (with no related code changes) one test fails everytime with the following error: Error Message Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit. Stacktrace junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit. googling shows many others seem to have run into the same problem but there I couldn't find any answer. Maybe provide the full stacktrace. Pascal: there was no full stacktrace. I still found no answer to this but we've worked around it by moving the hudson job to a different Mac running OS X 10.5 and Java 1.6. The problem does not manifest itself on this setup. That project don't happen to be using JMock with javaagent parameter declared right? @Zefi no it's not using JMOck How did it get solved? Which of the following is the correct answer? @java_enthu I never found a solution but we no longer run the test on Mac OS so it's no longer a problem for me. For me it was an ""java.lang.OutOfMemoryError"" in the forked VM (junit task with fork=""yes"") which made this message appear in the main VM. The OutOfMemory was visible in the ant log (well is visible since it's still present). I use ant 1.7.1 so no hope with upgrading ant. After putting the same VM parameters in ""Run>External tools>External tools>JRE"" than in Eclipse.ini (-Xms40m -Xmx512m -XX:MaxPermSize=256M) the problem is solved. I keep fork to ""no"" to be sure ant use the parameters. Another way to allow the forked junit process to access more memory (detailed here: http://ant.apache.org/manual/Tasks/junit.html) is to use the maxmemory attribute on the junit ant task. Its the case with eclipse what can we do when running with ANT?  I had the exact same thing a while back. The problem is that System.exit() is being called somewhere. It can be difficult to find though as the call could come from either your code or one of the libraries you use. Whoever publishes a library for general use that calls System.exit() needs to be hunted down and shot.  Is the VM crashing ? Can you find a dump file (called hs_err_pid*.log) ? If that's the case the dump file will give you clues to why this is crashing out. What about windows machine?  I faced the problem after reinstalling a new version of NetBeans to an external hard disk upgrading Junit at the same time and using my old workspace. For me the solution to the same problem was simple: Just add the JUnit-Library to project properties => Libraries => Compile Tests and Run Tests. So in my case it was just a missing library or a JUnit version conflict.  In my case it's an uncaught exception in a static initializer/method/block inside a class. Specifically I had one class calling a static method in another class and it was triggering a NumberFormatException. BTW adding ""showoutput=true"" to the task in build.xml did not help troubleshoot. Since the static block is one of the first things to run the JVM was blowing up before it could output anything at all.  I had this issue too. Changing the junit task from: <batchtest fork=""yes"" ... /> to <batchtest fork=""no"" ... /> fixed it for me. I don't fully understand this area of ant though or why doing this would fix it. In my scenario it was an error in ""BeforeFirstTest"" and I think it barfs because of two ant files in my classpath (which is probably what I ought to fix) I think the issue is with one of the versions of ant: http://track.pmease.com/browse/QB-500;jsessionid=C1CF6999CBBDB5097A9CFCF4A11AF6C0?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ETA: I think batchtest=""no"" actually changes the classpath and hence results in exclusion of my offending ant jar.  I faced similar issue. I ran junit tests as ant task. Added showoutput=""yes"" ant junit property and ran the ant junit task. It then showed the exception stacktrace which caused the forked jvm to exit.  I believe I saw this error once when I ended up with multiple versions of junit on my classpath. Might be worth checking out.  I have multiple junit jars in my classpath. one is of ant and another is from WAS. As I removed that the error went away... Ant version that I am using 1.8  I faced the same issue. The problem was with byte code generation with mocking the Config class; We changed the import to import static org.junit.Assert.assertNotNull;import static org.mockito.Mockito.times; import static org.mockito.Mockito.verify; import static org.mockito.Mockito.when; and it worked.  I had this problem and it turns out that the process was actually calling System.exit(). However there was also a bug in Ant where this was showing up sometimes. I think Ant 1.7.1 has the bug fixed. So make sure you are running that version.  I solved my issue by setting the following environment variable: Variable: _JAVA_OPTIONS Value: -Xms128m -Xmx512m  For us it was actually that we by accident (used a newer version of eclipse) started to use Ant 1.7.x instead of our old ant version which was compatible with our Weblogic 8.1/JDK 1.4.x environment. We fixed this by changing back the Ant Home in Eclipse->Windows->Preferences->Ant->Runtime to our old version of Ant. Regards Klas  This can occur when an uncaught RuntimeException is thrown. Unfortunately the junit ant task doesn't output the exception so there isn't an easy way to determine the root cause. You can work around this by running the test case from the command line where the exception will be shown. java <vm-args> org.junit.runner.JUnitCore <test-class-name> In my case an IllegalArgumentException was being thrown."
382,A,How to write multi-threaded unit tests? I'd like to know if there are some unit testing frameworks which are capable of writing multi-threaded tests easily? I would imagine something like: invoke a special test method by n threads at the same time for m times. After all test threads finished an assertion method where some constraints should be validated would be invoked. My current approach is to create Thread objects inside a junit test method loop manually the real test cases inside each run() method wait for all threads and then validate the assertions. But using this I have a large boilerplate code block for each test. What are your experiences? I asked a related (not duplicate) question some time back and got some good answers from folks: http://stackoverflow.com/questions/537014/using-tdd-to-drive-out-thread-safe-code Just using the concurrency libraries would simplify your code. You can turn your boiler plate code into one method. Something like public static void runAll(int times Runnable... tests) { } This does not answer the question. Yes the code would be simpler and some risks eliminated but still it should be tested somehow.  There is ConTest and also GroboUtils. I've used GroboUtils many years ago and it did the job. ConTest is newer and would be my preferred starting point now since rather than just relying on trial and error the instrumentation forces specific interleavings of the threads providing a deterministic test. In contrast GroboUtils MultiThreadedTestRunner simply runs the tests and hopes the scheduler produces an interleaving that causes the thread bug to appear. EDIT: See also ConcuTest which also forces interleavings and is free. There's also ConcuTest which I've added as an edit. ConTest looks good but seems not to be freely available (I can only find a trial).  There is also MultithreadedTC by Bill Pugh of FindBugs fame. +1: I've used it before and found it very good at generating specific interleavings and asserting properties of your implementation.
383,A,"No Runnable methods Error From Base Test class I have a A few base test classes that setup common configurations for springloggingjndi etc using test execution listeners that are then inherited by subclasses. This is done so tests can just run code without having to worry about getting jndi and logging services in place before being able to run testing code. Using intellij and invoking ""run all tests"" from the project base the IDE attempts to run the base test class as a unit test and gives me the ""No runnable methods"" error. I know I could put an empty runnable method in the base class but I was hoping some one has a better idea. The Base class is:  @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:spring-jndi.xml"" }) @TestExecutionListeners({ Log4JSetupListener.class JndiSetupListener.class DependencyInjectionTestExecutionListener.class DirtiesContextTestExecutionListener.class TransactionalTestExecutionListener.class }) public class SpringAppTestCase extends Assert implements ApplicationContextAware { protected JndiTemplate jndiTemplate = new JndiTemplate(); @Autowired protected JdbcTemplate jdbcTemplate; protected ApplicationContext applicationContext; public void setApplicationContext(ApplicationContext ac) { this.applicationContext = ac; } // // @Test // public void doit(){ // // this would prevent blow up but // all subclass tests would run an extra method // } protected Logger log = Logger.getLogger(getClass().getName()); } The error: java.lang.Exception: No runnable methods at org.junit.internal.runners.MethodValidator.validateInstanceMethods(MethodValidator.java:32) at org.junit.internal.runners.MethodValidator.validateMethodsForDefaultRunner(MethodValidator.java:43) at org.junit.internal.runners.JUnit4ClassRunner.validate(JUnit4ClassRunner.java:36) at org.junit.internal.runners.JUnit4ClassRunner.<init>(JUnit4ClassRunner.java:27) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.<init>(SpringJUnit4ClassRunner.java:76) Actually there are not tests in the code above.... You have it commented out so the error message is correct? Yes explained by the comment inside the commented code  Make the parent class abstract or rename it such as it does not end in Test or TestCase. Thanks that is exactly what I needed Thanks! I had my class starting with ""Test"" and that caused the same issue. Thanks too! I had almost the same problem except that I had a class that was NOT a class test but with a name starting by Test and that led to the same error message. Taking ""Test"" out of the class name didn't work for me. But adding ""abstract"" did."
384,A,"Is it really necessary to nullify objects in JUnit teardown methods? I was intrigued by the answer to a similar question. I believe it is incorrect. So I created some test code. My question is does this code prove/disprove/inconclusive the hypothesis that it is useful to nullify member variables in teardown methods? I tested it with JUnit4.8.1. JUnit creates a new instance of the test class for each of the 4 tests. Each instance contains an Object obj. This obj is also inserted as the key of a static WeakHashMap. If and when JUnit releases its references to a test instance the associated obj value will become weakly referenced and thus eligible for gc. The test tries to force a gc. The size of the WeakHashMap will tell me whether or not the objs are gc'ed. Some tests nullified the obj variable and others did not. import org . junit . Before ; import org . junit . After ; import org . junit . Test ; import java . util . ArrayList ; import java . util . WeakHashMap ; import java . util . concurrent . atomic . AtomicInteger ; import static org . junit . Assert . * ; public class Memory { static AtomicInteger idx = new AtomicInteger ( 0 ) ; static WeakHashMap < Object  Object > map = new WeakHashMap < Object  Object > ( ) ; int id ; Object obj ; boolean nullify ; public Memory ( ) { super ( ) ; } @ Before public void before ( ) { id = idx . getAndIncrement ( ) ; obj = new Object ( ) ; map . put ( obj  new Object ( ) ) ; System . out . println ( ""<BEFORE TEST "" + id + "">"" ) ; } void test ( boolean n ) { nullify = n ; int before = map . size ( ) ; gc ( ) ; int after = map . size ( ) ; System . out . println ( ""BEFORE="" + before + ""\tAFTER="" + after ) ; } @ Test public void test0 ( ) { test ( true ) ; } @ Test public void test1 ( ) { test ( false ) ; } @ Test public void test2 ( ) { test ( true ) ; } @ Test public void test3 ( ) { test ( false ) ; } @ After public void after ( ) { if ( nullify ) { System . out . println ( ""Nullifying obj"" ) ; obj = null ; } System . out . println ( ""<AFTER TEST "" + id + "">"" ) ; } /** * Try to force a gc when one is not really needed. **/ void gc ( ) { ArrayList < Object > waste = new ArrayList < Object > ( ) ; System . gc ( ) ; // only a suggestion but I'll try to force it list : while ( true ) // try to force a gc { try { waste . add ( new Object ( ) ) ; } catch ( OutOfMemoryError cause ) { // gc forced? should have been waste = null ; break list ; } } System . gc ( ) ; // only a suggestion but I tried to force it } } I ran the code using the command line interface (utilizing the -Xmx128k option to increase garbage collection) and got the following result .<BEFORE TEST 0> BEFORE=1 AFTER=1 Nullifying obj <AFTER TEST 0> .<BEFORE TEST 1> BEFORE=2 AFTER=1 <AFTER TEST 1> .<BEFORE TEST 2> BEFORE=2 AFTER=1 Nullifying obj <AFTER TEST 2> .<BEFORE TEST 3> BEFORE=2 AFTER=1 <AFTER TEST 3> The Test0 obj was nullified and in Test1 it is gc'ed. But the Test1 obj was not nullified and it got gc'ed in Test2. This suggests that nullifying objects is not necessary. It is indeed not necessary but it does help the garbage collector when it needs to know what variables are used or not; null variable are pretty much garantied to be good candidate for garbage collection. From he official jUnit page : ""When are tests garbage collected? [...] Therefore if you allocate external or limited resources in a test you are responsible for freeing those resources. Explicitly setting an object to null in the tearDown() method for example allows it to be garbage collected before the end of the entire test run."" (http://junit.sourceforge.net/doc/faq/faq.htm) But in my experiment the gc collector always managed to figure it out whether or not I nullified. Have you ever worked with different garbage collectors? Some might be pretty good but there are some garbage collectors that need a little bit of help @Drahakar if my gc is superior to others then that would mean I could write code and tests that compile and pass on my computer but compile and fail (due to memory leak) on other ppls computers. My JUnit tests are platform dependent? If I understand the official JUnit page correctly nullification is necessary (for garbage collection). I emailed the FAQ maintainer for clarification. 'It does help the garbage collector'. No it doesn't. The entire object is released after each test. Nulling the object references doesn't help in the slighest.  JUnit 4.x style tests and test suites handle this differently than JUnit 3.x test suites. With JUnit 3.x style tests a TestSuite contains references to other Test objects (which may be TestCase objects or other TestSuite objects). If you create a suite with many tests then there will be hard references to all of the leaf TestCase objects for the entire run of the outermost suite. If some of your TestCase objects allocate objects in setUp() that take up a lot of memory and references to those objects are stored in fields that are not set to null in tearDown() then you might have a memory problem. In other words for JUnit 3.x style tests the specification of which tests to run references the actual TestCase objects. Any objects reachable from a TestCase object will be kept in memory during the test run. For JUnit 4.x style tests the specification of which tests to run uses Description objects. The Description object is a value object that specifies what to run but not how to run it. The tests are run by a Runner object that takes the Description of the test or suite and determines how to execute the test. Even the notification of the status of the test to the test listener uses the Description objects. The default runner for JUnit4 test cases JUnit4 keeps a reference to the test object around only for the duration of the run of that test. If you use a custom runner (via the @RunWith annotation) that runner may or may not keep references to the tests around for longer periods of time. Perhaps you are wondering what happens if you include a JUnit3-style test class in a JUnit4-style Suite? JUnit4 will call new TestSuite(Class) which will create a separate TestCase instance per test method. The runner will keep a reference to the TestSuite for the entire life of the test run. In short if you are writing JUnit4-style tests do not worry about setting your test case's fields to null in a tear down (do of course free resources). If you are writing JUnit3-style tests that allocate large objects in setUp() and store those objects in fields of the TestCase consider setting the fields to null. This is the best answer. Nullifying is necessary (to avoid memory leakage) in JUnit 3.x. If you use JUnit4.x and the default runner nullifying is not necessary. If you use a custom runner then you may need to nullify.  No it is not necessary. Tear-down methods are for life-cycled objects that likes to be explicitly closed terminated shutdown disposed disconnected unregistered or whatever. Even if your references survive to the next test case they will be overwritten by your set-up method and become unreferenced and thus eligible for garbage collection. And event if JUnit creates a new instance of your test case for every method (which seems to be the case) those test objects are not held on to. At least not if the test passes according to a quick experiment. So the lot of it will be collected at leisure anyway. Both JUnit3 style tests and JUnit4 style tests create a separate object per test method that is executed so unless the references are stored in statics (not recommended) the references will not be overwritten by the next set-up method @NamshubWriter is that a guarantee or an implementation detail? If the latter then it might change (though unlikely). Regardless these instances aren't kept around (in JUnit 4.8.1 according to experiment) — at least not if the test passes. @NamshubWriter I updated my answer. Thanks for info. It's not an implementation detail it's the documented behavior. JUnit does this so it's harder to have the result of one test affect the behavior of another test."
385,A,"How do I use the Antlr generated junit file made by translating a gunit file I am trying to make unit tests for multiple return values in Antlr. I have regular unit tests working using gunit. However I am not too sure what to do with the junit Testgrammar.java file that is generated as per the instructions at http://www.antlr.org/wiki/display/ANTLR3/gUnit+-+Grammar+Unit+Testing I've tried running: java -cp ""./antlr.jar"" Testgrammar.java but I get the following error: Exception in thread ""main"" java.lang.NoClassDefFoundError: Testgrammar/java You run it like any other Java app: with the classname not the file name. java -cp ""./antlr.jar"" Testgrammar it has something to do with running it with junit. I am just not that familiar with setting it up from the command line. Not sure about running with JUnit just spotted the error in your command line call there  It needs to be compiled linking to the build output of the grammar files and then run with: java org/junit/runner/JUnitCore Testgrammar"
386,A,"Problem using MockRoundtrip class I have following code: @Test public void testSaveValid() throws Exception { MockRoundtrip trip = new MockRoundtrip(mockServletContext ContactFormActionBean.class mockSession); trip.setParameter(""contact.email"" ""test@test.com""); trip.setParameter(""contact.phoneNumber"" ""654-456-4567""); trip.execute(""save""); ContactFormActionBean bean = trip.getActionBean(ContactFormActionBean.class); assertEquals(0 bean.getContext().getValidationErrors().size()); PhoneNumber pn = bean.getContact().getPhoneNumber(); assertEquals(""654"" pn.getAreaCode()); assertEquals(""456"" pn.getPrefix()); assertEquals(""4567"" pn.getSuffix()); assertTrue( trip.getDestination().startsWith(""/ContactList.action"")); } I encounter this error: net.sourceforge.stripes.exception.StripesServletException: Unhandled exception in exception handler. at net.sourceforge.stripes.exception.DefaultExceptionHandler.handle(DefaultExceptionHandler.java:158) at net.sourceforge.stripes.controller.StripesFilter.doFilter(StripesFilter.java:249) at net.sourceforge.stripes.mock.MockFilterChain.doFilter(MockFilterChain.java:63) at net.sourceforge.stripes.mock.MockServletContext.acceptRequest(MockServletContext.java:255) at net.sourceforge.stripes.mock.MockRoundtrip.execute(MockRoundtrip.java:195) at net.sourceforge.stripes.mock.MockRoundtrip.execute(MockRoundtrip.java:207) at stripesbook.test.stripesmock.ContactFormActionBeanTest.testSaveValid(ContactFormActionBeanTest.java:96) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:585) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.runners.ParentRunner.run(ParentRunner.java:236) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) If I delete trip.setParameter(""contact.email"" ""test@test.com""); trip.setParameter(""contact.phoneNumber"" ""654-456-4567""); I won't get any errors but will get following message from jUnit: java.lang.AssertionError: expected:<0> but was:<1> which seems logical. This is my ContactFormActionBean class public class ContactFormActionBean extends ContactBaseActionBean { private static final String FORM=""/WEB-INF/jsp/contact_form.jsp""; @DefaultHandler public Resolution form() { return new ForwardResolution(FORM); } public Resolution save() { Contact contact = getContact(); contact.setUser(getUser()); contactDao.save(contact); contactDao.commit(); getContext().getMessages().add( getLocalizableMessage(""contactSaved"" contact) ); return new RedirectResolution(ContactListActionBean.class); } @ValidationMethod(on=""save"") public void validateEmailUnique(ValidationErrors errors) { String email = getContact().getEmail(); Contact other = contactDao.findByEmail(email getUser()); if (other != null && !other.equals(getContact())) { errors.add(""contact.email"" new LocalizableError( getClass().getName()+"".contactEmailAlreadyUsed"" other)); } } } Why this error happens? [ADDED] This is my Setup() function where I configure mockServletContext before running any test functions: ... private static MockServletContext mockServletContext; private static MockHttpSession mockSession; @BeforeClass public static void setup() throws Exception { mockServletContext = new MockServletContext(""webmail""); Map<StringString> params = new HashMap<StringString>(); params.put(""ActionResolver.Packages"" ""stripesbook.action""); params.put(""Extension.Packages"" ""stripesbook.ext"" + ""net.sourceforge.stripes.integration.spring""); mockServletContext.addFilter(StripesFilter.class ""StripesFilter"" params); mockServletContext.setServlet(DispatcherServlet.class ""DispatcherServlet"" null); mockSession = new MockHttpSession(mockServletContext); mockServletContext.addInitParameter(""contextConfigLocation"" ""/WEB-INF/applicationContext-test.xml""); ContextLoaderListener springContextLoader = new ContextLoaderListener(); springContextLoader.contextInitialized( new ServletContextEvent(mockServletContext)); // Load mock user MockRoundtrip trip = new MockRoundtrip(mockServletContext MockDataLoaderActionBean.class mockSession); trip.execute(); // Login mock user trip = new MockRoundtrip(mockServletContext LoginActionBean.class mockSession); trip.setParameter(""username"" ""freddy""); trip.setParameter(""password"" ""nadia""); trip.execute(""login""); } ... I think there could be some config problems cause when I remove <classpathentry kind=""src"" path=""src/main/webapp"" /> from class path I get different error : org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from ServletContext resource [/WEB-INF/applicationContext-test.xml]; nested exception is java.io.FileNotFoundException: Could not open ServletContext resource [/WEB-INF/applicationContext-test.xml] I have posted the full stack error. I don't what else is needed? StripesServletException should be wrapping an exception (the one that we're really interested in). Can you post the whole error message? There seems to be a Stripes configuration problem. Did you configure the mockServletContext correctly? **(1)** First make sure your app. Runs in your servlet container (for example Tomcat). **(2)** Use this configuration as you’re JUnit configuration: use an identical class path same Spring applicationContext.xml and make sure you’re MockServletContext config has the same configuration options as the web.xml you use in your servlet container. **(3)** Done! Thanks for your info. Thanks for your postI have edited my question and added stuff related to configuration.It would be great if you mind having a look.  U tried with Spring ? I have done it with : Object test= springContextListener.getContextLoader().getCurrentWebApplicationContext().getBean(""MyActionBean""); annoted your class with : @RunWith(SpringJUnit4ClassRunner.class) No I am using Stripes and also injected spring beans.THis is Fredric's bookPragmatic programmers. P272. I meant the Test Class  concerning org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from ServletContext resource [/WEB-INF/applicationContext-test.xml]; nested exception is java.io.FileNotFoundException: Could not open ServletContext resource [/WEB-INF/applicationContext-test.xml] add : @RunWith(SpringJUnit4ClassRunner.class to your test Class and add Spring-test Jar to your project. You don't need to add the @RunWith annotation all Spring initialization is done by the Stripes Mock environment! Kdevelopper is correct. I did that but when it runs any code that has return new RedirectResolution it will throw exception ERROR: java.lang.NoSuchMethodError: org.junit.runner.notification.RunNotifier.testAborted(Lorg/junit/runner/Description;Ljava/lang/Throwable;)V  recheck the line : filterParams.put(""ActionResolver.Packages"" ""yourapplicationpackagesPath.action"");"
387,A,"When running Android JUnit tests the values of text in TextView can not be accessed by .getText().toString() causing tests to fail When running Android JUnit tests the values of text in TextView can not be accessed by .getText().toString() causing tests to fail. http://www.android10.org/index.php/articlesother/203-unit-testing-with-the-junit-testing-framework I downloaded the code from the above web-page and ran the unit tests. All unit tests failed. The framework did not return any values from the result.getText().toString(); Why should this be? CODE // get result String mathResult = result.getText().toString(); assertTrue(""Add result should be 98 "" + ADD_RESULT + "" but was "" + mathResult mathResult.equals(ADD_RESULT)); } public void testAddDecimalValues() { sendKeys(NUMBER_5_DOT_5 + NUMBER_74 + ""ENTER""); String mathResult = result.getText().toString(); assertTrue(""Add result should be "" + ADD_DECIMAL_RESULT + "" but was "" + mathResult mathResult.equals(ADD_DECIMAL_RESULT)); } TRACE junit.framework.AssertionFailedError: Add result should be 79.5 but was at com.mamlambo.article.simplecalc.test.MathValidation.testAddDecimalValues(MathValidation.java:57) at java.lang.reflect.Method.invokeNative(Native Method) at android.test.InstrumentationTestCase.runMethod(InstrumentationTestCase.java:204) at android.test.InstrumentationTestCase.runTest(InstrumentationTestCase.java:194) at android.test.ActivityInstrumentationTestCase2.runTest(ActivityInstrumentationTestCase2.java:186) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:169) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:154) at android.test.InstrumentationTestRunner.onStart(InstrumentationTestRunner.java:520) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1447) Where do you have the Object result initialized? Add result should be 79.5 but was """" at ... Maybe that will help you."
388,A,"Junit test of the same object Hi i want to make a unit test suite on same object with same variable but different values But if the object get the same name (created by this.setName(""testlaunch""); (who must have the name of a function launch by junit) it makes only one test. if i don't write this.setName(""testlaunch""); he shout on me an [code]junit.framework.AssertionFailedError: TestCase.fName cannot be null[/code]. I don't know what to do...  public class LanceurRegleGestion extends TestSuite { public static Test suite() { Class maClasse = null; TestSuite suite = new TestSuite(); String filtre = "".*.xml""; // on compile le pattern pour l'expression réguliere Pattern p = Pattern.compile(filtre); String path = ""D:/Documents/workspace/Solipsisme/src/ReglesGestion/XML/""; // on liste les fichiers du repertoire String [] u = new File(path).list(); // on parcours la liste de fichier System.out.println(""Initialisation""); for (int i=0; i et le code de l'objet serialisé public class Application extends TestCase { private String nomappli; private String id2_1; private String id3_1; private String id4_1; private String id2_2; private String id3_2; private String id4_2; private String id5_2; private String id6_2; private String id7_2; private String id8_2; private String id9_2; private String id2_3; private String id3_3; private String id4_3; private String id2_4; private String id3_4; private String id4_4; private String id2_5; private String id3_5; private String id4_5; private String id5_5; private String id6_5; private String id7_5; private static Selenium selenium; public Application(String nomappliString id2_1String id3_1String id4_1String id2_2String id3_2String id4_2String id5_2String id6_2String id7_2String id8_2String id9_2String id2_3String id3_3String id4_3String id2_4String id3_4String id4_4String id2_5 String id3_5String id4_5String id5_5String id6_5String id7_5) { this.setName(""testlaunch""); this.nomappli = nomappli; this.id2_1 = id2_1; this.id3_1 = id3_1; this.id4_1 = id4_1; this.id2_2 = id2_2; this.id3_2 = id3_2; this.id4_2 = id4_2; this.id5_2 = id5_2; this.id6_2 = id6_2; this.id7_2 = id7_2; this.id8_2 = id8_2; this.id9_2 = id9_2; this.id2_3 = id2_3; this.id3_3 = id3_3; this.id4_3 = id4_3; this.id2_4 = id2_4; this.id3_4 = id3_4; this.id4_4 = id4_4; this.id2_5 = id2_5; this.id3_5 = id3_5; this.id4_5 = id4_5; this.id5_5 = id5_5; this.id6_5 = id6_5; this.id7_5 = id7_5; } public Application(){ } public String toString() { return getNomappli(); } public void setNomappli(String nomappli) { this.nomappli = nomappli; } public String getNomappli() { return this.nomappli; } public void setId2_1(String id2_1) { this.id2_1 = id2_1; } public String getId2_1() { return this.id2_1; } public void setId3_1(String id3_1) { this.id3_1 = id3_1; } public String getId3_1() { return this.id3_1; } public void setId4_1(String id4_1) { this.id4_1 = id4_1; } public String getId4_1() { return this.id4_1; } public void setId2_2(String id2_2) { this.id2_2 = id2_2; } public String getId2_2() { return this.id2_2; } public void setId3_2(String id3_2) { this.id3_2 = id3_2; } public String getId3_2() { return this.id3_2; } public void setId4_2(String id4_2) { this.id4_2 = id4_2; } public String getId4_2() { return this.id4_2; } public void setId5_2(String id5_2) { this.id5_2 = id5_2; } public String getId5_2() { return this.id5_2; } public void setId6_2(String id6_2) { this.id6_2 = id6_2; } public String getId6_2() { return this.id6_2; } public void setId7_2(String id7_2) { this.id7_2 = id7_2; } public String getId7_2() { return this.id7_2; } public void setId8_2(String id8_2) { this.id8_2 = id8_2; } public String getId8_2() { return this.id8_2; } public void setId9_2(String id9_2) { this.id9_2 = id9_2; } public String getId9_2() { return this.id9_2; } public void setId2_3(String id2_3) { this.id2_3 = id2_3; } public String getId2_3() { return this.id2_3; } public void setId3_3(String id3_3) { this.id3_3 = id3_3; } public String getId3_3() { return this.id3_3; } public void setId4_3(String id4_3) { this.id4_3 = id4_3; } public String getId4_3() { return this.id4_3; } public void setId2_4(String id2_4) { this.id2_4 = id2_4; } public String getId2_4() { return this.id2_4; } public void setId3_4(String id3_4) { this.id3_4 = id3_4; } public String getId3_4() { return this.id3_4; } public void setId4_4(String id4_4) { this.id4_4 = id4_4; } public String getId4_4() { return this.id4_4; } public void setId2_5(String id2_5) { this.id2_5 = id2_5; } public String getId2_5() { return this.id2_5; } public void setId3_5( String id3_5) { this.id3_5 = id3_5; } public String getId3_5() { return this.id3_5; } public void setId4_5(String id4_5) { this.id4_5 = id4_5; } public String getId4_5() { return this.id4_5; } public void setId5_5(String id5_5) { this.id5_5 = id5_5; } public String getId5_5() { return this.id5_5; } public void setId6_5(String id6_5) { this.id6_5 = id6_5; } public String getId6_5() { return this.id6_5; } public void setId7_5(String id7_5) { this.id7_5 = id7_5; } public String getId7_5() { return this.id7_5; } public void setSelenium(Selenium selenium) { this.selenium = selenium; } public Selenium getSelenium() { return this.selenium; } public final static void login() { selenium.open(""apj/ident""); selenium.type(""username"" ""hsuzumiya-cp""); selenium.type(""password"" ""1""); selenium.click(""enterButton""); selenium.waitForPageToLoad(""9999999""); } public void testlaunch() { generique(this.nomapplithis.id2_1this.id3_1this.id4_1this.id2_2this.id3_2this.id4_2this.id5_2this.id6_2this.id7_2this.id8_2this.id9_2this.id2_3this.id3_3this.id4_3this.id2_4this.id3_4this.id4_4this.id2_5this.id3_5this.id4_5this.id5_5this.id6_5this.id7_5); } public void setUp() throws Exception { System.out.println(""Initialisation""); selenium = new DefaultSelenium(""127.0.0.1""4444""*iexplore"" ""http://hsuzumiya/""); selenium.start(); selenium.setTimeout(""90000""); selenium.setSpeed(""500""); login(); } public void generique(String nomappliString id2_1String id3_1String id4_1String id2_2String id3_2String id4_2 String id5_2String id6_2String id7_2String id8_2String id9_2String id2_3String id3_3String id4_3String id2_4 String id3_4String id4_4String id2_5 String id3_5String id4_5String id5_5String id6_5String id7_5 ) { System.out.println(nomappli); selenium.click(""valider""); selenium.waitForPageToLoad(""30000""); selenium.click(""validertout""); } public final void tearDown() throws Exception { System.out.println(""Killing session""); selenium.stop(); } } If anyone get an idea thanks Merci Bussiere It is hard to follow your description of the problem. Can you post the stack trace for the assertion that is failing? Being new to junit I stumbled upon this question hoping to solve a problem I had getting the same message. Through further research I found that it is necessary to pass the name of the test function you want to invoke through addTest to the constructor of the test case class. A simple (and useless other than illustration) example follows: JunitTestCases.java import junit.framework.TestCase; public class JunitTestCases extends TestCase { public JunitTestCases(String fnName) { super(fnName); } public void testA() { assertTrue(""assertTrue failed"" true); } } JunitTestSuite.java: import junit.framework.*; public class JunitTestSuite { public static Test suite() { TestSuite suite = new TestSuite(); suite.addTest(new JunitTestCases(""testA"")); return suite; } public static void main(String[] args) { junit.textui.TestRunner.run(suite()); } } When I compiled with: javac -cp .:path/to/junit-X.X.X.jar JunitTestSuite.java and ran with java -cp .:path/to/junit-X.X.X.jar JunitTestSuite this worked with no errors with junit giving me an OK message."
389,A,"How to use an uncaught exception handler for a multi-thread test in junit? I have the following code that I expect to run successfully to completion but the code fails at the line ""fail(""this should not be reached"");"". Can someone please explain why the default uncaught exception handler is not called: public class UncaughtExceptionTest extends TestCase implements UncaughtExceptionHandler { private final List<Throwable> uncaughtExceptions = new CopyOnWriteArrayList<Throwable>(); class UncaughtExceptionTestInnerClass implements Runnable { private final ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(1); private final CountDownLatch latch; UncaughtExceptionTestInnerClass(CountDownLatch latch) { this.latch = latch; executor.schedule(this 50 TimeUnit.MILLISECONDS); } @Override public void run() { System.out.println(""This is printed""); fail(""this should fail""); latch.countDown(); } } @Test public void testUncaughtExceptions() { Thread.setDefaultUncaughtExceptionHandler(this); CountDownLatch latch = new CountDownLatch(1); UncaughtExceptionTestInnerClass testTheInnerClass = new UncaughtExceptionTestInnerClass(latch); try { if (!latch.await(1 TimeUnit.SECONDS)) { if (uncaughtExceptions.size() > 0) { Throwable exception = uncaughtExceptions.get(0); System.out.println(""First uncaught exception: "" + exception.getMessage()); } else { fail(""this should not be reached""); } } } catch (InterruptedException e) { e.printStackTrace(); } } @Override public void uncaughtException(Thread t Throwable e) { uncaughtExceptions.add(e); } } Why is the `fail(""this should fail"");` line there? Is this intentional? This way the latch never counts down. @BalusC: yes this is intentional. To pass the test correctly the latch.await() method should time out causing it to return false. The code should then print the message from the uncaught AssertionFailedError exception that was thrown on the line you are questioning. ScheduledThreadPoolExecutor.schedule() takes a Runnable/Callable argument not Thread. The former don't have runtime exception handlers. Have a try/catch block for a RuntimeException in your run or call method.  It has to do with the fact you're using an Executor to run the task. The uncaught exception handler is invoked only if the thread is about to be terminated due to an uncaught exception. If you change your implementation to use a plain thread so that the thread will terminate with the exception you will see the expected behavior. Depending on how you submit tasks the executor thread may catch all Throwables and handle them. Therefore the thread does not terminate due to these exceptions and thus the uncaught exception handler does not get involved. For example ThreadPoolExecutor.execute(Runnable) will trigger the uncaught exception handler. However ThreadPoolExecutor.submit(Callable) does not. Also ScheduledThreadPoolExecutor.schedule() does not either (it has to do with their use of FutureTask for implementation). A better way of accessing unexpected exceptions with an executor service is via Future."
390,A,"Problems running unit tests after upgrading from Netbeans 6.5 to Netbeans 6.7 I am experiencing issues with existing projects from Netbeans 6.5 in Netbeans 6.7 when selecting a file under ""Test Packages"" and performing a right click-->Debug or right click-->Run. The error reported is: java.lang.NoSuchMethodError: main Exception in thread ""main"" Java Result: 1 I am using JUnit 4.x I have not changed any code since the upgrade and have tried this on multiple projects that were working and I still experience the problem. I am able to perform right click-->Test on the entire project. Is anyone aware of any changes I would need to make due to the upgrade or of anything broken in Netbeans 6.7? Thank you for your help bn Work-around Solution Found If the following setting in Netbeans 6.7 is not checked the tests work as they did in 6.5: Right Click --> ""Project Properties"" --> ""Build"" --> ""Compiling"" --> ""Compile on Save"" So this is at least a work-around until real issue is understood. -bn"
391,A,Using JDBC in setup for a Jmeter Junit test I was trying to use the JDBC to connect to my MySQL database in the setup and teardown methods of my Junit tests that are being run through Jmeter. Jmeter will create multiple threads each of which will run one instance of my test. The issue that I run into is if I instantiate my datasource in the setup of my Junit test then my connection pool runs out of connections. Is there a way to design my test so that this does not happen? I'm trying to avoid increasing my max MySQL connections to solve this issue. What do you want to test ? Your DB or your Java code ? Can we have a look at one of your tests and the class it's testing ? About your connection pool if you have a max_connection_pool it sounds pretty strange to use more connections than possible to test. It's a load test and I want to test my Java code. The DB connection is just for the test's setup. Yes make sure that each test closes the connection and returns it to the pool when it's done. Sounds like that's not happening but I can't be sure. If that's not the case I'd recommend tuning your connection and thread pool sizes better so you don't run out.  A couple suggestions: If possible use mocks or a mocking framework like mockito. Use connection pooling if you're not already - this might alleviate the overhead of having more connections that you need.
392,A,"How to JUnit test for object immutabily? I have a method similar to this public void method(final Object A){ .... } now I want to write a test which ensures Object A is always final. How do I write such test ? I'm not sure what you're asking... it is final; it is marked as such. Are you wanting to test that its value never changes during the execution of `method`? basically I don't want the Object A to be changed within this Method. So I want to prevent other developers to come and change the 'final' type of the Object. I want a unit test to guard against such shenanigans. I'm afraid you do not understand what your method does. A final parameter in a method means only that the parameter cannot be reassigned in the method.  public void doStuff( String s ){ s = ""OK""; } but  public void doStuff( final String s ){ s = ""ERROR s cannot be assigned because it is final!"";//causes error } As such there is no behaviour that can test for. If it is not valid it will not compile. Edit: If you want you unit test for a final I don't that is possible either since I believe that information is lost at time of compilation once it is checked internally. @paranoid That is probably a terrible thing to achieve in code. If they have access to the class code they probably have access to the test case and can change it there as well. I don't think there's a way to enforce programmer behavior in that scenario save that you comment the code appropriately and have reasonable expectations from your team. @arpanoid You are looking for something like ""const"" in C++... and that doesn't exist in Java. What you could do is this: create a subclass of Object (or whatever class you accept as a parameter) and then override any method that would modify the class to call org.junit.Assert.fail( ""Attempted to call modifier setXXX()"" ). Then pass that in. Or you should look into Mock objects with something like Mockito which lets you specify the contract. I think it's a perfectly good question - is it possible to unit-test the presence of a `final` method parameter? @skaffman I think not since I believe ""final"" is essentially a compile-time annotation Well I don't have anymore time to spend on this so this is just a pointer not an answer. What you want to do is use reflection to get the parameters for a method (couldn't see how to do this) and check it's modifiers to see if it's final. http://download.oracle.com/javase/1.4.2/docs/api/java/lang/reflect/Modifier.html#FINAL @user381091 Those are modifiers for class members (aka fields) and methods but not for method parameters. You will notice that method parameters also cannot be static abstract volatile transient etc. If this was an actual _runtime_ annotaiton you could use Method.getParameterAnnotations() . basically I don't want the Object A to be changed within this Method. So I want to prevent other developers to come and change the 'final' type of the Object. I want a unit test to guard against such shenanigans"
393,A,Why can't java find the JUnit framework? I am trying to get a test class compiling with JUnit and I can't figure out why it will not compile. I have the following lines at the top of my class: import java.util.*; import org.junit.*; And the error I am getting is package org.junit does not exist JUnit.jar is currently located in Program Files\JUnit\junit.jar which currently also resides in my class path. I am working on Windows Vista if that helps. Any ideas on how I can compile this test class with JUnit? Thanks very much What version of JUnit are you using? I think that until JUnit 3 the package was different:  import junit.framework.*; Also if you are using Eclipse you can pick the JUnit framework to use. True just checked
394,A,"creating an executable jar file with ant which includes the build.xml file I am trying to use ant to build an application run the application's main() method run junit tests and package everything in a jar file (source+build+libraries+build.xml). The classes include a runner class with a main() method entry point. The objective however is to inlcude all libraries used like junit.jar and the ant build.xml itself. I was wondering if there is a way for the executable jar file to run selected targets of the ant build file itslef instead of just the main() method. Obviously I wouldn't need to run the compile targets again but only the main() method (the java element in my run target) and the junit target. Is that possible? Many thanks in advance for the insight! denchr did I answer your question? If not do clarify what are you trying to achieve and I'll edit my reply (or someone else would provide a better one). And if I did please vote for or accept the answer - that's how SO works and you don't seem to be doing that for most of your questions. Yes you actually did. Thanks a bunch. Let me accept your answer sorry about that. ""Executable jar"" is not what you think it is. It allows you pack all your classes together (you can add source to it as well though I see little point in that) and declare a main class using Main-Class attribute in the jar manifest. Details are here. You can then launch that jar using ""java -jar my.jar"" command line which would invoke main() method of the class you've specified. You can also specify classpath via Class-Path attribute in the manifest file that can point to other jars needed by your application. That DOES NOT MEAN those jars are archived within your jar; in fact if you were to do that JVM won't be able to load classes from those jars unless you were to take special precautions and write a custom classloader."
395,A,"Adding test files to a Junit test in Netbeans 6.7.1 - getResource issues missing file in build directory I'm adding a Junit test to one of my classes using NetBeans 6.7.1 I need to be able to load a an xml file specific to testing so I've included it in the ""Test Packages"" folder (along with my actual test). I'm running into 2 issues (1) getResource is looking in the wrong directory (2) my xml test file doesn't get copied when I run tests (note this functionality works with I add files to the ""Sources Packages"" directory). In my test class: this.getClass().getResource(""/"") returns: D:\UserProfiles\myName\My Documents\NetBeansProjects\ProjectExample\build\classes I need it too return: D:\UserProfiles\myName\My Documents\NetBeansProjects\ProjectExample\build\test\classes (As that is where the test classes are being compiled) It seems rather hacky calling getResource getting the parent and then looking in test\classes. Is this the proper way of getting the path to my test file ? maybe this is a bug in netbeans ? Also when I right click on my testFile and ""run tests""  only my test class files get copied to the test/classes directory and not my xml test file. How do I tell Netbeans to make sure to copy a regular xml file along with class files to the build directory. I would also like to avoid hacking the ant build to copy my test files. Ok issue 2 was solved by doing a Clean Build. Although whenever I run a new test these files get deleted. Just put your file in the same package as your test say data.xml in package foo.bar. No copying or build script hacking is necessary just refer to the file like this: getClass().getResource(""data.xml""); Alternatively you can do this: getClass().getResource(""/foo/bar/data.xml""); Thanks but I would prefer that I was able to have access to these files before I do a package .. i.e. I just run the tests and the java files get compiled to class files. How do you mean that? NetBeans will just compile your classes and run your tests without creating a *.jar and you can reference a file like this. Just right clicking on the test file and selecting ""test file"" ... only the selected file gets built and tested. I'm finding this much more efficient then rebuilding the entire project when I just want to run one new (using TDD).  I recently ran into this problem using NetBeans 6.9.1. Here's how I solved it. Open Project->Properties->Libraries Select the Run Tests Tab Click on Add Jar/Folder Navigate to where you've stored the resource files Add the folder Now running tests using those resources will work. I tested this with NetBeans 6.9.1 and a simple Java Application. NetBeans creates resource files in the src directory by default (default package) so I added the src folder in step 5 above. Once you do that tests looking for a resource file in the classpath will find it. Since NetBeans packages the resources found in src folder by default you don't need to copy the files around and keep them in sync. If you want test resources different from production resources you can add the test resources in the default package under the test folder. Then instead of adding the src folder in step 4/5 above add the test folder."
396,A,"Maven - synch ""main"" folder with ""tests"" folder I'm just starting to use Maven with my project. All of my production code is of course inside the main directory. As I add unit tests to my tests directory is there a way to synchronize the main dir with my tests dir? For example say I add a new package org.bio.mynewpackage. I have to go in my main folder and add the same package name... this is rather annoying. Any ideas? What IDE are you using? It sounds like the sort of thing that it might be able to do for you. (Maven doesn't care; *it's* job is particularly the production of the contents of the `target` directory so that you can focus on your source…) I don't know if I get your problem right: usually by convention maven will detect all class files in main/java and all class files in test/java. You don't have to declare the package names. So if you add an new package an classes to ""main"" they will be compiled and packaged if you add some new tests to ""test"" the will be autodiscovered and executed in the test phase. @Luca - but this is not maven's job. Maven is used for building your application (compile test package deploy) not for creating source folders or organizing code. Yes sure I understand this. However it sounds logical to keep the same directory structure in ""main"" as you have in ""test"". Was just wondering if Maven did this automatically. So for example if you have `org.bio.something.MyApp` in main you probably want the tests for it located under `org.bio.something.MyAppTest` in the test dir.  I don't know if you were looking for something specific regards how maven might help you do this. Still i've always used rsync to match to target folders. Something along the lines of: rsync -Crit ./source ./target where C ignores versioning files/folders such as .svn r is recursion i is information output. t is timestamp. i've always put this to ensure differences in files are based on time stamp. Add 'n' to run in test mode it will output what will change rather than actually do it. Always do this first as rsync can totally mess things up if you don't have it right. You can also add pattern matching rules either in a file in each directory or once in the command line. rsync would do that also. Given i've not used maven extensively i'm probably almost off topic. I think he's only looking for a way to sync directories not files.  Typically I would rely on the IDE to do this when I create tests. Eg: I create a new class org.bio.mynewpackage.MyNewClass in main/. Now when I create a test org.bio.mynewpackage.MyNewClassTest the IDE should automatically create the necessary directory tree. Alright. I'm not a big IDE fan just yet I thought Maven had something for this. Maven has many purposes but substituting for an IDE is not one of them. @S.P.Floyd - sure of course not I never said that. But considering that Maven brings directory structure as its main feature I thought something like this would be built in. @Luca I'm sure you could write a plugin to replicate the directory structure in test/ and main/ but what's the point? Also do your tests always _have_ to have the same package structure as your sources? @Here Be Wolves - having the same packages structure is just an extra standard practice that seems logical. If other people are looking at my code they know exactly where to find the tests for a specific package class. It's also convenient as it gives the tests access to package-scoped classes and methods. @Luca I mean: what you need is an identical _package_ structure not an identical _directory_ structure. It is just coincidence that both mean the same but if classes in some package do not have their own unit tests it would be pointless to have directories for those pakcages.  Here is a Groovy Script embedded in a GMaven plugin execution. It does exactly what you are asking for. <plugin> <groupId>org.codehaus.gmaven</groupId> <artifactId>gmaven-plugin</artifactId> <version>1.3</version> <executions> <execution> <id>mirror-folder-structure</id> <phase>generate-test-sources</phase> <goals> <goal>execute</goal> </goals> <configuration> <source> <![CDATA[ static void createShadow(File base File shadow){ if(base.exists()&&!shadow.exists())shadow.mkdirs(); base.eachDir { createShadow(it new File(shadow it.name))}; } createShadow(pom.build.sourceDirectorypom.build.testSourceDirectory); ]]> </source> </configuration> </execution> </executions> </plugin> The problem is: it won't run automatically. I have bound it to the phase generate-test-sources but you can choose any other phase. You will however have to execute that phase manually e.g. mvn generate-test-sources. If you would however consider using Eclipse with the m2eclipse plugin m2eclipse lets you define lifecycle phases that it runs automatically when you have saved a file so that would be easier.  I don't know of any plugin that does this but it should be pretty easy to write one. It could possibly even be done with some simple Groovy scripting using gmaven-plugin or the like. Alternatively this shell command should do what you want: # ! -wholename '*/.*' excludes hidden dirs like .svn $( cd src/main/java/ && find -type d ! -wholename '*/.*' -exec mkdir -p ../../test/java/{} \; ) EDIT: Here's a simple Maven plugin (this plugin sorts entries of eclipse .classpath files by name) that should give you a quick start into Maven plugin development."
397,A,How to verify list of items in a test I am writing a test case where I do send a list of Strings to be saved in the database. Then I will retrieve those from database and has to verify that everything is fine. I have written a assertNotNull(list) assertEquals(listSize response.listSize()) However I want to verify the actual contents are also same. But my assertEquals is failing since the list of strings are not in the same order when they are returned. How do you verify this type of thing usually? Er... why not just force the ordering by creating the initial list alphabetically (or use a sort) and then use the ORDER BY clause in SQL? That said you may need to iterate through the elements in the list and compare them (as the keys may also be different in your original list and that retrieved from the database). I have done this by using Arrays.sort() and Arrays.equals()  assuming you have a List expected which are the strings you expect you can do assertTrue (response.containsAll(expected)) that combined with your size verification makes sure that the list is complete and doesn't contain extras.
398,A,"how to test this portion of code Hello I'm writting junit test how can I test this method .. this is only part of this method : public MyClass{ public void myMethod(){ List<myObject> list = readData(); } } How will I make the test for this? ReadData is a private method inside MyClass? I think you need to extend your example a bit. As it stands the method has no visible behavior at all (except for returning) so there's nothing to test. Does 'readData' modify a MyClass instance's state? If so you should test for the changed state. If not I don't understand what 'readData' does. List is wrong. You should specify a class between the angle brackets for example List. As written it doesn't make sense to test myMethod() unless readData() changes the instance state the way Frank Grimm mentioned. One thing to do would be to change myMethod() so that it puts list into a List instance variable. Then you might do something like this: @Test public void testThatReadDataReturnsACorrectList(){ MyClass inst = new MyClass(); // Add args to ctor call if needed - maybe a file path that readData() will use? inst.myMethod(); // Create a list of MyClasses that match what you expect readData() to return: List<MyClass> expectedList = new List<>(); expectedList.Add(new MyClass(/* Some arguments */)); expectedList.Add(new MyClass(/* Some more arguments */)); expectedList.Add(new MyClass(/* Some other arguments */)); // Assert that the list you created matches the list you get back from assertArrayEquals(""Did not get the list expected"" expectedList.ToArray() inst.getList().ToArray()); } You'd still have to write MyClass.getList() to return the List instance variable. To be robust you could make the MyClass constructor accept an object that implements an interface like IMyReadInterface. readData() would use that object. Then in your test you could instantiate a mock that also implements IMyReadInterface configure the mock to provide the data needed so that readData() works correctly and construct inst with that mock.  Unless we know more about the method all you really need to test is that the return from readData is in a format that fits into your generic list. Otherwise its hard to recommend anything without knowing more about whats going on in your private method.  You can always test the List object to see if it contains all of the elements that readData() is supposed to insert into the list. Make a public method that returns the list and you can compare the length and elements in that list to what you are expecting to be in there."
399,A,"How to configure IntelliJ for running test with JUnit 4? Should be simple but I couldn't figure it out. When running my unit test inside IntelliJ I could not find a way to tell IntelliJ-9.0 that it should use JUnit4 instead of JUnit3. When a test fails IntelliJ console displays: MyTests.testConstraints(MyTests.groovy:20) at ... com.intellij.junit3.JUnit3IdeaTestRunner.doRun(JUnit3IdeaTestRunner.java:108) at com.intellij.junit3.JUnit3IdeaTestRunner.startRunnerWithArgs(JUnit3IdeaTestRunner.java:42) ... Do you know how to replace JUnit3 by JUnit4 ? Put the JUnit 4 JAR in your CLASSPATH and see if IntelliJ picks it up. The JUnit plug-in appears to run either version 3 or 4. I'll bet that it has to do with the way you're writing your JUnit tests. Post one to confirm. If you use the JUnit 4 style I'll bet IntelliJ would run it properly. I am using a Grails application and it is in the grails CLASSPATH ($GRAILS_HOME/lib/junit-4.8.1.jar). Or maybe you mean IntelliJ CLASSPATH ? If yes in which folder should I put the JUnit4 JAR ? (running IntelliJ on Windows at d:/install/IntelliJ) FYI two JUnit JARS are in /lib : junit.jar and junit-4.7.jar. Removing junit.jar does not change anything : com.intellij.junit3.JUnit3IdeaTestRunner.doRun is still called !  I tried to put: @RunWith(JUnit4.class) at the beginning of a test. IntelliJ complained about this but asked to 'load' JUnit4.class. So I deleted @RunWith(JUnit4.class). But the 'loading' seems to have fixed the problem - @Ignore is now respected!  You can annotate your test class with an annotation to indicate junit the runner it will use @RunWith(JUnit4.class) MyTestClass {} This solved the problem for me using IntelliJ IDEA 10. It seems that IDEA defaults to the use of JUnit3 as a test runner for any classes that have junit.framework.TestCase in the hierarchy (which is the case for GrailsUnitTestCase/ControllerUnitTestCase). Using @RunWith forces IDEA to JUnit4. ...I'd prefer a solution that didn't required modifying every test class though... By the way if you have junit.framework.TestCase in the hieararchy it means that this test is intended to be run with JUnit 3 or some things like setup and teardown methods could not work as expected  It sounds like the real problem may be that you are trying to use junit 4 with a grails version less than 1.3. Grails 1.2.x and lower only support Junit 3 tests. Grails 1.3 will finally have junit 4 support. This was discussed on stackoverflow link text It could have been that but I am using Grails-1.3-RC2.  'com.intellij.junit3' package belongs to IDEA binaries not to junit3 or junit4. So the question itself seems to be incorrect in essence - there is no difference in what package name is used by IDEA codebase internally if it correctly executes the tests. Right and Wrong. com.intellij.junit3' package belongs to IDEA binaries BUT this is the library used for running JUnit test. So it means that if you are using some JUnit 4 features (like @Test annotations) it will not run correctly. Try it yourself. Also the question is absolutely correct. It is asking about ""how to configure IntelliJ for running JUnit 4 tests"" and not what is the junit package to use !!  I found it! Go to Run/Debug Configurations Add new configuration and choose a JUnit In the configuration tab add ""-junit4"" to the Test run parameters input field And that's done ! 10.5 no longer appears to have a ""test run parameters"" input field. Yep. No such field.  I had the same problem with a java app inside 10.5 and it turned out to be my Project language level set to 5.0 as opposed to 8.0. To change this go to File->Project Structure->Project->Project language level And change this to the required level. Not sure at which level you can use JUnit4 but setting this to 5.0 will make it use JUnit3. Setting it to 8.0 makes it use JUnit4"
400,A,"Does XMLUnit have an assert to ignore whitespace I want to compare two xml strings in a test but the test keeps failing due to whitespace. @Test public void testForEquality() throws Exception { String myControlXML = ""<msg><uuid>0x00435A8C</uuid></msg>""; String myTestXML = ""<msg><uuid>0x00435A8C</uuid> </msg>""; assertXMLEqual(myControlXML myTestXML); Diff diff = new Diff(myControlXML myTestXML); assertTrue(diff.similar()); } Guess I should have looked for 5 more minutes XMLUnit.setIgnoreWhitespace(true); Yes XMLUnit can ignore whitespaces. See API documentation for details. You can enable it by setting: XMLUnit.setIgnoreWhitespace(true) Ya I found that a few minutes after I posted but it wouldn't let me close the issue. You probably know this now but the standard SO thing to do at that point is just answer your own question."
401,A,"Unit-testing servlets I have a bunch of servlets running under the Tomcat servlet container. I would like to separate test code from production code so I considered using a test framework. JUnit is nicely integrated into Eclipse but I failed to make it run servlets using a running Tomcat server. Could you please recommend a unit testing framework that supports testing Tomcat servlets? Eclipse integration is nice but not necessary. I use jUnit to launch a Jetty servlet container Before and turn if off After. The Spring Framework has nice ready made mock objects for several classes out of the Servlet API: http://static.springframework.org/spring/docs/2.5.x/api/org/springframework/mock/web/package-summary.html  Check out ServletUnit which is part of HttpUnit. In a nutshell ServletUnit provides a library of mocks and utilities you can use in ordinary JUnit tests to mock out a servlet container and other servlet-related objects like request and response objects. The link above contains examples. The HttpUnit project seems to have had no activity since 2008-05. Perhaps it is dead?  Separate the parts of that code that deal with HTTP requests and response from the parts that do business logic or data-base manipulation. In most cases this will produce a three tier architecture with a data-layer (for the data-base/persistence) service-layer (for the business logic) and a presentation-layer (for the HTTP requests and responses). You can unit test the first two layers without any servlet stuff at all; it will be easier to test that way. You can test the presentation layer as others suggest using mock HTTP request and response objects. Finally if you feel it really is necessary you can do integration tests using a too such as HtmlUnit or JWebUnit .  If you want a newer alternative to ServletUnit for JUnit testing of Servlets you might find my company's ObMimic library useful. It's available for free from the website's downloads page. As with ServletUnit it provides a library of classes that you can use in normal JUnit or TestNG tests outside of any servlet container to simulate the Servlet API. Its Servlet API objects have no-argument constructors are fully configurable and inspectable for all relevant Servlet API data and settings and provide a complete simulation of all of the behaviour specified by the Servlet API's javadoc. To help with testing there's support for selective recording of Servlet API calls control over any container-dependent behaviour checks for any ambiguous calls (i.e. where the Servlet API behavour isn't fully defined) and an in-memory JNDI simulation for any servlet code that relies on JNDI lookups. For full details example code ""how to"" guides Javadoc etc see the website.  Okay. Ignoring the 'tomcat' bit and coding to the servlet your best bet is to create mocks for the response and request objects and then tell it what you expect out of it. So for a standard empty doPost and using EasyMock you'll have public void testPost() { mockRequest = createMock(HttpServletRequest.class); mockResponse = createMock(HttpServletResponse.class); replay(mockRequest mockResponse); myServlet.doPost(mockRequest mockResponse); verify(mockRequest mockResponse); } Then start adding code to the doPost. The mocks will fail because they have no expectations and then you can set up the expectations from there. Note that if you want to use EasyMock with classes you'll have to use the EasyMock class extension library. But it'll work the same way from then on.  For ""in-container"" testing have a look at Cactus If you want to be able to test without a running container you can either simulate its components with your own mockobjects (e.g. with EasyMock) or you could try MockRunner which has ""pre-defined"" Stubs for testing servlets jdbc-connections etc. ""2011/08/05 - Jakarta Cactus has been retired."""
402,A,"Start Java main in another process (VM) I have a server client project and for testing purposes I want to start the server in a whole new process. The problem is I have just a main() method in project no jar. So my guess would be something like Runtime.getRuntime().exec(""javac MyServer.java""); Runtime.getRuntime().exec(""java class MyServer""); But I am really not sure about it and also I don't exactly like the need to start javac for each test case. How should I proceed? EDIT I want to start the process in the @Before method and destroy it in @After. It has to run automatically so manual turning on the server is not an option. What I was looking for is a way to eliminate compilation of the server class. But now I guess there is no other way. Java allows you to make system calls like so.  Runtime r = Runtime.getRuntime(); Process p = r.exec(""java otherMain arg0 arg1""); This would allow you to start another java process. Process also has methods to get the output of that process if you need it. Which is pretty much what I have written in the question minus the need to compile the class or is it?  If this is for testing purposes just launch the other process from the command line or use Eclipse to take care of it. In Eclipse the same project can have multiple main() entry points. When you wish to run the app you create a run / debug configuration that says which entry point you wish to invoke. So you could define one for the client and one for the server and run them with a button click. Expanded: Prerequisite - import your project into Eclipse first before doing any of this. Run Eclipse from the Java perspective (which is normally the case for a Java project) You will see two toolbar buttons marked Debug As... and Run As.... I will describe the Debug button from now on but the same principle applies to Run As.. Next to the Debug As... button is a drop down button. Click it and from the drop down choose Debug Configurations... A configuration dialog will open. In the Dialog double click on ""Java Application"". Eclipse will create a new debug configuration for debugging a Java application. Use the fields on the right to choose which Eclipse project you are debugging the main class (i.e. the one with the static main you wish to invoke) and any other args you want to set. You can give your configuration a meaningful name and apply the changes. Create two configurations one for your client and one for your server e.g. ""debug client"" and ""debug server"" Exit the dialog Now the Debug As... drop down contains two actions with your new configurations. You can click them to launch your apps in debug mode. You will observe that each instance is running in a separate java.exe process. Eclipse also has a debug perspective where you can stop / pause running processes set breakpoints and whatnot. It can even debug more than one thing simultaneously so you could launch both client and server in debug and set breakpoints either side of the call. Ok so assuming this would start a new JVM (which I don't exacly believe but don't know) I am really new to exclipse and can't find in run configurations what you are talking about. Can you please direct me a bit more? I've updated my response with steps for how to debug in Eclipse. Eclipse can spawn as many instances of Java as you would like it to. It can even spawn J2EE app servers and even allows you to specify which JRE you wish your app to run against but those are more advanced topics. I see now what you mean but that's not exactly what I am looking for. I want to start a new server for each test and then kill it. This way I would die clicking. However thanks anyway (vote up).  Take a look at JUnit what I think you want to do is run a series of tests against your client with a server running. You can accomplish this by writing a JUnit test which makes use of the @Before annotation to run a setup method before each test. For example: import org.junit.Before public class TestClient { private static final int port = 20000 + new Random().nextInt(10000); private MyServer server; @Before public void setup() { // setup server server = new MyServer(port); server.start(); } @Test public void testX() { // test x } @Test public void testY() { // test y } } You are correct I will update the question but in the Before method I want to start a new virtual machine with the server. That's where the question is going Why do you need a new VM? Are you absolutely sure thats necessary? My boss demands it :) It is necessary. In that case you can go with your original idea of running java via `Runtime.exec` - there's nothing stopping you from calling that in a @Before method. As long as your `MyServer` class is in the same project you won't need to re-compile it as all your code should be compiled before running tests."
403,A,"Why Does This Groovy MetaClass Statement Work with Sql class? Why does this line of unit test code work? groovy.sql.Sql doesn't have a no argument constructor. Sql.metaClass.constructor = { dataSource -> return new Sql(); } That line is amongst some others in a grails app which mocks out a Sql object's constructor and one of its methods. It works great. Looking at the API for the Sql object I do not see a no argument constructor: http://groovy.codehaus.org/api/groovy/sql/Sql.html This style of overriding the constructor using Sql.metaClass.constructor is something I found at: http://manuel-palacio.blogspot.com/2010/07/groovy-tip-metaprogramming-1.html Thanks! groovy.sql.Sql has no public no-args constructor but as can be seen in the source it does have a private no-args constructor -- I guess in order to support the syntax new Sql(connection: connection)?. I'm kind of surprised though that that technique for stubbing doesn't generate an exception e.g. when running sql.execute or the like. Nice! Thank you for the convenient link to the source too! Instead of changing the constructor I found I can set the DataSource into the service. Implementing the DataSource interface is easy and small using the ""as DataSource"". See http://groovy.codehaus.org/Groovy+way+to+implement+interfaces where it says ""as X"" Groovy 1.7 also supports anonymous inner classes. Work best with a recent release though (e.g. 1.7.6)."
404,A,"How to get Junit 4 to ignore a Base Test Class? I have a base class for many tests that has some helper methods they all need. It does not by itself have any tests on it but JUnit (in eclipse) is invoking the test runner on it and complaining that there are no methods to test. How can I make it ignore this class? I know I could add a dummyTest method that would solve the problem but it would also appear for all the children classes. Suggestions? Is BaseTest abstract? It is and still gets run See similar question: http://stackoverflow.com/questions/672466/junit-how-to-avoid-no-runnable-methods-in-test-utils-classes @duffymo Thanks. That did it for me. i just came to know that incase we write a testcase then there should be atleast 1 @Test method inside the testcase and its mandotory. Other wise it would give us initialization error. Is it true? Use to @Ignore annotation. It also works on classes. See this one: @Ignore public class IgnoreMe { @Test public void test1() { ... } @Test public void test2() { ... } } Also you can annotate a class containing test methods with @Ignore and none of the containing tests will be executed. Source: JUnit JavaDoc I agree that this should work so +1. Except with netbeans 6.2 and jUnit 4.5 @ignore is listed as only being valid for Methods. Looking it up and Libary's called 4.5 but the jar is junit4-1.jar weird. Accepted your answer. Thanks. Can you point me to valid reference where it says @Ignore is valid for methods only from junit 4.5? I can't seem to find it.  Just as a note I'd always recommend giving a reason for the ignore: @Ignore(""This test will prove bug #123 is fixed once someone fixes it"") I'm hoping the junit xml report formatter used when running tests from ant will one day include the ignored count (and the reasons) along with pass fail and error. I'm looking forward to that too now. :) thanks  Annotate it with @Ignore and make it abstract (http://stackoverflow.com/a/3016401/34088). This also communicates your intent that this is just a fragment of a test. It also prevents JUnit from counting the tests in the class as ""ignored"" (so the final count of ignored tests will be what you expect). But the underlying issue is probably that your test runner shouldn't try to run this class. Most of the runners use the class name to determine which classes are tests and which are helpers. In Maven with the default settings you just have to make sure the base class doesn't begin or end with Test and doesn't end with TestCase (http://maven.apache.org/surefire/maven-surefire-plugin/examples/inclusion-exclusion.html) The combination of @Ignore and abstract is excellet!"
405,A,Subclassing a test subject for Junit testing I want to test validation logic in a legacy class. The class uses a method to load effective dates from a config file. I have written a subclass of the class in question and overridden the config method so I can run my unit test against the subclass with any combination of effective dates. Is this an appropriate strategy? It strikes me as a clean technique for testing code that you don't want to mess with. It seems to be an appropriate strategy to me. Ofcourse with this override you won't be able to test the code (in the original class) that loads the config data but if you have other tests to cover this sceario then I think the approach you outlined is fine.  I like it its the most simple and straight forward way to get this done. And since it is a legacy class it will not change anymore so you don't run danger of bumping into the fragile base class problem neither.
406,A,Automated testing in Android development I have an ordinary project with JUnit tests that are connected to the classes in my Android Project. I want my server to run some JUnit tests in my testproject everytime I commit my code from my Android Project. Is there a best practise to do this? So far I only managed to run the tests when they are a part of a while the JUnit tests and Android classes are separated into 2 different projects since JUnit runs on JVM and Android in an emulator on DVM (Dalvik Virtual Machine). You can use Hudson to achieve this. I've written some articles describing this for example Android Continuous Integration: Build with Maven. To be able to run the tests some emulator instances fulfilling your project's requirement must be running on the CI server preferably in headless mode. Changing the Android emulator locale automatically and How to Get Serial Number or ID of Android Emulator After it Runs? could also come in handy.
407,A,Why do you use .proxy() in JMock? I'm trying to learn how to use JMock and I'm a bit confused about what you use the .proxy() method for. I know its invoked on your Mock class but I don't understand what its puprose is. I haven't had any luck finding a good description about how it works on google. Any help is much appreciated. I strongly recommend upgrading to jMock2 we don't support jMock 1 any more  Proxy is only used in jmock1 if you are learning jmock use jmock2 and follow this cheat here http://www.jmock.org/cheat-sheet.html If you want to read what proxying does look at the jmock1 docs here http://www.jmock.org/jmock1-getting-started.html Hope this helps
408,A,"Selenium running as junit in ant prematurely closes browser but not in IDE I've got a bunch of selenium test cases set up in a JUnit class as four methods. The first runs fine - but the remaining three close the Firefox browser before the final step of the method is complete - giving a ERROR: Command execution failure. Please search the forum at http://clearspace.openqa.org for error details from the log window. The error message is: this.page().currentDocument is undefined com.thoughtworks.selenium.SeleniumException: ERROR: Command execution failure. Please search the forum at http://clearspace.openqa.org for error details from the log window. The error message is: this.page().currentDocument is undefined Setup is following: Ant based execution Java 1.5 Eclipse 3.5 Selenium Server 1.0.1 Selenium Client Driver 1.0.1 IDE Java 1.5 Eclipse 3.5 Selenium Server 1.0.1 Selenium Client Driver 1.0.1 I'm hoping to track down the root cause It is crashing on the following line: This is the modification to the user-extensions.js file: Selenium.prototype.getElementBody = function(elementId) { return this.outerHTML(this.page().currentDocument.getElementById(elementId)); }; This is what is being called: commandProcessor.getString(""getElementBody"" ""idOfElement""); Can you give us code so we can see what the issue may be? Can you show us your JUnit test class ? I know from personnal experience that once in a while Selenium tests can crash for no apparent reason but your problem seems to be happening each time so it might have something to do with your code/script. Afraid I can't help you without some more details.  I have never seen a command called getString part of the Selenium API so think that is why your tests are failing. The other thing is have you told Selenium RC to use the user-extensions.js file that you have modified with the following -userExtensions user-extensions.js argument? I have never found it beneficial to use user-extensions in Se:RC and instead use .getEval() commands. what you want would be selenium.getEval(""this.browserbot.getUserWindow().getElementByID(id).outerHTML""); I have cleaned up your JavaScript for you. The this.browserbot.getUserWindow() gives you access to the Selenium JavaScript object which has the page in it. I hope that helps"
409,A,"I've injected HttpServletRequest into a bean. How do I unit test it? I have a bean in which I've injected an HttpServletRequest using the @Autowired annotation. This injection works correctly when the application context is a web application Context. That's not the case for application contexts for JUnit tests with Spring. How can I test this bean ? Maybe I can mock an http request but then how to inject this mock in the bean ? This is on Spring 3.0 and Junit 4.4 Create a bean of type MockHttpServletRequest and add it to your test context. This should then be autowired into your target bean. Thanks but how to ""add it to your test context""? [Here](http://forum.springsource.org/showthread.php?69604-Adding-beans-to-application-context) guys haven't found a solution."
410,A,"JUnit NullPointerException error in Android's ""Hello Testing"" tutorial I followed Android's Hello Testing tutorial verbatim yet when I run it I receive an error with the following Failure Trace: java.lang.NullPointerException at com.example.helloandroid.test.HelloAndroidTest.testText(HelloAndroidTest.java:72) at java.lang.reflect.Method.invokeNative(Native Method) at android.test.InstrumentationTestCase.runMethod(InstrumentationTestCase.java:204) at android.test.InstrumentationTestCase.runTest(InstrumentationTestCase.java:194) at android.test.ActivityInstrumentationTestCase2.runTest(ActivityInstrumentationTestCase2.java:186) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:169) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:154) at android.test.InstrumentationTestRunner.onStart(InstrumentationTestRunner.java:520) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1447) The exception occurs in testText()'s single statement:  public void testText() { assertEquals(resourceString(String)mView.getText()); } I don't understand why I am receiving this null pointer exception since both resourceString and mView are initialized in onCreate():  @Override protected void setUp() throws Exception { super.setUp(); mActivity = this.getActivity(); mView = (TextView) mActivity.findViewById(com.example.helloandroid.R.id.textview); resourceString = mActivity.getString(com.example.helloandroid.R.string.hello); } What could explain this in such a simple app? I fix it adding the @UiThreadTest to the test like this: @UiThreadTest public void testText() { assertEquals(resourceString(String)mView.getText()); } I hope it helps.  Answering my own question: The Hello Testing tutorial is made for the XML version of ""Hello Android"" not for the second dynamically constructed version. I fixed the problem by modifying HelloAndroid's onCreate():  if (false) { // don't use main.xml layout TextView tv = new TextView(this); tv.setText(""Hello Android""); setContentView(tv); } else setContentView(R.layout.main);  I am not positive but I think the way you are trying to cast as a String might be part of the problem. Try mView.getText().toString() instead of casting it how you are now. public void testText() { assertEquals(resourceString(String)mView.getText()); }"
411,A,"Running the same JUnit test case multiple time with different data Is there there any way to tell JUnit to run a specific test case multiple times with different data continuously before going on to the next test case? By using theories (as pointed out by @dfa) Junit will treat all the tests as one test & will make it harder to figure out which test has failed. Whereas Parameterized Tests (as pointed out by@jjnguy) treat them all as different tests clearly indicating which test failed. recently i started zohhak project. it lets you write: @TestWith({ ""25 USD 7"" ""38 GBP 2"" ""null 0"" }) public void testMethod(Money money int anotherParameter) { ... }  It sounds like that is a perfect candidate for parametrized tests. But basically parametrized tests allow you to run the same set of tests on different data. Here are some good blog posts about it: Writing a parameterized JUnit test Unit Testing with JUnit 4.0.  Here is a post I wrote that shows several ways of running the tests repeatedly with code examples: Run a JUnit test repeatedly You can use the @Parametrized runner or use the special runner included in the post  I always just make a helper method that executes the test based on the parameters and then call that method from the JUnit test method. Normally this would mean a single JUnit test method would actually execute lots of tests but that wasn't a problem for me. If you wanted multiple test methods one for each distinct invocation I'd recommend generating the test class.  If you don't want or can't use custom runner (eg. you are already using an other runner like Robolectric runner) you can try this DataSet Rule.  take a look to junit 4.4 theories: import org.junit.Test; import org.junit.experimental.theories.*; import org.junit.runner.RunWith; @RunWith(Theories.class) public class PrimeTest { @Theory public void isPrime(int candidate) { // called with candidate=1 candidate=2 etc etc } public static @DataPoints int[] candidates = {1 2 3 4 5}; } I have few test cases which follows the format like testxyz() testpqr() .my existing test class extends TestCase. Thos are not running if i am folowing this format. first you must convert them to JUnit 4 (by simply using @Test annotation in most cases) simply great that works.In this case you had suggested me to use certain annotations.How do i know what annotations are there and what to use with respect to junit? http://www.cavdar.net/2008/07/21/junit-4-in-60-seconds/ a nice intro What if I wanted this to act like a set of different tests so that I may see which data point makes the test fail? (i.e. in your example I would have 4 passing tests and 1 failing instead of a single failed test)"
412,A,Unit testing a Java constructor that exits the application Duplicate: Java: How to test methods that call System.exit()? I am having a bit of trouble designing a unit test for a method that exits the application by calling system.exit(). Actually this is the constructor of a class which tests some conditions and decides to exit the application. So it is this particular eventuality that I'd like to test. Is there a particular assert that I could use or any other suggestions? public MyClass(arg1 arg2 arg3){ if(argsTestingIsOK){ continue; }else{ System.exit(0); } } Instead of exit()-ing in the constructor throw an IllegalArgumentException instead (since that's what's really happening) and leave it to the caller to handle the exception. The application code can be written to process the exception while the JUnit test can assert that the exception occurs. thanks Mike that's a great suggestion actually! Will try that now!  Don't do this in a constructor. It's a bad idea and it's misleading to anyone using your code. The best practice is to only use something like System.exit() in a main method or in the entry point to your application - definitely not in the middle of object construction code.
413,A,How to make JUnit behave the same as the Java running from command line I noticed that when a unit test exits all the threads spawned are automatically killed this is quite different from the java program running from command line. In the command line the program does not exit until all the non deamon threads exit. How to test the case involving spawned thread using jUnit? @Dave thanks. Could you add those comments to answers? So you have a test case that spawns a thread and needs to wait for it to finish in order to check its results? If that's the case can't you get the test case to wait for the thread to finish by using join? And could you refactor the code so that the functionality could be tested without it being in a separate thread? The method I need to test spawn a thread. If the method you are testing returns a reference to the thread is creates then join on that. If it doesn't create your own thread to call the method under test and join on that. If the method you are testing returns a reference to the thread it creates then join on that. If it doesn't create your own thread to call the method under test and join on that.  I would suggest you make your unit test shutdown() its components such that it returns to the state before the test started. This may not be actually required in the running system but it allows you to run multiple tests many times. You may even like to take a snapshot of the running threads at the start and check after the test that there are no new threads (there may be some expected ones which are okay)
414,A,junit test case generator Is there a good tool out there to automatically generate jUnit test cases based on some primitive template? This is so that test cases can be written by engineers who do not have a lot of Java or jUnit background. As background information this is for black box testing. If there is some other alternative to run regression tests than using jUnit I would also appreciate hearing about it. Thx Another alternative could be to create a higher level domain specific language that makes sense to the engineers for them to code their tests in. Groovy is an easy way to do that (google groovy & DSL) or at the other end of the spectrum use JavaCC.  Here is a typical tool: http://mediakey.dk/~cc/generate-junit-tests/ TestGen4J is a collection of open-source tools that automatically generates unit test cases. TestGen4J automatically generates JUnit test cases from your own Java class files or source files. Its primary focus is to exercise boundary value testing of the arguments passed to the method. It uses rules written in a user-configurable XML file that defines boundary conditions for the data types. The test code is separated from test data with the help of JTestCase. The test code is generated in a hierarchical manner. A main test suite is generated which invokes test suites of individual classes. The individual class test suite is formed by collection of test methods of that class. The test data is also organized hierarchically corresponding to the structure of the test code in XML format. This XML file actually has data for all unit test cases for each method. JTestCase helps to loop through all the test cases of each method and executing one by one against JUnit. source code will not be available to parse through to generate test cases.  Have you looked at fit? Fit lets you make an html table and then uses those values in your junit tests turning the table elements red or green depending on the results of the test. It comes packaged with JUnit. You do need to wire up the fixture to translate table emenents into java but there's support for that. There a number of good resources floating around.  Parasoft's JTest is a commercial tool but it is quite good for: generating unit tests for an existing codebase creating regression tests For a free solution you can try the JUnit generation functionality of the CodePro Analytix Eclipse plugin.
415,A,"Webdriver - Opening a new browser with the same profile I am running some java tests for a web application with Webdriver on Firefox trying to verify the functionality of a ""Keep me logged in"" button. Each time a new browser is launched it is done so with a new profile. Is there a way to instruct it to open the new browser with the same profile as before the one that logged in and pressed on ""keep me logged in""? I'm sorry if the question doesn't make sense I'm still a bit new at this. Thanks Ragnar First of all like Jarib said it is possible to use an existing profile as a model. It means no specific data like favorites history homepage will be really loaded. But there is a possibility to set some special sytem properties to driver. webdriver.reap_profile Should be “true” if temporary files and profiles should not be deleted System.setProperty(""webdriver.reap_profile"" ""true""); Most probably there is a possibility to use that kind of solution in your situation too.  You can not do that with obtaining a new WebDriver instance. but you can do that with JavaScript as below ((JavascriptExecutor)webDriver).executeScript(""window.open('""+ConfigLocator.getTargetServer()+""' '_blank');"");  This isn't currently possible. WebDriver will use a fresh profile every time you launch the browser (although you can use an existing profile as a model). You can file a feature request in the Selenium tracker  You can use FirefoxProfile with an existing profile: FirefoxProfile profile = new FirefoxProfile(path to profile dir); WebDriver driver = new FirefoxDriver(profile); Thank you but that does not help. I absolutely need to use a fresh profile to login and click the ""Keep me logged in"" button each time and then the same profile to load a new page. @Ragnar - this last comment conflicts with what you asked in your original question. in your original question you ask if there is a way to reuse profiles and then in this comment you say ""i absolutely need a fresh profile to login"". So what is it? What do you really want? To me this answer by ZloiAdum seems to me to be the correct answer. You could also copy a fresh profile and then specifically refer to it if you wanted to. @djangofan: It's been a couple of years since I asked this question but I think I wasn't clear enough with that comment. If I remember correctly what I needed was to use a new profile each time I ran the ""Keep Me Logged In"" test i.e. New Profile clicks on Keep Me Logged In login close the browser relaunch the browser verify that Keep Me Logged In works correctly close the browser and discard the profile. Rinse & repeat. Using an existing profile would have conflicted with the testing requirements of that project."
416,A,"AbstractTransactionalJUnit4SpringContextTests I am using AbstractTransactionalJUnit4SpringContextTests as super class for my test classes. but ""mvn test"" runs out of memory. Here is a snippet my BaseTest class : @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations= {""classpath:**/applicationContext.xml"" ""classpath:**/simple-test-datasource.xml""}) @TransactionConfiguration( transactionManager=""myService.transactionManager""defaultRollback=true) public class ServiceBaseTest extends AbstractTransactionalJUnit4SpringContextTests{ ---- } Am i missing something like @Before @After or @BeforeClass @AfterClass that is making these tests really heavy and hence I am running out of memory? Please help! Thanks! Here is the stack trace: java.lang.OutOfMemoryError: PermGen space at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632) at java.lang.ClassLoader.defineClass(ClassLoader.java:616) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141) at java.net.URLClassLoader.defineClass(URLClassLoader.java:283) at java.net.URLClassLoader.access$000(URLClassLoader.java:58) at java.net.URLClassLoader$1.run(URLClassLoader.java:197) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at sun.misc.Launcher$ExtClassLoader.findClass(Launcher.java:229) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) at com.apple.crypto.provider.Apple.doSelfVerification(Apple.java:271) at com.apple.crypto.provider.Apple.verifySelfIntegrity(Apple.java:192) at com.apple.crypto.provider.HmacCore.<init>(HmacCore.java:45) at com.apple.crypto.provider.HmacSHA1.<init>(HmacSHA1.java:30) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at java.lang.Class.newInstance0(Class.java:355) at java.lang.Class.newInstance(Class.java:308) at java.security.Provider$Service.newInstance(Provider.java:1221) at javax.crypto.Mac.a(DashoA13*..) at javax.crypto.Mac.init(DashoA13*..) at org.springframework.security.oauth.common.signature.HMAC_SHA1SignatureMethod.verify(HMAC_SHA1SignatureMethod.java:122) at org.springframework.security.oauth.provider.OAuthProviderProcessingFilter.validateSignature(OAuthProviderProcessingFilter.java:261) at org.springframework.security.oauth.provider.OAuthProviderProcessingFilter.doFilter(OAuthProviderProcessingFilter.java:146) at org.springframework.security.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:390) at org.springframework.security.oauth.provider.OAuthProviderProcessingFilter.doFilter(OAuthProviderProcessingFilter.java:193) at org.springframework.security.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:390) at org.springframework.security.ui.AbstractProcessingFilter.doFilterHttp(AbstractProcessingFilter.java:277) The stack trace is about the perm gen memory and not the heap. So please try to increase the per gen memory e.g -XX:MaxPermSize=1024M  You appear to be loading the entire Spring application context into memory via the ContextConfiguration annotation. This could use up a lot of memory depending on what beans you have defined. Try increasing the memory available to the JVM by setting M2_OPTS=-Xmx256m in your environment. Can you post the stack trace you're getting? I suspect the options aren't getting passed to the JVM. Use jconsole to check the running Java process and look at the PermGen space - in particular check the max is 512m as you've specified. -Xms512m -Xmx1024m -XX:MaxPermSize=512m Even this is not working for me. Still getting out of memory. Can you post the stack trace you get that has the out of memory exception in it? @artbristol- I have added the stack in the question. I fixed it with M2_OPTS=-Xmx256m there was some DEV environment issue thats why it was not getting picked up earlier. I appreciate your help artbristol. I have already tried this but still having problem."
417,A,"Testing validation constraints The Hibernate Validator documentation has a simple Getting Started guide which outlines testing validation rules. The relevant chunk is @Test public void manufacturerIsNull() { Car car = new Car(null ""DD-AB-123"" 4); Set<ConstraintViolation<Car>> constraintViolations = validator.validate(car); assertEquals(1 constraintViolations.size()); assertEquals(""may not be null"" constraintViolations.iterator().next().getMessage()); } It seems to me a fairly vague route to test if the NotNull constraint has been violated. My simple solution would be something like public static <T> boolean containsConstraintViolation( Set<ConstraintViolation<T>> violations Class<?> constraint) { for (ConstraintViolation<?> violation : violations) { ConstraintDescriptor<?> descriptor = violation.getConstraintDescriptor(); if (constraint.isAssignableFrom(descriptor.getAnnotation().getClass())) return true; } return false; } Which allows me to do tests like assertTrue(ValidationUtils.containsConstraintViolation(violations NotNull.class)); However I'm sure this is going to be naive in the long run and am wondering if there's not some other library or API I'm missing to assist in unit testing constraints. Testing the bean validation against a concrete constraint class is a bad idea because will tightly couple the test with the implementation. So instead of this: assertTrue(ValidationUtils.containsConstraintViolation(violations NotNull.class)); You could test the outcome of the validation to be that of what you expect: assertThat(validationFor(car onField(""manufacturer"")) fails()); So you tests will become like this @Test public void car_with_null_manufacturer_is_invalid() { Car car = new Car(null ""DD-AB-123"" 4); assertThat(validationFor(car onField(""manufacturer"")) fails()); } The utility class that provides the matchers is public class HibernateValidationUtils { private static Validator VALIDATOR; static { ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); VALIDATOR = factory.getValidator(); } public static Set<ConstraintViolation<Object>> validationFor(Object object String fieldname) { return VALIDATOR.validateProperty(object fieldname); } public static String onField(String fieldname) { return fieldname; } public static Matcher<Set<ConstraintViolation<Object>>> succedes() { return new PassesValidation(); } public static Matcher<Set<ConstraintViolation<Object>>> fails() { return new Not(new PassesValidation()); } static class PassesValidation extends BaseMatcher<Set<ConstraintViolation<Object>>> { @Override public boolean matches(Object o) { boolean result = false; if (o instanceof Set) { result = ((Set) o).isEmpty(); } return result; } @Override public void describeTo(Description description) { description.appendText(""valid""); } } }  You could have a look at the class org.hibernate.validator.test.util.TestUtil which is used for Hibernate Validator's own tests and offers functionality for testing expected constraint violations (amongst others there is assertCorrectConstraintTypes() for example). Just note that this class is not part of Hibernate Validator's public API so it may be best to just use it to get some ideas. When comparing error messages always be sure to have the VM's locale correctly set. Even better is to match against localized messages loaded via the correct resource bundle (org.hibernate.validator.ValidationMessages for the standard constraints in the case of Hibernate Validator)."
418,A,"Grails test-app: How can I disable junitreport? I am trying to run some unit tests for my grails app via ""grails test-app -unit"". This works as expected. However when the tests are done junitreport takes way too long to generate the HTML results eg: [junitreport] Transform time: 33294ms 33 seconds (on top of the rest of the time) is ridiculous when I want to rapidly make modifications and re-test. Is there a way I can disable this and just use the plain-text output? It looks like you can do: grails test-app -no-reports It looks like this causes none of the reports to be generated I'm not sure if that works for you or not. Also available is a -xml argument. Perhaps its transformation will be quicker; you might give it a try. I found this out by digging through $GRAILS_HOME/scripts/TestApp.groovy - if you dig through it you might find something better that works for you. I took a look - after a bit of fiddling I discovered that -no-reports still outputs the plain-text reports - perfect! Thanks! +1 great tip I didn't know you could turn off html reports and I never look at them"
419,A,How can I generate an HTML report for Junit results? Is there a way to (easily) generate a HTML report that contains the tests results ? I am currently using JUnit in addition to Selenium for testing web apps UI. PS: Given the project structure I am not supposed to use Ant :( What (build) technology are you currently using? If you don't provide any hints about this there are just too many answers! Alternatively for those using Maven build tool there is a plugin called Surefire Report. The report looks like this : Sample  If you could use Ant then you would just use the JUnitReport task as detailed here: http://ant.apache.org/manual/Tasks/junitreport.html but you mentioned in your question that you're not supposed to use Ant. I believe that task merely transforms the XML report into HTML so it would be feasible to use any XSLT processor to generate a similar report. Alternatively you could switch to using TestNG ( http://testng.org/doc/index.html ) which is very similar to JUnit but has a default HTML report as well as several other cool features. Thanks for your answer! I will try to convince the staff that we need ANT and the whole idea of an IDE-Centric project is bad.
420,A,Run JUnit Test suite from command line How do I run a Junit 4.8.1 Test suite from command line ? Also I want to use the categories introduces with JUnit 4.8  is there a way where I can specify from command line the category which I want to run. I can suggest two approaches: 1. Create Ant file with junit target and then invoke this target from commend line. 2. Implement test suite class in it in some class with main() method. So you will be able to run it.  There is no way (as of 4.8) to specify categories from the command line.  Using java run JUnitCore class (also see here). Categories are supposed to be used with test suites with @RunWith(Categories.class)  @IncludeCategory and @ExcludeCategory. I am not aware of any dynamic way to use categories to run tests but I'd like to know of such it it exists. You can have pre-defined test suites for certain categories to run them. It's possible to create custom test runner that dynamically selects tests to run based on categories. Here's one way to easily run all tests in a suite/category (without explicitly enumerating the tests): http://stackoverflow.com/questions/2176570/how-to-run-all-tests-belonging-to-a-certain-category-in-junit-4/2176791#2176791 (Let me know if this is not what you meant.) Yes I am aware of this (http://stackoverflow.com/questions/3324623/how-can-i-run-all-junit-tests-in-one-package-netbeans/3332970#3332970). I think what author means is running tests based on category as a parameter. The obvious reason to do this is to exclude slow tests (i.e. touches network database or disk) from our 10 minute CI build. This gives organizations that have been misusing JUnit a way to filter out that misuse.
421,A,"ant junit task does not report detail I tried to write an ant with junit test but get below result unittest: [junit] Running com.mytest.utiltest [junit] Tests run: 1 Failures: 0 Errors: 1 Time elapsed: 0 sec [junit] Test com.mytest.utiltest FAILED it just shows error without print details i specify below parameter in build.xml also tried to start with ant -v or ant -debug but did not get any luck. Can anyone help? <junit printsummary=""yes"" showoutput=""true""> ant 1.8.2 sun jdk1.6.0_20 junit 4.8.2 to narrow down the problem i created a seperate project this is my build.xml <project name = ""TestPrj"" default=""unittest"" basedir = "".""> <target name=""unittest"" > <junit printsummary=""yes"" showoutput=""true"" > <classpath> <pathelement location=""./junit-4.8.2.jar""/> <pathelement location=""./ant-junit4.jar""/> </classpath> <test name = ""com.mytest.unittest.SimpleTest"" todir="".""/> </junit> </target> </project> below is simpletest.java package com.mytest.unittest; import junit.framework.TestCase; public class SimpleTest extends TestCase{ public void testFirst() { assertTrue(true); } } C:\TestPrj>ant Buildfile: C:\TestPrj\build.xml unittest: [junit] Running com.mytest.unittest.SimpleTest [junit] Tests run: 1 Failures: 0 Errors: 0 Time elapsed: 0 sec BUILD SUCCESSFUL Total time: 0 seconds C:\TestPrj>dir  Directory of C:\TestPrj 04/02/2011 02:00 PM <DIR> . 04/02/2011 02:00 PM <DIR> .. 04/02/2011 01:56 PM 280 .classpath 04/02/2011 01:54 PM 519 .project 04/02/2011 02:00 PM 7120 ant-junit4.jar 04/02/2011 02:00 PM 403 build.xml 04/02/2011 01:55 PM <DIR> com 11/17/2010 05:36 PM 237344 junit-4.8.2.jar 5 File(s) 245666 bytes 3 Dir(s) 28451311616 bytes free my real question is why there's not a junit results/detail/report generated? so that in my real case of failure i can not troubleshoot my questions? thx very much You have to use a formatter element inside the junit task. The formatter will create a report file by default but you can force it to print the results on screen. You can use two formatters: one for output to screen another for output to file. Note that you no longer need the attributes printsummary=""yes"" and showoutput=""true"" in the junit task. The formatter is taking care of output now. <project name = ""TestPrj"" default=""unittest"" basedir = "".""> <target name=""unittest"" > <junit> <classpath> <pathelement location=""./junit-4.8.2.jar""/> <pathelement location=""./ant-junit4.jar""/> </classpath> <formatter type=""plain"" usefile=""false"" /> <!-- to screen --> <formatter type=""plain"" /> <!-- to file --> <test name = ""com.mytest.unittest.SimpleTest"" todir="".""/> </junit> </target> </project> Read the junit page in the ant manual for more information. This works but it outputs the results to the screen once all the tests are done running. I would like to see them running as they go so that I can monitor progress. Any way that you know of to do that? see here: http://stackoverflow.com/questions/18293054/immediate-junit-test-logging-with-junit-ant-task  Could you post the snippet which calls junit from build.xml here? There can be several reasons why the build fails. Also please post the testcase which you are trying to test. EDIT: Do you need a static for a test? public class utilTest extends TestCase { public void testfun() { assertTrue(true); } } EDIT: Try using the argument outfile <target name=""unittest""> <junit printsummary=""yes"" showoutput=""true""> <classpath> <pathelement location=""./junit-4.8.2.jar""/> <pathelement location=""./ant-junit4.jar""/> </classpath> <formatter type=""plain"" /> <test name=""com.mytest.unittest.SimpleTest"" outfile=""./testresult"" /> </junit> <fail message=""test failed"" if=""test.failure"" /> </target> public class utilTest extends TestCase {public static void testfun(assertTrue(true);)} Please remove the static modifier and let me know if it works fine. thx for your help unluckily still got errors. meanwhile my question is more about: why isn't there a report in current folder? as from ""test"" of ant junit task by default it should generate a report which is expected to see a report but i did not see it.. Could you please post the entire contents of the build file? You could post it by editing your question. Unless I see the entire file I cannot say why the build fails since you have printsummary and showoutput set to true you should be able to see the report in the folder. Please check the folder from where you are running the command. But otherwise I cannot think of any reason why it should fail probably I need more information on the contents of the build file. thx i've listed my detail there with build.xml/ant/dir but SO's edit box seems to loose the formatespecially the return (not WYSIWYG?) Thats okay but you have not yet given me the code you have written in the build.xml file. I could see only the output generated by the ant task on the command line. now i've formatted it there and i've figured out i should add it will generate xml; html report is available from junitreport Let me know if the above thing works. well the key thing is add and it has worked. thx for your really kind help:) i'm a bit surprised that SO users like u really like to help others in such a short time and continuous effort. amazing! Thnx good that you could find the solution to your problem!"
422,A,"Where can I find good unit testing resources for EJB and J2EE? Which online resources tutorials or books can you recommended to get started with unit testing J2EE / EJB3 applications? So far I have found ejb3unit Jakarta Cactus (retired 2011/08) and the Maven Cargo plugin. It would be helpful if there are complete working examples ready to run. Target containers are the open source products GlassFish JBoss and Apache OpenEJB. Yes! Both!! The more the merrier ... :) Do you really mean unit testing or integration/functional testing? JSFUnit is ""a test framework for JSF applications. It is designed to allow complete integration testing and unit testing of JSF applications using a simplified API. JSFUnit tests run inside the container which provides the developer full access to managed beans the FacesContext EL Expressions and the internal JSF component tree. At the same time you also have access to parsed HTML output of each client request.""  The next version NetBeans 6.8 includes a nice new feature: it generates Unit-Tests for EJB 3.1 with Embeddable Container code. @Test public void testHello() throws Exception { System.out.println(""hello""); HelloService instance = (HelloService)javax.ejb.embeddable.EJBContainer.createEJBContainer().getContext().lookup(""java:global/classes/HelloService""); String expResult = """"; String result = instance.hello(); assertEquals(expResult result); // TODO review the generated test code and remove the default call to fail. fail(""The test case is a prototype.""); } +1 For using the embedded API (and GlassFish!). See http://blogs.sun.com/alexismp/entry/testing_ejb_3_1_s  There is nice book on TDD written by Lasse Koskela It has a free to download web extra based on EJB testing.Grab it and roll up your sleeves.  EJB Unit Testing with Eclipse and OpenEJB Article on Testing EJB Glassfish EJB 3.0 Unit testing JUnitEE Tutorial Effective Unit Testing EJB 3.0 with OpenEJB JUnitEE IBM Tutorial  EJB out-of-container testing by Erwann ""Airone"" Wernli"
423,A,"Is Assert-ing on tearDown (@After) method wrong? I have multiple test cases even and if the logic is different the output must be equal on all of them. So I was thinking in how to generalize them and place the Assert method only once. Is there any way better to do it than this one: static public class Tests() { private static String expected = null; private String actual = null; @BeforeClass public static void setUpBeforeClass() throws Exception { expected = new String(""My Desired Output""); } @Before public void setUp() { actual = new String(); } @Test public void test1() throws Exception { actual = ... } @Test public void test2() throws Exception { actual = ... } @After public void tearDown() throws Exception { assertThat(actual is(equalTo(expected))); } @AfterClass public static void tearDownAfterClass() { } } Running method: @Test public void runTests() { Result result = JUnitCore.runClasses(Tests.class); assertThat(result.getRunCount() is(2)); assertThat(result.getFailureCount() is(0)); } Yes asserting in the tearDown method is a bad idea. This method exists according to the JUnit documentation to Tears down the fixture for example close a network connection. This method is called after a test is executed. I think that storing your expected and actual values in the test class are a bad idea in general. These variables are test-dependent so store them inside your test case and do your assert in the test case. For example: public class FooTest { @Test public void testFoo() { Object expected = // ... Object actual = // ... assertThat(actual is(equalsTo(expected))); } } Also I see in your code that all test have the same expected value. It might be a good idea to vary your tests so returned values are always different. Testing only one expected value all the time make you sure the code works for this expected result. Try with some more possibly very different and try to test some corner cases. The class I'm testing is kind of a builder class so I can build my output using different methods. Doing these tests I can assure that all my methods are working and the output is generated properly. I already detected a bug in one of my methods. In addition using statics in ur test is a bad idea. @emory you mean asserting against an static variable or using static methods or both? @Alexander: In this case try other methods with other values that will generate a different output. You method can work fine for a given output and be broken for another so testing different values is always a good idea it broadens your tests cases. @Vivien got it. @Alexander if u do as Vivien suggests then this won't be an issue at all. if u have static expected variable then the tests are not really independent of each other. I agreed with the statement that the test cases must be independent from each other.  If you must generalize then you could create one method like private void testIt ( String actual ) { assertThat(actual is(equalTo(expected))); } and call it from all your test methods. If and when a test fails it will be more obvious which test failed."
424,A,"Mocking objects in JUnit tests - best practice? Do you think mocking objects in JUnit test is a bests practice? I don't see the big advantage. Sure if you have a database which should not be considered in your test it makes sense but why isn't just injected an other implementation of that component (if spring is used). An object factory for the tests would make this much easy. I don't have much experience (we are using Mockito) but I've already seen that application code gets modified so that some properties gets mockable! Test cases should never efford such changes in productive code in my oppinion. So what do you think of this topic? In which cases do you are mocking your object or why you don't? I've already seen that application code gets modified so that some properties gets mockable! Test cases should never efford such changes in productive code in my oppinion. The core idea of TDD is that by forcing you to make all your code testable the design in general will become better. This doesn't necessarily mean just to have everything mockable it could also mean reducing coupling so that less mocking is necessary. Even if you don't agree with that philosophy (I don't buy it 100% myself) as long as you believe that automated tests provide value then changing the production code to support that value makes sense (unless it seriously compromises the design in some other way).  Defines calibrating classes interfaces (interface discovery) Allows top to bottom design Isolation (unit testing) Clarifies the interactions between classes Sometimes the only way to see if object does what you want( Mocks and Tell Don’t Ask) Encouraged better structured tests. For example jukito auto-injects mocks to enable you to focus on things you rely want to test. Allows preserving encapsulation Reduces dependencies Value object should not be mocked Mocking frameworks grow-out from necessity. As Matthew Gilliard said if there is some kind of mockery going on then it is sign that design can be improved or lack of test focus. Tests reveals lots of problems in code. but why isn't just injected an other implementation of that component (if spring is used). You have to write implementation. Using mocking framework it is done for you. I've already seen that application code gets modified so that some properties gets mockable! Test cases should never effort such changes in productive code in my opinion. If mockable means testable then it is other way around. For example in TDD test defines production code.  The idea of mocking is that you totally isolate the thing that you are testing. Then when the test fails you can be sure where the problem is without having to hunt through the whole class dependency tree. If you are testing the behaviour of multiple classes together then this is not really unit testing. An object factory for the tests would presumably make objects with stubbed methods and the mocking frameworks are essentially generic object factories for use in tests. But mocks provide a lot more than stubs do - a difference which Martin Fowler goes into detail on here: http://martinfowler.com/articles/mocksArentStubs.html. If you find mocking arduous and you also find that you are doing a lot of it then that is a classic example of TDD telling you that your design could be improved.  I think the real question is whether unit testing is a best practice or not. If you believe it is then the use of mocking is a necessity from the point of view of having a tested unit isolated from the implementation of its dependencies. There is some confusion in how this relates to the concept of testability though. Complicated convoluted code is not testable essentially because it is hard to understand. Well-factored code that has a clean and simple design is generally easier to understand and maintain; why should it be hard to unit test then? The confusion arises from certain arbitrary limitations found in some mocking tools such as the inability to mock final or static methods or the need to have the tested unit directly use mock objects created in test code. Other mocking tools don't have these limitations and therefore don't require that ""application code gets modified so that some properties get mockable"". With modern programming languages/platforms (Java C# Python Ruby etc.) everything is mockable; it's just a matter of exposing this power in a mocking API and it has already been done for each of those languages (in the interest of full disclosure I develop one such tool)."
425,A,"request scoped beans in spring testing I would like to make use of request scoped beans in my app. I use JUnit4 for testing. If I try to create one in a test like this: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:spring/TestScopedBeans-context.xml"" }) public class TestScopedBeans { protected final static Logger logger = Logger .getLogger(TestScopedBeans.class); @Resource private Object tObj; @Test public void testBean() { logger.debug(tObj); } @Test public void testBean2() { logger.debug(tObj); } With the following bean definition:  <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd""> <bean class=""java.lang.Object"" id=""tObj"" scope=""request"" /> </beans> And I get: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'gov.nasa.arc.cx.sor.query.TestScopedBeans': Injection of resource fields failed; nested exception is java.lang.IllegalStateException: No Scope registered for scope 'request' <...SNIP...> Caused by: java.lang.IllegalStateException: No Scope registered for scope 'request' So I found this blog that seemed helpful: http://www.javathinking.com/2009/06/no-scope-registered-for-scope-request_5.html But I noticed he uses AbstractDependencyInjectionSpringContextTests which seems to be deprecated in Spring 3.0. I use Spring 2.5 at this time but thought it shouldn't be too hard to switch this method to use AbstractJUnit4SpringContextTests as the docs suggest (ok the docs link to the 3.8 version but I'm using 4.4). So I change the test to extend AbstractJUnit4SpringContextTests... same message. Same problem. And now the prepareTestInstance() method I want to override is not defined. OK maybe I'll put those registerScope calls somewhere else... So I read more about TestExecutionListeners and think that would be better since I don't want to have to inherit the spring package structure. So I changed my Test to: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:spring/TestScopedBeans-context.xml"" }) @TestExecutionListeners({}) public class TestScopedBeans { expecting I would have to create a custom listener but I when I ran it. It works! Great but why? I don't see where any of the stock listeners are registering request scope or session scope and why would they? there's nothing to say I want that yet this might not be a Test for Spring MVC code... Link moved to http://www.javathinking.com/2009/06/no-scope-registered-for-scope-request_5.html fixed the blog link thanks. The test passes because it isn't doing anything :) When you omit the @TestExecutionListeners annotation Spring registers 3 default listeners including one called DependencyInjectionTestExecutionListener. This is the listener responsible for scanning your test class looking for things to inject including @Resource annotations. This listener tried to inject tObj and fails because of the undefined scope. When you declare @TestExecutionListeners({}) you suppress the registration of the DependencyInjectionTestExecutionListener and so the test never gets tObj injected at all and because your test is not checking for the existence of tObj it passes. Modify your test so that it does this and it will fail: @Test public void testBean() { assertNotNull(""tObj is null"" tObj); } So with your empty @TestExecutionListeners the test passes because nothing happens. Now on to your original problem. If you want to try registering the request scope with your test context then have a look at the source code for WebApplicationContextUtils.registerWebApplicationScopes() you'll find the line: beanFactory.registerScope(WebApplicationContext.SCOPE_REQUEST new RequestScope()); You could try that and see how you go but there might be odd side-effects because you're not really meant to do this in a test. Instead I would recommend rephrasing your test so that you don't need request scoped beans. This shouldn't be difficult the lifecycle of the @Test shouldn't be any longer than the lifecycle of a request-scoped bean if you write self-contained tests. Remember there's no need to test the scoping mechanism it's part of Spring and you can assume it works. Ah yes thank you. I wasn't concerned with the test passing because I just wanted the bean created and when I wrote the test I didn't think I'd be turning off injection at some point:-). As for rephrasing the test... no. The whole point is to see how I can get request scoped beans to work in JUnit or the web app. Oh and when I said works I meant 'runs'. Again I was just looking for the Exception to go away thinking that would mean I now have a request scoped bean. Thank you but how would you ""rephrase"" your test to not _need_ request scoped beans? Suppose I'm testing FooController that has @Autowired Provider fooDao. I'm not mocking fooDao because this is integration test not unit (unit-tests don't need Spring context at all) I really need real FooDao. How do you inject request-scoped fooDao? Also to obtain a beanFactory you need some Context. And it fails while reading that context: `No Scope registered for scope 'request'`. See also my comment to MariuszS answer.  Test Request-Scoped Beans with Spring explains very well how to register and create a custom scope with Spring. In a nutshell as Ido Cohn explained it's enough to add the following to the text context configuration: <bean class=""org.springframework.beans.factory.config.CustomScopeConfigurer""> <property name=""scopes""> <map> <entry key=""request""> <bean class=""org.springframework.context.support.SimpleThreadScope""/> </entry> </map> </property> </bean> Instead of using the predefined SimpleThreadScope based on ThreadLocal it's also easy to implement a Custom one as explained in the article. import java.util.HashMap; import java.util.Map; import org.springframework.beans.factory.ObjectFactory; import org.springframework.beans.factory.config.Scope; public class CustomScope implements Scope { private final Map<String  Object> beanMap = new HashMap<String  Object>(); public Object get(String name ObjectFactory<?> factory) { Object bean = beanMap.get(name); if (null == bean) { bean = factory.getObject(); beanMap.put(name bean); } return bean; } public String getConversationId() { // not needed return null; } public void registerDestructionCallback(String arg0 Runnable arg1) { // not needed } public Object remove(String obj) { return beanMap.remove(obj); } public Object resolveContextualObject(String arg0) { // not needed return null; } }  Solution for Spring 3.2 or newer Spring starting with version 3.2 provides support for session/request scoped beans for integration testing. @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = TestConfig.class) @WebAppConfiguration public class SampleTest { @Autowired WebApplicationContext wac; @Autowired MockHttpServletRequest request; @Autowired MockHttpSession session; @Autowired MySessionBean mySessionBean; @Autowired MyRequestBean myRequestBean; @Test public void requestScope() throws Exception { assertThat(myRequestBean) .isSameAs(request.getAttribute(""myRequestBean"")); assertThat(myRequestBean) .isSameAs(wac.getBean(""myRequestBean"" MyRequestBean.class)); } @Test public void sessionScope() throws Exception { assertThat(mySessionBean) .isSameAs(session.getAttribute(""mySessionBean"")); assertThat(mySessionBean) .isSameAs(wac.getBean(""mySessionBean"" MySessionBean.class)); } } Read more: Request and Session Scoped Beans Solution for Spring before 3.2 with listener @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = TestConfig.class) @TestExecutionListeners({WebContextTestExecutionListener.class DependencyInjectionTestExecutionListener.class DirtiesContextTestExecutionListener.class}) public class SampleTest { ... } WebContextTestExecutionListener.java public class WebContextTestExecutionListener extends AbstractTestExecutionListener { @Override public void prepareTestInstance(TestContext testContext) { if (testContext.getApplicationContext() instanceof GenericApplicationContext) { GenericApplicationContext context = (GenericApplicationContext) testContext.getApplicationContext(); ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); beanFactory.registerScope(WebApplicationContext.SCOPE_REQUEST new SimpleThreadScope()); beanFactory.registerScope(WebApplicationContext.SCOPE_SESSION new SimpleThreadScope()); } } } Solution for Spring before 3.2 with custom scopes @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = TestConfig.class locations = ""test-config.xml"") public class SampleTest { ... } TestConfig.java @Configuration @ComponentScan(...) public class TestConfig { @Bean public CustomScopeConfigurer customScopeConfigurer(){ CustomScopeConfigurer scopeConfigurer = new CustomScopeConfigurer(); HashMap<String Object> scopes = new HashMap<String Object>(); scopes.put(WebApplicationContext.SCOPE_REQUEST new SimpleThreadScope()); scopes.put(WebApplicationContext.SCOPE_SESSION new SimpleThreadScope()); scopeConfigurer.setScopes(scopes); return scopeConfigurer } or with xml configuration test-config.xml <bean class=""org.springframework.beans.factory.config.CustomScopeConfigurer""> <property name=""scopes""> <map> <entry key=""request""> <bean class=""org.springframework.context.support.SimpleThreadScope""/> </entry> </map> <map> <entry key=""session""> <bean class=""org.springframework.context.support.SimpleThreadScope""/> </entry> </map> </property> </bean> Source code Source code for all presented solutions: https://github.com/mariuszs/spring-test-web Well it fails on the first line `testContext.getApplicationContext()` with error message `No Scope registered for scope 'request'` because it reads context from XML and @Configuration and some beans are defined there with ""request"" scope. For example I have: `@Configuration class MyConf { @Bean @Scope(""request"") provideFoo() {return new Foo()}}` Solution with listener works just fine for me tested full code here: https://github.com/mariuszs/spring-test-web/blob/master/src/test/java/sample/MainControllerWithListenerTest.java Because your `@Configuration` have no beans defined. Try adding a method in @Configuration for example `@Bean @Scope(""request"") @Autowired public String provideFoo(SomeBean dependency) {return dependency.toString()}`. It fails because SomeBean is not created yet.  This is still an open issue: https://jira.springsource.org/browse/SPR-4588 I was able to get this to work (mostly) by defining a custom context loader as outlined in http://forum.springsource.org/showthread.php?p=286280  I've tried several solutions including @Marius's solution with the ""WebContextTestExecutionListener"" but it didn't work for me as this code loaded the application context before creating the request scope. The answer that helped me in the end is not a new one but it's good: http://tarunsapra.wordpress.com/2011/06/28/junit-spring-session-and-request-scope-beans/ I simply added the following snippet to my (test) application context: <bean class=""org.springframework.beans.factory.config.CustomScopeConfigurer""> <property name=""scopes""> <map> <entry key=""request""> <bean class=""org.springframework.context.support.SimpleThreadScope""/> </entry> </map> </property> </bean> Good luck!  MariuszS' solution works except I couldn't get the transaction committed properly. It seems the newly released 3.2 has finally made testing request/session scoped beans first class citizens. Here's a couple of blogs for more details. Rossen Stoyanchev's Spring Framework 3.2 RC1: Spring MVC Test Framework Sam Brannen's Spring Framework 3.2 RC1: New Testing Features  NOT reading the docs sometimes drives one crazy. Almost. If you are using shorter-lived beans (request scope for example) you most likely also need to change your lazy init default! Otherwise the WebAppContext will fail to load and tell you something about missing request scope which is of course missing because the context is still loading! http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/beans.html#beans-factory-lazy-init The Spring guys should definitely put that hint into their exception message... If you don't want to change the default there is also the annotation way: put ""@Lazy(true)"" after @Component etc. to make singletons initialize lazy and avoid instantiating request-scoped beans too early."
426,A,"JUnit tests in eclipse failing when run together I have a set of 44 JUnit tests that I run using Eclipse (I got those tests from someone else - I am new to JUnit tests). When I run them all together 24 of them fail. However if I then run the failing ones individually some of them do pass after all. The tests do take a little time - one of the failing ones for example takes about one or two minutes to complete while just letting all of them run finishes in just a few seconds. I am starting multiple tests by right-clicking on the folder they are in and selecting ""Run As -> JUnit Test"". I am using JUnit 3. Am I doing something wrong in starting them / is there some kind of option I am missing? To expand on Gary's answer when right-clicking and doing the Run As -> JUnit you can't guarantee the order in which the tests are run which could also help to corrupt a shared resource. I would start by looking at your setup() and teardown() methods to ensure that shared resources are being reset properly. Also since you inherited these tests you may want to look at whether any of the tests are dependent upon one another. While this is a bad practice and should be changed you could perhaps create a test suite() to ensure the order in which they're run (at least until you can re-factor and decouple the tests).  GaryF's answer is one possibility. Another is that the tests have a race-condition: whether or not a test succeeds depends on how fast something occurs (which can vary due to the vagaries of the O/S). If you run the failing tests separately do they always succeed or do they sometimes fail. If they sometimes fail then you likely have a race-condition. @pyvi Then GaryF's scenario is more consistent with the evidence. I can't check now because the code is at work but the 4 or 5 times I tried they always succeeded. I also did not se anything in the JUnite code to suggest to me that there is a time condition in them.  It's hard to say for sure without seeing the tests but it sounds to me like they share some state or resource that is not being reset correctly for the next test."
427,A,"JUnit: Could not find class I just installed JUnit but I can't get it to work: C:\tools\junit4.6>java org.junit.runner.JUnitCore org.junit.tests.AllTests. JUnit version 4.6 Could not find class: org.junit.tests.AllTests. Time: 0 OK (0 tests) This are my environment variables: CLASSPATH=.;C:\Program Files\Java\jre1.5.0_11\lib\ext\QTJava.zip;c:\tools\junit4 .6\junit-4.6.jar;c:\tools\junit4.6\ JAVA_HOME=c:\Program Files\Java\jdk1.6.0_07 I just followed these steps: Below are the installation steps for installing JUnit: unzip the junit4.6.zip file add junit-4.6.jar to the CLASSPATH. For example: set classpath=%classpath%;INSTALL_DIR\junit-4.6.jar;INSTALL_DIR test the installation by running java org.junit.runner.JUnitCore org.junit.tests.AllTests. What could be wrong? I am on OS X. I had the same problem. The solution was to re-check my $CLASSPATH. In .profile in my home directory I had: $ANT_HOME=""ant/ant.jar"" I changed it to: $ANT_HOME=""ant"" and changed the PATH and CLASSPATH entries by adding /ant.jar to the {$ANT_HOME} entries. Here is what my classpath and path look like in my .profile with Ant JUnit and Cobertura installed and working: export JUNIT_HOME=""junit"" export ANT_HOME=""ant"" export COBERTURA_HOME=""cobertura/cobertura-1.9.3"" export PATH=""/usr/local/bin:/usr/local/sbin:$PATH:{$JUNIT_HOME}/junit-4.7.jar:$JUNIT_HOME:$COBERTURA_HOME:{$ANT_HOME}/bin"" export CLASSPATH=""$CLASSPATH:$ANT_HOME:$JUNIT_HOME/junit-4.7.jar:$JUNIT_HOME:$COBERTURA_HOME"" Hopefully this helps someone else. This was really painful the first time. I'm still working on getting Cobertura to work in Eclipse and Netbeans.. .sigh..  If you observe you are giving the Path under the classpath.remove it and try. And go the jar file directory and unjar the jar file and see org.junit.tests.AllTests exists or not. This comment is very useful. I was messing with `java org.junit.runner.JUnitCore org.junit.tests.AllTests` for about an hour. And it didnt occur to me to check if `org.junit.tests.AllTests` exists at all. Which it doesnt... Version: `JUnit version 4.10`  I was having an error just like this (but by classpath was correct) so I copied the test case directory from the ZIP from org/junit/ to junit. Then use: java org.junit.runner.JUnitCore org.junit.tests.AllTests junit.tests.AllTests and everything ran fine.  Could not find class: org.junit.tests.AllTests. If it is not a typo try removing the dot from the end of the classname?  C:\Documents and Settings\xxx>set CLASSPATH=%CLASSPATH%;C:\JUnit\junit 4.8.1\junit-4.8.1.jar;C:\JUnit\junit4.8.1 Note the directory structure marked in bold C:\JUnit\junit4.8.1 is the JUNIT_HOME"
428,A,"Is it possible to ignore certain unit tests? I'm currently working on a project which uses JUnit4 extensively throughout all the modules. We're using Maven2 to build the project and Hudson for continuous integration. The project is built and run under Java 1.5. We've recently added quite a large raft of unit tests which are needed at times but don't want to be run during the build process (but other unit tests in the same project do). I want to know if there's some kind of annotation that can be applied to the tests or some configuration that can be made to Maven that would ignore unit tests that live under a certain package or in a certain test class at build time? There's possibly the alternative of just putting these particular tests in a normal class that would be run through main but that's not ideal. Thanks for your suggestions JUnit 4.7 I believe supports ""Rules"" for this kind of thing (pre 4.7 I think you could have used custom Runners who would check an environment variable). Or you could look at the includes/excludes tags of your build system.  Maven's Surefire Plugin: Inclusions and Exclusions of Tests Damn I didn't notice that I open this question a while ago :)  You need to add the following into your POM.xml <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.12.4</version> <configuration> <skipTests>true</skipTests> </configuration> </plugin> Check out this URL : http://maven.apache.org/surefire/maven-surefire-plugin/examples/skipping-test.html --Abhijit Das  You can configure maven-surefire-plugin. For example: <project> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.4.2</version> <configuration> <excludes> <exclude>**/TestCircle.java</exclude> <exclude>**/TestSquare.java</exclude> </excludes> </configuration> </plugin> </plugins> EDIT: Another solution is to define profiles in which all test will be run e.g. by hudson. In development mode you can use subset of all test to speed up development (or all test might be in default profile choosen test in dev profile - so you need run maven with -PprofileName attribute). This example better suits for integration tests that take loger to run. +1 for profiles (regarding integration tests I'd rather place them in `src/test/it` and run them during the `integration-test` phase) Great thanks for this I'll take a look into both solutions and decide which suits best. @James. Notice that solution with profiles also need surefire plugin configuration. @Pascal. Yes you are right I do the same as you. But sometimes writing a *pure* unit test it's difficult and always is a temptation to test a little more and still call it *unit test* :)  I never tried it but could you put the additional tests in a different source folder and configure your build script to include or exclude it according to your build target? Interesting not a bad idea. If nothing else comes up I'll look into it. Cheers  'exclude' surefire plugin setting  Try JUnit Assumptions which seem to be since 4.4 http://junit.sourceforge.net/doc/ReleaseNotes4.4.html http://junit.sourceforge.net/javadoc/org/junit/Assume.html http://eclipsesource.com/blogs/2009/10/07/using-junits-assume-for-faster-tests/ Presuming you can detect at runtime if the tests should be run (via defines properties etc) this will allow you to keep a nice clean test suite.  There is an ""Ignore"" annotation but it's manual work to add them not sure if that helps. It can be used for a test method or a whole class @Ignore(""not ready yet"") @Test public void testSomething() {... or @Ignore public class SomeTest { @Test public void testSomething() {... } []]"
429,A,problem with using JPA I am getting following error while executing a unti test case(JUNIT).I am using it to understand the part of the project.Project is a web based project.Project is using OPENJPA <openjpa-1.2.1-SNAPSHOT-r422266:686069 fatal store error> org.apache.openjpa.persistence.RollbackException: Unable to obtain a TransactionManager using null. at org.apache.openjpa.persistence.EntityManagerImpl.commit(EntityManagerImpl.java:523) at com.XYZ.cloud.admin.loadCatalog.LoadCatalogTest.populateOffering(LoadCatalogTest.java:253) at com.XYZ.cloud.admin.loadCatalog.LoadCatalogTest.CatalogUploadTest(LoadCatalogTest.java:160) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37) at java.lang.reflect.Method.invoke(Method.java:599) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41) at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) at org.junit.runners.ParentRunner.run(ParentRunner.java:220) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) Caused by: <openjpa-1.2.1-SNAPSHOT-r422266:686069 nonfatal general error> org.apache.openjpa.persistence.PersistenceException: Unable to obtain a TransactionManager using null. at org.apache.openjpa.jdbc.sql.DBDictionary.narrow(DBDictionary.java:4231) at org.apache.openjpa.jdbc.sql.DBDictionary.newStoreException(DBDictionary.java:4196) at org.apache.openjpa.jdbc.sql.DB2Dictionary.newStoreException(DB2Dictionary.java:503) at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:102) at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:88) at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:64) at org.apache.openjpa.jdbc.kernel.AbstractJDBCSeq.next(AbstractJDBCSeq.java:65) at org.apache.openjpa.util.ImplHelper.generateValue(ImplHelper.java:160) at org.apache.openjpa.util.ImplHelper.generateFieldValue(ImplHelper.java:144) at org.apache.openjpa.jdbc.kernel.JDBCStoreManager.assignField(JDBCStoreManager.java:698) at org.apache.openjpa.util.ApplicationIds.assign(ApplicationIds.java:487) at org.apache.openjpa.util.ApplicationIds.assign(ApplicationIds.java:463) at org.apache.openjpa.jdbc.kernel.JDBCStoreManager.assignObjectId(JDBCStoreManager.java:682) at org.apache.openjpa.kernel.DelegatingStoreManager.assignObjectId(DelegatingStoreManager.java:134) at org.apache.openjpa.kernel.StateManagerImpl.assignObjectId(StateManagerImpl.java:519) at org.apache.openjpa.kernel.StateManagerImpl.preFlush(StateManagerImpl.java:2823) at org.apache.openjpa.kernel.PNewState.beforeFlush(PNewState.java:39) at org.apache.openjpa.kernel.StateManagerImpl.beforeFlush(StateManagerImpl.java:959) at org.apache.openjpa.kernel.BrokerImpl.flush(BrokerImpl.java:1948) at org.apache.openjpa.kernel.BrokerImpl.flushSafe(BrokerImpl.java:1908) at org.apache.openjpa.kernel.BrokerImpl.beforeCompletion(BrokerImpl.java:1826) at org.apache.openjpa.kernel.LocalManagedRuntime.commit(LocalManagedRuntime.java:81) at org.apache.openjpa.kernel.BrokerImpl.commit(BrokerImpl.java:1350) at org.apache.openjpa.kernel.DelegatingBroker.commit(DelegatingBroker.java:877) at org.apache.openjpa.persistence.EntityManagerImpl.commit(EntityManagerImpl.java:512) ... 26 more Caused by: java.sql.SQLException: Unable to obtain a TransactionManager using null. at org.apache.openjpa.jdbc.kernel.TableJDBCSeq.allocateSequence(TableJDBCSeq.java:419) at org.apache.openjpa.jdbc.kernel.TableJDBCSeq.nextInternal(TableJDBCSeq.java:290) at org.apache.openjpa.jdbc.kernel.AbstractJDBCSeq.next(AbstractJDBCSeq.java:60) ... 44 more Caused by: javax.transaction.NotSupportedException: Unable to obtain a TransactionManager using null. at org.apache.openjpa.ee.AutomaticManagedRuntime.doNonTransactionalWork(AutomaticManagedRuntime.java:306) at org.apache.openjpa.jdbc.kernel.TableJDBCSeq.allocateSequence(TableJDBCSeq.java:415) ... 46 more Caused by: <openjpa-1.2.1-SNAPSHOT-r422266:686069 fatal user error> org.apache.openjpa.util.InvalidStateException: Could not perform automatic lookup of EJB container's javax.transaction.TransactionManager implementation. Please ensure that you are running the application from within an EJB 1.1 compliant EJB container and then set the org.apache.openjpa.ManagedRuntime property to at org.apache.openjpa.ee.AutomaticManagedRuntime.getTransactionManager(AutomaticManagedRuntime.java:250) at org.apache.openjpa.ee.AutomaticManagedRuntime.doNonTransactionalWork(AutomaticManagedRuntime.java:304) ... 47 more Caused by: javax.naming.ConfigurationException: Name space accessor for the java: name space has not been set. Possible cause is that the user is specifying a java: URL name in a JNDI Context method call but is not running in a J2EE client or server environment. at com.XYZ.ws.naming.java.javaURLContextFactory.isNameSpaceAccessable(javaURLContextFactory.java:93) at com.XYZ.ws.naming.urlbase.UrlContextFactory.getObjectInstance(UrlContextFactory.java:82) at javax.naming.spi.NamingManager.getURLContext(NamingManager.java:655) at javax.naming.InitialContext.getURLOrDefaultInitCtx(InitialContext.java:434) at javax.naming.InitialContext.lookup(InitialContext.java:450) at org.apache.openjpa.ee.RegistryManagedRuntime.getTransactionManager(RegistryManagedRuntime.java:61) at org.apache.openjpa.ee.AutomaticManagedRuntime.getTransactionManager(AutomaticManagedRuntime.java:154) ... 48 more Did you start transaction at all? EntityManager.getTransaction().begin() My guess is that you are trying to use a persistence.xml declaring a jta-data-source in a unit test context (i.e. in a Java SE context). In other words you're not using an appropriate persistence.xml for a testing context. You need a specific persistence.xml using a RESOURCE_LOCAL transaction type and configure it to use a built-in connection pool (instead of a datasource). Show your persistence.xml if you need more guidance. i have posted the questions http://stackoverflow.com/questions/3952124/unit-testing-code-which-uses-jpa-jta thanks: i commented out jta and it worked But i have one more problem I use 2 separate database so i have to use JTA to handle distributed databases.So how can i test my code using JTA? @akp This is somehow a different problem and I'm not going to cover it in a comment box. Open another question with the required details.  Like the error message suggests: You need to make sure that you are running the application from within an EJB 1.1 compliant EJB container. Thanks for replying:But i want to unit test the only this part of project how can i do it? I debugged the code and found that an exception is thrown when tx.commit() is called.I have pasted proper exception now.Can you plese suggent me some way by which i can run only this part of project? Consider using a Mocking framework instead of connecting to a real DB or try DBUnit Thanks again.But the my whole purpose is to understandt how data goes in which table.b Can u please suggest me by which i can run this unit test case under a EJB container without using the entire projectI mean only the part of project am concerned with After some googling I found this: http://knol.google.com/k/how-to-unit-test-enterprise-java-beans-ejb# thanks a lot .Am goiing through that article.Will try it and get back to you @jeroen.I dint understand much from that article.I dont have much experince.Can u plese help
430,A,"Rerunning a set of JUnit tests multiple times I've got a set of tests that test some timzeone related functionality I have a Before method that sets the default timezone to one of my choosing and an after method that restores whatever the default was before the tests. So I basically want to do Set timezone to UTC Run tests Restore timezone Set timezone to EST Run tests Restore timezone Set timezone to JST Run tests Restore timezone The tests are identical in each case. Is there an easy way to do this? recently I created zohhak project. you may find it useful. it lets you write: @TestWith({ ""EST"" ""UTC"" ""null"" }) public void testMethod(TimeZone timeZone) { setTimeZone(timeZone); ... } @After public void restoreTimezone() { ... }  Yup there is a nice way to do the same test over and over again just with some other dataset. The solution is ""@RunWith(Parameterized.class)"" - link to javadoc. I'm assuming you're using JUnit 4 - but TestNG also has such functionality from what I remember. The code you'll need to write will look somewhat like this:  @RunWith(Parameterized.class) public class FibonacciTest { @Parameters public static Collection<Object[]> data() { return Arrays.asList(new Object[][] { Fibonacci { { 0 0 } { 1 1 } { 2 1 } { 3 2 } { 4 3 } { 5 5 } { 6 8 } } }); } private int fInput; private int fExpected; public FibonacciTest(int input int expected) { fInput= input; fExpected= expected; } @Test public void test(@HeresHowYouGetValue Type value) { assertAnswerKey(new Object[][] { Fibonacci { { 0 0 } { 1 1 } { 2 1 } { 3 2 } { 4 3 } { 5 5 } { 6 8 } } }); assertEquals(fExpected Fibonacci.compute(fInput)); } }  Here is a post I wrote that shows several ways of running the tests repeatedly with code examples: http://codehowtos.blogspot.com/2011/04/run-junit-test-repeatedly.html You can use the @Parametrized runner or use the special runner included in the post  I solved the problem by using the Parameterized and RunWith notations: See: http://ourcraft.wordpress.com/2008/08/27/writing-a-parameterized-junit-test/"
431,A,"What is a test case? I'm having difficulties finding a hard definition of the word ""test case"". Some sources claim that a test case is a class that extends TestCase. Other sources claim that a test case is a single test method. The JUnit documentation is unclear it appears to me the words ""test case"" and ""test"" mean the same: The Test annotation tells JUnit that the public void method to which it is attached can be run as a test case. To run the method JUnit first constructs a fresh instance of the class then invokes the annotated method. Any exceptions thrown by the test will be reported by JUnit as a failure. If no exceptions are thrown the test is assumed to have succeeded. So what exactly is a ""test case"" and what is its relationship to a ""test""? I think you're unable to find a ""hard definition"" because there isn't one. Does it really matter? If you're having difficulties communicating what you mean with the ""vague"" terminology just be clear about _your_ usage. I think (**warning**: speculation ahead) some of the confusion arrose because at some point someone realised they could refactor some of the JUnit code to introduce an abstract base class (or interface) and so use the Composite design pattern to simplify some code resulting in `Test` and `TestCase` became less distinct in meaning. I'd point you over to the JUnit documentation on TestCase. In it it describes three conditions for what it denotes a TestCase: implement a subclass of TestCase define instance variables that store the state of the fixture initialize the fixture state by overriding setUp() clean-up after a test by overriding tearDown(). The setUp and tearDown portion I think are critical to understanding here. It's not just simply that a test case is but one annotated method. The test case is that method plus the initialization and destruction of the frame on which a test will be run. So to answer your question a test is one annotated method which attempts to verify the workings of a single method while a test case is that test plus the frame in which it will be run. It also says ""A test case defines the fixture to run multiple tests. [...] Each test runs in its own fixture [...] For each test implement a method which interacts with the fixture."" That sounds to me like ""method = test"" and ""class = test case""... @FredOverflow I think the fixture in this case is the setUp and tearDown. Those are supposed to be the invariants of all the labled tests. I just realized that `TestCase` implements `Test`. Does that imply that a test case is one possible kind of test in the sense that every test case is also a test? More confusion :) @FredOverflow I think you're confusing the semantics of ""TestCase"" with ""test case."" One refers to a class within the JUnit hierarchy the other refers to the conceptual formulations of unit testing in general. Maybe I have been ""spoiled"" by good naming conventions? I would not name a class `TestCase` unless its objects (or objects from classes derived from it) represented test cases. @FredOverflow instances of classes that extend `junit.framework.TestCase` do represent test cases! Each instance runs one method with a name starting with ""test"" (running `setUp()` before the test and `tearDown()` after the test). When you tell your IDE to run a class that extends `TestCase` it creates a suite to run the tests defined by your class (specifically it creates a `TestSuite` that contains one `TestCase` instance per test method). As for why `TestCase` implements `Test` see http://junit.sourceforge.net/doc/cookstour/cookstour.htm  In the JUnit context ""test case"" can mean a class which contains related tests (in JUnit 4 you don't have to extend TestCase anymore) or a single test method. There really isn't a hard definition of the term. In my book a test case is the answer to this question: Does the piece of code under test behave as expected? Test cases can be tiny: Foo foo = new Foo(); assertEquals( ""..."" foo.bar(5) ); or they can setup a database will it with test data create an object model and then invoke a couple of methods to verify that modifications of the object model work as expected."
432,A,How can I write JUnit tests for the clipboard in Java? My problem with JUnit tests for the clipboard is that Java hold a copy of the clipboard data. That the behavior of the clipboard is completely different if you copy data in the same Java VM or if you copy data in an external process to the system clipboard. Are there any trick to clear the local copy of the clipboard data? This should work platform independent. The only idea that I have is to start a second Java process that copy data to the clipboard. But this has a very bad performance if every test start a second JVM. Why do you feel the need to test the clipboard? A better approach would be to test your Transferable and assume that the clipboard works as advertised (or it would have been fixed). @Horcrux7 - IKVM is a VM written in .Net correct? If that's the case then I would expect that any tests are written in C# or some other .Net language and would have access to the .Net clipboard interfaces. So I'd think that you'd start the VM execute whatever Java code updates the clipboard then reset the clipboard via .Net code. I write tests for the IKVM.
433,A,"JUnit + Derby + Spring: drop in-memory db after every test In my unit tests I autowired some DataSources which use URLs like jdbc:derby:memory:mydb;create=true to create an in-memory DBs. To drop an in-memory Derby db you have to connect with: jdbc:derby:memory:mydb;drop=true I would like this to happen after every test and start with a fresh db. How can I do this using Spring? How to shutdown Derby In Memory Database Properly gave me a hint to a solution:  mydb.drop.url = jdbc:derby:memory:mydb;drop=true ... <bean id=""mydbDropUrl"" class=""java.lang.String""> <constructor-arg value=""${mydb.drop.url}"" /> </bean> ... @Resource private String mydbDropUrl; @After public void tearDown() { try { DriverManager.getConnection(mydbDropUrl); } catch (SQLException e) { // ignore } } A downside is the use of the String constructor which accepts a String (an immutable String object around an immutable String object). I read that there is a @Value annotation in Spring 3 which might help here but I'm using Spring 2.5. Please let me know if you have a nicer solution. Following http://docs.oracle.com/javadb/10.8.1.2/getstart/rwwdactivity3.html as an example rather than simply catching and discarding the `SQLException` it might be better to discard it only if `e.getSQLState().equals(""08006"")`  If you use the spring-test.jar library you can do something like this: public class MyDataSourceSpringTest extends AbstractTransactionalDataSourceSpringContextTests { @Override protected String[] getConfigLocations() { return new String[]{""classpath:test-context.xml""}; } @Override protected void onSetUpInTransaction() throws Exception { super.deleteFromTables(new String[]{""myTable""}); super.executeSqlScript(""file:db/load_data.sql"" true); } } And an updated version based on latest comment that drops db and recreates tables before every test: public class MyDataSourceSpringTest extends AbstractTransactionalDataSourceSpringContextTests { @Override protected String[] getConfigLocations() { return new String[]{""classpath:test-context.xml""}; } @Override protected void onSetUpInTransaction() throws Exception { super.executeSqlScript(""file:db/recreate_tables.sql"" true); } } I don't want to delete data from tables or even to drop tables. I want to drop the whole db an recreate it for every test which is safer IMHO. You could create a sql script that drops the db and recreates the tables and execute it as shown above. I'll update my answer appropriately. I'm not sure this will work since ""jdbc:derby:memory:mydb;drop=true"" is a URL and not an SQL statement. yup know your libs (+1)  Just do something like: public class DatabaseTest implements ApplicationContextAware { private ApplicationContext context; private DataSource source; public void setApplicationContext(ApplicationContext applicationContext) { this.context = applicationContext; } @Before public void before() { source = (DataSource) dataSource.getBean(""dataSource"" DataSource.class); } @After public void after() { source = null; } } Make your bean have a scope of prototype (scope=""prototype""). This will get a new instance of the data source before every test. I have doubts if this approach really works: Are you saying that context.getBean() will return a new instance rather than the same instance? I don't think this is the case which means you will gain nothing. There is a @DirtiesContext annotation but even that would not open a connection with ""jdbc:derby:memory:mydb;drop=true"" to delete the in-memory db. That's clever (+1) but not very practical (it means you have to manually wire all beans that use the datasource)  There is a database-agnostic way to do this if you are using Spring together with Hibernate. Make sure the application context will be created / destroyed before / after every test method: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration({""classpath*:application-context-test.xml""}) @TestExecutionListeners({DirtiesContextTestExecutionListener.class DependencyInjectionTestExecutionListener.class}) @DirtiesContext(classMode = ClassMode.AFTER_EACH_TEST_METHOD) public abstract class AbstractTest { } Instruct Hibernate to auto create the schema on startup and to drop the schema on shutdown: hibernate.hbm2ddl.auto = create-drop Now before every test the application context is created and the required spring beans are injected (spring) the database structures are created (hibernate) the import.sql is executed if present (hibernate) and after every test the application context is destroyed (spring) the database schema is dropped (hibernate). If you are using transactions you may want to add the TransactionalTestExecutionListener.  After spring test 3 you can use annotations to inject configurations: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(""/spring-test.xml"") public class MyTest { }"
434,A,spirit of a jUnit test Suppose that you have the following logic in place: processMissing(masterKey masterValue p.getPropertiesData().get(i).getDuplicates()); public StringBuffer processMissing(String keyA String valueA Set<String> dupes) { // do some magic } I would like to write a jUnit test for processMissing testing its behavior in event dupes is null. Am i doing the right thing here? Should I check how method handles under null or perhaps test method call to make sure null is never sent? Generally speaking what is the approach here? We can't test everything for everything. We also can't handle every possible case. How should one think when deciding what tests to write? I was thinking about it as this: I have a certain expectation with the method Test should confirm define my expectation and confirm method works under that condition Is this the right way to think about it? Thanks and please let me know First define whether null is a valid value for the parameter or not. If it is then yes definitely test the behavior of the method with null. If it is not then: Specify that constraint via parameter documentation. Annotate that constraint on the parameter itself (using an annotation compatible with the tool below). Use a static analysis tool to verify that null is never passed. No unit test is required for the invalid value unless you're writing code to check for it. The static analysis tool FindBugs supports annotations such as @NonNull with some limited data-flow analysis. I personally think it would be unnecessarily expensive within large Java codebases to always write and maintain explicit checks for NULL and corresponding non-local unit tests. I would have said explicitly throw an IllegalArgumentException or NullPointerException in the method as well. But I'm wondering if this is somehow superseded by the annotations. Certainly I wouldn't rely on the static analysis tool unless that's part of a build/test process. I personally find annotations usually sufficient -- presuming that static analysis is actually performed preferably as part of the build.  If you want to ensure that people don't call your API with a null argument you may want to consider using annotations to make this explicit JSR 305 covers this and its used in Guava. Otherwise you're relying on users reading javadoc. As for testing you're spot on in that you can't handle every possible case assuming you don't want to support null values I'd say that you may want to throw an IllegalArguemntException rather than a NullPointerException so you can be explicit about what is null then you can just test for that exception being thrown - see JUnit docs.
435,A,"How do Java mocking frameworks work? This is NOT a question about which is the best framework etc. I have never used a mocking framework and I'm a bit puzzled by the idea. How does it know how to create the mock object? Is it done in runtime or generates a file? How do you know its behavior? And most importantly - what is the work flow of using such a framework (what is the step-by-step for creating a test). Can anyone explain? You can choose whichever framework you like for example just say what it is. You may not have wanted ""best framework"" answers but that's what you're going to get... See also http://stackoverflow.com/questions/2277039/how-do-mock-frameworks-work although the answers there are very technical. I think that EasyMock's documentation is a get way to get you started. It features a lot of examples that are easy to grasp.  A mocking framework takes the redundancy and boilerplate out of a mocking test. It knows to create the Mock object because you tell it to of course (unless you meant something else with that question). The ""standard"" way of creating a mock object is to use/abuse the java.lang.reflect.Proxy class to create a runtime implementation of the interface. This is done at runtime. Proxy has a limitation in that it cannot proxy concrete classes. To accomplish mocking of concrete classes requires dynamic bytecode creation that creates subclasses that override the real implementation of the public methods with essentially what would be done with a Proxy (record the method parameters and return a pre-determined value). This has a limitation in that it cannot subclass final classes. For that you have solutions like JDave which (I believe I didn't confirm this) muck with the classloader to remove the final designation on the class before loading it so the class isn't in fact final at runtime as far as the JVM is concerned. The Mocking framework is basically all about capturing the parameters and verifying them against pre-determined expectations and then returning a pre-configured or reasonable default value. It doesn't behave in any particular way which is the point. The calling code is being verified that it calls the method with the correct parameter and perhaps how it reacts to a specific return values or thrown exceptions. Any side effects or real accomplishments of the call on the real object do not happen. Here is a real example from a project using JMock with JUnit4. I have added comments to explain what is going on.  @RunWith(JMock.class) //The JMock Runner automatically checks that the expectations of the mock were actually run at the end of the test so that you don't have to do it with a line of code in every test. public class SecuredPresentationMapTest { private Mockery context = new JUnit4Mockery(); //A Mockery holds the state about all of the Mocks. The JUnit4Mockery ensures that a failed mock throws the same Error as any other JUnit failure. @Test public void testBasicMap() { final IPermissionsLookup lookup = context.mock(IPermissionsLookup.class); //Creating a mock for the interface IPermissionsLookup. context.checking(new Expectations(){{ //JMock uses some innovative but weird double brace initialization as its standard idom. oneOf(lookup).canRead(SecuredEntity.ACCOUNTING_CONTRACT); //expect exactly one call to the IPermissionsLookup.canRead method with the the enum of ACCOUNTING_CONTRACT as the value. Failure to call the method at all causes the test to fail. will(returnValue(true)); //when the previous method is called correctly return true; }}); Map<String Component> map = new SecuredPresentationMap(lookup SecuredEntity.ACCOUNTING_CONTRACT); //This creates the real object under test but passes a mock lookup rather than the real implementation. JLabel value = new JLabel(); map.put("""" value); assertThat(((JLabel) map.get("""")) is(value)); //This ensures that the Map returns the value placed which it should based on the fact that the mock returned true to the security check. } } If the mock passed in was ignored the test would have failed. If the map fails to return the value placed in it the test fails (that is standard JUnit). What is being tested here and in another opposite test is that depending on what the IPermissionsLookup interface says about the security the Map changes its behavior about what is returned. This is the base good case. The other test the mock returns false and something else is expected from the Map. Using the mock ensures that the map relies on the IPermissionsLookup method to determine the security posture and what is being returned. I should say because of JMock I learned of the existence of the instance initializer (double braces) and indeed it IS really weird if you see it for the first time. @Alexander indeed JMock tests are the only time I use it. Although I have to admit it makes a good idiom in this particular case I would still think an abstract method (like Guice's AbstractModule) would be the way to go in the general case.  I will speak of the framework that I use ( jmock ) but others do something very similar. How does it know how to create the mock object? It does not you tell the framework that you require a mock object and you give it a type ( ideally an interface ) of the object that you would like to mock. Behind the scenes the mock framework uses a combination of reflection and sometimes byte-code rewrites to create a mock object. Is it done in runtime or generates a file? jMock creates it at runtime. How do you know its behavior? Mock objects are pretty dumb. You specify the behavior that you expect of them. And most importantly - what is the work flow of using such a framework (what is the step-by-step for creating a test). One very important thing that framework must provide is an ability to check that expected behavior has been observed during the test execution. jmock does it by introducing specific test runner that checks that all expectations on all declared mock objects after the test has finished. If something does not match the exception is thrown. Usually the pattern is: Acquire mock factory ( called Mockery in jmock )out of thin air ( with specific no-args constructor ). Create required mock objects. Setup expectation on mock objects Call the method you are testing passing mocks instead of real objects If your method returns some values check expected value with regular assertXXX methods.  For a better understanding of mocking you might like to make your own mock object. It's a pretty simple process - you make a class which implement the same interface as what you're mocking and give it the behaviour you require either recording calls to a method or responding in a set way when called. From there the way that mocking frameworks are set up begins to make more sense. I would imagine that the mock library knows what interface to mock because it uses reflection to determine what the interface looks like. I've created mock objects before... Just not using a framework. Why did this get downvoted? It's the only answer that addresses the OP's question. This answer does not address the question about how the existing mock frameworks work. Also if the question were ""How do I write my own mocking framework?"" only half the question was addressed. A complete answer would address mocking concrete objects as well. @Robert Harvey I downvoted it because any java programmer is capable of writing mock class implementing his interface. The more interesting question would be how mocking frameworks do it at runtime. So if author would've written about proxies phrase ""from there the way that mocking frameworks are set up begins to make more sense"" would be more appropriate.  EasyMock (http://easymock.org/) is the mocking library that I have used the most. (There are others: jMock Mockito etc.) Most of these create an object at runtime that you can control. The basic flow is: Create a mock Specify how it should act by telling the mock framework what calls to expect. (This varies based on the framework.) Use the mock in your test as if it were the real thing Some frameworks allow (or require) that you ""verify"" that the mock was used correctly."
436,A,"Hudson CI and PHPUnit: None of the test reports contained any result Im trying to integrate my PHPUnit tests into Hudson using the xUnit plugin. After a successfull build with Ant in hudson the console output shows: Recording test results None of the test reports contained any result My junit.xml test output using phpunit --log-junit looks as follows: <testsuites> <testsuite name=""Unit Tests"" tests=""1"" assertions=""1"" failures=""0"" errors=""0"" time=""0.005112""> <testsuite name=""DbTest"" file=""src/tests/unit/DbTest.php"" tests=""1"" assertions=""1"" failures=""0"" errors=""0"" time=""0.005112""> <testcase name=""testConnection"" class=""DbTest"" file=""src/tests/unit/DbTest.php"" line=""4"" assertions=""1"" time=""0.005112""/> </testsuite> </testsuite> </testsuites> Any help is appreciated EDIT: I just created a test junit.xml containing: <testsuites> <testsuite name=""DbTest"" file=""src/tests/unit/DbTest.php"" tests=""1"" assertions=""1"" failures=""0"" errors=""0"" time=""0.005112""> <testcase name=""testConnection"" class=""DbTest"" file=""src/tests/unit/DbTest.php"" line=""4"" assertions=""1"" time=""0.005112""/> </testsuite> </testsuites> With this in place a build was successful. The problem seems to be the nested -tags. Any idea how to prevent PHPUnit to create these nested tags? I finally found a solution at How might I integrate phpunit with Hudson CI? using th xslt workaround. If anyone has the same problem: Create a new .xsl file anywhere below your project root: <?xml version=""1.0"" encoding=""UTF-8""?> <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""> <xsl:template match=""/""> <xsl:element name=""testsuites""> <xsl:for-each select=""//testsuite[@file]""> <xsl:copy-of select=""."" /> </xsl:for-each> </xsl:element> </xsl:template> </xsl:stylesheet> Then add to your build.xml (don't forget to include in the build target): <target name=""phpunit_to_xunit""> <xslt in=""build/logs/phpunit.xml"" out=""build/logs/junit.xml"" style=""phpunit_to_xunit.xsl""/> </target> Anyway this is somehow cumbersome. Therefore I already created a improvement suggestion to the xUnit project. Thanks for sharing your solution ! +1"
437,A,"Eclipse: bind some key for all unit tests I practice TDD and run my tests very often. Eclipse has a nice command to run the last-launched configuration. But when I invoke the command in some unit test class Eclipse runs only the tests for current unit test class. I want run all my unit tests instead. Yes I can use the mouse to invoke the command that runs all JUnit tests but to repeat: I run tests very often. There is also Junit Max. It is not free but very cheap. It will run all your tests every time you save a file and indicate if the tests were successful in the right bottom corner of eclipse. If a tests fails the line where it fails is marked as an error so you can keep working with the source files and don't have to wait for your tests and have your workflow interrupted.  You need to change eclipse's run mode. By default it will try to run whatever is selected or being edited. You want to it always run the ""last thing executed"". Go to Window->Preferences->Run/Debug->Launching Choose ""Always Launch the Previous Application"" Then you can define a TestSuite with all of your test cases and run it or run all test cases for the project. Once run you can just keep re-running with F11 or control-F11. Hope this helps! -- Scott +1 for including the menu path to the solution; specificity in answers is most helpful. Thanks. Scott! It is whats I needed! Glad to help! Take care!  Maybe Infinitest is what you need. I love Infinitest but I find it a little unreliable with large multi-module Maven solutions and also when the tests are doing something a little unusual (e.g. using Hazelcast). You can use a filter to exclude tests though and that helps a little.  To run all unit tests in a project package or source root select the item in the package explorer and hit Shift+Alt+X then T (or right click->Run As-> JUnit test). To get the focus on the package explorer hit Ctrl+F7 to bring up the ""Next View"" menu hit it repeatedly to cycle through the list or hit up and down arrow then enter to select the view. Once you've run it once F11 can be set to run the last launched if you tweak the debug preferences (the default is to run for the current selection).  You can create a JUnit launch configuration for all of your tests by right clicking the tests folder and choosing ""Run As > JUnit Test"". Then see http://stackoverflow.com/a/8176077/255961 which describes how to use the Practically Macro plugin to assign shortcuts to launch configurations. PS If for some reason you have tests in more than one folder you can then edit the JUnit launch configuration to work at the project level as well:"
438,A,Run JUnit test againsts arbitrary loaded class As part of a larger project I am attempting to achieve something that I'm not sure is possible so am eager to see if anyone has any suggestions! The overall system: As a whole my system should be able to be provided with a JUnit test class that matches some provided interface. Classes will be then given that do not implement this interface but need to be checked to see if they would be able to (a.k.a. if they implement all necessary methods). If so some transformation should take place such that the JUnit test class can be run against it. So far I have implemented: - A package that loads other classes given a path and name using URLClassLoader - A package that runs a JUnit test case and returns the results using JUnitCore The problem: 1. At first how could I run the JUnit test against a class that does implement the interface when the test is designed to match the interface? How do I (at runtime) dictate that the instance being tested by the interface is the loaded class? Is it possible to then extend this such that I could i) verify that it does match the interface (I assume using Reflection to check for corresponding methods?) and then ii) modify that class such that it can be tested using the JUnit test class? Thanks for any advice that might help towards part of this problem. I appreciate my description may be lacking so please comment if you have any extra information that would help you give any answer! What is the root problem you are trying to solve here? You can do everything you want with the reflection API. It sounds like you should start with the tutorial and then come back here for specific questions. Given a Class object you can check if it implements a given interface create an instance of it and then treat it like any other class. Edit: I don't think I got that from your question but in that case you are looking for the Proxy part of the reflection API. Thanks I know that's the right direction for that part of things. However how would I go about the extended version of this that I described whereby I need to i) see if the class has all the methods to implement the interface ii) if so treat it as if it does for the tests. Also how can I test the classes that match the interface within JUnit when the test only matches the interface?  For the first part of your question; if you have the loaded Class instance for the class you want to test you can construct one with newInstance() if it has a default constructor or via the getConstructor methods if you need to pass parameters. You should be able to get this Class instance from the class loader. For the second part. You should be able to check the public methods via getMethods() (again on the Class instance) then look through the returned array for the methods you want. There are methods on the Method class that will return information about parameters exceptions and return type to verify they are what you require. However I am pretty certain it is not possible to modify the class at runtime to add the interface. It might be possible by modifying the byte code but I don't know about that. An alternative would be to write your test to call all method via reflection then it doesn't matter what the type of the object is just that it has the right methods (which you've already checked).  If you want to make arbitrary class to implement given interface at runtime if its public API matches the interface you have several options in Java. Creating java.lang.Proxy to bridge the target class exposing YourInterface is the easiest way. YourInterface i = (YourInterface) Proxy.newProxyInstance( this.getClass().getClassLoader() new Class[]{YourInterface.class} new InvocationHandler() { @Override public Object invoke(Object o Method method Object[] objects) throws Throwable { //run method on your target class here using reflection } }); You can also use mixins in AspectJ or subclass your target class using CGLIB and add interface at runtime. But the proxy approach is not that hard-core to implement.  how could I run the JUnit test against a class that does implement the interface when the test is designed to match the interface Since you have the class you can use the isAssignableFrom method offered by the class such that Class loadedJunitClass = clazz; MyInterface impl = null; if(MyInterface.class.isAssignableFrom(loadedJunitClass )){ impl = (MyInterface) loadedJunitClass.newInstance(); } For the second question you can check each method and see 1. If there exists a method with the same method name as defined in the interface 2. If the method return type is the same from the interface and 3. If the method parameter types and length are the same. Of course 2 and 3 can be tricky to get right. At that point I would just create an instance of that interface (anonymous or a private class) create a newInstance of that matching class. And invoke the methods through reflection within the interface's methods. Now that is how you can get it done with reflection. I am not advicating reflection as you can imagine :)
439,A,Checking for a time-out in a JUnit test case I have a JUnit test case where I'm expecting a particular method call to take a long time (over a minute). I want to Make the method call. Make sure the method call takes at least a minute and have a JUnit assertion fail if it doesn't. Then kill the method call (assuming it took more than a minute as it should) because it could take a really long time. How do I do this? You can write a class implementing runnable that wraps around the method of interest; assuming spawning threads is allowed. public class CallMethod implements Runnable { //time in milli public long getStartTime() { return startTime; } //time in milli public long getEndTime() { return endTime; } public void run() { startTime = ...; obj.longRunningMethod(); endTime = ...; } } Then in your JUnit you can do something like: public void testCase1() { CallMethod call = new CallMethod(); Thread t = new Thread(call); t.start(); t.join(60000); // wait one minute assertTrue(t.alive() || call.getEndTime() - call.getStartTime() >= 60000); }
440,A,Unit-testing multithreaded applications Has anyone got any advice or know of any frameworks for unit-testing of multithreaded applications? Typically you don't unit test the concurrency of multi threaded applications as the unit tests aren't dependable and reproducible - because of the nature of concurrency bugs its not generally possible to write unit tests that consistently either fail or succeed and so unit tests of concurrent code generally don't make very useful unit tests. Instead you unit test each single threaded components of your application as normal and rely on load testing sessions to identify concurrency issues. That said there are some experimental load testing frameworks for testing concurrent applications such as Microsoft CHESS - CHESS repeatedly runs a given unit test and systematically explores every possible interleaving of a concurrent test. This makes your unit tests dependable and reproducible. For the moment however CHESS is still experimental (and probably not usable with the JVM) - for now stick with load testing to weed out concurrency issues. Here is the link to CHESS http://chesstool.codeplex.com/. It seems like project has no activities  Try multithreadedTC http://code.google.com/p/multithreadedtc/ http://code.google.com/p/multithreadedtc-junit4/  Do not test multithreaded applications. Refactor the code to remove coupling between work that is done in different threads. Then test it separately.
441,A,"Junit - Test Hibernate Service Results I have a DAO service which retrieves and saves data to Hibernate. Could anybody point me to testing Methodology using JUnit for such service. What is the best practice? When should I have passed test failed test and should I test exception? I would also look into using DBUnit to ensure that your database is consistent and test independent. They have articles on how to get started and a tutorial. There a quite a few articles on IBM's DeveloperWorks blog as well that you might want to read.  You should test every possible path in your DAO. You would definitely use an in-memory database for your tests such as HSQLDB. One strategy would be to give the database some initial data in your setUp() method and clean it up on tearDown() so all tests have a consistent working environment. With these setup you can basically do everything... For example if you want to test a save() method in your DAO just add the new element and make sure that your table now has one more element. Moreover fetch that element and compare it to the one you've inserted it should obviously be the same. Remember that you should always test every possible path. Thanks for tearDown() and setUp() but how many test should I do for each method? Should I have pass/fail and exception tests? I can realize that time test is very interesting test in matter of database testings. Each path in the DAO method should be a separate test. For example if one DAO method has an if-else block that sets a condition on a criteria query it would make sense to have two tests for this method each of which tests one of the paths through the if-else. If you have custom logic in a DAO method which will throw an exception this also merits a test. However I wouldn't spend time testing exceptions thrown by the underlying framework (Hibernate JDBC etc) as you are testing someone else's code rather than your own - and you'd end up repeating the test in N places You should check the happy paths as well as the unhappy ones. For example what happens if you insert two elements that are ""equal"" on a field you have declared as UNIQUE? Do you handle that case? What if I try to update a non-existent element? You have to be able to answer these questions in order to provide a solid piece of code. If you throw an exception to signal some error be sure that the code is actually throwing it by writing a test that triggers that error.  I think using an in-memory database is often a good idea - for unit test. But the ideal is to combine fast unit tests with such a database with integration tests hitting the real database. There are subtle differences between e.g. Hypersonic SQL and most production servers. Consider using mocks for testing your error handling (i.e. mock the Hibernate session object and make it throw exceptions for certain operations)"
442,A,Organising JBehave stories We've just started looking at using JBehave for acceptance tests and I was wondering how people that are using it are organising the writing of stories and the storage of story files. It's just development that are working on them at the moment so we have the story files stored in the resources folder alongside the Java code to implement them. I guess my actual question is how and where are you storing your story files and how does this work with the product owner or QA writing stories? Apologies for that - I've only just seen the notifications bubble that tells me I have replies. Embarrassed. It's fun not a big issue MrWiggles @MrWiggles as t0rx told you are lucky to have QA to write stories/scenarios. coming to your question: Behaviour-Driven Development encourages you to start defining the stories via scenarios that express the desired behavior in a textual format. JBehave Stories you can run by configuring in Maven (pom.xml). You can make a folder for storing your story files in your package structure like below: Your_Project | | |--Source_Code | |--Stories | |--Testing | *pom.xml By configuring your stories in maven every time you build project it will give result with succeeded and failed stories/scenarios results. QA will update the scenarios in the folder Stories and the developer will implement the scenarios step by step by omitting existing steps (which are already developed and came in other scenarios). QA simply run the scenario/story and he will find out the result in a textual (understandable) format. Like below: Behaviour-Driven Development in test levels. Some of the JBehave features concentrate on easy organizing. Annotation-based configuration and Steps class specifications Dependency Injection support allowing both configuration and Steps instances composed via your favourite container (Guice PicoContainer Spring). Extensible story reporting: outputs stories executed in different human-readable file-based formats (HTML TXT XML). Fully style-able view. Auto-generation of pending steps so the build is not broken by a missing step but has option to configure breaking build for pending steps. Localisation of user stories allowing them to be written in any language. IDE integration: stories can be run as JUnit tests or other annotation-based unit test frameworks providing easy integration with your favourite IDE. Ant integration: allows stories to be run via Ant task Maven integration: allows stories to be run via Maven plugin at given build phase  If you are lucky enough to have the product owner or QA writing stories then you probably want them in a specific area of your source code repository so you can control access independently from your main source (and also give you more flexibility with when CI builds are triggered if you're doing that). You'll likely find a lot of back-and-forth to minimise the number of new steps the devs have to write (i.e. stop them using ten different ways to write the same step) so will also need to run with pending steps not failuring the scenario (which is the default out of the box). An alternative approach is that QA/product owner send scenarios to the devs who then cleanse them before adding to source control but this puts effort back on the devs.
443,A,"Runtime Exception in Android JUnit testing I have a simple HelloWorld Activity that I try to test with an Android JUnit test. The application itself runs like it should but the test fails with an ""java.lang.RuntimeException: Unable to resolve activity for: Intent { action=android.intent.action.MAIN flags=0x10000000 comp={no.helloworld.HelloWorld/no.helloworld.HelloWorld} } at no.helloworld.test.HelloWorldTestcase.setUp(HelloWorldTestcase.java:21)"" This is my activity class: package no.helloworld; import android.app.Activity; import android.os.Bundle; public class HelloWorld extends Activity { @Override public void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.main); } } And the test: public class HelloWorldTestcase extends ActivityInstrumentationTestCase2 { private HelloWorld myActivity; private TextView mView; private String resourceString; public HelloWorldTestcase() { super(""no.helloworld.HelloWorld"" HelloWorld.class); } @Override protected void setUp() throws Exception { super.setUp(); myActivity = this.getActivity(); mView = (TextView) myActivity.findViewById(no.helloworld.R.id.txt1); resourceString = myActivity .getString(no.helloworld.R.string.helloworld); } public void testPreconditions() { assertNotNull(mView); } public void testText() { assertEquals(resourceString (String) mView.getText()); } protected void tearDown() throws Exception { super.tearDown(); } Why does the test fail? The activity is (of course) defined in AndroidManifest.xml and the application runs as it should. The package in the constructor call should match the instrumentation target in the manifest. It should be ""no.helloworld"" instead of ""no.helloworld.HelloWorld"" ""The package in the constructor call should match the instrumentation target in the manifest"" - This helped me. I spent some hours with this problem."
444,A,"JUnit Easymock Unexpected method call I'm trying to setup a test in JUnit w/ EasyMock and I'm running into a small issue that I can't seem to wrap my head around. I was hoping someone here could help. Here is a simplified version of the method I'm trying to test: public void myMethod() { //Some code executing here Obj myObj = this.service.getObj(param); if (myObj.getExtId() != null) { OtherObj otherObj = new OtherObj(); otherObj.setId(myObj.getExtId()); this.dao.insert(otherObj); } //Some code executing there } Ok so using EasyMock I've mocked the service.getObj(myObj) call and that works fine. My problem comes when JUnit hits the dao.insert(otherObj) call. EasyMock throws a ""Unexpected Method Call"" on it. I wouldn't mind mocking that dao in my test and using expectLastCall().once(); on it but that assumes that I have a handle on the ""otherObj"" that's passed as a parameter at insert time... Which of course I don't since it's conditionally created within the context of the method being tested. Anyone has ever had to deal with that and somehow solved it? Thanks. If you can't get a reference to the object itself in your test code you could use EasyMock.anyObject() as the expected argument to yourinsert method. As the name suggests it will expect the method to be called with.. well any object :) It's maybe a little less rigorous than matching the exact argument but if you're happy with it give it a spin. Remember to include the cast to OtherObjwhen declaring the expected method call. Yep that did the trick thanks DoctorRuss. :)  The anyObject() matcher works great if you just want to get past this call but if you actually want to validate the constructed object is what you thought it was going to be you can use a Capture. It would look something like: Capture<OtherObj> capturedOtherObj = new Capture<OtherObj>(); mockDao.insert(capture(capturedOtherObj)); replay(mockDao); objUnderTest.myMethod(); assertThat(""captured what you expected"" capturedOtherObj.getValue().getId() equalTo(expectedId)); Also PowerMock has the ability to expect an object to be constructed so you could look into that if you wanted.  You could also use EasyMock.isA(OtherObj.class) for a little more type safety."
445,A,"GWT JUnit Testing I'm trying a very JUnit Testing for GWT put it is failing public class CheckTest extends TestCase { private ServiceAsync RPC; private HandlerManager EventBus; private CreateTopicPresenter.Display CTV; private CreateTopicPresenter CTP; protected void setUp() { RPC= createStrictMock(ServiceAsync.class); EventBus = new HandlerManager(null); CTV= createStrictMock(CreateTopicView.class); CTP= new CreateTopicPresenter(CTVRPCEventBus); } public void testCheck() { CTP.View.getFirstMessage().setValue(""MessageTest""); assertTrue(CTP.View.getFirstMessage().getValue().equals(""MessageTest"")); } Stack Trace:  java.lang.ExceptionInInitializerError at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at net.sf.cglib.proxy.Enhancer.setCallbacksHelper(Enhancer.java:619) at net.sf.cglib.proxy.Enhancer.setThreadCallbacks(Enhancer.java:612) at net.sf.cglib.proxy.Enhancer.registerCallbacks(Enhancer.java:581) at org.easymock.internal.ClassProxyFactory.createProxy(ClassProxyFactory.java:194) at org.easymock.internal.MocksControl.createMock(MocksControl.java:60) at org.easymock.EasyMock.createStrictMock(EasyMock.java:70) at com.BiddingSystem.client.CheckTest.setUp(CheckTest.java:26) at junit.framework.TestCase.runBare(TestCase.java:128) at junit.framework.TestResult$1.protect(TestResult.java:106) at junit.framework.TestResult.runProtected(TestResult.java:124) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:120) at junit.framework.TestSuite.runTest(TestSuite.java:230) at junit.framework.TestSuite.run(TestSuite.java:225) at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: java.lang.UnsupportedOperationException: ERROR: GWT.create() is only usable in client code! It cannot be called for example from server code. If you are running a unit test check that your test case extends GWTTestCase and that GWT.create() is not called from within an initializer or constructor. at com.google.gwt.core.client.GWT.create(GWT.java:91) at com.google.gwt.user.client.ui.UIObject.<clinit>(UIObject.java:188) ... 24 more You need to extend the GWTTestCase base type in order to test code that depends on the GWT runtime (e.g. calls to GWT.create() or JSNI methods). Note that the GWTTestCase must be GWT-compatible as well so this will limit your choice of testing libraries. Ideally most of your app code is testable with ""pure"" TestCases with only client-specific code being tested by GWTTestCases since the latter have a higher overhead. http://code.google.com/webtoolkit/articles/testing_methodologies_using_gwt.html"
446,A,"Running each JUnit test in a separate JVM in Eclipse? I have a project with nearly 500 individual tests in around 200 test classes. Some of these tests don't do a great job of tearing down their own state after they're finished and in Eclipse this results in some tests failing. The tests all pass when running the test suite from the command line via Ant. Can I enable 'test isolation' somehow in Eclipse? I don't mind if it takes longer to run. Long term I will clean up the misbehaving tests but in the short term I'd like to get the tests working. If you use Ant in Eclipse you can set the JUnit task to fork a new JVM process for each test providing isolation. http://ant.apache.org/manual/Tasks/junit.html @Michael - if the forkmode attribute is set to ""perTest"" then a new JVM should be created for each test - this is the default. It will only fork one new JVM for the entire test suite not one for each single test. Might still be useful. The link appears broken -- try this one: http://ant.apache.org/manual/Tasks/junit.html Ohh it worked when I first answered this -- I'll edit my answer :)"
447,A,"AbstractTransactionalJUnit4SpringContextTests: can't get the dao to find inserted data I'm trying to set up integration tests using the AbstractTransactionalJUnit4SpringContextTests base class. My goal is really simple: insert some data into the database using the simpleJdbcTemplate read it back out using a DAO and roll everything back. JPA->Hibernate is the persistence layer. For my tests I've created a version of the database that has no foreign keys. This should speed up testing by reducing the amount of fixture setup for each test; at this stage I'm not interested in testing the DB integrity just the business logic in my HQL. /* DAO */ @Transactional @Repository(""gearDao"") public class GearDaoImpl implements GearDao { @PersistenceContext private EntityManager entityManager; /* Properties go here */ public Gear findById(Long id) { return entityManager.find(Gear.class id); } } /* Test Page */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""/com/dom/app/dao/DaoTests-context.xml""}) @TransactionConfiguration(transactionManager=""transactionManager"" defaultRollback=false) public class GearDaoImplTests extends AbstractTransactionalJUnit4SpringContextTests { @Autowired private GearDao gearDao; @Test @Rollback(true) public void quickTest() { String sql; // fields renamed to protect the innocent :-) sql = ""INSERT INTO Gear (Gear_Id fld2 fld3 fld4 fld5 fld6 fld7) "" + "" VALUES (???????)""; simpleJdbcTemplate.update(sql 1L 1L 1L ""fld4"" ""fld5"" new Date() ""fld7""); assertEquals(1L simpleJdbcTemplate.queryForLong(""select Gear_Id from Gear where Gear_Id = 1"")); System.out.println(gearDao); Gear gear = gearDao.findById(1L); assertNotNull(""gear is null."" gear); // <== This fails. } } The application (a Spring MVC site) works fine with the DAO's. What could be happening? And where would I begin to look for a solution? The DAO somehow has a different dataSource than the simpleJdbcTemplate. Not sure how this would be though since there's only one dataSource defined in the DaoTests-context.xml file. Hibernate requires all foreign key relations to be present in order to select out the Gear object. There are a couple of joins that are not present since I'm hardcoding those in fld2/fld3/fld4. The DAO won't act on the uncommitted data. But why would the simpleJdbcTemplate honor this? I'd assume they both do the same thing. Underpants gnomes. But where's the profit? This post might provide some inspiration http://stackoverflow.com/questions/3891624/migrating-junit-jpa-tests-from-spring-2-5-5-to-spring-3-0-4 What a difference a couple hours of sleep makes. I woke up and thought ""I should check the logs to see what query is actually being executed."" And of course it turns out that hibernate was configured to generate some inner joins for a few of the foreign keys. Once I supplied those dependencies it worked like a charm. I'm loving the automatic rollback on every test concept. Integration tests here I come!"
448,A,"Not hitting breakpoints debugging Android Junit tests in Eclipse Getting an error running a Android unit test which extends ActivityInstrumentationTestCase2 and so want try and debug the test. However the break points are being ignored and when debug as Android Junit Test it gets to Running tests... Here's the console log: [2011-03-16 17:02:54 - AppName] Launching instrumentation android.test.InstrumentationTestRunner on device [my device ID here] [2011-03-16 17:02:55 - AppName] Collecting test information [2011-03-16 17:02:55 - AppName] Test run failed: Process crashed. [2011-03-16 17:02:55 - AppName] Test run finished [2011-03-16 17:02:56 - AppName] Sending test information to Eclipse [2011-03-16 17:02:56 - AppName] Running tests... Any ideas? Are you sure the debugger is attaching to the process? Are you sure the breakpoints are being accepted by Eclipse? (They change from a bullet icon to a bullet-overlaid-with-a-checkmark icon when Eclipse sends them to the VM.) @RobertKarl something wasn't getting initialised correctly and so the debugger never connected. But tbh i'm struggling to remember how i fixed it. I am seeing this problem as well. How did the answer below help? Check if the application you are testing has defined android:debuggable=""true"". If your application is not debuggable running the tests in debug mode does not work. E.g. you will not hit any breakpoints and you get the error message in the log as mentioned above.  It should look like this: [2011-03-15 22:46:12 - TemperatureConverterTest] Collecting test information [2011-03-15 22:46:13 - TemperatureConverterTest] Sending test information to Eclipse [2011-03-15 22:46:13 - TemperatureConverterTest] Running tests... **[2011-03-15 22:46:14 - TemperatureConverterTest] Attempting to connect debugger to 'com.example.aatg.tc' on port 8654** [2011-03-15 22:46:31 - TemperatureConverterTest] Test run finished Notice that your test is crashing even before getting the debugger connected. There might be a problem in you constructor."
449,A,"Is there any reason in Junit not to create your objects at the declaration? Is there any reason to put object creation inside of setUp() rather than at an instance variable declaration? I've seen it done this way in books but the effect is the same and I'm not sure if it was done for a best practice reason because an earlier version of Junit did not instantiate the object for each test (see here) or if it's just a style thing. How were you able to edit my post? I had read that but hadn't read far enough down I guess. It's down there with ""Punch people in the nose"" and ""Use loose for lose"". http://stackoverflow.com/faq - What is reputation? If the instantiation of the object in question does not depend on external factors it is perfectly OK to declare and define it at once. However often it depends on other factors (e.g. initialization of a singleton*) or requires constructor parameters - some of which may even be test-dependent - or its initialization takes multiple steps. Then you have to defer instantiation to the setup method or even to the test method itself. Note that JUnit creates a new instance of the test class thus a new instance of its data members for each test method execution anyway. So if you have none of the dependencies mentioned above semantically there is no difference between instantiating a member at the point of declaration or in the setup method. *this is one of the reasons Singletons are not liked. However often you still have them especially in legacy code. thank you this makes sense"
450,A,"DbUnit: problem with increment id generation I am using DbUnit together with Unitils which works great most of the time. Today I found a strange problem. Situation is: I use Hibernate and have id with ""increment"" generator:  <id name=""Id""> <generator class=""increment""/> </id> I prepare test dataset where maximal id is 5. I use clean-insert loading strategy. I have two test methods test1 and test2 each adding one row in this table. After test1 method newly added row has id=6. After test2 method newly created row has id=7. This is all OK and I get why this is like that. It is a problem from maintenance perspective though. If I ever add third test method between the two method test2 will suddenly fail even though nothing changed just because row will get different id. Is there anyway I can force DbUnit or Hibernate to calculate next id value before each test method? First thing you should provide the complete dataset yes with id as well. If not don't test or base your test upon ids. Why not test? because its already well tested and reliable thing. Always remember never test third party libraries most of them already well tested. But it seems quite impossible to not depending upon ids. I agree you should write some mock class to tackle this issue for you or may be you can provide some setter method to overwrite the value generated by your own. Second option is always start your test case with empty table. You can write a fixture to clean the table for you before every test case. rightly said as well  The solution is not to rely on generated ids: they are outside the control of your test. if you make them controlled by the test you are no longer testing the class-under-test I think I implied ""don't do it"" ;) Read Vinegar's answer for better understanding. Rightly said. +1 How can I do this? Is there a way to ignore key columns somehow? I don't know I have no information about your test class and class-under-test I meant how to do this in DbUnit and Unitils?"
451,A,"Spring and JUnit annotated tests: creating fixtures in separate transactions I am testing my Hibernate DAOs with Spring and JUnit. I would like each test method to start with a pre-populated DB i.e. the Java objects have been saved in the DB in a Hibernate transaction that has already been committed. How can I do this? With @After and @Before methods execute in the same Hibernate transaction as the methods decorated with @Test and @Transactional (first level cache may not be flushed by the time the real test method starts). @BeforeTransaction and @AfterTransaction apparently cannot work with Hibernate because they don't create transactions even if the method is annotated with @Transactional in addition to @Before/AfterTransaction. Any suggestion? dbUnit is a good framework for the job. In short it should be started from the setUp() method and it deletes all contents from specified tables then fill them up with content from a XML file. Else you can try to execute the setUp() method in a new transaction like this: @Before @Transactional(propagation=Propagation.REQUIRES_NEW) public void setUp() { // initial logic .. }  One way could be to externalize your intialization logic to an external service with transactional methods which are executed from your @BeforeTransaction and @AfterTransaction annotated methods in the test class. Another benefit of this approach is the reusability of initialization code across tests. You could for example use the SpringJunit4ClassRunner like described here like this: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""testContext.xml""""services.xml""}) public class MyServiceTest { @Autowired private TestDataService testDataService; @Before public void setUp(){ testDataService.addTestData(); } @Test public void testSomething() throws Exception { // ... } } public interface TestDataService { void addTestData(); } public class TestDataServiceImpl implements TestDataService { @Transactional public void addTestData(){ // TODO } } That's something we do in our Spring based projects. Make sure the transactional configuration is correct. If you want to avoid the class/interface separation then set proxy-target-class to true in the element. With JUnit 4.5 and Spring 3 I cannot inject in my test classes services annotated with @Transactional. Services not annotated with @Transactional can be injected just fine though. @Francois do they simply not get injected or do they throw any Exception? Also do yo use Interface/Class separation? Autowired matches by type not by name. I need some more information to help you. Thanks Timo I like your answer but I could not inject such an initialization service into my test object (simple DAOs and beans that do no depend on a Hibernate session factory work great though). I have the impression that the application context startup is not standard. Any idea of what might be going on? Thanks Timo. Do they simply not get injected or do they throw any Exception? They throw an exception when injected. If I list the beans from the application context they are here but casting them fails. If I remove all @Transaction annotations it works. Also do yo use Interface/Class separation? The TestSetup service family is entirely separate from the test classes. The TestService family follows a simple singletons / autowire by type paradigm. I think so too. Thanks for your help though. Your solution should work. Ok my guess is that something goes wrong with Spring AOP proxying I posted an example that should work."
452,A,"Problems using DbUnit with Spring TestContext I'm trying to test my DAO layer (which is built on JPA) in separation. In the unit test I'm using DbUnit to populate the database and Spring Test to get an instance of ApplicationContext. When I tried to use the SpringJunit4ClassRuner the ApplicationContext got injected but the DbUnit's getDataSet() method never got called. @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = ""/testdao.xml"") public class SimpleJPATest extends DBTestCase implements ApplicationContextAware { ... Then I tried to remove the @RunWith annotation that removed the problems with getDataSet() method. But now I no longer get ApplicationContext instance injected. I tried to use the @TestExecutionListeners annotation which is supposed to configure the DependencyInjectionTestExecutionListener by default but the AppContext still doesn't get injected. @TestExecutionListeners @ContextConfiguration(locations = ""/testdao.xml"") public class SimpleJPATest extends DBTestCase implements ApplicationContextAware { ... Does anyone have any ideas? Is it generally a bad idea to combine these two frameworks? EDIT: here is the rest of the source for the test class: @TestExecutionListeners @ContextConfiguration(locations = ""/testdao.xml"") public class SimpleJPATest extends DBTestCase implements ApplicationContextAware { static final String TEST_DB_PROPS_FILE = ""testDb.properties""; static final String DATASET_FILE = ""testDataSet.xml""; static Logger logger = Logger.getLogger( SimpleJPATest.class ); private ApplicationContext ctx; public SimpleJPATest() throws Exception { super(); setDBUnitSystemProperties(loadDBProperties()); } @Test public void testSimple() { EntityManagerFactory emf = ctx.getBean(""entityManagerFactory"" EntityManagerFactory.class); EntityManager em = emf.createEntityManager(); GenericDAO<Club> clubDAO = new JpaGenericDAO<Club>(ClubEntity.class ""ClubEntity"" em); em.getTransaction().begin(); Collection<Club> allClubs = clubDAO.findAll(); em.getTransaction().commit(); assertEquals(1 allClubs.size()); } @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.ctx = applicationContext; } private void setDBUnitSystemProperties(Properties props) { System.setProperty(PropertiesBasedJdbcDatabaseTester.DBUNIT_DRIVER_CLASS props.getProperty(""db.driver"")); System.setProperty(PropertiesBasedJdbcDatabaseTester.DBUNIT_CONNECTION_URL props.getProperty(""db.url"")); System.setProperty(PropertiesBasedJdbcDatabaseTester.DBUNIT_USERNAME props.getProperty(""db.username"")); System.setProperty(PropertiesBasedJdbcDatabaseTester.DBUNIT_PASSWORD props.getProperty(""db.password"")); } private Properties loadDBProperties() throws Exception { URL propsFile = ClassLoader.getSystemResource(TEST_DB_PROPS_FILE); assert (propsFile != null); Properties props = new Properties(); props.load(propsFile.openStream()); return props; } @Override protected void setUpDatabaseConfig(DatabaseConfig config) { config.setProperty( DatabaseConfig.PROPERTY_DATATYPE_FACTORY new HsqldbDataTypeFactory() ); } @Override protected DatabaseOperation getSetUpOperation() throws Exception { return DatabaseOperation.CLEAN_INSERT; } @Override protected DatabaseOperation getTearDownOperation() throws Exception { return DatabaseOperation.DELETE_ALL; } @Override protected IDataSet getDataSet() throws Exception { logger.debug(""in getDataSet""); URL dataSet = ClassLoader.getSystemResource(DATASET_FILE); assert (dataSet != null); FlatXmlDataSet result = new FlatXmlDataSetBuilder().build(dataSet); return result; } } The reason why none of the DbTestCase methods get called is undoubtebly the fact that DbTestCase class uses JUnit 3.8 API and I'm trying to run in with SpringJUnit4ClassRunner. I've managed to use DbUnit with JUnit 4. This blog post helped me a lot (looking through the source of few key DbUnit classes didn't hurt either). http://ralf.schaeftlein.de/2009/01/05/dbunit-with-junit-4x-and-spring-for-testing-oracle-db-application/ I've used these two frameworks together without any issues. I've had to do some things a little different from the standard though to get it to work: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:applicationContext.xml"" }) @TestExecutionListeners({ DependencyInjectionTestExecutionListener.class DirtiesContextTestExecutionListener.class }) public class MyDaoTest extends DBTestCase { @Autowired private MyDao myDao; /** * This is the underlying BasicDataSource used by Dao. If The Dao is using a * support class from Spring (i.e. HibernateDaoSupport) this is the * BasicDataSource that is used by Spring. */ @Autowired private BasicDataSource dataSource; /** * DBUnit specific object to provide configuration to to properly state the * underlying database */ private IDatabaseTester databaseTester; /** * Prepare the test instance by handling the Spring annotations and updating * the database to the stale state. * * @throws java.lang.Exception */ @Before public void setUp() throws Exception { databaseTester = new DataSourceDatabaseTester(dataSource); databaseTester.setDataSet(this.getDataSet()); databaseTester.setSetUpOperation(this.getSetUpOperation()); databaseTester.onSetup(); } /** * Perform any required database clean up after the test runs to ensure the * stale state has not been dirtied for the next test. * * @throws java.lang.Exception */ @After public void tearDown() throws Exception { databaseTester.setTearDownOperation(this.getTearDownOperation()); databaseTester.onTearDown(); } /** * Retrieve the DataSet to be used from Xml file. This Xml file should be * located on the classpath. */ @Override protected IDataSet getDataSet() throws Exception { final FlatXmlDataSetBuilder builder = new FlatXmlDataSetBuilder(); builder.setColumnSensing(true); return builder.build(this.getClass().getClassLoader() .getResourceAsStream(""data.xml"")); } /** * On setUp() refresh the database updating the data to the data in the * stale state. Cannot currently use CLEAN_INSERT due to foreign key * constraints. */ @Override protected DatabaseOperation getSetUpOperation() { return DatabaseOperation.CLEAN_INSERT; } /** * On tearDown() truncate the table bringing it back to the state it was in * before the tests started. */ @Override protected DatabaseOperation getTearDownOperation() { return DatabaseOperation.TRUNCATE_TABLE; } /** * Overridden to disable the closing of the connection for every test. */ @Override protected void closeConnection(IDatabaseConnection conn) { // Empty body on purpose. } // Continue TestClass here with test methods. I've had to do things a little more manual than I would like but the same scenario applies if you try to use the JUnit Parameterized runner with Spring (in that case you have to start the TextContext manually). The most important thing to note is that I override the closeConnection() method and leave it blank. This overrides the default action of closing the dataSource connection after each test which can add unnecessary time as the connection will have to be reopened after every test."
453,A,What happens if I terminate jUnit tests I have a weird question. If I get somehow stuck in my tests I have to terminate them. But still I need to destroy a background process. Meaning I need some method that is called on the jUnit termination to override. Afterclass method or Finalize aren't it. Is there something? No when you terminate a JVM there's not much you can do to react. Depending on how you stop it shutdown hooks may or may not run but that's pretty much your only chance.
454,A,"jUnit ignore @Test methods from base class Let's say I have a test class called testFixtureA with several methods testA testB testC etc each with @Test annotation. Let's now say I subclass testFixtureA into class called testFixtureAB and I don't overwrite anything. testFixtureAB is empty as for now. When I run tests from testFixtureAB methods testA testB and testC are executed by test runner because test runner doesn't distinguish between test methods from class and baseclass. How can I force test runner to leave out tests from baseclass? It's quite easy to achieve implementing some few classes: Create your own TestRunner Create an annotation like @IgnoreInheritedTests Create a class that extends org.junit.runner.manipulation.Filter On the filter class: public class InheritedTestsFilter extends Filter { @Override public boolean shouldRun(Description description) { Class<?> clazz = description.getTestClass(); String methodName = description.getMethodName(); if (clazz.isAnnotationPresent(IgnoreInheritedTests.class)) { try { return clazz.getDeclaredMethod(methodName) != null; } catch (Exception e) { return false; } } return true; } @Override public String describe() { // TODO Auto-generated method stub return null; } } on your custom runner:  /** * @param klass * @throws InitializationError * @since */ public CustomBaseRunner(Class<?> klass) throws InitializationError { super(klass); try { this.filter(new InheritedTestsFilter()); } catch (NoTestsRemainException e) { throw new IllegalStateException(""class should contain at least one runnable test"" e); } }  and I don't overwrite anything. testFixtureAB is empty as for now There's your answer. If you want to not run testB from the main class overrride it: public class testFixtureAB extends testFixtureA { @override public void testB() {} } Easy and obvious solution +1  In the base test class' @Test methods: assumeTrue(getClass().equals(BaseClassTest.class)); It will ignore those in the subclass tests but not completely leave them out.  Restructure your test classes. If you don't want to use the tests from the baseclass then don't extend it If you need other functionality from the base class split that class in two - the tests and the other functionality  In the latest JUnit you can use the @Rule annotation on the subclass to inspect the test name and intercept the test run to ignore the test dynamically. But I would suggest that @Bozho's idea is the better one - the fact that you need to do this indicates a bigger problem that probably shows inheritance is not the right solution here.  I know it's not the answer... Consider the reason why you extend concrete test classes. You do duplicate test methods that way. If you share code between tests then consider writing base test classes with helper and fixture setup methods or test helper class. If for running tests then try organizing tests with suites and categories.  ignoring the whole base class: @Ignore class BaseClass { // ... } check out this example This will ignore the base class everywhere making it basically useless. inherited test methods will be executed as tests in the derived classes...  What if you want to execute the same test for different configurations of the same test suite? For example let's say you have Class A with test1 test2 and test3 methods that hit an embedded database then you want to create separated ""setUp"" and ""tearDown"" for every embedded vendor (H2 HyperSQL etc) but runs the same testing for each one. I would like to extend a class that contain those test methods and configure it in a subclass. My problem is that the super class SHOULD NOT be considered as eligible for the test runner. The problem arises when the test runner executes the super class and given that don't found the corresponding setup and teardown methods it crashs :("
455,A,"Monitoring Grails performance with Hudson I'm working on a project where performance is important and I would like to be able to monitor how my changes affect performance over time. Has anyone done this with Hudson before? http://wiki.hudson-ci.org//display/HUDSON/Performance+Plugin This Hudson plugin mentions something about supporting ""JUnit format"" but I can't find anything about it. Any pointers? Thanks! One thing you could do is create an integration test that runs a set of common tasks on the application then thread the test (obviously this will require some though so that the test is valid) and take the times of each run (10 threads 50 threads 100 threads) you will need to perform each test a few times and take average run-times so that you're not dealing with anomalous data Decide what is acceptable and have Hudson fail the project if it creeps over You may want to look at stress testing articles for this kind of question too  To monitor Grails performance there is the JavaMelody Grails plugin: http://www.grails.org/plugin/grails-melody And if there are also integration tests for the app (for example using selenium or jmeter with hudson) then you just have to look at the monitoring report in the running app after some tests: graphics and statistics in the reports are persisted on disk by JavaMelody.  I would suggest looking at jMeter & the jMeter Ant task. This would allow you to perform jMeter performance tests from Ant/Hudson. Depending on your performance testing needs there are multiple jMeter samplers you could use (Http JMS TCP JUnit etc.)."
456,A,"JUnit assertion methods should be phrased in the positive or the negative? Should I be writing assertTrue(""User logged in"" user.isLoggedIn()); or assertTrue(""User is not logged in"" user.isLoggedIn()); The former provides better reading inside the source files: ""I assert that the following is true: User logged in."" The error message could be read both ways: java.lang.AssertionError: User logged in ""There is an error in asserting that the user is logged in"" ""The error is that the user is logged in."" JUnit documentation doesn't provide a clear guide to which it should be except it is ""the identifying message for the {@link AssertionError}"" And in both cases the text identifies the test being run. What's the common usage? Interesting I would use: assertTrue(""user should be logged in"" user.isLoggedIn()); which tells me what is expected state of this assertion. I think the best choice is the one you understand.  You could use : assertTrue(""Test if user is logged in"" user.isLoggedIn()); When you do this you're verifying that user.isLoggedIn() is true you can't really say that user is logged in or not you don't know yet you're just testing it.  You should include both cases. You have better test case when you triangulate your assertions.  To avoid that question I more and more tend to use assertThat instead of ""low-level"" assert* methods. Indeed like this article explains assertThat will give you a very clear error message in case of failure.  Well you could also state your assumption and then how the assumption didn't hold. Like so: assertTrue(""Expected user to be logged it and wasn't"" user.isLoggedIn()); Makes for clearer messages but longer to type and read. +1 it's absolutely worth being as clear as possible - when a failure pops up in the midst of many hundreds of tests you want as much context as possible to help you get your head into the reason for it This is the negative form then. @jjnguy I disagree. ""Absolutely"" knowing the context could mean you would printout dump of complete OS memory (a bit overexaggerating). The point is to provide just the relevant information. And positive assertion texts are just for this purpose.  Be strictly positive in your assert messages Use positive assert text like in your first example or like: assertTrue(""User is logged in"" user.isLoggedIn()); The reasons are: positive assertion is shorter there is one asserted condition you are checking and many possible reasons why it went wrong. Do not attempt to detect the cause just state what assertion has failed. it is more readable in your code. It is generally recommended to code in positive expressions which save few negations of the conditions in the mind of the reader it is still readable in error traces which are not supposed to be understood by common user but by a programmer who will end up in code anyway. And even sysadmin who would not have access to the code would provide author with complete error message and programmer would understand it comes from an assert. Trying to provide ""all contextual information"" in assert message is not improving the situation it rather creates information mess. You know good programmers debug a code and provide working and shorter code. Take using positive assert messages as first thing to do in this direction. The other direction - patching the code with more and more unnecessary stuff is paving way to the programming hell.  How about: assertTrue(""User should be logged in"" user.isLoggedIn()); Works both ways. I like this short but to the point."
457,A,why testing an individual junit test works while testing them together won't? The test that fails when tested together with mvn test (or through the ide) is called EmpiricalTest. If I test the file alone it goes through but not otherwise. Why could that be? You can checkout the Maven source code (to test) from here. This is how I make sure the database is 'blank' before each test: abstract public class PersistenceTest { @Before public void setUp() { db.destroy(); assertIsEmpty(MUser.class); assertIsEmpty(Meaning.class); assertIsEmpty(Expression.class); } private <Entity> void assertIsEmpty(final Class<Entity> entityClass){ final List<Entity> all = db.getAll(entityClass); Assert.assertTrue(all.isEmpty()); } and the test that fails: public class EmpiricalTest extends PersistenceTest { It got to do with the id automatically assigned. The PU creates a SEQUENCE table and although I empty the database from my entities I don't actually drop that table. So when I'm testing EmpiricalTest alone the sequence starts as expected from 1 while when testing together the test is executed later and starts with a higher unexpected number. This leads to this question. because as we discussed in another question I test from 'outside'. That is the transaction is committed from within the database code I test as a client. I insert data (and here db code commits) and then check that the data i retrieve about what has been persisted is up to expectations. Why don't you run your tests inside a transaction and rollback the tx after each test?  Your problem is very common. In ideal TDD world each test should be executed in the perfect isolation from the other test. You violated the isolation and that's the problem. However there is no simple solution for the test isolation problem. The main reason is that SQL DLL doesn't support database creation/deletion while automatically droping tables is complicated due to the possible complex foreign keys constrains. In my experience the best idea is to execute tests within transaction and rollback data on the end of the test (just like Pascal suggested). Spring test module provides great support for that. In case you cannot execute test within the transaction boundaries (like yours) you must be sure that each of your test doesn't leave anything in the database (including foreignkeys constrains sequences etc.) and also that tests are designed to be independent of each others (for example don't depend on autogenerated id value because sequence generation could be executed in previous tests). You must debug your Maven test session order to check out what is wrong with the assertion (I guess that you cannot tell that from the Surefire logs). Then fix the tests (both the failing one and the other one which leaves the rubbish in the DB) to be isolated from each others.  This very much sounds as if there are dependencies between the test. As far as I understand from looking at your test you're accessing the your data storage in the test. Is there a chance that one of the tests doesn't properly cleanup his traces therefore causing others to fail?? Testing against a DB is usually not considered a unit test though it is very useful. Those kind of tests (you may call them integration tests) are however more difficult and time consuming to code because you have to pay a lot of attention that your test leaves the environment in the exact state it found it before.
458,A,"Ignore Assertion failure in a testcase (JUnit) Currently I am writing the automation testing using java and selenium rc. I would like to verify all the contents present on the user interface the function is ie below: public String UITest() throws IOException { String result=""Test Start<br />""; try { openfile(1); for (String url : uiMaps.keySet()) { selenium.open(url); for (String item : uiMaps.get(url)) { assertEquals(url+"" check: "" + item trueselenium.isTextPresent(item)); result+=url+"" check: "" + item+"" : OK<br />""; } } } catch (AssertionError e) { result+=e.getMessage(); } result+=""Test finished<br />""; return result; } the function suppose return a String contains information about the testing. However the function stopped once there is an assertion error happened. So I want to know whether there is a way to ignore the failure and keep executing all the assertion verifications. Thanks for any help You could use a JUnit 4 error collector rule: The ErrorCollector rule allows execution of a test to continue after the first problem is found (for example to collect all the incorrect rows in a table and report them all at once) For example you can write a test like this. public static class UsesErrorCollectorTwice { @Rule public ErrorCollector collector= new ErrorCollector(); @Test public void example() { String x = [..] collector.checkThat(x not(containsString(""a""))); collector.checkThat(y containsString(""b"")); } } The error collector uses hamcrest Matchers. Depending on your preferences this is positive or not. Due the accessibility of the errors list of original errorcollector I implemented my own errorcollector by inheriting the junit one. Now it works. Thanks a lot for the help. It seems it will execute all the testings even there is one failed. But I got another question. I check the API and it says that all the incorrect rows in a table and report them all at once. My question is that any way I can grab the information of the table? Thanks. @user638297 Yes it's not to hard to implement a MethodRule that does exactly what you want. Could you gather all the information you where looking for?  From Selenium documentation: All Selenium Assertions can be used in 3 modes: ""assert"" ""verify"" and ""waitFor"". For example you can ""assertText"" ""verifyText"" and ""waitForText"". When an ""assert"" fails the test is aborted. When a ""verify"" fails the test will continue execution logging the failure. This allows a single ""assert"" to ensure that the application is on the correct page followed by a bunch of ""verify"" assertions to test form field values labels etc. I thought the problem is the assertEquals function of JUnit rather than selenium since the function fo selenium I used is ""isTextPresent"" and it is only in charge of returning a boolean value. Correct me if I am wrong. Thanks. According to the documentation you always have choice between: *assert* TextPresent *verify* TextPresent and *is* TextPresent. Each version has different behavior.  I'm sure you've figured it out yourself by now: the try-catch should be inside the for loop not outside of it ;)  Don't assert anything in the function. Return say null instead and have whoever's calling it to keep going but then fail if the function returned null"
459,A,"Many small dbunit data sets or one large one? Spreading test data across multiple small data sets seems to me to create a maintenance headache whenever the schema is tweaked. Anybody see a problem with create a single larger test data set? By ""larger"" I'm still only talk about a couple hundred records in total. I would not use a unique large dataset (you want to avoid any overhead if you don't need it) and follow DbUnit's Best Practices recommendations: Use multiple small datasets Most of your tests do not require the entire database to be re-initialized. So instead of putting your entire database data in one large dataset try to break it into many smaller chunks. These chunks could roughly corresponding to logical units or components. This reduces the overhead caused by initializing your database for each test. This also facilitates team development since many developers working on different components can modify datasets independently. For integrated testing you can still use the CompositeDataSet class to logically combine multiple datasets into a large one at run time. Some more feedback from the Unitils folks: Automatic test database maintenance When writing database tests keep in mind following guidelines: Use small sets of test data containing as few data as possible. In your data files only specify columns that are used in join columns or the where clause of the tested query. Make data sets test class specific. Don't reuse data sets between different test classes for example do not use 1 big domain data set for all your test classes. Doing so will make it very difficult to make changes to your test data for a test without braking anything for another test. You are writing a unit test and such a test should be independent of other tests. Don't use too many data sets. The more data sets you use the more maintenance is needed. Try to reuse the testclass data set for all tests in that testclass. Only use method data sets if it makes your tests more understandable and clear. Limit the use of expected result data sets. If you do use them only include the tables and columns that are important for the test and leave out the rest. Use a database schema per developer. This allows developers to insert test data and run tests without interfering with each other. Disable all foreign key and not null constraints on the test databases. This way the data files need to contain no more data than absolutely necessary Using small datasets with just enough data has worked decently for us in the past. Sure there is some maintenance if you tweak the database but this is manageable with some organization. I think the best point they make is ""Doing so will make it very difficult to make changes to your test data for a test without braking anything for another test."" Also the point about multiple developers is well taken. Thanks for the info. @HDave: Yes this is a very good point. Actually I started to put some parts in bold (including this one) but at the end almost 3/4 was is bold so I decided to quote it ""as is""."
460,A,"How to instantiate a shared resource in JUnit I noticed that jUnit runs the constructor of my test class for each method being tested. Here's an example:  public class TestTest { protected BigUglyResource bur; public TestTest(){ bur=new BigUglyResource(); System.out.println(""TestTest()""); } @Test public void test1(){ System.out.printf(""test1()\n""); } @Test public void test2(){ System.out.printf(""test2()\n""); } @Test public void test3(){ System.out.printf(""test3()\n""); } } Gives the following result:  TestTest() test1() TestTest() test2() TestTest() test3() Calling the constructor to BigUglyResource is too time-consuming I'd prefer to build it only once. I know you can use @BeforeClass to run a method once but @BeforeClass is only for static methods. Static methods can't access a class property like BigUglyResource in the example above. Other than building a Singleton what options are there? You could make ""bur"" static: protected static BigUglyResource bur; And use @BeforeClass.  Can't you declare the BigUglyResource static? This is how I normally do it. private static BigUglyResource bur; @BeforeClass public static void before(){ bur=new BigUglyResource(); } I'm surprized this didn't occur to me."
461,A,"Java JUnit: The method X is ambiguous for type Y I had some tests working fine. Then I moved it to a different package and am now getting errors. Here is the code: import static org.junit.Assert.*; import java.util.HashSet; import java.util.Map; import java.util.Set; import org.jgrapht.Graphs; import org.jgrapht.WeightedGraph; import org.jgrapht.graph.DefaultWeightedEdge; import org.jgrapht.graph.SimpleWeightedGraph; import org.junit.*; @Test public void testEccentricity() { WeightedGraph<String DefaultWeightedEdge> g = generateSimpleCaseGraph(); Map<String Double> eccen = JGraphtUtilities.eccentricities(g); assertEquals(70 eccen.get(""alpha"")); assertEquals(80 eccen.get(""l"")); assertEquals(130 eccen.get(""l-0"")); assertEquals(100 eccen.get(""l-1"")); assertEquals(90 eccen.get(""r"")); assertEquals(120 eccen.get(""r-0"")); assertEquals(130 eccen.get(""r-1"")); } The error message is this: The method assertEquals(Object Object) is ambiguous for the type JGraphtUtilitiesTest How can I fix this? Why did this problem occur as I moved the class to a different package? tell us how your class is declared. Looks to me as if you've inherited from JUnit3 and then tried to statically import from JUnit4. yeah actually I had JUnit3 in package A and used JUnit4 in package B where I originally wrote these tests. Then I switched from Package B to Package A and the problem arose. But I don't see anything in this class that would indicate JUnit 3. Where is that declared? @Rosarch Are these JGraphtUtilities available anywhere? I can't see methods to produce eccentricities in JGraphT! The method assertEquals(Object Object) is ambiguous for the type ... What this error means is that you're passing a double and and Double into a method that has two different signatures: assertEquals(Object Object) and assertEquals(double double) both of which could be called thanks to autoboxing. To avoid the ambiguity make sure that you either call assertEquals(Object Object) (by passing two Doubles) or assertEquals(double double) (by passing two doubles). So in your case you should use: assertEquals(Double.valueOf(70) eccen.get(""alpha"")); Or: assertEquals(70.0d eccen.get(""alpha"").doubleValue()); ok or I could just switch it to use JUnit 4 instead of JUnit 3. How do I do that? Anyway shouldn't it be assertEquals(70.0d eccen.get(""alpha"")); ? The solution is not really to switch from one version to the other. Instead help the compiler and remove the ambiguity as I suggested. but it wasn't a problem in JUnit 4 and I'd like to avoid changing a bunch of things in my code. @mahller Not sure who you are talking to but even if it's more correct than the OP's code it still ambiguous if the version of JUnit has both `assertEquals(Object Object)` and `assertEquals(double double)` which is the case of JUnit 4.4 4.5. But as I said changing the version of JUnit is not the real solution just fix the problem. @Rosarch For this particular case it isn't a problem in JUnit 3.8.1 it isn't a problem in JUnit 4.3 it **is** a problem in JUnit 4.4 it **is** a problem in JUnit 4.5 (but the method taking 2 doubles is deprecated) it isn't a problem in JUnit 4.6 (the method has been removed). So make your choice but you should fix the code. Double.valueOf(70) helped me I am using junit 4.4. I met the same issue. Your solution helps me much. thanks."
462,A,"Trouble running JUnit from Ant Here's the target I'm using to run my tests: <target name=""run-tests"" description=""run the tests"" depends=""compilation""> <junit> <sysproperty key=""tests.basedir"" value=""${SPECIAL_PATH}/unit_tests""/> <classpath> <pathelement location=""${COMPILED_CLASSES}""/> <pathelement location=""${basedir}/junit-4.8.1.jar""/> </classpath> <batchtest> <fileset dir=""${COMPILED_CLASSES}/unit_tests/""> <include name=""**/Test*.class""/> <exclude name=""**/*$*""/> </fileset> </batchtest> </junit> </target> However every time I try to run this target all my tests fail with something like: [junit] java.lang.ClassNotFoundException: testpackage.TestMyClass [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:251) [junit] at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319) [junit] at java.lang.Class.forName0(Native Method) [junit] at java.lang.Class.forName(Class.java:247) The SPECIAL_PATH property points to the source code of the classes. The COMPILED_CLASSES property points to the place the .class files have been put. And I need the tests.basedir property because I use it from my unit tests. What am I doing wrong? EDIT:I also thought I should explain the exclude of the $. I'm excluding anonymous classes because they don't represent TestCases they're only used from them. Since your claspath has ${COMPILED_CLASSES} and your test classes are in ${COMPILED_CLASSES}/unit_tests they would need to be in package unit_tests.<whatever the classpath is> traditionally this is why people compile normal sources to target/classes and test sources to target/test-classes  You might need to compile the UnitTests. Could you post where you use the javac task?  You are telling Junit to execute each test class in ${COMPILED_CLASSES}/unit_tests/ but you are putting just ${COMPILED_CLASSES} on the classpath. You probably need to change your classpath entry to <pathelement location=""${COMPILED_CLASSES}/unit_tests/""/> Shouldn't Java descend automatically in that folder? And if I have 20 packages for tests I need to add them all in the classpath? @Geo For folders it needs start at the package root. Currently junit thinks the class should be unit_tests.testpackage.TestMyClass instead of testpackage.TestMyClass @Kevin I tried to run a class by it's own using the `test` task and it works. It's using the same classpath as the one posted by me. Any ideas why that could be? Might be helpful to post the configuration that works to figure out why the original does not It's exactly the same. Kevin based on your comment with the `unit_tests.testpackage.TestMyClass` package suggestion I made it work. When I put only `COMPILED_CLASSES` on the fileset it worked. If you care to write an answer I'll mark it. So you changed your package name to match your Ant configuration? No I only removed the `unit_tests` from the string passed to the `dir` property of fileset. Kevin's comment ( and running a single test with the `test` task ) made me realize the bug."
463,A,"Unit testing a broadcast receiver? Here's a BroadcastReceiver from my project which I'm looking to unit test. When the user makes a phone call it grabs the phone number and sets up an intent to start a new activity passing in the phone number. public class OutgoingCallReceiver extends BroadcastReceiver { @Override public void onReceive(Context xiContext Intent xiIntent) { if (xiIntent.getAction().equalsIgnoreCase(Intent.ACTION_NEW_OUTGOING_CALL)) { String phoneNum = xiIntent.getStringExtra(Intent.EXTRA_PHONE_NUMBER); Intent intent = new Intent(xiContext MyActivity.class); intent.putExtra(""phoneNum"" phoneNum); intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK); xiContext.startActivity(intent); setResultData(null); } } } So far my unit test looks like this: public class OutgoingCallReceiverTest extends AndroidTestCase { private OutgoingCallReceiver mReceiver; @Override protected void setUp() throws Exception { super.setUp(); mReceiver = new OutgoingCallReceiver(); } public void testStartActivity() { Intent intent = new Intent(Intent.ACTION_NEW_OUTGOING_CALL); intent.putExtra(Intent.EXTRA_PHONE_NUMBER ""01234567890""); mReceiver.onReceive(getContext() intent); } } This runs through the code but I want my test to be able to check that the intent was sent out and to check the phone number on it. How do I do this? Can I also test that the phone call gets cancelled (because of the setResultData(null) line)? corlettk pointed me at the MockContext object in Android which does the trick. I've made a subclass of it TestContext which looks like this: public class TestContext extends MockContext { private List<Intent> mReceivedIntents = new ArrayList<Intent>(); @Override public String getPackageName() { return ""com.mypackage.test""; } @Override public void startActivity(Intent xiIntent) { mReceivedIntents.add(xiIntent); } public List<Intent> getReceivedIntents() { return mReceivedIntents; } } And my test case now looks like this: public class OutgoingCallReceiverTest extends AndroidTestCase { private OutgoingCallReceiver mReceiver; private TestContext mContext; @Override protected void setUp() throws Exception { super.setUp(); mReceiver = new OutgoingCallReceiver(); mContext = new TestContext(); } public void testStartActivity() { Intent intent = new Intent(Intent.ACTION_NEW_OUTGOING_CALL); intent.putExtra(Intent.EXTRA_PHONE_NUMBER ""01234567890""); mReceiver.onReceive(mContext intent); assertEquals(1 mContext.getReceivedIntents().size()); assertNull(mReceiver.getResultData()); Intent receivedIntent = mContext.getReceivedIntents().get(0); assertNull(receivedIntent.getAction()); assertEquals(""01234567890"" receivedIntent.getStringExtra(""phoneNum"")); assertTrue((receivedIntent.getFlags() & Intent.FLAG_ACTIVITY_NEW_TASK) != 0); } } Good one Matt... That's a nice solid sucinct test. Yep you're right. I've corrected this. Thanks! Hm. I noticed that the code above still calls onReceive(getContext()) - shouldn't 'getContext()' now be 'mContext'? typo? :)  Matt Sounds like you need to mock-up a Context ... and then swap your methods over to accepting interfaces instead of concrete classes: public void onReceive(IContext c IIntent i) just for the purposes of testing. But then the Context and Intent classes aren't yours are they... they're Android's... so you can't ""just"" make them implement your interfaces so you'd have to ""wrap"" them in order to expose a your interface which is RATHER a lot of code for not much gain. Very Yucky!!! So I started to wonder if someone's been through all this before and done the hard-yards for us... and tada: http://developer.android.com/reference/android/test/mock/package-summary.html Cheers. Keith.  Since this question was asked mocking Frameworks have evolved pretty much. With mockito you can now mock not only interfaces but as well classes. So I would suggest to solve this problem by mocking a context and using ArgumentCapture: import static org.mockito.Mockito.*; public class OutgoingCallReceiverTest extends AndroidTestCase { private OutgoingCallReceiver mReceiver; private Context mContext; @Override protected void setUp() throws Exception { super.setUp(); //To make mockito work System.setProperty(""dexmaker.dexcache"" mContext.getCacheDir().toString()); mReceiver = new OutgoingCallReceiver(); mContext = mock(Context.class); } public void testStartActivity() { Intent intent = new Intent(Intent.ACTION_NEW_OUTGOING_CALL); intent.putExtra(Intent.EXTRA_PHONE_NUMBER ""01234567890""); mReceiver.onReceive(mContext intent); assertNull(mReceiver.getResultData()); ArgumentCaptor<Intent> argument = ArgumentCaptor.forClass(Intent.class); verify(mContext times(1)).startActivity(argument.capture()); Intent receivedIntent = argument.getValue(); assertNull(receivedIntent.getAction()); assertEquals(""01234567890"" receivedIntent.getStringExtra(""phoneNum"")); assertTrue((receivedIntent.getFlags() & Intent.FLAG_ACTIVITY_NEW_TASK) != 0); } } What is 'mock' in 'mContext = mock(Context.class);' ?"
464,A,"Should the JUnit message state the condition of success or failure? I can write an assertion message one of two ways. Stating success: assertEquals( ""objects should be identical"" expected actual ); Or stating the condition of being broken: assertEquals( ""objects aren't identical"" expected actual ); Is there a standard for this in JUnit specifically? If not what are the arguments for each side? P.S. I've seen articles on the web demonstrating both of these without explanation so just saying ""search Google"" is not an answer! [UPDATE] Everyone is getting hung up on the fact that I used assertEquals and therefore the message is probably useless. But of course that's just because I wanted to illustrate the question simply. So imagine instead it's: assertTrue( ... big long multi-line expression ... ); Where a message is useful. At which point as various answers have said it doesn't matter. The *only* important point is: is the message informative? If so great - job done. By all means get in to your own conventions but in this case I don't think it matters much at all so long as it's clear. I rarely even bother with a message at least for assertEquals. Any sensible test runner will explain that you were using assertEquals and the two things which were meant to be equal. Neither of your messages give more information than that. I usually find that unit test failures are transient things - I'll rapidly find out what's wrong and fix it. The ""finding out what's wrong"" usually involves enough detail that a single message isn't going to make much difference. Consider ""time saved by having a message"" vs ""time spent thinking of messages"" :) EDIT: Okay one case where I might use a message: when there's a compact description in text which isn't obvious from the string representation of the object. For example: ""Expected date to be December 1st"" when comparing dates stored as milliseconds. I wouldn't worry about how you express it exactly though: just make sure it's obvious from the message which way you mean. Either ""should be"" or ""wasn't"" is fine - just ""December 1st"" wouldn't be obvious. I'm accepting this answer since it does summarize the consensus of the answers which is: (a) try not to use messages and (b) it doesn't matter. I tend to agree however you're not answering the question. *When* a message makes sense -- and sometimes it does -- how do you phrase it? I'm asking an API definition question so saying ""Don't use the API"" is not an answer. You're asking it in a case where the API isn't generally useful though. I'll edit anyway... Here's an example of the message adding to the assert failing - assertEquals(""Roll value not zero when there are no existing hedges."" new BigDecimal(""0.00"") hedgeValue.getRoll()) - it tells anyone seeing the assertion fail why you were expecting the 2 values to be equal. So how long does it take you to add that message to every test knowing that relatively few tests will ever fail compared with the time taken to just look at the test code (and name) when it *does* fail? My experience is that it's not a good trade-off. When doing unit testing I find that the more I make the test methods named so it's clear what the intended behavior is the less I need to specify messages for assertEquals() fail(). If I do need a message often it's a sign that the test is doing too much which could mean the code under test needs to be modified. Having good names for tests have many other advantages. @Jon Skeet - I don't think I suffer much of a time penalty for adding the message probably because I've made it a habit to write them - this removes the decision as whether or not to write a message leaving me just to think of a suitable sentence to write which as I normally can't shut-up is never much of a problem ;-) I'm testing an implementation of SHA1. There are hundreds of tests specified by NIST. Copy-pasting functions with good naming schemes is not an option. I'm parsing a file and test each entry within a loop i.e.: `while(...) assertEquals(hash SHA.digest(msg))`. In this case it makes **very** much sense to do prints like `assertEquals(""For msg <"" + msg + ""> ""hash SHA.digest(msg))`. @Zut: Yes on *occasion* it makes sense - but I'd say this is the exception rather than the rule.  From the javadocs of JUnit: Asserts that two objects are equal. If they are not an AssertionFailedError is thrown with the given message. According to the API the message can be whatever you want. I would argue that the two options you have are both the same and both superfluous. The success or failure of the assert already provides all the information you are providing in the message. It follows for me that you should have either nothing (there is an assert that doesn't take a string on purpose) OR include a message with meaning beyond what is already there. So I guess this is a reiteration of Jon's answer but too verbose to be a comment.  Vote me down too (like Jon) but the only time I've ever use a message like this (on assert equals) is when building a single test with a matrix of values and one of the test elements fails: I use the message to indicate which test case failed. Otherwise the text is totally redundant. downvote? why that? I do exactly the same (being it a matrix or a loop...). and I also think that unit tests despite being important are not the final product no need to have *nice* messages... better spend the time to write more tests!  I agree that providing a message is helpful and I always provide one. To me the useful thing to include is a clear statement of what went wrong - usually involving the words 'should' or 'should not'. E.g. ""objects are equal"" is ambiguous - does it mean the objects are equal and that's why the test failed? Or that objects should be equal but they aren't? But if you say ""Objects should be equal"" or ""Objects should not be equal"" it's obvious why the assertion failed.  I don't put a message for the case you cite unless I'm running a test where I have an array of similar test values that I'm running in a loop and I want to pinpoint exactly which one failed. Then I add a message to tell me which one.  I would like to answer the question without considering if a message in generel is useful. If a test fails something is wrong. I know this. I want to know why it is broken. That's very easy to find out because I just have to open the test case and the SUT. Like Jon said it's very easy to fix it (hopefully ;-) ). But what about the message? The message is for me an advice what could be done to turn it into a green test case. So I would appreciate if there's an advice given in the message text how to fix this problem or where to search for the problem. Another interesting aspect would be the usage of positive expressions. It's worth a consideration to use positive text messages. In your example I would use Objects should be identical. But that's a small reason.  According to the junit API the message is the ""the identifying message for the AssertionError"" so its not a message describing the condition that should be met but a message describing what's wrong if the condition isn't met. So in your example ""objects aren't identical"" seems to be more conformant.  I don't think it matters at all - You already know that a failure happened and therefore it doesn't matter if the message states what should have happened or what shouldn't happen. The goal of the message is to help you when it can not to obtain some completeness. Obviously in the case of assertEquals this is less important but the message is important in the case of general asserts. The message should help you obtain enough context to understand right away what exactly failed. However the amount of needed context (and thus the details in the message) should depend on how you get the report. For example if you get it in Eclipse you can easily go and interact and see what happened so the message is less imporrtant. However if you get your reports emailed to you (e.g. from a continuous build server) then you want the message to provide enough information so that you will have an idea of what is going on before you even go to the corresponding source code. The message will get into the test reports generated by the JUnit Ant task and I've generally found the context useful in homing in on the problem when a test is broken.  Unlike many others I feel that using a message is extremely helpful for many reasons: The person looking at the logs of a failed test may not be the person who wrote the test. It can take time to read through the code and understand what case the assertion is meant to address. A helpful message will save time. Even in the event it is the developer of the test who is looking at the logs it may have been days or months since the test was written and again a message can save time. My advice would be to write the message with a statement of the expected behavior. For example: assertEquals(""The method should be invoked 3 times"" 3 invocationCount);"
465,A,testNG tests extending BaseTest We noticed that when testNG test cases extend TestCase (JUnit) those tests start executing as Junit tests. Also I should probably mention the tests are run through Maven. Is this a bug or a feature? Is it possible to override this behavior and still run those types of tests as TestNG tests? Do you know a link where TestNG talks about this? thanks. I didn't think either TestNG or JUnit required any base classes now that both use annotations to specify test methods. Why do you think you need to extend a class? And why on earth would a TestNG class extend the JUnit base class TestCase? Is it any surprise that they run as JUnit tests? It sounds like neither bug nor feature but user error on your part. I'm not sure what you're getting at here. Why would you do this? UPDATE: Your question is confusing me. Did you have JUnit tests running successfully that you're not trying to convert to TestNG or visa versa? I'm having a very hard time understanding what you're trying to achieve here. Leave Maven out of it. It's immaterial whether they're run by you Ant or Maven. I am not saying one should extend Junit's BaseCase. If writing a TestNG test from scratch then yes the proper way to do it is to annotate your test classes with the @Test annotation and that's it. However you don't have that ability when you are trying to migrate from Junit to TestNG. You cannot just blindly remove the extends clause in this case and throw @Test annotation everywhere Uh yes you can if you're using JUnit 4. And you should be. ok one last try. in my project i have junit tests that i am running with Tesng. Those are legacy tests and since they are junit tests they extend a class. So now when you annotate that test with the @Test annotation and run it with TestNG it gets run as a Junit test but i want it to run with the characteristics of TestNG like setUp/teardown to be called once per test class test methods don't need to start with the word test etc. But it looks like it's a default behavior of testng of the test extends BaseCase it's a junit test and that cannot be overridden  Looking at the maven surefire plugin info I can't see any way to select a test for TestNG processing only if it also extends a jUnit 3 class. IFAIK your best bet is to just work on each class seperately removing the jUnit references and then retesting. That way you never have the mixture in one class and you should avoid problems. To make the work manageable I would be inclined to do this only when I was changing a test case for some other reason. thanks Michael. I totally agree
466,A,Running JUnit integration tests from a JSP? we need to do some integration testing of some Java classes which we want to run inside a Weblogic server. We have a JUnit test suite that runs a set of Junit test classes and I would like to know if is a good idea to write a JSP that runs the test suite and prints out the results in a graphical way in case we won't be able to access via SSH to the server and run the tests via console. Step 1: We upload the jar file with the tests and the JSP Step 2: We call the JSP via a browser Step 3: Profit! Our tests are running and we have visual feedback. Am I right or wrong? Thanks for your advices! You should be able to use Cactus to do this See the following article for reference: http://www.oracle.com/technetwork/articles/server-side-unit-tests-096611.html That should save you having to write extra code not sure about visual feedback but the Cactus page seems to suggest that when running in a servlet container like Tomcat you should get the results visually: http://jakarta.apache.org/cactus/integration/howto_tomcat.html It looks very interesting thanks a lot! -1 because Cactus is retired project.
467,A,What can cause intermittent ORA-12519 (TNS: no appropriate handler found) errors We are running our Junit 4 test suite against Weblogic 9 in front of an Oracle 10 database (using Hudson as a continuous integration server) and occasionally we will get an ORA-12519 crash during script teardown. However the error is very intermittent: It usually happens for the same Test class It doesn't always happen for the same test cases (sometimes they pass) It doesn't happen for the same number of test cases (anywhere from 3-9) Sometimes it doesn't happen at all everything passes While I can't guarantee this doesn't happen locally (when running against the same database of course) I have run the same suite of class multiple times with no issues. Any ideas? Another solution I have found to a similar error but the same error message is to increase the number of service handlers found. (My instance of this error was caused by too many connections in the Weblogic Portal Connection pools.) Run SQL*Plus and login as SYSTEM. You should know what password you’ve used during the installation of Oracle DB XE. Run the command alter system set processes=150 scope=spfile; in SQL*Plus VERY IMPORTANT: Restart the database. From here: http://www.atpeaz.com/index.php/2010/fixing-the-ora-12519-tnsno-appropriate-service-handler-found-error/ The article mentions this being a specific problem on Oracle Database XE (Express Edition) the same setting appears in Oracle (full product) as well except there it's at 150 by default. I'm seeing this issue running with jmeter running 40 threads. My processes is set to 300 though and has max utilization of 128 so far. `select * from v$resource_limit where resource_name = 'processes';` = current=88 max=128 limit=300  Don't know if this will be everybody's answer but after some digging here's what we came up with. The error is obviously caused by the fact that the listener was not accepting connections but why would we get that error when other tests could connect fine (we could also connect no problem through sqlplus)? The key to the issue wasn't that we couldn't connect but that it was intermittent After some investigation we found that there was some static data created during the class setup that would keep open connections for the life of the test class creating new ones as it went. Now even though all of the resources were properly released when this class went out of scope (via a finally{} block of course) there were some cases during the run when this class would swallow up all available connections (okay bad practice alert - this was unit test code that connected directly rather than using a pool so the same problem could not happen in production). The fix was to not make that class static and run in the class setup but instead use it in the per method setUp and tearDown methods. So if you get this error in your own apps slap a profiler on that bad boy and see if you might have a connection leak. Hope that helps. My situation was very different in the details but it came down to a connection leak too so thanks for pointing me in the right direction. Same here. I had to manually add a call to `close()` on the connection object.
468,A,JUnit won't stop at breakpoints in Eclipse (using JDK 1.6.0.20) my breakpoints in Eclipse won't stop the execution of a JUnit test. It doesn't matter where I set the breakpoint in the JUnit method it simply won't stop the code from flowing. Placing it in a class called in the JUnit test won't work either. I am using the JDK in the version of 1.6.0.20 so I guess I'm not affected by the bug in version 1.6.0.14. Do you know any hints concerning this matter? If your break points appear with a back slash across you have to go to the debug perspective (Window → Open perspective) and in the ‘Breakpoints’ window unselect the ‘Skip all break points’ button (circle with backslash across)  You should start your unit-tests with Debug as > JUnit test  Plese use -XX:+UseParallelGC in debug configuration
469,A,"Testing for multiple exceptions with JUnit 4 annotations Is it possible to test for multiple exceptions in a single JUnit unit test? I know for a single exception one can use for example  @Test(expected=IllegalStateException.class) Now if I want to test for another exception (say NullPointerException) can this be done in the same annotation a different annotation or do I need to write another unit test completely? If more than one exception indicates that the tested code works as expected then the test isn't well defined. You should expect exactly one thing from a test. How would you expect to ""expected""s to work? A method can only throw one exception. You would have to write a different unit test for each way the method can fail. So if the method legitimately throw two exceptions then you need two tests set up to force the method of throwing each exception. A method can only throw one exception but can be declared as throwing many different exceptions. Say that a method can throw 4 exception types throwing either A or B passes the test throwing C or D fails the test. This would be easy is 'expected' tool an array of classes instead of a single class But under one set of data is can only fail in one way.  This is not possible with the annotation. With JUnit 4.7 you can use the new ExpectedException rule public static class HasExpectedException { @Interceptor public ExpectedException thrown= new ExpectedException(); @Test public void throwsNothing() { } @Test public void throwsNullPointerException() { thrown.expect(NullPointerException.class); throw new NullPointerException(); } @Test public void throwsNullPointerExceptionWithMessage() { thrown.expect(NullPointerException.class); thrown.expectMessage(""happened?""); throw new NullPointerException(""What happened?""); } } More see JUnit 4.7: Interceptors: expected exceptions Rules in JUnit 4.7 If updating to JUnit 4.7 is not possible for you you have to write a bare unit test of the form public test() { try { methodCall(); // should throw Exception fail(); } catch (Exception ex) { assert((ex instanceof A) || (ex instanceof B) || ...etc...); ... } }  Use catch-exception: // test public void testDo() { // obj.do(1) must throw either A or B catchException(obj).do(1); assert caughtException() instanceof A || caughtException() instanceof B; // obj.do(2) must throw A but not SubclassOfA catchException(obj).do(2); assert caughtException() instanceof A && !(caughtException() instanceof SubclassOfA); }  You really want the test to do one thing and to test for that. If you're not sure as to which exception is going to be thrown that doesn't sound like a good test to me. e.g. (in pseudo-code) try { badOperation(); /// looks like we succeeded. Not good! Fail the test } catch (ExpectedException e) { // that's fine } catch (UnexpectedException e) { // that's NOT fine. Fail the test } so if you want to test that your method throws 2 different exceptions (for 2 sets of inputs) then you'll need 2 tests. The ""catch (UnexpectedOperation e)"" is unnecessary; if a test method throws an exception that's considered a failure. As Yuval mentioned you need to do a fail() immediately after badOperation() @NamshubWriter - you're missing the point. The above is pseudo-code illustrating when you need to take action. I usually call fail() within the try block immediately after badOperation(). At any given point I am sure which statement I expect to throw an error. In one test I wish to test that all expected exceptions gets thrown. I'll be honest I'm not thrilled about the annotation way of testing for exceptions: a bad test may cause an expected exception at an unexpected point in the test. In that sense I favour the try-catch-fail of testing for an exception but just looks ugly. I suppose you are right in a sense though: my tests should be more atomic (rather than testing everything for one method in a single unit test as I am trying to do). @Yuval - I was trying to be test-framework agnostic in the above. However I've amended to be clearer. @Phantom - atomicity for tests can be a pain but I believe that's the way forward and reduces ambiguity when your tests fail. I have written tests that check for multiple exceptions though by not using annotations and nesting tests within the catch blocks of preceeding tests. After a while it gets a bug ugly...  keep the tests as simple and short as possible. the intention of a JUnit-Test is to test only one simple functionality or one single way of failure. indeed to be safe you should create at least one test for every possible execution way. normally this is not always possible because if you have a method that analyzes a string there are so many possible string combinations that you cannot cover everything. keep it short and simple. you can have 30-40 testing methods for one single method easily... does it really matter? regards"
470,A,"What's the difference between failure and error in JUnit? I'm running JUnit tests on a large code base and I've been realizing that sometimes I get ""Errors"" while other times I get ""Failures"". What's the difference? If your test throws an exception which does not get bubbled up through the Assertion framework in Junit it gets reported as an error. For example a NullPointer or a ClassNotFound exception will report an error: String s = null; s.trim(); or try { // your code } catch(Exception e) { // log the exception throw new MyException(e); } Having said that the following will report a failure: Assert.fail(""Failure here""); or Assert.assertEquals(1 2); or even: throw new AssertionException(e); It depends on the Junit version you are using. Junit 4- will make the distinction between a failure and an error but Junit 4 simplifies it as failures only. Following link provides more interesting inputs: http://www.devx.com/Java/Article/31983/1763/page/2  Ok I've just noticed a pattern and think I've figured it out (correct me if I'm wrong). It seems to me that failures are when your test cases fail - i.e. your assertions are incorrect. Errors are unexpected errors that occur while trying to actually run the test - exceptions etc. You are correct sir. Though if anything extending `java.lang.AssertionError` is thrown it will be shown as a test failure instead of a test error. You should consider accepting your own answer because it is correct. Yes that's exactly the difference. And from a pragmatic perspective there is ""no difference"" -- in that if you get an error or a failure then you need to fix it. So it was probably a mistake to count ""failures"" and ""errors"" separately in JUnit. JUnit 4 combines the two (as explained in an answer below)."
471,A,"Weird JUnit logging behavior I'm noticing some weird behavior using standard logging during JUnit tests. Does JUnit redirect standard output to a different stream? How can I get access to that? Here's a simple JUnit test that demonstrates the behavior I'm describing. @Test public void logMessage() { // set up new logger with output directed to standard out Logger logger = Logger.getLogger(""my.test.logger""); logger.addHandler(new StreamHandler(System.out new SimpleFormatter())); // log a warning message logger.warning(""logger message""); // message 1 // turn off parent handlers logger.setUseParentHandlers(false); // log a second warning message logger.warning(""second logger message""); // message 2 // print somehting to standard output System.out.println(""standard output message""); //message 3 } Notice that I've created a new logger that simply sends its log messages to standard output (System.out). Here's the Junit output Testsuite: com.my.FormatterTest Feb 19 2009 12:02:33 PM com.my.FormatterTest logMessage WARNING: logger message standard output message Tests run: 1 Failures: 0 Errors: 0 Time elapsed: 0.079 sec ------------- Standard Output --------------- standard output message ------------- ---------------- --------------- ------------- Standard Error ----------------- Feb 19 2009 12:02:33 PM com.my.FormatterTest logMessage WARNING: logger message ------------- ---------------- --------------- Feb 19 2009 12:02:33 PM com.my.FormatterTest logMessage WARNING: logger message Feb 19 2009 12:02:33 PM com.my.FormatterTest logMessage WARNING: second logger message test: BUILD SUCCESSFUL (total time: 2 seconds) Why don't message 1 or message 2 show up in the Standard Output portion of the JUnit output? Thanks! As you indirectly suggest the junit testrunners redirect stdout and stderr to a separate stream while the test is running. Your logger is normally initialized before this redirection occurs which means it uses the regular System.out/System.err for logging to console. This can be fairly easily seen by looking at the source for the JunitTestRunner class. Edit: I have looked at some of the source code because your question made me curious. I do not know which TestRunner you are using and the answer may lay there; the stream redirections etc are not a part of the junit framework but are implemented by ant/eclipse/maven/idea. It looks like you addHandler has no effect whatsovever  so I suspect something is intercepting it (your output would be logical if all the logging was done by a parent logger). In my example I'm adding a new handler to the logger pointed at System.out *inside the test itself*. Why doesn't this also use the redirected stdout set by TestRunner? Good point on the JUnit Test runner. I am using the test runner in NetBeans. I still don't understand how 2 references to System.out can write stings to two different places in the same method. That's a head-scratcher for me and is frustrating.  krosenvold's comments led me to the right answer (thanks!). It appears as if stdout is in fact redirected to another (JUnit) stream. The real problem however was that the the StreamHandler's buffer was not flushed until after stdout was reset to its original output stream (resulting in the log messages not appearing in JUnit's output for stdout). Changing the code to the following gives the right behavior. @Test public void logMessage() { // set up new logger with output directed to standard out Logger logger = Logger.getLogger(""my.test.logger""); StreamHandler sh = new StreamHandler(System.out new SimpleFormatter()); logger.addHandler(sh); // log a warning message logger.warning(""logger message""); // message 1 // turn off parent handlers logger.setUseParentHandlers(false); // log a second warning message logger.warning(""second logger message""); // message 2 // print somehting to standard output System.out.println(""standard output message""); //message 3 // FLUSH THE STREAM HANDLER !!! sh.flush(); } yeilds Testsuite: com.my.FormatterTest Feb 20 2009 12:58:17 PM com.my.FormatterTest logMessage WARNING: logger message standard output message Feb 20 2009 12:58:17 PM com.my.FormatterTest logMessage WARNING: logger message Feb 20 2009 12:58:17 PM com.my.FormatterTest logMessage WARNING: second logger message Tests run: 1 Failures: 0 Errors: 0 Time elapsed: 0.078 sec ------------- Standard Output --------------- standard output message Feb 20 2009 12:58:17 PM com.my.FormatterTest logMessage WARNING: logger message Feb 20 2009 12:58:17 PM com.my.FormatterTest logMessage WARNING: second logger message ------------- ---------------- --------------- ------------- Standard Error ----------------- Feb 20 2009 12:58:17 PM com.my.FormatterTest logMessage WARNING: logger message ------------- ---------------- --------------- test: BUILD SUCCESSFUL (total time: 2 seconds) time: 2 seconds) +1 Thank you! I setup a test that outputs to both a log file and the console with a custom. The log file was good but the console kept printing the results out of order. Changing it to a StreamHandler fixed the issue."
472,A,"Run all unit tests with Ant builder I have a directory with a bunch of JUnit tests in my project. So far I have used separate target for each unit test. For example:  <target name=""MyTest""> <mkdir dir=""${junit.output.dir}""/> <junit fork=""yes"" printsummary=""withOutAndErr""> <formatter type=""xml""/> <test name=""tests.MyTest"" todir=""${junit.output.dir}""/> <classpath refid=""MyProject.classpath""/> </junit> </target> This method requires me to change build file every time I add a Unit test. I want to able able to to run all unit tests in the project with a single Ant builder target. Is it possible to do? Yep it is you need to look at the fileset tag e.g: <junit printsummary=""yes"" haltonfailure=""yes""> <classpath> <pathelement location=""${build.tests}""/> <pathelement path=""${MyProject.classpath}""/> </classpath> <formatter type=""xml""/> <batchtest fork=""yes"" todir=""${reports.tests}""> <fileset dir=""${src.tests}""> <include name=""**/*Test*.java""/> <exclude name=""**/AllTests.java""/> </fileset> </batchtest> </junit> The important part is the use of fileset and a glob/wildcard pattern to match the names of the tests. Full docs on the junit task with examples here: http://ant.apache.org/manual/Tasks/junit.html  Yep! We do it using an ant command batchtest. Looks like this:  <batchtest todir=""${junit.report.dir}""> <fileset dir=""${basedir}\test\unit""> <include name=""**/*Test.java"" /> </fileset> </batchtest> Google it it should sort you out"
473,A,"Adding Custom Messages to JUnit4 Style Exception Tests I have the following test: @Test(expected=ArithmeticException.class) public void divideByZero() { int n = 2 / 1; } as seen here. I would like to add a message that will print if this test fails. For instance if I was doing an Assertion test I would do the following to add a message: @Test public void assertFail(){ Assert.fail(""This is the error message I want printed.""); Assert.assertEquals(true false); } The second example should print out ""This is the error message I want printed."". How do I set the first example message text? I don't think you can easily but this guy seems to have partially worked his way around it.  Maybe @Rule annotation should help. Into your unit test class add sth like this: import org.junit.Rule; import org.junit.rules.MethodRule; import org.junit.runners.model.Statement; import org.junit.runners.model.FrameworkMethod; import org.junit.internal.runners.model.MultipleFailureException; ... @Rule public MethodRule failureHandler = new MethodRule() { @Override public Statement apply(final Statement base FrameworkMethod method Object target) { return new Statement() { @Override public void evaluate() throws Throwable { List<Throwable> listErrors = new ArrayList<Throwable>(); try { // Let's execute whatever test runner likes to do base.evaluate(); } catch (Throwable testException) { // Your test has failed. Store the test case exception listErrors.add(testException); // Now do whatever you need like adding your message // capture a screenshot etc. // but make sure no exception gets out of there - // catch it and add to listErrors } if (listErrors.isEmpty()) { return; } if (listErrors.size() == 1) { throw listErrors.get(0); } throw new MultipleFailureException(listErrors); } }; } }; Instead of collecting all the exceptions in listErrors you may consider wrapping testException with your exception with additional message and just throwing it. Working on proving that this is a solution. Thanks. @KevinO I did something similar with rules once (a screen shot after failed test). If you're interested I can provide my solution. However I'm not sure how to handle exceptions. Hence I'd be pleased if you share your solution. I was unsuccessful at adding a junit failure message using ExpectedException.  I recommend instead naming the test to make it obvious what the test is testing so when some of your tests fail they tell you what the problem is. Here's an example using the ExpectedException rule: @RunWith(JUnit4.class) public class CalculatorTest { @Rule public ExpectedException exception = ExpectedException.none(); @Test public void divisionByZeroShouldThrowArithmeticException() { Calculator calculator = new Calculator(); exception.expect(ArithmeticException.class); calculator.divide(10 0); } } For details on ExpectedException see this article and the ExpectedException JavaDoc  If you are willing to use catch-exception instead of JUnit's built-in exception handling mechanisms then your problem can be easily solved: catchException(myObj).doSomethingExceptional(); assertTrue(""This is the error message I want printed."" caughtException() instanceof ArithmeticException);"
474,A,"How to mock getApplicationContext I have an application that stores app context information. The app context information is shared between activities in MyApp class which extends Application class. I am writing a unit test for my activity and I want to check that when user clicks a button in the activity an application state will change. Something like this: @Override public void onClick(View pView) { ((MyApp)getApplicationContext()).setNewState(); } The problem is that I don't know how to mock that application context. I am using ActivityUnitTestCase as a test case base. When I call setApplication it changes the value of mApplication member of Activity class but not application context. I've tried setActivityContext also but it seems wrong (it is not app context but activity context) and it fires assert inside startActivity). So the question is - how to mock getApplicationContext()? I've came up with idea of replacing _getApplicationContext()_ with _getApplication()_. Now I can mock _Application_ object and use _setApplication()_. It is kinda workaround. However I don't get the difference between those methods. And the [question](http://stackoverflow.com/questions/5018545) related to that is not answered. Since the method getApplicationContext is inside the class that you're extending it becomes somewhat problematic. There are a couple of problems to consider: You really can't mock a class that is under test which is one of the many drawbacks with object inheritance (i.e. subclassing). The other problem is that ApplicationContext is a singleton which makes it all more evil to test since you can't easily mock out a global state that is programmed to be irreplaceable. What you can do in this situation is to prefer object composition over inheritance. So in order to make your Activity testable you need to split up the logic a little. Lets say that your Activity is called MyActivity. It needs to be composed of a logic component (or class) lets name it MyActivityLogic. Here is a simple class-diagram figure: To solve the singleton problem we let the logic be ""injected"" with an application context so it can be tested with a mock. We then only need to test that the MyActivity object has put the correct application context into MyActivityLogic. How we basically solve both problems is through another layer of abstraction (paraphrased from Butler Lampson). The new layer we add in this case is the activity logic moved outside of the activity object. For the sake of your example the classes need to look sort-of like this: public final class MyActivityLogic { private MyApp mMyApp; public MyActivityLogic(MyApp pMyApp) { mMyApp = pMyApp; } public MyApp getMyApp() { return mMyApp; } public void onClick(View pView) { getMyApp().setNewState(); } } public final class MyActivity extends Activity { // The activity logic is in mLogic private final MyActivityLogic mLogic; // Logic is created in constructor public MyActivity() { super(); mLogic = new MyActivityLogic( (MyApp) getApplicationContext()); } // Getter you could make a setter as well but I leave // that as an exercise for you public MyActivityLogic getMyActivityLogic() { return mLogic; } // The method to be tested public void onClick(View pView) { mLogic.onClick(pView); } // Surely you have other code here... } It should all look something like this: To test MyActivityLogic you will only need a simple jUnit TestCase instead of the ActivityUnitTestCase (since it isn't an Activity) and you can mock your application context using your mocking framework of choice (since handrolling your own mocks is a bit of a drag). Example uses Mockito: MyActivityLogic mLogic; // The CUT Component Under Test MyApplication mMyApplication; // Will be mocked protected void setUp() { // Create the mock using mockito. mMyApplication = mock(MyApplication.class); // ""Inject"" the mock into the CUT mLogic = new MyActivityLogic(mMyApplication); } public void testOnClickShouldSetNewStateOnAppContext() { // Test composed of the three A's // ARRANGE: Most stuff is already done in setUp // ACT: Do the test by calling the logic mLogic.onClick(null); // ASSERT: Make sure the application.setNewState is called verify(mMyApplication).setNewState(); } To test the MyActivity you use ActivityUnitTestCase as usual we only need to make sure that it creates a MyActivityLogic with the correct ApplicationContext. Sketchy test code example that does all this: // ARRANGE: MyActivity vMyActivity = getActivity(); MyApp expectedAppContext = vMyActivity.getApplicationContext(); // ACT: // No need to ""act"" much since MyActivityLogic object is created in the // constructor of the activity MyActivityLogic vLogic = vMyActivity.getMyActivityLogic(); // ASSERT: Make sure the same ApplicationContext singleton is inside // the MyActivityLogic object MyApp actualAppContext = vLogic.getMyApp(); assertSame(expectedAppContext actualAppContext); Hope it all makes sense to you and helps you out. Thanks for so detailed answer! Great answer. Indeed your introducing MVP pattern for android :) This is an excellent answer. Your keeping of OO theory is impressive."
475,A,"Asserting in the example below testLogicalDoc = new LogicalDocumentImpl(-4); assertTrue(testLogicalDoc==null); In my code above I have an assert condition with which I want to make sure I don't create my object with negative size. It is a stringBuilder beneath the covers which throws NegativeArrayBoundsException for a size less than zero. But my junit test fails here. I don't know any other way of making sure an object is not created with a negative size. Any thoughts on how this could be tested ? or should it be a Junit test at all ?? Many thanks -Pan EDIT:  @Test(expected=NegativeArraySizeException.class) public void testCreate4b() { LogicalDocumentImpl testLogicalDoc = new LogicalDocumentImpl(-4); } I'm catching the exception in the LogicalDocumentImpl class but still this test fails with an assertion error but only succeeds when I do a try catch on assertion error ..why is that so ?? Catch AssertionError and fail otherwise: try { LogicalDocumentImpl testLogicalDoc = new LogicalDocumentImpl(-4); fail(""should throw""); } catch (AssertionError e) { } the `fail` method throws `AssertionError` (or a subclass named `AssertionFailedError` in JUnit 3) so this actually doesn't do what it appears to  Usually Junit test cases are meant to test that the behavior of your code in certain cases is what you expect. Therefore for this case you expect that an exception be thrown. Looking at the JUnit faq (http://junit.sourceforge.net/doc/faq/faq.htm#tests_7) you want to use something like the following: @Test(expected=NegativeArrayBoundsException.class) Will this work if I do a try/catch clause in my method where I create the object ?? or do I need to do a throws only ??  if you are throwing NegativeArrayBoundsException your test case could check like this @Test(expected= NegativeArrayBoundsException.class) That means your test should throw the exception NegativeArrayBoundsException. Alternatively you can use fail('should never come here for negative values..') testLogicalDoc = new LogicalDocumentImpl(-4); fail('should never come here for negative values..');"
476,A,"Is running tests with JUnit 3.x vs JUnit 4.x still a best practice? I haven't looked at this in a while but if I recall correctly both ant and maven still rely on JUnit 3 for unit tests (as of maven 2.09 the default POM file still has JUnit 3.81). Does it still make sense to stick to JUnit 3 instead of using the latest and greatest? Any good reason I might be missing? Both JUnit and Maven can run JUnit 4 tests just fine. The only reasons to use JUnit 3 are legacy systems: Either you're force to use a Pre-Java 5 JDK or you have extensive tooling that depends on JUnit 3 and doesn't support JUnit 4.  There are some reasons for JUnit 4.x and some against it. Pros: The annotations are easy to understand and increase readability. Ignoring tests are easier. You can name methods as you like. Cons: You need Java 5 (annotations). Migration probably needed(?). I use JUnit 4 because it's nicer but that's my personal opinion.  Ant doesn't reply on JUnit 3. I'd recommend moving to the new JUnit for the simple reason that you're going to see more and more tools and examples that are JUnit4 only. Sticking with JUnit3 means living in a dwindling ecosystem.  I don't see a reason to stick to the 3.x versions. Most tools have been compatible with 4.x for a while now. The only reason I would stick to 3.x is in a java 1.4 environment (because there is no other way). By the way maven is switching to Java 5 in 2.1 so there is a chance they will propose junit 4.x  JUnit 4 has lots of advantages over 3.x. The most important is that you no longer have to extend TestCase nor do your test methods have to begin with ""test."" It's all annotation-based now. You can also add the Hamcrest matchers which gives you a really nice and expressive way of writing test assertions. If you're stuck on a pre-Java-1.5 project you may have to stick with JUnit 3.x though. But isn't it easy to miss adding @Test?  Maven has no explicit binding to Junit 3.81. You can switch to any version of junit you want. If you're running java 5 I think you should switch to at least 4.4 Spring 2.5 is not compatible with 4.5. Also if you're using maven you should probably explicitly state what source/target settings you want by explicitly configuring the maven-compiler-plugin in your POM I know I can setup maven to use whatever version of JUnit I prefer but the default for archetype:create is still 3.81. Just wondering if there's any good reason. Java 4 compatibility"
477,A,"controlling which JUnit tests are run by Ant I have an ant task set up like so: <target name=""unit-test"" description=""unit tests"" depends=""compile-tests""> <mkdir dir=""${build}/test""/> <mkdir dir=""${build}/test/raw""/> <mkdir dir=""${build}/test/reports""/> <!-- set up scratch database for tests --> <mkdir dir=""${build.dbTest}"" /> <junit printsummary=""yes"" haltonfailure=""no"" maxmemory=""512m"" > <classpath> <pathelement path=""${java.class.path}""/> <pathelement path=""${build.classes}""/> <pathelement path=""${build.test-classes}""/> <fileset dir=""lib"" includes=""*.jar""/> <fileset dir=""lib-test"" includes=""*.jar""/> </classpath> <formatter type=""xml""/> <sysproperty key=""derby.system.home"" value=""${build.dbTest}"" /> <batchtest fork=""yes"" todir=""${build}/test/raw""> <fileset dir=""${src.test}""> <include name=""**/*Test.java""/> </fileset> </batchtest> </junit> <junitreport todir=""${build}/test""> <fileset dir=""${build}/test/raw""/> <report todir=""${build}/test/reports""/> </junitreport> </target> Which works pretty well for running all my tests but running all my tests really slows down my TDD Fail-Pass-Refactor groove. My full test suite takes about six minutes to run which is way too long for quick response changes during TDD especially since most of the time I only care about results from one test. The work flow I'd like to have would be create test for new feature/bug run only the new test (or at most only the test class I just modified) write some code iterate 2-3 until the new tests are passing run full set of tests to make sure nothing else broke plug any broken test into the 2-3 cycle above and repeat full cycle when all tests pass declare victory. TestNG seems to have capability for grouping tests which seems ideal (I could have a ""TDD"" group for the tests I'm currently working with. Changing that when I start working on something is an acceptable level of manual configuration here) but I don't want to switch test frameworks unless I absolutely have to. Is there any way to do something similar or another way to achieve my desired work flow using JUnit? http://stackoverflow.com/questions/817135/grouping-junit-tests could be related I use in my scripts instead of <include name=""**/*Test.java""/> the snippet <include name=""${test}""/> and set the property test to **/*Test.java earlier in the script. Now I can start ant setting the property to a different value: ant test -Dtest=**/*AcceptanceTests.java That works like a charm thanks."
478,A,"How to do a junit assert on a message in a logger I have some code-under-test that calls on a java logger to report its status. In the junit test code I would like to verify that the correct log entry was made in this logger. Something along the following lines: methodUnderTest(bool x){ if(x) logger.info(""x happened"") } @Test tester(){ // perhaps setup a logger first. methodUnderTest(true); assertXXXXXX(loggedLevel()Level.INFO); I suppose that this could be done with a specially adapted logger (or handler or formatter) but I would prefer to re-use a solution that already exists. (And to be honest it is not clear to me how to get at the logRecord from a logger but suppose that thatś possible.) Thanks a lot for these (surprisingly) quick and helpful answers; they put me on the right way for my solution. The codebase were I want to use this uses java.util.logging as its logger mechanism and I don't feel at home enough in those codes to completely change that to log4j or to logger interfaces/facades. But based on these suggestions I 'hacked-up' a j.u.l.handler extension and that works as a treat. A short summary follows. Extend java.util.logging.Handler: class LogHandler extends Handler { Level lastLevel = Level.FINEST; public Level checkLevel() { return lastLevel; } public void publish(LogRecord record) { lastLevel = record.getLevel(); } public void close(){} public void flush(){} } Obviously you can store as much as you like/want/need from the LogRecord or push them all into a stack until you get an overflow. In the preparation for the junit-test you create a java.util.logging.Logger and add such a new LogHandler to it: @Test tester() { Logger logger = Logger.getLogger(""my junit-test logger""); LogHandler handler = new LogHandler(); handler.setLevel(Level.ALL); logger.setUseParentHandlers(false); logger.addHandler(handler); logger.setLevel(Level.ALL); The call to setUseParentHandlers() is to silence the normal handlers so that (for this junit-test run) no unnecessary logging happens. Do whatever your code-under-test needs to use this logger run the test and assertEquality:  libraryUnderTest.setLogger(logger); methodUnderTest(true); // see original question. assertEquals(""Log level as expected?"" Level.INFO handler.checkLevel() ); } (Of course you would move large part of this work into a @Before method and make assorted other improvements but that would clutter this presentation.) Thanks this is exactly what I was looking for!  As mentioned from the others you could use a mocking framework. For this to make work you have to expose the logger in your class (although I would propably prefere to make it package private instead of creating a public setter). The other solution is to create a fake logger by hand. You have to write the fake logger (more fixture code) but in this case I would prefer the enhanced readability of the tests against the saved code from the mocking framework. I would do something like this: class FakeLogger implements ILogger { public List<String> infos = new ArrayList<String>(); public List<String> errors = new ArrayList<String>(); public void info(String message) { infos.add(message); } public void error(String message) { errors.add(message); } } class TestMyClass { private MyClass myClass; private FakeLogger logger; @Before public void setUp() throws Exception { myClass = new MyClass(); logger = new FakeLogger(); myClass.logger = logger; } @Test public void testMyMethod() { myClass.myMethod(true); assertEquals(1 logger.infos.size()); } }  Effectively you are testing a side-effect of a dependent class. For unit testing you need only to verify that logger.info() was called with the correct parameter. Hence us a mocking framework to emulate logger and that will allow you to test your own class's behaviour.  Mocking is an option here although it would be hard because loggers are generally private static final - so setting a mock logger wouldn't be a piece of cake or would require modification of the class under test. You can create a custom Appender (or whatever it's called) and register it - either via a test-only configuration file or runtime (in a way dependent on the logging framework). And then you can get that appender (either statically if declared in configuration file or by its current reference if you are plugging it runtime) and verify its contents.  Here is what i did for logback. I created a TestAppender class: public class TestAppender extends AppenderBase<ILoggingEvent> { private Stack<ILoggingEvent> events = new Stack<ILoggingEvent>(); @Override protected void append(ILoggingEvent event) { events.add(event); } public void clear() { events.clear(); } public ILoggingEvent getLastEvent() { return events.pop(); } } Then in the parent of my testng unit test class I created a method: protected TestAppender testAppender; @BeforeClass public void setupLogsForTesting() { Logger root = (Logger)LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME); testAppender = (TestAppender)root.getAppender(""TEST""); if (testAppender != null) { testAppender.clear(); } } I have a logback-test.xml file defined in src/test/resources and I added a test appender: <appender name=""TEST"" class=""com.intuit.icn.TestAppender""> <encoder> <pattern>%m%n</pattern> </encoder> </appender> and added this appender to the root appender: <root> <level value=""error"" /> <appender-ref ref=""STDOUT"" /> <appender-ref ref=""TEST"" /> </root> Now in my test classes that extend from my parent test class I can get the appender and get the last message logged and verify the message the level the throwable. ILoggingEvent lastEvent = testAppender.getLastEvent(); assertEquals(lastEvent.getMessage() ""...""); assertEquals(lastEvent.getLevel() Level.WARN); assertEquals(lastEvent.getThrowableProxy().getMessage() ""..."");  I've needed this several times as well. I've put together a small sample below which you'd want to adjust to your needs. Basically you create your own Appender and add it to the logger you want. If you'd want to collect everything the root logger is a good place to start but you can use a more specific if you'd like. Don't forget to remove the Appender when you're done otherwise you might create a memory leak. Below I've done it within the test but setUp or @Before and tearDown or @After might be better places depending on your needs. Also the implementation below collects everything in a List in memory. If you're logging a lot you might consider adding a filter to drop boring entries or to write the log to a temporary file on disk (Hint: LoggingEvent is Serializable so you should be able to just serialize the event objects if your log message is.) import org.apache.log4j.AppenderSkeleton; import org.apache.log4j.Level; import org.apache.log4j.Logger; import org.apache.log4j.spi.LoggingEvent; import org.junit.Test; import java.util.ArrayList; import java.util.List; import static org.hamcrest.CoreMatchers.is; import static org.junit.Assert.assertThat; public class MyTest { @Test public void test() { final TestAppender appender = new TestAppender(); final Logger logger = Logger.getRootLogger(); logger.addAppender(appender); try { Logger.getLogger(MyTest.class).info(""Test""); } finally { logger.removeAppender(appender); } final List<LoggingEvent> log = appender.getLog(); final LoggingEvent firstLogEntry = log.get(0); assertThat(firstLogEntry.getLevel() is(Level.INFO)); assertThat((String) firstLogEntry.getMessage() is(""Test"")); assertThat(firstLogEntry.getLoggerName() is(""MyTest"")); } } class TestAppender extends AppenderSkeleton { private final List<LoggingEvent> log = new ArrayList<LoggingEvent>(); @Override public boolean requiresLayout() { return false; } @Override protected void append(final LoggingEvent loggingEvent) { log.add(loggingEvent); } @Override public void close() { } public List<LoggingEvent> getLog() { return new ArrayList<LoggingEvent>(log); } } YES! This feels way superior to traditional mocking in this scenario. I love this solution. Brilliant. Thanks. This works great. The only improvement I would make is to call `logger.getAllAppenders()` then step through and call `appender.setThreshold(Level.OFF)` on each (and reset them when you're done!). This makes sure that the ""bad"" messages you're trying to generate don't show up in the test logs and freak out the next developer. Thanks so much!! This really helped!! Absolutely awesome solution thank you!"
479,A,"JUnit: how to avoid ""no runnable methods"" in test utils classes I have switched to JUnit4.4 from JUnit3.8. I run my tests using ant all my tests run successfully but test utility classes fail with ""No runnable methods"" error. The pattern I am using is to include all classes with name *Test* under test folder. I understand that the runner can't find any method annotated with @Test attribute. But they don't contain such annotation because these classes are not tests. Surprisingly when running these tests in eclipse it doesn't complain about these classes. In JUnit3.8 it wasn't a problem at all since these utility classes didn't extend TestCase so the runner didn't try to execute them. I know I can exclude these specific classes in the junit target in ant script. But I don't want to change the build file upon every new utility class I add. I can also rename the classes (but giving good names to classes was always my weakest talent :-) ) Is there any elegant solution for this problem? Does your tests work in Eclipse/NetBeans/your favourite IDE? I use eclipse. Actually there is no problem there somehow eclipse doesn't try to run these classes. I wonder how? I don't know if we understood your question. Please re-read your question and probably add some more information. @guerda: The question seems pretty clear to me. His Ant task is finding classes which don't contain tests because the filter is picking up the utility class. Hence my answer which I still believe is entirely relevant. LiorH: Thanks for clarification so my answer is waste :) To prevent JUnit from instantiating your test base class just make it public abstract class MyTestBaseClass { ... whatever... } (@Ignore reports it as ignored which I reserve for temporarily ignored tests.) JUnit runners often try to instantiate abstract classes as well and then fail with an instantiation error.  Ant now comes with the skipNonTests attribute which was designed to do exactly what you seem to be looking for. No need to change your base classes to abstract or add annotations to them. It looks like the `skipNonTests` attribute is only available in ant 1.9+ which is a shame since it looks incredibly useful. It will also exclude abstract test superclasses.  Assuming you're in control of the pattern used to find test classes I'd suggest changing it to match *Test rather than *Test*. That way TestHelper won't get matched but FooTest will. I don't think it would help because he moved to JUnit 4.4 and that should not matter. You seem to have missed the point of my answer. He has a name filter to determine the classes to be considered as tests. If he changes the filter he can easily exclude the helper classes. Your suggestion is valid however I checked out my tests classes and some start with Test and some end with Test. no clear distinction between utility classes and real test classes. Do you think the convention you suggested is a good practice? (i.e. utils start with Test and tests end with Test) It's almost a convention that you suffix the testcase classes with *Test. You might need to refactor by renaming test classes appropriately and also rename the helpers so they won't use that suffix convention. I agree with Spoike - if you can't tell from the name of the class whether it's a test or a helper you should rename the class. The convention is more ""the class is a test if and only if it ends with Test."" Utility classes may or may not begin with Test - it doesn't matter. we'll go with the convention. Thanks  My specific case has the following scenario. Our tests public class VenueResourceContainerTest extends BaseTixContainerTest all extend BaseTixContainerTest and JUnit was trying to run BaseTixContainerTest. Poor BaseTixContainerTest was just trying to setup the container setup the client order some pizza and relax... man. As mentioned previously you can annotate the class with @Ignore But that caused JUnit to report that test as skipped (as opposed to completely ignored). Tests run: 4 Failures: 0 Errors: 0 Skipped: 1 That kind of irritated me. So I made BaseTixContainerTest abstract and now JUnit truly ignores it. Tests run: 3 Failures: 0 Errors: 0 Skipped: 0  Annotate your util classes with @Ignore. This will cause JUnit not to try and run them as tests.  I was also facing a similar issue (""no runnable methods.."") on running the simplest of simple piece of code (Using @Test @Before etc.) and found the solution nowhere. I was using Junit4 and Eclipse SDK version 4.1.2. Resolved my problem by using the latest Eclipse SDK 4.2.2. I hope this helps people who are struggling with a somewhat similar issue.  What about adding an empty test method to these classes? public void avoidAnnoyingErrorMessageWhenRunningTestsInAnt() { assertTrue(true); // do nothing; } interesting thanks but that falsely increases the number the tests we have :) not that its a big deal"
480,A,How to make JUnit test cases execute in parallel? Possible Duplicate: Running junit tests in parallel? I found the test cases inside jUnit are executed in sequence how to make them execute in parallel? Junit4 provides parallel feature using ParallelComputer: public class ParallelComputerTest { @Test public void test() { Class[] cls={ParallelTest1.classParallelTest2.class }; //Parallel among classes JUnitCore.runClasses(ParallelComputer.classes() cls); //Parallel among methods in a class JUnitCore.runClasses(ParallelComputer.methods() cls); //Parallel all methods in all classes JUnitCore.runClasses(new ParallelComputer(true true) cls); } public static class ParallelTest1 { @Test public void a(){} @Test public void b(){} } public static class ParallelTest2 { @Test public void a(){} @Test public void b(){} } } Hi This is really helpful. but able to run 4 classes only in parallel at a time even though I've more classes in array. Is there any limitation on no of classes running in parallel ? What if I don't want to list every single test class (that's seems like a huge pain)? Is there a way to have it pick up every class automatically and run them in parallel?  Here is some sample code. This works for me really well. ExecutorService. public class TestCases { static ExecutorService exe ; public static void main(String[] args) throws Throwable { test1() ; test2() ; test3() ; } public static void test1() { exe = Executors.newCachedThreadPool() ; for (int i = 0 ; i < 10 ; i++) { Test1 test1 = new Test1() ; exe.execute(test1) ; } exe.shutdown() ; while(!exe.isShutDown()) { } } //same for test2 and test3 } public class Test1 implements Runnable { public Test1() { } @Test public myTest throws Throwable { } }
481,A,"How to set a long Java classpath in MSDOS/Windows? I'm trying to run a particular JUnit test by hand on a Windows XP command line which has an unusually high number of elements in the class path. I've tried several variations such as: set CLASS_PATH=C:\path\a\b\c;C:\path\e\f\g;.... set CLASS_PATH=%CLASS_PATH%;C:\path2\a\b\c;C:\path2\e\f\g;.... ... C:\apps\jdk1.6.0_07\bin\java.exe -client oracle.jdevimpl.junit.runner.TestRunner com.myco.myClass.MyTest testMethod (Other variations are setting the classpath all on one line setting the classpath via -classpath as an argument to java""). It always comes down to DOS throwing up it's hands with this error: The input line is too long. The syntax of the command is incorrect. This is a JUnit test testing a rather large existing legacy project so no suggestions about rearranging my directory structure to something more reasonable those types of solutions are out for now. I was just trying to gen up a quick test against this project and run it on the command line and DOS is stonewalling me. Help! The DOS command line is very limiting in this regard. A workaround is to create a ""pathing jar"". This is a jar containing only a Manifest.mf file whose Class-Path specifies the disk paths of your long list of jars etc. Now just add this pathing jar to your command line classpath. This is usually more convenient than packaging the actual resources together. As I recall the disk paths can be relative to the pathing jar itself. So the Manifest.mf might look something like this: Class-Path: this.jar that.jar ../lib/other.jar If your pathing jar contains mainly foundational resources then it won't change too frequently but you will probably still want to generate it somewhere in your build. For example: <jar destfile=""pathing.jar""> <manifest> <attribute name=""Class-Path"" value=""this.jar that.jar ../lib/other.jar""/> </manifest> </jar> The ManifestClassPath task available in Ant since version 1.7 can be used to generate a suitable property for the Class-Path attribute from an Ant Path.  if I were in your shoes I would download the junction utility from MS : http://technet.microsoft.com/en-us/sysinternals/bb896768.aspx and then map your ""C:\path"" to say ""z:\"" and ""c:\path2"" to say ""y:\"". This way you will be reducing 4 characters per item in your classpath. set CLASS_PATH=C:\path\a\b\c;C:\path\e\f\g;.... set CLASS_PATH=%CLASS_PATH%;C:\path2\a\b\c;C:\path2\e\f\g;.. Now your classpath will be : set CLASS_PATH=z\a\b\c;z\e\f\g;.... set CLASS_PATH=%CLASS_PATH%;y:\a\b\c;y:\e\f\g;.. It might do more depending on your actual classpath. Junction is one front-end for this in NTFS. `mklink /D` is another may already be present in later versions of Windows. mlink doesn't seem to be included on windows 7. junction is included as part of windows 7(enterprise).  You could try this @echo off set A=D:\jdk1.6.0_23\bin set B=C:\Documents and Settings\674205\Desktop\JavaProj set PATH=""%PATH%;%A%;"" set CLASSPATH=""%CLASSPATH%;%B%;"" go to a command prompt and run it twice(no idea why....i have to do so on a windows XP machine) also the paths r set only for the current command prompt session  You could use classpath wildcards. This appears to be new in Java 6. Hey that's cool--I didn't know about that. That's definitely a helpful option especially if (as in our case) the classpath is full of jars many of which are in the same directory.  As HuibertGill mentions I would wrap this in an Ant build script just so that you don't have to manage all of this yourself.  (I suppose you do not really mean DOS but refer to cmd.exe.) I think it is less a CLASSPATH limitation than an environment size/environment variable size limit. On XP individual environment variables can be 8k in size the entire environment is limited to 64k. I can't see you would hit that limit. There is a limit on windows that restricts the length of a command line on WindowsNT+ it is 8k for cmd.exe. A set command is subject to that restriction. Can it be you have more than 8k worth of directories in your set command? You may be out of luck then - even if you split them up like Nick Berardi suggested. Woops yeah old-school is seeping through. Yes cmd.exe.  Have you tried stacking them? set CLASS_PATH = c:\path set ALT_A = %CLASS_PATH%\a\b\c; set ALT_B = %CLASS_PATH%\e\f\g; ... set ALL_PATHS = %CLASS_PATH%;%ALT_A%;%ALT_B% We tried a couple variations of this to no avail. CMD seems to substitute all those %ALT_A% etc on-the-fly and the final path winds up being too long for it to handle giving me the same error.  I think you are up the creek without a paddle here. The commandline has a limit for arguments to call a programm. I have 2 sugestion you could try. First prior to running the junit tests you can let a script/ant_task create JARs of the various classes on the classpath. Then you can put the JARs on the classpath which should be shorter. Another way you could try is to create an antscript to run JUNIT in ANT there should not be such a limit for classpath entries."
482,A,"Mock Runtime.getRuntime()? Can anyone make any suggestions about how best to use EasyMock to expect a call to Runtime.getRuntime().exec(xxx)? I could move the call into a method in another class that implements an interface but would rather not in an ideal world. interface RuntimeWrapper { ProcessWrapper execute(String command) throws IOException; } interface ProcessWrapper { int waitFor() throws InterruptedException; } I was wondering if anyone had any other suggestions? Your class shouldn't call Runtime.getRuntime(). it should expect a Runtime to be set as its dependency and work with it. Then in your test you can easily provide a mock and set it as a dependency. As a sidenote I'd suggest watching this lecture on OO Design for testability. Update: I didn't see the private constructor. You can try using java bytecode instrumentation in order to add another constructor or make the constructor public but that might turn out to be impossible as well (if there are some restrictions on that class). So your option is to make a wrapper (as you suggested in the question) and follow the dependency-injection approach. Thanks for the suggestion - I agree that injecting the dependency is the best way but I would prefer to mock it. However I can't see a way to get a mocked instance of Runtime - it's not an interface and I'm not sure I can subclass it because it has a private constructor. Perhaps I'm missing something? I'm going to go with the wrapper approach :) Thanks again! yup that makes it next to impossible. Check my update.  Perhaps instead of mocking Runtime.getRuntime().exec() you could ""mock"" the script/program/etc. it's supposed to be calling. Instead of passing the real command-line string into exec() write a test script and execute it instead. You could have the script return hard-coded values you could test against just like a mocked class. That's what I tried at first but I found a few problems with that. First of all it breaks the platform independence of the tests (even though the code is designed for Windows the tests are often run on a Linux box) and secondly for some reason it scares me mocking the script. Probably because I'm scared of checking it in :) Also mocking runtime lets me simulate different scenarios more easily. Thanks anyway!  Here is how you would do it with EasyMock 3.0 (and JUnit 4): import org.junit.*; import org.easymock.*; import static org.easymock.EasyMock.*; public final class EasyMockTest extends EasyMockSupport { @Test public void mockRuntimeExec() throws Exception { Runtime r = createNiceMock(Runtime.class); expect(r.exec(""command"")).andReturn(null); replayAll(); // In tested code: r.exec(""command""); verifyAll(); } } The only problem with the test above is that the Runtime object needs to be passed to code under test which prevents it from using Runtime.getRuntime(). With JMockit on the other hand the following test can be written avoiding that problem: import org.junit.*; import mockit.*; public final class JMockitTest { @Test public void mockRuntimeExec() throws Exception { final Runtime r = Runtime.getRuntime(); new NonStrictExpectations(r) {{ r.exec(""command""); times = 1; }}; // In tested code: Runtime.getRuntime().exec(""command""); } }  Bozho above is IMO the Correct Solution. But it is not the only solution. You could use PowerMock or JMockIt. Using PowerMock: package playtest; public class UsesRuntime { public void run() throws Exception { Runtime rt = Runtime.getRuntime(); rt.exec(""notepad""); } } package playtest; import org.junit.Test; import org.junit.runner.RunWith; import org.powermock.core.classloader.annotations.PrepareForTest; import org.powermock.modules.junit4.legacy.PowerMockRunner; import static org.powermock.api.easymock.PowerMock.*; import static org.easymock.EasyMock.expect; @RunWith(PowerMockRunner.class) @PrepareForTest( { UsesRuntime.class }) public class TestUsesRuntime { @Test public void test() throws Exception { mockStatic(Runtime.class); Runtime mockedRuntime = createMock(Runtime.class); expect(Runtime.getRuntime()).andReturn(mockedRuntime); expect(mockedRuntime.exec(""notepad"")).andReturn(null); replay(Runtime.class mockedRuntime); UsesRuntime sut = new UsesRuntime(); sut.run(); } } Thanks for the suggestion I hadn't heard of Powermock before."
483,A,"Are these the sort of edge cases I should think of when using unit testing? When writing unit tests it is normally advisable to test the edge cases. However are these the right sort of things? Test the connection to the db is not open (assert an exception is thrown) Assert that a table which must always have >1 row does so Assert that a field which is required is != null. Assert that an ip has been set in the right format (I can parse the string to IP). Thanks Your tests seem OK but the phrase ""edge case"" normally refers to the tests and checks you need to do around the limits of the input. Say you have a column in your database that can accept 50 characters. Your edge case tests are: Save a string of 49 characters - success Save a string of 50 characters - success (or perhaps failure because of the null termination character depending on your language) Save a string of 51 characters - failure You can see that you are testing around and at the edges of your application where there are most likely to be errors. In this case there could be some confusion of the number of usable characters you can store which could cause errors in applications writing to your database. Other tests where you would test saving a string of 20 characters and saving a string of 100 characters (say) should be done but these are going to be more stable. So basiucally test the parameter. So if my db query took a parameter test around that (GUID which is in wrong format etc). But I guess the tests I wrote out above will still be good. @dotnetdev - that's the idea and yes the tests you have are still good."
484,A,"Java test framework for Selenium RC I'm going to use Selenium RC to replay some tests for a website. I want to kickoff those tests from a Java test framework so that I get nice reports how many tests failed etc. Which java test framework should I use? Is JUnit the preferred framework for this purpose? We use Selenium in conjunction with JUnit. The beauty of this method is the ease of creation of the JUnit tests since we record the Selenium tests in the Selenium IDE and then simply export them to JUnit (though some small tweaking is often necessary). You can then fairly easily integrate them into your continuous integration build and set a job to run them every hour or whatever suits. What I like about the JUnit approach is that everyone on my team is already intimately familiar with JUnit so no learning curve required. I'd also recommend checking out Selenium Grid which allows you to execute your tests in parallel allowing you to get through a lot more in a shorter time especially if you can take advantage of executing them across multiple machines. Thanks for sharing your experience. I think being able to directly export from Selenium IDE to JUnit tests is a great plus. But let's see what other people think.  Below some of my experiences. JUnit has ""rules"" which allow lowering the amount of the boilerplate code. For instance a rule can start a servlet container before the test and stop it after create and inject a Selenium instance and so on. This makes tests much more elegant. TestNG has support for groups (and their dependencies). This is extremely useful for integration testing - for instance you can turn of certain groups in certain environments. JUnit 4.8 introduces ""categories"" which may be something similar. Spring Test has a very nice approach with ""test execution listeners"" which can ""prepare"" your test instances. This is somewhat similar to JUnit rules. Spring Test is also test framework-agnostig that is your tests will almost look the same on JUnit or TestNG. There's also ""profiles"" which are similar to TestNG groups. Here's a sample of Selenium-tested JSF application TestNG-based. Also take a look at my Hifaces20 Testing package maybe you'll find some ideas useful.  We have used JUnit for this purpose and it has worked quite well. The Selenium test are just treated like JUnit tests and run over the selenium server. Though Selenium test seem more closer to Integration tests and therefore you should evaluate testNG once if it suits your needs better. TestNG gives more configuration control. In our case we were happy with Web driver and setup method of JUnit for us so we didn't move to TestNG. I'm heading for a functional test so checking if the most important features of the website are still working. Indeed JUnit is most often used for class tests only so I'm not sure if I should go for JUnit.  Just for supporting one of the answers Yahoo adapt selenium + testng for auto testing web pages. Junit vs. Testng: Testng can have more control such as grouping test cases run in parallel re-try failed test case more flexible on annotating test case. ref: http://www.mkyong.com/unittest/junit-4-vs-testng-comparison/ ""http://www.ibm.com/developerworks/java/library/j-cq08296/"" JUnit is great anyway. have fun. I ended up using JUnit. The reason is simple: I started setting up the whole thing with JUnit and now I run out of time to also try testng :-("
485,A,"NullPointerException on reflection during Robolectric startup - any hints? I am developing an Android project on Eclipse and I'm trying to switch from running tests on the emulator/device (which is very slow) to Robolectric. I replaced the Android libs with Robolectric's added JUnit to the path changed the test case back to a regular TestCase and added the suggested @RunWith(RobolectricTestRunner.class) from the Quick Start guide (making the needed changes to instantiate my Activity instead of relying on Android's activity testing to do it for me). However when I run the test I get: java.lang.NullPointerException at com.xtremelabs.robolectric.RobolectricTestRunner.isInstrumented(RobolectricTestRunner.java:123) at com.xtremelabs.robolectric.RobolectricTestRunner.<init>(RobolectricTestRunner.java:72) at com.xtremelabs.robolectric.RobolectricTestRunner.<init>(RobolectricTestRunner.java:57) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:532) at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:31) at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:24) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:29) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:24) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.<init>(JUnit4TestReference.java:29) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestClassReference.<init>(JUnit4TestClassReference.java:25) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestLoader.createTest(JUnit4TestLoader.java:40) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestLoader.loadTests(JUnit4TestLoader.java:30) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:452) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) This seems to be related to this code (from 0.9 tag since 0.9.1 tag doesn't have it) in RobolectricTestRunner.java: private static boolean isInstrumented() { return RobolectricTestRunner.class.getClassLoader().getClass().getName().contains(RobolectricClassLoader.class.getName()); } but I can't understand how since this reflection should work from the class itself. Also I could swear that it ran once before failing repeatedly - I've deleted the robolectric cache (as suggested here) and replaced Eclipse's JUnit with the latest (4.8.2) but nothing changed. Any hints? UPDATE: I tried to create a new Java (i.e. plain non-Android) test project just as described on the quick start guide. However now the test run complains about not having AndroidManifest.xml: java.lang.RuntimeException: java.io.FileNotFoundException: /Users/chester/Documents/workspace/minitruco-android/minitruco-android-robolectric-test/AndroidManifest.xml (No such file or directory) at com.xtremelabs.robolectric.RobolectricTestRunner.createResourceLoader(RobolectricTestRunner.java:245) at com.xtremelabs.robolectric.RobolectricTestRunner.setupApplicationState(RobolectricTestRunner.java:215) at com.xtremelabs.robolectric.RobolectricTestRunner.internalBeforeTest(RobolectricTestRunner.java:163) at com.xtremelabs.robolectric.RobolectricTestRunner.methodBlock(RobolectricTestRunner.java:143) at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184) at org.junit.runners.ParentRunner.run(ParentRunner.java:236) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: java.io.FileNotFoundException: /Users/chester/Documents/workspace/minitruco-android/minitruco-android-robolectric-test/AndroidManifest.xml (No such file or directory) at java.io.FileInputStream.open(Native Method) at java.io.FileInputStream.<init>(FileInputStream.java:137) at java.io.FileInputStream.<init>(FileInputStream.java:96) at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:87) at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:178) at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(Unknown Source) at com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(Unknown Source) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(Unknown Source) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(Unknown Source) at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(Unknown Source) at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(Unknown Source) at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(Unknown Source) at javax.xml.parsers.DocumentBuilder.parse(Unknown Source) at com.xtremelabs.robolectric.RobolectricTestRunner.findResourcePackageName(RobolectricTestRunner.java:255) at com.xtremelabs.robolectric.RobolectricTestRunner.createResourceLoader(RobolectricTestRunner.java:236) ... 18 more I guess the test should load the files from the original project (minitruco-android) which shows up in both the test project's build path and on the Run/Debug build configuration on Eclipse (under the test project's classpath entry). Anyway I've copied the file from my original test project (just for the sake of testing) but then it complains about R.java (and that should really have been imported from the original project): java.lang.RuntimeException: java.lang.ClassNotFoundException: caught an exception while obtaining a class file for me.chester.minitruco.test.R at com.xtremelabs.robolectric.RobolectricTestRunner.createResourceLoader(RobolectricTestRunner.java:245) at com.xtremelabs.robolectric.RobolectricTestRunner.setupApplicationState(RobolectricTestRunner.java:215) at com.xtremelabs.robolectric.RobolectricTestRunner.internalBeforeTest(RobolectricTestRunner.java:163) at com.xtremelabs.robolectric.RobolectricTestRunner.methodBlock(RobolectricTestRunner.java:143) at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184) at org.junit.runners.ParentRunner.run(ParentRunner.java:236) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: java.lang.ClassNotFoundException: caught an exception while obtaining a class file for me.chester.minitruco.test.R at javassist.Loader.findClass(Loader.java:359) at com.xtremelabs.robolectric.RobolectricClassLoader.findClass(RobolectricClassLoader.java:60) at javassist.Loader.loadClass(Loader.java:311) at java.lang.ClassLoader.loadClass(ClassLoader.java:268) at com.xtremelabs.robolectric.RobolectricClassLoader.loadClass(RobolectricClassLoader.java:37) at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:336) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:186) at com.xtremelabs.robolectric.RobolectricTestRunner.createResourceLoader(RobolectricTestRunner.java:237) ... 18 more Caused by: javassist.NotFoundException: me.chester.minitruco.test.R at javassist.ClassPool.get(ClassPool.java:436) at com.xtremelabs.robolectric.AndroidTranslator.onLoad(AndroidTranslator.java:68) at javassist.Loader.findClass(Loader.java:340) ... 26 more The only different thing between my setup and the quick start is that I'm keeping test sources on the test project and regular sources on the regular project (since the first builds I'd assume the test project sees the regular sources - which of course it depends heavily upon). I'm using SoyLatte but tried also Mac OS's default 1.6 JVM with the same results. Thank you. We have had the same problems in the past. What works for us is extending RoboletricTestRunner and providing it a RoboletricConfig instance that is aware of our Project's location. This way I don't have to tinker with any run configurations with eclipse it just works.  package ...; import java.io.File; import org.junit.runners.model.InitializationError; import com.xtremelabs.robolectric.RobolectricConfig; import com.xtremelabs.robolectric.RobolectricTestRunner; public class TestRunner extends RobolectricTestRunner { public static final string MAIN_PROJECT_PATH = ""../path_to_android_project""; public TestRunner(Class<?> testClass) throws InitializationError { super(testClass new RobolectricConfig(new File(MAIN_PROJECT_PATH))); } } I think that this is a right answer but it posted too late Saved my day!!!Thanks!  No need to create a custom RoboelectricTestRunner just play a bit with your 'Run Configurations' on Eclipse. On the configuration for running your tests go to the classpath tab and ensure that the libraries required for your test e.g. robolectric-2.2-jar-with-dependencies.jar and android.jar and the project itself are in the User Entries and ONLY the Java SDK is in the Bootstrap Entries. Then add this file: [project_path]/test/org.robolectric.Config.properties  manifest=../AndroidManifest.xml  Normally you won't need to extend TestCase but I don't think that should be causing this problem. Could you include the source of the failing test? Regarding the simplified project in your update make sure that Eclipse is configured to set the current working directory to the root of the main project not the test project so it will find AndroidManifest.xml and the res directory. That was it! By setting the working directory on the simplified test project to the main one it found all it needed. Not sure why converting the old project didn't work but that was a shortcut anyway. Thank you very much for your help and the excellent project! Just a comment to anyone like me who had to hunt around for right place to set the working Directory. It's located in the Run Configuration setting on the Arguments tab under the heading ""Working directory"" Similar problem here. I removed/added an emulator and had set the application to launch in the Run Configuration but it was returning null because it didn't know what to launch against. Once I selected the emulator it started up without a nullpointer."
486,A,"JUnit Eclipse Plugin? I feel stupid for not being able to find this but where is the JUnit plugin for Eclipse? I've included the latest .jar in my buildpath but I still don't have the option to create a new test case run test cases with the green/red bar etc. I need the plugin for this right? EDIT: I'm using ""Eclipse for PHP Developers"". So perhaps that wouldn't include JUnit automatically. Do I have to download Eclipse again or can I just get that functionality somehow? JUnit is part of Eclipse Java Development Tools (JDT). So either install the JDT via Software Updates or download and install Eclipse IDE for Java Developers (actually I'd recommend installing Eclipse IDE for Java EE Developers if you want a complete built-in environment for server side development).  Eclipse has built in JUnit functionality. Open your Run Configuration manager to create a test to run. You can also create JUnit Test Cases/Suites from New->Other.  You should be able to add the Java Development Tools by selecting 'Help' -> 'Install New Software' there you select the 'Juno' update site then 'Programming Languages' -> 'Eclipse Java Development Tools'. After that you will be able to run your JUnit tests with 'Right Click' -> 'Run as' -> 'JUnit test'.  Junit is included by default with Eclipse (at least the Java EE version I'm sure). You may just need to add the view to your perspective. Just a note J2EE is renamed Java EE since a year or three. Good point Thanks.  It's built in Eclipse since ages. Which Eclipse version are you using? How were you trying to create a new JUnit test case? It should be File > New > Other > Java - JUnit - JUnit Test Case (you can eventually enter Filter text ""junit""). actually I'm using PHP Eclipse (see above). Can this be fixed or do I need to download again? I am not sure about that sorry. But if you can't find it anywhere in the list then I recommend to go get ""Eclipse for Java developers"" (or ""Eclipse for Java EE developers"" if you like to play with Java/JSP/Servlet webapps as well) and then add the PHP plugin afterwards. I can't find the Junit package fore eclipse after googleing around a bit so I think your best bet would just be to download a java version of eclipse. @aubreyrhodes FYI it's included in the JDT.  You do not need to install or update any software for the JUnit. it is the part of Java Development tools and comes with almost most of the latest versions in Eclipse. Go to your project. Right click onto that->Select buildpath->add library->select JUnit from the list ->select the version you want to work with-> done build you project again to see the errors gone:)  Maybe you're in the wrong perspective? Eclipse has a construct called a ""perspective""; it's a task-oriented arrangement of windows toolbar buttons and menus. There's a Java perspective a Debug perspective there's probably a PHP perspective etc. If you're not in the Java perspective you won't see some of the buttons you expect (like New Class). To switch perspectives see the long-ish buttons on the right side of the toolbar or use the Window menu."
487,A,"JUnit: How to simulate System.in testing? I have a Java command-line program. I would like to create JUnit test case to be able to simulate System.in. Because when my program runs it will get into the while loop and waits for input from users. How do I simulate that in JUnit? Thanks You could create a custom InputStream and attach it to the System class class FakeInputStream extends InputStream { public int read() { return -1; } } And then use it with your Scanner System.in = new FakeInputStream(); Before: InputStream in = System.in; ... Scanner scanner = new Scanner( in ); After: InputStream in = new FakeInputStream(); ... Scanner scanner = new Scanner( in ); Although I think you should better to test how your class should work with the data read from the input stream and not really how it reads from there. Oohh yeap I got confused. What I tried to say was ... well I will edit my entry :) From a TDD perspective this avoids the design that the test is ""driving"" or attempting to indicate. However the OP didn't specify TDD and from a test-after perspective it is a very reasonable thing to do - take advantage of the system global. You can't just do System.in = xxx as System.in is final. You can use System.setIn but make sure you return to the default in the tear down. Also you don't need to roll your own InputStream ByteArrayInputStream will do the job nicely.  Try to refactor your code to use dependency injection. Instead of having your a method that uses System.in directly have the method accept an InputStream as an argument. Then in your junit test you'll be able to pass a test InputStream implementation in place of System.in.  It is technically possible to switch System.in but in general it would be more robust not to call it directly in your code but add a layer of indirection so the input source is controlled from one point in your application. Exactly how you do that is an implementation detail - the suggestions of dependency injection are fine but you don't necessarily need to introduce 3rd party frameworks; you could pass round an I/O context from the calling code for example. How to switch System.in: String data = ""Hello World!\r\n""; InputStream stdin = System.in; try { System.setIn(new ByteArrayInputStream(data.getBytes())); Scanner scanner = new Scanner(System.in); System.out.println(scanner.nextLine()); } finally { System.setIn(stdin); }  There are a few ways to approach this. The most complete way is to pass in an InputStream while running the class under test which is a fake InputStream which passes simulated data to your class. You can look at a dependency injection framework (such as Google Guice) if you need to do this a lot in your code but the simple way is:  public class MyClass { private InputStream systemIn; public MyClass() { this(System.in); } public MyClass(InputStream in) { systemIn = in; } } Under test you would call the constructor that takes the input stream. You cloud even make that constructor package private and put the test in the same package so that other code would not generally consider using it. +1. I'm with you on this. I would go a step further: `InputData` as a high level wrapper around `InputStream` in unit testing you should care more about what your class does and not really about the integration."
488,A,Java / JUnit and Selenium RC - is there a wrapper library? I use JUnit and Selenium (1) to write automated integration tests. I've used it on a couple of projects so far. I find the Selenium API too low-level and end up writing generic 'wrapper' code to deal with things like: checking the fields of a form are the same as they were before some 'save' button was clicked sharing the selenium instance between tests checking the browser isn't showing a stack trace waiting for elements to become present (from AJAX calls) Is this an inevitable part of writing your own test harness? Or is there a wrapper library somewhere / should I bother to create one? These days I would use FluentLenium which covers a lot of what I asked for originally: https://github.com/FluentLenium/FluentLenium  You can create DSL for application driver using the Page Object pattern and the windowlicker (Java GUI testing framework which uses Selenium 2) for implementation. More theory and practice techniques see in: http://www.wakaleo.com/blog/279-selenium-2web-driver-the-land-where-page-objects-are-king http://www.slideshare.net/alimenkou/dsl-page-object-and-selenium-a-way-to-reliable-functional-tests/ http://www.growing-object-oriented-software.com/ Groove developers can use Tellurium Automated Testing Framework: http://code.google.com/p/aost/ Tellurium vs Selenium : Compare  Personally I think that Selenium 2 is about the right abstraction level for a browser automation framework. I agree with you that there are some annoying things like handling waiting for ajax calls which could have a bit more support in the framework but in Selenium 2 there's a support package which will probably contain those features when it is right to include them. The rest of the points you mention seem to me like project specific features which you wouldn't want in the core framework. You can easily roll your own framework which is right for your kind of projects (and you should) or you can search the internet for projects which contain stuff you also find usefull and adjust. For example I've based my testsuite on this: http://code.google.com/p/design-of-selenium-tests-for-asp-net/ but there are many more higher level frameworks ( some super highlevel ) built on top of selenium which you will find with a search ( http://code.google.com/hosting/search?q=selenium&projectsearch=Search+projects ). So yeah I think it's inevitable but it shouldn't cost you that much work. Fair points thanks for your answer. Will take a look at Selenium 2 should be out of beta soon. Selenium 2 b3 is pretty stable so i suggest you give it a go.
489,A,"Running Junit & PowerMock with Mockito through PowerMockRunner from maven I am not being able to run Powermock through maven. I'm the PowerMock Mockito and PowerMockRunner for driving a jUnit test. Here's the test: @RunWith(PowerMockRunner.class) @PrepareForTest( { UserLocalServiceUtil.class ExpandoBridge.class }) public class AlertNotificationsTest { //... I haven't configured anyting special for running the test. My pom references the following deps: org.mockito | mockito-all | 1.8.0 junit | junit | 4.6.0 org.powermock.modules | powermock-module-junit4 | 1.3.1 org.powermock.api | powermock-api-mockito | 1.3.1 when I run mvn -Dtest=AlertNotificationsTest test mvn says there's no test to run. But if I run the same test class from eclipse everything runs ok. Am I doing something wrong? Here's my pom.xml below (the relevant parts)  <dependency> <groupId>org.testng</groupId> <artifactId>testng</artifactId> <version>5.9</version> <classifier>jdk15</classifier> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.6</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-all</artifactId> <version>1.8.0</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock.modules</groupId> <artifactId>powermock-module-junit4</artifactId> <version>1.3.1</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock.api</groupId> <artifactId>powermock-api-mockito</artifactId> <version>1.3.1</version> <scope>test</scope> </dependency> </dependencies> Here's the output from maven mvn -Dtest=AlertNotificationsTest test ... [INFO] Surefire report directory: C:\Devel\Java\EP_PORTAL\information-provider\target\surefi ------------------------------------------------------- T E S T S ------------------------------------------------------- Running TestSuite Tests run: 0 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.313 sec Results : Tests run: 0 Failures: 0 Errors: 0 Skipped: 0 [INFO] ------------------------------------------------------------------------ [ERROR] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] No tests were executed! (Set -DfailIfNoTests=false to ignore this error.) [INFO] ------------------------------------------------------------------------ Note: I can run other tests I just can't run this test. If I make the AlertNotificationsTest class extend junit.framework.TestCase the class gets picked up by maven but it seems that it does not get driven by PowerMockRunner. Here's the output of that: Running TestSuite [ERROR]: No test suite found. Nothing to run Tests run: 4 Failures: 2 Errors: 0 Skipped: 0 Time elapsed: 1.053 sec <<< FAILURE! Results : Failed tests: testSingleEventNotification(pt.estradasportugal.traffic.services.events.AlertNotificationsTest) testTwoEventNotification(pt.estradasportugal.traffic.services.events.AlertNotificationsTest) Tests run: 4 Failures: 2 Errors: 0 Skipped: 0 Again these tests run just fine with Eclipse. Update I found a possible problem & workaround. I have tests with TestNG and JUnit. If I remove TestNG from my pom and migrate all my tests to JUnit I am able to run my PowerMock test with mvn test. So it seems that there's a problem with maven and the junit/testng combo. I'd like to be able to run both but If I don't find a way I'll go and answer my own question. Thanks guys&gals Is the test (not extending `junit.framework.TestCase`) triggered when you just run `mvn test`? No it is not triggered. Then don't even expect it to be when using `-Dtest`. You have another problem (and it looks like you found the source). I came across this issue as well but its not a PowerMock issue. My Test class was named XStaticTests.java. When I run ""mvn clean test"" this test would not run it only ran when I specified the test using ""-Dtest=..."" The surefire documentation mentions that by default only these patterns are searched for : ""/Test*.java"" - includes all of its subdirectories and all java filenames that start with ""Test"". ""/Test.java"" - includes all of its subdirectories and all java filenames that end with ""Test"". ""*/*TestCase.java"" - includes all of its subdirectories and all java filenames that end with ""TestCase"". Therefore changing the classname to one that ends with one of these will run when ""mvn test"" is called else the surefire plugin needs to be configured with the class name specifically.  I can't reproduce your problem. With the following content in my pom.xml:  <repositories> <repository> <id>powermock-repo</id> <url>http://powermock.googlecode.com/svn/repo/</url> </repository> </repositories> <properties> <powermock.version>1.3.1</powermock.version> </properties> <dependencies> <dependency> <groupId>org.powermock.modules</groupId> <artifactId>powermock-module-junit4</artifactId> <version>${powermock.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock.api</groupId> <artifactId>powermock-api-mockito</artifactId> <version>${powermock.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.6</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-all</artifactId> <version>1.8.0</version> </dependency> </dependencies> And the following test class (skipping the imports): @RunWith(PowerMockRunner.class) @PrepareForTest( { App.class }) public class AppTest { @Test public void testApp() { assertTrue(true); } } Running mvn test -Dtest=AppTest just works fine and give me the following output:  ... ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.mycompany.app.AppTest Tests run: 1 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.135 sec Results : Tests run: 1 Failures: 0 Errors: 0 Skipped: 0 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESSFUL [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3 seconds [INFO] Finished at: Wed Nov 25 17:34:32 CET 2009 [INFO] Final Memory: 9M/79M [INFO] ------------------------------------------------------------------------ So the question is: do you have a method annotated with @Test in AlertNotificationsTest? Yes I have alot of tests annotated with @org.junit.Test. Please post your `pom.xml` then. As I said I can't reproduce the behavior you describe. And BTW what do you observe when you run `mvn test`? Have posted what you requested  I had the same problem and it took me a while to figure out. My setup was pulling in an older version of jboss.javassist which oddly was preventing the PowerMockRunner from working at all. It's worth noting that I also have a mixed JUnit / TestNG environment. I previously tried the solution of adding multiple surefire providers and that didn't work either (using surefire 2.14.1). After upgrading to surefire 2.17 both my JUnit and TestNG tests started running without needing to declare any surefire providers. Here's my plugin section...  <plugin> <artifactId>maven-surefire-plugin</artifactId> <version>2.17</version> <configuration> <groups>spring unit integration</groups> <systemPropertyVariables> <java.awt.headless>true</java.awt.headless> <org.apache.activemq.default.directory.prefix>target/test/</org.apache.activemq.default.directory.prefix> <log4j.configuration>file:${project.basedir}/src/test/resources/log4j.properties</log4j.configuration> </systemPropertyVariables> <argLine>${surefire.args}</argLine> </configuration> </plugin> ... and the relevant testing deps ...  <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-all</artifactId> <version>1.9.5</version> <scope>test</scope> </dependency> <!-- PowerMock versions are compatible with specific Mockito versions. https://code.google.com/p/powermock/wiki/MockitoUsage13 --> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-module-junit4</artifactId> <version>1.5.4</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-api-mockito</artifactId> <version>1.5.4</version> <scope>test</scope> </dependency> <!-- without this PowerMock tests don't run in maven --> <dependency> <groupId>jboss</groupId> <artifactId>javassist</artifactId> <version>3.8.0.GA</version> <scope>test</scope> </dependency>  If you look into the source of the Surefire plugin it does some sneaky stuff. If it finds any TestNG packages in the Classloader it will opt to run a TestNG TestRunner. I haven't yet seen any examples of both JUNit and TestNG tests running well side-by-side. This is possible using two executions.  Powermock setup looks Ok to me and the jars seem fine (assuming maven transitive dependencies get the other powermock jars - we have about 6-7 after our ivy resolve gets them) Eclipse might be using it's own ""internal"" JUnit library thus the different behaviours ? Are the test annotated with org.junit.@Test ? Yes the tests are annotated with @org.junit.Test  I just had this error and worked through the solution. My pom.xml file had the following dependency: <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-mockito-release-full</artifactId> <version>1.5</version> <classifier>full</classifier> <scope>test</scope> </dependency> The problem comes from the fact my code uses JUnit and the above dependency has an external dependency on TestNG. This was stopping my test from running. Why I don't know - you would have though a test framework would have been tested a little bit better!!! Anyway the solution was to break down the 'full' dependencies to just those required: <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-api-mockito</artifactId> <version>1.5</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-core</artifactId> <version>1.5</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-module-junit4</artifactId> <version>1.5</version> <scope>test</scope> </dependency> That solved it. BTW I used mvn dependency:tree to understand the associated dependencies.  There was a problem when mixing both TestNG & JUnit tests. Migrating all tests to Junit solved my problem. Thanks guys."
490,A,Jmock mock DAO object I wrote a method that retrieves certain list of strings given a correct string key. Now when I create a list(the one to be retrieved by method descibed in previous sentence) and create test I can easily get results and test passes successfully. Now on the other hand if I save the content of this list to database in 2 columns key and value I wrote a class which retrieves this items with method inside it. And when I print it out to console the expected results are correct now I initialize my DAO from application context where inside its bean it gets session and because of DAO works. Now I'm trying to write a test which will mock the DAO because I'm running test localy not on the server .. so I told jmock to mock it : private MyDAO myDAO; in the setup() myDAO = context.mock(MyDAO.class); I think I'm mocking it correctly or not how can I mock this data from database? what is the best way? Is there somewhere good Jmock documentation? on their official site its not very good and clear you have to know what you seek in order to find it can't discover something cool in the mean time. OR can someone help me with this approach : How can I create application context which I will use just for tests to instansiate DAO and few beans there like on server. So I can use it in the tests? Also suggestions explanation all is welcome . thank you @Bozho write anything in post so I accept your answer I managed to accomplish this from the link you provided and I want to accept your answer http://techblog.bozho.net/?p=118 You haven't told us what the problem is... did `mock(MyDAO.class)` not work? @skaffman nop mock didn't work .. I mock did create the false dao object but I didn't get any result from its method which should retrive string given a string key MyDAO is a interface you only can make a mock of interfaces. Actually you can mock classes but we don't recommend it except in emergencies.  This and this posts describe how and why to test DAOs. (you can easily isolate spring and maven from the example if you are not using them - the point is using HSQLDB) With mocks the downside is that you have to implement potentially complex DAO/JPA/Database behavior (e.g. a store causes a private @Id field to become set). But with a mock you can generate error responses that would be perhaps difficult to create with staged data. Also with a mocking framework you keep the data for the test local to the test itself not in a separate file. Is this correct? How do you weight this trade-off? I think it's rarely justified to mock the dao - you need to have a very complex mock. If you end up with a complex mock you have a different design problem. It suggests that some of your interfaces are too large.
491,A,is there an option for testing java applications from the outside like we do with the web applications? I would like some suggestions if your project has some java application elements as well as web then is there an option for testing java applications from the outside like we do with the web applications do you mean testing a Swing applications GUI? Are you talking about GUI applications ? I mean Application having java application elements as well as a web application that can be tested via UI This is called Blackbox testing. Typically you would write another program that provides inputs to your applications and verifies the output. This is easier for a command line application but for GUI testing you would need to use a framework like others have mentioned. Another approach is to write a drive that includes the jar files and directly call the public functions and do input/output testing. Just found a tool named Jameleon (automation frameowrk havng plug ins for all types of applications). ANy comments on it  Do you mean testing desktop applications through the GUI? If so there are many tools for doing this both commercial and open source. Some open-source projects to google for: Window Licker (I am an author of this one) FEST Marathon Abbot  Your question is not very clear to me. But from what I understand I feel jMeter could be something that might help. jMeter can not only test web applications (from outside) but ALSO standalone java applications. Also do not dismiss jMeter as a performance testing tool. There are ample features in it that makes it a good enough tool for Functional testing as well. Add a link to jMeter for an upvote ;) shd have done that earlier itself. http://jakarta.apache.org/jmeter/
492,A,"How to test Menu I need to cover Menu functionality by unit tests however I'm struggling to obtain Menu object. Following test case fails (mMenu is null): sendKeys(KeyEvent.KEYCODE_MENU); mMenu = (Menu) mActivity.findViewById(com.###.###.R.id.main_menu); assertNotNull(mMenu); Please advice. Thank you. If you want to do UI system or function tests I would recommend you to use Robotium. Then you can just use sendKey(Solo.MENU) and then click on the menu items by using clickOnText() or clickOnView(). When you have done that you can assert right behaviour. Just asserting that it should not be null is not enough. You should check Robotium out its way more appropriate to use when testing things like this. +1. Just started using it. Looks very handy.  I ran into this same scenario and came up with the following (very simple) solution in my implementation of ActivityInstrumentationTestCase: ... ActivityMonitor am = getInstrumentation().addMonitor(LoginActivity.class.getName() null false); // Click the menu option getInstrumentation().sendKeyDownUpSync(KeyEvent.KEYCODE_MENU); getInstrumentation().invokeMenuActionSync(mActivity R.id.logout 0); Activity a = getInstrumentation().waitForMonitorWithTimeout(am 1000); assertEquals(true getInstrumentation().checkMonitorHit(am 1)); a.finish(); ... This snippet of code does three things: Clicks the menu option Ensures we go to the appropriate activity after the menu option is clicked and finishes the activity that was started (very important for the tests following this one). I hope this helps. This was a useful answer and code snippet most importantly it highlights that you have to send the 'KeyEvent.KEYCODE_MENU' first before invoking the menu action. In hindsight it's obvious I guess - you have to have the menu showing before you can invoke one it's items. FWIW I use getInstrumentation().invokeMenuActionSync() without prior call to sendKeyDownUpSync() and it works just fine as well. See the answer to this question: http://stackoverflow.com/questions/5209154/test-menu-items I observed that the call to sendKeyDownUpSync is necessary if you want to test if under conditions that are the result of work done in onPrepareOptionMenu invokeMenuActionSync returns true. Without the call onPrepareOptionsMenu was not invoked.  Use instrumentation to test the pressing of the menu item for you. Here is a sample test case of mine that invokes the ""Settings"" menu which starts another activity. public void testCanGoToSettings() { final MainActivity activity = getActivity(); Instrumentation.ActivityMonitor am = getInstrumentation().addMonitor(ConfigureActivity.class.getName() null /* result */ true /* block */); getInstrumentation().sendKeyDownUpSync(KeyEvent.KEYCODE_MENU); getInstrumentation().invokeMenuActionSync(activity R.id.menu_settings 0 /* flags */); /* if not block in addMonitor() above then comment out this... Activity a = getInstrumentation().waitForMonitorWithTimeout(am 1000); a.finish(); */ }   Activity act =launchActivity(intent); MenuBuilder builder=new MenuBuilder(mInst.getTargetContext()); act.onCreateOptionsMenu(builder); act.onPrepareOptionsMenu(builder); Log.i(TAG ""BuilderSize: ""+builder.size()); int visible=0; for(int i=0;i<builder.size();i++) { MenuItem item=builder.getItem(i); if(item.isVisible()&& item.isEnabled()) { Log.i(TAG item.getTitle().toString()); visible++; } } act.finish(); MenuBuilder cannot be resolved into a type!  What exactly are you trying to test? That menu items do the correct action? You can call Activity.openOptionsMenu() to open the menu and get a reference to the menu by overriding one of the onMenu methods. At that point you can use Menu.performIdentifierAction to select menu items. Yes I need to check that all menu and sub-menu items are doing correct actions. sendKeys(KeyEvent.KEYCODE_MENU) works correctly and I can see Menu on emulator (I can't see it if I call Activity.openOptionsMenu()). Do you mean I need to override onMenuOpened in target app and save Menu reference inside of activity? Maybe there is a way to not touch target application?"
493,A,"JUnit JPA and Spring: How to ensure that unit tests leave database clean on completion I'm trying to use SpringJunit4ClassRunner to test my DAO classes without leaving data behind when I've finished through the use of the @Transactional annotation. My DAO class contains (stripped down): @Repository public class IdsFunctionJpaController { @PersistenceContext EntityManager em; public void save(IdsFunction function) { if (function.getId() == 0) { create(function); } else { update(function); } } @Transactional private void create(IdsFunction idsFunction) { try { em.persist(idsFunction); } catch (Exception e) { System.out.println(e); } finally { em.close(); } } @Transactional private void update(IdsFunction function) { try { em.merge(function); } finally { em.close(); } } } and my starting JUnit test case is @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""/applicationContext.xml""} ) public class IdsFunctionJpaControllerTest { @Autowired IdsFunctionJpaController dao; @Test @Transactional public void addFunction() { IdsFunction function = new IdsFunction(); function.setDescription(""Test Function Description""); dao.save(function); assertTrue(function.getId() != 0); } } What I'm trying to do here is simply test that the entity has been created but this test fails. If I remove the @Transactional annotation then the test passes but the test entity remains in the database. What am I doing wrong? Regards Proxying mechanisms You are banging your head against JDK proxies. Your dao.save() method is non-transactional and it tries to call the transactional methods create() and update(). But the transactional stuff is happening in a JDK proxy outside the class while the save method is already in the class. See this previous answer of mine for reference. Solutions: make your save() method transactional (much better) don't make your DAOs transactional at all. Transactions belong in the service layer not the DAO layer. Reference: Understanding AOP Proxies Declarative Transaction Management Update: I was misguided by the presence of the confusing @Transactional annotations on the Dao methods. You should delete them they do nothing and confuse people. As you can read in the links I posted above @Transactional annotations only have effect when they are present on public Methods that will be called from outside the Spring bean (so you can't have one method in a class that delegates to one or more proxied methods of the same class). Transactional Tests Spring provides special support classes for transactional testing as outlined in 9.3.5.4 Transaction management . If you let your test class extend from AbstractTransactionalJUnit4SpringContextTests you get an automatic transaction rollback after every test. In most cases that is exactly what you need. -1 as it has nothing to do with proxies and transactions do not propagate like that. +1 for pointing out that the dao layer should not be transactional. @OrangeDog I have now seen that there is also a `@Transactional` attribute on the test method. I had only seen the `@Transactional` annotations on the DAO methods. These annotations are nonsense and I have pointed out why.  You need to flush the session. Ordinarily this happens at the end of a transaction which is why you usually only have to worry about it in tests. Inject the EntityManager into your test class and call em.flush() after the save. Also your DAO layer shouldn't be transactional. Transactions typically only make sense in the service layer. Edit: In fact your DAO is also completely wrong which this test won't be able to show. Those transactional annotations will have no effect as they are internal method calls. You should also never close the EntityManager yourself - the container (Spring) will do this for you. Also don't catch generic Exceptions and when you do don't just log and ignore them. Exceptions should be propagating to the service layer where they should be handled properly. Also don't print to stdout use a proper logging framework. *Those transactional annotations will have no effect as they are internal method calls.* Yes as I have correctly pointed out. @Sean - Yes but that's not part of the solution to this specific question. true. but you made it sound like I misunderstood the concept."
494,A,How to add a spring application context to the java runtime to be used for integration tests within eclipse? I am trying to add a test spring application context to the java runtime so that my beans can be wired properly for my integration tests. Never mind I solved it. I just had to add the folder containing my test application context as a source folder and VOILA! I am pretty sure there are better ways to do it.  Spring supplies some tools that can help with such testing. See Spring TestContext Framework.
495,A,Need advice on improving testability I have developed a stack of web Services based on: Spring ws 2.0 with jaxb2 maven plugin (to ease the pain). Hibernate. PostgResql. We are using the following to test: Junit test with Mockito. Spring test for Dao & service layer. The new Spring ws test & Smock api. SoapUi Api for testing with their maven plugin. We have TracWiki for the wiki side. All is fully automated in a maven build with Hudson even the deployment of the webapp with cargo on distant server. We have 5 virtual servers on a single machine on Debian (using vserver). We don't have a single performance test and we don't have any webapp tools to monitor. What do you recommend to go a step further? I'm really looking for new ways and/or tools to improve everything. I don't see you mentioning any code coverage tools. While coverage isn't everything it can help finding the parts of your code which aren't covered by the tests (or perhaps even dead).  Hey. Incorporate Sonar into your builds. You will get lots of informations about your code. Than you @yoosiba +1 being able to see the history of **every** metric is priceless.
496,A,"How do I Unit Test a servlet? I have a servlet called Calculator. It reads the parameters left right and op and returns by setting an attribute result in the response. What is the easiest way to unit test this: basically I want to create an HttpServletRequest set the parameters and then checking the response - but how do I do that? Here's the servlet code (it's small and silly on purpose): public class Calculator extends javax.servlet.http.HttpServlet implements javax.servlet.Servlet { public Calculator() { super(); } protected void doGet(HttpServletRequest request HttpServletResponse response) throws ServletException IOException { } protected void doPost(HttpServletRequest request HttpServletResponse response) throws ServletException IOException { Integer left = Integer.valueOf(request.getParameter(""left"")); Integer right = Integer.valueOf(request.getParameter(""right"")); Integer result = 0; String op = request.getParameter(""operator""); if (""add"".equals(op)) result = this.opAdd(left right); if (""subtract"".equals(op)) result = this.opSub(left right); if (""multiply"".equals(op)) result = this.opMul(left right); if (""power"".equals(op)) result = this.opPow(left right); if (""divide"".equals(op)) result = this.opDiv(left right); if (""modulo"".equals(op)) result = this.opMod(left right); request.setAttribute(""result"" result); // It'll be available as ${sum}. request.getRequestDispatcher(""index.jsp"").forward(request response); } } ... } Can you show some of the code for the servlet? possible duplicate of [Unit-testing servlets](http://stackoverflow.com/questions/53532/unit-testing-servlets) Often the important logic of a program is factored out into other classes that are usable in a variety of contexts instead of being tightly coupled to a Servlet Engine. This leaves the servlet itself as a simple adapter between the web and your application. This makes the program easier to test and easier to reuse in other contexts like a desktop or mobile app. You still need to be able to test the ""simple adapter between the web and your application""..  Can't say this is the best method to do so : but to unit test a simple servlet like that (one not using forwards context etc..) what you could simply do is : Create mock HttpServletReqeust and HttpServletResponse instances using any mocking library. Even simpler would be using RequestWrapper and ResponseWrapper classes (simple custom classes implemented by extending the HttpServletReqeust and HttpServletResponse classes). On these mock (or custom) instances set certain properties - the parameters you want to test against in each test case - e.g. op=add for a addition unit test. If you are using custom classes you can simply set them in an internal properties object. If you are using mocks then settings expectations would do. Create an instance of the servlet - new Calculator() keeping the required libs in the class path. Now call the service method on this instance. When the call returns get the o/p from the response class and assert it. Since the response class is again a custom class or a mocked version this should be easy. For mocking a simply starting point would be EasyMock or Mockito (my fav) An example for the wrapper : http://tomcat.apache.org/tomcat-5.5-doc/servletapi/javax/servlet/http/HttpServletRequestWrapper.html HTH  Check out ServletUnit. It's part of HttpUnit. http://httpunit.sourceforge.net/doc/servletunit-intro.html  Generally you should abstract your business logic from Servlet container details. You can mock ServletRequest using Spring test package but it would be a bad idea to simulate Servlet container. So you should either run system tests on a real container or move your logic from servlet into a separate bean and test it in isolation.  public class Calculator { public Integer calculate(Integer left Integer right String op) { Integer result = 0; if (""add"".equals(op)) result = this.opAdd(left right); if (""subtract"".equals(op)) result = this.opSub(left right); if (""multiply"".equals(op)) result = this.opMul(left right); if (""power"".equals(op)) result = this.opPow(left right); if (""divide"".equals(op)) result = this.opDiv(left right); if (""modulo"".equals(op)) result = this.opMod(left right); return result; } } public class CalculatorServlet extends HttpServlet { protected void doPost(HttpServletRequest request HttpServletResponse response) throws ServletException IOException { Integer left = Integer.valueOf(request.getParameter(""left"")); Integer right = Integer.valueOf(request.getParameter(""right"")); String op = request.getParameter(""operator""); Integer result = calculator.calculate(left right op); request.setAttribute(""result"" result); // It'll be available as ${sum}. request.getRequestDispatcher(""index.jsp"").forward(request response); } }  There are a few libraries out there that you can use. Are you using Spring http://www.springsource.org/ in your application? If so there is one application for spring (spring-test) that contains MockHttpServletRequest. For example: @Test public void shouldReturnAValidaRedirectionMessage() { MockHttpServletRequest request = new MockHttpServletRequest(); request.addParameter(""op"" ""addition""); request.addParameter(""left"" ""1""); request.addParameter(""right"" ""5""); CalculatorServlet servlet = new CalculatorServlet(); Operation operation = servlet.getOperation(request); assertNotNull(operation); assertEquals(ADDITION operation.getOperationType()); ..."
497,A,How to select some of test cases to compose the BAT test(build acceptance test)? In junit test should I create another test suite named as ExampleBAT to do this and this test suite contains the selected test cases? The BVT/BAT is a short set of tests which is done after the build and exercises the main functionality. So for (large) applications JUnit might not be the right choice for this kind of tests. I'd design those test to be performed by hand or a different test framework which is capable to automate tests for applications or systems. The BVT/BAT just proves that the build was good enough to create a useable application (can be executed user interface is accessible can load data from database ...). Firstly write test cases with Junit then we could leverage ant to automate the test.
498,A,"how to test row locked exception in junit Is there a way in junit to test rowlocked exception? Th provided information is not really enough to answers the question. The behaviour could be JDBC driver dependent not every RDBMS is suitable for unit testing etc. This question is a bit vague about what exactly is supposed to be tested. Testing that the DB throws the exception? Testing that the app responds properly to the exception? Something like this: @Test(expected=SQLException.class) public void testReadLockedRowException() { ... }  If you mock your database dependencies then you can have your mock object throw the locked row exception. As you'll have removed your direct dependency on the database the test should run faster and you'd be less prone to ""flickering tests"" due to database problems such as the database not being a available or something similar. Also this way you are only testing your code and nothing to do with the database - it is implementation agnostic. Should you choose to change database vendor in the future this (a) shouldn't matter to your code and (b) your test doesn't care what database it is using. Some example mocking frameworks to get you started: EasyMock Mockito jMock  With mock objects throw row locked exception from your mock DB. With a real DB: Open TWO DB sessions In session 1 lock interesting row In session 2 access locked row in a conflicting mode without unlocking it before in session 1.  Georgy is correct but you'll need more in the implementation: two clients to access the database. One to read and hang onto the row another to try and access it and generate the exception. I bet you do not need two threads for that. Two transactions should be enough. (Two connections for transaction-per-connection scenario)"
499,A,"How to get the 'diff' between arrays in Java I am attempting to write some test cases for some classes that involve testing the equality of two dimensional arrays of data. Here is my first stab at it: double[][] expected = { {0 104 0} {145.5 0 0} {83 0 0} }; double[][] actual = someObject.getArray(); Now I see that JUnit does not have an 'arrayEquals' for double arrays probably due to the floating point issue (you really want to use a delta vs equality). I notice that Junit-Addons has exactly the method I need to determine if they're equal: static void assertEquals(java.lang.String message double[] expected double[] actual double delta) Now that's all well and fine. What I want to be able to do is give a meaningful error message not just saying that the two are unequal but where they are unequal. Is there an easy way of doing this short of comparing dimensions then iterating over each corresponding element and testing for equality? It seems silly to have to do the same thing that's being done in the assertion just to get a meaningful error message. Do I get it right you're happy with the assertEquals(String double[] double[] double) method from JUnit-Addons except for the output? Assuming you have or can get the source of the ArrayAssert class just reimplement the method (copy&paste but check the license first) and enhance the outputs in the new custom assertEquals method to your needs. I think it won't take too long!  I would think that you are conflating unit testing with instrumentation/debugging. The purpose of the unit test is to determine if your object works as expected. If you get into the habit of including code that is (effectively) instrumentation in your test cases you increase the probability that you will write test cases that are designed to pass. Your object has a method that is to return a double[][]. Your test case is telling you that it is not working as expected. Time to debug your code. (I understand that this may be a controversial point of view.) As far as I understand the question I82Much wants to test equality of 2-dimensional arrays and a detailed error/fail message (like 'cell a12 expected value 104.0 but found value -104.0'). Yes. My (potentially pedantic possibly wise :) point is that if his object had a method getCell(row col) then identifying its failure for a given input would fall in the domain of testing. As it is he is looking for his unit tests to do double duty as debuggers. To clarify assume returned array (n x m) has multiple errors. Adding a ""feature"" to the test framework that identifies the first case of failure tells you the first instance. Should it list all the instances of failure? What happens when you change the test data? Do you get a different set of failed cells? The test fails and it tells you what you need to know: you have a bug. I guess I never thought that providing more data about a failure of a test is encroaching into the 'debugging' territory but maybe you're right.  So are you trying to compare double[]s or double[][]s? If you are comparing double[]s (1D arrays) then yeah how about using Junit-Addons? is its default error message not what you need? If you are able to write your tests to use Double[] instead then JUnit's assertArraysEquals() method which operates on Object[] will work. It will compare doubles for exact equality though -- assuming that's what you intend. If you're comparing double[][] then neither of these work. Even though double[][] is an Object[] you don't intend to compare individual double[]s since they just use reference equality. You can use commons-lang to do the comparison true though it won't help you generate a useful message about where the difference is. For that I think you need to roll your own method."
500,A,"JPA DAO integration test not throwing exception when duplicate object saved? I am in the process of unit testing a DAO built with Spring/JPA and Hibernate as the provider. Prior to running the test DBUnit inserted a User record with username ""poweruser"" -- username is the primary key in the users table. Here is the integration test method: @Test @ExpectedException(EntityExistsException.class) public void save_UserTestDataSaveUserWithPreExistingId_EntityExistsException() { User newUser = new UserImpl(""poweruser""); newUser.setEmail(""kuser@null.com""); newUser.setFirstName(""New""); newUser.setLastName(""User""); newUser.setPassword(""secret""); dao.persist(newUser); } I have verified that the record is in the database at the start of this method. Not sure if this is relevant but if I do a dao.flush() at the end of this method I get the following exception: javax.persistence.PersistenceException: org.hibernate.exception.ConstraintViolationException: Could not execute JDBC batch update I assume you're using the transactional integration tests? These tests rollback at the end of each test method so you are indeed missing the flush. I don't know if this is useful but if you were autogenerating the ID then a flush would happen at the persist() because a trip to the database is needed to get the generated ID. But if you're assigning your own ID's then the flush is delayed until the commit as you've seen. I am using the transactional integration tests. Great point I hadn't thought of the autogenerated key. However my entire app uses application assigned UUIDs as surrogate primary key. So flush() is my new friend....  Apparently unless flush is called explicitly or implicitly (via a query or a commit) then any potential exception will not be thrown. So I guess I am going to have to call flush() in all of my integration tests that modify data. Look at http://docs.jboss.org/hibernate/core/3.3/api/org/hibernate/Session.html#setFlushMode(org.hibernate.FlushMode) Its good stuff but it still doesn't talk about *when* a persistence exception is thrown...e.g. during persist() or during flush(). I just think its a little weird that I have to call flush() in all my integration tests...is this normal?"
501,A,"How do I set file.encoding for a junit test in ant? I'm not quite done with file.encoding and ant. How do I set the file.encoding for junit tests in ant? The junit ant task doesn't support the encoding attribute like the javac task does. I've tried running «ant -Dfile.encoding=UTF-8» and «ANT_OPTS=""-Dfile.encoding=UTF-8"" ant» without success. System.getProperty(""file.encoding"") within a test still returns MacRoman. JUnit supports a child element <jvmarg ...> which should do what you want. <junit fork=""yes""> <jvmarg value=""-Dfile.encoding=UTF-8""/> ... </junit> I assume you were using the fork=yes attribute since this starts a new JVM for the test run thus the parameters you send into ant at the command line ant -Dfoo=bar do not necessarily propagate to the JVM running the tests."
502,A,"Problem at JUnit test with generics In my utility method: public static <T> T getField(Object obj Class c String fieldName) { try { Field field = c.getDeclaredField(fieldName); field.setAccessible(true); return (T) field.get(obj); } catch (Exception e) { e.printStackTrace(); fail(); return null; } } The line return (T) field.get(obj); gives the warning ""Type safety: Unchecked cast from Object to T""; but I cannot perform instanceof check against type parameter T so what am I suppose to do here? As suggested above you can specify the expected type of the field and call the cast method. Also. you don't need to pass argument object's class. You can find out what it is by calling obj.getClass() This simplifies your code to public static <T> T getField(Object obj Class<T> fieldClass String fieldName) { try { Class<?> declaringClass = obj.getClass(); Field field = declaringClass.getDeclaredField(fieldName); field.setAccessible(true); return fieldClass.cast(field.get(obj)); } catch (Exception e) { throw new AssertionFailedError(); } }  Generics are there to provide type safety in places where you didn't previously have any in Java. So it used to be that if you had a list full of Strings you had to do:  String myString = (String)myList.get(0); but now you can retrieve it without casting it:  String myString = myList.get(0); //Compiler won't complain When you generify using the variable T you are saying T is a placeholder for a specific type which will be defined on the instance of the class at instantiation time. For instance: public class ArrayList<T> { public ArrayList<T> { .... } } allows you to instantiate the list with:  ArrayList<String> myList = new ArrayList<String>(); Now every function on ArrayList will return a String and the compiler knows this so it doesn't require a cast. Each of those functions was defined much like yours above:  public T get(int index); public void set(int index T object); at compile time they become:  public String get(int index); public void set(int index String object); In your case however you seem to be trying to use T as a wildcard which is different from a placeholder for a specific type. You might call this method three times for three different fields each of which has a different return type right? This means that when you instantiate this class you cannot pick a single type for T. In general look at your method signatures and ask yourself ""will a single type be substituted for T for each instance of this class""?  public static <T> T getField(Object obj Class c String fieldName) If the answer is ""no"" that means this is not a good fit for Generics. Since each call will return a different type you have to cast the results from the call. If you cast it inside this function you're losing any benefits Generics would provide and might as well save yourself the headaches. If I've misunderstood your design and T does refer to a single type then simply annotating the call with @SuppressWarnings(value=""unchecked"") will do the trick. But if I've understood correctly fixing this error will just lead you to a long road of confusion unless you grok what I've written above. Good luck!  You can easily solve this problem by adding an additional parameter to your method which will specify the type of the filed the method will then look as follows:  public static <T> T getField(Class<T> fieldType Object obj Class<?> c String fieldName) { try { Field field = c.getDeclaredField(fieldName); field.setAccessible(true); Object value = field.get(obj); return fieldType.cast(value); } catch (Exception e) { e.printStackTrace(); fail(); return null; } } And here's how you can use it: getField(String.class new G() G.class ""s"") where G is defined as:  public class G { String s = ""abc""; } A 2nd improvement is to eliminate the c parameter of getFiled(). c can be obtained inside the method by invoking obj.getClass(). The only caveat is that this will give you the dynamic type of the object so you mat want to loop over all of C's superclasses until you find the field you're looking for or until you arrive at Object (You will also need to use c.getFields() and look for the field in the resulting array). I think that these changes will make your method easier to use and less prone to errors so it's worth the effort.  The annotation @SuppressWarnings will stop the compiler reporting this warning. I don't think there's any way you can get away from the compiler warning when using reflection like this. Something like the following: Field field = c.getDeclaredField(fieldName); field.setAccessible(true); @SuppressWarnings(value=""unchecked"") T t = (T) field.get(obj); return t; You can specify `Class` (for non-generic `T`) as mentioned in other answers but there isn't really any point in test code. Generics on reflection are really just a helping hand - they aren't really correct (see `Object.getClass`) and that doesn't matter because when you are using reflection you are blowing away type safety anyway."
503,A,"Testing what's written to a Java OutputStream I am about to write junit tests for a XML parsing Java class that outputs directly to an OutputStream. For example xmlWriter.writeString(""foo""); would produce something like <aTag>foo</aTag> to be written to the outputstream held inside the XmlWriter instance. The question is how to test this behaviour. One solution would of course be to let the OutputStream be a FileOutputStream and then read the results by opening the written file but it isn't very elegant. If you can pass a Writer to XmlWriter I would pass it a StringWriter. You can query the StringWriter's contents using toString() on it. If you have to pass an OutputStream you can pass a ByteArrayOutputStream and you can also call toString() on it to get its contents as a String. Then you can code something like: public void testSomething() { Writer sw = new StringWriter(); XmlWriter xw = new XmlWriter(sw); ... xw.writeString(""foo""); ... assertEquals(""...<aTag>foo</aTag>..."" sw.toString()); }  Use a ByteArrayOutputStream and then get the data out of that using toByteArray(). This won't test how it writes to the stream (one byte at a time or as a big buffer) but usually you shouldn't care about that anyway."
504,A,JUnit terminates child threads When i test the execution of a method that creates a child thread the JUnit test ends before the child thread and kills it. How do i force JUnit to wait for the child thread to complete its execution? Thanks  while (s.isRunning()) { try { Thread.sleep(SLEEP_TIME); } catch (InterruptedException e) { e.printStackTrace(); } } You could try to block the JUnit thread and have it wait for your thread to complete. The Thread.sleep() is needed so that your thread does not hog CPU. In this example s would be your thread you will need to have an isRunning() method so you can check if the thread is still running every SLEEP_TIME milliseconds. I know this isn't the greatest solution but if you only have 2 threads and you dont want JUnit killing your thread this works. The while loop with cause this JUnit thread to stay alive and wait for your other thread to finish.  Try using thread.join() on the created thread. This will wait for that thread to die. Edit: to avoid this try Thread.getThreadGroup().setDaemon(true); in the test or perhaps in the setUp() method. I haven't tested this though. A daemon thread group is automatically destroyed when its last thread is stopped or its last thread group is destroyed. I wonder if JUnit is calling System.exit() or something as soon as the test finishes however. Actually I'd would avoid this solution instead i was looking for a better way to run this kind of tests and force the main thread to wait for the end of its childs. @Marco: can you please elaborate? what is wrong with Chris' solution? I'm testing a method doSomething() that internally starts some threads that update some db tables. What I'd like to do is to run this method inside my JUnit test and then check if all the edits on the database have been saved successfully. It's important for the method doSomething() to not join the threads when they're created and so i can't adopt the join() solution. I'd recommend creating a thread group creating a new thread attaching it to that group and creating your new threads in that. After that joining the created ThreadGroup but ThreadGroup doesn't seem to have the join() method. Argh :) You could always spin on activeCount() > 0 inside the test on that ThreadGroup (but would that capture child thread groups?). Something a little strange here. How do you know when to test the database for created objects if you don't know when doSomething() has actually done something? Calling doSomething() and checking the results should be part of the same test since unit tests are meant to be independent. To do this you need some kind of synchronization - you have to wait for the work to be done before you can verify the results. It's not the test runner's job to wait but your test code's responsibility.  After reading the question and some comments it seems that what you need is a technique for unit testing asynchronous operations. doSomething() returns immediately but you want the test code to wait for its completion and then do some validations. The problem is that the test is not aware of the threads being spawned by the call so apparently it has no means of waiting for them. One can think of many sophisticated (and probably flawed) ways to solve this but in my opinion there is a design problem here. A unit test should simulate a client of some API and it should not assume anything about the implementation; It should only test functionality as reflected by the API and its documentation. Therefore I would avoid trying to detect and track the threads created by the asynch call. Instead I would improve the API of the tested class if needed. The class where the asynch call belongs to should provide some mechanism for detecting termination. I can think of 3 ways but there are probably more: 1) Allow registering a listener that gets notified once the operation is completed 2) Providing a synchronous version of the operation. The implementation can call the asynch version and then block until completion. If the class should not be exposing such a method its visibility can be reduced to package protected so that the test can access it. 3) Using the wait-notify pattern on some visible object. If the class provides no such mechanism then it is not really testable and worse it is probably not very reusable either.  The basic technique that @Eyal outlined is what ConcurrentUnit is intended for. The general usage is: Spawn some threads Have the main thread wait or sleep Perform assertions from within the worker threads (which via ConcurrentUnit are reported back to the main thread) Resume the main thread from one of the worker threads once all assertions are complete See the ConcurrentUnit page for more info.  Perhaps group your threads with an ExecutorService then use shutdown and awaitTermination method ? The condition is to use Runnable or Future not Threads themselves.
505,A,"Eclipse: How to configure a default unit test directory I have two source folders src and test. The Quick JUnit plugin allows me to ""Open Testing Pair"" quickly. If there is no testing pair it opens the default JUnit Test Wizard but the default directory is always src and and not test. Is there a way to configure a default unit test directory? The question is already answered sorry! How to default the source folder for new JUnit tests in Eclipse? MoreUnit works fine."
506,A,"Connection Pool ""Leak"" in development. Could it be due to JUnit test setup? I am trying to track down a database connection pool ""leak"" in development and I am wondering if it is resulting from how the unit tests are set up. Something is grabbing database connections from the Glassfish pool and not closing them when done. Eventually the pool max connections are used up and the application is unable to get any new db connections. Our JUnit tests get a connection from the pool in the setUp() method and then close that connection in the tearDown() method. Can we be sure that the tearDown() method will ALWAYS run? If an unhandled exception occurs could the tearDown() method be bypassed? Any other ideas on what we should look for? I should note that we are using Jakarta Cactus to run these unit tests on the Glassfish application server. Both the tests and the application are using db connections. I am not sure which one is to blame for the leaks. The unit tests stress the system much more than we can manually. We only see the problem when the unit tests are run. Are you sure that only the tests are using the connections and not the application ? Do you get connection leaks when the application is used without the automated tests ? Try commenting out everything in your test except the open and close code. At least you'll know that part is working properly and it might point you towards the application being the issue. One suggestion to prevent and report the database connection leaks: First find out the scope of each connection. Example for many web applications a connection is needed in the scope of a request. With the scopes defined all you need to do is to close the connection in a deterministic way by doing it at the end of the scope life cycle. One way to of asserting that a database connection is always closed in a web application is to create a servlet filter that will get the connection when a request comes in and close the connection when the response is sent. The connection can be passed from the filter to other objects by putting it in a ThreadLocal variable. Another example of scope is when a connection is needed per transaction. You may want to use the Execute Around Method pattern to get the connection before the scope begin and close it at the end in a deterministic way. If you implement any of these ideas you may even log which connections were not closed before you close it to help identify the leak. Good luck I hope this helps please let me know otherwise. Update: I just solved a database connection leak in legacy code by adding debugging parameters to the database connection pool implementation apache DBCP. Even if you don't want to use DBCP in production you could still set it up in test just to detect the exact line code that borrowed the unclosed connection. In my environment I used tomcat with the JNDI datasource config like so :  <Resource auth=""Container"" name=""jdbc/APP_NAME"" username=""user"" password=""password"" url=""jdbc:oracle:thin:@server.domain:1521:development"" type=""javax.sql.DataSource"" driverClassName=""oracle.jdbc.driver.OracleDriver"" maxIdle=""10"" maxWait=""5000"" maxActive=""10"" validationQuery=""select 1 from dual"" validationInterval=""30000"" testOnBorrow=""true"" testOnReturn=""false"" testWhileIdle=""true"" timeBetweenEvictionRunsMillis=""5000"" numTestsPerEvictionRun=""3"" minEvictableIdleTimeMillis=""30000"" <!-- These 3 settings saved me hours of headache --> logAbandoned=""true"" <!-- Will report the stacktrace of the faulty code --> removeAbandoned=""true"" <!-- Will remedy the connection starvation while leaky code is not fixed--> removeAbandonedTimeout=""60""<!-- Interval for fixing connection starvation while leaky code is not fixed--> /> See : Apache DBCP configuration  I have never seen that tearDown gets missed IF it is a proper tearDown (proper method signatur or annotation depending on the JUnit version) But: You might skip parts of the tearDown when an exception gets thrown inside the tearDown. I'd test your theory by running your TestSuite and watching the connection pool. Sounds fairly easy to me.  A connection is returned to the pool when A connection is returned to the pool when you close it pragmatically (finally block!) A full garbage collection occurs When the java process is killed (stop server end of unit cycle) In short it is highly unlikely your pooling problems are caused by running unit tests."
507,A,"How to generate JUnit sources using maven-gunit-plugin I have maven configured to run gunit (an ANTLR grammar unit testing tool) through the maven-gunit-plugin. gunit however has two different modes. The first mode causes gunit to act as an interpreter reading through the *.gunit (or *.testsuite) file interpreting it and displaying the results. It can be configured as such:  <plugin> <groupId>org.antlr</groupId> <artifactId>maven-gunit-plugin</artifactId> <version>3.1.3</version> <executions> <execution> <id>maven-gunit-plugin</id> <phase>test</phase> <goals> <goal>gunit</goal> </goals> </execution> </executions> </plugin> The second mode causes gunit to generate source code that can be run by JUnit. How can I instruct the maven-gunit-plugin to generate JUnit sources instead of acting as an interpreter? A few notes: I can change the test phase to ""generate-test-sources"" to cause the maven plugin to run at the correct time. I couldn't find any useful documentation on the maven-gunit-plugin I've seen people use exec-maven-plugin to run gunit with a specific command line option but I'm not looking to do that. EDIT / RESOLUTION: After reading the various responses I downloaded the ANTLR source code which includes the maven-gunit-plugin. The plugin does not support junit generation. It turns out that the codehaus snapshot of the gunit-maven-plugin and the exec plugin are currently the only options. I found a discussion through MNG-4039 that is illustrated with a maven-gunit-plugin gunit-maven-plugin sample. I'll let you read the whole article but according to the author you should end up with something like this: <dependencies> <dependency> <groupId>org.antlr</groupId> <artifactId>antlr-runtime</artifactId> <version>3.1.1</version> </dependency> <!-- Here is the 'extra' dep --> <dependency> <groupId>org.antlr</groupId> <artifactId>antlr</artifactId> <version>3.1.1</version> <!-- we try to use scope to hide it from transitivity --> <scope>test</scope> <!-- or perhaps 'provided' (see later discussion) or 'import' (maven >= 2.0.9) --> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>gunit-maven-plugin</artifactId> <version>1.0.0-SNAPSHOT</version> <executions> <execution> <goals> <goal>generate</goal> </goals> </execution> </executions> </plugin> </plugins> </build> I didn't test this configuration myself and can't thus confirm everything is working out of the box. I don't even know if the plugin has been released in a non SNAPSHOT version. The only thing I can confirm is that it seems indeed very hard to find ""real"" documentation about the maven-gunit-plugin. @Pascal +1 @Kaleb the plugins are indeed different. The codehaus mojo has been written specifically as a workaround to generate the JUnit test classes. If you are determined to avoid the exec plugin then this Mojo currently looks to be the best option (assuming it works). Though be warned that the SNAPSHOT suffix means the plugin is volatile and subject to change you may want to take a local copy and rename/reversion it to avoid picking up breaking changes. +1 with Rich's wise words: using a SNAPSHOT version can be fatal to your build stability and I prefer to use ""fixed"" version for the sake of build reproductability. gunit-maven-plugin from org.codehaus.mojo isn't the same as maven-gunit-plugin from org.antlr. I'll see however if there's a repository/snapshot that works as indicated above.  There is sad news here I found out so far there is no GUnit-functionality (be it JUnit Test-Generation or direct invocation of GUnit) for maven right now. I already mailed with Jim Idle concering the state of GUnit in the antlr3-maven-plugin and learned that there is a patch to the old version of the maven-plugin waiting in the queue. I think this workaround that is the only option. Both referenced posts are older than the original release of the maven-gunit-plugin on May 1 2009 (see http://www.antlr.org/pipermail/antlr-interest/2009-May/034276.html) so I'm still doubtful that I have a correct answer. Investigating more...."
508,A,"Junit output and OutOfMemoryError I'm running some JUnit tests on my applications. Every test has a for loop calling respective method 10000 times. The tested methods produce a lot of log. These logs are also automatically collected by JUnit as test output. This situation takes to OutOfMemoryError because the string buffer where JUnit keeps the output becomes too large. I dont' need these logs during tests so if there is a way to tell JUnit ""don't keep program output"" it would be enough. Any ideas? I would just increase the available memory.. Try adding -Xmx256m -Xmx256m to your VM.  What type of logging are you using? Is there some way you can override the default logging behavior to just disregard all log messages?  Some options: Change your logging so that it dumps to a file instead of standard output. Increase the maximum heap size with -Xmx <some number>M like -Xmx 256M.  I see that an answer has been accepted already but here's what I would have submitted if I had gotten it typed up and tested faster: If by ""logging"" you mean System.out.println() or System.err.println() and if you're sure that your test really doesn't need the logs then you can redirect stdout and stderr programmatically. // Save the original stdout and stderr PrintStream psOut = System.out; PrintStream psErr = System.err; PrintStream psDevNull = null; try { // Send stdout and stderr to /dev/null psDevNull = new PrintStream(new ByteArrayOutputStream()); System.setOut(psDevNull); System.setErr(psDevNull); // run tests in loop for (...) { } } finally { // Restore stdout and stderr System.setOut(psOut); System.setErr(psErr); if (psDevNull != null) { psDevNull.close(); psDevNull = null; } } This way your test output will be disabled but the other output from JUnit will not be as it would be if you used redirection on the command line like this: ant test &> /dev/null The command line redirection causes all of Ant/JUnit's output to be redirected not just what is coming from the class you are testing so that's probably not what you want. The programmatic redirection causes only the prints/writes to System.out and System.err in your program to be redirected and you will still get the output from Ant and JUnit.  The solution was overriding logging properties. Now I disabled logging and all seems to work (test is still running). If it works I'll configure a way for logging to a file. Thanks everybody (and congratulations to Jeff & friends for this site).  If it's any help setting outputtoformatters=""no"" solved all my memory problems (in Ant 1.7.1 it this prevents output generated by tests being sent to the test formatters)."
509,A,"JUnitReport HTML - Unit Test Results Order Does anyone know if it is possible to order the HTML page generated by the junitreport task in ant to order the test results by timestamp rather than alphabetically by Class name. The report does show the timestamp but I can't find anyway to sort the results by it. I would like the default to be sorting by timestamp. Many thanks Stef Thanks to your post above I found the solution which involved using a custom stylesheet (a modified version of the default xsl file) as you suggested. I changed line 656 from: <xsl:sort select=""@name""/> to <xsl:sort select=""@timestamp""/> and it did exactly what I wanted. Thanks again for your help. Cheers Stef  JUnitReport uses XSL stylesheets in order to transform the raw XML files into HTML files. You might change the stylesheets in order to generate custom HTML (and thus sort by timestamp). One your stylesheets work use the styledir attribute (see http://ant.apache.org/manual/Tasks/junitreport.html) to tell ant to use your stylesheets rather than the default ones. That's helpful I'll look into that Thanks"
510,A,How to output additional debugging information using Eclipse and Android JUnit I am developing a simple android application using eclipse. I wrote a JUnit TestCase for a class I wrote. One particular test compares two 2d arrays so see if they are equal. If they are not equal the test fails. When the test fails I would like to print out the contents of the offending array to see what went wrong. I have tried System.out.print and it doesn't show up in the console or JUnit results. What am I doing wrong? Is it even possible? You can use Log to write to the LogCat window in Eclipse. ETA: Screenshot of the LogCat window in Eclipse with some logging code highlighted. Thanks I couldn't get it to print in eclipse for some reason but in the shell I ran adb logcat Class:V *:S and it work works perfect.
511,A,"How to handle large strings in unit tests? I've got a question about testing methods working on strings. Everytime I write a new test on a method that has a string as a parameter. Now some issues come up: How to include a test string with \n \r \t umlauts etc? How to set the encoding? Should I use external files that are opened by a FileInputStream? (too much overhead imho) So... what are your approaches to solve this? If you have a lot of them keep test strings in separate class with string consts Try not to keep the files on disk unless you must. I agree with your claim - this brings too much overhead (not to mention what happens if you start getting I/O errors) Make sure you test strings with different line breaks (\n \r\n \r\n\r) for different OSs So you propose to use string literals in unit tests? Even if they are loooong (e.g. 200 lines?) It's a matter of convenience. If you believe they are long enough to put in an external file and start dealing with I/O in tests then do that. Otherwise yes keep them in the tests but organize them nicely.  How to include a test string with \n \r \t umlauts etc? Um... just type it the way you want? You can use \n \r and \t umlauts stc. in Java String literals; if you're worried about the encoding of the source code file you can use Unicode escape sequences and you can produce them using the native2ascii tool that comes with the JDK. How to set the encoding? Once you have a Java String it's too late to worry about encodings - they use UTF-16 and any encoding problems occur when translating between Strings and byte arrays (unlike C Java keeps these concepts clearly separate) Edit: If your Strings are too big to be comfortably used in source code or you're really worried about the treatment of line breaks and white space then keeping each String in a separate file is probably best; in that case the encoding must be specified when reading the file (In the constructor of InputStreamReader)  If you repeatedly use characters that are difficult to express in literal Strings (such as "" \ characters not in [ -~]) then you might want to consider doing a quick find-and-replace on the string before using it. For instance if you use \ a lot then you might wirte a function to exchange \ and /. You might use a multi-character sequence to represent accented characters. However there is obvious danger in ending up with a solution out of proportion to the problem. Sometimes \u#### is just easier. If you are going for non-Java files I suggest opening them as resources (Class.getResourceAsStream/getResource) rather than as loose files.  You could use a scripting language to code your tests. JRuby and Groovy support HERE documents that make it easier to define a big string that spans multiple lines # In JRuby mystring = <<EOS This is a long string that spans multiple lines. EOS # In Groovy def mystring = """"""This is a long string that spans multiple lines."""""" This will also make your test code more easy to write as both languages have a lot of shortcuts that help write simpler code (but some might say less robust which does not matter as much if it is only unit testing code). I don't get your idea. Why should I _script_ a test for a java class? Not script but to write the actual test in Groovy/JRuby. Not sure about JRuby but you have java interoperability in Groovy and thus can test you classes using Groovy.  For LARGE strings I would use files. The performance is plenty fast enough for unit tests. For that small trade-off you: Don't have to worry about escaping characters Can diff the content in source control Can validate the documents independently (ie xml/html)"
512,A,getting the context object after setting it using @ContextConfiguration How do I get ApplicationContext object so that i can use it in my code after setting context using@ContextConfiguration. My test class extends AbstractTransactionalJUnit4SpringContextTests. AbstractTransactionalJUnit4SpringContextTests extends AbstractJUnit4SpringContextTests which has a protected applicationContext field. Your test class can just use that.  I generaly just inject a copy of the Application Context into the unit test:  @Autowired ApplicationContext context;
513,A,"The XML code runs properly but junit fails with NoClassDefFound I am upgrading my environment from eclipse 3.3.1 and java 1.4 to eclipse 3.4.1 and java 1.5. My unit tests are in jUnit 3. eclipse java version 1.5.0__17 stand alone env version 1.5.0__12 or 1.5.0-17 both work. I have a method on a class that writes an XML file to disk. It calls TransformerFactory tf = [javax.xml.transform.]TransformerFactory.newInstance(); When I run the code outside of eclipse it runs fine. When I run the code in jUnit in eclipse I get the stack trace below. The missing class is in the rt.jar of java 1.4 and not in java 5 but shouldn't that be abstracted from me? How can I make the test pass? I get the same error when I run the code in eclipse from an application.  java.lang.NoClassDefFoundError: org/apache/xalan/processor/TransformerFactoryImpl at weblogic.xml.jaxp.RegistryTransformerFactory.(RegistryTransformerFactory.java:62) at weblogic.xml.jaxp.RegistrySAXTransformerFactory.(RegistrySAXTransformerFactory.java:12) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:494) at java.lang.Class.newInstance0(Class.java:350) at java.lang.Class.newInstance(Class.java:303) at javax.xml.transform.FactoryFinder.newInstance(FactoryFinder.java:100) at javax.xml.transform.FactoryFinder.findJarServiceProvider(FactoryFinder.java:278) at javax.xml.transform.FactoryFinder.find(FactoryFinder.java:185) at javax.xml.transform.TransformerFactory.newInstance(TransformerFactory.java:103) at com.bellsouth.snt.cnmp.sso.netcool.NetcoolAccessThread.writeXmlFile(NetcoolAccessThread.java:278) at com.bellsouth.snt.cnmp.sso.netcool.NetcoolAccessThreadTest.testWriteXmlFile(NetcoolAccessThreadTest.java:83) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:585) at junit.framework.TestCase.runTest(TestCase.java:164) at junit.framework.TestCase.runBare(TestCase.java:130) at junit.framework.TestResult$1.protect(TestResult.java:106) at junit.framework.TestResult.runProtected(TestResult.java:124) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:120) at junit.framework.TestSuite.runTest(TestSuite.java:230) at junit.framework.TestSuite.run(TestSuite.java:225) at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) update I did some more research in the bowels of the stack trace. The working versions (outside eclipse) are returning an instance of com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl which is the fallback impl class name in javax.xml.transform.TransformerFactory.newInstance()  public static TransformerFactory newInstance() throws TransformerFactoryConfigurationError { try { return (TransformerFactory) FactoryFinder.find( /* The default property name according to the JAXP spec */ ""javax.xml.transform.TransformerFactory"" /* The fallback implementation class name XSLTC */ ""com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl""); } catch (FactoryFinder.ConfigurationError e) { throw new TransformerFactoryConfigurationError( e.getException() e.getMessage()); } } You could check if you are running with the same Java version when running in Eclipse as when running outside of it (in Eclipse: Run As -> Run Configuration... -> JRE tab). same version (some of the time) the question is updated above.  Add the Xalan JAR to the classpath. Also see here if you are using WebLogic. You'll have to put the JAR in shared/lib. Why does it run outside of eclipse then? I haven't added the xalan jar to my java path. To try to narrow down the probable causes just add the Xalan JAR. Then hopefully you can get it to work and you'll know better what to look for when comparing the different runtime environments.  I added the following line to setup of the unittest  System.setProperty(""javax.xml.transform.TransformerFactory"" ""com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl""); I figured out what to do with a bit of RTFM. http://java.sun.com/j2se/1.5.0/docs/api/javax/xml/transform/TransformerFactory.html#newInstance()"
514,A,"verifyTrue(false) is not failing the testcase As per the documents “assert” will fail the test and abort the current running test case whereas a “verify” will fail the test and continue to run the test case. But verifyTrue(false) is not failing the case(rather continue with the next step and mark the case as passed). Assuming that's a Selenium call then according to this ""[verify methods] don't stop the test when they fail. Instead verification errors are all thrown at once during tearDown."" How can I get the fail result during using verifyTrue()? Assume my test is failed: as test is continuing and it shows pass I am not getting the fail result after execution"
515,A,"Testing GUI with JUnit Well we all know test-driven development. I'd need to write a GUI-based library but to be honest I always neglected the testing. Did JUnit for university and such but we never got any deeper than the usual ""Implement a list and test it."" So since I don't want to write a thousand applications for the features I'd like to know what's the ""professional"" approach to GUI-based library testing with Scala and JUnit? Thanks for listening. Read the book Test Driven a practical TDD and acceptance TDD for java developpers. It covers the basics on how to unit-test Swing Applications and also some TDD techniques. I haven't finished the book so i don't know if it covers Scala but i highly recommend this book (already)! Some personal note : if you want action skip chapter one but return back to it. It covers the ""How to start using TDD in general"" Test Driven book link  Separate your Presentation layer from everything else. Keep the Presentation layer as thin as possible so that testing can in theory take place within a middle-man of a given pattern; MVC MVVM etc... The moment you begin to couple the Presentation layer with underlying logic your testing will become a nightmare to maintain as well as execute. At the end of thoroughly testing your Models/ViewModels/Controllers etc...testing the Presentation layer can often times lose its highly regarded value. Is it still valuable? Yes...but the return has diminished considerably. These concepts apply to many frameworks/languages. Once you grasp this understanding the technological benefits of a given framework/language will surface naturally. Do not rely on the framework/language to answer this though. A framework/language can definitely lend itself to provide separation of concerns in a much more friendly manner however the separation of concerns is and has always been at the forefront of any type of testing; GUI included."
516,A,"Is there a customizable class loading order using ant junit task? in our web-app project we include some jar files. For patching some issues of one of the classes in a jar file we changed the implemention of this class in a patches source folder. Since there is a defined class loading order in tomcat (WEB-INF/classes before WEB-INF/lib) the patched version of the class is loaded by tomcat not the original one in the jar file. So as soon as we deploy our application everything works as expected. Now we want to run junit tests from ant against this patched class. So we configure the class path to hold both the original jar and the patched class file. But there seems to be no way to tell the ant's junit task to first load the patched class not the unpatched version from the jar file. Is there a way to get around the problem? Is there a way to determine the order in which the classes are loaded by ant's junit task? Is there any other way to test our patched class from ant? I think an ant classpath works just like the standard Java classpath. The classpath is searched in the order that paths are declared and a class is loaded from the first path where it is found. Your classpath element for your junit task should be something like: <classpath> <pathelement location=""${patched.class.folder}""/> <pathelement location=""${original.class.jar}""/> </classpath>"
517,A,Skipping tests in some modules in Maven I would like my Maven builds to run most unit tests. But there are unit tests in one project which are slower and I'd like to generally exclude them; and occasionally turn them on. How do I do this? I know about -Dmaven.test.skip=true but that turns off all unit tests. I also know about skipping integration tests described here. But I do not have integration tests just unit tests and I don't have any explicit calls to the maven-surefire-plugin. (I am using Maven 2 with the Eclipse-Maven plugin). I think this is easier and also has the benefit of working for non-surefire tests (in my case FlexUnitTests) <profile> <id>noTest</id> <properties> <maven.test.skip>true</maven.test.skip> </properties> </profile> This won't skip testing here with or without -DnoTest=true Thats cause you should use -PnoTest instead of -DnoTest=true  What about skipping tests only in this module ? In the pom.xml of this module: <project> [...] <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.4.2</version> <configuration> <skipTests>true</skipTests> </configuration> </plugin> </plugins> </build> [...] </project> Eventually you can create a profile that will disable the tests (still the pom.xml of the module) : <project> [...] <profiles> <profile> <id>noTest</id> <activation> <property> <name>noTest</name> <value>true</value> </property> </activation> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.4.2</version> <configuration> <skipTests>true</skipTests> </configuration> </plugin> </plugins> </build> </profile> </profiles> [...] </project> With the latter solution if you run mvn clean package it will run all tests. If you run mvn clean package -DnoTest=true it will not run the tests for this module. Thanks that worked. The first code snippet skips the test; I may later use your further suggestion to define another profile. My confusion was in the fact that my pom was invoking surefire implicitly. There was no mention of the surefire plugin in my pom.xml. Nonetheless the code to configure the surefire plugin correctly did so.
518,A,"How to make junit testing to stop after first failing test Is there a way to make running junit test to stop after a test fails? I don't know the answer but I've been thinking about it for a while. The thing that keeps coming to my mind is: Why do you want that? Exactly: why not see all failing tests? I know that in Ant the junit task has options ""haltonerror"" and ""haltonfailure"" that controls this behavior.  Yes ... this ability (or the lack of it) is built into the various TestRunners (Console AWT Swing Ant Maven or the one built into Eclipse etc). You'll have to look for this control in the documentation for the specific platform you're using."
519,A,Is there an AJAX web-based Junit TestRunner? What I'm looking for is a TestRunner implementation for running a suite of Java JUnit tests within a browser. The intention is that non-developers can execute the tests by visiting a browser page. Test results should be dynamically updated to the browser page with something like AJAX after each test and/or suite has completed. I've seen a few attempts at this but the ones I'm aware of execute all of the tests before showing any results to the browser. Take mistletoe for example. Ideally the implementation would be inspired by the JUnit testrunners found in IDEs such as IntelliJ IDEA or Eclipse. Is anyone aware of such a TestRunner? I was looking for the same thing and I've just come across ktrwjr which I'm just about to use to integrate into my web app to run my JUnit integration tests. Edit: I've just realised this is for testing apps developed on GAE/j. So close...  Answers to this Question Might Help: Looking for a better JavaScript unit test tool There is some good stuff here. However for now I'm not interested in testing JavaScript code. I just want to run regular Java JUnit tests in a browser.
520,A,"Embedded jetty ServletTester serving single static file I'm unit testing with jetty and I want to serve not only my servlet under test but a static page as well. The static page is needed by my application. I'm initializing jetty like this tester = new ServletTester(); tester.setContextPath(""/context""); tester.addServlet(MyServlet.class ""/servlet/*""); tester.start(); What I need now is something like tester.addStaticPage(""local/path/in/my/workspace"" ""/as/remote/file""); Is this possible with jetty? I don't think you can do this with ServletTester. ServletTester creates a single Context for the servlet. You need to set up embedded jetty with at least two contexts: one for the servlet and one for the static content. If there was a full WebAppContext you'd be set but there isn't. You could make a copy of ServletTester and add hair or you can just read up on the API and configure the necessary contexts. Here's a code fragment to show you the basic idea you will not be able to compile this as-is. You will need to create a suitable context for the static content.  server = new Server(); int port = Integer.parseInt(portNumber); if (connector == null) { connector = createConnector(port); } server.addConnector(connector); for (Webapp webapp : webapps) { File sourceDirFile = new File(webapp.getWebappSourceDirectory()); WebAppContext wac = new WebAppContext(sourceDirFile.getCanonicalPath() webapp.getContextPath()); WebAppClassLoader loader = new WebAppClassLoader(wac); if (webapp.getLibDirectory() != null) { Resource r = Resource.newResource(webapp.getLibDirectory()); loader.addJars(r); } if (webapp.getClasspathEntries() != null) { for (String dir : webapp.getClasspathEntries()) { loader.addClassPath(dir); } } wac.setClassLoader(loader); server.addHandler(wac); } server.start(); Thanks for your response. I thought that ServletTester won't give me as much choice. I don't know if I want to setup a full jetty server for my unit test but I'll take a look at it.  Set the resource base to the directory containing your static content and add the jetty ""default servlet"" to serve that content. I have added the appropriate code to your example below. tester = new ServletTester(); tester.setContextPath(""/context""); tester.setResourceBase(""/path/to/your/content""); tester.addServlet(MyServlet.class ""/servlet/*""); tester.addServlet(org.eclipse.jetty.servlet.DefaultServlet.class ""/*""); tester.start();"
521,A,"How to test a Grails Service that utilizes a criteria query (with spock)? I am trying to test a simple service method. That method mainly just returns the results of a criteria query for which I want to test if it returns the one result or not (depending on what is queried for). The problem is that I am unaware of how to right the corresponding test correctly. I am trying to accomplish it via spock but doing the same with any other way of testing also fails. Can one tell me how to amend the test in order to make it work for the task at hand? (BTW I'd like to keep it a unit test if possible.) The EventService Method public HashSet<Event> listEventsForDate(Date date int offset int max) { date.clearTime() def c = Event.createCriteria() def results = c { and { le(""startDate"" date+1) // starts tonight at midnight or prior? ge(""endDate"" date) // ends today or later? } maxResults(max) order(""startDate"" ""desc"") } return results } The Spock Specification package myapp import grails.plugin.spock.* import spock.lang.* class EventServiceSpec extends Specification { def event def eventService = new EventService() def setup() { event = new Event() event.publisher = Mock(User) event.title = 'et' event.urlTitle = 'ut' event.details = 'details' event.location = 'location' event.startDate = new Date(20101120 9 0) event.endDate = new Date(2011 3 718 0) } def ""list the Events of a specific date""() { given: ""An event ranging over multiple days"" when: ""I look up a date for its respective events"" def results = eventService.listEventsForDate(searchDate 0 100) then: ""The event is found or not - depending on the requested date"" numberOfResults == results.size() where: searchDate | numberOfResults new Date(20101019) | 0 // one day before startDate new Date(20101020) | 1 // at startDate new Date(20101021) | 1 // one day after startDate new Date(2011 1 1) | 1 // someday during the event range new Date(2011 3 6) | 1 // one day before endDate new Date(2011 3 7) | 1 // at endDate new Date(2011 3 8) | 0 // one day after endDate } } The Error groovy.lang.MissingMethodException: No signature of method: static myapp.Event.createCriteria() is applicable for argument types: () values: [] at myapp.EventService.listEventsForDate(EventService.groovy:47) at myapp.EventServiceSpec.list the Events of a specific date(EventServiceSpec.groovy:29) You should not use unit tests to test persistence - you're just testing the mocking framework. Instead move the criteria query to an appropriately named method in the domain class and test it against a database with an integration test: class Event { ... static Set<Event> findAllEventsByDay(Date date int offset int max) { ... } } class EventService { Set<Event> listEventsForDate(Date date int offset int max) { ... return Event.findAllEventsByDay(date offset max) } } If there's still value in having the service method as a wrapper (e.g. if it implements some business logic above and beyond the database query) it will now be easy to unit test since it's trivial to mock out the static domain class method call: def events = [new Event(...) new Event(...) ...] Event.metaClass.static.findAllEventsByDay = { Date d int offset int max -> events } And that's appropriate since you're testing how the service uses the data it receives and assuming that the retrieval is covered in the integration tests.  Criteria queries are not supported in unit tests. From the mockDomain documentation: [T]he plugin does not support the mocking of criteria or HQL queries. If you use either of those simply mock the corresponding methods manually (for example with mockFor() ) or use an integration test with real data. You'll have to make your test an integration test. You'll see that the exception goes away if you move the test from the test/unit folder to the test/integration folder. There is some work being done on criteria support in unit tests and if you're feeling adventurous you can try it out today. See this mailing list discussion of the DatastoreUnitTestMixin."
522,A,Best way to create / drop a database before / after integration testing on a Maven/Junit/DBUnit project? I've seen some people use the maven-sql-plugin to do this. But it seems like a task that is better suited for DBUnit....perhaps at the beginning of an entire test suite. What's the best practice here? It took some fiddling around but I got it to drop create and create the schema for H2 and MySQL. Still need to finish it for Oracle and SQL*Server 2008. I tucked the exact DROP and CREATE commands into properties and in some cases (such as H2) needed to skip the create database altogether. Here is what it looks like:  <plugin> <!-- Used to automatically drop (if any) and create a database prior to running integration test cases. --> <groupId>org.codehaus.mojo</groupId> <artifactId>sql-maven-plugin</artifactId> <dependencies> <dependency> <!-- Adds the correct JDBC driver as a dependency of this plugin --> <groupId>${database.groupId}</groupId> <artifactId>${database.artifactId}</artifactId> <version>${database.version}</version> </dependency> </dependencies> <configuration> <!-- common configuration shared by all executions --> <driver>${database.class}</driver> <username>${database.username}</username> <password>${database.password}</password> <url>${database.url}</url> </configuration> <executions> <execution> <!-- Start by dropping the database (we'll leave it intact when finished) --> <id>drop-db</id> <phase>pre-integration-test</phase> <goals> <goal>execute</goal> </goals> <configuration> <!-- Can't use regular URL in case database doesn't exist --> <url>${database.url.alternate}</url> <skip>${database.sqlDrop.skip}</skip> <autocommit>true</autocommit> <sqlCommand>${database.sqlDrop};</sqlCommand> <onError>continue</onError> </configuration> </execution> <execution> <!-- then create a new database --> <id>create-db</id> <phase>pre-integration-test</phase> <goals> <goal>execute</goal> </goals> <configuration> <!-- Can't use regular URL in case database doesn't exist --> <url>${database.url.alternate}</url> <skip>${database.sqlCreate.skip}</skip> <autocommit>true</autocommit> <sqlCommand>${database.sqlCreate};</sqlCommand> <onError>continue</onError> </configuration> </execution> <execution> <!-- and finally run the schema creation script we just made with the hibernate3-maven-plugin --> <id>create-schema</id> <phase>pre-integration-test</phase> <goals> <goal>execute</goal> </goals> <configuration> <skip>${database.sqlSchema.skip}</skip> <autocommit>true</autocommit> <srcFiles> <srcFile>target/hibernate3/sql/create-${database.vendor}-schema.sql</srcFile> </srcFiles> <onError>continue</onError> </configuration> </execution> </executions> </plugin>  I use the Maven SQL Plugin You're much better off using it and making sure that you create and populate before your tests and then drop after your tests. You'll also want to use create or replace or drop if exists in your creation script (assuming your database supports it) in the event that a test fails and leaves the database in some inconsistent state.
523,A,"Any disadvantages to using spring to separate tests and data? I've been struggling coming up with a good solution to separate my testing data from unit tests (hard coded values). Until it dawned on me that I could create beans with spring and use those beans to hold my data. Are there any draw backs to coding my unit tests this way? Albeit they run a bit slower seeing as how spring has to configure all the beans and what not. For unit tests you can always wire in your beans by hand as in create and inject them yourself then leave the Spring wiring for integration tests and production/development.  Fine practice - one of the prime motivators for dependency injection in my view (easier to unit test) It will be slower because of the need to bring up the Spring application context so you might reserve this technique for ""integration tests""."
524,A,JUnit assertion extensions I'm often creating custom assertion methods for my JUnit tests. eg: public void assertArrays(String[] actual String[] expected) I was wondering if there were any decent third party libraries that can provide a wider range of assertions than what comes by default in JUnit. Am using JUnit 4. Which more assertions do you need? Well any reusable asserts would be helpful. The Google example below has some pretty cool ones. Such as assertContainsRegex assertContentsAnyOrder(Iterable) checkEqualsAndHashCodeMethods etc. There is standard method for this purposes in JUnit4: assertArrayEquals Thanks Andrey I had forgotten about that one.  The one that has been around for ever is JUnit Addons.  I do not know of any such library. However you might not really need this. JUnit contains assertEquals for most value types of the JDK. For your own classes you can simply override the equals() method and use assertEquals(ObjectObject). This is what I usually do for my own classes. Works well and a proper equals() method is useful anyway.  Google names their Asserts classes MoreAsserts. Here's one for one particular project and you can search for more as most projects have their own and they are frequently open source'd. Edit: Android has a pretty great one too not sure if that source is available though. Yup that's the kind of thing I was after. Thanks. If anyone else knows of any others available please post.
525,A,Is there a existing way to generate surefire reports with simple-build-tool? I'm working on a project using Scala running Selenium tests as part of a continuous integration process. Hudson displays very helpful information based on the results in the surefire reports but I would much prefer using sbt to Maven. Is there an existing way to get sbt to generate surefire reports from sbt tests? Development of the TestListener for Surefire XML reports started at http://github.com/bryanjswift/surefire-sbt There are three approaches to achieve this. I think the first would be a great contribution to make to SBT! Write a TestsListener for SBT to generate Surefire XML reports There is currently no unified reporting as per the Maven surefire plugin. It would be possible to add this to SBT with a custom TestsListener. Pass arguments to the test framework to trigger XML generation I don't think Specs or ScalaTest support the Surefire XML format directly. It was recently requested for ScalaTest. Use a JUnit adapter for your tests and use the JUnitRunner ScalaTest and Specs provide traits you can mix in to your tests so they are compatible with JUnit. JUnit support in SBT is being re-integrated right now but I don't know if it will support this. Specs itself uses the Maven + Surefire for it's Hudson build.
526,A,"Dependency Injection with Spring/Junit/JPA I'm trying to create JUnit tests for my JPA DAO classes using Spring 2.5.6 and JUnit 4.8.1. My test case looks like this:  @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""classpath:config/jpaDaoTestsConfig.xml""} ) public class MenuItem_Junit4_JPATest extends BaseJPATestCase { private ApplicationContext context; private InputStream dataInputStream; private IDataSet dataSet; @Resource private IMenuItemDao menuItemDao; @Test public void testFindAll() throws Exception { assertEquals(272 menuItemDao.findAll().size()); } ... Other test methods ommitted for brevity ... } I have the following in my jpaDaoTestsConfig.xml:  <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:p=""http://www.springframework.org/schema/p"" xmlns:tx=""http://www.springframework.org/schema/tx"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd""> <!-- uses the persistence unit defined in the META-INF/persistence.xml JPA configuration file --> <bean id=""entityManagerFactory"" class=""org.springframework.orm.jpa.LocalEntityManagerFactoryBean""> <property name=""persistenceUnitName"" value=""CONOPS_PU"" /> </bean> <bean id=""groupDao"" class=""mil.navy.ndms.conops.common.dao.impl.jpa.GroupDao"" lazy-init=""true"" /> <bean id=""permissionDao"" class=""mil.navy.ndms.conops.common.dao.impl.jpa.PermissionDao"" lazy-init=""true"" /> <bean id=""applicationUserDao"" class=""mil.navy.ndms.conops.common.dao.impl.jpa.ApplicationUserDao"" lazy-init=""true"" /> <bean id=""conopsUserDao"" class=""mil.navy.ndms.conops.common.dao.impl.jpa.ConopsUserDao"" lazy-init=""true"" /> <bean id=""menuItemDao"" class=""mil.navy.ndms.conops.common.dao.impl.jpa.MenuItemDao"" lazy-init=""true"" /> <!-- enables interpretation of the @Required annotation to ensure that dependency injection actually occures --> <bean class=""org.springframework.beans.factory.annotation.RequiredAnnotationBeanPostProcessor""/> <!-- enables interpretation of the @PersistenceUnit/@PersistenceContext annotations providing convenient access to EntityManagerFactory/EntityManager --> <bean class=""org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor""/> <!-- transaction manager for use with a single JPA EntityManagerFactory for transactional data access to a single datasource --> <bean id=""jpaTransactionManager"" class=""org.springframework.orm.jpa.JpaTransactionManager""> <property name=""entityManagerFactory"" ref=""entityManagerFactory""/> </bean> <!-- enables interpretation of the @Transactional annotation for declerative transaction managment using the specified JpaTransactionManager --> <tx:annotation-driven transaction-manager=""jpaTransactionManager"" proxy-target-class=""false""/> </beans> Now when I try to run this I get the following: SEVERE: Caught exception while allowing TestExecutionListener [org.springframework.test.context.support.DependencyInjectionTestExecutionListener@fa60fa6] to prepare test instance [null(mil.navy.ndms.conops.common.dao.impl.MenuItem_Junit4_JPATest)] org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'mil.navy.ndms.conops.common.dao.impl.MenuItem_Junit4_JPATest': Injection of resource fields failed; nested exception is java.lang.IllegalStateException: Specified field type [interface javax.persistence.EntityManagerFactory] is incompatible with resource type [javax.persistence.EntityManager] at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessAfterInstantiation(CommonAnnotationBeanPostProcessor.java:292) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:959) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:329) at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:110) at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75) at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:255) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:93) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.invokeTestMethod(SpringJUnit4ClassRunner.java:130) at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:61) at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:54) at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34) at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44) at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:52) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) Caused by: java.lang.IllegalStateException: Specified field type [interface javax.persistence.EntityManagerFactory] is incompatible with resource type [javax.persistence.EntityManager] at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.checkResourceType(InjectionMetadata.java:159) at org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor$PersistenceElement.(PersistenceAnnotationBeanPostProcessor.java:559) at org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor$1.doWith(PersistenceAnnotationBeanPostProcessor.java:359) at org.springframework.util.ReflectionUtils.doWithFields(ReflectionUtils.java:492) at org.springframework.util.ReflectionUtils.doWithFields(ReflectionUtils.java:469) at org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor.findPersistenceMetadata(PersistenceAnnotationBeanPostProcessor.java:351) at org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor.postProcessMergedBeanDefinition(PersistenceAnnotationBeanPostProcessor.java:296) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyMergedBeanDefinitionPostProcessors(AbstractAutowireCapableBeanFactory.java:745) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:448) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:409) at java.security.AccessController.doPrivileged(AccessController.java:219) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:380) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:264) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:221) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:261) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:185) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:168) at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.autowireResource(CommonAnnotationBeanPostProcessor.java:435) at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.getResource(CommonAnnotationBeanPostProcessor.java:409) at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor$ResourceElement.getResourceToInject(CommonAnnotationBeanPostProcessor.java:537) at org.springframework.beans.factory.annotation.InjectionMetadata$InjectedElement.inject(InjectionMetadata.java:180) at org.springframework.beans.factory.annotation.InjectionMetadata.injectFields(InjectionMetadata.java:105) at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessAfterInstantiation(CommonAnnotationBeanPostProcessor.java:289) ... 18 more It seems to be telling me that its attempting to store an EntityManager object into an EntityManagerFactory field but I don't understand how or why. My DAO classes accept both an EntityManager and EntityManagerFactory via the @PersistenceContext attribute and they work find if I load them up and run them without the @ContextConfiguration attribute (i.e. if I just use the XmlApplcationContext to load the DAO and the EntityManagerFactory directly in setUp ()). Any insights would be appreciated. These are the correct combinations of annotation + interface: @PersistenceContext private EntityManager entityManager; @PersistenceUnit private EntityManagerFactory entityManagerFactory; But when using spring's transaction and entity manager support you don't need the EntityManagerFactory at all. The reason why you don't need EntityManagerFactory is because the creation of the EntityManager is responsibility of the transaction manager. Here's what happens in short: the transaction manager is triggered before your methods the transaction manager gets the EntityManagerFactory (it is injected in it) creates a new EntityManager sets in in a ThreadLocal and starts a new transaction. then it delegates to the service method whenever @PersistenceContext is encountered a proxy is injected (in your Dao) which whenever accessed gets the current EntityManager which has been set in the ThreadLocal You still need both the EntityManager and Factory in the DAOs which is the only place they exist in my code. no you don't. For example my entire application has no access to the factory whatsoever. The EntityManager is injected by spring so no need to use the factory manually. Right. We're doing the same thing with the DAOs (injecting the EntityManager). I'm also injecting the Factory (although I'm not currently using it for anything). It turns out that the whole problem was a simple cut-and-paste error. I was using @PersistenceContext on the EntityManagerFactory rather than @PersistenceUnit. Once I changed that it worked... Thanks for the pointer... @Steve - accepted answer with 0 upvotes seems strange so I'd suggest your practice being the regular - accept + upvote IMO this answer is not complete since a clear spring config should be provided for @PersistenceContext private EntityManager entityManager; example. Why group the EntityManager and EntityManagerFactory in the same block when they should never appear in the same class together.  I had to do the below combination apart from adding spring-aspects jar to the project properties->Aspect Path and enabling spring aspects in sts. Ofcourse in my application context config file i defined the Entitymanagerfactory. @ContextConfiguration(locations = { ""/META-INF/spring/applicationContext-domain.xml"" }) public class ReaderTest extends AbstractJUnit4SpringContextTests { @PersistenceContext private EntityManager entityManager;"
527,A,"JUnit test with dynamic number of tests In our project I have several JUnit tests that e.g. take every file from a directory and run a test on it. If I implement a testEveryFileInDirectory method in the TestCase this shows up as only one test that may fail or succeed. But I am interested in the results on each individual file. How can I write a TestCase / TestSuite such that each file shows up as a separate test e.g. in the graphical testrunner of eclipse? (Coding an explicit test method for each file is not an option.) Compare also the question ParameterizedTest with a name in Eclipse Testrunner. JUnit 3 public class XTest extends TestCase { public File file; public XTest(File file) { super(file.toString()); this.file = file; } public void testX() { fail(""Failed: "" + file); } } public class XTestSuite extends TestSuite { public static Test suite() { TestSuite suite = new TestSuite(""XTestSuite""); File[] files = new File(""."").listFiles(); for (File file : files) { suite.addTest(new XTest(file)); } return suite; } } JUnit 4 import org.junit.Test; import org.junit.runner.RunWith; import org.junit.runners.Parameterized; import org.junit.runners.Parameterized.Parameters; @RunWith(Parameterized.class) public class TestY { @Parameters public static Collection<Object[]> getFiles() { Collection<Object[]> params = new ArrayList<Object[]>(); for (File f : new File(""."").listFiles()) { Object[] arr = new Object[] { f }; params.add(arr); } return params; } private File file; public TestY(File file) { this.file = file; } @Test public void testY() { fail(file.toString()); } }  If TestNG is an option you could use Parameters with DataProviders. Each individual file's test will have its result shown in the text-based report or Eclipse's TestNG plugin UI. The number of total tests run will count each of your files individually. This behavior differs from JUnit Theories in which all results are lumped under one ""theory"" entry and only count as 1 test. If you want separate result reporting in JUnit you can try Parameterized Tests. Test and inputs public class FileTest { @DataProvider(name=""files"") public File[][] getFiles(){ return new File[][] { { new File(""file1"") } { new File(""file2"") } }; // or scan a directory } @Test(dataProvider=""files"") public void testFile(File file){ //run tests on file } } Example output PASSED: testFile(file1) PASSED: testFile(file2) =============================================== Default test Tests run: 2 Failures: 0 Skips: 0 =============================================== Good catch I will correct my answer. Thanks. I don't know about theories but parameterized tests in JUnit are shown separately in eclipse not lumped together.  Should be possible in JUnit 3 by inheriting from TestSuite and overriding the tests() method to list the files and for each return an instance of a subclass of TestCase that takes the filename as constructor parameter and has a test method that tests the file given in the constructor. In JUnit 4 it might be even easier.  Take a look at Parameterized Tests in JUnit 4. Actually I did this a few days ago. I'll try to explain ... First build your test class normally as you where just testing with one input file. Decorate your class with: @RunWith(Parameterized.class) Build one constructor that takes the input that will change in every test call (in this case it may be the file itself) Then build a static method that will return a Collection of arrays. Each array in the collection will contain the input arguments for your class constructor e.g. the file. Decorate this method with: @Parameters Here's a sample class. @RunWith(Parameterized.class) public class ParameterizedTest { private File file; public ParameterizedTest(File file) { this.file = file; } @Test public void test1() throws Exception { } @Test public void test2() throws Exception { } @Parameters public static Collection<Object[]> data() { // load the files as you want Object[] fileArg1 = new Object[] { new File(""path1"") }; Object[] fileArg2 = new Object[] { new File(""path2"") }; Collection<Object[]> data = new ArrayList<Object[]>(); data.add(fileArg1); data.add(fileArg2); return data; } } Also check this example Thanks! The JUnit 4 Method is better than the JUnit 3 Method given in another answer since the JUnit 3 confuses the eclipse test runner and with JUnit 4 Method you can re-execute the tests etc. I am only wondering how I can have eclipse show a name for the test - it only shows [0] [1] etc. @hstoerr Looks like this will be in the next release of JUnit :-) https://github.com/KentBeck/junit/commit/3a5c9f2731462e36dd1c173ea8840d7b9b34b0ab How would you transform this for if you wanted each run [with a different data combination] to modify the name of thes test run? [I.e. Path1 file would be tested as: test1Path1 test2Path? the example cited n the answer is offline now..."
528,A,"Testing a method instead of testing a whole file in Netbeans w/ JUnit I'm using Netbeans 6.8 and the finest-grained way to run my JUnit tests from the IDE appears to be to right-click a class under Test Packages and click Test File In Eclipse it's possible to narrow the scope to testing an individual method in a given test harness. How do I test only one individual test out of a harness in Netbeans? In IntelliJ it's possible to right-click at the test-method name and select run perhaps there is a similar way to run it in Netbeans? It's possible to see methods and right click them in Netbeans' Navigator view but as far as I can tell so far there's no context menu item to test that method. Thanks though I'm still digging :) NetBeans now has just such a feature; see my answer below. Have been using this feature in Netbeans 7.4 and works great! https://blogs.oracle.com/netbeansphp/entry/run_debug_focused_test_method  In response to Roger Keays were ""Run Focused Test"" isn't working: The option ""Run Focused Test"" was grayed-out on my project too because there was no action named ""run.single.method"" in project.xml. Workaround to get it working: 1) create new ""run.single.method"" action in project.xml file (you could copy ""run.single"" action). project.xml fragment: <action name=""run.single.method""> <script>nbproject/ide-file-targets.xml</script> <target>run.single.method</target> <context> <property>run.class</property> <folder>test/src</folder> <pattern>\.java$</pattern> <format>java-name</format> <arity> <one-file-only/> </arity> </context> </action> 2) create new ""run.single.method"" target in ide-file-targets.xml (you could copy ""run-selected-file-in-src"") and make sure the test tag has a parameter ""methods"" using the property ""${method}"". ide-file-targets.xml fragment: <target name=""run.single.method"" depends=""compile""> <fail unless=""run.class"">Must set property 'run.class'</fail> <junit maxmemory=""256m"" fork=""true"" haltonerror=""true"" haltonfailure=""true"" printsummary=""on"" jvm=""${jdk.dir}/bin/java"" showoutput=""true""> <sysproperty key=""build.basedir"" value=""${basedir}""/> <!-- method attribute makes the selected method to be unit tested only --> <test name=""${run.class}"" methods=""${method}""/> <formatter type=""plain"" usefile=""false""/> <classpath refid=""CLASSPATH""/> </junit> </target> Now you can use the right mouse menu ""Run focused test"". The option ""Run focused test"" might also be temporary be grayed-out when the project is scanning Handy to know thanks.  Netbeans 7.1 ""Run Focused Test"" isn't working with my build so I'm using use a test group to mark the tests I want to run. @Test(groups={""DEBUG""}) You can configure Netbeans to run these tests by adding -Dgroups=DEBUG to your ""Debug Test File"" action in the project properties. Note: these are TestNG methods run with maven/surefire. Did you ever get this working Roger? All of a sudden my project will not run focused tests - I am not sure why - we have a few developers on the project (including writing JUnit tests). But the test I have done will not run. I'm using Netbeans 7.1.2 Yes I'm using a @Test(groups={DEBUG}) to run individual tests. I setup NetBeans to use -Dgroups=DEBUG and when I run ""Debug Test File"". It works well and I also add other system properties for debugging (e.g. browser=chrome). Note the groups attribute is for TestNG. JUnit has it's own grouping annotations. But yeh my Netbeans doesn't run focused tests out of the box - I thought it was because I use inner classes for my test cases (http://www.ninthavenue.com.au/how-to-use-inner-classes-for-your-unit-tests).  In NetBeans 7.1 open the unit test file right click within the specific unit test and select ""Run Focused Test.""  UPDATE: NetBeans supports this (e.g. see NetBeans 6.8). Just right click the passed or failed test and then click 'Run Again' or 'Debug'. Or right click in the editor and click Run/Debug focused test. Old: To my knowledge this is not possible. And maybe the NetBeans-guys had in mind that always all tests should pass for one unit. I know that tools should not limit the developers but If one method takes too long - maybe you should consider a separate integration test? Also take a look at this post. (BTW: in maven projects running tests of a unit is possible ...) Thanks for coming back and updating this :) no problem. as a netbeans user this bothered me too. is this working for you? it seems to be only available for maven projects!? It now works even w/o Maven. See my answer.  As best as I can tell NetBeans doesn't support this. NetBeans does all of it's execution through ant scripts. The most granular ant target they build is test-single. The test-single target uses a property named test.includes to determine which files to execute through JUnit. It's a file name matching pattern and will not be able to specify a single method."
529,A,"Testing against Java EE 6 API I write an addition to JAX-RS and included the Java EE 6 API as a Maven dependency. <dependency> <groupId>javax</groupId> <artifactId>javaee-api</artifactId> <version>6.0</version> <scope>provided</scope> </dependency> Then I have a little test case:  @Test public void testIsWriteable() { class SpecialViewable extends Viewable { public SpecialViewable() { super(""test""); } } FreeMarkerViewProcessor processor = new FreeMarkerViewProcessor(null); assertTrue(processor.isWriteable(SpecialViewable.class null null MediaType.WILDCARD_TYPE)); } But I get an error: java.lang.ClassFormatError: Absent Code attribute in method that is not native or abstract in class file javax/ws/rs/core/MediaType ... If I include Jersey as a JAX-RS implementation instead of the Java EE API everything is fine. Thanks to BalusC's hint I know what I had guessed: Java EE 6 is only an API without method bodies: From the java.net blog You can compile you code with this jar but of course you cannnot run your application with it since it contains only the Java EE 5 APIs and does not contain any method bodies. If you try to run you would get this exception: Exception in thread ""main"" java.lang.ClassFormatError: Absent Code attribute in method that is not native or abstract in class file javax/mail/Session In order to execute a Java EE 5 application you'll still need a Java EE 5 container like for example the GlassFish application server. I've tried to add Jersy with test scope but it didn't work. <dependency> <groupId>javax</groupId> <artifactId>javaee-api</artifactId> <version>6.0</version> <scope>provided</scope> </dependency> <dependency> <groupId>com.sun.jersey</groupId> <artifactId>jersey-server</artifactId> <version>${jersey-version}</version> <scope>test</scope> </dependency> How can I test software that depends only on the official Java EE API? Solution The provider (Jersey) needs to be placed before the API (javeee-api) in the pom.xml. <dependency> <groupId>com.sun.jersey</groupId> <artifactId>jersey-server</artifactId> <version>${jersey-version}</version> <scope>test</scope> </dependency> <dependency> <groupId>javax</groupId> <artifactId>javaee-api</artifactId> <version>6.0</version> <scope>provided</scope> </dependency> Here are some related problems with insightful answers: http://www.google.com/search?q=%22java.lang.ClassFormatError%3A+Absent+Code+attribute+in+method+that+is+not+native+or+abstract+in+class+file%22 I was really gnawing my nails with this problem before I ran into this question. Thanks! Not sure this will solve your problem but GlassFish Embedded provides a Java EE 6 implementation. Add this to your pom.xml: <project> ... <repositories> <repository> <id>glassfish-extras-repository</id> <url>http://download.java.net/maven/glassfish/org/glassfish/extras</url> </repository> </repositories> ... <dependencies> <dependency> <groupId>org.glassfish.extras</groupId> <artifactId>glassfish-embedded-all</artifactId> <version>3.0.1</version> <scope>test</scope> </dependency> <dependency> <groupId>javax</groupId> <artifactId>javaee-api</artifactId> <version>6.0</version> <scope>provided</scope> </dependency> ... </dependencies> ... </project> It's important to declare the glassfish-embedded-all artifact before the javaee-api. Maybe also try with the `jersey-server` artifact (but declare it **before** the `javaee-api`). I don't need a full application server like GlassFish but your hint to place the provider (GlassFish or in my case Jersy) before the API was the solution! Just a note that the repository URL should actually be http://download.java.net/maven/glassfish/ Is there an embedded glassfish for Java EE 5? See http://mvnrepository.com/artifact/org.glassfish.main.extras/glassfish-embedded-all/3.1.2.2 The problem with this is that they seem to have exploded a bunch of jars and packaged them back together inside `glassfish-embedded-all`. You better hope you're using the same slf4j version they packaged in there because they didn't give a way to exclude what they give you. There has got to be a better way.  An alternative that is JSR provider agnostic is <dependency> <groupId>javax.ws.rs</groupId> <artifactId>jsr311-api</artifactId> <scope>provided</scope> </dependency> <dependency> <groupId>javax</groupId> <artifactId>javaee-api</artifactId> <version>6.0</version> <scope>provided</scope> </dependency> This allows you to swap Jersey with a different provider. For Glassfish 3.1.2 it uses jersey-server 1.11 which uses jsr311 version 1.1 according to the jersey pom. The question is about unit test which is not necessarily run in a EE container.  As for me JBoss' implementation is smaller than the whole Glassfish so I'm using:  <dependency> <groupId>org.jboss.spec</groupId> <artifactId>jboss-javaee-6.0</artifactId> <version>${version.jboss-javaee-6.0}</version> <type>pom</type> </dependency> <scope>test</scope> should also do no harm."
530,A,"Run JUnit automatically when building Eclipse project I want to run my unit tests automatically when I save my Eclipse project. The project is built automatically whenever I save a file so I think this should be possible in some way. How do I do it? Is the only option really to get an ant script and change the project build to use the ant script with targets build and compile? Update I will try 2 different approaches now: Running an additional builder for my project that executes the ant target test (I have an ant script anyway) ct-eclipse recommended by Thorbjørn I would recommend Inifinitest for the described situation. Infinitest is nowadays a GPL v3 licensed product. Eclipse update site: http://infinitest.github.com  For sure it it unwise to run all tests because we can have for example 20.000 tests whereas our change could affect only let's say 50 of them among which are tests for the class we have changed and tests for classes that collaborate with our class. There is an unseful plugin called infinitetest http://improvingworks.com/products/infinitest/ which runs only some tests ( related to class we've changed ) just after we save changes. It also integrate quite nicely with editor ( using annotations ) and problem view - displaying not-passing tests like errors. After thinking about it I'd like to see a plugin execute the last tests I started manually. That would help a lot right now. Inifitiest looks nice maybe I try it. But payware is not getting priority... ;-) You could try http://www.junitmax.com/ for paid alternative. Agreed. Inifinitest is the best one I've found and one of the only ones in active development.  I believe you are looking for http://ct-eclipse.tigris.org/ I've experimented with the concept earlier and my personal conclusion was that in order for this to be useful you need a lot of tests which take time. Personally I save very frequently so this would happen frequently and I didn't find it to be an advantage. It might be different for you. Instead we bit the bullet and set up a ""build server"" which watches our CVS repository and builds projects as they change. If the compilation fails or the tests fail we are notified quickly so we can remedy it. It is as always a matter of taste what works for you. This is what I've found. Ah sure. Hudson is doing integration but I find it somewhat annoying to click around in Eclipse just to start the tests after changing. ;-) Maybe the plugin should be so clever to just run the testcases that test the class I changed. Running all tests on every change would be too annoying. Just a question: You do run your unit tests locally before checking in right? Do you have rule of thumb when you run them? After you think your edit could break something? Or only once before commiting? We do not have full test coverage (ah the joy of legacy code). I run the tests if I think it's necessary. I _know_ that the build server will run the tests and notify me if they are broken so I don't _have_ to. consider a save action.  Right click on your project > Properties > Builders > New and there add your ant ant builder. But in my opinion it is unwise to run the unit tests on each save.  See if Eclipse has a plugin for Infinitest. I'd also consider TestNG as an alternative to JUnit. It has a lot of features that might be helpful in partitioning your unit test classes into shorter and longer running groups. It does indeed have a plugin that works really well. Tests run quickly and show up as if they ware copiel errors right in the IDE"
531,A,"Ant + JUnit = ClassNotFoundExceptions when running tests? I'm trying to run some tests in Ant presently using JUnit and all of my tests are failing with the following stacktrace: java.lang.ClassNotFoundException: com.mypackage.MyTestCase It doesn't make too much sense to me. I'm first compiling my test cases using <javac> then directly running the <junit> task to run the tests. My buildfile looks like this: <target name=""compile.webapp.tests"" depends=""compile.webapp""> <javac srcdir=""${test.java.src.dir}"" destdir=""${test.java.bin.dir}""> <classpath> <filelist> <file name=""${red5.home}/red5.jar""/> <file name=""${red5.home}/boot.jar""/> <file name=""${bin.dir}/${ant.project.name}.jar""/> </filelist> <fileset dir=""${red5.lib.dir}"" includes=""**/*""/> <fileset dir=""${main.java.lib.dir}"" includes=""**/*""/> <fileset dir=""${test.java.lib.dir}"" includes=""**/*""/> </classpath> </javac> </target> <target name=""run.webapp.tests"" depends=""compile.webapp.tests""> <junit printsummary=""true""> <classpath> <filelist> <file name=""${red5.home}/red5.jar""/> <file name=""${red5.home}/boot.jar""/> <file name=""${bin.dir}/${ant.project.name}.jar""/> </filelist> <fileset dir=""${red5.lib.dir}"" includes=""**/*.jar""/> <fileset dir=""${main.java.lib.dir}"" includes=""**/*.jar""/> <fileset dir=""${test.java.lib.dir}"" includes=""**/*.jar""/> <fileset dir=""${test.java.bin.dir}"" includes=""**/*.class""/> </classpath> <formatter type=""xml""/> <batchtest todir=""${test.java.output.dir}""> <fileset dir=""${test.java.bin.dir}"" includes=""**/*TestCase*""/> </batchtest> </junit> <junitreport> <fileset dir=""${test.java.output.dir}"" includes=""**/*""/> <report todir=""${test.java.report.dir}""/> </junitreport> </target> This is really weird I can't seem to fix this. Is there something I'm doing wrong here? My project's directory layout looks somewhat like this: ${basedir}/src/test/java # this is ""test.java.src.dir"" ${basedir}/build/test/java # this is ""test.java.bin.dir"" ${basedir}/lib/main/java # this is ""main.java.lib.dir"" ${basedir}/lib/test/java # this is ""test.java.lib.dir"" ${basedir}/build/test/junit # this is ""test.java.output.dir"" My complete buildfile is available here: http://pastebin.com/SVnciGKR My properties file is available here: http://pastebin.com/9LCtNQUq UPDATE By modifying my targets to look like below I was able to get things working. Unfortunately I have to manually embed ant-junit.jar and junit.jar into my repository but it works so I guess this solves it. If anyone can help me get rid of the need to embed ant-junit.jar and junit.jar I'd really appreciate it: <path id=""webapp.tests.path"" > <pathelement location=""${red5.home}/red5.jar""/> <pathelement location=""${red5.home}/boot.jar""/> <pathelement location=""${bin.dir}/${ant.project.name}.jar""/> <pathelement path=""${red5.lib.dir}""/> <pathelement path=""${main.java.lib.dir}""/> <pathelement path=""${test.java.lib.dir}""/> </path> <target name=""compile.webapp.tests"" depends=""compile.webapp""> <javac srcdir=""${test.java.src.dir}"" destdir=""${test.java.bin.dir}""> <classpath refid=""webapp.tests.path""/> </javac> </target> <target name=""run.webapp.tests"" depends=""compile.webapp.tests""> <junit printsummary=""true""> <classpath> <path refid=""webapp.tests.path""/> <pathelement location=""${test.lib.dir}/ant/ant-junit.jar""/> <pathelement location=""${test.lib.dir}/ant/junit-4.8.2.jar""/> <pathelement path=""${test.java.bin.dir}""/> </classpath> <formatter type=""xml""/> <batchtest todir=""${test.java.output.dir}""> <fileset dir=""${test.java.bin.dir}"" includes=""**/*TestCase*""/> </batchtest> </junit> <junitreport todir=""${test.java.report.dir}""> <fileset dir=""${test.java.output.dir}"" includes=""**/*""/> <report todir=""${test.java.report.dir}""/> </junitreport> <delete file=""${test.java.report.dir}/TESTS-TestSuites.xml""/> </target> If I don't include the jars in the classpath I get errors telling me that junit.jar must be on the classpath in order to run the <junit> task. Weird huh? @Yishai - yup great pointer. We essentially did a small variant for #5 of the documentation you referenced. Adding `junit.jar` to the classpath is not so weird - we had to do that same thing except we had to add it to the classpath for compiling (`javac`) the test classes (which referenced JUnit classes) as well as to the `junit` classpath. I can't explain why your test classes compile without `junit.jar` in the compile classpath. Adding `ant-junit.jar` to the build.xml is weird - we did not have to do that. We had only to add `ant-junit.jar` to the classpath of the command-line used to invoke ant. JUnit is going to have to be in your compilation path (or your ANT_HOME/lib directory) no question about it. ant-junit.jar is the same deal. This is documented in the Ant documentation. http://ant.apache.org/manual/OptionalTasks/junit.html You need to reference ${test.java.bin.dir} in your junit task classpath. Edit: Hmm if by ""still failing"" you mean with the same error then my next guess is that something must be messed up about where the root directory is relative to where the packages start but I need to know more about your directory structure to be sure. Did that all of my tests are still failing :( I just edited the post with a link to my buildfile and a link to my properties file so you can get a feel for what the project layout looks like. I also gave a brief description of my directory layout as well. Thanks so much for your help. @rfkrocktk I think @Bert F figured it out.  I think this is the problem: <junit printsummary=""true""> <classpath> ... <fileset dir=""${test.java.bin.dir}"" includes=""**/*.class""/> </classpath> I believe the classpath shouldn't be including the .class files directly just the root of the bin dir. <junit printsummary=""true""> <classpath> ... <pathelement path=""${test.java.bin.dir}""/> </classpath> A good way to debug this is to run your build verbosely and examine the classpath closely to verify it is set how you expect it to be set. See Ant command-line options -verbose and possibly -debug. I'm surprised to set classpath set with only fileset and file elements instead of pathelements. Take a look at the Ant manual on Path-like Structures for examples to see what I mean. Finally the duplication between the classpaths in javac and junit may cause problems in the future. You'll likely want to name certain paths or collection of paths so you won't be duplicating them. See Ant manual on References. Yes I made the recommended changes but ant was throwing an error telling me that it needed junit on the classpath. So I added junit to the classpath and then I ran it. Unfortunately some of my tests are failing saying they can't get classes that are defined on my path. It's really weird for sure. It'll compile just fine but as soon as I try to run the tests everything goes weird. I've never had problems like this with JUnit. @rfkrocktk - okay thanks for the clarification. I just wanted to make sure I was not reading the problem wrong. The above changes were correct in that they addressed the original posted problem (class not found) but then there were subsequent problems (junit missing). I'm glad the answer helped. Thanks a lot! Unfortunately I'm still having the issue but I did find a workaround. I have to manually include junit and ant-junit.jar but it works. I'll update the post above to include this. A couple of my tests are still throwing ClassNotFoundExceptions when trying to load classes that I know exist and are definitely on the classpath. Did I do something wrong? @rfkrocktk - If I'm understanding you if your first comment you made the changes I recommended and you are still having ""the issue"" you originally posted - that com.mypackage.MyTestCase class was not found? But then by adding junit.jar and ant-junit.jar to the junit classpath fixed it? I guess that might make sense if MyTestCase was deriving from JUnit classes but I don't get why the compile didn't fail if the junit classes were missing in the first place. I had the same issue and BertF is right. The problem wasn't the junit jars but trying to set the classpath of unarchived classes with the fileset element  According to the ANT documentation for JUnit Task you have to do one of the following options to get junit tests to run as there is a dependency on an the junit.jar which is external to ANT: Note: You must have junit.jar available. You can do one of: Put both junit.jar and ant-junit.jar in ANT_HOME/lib. Do not put either in ANT_HOME/lib and instead include their locations in your CLASSPATH environment variable. Add both JARs to your classpath using -lib. Specify the locations of both JARs using a element in a in the build file. Leave ant-junit.jar in its default location in ANT_HOME/lib but include junit.jar in the passed to <junit>. (since Ant 1.7) I have verified that #1 putting junit.jar in ANT_HOME\lib worked (ant-junit.jar was already there ANT 1.8.0) and I did not need to specify a junit.jar in the classpath for the JUNIT tag. Essentially you took option #5."
532,A,"Need to run ""maven package"" to update unit tests I recently ""enabled dependency management"" in maven2 for a web project which I had a simple test class running in. Before I added the maven2 management whenever I updated the test - the change would appear instantaneous. However now I have to run ""mvn clean package"" before the slightest change can be picked up. I can see ""Maven Builder: AUTO_BUILD"" run whenever I save a class but the update just isn't picked up until I run the mvn goal. I also can't seem to configure a ""Maven Builder"" when I right click on the project and go to Properties > Builders. Any help on this would be much appreciated. Thanks Gearoid. Are you working in Eclipse Netbeans? Eclipse using m2eclipse? How dows your project look like? Structure? Does a mvn clean package correctly work on command line? Yes Eclipse with m2eclipse plugin. Strange one - I right clicked on my project and went Properties > Maven > LifeCycle Mapping. I added ""package"" to the mapping and everything started working correctly. Interestingly I went back and removed ""package"" and everything worked as normal. I had the same issue. I installed the Maven plugin through the Market place instead of the update site and didn't have the problem any more..."
533,A,"JUnit testing a call to @transactional @Async method causes Lock wait timeout exceeded I'm trying to test a service method that runs asynchronously (@Async). Here is the async method : @Async @Transactional(propagation=Propagation.SUPPORTS isolation = Isolation.READ_UNCOMMITTED) public Future<UserPrefs> checkLanguagePreference(long id) { UserPrefs prefs = prefsDao.retrieveUserPreferences(id); if(prefs == null || !StringUtils.hasLength(prefs.getLanguage())) { //Save a new sms-command object SmsBean command = SmsHelper.buildSmsCommand(); if(! smsDao.checkSameCommandExists(id command)) { smsDao.saveSms(id new SmsBean[] {command}); //Will wait until Lock wait timeout } } return new AsyncResult<UserPrefs>(prefs); } And here is The test method calling the async one : @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(location = ""..."") @TransactionConfiguration(transactionManager = ""txManager"" defaultRollback = false) @Transactional(isolation = Isolation.READ_UNCOMMITTED) @TestExecutionListeners( { DependencyInjectionTestExecutionListener.class DirtiesContextTestExecutionListener.class TransactionalTestExecutionListener.class }) public class MessagingServiceTest { @Before public void setUp() { //Avant tout mettre tout les sms en lu smsDao.deleteAllSms(1); sessionFactory.getCurrentSession().flush(); //On vérifie bien qu'il n y a plus de sms List<SmsBean> list = smsDao.getNewSmsList(1); assertEquals(0list.size()); } @Test public void checkLanguagePreferenceTest() throws InterruptedException ExecutionException { User user = (User) sessionFactory.getCurrentSession().load(User.class new Long(1));//idUser = 1 // We explicitly blank the preference from db prefsDao.saveLanguagePref(new UserPrefs(""""user)); Future<UserPrefs> prefs = messagingService.checkLanguagePreference(user.getId()); System.out.println(""wait completion of async task""); prefs.get(); System.out.println(""Async task has finished""); } } When prefs.get() is executed i have this error: Caused by: org.springframework.orm.hibernate3.HibernateJdbcException: JDBC exception on Hibernate data access: SQLException for SQL [insert into SmsBean (destination message origin sens status USER_ID) values (? ? ? ? ? ?)]; SQL state [41000]; error code [1205]; could not insert: Caused by: java.sql.SQLException: Lock wait timeout exceeded; try restarting transaction at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1075) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3562) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3494) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1960) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2114) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2696) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2105) at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2398) at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2316) at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2301) at org.apache.commons.dbcp.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:101) at org.hibernate.id.IdentityGenerator$GetGeneratedKeysDelegate.executeAndExtract(IdentityGenerator.java:94) at org.hibernate.id.insert.AbstractReturningDelegate.performInsert(AbstractReturningDelegate.java:57) ... 39 more This happens because smsDao.deleteAllSms in the setup Method is holding a lock on the sms table. How can I correctly avoid this lock timeout and be able to run my test successfully? Thanks for your help. FYI here is some console output :  DEBUG - Adding transactional method 'checkLanguagePreferenceTest' with attribute: PROPAGATION_REQUIREDISOLATION_READ_COMMITTED; '' DEBUG - Explicit transaction definition [PROPAGATION_REQUIREDISOLATION_READ_COMMITTED; ''] found for test context [[TestContext@b76fa testClass = MessagingServiceTest locations = array['file:src/main/resources/myapp-context.xml' 'file:src/main/resources/myapp-data.xml' 'file:src/main/resources/myapp-services.xml'] testInstance = fr.myapp.service.MessagingServiceTest@b01d43 testMethod = checkLanguagePreferenceTest@MessagingServiceTest testException = [null]]] DEBUG - Retrieved @TransactionConfiguration [@org.springframework.test.context.transaction.Tran sactionConfiguration(defaultRollback=false transactionManager=txManager)] for test class [class fr.myapp.service.MessagingServiceTest] DEBUG - Retrieved TransactionConfigurationAttributes [[TransactionConfigurationAttributes@5f7d3f transactionManagerName = 'txManager' defaultRollback = false]] for class [class fr.myapp.service.MessagingServiceTest] DEBUG - Returning cached instance of singleton bean 'txManager' DEBUG - Creating new transaction with name [checkLanguagePreferenceTest]: PROPAGATION_REQUIREDISOLATION_READ_COMMITTED; '' DEBUG - Opened new Session [org.hibernate.impl.SessionImpl@666a53] for Hibernate transaction DEBUG - Preparing JDBC Connection of Hibernate Session [org.hibernate.impl.SessionImpl@666a53] DEBUG - Changing isolation level of JDBC Connection [org.apache.commons.dbcp.PoolableConnection@1bde3d2] to 2 DEBUG - Exposing Hibernate transaction as JDBC transaction [org.apache.commons.dbcp.PoolableConnection@1bde3d2] DEBUG - No method-level @Rollback override: using default rollback [false] for test context [[TestContext@b76fa testClass = MessagingServiceTest locations = array['file:src/main/resources/myapp-context.xml' 'file:src/main/resources/myapp-data.xml' 'file:src/main/resources/myapp-services.xml'] testInstance = fr.myapp.service.MessagingServiceTest@b01d43 testMethod = checkLanguagePreferenceTest@MessagingServiceTest testException = [null]]] INFO - Began transaction (1): transaction manager [org.springframework.orm.hibernate3.HibernateTransa ctionManager@17753a8]; rollback [false] Hibernate: delete from SmsBean where USER_ID=? Hibernate: select user0_.id as id3_1_ user0_.email as email3_1_ user0_.login as login3_1_ user0_.passwd as passwd3_1_ smsbeans1_.USER_ID as USER7_3_3_ smsbeans1_.id as id3_ smsbeans1_.id as id0_0_ smsbeans1_.destination as destinat2_0_0_ smsbeans1_.message as message0_0_ smsbeans1_.origin as origin0_0_ smsbeans1_.sens as sens0_0_ smsbeans1_.status as status0_0_ smsbeans1_.USER_ID as USER7_0_0_ from User user0_ left outer join SmsBean smsbeans1_ on user0_.id=smsbeans1_.USER_ID where user0_.id=? Hibernate: select user0_.id as id3_ user0_.email as email3_ user0_.login as login3_ user0_.passwd as passwd3_ from User user0_ where user0_.login=? Hibernate: select userprefs0_.id as id2_ userprefs0_.language as language2_ userprefs0_.USER_ID as USER3_2_ from user_prefs userprefs0_ where userprefs0_.USER_ID=? wait completion of async task DEBUG - Returning cached instance of singleton bean 'txManager' INFO - Ener dans checkLanguagePreference(1) DEBUG - Opening Hibernate Session DEBUG - Registering Spring transaction synchronization for new Hibernate Session Hibernate: select userprefs0_.id as id2_ userprefs0_.language as language2_ userprefs0_.USER_ID as USER3_2_ from user_prefs userprefs0_ where userprefs0_.USER_ID=? Hibernate: select user0_.id as id3_1_ user0_.email as email3_1_ user0_.login as login3_1_ user0_.passwd as passwd3_1_ smsbeans1_.USER_ID as USER7_3_3_ smsbeans1_.id as id3_ smsbeans1_.id as id0_0_ smsbeans1_.destination as destinat2_0_0_ smsbeans1_.message as message0_0_ smsbeans1_.origin as origin0_0_ smsbeans1_.sens as sens0_0_ smsbeans1_.status as status0_0_ smsbeans1_.USER_ID as USER7_0_0_ from User user0_ left outer join SmsBean smsbeans1_ on user0_.id=smsbeans1_.USER_ID where user0_.id=? INFO - Checking if same sms command already exist Hibernate: select * from smsbean S where S.USER_ID=? and S.status=? and S.message=? DEBUG - Flushing Hibernate Session on transaction synchronization //Deadlock here : Hibernate: insert into SmsBean (destination message origin sens status USER_ID) values (? ? ? ? ? ?) DEBUG - Closing Hibernate Session 58799 [SimpleAsyncTaskExecutor-1] WARN org.hibernate.util.JDBCExceptionReporter - SQL Error: 1205 SQLState: 41000 58799 [SimpleAsyncTaskExecutor-1] ERROR org.hibernate.util.JDBCExceptionReporter - Lock wait timeout exceeded; try restarting transaction Resolved but FYI I've created before a thread on MySQL's forum about why I was getting this deadlock from the DBMS point of view. Here is the link (Well explained also): http://forums.mysql.com/read.php?97409237409237#msg-409237 Since your test is declared as @Transactional you have one big transaction that spreads over execution of setUp method and your test method. This transaction deadlocks with another transaction started in async action (async action waits for release of locks acquired by main transaction main transaction waits for completion of async action). You can solve it by breaking main transaction into several separate transactions: @Before @Transactional // separate transaction for setUp public void setUp() { //Avant tout mettre tout les sms en lu smsDao.deleteAllSms(1); sessionFactory.getCurrentSession().flush(); //On vérifie bien qu'il n y a plus de sms List<SmsBean> list = smsDao.getNewSmsList(1); assertEquals(0list.size()); } @Test @Transactional(propagation = NEVER) // Disable main transaction public void checkLanguagePreferenceTest() throws InterruptedException ExecutionException { // Programmatic transaction for test preparation User user = tx.execute(new TransactionCallback<User>() { public User doInTransaction(TransactionStatus status) { User user = (User) sessionFactory.getCurrentSession().load(User.class new Long(1));//idUser = 1 // We explicitly blank the preference from db prefsDao.saveLanguagePref(new UserPrefs(""""user)); return user; } }); Future<UserPrefs> prefs = messagingService.checkLanguagePreference(user.getId()); System.out.println(""wait completion of async task""); prefs.get(); System.out.println(""Async task has finished""); } private TransactionTemplate tx; @Autowired public void setPtm(PlatformTransactionManager ptm) { tx = new TransactionTemplate(ptm); } Thank you. PERFECT!"
534,A,"JUnit mocking with Mockito EasyMock etc I'm trying to mock a method of an object inside the class I'm testing. For instance class ClassToTest { public doSomething () { SomeObject a = new SomeObject (); a.doSomethingElse (); } } Is there a way to mock the methods of the variable ""a""? I'd like doSomethingElse to do nothing during testing. I'm currently using Mockito but I'm open to any mocking framework. Thanks If you want a new instance in each call I'd suggest refactoring in the following way: class ClassToTest { public doSomething () { SomeObject a = getInstance(); a.doSomethingElse (); } protected SomeObject getInstance() { return new SomeObject(); } } Then you can create a testclass extending ClassToTest overriding the getInstance() method with one supplying a mock object. This is of course only viable if you are ok with exposing the getInstance() method so I don't recommend it if the class is part of a public API. If this is the case consider supplying a factory class using dependency injection. If you are going down this route I think I'd create a Provider (Factory) and inject that. A decent DI framework (like say Guice) will create the factory for you.  I believe you can use EasyMock Class Extensions for EasyMock 2.5 or earlier and apparently it is included in 3.0. See this part of the previous page for information on what you are trying to do. That said I haven't personally tried to do that so I don't know how well it will work. with EasyMock class extensions you can mock classes where you can only mock interfaces without class extensions. However you cannot mock references declared as local variables.  Yes there is a way as shown by the following JMockit test: public void testDoSomething(final SomeObject mock) { new ClassToTest().doSomething(); new Verifications() {{ mock.doSomethingElse(); }}; } No need to refactor code under test to use a wrapper DI etc; simply mock whatever you need to be mocked.  With some refactoring it is possible of course: class SomeObject { public void doSomethingElse() { } } class ClassToTest { private final SomeObject someObject; public void doSomething() { someObject.doSomethingElse(); } public ClassToTest(SomeObject someObject) { this.someObject = someObject; } } class Test { @Test public void testDoSomething() { SomeObject someObject = Mockito.mock(SomeObject.class); new ClassToTest(someObject).doSomething(); Mockito.verify(someObject Mockito.atLeastOnce()).doSomethingElse(); } } Yes this was my thought as well. I just wasn't sure if mocking a local variable was possible (yet)  It's not possible to mock the reference ""a"" when it's declared as a local variable as in your case. You could consider injecting the dependency to SomeObject e.g. as a parameter of doSomething method. That way you can inject a mock of SomeObject in your test instead. One of the benefits of dependency injection is increased testability.  class ClassToTest { private SomethingElseInterface somethingElseDoer ; public ClassToTest(SomethingElseInterface somethingElseDoer) { this.somethingElseDoer = somethingElseDoer; } public doSomething () { somethingElseDoer.doSomethingElse(); } } And where you use it: SomethingElseInterface somethingElseDoer = ...; // in a test this is where you mock it ClassToTest foo = new ClassToTest(somethingElseDoer); // inject through constructor foo.doSomething();"
535,A,"Selenium not opening virtual browser window on Linux I am using Selenium to conduct user interface tests with JUnit in a Maven project. The project is located on a Linux box running IEs4Linux and Wine to allow us to run the tests in IE. Right now I'm using Xming along with Putty to view the virtual browser windows and I am able to open an IE window with the command ""ie6"" and I can see it functioning correctly in Xming. When setting up the Selenium RC using the *iexplore browser mode I get the following in the RC server window: INFO - Command request: getNewBrowserSession[*iexplore http://asdfasdfasdf.com:7011/ ] on session null INFO - creating new remote session INFO - Allocated session asdfasdfasdfasdfasdfasdfadsf for http://asdfasdfasdf.com:7011/ launching... INFO - Launching Embedded Internet Explorer... INFO - Launching Internet Explorer HTA... Which is normal except the browser never opens and the tests never run. Using *iexploreproxy or *piiexplore I get the following error from the RC: 13:46:06.957 INFO - Got result: Failed to start new browser session: org.openqa.selenium.server.browserlaunchers.WindowsUtils$WindowsRegistryException: Problem while managing the registry OS Version '2.6.18-164.11.1.el5' regVersion1 = null on session null Is there any reason Selenium would fail to launch the IE window using *iexplore? I would use *firefox or *chrome but the application I am testing is only compatible with IE. Also note that these test cases run perfectly fine on my local Windows machine. Please let me know if you need more details. After a bit of frustration with Wine I got it working using ""*custom ie6""...which opens up an IE window (good) but then I get a 404 Not Found when it tries to load the RemoteRunner. It looks like it's trying to find RemoteRunner.html on the server I'm testing rather than running the local scripts. I'll have to look into what is causing this. i have never used those but have you tried using *custom since IE isnt made for linux? Got it working...in case anybody else attempts to use a similar configuration to mine...for whatever reason it seems you cannot start up the server with the Http.proxyName and Http.proxyPort settings when using IEs4Linux to open IE6...it will just ignore them. You have to manually open the IE6 browser and enter the proxy settings yourself (most likely localhost and 4444). I think I'll start looking into IE7 for Linux...apparently IE6 doesn't like Selenium's use of XPaths."
536,A,"Compare two JSON objects in Java I'm looking for a JSON paring library that supports comparing two JSON objects ignoring child order specifically for unit testing JSON returning from a web service against an expected value. Do any of the major JSON libraries support this? the org.json simply does a reference comparison. That assumes that order on serialization to and from strings are always the same. I'm not comfortable making that assumption. Can't serialize both objects to string representation and compare? I guess all of the libraries support `toString()` to convert the object to `JSON` string. You're right Jeff it's not safe at all. This test shows a scenario where the mappings are the same but toString() does not return the same output: https://gist.github.com/anonymous/5974797. This is because the underlying HashMap can grow and if you remove keys the HashMap internal array does not shrink. Try this: public static boolean jsonsEqual(Object obj1 Object obj2) throws JSONException { if (!obj1.getClass().equals(obj2.getClass())) { return false; } if (obj1 instanceof JSONObject) { JSONObject jsonObj1 = (JSONObject) obj1; JSONObject jsonObj2 = (JSONObject) obj2; String[] names = JSONObject.getNames(jsonObj1); String[] names2 = JSONObject.getNames(jsonObj1); if (names.length != names2.length) { return false; } for (String fieldName:names) { Object obj1FieldValue = jsonObj1.get(fieldName); Object obj2FieldValue = jsonObj2.get(fieldName); if (!jsonsEqual(obj1FieldValue obj2FieldValue)) { return false; } } } else if (obj1 instanceof JSONArray) { JSONArray obj1Array = (JSONArray) obj1; JSONArray obj2Array = (JSONArray) obj2; if (obj1Array.length() != obj2Array.length()) { return false; } for (int i = 0; i < obj1Array.length(); i++) { boolean matchFound = false; for (int j = 0; j < obj2Array.length(); j++) { if (jsonsEqual(obj1Array.get(i) obj2Array.get(j))) { matchFound = true; break; } } if (!matchFound) { return false; } } } else { if (!obj1.equals(obj2)) { return false; } } return true; } within jsonArrays this returns true if *some* of the elements coincide as opposed to *all* the elements coincide. @matiasg - does the `if (obj1Array.length() != obj2Array.length())` not ensure *all* elements coincide? @kwah: nope. Consider this example: obj1Array = [111] obj2Array = [123]. This would return true. Also even if elements coincide they should be in the same order. This would return true for [123] and [231] too which is wrong  jsonObject implement a comparable interface try to use collection.sort().  This Solution for me work's very good: try { // Getting The Array ""Courses"" from json1 & json2 Courses1 =json1.getJSONArray(TAG_COURSES1); Courses2 = json2.getJSONArray(TAG_COURSES); //LOOP FOR JSON1 for(int i = 0; i < Courses1.length(); i++){ //LOOP FOR JSON2 for(int ii = 0; ii < Courses2.length(); ii++){ JSONObject courses1 = Courses1.getJSONObject(i); JSONObject courses2 = Courses2.getJSONObject(ii); // Storing each json1 item in variable int courseID1 = courses1.getInt(TAG_COURSEID1); Log.e(""COURSEID2:"" Integer.toString(courseID1)); String Rating1 = courses1.getString(TAG_RATING1); int Status1 = courses1.getInt(TAG_STATUS1); Log.e(""Status1:"" Integer.toString(Status1)); //Put the actual value for Status1 in log. // Storing each json2 item in variable int courseID2 = courses2.getInt(TAG_COURSEID); Log.e(""COURSEID2:"" Integer.toString(courseID)); //Put the actual value for CourseID in log String Title2 = courses2.getString(TAG_TITLE); String instructor2 = courses2.getString(TAG_INSTRUCTOR); String length2 = courses2.getString(TAG_LENGTH); String rating2 = courses2.getString(TAG_RATING); String subject2 = courses2.getString(TAG_SUBJECT); String description2 = courses2.getString(TAG_DESCRIPTION); //Status1 = 5 from json1; Incomplete Status1 =-1 Complete if(Status1 == 5 && courseID2 == courseID1){ // creating new HashMap HashMap<String String> map = new HashMap<String String>(); //Storing the elements if condition is true. map.put(TAG_COURSEID Integer.toString(courseID2)); //pend for compare map.put(TAG_TITLE Title2); map.put(TAG_INSTRUCTOR instructor2); map.put(TAG_LENGTH length2); map.put(TAG_RATING rating2); map.put(TAG_SUBJECT subject2); //show it map.put(TAG_DESCRIPTION description2); //adding HashList to ArrayList contactList.add(map); }//if }//for2 (json2) } //for1 (json1) }//Try Hope this help others. of course Just put your values and conditions and the kind of view in this case; Hashmap over a listview.  I'd take the library at http://json.org/java/ and modify the equals method of JSONObject and JSONArray to do a deep equality test. To make sure that it works regradless of the order of the children all you need to do is replace the inner map with a TreeMap or use something like Collections.sort(). I have used this library - it works well. its not that great - it really should have come with code to do json comparison. But imagine writing that code where JSON can be anything in any structure...write the compare on that! Its like writing a compare for all types HTML pages.  Using GSON JsonParser parser = new JsonParser(); JsonElement o1 = parser.parse(""{a : {a : 2} b : 2}""); JsonElement o2 = parser.parse(""{b : 2 a : {a : 2}}""); assertEquals(o1 o2);  For org.json I've rolled out my own solution a method that compares to JSONObject instances. I didn't work with complex JSON objects in that project so I don't know whether this works in all scenarios. Also given that I use this in unit tests I didn't put effort into optimizations. Here it is: public static boolean jsonObjsAreEqual (JSONObject js1 JSONObject js2) throws JSONException { if (js1 == null || js2 == null) { return (js1 == js2); } List<String> l1 = Arrays.asList(JSONObject.getNames(js1)); Collections.sort(l1); List<String> l2 = Arrays.asList(JSONObject.getNames(js2)); Collections.sort(l2); if (!l1.equals(l2)) { return false; } for (String key : l1) { Object val1 = js1.get(key); Object val2 = js2.get(key); if (val1 instanceof JSONObject) { if (!(val2 instanceof JSONObject)) { return false; } if (!jsonObjsAreEqual((JSONObject)val1 (JSONObject)val2)) { return false; } } if (val1 == null) { if (val2 != null) { return false; } } else if (!val1.equals(val2)) { return false; } } return true; } It happens to the best of us also :). +1 for fixing it in your answer. this is embarrassing Since you mentioned optimizations :) if `val1` is null you will get a NullPointerException from this code `if (!val1.equals(val2)) {` point taken :) Hope that it doesn't get too much votes though otherwise it's gonna be lots of support. Thank you Sean fixed it You did not consider if `js2` is `null` or not when `js1` is not `null`  You can try JsonUnit. It can compare two JSON objects and report differences. It's built on top of Jackson. For example assertJsonEquals(""{\""test\"":1}"" ""{\n\""test\"": 2\n}""); Results in java.lang.AssertionError: JSON documents are different: Different value found in node ""test"". Expected 1 got 2.  As a general architectural point I usually advise against letting dependencies on a particular serialization format bleed out beyond your storage/networking layer; thus I'd first recommend that you consider testing equality between your own application objects rather than their JSON manifestations. Having said that I'm currently a big fan of Jackson which my quick read of their ObjectNode.equals() implementation suggests does the set membership comparison that you want: public boolean equals(Object o) { if (o == this) return true; if (o == null) return false; if (o.getClass() != getClass()) { return false; } ObjectNode other = (ObjectNode) o; if (other.size() != size()) { return false; } if (_children != null) { for (Map.Entry<String JsonNode> en : _children.entrySet()) { String key = en.getKey(); JsonNode value = en.getValue(); JsonNode otherValue = other.get(key); if (otherValue == null || !otherValue.equals(value)) { return false; } } } return true; } @Yoni: Not true as there's a size comparison. They must have the exact same number of children as well as the same children. @Jolly Roger: In this case I'm not serializing the object from JSON back into a POJO but when sending JSON a system that does I can't rely on it sending it back in the exact same format in which I sent it. @Jeff you're right I missed that This method is not symmetric as it only tests 'subset' relationship of the children not equality. the 'other' object may have more children then in _children and this method would still return true.  If you are already using JUnit the latest version now employs Hamcrest. It is a generic matching framework (especially useful for unit testing) that can be extended to build new matchers. There is a small open source library called hamcrest-json with JSON-aware matches. It is well documented tested and supported. Below are some useful links: Source code Home page Javadocs for main matcher: Example code using objects from the JSON library org.json.simple: Assert.assertThat( jsonObject1.toJSONString() SameJSONAs.sameJSONAs(jsonObject2.toJSONString())); Optionally you may (1) allow ""any-order"" arrays and (2) ignore extra fields. Since there are a variety of JSON libraries for Java (Jackson GSON json-lib etc.) it is useful that hamcrest-json supports JSON text (as java.lang.String) as well as natively supporting objects from Douglas Crockford's JSON library org.json. Finally if you are not using JUnit you can use Hamcrest directly for assertions. (I wrote about it here.)  You could try using json-lib's JSONAssert class: JSONAssert.assertEquals( ""{foo: 'bar' baz: 'qux'}"" JSONObject.fromObject(""{foo: 'bar' baz: 'xyzzy'}"")); Gives: junit.framework.ComparisonFailure: objects differed at key [baz]; expected:<[qux]> but was:<[xyzzy]> Or even simpler: JSONAssert.assertJsonEquals( ""{foo: 'bar' baz: 'qux'}"" {foo: 'bar' baz: 'xyzzy'}""); Whil this solution works for the order of data items within the JSON it will fail if the order of elements within arrays does not match. If your code uses a Set that is converted to JSON for example. The following JSON compare would fail: `JSONAssert.assertJsonEquals( ""{foo: 'bar' list: [{test: '1'} {rest: '2'}] }"" ""{ foo: 'bar' list: [{rest: '2'} {test: '1'}] }"");` With the message: `junit.framework.AssertionFailedError: : : objects differed at key [list];: arrays first differed at element [0];: objects differed at key [test];`  Try Skyscreamer's JSONAssert. Its non-strict mode has two major advantages that make it less brittle: Object extensibility (e.g. With an expected value of {id:1} this would still pass: {id:1moredata:'x'}.) Loose array ordering (e.g. ['dog''cat']==['cat''dog']) In strict mode it behaves more like json-lib's test class. A test looks something like this: @Test public void testGetFriends() { JSONObject data = getRESTData(""/friends/367.json""); String expected = ""{friends:[{id:123name:\""Corby Page\""}"" + ""{id:456name:\""Solomon Duskis\""}]}""; JSONAssert.assertEquals(expected data false); } The parameters in the JSONAssert.assertEquals() call are expectedJSONString actualDataString and isStrict. The result messages are pretty clear which is important when comparing really big JSON objects. I've been using this solution but I've just found that you could also provide a JSONCompareMode of you choosing. One of which is NON_EXTENSIBLE. So you'd have something like this: `JSONAssert.assertEquals(expected data JSONCompareMode.NON_EXTENSIBLE);` The NON_EXTENSIBLE mode means that any new or missing fields cause failures but the order doesn't. Using false would instigate lenient mode which wouldn't report any extra or missing child elements. The NON_EXTENSIBLE compare mode is *exactly* what I was looking for. Thanks for this Dan. Before I get all excited: does this support nested JSON objects and arrays as well? :) Yup. Objects containing objects containing arrays containing objects etc..  I would do the following final JSONObject obj1 = /*json*/; final JSONObject obj2 = /*json*/; final ObjectMapper mapper = new ObjectMapper(); final JsonNode tree1 = mapper.readTree(obj1.toString()); final JsonNode tree2 = mapper.readTree(obj2.toString()); return tree1.equals(tree2);  One thing I did and it works wonders is to read both objects into HashMap and then compare with a regular assertEquals(). It will call the equals() method of the hashmaps which will recursively compare all objects inside (they will be either other hashmaps or some single value object like a string or integer). This was done using Codehaus' Jackson JSON parser. assertEquals(mapper.readValue(expectedJson new TypeReference<HashMap<String Object>>(){}) mapper.readValue(actualJson new TypeReference<HashMap<String Object>>(){})); A similar approach can be used if the JSON object is an array instead."
537,A,"How does JUnit find the eclipse plug-in being tested? I am writing a plug-in (ClassRefactoringPlugin) that examines source code in Eclipse 3.6.1. The plug-in contains a CallData class that examines a Java source file and figures out which Java elements are called from a method using JDT operations. I wrote a JUnit 4 test for this class that also resides in the ClassRefactoringPlugin project. When I ran it as a JUnit plug-in test I got: Java Model Exception: Java Model Status [ClassRefactoringPlugin does not exist] What have I done wrong? The configuration specifies to launch with all workspace and enabled target plug-ins and ClassRefactoringPlugin is in my dropins directory. (Although shouldn't the project's version of the plug-in be recognized by the spawned workspace?) Here's the stack trace: !MESSAGE CallData.calculateCalledMethods: Java Model Exception: Java Model Status [ClassRefactoringPlugin does not exist] Java Model Exception: Java Model Status [ClassRefactoringPlugin does not exist] at org.eclipse.jdt.internal.core.JavaElement.newJavaModelException(JavaElement.java:502) at org.eclipse.jdt.internal.core.Openable.generateInfos(Openable.java:246) at org.eclipse.jdt.internal.core.Openable.openAncestors(Openable.java:504) at org.eclipse.jdt.internal.core.Openable.generateInfos(Openable.java:240) at org.eclipse.jdt.internal.core.Openable.openAncestors(Openable.java:504) at org.eclipse.jdt.internal.core.Openable.generateInfos(Openable.java:240) at org.eclipse.jdt.internal.core.Openable.openAncestors(Openable.java:504) at org.eclipse.jdt.internal.core.CompilationUnit.openAncestors(CompilationUnit.java:1170) at org.eclipse.jdt.internal.core.Openable.generateInfos(Openable.java:240) at org.eclipse.jdt.internal.core.SourceRefElement.generateInfos(SourceRefElement.java:107) at org.eclipse.jdt.internal.core.JavaElement.openWhenClosed(JavaElement.java:515) at org.eclipse.jdt.internal.core.JavaElement.getElementInfo(JavaElement.java:252) at org.eclipse.jdt.internal.core.JavaElement.getElementInfo(JavaElement.java:238) at org.eclipse.jdt.internal.core.JavaElement.getChildren(JavaElement.java:193) at org.eclipse.jdt.internal.core.JavaElement.getChildrenOfType(JavaElement.java:207) at org.eclipse.jdt.internal.core.SourceType.getMethods(SourceType.java:403) at nz.ac.vuw.ecs.kcassell.utils.EclipseSearchUtils.addDesiredMethods(EclipseSearchUtils.java:333) at nz.ac.vuw.ecs.kcassell.utils.EclipseSearchUtils.getMethods(EclipseSearchUtils.java:210) at nz.ac.vuw.ecs.kcassell.callgraph.CallData.collectMethodCallData(CallData.java:203) at nz.ac.vuw.ecs.kcassell.callgraph.CallData.calculateCalledMethods(CallData.java:176) at nz.ac.vuw.ecs.kcassell.callgraph.CallData.collectCallData(CallData.java:151) at nz.ac.vuw.ecs.kcassell.callgraph.CallDataTest.testCollectCallData(CallDataTest.java:67) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at junit.framework.TestCase.runTest(TestCase.java:168) at junit.framework.TestCase.runBare(TestCase.java:134) at junit.framework.TestResult$1.protect(TestResult.java:110) at junit.framework.TestResult.runProtected(TestResult.java:128) at junit.framework.TestResult.run(TestResult.java:113) at junit.framework.TestCase.run(TestCase.java:124) at junit.framework.TestSuite.runTest(TestSuite.java:232) at junit.framework.TestSuite.run(TestSuite.java:227) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.pde.internal.junit.runtime.RemotePluginTestRunner.main(RemotePluginTestRunner.java:62) at org.eclipse.pde.internal.junit.runtime.UITestApplication$1.run(UITestApplication.java:116) at org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35) at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:134) at org.eclipse.swt.widgets.Display.runAsyncMessages(Display.java:3515) at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3164) at org.eclipse.ui.internal.Workbench.runEventLoop(Workbench.java:2640) at org.eclipse.ui.internal.Workbench.runUI(Workbench.java:2604) at org.eclipse.ui.internal.Workbench.access$4(Workbench.java:2438) at org.eclipse.ui.internal.Workbench$7.run(Workbench.java:671) at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332) at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:664) at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:149) at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:115) at org.eclipse.pde.internal.junit.runtime.UITestApplication.start(UITestApplication.java:47) at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196) at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110) at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79) at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:369) at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:179) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:619) at org.eclipse.equinox.launcher.Main.basicRun(Main.java:574) at org.eclipse.equinox.launcher.Main.run(Main.java:1407) at org.eclipse.equinox.launcher.Main.main(Main.java:1383) I am wondering if the error message might be a red herring. The error occurs when making a call to IMethod[] methods = type.getMethods(); If I set a breakpoint there and look at type in the Variables view of the debugger I see: CallDataTest (not open) [in CallDataTest.java [in nz.ac.vuw.ecs.kcassell.callgraph [in test [in ClassRefactoringPlugin]]]] I wonder if I am omitting some important preliminary step to make the project available for examination. First I attempt to activate the workbench like so: public static void activateWorkbench() { // possible for PlatformUI.getWorkbench to throw an IllegalStateException // if the workbench is not yet started e.g createAndRunWorkbench() has not yet been called IWorkbench workbench = PlatformUI.getWorkbench(); IWorkbenchWindow workbenchWindow = workbench.getActiveWorkbenchWindow(); workbenchWindow.getActivePage(); } Then I try to get type using its handle: protected IType iType = EclipseUtils.getTypeFromHandle( ""=ClassRefactoringPlugin/test<nz.ac.vuw.ecs.kcassell.callgraph{CallDataTest.java[CallDataTest""); public static IType getTypeFromHandle(String handle) { IType type = null; IJavaElement element = JavaCore.create(handle); if (element == null) { System.err.println("" No element created from "" + handle); } else if (element instanceof IType) { type = (IType) element; } return type; } I am new to plug-in development so any help would be much appreciated. Cheers Keith I confirm that you don't need to put your exported plugin in the dropins folder because Eclipse is using the project's version of the plugin you are writing when running the JUnit Launch configuration. Can you paste the full stack trace because I don't think the issue is about the Junit run config. If you are able to lauch it I guess the test class is available in the main Junit Run configuration tab ? I'm not entirely sure what you mean by the ""main Junit Run configuration tab""? From Eclipse if I do Run->Run Configurations I get a ""Run Configurations"" window that lists CallDataTest as a ""JUnit Plug-in Test"". Is this what you wanted to know? I'm editing the original post to include the stack trace. Hello kc2001. You've offered a bounty and up-voted my answer for a similar question as the solution to yours. Please mark my question as the accepted answer. It'd be a shame to throw those points to waste. Thank you. I've updated the information in the main post to reflect a move to a newer Eclipse version. Had a similar error for different reasons. My error started after renaming the target package in the project. After I tried the solution from paskster without success I started diggin'. The actual problem was that renaming the package name from the manifest file didn't work as expected. I thought it would change everything everywhere. However the old package name was still in the manifest file in the test directory and in some views in the resources. After renaming those manually and rebuilding the project the error was gone. This is the accepted answer for another question: Java Model Exception: Java Model Status [gen [in MyApp] does not exist] after Eclipse Android project Clean Since the owner of this question up-voted this answer as a solution with a link to this question I thought I'd post it here as well. Unfortunately my initial joy on reading your post was premature. I made every name consistent that I could think of and still had the problem. However I did not notice any specific ""test"" information in the MANIFEST.  I am a little unsure exactly what you are trying to do here but it looks like you are trying to write a JUnit test for your plugin. Is this right? According to the java element handle identifier that you are creating there should be a project called ClassRefactoringPlugin in your workspace and in there is a source folder called test and a java class called CallDataTest in a package called nz.ac.vuw.ecs.kcassell.callgraph. I'm guessing that this is not the case and that you want to reference a class file in the plugin that you just created. Before you can do any of that you need to import a project into your test workspace set its classpath and then you can access its contents using handle identifiers. I may be misunderstanding what you are trying to do but it does seem like you are trying to access a java file that doesn't exist in your workspace. If you can confirm that this is what you want to do I can point you to some test code that does exactly what you want. There are some open source projects that do exactly this and you can borrow some of their source code for your purpose. A project that I am familiar with is Groovy-Eclipse http://groovy.codehaus.org/Eclipse+Plugin (since I am the lead on that project). Here is a link to the class that we use to create and manage test projects: https://svn.codehaus.org/groovy/eclipse/trunk/ide-test/org.codehaus.groovy.eclipse.tests/src/org/codehaus/groovy/eclipse/test/TestProject.java You can use some or all of this code for your own tests. Just remember to delete all projects at the end of each test. I believe that all of the preconditions you mentioned are set correctly. I don't understand is your reference to a ""test workspace"". I have a workspace containing my ClassRefactoringPlugin that has both src and test folders. When I right click on CallDataTest and ""Run as JUnit plugin test"" a new workbench is spawned. If I do that in debug mode I can actually step through the code in the test using the debugger. That's why I'm wondering if the error message may be misleading. BTW I plugin code runs fine in a spawned workspace when I debug it just not when I try to run JUnit tests. This is not easy stuff. :) And it took us a long time to get it right. So feel free to come back if you have more questions. I think that my major problem was the failure to set up the project in the junit-workspace. While I don't have it working just yet I feel like your help should be enough for me to get it. Unless a super answer arrives soon you'll get the bounty. Cheers! Right. JUnit plugin tests spawn a new workspace but this workspace is initially empty. You are trying to access a project (and its source code) that exists in the regular workspace but not your runtime JUnit workspace. I am presuming that when you launch a runtime (not a JUnit runtime) workspace then you have a project in that workspace called `CallDataTest`. You need to import `CallDataTest` into your JUnit workspace if you want to create an `IJavaElement`s based on it. There is no simple API to do this but there many open source projects that need to do a similar thing. Updated my answer to include a link to some sample source code."
538,A,"Does JUnit support properties files for tests? I have JUnit tests that need to run in various different staging environments. Each of the environments have different login credentials or other aspects that are specific to that environment. My plan is to pass an environment variable into the VM to indicate which environment to use. Then use that var to read from a properties file. Does JUnit have any build in capabilities to read a .properties file? // // Load properties to control unit test behaviour. // Add code in setUp() method or any @Before method (JUnit4). // // Corrected previous example: - Properties.load() takes an InputStream type. // import java.io.File; import java.io.FileInputStream; import java.util.Properties; Properties p = new Properties(); p.load(new FileInputStream( new File(""unittest.properties""))); // loading properties in XML format Properties pXML = new Properties(); pXML.loadFromXML(new FileInputStream( new File(""unittest.xml"")));  java has built in capabilities to read a .properties file and JUnit has built in capabilities to run setup code before executing a test suite. java reading properties: Properties p = new Properties(); p.load(new FileReader(new File(""config.properties""))); junit startup documentation put those 2 together and you should have what you need. A ClassLoader might need to be used: http://stackoverflow.com/a/9983486/640378  Can't you just read the properties file in your setup method?  It is usually preferred to use class path relative files for unit test properties so they can run without worrying about file paths. The path may be different on your dev box or the build server or where ever. This will also work from ant maven eclipse without changes. private Properties props = new Properties(); InputStream is = ClassLoader.getSystemResourceAsStream(""unittest.properties""); try { props.load(is); } catch (IOException e) { // Handle exception here } putting the ""unittest.properties"" file at the root of the classpath. This is good information. You might consider making this a comment to the already accepted answer; there it would be far more likely to be read by someone who comes across this in their results."
539,A,"Best way to automagically migrate tests from JUnit 3 to JUnit 4? I have a bunch of JUnit 3 classes which extend TestCase and would like to automatically migrate them to be JUnit4 tests with annotations such as @Before @After @Test etc. Any tool out there to do this in a big batch run? Not answering the question but you do realize that you can run JUnit3 tests under JUnit4 without modification yes? Unless you use Junit3 `TestSuite` in which case you're stuffed. If you keep those JUnit 3 tests around other developers (depending on your team size) will keep copying them to new tests. And if they need to temporary disable a test (yes the shouldn't but sometimes they will need to) they 'll keep renaming it from testX to FIMXEtestX instead of annotating it with @Ignore. So you won't have any idea how many tests are currently being ignored (so how worried you should be). Just take any big project and look for regex ""public void \w+test"" Nice post. I did the upgrade using Netbeans with the following RegEx strings: (First line search-string second one replace-string) public void test @Test\n public void test @Override\n.*protected void onSetUp @Before\n protected void onSetUp @Override\n.*protected void onTearDown @After\n protected void onTearDown Don't forget to flag the Regular Expression checkbox!  Here are the actual regular expressions I used to execute furtelwart's suggestions: // Add @Test Replace: ^[ \t]+(public +void +test) With: @Test\n $1 Regular Expression: on Case sensitive: on File name filter: *Test.java // Remove double @Test's on already @Test annotated files Replace: ^[ \t]+@Test\n[ \t]+@Test With: @Test Regular Expression: on Case sensitive: on File name filter: *Test.java // Remove all empty setUp's Replace: ^[ \*]+((public|protected) +)?void +setUp\(\)[^\{]*\{\s*(super\.setUp\(\);)?\s*\}\n([ \t]*\n)? With nothing Regular Expression: on Case sensitive: on File name filter: *Test.java // Add @Before to all setUp's Replace: ^([ \t]+@Override\n)?[ \t]+((public|protected) +)?(void +setUp\(\)) With: @Before\n public void setUp() Regular Expression: on Case sensitive: on File name filter: *Test.java // Remove double @Before's on already @Before annotated files Replace: ^[ \t]+@Before\n[ \t]+@Before With: @Before Regular Expression: on Case sensitive: on File name filter: *Test.java // Remove all empty tearDown's Replace: ^[ \*]+((public|protected) +)?void +tearDown\(\)[^\{]*\{\s*(super\.tearDown\(\);)?\s*\}\n([ \t]*\n)? With nothing Regular Expression: on Case sensitive: on File name filter: *Test.java // Add @After to all tearDown's Replace: ^([ \t]+@Override\n)?[ \t]+((public|protected) +)?(void +tearDown\(\)) With: @After\n public void tearDown() Regular Expression: on Case sensitive: on File name filter: *Test.java // Remove double @After's on already @After annotated files Replace: ^[ \t]+@After\n[ \t]+@After With: @After Regular Expression: on Case sensitive: on File name filter: *Test.java // Remove old imports add new imports Replace: ^([ \t]*import[ \t]+junit\.framework\.Assert;\n)?[ \t]*import[ \t]+junit\.framework\.TestCase; With: import org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\nimport static org.junit.Assert.*; Regular Expression: on Case sensitive: on File name filter: *Test.java // Remove all extends TestCase Replace: [ \t]+extends[ \t]+TestCase[ \t]+\{ With: { Regular Expression: on Case sensitive: on File name filter: *Test.java // Look for import junit.framework; Find: import junit\.framework Manually fix Regular Expression: on Case sensitive: on // Look for ignored tests (FIXME disabled ...) Find: public[ \t]+void[ \t]+\w+test Manually fix Regular Expression: on Case sensitive: on // Look for dummy/empty tests Find: public[ \t]+void[ \t]+test[\w\d]*\(\s*\)\s*\{\s*(//[^\n]*)?\s*\} Manually fix Regular Expression: on Case sensitive: on Note: it's important to do them in the order shown above. Is there a way to run this? Yes. In IntelliJ do ""Replace in path"" (ctrl-shift-r) and fill in that dialog for each of these. For those that need to be done manually use ""Find in path"" (ctrl-shift-f).  There are to my best knowledge no available migration tools (yet). What I know is this: Last year at OOPSLA in Nashville was a paper about API migration but alas their tools seems not be openly available. I'll provide the link to the paper (even though I dare it is of little use for you since it is rather theory heavy): ""Annotation Refactoring: Inferring Upgrade Transformations for Legacy Applications"". Above I wrote ""no available tool (yet)"" because my student Lea Hänsenberger is currently working on an auotmated API migration from not onyl JUnit 4 a to JExample but also from JUnit 3 to JUnit 4. Please follow JExample on Twitter to get notified when she releases a first beta. I hope this information was of help for you.  In my opinion it cannot be that hard. So let's try it: 0. Imports You need to import three annotations: import org.junit.After; import org.junit.Before; import org.junit.Test;` After you've made the next few changes you won't need import junit.framework.TestCase;. 1. Annotate test* Methods All methods beginning with public void test must be preceded by the @Test annotation. This task is easy with a regex. 2. Annotate SetUp and TearDown methods Eclipse generates following setUp() method: @Override protected void setUp() throws Exception { } Must be replaced by: @Before public void setUp() throws Exception { } Same for tearDown(): @Override protected void tearDown() throws Exception { } replaced by @After public void tearDown() throws Exception { } 3. Get rid of extends TestCase Remove exactly one occurence per file of the string "" extends TestCase"" 4. Remove main methods? Probably it's necessary to remove/refactor existing main methods that will execute the test. 5. Convert suite() method to @RunWithClass According to saua's comment there must be a conversion of the suite() method. Pattern will follow. Thanks saua! Conclusion I think it's done very easy via a set of regular expressions even if it will kill my brain ;) The imports can be cleaned up automatically by Eclipse (Ctrl + Shift + O) I don't think that he wants to do this with a bunch of files. And they should be runnable. I'd add conversion of suite() methods to @RunWith(Suite.class) @SuiteCkasses() the setUp() tearDown() methods have to be public in JUnit4 @awx: Thanks I updated it! One additional step that I needed was to add @RunWith(JUnit4.class) to my class + import those things: import org.junit.runner.RunWith; import org.junit.runners.JUnit4; need to convert the fail(string) and assertXX(...) calls to respective classes as we do not extend TestCase import static org.junit.Assert.*;  We are in the middle of migrating a reasonably large code base to JUnit4. Since this is the second time I'm doing a migration such as this I decided to save the code somewhere: https://github.com/FranciscoBorges/junit3ToJunit4 It deals with more corner cases than the ones enumerated in answers above. Such as: calls to TestCase.setUp() and TestCase.tearDown() calls to TestCase(String) constructor within a sub-class constructor calls to TestCase.assert* methods that moved to Assert. fixing package names junit.framework to org.junit etc  I don't know of a tool that would do this at the moment - I'd expect Eclipse to provide some plugin fairly shortly - but you could knock up a simple source tree exploring Java class that would do it for you if you only want to do a basic conversion. I had to write something similar to automatically generate skeleton test cases for a legacy application so I've got a fair amount of the support code already. You're welcome to use it. Do you use java6 AST feature or eclipse AST ? Anyway I am interested also: may be you may consider making this question a 'code-challenge' and publish your anonymized code on DZones ? (see http://stackoverflow.com/questions/190007 for an example of 'code-chalenge')"
540,A,Easiest way to unit test SWT and Swing apps in a headless environment? I'm looking to unit test some SWT and Swing code for a project I'm working on and the tests run fine as long as I'm running them from eclipse. As soon as I run them in my hudson environment it fails since hudson runs the tests in headless mode. What's the best way of doing this? Open source solutions only please (since the project is open source). I was sure I posted this here before not sure what happened to it. Cacio allows for running Swing app headless. http://rkennke.wordpress.com/2011/10/17/cacio-for-ui-testing/  Using Swing I tend to organise things so that the component tree can be created without a Window at the top. Doing this allows you to simply create a JPanel in a unit test and use that as your top-level component. There are certain things you cannot test such as focus and any logic involved in the creation of the Frame for normal operation but the vast majority can be tested. You may want to look into the FEST library to make life easier whether you go headless or not it looks very good: http://fest.easytesting.org/swing/wiki/pmwiki.php  Try the Abbot Java GUI Testing Framework and SWTbot. At least SWTbot should be able to do it. If neither offers a headless mode then this blog post might give you some ideas how to get rid of the UI for testing.  I don't know about SWT but with Swing you can't. Any instantiation of a Window (JFrame JDialog etc.) even if it is never set to visible will blow up in headless mode (on JDK 5). What we did was not run in headless mode and install Xvfb to provide the windowing without actually having a real windowing system installed.  You could run Xvfb (X virtual framebuffer an X11 server that performs all graphical operations in memory) and this works fine. But there is another solution with Hudson's plugin for Xvnc. Simply install the plugin and check the checkbox in the job configuration screen: Hudson will then automatically start up a Xvnc session and set the DISPLAY environment variable to the appropriate value and then shut down the session when the build is complete. One advantage this has over the Xvfb method is that if you have multiple Swing/SWT projects building simultaneously each build has its own X session. This may not be an issue at all but it seems like a good idea. Before using this plugin you obviously have to have Xvnc installed. What's less obvious (although sensible) is that you must also set a password. You do this by running: $ vncpassword This has to be done as the same user Hudson runs as. If this works this is the neatest trick I've seen in years! +1 Works for me :-)
541,A,"Scala Lift JUnit Test Fails With ""error: not found: type foo"" Using mvn test Wanted to do a simple unit test of the default Lift snippet class generated by the maven-lift-eclipse archetype. I called it HelloWorld.scala (in project/src/main/scala/my/namespace/helloworld/snippet/HelloWorld.scala) package my.namespace.helloworld { package snippet { import _root_.scala.xml.NodeSeq import _root_.net.liftweb.util.Helpers import _root_.java.util.Date import Helpers._ class HelloWorld { def howdy(in: NodeSeq): NodeSeq = Helpers.bind(""b"" in ""time"" -> (new Date).toString) }}} So I added added a new test testHowdy to the default generated AppTest.scala (in project/src/test/scala/my/namespace/AppTest.scala) package my.namespace import _root_.java.io.File import _root_.junit.framework._ import Assert._ import _root_.scala.xml.XML import _root_.net.liftweb.util._ import _root_.net.liftweb.common._ import my.namespace._ object AppTest { def suite: Test = { val suite = new TestSuite(classOf[AppTest]) suite } def main(args : Array[String]) { _root_.junit.textui.TestRunner.run(suite) } } /** * Unit test for simple App. */ class AppTest extends TestCase(""app"") { /** * Rigourous Tests :-) */ def testOK() = assertTrue(true) // def testKO() = assertTrue(false); def testHowdy() = { val h = new HelloWorld() assertFalse(h.howdy(""<!DOCTYPE html><title></title><p><b:time></p>"") contains """") } def testXml() = { <snip> } } With that additional test I generate a compile error running $ mvn test: [INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Building helloworld Project [INFO] task-segment: [test] [INFO] ------------------------------------------------------------------------ [INFO] [resources:resources {execution: default-resources}] [INFO] Using 'UTF-8' encoding to copy filtered resources. [INFO] Copying 0 resource [INFO] [resources:copy-resources {execution: default-copy-resources}] [INFO] Using 'UTF-8' encoding to copy filtered resources. [INFO] Copying 1 resource [INFO] [yuicompressor:compress {execution: default}] [INFO] nb warnings: 0 nb errors: 0 [INFO] [compiler:compile {execution: default-compile}] [INFO] Nothing to compile - all classes are up to date [INFO] [scala:compile {execution: default}] [INFO] Checking for multiple versions of scala [INFO] includes = [**/*.scala**/*.java] [INFO] excludes = [] [INFO] Nothing to compile - all classes are up to date [INFO] [resources:testResources {execution: default-testResources}] [INFO] Using 'UTF-8' encoding to copy filtered resources. [INFO] Copying 0 resource [INFO] [compiler:testCompile {execution: default-testCompile}] [INFO] Nothing to compile - all classes are up to date [INFO] [scala:testCompile {execution: default}] [INFO] Checking for multiple versions of scala [INFO] includes = [**/*.scala**/*.java] [INFO] excludes = [] [INFO] /home/noel/workspace/helloworld/src/test/scala:-1: info: compiling [INFO] Compiling 2 source files to /home/noel/workspace/helloworld/target/test-classes at 1301433670806 [ERROR] /home/noel/workspace/helloworld/src/test/scala/my/namespace/helloworld/AppTest.scala:34: error: not found: type HelloWorld [INFO] val h = new HelloWorld() [INFO] ^ [ERROR] one error found [INFO] ------------------------------------------------------------------------ [ERROR] BUILD ERROR [INFO] ------------------------------------------------------------------------ [INFO] wrap: org.apache.commons.exec.ExecuteException: Process exited with an error: 1(Exit value: 1) [INFO] ------------------------------------------------------------------------ [INFO] For more information run Maven with the -e switch [INFO] ------------------------------------------------------------------------ [INFO] Total time: 14 seconds [INFO] Finished at: Tue Mar 29 16:21:18 CDT 2011 [INFO] Final Memory: 25M/61M [INFO] ------------------------------------------------------------------------ This seems like a simple namespace issue of some sort but I can't track down where I'm doing the import wrong! Note: the default generated AppTest.scala does run successfully. It only started failing to compile after my additional test. I can't seem to find any import you do that implies... import my.namespace.helloworld.snippet.HelloWorld This may reflect my ignorance of `import` but wouldn't `import my.namespace._` take care of that? Of course it doesn't. I didn't see any mention of import matching depth in the namespace docs. What does the `_` actually do? The _ only matches one level of entities (class/trait/object/package). So you can also write: ""import my.namespace.helloworld.snippet._"""
542,A,"How do I define a TestSuite without using @SuiteClasses in Junit 4.5? I'm trying to migrate to JUnit 4 and I'm not clear about the correct way to set up test suites. I know how to set up a test suite with fixed tests using the @SuitesClasses annotation. However I want to have a top-level suite class where I can programatically decide which test classes or suites I want to load. I know that there are addTest and addTestSuite operations in the TestSuite class. However if I define a TestSuite subclass with a constructor that attempts to add these tests and try to run it I get an error ""Must have SuiteClasses annotation"". Any idea how to do this? I would recommend creating a subclass of the BlockJUnit4ClassRunner and pull in the classes you want to test manually. The protected methods of the class do all the hard work for you although you might want to tweak the Descriptions a bit to make sure the results are all unique in the output files."
543,A,"Xml string representation of expected result for Junit test I'm writing a Junit test in which the expected result is a XML string. What's the best way represent the expected string? Right now I have it as a stringBuilder and I do the following. I want to assert that the actual and expected are same. What would be the most efficient way to represent this expected string? Also I don't want anyone to alter the expected string so it should be final too. @Before public void setUp() { expected.append(""<?xml version=\""1.0\"" encoding=\""utf-8\""?>""); expected.append(""<n-relationship guid=\""IEEBFAD40BC5711DFAE41F5F92790F896\"" control=\""add\"">""); expected.append(""<n-relbase>ID17A8B10BC5711DFAE41F5F92790F896</n-relbase><n-reltype>IPLS</n-reltype>""); expected.append(""<ip.content><pub.no>1234567</pub.no>""); expected.append(""<event.date>2010-09-10</event.date><event.code>02394802323</event.code>""); expected.append(""<event.treatment>+</event.treatment><event.description>OPPOSITION DISMISSED</event.description>""); expected.append(""</ip.content></n-relpayload></n-relationship""); } This is harder than it looks - string comparisons will certainly fail. This is because at least different representations for same entity (& apos; etc.) different order of attributes different quotes whitespace The simplest way is to canonicalise the XML in both the expected and test and compare those. All good XML tools should have a canonicalizer. I have had to write a lot of XML machinery to support my Unit tests - admittedly they contain floating point numbers and I have to allow for rounding errors. But comparing strings will certainly fail. +1 for the canonicalizer  Comparing 2 XML using their String representation is dangerous. Indeed do you consider the following XML : <foo> <bar>xxx</bar> <baz>yyy</baz> </foo> equals to: <foo> <baz>yyy</baz> <bar>xxx</bar> </foo> ? If yes then I suggest that you use XMLUnit to test your current XML with the expected one. Regarding the expected XML the best way is to store them as external files and load them during your test (in a @BeforeClass snippet for example).  a) XML does not belong inside java code. Put it in the same package (preferably in the src/main/resources folder if you use maven) and load it as a resource. b) Don't do a string comparison with XML: do an XML structure comparison. The canonicalizer is a great idea.  I think you are on the correct track -- define a string constant ""EXPECTED_XML"" and just assert off that."
544,A,"Access POP3 account from JUnit tests I'm looking for a way to access a POP3 account while running a JUnit test. Does anyone know how to do that? I probably need some kind of library for POP3 access I guess. I'm intensely curious about why you need to do such a thing! I plan on ""misusing"" JUnit to do some integration testing (and not pure class testing). The functionality I test sends out an email. I simply want to check if this email was received and if it contains the correct content. Ok actually it is quite simple and not really related to junit. I just use the java.mail.* libs to access the POP3 account."
545,A,"What is the equivalent to JUnit in C#? I am coming from Java and am currently working on a C# project. What is the recommended way to go about a) unit testing existing C# code and b) accomplishing TDD for C# development? Also is there an equivalent to EMMA / EclEmma (free yet powerful code coverage tool) for Visual Studio and C# code? IF another person says NUnit I am gonna puke! Where TF did you people learn to read? as I've already said to one of the ""answers"" the key word in my question is ""recommended"" - I had obviously heard of NUnit but wanted to know whether it was the best one to go with. Sorry should have added that to the question. NUnit is patterned after JUnit but if you're using Visual Studio 2008 then consider the built-in unit testing framework. Apparently the first NUnit was simply the Junit source run through CSC to see what would compile. They went through the minimal conversions necessary. I use the built-in testing framework but also install TestDriven.Net to get the nice right-click menu extensions that allow me to run individual tests and suites easily.  I would highly recommend Gallio (formally mbUnit) for unit testing and (unfortunately not free) NCover for code coverage.  NUnit but NCover is only part of the answer as it isn't free. I've asked elsewhere about that.  NUnit for sure.  VS2008 Professional has the Team System unit testing functionality baked in.  NUnit would be it.  1 Nunit 2 NCover or 3 PartCover (I never used it) NCover is not free unfortunately :(  Regarding your question about unit test frameworks: NUnit 1.0 was a direct port of JUnit. NUnit 2.0 moved away from JUnit syntax in order to take advantage of the .NET platform. xUnit.net is a newer unit test framework (from Jim Newkirk - one of the NUnit 2.0 developers - and Brad Wilson) that states as a goal exposing ""advances in other unit test library implementations that have not really surfaced in .NET"" which I read as ""keeping up with JUnit.""  I'd install: NUnit for your Unit testing framework http://www.nunit.org/index.php Test driven.net for runing your tests http://www.testdriven.net/ Rhino Mocks as your mockign framework http://ayende.com/projects/rhino-mocks.aspx As and aside I find it odd that the NUnit guys seem to be using php to host their homepage...  Unit test framework: NUnit Unit test runner: Various but personally I like the one in ReSharper. (ReSharper costs money but is easily worth it for the various productivity improvements.) Coverage: NCover (I think this used to be free but it now costs money. Hmm.)"
546,A,Question about DBUNIT and Junit I have a database process written in PL/SQL that i would like to test using DBUNIT. The pl/sql program processes data from one table and generates new data into a new table. In some cases it also updates fields on the original table. I am a bit confused in how i can use dbunit to test this. Reading up on it it looks like i have to specify the data in an xml file but i cant figure out how to structure the xml files. Thinking about it i think i would need the following data files xml file containing data for tableA xml file containing data for tableB xml file containing data for updated tableA The workflow is something like this Load tableA the pl/sql process will process the data and generate the results in tableB and update some fields in tableA Load xml file for tableB and compare the results. I am slightly confused as to how dbunit can help with this. For example is the xml data loaded into memory and then compared to what is in TableB? how would i compare the updates to tableA? Please also do let me know if you can suggest a better alternative or approach to do this. Thanks You might also want to look at utPL/SQL which is similar to frameworks such as JUnit. The dataset(s) in the XML file(s) can serve (at least) two purposes: to initialize the table(s) used in the tests. I.e. before executing the tests in the setup method DbUnit is called to load the contents of the XML file(s) and insert them into the desired table(s). Then the tests have the data to work on to verify the contents of table(s) after the tests. This introduction seems to explain the how-tos clearly.
547,A,Unit testing for safe publication How would you unittest a safe-publication guarantee in Java? To be concrete: I have a Cache interface that has a method getOrLoad(K key ObjectLoader loader). Thing is if a Cache cannot find an object for the given key then it must load it from the ObjectLoader instance. However the Cache is required to guarantee that the act of loading an object from the loader and putting it into the Cache constitutes a safe publication. I'm now in the midst of writing a generic junit test for this Cache interface and I wonder how I would test that the Cache implementations adhere to this safe publication guarantee. Any ideas? The actual code is in the test-systest module part of the code repository in case you want to poke at the real files. I discovered a JavaOne presentation by Bill Pugh Brian Goetz and Cliff Click on the subject of testing concurrent code. They suggested this approach witch I think is the best I've heard: A number of producers create stateful and thread-unsafe objects with a state-dependant hashCode implementation. As the objects are sendt through the supposed synchronizatoin point the hashCodes are summed up (thread-locally). Likewise the consumers on the other side of the gate sum up the hashCodes. At the end of the test we sum up all the results for the producers and consumers respectively. If the two sums are equal the test passes. We could also use XOR as an alternative to sum. In fact any commutative operation will do. Just keep in mind that the test harness itself must not introduce any additional synchronization.  Actually getting an error due to unsafe publication is very difficult (if anyone knows how let me know). Static analysis is your best bet for an automated solution. I would stick to code review and not worrying about it unduly. Maybe you don't need someone who is a better fit for reviewing concurrent code than yourself but just someone who you can explain to why you think you're code is correct. This has helped me a lot of times. The trouble with static analysis and code review in this case is the number of different implementations I intend for this interface. And the allowance of 3'rd party implementations. Also code review that will have to be done by you lot on the internat. I don't personally know anyone who is a better fit for reviewing concurrent code than myself :-/ That would usually be good enough but in this particular case I have an interface that promises and mandates that implementations make safe-publication guarantees. So I'd like to write a test to the interface that can check all the implementations I can throw at it.  Maybe you can use ConTest to at least give you a little more confidence that your code is correct. You'll need to implement a couple of tests that run several threads concurrently. ConTest will then increase the probability that a concurrency bug is actually revealed by instrumenting byte code (adding heuristically-controlled conditional sleep and yield instructions).
548,A,Junit with .Net Technologies Is it possible for us to integrate the junit test cases for an application for which we are developing using the ASP.Net Platform? Since I am from Java j2ee Tech background have a very less knowledge on .Net Technology You can use NUnit: http://www.nunit.org/ So that means we cant use Junit and instead we can use Nunit? Yes I was a bit cryptic about that. Sorry. :) If you want to test your .net code you must use a .net unit test library. NUnit is very similar to JUnit so it should be the best solution for you.
549,A,"Why is a junit test that is skipped because of an assumption failure is not reported as skipped? I use junit assumptions to decide whether to run a test or not. Tests where the assumption fails are ignored/skipped by the junit framework. I wonder why skipped tests are not reported as 'skipped'? Please have a look at the example: import static org.junit.Assert.fail; import org.junit.Assume; import org.junit.Test; public class AssumptionTest { @Test public void skipAssumptionFailureTest() { Assume.assumeTrue(""foo"".equals(""bar"")); fail(""This should not be reached!""); } } Running this test in a maven project results in: Running AssumptionTest Tests run: 1 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.015 sec I would prefer to have this test reported as 'skipped'. Is there any chance to achieve this? (junit 4.8.1; maven 2.2.1; java 1.6.0_21) Spring brings another variant of an Ignore annotation one that is able to decide at runtime whether to ignore a test: @IfProfileValue. In combination with a user supplied ProfileValueSource in a @ProfileValueSourceConfiguration you can do some limited tricks. See Spring Test Documentation. But it is not the same as being able to perform some test code and in the middle of the test method decide to ignore the test as you can with assumptions. Good remark - surely helpful for some use cases. Note: Spring `@IfProfileValue` only **skips/ignores** the test if the name and value doesn't match and it doesn't **not-run** it. If you want to implement JUnit Categories type of implementation with this you may need to end up extending `SpringJUnit4ClassRunner#runChild` to _not-run_ instead of _ignoring_. This mainly matters if its results are in jenkins.  You can't do this out-of-the-box as the only way to skip tests is with the @Ignore annotation. However I found a blog post which might just be what you are looking for: Thanks for the response. Too bad that this does not work out of the box. Implementing a custom runner will not work for me - I normally already use a custom runner (SpringJUnit4ClassRunner) in my tests which I will not extend. I would have preferred a build in solution.  While previous answers are good on their own I'd like to address the ""WHY"" question. The problem stemmed from the fact how tests failed / ignored were counted. When assumptions were added usual counts were kinda skewered (assumption can IGNORE the tests half-way through it's run). Tools runners had problems with that as well causing messy reporting. See here for Surefire dev conversation with JUnit people and here for related Eclipse bug. Right now this ain't a problem in Maven and it should not be in newer Eclipses. I use STS based on eclipse.buildId = 2.9.2.201205071000-RELEASE and I still experience this behaviour. EDIT: reference type links did not work."
550,A,How to execute ant using java and captured the output? I have an ant build file that contains JUnit test suite that I would like to execute. Currently I just right click and run the build file from Eclipse. I want to write a java code that can execute the ant build file automatically. So I just run the code and ant will be executed. Second is I want to capture the test result. Currently the result is based on JUnit HTML report. I want to make my own simple test report. I read there is JUnitResultFormatter but I can't find the instructional step by step how to use it. Can anyone point me the reference? The easiest way to do that is to use the JunitCore class from java. It is not advised to call the main from ant directly see the Junit Faq and http://www.answerspice.com/c119/1497833/how-do-i-run-junit-tests-from-inside-my-java-application. It is very common to define a main like this for each test case to be able to run the tests individually from command line. I usually also change the logging settings in those methods to get more information when I run a single test manually than from within ant. In order then to create a custom report you will have to implement a RunListener that creates your report and register it as described in the javadoc: public void main(String... args) { JUnitCore core= new JUnitCore(); core.addListener(new RingingListener()); core.run(MyTestClass.class); } Your listener will then be called before and after each test run and passed descriptive information about the test that is about to run and how the test went once it is done. sorry to ask again but how do I specify the path to the test class if its not in the same folder? These are my folder structure: SampleTest -> build -->test --->classes ---->test.diagram1_Suite1.class (this the class file) ->test -->RunTest.class (this is the executor) thanks again See in the code sample the line `core.run(MyTestClass.class);`. The `JunitCore` loads tests based on class names. You simply have to reference the fully qualified class name of your test and make sure this class is in the classpath. thanks for the detail :) Is it by doing this it does eliminate the need for ant build file because it specifically call the test class? Yes with this you will be able to call the tests without `ant`. You can also mix both and still use ant to automate running all your tests for example in continuous integration. :( the code works fine but I found out that I need to run the ant file and can't use this way to execute my JUnit (because my java code is not in a java project). any idea how to overcome this? I'm not sure to understand the problem here. You can run your tests from ant without the junit task if you need to using the `java` task.
551,A,"Junit throw warning while still passing test I'm working on a project at the moment that we're using junit to test but as its still fairly early stages a lot of features aren't yet implemented though they already have tests written for them this means these tests (obviously) always fail I was wondering if anyone knew a way to get JUnit to pass the test while displaying a warning. preferably with a customizable message so we can note that the feature is not yet implemented. The point of this being that we want to code to compile only if all tests pass and at the moment this simply isn't possible. I realise we can just comment out or remove the problem tests but we then run the risk of forgetting to add them back in later. You can @Ignore in front of the test method. It then depends on the test runner what kind of output you get. You can get something like successfully run 25 tests and 6 tests are ignored... All further things depend on how you want to run the unit tests. This doesn't seem to generate any kind of indication of ignored tests for me but thanks all the same definitely something to keep in mind seems that this is te closest I could get thanks mate  Would org.junit.Assume do what you need?  Most JUnit runners I've seen will give you one of four statuses for a test case: passed failed had errors or ignored. As had errors (threw an Exception) is reported in a similar fashion as failed (""red result"") there isn't a good way to generate a warning and still have a ""green result"" - I assume that is what you are looking for? As mentioned by Janusz you can use the @Ignore attribute to ignore a test case which can contain a message as well: @Ignore(""disabled until implementation is finished"") public void testMe() { //do something } Most runners will not list them explicitely in the results but you are able to look for any ignored test cases automatically using tools by looking for the @Ignore attribute and generate a report afterwards which lists all test cases which have been skipped and the reason (given in the attribute) why.  You could throw a NotImplementedException in your new methods: Apache Commons NotImplementedException e.g. public String getName() { throw new NotImplementedException(""Oh so soon...""); } and then specify in your JUnit test that it should expect this exception to be thrown: @Test (expected=NotImplementedException.class) public void testGetName() { myKingdom.getName(); } That serves as a reminder to everyone to implement that functionality of course when they do add it (or remove it by mistake) the unit test will notice that the exception is no longer thrown and throw a big wobbly. This is what I'm currently doing but since it just happily passes the test with no indication that anything is out of the normal (because nothing is) it can be fairly easy to miss the fact that the feature is still not implemented. But thanks anyway  Unit testing works best when the tests are full i.e. they do something and when the fail something is wrong. they are thus not designed to ""half fail"" or ""pass with a warning"". This is because unit tests are designed to test. Thus when they fail something is wrong. Wouldn't it thus be better to not use tests to track what is not implemented yet but rather use tests to test the functionality that is there. So those tests that are currently testing functionality that is not there should probably not be run until that functionality is there.  I know i'm about 2 years out but I would comment/ignore the tests you don't need and as a process make sure public code reviews also cover unit tests along with the submitted code. Which should improve your testing methodologies & coverage overall."
552,A,Upgrading to JUnit4 and keeping legacy JUnit 3 tests and test suites by running them together I was surprised not to find the answer so far. If I am missing something basic I will be more than happy to know that. There is a large legacy code base that was upgraded to Java 6 (from 1.4). Large number of JUnit 3 tests are present in the code and are organized into test suite that runs successfully with JUnit 4 default runner in Eclipse. Now I am adding new tests that are pure JUnit 4 tests (annotations no TestCase etc.). What would be a way of running both old JUnit 3 test suite and new JUnit 4 tests together? I would answer that we do this with an AntTask but I think you mean Eclipse only. If so please tag for eclipse. @shoover - No I don't mean Eclipse only. I am looking for generic solution that would run tests with java only. Just use 'JUnit4' test runner in your run configuration. JUnit4 binaries have a backward compatibility layer that allows it to have both JUnit3 and JUnit4 style classes in the same test suite. For command line builds just use JUnit4 jars instead of JUnit3. This was done specifically to ease migration that you are doing now. Also it works fine in my project. Alex thank u but I was looking for more concrete solution - thanks to ur answer I believe I came up with one. I'll add it as an answer and will give u a credit for it.  The @RunWith(Suite.class) gives me opportunity to combine both JUnit 4 and JUnit 3 tests and test cases together: @RunWith(Suite.class) @Suite.SuiteClasses({ ExampleOfJunit3TestSuite.class ExampleOfJUnit3TestCase.class ExampleOfJUnit4TestSuite.class ExampleOfJUnit4TestCase.class}) public class BothJUnit4and3TestSuite { } The BothJUnit4and3TestSuite runs all tests and test suites listed in @Suite.SuiteClasses.
553,A,"AssertContains on strings in jUnit Is there a nicer way to write in jUnit String x = ""foo bar""; Assert.assertTrue(x.contains(""foo"")); Use hamcrest Matcher containsString() // Hamcrest assertion assertThat(person.getName() containsString(""myName"")); // Error Message java.lang.AssertionError: Expected: a string containing ""myName"" got: ""some other name"" You can optional add an even more detail error message. // Hamcrest assertion with custom error message assertThat(""my error message"" person.getName() containsString(""myName"")); // Error Message java.lang.AssertionError: my error message Expected: a string containing ""myName"" got: ""some other name"" Posted my answer to a duplicate question here  If you add in Hamcrest and JUnit4 you could do: String x = ""foo bar""; Assert.assertThat(x CoreMatchers.containsString(""foo"")); With some static imports it looks a lot better: assertThat(x containsString(""foo"")); The static imports needed would be: import static org.junit.Assert.assertThat; import static org.hamcrest.CoreMatchers.containsString; A code sample always wins :) It doesn't compile :) Be sure you're using `org.junit.Assert` versus `junit.framework.Assert` as the latter doesn't have the Hamcrest Matcher `assertThat()` I think when running JUnit 4.10 the class to use is org.junit.matchers.JUnitMatchers e.g.: assertThat(""something"" JUnitMatchers.containsString(""some"")); The failure message for a failing `assertThat` is way more helpful then an `assertTrue` static imports needed are `import static org.junit.Assert.assertThat; import static org.hamcrest.CoreMatchers.containsString;` - just to save someone from trouble ... and `org.hamcrest.Matchers.containsString;` in the latest api in the `hamcrest-library` dependency.  Use the new assertThat syntax together with Hamcrest. It is available starting with JUnit 4.4.  use fest assert 2.0 whenever possible EDIT: assertj may have more assertions (a fork) assertThat(x).contains(""foo"");"
554,A,"Help writing JUnit for JDBC I created one classin which i am inserting values into SQL as follows:  public class ABC{ some code here.......... ............... public void insertUsers(String firstNameString lastNameString location){ pre.setString(1firstName); I created test class for this class. I want to write test case for this method insertUsers()using assert statement. how to write assert statement for above method. A couple of remarks. I'd use the standard assert during development only. It will check a condition and throw a runtime exception if the condition evaluates to false. If you expect illegal arguments than it's much better to add some ""normal"" code to the method to handle those values or throw an IllegalArgumenException and write log entry. Do not close the connection in this method! Do it only when you open/create the connection in the very same method. In larger applications you won't be able find out who closed the connection after a while. If the caller of insertUsers opened the connection the caller should close it itself! (more help possible if you tell us what exactly you want to test - the method parameters or if the insert was a success) Thank you Andreas_D.I want to test whether insert is succussful or not using assert() statementtell me how assert statement is written.  When doing unit testing one should avoid accessing external resources such as databases filesystems network etc. This is to keep the tests in memory (fast) but also isolated from external failures. You only want to test a specific part of some functionality in e.g. a class nothing else. What this means for you is that the conn variable (I assume is the db connection) needs to be mocked out. You can do this easily with something like dependency injection which means you pass in things into your class when constructing it. In this case you would pass in an interface which has the necessary functions conn uses. Then in production you pass in the real db connection object while in test you pass in a mock which you control. Hence you can then check that ABC calls and does what you expect it to do with conn. The same goes for pre you're using. You can see it like this: I would like to test class ABC and in order to do that I need to see how it uses pre and conn so I replace those with my own test implementations I can check after doing something with ABC. In order to specifically help you with what you're doing you need to show what pre is and tell us what you intend to test.  I wouldnt test the insertion of the data to the database actually its not performant to access database during unittesting this can be covered threw automated functional GUI testing tools of your application. what you may want to test is the generation of the expected queries this can realised if you seperate the geenration and the execution of the statements you will be able to compare generated statements with expected ones without having to access you database from the unitest. Thank you O.DIt helped me  Well if you really want to test updating your database you can do that. Usually people follow one of the below two approaches - Use Spring AbstractTransactionalDataSourceSpringContextTests This allows you to add any values to the database and then spring will take care and revert the values that you have inserted. Use a seperate database Just for your JUnit tests. You really dont need anything heavy. You can use something like the HSQLDB which is really a lightweight java database. This will allow you to have separate test data from your production/QA database. After the above is done(and you have run the insert statement) simply run select statement from your JUnit to get the data and then compare the previous data with the actual data."
555,A,Simulating JMS - jUnit I need to simulate JMS behavior while performing automated tests via maven/hudson. I was thinking about using some mock framework i.e. Mockito to achieve that goal but maybe there is some easier tool which can accomplish this task? I have read a little bit about ActiveMQ but from what I have found out it requires to install broker prior using it. In my case it is important to have everything run by maven only because I don't have any privileges to install anything on the build server. Thanks in advanced. You can run ActiveMQ in embedded mode - the broker starts within your application and queues are created on the fly. You just need to add activemq.jar and run few lines of code. On the other hand there is a Mockrunner library that has support for JMS - although it was designed mainly for unit tests not integration. wielkie dzieki :)
556,A,Customize execution of Junit test suite in modules I have a set of Junit 4 test classes which I want to run in multiple modules with differing BeforeClass AfterClass Before and After hooks. With @Rule injections I get only wrapping of test methods but no BeforeClass and AfterClass behaviour. Also I don't want to do this with a test runner since then I have fixed the test runner to be used for a large set of tests. Subclassing each test class in the target modules and applying the customizations there doesn't seem to be a good solution. The best would be to just declared something like this in the target modules @RunWith(Suite.class) @Suite.SuiteClasses({ investmentTests.class catalogTests.class markerTests.class }) public class AllTests { // why on earth I need this class I have no idea! } and have some environmental hooks to apply the before/after code. Have you come across a solution for this problem? do you need a class-level Rule as NamshubWriter commented? JUnit 4.9 will have class-level rules. The class level rule seems to be just what I need. Thanks for the comments. As NamshubWriter commented org.junit.ClassRule is found in JUnit4.9. @ClassRule instead of @Rule; TestRule instead of MethodRule. See the sample: public static class CustomCounter extends TestRule { public int count = 0; @Override protected Statement apply(final Statement base Description description) { return new Statement() { @Override public void evaluate() throws Throwable { count++; base.evaluate(); } }; } } public static class ExampleTestWithCustomClassRule { @ClassRule public static CustomCounter counter= new CustomCounter(); @Test public void firstTest() { assertEquals(1 counter.count); } @Test public void secondTest() { assertEquals(1 counter.count); } }
557,A,"Is there a Java unit-test framework that auto-tests getters and setters? There is a well-known debate in Java (and other communities I'm sure) whether or not trivial getter/setter methods should be tested. Usually this is with respect to code coverage. Let's agree that this is an open debate and not try to answer it here. There have been several blog posts on using Java reflection to auto-test such methods. Does any framework (e.g. jUnit) provide such a feature? e.g. An annotation that says ""this test T should auto-test all the getters/setters on class C because I assert that they are standard"". It seems to me that it would add value and if it were configurable the 'debate' would be left as an option to the user. Answering the previous comment at @me here because of my reputation: Vlookward not writing getters/setters makes no sense at all. The only options for setting private fields is to have explicit setters to set them in your constructor or to set the indirectly via other methods (functionally deferring the setter to another place). Why not use setters? Well sometimes there is no need to the field be private (Sorry if my English is not very good). Often we write our software as it was a library and we encapsulate our fields (our business logic fields) with unnecessary getters/setters. Other times that methods are actually necessary. Then there are two possibilities: 1. There is business logic inside them. Then they sould be tested but they aren't real getters/setters. I always write that logic in other classes. And the tests test that other classes not the POJO. 2. There is not. Then do not write them by hand if you can. For example an implementation for the next interface may be fully autogenerated (and also in runtime!) : interface NamedAndObservable { String getName(); void setName(String name); void addPropertyChangeListener(PropertyChangeListener listener); void addPropertyChangeListener(String propertyName PropertyChangeListener listener); } So test only what is written by hand. No matter if it is a getter/setter.  I've done something like that. A simple java class that takes an object and test all the getters and setter methods. http://sourceforge.net/projects/getterandsetter/ I do think you should avoid getter and setter methods as much as possible but as long as they're around and it takes two lines to test them it's a good thing to do it.  In the most cases setter and getter do more as only setting and getting an internal field. An Object has to check internal rules that it hold only valid values. For example are null values possible? are empty strings possible? or negative values? or a zero value? or values from a list are valid? or is there a maximal value? or is there a maximum precision on BigDecimal values? The unit test should check if the behavior correct if there invalid values. This can not be automated. If you have no logic on the setter and getter then it must be used anywhere in your application. Write a test where your object is a parameter for a more complex test. You can test it then with different values from the list. Test your business logic and not the getter and setter. The result should also a coverage of the getter and setter. The methods should be any result in your business logic also if you have only a public library. If the getter and setter have no code coverage then removed it.  I guess this library is the answer to your question it tests all the bean's initial values the setters the getters hashCode() equals() and toString(). All you have to do is define a map of default and non default property/value. It can also test objects that are beans with additional non default constructors.  I created the OpenPojo project for solving this exact problem. The project allows you to validate: Enforce Pojo coding standard (i.e. All fields private or no native variables ...etc) Enforce Pojo behaviour (i.e. setter does JUST setting no transformation etc) Validate Pojo Identity (i.e. Use annotation based equality & hashcode generation) See Tutorial My qualm is with the name: that you aren't testing POJOs you are actually testing Java Beans. [_""The term ""POJO"" is mainly used to denote a Java object which does not follow any of the major Java object models conventions or frameworks.""_](http://en.wikipedia.org/wiki/Plain_Old_Java_Object) the getter/setter is a convention. [_""Java Beans are serializable have a 0-argument constructor and allow access to properties using getter and setter methods.""_](http://en.wikipedia.org/wiki/Java_Beans).  I'm not aware of any readily available library or class that does this. This may mainly be because I don't care as I am on the side of strongly opposing such tests. So even though you asked there must be a bit of justification for this view: I doubt that autotesting getters and setters benefit your code quality or your coverage: Either these methods are used from other code (and tested there e.g. 100% covered) or not used at all (and could be removed). In the end you'll leave getters and setters in because they are used from the test but nowhere else in the application. It should be easy to write such a test e.g. with Apache Commons BeanUtils but I doubt you really need it if you have good tests otherwise. We once had to implement tests like that because the getters and setters were only used in a dependent project and as such not touched in the unit tests of their own project. Late comment - sorry: This would most likely be for code coverage reasons. You might need to do that for these reasons but I doubt you'll get quality tests from it. I can easily write crappy tests that provide 100% coverage without any benefit at all. Was that why you ""had to""?  I don't write test cases for each property but instead test all of the setters/getters in a single test case using reflection/introspector to determine the type(s). Here is a great resource that shows this: http://www.nearinfinity.com/blogs/scott_leberknight/do_you_unit_test_getters.html  I'll favor OO design over code coverage and see if I cannot move those fields to the class that needs them. So I would try to see if those getters and setters can be removed as suggested before. getters and setters are breaking encapsulation.  Unitils does this w/ the static method assertRefEquals.  I am trying out openpojo I have kicked the tires and it seems to do the job. It allows you to check all the pojo's in your project. It seems to check the best practices on pojo's Check this tutorial for a quick start Tutorial"
558,A,JUnit Report single page XSLT for email I have a Junit process that runs a bunch of data integrity tests every night and I would like to have it email the results in a nicely formatted HTML email. The issue is the HTML formatter built into JUnit uses frames and an external stylesheet so it is not appropriate for email. The plain formatter has the results buried in the midst of a bunch of otherwise worthless data. I thought it would be simple to find a XSLT stylesheet that formatted the XML output into an email-friendly format but after a couple hours of googling I have not been able to find one. If any of you have a email-friendly Junit formatter I'd be eternally grateful. @Dimitre: Besides JUnit default stylesheet use some extension elements this is an XSLT question. You could say it's a very specific one like... http://stackoverflow.com/questions/299235/convert-xhtml-to-word-ml as example. If Jhon P post some input sample for JUnit XML output maybe someone would use a lot of free time to write some stylesheet... @Alejandro: I retagged it because I didn't want to vote to close it. As of now this question is quite incomplete and thus isn't a real question. Any question in this tag that lacks either or both source XML document and/or desired result is incomplete. @Dimitre: I completely agree with you: this is a question that has hardly answer. @Dimitre: I found an option on the junitreport task that will compile the test results into a single html file using junit-noframes.xsl. The problem is it sticks all the
559,A,Unit testing a JAX-RS Web Service? I'm currently looking for ways to create automated tests for a JAX-RS (Java API for RESTful Web Services) based web service. I basically need a way to send it certain inputs and verify that I get the expected responses. I'd prefer to do this via JUnit but I'm not sure how that can be achieved. What approach do you use to test your web-services? Update: As entzik pointed out decoupling the web service from the business logic allows me to unit test the business logic. However I also want to test for the correct HTTP status codes etc. Good question - however I'd say that if you're testing over HTTP then it strikes me that this is integration testing. You can try out REST Assured which makes it very simple to test REST services and validating the response in Java (using JUnit or TestNG). I voted up your post cause the library looked good but they sure use a lot of dependent jars...  Though its too late from the date of posting the question thought this might be useful for others who have a similar question. Jersey comes with a test framework called the Jersey Test Framework which allows you to test your RESTful Web Service including the response status codes. You can use it to run your tests on lightweight containers like Grizzly HTTPServer and/or EmbeddedGlassFish. Also the framework could be used to run your tests on a regular web container like GlassFish or Tomcat.  An important thing to do is to independently test your business logic I certainly would not assume that the person who wrote the JAX-RS code and is looking to unit test the interface is somehow for some bizarre inexplicable reason oblivious to the notion that he or she can unit testing other parts of the program including business logic classes. It's hardly helpful to state the obvious and the point was repeatedly made that the responses need to be tested too. Both Jersey and RESTEasy have client applications and in the case of RESTEasy you can use the same annoations (even factor out annotated interface and use on the client and server side of your tests). REST not what this service can do for you; REST what you can do for this service.  You probably wrote some java code that implements your business logic and then you have generated the web services end point for it. An important thing to do is to independently test your business logic. Since it's pure java code you can do that with regular JUnit tests. Now since the web services part is just an end point what you want to make sure is that the generated plumbing (stubs etc) are in sync with your java code. you can do that by writing JUnit tests that invoke the generated web service java clients. This will let you know when you change your java signatures without updating the web services stuff. If your web services plumbing is automatically generated by your build system at every build then it may not be necessary to test the end points (assuming it's all properly generated). Depends on your level of paranoia. You are quite right although I also need to test the actual HTTP responses that get returned in particular the HTTP status codes.  You can find an example here. Link is broken.  Jersey comes with a great RESTful client API that makes writing unit tests really easy. See the unit tests in the examples that ship with Jersey. We use this approach to test the REST support in Apache Camel if you are interested the test cases are here re: now bad link You can find the examples mentioned in the jersey /samples that show unit tests basically by using jersey's consumers to consume web resources. http://download.java.net/maven/2/com/sun/jersey/samples/bookstore/1.1.5-ea-SNAPSHOT  I use Apache's HTTPClient (http://hc.apache.org/) to call Restful Services. The HTTP Client library allows you to easily perform get post or whatever other operation you need. If your service uses JAXB for xml binding you can create a JAXBContext to serialize and deserialize inputs and outputs from the HTTP request.
560,A,"How do I get Eclipse to run all of my Groovy unit tests? I have an Eclipse project with many unit tests written in Groovy. I can run the tests in each individual class using Eclipse's GUnit run configuration. I open this configuration select the ""Run a single test"" radio button and select the class whose tests I want to run. This works fine however I want to run all of the tests in the the project at once. When I open my run configuration select the ""Run all tests in the selected project package or source folder"" radio button and select my project Eclipse gives the error ""No tests found with test runner 'JUnit 4'."" How do I get Eclipse to run all of the tests in my project? Please upgrade to the Alpha version of the groovy-eclipse plugin V2. You can run all unit tests in a project by right-clicking and selecting Run as->JUnit. In the new version of the plugin Junit tests are treated identically whether they are written in Java or Groovy.  Figured this out with the help of the documentation. Groovy has a utility class groovy.util.AllTestSuite. I created a new GUnit run configuration with this as my test class and added VM arguments like this: -Dgroovy.test.dir=src -Dgroovy.test.pattern=**/*Tests.groovy I also had to add ant.jar and ant-launcher.jar to the configuration classpath. That configuration happily runs all my tests."
561,A,"How to deal with the test data in Junit? In TDD(Test Driven Development) development process how to deal with the test data? Assumption that a scenario parse a log file to get the needed column. For a strong test How do I prepare the test data? And is it properly for me locate such files to the test class files? Maven for example uses a convention for folder structures that takes care of test data: src main java <-- java source files of main application resources <-- resource files for application (logger config etc) test java <-- test suites and classes resources <-- additional resources for testing If you use maven for building you'll want to place the test resources in the right folder if your building with something different you may want to use this structure as it is more than just a maven convention to my opinion it's close to 'best practise'. OK. I see. Thx. Is this a test convention in Maven? Maven simplifies the build process for Java applications and makes big use of conventions like folders structures. If a project follows those conventions then the build files will be pretty small because maven 'nows' what to do based on the files and folders it sees. (just in brief)  getClass().getClassLoader().getResourceAsStream(""....xml""); inside the test worked for me. But getClass().getResourceAsStream(""....xml""); didn't worked. Don't know why but maybe it some others.  When my test data must be an external file - a situation I try to avoid but can't always - I put it into a reserved test-data directory at the same level as my project and use getClass().getClassLoader().getResourceAsStream(path) to read it. The test-data directory isn't a requirement just a convenience. But try to avoid needing to do this; as @philippe points out it's almost always nicer to have the values hard-coded in the tests right where you can see them.  Hard code them in the tests so that they are close to the tests that use them making the test more readable. Create the test data from a real log file. Write a list of the tests intended to be written tackle them one by one and tick them off once they pass.  Another option is to mock out your data eliminating any dependency on external sources. This way it's easy to test various data conditions without having to have multiple instances of external test data. I then generally use full-fledged integration tests for lightweight smoke testing."
562,A,Running JUnit through Eclipse I run my JUnit tests through Eclipse (Helios 1.3.0) through an ant build file and an external ant builder. I was wondering if it's possible to use Eclipes's JUnit UI when running them so that I can see the 'green' or 'red' bar in there instead of seeing success/failure messages in the console. Any pointers are greatly appreciated. Thanks! If you have JUnit reference for a project I think you can right click on a project and select Run As > JUnit Test and that will run all the tests in the Project. Check out this post thanks for a quick reply. My Eclipse doesn't have a JUnit reference for them. I'm also using Cobertura through ant build file and don't want to miss on generating coverage reports. @shrini1000 you could try http://www.eclemma.org/ for code coverage. it's more visual than cobertura reports.  Eclipse: Writing and running JUnit tests @卢声远 Shengyuan Lu: thanks for your reply; but this is not what I'm looking for. The reason I'm running JUnit test cases through ant builder is so I won't have to create and maintain my own test suites. I just use '' and point it to '*Test.java' from my tests folder and all tests get automatically run. I just want to see the result in Eclipse JUnit UI instead of in its console.  If you must run your tests via ant then there is no direct way to see the progress in the JUnit view. However if you are using the JUnit ant task and set the output format of your test results to xml : Then you can open this file up in the JUnit window. Click on the button on the far right and select Import. Then navigate to the file you want to open:
563,A,easiest Automated testing tool in Java I want to learn quickly like in an few hours an automated testing tools. Does somebody know any automated testing tool which is very easy and how can we do same automation in java. I want to use Junit I think you are on the right track JUnit is definitely your best bet for Java testing automation. You can learn more about JUnit and even build a quick example application here. Other tools to look at: Cactus EasyMock Fit Others that can be viewed here.
564,A,When should we use Mockery vs JUnit4Mockery? If writing a Java unit test with mocking using JMock should we use Mockery context = new Mockery() or Mockery context = new JUnit4Mockery() What is the difference between the two and when should we use which? Better yet per http://incubator.apache.org/isis/core/testsupport/apidocs/org/jmock/integration/junit4/JUnitRuleMockery.html use @Rule and avoid @RunWith which you might need for some other system: public class ATestWithSatisfiedExpectations { @Rule public final JUnitRuleMockery context = new JUnitRuleMockery(); private final Runnable runnable = context.mock(Runnable.class); @Test public void doesSatisfyExpectations() { context.checking(new Expectations() { { oneOf(runnable).run(); } }); runnable.run(); } }  When using JMock with JUnit 4 you can avoid some boilerplate code by taking advantage of the JMock test runner. When you do this you must use the JUnit4Mockery instead of the regular Mockery. Here is how you'd structure a JUnit 4 test: @RunWith(JMock.class) public void SomeTest() { Mockery context = new JUnit4Mockery(); } The main advantage is there is no need to call assertIsSatisfied in each test it is called automatically after each test. I think that was my confusion too.  @Rhys It's not the JUnit4Mockery that replaces the need to call assertIsSatisfied its the JMock.class (combined with the @RunWith). You wont need to call assertIsSatisfied when you create a regular Mockery. The JUnit4Mockery translates errors. By default expectation exceptions are reported in Junit as ExpectationError so for example using Mockery context = new Mockery(); you'll get unexpected invocation: bar.bar() no expectations specified: did you... - forget to start an expectation with a cardinality clause? - call a mocked method to specify the parameter of an expectation? and using Mockery context = new JUnit4Mockery(); you'll get java.lang.AssertionError: unexpected invocation: bar.bar() no expectations specified: did you... - forget to start an expectation with a cardinality clause? - call a mocked method to specify the parameter of an expectation? what happened before this: nothing! The JUnit4Mockery converted the ExpectationError to an java.lang.AssertionError which JUnit deals with. Net result is that it'll show up in your JUnit report as an failure (using JUnit4Mockery) rather than an error.
565,A,"using eclipse template to create test cases Often do I find myself creating the same unit tests methods to getters\setters c'tors and Object methods (hashCode equals and toString). What I'm trying to achieve with the help of Eclipse IDE is automation of this procedure. consider this example: public Class Person { private String id; private String name; public Person(String id String name){ this.id = id; this.name = name; } public String getId() { return id; } public void setId(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } @override public int hashCode(){ ... } public boolean equals(Person other){ ... } public String toString(){ ... } /* this class may implement other logic which is irrelevant for the sake of question */ } The unit test class will look something like this: public class PersonTest extends TestCase { @override public void setup() { Person p1 = new Person(""1""""Dave""); Person p2 = new Person(""2""""David""); } @override public void tearDown() { Person p1 = null; Person p2 = null; } public void testGetId() { p1.setId(""11""); assertEquals(""Incorrect ID: "" ""11"" p1.getId()); } public void testGetName() { /* same as above */ } public void testEquals_NotEquals() { /* verify that differently initialized instances are not equals */ } public void testEquals_Equals() { /* verify that an object is equals to itself*/ } public void testHashCode_Valid() { /* verify that an object has the same hashcode as a similar object*/ } public void testHashCode_NotValid() { /* verify that different objects has different hashcodes*/ } public void testToString() { /* verify that all properties exist in the output*/ } } This skeleton is similar to the vast majority of classes created. can it be automated with Eclipse? Have a look at Fast Code. It is an eclipse plugin that provides very nice feature of templating stuff which is what you seem to be looking for. On the documentation page look for Create Unit Test section. A very useful feature of this plugin is to create unit tests automatically. Unit tests can be of type Junit 3 Junit 4 or TestNG. For Junit 4 or TestNG tests appropriate annotations will be automatically added. One needs to configure it just once. You may also find this Stack Overflow discussion useful. Hope that helps. Thanks Nilesh I will have a look This plugin is great for creating unit tests.  Unit tests are meant to show that an Object's behaviour is conforming to it's expected behaviour. They are not meant to make sure that the Java language is working correctly. What you have here is a fancy data structure with no behaviour. In that case every assignment is mediated by a method call and every dereference is also mediated by a method call. Since Object Oriented programming is ""data + behaviour"" = objects and this code lacks behaviour it's a candidate for being called non-object-oriented code. Sometimes Java uses non-object-oriented classes to facilitate transfer of information. The class guarantees that all information gets transferred as one unit when doing serialization. So having such a class isn't an indicator that the code is wrong; however if you run into too many classes like this then something is very wrong. One key element of testing is that it's not really a test if the test cannot fail. If the test cannot fail it's just busywork. Assuming that one of these fields cannot be null then the setter might look like public void setName(String name) { if (name == null) throw new IllegalArgumentException(""name cannot be null""); this.name = name; } And then you have something to test. Otherwise your just checking to see if the assignment operator failed. As an aside if the assignment operator failed then I'd wager that the JVM is going to come down pretty hard sooner (rather than later) and you can't trust your tests to report correctly either. Edwin I don't mean to test java. It worked fine long before I started using it... getters are usually logic free I can understand why there no need to check them (other than increasing coverage). However Object methods are bound for mistakes. especially equals and hashCode. Same goes for c'tor. Having a unit test skeleton with method stubs prepared javadoc pointing to the tested methods is a great time saver. I was hoping to get a link to an existing code template as I suspect that this issue bugs others. Assaf is correct in stating testing equals and hashCode is important. These implementations need to insure they work in concert and that they support their respective contracts. With regards to the 'Otherwise your just checking to see if the assignment operator failed' statement. Actually you are testing if the setter method is coded correctly. Many times through the use of IDE code generation shortcuts or copy-n-paste even these simple methods get coded incorrectly."
566,A,"Android JUnit: Define a different Application subclass So for my normal Android project I have the following in AndroidManifest.xml: <application android:name="".utilities.App"" ...> .... </application> And then I have my App class: public class App extends Application { .... } And then I have an Android JUnit Test project associated with the Android project. Everything is all fine and dandy and I can write JUnit tests. However I'm trying to run code coverage with my JUnit tests and I'm getting bloated results. The reason is because my App class gets called and initialized as if my application were actually started. I do not want my custom App class to execute when I run the JUnit tests or code coverage. Any setup I would need for the JUnit tests will go in the appropriate JUnit setup() method. Is there any way I can prevent it from executing my custom App class or a way that any classes/methods/lines that are executed due to the creation of my App class aren't counted towards the code coverage? You didn't tell us how are you generating code coverage reports. Are you using an EMMA instrumented android build or something else ? There is only one way that you can run code coverage with Android AFAIK and that is the EMMA utility. I'm just running the ""ant coverage"" command that Android provides. did you find another solution? A temporary solution that I've found will work unless someone has any better ideas. Go into the main Android project's AndroidManifest.xml. Change the android:name attribute from "".utilities.App"" to ""android.app.Application"" Run the code coverage utility/JUnit tests Change the android:name attribute back from ""android.app.Application"" to "".utilities.App"" Re-deploy the app onto the device (so that it uses the right Application class when it runs external to the code coverage/JUnit tests) I'm sure the real solution is to automate this process but I'm too lazy to do so and it just feels hackish and wrong. But at least it's a workaround unless someone has any ideas."
567,A,"JUnit + Maven + Eclipse: Why @BeforeClass does not work? I am using JUnit 4 Maven 2 and latest Eclipse. Problem is simple: I would like to perform some setup (connecting to a database) before my tests are executed. I tried @BeforeClass in many different locations but Eclipse and Maven are ignoring this. Any help on accomplishing this initial setup? Thanks! public abstract class BaseTestCase extends TestCase { @BeforeClass public static void doBeforeClass() throws Exception { System.out.println(""No good @BeforeClass""); // DO THE DATABASE SETUP } } Now the tests extending BaseTestCase: public class LoginActionTest extends BaseTestCase { @Test public void testNothing() { System.out.println(""TEST HERE""); assertEquals(true true); } } Maven and Eclipse just ignore my @BeforeClass ??? Any other way to perform setup before tests? Can you please confirm how you launch your test case from within Eclipse? Are you using the JUnit4? I am using JUnit 4. I am launching with Run As... -> JUnit Test... The problem is related to extending TestCase. If you give up extending that class then you are fine. You actually don't need this class as you can import the Assert class to perform the checking... Don't extend TestCase and everything works as expected... I have similar problem and I fixed it by specifying surefile and junit versions explisitly: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.8.1</version> <dependencies> <dependency> <groupId>org.apache.maven.surefire</groupId> <artifactId>surefire-junit47</artifactId> <version>2.8.1</version> </dependency> </dependencies> <configuration> <parallel>methods</parallel> <threadCount>10</threadCount> <excludes> <exclude>**/*IntegrationTest.java</exclude> </excludes> </configuration> </plugin> More info is here: http://maven.apache.org/plugins/maven-surefire-plugin/examples/junit.html It seems junit 3.8.1 version is used transiently through maven-resources-plugin and plexus-container-default. You can print dependency tree by calling mvn dependency:tree. I think there's no other way to make surefire use junit 4.  I suspect that you are running with JUnit 3. Try renaming your test to something which does not start with ""test"". If the test is no longer executing you are using JUnit 3 (which assumes that test methods are methods which starts with ""test""). Please post your Eclipse launch config.  Sergio you were right about extending TestCase causing the problem. If you extend TestCase JUnit treats your test class as an old (pre JUnit 4 class) and picks org.junit.internal.runners.JUnit38ClassRunner to run it. JUnit38ClassRunner does not know about @BeforeClass annotation. Check out source code of runnerForClass method of AllDefaultPossibilitiesBuilder and runnerForClass method of JUnit3Builder for more details. Note: This problem is not related to Eclipse or Maven. Yeah. I gave up extending TestCase and everything works fine."
568,A,"Basic jUnit Questions I was testing a String multiplier class with a multiply() method that takes 2 numbers as inputs (as String) and returns the result number (as String) public String multiply(String num1 String num2); I have done the implementation and created a test class with the following test cases involving the input String parameter as valid numbers characters special symbol empty string Null value 0 Negative number float Boundary values Numbers that are valid but their product is out of range numbers will + sign (+23) Now my questions are these: I'd like to know if ""each and every"" assertEquals() should be in it's own test method? Or can I group similar test cases like testInvalidArguments() to contains all asserts involving invalid characters since ALL of them throw the same NumberFormatException ? If testing an input value like character (""a"") do I need to include test cases for ALL scenarios? ""a"" as the first argument ""a"" as the second argument ""a"" and ""b"" as the 2 arguments As per my understanding the benefit of these unit tests is to find out the cases where the input from a user might fail and result in an exception. And then we can give the user with a meaningful message (asking them to provide valid input) instead of an exception. Is that the correct? And is it the only benefit? Are the 11 test cases mentioned above sufficient? Did I miss something? Did I overdo? When is enough? Following from the above point have I successfully tested the multiply() method? 1) It's best to keep your tests small and focused. That way when a test fails it's clear why the test failed. This usually results in a single assertion per test but not always. However instead of hand-coding a test for each individual ""invalid scenario"" you might want to take a look at JUnit 4.4 Theories (see the JUnit 4.4 release notes and this blog post) or the JUnit Parameterized test runner. Parametrized tests and Theories are perfect for ""calculation"" methods like this one. In addition to keep things organized I might make two test classes one for ""good"" inputs and one for ""bad"" inputs. 2) You only need to include the test cases that you think are most likely to expose any bugs in your code not all possible combinations of all inputs (that would be impossible as WizardOfOdds points out in his comments). The three sets that you proposed are good ones but I probably wouldn't test more than those three. Using theories or parametrized tests however would allow you to add even more scenarios. 3) There are many benefits to writing unit tests not just the one you mention. Some other benefits include: Confidence in your code - You have a high decree of certainty that your code is correct. Confidence to Refactor - you can refactor your code and know that if you break something your tests will tell you. Regressions - You will know right away if a change in one part of the system breaks this particular method unintentionally. Completeness - The tests forced you to think about the possible inputs your method can receive and how the method should respond. 5) It sounds like you did a good job with coming up with possible test scenarios. I think you got all the important ones. @WizardofOdds - Good comments. Of course I know that it is impossible to test every possible inupt hence my answer to #2. And of course you can never be 100% confident that the code is correct or that every regression will be caught but you can certinaly be more confident than if you had no unit tests at all. I'll edit the answer to make this more obvious. +1 - Good stuff on point #3. @Jim Hurne: It is not because a unit test passes that *""You KNOW it works""* [sic]. When a unit test is passing you know that *""it does not not work""* and that is **very** different. This is one of the most common mistake people make with unit testing: they are overly confident that their code works... (btw I've got as many lines of unit tests as lines of code so don't mistake my point on unit testing) @Jim Hurne: In the same way you are in no way *guaranteed* to catch every single regression. There are definitely a lot of projects (and lots of very high-profile projects that are very well unit tested) where regression bug appears that are sadly not caught by any unit test. However when a test fails you *know* you just had a regression. But there's no guaranteed **at all** that you're going to catch your regression right away. @Jim Hurne: And it is impossible to test all the possible inputs. A method taking say a *long*? Well **good luck** testing all the possible inputs. I'm sorry but your post is utterly misleading: you mistake *""know it works""* for *""know it does not not work""* you think you'll hit every single regression and you think somehow can test every single input which is completely delusional.  1) There is a tradeoff between granularity of tests (and hence ease of diagnosis) and verbosity of your unit test code. I'm personally happy to go for relatively coarse-grained test methods especially once the tests and tested code have stabilized. The granularity issue is only relevant when tests fail. (If I get a failure in a multi-assertion testcase I either fix the first failure and repeat or I temporarily hack the testcase as required to figure out what is going on.) 2) Use your common sense. Based on your understanding of how the code is written design your tests to exercise all of the qualitatively different subcases. Recognize that it is impossible to test all possible inputs in all but the most trivial cases. 3) The point of unit testing is to provide a level of assurance that the methods under test do what they are required to do. What this means depends on the code being tested. For example if I am unit testing a sort method validation of user input is irrelevant. 4) The coverage seems reasonable. However without a detailed specification of what your class is required to do and examination of the actual unit tests it is impossible to say if you ave covered everything. For example is your method supposed to cope with leading / trailing whitespace characters numbers with decimal points numbers like ""123456"" numbers expressed using non-latin digits numbers in base 42? 5) Define ""successfully tested"". If you mean do my tests prove that the code has no errors then the answer is a definite ""NO"". Unless the unit tests enumerate each and every possible input they cannot constitute a proof of correctness. (And in some circumstances not even testing all inputs is sufficient.) In all but the most trivial cases testing cannot prove the absence of bugs. The only thing it can prove is that bugs are present. If you need to prove that a program has no bugs you need to resort to ""formal methods""; i.e. applying formal theorem proving techniques to your program. And as another answer points out you need to give it to real users to see what they might come up with in the way of unexpected input. In other words ... whether the stated or inferred user requirements are actually complete and valid.  1) I do think it's a good idea to limit the number of assertions you make in each test. JUnit only reports the first failure in a test so if you have multiple assertions some problems may be masked. It's more useful to be able to see everything that passed and everything that failed. If you have 10 assertEquals in one test and the first one fails then you just don't know what would have happened with the other 9. Those would be good data points to have when debugging. 2) Yes you should include tests for all of your inputs. 3) It's not just end-user input that needs to be tested. You'll want to write tests for any public methods that could possibly fail. There are some good guidelines for this particularly concerning getters and setters at the JUnit FAQ. 4) I think you've got it pretty well covered. (At least I can't think of anything else but see #5). 5) Give it to some users to test out. They always find sample data that I never think of testing. :) @Stephen C: Oh right. I got a little off track there. Thanks and I've edited my response. @Bill - this is multiplication. Division by zero should not be relevant unless the implementation is doing something strange.  True numbers of tests are of course infinite. That is not practical. You have to choose valid representative cases. You seem to have done that. Good job.  Unit testing is great (in the 200 KLOC project I'm working I've got as many unit test code as regular code) but (assuming a correct unit test): a unit test that passes does not guarantee that your code works Think of it this way: a unit test that fails proves your code is broken It is really important to realize this. In addition to that: it is usually impossible to test every possible input And then when you're refactoring: if all your unit tests are passing does not mean you didn't introduce a regression But: if one of your unit test fails you know you have introduced a regression This is really fundamental and should be unit testing 101. +1 to Stephen C and to fastcodejava +1 to you also well said.  I just want to add that with unit testing you can gain even more if you think first of the possible cases and after that implement in the test driven development fashion because this will help you stay focuesed on the current case and this will enable you to create easiest implementation possible in DRY fashion. You might also be usng some test coverage tool e.g. in Eclipse EclEmma which is really easy to use and will show you if tests have executed all of your code which might help you to determine when it is enough (although this is not a proof just a metric). Generally when it comes to unit testing I was much inspired by Kent Becks's Test Driven Development by Example book I strongly recommend it."
569,A,Including jMock 2.6.0 in Maven I am unable to include the new jMock release as Maven dependency. Here is what I tried including: <dependency> <groupId>org.jmock</groupId> <artifactId>jmock</artifactId> <version>2.6.0</version> </dependency> I have tried various combinations with the version and artifactId but it doesn't help. Thanks Julia. AFAIK version 2.6.0 isn't final yet the latest stable version is 2.5.1 and that's the version you can get in Maven Central. Hey can you help me with this: http://stackoverflow.com/questions/3734977/m2eclipse-filtering-test-resources Thanks Julia. JMock 2.6.0-RC2 is now in Maven Central  JMock 2.6.0 is now on Maven Central: http://mvnrepository.com/artifact/org.jmock/jmock/2.6.0  Version 2.6.0-RC2 can't be found on public Maven repositories yet. You may want to download it manually from jMock.org download page. If your company uses local Maven repository you can deploy it there. If it's just for your own usage install it in your local Maven repo. I can't deploy it on the company repo as the dev QA environments and the CI server fall outside it.  Now superceded - see my other answer Just had to solve this problem myself and I haven't found a simple way :-( The complicated way is this: Download jmock-2.6.0-RC2-jars.zip and unzip into a directory. Download http://www.oneeyedmen.com/misc/jmock-2.6.0-RC2.poms.zip and unzip into the same directory. Issue the following commands in that directory to install into your local Maven repository mvn install:install-file -DpomFile=jmock-2.6.0-RC2.pom -Dfile=jmock-2.6.0-RC2.jar mvn install:install-file -DpomFile=jmock-legacy-2.6.0-RC2.pom -Dfile=jmock-legacy-2.6.0-RC2.jar mvn install:install-file -DpomFile=jmock-junit4-2.6.0-RC2.pom -Dfile=jmock-junit4-2.6.0-RC2.jar mvn install:install-file -DgeneratePom -DgroupId=org.hamcrest -DartifactId=hamcrest-core -Dversion=1.3.0RC1 -Dpackaging=jar -Dfile=hamcrest-core-1.3.0RC1.jar mvn install:install-file -DgeneratePom -DgroupId=org.hamcrest -DartifactId=hamcrest-library -Dversion=1.3.0RC1 -Dpackaging=jar -Dfile=hamcrest-library-1.3.0RC1.jar mvn install:install-file -DgeneratePom -DgroupId=org.hamcrest -DartifactId=hamcrest-unit-test -Dversion=1.3.0RC1 -Dpackaging=jar -Dfile=hamcrest-unit-test-1.3.0RC1.jar Then you should be able to use jmock jmock-junit4 and jmock-legacy at version 2.6.0-RC2 with Junit 4.8.1 Caveats - this is entirely unofficial may conflict horribly with any official jmock poms may be completely the wrong way to solve this problem but works for me. thanks :) for some reason i had to install hamcrest-core manually ie mvn install:install-file -DgroupId=org.hamcrest -DartifactId=hamcrest-core -Dversion=1.3.0RC1 -Dpackaging=jar -Dfile=hamcrest-core-1.3.0RC1.jar ...
570,A,Powermock Slows Down Test Startup on Eclipse/Fedora 10 when on NTFS partition I've just started having a proper play with Powermock and noticed that it slows down test startup immensely. A quick look at top while it was running shows that mount.nfts-3g was taking up most of the CPU. I moved Eclipse and my source directory to ext3 partitions to see if that was a problem and the tests now startup quicker but there's still a noticeable delay. Is this normal with Powermock or am I missing something obvious? Try to run Powermock without Eclipse. Does the same symptoms occur? Mock veeery small units. Is the performance loss as high as before?  PowerMock is a bit slower at startup since it uses it's own classloader but the time needed also depends on how you write the tests using PowerMock. For instance if you are using the PrepareForTest annotation at the class or method level of your test. If you're using it at the method-level a new classloader is created for each test-method. The time may also depend on the junit fork mode. PrepareForTest is on the class. Not sure what the fork mode is as I'm running it from within Eclipse. Can I presume this startup time will increase the more PowerMock is being used then? Well it will be a bit slower but it shouldn't be significantly so. Whether it will increase the more you use PowerMock is dependent on the junit fork mode I think. But by default I don't think it should be any problem at least not in Eclipse.
571,A,"How can I make my JUnit tests run in random order? I have the classical structure for tests I have a test suite of different suites like DatabaseTests UnitTests etc. Sometimes those suites contains other suites like SlowDatabaseTests FastDatabaseTests etc. What I want is to randomize the running order of tests so I will make sure they are not dependent to each other. Randomization should be at every level like suite should shuffle test class order and test class should shuffle test method order. If it is possible to do this in Eclipse that will be the best. You could make it repeatable by giving Random() a seed that is a function of the date without the time. If you do this I strongly recommend a continuous build so you notice when the tests fail. Randomizing the order wouldn't prove anything since the random might work ""by accident"". Also it would not be repeatable and so you'd never be able to track down the cause of a failure. If I write my execution order to somewhere everytime tests are run I will be able to track the cause. @skaffman That may be true but if you don't randomise the order the bug will still be there but you'll never find out about it. So it depends what you prefer: (a) a bug which you don't know about or (b) a bug which you know about but can't track down. Mannn this sounds like a terrible idea to me. Repeatability is crucial to finding/resolving bugs. If you want to try varied order permutations code it up to do so explicitly. Tracking down issues with non-deterministic ""unit"" tests is one of the biggest sources of wasted time for me. Fortunately in this case at least the logs are likely to tell you which order the tests ran in so you could explicitly repeat the situation if need be. Non-determinism + minimal logging = complete waste of time but no logging is the ideal along with complete determinism is the ideal. @Ryan: The tests can output the RNG seed at the beginning of the tests so it can be repeated. In general what you need to do is to write your own test runner and in the test runner class aggregate the methods and randomly run each test (make sure you don't run a test twice). Read more about the test framework and how to write your own test runner here: http://www.ddj.com/architect/184415674 Note that the ddj.com article describes the JUnit3 test runner. If you try this be aware that if your tests use TestSetup or try to do suite-level setup and tear down by extending TestSuite the approach suggested here won't work; the test runner doesn't ""see"" TestSuites or TestDecorators. Writing your own JUnit3 test runner also won't work if you run tests from an IDE like Eclipse.  I will make sure they are not dependent to each other You should make sure that this is the case without relying on random execution order. What makes you fear that dependencies may exist? May I ask why people downvote my answer? What is wrong with it? We are being very careful while writing unit tests and making them independent. But we are also writing production code very careful bugs happen to appear there is no guarantee that they will be independent. PS. We are using JUnit for not only unit tests but for functional tests too so they sometimes leave the DB at some state. @lutz I guess because you countered with another question without answering OP's question ""Make sure that this is the case""? How would you make sure that this is the case if you're always running the tests in order?  You do have a Sortable but I can't see how you would use it. You could extend BlockJUnit4ClassRunner and have computeTestMethods() return a randomized copy of super.computeTestMethods(). Then use the @RunWith to set that as the runner to use. e.g. package com.stackoverflow.mlk; import java.util.Collections; import org.junit.runners.BlockJUnit4ClassRunner; import org.junit.runners.model.InitializationError; public class RandomBlockJUnit4ClassRunner extends BlockJUnit4ClassRunner { public RandomBlockJUnit4ClassRunner(Class<?> klass) throws InitializationError { super(klass); } protected java.util.List<org.junit.runners.model.FrameworkMethod> computeTestMethods() { java.util.List<org.junit.runners.model.FrameworkMethod> methods = super.computeTestMethods(); Collections.shuffle(methods); return methods; } } Then @RunWith(com.stackoverflow.mlk.RandomBlockJUnit4ClassRunner.class) public class RandomOrder { @Test public void one() { } @Test public void two() { } @Test public void three() { } } good answer but not quite sufficient I need to implement a Suite runner too to randomize test classes order. Besides I have lots of tests and I don't want to put @RunWith annotation into all of them. I think that can be handled in Suite runner I found this very useful and I created a small Java project that randomize tests and suites. For more information visit its page: http://randomjunit.sourceforge.net/  https://github.com/KentBeck/junit/pull/386 introduces some orders but not RANDOM. Probably you do not really want this; tests should run deterministically. If you need to verify that different permutations of tests still pass either test all permutations; or if this would be impractically slow introduce a “random” seed for shuffling that is determined by an environment variable or the like so that you can reproduce any failures. http://hg.netbeans.org/main/file/66d9fb12e98f/nbjunit/src/org/netbeans/junit/MethodOrder.java gives an example of doing this for JUnit 3."
572,A,"After a junit test case ran should I delete the test data related with this test case? After a junit test case ran should delete test data related with this test case? Will keeping the test data help the developers to debug the code? Thanks Joseph As a good practice the test case must remove its test data after it is finished so that next test case can run with a known initial db state. The test cases should not depend upon the order of run. This also makes debugging a test case easy since it runs from a known initial state.  I recommend to start the test with cleaning up and inserting the preferred test data. And leave the database as is afterwards. Advantages with this approach: Easy to manually verify that your services works as expected. The test data isn't corrupted by other services before start since the job is done immediately before the test. The DbUnit framework is actually built to clean and insert the test data into your test database in the setUp() method before each test method. With this approach it is recommended with a separate database for each developer.  Yes unit tests should begin and end with ""clean"" database file system etc. Each test should leave things as it found them. Apart from anything else this helps with re-runnability - you can keep re-running your tests time after time. Sometimes however when you're developing and debugging it can be useful to disable data-removal. There is a real craft to achieving all of this for example when working with Java Spring and databases you can use Spring's transaction management to simply roll back all your changes with zero effort."
573,A,"GWT and Mock the view in MVP pattern i dunno if the question is already ask but i couldn't find it... i'm searching a way to mock my view in order to test my presenter ? i try to use mockito for the view and set it in the presenter but in result in presenter when i call presenter.getDisplay() (the getter for the view) all of my widget is null ? as i believe it's normal mockito will not mock the widget. i'm 100% sure i mistaken something but i couldnt find it. thanks for your enlightement :) Best to post some code for a very simple test case. Hello finally we find a way to test the presenter. we wrap every widget in a class and use the interface such as : ` public class TextAreaWrapper extends WidgetWrapper implements IsTextArea public interface IsTextArea extends IsWidget` then we are able to mock IstextArea with mockito very easily With MVP Presenter depends on View class (Display) via gwt interfaces such as HasValue HasHTML HasClickHandlers etc. and possibly new interfaces as necessary. Presenter classes should use these interfaces instead of widget classes directly. Therefore Mocking View interfaces is rather simple and should be shared across test methods (using setUp or @Before). This should also include mocking GWT infrastructure such as EventBus etc. For nice blog with examples with EasyMock (easy but not straight forward to convert to mockito) see here.  you need to make sure that you told mockito to return the mocked view when you call getDislay(). Sth like when(presenter.getDisplay()).thenReturn(mockView); indeed but should i mock every widget one by one into the view ? mock every widget mock the view and tell mockito to return widget when ask ? Your view should not return widgets -- it should have methods which do something to the widgets (e.g. show/hide set a string in a label get a string from an input) but your Presenter should *not* know that the Display contains widgets when you use mock(yourClass.class) you get a reference to a object that doesn't know how to anything. You need to tell mockito what you expect to happen after a call to each method. one more thing - tdavies is right - your presenter should not do anything to the widgets. View is the object that is supposed to manipulate widgets. You just call methods on your view object (for example (myView.show())) and then the view object handles manipulating widgets. I seem to have misunderstood your question. No you should NOT mock every widget. The only thing you should mock is the view. Then you tell the mockito what you expect to happen when some method in the view object is called. This way your tests will be totally independent of the view implementation. You will be able to replace the view (for example you decided that you want to use some other windows from some other library) implementation and still have the tests valid.  Here's a very simple working example: import junit.framework.TestCase; import org.mockito.Mockito; public class SimpleGwtTest extends TestCase { private static class UpperCasePresenter { private final Display display; public interface Display { void setString(String s); } private UpperCasePresenter(String s Display display) { this.display = display; display.setString(s.toUpperCase()); } } public void testPresenter() { UpperCasePresenter.Display d = Mockito.mock(UpperCasePresenter.Display.class); new UpperCasePresenter(""foo"" d); Mockito.verify(d).setString(""FOO""); } } Of course normally your Presenter wouldn't be inside your test case."
574,A,"Selenium - Clicking on a hidden button that appears only when hovering above it I am attempting to run some automated tests in Eclipse using Java (running them as JUnit tests) and I'm using Selenium IDE 1.0.10 plugin for Firefox in order to find the button IDs. The problem is that on the webpage I have a hidden button on a row that becomes visible only when hovering above it. If I click on the button Selenium registers the click but if I attempt to run the test case it stops at that click since the button is only visible when the mouse hovers over it. Is there a way a command in Selenium for ""mouse hover"" or something that makes the button visible before clicking it? If you need more information please ask and I will provide everything I can. I don't know what else is needed since I'm pretty new at both Java and Selenium. Thank you! possible duplicate of [Selenium and :hover css](http://stackoverflow.com/questions/2973145/selenium-and-hover-css) Try: selenium.mouseOver(""mylocator""); Are you sure it's a hover that is activating the hidden buttons? Can you provide the HTML and associated JavaScript of the element? Thank you but I still can't get it to work. I use mouseOver command in the Selenium plugin enter the correct element (as far as I can tell anyway) yet nothing happens when I run the test. It doesn't fail because the element I mouseOver is present yet the hidden buttons do not appear. Again I'm sorry if I'm not more explicit."
575,A,"Android Junit and testPreconditions() I'm trying to do some tests on my application's database. I only got one activity in my application (""Home"") so my test class is public class HomeTest extends ActivityInstrumentationTestCase2 {..} My troubles : 1) In Android developers I've seen that testPreconditions() method is supposed to be launch before all tests but in my app it's acting like a normal test ... 2) I'd like to fill my database before all other tests to begin. But to do so I need a reference on my Activity but I can't call ""this.getActivity"" in the tests constructor ... Is there a way to do something after the constructor ? (The setUp() method isn't appropriate because it is called before each test not just one time before all) Thanks The order in which the test are run is not guaranteed to run before other tests as junit 3 uses reflection to find the tests. Use the setUp() method this also assure that your tests run with a fresh database Ok for the 1) ! For the 2) I needed a method that is launched just one time before all tests but I finaly changed my way of testing ... Thanks ! You may add a variable to make it run once only?"
576,A,Run JUnit Tests through web page We would like to have a set of tests as part of our web application. The tests will be used for analyzing the health status of the application so a support person or a scheduler can run the test to see if the application itself and various required remote systems are available. I have seen this being done using some kind of webbased JUnit frontend it allowed to run tests and reported the results as HTML. This would be great because the developers know JUnit but I couldn't find the library in the intertubes. Where can I find a library doing this? If you're goal is seeing what's available I don't think you want tests for that you want to add monitoring capabilities (like with JMX) so you can have some sort of dashboard that tells you what's going on. If the developers know JUnit then integrating it with cruisecontrol will definitely give you the reporting you're after... Bozho is correct these are not unit tests but I have done something similar. At my company I am not the one that ultimately deploys these things to our test environment or production environment. During development I create a couple of servlets that test things like it can get a valid database connection it can hit our AD server etc. It than basically prints out a message and indicates success or failure. That way when I have the code deployed to one of our environments I can have the person deploying it hit the URL and make sure everything comes back OK. When I get ready to do the final deployment I just remove the servlet config. I didn't run any junit tests or create a test framework or anything I just created a simple Servlet that I could do what I needed to do. The amount of code was really minimal. That sounds pretty much like what I am trying to do. So you wrote the stuff yourself? Hmm. Was hoping there would be some open source project doing this ..  You should look for a Continous Integration tool like Jenkin.  You can use some free services to verify the availability of your system. Here are two that I've used: mon.itor.us pingdom Another thing you can take a look at is JMeter but it does not have a web UI. Original answer: Perhaps you mean functional tests (that can be run through JUnit). Take a look at Selenium - it's web functional testing tool. (Note that these are not unit tests. They don't test individual units of the code. Furthermore unit tests are executed at build time not at runtime.) I'm aware that these aren't unit tests. Thats why I didn't call them unit tests. AFAIK Selenium tests a web app through the GUI. If this is true this is not what I am looking for. I want to control my tests through a webgui not the other way round. and what should these tests test then? The availability of remote systems that are needed by the webapplication. I'd call them 'monitoring tests' @Jens Schauder see updated.  If you already have a set of tests composed and ready to run then Hudson can run those tests on a schedule and report on the results.  What you're probably looking for is CruiseControl.Net - it combines with NUnit/JUnit etc to make an automated testing framework with HTML reporting tools and a tray app for your desktop as well. I actually just downloaded it again an hour ago for a new role - it's really good. It can be used to run anything from unit tests to getting files from source control to kicking off compiler builds or rebooting servers (when used with NAnt - a .Net build tool).  Update: If you're looking for a tool to check your servers and applications every few minutes for availability check out Nagios. Maybe you mean some kind of acceptance test tool. If so have a look at Fitnesse.
577,A,"Run Junit-Tests from several projects conveniently fast in Eclipse Is there a way to run JUnit-Tests from several projects conveniently fast in Eclipse? The JUnit-Runner lets you define a package or a folder where from all contained tests are executed. Is there a way to do this with tests from several projects inside Eclipse? Preferably it should be via the Junit-Runner. If there is some way to have it fast via an Ant-job (so not depend on a complete build with ant before) that would be also nice. It’s actually quite easy to perform JUnit tests across multiple projects from within Eclipse. Have a look at Classpath Suite. It’s not with the standard JUnit runner but you did not mention where that requirement came from so I’m not sure whether this will affect you. All the usage information is on that page but to summarize: Create an Eclipse project depending on all the projects you want to test. Write a simple test suite including everything: @RunWith(ClasspathSuite.class) public class MySuite {}  You can't do it through the UI. Looking at the extension-points the highest-level element JUnit will collect for is the Project. I suppose you could write a plugin to contribute an additional context item/shortcut for a working set make working sets the top-level items in the package explorer and group the projects you want to test together below that working set. The problems with doing this is you'd have trouble defining the context rules for enabling/disabling the ""run as"" contribution and I'm not sure the semantics extend to working sets. So you'd have to write some sort of wrapper to iterate the contained projects and collect their test types. This does seem an interesting little problem. I might have a play with it after school today. Another (slightly less) hacky way would be to set up another project with project dependencies on all your target projects then use linked resources to bring all the test types into the new project (I've posted an answer before that describes how to link sources across projects). Of course if you do this you will need to manage the dependencies of the test project as well. If you create a TestSuite for each project and another uber TestSuite that references all the projects' suites you have to check every test is included which is error-prone. If you don't fancy mucking about with plugins or linked-resources you're probably best off using Ant.  I am not sure you can't do it from UI but from ant it's possible even if you are not building you plugins using ant. Nevertheless this method is not so trivial but once it's set up things are cool ;o) Check here for more informations: http://www.eclipse.org/articles/article.php?file=Article-PDEJUnitAntAutomation/index.html  You could define a separate project that depends on the other projects which contains a suite referencing the individual tests or suites from the different projects. Something like this: @RunWith(Suite.class) @Suite.SuiteClasses( { FirstProjectSuite.class SecondProjectSuite.class} ) public class AllSuites { }"
578,A,"How to default the source folder for new JUnit tests in Eclipse? Most of our Eclipse projects have multiple source folders for example: src/main/java src/test/java When you right-click on a class and choose New JUnit Test the default source folder for the new test is ""src/main/java"" (presumably the first source folder listed in the project properties). Is there any way to change the default source folder for new JUnit tests so that when I do the above action the new test will be created in say the ""src/test/java"" folder by default? No. Unless you change the plugin code the default source folder is always the same as that containing the class you right clicked on (not necessarily the first source folder listed). I agree it would be nice to be able to change it! Dave - Have you tried my plug-in? It does exactly that : http://fast-code.sourceforge.net/.  Now you can use my fast code eclipse plug-in. With this plug-in you can configure the test path to be src/test/java only once. It also has a jump to the unit test feature. It is available at : http://fast-code.sourceforge.net/.  I use moreUnit an Eclipse plugin to assist writing unit tests. Among other features it lets you configure the default source folder of tests. I'd vote you up but I'm out of votes for the next hour! :-) Voted you up. Now I love moreUnit and couldn't live without my Ctrl-J to jump between the code and its test. moreunit looks awesome thanks"
579,A,"Unit tests vs integration tests with Spring I'm working on a Spring MVC project and I have unit tests for all of the various components in the source tree. For example if I have a controller HomeController which needs to have a LoginService injected into it then in my unit test HomeControllerTest I simply instantiate the object as normal (outside of Spring) and inject the property: protected void setUp() throws Exception { super.setUp(); //... controller = new HomeController(); controller.setLoginService( new SimpleLoginService() ); //... } This works great for testing each component as an isolated unit - except now that I have a few dozen classes in the project after writing a class and writing a successful unit test for it I keep forgetting to update my Spring MVC context file that does the actual wiring-up in the deployed application. I find out that I forgot to update the context file when I deploy the project to Tomcat and find a bunch of NullPointers from non-wired-up beans. So here are my questions: This is my first Spring project - is it normal to create unit tests for the individual beans as I have done and then create a second suite of tests (integration tests) to test that everything works as expected with the actual application context? Is there an established best practice for this? In addition how do you separate the unit tests from the integration tests? I have all of the source code in src the unit tests in test - should there be a 2nd test folder (such as test-integration) for integration test cases? Since this is my first Spring project I'm curious how others usually go about doing this sort of thing - and rather than re-invent the wheel I rather ask the rest of the community. A lot of the tedious double-book-keeping with spring goes away if you also switch to a purely annotated regime where you annotate all your beans with @Component @Controller @Service and @Repository. Just add @Autowired to the attributes you need to get injected. See section 3.11 of the spring reference manual. http://static.springframework.org/spring/docs/2.5.x/reference/beans.html#beans-annotation-config On a related note we have been using the division Unit/Integratrion tests that KenG describe. In my most recent regime we have also introduced a third ""class"" of tests ""ComponentTests"". These run with full spring wiring but with wired stub implementations (using component-scan filters and annotations in spring). The reason we did this was because for some of the ""service"" layer you end up with an horrendous amount of hand-coded wiring logic to manually wire up the bean and sometimes ridiculous amounts of mock-objects. 100 lines of wiring for 5 lines of test is not uncommon. The component tests alleviate this problem. Unfortunately I'm still using Java 1.4 :(  When I've created integration tests for web applications I've put them in a separate directory. They are built using jUnit or TestNG and interact with the system under test using something like Selenium that hits the web pages as if they were users. The cycle would go like this: compile run unit tests build the web app deploy it to a running server execute the tests undeploy the app and report results. The idea is to test the whole system.  A few isolated points: Yes it's a common approach to Spring testing - seperate unit tests and integration tests where the former doesn't load any Spring context. For your unit tests maybe consider mocking to ensure that your tests are focussed on one isolated module. If you're tests are wiring in a ton of dependencies then they aren't really unit tests. They're integration tests where you are wiring of dependencies using new rather than dependency injection. A waste of time and duplicated effort when your production application uses Spring! Basic integration tests to bring up your Spring contexts are useful. The @required annotation may help you to ensure you catch required dependencies in your Spring wiring. Maybe look into Maven which will give you explicit phases to bind your unit and integration tests on to. Maven is quite widely used in the Spring community.  I can't speak to being a best practice but here's what I've done in the past. Unit tests: Create unit tests for non-trivial beans (ie most of your Spring related beans) Use Mocks for injected services where practical (ie most if not all the time). Use a standard naming convention for these tests in the project test directory. Using Test or TestCase as a prefix or suffix to the classname seems to be widely practiced. Integration Tests: Create an AbstractIntegrationTestCase that sets up a Spring WebApplicationContext for use in intetgration test clases. Use a naming convention for integration tests in the test directory. I've used IntTest or IntegrationTest as a prefix or suffix for these tests. Set up three Ant test targets: test-all (or whatever you want to name it): Run Unit and Integration Tests test: Run Unit tests (just because test seems to be the most common usage for unit testing test-integration: run the integration tests. As noted you can use the naming conventions that make sense for your project. As to separating unit from integration tests into a separate directory I don't think it matters as long as the developers and their tools can find and execute them easily. As an example the last Java project I worked on with Spring used exactly what is described above with integration tests and unit tests living in the same test directory. Grails projects on the other hand explicitly separate unit and integration test directories under a general test directory. This sounds like a good strategy. But with the unit tests and integration tests in the same directories there's no way for Eclipse to tell them apart is there? Eclipse's only options for running junit tests is to run a just one or run all in a certain folder - can't split them by name like Ant. I had to go back and check Eclipse - you're correct I don't see a way to differentiate the tests if they're in the same directory. My last Spring project was done in Idea/Intellij and it has been long enough that I don't remember exactly what the IDE configuration was.  With regard to running unit tests separately from integration tests I put all the latter into an integration-test directory and run them using IDE/Ant using an approach like this. Works for me.  the difference between unit test and integration test is  unit test does not necessarily load your context you are focusing on the code which you have written - it works fails fast  that is with and without exceptions by mocking any depends calls in it. But in case of integration tests  you load context and perform end to end test like actual scenarios.  Use the InitializingBean interface (implements a method ""afterPropertiesSet"") or specify an init-method for your beans. InitializingBean is typically easier because you don't need to remember to add the init method to your beans. Use afterPropertiesSet to ensure everything is injected as non-null if it is null throw an Exception."
580,A,Classloading issues in maven2 with JUnit I have a project that builds with maven2 and runs a series of JUnit test cases against the code. This has worked fine up until this point where I now have 2 tests that must run in a certain sequence for things to work correctly let's say TestA and Test (A then B). Unfortunately maven2 doesn't understand this so I'm looking for a way of convincing it that it needs to run the tests in this order. The problem is that I'm setting some final static fields in TestB but I'm doing this from TestA which itself uses those fields and successful execution of the test depends on those fields being set to their new values (there is absolutely no way around this otherwise I would have taken that road long before now). So it is imperative that TestA loads first and it will of course cause TestB to be loaded when tries to access it. However maven2 has decided that it will run TestB then TestA which means those final fields are already set and cannot be changed. So what I'm looking for is either a way to specify the order in which the tests are executed (A then B every time) or a way to easily cause TestB to be reloaded by whichever classloader that JUnit uses. EDIT - one other option might be some option like the old JUnit GUI tool has that causes all classes to be reloaded for each test. I have looked and looked and not found such a flag in the maven junit plugin if such a thing exists then that would also work. The test order in JUnit is intentionally undefined this is not a Maven issue and it's probably just luck that your tests were running OK up until now. Sal's answer addresses your question directly however forking the JVM on each test for large test counts can hugely increase your build time. An alternative approach would be to use a testing library such as PowerMock (it currently works with EasyMock and Mockito) to clear the static fields in TestB's initialisation this avoids the need for any JVM forking and ensures your tests are portable. From the PowerMock website: PowerMock is a framework that extend other mock libraries such as EasyMock with more powerful capabilities. PowerMock uses a custom classloader and bytecode manipulation to enable mocking of static methods constructors final classes and methods private methods removal of static initializers and more. By using a custom classloader no changes need to be done to the IDE or continuous integration servers which simplifies adoption. Developers familiar with EasyMock will find PowerMock easy to use since the entire expectation API is the same both for static methods and constructors. PowerMock extends the EasyMock API with a small number of methods and annotations to enable the extra features. From version 1.1 PowerMock also has basic support for Mockito. @rich you are right about the potential performance issue with forking. Assuming two tests; A & B the slowdown is likely to be minor. @sal agreed if the number of tests is small there is minimal overhead and your answer addresses the problem. I wouldn't introduce a new testing framework to address a single issue but if there are multiple instances it is worth considering. I thought I'd upvoted your answer at the time I shall correct that now Using the pertest option in maven was the solution I eventually landed on but I like the idea of using the libraries you mentioned. I will take a look. I was concerned about performance/build time overhead when I arrived at that conclusion before and yes there are more than 2 tests. Those 2 just happen to be the only ones where the order matters.  Fork mode can force each test to run in its own JVM so every class is re-loaded for each test.  <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <forkMode>pertest</forkMode> </configuration> </plugin>
581,A,"Testing jpa ddl types I'm trying to run some tests and every time I run play it clears the database. I guess this is due to my ""test.jpa.ddl"" property. Current code: %test.jpa.ddl=create-drop Is it just removing this line to solve the problem? Or do I need to setup another value? (I'm afraid of testing and remove all the data again...) Try setting a value of test.jpa.ddl=none That's assuming your JPA provider is doing that. Alternatively you could be using DBUnit to do unit tests. In that case it will wipe out your data every time they are run. There really isn't a way around that other than saving your data and loading back up afterwards.  It depends on what your other settings are for your test mode. By default the database is set up as in memory so this may be your issue. Also are you using Fixtures? If you are not I would highly recommend using fixtures as it allows you to specify your test data in a YAML file and load it fresh at the start of every test cycle which guarantees the consistency of your data.  Check whether you are using h2 db (it's by default). The simplest way to keep your data is comment this line: db=mem And change it to db=fs If you are using standalone database (like mysql) then you definitely need to go to ""jpa.ddl"". Change it to ""none"" as the others say."
582,A,Best Mock object framework for EJB junit tests Which mocking framework is preferred for testing EJB's (specifically Message Driven Beans) I'm looking at MockObjects easyMock MockEJB and mockito but open all. I've not used mocking framework before and particularly interested in low learning curve / great getting started guide. I should clarify using EJB 2. possible duplicate of [What's the best mock framework for Java?](http://stackoverflow.com/questions/22697/whats-the-best-mock-framework-for-java) @dogbane - I'm thinking specifically for EJB junit tests If you're using EJB 3.x then is heavily interface-based and so your usual run-of-the-mill mocking frameworks will do just fine for unit tests; the fact that the code is used by the EJB framework isn't really relevant. EJB 2.x is a different (and ugly) kettle of fish though since it doesn't conform to any of the usual sane rules of software design. Your best bet there is probably a framework like Cactus.  A framework specifically build for this is Arquillian - http://www.jboss.org/arquillian. This lets you test in-container via JUnit. You can test individual beans or larger collections. You can provide mocks by simply packaging other implementations in the test archive. When using Java EE 6 (JBoss AS 6 Glassfish V3) you can run the container in embedded mode which simplifies matters and saves you in run-time overhead. EJB2 is notoriously difficult to test though. If possible I suggest you drop it completely in favor of EJB3.x but of course this might not be an option for you. Not really a mocking (or stubbing) solution but a different approach to the problem of testing EJBs. Quite probably a better approach!  It's not a mocking framework but you might like to have a look at OpenEJB which is a lightweight EJB container suitable for amongst other things use in unit tests. It will save you having to mock the container's interfaces and you can still mock interfaces to other components. On a mobile tried to do hyperlinks by hand failed.
583,A,"Intellij Idea ""Move refactoring"" with Junit tests Seems when I make ""move"" refactoring all my junit tests lays on its old place. Often I tests ""package"" visible classes so they becomes invisible if SUT moves to another package. Do you move tests by hand? I'm afraid I cannot understand your question. Could you please provide some more infomation? That could help to answer this question (+1 for you to be able to comment) suppose i have class AAA in package xxx. Destination of class is src/xxx/AAA.java and suppose i have a test located at test/xxx/AAATest.java So when I move class AAA to package yyy my test stay at xxx package. The behaviour you describe is perfectly normal. src/package1/A.java test/package1/ATest.java In your ATest.java there's an import package1.A;. After your refactored it looks like this: src/package2/A.java test/package1/ATest.java The test code stayed where he was. You did not moved the test code but your source code. It should not affect any other folders (like in your example). The reference in the ATest.java must now be import package2.A;. Otherwise the refactoring went wrong. Nontheless your tests should work even if they are in a different directory. That's because the import was changed by the refactoring method. If you want to clean up your folder structure you have to manually rename the package test/package1 to test/package2 (I know the package is package1 and package2 but I want to strengthen the focus on the folder structure. I hope I could help you!  I have 4 options for you: Go to the ""Package"" view in the left select both files and then hit F6. It should move them both to the right place. Make the class public temporarily before you do your refactor and switch back afterwards. Try moving the test first. I seem to remember that avoids breaking any of the dependencies. There is a plugin (I think it's toggleTest or unitTest-- I had both of them installed) that patches the Move Refactor to also bring the test with it. Worked great. Unfortunately it looks like these may not work with the latest IDEA."
584,A,"Eclipse junit testing in the same project This is a relatively open question. If I have built an application in a project in Eclipse and I then want to test this project should I create the JUnit code within the same project or create a separate project. For instance... ShopSystem maybe the name of my main project - should I create a project called say ShopSystemTest? In general - how far ""away"" should the testing code be stored from the main project folder? If I store the testing code within the main project and then export the main project as a runnable jar it will take the testing code with it which isn't ideal... Suggestions? Typically you have - /src/main/java (for codes) /src/test/java (for tests)  I like the maven convention a lot: There is a separate source tree for main and test in the same project main code gets deployed test code doesn't. Package structures can be (but don't have to be) identical. project src main java // source files resources // xml properties etc test java // source files resources // xml properties etc And in eclipse when you choose new -> JUnit test case you just change the source folder to src/test/java and leave the suggested package as is. (One of the benefits of remaining in the same package is having access to protected and package scoped members although this is not 'proper' unit test behavior) Update: Here's some code to illustrate my last point: Main class (in src/main/java): package com.test; public class Foo{ static class Phleem{ public Phleem(final String stupidParameter){ } } String bar; protected String baz; protected Object thingy; } Test class (in src/test/java): package com.test; import org.junit.Test; public class FooTest{ @Test public void testFoo(){ final Foo foo = new Foo(); foo.bar = ""I can access default-scoped members""; foo.baz = ""And protected members too""; foo.thingy = new Foo.Phleem(""And I can access default-scoped classes""); } } ""access to protected and package scoped members"" - to have access to protected members you'd have to subclass the class under test. What you mean is being able to access package-scoped classes right? Oh and members too of course you're right. I've used default-scoped members so seldomly that I sometimes forget they exist. Ouch! I did not know `protected` allowed access from other classes in the same package. Okay that clears things up thanks. yes the whole access level concept is rather stupid. e.g. there's no way to make packages accessible to sub-packages without making the classes public. scala has a much better system there. @seanizer: I agree. If Java was designed today private would probably be the default protected would allow access only from subclasses there would be a concept of sub-packages and `final` would be the default for classes local variables and maybe even members. Oh well. @Henning final would be default? you mean there would be a keyword to mark classes and methods as extensible? I'm not sure whether I'd want that because you'd have to bug API designers all the time to please add the keyword to this and that method. @seanizer: Well it would put the rule to either design and document for inheritance or prohibit it (Josh Bloch) right into the language. IMO you can hardly ever extend a class well that wasn't designed for it from the ground up. I know I tried to extend the Hibernate Validator once. I gave up soon and re-wrote most of it...  Consider the maven way : In a maven project soruces are organized this way src |--main | |--java |--test |--java Your source code goes in src/main/java your junit test code goes in src/test/java they both are source folder (and as a consequence you can put your jUnit code in the same package as your Java code but in a different source folder). The interest is that for usual coding your jUnit classes are in code packages but on jar creation you can take classes coming only from src/main/java and not release your tests.  While there is no only right way the usual approach is to keep unit tests in the same project. You can create a second source folder (like test) where you put your test classes into the same packages as the classes under test. This also allows you to test package-private classes while not flooding your main source packages with test classes. Your source folder/package structure would then look like this: -sources -main -my.package -MyClass.java -test -my.package -MyClassTest.java You can then configure your build to not include the test source folder when packing the JAR."
585,A,"Large amount of unit tests cause freezes with Failed Binder Transaction I have about 400 unit tests that I have written for my Android application. If I run the tests package by package everything works fine and all my tests pass. However if I try to run them all at once eventually (about 360 tests in) Android starts spitting out !!! Failed Binder Transaction Errors !!!. After about 10-20 of these errors the process for the application I am testing gets killed and the unit tests never even complete. I should note that during this time Eclipse reports Collecting test information in the Console. I think that is odd because it shows that it is running tests even though the JUnit UI doesn't reflect that. When everything works properly during the Collecting test information phase I see the tests run the JUnit UI then comes up and then all the tests are run again (I know this by reading logcat). That is (I think) a separate issue but I felt I would mention it if anybody knew what that was about. Edit as of June 6th 2011 As per Christopher's answer below I have verified that this only happens when attempting to run this through Eclipse. If i run my test suite using ANT all the tests finally execute. When I attempt to run my test suite now Eclipse essentially hangs on ""collecting test information"". I haven't let it run for an extended period of time but I will give that a try as soon as I can to see if it ever completes. We're seeing the same with around 500 tests. Did you manage to alleviate this problem at all? Sorry I haven't figured out a way to solve this problem quite yet. Maybe someone on the Android team has some ideas... I investigated this again and as far as I can tell this is purely an issue with the Android Eclipse plugin. An issue that seems to be fixed but still can occur sometimes (possibly it's machine-dependent). As you mention the Eclipse plugin appears to run all the tests twice. What it's actually doing the first time is gathering the test suite and test names so that it can show all the test names in that nice hierarchical JUnit UI. However ""running"" the tests like this seems to cause a problem. As part of Android SDK Tools v8 a ""workaround"" was added which places a 15ms delay between checking each test. This is done to prevent ""Binder transaction failures ... for large test suites"". Indeed I went back to my project that was having the ""FAILED BINDER TRANSACTION"" errors and I could not reproduce it on the command line. We also tried in Eclipse and it couldn't be reproduced any more (even although I'm sure we had SDK Tools r8+ when we originally saw this). However I can still generally reproduce it in Eclipse. Try running your tests again from Eclipse with the latest Android tools or try out this minimal GitHub repository I created and see if you can reproduce it: https://github.com/orrc/android-large-test-failures#readme I edited my question with the results from your suggestion. I ran my test suite with ANT (oddly enough we just recently switched to ANT) and it worked! I will try and run my test project again in Eclipse to see what log cat says while its ""collecting test information"". Also just to be clear I had this error on both a device and emulator. Okay. So my project is still definitely throwing failed binder transactions in eclipse but this time its when trying to collect test information instead of when running the tests the second time.. How strange.. To update this effort even further I think I have narrowed down the problem. I can very rarely get this problem to occur on a Nexus S. However on an older phone with a slower processor (in my case HTC Aria) I can reproduce the problem regularly. I suppose the delay is not enough on that phone? Thanks for the info. I haven't tried it on differing speeds of phones so far. In any case I just filed http://b.android.com/18660 Chris since this doesn't seem like it will be addressed (I know I still revisit it!!) I think I'm just going to accept your answer. At least we know we both aren't crazy and alone in this.  You might want to try running tests in your local JVM instead of deploying them to the emulator or a phone. Check out Robolectric for some help with that. While that would probably work I'm more interested in why they don't work on my device. Its very odd to me."
586,A,"How to list the slowest JUnit tests in a multi-module Maven build How can I list the slowest JUnit tests in a multi-module Maven build? This should be accross all modules. A Hudson/Jenkins solution could also do. Disclaimer: I truly apologize for my bash solution although it works and fits in one line :-). If you are impatient go to the bottom. First we need to find all TEST-*.xml files produced by maven-surefire-plugin. Run this after mvn test in the root directory of your project to discover test results in all submodules: $ find . -iname ""TEST-*.xml"" Fortunately the format of these files is pretty straightforward a simple grep and we have what we need: $ grep -h ""<testcase"" `find . -iname ""TEST-*.xml""` Now some sed magic to extract invocation time test case class and method name: $ sed 's/<testcase time=""\(.*\)"" classname=""\(.*\)"" name=""\(.*\)"".*/\1\t\2.\3/' There's nothing more left just to sort the result and display longest running tests: $ sort -rn | head Promised one-liner: $ grep -h ""<testcase"" `find . -iname ""TEST-*.xml""` | sed 's/<testcase time=""\(.*\)"" classname=""\(.*\)"" name=""\(.*\)"".*/\1\t\2.\3/' | sort -rn | head Amazingly the results look reasonable (Activiti 5.1 multi-module code-base taken as an example): 3.029 org.activiti.examples.variables.jpa.JPAVariableTest.testStoreJPAEntityAsVariable 2.904 org.activiti.engine.test.forms.FormsTest.testTaskFormPropertyDefaultsAndFormRendering 1.594 org.activiti.engine.test.api.mgmt.ManagementServiceTest.testGetJobExceptionStacktrace 1.114 org.activiti.examples.variables.jpa.JPAVariableTest.testUpdateJPAEntityValues 1.006 org.activiti.engine.test.db.EngineRebootProcessDefinitionCacheTest.testStartProcessInstanceByIdAfterReboot 0 org.activiti.engine.test.pvm.PvmVariablesTest.testVariables 0 org.activiti.engine.test.pvm.PvmScopeWaitStateTest.testWaitStateScope 0 org.activiti.engine.test.pvm.PvmScopesAndConcurrencyTest.testConcurrentPathsGoingIntoScope 0 org.activiti.engine.test.pvm.PvmEventTest.testNestedActivitiesEventsOnTransitionEvents 0 org.activiti.engine.test.pvm.PvmEventTest.testEmbeddedSubProcessEvents Great stuff! It works! In my case `time` and `name` attributes are inverted. The one-liner becomes: `grep -h """
587,A,"Database cleanup after Junit tests I have to test some Thrift services using Junit. When I run my tests as a Thrift client the services modify the server database. I am unable to find a good solution which can clean up the database after each test is run. Cleanup is important especially because the IDs need to be unique which are currently read form an XML file. Now I have to manually change the IDs after running tests so that the next set of tests can run without throwing primary key violation in the database. If I can cleanup the database after each test run then the problem is completely resolved else I will have to think about other solutions like generating random IDs and using them wherever IDs are required. Edit: I would like to emphasize that I am testing a service which is writing to database I don't have direct access to the database. But since the service is ours I can modify the service to provide any cleanup method if required. When writing JUnit tests you can override two specific methods: setUp() and tearDown(). In setUp() you can set everything thats necessary in order to test your code so you dont have to set things up in each specific test case. tearDown() is called after all the test cases run. If possible you could set it up so you can open your database in the setUp() method and then have it clear everything from the tests and close it in the tearDown() method. This is how we have done all testing when we have a database. Heres an example: @Override protected void setUp() throws Exception { super.setUp(); db = new WolfToursDbAdapter(mContext); db.open(); //Set up other required state and data } @Override protected void tearDown() throws Exception { super.tearDown(); db.dropTables(); db.close(); db = null; } //Methods to run all the tests  Assuming you have access to the database: Another option is to create a backup of the database just before the tests and restore from that backup after the tests. This can be automated. yes I have access to the database and that is what I am also planning to do after talking to a couple of collegues. thanks John.  If you are using Spring + Junit 4.x then you don't need to insert anything in DB. Look at AbstractTransactionalJUnit4SpringContextTests class. Also check out the Spring documentation for JUnit support.  It's a bit draconian but I usually aim to wipe out the database (or just the tables I'm interested in) before every test method execution. This doesn't tend to work as I move into more integration-type tests of course. In cases where I have no control over the database say I want to verify the correct number of rows were created after a given call then the test will count the number of rows before and after the tested call and make sure the difference is correct. In other words take into account the existing data then see how the tested code changed things without assuming anything about the existing data. It can be a bit of work to set up but let's me test against a more ""live"" system. In your case are the specific IDs important? Could you generate the IDs on the fly perhaps randomly verify they're not already in use then proceed?  How about using something like DBUnit?  I agree with Brainimus if you're trying to test against data you have pulled from a database. If you're looking to test modifications made to the database another solution would be to mock the database itself. There are multiple implementations of in-memory databases that you can use to create a temporary database (for instance during JUnit's setUp()) and then remove the entire database from memory (during tearDown()). As long as you're not using an vendor-specific SQL then this is a good way to test modifying a database without touching your real production one. Some good Java databases that offer in memory support are Apache Derby Java DB (but it is really Oracle's flavor of Apache Derby again) HyperSQL (better known as HSQLDB) and H2 Database Engine. I have personally used HSQLDB to create in-memory mock databases for testing and it worked great but I'm sure the others would offer similar results.  If you are using Spring everything you need is the @DirtiesContext annotation on your test class. @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(""/test-context.xml"") @DirtiesContext(classMode = ClassMode.AFTER_EACH_TEST_METHOD) public class MyServiceTest { .... }  Unless you as testing specific database actions (verifying you can query or update the database for example) your JUnits shouldn't be writing to a real database. Instead you should mock the database classes. This way you don't actually have to connect and modify the database and therefor no cleanup is needed. You can mock your classes a couple of different ways. You can use a library such as JMock which will do all the execution and validation work for you. My personal favorite way to do this is with Dependency Injection. This way I can create mock classes that implement my repository interfaces (you are using interfaces for your data access layer right? ;-)) and I implement only the needed methods with known actions/return values. //Example repository interface. public interface StudentRepository { public List<Student> getAllStudents(); } //Example mock database class. public class MockStudentRepository implements StudentRepository { //This method creates fake but known data. public List<Student> getAllStudents() { List<Student> studentList = new ArrayList<Student>(); studentList.add(new Student(...)); studentList.add(new Student(...)); studentList.add(new Student(...)); return studentList; } } //Example method to test. public int computeAverageAge(StudentRepository aRepository) { List<Student> students = aRepository.GetAllStudents(); int totalAge = 0; for(Student student : students) { totalAge += student.getAge(); } return totalAge/students.size(); } //Example test method. public void testComputeAverageAge() { int expectedAverage = 25; //What the expected answer of your result set is int actualAverage = computeAverageAge(new MockStudentRepository()); AssertEquals(expectedAverage actualAverage); } ""`Unless you as testing specific database actions (verifying you can query or update the database for example) your JUnits shouldn't be writing to a real database.`"" There's not a universal agreement about this. In one camp we have those people who believe that the database should be mocked and in the other hand we have those that believe that all database tests should be performed against a real database ideally one that matches the production database. When you need to retrieve data you create a class and implement the method you need returning the data that you need. If you need to save data you create a class and implement the save (whatever it is called) method. This method can just store the data in instance variable(s). If you need to save and retrieve then those methods just need to read and write from the same instance variable(s). Sorry I am a little new to all this stuff was coding in flex for some time. Could you please elaborate on what I am required to do. I am not sure what you mean by mock classes and not modifying the database. Typically  the tests I run create new Customer and use that customer to create orders and other things. Where would all this data go if not to a database?  Spring's unit testing framework has extensive capabilities for dealing with JDBC. The general approach is that the unit tests runs in a transaction and (outside of your test) the transaction is rolled back once the test is complete. This has the advantage of being able to use your database and its schema but without making any direct changes to the data. Of course if you actually perform a commit inside your test then all bets are off! For more reading look at Spring's documentation on integration testing with JDBC."
588,A,"Using JUnit with App Engine and Eclipse I am having trouble setting up JUnit with App Engine in Eclipse. I have JUnit set up correctly that is I can run tests that don't involve the datastore or other services. However when I try to use the datastore in my tests they fail. The code I am trying right now is from the App Engine site (see below): http://code.google.com/appengine/docs/java/tools/localunittesting.html#Running_Tests So far I have added the external JAR (using Eclipse) appengine-testing.jar. But when I run the tests I get the exception below. So I am clearly not understanding the instructions to enable the services from the web page mentioned above. Can someone clear up the steps needed to make the App Engine services available in Eclipse? java.lang.NoClassDefFoundError: com/google/appengine/api/datastore/dev/LocalDatastoreService at com.google.appengine.tools.development.testing.LocalDatastoreServiceTestConfig.tearDown(LocalDatastoreServiceTestConfig.java:138) at com.google.appengine.tools.development.testing.LocalServiceTestHelper.tearDown(LocalServiceTestHelper.java:254) at com.cooperconrad.server.MemberTest.tearDown(MemberTest.java:28) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:37) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:73) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:46) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41) at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) at org.junit.runners.ParentRunner.run(ParentRunner.java:220) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: java.lang.ClassNotFoundException: com.google.appengine.api.datastore.dev.LocalDatastoreService at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) ... 25 more Here is the actual code (pretty much copied from the site): package com.example; import static org.junit.Assert.*; import org.junit.After; import org.junit.Before; import org.junit.Test; import com.google.appengine.api.datastore.DatastoreService; import com.google.appengine.api.datastore.DatastoreServiceFactory; import com.google.appengine.api.datastore.Entity; import com.google.appengine.api.datastore.Query; import com.google.appengine.tools.development.testing.LocalDatastoreServiceTestConfig; import com.google.appengine.tools.development.testing.LocalServiceTestHelper; public class MemberTest { private final LocalServiceTestHelper helper = new LocalServiceTestHelper(new LocalDatastoreServiceTestConfig()); @Before public void setUp() { helper.setUp(); } @After public void tearDown() { helper.tearDown(); } // run this test twice to prove we're not leaking any state across tests private void doTest() { DatastoreService ds = DatastoreServiceFactory.getDatastoreService(); assertEquals(0 ds.prepare(new Query(""yam"")).countEntities()); ds.put(new Entity(""yam"")); ds.put(new Entity(""yam"")); assertEquals(2 ds.prepare(new Query(""yam"")).countEntities()); } @Test public void testInsert1() { doTest(); } @Test public void testInsert2() { doTest(); } @Test public void foo() { assertEquals(4 2 + 2); } } In Eclipse have you added all the jars mentioned here? You need a few more jars than just appengine-testing.jar. However if your tests or code under test have these dependencies you'll need a few more JARs on your testing classpath: ${SDK_ROOT}/lib/impl/appengine-api.jar ${SDK_ROOT}/lib/impl/appengine-api-labs.jar and ${SDK_ROOT}/lib/impl/appengine-api-stubs.jar. These JARs make the runtime APIs and the local implementations of those APIs available to your tests. Thanks Peter that did the trick. The documentation made it sound like an either or situation (at least to me) but once I added the three jars mentioned above everything worked. Mark and Peter what does your .classpath file look like? Mine has lines like """" but this doesn't work. Is ""${SDK_ROOT}"" code or am I to fill the path in?"
589,A,JSF unit testing I'm trying to find a practical unit testing framework for JSF. I know about JSFUnit but this is very impractical to me. I need to include about 10 JARs to my project and jump through many other hoops just to get it running. I realize that -- due to the need to simulate a platform and a client -- unit testing web applications is difficult. But is there a better way? What frameworks are you using with JSF or are you using JSF raw? I'm using richFaces for AJAX functionality and facelets for my rendering. Also I'm using JSF RI 1.2 for my implementation. For my app server I'm using glassfish v2. Some responders mention Selenium. Note that Selenium isn't unit testing tool. It's for integration testing an assembled and deployed application. Unit test means testing individual parts of code raised out from environment. I'm with Paul on Selenium being very easy to setup and start working with. I use Selenium IDE in Firefox with some customization at that level then you can export these to other platforms such as Java JUnit tests. It was quite easy to download and launch the selenium-server.jar add the selenium-java-client-driver to my existing Eclipse Maven POM driver project; then launch the same exported JUnit test in Eclipse. I mainly wanted to use Java just for looping which the basic Selenium IDE didn't support. I have configured JSF Unit for my project too which does require more time to configure... more importantly though with in-container tests like JSFUnit changes to the test require rebuilding the WAR redeploying in the container and then executing from Eclipse or via a browser. So for quickly trying a small change this is time consuming. Of course with JSFUnit you have access to all the internals of the JSFSession etc so it depends what granularity of testing you need I guess. I'd be interested if anybody knows a faster way to turnaround changes to a JSFUnit test and execute it. Definitely Selenium tests feel more like JUnit tests in that regard.  On the project I'm working on at the moment we dabbled with using selenium. We actually spent a lot of time writing these selenium tests but found that they added little value because the UI changes so much and you just end up doubling your effort for very little return on investment. Another problem with selenium is that it requires your code to be deployed which means it doesn't play well with unit test frameworks eg maven. What I would say is that writing really good unit tests for your managed beans is invaluable. you can use selenium with maven as integration test  Have you thought about doing integration testing with Selenium or another tool? Selenium allows you to record and run tests directly in the browser. You can also run tests in multiple browsers and on multiple platforms with Selenium Remote Control. Writing unit tests is good but it might provide more to create some functional integration tests rather than unit-testing the presentation layer code. This seems like an very interesting alternative. I will look into it further.  Selenium is superficial jsfunit is inward. I recommend that use jsfunit if project is not simple. Because team member can change jsf managedbean names or etc you can catch that with jsfunit.  HttpUnit can also be an alternative. It provides apis so you have a choice to automate the tests. http://httpunit.sourceforge.net/index.html  Have you taken a look at the jsfunitwar Ant task or alternatively the Maven plugin provided by JSFUnit? Both greatly reduce the complexity of generating the .war file to be tested. I'm using JSFUnit on my current project and find the combination of white box and black box testing capabilities to be very powerful. Because JSFUnit uses HtmlUnit under the covers you can very easily and effectively examine the generated HTML or conversely verify the state of your internal JSF backing beans. I was able to incorporate the JSFUnit tests into my Continuous Integration process and have been quite pleased with the outcome.
590,A,"Java: Can't implement runnable on a test case: void run() collides So I have a test case that I want to make into a thread. I cannot extend Thread nor can I implement runnable since TestCase already has a method void run(). The compilation error I am getting is Error(6217): method run() in class com.util.SeleneseTestCase cannot override method run() in class junit.framework.TestCase with different return type was class junit.framework.TestResult. What I am trying to do is to scale a Selenium testcase up to perform stress testing. I am not able to use selenium grid/pushtotest.com/amazon cloud at this time (installation issues/install time/resource issues). So this really is more of a Java language issue for me. FYI: SeleniumTestCase is what I want to make multi threaded to scale it up for stress testing. SelniumTestCase extends TestCase (from junit). I am extending SeleniumTestCase and trying to make it implement Runnable. Keep in mind that if the thread throws any exceptions the test would not necessarily fail. Instead of using Runnable and Thread you might want to use ExecutorService.submit(Callable<T>): public class SeleneseTestCase extends SeleniumTestCase { private class StressServer implements Callable<Void> { public Void call() { // do your work here return null; } } public void testUnderLoad() throws Exception { ExecutorService executorService = Executors.newFixedThreadPool( NUM_CONCURRENT_WORKERS); List<Callable<Void>> stressers = new ArrayList<Callable<Void>>(); for (int i = 0; i < NUM_WORKERS; i++) } stressers.add(new StressServer()); } List<Future<Void>> futures =if ( executorService.invokeAll( stressers TIMEOUT_IN_SECS TimeUnit.SECONDS); for (Future<Void> future : futures) { if (!future.isCancelled()) { future.get(1 TimeUnit.MILLISECONDS); // may throw exception } } executorService.shutdown(); } } Note if you want the workers to return the result you can change the type of StressServer to Callable<YourResultType> Heheh. I did end up doing this.  extending TestCase is almost certainly not the correct OO model anyhow. model with ""has a"" not ""is a"". that is create a new class that extends thread and use instances of that class within your test methods. Extends Thread class isn't a correct OO model either unless you're adding behavior specific for threads which isn't common. Implements Runnable is the best option when we want to execute things in another thread  Create a inner class that implements Runnable and call it from a new Thread in com.util.SeleneseTestCase run() method. Something like this: class YourTestCase extends SeleneseTestCase { public class MyRunnable implements Runnable { public void run() { // Do your have work here } } public void testMethodToExecuteInThread() { MyRunnable r = new MyRunnable(); Thread t = new Thread(r); t.start(); } } Update to use outside YourTestCase class To run an inner class from another class you would need to make it public and then from the outer class execute this: YourTestCase testCase = new YourTestCase(); YourTestCase.MyRunnable r = testCase.new MyRunnable(); But if you don't need to call it from inside your test case you'd better go with a normal class make MyRunnable a public class without being in YourTestCase. Hope it helps. This won't compile since `void run()` can't be defined inside of SeleneseTestCase. Just remove void run() from YourTestCase and use the appropriate method from SeleneseTestCase but you can keep the implementation to call MyRunnable. I don't know Selenium I can't exactly which method you should override but this the idea. Hm I can't seem to instantiate the Runnable inner class outside of that class which is what I am trying to do. For example ` new Thread(YourTestCase.MyRunnable.class.newInstance()).start();` causes an InstantiationException. Edited to show how to instantiate an inner class from any other class.  In this case you don't have other option: you have to delegate to another object instead of inheritance."
591,A,"Testing simultaneous calls to transactional service How should I test a service method that is transactional for its simultaneous use (it updates a database row by decreasing a value)? I have setup a JUnit test class with SpringJunit4ClassRunner and components are @autowired. Just spawning threads which would call the method doesn't seem to work. I'm not sure whether this has something to do with the Spring proxy mechanism. What I would like to achieve is to create a situation where simultaneously two threads are ""inside"" the tested method and other one will fail and rollback. e.g. The row value is 3 and both method calls try to decrease the value by 2; if the method wouldn't work the value would be -1 which is illegal. But I want that either both of the calls fail and rollback or failing the one that tries to update it an instant later than the other. Is this even possible? You should explain exactly what isn't working when spawning threads. I either got a nullpointer exception at the very start of the callable method (it was just a private method call) or hibernate exception that said that table didn't exist which was weird since the same call in a test method that hadn't threads worked fine. The first problem is that the transaction context is bound to one thread (with a thread local). So you have to start a transaction in each of your threads. (I think there is no support for this in spring. You can start transaction programmatically with the transaction manager.) The code you described: read decrement write does only work with the right isolation level (serialized and repeatable read would work). After this setup is done you can test the behavior by blocking one thread while he has the database lock. You can use a Latch for this. The thread without database lock will now still not rollback. It will block until the database lock is available again. The scheme you're describing is quite similiar to Optimistic concurrency control so maybe this is already implemented. This sounds pretty much what I wanted. I'll try to see if I get everything to work. Thank you!"
592,A,"Don't startup spring context in uni-test I have web application on spring mvc and maven. When I execute ""mvn clean install"" I got nullpointerexception from some uni-test. It's happens because one of resource is null but why ? Uni-test: package myapp.services.impl .... @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""classpath:META-INF/spring/applicationContext.xml""}) @TransactionConfiguration public class MyServiceImplTest { @Resource private MyService myService; @Transactional @Test public void someTest() { SomeEntity entity = new SomeEntity(); myService.createSomething(entity); // THROW - NullPointerException myService is NULL ... } } surefire plugin in pom.xml:  <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.4.3</version> <configuration> <systemProperties> <property> <name>myapp.env</name> <value>test</value> </property> </systemProperties> <junitArtifactName>org.junit:com.springsource.org.junit</junitArtifactName> </configuration> </plugin> And applicationContext.xml: ... <context:component-scan base-package=""myapp.servicesmyapp.services.impl""/> <context:property-placeholder location=""classpath*:META-INF/spring/common_${myapp.env}.properties"" system-properties-mode=""OVERRIDE"" ignore-resource-not-found=""true""/> .... PS: When I execute this test in eclipse Run as JUnit Test - execute is fine without exception Stack-trace after surefire report: Test set: myapp.services.impl.MyServiceImplTest Tests run: 1 Failures: 1 Errors: 0 Skipped: 0 Time elapsed: 1.115 sec <<< FAILURE! myapp.services.impl.MyServiceImplTest.someTest() Time elapsed: 1.068 sec <<< FAILURE! java.lang.NullPointerException at myapp.services.impl.MyServiceImplTest.someTest(MyServiceImplTest.java:42) MAVEN OUTPUT: ... [INFO] --- maven-surefire-plugin:2.4.3:test (default-test) @ mywebapp --- [INFO] Surefire report directory: /home/xxx/Work/mywebapp/target/surefire-reports ------------------------------------------------------- T E S T S ------------------------------------------------------- Running myapp.services.impl.MyServiceImplTest log4j:WARN No such property [maxFileSize] in org.apache.log4j.DailyRollingFileAppender. log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender. Tests run: 1 Failures: 1 Errors: 0 Skipped: 0 Time elapsed: 0.919 sec <<< FAILURE! Results : Failed tests: myapp.services.impl.MyServiceImplTest.someTest() Tests run: 1 Failures: 1 Errors: 0 Skipped: 0 [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time: 41.465s [INFO] Finished at: Mon May 09 00:05:06 MSD 2011 [INFO] Final Memory: 26M/70M ... Can you post the stacktrace and the output of the context loading from the test execution. Sure! I've added output in question after ""Stack-trace after surefire report:"". But I don't think that it help. You haven't attached the output of the spring context loading. Without this my guess would be that you have a classpath issue. The application-context.xml is probably not on the maven test classpath. Generally when something works in Eclipse and not in Maven I find this is the case. Try copying the application-context.xml in the test/resources area. Do you mean maven build output or test execution output? I don't have spring context output when I execute mvn clean install... ""Try copying the application-context.xml in the test/resources area"" yes - I've tried to do but does not change anything... :( The test execution output during the maven build should show the loading of the spring application context for the test. Ok I've attached output please see Can someone help me ? Can you show me MyService implementation and its appcontext configuration? Immediately after the 'Running myapp.services.impl.MyServiceImplTest' you should be seeing some logging about loading the spring context. Similar to (XmlBeanDefinitionReader.java:315) INFO org.springframework.beans.factory.xml.XmlBeanDefinitionReader - Loading XML bean definitions from class path resource [META-INF/spring/application-context.xml]. Since you are not seeing this log information I don't think the spring context is getting loaded and that is what is causing the null pointer. If fact it would look like the test isn't being runwith the SpringJUnit4Runner. Just a thought but... You say you've placed applicationContext.xml in the test/resources folder - have you recreated the directory structure you define within that directory? I.e. test/resources/META-INF/spring/applicationcontext.xml? If not then the applicationcontext will appear in test-classes/ and won't be found according to the classpath location you've defined... If it's just in test/resources change your @ContextConfiguration annotation to @ContextConfiguration(locations = {""classpath:applicationContext.xml""})` `"
593,A,"How to access Spring context in jUnit tests annotated with @RunWith and @ContextConfiguration? I have following test class @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""/services-test-config.xml""}) public class MySericeTest { @Autowired MyService service; ... } Is it possible to access services-test-config.xml programmatically in one of such methods? Like: ApplicationContext ctx = somehowGetContext(); Since the tests will be instantiated like a Spring bean too you just need to implement the ApplicationContextAware interface: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""/services-test-config.xml""}) public class MySericeTest implements ApplicationContextAware { @Autowired MyService service; ... @Override public void setApplicationContext(ApplicationContext context) throws BeansException { // Do something with the context here } } Not possible in Spring 3.0 and up up to this date. Weird I am running all my Spring 3.0 tests like that...  If your test class extends the Spring JUnit classes (e.g. AbstractTransactionalJUnit4SpringContextTests or any other class that extends AbstractSpringContextTests) you can access the app context by calling the getContext() method. Check out the javadocs for the package org.springframework.test.  This works fine too: @Autowired ApplicationContext context;"
594,A,"Under what conditions should I test get() and set() methods? I could not confirm whether to do these tests. It seems the set and get method is so simplesuch as: setA(String A) { this.A = A; } getA(){ return A; } Any ideas would be appreciated! Thanks Joseph Unit tests are supposed to be the documentation for how the system is supposed to work. Although people often skip unit tests for Properties because the functionality is trivial if the tests are the documentation and especially if someone else is going to do the implementation then tests for Properties should be written. That said when I am both writing the tests and doing the implementation I usually skip writing tests for Properties unless they do something more than a simple get/set or if I have spare time which is a rare thing.  I've only seen a very few problems with getters and setters in the wild and only one of those could have been detected via a unit test and only then if all of the getters and setters were tested together rather than in individual test methods. Consider the copy/paste mistake of reusing the same field from two different pairs of getters/setters. I.e. public void setA(Object a) { this.a = a; } public Object getA() { return a; } public void setB(Object a) { this.a = a; } public Object getB() { return a; } Unit tests that focus on one setter/getter pair at a time won't expose the problem. Modern IDEs will generate getters and setters on request so this mistake is unlikely but not everyone uses modern IDEs. (A vi user created the bug above.) And if these methods reside in a simple data-holder object the problem may only show up a bit far from the cause. For this reason if I test getters and setters at all (and I often don't) it's from a single test method that calls all of the setters first with distinct values then asserts on all of the the getters. One problem you've got to live with though is that there's no guarantee that a method that starts life as a ""simple"" getter or setter will stay that way when someone else gets their hands on the code and decides say that a geetter is a good place do something that involves a side-effect.  General rule: Not much point in writing tests for getters and setters. Only if they have some additional logic ie. are not pure accessors you should write the tests. Well one example where the setter needs to do something else is when the setting of a property must fire an event. And this case should of course be covered with tests. If they're not pure accessors it's arguably misleading to have them as getters and setters (imho). @Cletus: I agree with that. Having getters and setters never be allowed to do anything but set private variables is not really any meaningful form of encapsulation - you may as well have public fields! @Steven it's a blurry line. At some point the level of side effects riches a point where ""accessoor"" is not an accurate term.  Writing test cases for methods which can't fail seems disproportionate to me. Only if the value of A is initialized by configuration or something which could fail it is worth testing. EDIT: Another example when testing makes sense could be a flag 'overdrawn' if an accounts balance becomes negative and you want to check whether the flag was set correctly after calling a method withdraw().  A smart man once said ""Test until fear turns to boredom"". If you no longer fear that your super-simple code will break don't write tests unless you're not bored writing those tests. And don't write tests just to ""improve your metrics"" that's just gaming the system. Write tests to make sure your code works to improve robustness and to create confidence that you can refactor freely.  The only time I would write tests specifically for set() and get() methods is if there is some sort of logic inside them. Eg. limit an integer to between 1 and 8  public void SetA(int a) { if(a > 8 || a < 1) { throw new IndexOutOfBoundsException(); } this.a = a; } Even though the code above is a very simple example when you do this type of logic it can be a good idea to run a test on them. Mainly for when your business rules change and you have to limit it to between 9 and 1 :) Yes it is necessary.  They are all same I say like blank interfaces or business classes. Preprocessing should enable all needed or they are other kinds (doers that both shall return like respond and take 2 variables) language agnostically (even POSIX exit that now is void should use arguments since knowing way is very important)  Yes in your case they are trivial - but on the other hand - two simple tests that fully count for quality metrics ;-) I would create tests. Your application actually relies on the behaviour that the methods really store/access the field values and do not change anything. Maybe one day someone decides to change a field type or to add some unit conversion code to a setter/getter - a test will show if the code still works or it will show that more work is needed. I would argue that the property would get used in other testcases anyway. If it weren't it shouldn't be there. A practical compromise for a bean: one test that covers all 'trivial' getters and setters. But public methods are an interface to classes and thus should be tested and reliable.  Make a cost/benefit analisis What would it gain knowing that the private variable indeed get read/written What would it cost the time taken to write the testcase the time spend each time executing your testsuite If you know there are no observable side-effects calling the getter or setter I wouldn't bother."
595,A,"HTML Custom JUnit Report Uneven Table Alignment I am coding a java class that generates HTML table reports for JUnit tests and use CSS for visual formatting. I am having an issue aligning the cells since the number of colummns generated is unforseeable since some of these columns represent parameters passed into a variadic function. Therefore there is inherent misalignment in the columns. Is there any way to align these cells through a CSS attribute or something? I dont really want to alter the underlying java code to change this aesthetic issue. Here is what a sample table generated would look like: Here is the HTML source for the table (formatted properly):  <html> <head> <style type=""text/css""> td { font-family: ""Trebuchet MS"" Arial Helvetica sans-serif; font-size: 1em; border: 1px solid black; padding: 3px 7px 2px 7px; } </style> </head> <body> <table> <tr> <td> <b>Method:</b> <font color=""blue"" face=""Verdana""> testOne </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 1 </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 1 </font> </td> <td> <b>Result:</b> <font color=""green"" face=""Verdana""> Passed </font> </td> </tr> <tr> <td> <b>Method:</b> <font color=""blue"" face=""Verdana""> testTwo </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> BMW </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Audi </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Mercedes </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Porsche </font> </td> <td> <b>Result:</b> <font color=""green"" face=""Verdana""> Passed </font> </td> </tr> <tr> <td> <b>Method:</b> <font color=""blue"" face=""Verdana""> testThree </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 21154423 </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 2443 </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 12121 </font> </td> <td> <b>Result:</b> <font color=""green"" face=""Verdana""> Passed </font> </td> </tr> <tr> <td> <b>Method:</b> <font color=""blue"" face=""Verdana""> testFour </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 4.1222 </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 2.0001 </font> </td> <td> <b>Result:</b> <font color=""red"" face=""Verdana""> Failed </td> </tr> <tr> <td> <b>Method:</b> <font color=""blue"" face=""Verdana""> testFive </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 10 </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> 10 </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Kungsholmens Hamn </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Melissa Horn </font> </td> <td> <b>Result:</b> <font color=""green"" face=""Verdana""> Passed </font> </td> </tr> <tr> <td> <b>Method:</b> <font color=""blue"" face=""Verdana""> testSix </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Sweden </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Sweden </font> </td> <td> <b>Result:</b> <font color=""green"" face=""Verdana""> Passed </font> </td> </tr> <tr> <td> <b>Method:</b> <font color=""blue"" face=""Verdana""> testSeven </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Lisa Ekdahl </font> </td> <td> <b></b> <font color=""purple"" face=""Verdana""> Lisa Ekdahl </font> </td> <td> <b>Result:</b> <font color=""green"" face=""Verdana""> Passed </font> </td> </tr> </table> </body> </html> </table> </body> </html> Any browser requirements? I want it to render properly in at least IE and Chrome. I don't think you will be able to that with simple CSS. You don't want to change your java code for an aesthetic issue but the problem is that your table really shouldn't have a changing number of columns. The parameters columns should be only one column with some other means to separate the values. You could generate a comma separated list of the parameters in the second column. That way you would always have 3 columns and the thing will be far easier to format properly. If you really want to keep the values in separate columns you should use the colspan attribute on the last parameter. For example the last parameter column could look like this with in your java string (or whatever you use to generate the html) looking somewhat like : ""<td colspan=""+ (maxNumberOfColumns-currentColumnIndex) +"">"" I strongly suggest you adjust you java code to do this as the alternatives (modifying with javascript) are a lot more painful (even with things like JQuery). Yes I started coding logic to store the lines of Strings to a buffer and then iterate through them all adding empty cells to compensate for the uneven column number. It does seem that modifying the Java code is the only solution. Thanks for your suggestion."
596,A,"JUnit Eclipse plugin source-code? I'm looking into writing an Eclipse plugin for FlexUnit and was wondering where I could get the sources for the JUnit Eclipse plugin. I checked the JUnit sources at sourceforge but couldn't spot any code that looked like the plugin code. Any idea where this code is available? See also: http://stackoverflow.com/questions/1084176/how-do-i-configure-junits-source-in-eclipse Since you are in all likelihood using Eclipse there's a far easier way to import it right into your workspace. The source is bundled with your eclipse distribution. Just do File -> Import -> Plug-ins and Fragments Keep defaults (""Active target platform"" ""Select from all plug-ins"" & ""Projects with source folders"" Hit next and select the JUnit plugin-packages and hit Finish and they will be imported as Eclipse projects into your workspace. This is awesome!! However it does not give source of Junit 4.8.2xxx  There are now git mirrors of the CVS repositories: org.eclipse.jdt.junit: git://dev.eclipse.org/org.eclipse.jdt/org.eclipse.jdt.junit.git org.eclipse.jdt.junit.core: git://dev.eclipse.org/org.eclipse.jdt/org.eclipse.jdt.junit.core.git org.eclipse.jdt.junit.runtime: git://dev.eclipse.org/org.eclipse.jdt/org.eclipse.jdt.junit.runtime.git org.eclipse.jdt.junit4.runtime: git://dev.eclipse.org/org.eclipse.jdt/org.eclipse.jdt.junit4.runtime.git It's also mirrored on Github. For example org.eclipse.jdt.junit can be found at: https://github.com/eclipse/eclipse.jdt.ui @porcoesphino the junit ones don't seem to be there. @porcoesphino sorry you're right didn't see them. Define ""ones"". You can find org.eclipse.jdt.junit at the link above or (slightly) more directly with this link: https://github.com/eclipse/eclipse.jdt.ui/tree/master/org.eclipse.jdt.junit  You can find it on Eclipse's repository: http://dev.eclipse.org/viewcvs/index.cgi/org.eclipse.pde.junit/ Awesome thanks! the CVS repo seems deprecated"
597,A,"What is the ""right"" way to test the output of Java methods? In Python I often have tests which look something like this: tests = [ (2 4) (3 9) (10 100) ] for (input expected_output) in tests: assert f(input) == expected_output What is the ""right"" way to write tests like this (where a set of test cases is specified then a loop runs each of them) in Java with JUnit? Thanks! Preemptive response: I realize I could do something like: assertEquals(4 f(2)) assertEquals(9 f(3)) .... But... I'm hoping there is a better way. With such a simple eaxmple I think the Java way is the better way. Yes with such a simple example but it was just that -- an example. I'm sure you can imagine a situation where you'd need to do a couple lines of setup make the function call then perform a couple of checks on the result. The right way is writing separate assert statements even if you don't like it. It avoids unnecessary complications and when it fails it is sometimes easier to see which value failed (no need to start up the debugger). However if you generate your test data automatically it is a different story. :( That's not very happy. What happens if you need to do a couple of lines of setup? Well you create a new method... But now each test needs two methods instead of one... And... Java makes me sad :( If the setup is the same I would use the JUnit setup method. If it is different I would write a private method with the setup and assert and call it several times.  Have a look at the Parameterized test runner in Junit. http://junit.org/apidocs/org/junit/runners/Parameterized.html It looks like it will do precisely what you are looking for. Awesome -- that looks like what I want. I'll play with it and see. I played with it once but it's been too long ago for me to give you more guidance than a link to the javadoc.  Wouldn't you just define a simple class with two fields real result and expected result and then loop over the collection in a similar way to what your Python snippet is doing? I've considered that... I'd initially thought it was a little bit silly to create all the classes that'd be needed for that... But maybe that's just showing my inexperience with Java.  Same thing.  int[][] tests = { {2 4} {3 9} {10 100} }; for (int[] test : tests) { assertEquals(test[1] f(test[0])); } Certainly not as pretty as python but few things are. You may also want to look into JUnit Theories a future feature... You could make a List of an inner tuple class with one String param representing the parameter and one Integer param representing the result. It's a question of how much work you want to do in order to be correct... Perfect. Minor quibble: it's more Java-ish to use a List rather than an array. Most definitely however if we're speaking to a follower-of-the-python busting out List a = new ArrayList(); a.add(); a.add(); a.add(); their eyes will bleed! I don't see why a List is better than an array in this case. Better? No. More Java-ish? Sure why not... I've been using this a bit... But it starts to break down when you have many types: what if 'f' accepts a string? You'd just use a 2D String array. This scales exactly the same as the python example. Only difference is the initial type declaration for loop syntax {}'s and ;'s. Err sorry ""f accepts a string and returns an int"" is what I meant to say. You'd get an object array then need to cast it later... And... Python has spoiled me :(  Definitely not an expert on unit testing but i would prefer to have a separate method for each case that I am testing against and use some test running tools (like NUnit-GUI for C#). That way I would exactly know which case fails if it does. Its more work to do but i think it eventually pays off well. I tend to agree however if you parameterize your failure messages you can get this information out. This becomes ridiculous very very quickly though... Say you've got three utility functions: for each one you should be testing at least three cases (zero case one case many case). All of a sudden you need nine methods each of which would no doubt duplicate code.  There are no tuples in Java but you could use a Map or two parallel arrays to specify input/output pairs and then do a loop just like your Python example.  Um... int[][] tests = new int[][]{ {2 4} {3 9} {10 100} }; for(int[] i : tests) { assertEquals(i[1] f(i[0]); } Same thing really. The only problem is Java's lack of a tuple literal so for more complex cases you'll have to use Object[] arrays and cast or write a Tuple class. As you mention you've got to use Object[] arrays and casting later... Which is no fun... But it's looking like there is no ""nice"" way around that. Oh well."
598,A,"Test-resources of dependecies not in classpath? I have a multi module Spring project that I set up using Maven: my-root (pom) - my-logic - my-webapp (depending on my-logic) - my-consoleapp (depending on my-logic) My Test classes inherit from AbstractTransactionalJUnit4SpringContextTests and use @ContextCofiguration for setting up the ApplicationContext. E.g. the test class for a Spring Controller: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:applicationContext-logic-test.xml"" ""classpath:applicationContext-web-test.xml""}) public class ControllerTest extends AbstractTransactionalJUnit4SpringContextTests { @Autowired private ApplicationContext applicationContext; ... } As you can see there is a config XML per module. I have seperate configs for tesing residing in test/resources of each module (and additionaly having the suffix ""-test""). This all works (the class compiles runs and the JUnit tests are successful) if I run the JUnit test in Eclipse. Now to my problem: Running the test using Maven will NOT work! (e.g. with ""Run As"">""Maven install"" on my-root (I use m2eclipse)). Specifically it will throw the following exception: java.io.FileNotFoundException: class path resource [applicationContext-logic-test.xml] cannot be opened because it does not exist` It seems that Maven does not add the files from my-logic/src/test/resources to the classpath that is set up when running the unit tests of my-webapp. How can I fix that? That's by design. Test resources are not placed into the output artifact so any test dependencies of something you depend on would not be accessible even in your unit test classpath. If you want to accomplish what you're trying to do you should create a project which contains the resources you need for testing and make it a test scoped dependency of both my-logic and my-webapp  It seems that Maven does not add the files from my-logic/src/test/resources to the classpath that is set up when running the unit tests of my-webapp. No indeed it doesn't. First Maven uses binary dependencies that are always resolved through the local repository. And second binary dependencies don't include test stuff. But what you could do is: configure the my-logic module to create a test JAR using jar:test-jar configure the my-webapp module to depend on this test JAR (using a test scope). For #1 you'll have to configure the Maven Jar Plugin in the pom.xml of my-logic: <project> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <version>2.2</version> <executions> <execution> <goals> <goal>test-jar</goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project> And Maven will create a JAR with the content of target/test-classes during package and install / deploy it. For #2 declare a dependency on the test JAR in the pom.xml of my-webapp: <project> ... <dependencies> <dependency> <groupId>com.myco.app</groupId> <artifactId>foo</artifactId> <version>1.0-SNAPSHOT</version> <type>test-jar</type> <scope>test</scope> </dependency> </dependencies> ... </project> That should do it. Thanks a lot for this excellent answer! I used this solution and it works. Now that you explained this it's obvious to me as well that test resources of all (external) dependencies are not included in the classpath. How should they? But somehow it would be nice if resources from my own modules would be included. Even though I don't know how exactly this would work now that I think about it ;) @apropoz You're welcome. The idea with Maven is that modules are self contained: you can checkout a single module without its brothers and build it. Anything that would involve relative paths would break this which is not desirable."
599,A,"Parameterized Unit Tests in Scala (with JUnit4) Is there a way to implement a parameterized unit test with Scala? Currently I use JUnit4 in the rest of my programs and I would like to continue using only ""standard"" APIs. I found an example for Junit4 with Groovy but I have problems defining the static parts. Could be because I am also quite new with Scala :-) I am currently as fas as  import org.junit.Test import org.junit.Assert._ import org.junit.runner.RunWith import org.junit.runners.Parameterized import org.junit.runners.Parameterized.Parameters @RunWith(classOf[Parameterized]) class MyTest extends junit.framework.TestCase { @Parameters object data { ... } @Parameter ... @Test def testFunction() = { } That's quite a nuisance but it works. Two important things I discovered: companion object must come after test class the function returning the parameters must return a collection of arrays of AnyRef (or Object). arrays of Any won't work. That the reason I use java.lang.Integer instead of Scala's Int. import java.{util => ju lang => jl} import org.junit.Test import org.junit.runner.RunWith import org.junit.runners.Parameterized import org.junit.runners.Parameterized.Parameters @RunWith(value = classOf[Parameterized]) class JUnit4ParameterizedTest(number: jl.Integer) { @Test def pushTest = println(""number: "" + number) } // NOTE: Defined AFTER companion class to prevent: // Class com.openmip.drm.JUnit4ParameterizedTest has no public // constructor TestCase(String name) or TestCase() object JUnit4ParameterizedTest { // NOTE: Must return collection of Array[AnyRef] (NOT Array[Any]). @Parameters def parameters: ju.Collection[Array[jl.Integer]] = { val list = new ju.ArrayList[Array[jl.Integer]]() (1 to 10).foreach(n => list.add(Array(n))) list } } The output should be as expected: Process finished with exit code 0 number: 1 number: 2 number: 3 number: 4 number: 5 number: 6 number: 7 number: 8 number: 9 number: 10 perfect! will try it out soon!  You are probably better off with ScalaTest or Specs. The latter definitely supports parameterized tests and is widely used in the Scala community. JUnit's syntax for parameterized tests is pretty horrible and its reliance on static declarations won't make your task easier in Scala (probably you need a companion object)."
600,A,"How to write custom constraint using Jmock when the method to be mocked is having multiple argument I am facing difficulty in writing Unit test case for some large code base where I have to mock a lot of classes so that I can proceed with the testing easily. I found in the API documentation of Jmock that the customeconstraint I can use is containing a method eval(Object argo) Which will return true if the argument is meeting the expectations. But my method is invoked with multiple arguments. How can I evaluate the arguments and make sure that the arguments with which the method was invoked is correct. Thanks in advance. Often it is sufficient to create objects that are equal to the expected parameter values: context.checking(new Expectations() {{ allowing(calculator).add(1 2); will(returnValue(3)); DateTime loadTime = new DateTime(12); DateTime fetchTime = new DateTime(14); allowing(reloadPolicy).shouldReload(loadTime fetchTime); will(returnValue(false)); }}); JMock also provides some predefined constraints: context.checking(new Expectations() {{ allowing(calculator).sqrt(with(lessThan(0)); will(throwException(new IllegalArgumentException()); }}); You also can use a custom matcher using with: context.checking(new Expectations() {{ DateTime loadTime = new DateTime(12); allowing(reloadPolicy).shouldReload(with(equal(loadTime)) with(timeGreaterThan(loadTime)); will(returnValue(false)); }}); Here timeGreaterThan could be defined as: public class TimeGreaterThanMatcher extends TypeSafeMatcher<DateTime> { private DateTime minTime; public TimeGreaterThanMatcher(DateTime minTime) { this.minTime = minTime; } public boolean matchesSafely(DateTime d) { return d != null && minTime.isBefore(d); } public StringBuffer describeTo(Description description) { return description.appendText(""a DateTime greater than "").appendValue(minTime); } public static Matcher<DateTime> timeGreaterThan(DateTime minTime) { return new TimeGreaterThanMatcher(minTime); } } See the JMock Cookbook for more information"
601,A,"Can I use jtracert to sequence diagram a unit test running in eclipse? How would I go about using jtracert to sequence diagram a junit test being run within eclipse? In the ""Run"" dialog select your JUnit configuration and go the ""Arguments"" tab. Add jTracert parameters to ""VM arguments"" section (something like -javaagent:/home/dmitrybedrin/work/jtracert/deploy/jTracert.jar) Click on ""Run"" button. You will see the following in the ""Console"" tab: jTracert agent started Waiting for a connection from jTracert GUI on port 7007 Now execute jTracert gui and connect to the jTracert agent. That's it - you will see the sequence diagrams now! You so deserve a medal for this answer. I was extremely frustrated trying to get this to work... Shoot me. So happy now that it does. Thank you  There's a small mistake in my previous answer - you should specify -javaagent:/home/dmitrybedrin/work/jtracert/deploy/jTracert.jar=7007 to ""VM arguments"" section."
602,A,"Creating an Android JUnit Test project in Eclipse How do I create an Android Test project in Eclipse in the target project's /test directory? Note: I'm just doing my own Q+A for people according to the Stackoverflow FAQ on answering your own question. I found here a very detailed solution to this problem. It seems to be an ADT bug. http://jonblack.org/2012/11/24/creating-an-android-test-project-within-a-project Still when I import the test project back into the workspace at Step 5 I get the following error message: java.io.FileNotFoundException: C:\projects\MyApp\com.myapp.test-test\project.properties ( The system could not finde the file specified ). I simply closed the error dialog and the test project appeared in my workspace at the same level as my android project but the location was correctly set to MyApp/tests. So I wrote a test run it and it worked.  I am using Helios with the latest current version of Android (10.0.1) The Android Testing fundamentals recommends you place the tests in the same project folder as your src folder. Some people have recommended that you create a new test project in its own folder so it doesn't get packaged into the apk. I looked inside my apk and I don't think the test stuff was included (but I could be wrong). I like having it in the same directory as its more convenient for revision control - you don't need to sync two projects each time. Assume we want to create a test suite for the MyAndroidApp project Goto File.. New.. Other.. Android...Android Test Project Set ""Test Project Name"" to MyAndroidAppTest For Test Target choose ""An existing Android project"".. Use the Browse button to select MyAndroidApp Unselect ""Use default location"". This changes the location to be inside the MyAndroidApp project under the tests directory Click Finish When it is being created if you get a An internal error occurred during: ""Refreshing workspace"". error (or other errors) just ignore it. Click OK. -Clean the MyAndroidApp project Even though the project is in the <workspace path>/MyAndroidApp/tests directory it will appear as a normal Eclipse project in the package explorer. Clean the MyAndroidAppTest project. This worked for me. EDIT If you upload to SVN in Eclipse (by syncing the MyAndroidApp project not the MyAndroidAppTest project) you need to do a little more (I've just done it now). So when you update your other working copy it will pull down the /tests directory. The MyAndroidAppTest project does not automatically show up in the Package Explorer. So you need to go to File.. Import...Existing Projects into Workspace.... Then select the root directory by browsing to your <workspace path>/MyAndroidApp/tests directory and click Finish. Works fine. No SVN issues for me. I'm using Ubuntu 10.04 Helios Subclipse and Unfuddle. When I do this I get the test project in a subdirectory ""MyAndroidAppTestTest"" (yep with two Tests) rather than ""tests"". Thanks man you just stoped my stress. @Kurt if you rename your ""MyAndroidAppTestTest"" to ""tests"" and then create new project from existing source and then again the new created project to ""MyAndroidAppTest"" problem solved. I have the same problem than @Kurt is this a bug ?. I also have the same problem as @Kurt and Sergio and I agree it seems like a bug. If you have this problem too please star the bug report I have created here: https://code.google.com/p/android/issues/detail?id=54876 Btw @pharaoh please could you explain exactly the steps you meant when you advised to ""create new project from existing source""? Thanks in advance! My project is for some reason not added properly and I get following error when trying to run (it can't run the correct project): Activity does not specify a android.test.InstrumentationTestRunner instrumentation or does not declare uses-library android.test.runner in its AndroidManifest.xml"
603,A,"Debugging breakpoints on JUnit test in Eclipse are not working I am trying to debug a junit test in eclipse but my breakpoints are not firing (unless they are on the first or second line). I've tried deleting and recreating all breakpoints in the workspace cleaning the project creating a new debug configuration and running the test method individually and as part of a test class with other methods. But all to no avail :-(  public void testLoadPatientsAndConvertToBeans() throws IOException CDataGridException { File file = fileutil.getFileFromPrefsOrPrompt(basefileDef); CDataBuilder builder = new CDataDelimitedFileBuilder(file CDataDelimitedFileBuilder.DelimiterSettings.WINDOWS_CSV basefileDef); // breakpoints placed on lines from here on do not fire CDataCacheContainer container = cacheIO.construct( new CDataNarrower( cacheIO.construct(builder) ).setConvertMissing(true)); assertEquals(13548 container.size()); cacheIO.export(container patients); Collection<Patient> pBeans = patients.getBeans(); assertEquals(container.size() pBeans.size()); Patient patient = pBeans.iterator().next(); Map props = patient.getPropertyMap(); System.out.println(props); } My first intuition is that the cached class in eclipse is out of sync with your codes. However given you have tried clean/rebuild your project it should have fixed it. As you can put a breakpoint on the 1st/2nd line what happens when you step through the codes? Does the code align with each steps? If not it shows that the eclipse has a different version of class from your source codes. If they are the same I would try to down a newer copy of eclipse (I assumed you have already tried restart your eclipse) since you may have discovered a strange bug (new version may have fixed it or clean some stale data). Sorry that I can't be any more helpful. The code does step through and aligns. Thanks Oscar.  This is likely if you are using Sun JDK 6 Update 14. See another similar SO question here. The likely resolution in such a case is to use Sun JDK 6 Update 16. Upgrading to Sun JRE 6 Update 16 did the trick. Thanks Vineet. Unfortunately I'm getting ""Vote too old to be changed unless the answer is edited"" and can't upvote this. Can you edit and then I will? +1 @tukushan you should be able to vote for it now if you haven't already @Rich thanks for the edit."
604,A,Is there a way to distribute junit tests so they all get done faster? Our JUnits take a total of 6 hours to run. Is there an easy way to run 1/n of them on n different machines? Seems like you are witnessing http://www.infoq.com/presentations/integration-tests-scam ;) Just off the top of my head one option is to use TeamCity and have different build scripts that know how you want to partition the tests and set those up as separate projects (each partition) and then set up n agents which can be done using Amazon EC2 allowing for large values of n. Since you probably want more than 3 agents you would be out of their free product territory. Some assumptions here: There is no common database that ties all of these tests together and all the tests can otherwise run independently of each other. If there is a common database that makes things much more complicated as you would need a database for each agent so the tests don't step on each other (and of course farming out the computing power to EC2 would probably be impractical).  GridGain (a free cloud implementation) is able to distribute JUnit tests runs across a cluster of nodes. See Distributed JUnit Overview. Just in case this is not exactly what you're asking for but TestNG can run tests in parallel (thi would already make your build faster). See Advanced parallel testing with TestNG and data providers. See also Running JUnit in Parallel Parallelizing JUnit test runs
605,A,Why do certain JUnit tests not get executed in NetBeans and/or Hudson? When I right click a project in NetBeans and choose Test certain unit tests aren't running. If I right click the .java file that contains the unit tests and choose Test File the unit tests do run. Also I have this project in Hudson CI and those same unit tests don't get run. Only the tests in classes named after a class in the project with the suffix Test will get executed. For example tests in unit test class FooTests will never run (suffix isn't exactly Test). Tests in unit test class FooTest will run only if the project has a class named Foo. It would be nice to know why it has to be this way... Edit: I was only partially correct in my initial diagnosis. I found this forum post which explains that it is a NetBeans/JUnit issue. The NetBeans ant target that runs tests only looks for tests in all files matching the mask: *Test.java. You can see this in the -do-test-run target in build-impl.xml in the nbproject folder. Thus you just have to make sure all unit test classes end with the suffix Test and you should be fine. Seems to me its because your tests should be specific to each class. The JUnit approach to testing is that each application class has a corresponding class to test it. Which is the behaviour you're seeing. After further research it actually doesn't matter that each class have a corresponding test class. See my edit in the answer for an explanation.
606,A,"How to let maven run a single test class with non-default profile activated? I'm trying to let maven run a single test class but I need to use an additional profile (which in fact is already created). Normally when I run: mvn clean install -PmyProfile ""myProfile"" is being activated. So I tried: mvn -Dtest=myTest -PmyProfile test Which resulted in ""[WARNING] Profile with id: 'myProfile' has not been activated."" What am I doing wrong and how can I achieve my goal? I cannot reproduce. I have a ""sandbox"" profile defined in my ~/.m2/settings.xml and the following command just works without complains:  $ mvn -Dtest=AppTest -Psandbox test [INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Building Q3372129 [INFO] task-segment: [test] [INFO] ------------------------------------------------------------------------ ... While using an undefined profile generates the WARNING you're talking about:  $ mvn -Dtest=AppTest -Pfoo test [INFO] Scanning for projects... [WARNING] Profile with id: 'foo' has not been activated. [INFO] ------------------------------------------------------------------------ [INFO] Building Q3372129 [INFO] task-segment: [test] [INFO] ------------------------------------------------------------------------ ... This begs the question: where is this profile defined? myProfile is defined in my pom.xml. I didn't think of defining profiles in my settings.xml. Thanks a lot for your response.  I believe you need a space after the -P before you list the profile(s) you would like. The documentation also always uses that argument at the end of the command. Try: mvn -Dtest=MyTest test -P myProfile For more info on using profiles: Introduction to build profiles Using `-Pmyprofile` is valid. downvote is not mine by the way @Pascal - no worries either way; the answer is wrong I expected a hit. ;) I probably should have tried it first LOL"
607,A,"How to test a mocked JNDI datasource with Spring? I am fairly new to Spring and wondering how to create JUnit tests that use a mocked datasource and how to use a JNDI context with that? Currently my application uses a JNDI context from tomcat to retrieve a connection and via that connection retrieves data from a database. So I guess I need to mock the JNDI calls and the data retrieval. Any good pointers on what the best way to tackle this would be great! Thanks a lot! You can use SimpleNamingContextBuilder to make a jndi datasource available to your tests:  SimpleNamingContextBuilder builder = new SimpleNamingContextBuilder(); builder.bind(""java:comp/env/jdbc/mydatasource"" dataSource); builder.activate(); https://fisheye.springsource.org/browse/spring-framework/spring-test/src/main/java/org/springframework/mock/jndi/SimpleNamingContextBuilder.java?hb=true This isn't exactly mocking the datasource but it does make the datasource available via jndi for your tests. I did that but I'm still getting exception Caused by: javax.naming.NoInitialContextException: Need to specify class name in environment or system property or as an applet parameter or in an application resource file: java.naming.factory.initial  You can create your own mock DataSource by extending Spring's AbstractDataSource. import java.sql.Connection; import java.sql.SQLException; import org.springframework.jdbc.datasource.AbstractDataSource; /** * Mock implementation of DataSource suitable for use in testing. * * */ public class MockDataSource extends AbstractDataSource { private Connection connection; /** * Sets the connection returned by javax.sql.DataSource#getConnection() * and javax.sql.DataSource#getConnection(java.lang.String java.lang.String) * * @param connection */ public void setConnection(Connection connection) { this.connection = connection; } /* * (non-Javadoc) * @see javax.sql.DataSource#getConnection() */ public Connection getConnection() throws SQLException { return connection; } /* * (non-Javadoc) * @see javax.sql.DataSource#getConnection(java.lang.String java.lang.String) */ public Connection getConnection(String username String password) throws SQLException { return connection; } } I'd separate the JNDI lookup of the connection from the rest of the code. Inject the DataSource into your Data Access Objects (DAOs) and use the MockDataSource for testing the DAOs. @Grzegorz Thanks! If I inject the datasource would this not eliminate the need of a JNDI lookup? It could. There are a number of ways in Spring to get the DataSource. Once you have it you can inject. Spring can read a DataSource from JNDI though. I edited your answer to remove the indent of the first line. Now the syntax highlighting works. I hope you don't mind.  You can allways create a beans.test.xml configuration where you first reference the beans.xml and then override the datasource configuration: src/main/resources/beans.xml <!-- Database configuration --> <import resource=""beans.datasource.jndi.xml"" /> src/test/resources/beans.test.xml <import resource=""beans.xml"" /> <import resource=""beans.datasource.test.xml"" /> JUnit Test Class: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:/beans.test.xml"" }) public class ASRTests { ... } In your jndi bean declare the reference <jee:jndi-lookup expected-type=""javax.sql.DataSource"" id=""mysqlDataSource"" jndi-name=""jdbc/mysql""/> In your test bean declare the datasource <bean id=""mysqlDataSource"" ...> ... </bean> Keep in mind to move the test datasource bean into test folder.  I usually define my JNDI dependencies in seperate file like datasource-context.xml: <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:jee=""http://www.springframework.org/schema/jee"" xmlns:p=""http://www.springframework.org/schema/p"" xsi:schemaLocation="" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee-3.0.xsd""> <jee:jndi-lookup id=""dataSource"" jndi-name=""java:comp/env/dataSource"" expected-type=""javax.sql.DataSource"" /> </beans> So that in test resources I can create another file and define the test datasource however it suits me like datasource-testcontext.xml: <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:p=""http://www.springframework.org/schema/p"" xsi:schemaLocation="" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd""> <bean id=""dataSource"" class=""org.springframework.jdbc.datasource.DriverManagerDataSource"" p:driverClassName=""org.hsqldb.jdbcDriver"" p:url=""jdbc:hsqldb:hsql://localhost:9001"" p:username=""sa"" p:password="""" /> </beans> And then in my test class I use the test configuration of the datasource instead of production one that depends on JNDI: @ContextConfiguration({ ""classpath*:META-INF/spring/datasource-testcontext.xml"" ""classpath*:META-INF/spring/session-factory-context.xml"" }) public class MyTest { } If the data source is not defined in a separate file You can still stub the object returned by JNDI calls easily: like this: Injecting JNDI datasources for JUnit Tests outside of a container or using classes in package org.springframework.mock.jndi ie. SimpleNamingContextBuilder (there's an example in the javadoc of this calass). I did that but I'm still getting exception Caused by: javax.naming.NoInitialContextException: Need to specify class name in environment or system property or as an applet parameter or in an application resource file: java.naming.factory.initial @fastcodejava You did what exactly? Used separate file for JNDI related configs? Created JNDI context in the test setup? Or used `SimpleNamingContextBuilder`?"
608,A,"Can selenium handle autocomplete? I have a test case that requires typing in a partial value into an ajax based textfield and verifying the list has the expected content. If it does select the content. Any idea how to make this work? You mean auto-complete as an AJAX feature not as the browser built in functionality? For WebDriver try this The below code is for searching a text automatically from the auto suggest; mainly for a list item. driver.findElement(By.id(""your searchBox"")).sendKeys(""your partial keyword""); Thread.sleep(3000); List <WebElement> listItems = driver.findElements(By.xpath(""your list item locator"")); listItems.get(0).click(); driver.findElement(By.id(""your searchButton"")).click(); Thank you!!! :-) Thank you very much! Thread.sleep is what I was missing  I found I needed to do a focus on the field before doing typeKeys to get it to work. this indeed is the real answer!!  I recently wrote a HOWTO on this very topic - using Selenium to test an AJAX-driven JQuery autocomplete menu: you are the hero I followed your blog post and it works perfectly thanks a lot  Please use typeKeys instead of type. Also use mouseDown instead of click. It works fine.  Sometime the TypeKeys Doesn't work. At this time you can use keyDown click the inputbox and type value and keyDown in the box.  In the help text for the typeKeys command it says: In some cases you may need to use the simple ""type"" command to set the value of the field and then the ""typeKeys"" command to send the keystroke events corresponding to what you just typed. So use this combination of type(selector text); typeKeys(selector text); This seems to work well.  We had some problems with typeKeys. sendKeys seems to become the final solution but it is still experimental. From the reference: This command is experimental. It may replace the typeKeys command in the future. For those who are interested in the details unlike the typeKeys command which tries to fire the keyDown the keyUp and the keyPress events this command is backed by the atoms from Selenium 2 and provides a much more robust implementation that will be maintained in the future.  Your question is slightly ambigious. Most browsers keep a value cache that is based on the name of the field: This is the value that is being suggested as autocompletion by your browser even though you may never have visited the site before. This feature is non-standard across all browsers and there's going to be no standard way for selenium to detect/analyze this. You can still do it but you'll have to make javascript functions that determine the values yourself. Then you can use ""eval"" in selenium to execute these functions. I have not seen any js libraries that can tell you these values in a cross-browser compatible way. The other alternative is that you use ajax to do a server-side submit of the partially entered value. In this case it's just a matter of typing the values into the textbox and asserting that the expected values turn up. Normally the autocomplete suggestions show up in some layer on the client side. I see why my question could be ambiguous but what i was driving at is in the second part of your answer. In my case i have an ajax-based textfield that provides user with options based on partial entered value. Can you use selenium to capture these options and verify the expected value is in it?  I used following sequence in IDE typeKeys waitForTextPresent mouseOver clickAt and worked well  The type command may not be enough to trigger the autocomplete. Dave Webb's suggestions are otherwise spot on. My only addition would be that you might need the typeKeys command which causes slightly different JavaScript events to be fired which may be more likely to trigger the autocomplete widget. Good point about type and typeKeys. Use both type and typeKeys in that order.  Patrick's answer is definitely important I also found that focus and mouseDown is needed in the last versions of Jquery UI. I recorded a video of a test so that you can see it running in Sauce Labs: https://saucelabs.com/jobs/ad8c561be39bb7a42c9bb3a063214c95  This may not work for everyone but I simply added in a method that allowed me to type in characters with a delay. Actions builder = new Actions(this.webDriver); WebElement element = this.getWebElement(); for (char c : value.toCharArray()) { builder = builder.sendKeys(element c + """"); builder.pause(100); } builder.build().perform(); I then found the item that I wanted to click ( resultsElement.findElement(By.xpath(""//li[.='"" + valueLabel + ""']"")) Where container is the resultsElement is the WebElement that contains the result set and value label is the value I want to click. Again it may not work for all but it worked for me and I thought it prudent to share.  I'd do this as follows: type to enter the value in the text field. waitForTextPresent or verifyTextPresent to check the autocomplete content click or mouseDown to click on the item in the autocomplete list The trick is going to be making the final click be just in the right place. You should be able to use an XPath expression that searches for the text you're expecting to find it."
609,A,"Spring Test / JUnit problem - unable to load application context I am using Spring for the first time and must be doing something wrong. I have a project with several Bean implementations and now I am trying to create a test class with Spring Test and JUnit. I am trying to use Spring Test to inject a customized bean into the test class. Here is my test-applicationContext.xml: <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns="".............""> <bean id=""MyUuidFactory"" class=""com.myapp.UuidFactory"" scope=""singleton"" > <property name=""typeIdentifier"" value=""CLS"" /> </bean> <bean id=""ThingyImplTest"" class=""com.myapp.ThingyImplTest"" scope=""singleton""> <property name=""uuidFactory""> <idref local=""MyUuidFactory"" /> </property> </bean> </beans> The injection of MyUuidFactory instance goes along with the following code from within the test class: private UuidFactory uuidFactory; public void setUuidFactory(UuidFactory uuidFactory) { this.uuidFactory = uuidFactory; } However when I go to run the test (in Eclipse or command line) I get the following error (stack trace omitted for brevity): Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'MyImplTest' defined in class path resource [test-applicationContext.xml]: Initialization of bean failed; nested exception is org.springframework.beans.ConversionNotSupportedException: Failed to convert property value of type 'java.lang.String' to required type 'com.myapp.UuidFactory' for property 'uuidFactory'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [com.myapp.UuidFactory] for property 'uuidFactory': no matching editors or conversion strategy found Funny thing is the Eclipse/Spring XML editor shows errors of I misspell any of the types or idrefs. If I leave the bean in but comment out the dependency injection everything work until I get a NullPointerException while running the test...which makes sense. Try <ref/> not <idref/>. ""The idref element is simply an error-proof way to pass the id of another bean in the container (to a <constructor-arg/> or <property/> element)."" That got past the 'unable to create application context' error but landed me a NullPointerException inside the test. Looks like Spring didn't inject the dependency. I needed to put ""@Resource"" annotation on the field within the test class. That with the use of ""ref"" instead of ""idref"" solved the problem. As a new Spring developer I have to say the documentation on this particular point is really bad. It makes it sound like idref is more of a ""typesafe"" way to reference a bean when really it is a way to reference the string name of a bean. Very confusing...."
610,A,"Non-void test methods in JUnit 4 I would like a JUnit 4 test class to implement the same interface as the class its testing. This way as the interface changes (and it will we're in early development) the compiler guarantees that corresponding methods are added to the test class. For example: public interface Service { public String getFoo(); public String getBar(); } public class ServiceImpl implements Service { @Override public String getFoo() { return ""FOO""; } @Override public String getBar() { return ""BAR""; } } public class ServiceTest implements Service { @Override @Test public String getFoo() { //test stuff } @Override @Test public String getBar() { //test stuff } } When I try this I get an error: ""java.lang.Exception: Method getFoo() should be void"" presumably because test methods must return void. Anybody know of any way around this? How would forcing that method to be implemented via an interface guarantee in any way it is being tested? Since you are trying to make use of the compiler to guarantee test coverage so to speak...But a separate instance from that of the production class implementing the same interface doesn't guarantee a test. It guarantees adhering to the interface. Look at PMD http://pmd.sourceforge.net/rules/junit.html specifically the JUnitTestsShouldIncludeAssert rule. Well true it guarantees that a method has been added to the test class for every service method. It doesn't guarantee that the method actually tests anything. Frankly neither do any code-coverage tools since you can exercise all the code and assert on nothing. I just want to alert the developer(s) that there are outstanding tests to consider. I've been using JUnit 4 for several years and I don't know of a way to do what you want without somehow deriving the JUnit test class from the class to be tested. A more natural way would probably be to use a code coverage tool such as Cobertura. It integrates with JUnit nicely AND it shows you cases where your tests may be deficient in some cases (there are many cases such a tool won't catch though).  I have to admit it is a neat trick though it doesn't scale well to multiple test scenarios. Anyways you can use custom runner. For example: @RunWith(CustomRunner.class) public class AppTest { @Test public int testApp() { return 0; } } public class CustomRunner extends JUnit4ClassRunner { public CustomRunner(Class<?> klass) throws InitializationError { super(klass); } protected void validate() throws InitializationError { // ignore } } Did you put the CustomRunner class into a separate file? I run this example in my IDE before posting here. That did the trick thanks. Now there is an issue with methods that take a parameter due to reflection but thats a different issue. Thanks much! You are welcome. For method with parameter it would be interesting to explore parametrized test runner. But I agree it is a different question. Nice! Thats exactly what I'm looking for. No check yet because I haven't quite gotten it to work. At least not yet... Its complaining that ""Custom runner class CustomRunner should have a public constructor with signature CustomRunner(Class testClass)"". But it does.... So I'm poking around a bit to try to get it working. Stay tuned... Great! But 'extends BlockJUnit4ClassRunner' would be better as JUnit4ClassRunner has been @Deprecated."
611,A,"How to retrieve maven properties inside a JUnit test? I'm writing a test for a file parser class. The parse method receives a file name as parameter and must open it in order to parse it ( duh ). I've writen a test file that I put into the test/resources directory inside my project directory and would like to pass this file to test my parse. Since this project is in CVS and will be manipulated by others I can't hard code the file path so I thought about use the maven ${basedir} property to build the file name in my test. Something like: public void parseTest() { ... sut.parse( ${basedir} + ""src/test/resources/testFile"" ); ... } Does someone knows how could I achieve this? You have 2 options: 1) Pass the file path to your test via a system property (docs) In your pom you could do something like: <project> [...] <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.4.2</version> <configuration> <systemProperties> <property> <name>filePath</name> <value>/path/to/the/file</value> </property> </systemProperties> </configuration> </plugin> </plugins> </build> [...] </project> Then in your test you can do: System.getProperty(""filePath""); 2) Put the file inside src/test/resources under the same package as your test class. Then you can get to the file using Class.getResourceAsStream(String fileName) (docs)). I would highly recommend option 2 over option 1. Passing things to your tests via system properties is very dirty IMO. It couples your tests unnecessarily to the test runner and will cause headaches down the road. Loading the file off the classpath is the way to go and that's why maven has the concept of a resources directory. That's cool. Just be aware that if you ever have to run the tests out of a JAR (which is unlikely) you might get some weird behavior. I'll take the option 2. But as I want to pass the file name instead of using the Class.getResourceAsStream( String fileName ) method I'm using Class.getResource( String fileName ).getFile(). Thanks Mike Not sure how complete this answer is - what about ""integration test"" properties like local DB address that you may want to store in your private settings.xml?"
612,A,"Basic Android JUnit testing problem I m trying to write some basic junit test but I get the following: java.lang.RuntimeException: Unable to resolve activity for: Intent { action=android.intent.action.MAIN flags=0x10000000 comp={michaels.pack.POI/michaels.pack.POI} } at android.app.Instrumentation.startActivitySync(Instrumentation.java:447) at android.test.InstrumentationTestCase.launchActivityWithIntent(InstrumentationTestCase.java:106) at android.test.InstrumentationTestCase.launchActivity(InstrumentationTestCase.java:84) at android.test.ActivityInstrumentationTestCase2.getActivity(ActivityInstrumentationTestCase2.java:87) at michaels.pack.test.POITest.setUp(POITest.java:21) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:164) at android.test.AndroidTestRunner.runTest(AndroidTestRunner.java:151) at android.test.InstrumentationTestRunner.onStart(InstrumentationTestRunner.java:418) at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:1520) Now the class that should be tested is called POI and its in the michaels.pack package! My test class is: package michaels.pack.test; import michaels.pack.POI; import android.test.ActivityInstrumentationTestCase2; import android.widget.TextView; public class POITest extends ActivityInstrumentationTestCase2<POI> { private TextView mView; private POI mActivity; private String resourceString; public POITest() { super(""michaels.pack.POI"" POI.class); } @Override protected void setUp() throws Exception { super.setUp(); mActivity = (POI)this.getActivity(); mView = (TextView) mActivity.findViewById(michaels.pack.R.id.username); resourceString= mActivity.getString(michaels.pack.R.id.username); } public void testPreconditions() { assertNotNull(mView); } public void testText() { assertEquals(resourceString(String)mView.getText()); } } Can anyone see whats wrong with that? Does it have anything to do with the fact that the project is called POIapp and my ""main"" class (POI)? Just to point out that they are in fact two different projects! Here are the manifests: Tests manifest: <?xml version=""1.0"" encoding=""utf-8""?> <manifest xmlns:android=""http://schemas.android.com/apk/res/android"" package=""michaels.pack.test"" android:versionCode=""1"" android:versionName=""1.0""> <uses-sdk android:minSdkVersion=""3"" /> <instrumentation android:targetPackage=""michaels.pack"" android:name=""android.test.InstrumentationTestRunner"" /> <application android:icon=""@drawable/icon"" android:label=""@string/app_name""> <uses-library android:name=""android.test.runner"" /> </application> </manifest> Apps manifest: <?xml version=""1.0"" encoding=""utf-8""?> <manifest xmlns:android=""http://schemas.android.com/apk/res/android"" package=""michaels.pack"" android:versionCode=""1"" android:versionName=""1.0""> <application android:icon=""@drawable/icon"" android:label=""@string/app_name"" android:debuggable=""true""> <uses-library android:name=""com.google.android.maps"" /> <activity android:name="".POI"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""> <intent-filter> <action android:name=""android.intent.action.MAIN"" /> <category android:name=""android.intent.category.LAUNCHER"" /> </intent-filter> </activity> <activity android:name="".Register"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <activity android:name="".MainMenu"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <activity android:name="".ManagePOIsList"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <activity android:name="".MapViewClass"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <activity android:name="".NewPOIForm"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <activity android:name="".ShowPOIsDetails"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <activity android:name="".CreatePoiCoordinates"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <activity android:name="".PreferenceClass"" android:label=""@string/app_name"" android:windowSoftInputMode=""adjustPan"" android:configChanges=""keyboardHidden|orientation""/> <service android:enabled=""true"" android:name="".MyServiceClass"" /> </application> <uses-permission android:name=""android.permission.INTERNET"" /> <uses-permission android:name=""android.permission.ACCESS_NETWORK_STATE"" /> <uses-permission android:name=""android.permission.ACCESS_FINE_LOCATION"" /> <uses-permission android:name=""android.permission.ACCESS_COARSE_LOCATION"" /> <uses-permission android:name=""android.permission.ACCESS_MOCK_LOCATION"" /> <uses-permission android:name=""android.permission.VIBRATE"" /> Thanks in advance! Mike My guess is that you don't have 'use mock location' set up on your emulator but you have it set up on your device. That's why this error tends to pop up when you have the permission set in the manifest.  It seems to work on an actual device! For some reason it was failing in the emulator!  I guess the problem is in your manifest files that's why in the log appears comp={michaels.pack.POI/michaels.pack.POI} and I think it should be comp={michaels.pack/michaels.pack.POI}. You also need two different projects one for your application and one for your tests something that you haven't mentioned either. Use this constructor: public POITest() { super(POI.class); } the other is deprecated since API level 8. They are in two different projects!I should have mentioned that sorry! I'll check that asap! Thanks! I suppose you mean the manifest of the app not the tests right? Can you have a look i edited the question! Thanks again!"
613,A,"JUnit expected tag not working as expected I have the following test case in eclipse using JUnit 4 which is refusing to pass. What could be wrong? @Test(expected = IllegalArgumentException.class) public void testIAE() { throw new IllegalArgumentException(); } This exact testcase came about when trying to test my own code with the expected tag didn't work. I wanted to see if JUnit would pass the most basic test. It didn't. I've also tested with custom exceptions as expected without luck. Screenshot: This one is really weird did some testing myself and this code runs fine (the test is successfull)... I added a screenshot just to show... I'd be doubtful too. The problem is that your AnnounceThreadTest extends TestCase. Because it extends TestCase the JUnit Runner is treating it as a JUnit 3.8 test and the test is running because it starts with the word test hiding the fact that the @Test annotiation is in fact not being used at all. To fix this remove the ""extends TestCase"" from the class definition. Thank you this fixed it as advertised. After removing the extends TestCase I had to add the additional import to ensure I had the static assert methods. import static org.junit.Assert.*; Awesome job at finding the solution hidden as a hint in a screenshot Solved!! Thank you saved me some time.  Just ran this in IntelliJ using JUnit 4.4:  @Test(expected = IllegalArgumentException.class) public void testExpected() { throw new IllegalArgumentException(); } Passes perfectly. Rebuild your entire project and try again. There's something else that you're doing wrong. JUnit 4.4 is working as advertised."
614,A,"How to mock HTTPSession/FlexSession with TestNG and some Mocking Framework I'm developing a web application running on Tomcat 6 with Flex as Frontend. I'm testing my backend with TestNG. Currently I'm trying to test the following method in my Java-Backend: public class UserDAO extends AbstractDAO { (...) public UserPE login(String mail String password) { UserPE dbuser = findUserByMail(mail); if (dbuser == null || !dbuser.getPassword().equals(password)) throw new RuntimeException(""Invalid username and/or password""); // Save logged in user FlexSession session = FlexContext.getFlexSession(); session.setAttribute(""user"" dbuser); return dbuser; } } The method needs access to the FlexContext which only exists when i run it on the Servlet container (don't bother if you don't know Flex it's more a Java-Mocking question in general). Otherwise i get a Nullpointer exception when calling session.setAttribute(). Unfortunately I cannot set the FlexContext from outside which would make me able to set it from my tests. It's just obtained inside the method. What would be the best way to test this method with a Mocking framework without changing the method or the class which includes the method? And which framework would be the easiest for this use case (there are hardly other things i have to mock in my app it's pretty simple)? Sorry I could try out all of them for myself and see how i could get this to work but i hope that i'll get a quickstart with some good advices! Obvious one approach is to re-factor it in a way that lets you inject things like the FlexContext. However this is not always possible. Some time ago a team I was part of hit a situation where we had to mock out some internal class stuff that we didn't have access to (like your context). We ended up using an api called jmockit which allows you to effective mock individual methods including static calls. Using this technology we where able to get around a very messy server implementation and rather than having to deploy to live servers and black box test we were able to unit test at a fine level by overriding the server technology that was effective hard coded. The only recommendation I would make about using something like jmockit is to ensure that in your test code there is clear documentation and seperation of jomockit from you main mocking framework (easymock or mockito would be my recommendations). Otherwise you risk confusing developers about the various responsibilities of each part of the puzzle which usually leads to poor quality tests or tests that don't work that well. Ideally as we ended up doing wrap the jmockit code into you testing fixtures so the developers don't even know about it. Dealing with 1 api is enough for most people. Just for the hell of it here's the code we used to fix testing for an IBM class. WE basically need to do two things Have the ability to inject out own mocks to be returned by a method. Kill off a constructor that went looking for a running server. Do the above without having access to the source code. Here's the code: import java.util.HashMap; import java.util.Map; import mockit.Mock; import mockit.MockClass; import mockit.Mockit; import com.ibm.ws.sca.internal.manager.impl.ServiceManagerImpl; /** * This class makes use of JMockit to inject it's own version of the * locateService method into the IBM ServiceManager. It can then be used to * return mock objects instead of the concrete implementations. * <p> * This is done because the IBM implementation of SCA hard codes the static * methods which provide the component lookups and therefore there is no method * (including reflection) that developers can use to use mocks instead. * <p> * Note: we also override the constructor because the default implementations * also go after IBM setup which is not needed and will take a large amount of * time. * * @see AbstractSCAUnitTest * * @author Derek Clarkson * @version ${version} * */ // We are going to inject code into the service manager. @MockClass(realClass = ServiceManagerImpl.class) public class ServiceManagerInterceptor { /** * How we access this interceptor's cache of objects. */ public static final ServiceManagerInterceptor INSTANCE = new ServiceManagerInterceptor(); /** * Local map to store the registered services. */ private Map<String Object> serviceRegistry = new HashMap<String Object>(); /** * Before runnin your test make sure you call this method to start * intercepting the calls to the service manager. * */ public static void interceptServiceManagerCalls() { Mockit.setUpMocks(INSTANCE); } /** * Call to stop intercepting after your tests. */ public static void restoreServiceManagerCalls() { Mockit.tearDownMocks(); } /** * Mock default constructor to stop extensive initialisation. Note the $init * name which is a special JMockit name used to denote a constructor. Do not * remove this or your tests will slow down or even crash out. */ @Mock public void $init() { // Do not remove! } /** * Clears all registered mocks from the registry. * */ public void clearRegistry() { this.serviceRegistry.clear(); } /** * Override method which is injected into the ServiceManager class by * JMockit. It's job is to intercept the call to the serviceManager's * locateService() method and to return an object from our cache instead. * <p> * This is called from the code you are testing. * * @param referenceName * the reference name of the service you are requesting. * @return */ @Mock public Object locateService(String referenceName) { return serviceRegistry.get(referenceName); } /** * Use this to store a reference to a service. usually this will be a * reference to a mock object of some sort. * * @param referenceName * the reference name you want the mocked service to be stored * under. This should match the name used in the code being tested * to request the service. * @param serviceImpl * this is the mocked implementation of the service. */ public void registerService(String referenceName Object serviceImpl) { serviceRegistry.put(referenceName serviceImpl); } } And here's the abstract class we used as a parent for tests. public abstract class AbstractSCAUnitTest extends TestCase { protected void setUp() throws Exception { super.setUp(); ServiceManagerInterceptor.INSTANCE.clearRegistry(); ServiceManagerInterceptor.interceptServiceManagerCalls(); } protected void tearDown() throws Exception { ServiceManagerInterceptor.restoreServiceManagerCalls(); super.tearDown(); } } Of course injection would be nice but it's not possible in my current environment. Thanks for your points now i have a direction to go! Will try JMockit now. Bummer. There seem to be issues regarding JMockit and TestNG: http://old.nabble.com/I-can%27t-run-testng-%2B-jmockit-test-cases-with-recent-testng-eclipse-release-td30002025.html http://code.google.com/p/jmockit/issues/detail?id=85 How did you work around these issues in your tests? I guess you're use JUnit? Yep junit. When we did this project we used easymock as our mocking framework. Once I removed jmockit from the developers view by coding the abstract parent test class there was a lot less confusion in the team about how to do unit tests. I've never use TestNG so I cannot comment on it as a test framework other than to say I've run into a lot more developers with JUnit experience :-)  Thanks to Derek Clarkson I successfully mocked the FlexContext making the login testable. Unfortunately it's only possible with JUnit as far as i see (tested all versions of TestNG with no success - the JMockit javaagent does not like TestNG See this and this issues). So this is how i'm doing it now: public class MockTests { @MockClass(realClass = FlexContext.class) public static class MockFlexContext { @Mock public FlexSession getFlexSession() { System.out.println(""I'm a Mock FlexContext.""); return new FlexSession() { @Override public boolean isPushSupported() { return false; } @Override public String getId() { return null; } }; } } @BeforeClass public static void setUpBeforeClass() throws Exception { Mockit.setUpMocks(MockFlexContext.class); // Test user is registered here (...) } @Test public void testLoginUser() { UserDAO userDAO = new UserDAO(); assertEquals(userDAO.getUserList().size() 1); // no NPE here userDAO.login(""asdf@asdf.de"" ""asdfasdf""); } } For further testing i now have to implement things like the session map myself. But thats okay as my app and my test cases are pretty simple."
615,A,Setup test data for Jmeter and Junit Is there any way I can setup my test data for load tests that use Jmeter and Junit? I would like to be able to do something like get a new user from a csv for each thread of a test. I have not used jmeter before but I know that for junit you can use the setUp() method for that kind of things like opening files setups loading/reseting configurations and so on. That method will be executed before the tests start being executed. Hope that helps. yeah I know man but let's say I have a list of predefined users that I want to loop through and log in with. I'm not sure how to do that in setUp with Jmeter. Yes I've used this a number of times. See JMeter : Reading variables from a file.  You can configure a CSV dataset to only be read once but having only ONE row of data in it. This will work with JUnit as the following happens: Junit opens Jmeter script and executes JMeter script. The Jmeter script when executed reads from the CSV file populates all defined data fields and then runs the requests  jMeter is an load testing framework based on HTTP. It can simulate multiple users using your website. So there is no concept of setup data. What you can essentially do is do some script to load massive data to load or use one request to store some details and other to retrieve it back. When doing jUnit testing to load data you can use dbUnit which can load data for you before testing it. Yeah I know man that's totally what I want to do. I just don't know if it's possible to use data read through CSV by Jmeter in my Junit tests. I want to be able to read the CSV once to prepare my test data and then run everything. Jmeter lets you load CSV files and set user defined variables. It also lets you use Junit which has setup and tear down. Even more you can define a Junit test runner. Jmeter uses its own implementation of Junit. To say Jmeter has no concept of setup data seems misleading to me. The CSV file is to load setup data to use for test cases not load the data to test against...
616,A,grails -functional tests won't run even from within Intelli-J? I have a tests in /test/functional that extends GrailsUnitTestCase. When I run grails test-app they don't run and when I try to run them from within Intelli-J I get no tests found. My test is simple enough but I just don't see why it's not running. If I put it in /integration/ or /unit/ it seems to run. I'm using grails 1.3.6. Is there any reason -functional test-phase tests wouldn't run for me? class MyServiceFunctionalTests extends GrailsUnitTestCase { MyService myService public void setUp() { myService = new MyService() } public void testSomething(){ assertTrue( true ); } } Do you have a functional test plugin installed? There's no direct support in Grails core for functional tests so you'd want to install http://grails.org/plugin/functional-test or http://grails.org/plugin/geb or one of the other func test plugins. I know your code is just an example but it wouldn't make sense to either extend GrailsUnitTestCase or create a new instance of MyService (or even attempt to use dependency injection). Functional tests are really just clients of a running application that make GET and POST requests and verify the responses so you won't have access to Spring beans services etc. Ah I see. I really don't need to start a server as I want to use the functional tests as REST method testing/selenium RC tests against an external system. Is there an easy way to add the running of the functional tests as if they were unit or integration? My services hit external systems that are guaranteed not to be running during the integration test phase. The functional test plugin has conflicting dependencies that don't seem fixable even with `transitive=false`. Is there an easy way to add a test phase *without* a plugin? Also the 1.3.6 and 1.3.7 documentation clearly states that *functional* and *other* are supported native test phases. You should ask on the User mailing list. Luke Daley has done a lot of work in this area and would probably be able to help you run a subset of your 'unit' tests (i.e. not functional or integration in the Grails sense but not mock-driven) as functional tests in their own phase. They're supported but there's nothing in core to run them start a server etc. There are just hooks that functional test plugins can use.
617,A,Junit test case - time elapsed is 0.00 Time elapsed (time taken?) to run(and pass) my Junit test case is shown as 0.00. I have tried failing the test case (by doing assertEquals for a wrong value) and its still showing time elapsed as 0.00. The rest of test cases have non-zero time elapsed values. EDIT: Assumption - I assumed that it would surely take non-zero time to execute and that if it didn't happen nothing was happening within the test case - example I have a if condition in there and no corresponding else. So if the condition failed it just came out of the method without doing anything and since an error didn't occur it didn't fail the test either. Is there any reason this is happening? Am I missing something? Thanks Pratyusha. It's not clear why you think it should be non-zero. Is it possible that your test case is just passing (or failing) really quickly? Just for the sake of experimentation put a Thread.sleep(1000) call in there... if that changes things it suggests that everything's fine and your test is just fast. Are all your test methods shown to be executing? It's not something simple like failing to annotate the methods or failing to keep to the naming convention of testXXX (depending on which version of JUnit you're using)? Thanks Jon. I have tried Thread.sleep(1000) and the test is executed in a non-zero time. So it seems to be executing really fast. I assumed that it would surely take non-zero time to execute and that if it didn't happen nothing was happening within the test case - example I have a if condition in there and no corresponding else. So if the condition failed it just came out of the method without doing anything and since an error didn't occur it didn't fail the test either.
618,A,"Spring Eclipse running Junit (or other type of) tests within a live spring application? I'm not quite sure if I'm missing something really obvious here but my searches on this topic aren't returning results of use to me. I have recently gotten more into unit testing and using mock objects. This is all well and good for testing objects in isolation but in a complex Spring MVC application that is interacting through AJAX with a lot of Javascript code I sometimes run into issues such as a value being passed in that is not what it needs to be and causes something to break. Of course it still fulfills the type such as being a String but it may be null which would cause something to break down the line. I figured if I could do something like below within an actual class of my application (not in an isolated unit test) then it could be useful. Sort of like a debug mode but instead its running the app as a unit test I would run it regularly and be able to interact with it (manual testing) and if a bug occurs that violates one of the tests I placed in there it would tell me. I realize I can do this with System.out.println but I tend to use this too much maybe sometimes so it can be hard to sift through all the outputs to pick out things like this so would be better I think if it had the regular Junit GUI dialog brought up when something occurred. String somestring if (somestring != null) { boolean checkSomestring = true; } else { boolean checkSomestring = false } assertEquals(checkSomestring true); I may have rambled a bit I'm guessing there is a way to do this but it is eluding me at the moment would appreciate any advice. Thanks We've just got a little ASSERT(bool shouldBeTrue string failureMessage) function in our javascript. When it fails it sends a logger request... just a GET request with url-encoded arguments: dateTime machineName currentCocument and the failureMessage. The web-server just writes it to a log-file. Then a ""tailing job"" (a korn-shell script based on ""tail -f"") periodically (every minute from memory) picks-up the message-lines and emails them to the developers-email-group for the system. The same kit is used on several systems. Don't ask me about the details; I'm ""the unix guru""... and a self-confessed bunny when it comes to javascript. The guy who wrote it is still around though; so I could (next week) ask him if he minds it being published on the net. I think the whole setup is pretty neat. Most of the time we just ignore the assert-mails (I have an outlook rule which sends them all straight to the deleted bucket); except when we're testing something... This way we get to use each others unit testing of the GUI to detect ""breakages"" in stuff we wrote. I also keep an eye on them for a couple days after each release. BTW: Once upon a time we tried automated web-app testing. We spent a small fortune on it until we figured out that were using our good software to test our crappy test-scripts... and we basically gave-up in disgust. But that was years ago and I believe that things have improved dramatically in this area so it might be worth having another look; atleast for the ""critical"" functionality of the system. Cheers. Keith. Ah a sorry tale of woe. I think we've all been through our own version of that inadequate tool/layer on top/descent into madness story at least once. We use HTMLUnit for web interface testing and our experience has been better than yours sounds (more of a shallow glide into madness). But still not perfect because its javascript handling is a bit wobbly when pushed to do complex things. If i started again now i'd use WebDriver. I hear that has problems of its own though. It's a tough game that's the bottom line. Sorry to hear about your automated test debacle. Could you expand briefly on what tools you used and why it went wrong? @Tom: We used a beast called QARun simply because we already had a (very expensive) licence for it from a VB project. It just wasn't at all suited to testing a web-client... so we wrote a meta-language on top of QA's primitives which had it's own bugs as did/does QARun. The whole thing was just too brittle. A UI change could take a day to code and then a week to update all the test scripts which touched that form/component. It was a loosing battle. Basically: It was too hard to change. I guess HtmlUnit (or something) would've been a LOT more cost effective."
619,A,"How do setUp and tearDown work for Load Testing with Jmeter and Junit? I have to do some setup and cleanup on the database I am load testing with Jmeter and Junit. I have put these in setUp and tearDown respectively. My question now is what happens when I fire off 100 threads that execute my test? Will Jmeter first run 100 setUp's then 100 threads of my test and then 100 tearDowns? Or does it just create 100 threads to execute my test and run them all in parallel? My concern is just that the setUp and tearDown may affect my testing depending on how Jmeter and Junit do things. In that case I would have to do setUp and tearDown completely outside of my test. I haven't used Junit and Jmeter together so I'm assuming setUp and tearDown are part of Junit. Given that I would assume Junit would execute setUp run the Jmeter script then run tearDown when Jmeter signals it is done. Since JMeter uses the same script for multiple threads and isn't ""done"" until the last thread finishes you shouldn't have multiple setUp/teardown Scripts. I would try it with 2 threads and see what happens."
620,A,"Different flavors of JUnit? I'm trying to fresh up my Java and also to learn how to use Maven and JUnit. Following the Maven quick start I ran the following in the console: mvn archetype:generate \ -DgroupId=com.mycompany.app \ -DartifactId=my-app \ -DarchetypeArtifactId=maven-archetype-quickstart \ -DinteractiveMode=false I then got a simple App.java and an AppTest.java in their proper folders. I'm now looking at the AppTest.java and trying to figure out how to use this JUnit stuff. The problem is that I don't understand it and I looks quite different from what I see for example in the JUnit Cookbook. For example the version I got from Maven has different package names and there is no annotation of the test method. What's going on here? Is Maven using something else than regular JUnit? Or is it just doing something fancy? More info Apache Maven 3.0.2 (r1056850; 2011-01-09 01:58:10+0100) Java version: 1.6.0_23 vendor: Sun Microsystems Inc. AppTest.java package com.mycompany.app; import junit.framework.Test; import junit.framework.TestCase; import junit.framework.TestSuite; /** * Unit test for simple App. */ public class AppTest extends TestCase { /** * Create the test case * * @param testName * name of the test case */ public AppTest(String testName) { super(testName); } /** * @return the suite of tests being tested */ public static Test suite() { return new TestSuite(AppTest.class); } /** * Rigorous Test :-) */ public void testApp() { assertTrue(true); } } pom.xml <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""> <modelVersion>4.0.0</modelVersion> <groupId>com.mycompany.app</groupId> <artifactId>my-app</artifactId> <packaging>jar</packaging> <version>1.0-SNAPSHOT</version> <name>my-app</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> </project> no maven uses regular Junit. But here the old version 3.8.1 is used (that works without annotations in so does not require java 5 or higher). Simply update you junit dependency to newest junit version (4.8.2) and then you can write tests as you are used to wirh annotations. Ahaaa. Tried to to just change the version number in my pom file now and seems like that did it actually. Thanks!"
621,A,Java: Mock testing probably with Mockito I think I'm not using verify correctly. Here is the test: @Mock GameMaster mockGM; Player pWithMock; @Before public void setUpPlayer() throws SecurityException NoSuchFieldException IllegalArgumentException IllegalAccessException { pWithMock = new Player(mockGM); } @Test public void mockDump() { pWithMock.testDump(); verify(mockGM).emitRandom(); // fails } Here is the code it calls: public boolean testDump() { Letter t = tiles.getRandomTile(); return dump(t); } private boolean dump(Letter tile) { if (! gm.canTakeDump() || tiles.count() == 0) { return false; } tiles.remove(tile); gm.takeTile(tile); for (int i = 0; i < 3; i++) { tiles.addTile(gm.emitRandom()); // this is the call I want to verify } return true; } Failure trace: Wanted but not invoked: gameMaster.emitRandom(); -> at nth.bananas.test.PlayerTest.mockDump(PlayerTest.java:66) However there were other interactions with this mock: -> at nth.bananas.Player.dump(Player.java:45) at nth.bananas.test.PlayerTest.mockDump(PlayerTest.java:66) The call I want to verify is several layers down. Is there a different way to check this? +1 for `gm.canTakeDump()` There is an error in your test method: it's missing a necessary expectation for the GameMaster#canTakeDump(). This method needs to return true when called from the tested method (because of its use in that if statement at line 45).  I'm not sure to understand what you are doing. Given the following Player class: public class Player { private final GameMaster gm; public Player(GameMaster gameMaster) { this.gm = gameMaster; } public void foo() { gm.bar(); // this is the call we want to verify } } And the following GameMaster class: public class GameMaster { public GameMaster() { } public void bar() { } } I'd write the test of Player like this: import static org.mockito.Mockito.verify; import org.junit.Test; import org.junit.runner.RunWith; import org.mockito.Mock; import org.mockito.runners.MockitoJUnitRunner; @RunWith(MockitoJUnitRunner.class) public class PlayerTest { @Mock private GameMaster gm; @Test public void testFoo() { Player player = new Player(gm); player.foo(); verify(gm).bar(); // pass } } I definitely don't know what I'm doing. How is this different from what I posted? also what if the GameMaster class doesn't have a constructor with no args? @Rosarch *How is that different?* I'm not totally sure as you didn't show all code but well this works :) *What if the GameMaster doesn't have a constructor with no args?* Well this is a sample but if you need to add this constructor to make the code testable just do it. ok but in your example what if `foo()` calls `odp()` which then calls `bar()`. Would `verify(gm).bar()` still detect it? I'm not sure of what `opd()` does but you may need to create a full graph of mocks if this is your question. `odp()` does nothing but call `bar()`. `void odp() { gm.bar();}` @Rosarch Isn't that precisely what `foo()` is doing? I don't get the difference. Right but `foo()` isn't calling `bar()` directly whereas `odp()` is. I don't think I'm still following you and because I can't reproduce your test I don't think we'll solve this using the commenting system. Maybe you should post something that readers could reproduce this would help IMO.
622,A,"Add Spring 3.0.0 java based IOC to JUnit 4.7 tests There is a doc http://static.springsource.org/spring/docs/2.5.6/reference/testing.html how to add IoC support to junit tests using xml-configuration but I can not find example for java-based configuration... For example I have java-based bean: public class AppConfig { @Bean public Test getTest() { return new Test(); } } And test: @RunWith(SpringJUnit4ClassRunner.class) public class IocTest { @Autowired private Test test; @Test public void testIoc() { Assert.assertNotNull(test); } } What should I add to enable java-based beans to my junit test without using xml-configs? Normally I use: new AnnotationConfigApplicationContext(AppConfig.class); but it does not work for tests... Genereally why this does not work (assert fails): public class IocTest { @BeforeClass public static void initSpringIoc() { AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(ApplicationConfig.class); NotExistsPredicate predicate = ctx.getBean(NotExistsPredicate.class); LoggerFactory.getLogger(IocTest.class).debug(predicate.toString()); } @Test public void testIoc() { TestService service = new TestService(); Assert.assertNotNull(service.getPredicate()); // assert fails } } public class TestService { @Autowired private NotExistsPredicate predicate; public NotExistsPredicate getPredicate() { return predicate; } } Log output: 1 [main] INFO org.springframework.context.annotation.AnnotationConfigApplicationContext - Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@e94e92: startup date [Tue Feb 09 15:32:48 EET 2010]; root of context hierarchy 2 [main] DEBUG org.springframework.context.annotation.AnnotationConfigApplicationContext - Bean factory for org.springframework.context.annotation.AnnotationConfigApplicationContext@e94e92: org.springframework.beans.factory.support.DefaultListableBeanFactory@a37368: defining beans [org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalRequiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessorapplicationConfig]; root of factory hierarchy 33 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.context.annotation.internalConfigurationAnnotationProcessor' 33 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'org.springframework.context.annotation.internalConfigurationAnnotationProcessor' 84 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'org.springframework.context.annotation.internalConfigurationAnnotationProcessor' to allow for resolving potential circular references 87 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'org.springframework.context.annotation.internalConfigurationAnnotationProcessor' 129 [main] DEBUG org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader - Registering bean definition for @Bean method com.mihailenco.config.ApplicationConfig.getNotExistsPredicate() 133 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.context.annotation.internalAutowiredAnnotationProcessor' 133 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'org.springframework.context.annotation.internalAutowiredAnnotationProcessor' 135 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'org.springframework.context.annotation.internalAutowiredAnnotationProcessor' to allow for resolving potential circular references 135 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'org.springframework.context.annotation.internalAutowiredAnnotationProcessor' 135 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.context.annotation.internalRequiredAnnotationProcessor' 135 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'org.springframework.context.annotation.internalRequiredAnnotationProcessor' 136 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'org.springframework.context.annotation.internalRequiredAnnotationProcessor' to allow for resolving potential circular references 137 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'org.springframework.context.annotation.internalRequiredAnnotationProcessor' 142 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'org.springframework.context.annotation.internalCommonAnnotationProcessor' 142 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'org.springframework.context.annotation.internalCommonAnnotationProcessor' 151 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'org.springframework.context.annotation.internalCommonAnnotationProcessor' to allow for resolving potential circular references 151 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'org.springframework.context.annotation.internalCommonAnnotationProcessor' 155 [main] DEBUG org.springframework.context.annotation.AnnotationConfigApplicationContext - Unable to locate MessageSource with name 'messageSource': using default [org.springframework.context.support.DelegatingMessageSource@86fe26] 164 [main] DEBUG org.springframework.context.annotation.AnnotationConfigApplicationContext - Unable to locate ApplicationEventMulticaster with name 'applicationEventMulticaster': using default [org.springframework.context.event.SimpleApplicationEventMulticaster@139eeda] 166 [main] INFO org.springframework.beans.factory.support.DefaultListableBeanFactory - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@a37368: defining beans [org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalRequiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessorapplicationConfiggetNotExistsPredicate]; root of factory hierarchy 166 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'org.springframework.context.annotation.internalConfigurationAnnotationProcessor' 166 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'org.springframework.context.annotation.internalAutowiredAnnotationProcessor' 166 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'org.springframework.context.annotation.internalRequiredAnnotationProcessor' 171 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'org.springframework.context.annotation.internalCommonAnnotationProcessor' 171 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'applicationConfig' 172 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'applicationConfig' 174 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'applicationConfig' to allow for resolving potential circular references 190 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'applicationConfig' 190 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating shared instance of singleton bean 'getNotExistsPredicate' 190 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Creating instance of bean 'getNotExistsPredicate' 196 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'applicationConfig' 225 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Eagerly caching bean 'getNotExistsPredicate' to allow for resolving potential circular references 232 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Finished creating instance of bean 'getNotExistsPredicate' 235 [main] DEBUG org.springframework.context.annotation.AnnotationConfigApplicationContext - Unable to locate LifecycleProcessor with name 'lifecycleProcessor': using default [org.springframework.context.support.DefaultLifecycleProcessor@134a7d8] 235 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'lifecycleProcessor' 237 [main] DEBUG org.springframework.beans.factory.support.DefaultListableBeanFactory - Returning cached instance of singleton bean 'getNotExistsPredicate' 253 [main] DEBUG IocTest - com.mihailenco.predicate.NotExistsPredicate@982589 this doesn't work because you instantiate it. But this shouldn't be an answer - it should be an edit to your question  Update: Spring 3.1 will support it out of the box see Spring 3.1 M2: Testing with @Configuration Classes and Profiles. It seems to be this feature is not supported by Spring yet. However it can be easily implemented: public class AnnotationConfigContextLoader implements ContextLoader { public ApplicationContext loadContext(String... locations) throws Exception { Class<?>[] configClasses = new Class<?>[locations.length]; for (int i = 0; i < locations.length; i++) { configClasses[i] = Class.forName(locations[i]); } return new AnnotationConfigApplicationContext(configClasses); } public String[] processLocations(Class<?> c String... locations) { return locations; } } - @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(loader = AnnotationConfigContextLoader.class value = ""com.sample.AppConfig"") public class IocTest { @Autowired TestSerivce service; @Test public void testIoc() { Assert.assertNotNull(service.getPredicate()); } } - @Configuration public class ApplicationConfig { ... @Bean public NotExistsPredicate getNotExistsPredicate() { return new NotExistsPredicate(); } @Bean public TestService getTestService() { return new TestService(); } } Thank you but still can't get this to work... My test class: http://pastebin.com/m4125a931  ApplicationConfig: http://pastebin.com/m2842d2a1  TestService: http://pastebin.com/m1ac16e48 . Then I click in NetBeans ""Test File"" and get assertion fail (assertNotNull) and this log: http://pastebin.com/mad10742 . Can you advice something? @Vladimir: You can't simply create autowired object with new. It should be obtained from the context. I added an example of how `TestService` can be created with `@Bean`-annotated method of annotation-configured context. Thank you axtavt.  If you are getting a null value probably it's not loading the application context. Note that by default it the runner loads it from ""classpath:/com/test/IocTest-context.xml"" (assuming IocTest.java package is the com.test) If it is not there try specifying it by adding @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""PATH_TO_YOUR_APP_CONTEXT/yourApplicationContext.xml"" }) public class IocTest { @Autowired private Test test; @Test public void testIoc() { Assert.notNull(""test is null"" test); } } I want to configure ApplicationContext without any xml configs using java-based configuration. There is an example in question (AppConfig class).  This is what worked for me... Taken from http://www.swiftmind.com/de/2011/06/22/spring-3-1-m2-testing-with-configuration-classes-and-profiles/ package com.example; @RunWith(SpringJUnit4ClassRunner.class) // ApplicationContext will be loaded from the // OrderServiceConfig class @ContextConfiguration(classes=OrderServiceConfig.class loader=AnnotationConfigContextLoader.class) public class OrderServiceTest { @Autowired private OrderService orderService; @Test public void testOrderService() { // test the orderService } }"
623,A,"How do I unit test jdbc code in java? I'd like to write some unit tests for some code that connects to a database runs one or more queries and then processes the results. (Without actually using a database) Another developer here wrote our own DataSource Connection Statement PreparedStatement and ResultSet implementation that will return the corresponding objects based on an xml configuration file. (we could use the bogus datasource and just run tests against the result sets it returns). Are we reinventing the wheel here? Does something like this exist already for unit testing? Are there other / better ways to test jdbc code? I would say that HSQL is the way to go during your unit tests. The point of your test is to test your jdbc code and make sure it works. Adding custom classes or mocking the jdbc calls can easily hide bugs. I mostly use mysql and when the tests run the driver class and url is changed to org.hsqldb.jdbcDriver and jdbc:hsqldb:mem:test.  Acolyte driver can be used to mock up a JDBC connection managing it during tests and returning data as result set (with its typesafe row list API): https://github.com/cchantep/acolyte  There is DBUnit. It won't allow you to test your jdbc code without a database but it seems like you could introduce a different set of buys by emulating a database.  While the way to mock jdbc in your application is of course dependant on how you've implemented your actual jdbc transactions. If you're using jdbc as is I'd assume you have written yourself an utility class of sorts to do some tasks in the line of DBUtils.getMetadataFor(String tablename). What this would mean is that you'd have to create a mock of that class and that could be all you need. This would be rather easy solution for you since you apparently already have a series of jdbc related mock objects available. Note that I'm assuming your jdbc code isn't exploded all around the application - if it is refactor!!! If you're however using any framework for database handling (like Spring Framework's JDBC Template classes) you can and should mock the interface class using EasyMock or some other equivalent. That way you can have all the power in the world required for easy mocking of the connection. And last if nothing else works you can do what others have said already and use DBUnit and/or derby.  Use any of the Mock frameworks for such a task. (jMock etc.) Some examples  You could use DBUnit together with a HSQLDB which can read its initial data from CSV files for example.  I like to use a combination of: DBUnit HSQLDB Unitils (specifically the database testing and maintenance modules) You can get pretty far with just DBUnit and HSQLDB. Unitils provides the last mile of code to manage and reset database state. It also provides a nice way of managing database schema changes and makes it easy to use specific RBDMS (Oracle DB2 SQL Server etc). Finally Unitils provides some nice wrappers around DBUnit which modernizes the API and makes DBUnit much more pleasant to work with. If you haven't checked out Unitils yet you definitely should. Unitils is often overlooked and under-appreciated. This is the stack I tend to use as well. DBUnit/H2/Unitils  If you want to do unit tests not an integration tests than you can use a very basic and simple approach using Mockito only like this: public class JDBCLowLevelTest { private TestedClass tested; private Connection connection; private static Driver driver; @BeforeClass public static void setUpClass() throws Exception { // (Optional) Print DriverManager logs to system out DriverManager.setLogWriter(new PrintWriter((System.out))); // (Optional) Sometimes you need to get rid of a driver (e.g JDBC-ODBC Bridge) Driver configuredDriver = DriverManager.getDriver(""jdbc:odbc:url""); System.out.println(""De-registering the configured driver: "" + configuredDriver); DriverManager.deregisterDriver(configuredDriver); // Register the mocked driver driver = mock(Driver.class); System.out.println(""Registering the mock driver: "" + driver); DriverManager.registerDriver(driver); } @AfterClass public static void tearDown() throws Exception { // Let's cleanup the global state System.out.println(""De-registering the mock driver: "" + driver); DriverManager.deregisterDriver(driver); } @Before public void setUp() throws Exception { // given tested = new TestedClass(); connection = mock(Connection.class); given(driver.acceptsURL(anyString())).willReturn(true); given(driver.connect(anyString() Matchers.<Properties>any())) .willReturn(connection); given(connection.prepareCall(anyString())).willReturn(statement); } } Than you can test various scenarios like in any other Mockito test e.g. @Test public void shouldHandleDoubleException() throws Exception { // given SomeData someData = new SomeData(); given(connection.prepareCall(anyString())) .willThrow(new SQLException(""Prepare call"")); willThrow(new SQLException(""Close exception"")).given(connection).close(); // when SomeResponse response = testClass.someMethod(someData); // then assertThat(response is(SOME_ERROR)); }  We use Mockrunner. http://mockrunner.sourceforge.net/ It has mock connections and datasources built in so there is no need to implement them your selves.  That's why you have derby (now called JavaDB) or sqlite -- they are small simple databases that you can create load test against and destroy relatively quickly and simply. But any code that depends on vendor-specific SQL that isn't supported by the lightweight db will not be testable. @Asaph: ""vendor-specific SQL"" is often a mistake. However when you go to test vendor-specific SQL you're not doing unit testing so that's not really a unit test issue is it? It's more of an integration test than a unit test at that point. But connecting to MySQL in a JUnit test is arguably not fundamentally different from connecting to sqlite from a JUnit test. I see both scenarios as integration tests. What would truly make it a _unit_ test would be having mocks or fakes at all the database seams ie. no database server (lightweight or heavyweight) at all. Having said that I'm not fanatic about adhering to this principle _all_ the time. As long as the tests run fast and don't require lots of manual setup it's okay with me. @Asaph: Mocking the database -- while technically possible -- is usually silly. You don't write it therefore you must *trust* it. If you don't trust it stop considering it and find something you do trust. Since you trust it use it for testing. You trust your test framework. (You don't test it do you?) You trust your compilers and libraries. It's okay to trust a JDBC service too.  I prefer using EasyMock for testing a not-so-easy-to-test code.  You have several options: Mock the database with a Mock library e.g. JMock. The huge drawback of this that your queries and the data will most likely be not tested at all. Use a light weight database for the tests such as HSQLDB. If your queries are simple this is probably the easiest way to go. Dedicate a database for the tests. DBUnit is a good option or if you are using Maven you can also use the sql-maven-plugin to set up and tear down the database properly (be careful of dependencies between tests). I recommend this option as it will give you the biggest confidence that the queries work properly with your db vendor. Sometimes it is necessary and useful to make these tests configurable so that these tests are only executed if the database is available. This can be done with e.g. build properties. Thanks I like the idea of using maven to setup / tear down the database. We're using maven now for builds so this would be pretty easy for us to use."
624,A,JUnit test and threads when i run multiple JUnit tests in a row does JUnit create a new thread for each execution or everything is wrapped in a single thread? Thanks JUnit 4.0 supports concurrent tests Not Found:node/589 =(  It's all on a single thread. You don't say how you're invoking the tests but for example running a TestSuite will run all the tests in the suite sequentially. If you want parallel execution TestNG has support for running tests concurrently as does JUnitPerf and GroboUtils. JUnit is also experimenting with parallel execution (search for the word parallel in the link): http://sourceforge.net/project/shownotes.php?release_id=675664&group_id=15278 Also when you run in a Maven build you can have each JUnit test set run in it's own environment - not just it's own thread.
625,A,"What's the actual use of 'fail' in JUnit test case? What's the actual use of 'fail' in JUnit test case? This is how I use the Fail method. There are three states that your test case can end up in 1. Passed : The function under test executed successfully and returned data as expected 2. Not Passed : The function under test executed successfully but the returned data was not as expected 3. Failed : The function did not execute successfully and this was not intended (Unlike negative test cases that expect a exception to occur). If you are using eclipse there three states are indicated by a Green Blue and red marker respectively. I use the fail operation for the the third scenario. e.g. : public Integer add(integer a Integer b) { return new Integer(a.intValue() + b.intValue())} Passed Case : a = new Interger(1) b= new Integer(2) and the function returned 3 Not Passed Case: a = new Interger(1) b= new Integer(2) and the function returned soem value other than 3 Failed Case : a =null  b= null and the function throws a NullPointerException  Some cases where I have found it useful: mark a test that is incomplete so it fails and warns you until you can finish it making sure an exception is thrown: try{ // do stuff... fail(""Exception not thrown""); }catch(Exception e){ assertTrue(e.hasSomeFlag()); } Note: Since JUnit4 there is a more elegant way to test that an exception is being thrown: Use the annotation @Test(expected=IndexOutOfBoundsException.class) However this won't work if you also want to inspect the exception then you still need fail().  lets say you are writing a test case for a -ve flow where the code being tested should raise an exception try{ bizMethod(badData); fail(); // FAIL when no exception is thrown } catch (BizException e) { assert(e.errorCode == THE_ERROR_CODE_U_R_LOOKING_FOR) }  I've used it in the case where something may have gone awry in my @Before method. public Object obj; @Before public void setUp() { // Do some set up obj = new Object(); } @Test public void testObjectManipulation() { if(obj == null) { fail(""obj should not be null""); } // Do some other valuable testing }  I think the usual use case is to call it when no exception was thrown in a negative test. Something like the following pseudo-code: test_addNilThrowsNullPointerException() { try { foo.add(NIL); // we expect a NullPointerException here fail(""No NullPointerException""); // cause the test to fail if we reach this } catch (NullNullPointerException e) { // OK got the expected exception } } If you do not check something in the catch block you can use the @ExpectedException(NullNullPointerException.class) method annotation to declare that you expect an exception (of a special kind)."
626,A,"Generating JUnit Testcases Our application depends on numerous resources that are provided by another system. To ensure the existence of those resources we currently have a JUnit test case (probably more an integration test) that takes a list of all the resources as a textfile fetches each and tracks success/failure. This is a very long running testCase that is not very ""tool friendly"". What we would really like to have is something along the lines of one test-method per resource. I am aware that this is not what JUnit was meant to do. But is there a way to generate those testmethods on the fly? Maybe something a bit more ""elegant"" than writing a perl-script to generate hundreds of methods? Thanks a lot! You might want to take a look at the TestSuite class and creating your own instance (rather than letting one of the junit runners just run all the tests in a certain dir) and/or subclassing it - the TestSuite has methods to programmatically addTests to it and then you can run all the tests within the suite.  This: http://github.com/adewale/cq-challenge-markup/blob/b99c098f0b31307c92bd09cb6a324ef2e0753a0b/code/acceptance-tests/AcceptanceTest.java is an example of a class that dynamically generates one test per resource using the JUnit4 @Parameterized annotation  You may want to look at parameterized tests. This is easier to achieve in JUnit 4 though can be done in JUnit 3. See this question for code: JUnit test with dynamic number of tests Jep! Perfect solution because you could implement an FileInputStream into this paramerized tests to load the resource list to check. In my oppinion this is a near perfect solution. In IDEA the parameterized tests sadly collapse to a single one. This means we still don't see at once which resources were not available. However thanks for the parameterized tests - I didn't knew them."
627,A,"Weird problem using JUnit in multi-thread environment I meet a weired problem when using JUnit in multi-thread environment. The following code should fail but it actually pass in eclipse. public class ExampleTest extends TestCase { private ExecutorService executor = Executors.newFixedThreadPool(10); private volatile boolean isDone = false; public void test() throws InterruptedException ExecutionException { executor.submit(new Runnable() { @Override public void run() { try { fail(); } finally { isDone = true; } } }); while (!isDone) { Thread.sleep(1000); } } } And here'a another piece of code here I use Future.get() to wait for thread stop in this case it will fail. public class ExampleTest extends TestCase { private ExecutorService executor = Executors.newFixedThreadPool(10); private volatile boolean isDone = false; public void test() throws InterruptedException ExecutionException { Future future=executor.submit(new Runnable() { @Override public void run() { try { fail(); } finally { isDone = true; } } }); future.get(); } } I googled it and found that JUnit can not handle Multiple-thread unit testingbut what's the differences between these two pieces of code ? Thanks Thanks ShiDoiSi JUnit cannot see the exceptions that occur in threads other than the thread in which the tests are running. In the first case through an exception occurs by calling fail it occurs in a separate thread run by the executor. Hence it is not visible to JUnit and the test passes. In the second case the same exception happens in the separate thread run by the executor but the exception is effectively ""reported back"" to the test thread when you call future.get. This is because future.get throws an ExecutionException if the computation of the future failed due to any exception. JUnit is able to see this exception and hence the test fails. So is there any replacement for Junit in this case ?  There is also the interesting fact that Eclipse and IDEA can spawn a VM in their junit test runners and end up calling system.exit() on it. This means if you don't wait properly in the test (as in the case when you sleep above and hope the the task has completed) it can exit unexpectedly. Interesting but not exactly what you were asking! see this link for details...  @zjffdu As @ShiDoiSi pointed out Thread.join() works fine if you have a single worker thread that you want to assert or fail from. If you have multiple worker threads or if you want a little more convenience there is a JUnit extension for performing multi-threaded assertions: ConcurrentUnit: public class ExampleTest extends ConcurrentTestCase { private ExecutorService executor = Executors.newFixedThreadPool(10); public void test() throws Throwable { executor.submit(new Runnable() { @Override public void run() { try { threadFail(""Failure message""); } finally { resume(); } } }); threadWait(); } } Good luck  As @abhin4v has pointed out the exception in the new thread gets swallowed. You could try providing your own fail-method that syncronises with the top-level thread very much like in your example with get(). But there's no need to use Futures just write to a shared variable indicating failure and use newThreadId.join(). Apart from that I'm not aware of any other way of solving this in plain JUnit.  Take a look at http://www.youtube.com/watch?v=wDN_EYUvUq0 (starting at 17:09) it explain problems you can get with JUnit and threads. I think that in your case get() throws a ExecutionException and that's why the second test fails. In the first testcase jUnit doesn't see the exception."
628,A,"Guice injector in JUnit tests Using Guice is it a good practice to get a new injector in each JUnit test class as each test class should be independant? Take a look at Guice Berry. http://code.google.com/p/guiceberry/ I won't recommend using it now (documentation is really terrible) but looking at their approach can make you think clear about how DI should be done in jUnit.  You should really avoid using Guice in Unit Tests as each test should be small enough that manual DI is manageable. For testing the bootstrapper code and integration tests then yes create a different injector for each test. +1 ... after using guice injection in all test I now feel the need to revert this decision as tests consume a lot time with Guice.createInjector :( I do not agree. With Guice you can use @Inject and inject fields with no setters or constructors. It is more readable. So manual dependency in such case should be what? I prefer use Injector than manual Reflection API because it first comes in mind to me. I never inject directly to field without setters. I virtually never use setter injection. Both of which I find ugly and hide the classes requirements from users of said class. I try to only use ctor injection. By using Guice (or any DI) in unit tests you are hiding away a warning that your class is getting to big and taking on too many responsibilities.  I suggest this framework I have recently written Guice-Behave. It is very simple with two annotations you can run the test in the same context of your application. You can define your mocks inside the Guice module and in this way it is very easy to re-use them.  I found AtUnit to be an excellent complement to Guice (it even deals with mock framework integration). This makes the Unit Test classes extremely clear and concise (never see an Injector there) and where appropriate also lets you exercise your production bindings as part of your unit tests.  I think using DI will make unit test code more simple I always Use DI for unit test and also for integration test. Without DI everything feels hard to code. Either using Guice Inject or Spring Autowired. like my test code bellow: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = ""/application-context.xml"") public class When_inexists_user_disabled { @Autowired IRegistrationService registrationService; private int userId; @Before public void setUp() { Logger.getRootLogger().setLevel(Level.INFO); Logger.getLogger(""org.springframework"").setLevel(Level.WARN); BasicConfigurator.configure(); userId = 999; } @Test(expected=UserNotFoundException.class) public void user_should_have_disabled() throws UserNotFoundException { registrationService.disable(userId); } } Personally I think this harder to work out as I need to look in the app context file to find out what IRegistrationService is being used if it is taking any mocks or stubs and how they are set up. If a test feels too hard to code manually then it is a sign that you *may* be testing too much or your object *may* require too much to get going. @mlk its no where near as bad with annotation config since you can setup everything you want including mocks within a single [at]Configuration bean which you can make as an inner class."
629,A,"is it possible to make test method parameterized not an entire class? As I understand with JUnit 4.x and its annotation org.junit.runners.Parameterized I can make my unit test ""parameterized"" meaning that for every set of params provided the entire unit test will be executed again from scratch. This approach limits me since I can't create a ""parameterized method"" for example: .. @Test public void testValid(Integer salary) { Employee e = new Employee(); e.setSalary(salary); assertEqual(salary e.getSalary()); } @Test(expected=EmployeeInvalidSalaryException.class) public void testInvalid(Integer salary) { Employee e = new Employee(); e.setSalary(salary); } .. As seen in the example I need two collections of parameters in one unit test. Is it possible to do in JUnit 4.x? It is possible in PHPUnit for example. ps. Maybe it's possible to realize such mechanism in some other unit-testing framework not in JUnit? TestNG does support that Simple workaround (that I assume is out of scope for this question) is to break them into 2 separate parameterized tests. If you are inclined on keeping them together then adding extra element to parameterized set of parameters does the trick for you. This element should indicate if given parameter is for one test or another (or for both if needed). private Integer salary; private boolean valid; @Test public void testValid() { if (valid) { Employee e = new Employee(); e.setSalary(salary); assertEqual(salary e.getSalary()); } } @Test(expected=EmployeeInvalidSalaryException.class) public void testInvalid() { if (!valid) { Employee e = new Employee(); e.setSalary(salary); }else { throw new EmployeeInvalidSalaryException(); } } JUnit parameterized tests serve classes - not methods so breaking it up works much better.  yes it's possible. recently i started zohhak project. it lets you write: @TestWith({ ""25 USD 7"" ""38 GBP 2"" ""null 0"" }) public void testMethod(Money money int anotherParameter) { ... } good idea I'll definitely give it a try. thanks!  Another option is to use JCheck or QuickCheck for Java Can you give us an example that shows how to use JCheck to solve the problem?"
630,A,"JUnit and Clojure unit testing I'm creating a library that includes both Clojure and Java code and would like to be able to run a single test JUnit suite across the entire code base. Seems that this should be possible since they are both running on the JVM and there is good Clojure/Java interop. However currently Clojure code seems to favour unit testing done through the clojure.test API using ""(run-all-tests)"" and friends. Is there a good way to wrap these Clojure tests in some sort of wrapper so that they can be automatically called from JUnit? (when I say automatically I mean without resorting to the manual solution of writing a JUnit test case to wrap and call each Clojure test case individually :-) ) you can easily setup maven for this.. start with [this](http://stackoverflow.com/questions/2574649/testing-clojure-in-maven) question.. i'd probably lean toward maven on leiningen The easiest thing to do would be to call run-all-tests from a single JUnit test. You could capture the output and look at the last line that indicates the pass/fail/error count. If you see a fail or an error you can make the JUnit test fail. However if you want interaction with each Clojure test you'll have to implement similar functionality to what's found in clojure.test. That is for each value in the each namespace look for a function stored in the :test meta-data value. That value is your test function. Lines 661-686 of test.clj give a good synopsis of how tests are stored and later run in Clojure.  Currently there exists no JUnit integration I know of. Besides calling run-all-tests from a single JUnit test as suggested by @psyllo you can build more thorough integration as follows: Build a function that generates an instance of junit.framework.Test for every test method defined in clojure. Have a look at clojure.test/test-ns on how to find all test methods. Make an instance of junit.framework.TestSuite which adds all those generated Tests for a certain ns and AOT compile this suite class. You can call the suite class from java / junit."
631,A,"How can I inject a data source dependency into a RESTful web service with Jersey (Test Framework)? I'm building a RESTful web service using Jersey that relies on MongoDB for persistence. The web service itself connects to the default database but for the unit tests I would like to use a separate test database. I would populate this test database in setUp run my tests and then destroy it in tearDown. Normally I would use dependency injection here to supply the data source to an entity manager that the service would use but in this case the web service is running independent of the unit tests. I'm using the Jersey Test Framework which starts up a Grizzly container to provide the web service interface and provides a web service client to the unit testing class. What is the best way to inject a dependency from my unit test class into the server instance (which Jersey Test Framework sets up in a Grizzly container)? After digging through the Jersey Test Framework source I've discovered an elegant way to inject dependencies into my RESTful resource classes. In my test class (which extends JerseyTest) I've added only an implementation for the configure() method: public AppDescriptor configure() { return new WebAppDescriptor.Builder() .contextListenerClass(ContextLoaderListener.class) .contextParam(""contextConfigLocation"" ""classpath:applicationContext.xml"") .initParam(""com.sun.jersey.config.property.packages"" ""[resource package]"") .build(); } This effectively provides a custom built WebAppDescriptor instead of relying on Jersey Test's Grizzly Web container to build one. This will use the ""applicationContext.xml"" file on the classpath which can be configured differently for running JUnit tests. Effectively I have two different applicationContext.xml files: one for my JUnit tests and the other for production code. The test's applicationContext.xml will configure the data access dependency object differently."
632,A,"How do I get Emma or Cobertura with Maven to report coverage on source code in other modules? I have a multi-module Maven setup with Java code. My unit tests in one of the modules exercise code in multiple modules. Naturally the modules have inter-dependencies and code in all relevant modules is compiled as needed in advance of test execution. So: How can I get a report on the coverage of the entire codebase? Note: I am not asking how to combine the results of coverage for tests in multiple modules. I am asking how to get coverage for tests in a single module using instrumented code from multiple modules. Anyone interested in the former might refer to these other questions and the recommendations by Crowne for Maven Dashboard and Sonar. I succeeded in getting a full coverage report using pure Ant. [EDIT:] I instrumented all jars from the development-runtime directory into a temporary directory; prepended the temporary directory to the classpath; then ran tests from Ant with batch-test. Ant can be run from Maven but the challenge here is seamless integration (i.e. feeding all the classpath and sourcepath elements from Maven to Ant automatically) which is why I did not use Maven's facilities for this purpose. There are also other questions about integration tests. However by default each project's report by default only reports coverage on code in the same project whereas my tests exercise code in multiple projects. This article in Spanish might be relevant. Here is another Seam-specific article. Generally reports pertain to their specific module however they can be aggregated two approaches are: maven-dashboard-plugin sonar I would recommend that you try using sonar to do the report aggregation for you. See their public instance ""nemo"" to see the impressive capabilities that are offered. Thank you. However I am not asking how to combine the results of coverage for tests in multiple modules. Rather I am asking how to get coverage for tests in a single module using instrumented code from multiple modules.  I doubt if would be possible since the coverage information is obtained by cobertura/emma by instrumenting the compiled classes. While this would work for classes in the specified project it is doubtful if these tools will instrument dependant libraries. A look at the maven cobertura plugin usage also does not seem to indicate any such possibility. All my modules are under my control. All are compiled and instrumented based on the dependency tree when I run Maven and the coverage tool. @Joshua Fox. I guess you need to check with cobertura developers or look at cobertura source.  Never tried but this may be a way to accomplish it: In each module just before the install phase let cobertura instrument the jar files and install the instrumented jar files (!) into the local Maven repository In the tests-module Maven will use the artifact dependencies from the local Maven repository to run the tests. These instrumented classes should now appear in the datafile e.g. cobertura.ser Run the cobertura-report generation as usual from within the tests-module of your project e.g. mvn site See cobertura documentation on how to manually invoke cobertura to instrument external JAR files in-place: ... You can also pass in jar files to be instrumented using standard ant filesets. Cobertura will extract each class from the jar and instrument it. If 'todir' was not specified then the original jar will be overwritten with an instrumented version ... The pom.xml's build plugins may look like this - you may want to add a profile or use classifiers to distinguish between the final jar file and the instrumented jar file if you don't want to overwrite them in your local repo. Then in the tests module you just need to define the dependencies to your other modules using the classifiers.  <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <executions> <execution> <id>cobertura-inplace-instrumentation</id> <phase>package</phase> <configuration> <tasks> <taskdef classpathref=""maven.plugin.classpath"" resource=""tasks.properties"" /> <cobertura-instrument datafile=""${project.build.directory}/cobertura-nop.ser""> <fileset dir=""${project.build.directory}""> <include name=""${project.build.finalName}.${project.packaging}"" /> </fileset> </cobertura-instrument> </tasks> </configuration> <goals> <goal>run</goal> </goals> </execution> </executions> <dependencies> <dependency> <groupId>net.sourceforge.cobertura</groupId> <artifactId>cobertura</artifactId> <version>1.9.4.1</version> </dependency> </dependencies> </plugin>  I found this pretty straightforward (although I did it a while ago and may be rusty with details... My core project contains all of the modules. I use Cobertura to measure my test coverage. I am using Hudson as a Continuous Integration engine and have the Cobertura plugin for Hudson. It has been working a treat for a while now. Good Luck !  This recent blog post by Thomas Sundberg contains a method that partially solves the issue by using ant for the cobertura calls instead of using the maven cobertura plugin. It relies on the following basic approach with specialised pom.xml and build.xml files : Start with a typical maven compile on the parent pom which will compile all classes in the child modules. mvn clean compile # maven-compile-plugin called for compiling Then instrument all of the module classes: ant instrument # cobertura called for instrumentation Then call the maven-surefire-plugin called for testing using the instrumented classes with cobertura as a test dependency mvn test Then use a custom report call to pull in all of the results from different modules: ant report # cobertura called for reporting The key elements of the ant build.xml file are to instrument all modules separately and then to report on all of the modules after merging the results. This function needs to be called for each module in his example: <target name=""instrumentAModule""> <property name=""classes.dir"" value=""target/classes""/> <cobertura-instrument todir=""./${module}/${classes.dir}""> <fileset dir=""./${module}/target/classes""> <include name=""**/*.class""/> </fileset> </cobertura-instrument> </target> Then after the testing is complete the reporting phase first merges all results from all of the different directories are merged into a new .ser file (called sum.ser in his example) <target name=""report"" depends=""merge""> <property name=""src.dir"" value=""src/main/java/""/> <cobertura-report datafile=""sum.ser"" format=""html"" destdir=""./target/report""> <!-- Add all modules that should be included below --> <!-- fileset dir=""./MODULE_NAME_TO_REPLACE/${src.dir}""/ --> <fileset dir=""./product/${src.dir}""/> </cobertura-report> </target> <target name=""merge""> <cobertura-merge datafile=""sum.ser""> <fileset dir="".""> <include name=""**/cobertura.ser""/> </fileset> </cobertura-merge> </target> It may be possible to integrate the ant components into maven using the antrun plugin but I am not familiar enough with the phases/lifecycles to know where to put the different calls. This is very useful for me as I write abstract test classes in my api modules and then provide them with an implementation in my lib modules. So far both cobertura and emma have been unable to handle this design so my code coverage is typically 0 or in the single digits."
633,A,"JUnit Eclipse show System.out.print()'s I am using JUnit 3 with Eclipse 3.4. When I run a JUnit test case everything works fine and the test completes perfectly. Only thing is I want to see the output of the classes that I am running. All the classes have basic System.out.print() to some output values. So when I run the tests I don't see any console messages at all. How do I get Eclipse to show System.out.print() output in Console window when a test is successful? I had the same problem and i solve it putting @Test above the function test with the output and that solved it ... Hoping that this can help ... :)  Strange - jUnit does not suppress System.out statements. If the statement is hit during the test run then it is executed and writes to System.out which is when executed within eclipse (Run Configuration) the console. window. But maybe you've redirected System.out to write to a file or a log !? Nope there is not redirection at all. I am using Eclipse's ""Run as Junit test"". If I run the code normally the Console shows the Sys.outs. If I run the same classes using the JUnit in Eclipse The Console window is blank.  Maybe other program was running on the console of Eclipse too. Please ensure Eclipse displays the active view of your JUnit code. From Eclipse Help: Select the “Display Selected Console” command to bring the console selected from the resulting list into focus. Note: this command is only enabled if you have more than one console open. Welcome! I also met the problem before:) Thanks that was the case...For some reason I had multiple views for console and when I'd run JUnit it would create a new console view but the old one was hidden where the System.outs would go to. Thanks!"
634,A,"Cant get Junit to work with Spring in an ANT build I am trying to get Spring MVC and Junit working with eachother and I am having trouble configuring the build.xml file. I am fairly new at using ANT and do not really understand all the tutorials. I put the junit.jar file in the lib direcotory but still am getting the following message from my console. test: BUILD FAILED C:\Java\mmz\WEB-INF\build.xml:63: The <classpath> for <junit> must include junit.jar if not in Ant's own classpath Total time: 223 milliseconds Edit Here is an example of wanting to run just one test and I am not sure what I need to do i tried doing something like the following but I dont quite understand what its doing so I dont know how to make changes  <path id=""classpath.test""> <pathelement location="""" /> <pathelement location=""${test.dir}"" /> </path> <target name=""test""> <junit> <classpath refid=""classpath.test"" /> <formatter type=""brief"" usefile=""false"" /> <test name=""TesterTest"" /> </junit> </target> I am pretty lost on what to do next. Thanks Could you include the section of your build.xml that contains the junit task declaration? Did you declare a `` section in it? I'm not sure where Spring comes into this either. If you copy-pasted your build.xml file then there is probably a classpath element somewhere. Probably near the top of the file. In that element they have probably listed some paths and/or files. You need to add the junit jar to this section."
635,A,"How to make jmunit run all tests and still fail at the end if any tests failed When I switch off haltonfailure or haltonerror in my jmunit tests like this: <jmunit haltonerror=""false"" haltonfailure=""false"" failureproperty=""testfailure""> <formatter type=""xml"" /> <classpath> <path path=""${jar_location}"" /> <path path=""${build}"" /> </classpath> <!-- Add --> <test name=""com.example.tests.Test1"" todir=""${reports}"" /> <test name=""com.example.tests.Test2"" todir=""${reports}"" /> <test name=""com.example.tests.Etc"" todir=""${reports}"" /> </jmunit > The build succeeds even when some tests failed. If I turn the halts on then of course it halts immediately upon the first failure. The behaviour I want is that all tests are executed regardless of how many of them fail but if ANY of them failed then the build won't be successful. The reason I want this is so that the reports can correctly show how many passes and failures and which tests are failing. How do I achieve this? and/or Is my thinking that I want all of the tests to be run flawed? Easy peasy: You need to add the command <fail message=""Something went wrong. Please check the test results."" if=""testfailure"" /> to the target that contains your jmunit command after the jmunit command itself. This way the jmunit task will set the variable testfailure when one of the tests fails and the command fail will make the build unsuccessful if that property is set. Hope that helps. It works and so easy. Thanks."
636,A,"How to speed up grails test execution While developing a Grails 1.0.5 app I'm appalled at how slow the grails test-app command is. Even though the actual tests take just ~10 seconds the whole execution adds up to real 1m26.953s user 0m53.955s sys 0m1.860s This includes grails bootstrapping loading plugins compiling all the code etc. Any hints on how to speed up the grails test-app execution would be greatly appreciated. Increasing the java memory/JVM options can definitely speed things up. The amount of memory you can give depends on your equipment. If you are running grails from the command line set the GRAILS_OPTS environment variable. Add something like this to ~/.bash_profile export GRAILS_OPTS=""-Xms3000M -Xmx3000M -XX:PermSize=256m -XX:MaxPermSize=512m"" If you use GGTS(Eclipse) you'll need to add this to the VM arguments of the run configuration. There are also a few JVM settings that can be modified to increase the speed: -XX:+UseCodeCacheFlushing -XX:MaxInlineLevel=15 -noverify (turns off class validation)  grails now comes with http://grails.org/plugin/testing installed. this mocks the domain stuff so you can do some testing of domain classes as unit tests. they run pretty fast.  There aren't any hard and fast rules for speeding it up and the performance issues that you're seeing might be specific to your app. If your bootstrapping is taking ~75 seconds that sounds pretty long. I'd take a close look at whatever you have in your Bootstrap.groovy file to see if that can be slimmed down. Do you have any extra plugins that you might not need (or that could have a major performance penalty)? This might not be a possibility for you right now but the speed improvements in grails 1.1.1/groovy 1.6.3 over grails 1.0.5/groovy 1.5.7 are fairly significant. Another thing that really helps me when testing is to specify only integration tests or only unit tests if I'm workiing on one or the other: grails test-app -unit grails test-app -integration You can also specify a particular test class (without the ""Tests"" prefix) to run a single test which can really help with TDD (ex for ""MyServiceTests"" integration): grails test-app -integration MyService In grails 1.1.1 bootstrapping with 5 plugins and ~40 domain classes takes me less than 20 seconds. Thanks for the suggestions - I'd like to add that this is my slow dev hardware as well. Moving to grails 1.6.3 cut off just 5%. @Robert You must mean groovy 1.6.3.  You can use interactive mode to speed up your test runs. Just run grails interactive Then type test-app The first time will be the same as usual but each time after that will be dramatically faster. There are currently some issues with interactive mode (like running out of memory after a few runs) but I still find it worth it. Brilliant! Thank you. I just recommend setting your permgen space more generously or you'll run out after a dozen cycles. This is what I use: -XX:PermSize=64m -XX:MaxPermSize=512m We run our grails project via Maven in this case the first command is: mvn grails:exec -Dcommand=""interactive"". Also after the first run you can rerun the test-app command by just pressing Enter.  Please see my answer here. A plugin relying on a poorly defined maven artifact can cause grails to go and look every time for a newer version. Grails very slow to resolve certain dependencies  You can choose to run unit and integration tests in parallel as well - see this article  If you're still using Groovy 1.5.x you could probably of shave a few seconds by upgrading to Groovy 1.6 Thanks for the answer. I'm not going to make that move since I'm uncertain of how Grails 1.0.x will work with 1.6.x and 1.5.x is the bundled one."
637,A,"Ant is telling me that my Junit test is successful when it clearly isnt I have the following build.xml file <property file=""build.properties""/> <property name=""src.dir"" value=""src""/> <property name=""build.dir"" value=""classes""/> <property name=""web.dir"" value=""war""/> <property name=""test.dir"" value=""test""/> <path id=""build.classpath""> <fileset dir=""lib""> <include name=""*.jar""/> </fileset> <!-- servlet API classes: --> <fileset dir=""${appserver.lib}""> <include name=""servlet*.jar""/> </fileset> <pathelement path=""${build.dir}""/> <pathelement path=""${test.dir}""/> </path> <path id=""classpath.base""/> <path id=""classpath.test""> <pathelement location=""c:/ant/lib/junit.jar"" /> <pathelement location=""${build.dir}""/> <pathelement location=""${src.dir}""/> <pathelement location=""${test.dir}"" /> <pathelement location=""classes""/> <path refid=""classpath.base"" /> </path> <target name=""build""> <!-- Following two lines creat src and test folders in WEB/INF folders --> <mkdir dir=""${build.dir}""/> <mkdir dir=""${test.dir}""/> <javac destdir=""${build.dir}"" source=""1.5"" target=""1.5"" debug=""true"" deprecation=""false"" optimize=""false"" failonerror=""true""> <src path=""${src.dir}""/> <classpath refid=""build.classpath""/> </javac> <javac destdir=""${build.dir}"" source=""1.5"" target=""1.5"" debug=""true"" deprecation=""false"" optimize=""false"" failonerror=""true""> <src path=""${test.dir}""/> <classpath refid=""build.classpath""/> </javac> </target> <target name=""test""> <junit haltonfailure=""true"" printsummary=""yes""> <classpath refid=""classpath.test"" /> <classpath refid=""build.classpath""/> <formatter type=""brief"" usefile=""false"" /> <batchtest fork=""yes""> <fileset dir=""${test.dir}""> <include name=""**/*Test.java""/> </fileset> </batchtest> </junit> and I have the following test class just to test out the build package com.mmz.mvc.test; import junit.framework.*; public class MemberDAOTest extends TestCase { public void test1() { assertTrue(""Test didn't work""false); } } Obviously this test is supposed to fail but its not. Can anybody tell me why I am getting the following output. Also if there is something you see wrong then please let me know if I can improve my build script I am new to writing any build files. test: [junit] Running com.mmz.mvc.test.MemberDAOTest [junit] Testsuite: com.mmz.mvc.test.MemberDAOTest [junit] Tests run: 1 Failures: 0 Errors: 0 Time elapsed: 0.018 sec [junit] Tests run: 1 Failures: 0 Errors: 0 Time elapsed: 0.018 sec BUILD SUCCESSFUL Total time: 1 second I am sorry but I doubt you are running the test from same file. Can you verify if you have some other version of same file at another location and you are mixing it up? what do you mean from the same file? I mean as you said this test should fail. Ant is not doing anything as such that can affect this test. Only thing seems possible to me is that ant is not running test from same folder and there may be some other place where you may have written similar text. You can try running ant in verbose mode using -v at the end and see if you get some idea. Easy way to confirm Ant runs your latest version of MemberDAOTest: Add method test2() to see if Ant runs test2() or not."
638,A,"Why would extending JerseyTest vs extending TestCase cause no tests to be found I am attempting to get the Jersey test framework working. We are building using maven 1.x. I've created the following testcase... public class SomeResourceTest extends JerseyTest { public SomeResourceTest () throws Exception { super(new WebAppDescriptor.Builder(PACKAGE_NAME) .contextPath(PATH).build()); } @Test public void testSomething() { Assert.assertEquals(true true); } } When I build I get no tests found in SomeResourceTest. Now when I change the testcase to extend junit.framework.TestCase the test runs just fine. Any clue what might be causing the problem? JerseyTest is supposed to extend TestCase so I am assuming it to be some other configuration problem. Perhaps you need a ""JerseyTest"" plugin for maven? You may want to add the junit tag to this question so more test folks see it. What version of JerseyTest are you using? The latest version relies on JUnit 4 and does not extend TestCase. Also your test is a bit confusing. It has @Test which implies you are using JUnit 4 and shouldn't extend TestCase. But it sounds like you are still relying on the testXXX and TestCase subclass convention from JUnit 3.8 in your description.  Any clue what might be causing the problem? JerseyTest is supposed to extend TestCase (...) The Jersey Test Framework is build over JUnit 4.x so no it doesn't. To run JUnit 4.x tests with Maven 1.x you'll have to: Add Junit 4.X in your dependencies Use the JUnit4TestAdapter in your test classes: /** * @return instance of this as Junit test case */ public static junit.framework.Test suite() { return new JUnit4TestAdapter(MyTestClass.class); } Use a JDK 5+ See http://jira.codehaus.org/browse/MPTEST-65. This worked along with upping my Jersey jars to the latest. I think I had some combination of Junit 3.x and a motely Jersey collection. Thanks!"
639,A,DB backend webapp testing in java [tool needed] I want to create a test suit for my java web application. Its a JSP applications with JDBC connectivity . My requirements are as follows 1 - I should be able to test my database logic (Queries etc) through my models. 2 - Its great if i could test my .jsp pages as well (if possible) After doing some research I found that DBUnit is good for database backend system testing but unfortunately i couldnt find any good resource as a starter What are you all think about testing options I have and it would be great if you could post some links to resources/ examples as well EDIT: and I have come across with mock objects (like JMock..) wonder I could use it as a replacement for DBUnit ? thanks in advance cheers sameera DBUnit's official getting-started article here worked for me. Re: database logic testing you might also want to check this out. As for JSP testing I have used Cactus to test my servlets before with success. However I'm don't know about its JSP-testing facilities. You're welcome :) thanks for the answer :D  For your 1st question have a look at this StackOverFlow thread... For 2nd I would go with Chry's suggestion of Cactus or Selenium. Hope that helps. thanks for the answer :D  It's not clear from your question if you want to run Integration tests (Front end + back end) or Unit Tests against you Database layer. If you need a tool that allows you to write Integration tests you should definitively look at Selenium. With Selenium you can generate functional tests by simply navigating your web site (JSP pages) and asserting that stuff on the page exists or it's equal to some values. Selenium comes with a Firefox plugin that will basically generate the code for you. You can replay the test in the browser or export them as Java code and make them part of your test suite. Selenium is an invaluable tool. The drawback of using a tool like Selenium is that your application need to be deployed somewhere before you can run your test suite. This may be a limitation if you plan to run automated tests generated using Selenium. If you are only interested in testing your database access code (DAO Data Access Layer) DBUnit is the perfect tool. Generally DBUnit is used to initialize the database tables before testing and less often to run assertions on the database content. DBUnit uses an XML based format to represent the data that will be inserted into the database. The XML files containing the data to pre-populate the db are normally triggered by a build script (Ant Maven etc.) or directly in your unit test code. I have used both approaches it really depends on how your code is structured and how you access the database (Hibernate Spring+Hibernate JDBC...). If your database is not too big I'd recommend you populate it just before running your test suite. Alternatively you can populate only the tables that you are interested in testing prior to every test. Here is a link to Unitils that is an additional library that can be used on top of DBUnit to simplify the database testing strategy. I think it can be used as a reference to get you started: http://www.unitils.org/tutorial.html#Database_testing Here is anoter link (quite old 2004) showing the basic mechanics of DBUnit: http://onjava.com/pub/a/onjava/2004/01/21/dbunit.html thanks for the answer :D
640,A,"NoClassDefFoundError when trying to use JMockit Coverage I am trying to use JMockit's code coverage abilities. Using the JVM parameter -javaagent:jmockit.jar=coverage=.*MyClass.java:html:: I am able to run my tests (jmockit.jar and coverage.jar are on the classpath) unfortunately my log file says: Loaded external tool: mockit.coverage.CodeCoverage=.*MyClass.java:html:: Loaded external tool: mockit.integration.junit3.JUnitTestCaseDecorator Loaded external tool: mockit.integration.junit4.JUnit4ClassRunnerDecorator Exception in thread ""Thread-0"" java.lang.NoClassDefFoundError at mockit.coverage.CodeCoverage$OutputFileGenerator.run(CodeCoverage.java:56) ...and no coverage file is generated. Has anyone gotten JMockit Coverage to work? If so any thoughts as to what is causing this error? Thanks... Answer: I needed to add coverage to the bootstrap entries rather than only the user entries (in the Eclipse run configuration) Actual Answer The actual answer is that I was running the test with JUnit 3 but the coverage needs JUnit 4. That fixed things and I didn't have to add any bootstrap entries. I was running the test with JUnit 3 but the coverage needs JUnit 4. That fixed things and I didn't have to add any bootstrap entries.  Random guess... Is coverage.jar on the classpath that jmockit uses - it might be a different one? Any idea how I can find out which classpath it's using and whether it is a different?"
641,A,"Maven Spring tests fail when run together but succeed individually (ehcache closed IllegalTransactionStateException) We're using Maven/Surefire and Spring/Hibernate transactional tests for a fairly large web application. There are 138 Test* classes running a total of 1178 tests. A straightforward ""mvn test"" will generate 82 errors the nature of which tend to imply a corrupt application context: Many of these: IllegalTransactionStateException: Pre-bound JDBC Connection found! A few of these: NoSuchMethodError: org.hibernate.cache.CacheException.(Ljava/lang/Exception;)V For every failed test running the test class individually ""mvn test -Dtest=TestFailingClass"" succeeds. Indeed using -Dtest=TestClass1TestClass2Etc."" with various subsets of all my test classes succeeds or fails in different ways. For instance running only the failing test classes succeeds with 0 errors. With no apparent means to control the order of classes tested by Surefire I have a difficult time determining which of my test classes seem to be leaving the context in a bad state. What I'm looking for is a strategy to help determine what is happening in some kind of deterministic manner. I can certainly see the order of tests run from the log but I cannot reproduce that order controllably. And of course suggestions for what to do about it ... Which Spring and JUnit version you use? Do your tests inherit from AbstractJUnit4SpringContextTests or AbstractTransactionalJUnit4SpringContextTests? Spring 2.5 JUnit 4.4 and mostly AbstractAnnotationAwareTransactionalTests though I suspect there is some version creep. Newer test classes may extend from newer Spring scaffolding. I haven't researched through all 138 test classes at this point. Indeed the problem comes from a corrupt Spring application context. One of the early tests is dirtying the context and causing the following tests to error out. One difficulty is trying to control the order of the tests while discovering the test causing the trouble. I was able to accomplish that by using Maven's log to find the order of the test classes run then excluding tests one at a time from the top. Thirty-four tests in I found the culprit. It was a test called TestSpringContexts. Adding @DirtiesContext to these tests solve the problem but it was also solved by removing calls to context.close() from the tests. I wrote a blog post about the process here but that's the gist of the matter: http://mojo.whiteoaks.com/2010/04/27/finding-the-test-that-corrupts-the-suite/ Mojo  With no apparent means to control the order of classes tested by Surefire I have a difficult time determining which of my test classes seem to be leaving the context in a bad state. Indeed. And running subsets of the tests will produce different results (from an execution order point of view) making it very hard to debug your problem. But you could maybe use the patch from SUREFIRE-321 (Run tests in alphabetical order) to get better control (check the comments one of the poster was facing a very similar problem to yours)."
642,A,HSQLDB Constraint Violation & SQL Query Log for an HSQLDB in-memory setup We have a setup where we are using an embedded HSQLDB for backing Hibernate/JPA unit tests in java and we are using the in-memory database mode since we simply want the database thrown away after the test run. My problem is that one of the tests is failing due to a constraint violation and HSQLDB lists the column as SYS_CT_286 and the query that appears in the log is the prepared statement where I cant see what the actual parameter values are (they are replaced by '?'). My questions are: 1- Is there a way in which I can see the actual SQL being executed? (like the mysql query log for example?). 2- What exactly is SYS_CT_286? it is not one of my columns is it a generated column? is there something obvious that may be wrong with it? Thanks. HSQLDB keeps a redo log which might be useful for debugging what sql has been run but I'm not sure if it does this for an in-memory db. If you change your db temporarily to a file-based db named test the redo log should be named test.log but it disappears on a clean shutdown. SYS_CT_286 is most likely a constraint with a system-generated name. Again if you make a file-based DB you might be able to look at it and find out what it's a constraint for. And if it's a constraint you're defining you might even be able to change your mapping so it gets a sensible name. I know you can do this with foreign key constraints anyway. The problem is that the database starts and shuts down with the test so to maintain the log probably I will need to do some standalone setup. I will try the P6Spy solution first thanks! Sounds sensible to me. Getting it to use a file-based db might help with diagnosing but you'd have to hack your tests a bit to do it. Another thing worth doing is to generate the ddl for your database using the generateSchemaCreationScript(Dialect dialect) method in org.hibernate.cfg.Configuration. If that constraint is generated by hibernate it should show up there.  Is there a way in which I can see the actual SQL being executed? I'm not sure HSQLDB allows to log the SQL statements (like select) being executed but you can use a proxy JDBC driver like P6Spy for this (already mentioned in this answer). What exactly is SYS_CT_286? This is a constraint (and I would bet on a unique constraint). Thanks P6Spy is a good idea will try it...
643,A,"Is there an alternative to mock objects in unit testing? It's a Java (using JUnit) enterprise Web application with no mock objects pre-built and it would require a vast amount of time not estimated to create them. Is there a testing paradigm that would give me ""some"" test coverage but not total coverage? Why would you write the same mock object multiple times if it is used in several tests? For instance Spring provides a ""pre-built"" MockHttpServletRequest that you can use outside of a running servlet container. I think you may be mis-understanding what a mock object is - you don't pre-build them you write them specifically for each test. Are you referring to stubs or dummy implementations or that sort of thing? (1) Alternatives to unit-testing (and mocks) include integration testing (with dbUnit) and FIT testing. For more see my answer here. (2) The mocking framework Mockito is outstanding. You wouldn't have to ""pre-build"" any mocks. It is relatively easy to introduce into a project.  Have you tried a dynamic mocking framework such as EasyMock? It does not require you to ""create"" a Mock object in that you would have to write the entire class - you specify the behavior you want within the test itself. An example of a class that uses a UserService to find details about a User in order to log someone in: //Tests what happens when a username is found in the backend public void testLoginSuccessful() { UserService mockUserService = EasyMock.createMock(UserService.class); EasyMock.expect(mockUserService.getUser(""aUsername"")).andReturn(new User(...)); EasyMock.replay(mockUserService); classUnderTest.setUserService(mockUserService); boolean isLoggedIn = classUnderTest.login(""username""); assertTrue(isLoggedIn); } //Tests what happens when the user does not exist public void testLoginFailure() { UserService mockUserService = EasyMock.createMock(UserService.class); EasyMock.expect(mockUserService.getUser(""aUsername"")).andThrow(new UserNotFoundException()); EasyMock.replay(mockUserService); classUnderTest.setUserService(mockUserService); boolean isLoggedIn = classUnderTest.login(""username""); assertFalse(isLoggedIn); }  I would echo what others are saying about EasyMock. However if you have a codebase where you need to mock things like static method calls final classes or methods etc. then give JMockit a look.  I think the opposite is hard - to find a testing methodology that gives you total coverage if at all possible in most cases.  You should give EasyMock a try.  Do you have real world data you can import into your testbed to use as your 'mock objects' that would be quick  Well one easy if not the easiest way to get an high level of code coverage is to write the code test-first following Test-Driven Development (TDD). Now that the code exists without unit tests it can be deemed as legacy code. You could either write end-to-end test external to your application those won't be unit tests but they can be written without resorting to any kind of mock. Or you could write unit tests that span over multiple classes and only mock the classes that gets in the way of your unit tests."
644,A,"Problem compiling Junit classes I have my java project compiled and working fine but now I want to compile the JUnit tests and I'm having some trouble doing it. I have all the tests.java in /test/model so I tried doing this: javac -cp org.junit.runner.JUnitCore ./test/model/Mytestclass.java But it doesn't detect the classes of my project so it fails. I also tried with this classpath: CLASSPATH=/usr/share/java/junit4.jar:/build/model:/src/model In /build/model there are the .class files and in /src/model there are the main Java files of my project. How can I compile this? what is the compiler error? What is the package name you are using for your junit test classes? The compiling error is: cannot find symbol. For example if I'm trying to do the JUnit of dogs in the dogsTest.java I have to create dogs and compare them but it's like it doesn't get the information of dogs.java do you know what I mean? The package name where my JUnit tests are is model as well as the main classes (they're in different folders). Do you really have the files at the ""/"" level or are they in a /user/yourUserName/... directory somewhere? Everything is in the folder of the project which is on the desktop. The structure is the one I said before: /build/model -- compiled .class files /src/model -- main .java files /test/model -- JUnit .java files If the package name is ""model"" then you should have the folder that contains the package folder in you classpath not the actual package folder itself. Try this classpath: CLASSPATH=/usr/share/java/junit4.jar;/build;/src;/test But that will only work if your code is actually in /build /src and /test. Let's assume that you have your main project folder is at: /somePath/dogs and your code is in: /somePath/dogs/src/model/Dog.java /somePath/dogs/test/model/DogTest.java and your jar files are in: /somePath/dogs/build then your classpath should look like: CLASSPATH=/usr/share/java/junit4.jar;/somePath/dogs/build;/somePath/dogs/src;/somePath/dogs/test try ';' in your classpath instead of ':' I was doing: export classpath... and then compiling with javac instead of doing: java -cp classpath... Now it works although it doesn't recognized the imports:""package org.junit does not exist import org.junit.Test;"" You do need to have a path to the junit jar file when you compile the program and when you run the program as well. I always open the terminal in my folder's project so I guess your CLASSPATH should have worked but it didn't. I've tried with the whole path but the error is the same. The weird thing it's that in Netbeans works fine but when I try to do it manually it doesn't work..."
645,A,"OpenEJB & JUnit: Sessioncontext.isCallerInRole returns allways false I need to write a session bean that somewhere in the code checks if the current user has some role(s). To unittest my EJB3 I'm trying out OpenEJB. I followed their example about testing security but if I test in my code for the role with SessionContect.isCallerInRole() it always returns false. Why doesn't it work? I've written some code to illustrate. My local interface: @Local public interface MyBean { boolean doSomething(); } My EJB: @Stateless public class MyBeanImpl implements MyBean { @Resource private SessionContext sessionContext; @Override public boolean doSomething() { return this.sessionContext.isCallerInRole(""role1""); } } My test: public class MyBeanTest { private Context context; @Before public void setUp() throws Exception { final Properties properties = new Properties(); properties.put(Context.INITIAL_CONTEXT_FACTORY ""org.apache.openejb.client.LocalInitialContextFactory""); this.context = new InitialContext(properties); } @Test public void test1() throws Exception { final Caller roleBean = (Caller) this.context.lookup(""RoleBeanLocal""); roleBean.call(new Callable<Object>() { @Override public Object call() throws Exception { final MyBean myBean = (MyBean) MyBeanTest.this.context.lookup(""MyBeanImplLocal""); Assert.assertTrue(myBean.doSomething()); return null; } }); } @Test public void test2() throws Exception { final Caller role2Bean = (Caller) this.context.lookup(""Role2BeanLocal""); role2Bean.call(new Callable<Object>() { @Override public Object call() throws Exception { final MyBean myBean = (MyBean) MyBeanTest.this.context.lookup(""MyBeanImplLocal""); Assert.assertFalse(myBean.doSomething()); return null; } }); } public static interface Caller { <V> V call(Callable<V> callable) throws Exception; } @Stateless @RunAs(""role1"") public static class RoleBean implements Caller { @Override public <V> V call(final Callable<V> callable) throws Exception { return callable.call(); } } @Stateless @RunAs(""role2"") public static class Role2Bean implements Caller { @Override public <V> V call(final Callable<V> callable) throws Exception { return callable.call(); } } } Well apparently it's not supposed to work. It's part of the spec that @RunAs doesn't change the Principal's permissions. I posted the same question on the OpenEJB forum (see it at Nabble) and got some more info there as well as a better solution."
646,A,"Running ant build gives ""package org.junit does not exist"" When I use a Java IDE to build projects (e.g. NetBeans) that have JUnit tests they compile fine but when I try to use ant outside of the IDE to run the build script I get the error ""package org.junit does not exist"". You should add your junit.jar into the classpath definition in your ant file. There are many way to do it one example is: <junit printsummary=""yes"" haltonfailure=""yes""> <classpath> <path refid=""your.classpath.refid"" /> <fileset dir=""${junit.dir}""> <include name=""**/junit.jar"" /> </fileset> </classpath> ... </junit> See Ant Manual for details on setting up your classpath. Most IDEs like NetBeans and Eclipse include junit.jar in the classpath automatically as part of their feature supporting unit testing within the IDE. Even when you use the IDE to run your ant build the IDE has the classpath environment setup already so ant is able to find your junit.jar. How come NetBeans uses ant to run JUnit tests fine but when I run the same ant build script outside the IDE it wouldn't find JUnit?  Late answer here. Copy the junit.jar file to the ${ANT_HOME}/lib folder.  The problem was that in the IDE it set the classpath correctly to include the .jar for JUnit. Running ant outside the IDE the classpath was different thus the error. The fix was to put the JUnit .jar in the folder ""C:\Program Files\Java\jre6\lib\ext"" so it would always be found outside of any IDE. That works but it is the hackish way to do it."
647,A,"Set property to mock object jmock Is it possible to assing value to mock object. Ex: myMockObject = context.mock(MyObject.class); myMockObject.setId(""someId""); My method which I'm testing reaches the end but at the end there is method for validation of that object so object without id is considered to be invalid. Is there anything else I can do about this? Can I somehow specify ok I'm expecting this exception but pass the test anyway? I found this link but I'm unable to found solution : http://www.jmock.org/yoga.html I'm expecting logger to throw an validation exception with message string did anyone have experience with this before? I tried this : context.checking(new Expectations() { { allowing(logger).error(with(exceptionMessage)); } }); Note exceptionMessage message is thrown by the validation method which validates the object at the end of the method which I'm testing. You need to add an Expectation that causes the mock method to return the value you expect: allowing (myMockObject).getId(); will(returnValue(""someId"")); This will cause getId to return the value you expect and since it is using the allowing invocation count it won't cause the test to fail if it is not called. where will I write this code? Inside `context.checking(new Expectations() {` or ? I'm getting compile error can't find stubs `The method stubs() is undefined for the type myMockObject` +1 for the effort @c0mrade what version of jMock are you using? I'm using 2.5.1 @c0mrade edited to reflect the correct version of jMock see above. this validate method doesn't call the getter method that is the issue but it does send the validation error to the logger so I though I could expect logger to send certain message but I guess version doesn't support with(eq(errorString)) as I saw in the link I posted in my edit  This was the answer I was looking for : http://www.jmock.org/throwing.html"
648,A,"Where should I put my JUnit tests? I've got 2 questions about organising Unit tests. Do I have to put test to the same package as tested class or can I organise tests in different packages? For example if I have validity and other tests is it correct to split them into different packages even if they are for same class? What about mock and stub classes? Shall I separate them from packages containing only tests or put them together? The way we do our JUnit test cases is to put them in the same package but in a different root directory. Since we use Maven we just use the standard locations making the structure similar to the following. src/main/java/com/foo/bar.java src/test/java/com/foo/barTest.java Obviously there's more to the structure but this lets us build the tests separately from the mainline code but still access protected classes and the like. With respect to different types of tests this is very subjective. When we started our testing effort (which unfortunately started after development) I tried to keep things pretty isolated. Unfortunately it quickly became a nightmare when we got to the 500+ test case point. I've since tried to do more consolidation. This led to reduced amounts of code to maintain. As I said though it's very subjective. As far as test-only code we keep it in a separate com.foo.test package that resides only in the src/test/java tree. Honestly the only reason I went with src/main/java and src/test/java is so I wouldn't have to do any additional configuration of Maven. Similar thing here except that we use ""src/com/foo/..."" and ""test/com/foo/..."". Personally I find Bob's answer even better. Even if a JUnit test _is_ source code.  Test classes should be rather in different packages it's easier to separate them from the production code when you package it for release. I usually keep lots of test fluff in those packages all sorts of mocks configurations scenarios.. But when you build - it doesn't get it. In some situations it's a good idea to keep your testing stuff even in different projects. Depends.  Keeping it the same package allows you to use package-private visibility for code that is intended to be accessed via the test only. Regarding using separate root directories that is a good practice. It also has an advantage for us since we use IDEA IDEA recognizes that production code cannot reference test code. In terms of keeping them separate there is a great power in having one and only one test class per production class at the unit level. Of course some classes get created in production as part of refactoring that have no test classes at all and that is fine but when you want to know what test tests a certain class having a convention that says ClassNameTest is the tests for ClassName is very helpful. TestNG is much friendlier to this paradigm than JUnit though.  I too tend to put my tests in the same package but under a different root directory. This allows me to test package-private classes or access packing-private classes while testing something else in the package. They are kept in a separate directory tree to allow excluding them from the deployed result (in particular to ensure that test code didn't accidentally get into production code). What matters most however is what works for your situation. In terms of how many test classes per production class the theory I've seen is that you write one test class per fixture that is per setup structure. In many cases that is the same (or close enough) to one test class per production class but I have sometimes written more test classes (in particular equality tests tend to be separated) for a give production class and occasionally one test class of for a group of (related) production classes (say for testing the Strategy pattern). Mostly I don't worry too much about the theory but rework the tests as needed to keep duplication to an absolute minimum."
649,A,In the Java version of Google App Engine how can you eval and execute Java code and unit tests passed in as strings? I am currently using an online system running in Google App Engine to enable students to practice coding in python. The system is inspired by codingbat by Nick Parlante and the public version of Beanshell running on appengine. I would like to extend our system to support Java. In our current system the Python doctest feature makes it very easy for instructors to type out a series of variable declarations and doctests which can then be executed against student-submitted code. It is very intuitive for the instructors which is important to ensure lots of people are writing tests. How do I replace the following comments with Java code to eval and execute tests in Java? String solution = “a=1;”; solution += “b=1;”; String problem = “self.assertEqual(a1);”; problem += “c=3;”; problem += “self.assertEqual(b2);”; //eval the solution string //eval the problem string possibly wrapped by additional junit or other Java code. //results = Run all the tests System.out.println(“Test expected received result”); //For result in results // print test expected received result(pass/fail) Desired output: Test expected received result self.assertEqual(a1) 1 1 pass self.assertEqual(b2) 2 1 fail Ideally any number of tests could be included in the problem string above and any number of Java statements could be included in the solution string in order to pass the tests included in the problem string. To the best of my knowledge you can't. Compiling Java code at runtime requires access to APIs that aren't available on App Engine; that's why things like BeanShell and LOTRepls don't support Java. I'm not familiar with beanshell sorry so I'm not the right person to ask. The only constraints that should matter are the class whitelist and the inability to run commands (such as javac). Thanks Nick. I can run setStrictJava(true) in beanshell in LOTRepls and ensure that the java syntax is obeyed. And it appears that I can also import static org.junit.Assert.*; Is there any architectural reason that I shouldn't be able to get Assert.assertEquals(xy); to run under the Java version of GAE? Not that I'm aware of. The issue is solely one of being able to compile Java source to bytecode on App Engine. If you're happy with a language like beanshell which doesn't require that you should be fine. What about setting setStrictJava(true) in beanshell? Do you think that would suffice to teach a good portion of Java syntax? Are there any GAE constraints that you are aware of?
650,A,"Unit testing nested subflows (subflows of subflows) I'm trying to write unit test for a flow which has subflow which itself has another subflow. I register first flow using FlowDefinitionResource getResource(FlowDefinitionResourceFactory resourceFactory). Then I register subflow definitions during test execution in FlowDefinitionRegistry before transitioning to them. Transitioning to ""first level"" subflow goes ok. The result of transitioning to subflow of current subflow - NoSuchFlowDefinitionException. The problem is that subflow definitions are all seem attached to the primary flow of the test and subflow can't be found within another subflow. Is there any way to attach subflow definition to another subflow in tests which extend AbstractXmlFlowExecutionTests? Does anybody know how to do this ? I'd very much like to find out as I'm having the same problem. I think this is possible but a little convoluted to get right. Have a look at this thread: Testing a flow with subflow doesn't work. The bottom line answer appears to be: override the configureFlowBuilderContext(MockFlowBuilderContext builderContext) virtual method and register your mock in there. This JIRA case for the Spring framework touches on this issue and also suggests it's possible but convoluted.  You should test every flow alone. So first time test top flow and mock first subflow then test only first subflow and mock second one and so on... http://static.springsource.org/spring-webflow/docs/2.3.x/reference/html/ch15s07.html Two years late but still could be useful for someone.:)"
651,A,"Run all tests in a source tree not a package My unit tests are in a separate directory tree from my integration tests but with the same package structure. My integration tests need external resources (e.g. a server) to be available but my unit tests are properly independent of each other and the environment. In IntelliJ-IDEA (v7) I have defined a JUnit Run/Debug Configuration to run all the tests in the top-level package and this of course picks up my integration tests which fail. I want to define a run-junit configuration that runs all my unit tests. Any ideas? The answer is to create a test suite that contains only those tests underneath the unit test folder and run that instead. There is a junit-addon which does just this called DirectorySuiteBuilder but I only found this after I had pretty much re-invented the wheel. And it's already been asked here! import junit.framework.JUnit4TestAdapter; import junit.framework.TestSuite; import java.io.File; import java.io.IOException; public class DirectoryTestSuite { static final String rootPath = ""proj\\src\\test\\java\\""; static final ClassLoader classLoader = DirectoryTestSuite.class.getClassLoader(); public static TestSuite suite() throws IOException ClassNotFoundException { final TestSuite testSuite = new TestSuite(); findTests(testSuite new File(rootPath)); return testSuite; } private static void findTests(final TestSuite testSuite final File folder) throws IOException ClassNotFoundException { for (final String fileName : folder.list()) { final File file = new File( folder.getPath() + ""/"" +fileName); if (file.isDirectory()) { findTests(testSuite file); } else if (isTest(file)) { addTest(testSuite file); } } } private static boolean isTest(final File f) { return f.isFile() && f.getName().endsWith(""Test.java""); } private static void addTest(final TestSuite testSuite final File f) throws ClassNotFoundException { final String className = makeClassName(f); final Class testClass = makeClass(className); testSuite.addTest(new JUnit4TestAdapter(testClass)); } private static Class makeClass(final String className) throws ClassNotFoundException { return (classLoader.loadClass(className)); } private static String makeClassName(final File f) { return f.getPath().replace(rootPath """").replace(""\\"" ""."").replace("".java"" """"); } }  Unfortunately there's no way to separate the output from the IntelliJ compile other than by classes and test classes within a single module (it's the classes that test runner is looking at). So when I have integration tests I simply use a second module specific to these tests to get round this problem specifying output directories as necessary for each module. Yes that would be a correct approach to use different modules for different types of tests. In the Run/Debug configuration you specify which module classpath will be used. I can't do this there are already multiple modules in the project and we're working on one module per deliverable artefact principle  IntelliJ IDEA CE 10.5 has a (new?) option to run all tests inside a configured directory:"
652,A,"Liferay Junit-Mockito testing I am trying to test my liferay portlet plugin code using JUNIT and Mockito. Currently I am mocking the service implementations to return mock data and test the functionalities. The problem I am facing is I need to test some code which takes properties as : PropsUtil.get(""someKey"") But when i run it as a standalone JUNIT test PropsUtil is not reading from any of the properties file. Is there any way I can make the test read from the liferay properties (portal*.properties) file without changing the source code ? Unless you're testing that values are actually set in portal.properties just call PropsUtil.set in your test. Hi I am using PropsUtil from portal-kernel.jar (and not from portal-impl.jar) so there is no setter method for setting the value manually.  you need to call InitUtil.init() which initializes the basic infrastructure properties including ... If you wanted to go further and boot up even the spring infrastructure you'd need to have liferay libraries on classpath. I'm explaining how to do that in maven environment in this blog post : how to use liferay third-party libraries in maven plugin SDK. If you do so then all you need to do is to setup spring.configs with portal spring xml definitions (infrastructure ones + those with spring services that you need to use) and call Init.initWithSpring(); that takes care of booting up liferay portal and it uses those spring beans that you mix up in spring.configs. You also would need to modify liferay properties a little. But it really depends on the use case.  You can also mock the call like this: mockStatic(PropsUtil.class); when( PropsUtil.get(PropsKeys.SOCIAL_ACTIVITY_COUNTER_PERIOD_LENGTH) ).thenReturn(""1""); Thanks for the code Julio. But i dont think mockStatic() is part of Mockito code.  I used the following method : My TestClass extends BaseServiceTestCase (available in liferay src) Keep portal-test.properties inside test folder (with the test values). Run the test case. In this case liferay loads all the properties as well as does the spring initializations.  As the last resort you could use PowerMock and mock PropsUtil.get() method call. Eventually it's a plain-old-java-singleton and code with singletons is not that easy to test.. Powermock enhances the EasyMock and Mockito APIs to allow mocking of static methods non-public code like private constructors/ variables/ methods final classes and methods. You may want to go easy on this since in some cases you probably should refactor your code instead of poking and mocking a dozen calls inside a meethod. Of course this allows you to not pollute your API by adding methods that exist only to allow unit testing."
653,A,"Autowire not working in junit test I'm sure I'm missing something simple. bar gets autowired in the junit test but why doesn't bar inside foo get autowired? @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration({""beans.xml""}) public class BarTest { @Autowired Object bar; @Test public void testBar() throws Exception { //this works assertEquals(""expected"" bar.someMethod()); //this doesn't work because the bar object inside foo isn't autowired? Foo foo = new Foo(); assertEquals(""expected"" foo.someMethodThatUsesBar()); } } What do you mean ""bar inside foo""? Foo isn't a managed spring bean you are instantiating it yourself. So Spring's not going to autowire any of its dependencies for you. heh. oh man I need sleep. that's so obvious. thanks!  You are just creating a new instance of Foo. That instance has no idea about the Spring dependency injection container. You have to autowire foo in your test: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration({""beans.xml""}) public class BarTest { @Autowired // By the way the by type autowire won't work properly here if you have // more instances of one type. If you named them in your Spring // configuration use @Resource instead @Resource(name = ""mybarobject"") Object bar; @Autowired Foo foo; @Test public void testBar() throws Exception { //this works assertEquals(""expected"" bar.someMethod()); //this doesn't work because the bar object inside foo isn't autowired? assertEquals(""expected"" foo.someMethodThatUsesBar()); } } makes perfect sense thanks much!"
654,A,"Running JUNIT Test Suites in a specifc order using ANT In the following example  1. <target name=""tests.unit""> 2. <junit> 3. <batchtest> 4. <fileset dir=""tsrc""> 5. <include name=""**/Test*.java""/> 6. <exclude name=""**/tests/*.java""/> 7. </fileset> 8. </batchtest> 9. </junit> 10. </target> 11. <target name=""tests.integration""> 12. <junit> 13. <batchtest> 14. <fileset dir=""tsrc""> 15. <include name=""**/tests/Test*.java""/> 16. </fileset> 17. </batchtest> 18. </junit> 19. </target> If i have several TestSuites in in the **/tests/ folder. How will it know which test suite to run first if i run the tests.integration target? If i have TestSuite1.java TestSuite2.java and TestSuite3.java i would like the test suites to run in the order as specified in teh filename. You could create a base test class put the login in its setUp() and inherit all your test cases from this one (and of course call super.setUp() everywhere). Note that this would only be a simple login not a proper unit test. You should unit test your login functionality with all possible crazy user input and whatnot in a separate test class but for the rest of the test cases you only need a plain simple login with a default username or something. For those test cases where on top of login you need a product as well you create a second base test class which extends the first and adds product creation to its setUp(). No code duplication - if login changes apart from the login test cases themselves you need to change a single method in your test code. It will probably be slower to execute 5000 unit tests this way - but much much safer. If you start to depend on the order of execution of your unit tests you're stepping on a slippery slope. It is very difficult to notice that you have inadvertently introduced an extra dependency between two unit tests if their order is fixed by your configuration. E.g. you set up a specific property of a product or change a global configuration setting in one unit test then you test something else on your product in the next test case - and it happens to work only because the previous unit test set things up that specific way. This leads to nasty suprises sooner or later.  Unless there are new features in JUnit this is a difficult thing to do. TestNG can manage it with dependent groups. Here's a question for you: Why does the order matter? Unit tests should not have dependencies. If you're doing that perhaps these are really integration tests. FitNesse might be a better way to go.  Yes i am trying to create a test suite for a functional test not unit tests. Im trying to use junit to build the functional tests package. I am using selenium which is based on Junit. Lets say i have a website where you cant do anything without logging on. In this case i have a test case that tests the logging on functionality and then i would have another test case that would test something else. The order they will be executed will matter because i cant test anything before logging on which means the order should be TestLogin TestCreateProduct TestReadProduct in the above test cases i cant read any product before it is created and that i have logged and i cant create a product before i have logged on. I have seen a lot of comments about using the setUp() and tearDown() methods but surely that would mean a lot of duplication. If for example i have to make TestReadProduct test case independent i would have to put the TestLogin and TestCreateproduct functionality in the setUp() method for the TestCreateProduct test case. Surely this is a maintenance nightmare. Imagine having to maintain 5000 of testcases. I would have to make a lot of changes in a lot of places if the TestLogin functionality changes. I am thinking of using the ""depends"" option in ANT. Something like this <target=TestReadProduct depends=TestLogin TestCreateProduct> isnt there a better way of doing this?"
655,A,"Can I make JUnit more verbose? I'd like to have it yell hooray whenever an assert statement succeeds or at the very least have it display the number of successful assert statements that were encountered. I'm using JUnit4. Any suggestions? Why exactly do you want that? I always try to follow the rule of silence (http://www.catb.org/~esr/writings/taoup/html/ch01s06.html#id2878450) that's why I'm asking It might be a useful way to see if someone is ""tweaking"" the test cases to make them pass by removing unconfortable assertions... Because it gives me the warm fuzzies when that number legitimately goes up. :) I don't think it's the goal of JUnit to count matched assertions or print out more verbose information. If tests are atomic you'll get most information in there. So I would review my tests. You're also able to establish a LogFile in JUnit. It's possible but it will decrease test execution performance...  junit's javadoc unfortunately says that only failed assertions are recorded (http://junit.sourceforge.net/javadoc_40/index.html) so it seems it would not be possible  I'm pretty sure you can create a custom TestRunner that does that. We ended up with something similar in our homemade Unit-testing framework (a clone of NUnit). Oh wait - now that I'm reading your question again if you really want output for each successful assertion you'll have to dig into the plumbing more. The TestRunner only gets called once for each testcase start/end so it'll count passed and failed tests not assertions. This isn't much of a problem for me since I tend towards one assertion per test generally.  Can you consider ? 1) download junit source 2) to modify the class org.junit.Assert to do whatever modifications you're looking for  Hard to be done. All assert methods are static members of the class Assert which implies that the RunNotifier (which counts the successful and failed tests) is not within reach. If you dont refrain from an ungly hack: take the sources from JUnit patch them to store the current notifier in a static field of Assert when running tests such that the static methods can report successful asserts to this notifier.  Adding some info that would have been helpful to me when I wanted JUnit to be more verbose and stumbled on this question. Maybe it will help other testers in the future. If you are running JUnit from Ant and want to see what tests are being run you can add the following to your task: <junit showoutput=""true"" printsummary=""on"" enabletestlistenerevents=""true"" fork=""@{fork}"" forkmode=""once"" haltonfailure=""no"" timeout=""1800000""> Note that showoutput printsummary and enabletestlistenerevents are what helped not the other task attributes. If you set these you'll get output like: Running com.foo.bar.MyTest junit.framework.TestListener: tests to run: 2 junit.framework.TestListener: startTest(myTestOne) junit.framework.TestListener: endTest(myTestOne) junit.framework.TestListener: startTest(myTestTwo) junit.framework.TestListener: endTest(myTestTwo) Tests run: 2 Failures: 0 Errors: 0 Time elapsed: 0.495 sec This was useful to me when my tests were timing out and I wasn't sure which tests were actually taking too long and which tests got cancelled because they were unlucky enough to be running when the time was up.  Are you really interested in an assertion that succeeds? Normally the only interesting assertions are ones that fail. Being a fervent JUnit devotee myself I try and make the output of the tests as quiet as possible because it improves the signal-to-noise ratio when something doesn't pass. The best test run is one where everything passes and there's not a peep from stdout. You could always work on your unit test until it succeeds and run ""grep Assert test.java | wc -l"". :-) Yup I really am.  You can use AOP (with Spring or AspectJ) define pointcuts on all assert methods in junit.framework.Assert class. Using spring you can implement your own class as after returning advice (http://static.springframework.org/spring/docs/2.5.x/reference/aop.html#aop-advice-after-returning) which will only be called if the assert method passed (otherwise it throws an exception: junit.framework.AssertionFailedError ). In you own class you can implement a simple counter and print it at the end.  If you want to see some output for each successful assertion another simple approach which requires no external dependencies or source code would be to define your own Assert class which delegates all methods to the standard JUnit Assert class as well as logging successful assertions (failed assertions will be reported as usual by the JUnit class). You then run a global search-and-replace on your test classes from ""org.junit.Assert"" => ""com.myco.test.Assert"" which should fix-up all regular and static import statements. You could also then easily migrate your approach to the quieter-is-better-camp and change the wrapper class to just report the total # of passed assertions per test or per class etc."
656,A,"how to export (JUnit) test suite as executable jar Is there a way in eclipse (Helios) to package/export my JUnit test suites (or maybe even test cases if possible) as executable jars? I know how to generate runnable jars from projects with a main class but i'm clueless about how to include a TestRunner. Is there a straightforward way or do I have to make a workaround main class calling the TestRunner somehow? Details would be great. You are correct that a main() method is needed for an executable jar. It's easy to add a main method to your test suite though. public static void main(String[] args) throws Exception { JUnitCore.main( ""com.stackoverflow.MyTestSuite""); } cool that one worked fine. Thank you Jeanne"
657,A,"Data-driven tests with jUnit What do you use for writing data-driven tests in jUnit? (My definition of) a data-driven test is a test that reads data from some external source (file database ...) executes one test per line/file/whatever and displays the results in a test runner as if you had separate tests - the result of each run is displayed separately not in one huge aggregate. In JUnit4 you can use the Parameterized testrunner to do data driven tests. It's not terribly well documented but the basic idea is to create a static method (annotated with @Parameters) that returns a Collection of Object arrays. Each of these arrays are used as the arguments for the test class constructor and then the usual test methods can be run using fields set in the constructor. You can write code to read and parse an external text file in the @Parameters method (or get data from another external source) and then you'd be able to add new tests by editing this file without recompiling the tests.  We currently have a props file with our ID numbers in it. This is horribly brittle but is easy to get something going. Our plan is to initially have these ID numbers overridable by -D properties in our ant builds. Our environment uses a legacy DB with horribly intertwined data that is not loadable before a run (e.g. by dbUnit). Eventually we would like to get to where a unit test would query the DB to find an ID with the property under test then use that ID in the unit test. It would be slow and is more properly called integration testing not ""unit testing"" but we would be testing against real data to avoid the situation where our app runs perfectly against test data but fails with real data.  Even though this is quite an old topic i still thought of contributing my share. I feel JUnit's support for data driven testing is to less and too unfriendly. for eg. in order to use parameterized we need to write our constructor. With Theories runner we do not have control over the set of test data that is passed to the test method. There are more drawbacks as identified in this blog post series: http://www.kumaranuj.com/2012/08/junits-parameterized-runner-and-data.html There is now a comprehensive solution coming along pretty nicely in the form of EasyTest which is a a framework extended out of JUnit and is meant to give a lot of functionality to its users. Its primary focus is to perform Data Driven Testing using JUnit although you are not required to actually depend on JUnit anymore. Here is the github project for refernece: https://github.com/anujgandharv/easytest If anyone is interested in contributing their thoughts/code/suggestions then this is the time. You can simply go to the github repository and create issues.  Typically data driven tests use a small testable component to handle the data. (File reading object or mock objects) For databases and resources outside of the application mocks are used to similate other systems. (Web services and databases etc). Typically I see is that there are external data files that handle the data and the output. This way the data file can be added to the VCS.  I'm with @DroidIn.net that is exactly what I am doing however to answer your question literally ""and displays the results in a test runner as if you had separate tests"" you have to look at the JUnit4 Parameterized runner. DBUnit doesn't do that. If you have to do a lot of this honestly TestNG is more flexible but you can absolutely get it done in JUnit. You can also look at the JUnit Theories runner but my recollection is that it isn't great for data driven datasets which kind of makes sense because JUnit isn't about working with large amounts of external data.  I use combination of dbUnit jMock and jUnit 4. Then you can ether run it as suite or separately Untils is a nice framework for putting it all together. You mean Unitils (http://www.unitils.org/)? It looks interesting indeed thanks for the tip!  If you want to make your JUnit tests data driven simply use @DataLoader annotation provided by EasyTest framework like this : @RunWith(DataDrivenTestRunner.class) @DataLoader(filePaths={testData.xml}  loaderType=LoaderType.XML) public class TestClass{ @Test public void simplTestMethod(@Param(name=""name"")String name  @Param(name=""age"")int age  @Param(name=""expectedOutput"")int expectedOutput){ ...............//your test conditions here } } This will then load your test data from an XML file and provide that test data to your test method. Thus in short three things that you have to do in order to make your JUnit tests data driven is : 1) Use DataDrivenTestRunner class in @RunWith annotation 2) Use @DataLoader annotation to load and provide your test data to the test method. 3) Use @Param annotation on your test method. You can find the details of how and what easytest supports here : https://github.com/EaseTech/easytest/wiki Enjoy Data Driven Testing with JUnit  Some tests will lend themselves to being interface driven. If the database/file reads are retrieved by an interface call then simply get your unit test to implement the interface and the unit test class can return whatever data you want.  This is where TestNG with its @DataSource shines. That's one reason why I prefer it to JUnit; the others are dependencies and parallel threaded tests. +1 that's exactly what I'd have written :-)  I use an in-memory database such as hsqldb so that I can either pre-populate the database with a ""production-style"" set of data or I can start with an empty hsqldb database and populate it with rows that I need to perform my testing. On top of that I will write my tests using JUnit and Mockito. Not sure why you were down voted on this answer. It is certainly a valid approach. This can be a very useful technique +1 - useful indeed.  You are better off extending TestCase with a ""DataDrivenTestCase"" that suits your needs. Here is working example: http://mrlalonde.blogspot.ca/2012/08/data-driven-tests-with-junit.html Unlike parameterized tests it allows for nicely named test cases. While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes. @Mathieu. this is hot stuff. test-frameworks come and go but this is rock solid also with plain old junit. Thank you so much!"
658,A,"How do I halt Maven when JUnit Fails? I am running Junit tests from Maven. The ant script has <junit failureproperty=""failproperty"" errorproperty=""errorproperty""> <classpath refid=""classpath"" /> <test name=""${unit-test-suite}"" /> <formatter type=""brief"" usefile=""false"" /> </junit> <echo> ----------> ${failproperty} </echo> <echo> ----------> ${errorproperty} </echo> <fail message=""something wrong"" if=""${failureproperty}""/> <fail message=""something wrong"" if=""${errorproperty}""/> I tried Halting on error or failure in Junit - This halts when I run the ant script only but the build succeeds in Maven and does not halt even thought JUnit has an error. I tried to send a fail message by setting the errorproperty and the failure property- This does not set the variables. How do I tell Maven to stop everything when Junit fails? Maven Details: Maven version: 2.0.10 Java version: 1.6.0_17 OS name: ""windows xp"" version: ""5.1"" arch: ""x86"" Family: ""windows"" Are you sure you're setting the properties correctly? Note that the if and unless attributes take property names not expressions. For example try using: <fail if=""failproperty""/> instead of <fail if=""${failproperty}""/> The <fail/> tag works in this simple example. Try mvn package and then mvn package -Dfailproperty=true <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""> <modelVersion>4.0.0</modelVersion> <groupId>ant-test</groupId> <artifactId>ant-test</artifactId> <version>1.0-SNAPSHOT</version> <packaging>jar</packaging> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.7</version> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <executions> <execution> <id>ant-test</id> <phase>package</phase> <configuration> <tasks> <junit failureproperty=""fail""> <classpath> <path refid=""maven.plugin.classpath""/> <path refid=""maven.test.classpath""/> </classpath> <formatter type=""plain"" /> <batchtest> <fileset dir=""src/test/java""/> </batchtest> </junit> <fail if=""fail""/> </tasks> </configuration> <goals> <goal>run</goal> </goals> </execution> </executions> <dependencies> <dependency> <groupId>org.apache.ant</groupId> <artifactId>ant-junit</artifactId> <version>1.7.1</version> </dependency> </dependencies> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <source>1.6</source> <target>1.6</target> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <skipTests>true</skipTests> </configuration> </plugin> </plugins> </build> </project> Assuming a Test case in src/test/java: import junit.framework.Assert; import org.junit.Test; public class FakeTestCase { @Test public void testThis() { Assert.fail(""Failure""); } } if I pass in the arguments like you do then yes it works but the fail property does not get set if I run from maven even when i get a JUnit Error. Try it in your project It works. See the updated project  This ""fail fast"" feature isn't currently supported by Maven 2 see SUREFIRE-580 (and maybe vote for it)."
659,A,multiple test classes for an interface rolled up into a suite in junit 4? I have an interface that defines the behaviour of an object that does some graph search called say GraphSearcher. I currently have a bunch of different implementations with more variations on the way and so I have defined the test classes as being abstract with concrete test methods and an abstract method that instantiates an implementation of graph searching object. This is all fine except that I have a bunch of different classes (about 10 at the moment) that each run whole lot of tests based on some real world data. This means that for each concreate implementation I end up writing a whole lot test classes just containing the instantiation method. This all seemed kind of messy to me and I was wondering if anyone knew of a better solution. I think ideally I would like to be able to pass in a GraphSearcher factory object into some sort of test suite and have all the various test classes run. This would mean that I would only have to write a tiny bit of code to have a new GraphSearcher implementation run against all the tests. I'm just not sure how to go about this using junit 4. I'm sure there must be some fairly standard way to do this but I haven't been able to find anything yet. Any pointers anyone has would be greatly appreciated If I understand your problem well you just have some testing code and would like to run it multiple times each time with a different implementation of GraphSearcher right? If so I would probably go with a Paremeterized JUnit runner. It could look like this: @RunWith(Parameterized.class) public class GraphSearcherTest { private GraphSearcher testedSearcher; public GraphSearcherTest(GraphSearcher searcher) { this.testedSearcher = searcher; } @Parameters public static Collection<Object[]> getParameters() { return Arrays.asList(new Object[][] { { new GraphSearcherImpl1() } { new GraphSearcherImpl2() } }); } @Test public void testGraphSearcher() { // execute the test testedSearcher.search(); // make some assertions } } The key parts are: the @RunWith(Parameterized.class) annotation the constructor which takes the tested implementation of GraphSearcher the @Parameters-annotated method (of arbitrary name) which returns Collection. This method will be called for every item of the collection and the object from the collection will be added to the test's constructor. the size of the Object[] in the collection must match the number of arguments of the constructor So in this case the test will be called twice. First test will get the GraphSearcherImpl1 instance and the second the GraphSearcherImpl2 instance in its constructor. That has worked perfectly and thank you so much for your quick reply I really appreciate it You are very welcome :)  You could use the @Parameters annotation to provide you the different interface implementations. For instance: http://www.mkyong.com/unittest/junit-4-tutorial-6-parameterized-test/ thanks also Guillaume!
660,A,"OpenJPA Jersey JUnit: Using a different persistence unit for my unit tests? I'm building an application using OpenJPA 2.0.0 Jersey 1.3 and JUnit 4.8.1. I've set it up so I have two different persistence units defined in my persistence.xml: ""default"" and ""unittest."" Default is set up to connect to an Oracle instance while unittest is set up to connect to a local H2DB embedded database file. I do this so that I always start my unit tests with a clean database which has certain known data in it which can be overwritten between each run of the unit tests. The problem is that now I want to use the Jersey Testing Framework to test my actual webservices rather than just the lower layers of the program. My root resource classes don't load the unittest persistence unit they always load the default. So what I probably need to do is to inject into the root resource classes which persistence unit should be used to instantiate the EntityManager and then have some way to inject unittest when I'm running my tests but inject default otherwise. But I can't think of any good way to do that. I'm brand new to Java EE development which might be obvious. Any help? Where is the EntityManager or rather the EntityManagerFactory coming from? Are you creating it by calling Persistence.createEntityManagerFactory() or is it being injected by some framework? You don't mention any kind of framework in your question. Are you using Maven ? If yes there is a trick that allows you to ""read"" a POM value in your Java class. It was very useful for me you would just have to set your Persistence Unit Name in your POM a different one during the test and that's it. Maven variables in java context  I think it should be possible to create a second persistence.xml in src/test/resources/META-INF. If I understand things correctly maven will put target/test-classes in the classpath ahead of target/classes so that in a unit test only your test persistence.xml will be read. That way you can use the same persistence unit name but with a different configuration. src/main/resources/ META-INF persistence.xml <-- for actual application src/test/resources/ META-INF persistence.xml <-- for unit tests EDIT: oops I was tricked into believing you used maven by the other answer. If you are not forget what I said..."
661,A,"running junits and cobertura with maven In our project we run both junits and cobertura using maven. The problem I am facing is that junit test cases are running twice once before the jar creation process and then once again for generating cobertura coverage reports. When running cobertura and junits with ant we run junits only once since cobertura runs along with junits. Is there a way to configure the above case with maven. I know we can use ""maven.test.skip"" property to skip junits. But when I do this I am not able to see junit xml & html file reports. Also in maven how to configure junits to run in batch or parallel ? thank you! your second question is really a separate issue cna seems to be reposted here: http://stackoverflow.com/questions/423627/running-junit-tests-in-parallel Did you ever find a real solution to this? I think this is a bug and I have outlined a solution here: http://nayidisha.com/techblog/an-over-eager-code-coverage-tool  This happens because the reporting execution requires the test execution so it can create the reports. If there were a ""site-only"" goal on the site plugin that didn't have the @requiresDependencyResolution test annotation it could be bound to the project's prepare-package phase and your reports would be generated without the tests running twice. Unfortunately there seems to currently be no such goal (see my question on the subject). See my answer to the question for details of a workaround.  While I cannot find the exact page anymore I recently read a discussion of why running the tests twice is considered a good idea. The key issues cited were around the effects of the Cobertura byte code alteration on the accuracy of your tests. In certain cases the timing of your code execution might be important the byte code alteration can cause tests that fail in JUnit to pass when run only in Cobertura and vice versa. For this reason it was recommended that the tests be allowed to execute twice. Most of the examples cited were around multi-threaded behaviors but I imagine that there could be other cases were the byte code alteration can cause issues in your tests. Having the tests execute both ways provides you with baseline results and also reduces the chances of sending you on a wild goose chase if Cobertura is in fact altering test success. This is a great point. Another thing I ran into one time is that because of the byte-code re-writing if your code uses reflection a lot it can cause issues. For example if you have a utility that extracts fields from a class and your unit test asserts that an example class has three fields this test will actually fail when you run it with code coverage. Not to mention the fact that tests without instrumentation run a lot faster. @DavidValeri: Could you please provide a link to the discussion thread you refer (just edit our post)? The fact that Cobertura alters the bytecode can be referenced on their website http://cobertura.sourceforge.net/introduction.html .  Try adding cobertura as a compile scope reference. And post the relevant parts of your pom. Wouldn't having as a dependency (with compile scope) only be helpful if your project is somehow trying to build something on top of cobertura? This is probably not what Ajay was looking for."
662,A,Include Unit tests in the same package as the source code in Java I'm getting back into Java after a long stint in the Ruby world and I've got a question about JUnit tests and the source I'm testing. If I've got a package of graphics code for my company lets call it com.example.graphics should I include my tests in that package too or should they be included in a seperate package like com.example.graphics.test? See also http://stackoverflow.com/questions/440786/junit-java-testing-non-public-methods I personally keep all my tests in the same package but in the test tree of Maven(which I always use for Java projects). Consider using Maven for your builds as well - it saves a lot of work on your part. It enforces a similar structure to what bkail mentions but gives you much more than a standardized project folder layout - project lifecycle(clean compile package test) plugins etc .  Putting tests in a separate package does tend to avoid the problem of accidentally having some method or class at package-scope that you intended clients to use.  If you do not need to access private classes it is actually a question of flavor. I sometimes even tend to create an additional test-project referenced to the product-project. Therefore product and test are clearly seperated.  In the same java package is fine. It's actually necessary if you need to access package-private classes methods or fields. However the source should be logically separate: src/main/com/example/graphics src/test/com/example/graphics +1 exactly what i would say. +1 as suggested by the Maven Java Archetype standard directory layout Cool much thanks man.
663,A,"SVN via Eclipse - Cannot run the code I've checked out I'm using SVN in Eclipse. I checked out a folder as a project in the workspace. The folder I checked out has a subfolder ""trunk"" and in the subfolder ""trunk"" I have my src folder (and other stuff). After checking out I tried to run a file (a JUnit test case): I right clicked on it and selected ""run as"" expected to find ""run as a JUnit test case"" but there wasn't such an option. Also it looks like Eclipse doesn't compile my code: I can edit the code but don't get any autocompletions or syntax error notifications. What have I done wrong? HH is correct - just want to add: You'll want to checkout the trunk not the directory above it. The .project file will name the local checkout appropriately and this way you won't check out any branches or tags (this could be a lot of data) @thekbb: Thanks I actually checked out the directory above the trunk. Please write this in an answer and I'll accept. Most probably Eclipse was not able to figure out in this new project: where is the source folder where are the test source files etc... This is due to a missing or corrupted project file which would normally have metadata about the project (it wasn't committed or you didn't check it out). For example in order to get autocompletion and syntax error notifications Eclipse must recognize your Java files as source files. In the project properties->Java Build Path->Source make sure your source folder is present.  Ahh not sure why this took me a while! Checking out a directory above the trunk is where things got wonky - eclipse couldn't find the project file - and as HH pointed out didn't know where the source was."
664,A,Run tests from inner classes via Maven I have following tests structure: public class WorkerServiceTest { public class RaiseErrorTest extends AbstractDbUnitTest{ @Test public void testSomething(){ } ... } ... } It's done because I don't want to create a separate class file for every test case extending AbstractDbUnitTest. The problem is that mvn test doesn't run test from my inner class. Is there is a way how to configure Maven to run such tests? Of course I can create methods in the parent class calling the inner class methods but I want a clearer way. Yes this is possible using the new (well it's not new anymore) Enclosed runner (since JUnit 4.5) that runs all static inner classes of an outer class. To use it just annotate the outer class with @RunWith(Enclosed.class) and make the inner classes static. @RunWith(Enclosed.class) public class WorkerServiceTest { public static class RaiseErrorTest extends AbstractDbUnitTest{ @Test public void testSomething(){ } ... } ... } And mvn test will run them. I'm trying this with maven 3.0.4 and Junit 4.11 and still doesn't work. From Eclipse works just fine but not with `mvn test`. Any ideas?
665,A,"Netbeans trunks (pre-7.0): JUnit bundling removed - no plugin? Since 2011-02-06 trunk build of Netbeans (probably earlier) JUnit is removed as it was proposed. Sadly there seem to be still no plugin for JUnit from Tools->Plugins->Availible... which would act as a drop-in replacement. Perhaps no post-installable drop-in replacement is even planned. Sure I could define a new custom Netbeans library and install JUnit manually (which is what I might do when there is no better solution). But I asking you guys: does anyone know of a better solution? Currently there doesn't seem to be a better solution. I hope they will solve problems with the licence soon and final version will be with jUnit. There's also the item ""Tools > Create JUnit Tests"" now missing  In more recent trunk builds of Netbeans (as of 2011-03-31) installation of JUnit plugin is offered separated during installation."
666,A,"Is there a multi-process unit testing framework / junit addon? Imagine two servers (each within an own jvm process) which communicate using some form of messages (e.g. simple producer/consumer) I'd like to write unit tests that will test the behaviour of these two servers. My questions are: Are there some frameworks (or a junit addon) for this problem? I'd like to run a junit test class (or even a single test) in a different process? It would be cool if one unit test can easily access the variables of the other test (in a different process) using some sort of inter-process communication. E.g. I'd like to check if the producer really produced the things i expected and afterwards if the consumer consumed them. (and maybe do some fuzzing to detect some race-condition-related problems) Are there some best practice for this kind of testing? If there isn't a unit-like testing approach how would you test such things during continous integration? Note: I don't want to use threads because this may alter the behaviour (if you think of thread-safety issues) What you are talking in point 1 is not a true unit testing scenario. In a perfect unit test you will not worry about who is producing the messages and who is consuming the messages. One of your test case will concentrate on imagining(or mocking) that you received different messages from a valid producer and your test cases will test how these are being consumed properly. What you are looking is more like a integration testing. Anyway some of the statements which I made may be subjective. You can use jMock easymock frameworks for your mocking needs.  I'd recommend using mock objects you can then test the producer and the consumer independently mocking out the other. So you test the producer produces and the consumer consumes. You can also test the mechanism the two communicate but I'd expect thats provided by a 3rd party? See Mockito jMock EasyMock  This is way beyond Unit testing and firmly inside Integration testing. I would recommend to test what you can by mocking out your comms layer with each piece separate. What I did in the past in such cases is to start a separate thread in the test which starts a Mock Receiver/Sender. In the main test I then do the Sender/Receiver part. In practice this means this is full of delays to make sure things start in the right order and this becomes dead slow so you just want to do this to test that the pieces of the puzzle fit and do as little as possible functional testing over it. I verify the desired behavior and terminate the helper thread before leaving the test. This can test a lot. Testing using separate processes (and hosts!) is a real pain. takes forever and I would limit that to manual testing.  I would describe what you are looking to do as an integration test and beyond the scope of JUnit as a framework (JUnit is only now dipping its toe into multi-thread testing multi-process is certainly not in the feature set). You could of course use JUnit to run such tests but they would really be limited to a runner the rest you have to do yourself. Others have already pointed to the fact that you would usually mock or otherwise send artificially constructed methods to the consumer and test what the producer produces independently at the ""unit test"" level. In terms of actually testing the interaction between the producer and the consumer one approach is to forget about the inter-process testing and test it intra-process on one thread via some sort of Dependency Injection where the producer sends the message via some fake way that just passes it on the the consumer without anything more under the hood than in-thread method calls. However it seems that you want to test things that can happen with the actual inter-process stuff on top of it (race conditions and the like) which makes this all-the-more an integration test. To approach this problem you need to start the process and wait for it to accept a message then your test would tell the producer what message to create and it would send it to the consumer. Then your test would ask the consumer what it got (with suitable delay). The need here would have to be compelling for me to do it. I would go for full blown automated acceptance testing and let that encompass this level of integration.  You should take a look at XHarness which I believe meets your needs. I've used it in the recent past and it worked great for our projects at the time. From its page: ""XHarness is a system test harness for Apache Ant. It allows developers to describe their product/system tests in the form of XML as part of an ant build file describing the tasks and processes that comprise the test cases and what their expected behaviour is. It has powerful process management capabilities that extend the existing Ant process tasks (exec java) to allow asynchronous execution of java and native processes (e.g. for client-server tests) synchronization between multiple processes and complex assertions on process and task output (stdout/stderr file output coupled with timeouts etc.). """
667,A,Does anyone know how to select parts of test methods in a test case run in a test suite? For example a test case named ExampleTest ExampleTest { testA{}; testB{}; testC(); } I could run all this class with TestSuite.addTestSuite(ExampleTest.class); but how to select testA and testB run into TestSuite? See also http://stackoverflow.com/questions/1368915/exclude-individual-junit-test-methods-without-modifying-the-test-class Can't you just mark testc to be ignored? 2 Ideas: See if it makes sense to split ExampleTest into 2 test classes based on your partition Otherwise use Categories to tag your tests and then run tests which belong to a specific category. Its simpler in NUnit JUnit seems to require you to create an empty/ marker interface to get this done. A sample link with code. +1 for categories but it looks as if question is for JUnit 3
668,A,"Generating Unit Tests Automatically I have a web tool which when queried returns generated Java classes based upon the arguments in the URL. The classes we retrieve from the webserver change daily and we need to ensure that they still can process known inputs. Note these classes do not test the webserver they run locally and transform xml into a custom format. I am not testing the webserver. These classes must then be placed in specific package structure compiled and run against a known set of input data and compared against known output data. I would like to do this automatically each night to make sure that the generated classes are correct. What is the best way to achieve this? Specifically whats the best way to: retrieve the code from a webserver and place it in a file compile the code and then call it I'm sure a mix of junit and ant will be able to achieve this but is there and standard solution / approach for this? Similar question was asked 18 mins before yours: http://stackoverflow.com/questions/2131935/automatic-generation-of-unit-tests-for-java You may want to track it as well :) *Update*: you both are from London.. Colleagues? :o Since this one gives a lot more detail how about we vote to close the other one as a dup of this one? Hahaha no we don't work together. London is a pretty big place. Thanks for your help I'm not sure that they are duplicates of each other… the other question is asking how to automatically generate unit tests of java code this is asking how to test some random code against known good inputs/outputs. I see this as a different problem. You should be creating a ""mock"" interface for your web service that (a) behaves the same way and (b) returns a known answer. You should then do some other integration testing with the live web service where a person looks at the results and decides if they worked. Hi I just clarified the question this is not for testing the webservice I just retrieve classes from it. I hate mocks. It's usually twice the work of creating the web service in the first place and you end up spending all your debugging time trying to find out if the bug is in the web service or the mock. @Paul Tomblin: Right and Wrong. It's supposed to be more work. If your mock requires a lot of debugging then you didn't understand the thing you were mocking well enough. If the web service is poorly defined then that's par for the course. Debugging the mock is usually simpler than debugging the application so it evens out in the long run. More up-front time to build the mock. Less ongoing to debug.  First up to answer your question: No I do not think that there is a standard approach for this. This sounds like quite an unusual situation ;-) Given that what I would do is to write your JUnit tests to all call a class GeneratedCode and then once you download the code rename the class to GeneratedCode compile and run your unit tests.  You have the same goal as continuous integration ;-) Maybe a bit overkill for this simple task but this is the standard way to get something compile something and test something regularly. E.g. you could try hudson. To me the task is more build automation then continuous integration. hudson is a good advice but I'd use maven (or ant) first and use hudson to execute the build scripts (end enjoy the great web based reports :) ) What does ""sth"" mean?  Can you only test the generated classes after they were published on the webservice ? You have no way to test during or just after the generation ? One idea if the generated code isn't to complex is to load it via the GroovyClassLoader and to run your tests against it. See this page for examples."
669,A,"Besides ""all of them"" as an answer what type of applications (be specific) have had true success with unit testing? What type of applications did you use this TDD unit testing approaching (web app compiler rails etc?). What language was your application written in? Did you use a preexisting unit testing frameworking like junit or nunit or did you you rollo your own. Did you use automatically code generate your test cases or was most of the work through manual creation? Did you integrate your unit tests with an automatic build program like cruise control or something similar? Would you consider your tests very fine grained low level or were they very high level tests (""Test the Entire Page""). This should be a wiki Rest assured ""all of them"" is not the answer. This isn't a question with a ""right"" answer. I have unit tested Rails web apps and standalone Ruby apps with Rspec and Test:Unit. Real tests cannot be generated perhaps stubbed out at best. I did use cruisecontrol but found it a bit cumbersome to use as an actual integration tool. It is useful as a badge of pride between devs ensuring vigilance in passing the test suite before you commit your code. My unit tests tend to be pretty low level as far as I know functional tests are for high level stuff. And yes I have never regretted writing unit tests for my code.  In general I would say applications where unit testing was done from an early stage of the process. Bolting it on to an existing zillion line application is not nearly as effective as doing it from the outset. It also helps if there's good support in the language. Java and .NET both have excellent XUnit frameworks -- C++ has a bit more of a hard time.  All of them. (Sorry this just has to be said) Kryalessa: yes they have.  #67299 #286587 #301693"
670,A,Runinng SQL script from JUnit In the setup method to a JUnit test case I'm working on I need it to run a sql script on my database before each test case and then a rollback afterwards. I have tried using a tokenizer which added each SQL command to a batch and then executing them. But I can't get working. So my question is if there is some standard method in JUnit to perform this action? It's not the task of JUnit to test SQL statements. You should create a Mocker (EasyMock e.g.) and isolate the connection. So the mocker can imitate the sql connection and its results. With this mocking object you can check if your sql connector class called the right statements. If you want to test the SQL statement its results and so on you should use DBUnit as Aaron said.  You can try DbUnit DbUnit is a JUnit extension (also usable with Ant) targeted at database-driven projects that among other things puts your database into a known state between test runs. This is an excellent way to avoid the myriad of problems that can occur when one test case corrupts the database and causes subsequent tests to fail or exacerbate the damage. DbUnit has the ability to export and import your database data to and from XML datasets. Since version 2.0 DbUnit can also work with very large datasets when used in streaming mode. DbUnit can also help you to verify that your database data match an expected set of values.
671,A,"JUnit for Functions with Void Return Values I've been working on a Java application where I have to use JUnit for testing. I am learning it as I go. So far I find it to be useful especially when used in conjunction with the Eclipse JUnit plugin. After playing around a bit I developed a consistent method for building my unit tests for functions with no return values. I wanted to share it here and ask others to comment. Do you have any suggested improvements or alternative ways to accomplish the same goal? Common Return Values First there's an enumeration which is used to store values representing test outcomes. public enum UnitTestReturnValues { noException unexpectedException // etc... } Generalized Test Let's say a unit test is being written for: public class SomeClass { public void targetFunction (int x int y) { // ... } } The JUnit test class would be created: import junit.framework.TestCase; public class TestSomeClass extends TestCase { // ... } Within this class I create a function which is used for every call to the target function being tested. It catches all exceptions and returns a message based on the outcome. For example: public class TestSomeClass extends TestCase { private UnitTestReturnValues callTargetFunction (int x int y) { UnitTestReturnValues outcome = UnitTestReturnValues.noException; SomeClass testObj = new SomeClass (); try { testObj.targetFunction (x y); } catch (Exception e) { UnitTestReturnValues.unexpectedException; } return outcome; } } JUnit Tests Functions called by JUnit begin with a lowercase ""test"" in the function name and they fail at the first failed assertion. To run multiple tests on the targetFunction above it would be written as: public class TestSomeClass extends TestCase { public void testTargetFunctionNegatives () { assertEquals ( callTargetFunction (-1 -1) UnitTestReturnValues.noException); } public void testTargetFunctionZeros () { assertEquals ( callTargetFunction (0 0) UnitTestReturnValues.noException); } // and so on... } Please let me know if you have any suggestions or improvements. Keep in mind that I am in the process of learning how to use JUnit so I'm sure there are existing tools available that might make this process easier. Thanks! Void functions generally have side effects so can you test those? please correct me if I am wrong. As I understood from the provided code you're only checking if there may be an exception while executing the function. But you're actually not verifying if the called functions ""works"" correctly unless the only way to end in case of an error would be an exception. I suggest writing additional tests like this: public void testTargetFunctionSomeValue() { int someValue = 0; callTargetFunction(someValue someValue); assertTrue(verifyTargetFunction(someValue someValue)); } public boolean verifyTargetFucntion(int someValue int someValue) { // verify that execution of targetFunction made expected changes. . . . . . } and the verifyTargetFunction would acutally check if calling targetFunction would have made the expected changes - let's say to a database table by returning true or false. Hope that helps. Cheers Markus  Looks like you reimplemented most of JUnit :) In general you don't need to do it. You just call the function you want to call and compare results. If it throws an exception JUnit will catch if for you and fail the test. If you expect an exception either you can use the explicit annotation if you are using JUnit 4 or you can use the following pattern: public void testThrows() { try { obj.DoSth(); //this should throw MyException assertFail(""Expected exception""); } catch (MyException e) { //assert the message etc } } again if obj.DoSth() throws a different exception JUnit will fail the test. So to sum up I am afraid I believe your approach is overcomplicated sorry.  If you have the possibility you should upgrade to JUnit 4.x. Then your first example can be rewritten to: @Test(expected=RuntimeException.class) public void testTargetFunction() { testObj.targetFunction (x y); } The advantage here is that you can remove you the private UnitTestReturnValues callTargetFunction (int x int y) method and use JUnit's built in support for expecting exceptions. You should also test for specific exceptions instead.  It is true that if you are using JUnit 3 and you are testing whether a particular exception is thrown or not thrown within a method you will need to use something like the try-catch pattern you define above. However: 1) I'd argue that there is a lot more to testing a method with a void return value then checking for exceptions: is your method making the correct calls to (presumably mocked) dependencies; does it behave differently when the class is initialized with a different context or different sets of dependencies etc. By wrapping all calls to that method you make it hard to change other aspects of your test. I'm also generally opposed to adding code and adding complexity if it can be avoided; I don't think it's a burden to have to put a try/catch in a given test when it's checking for exceptions. 2) Switch to JUnit 4! It makes it easy to check for expected exceptions: @Test(expected=IndexOutOfBoundsException.class) public void testIndexOutOfBoundsException() { ArrayList emptyList = new ArrayList(); Object o = emptyList.get(0); } Be aware that if you use this syntax the test will pass if any of the code running in the test threw IndexOutOfBoundsException. Sometimes that isn't a problem since you already have test cases that cover the non-exceptional cases but don't do this for long tests. If you use JUnit 4.7. try the ExpectedException Rule: http://www.infoq.com/news/2009/07/junit-4.7-rules I've rewritten my tests to use this format. It's much easier. Thanks!"
672,A,"Spring Web Service Unit Tests: java.lang.IllegalStateExcepton: Failed to load Application Context I am getting the error ""java.lang.IllegalStateExcepton: Failed to load Application Context"" when I am trying to run my unit test from within Eclipse. The Unit Test itself seems very simple: package com.mycompany.interactive.cs.isales.ispr.ws.productupgrade; import org.junit.Assert; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""/applicationContext.xml""}) public class ProductUpgradeTest { @Test public void getProductUpgrade () throws Exception { //System.out.println(PrintClasspath.getClasspathAsString()); Assert.assertTrue(true); } } But no matter what I seem to do with the @ContextConfiguration I still get the same error. The applicationContext.xml file resides within a folder /etc/ws but even if I put a copy in the same folder it is still giving me errors. I am very new to Java Spring Eclipse and Ant and really don't know where this is going wrong. The problem is that you use an absolute path in your @ContextConfiguration annotation. You now specify that your applicationContext.xml is located in your root folder of your system (where it probably isn't since you say it is at /etc/ws). I see two possible solutions: Use @ContextConfiguration(locations={""/etc/ws/applicationContext.xml""}) Move your applicationContext.xml file into your project (for example in your WEB-INF folder) and use @ContextConfiguration(locations={""classpath:applicationContext.xml""}) In case 2 you have to make sure that the directory in which you place your applicationContext.xml is located in your classpath. In Eclipse this can be configured in your project properties."
673,A,"How Can I test Servlets with JUnit? I saw Jetty has got and ServletTest class which you can use to test Servlets. tester = new ServletTester(); tester.setContextPath(""/""); tester.addServlet(TestServlet.class ""/servlet/*""); ... tester.start(); Do you know if there is something similar in Tomcat? How could I test a servlet? For unit testing it should not matter what servlet container you use. This is similar to using HSQL instead of Oracle for unit testing of database access code. So even if you write for a Tomcat deployment if Jetty is more suitable for unit testing (faster to start easier to configure etc) you can go with Jetty. Take a look at Jakarta Cactus Cactus is a simple test framework for unit testing server-side java code (Servlets EJBs Tag Libs Filters ...). Here's a servlet test how-to FYI Jakarta Cactus has been deprecated since 2011/08/05. :(  HttpUnit has a has a ""simulated container"" called ServletUnit.  I've never found a benefit to testing servlets directly (nor Struts actions say) especially given the work needed to do it. Most of my servlets/actions/whatever use POJOs for the bulk of their work and the POJOs are heavily tested. The webapps themselves have suites of HtmlUnit tests. Everything in between I assume to be just plumbing. I don't believe that I've even once encountered any sort of bug that would have ONLY been caught by testing the servlet classes directly and which would not be caught by the POJO or webapp tests. Point taken. However in my case I've inherited a junk servlet that is terribly complex and buggy. Being able to call it from a unit test for the specific scenario that is failing will greatly shorten my debugging process. :)"
674,A,"Break on Exception in Eclipse using jUnit Is there a way to make Eclipse break on uncaught exceptions while in debug mode for jUnit? Eclipse breaks fine when executing main(). Is there a command-line switch I can use? Thanks From the debug perspective you can filter exactly which exceptions you are interested in. In the Breakpoints view there is a ""J!"" button. This opens a window that allows you to choose which exceptions you want to break on. If the problem only occurs when JUnit tests you need to make sure you are launching the tests in debug mode. The Rerun button in the JUnit will run in ""normal"" mode. To run the tests in debug you can right click on the file and select ""Debug as -> JUnit Test"" from the menu. Thanks for the insight. I am doing Debug as -> JUnit Test. It is stopping at breakpoints but not at exceptions. It appears that JUnit swallows the exceptions (maybe with some sort of try/catch block). The code is a subclass of junit.framework.TestCase. The exception is caused by ""int i=1/0;"" for testing. I can definately reproduce your problem. I never used to have this behaviour before. I found the following comment in the 3.5 release notes: Developers debugging applications on Sun's 1.6.0_14 virtual machine should be aware that breakpoints are unreliable (i.e. do not always suspend execution). The problem occurs on Windows and Linux platforms. At this point it appears to be an issue with the VM rather than Eclipse. The workaround is to use the 1.6.0_13 virtual machine. (bug 279137). Did downgrading fix it on your side? I tried 1.6.0_13 and it didn't change anything. I'm pretty sure the downgrade is in effect but not certain how to verify. I just tried. It made no difference for me either. You can verify by looking at the debug launcher configuration for your test. Under the JRE tab it should tell you which jre you are using. I got this with testNG ... 1.6.0_16  If you run in debug mode this should be the default behaviour of later versions of Eclipse. Interesting. It only seems to happen when I have errors in jUnit tests. I updated the question. Important detail you added. How do you invoke junit?  As Thorbjørn Ravn Andersen said it's the default behaviour since Eclipse Ganymede. If it is not enabled goto Window->Preferences Java->Debug [X] Suspend execution on uncaught exceptions It's very useful but it may be annoying so try to figure out what's the best option for you.  If you debug a single method in jUnit the breakpoints start to work. If an entire class or package is debugged in jUnit the debugger doesn't work.  You have to select Run -> Debug from the menu. Eclipse will then stop on exceptions and breakpoints in your code."
675,A,"How do I configure JUnit Ant task to only produce output on failures? I am configuring JUnit in Ant so that unit tests will be run on each build. I would like the output of failing tests to be printed in the Ant console output whenever they are run. I don't need to see any output from succeeding tests. Here is the relevant bit of my build.xml file: <junit> <classpath> <pathelement path=""${build}""/> </classpath> <formatter type=""brief"" usefile=""false""/> <batchtest> <fileset dir=""${src}"" includes=""my/tree/junit/""/> </batchtest> </junit> This produces almost what I want failing tests are detailed in the Ant output except that succeeding tests also write the following output:  [junit] Testsuite: my.tree.junit.ExampleTest [junit] Tests run: 7 Failures: 0 Errors: 0 Time elapsed: 0.002 sec I believe I have tried all the combinations listed in the JUnit task documentation including: printsummary attribute showoutput attribute formatter element with each kind of type My use case is running ant from the command line. As I write more tests I don't want the output from succeeding tests to be so large that output from failing tests scrolls off the screen. I just want ant to be quiet unless there's a failing test that needs my attention. How can I configure Ant/JUnit to do this? I am using Ant version 1.6.4 and JUnit 4.6. Just added custom JUnit custom test runner possibility. One possibility would be to define your own xml formatter with the 'classname' attribute (and extending org.apache.tools.ant.taskdefs.optional.junit.XMLJUnitResultFormatter potentially doing nothing on endTest() or endTestsuite() methods). That formatter would ignore info message and only display failure messages. Note: this settings mention the possibility of only displaying failed tests: <junit showoutput=""true"" fork=""true"" failureproperty=""tests.failed"" errorproperty=""tests.failed""> <batchtest todir=""${test.results.dir}""> <fileset dir=""test""> <include name=""**/*Test.java""/> </fileset> </batchtest> <classpath path=""${classes.dir}:${junit.jar}:${test.classes.dir}""/> <formatter usefile=""false"" type=""brief""/> <!-- <formatter type=""xml""/> If missing only displays failed tests --> </junit> Did you test that ? Note: the ""showoutput=""true"" and <formatter type=""brief"" usefile=""false""/>"" can be a bit problematic as illustrated by this recent (February 2012) ticket) Yet another approch would be to define your ant Juint Test runner supporting ant JUnitResultFormatter and displaying only stderr messages. The EclipseTestRunner from eclipse is a good example. I implemented a custom formatter and was able to accomplish my original goal. @Greg: that's great. If the custom formatter is not too big could you post it here (or at least the part which only prints error messages)? Very nice answer! Thanks the example you showed is essentially the same as what I had posted in my question so I think the difference might be something to do with running it in the Netbeans context. I'll try implementing a custom formatter but I had hoped to avoid having to do that. Note: as of 3/27/12 the ""this settings"" is a dead link @stevebot ok I have found another link illustrating and using the same JUnit config. I have edited the answer."
676,A,Ant build - Emma code instrumentation - JUnit tests need interfaces Emma doesn't instrument them I have an Ant build file where I compile the Java source code of the application and of the tests instrument the application classes run JUnit tests and generate JUnit and Emma code coverage reports. The JUnit task is given the path to the instrumented classes. The problem is that the interfaces are not instrumented (Emma FAQ) but I use them in the tests and JUnit can't find them. I can think of 2 solutions: don't use interfaces in tests (goes against programming to interfaces - does it count in tests?) copy the interfaces next to the instrumented classes (hard-coding the path to the interfaces) How should I approach and solve this problem? It sounds to me as though you are saying that JUnit is having trouble because the interfaces are not on the class path? The usual answer would be to put them there. The quick and dirty answer might be to put the classpath for the not instrumented classes into the juint class path AFTER the path to the instrumented classes. The class loader should use the first match it finds so the instrumented implementations will be consumed instead of the non-instrumented implementations but the interfaces will still be available. If that solves your problem you may want to replace the quick and dirty with something more robust like making the interfaces available in a jar that is separate from the implementation. Including the non-instrumented classes after the instrumented ones worked thanks.
677,A,"Hudson results one step behind I'm using the filesytem plugin for Hudson and when a build happens it looks for new/modified files copies them to the Workspace runs tests using Rake and then publishes those junit xml result files. However the updated junit xml result files don't get pushed to the workspace until the next build. This means that when the publishing of the junit xml result files happens it's always one step behind. And this means I need to run a build twice before the results show. The Rake task is creating the junit xml files in the project directory. I've tried outputting to the workspace directory but it seems to make things worse and the results don't get published at all. Am I doing something fundamentally wrong here? Is there a simple way of getting those junit xml results pushed to the workspace so that the post-build ""Publish JUnit test result report"" actually runs against the newly created xml files? I am confused with the following sentence: However the updated junit xml result files don't get pushed to the workspace until the next build. Does Rake produce the junit.xml? The whole description sounds like: The build happens outside of Hudson. The build produces the junit.xml and Hudson just picks up the build and the junit.xml and copies it into the workspace for publishing the junit.xml. So hudson detects the changes before the build (including junit tests) is finished and there only gets half of the build or even the old build. The fix would be to configure either quite periode longer than what your build takes or let your build notify Hudson when to run your rake job. The configuration setting would be ""Trigger builds remotly"" instead of poll SCM. Even if rake produces the junit.xml it might be that rake actually runs on an old version of code because the rake job starts before the new build artifacts are available. My suggestion would be to use Hudson for building and chain both jobs together so that the rake job will be triggered when the build job is finished. The transfer of the necessary build artifacts can be through an repository outside of Hudson (could be as simple as a network share). In this case the build job copies the necessary files into the repository and the rake job pulls it again. You can also play around with the archive artifacts functions from Hudson. Another more elegant option is the Clone Workspace SCM Plugin which handles the transfer of a workspace to another job."
678,A,Reporting ignored tests and getting live reports in JUnit? Using the JUnit assumeTrue you can skip a test but is there any way to tell JUnit that the test has been skipped? It seems to just report that the test has passed.... Also is there any way to detect during the @After method if any tests have failed? E.g. @After public void tearDown() { //.... How many tests passed and how many failed? } I know this information is reported at the end of all the tests but I would like to be able to access it in the @After function. Sorry to ask but why? I am guessing you are referring to point 2... its a userbility improvement to log current statistics as the test is running. This is useful if the test suite takes 10+ minutes to run... its nice to know if there was a failure early on. If your tests are that slow you must make them faster. That sounds more like integration tests and not Unit tests... Its a selenium frontend with a JUnit back end... its not actually 'unit testing' per say but selenium uses JUnit to report usability test failures. For the first part couldn't you use the @Ignore tag? @Roflcoptr - Yes but I do not believe there is a way to make this conditional? Have a look at the experimental 'Max' feature in Junit 4.7 (see release nodes: https://3licenses.svn.codeplex.com/svn/thirdparty/junit/junit-4.7/README.html) -- may it helps you to run the relevant tests earlyer. Also is there any way to detect during the @After method if any tests have failed? You can implement your own Test Runner: @RunWith(MyRunner.class) @See: org.junit.runners.BlockJUnit4ClassRunner Or you can have a look at TestDecorators (in JUnit 4.7) junit.extensions.TestDecorator. That sounds like the thing you need but I have no expirance with.
679,A,"Forking jUnit with a selenium test using ant runs tests serially not concurrently I have an Ant task that runs a batch of test cases that I have written which runs perfectly fine... except Ant seems to be ignoring the fork=""true"" attribute in the <junit> and <batchtest> elements. Here is my Ant task: <target name=""run_tests"" depends=""init""> <java jar=""${dir.testLib}${seleniumJar}"" fork=""true"" spawn=""true""/> <junit fork=""yes"" haltonfailure=""no"" printsummary=""no""> <classpath refid=""test.classpath""/> <batchtest todir=""${test.reports}/acceptance/gui"" fork=""true""> <fileset dir=""${dir.classes}""> <include name=""**/*TestCase.class"" /> <include name=""**/*Test.class"" /> <include name=""**/Test*.class"" /> <exclude name=""**/AbstractSeleneseTestCase.class""/> </fileset> </batchtest> <formatter type=""xml"" /> <classpath refid=""test.classpath"" /> </junit> <junitreport todir=""${test.reports}/acceptance/gui""> <fileset dir=""${test.reports}/acceptance/gui""> <include name=""TEST-*.xml"" /> </fileset> <report todir=""${test.reports}/acceptance/gui"" /> </junitreport> </target> It's taking 8+ minutes to run my test cases one-at-a-time which is entirely way too long. I have noticed other flukes using Selenium is this just another nuance of using the project? Here is a synopsis of what is happening: 1. Run Ant task 2. Ant task spawns a thread to run Selenium server 3. Test cases (Selenium and jUnit) are running one-at-a-time 4. A dark cloud lingers over my cube as my keyboard is struck by lightning ;-( Here is a synopsis of what I want to happen: 1. Run Ant task 2. Ant task spawns a thread to run Selenium server 3. Tests cases (Selenium and jUnit) launch concurrently 4. Bonuses all around and high fives and pat's on the back for everyone! How many Selenium Remote Controls are in your Selenium Grid? Or are you running selenium locally? I'm running Selenium locally But you're not using the Grid are you? See my answer. BTW the Grid can be run locally you just need to set it up and set up a bunch of RCs. It would seem to me that you are not running a Selenium Grid that allows you to run tests in Parallel (as many concurrent ""threads"" as you have Remote Controls registered in the Grid). Here's more info: http://selenium-grid.seleniumhq.org/ +1 Grid seems to be a great tool now just to get approval to use it on our network... You can set up a grid locally too and launch the Grid plus the remote controls on your local machine. Where I work we sometimes do that but there's a server that has a Grid and several RCs already up (using different environments).  One solution that I have seen and done well mind you is creating a SeleniumFactory. This would be your own class that spawns a new thread every time you need another web browser test. It takes a little work but well worth the effort."
680,A,"When do Java generics require instead of and is there any downside of switching? Given the following example (using JUnit with Hamcrest matchers)  Map<String Class<? extends Serializable>> expected = null; Map<String Class<java.util.Date>> result = null; assertThat(result is(expected)); This does not compile with the JUnit AssertThat method signature of: public static <T> void assertThat(T actual Matcher<T> matcher) The compiler error message is: Error:Error:line (102)cannot find symbol method assertThat(java.util.Map<java.lang.Stringjava.lang.Class<java.util.Date>> org.hamcrest.Matcher<java.util.Map<java.lang.Stringjava.lang.Class <? extends java.io.Serializable>>>) However if I change the assertThat method signature to: public static <T> void assertThat(T result Matcher<? extends T> matcher) Then the compilation works. So three questions: Why exactly doesn't the current version compile? Although I vaguely understand the covariance issues here I certainly couldn't explain it if I had to. Is there any downside in changing the assertThat method to Matcher<? extends T>? Are there other cases that would break if you did that? Is there any point to the genericizing of the assertThat method in JUnit? The Matcher class doesn't seem to require it since JUnit calls the matches method which is not typed with any generic and just looks like an attempt to force a type safety which doesn't do anything as the Matcher will just not in fact match and the test will fail regardless. No unsafe operations involved (or so it seems). For reference here is the JUnit implementation of assertThat: public static <T> void assertThat(T actual Matcher<T> matcher) { assertThat("""" actual matcher); } public static <T> void assertThat(String reason T actual Matcher<T> matcher) { if (!matcher.matches(actual)) { Description description= new StringDescription(); description.appendText(reason); description.appendText(""\nExpected: ""); matcher.describeTo(description); description.appendText(""\n got: "").appendValue(actual) .appendText(""\n""); throw new java.lang.AssertionError(description.toString()); } } First - I have to direct you to http://www.angelikalanger.com/GenericsFAQ/JavaGenericsFAQ.html -- she does an amazing job. The basic idea is that you use <T extends SomeClass> when the actual parameter can be SomeClass or any subtype of it. In your example Map<String Class<? extends Serializable>> expected = null; Map<String Class<java.util.Date>> result = null; assertThat(result is(expected)); You're saying that ""expected"" can contain Class objects that represent any class that implements Serializable. Your result map says it can only hold Date class objects. When you pass in result you're setting T to exactly Map of String->Date class objects which doesn't match Map String -> anything that's Serializable. One thing to check -- are you sure you want Class<Date> and not Date? A map of String->Class<Date> doesn't sound terribly useful in general (all it can hold is Date.class as values rather than instances of Date) As for genericizing assertThat the idea is that the method can ensure that a Matcher that fits the result type is passed in. In this case yes I do want a map of classes. The example I gave is contrived to use standard JDK classes rather than my custom classes but in this case the class is actually instantiated via reflection and used based on the key. (A distributed app where the client doesn't have the server classes available just the key of which class to use to do the server side work). I guess where my brain is stuck is on why a Map containing classes of type Date doesn't just fit nicely into a type of Maps containing classes of type Serializable. Sure the classes of type Serializable could be other classes as well but it certainly includes type Date. On the assertThat making sure the cast is performed for you the matcher.matches() method doesn't care so since the T is never used why involve it? (the method return type is void) Ahhh - that's what I get for not reading the def of the assertThat close enough. Looks like it's only to ensure that a fitting Matcher is passed in...  what if you use Map<String ? extends Class<? extends Serializable>> expected = null; Yes this is kind of what my above answer was driving at. No that doesn't help the situation at least the way I tried.  Thanks to everyone who answered the question it really helped clarify things for me. In the end Scott Stanchfield's answer got the closest to how I ended up understanding it but since I didn't understand him when he first wrote it I am trying to restate the problem so that hopefully someone else will benefit. I'm going to restate the question in terms of List since it has only one generic parameter and that will make it easier to understand. The purpose of the parametrized class (such as List<Date> or Map<K V> as in the example) is to force a downcast and to have the compiler guarantee that this is safe (no runtime exceptions). Consider the case of List. The essence of my question is why a method that takes a type T and a List won't accept a List of something further down the chain of inheritance than T. Consider this contrived example: List<java.util.Date> dateList = new ArrayList<java.util.Date>(); Serilizable s = new String(); addGeneric(s dateList); .... private <T> void addGeneric(T element List<T> list) { list.add(element); } This will not compile because the list parameter is a list of dates not a list of strings. Generics would not be very useful if this did compile. The same thing applies to a Map<String Class<? extends Serializable>> It is not the same thing as a Map<String Class<java.util.Date>>. They are not covariant so if I wanted to take a value from the map containing date classes and put it into the map containing serializable elements that is fine but a method signature that says: private <T> void genericAdd(T value List<T> list) Wants to be able to do both: T x = list.get(0); and list.add(value); In this case even though the junit method doesn't actually care about these things the method signature requires the covariance which it is not getting therefore it does not compile. On the second question Matcher<? extends T> Would have the downside of really accepting anything when T is an Object which is not the APIs intent. The intent is to statically ensure that the matcher matches the actual object and there is no way to exclude Object from that calculation. The answer to the third question is that nothing would be lost in terms of unchecked functionality (there would be no unsafe typecasting within the JUnit API if this method was not genericized) but they are trying to accomplish something else - statically ensure that the two parameters are likely to match. EDIT (after further contemplation and experience): One of the big issues with the assertThat method signature is attempts to equate a variable T with a generic parameter of T. That doesn't work because they are not covariant. So for example you may have a T which is a List<String> but then pass a match that the compiler works out to Matcher<ArrayList<T>>. Now if it wasn't a type parameter things would be fine because List and ArrayList are covariant but since Generics as far as the compiler is concerned require ArrayList it can't tolerate a List for reasons that I hope are clear from the above. I still don't get why I can't upcast though. Why can't I turn a list of dates into a list of serializable? @ThomasAhle because then references that think it is a list of Dates will run into casting errors when they find Strings or any other serializables. I see but what if I somehow got rid of the old reference as if I returned the `List` from a method with type `List`? That should be safe right even if not allowed by java.  The reason your original code doesn't compile is that <? extends Serializable> does not mean ""any class that extends Serializable"" but ""some unknown but specific class that extends Serializable."" For example given the code as written it is perfectly valid to assign new TreeMap<String Long.class>() to expected. If the compiler allowed the code to compile the assertThat() would presumably break because it would expect Date objects instead of the Long objects it finds in the map. I'm not quite following - when you say ""does not mean ... but ..."": what is the difference? (like what would be an example of a ""known but nonspecific"" class which fits the former definition but not the latter?) Yes that is a bit awkward; not sure how to express it better... does it make more sense to say ""'?' is a type that is unknown not a type that matches anything?""  One way for me to understand wildcards is to think that the wildcard isn't specifying the type of the possible objects that given generic reference can ""have"" but the type of other generic references that it is is compatible with (this may sound confusing...) As such the first answer is very misleading in it's wording. In other words List<? extends Serializable> means you can assign that reference to other Lists where the type is some unknown type which is or a subclass of Serializable. DO NOT think of it in terms of A SINGLE LIST being able to hold subclasses of Serializable (because that is incorrect semantics and leads to a misunderstanding of Generics). That certainly helps but the ""may sound confusing"" is kind of replaced with a ""sounds confusing."" As a followup so why according to this explanation does the method with Matcher`` compile?  It boils down to: Class<? extends Serializable> c1 = null; Class<java.util.Date> d1 = null; c1 = d1; // compiles d1 = c1; // wont compile - would require cast to Date You can see the Class reference c1 could contain a Long instance (since the underlying object at a given time could have been List<Long>) but obviously cannot be cast to a Date since there is no guarantee that the ""unknown"" class was Date. It is not typsesafe so the compiler disallows it. However if we introduce some other object say List (in your example this object is Matcher) then the following becomes true: List<Class<? extends Serializable>> l1 = null; List<Class<java.util.Date>> l2 = null; l1 = l2; // wont compile l2 = l1; // wont compile ...However if the type of the List becomes ? extends T instead of T.... List<? extends Class<? extends Serializable>> l1 = null; List<? extends Class<java.util.Date>> l2 = null; l1 = l2; // compiles l2 = l1; // won't compile I think by changing Matcher<T> to Matcher<? extends T> you are basically introducing the scenario similar to assigning l1 = l2; It's still very confusing having nested wildcards but hopefully that makes sense as to why it helps to understand generics by looking at how you can assign generic references to each other. It's also further confusing since the compiler is inferring the type of T when you make the function call (you are not explicitly telling it was T is)."
681,A,Tests Struts2 struts.xml with simple junit I just want a basic Junit that that will test loading the configuration file struts.xml and making sure all the classes can be found and such. I'm using the struts junit 2.2.1 plugin. I recommend you to use the Config Browser Plugin of Struts2. Description : The Config Browser Plugin is a simple tool to help view an application's configuration at runtime. It is very useful when debugging problems that could be related to configuration issues. looks cool but I want something I can turn into a junit. I will use this to fail my hudson builds so the people who break it get emails. Its not for me.
682,A,"Appropriate use of assert Can you please help me better understand what is an appropriate use of “assert” vs “throwing an exception? When is each scenario appropriate? Scenario 1 CODE public Context(Algorythm algo) { if (algo == null) { throw new IllegalArgumentException(""Failed to initialize Context""); } this.algo = algo; } TEST public void testContext_null() { try { context = new Context(null); fail(); } catch (IllegalArgumentException e) { assertNotNull(e); } } Scenario 2 CODE public Context(Algorythm algo) { assert (algo != null); this.algo = algo; } TEST public void testContext_null() { try { context = new Context(null); fail(); } catch (AssertionFailedError e) { assertNotNull(e); } } You cannot catch a null exception. assertNotNull(e) has to be true. Are you using JUnit 4? If you are use the `@Test(expected=IllegalArgumentException.class)` annotation instead of having `try`/`catch` in your test method. And the fail(); or the test might pass for the wrong reason. @Asaph - good point. I totally forgot about that. Thx @Peter. The issue is not with assertNotNull but rather using general Java's ""assert"" statement vs writing up an ""if"" condition where if it fails exception is thrown Doing this right in JUnit 4 the body of the test method becomes a 1 liner. Like this: `@Test(expected=IllegalArgumentException.class) public void testContext_null() { new Context(null); }` @mac: What version of JUnit are you using? I figured it out ... weird thing actually. Initially i just added jUnit4.8.2 jar into my class. This caused the error. Once i added a JUnit Library"" everything worked. I am a bit confused now. I thought having added ""external jar to classpath"" does it. Ok really weird. In order to remove all errors - i removed ""extends TestCase"" from my jUnit class added jUnit Library and voila- everything is working. Is ""extends TestCase"" not needed for Junit4? http://stackoverflow.com/questions/2635839/junit-confusion-use-extend-testcase-or-test answers it all. Thanks again everyone. Assert is a macro (in C/C++ or a function in other languages) that validates a given expression as true or false and throw an exception in case of false values. Assert is something to use when ddebugging an application like when you must check if a math expression really gives you an appropriate value or if an object/structure member is not null or missing something important and things like that. An Exception throwing is more of a real error treatment. Exceptions are errors too and can stop your application but they are used as the (let's say) ""retail version"" error treatment of the application. That's because Exceptions can be caught and taken differently to the user with a little non-technical message instead of symbols and memory addresses while you can just serialize that into an app log for example. On the other hand asserts will just stop the running process and give you a message like ""Assertion failed on source_file.ext line X. The process will be terminated."" And that's not user-friendly :)  The main difference with assert is; the ability to turn on/off selected tests by class/package. the error thrown. assert is more approriate for tests which will be turned off in production. If you want a test which is checked every time esp if validating data from an input you should use the check which runs every time.  The assert keyword should be used when failure to meet a condition violates the integrity of the program. These are intended to be non-recoverable error situations. Exceptions on the other hand alert calling methods to the presence and location of an error but can be handled or ignored at the programmer's discretion. When testing you should use the Assert functions when a condition must be met for a test to pass. If you're expecting an exception in that particular test JUnit 4 has an annotation to signify that an test should throw a particular Exception: @Test(expected=MyException.class)  Outside of test code asserts are generally a bad idea. the reason is that unless there are very strict company guidelines in place you invariably end up with mixed usage which is bad. there are basically 2 usage scenarios for assert: extra possibly slow tests which will be turned off in production normal quick code sanity tests which should never be disabled (like requiring a given method parameter to be non-null) As long as you always follow one of the scenarios things are fine. however if your code base ends up with both scenarios then you are stuck. you have asserts which follow scenario 2 which you don't want to disable and you have asserts which follow scenario 1 (and are slowing down your production code) which you want to disable. what to do? most codebases which i have worked with which used asserts in normal code never ended up disabling them in the production build for exactly this reason. therefore my recommendation is always to avoid them outside of test code. use normal exceptions for the normal code and stick the extra possibly slow code (with asserts) in separate test code."
683,A,"Android JUnit Tests failing with java.lang.VerifyError Aloha I've been following the guidelines here: http://developer.android.com/resources/tutorials/testing/helloandroid_test.html To create some simple test cases for a new Android project. The first few test cases were working fine but now I am not able to get any tests to run. Here is an example of the output I am getting:  [2011-03-25 10:05:01 - application-tests] Android Launch! [2011-03-25 10:05:01 - application-tests] adb is running normally. [2011-03-25 10:05:01 - application-tests] Performing android.test.InstrumentationTestRunner JUnit launch [2011-03-25 10:05:01 - application-tests] Automatic Target Mode: using existing emulator 'emulator-5554' running compatible AVD 'GalaxyTabRunning2.3' [2011-03-25 10:05:02 - application-tests] Application already deployed. No need to reinstall. [2011-03-25 10:05:02 - application-tests] Project dependency found installing: application [2011-03-25 10:05:03 - application] Application already deployed. No need to reinstall. [2011-03-25 10:05:03 - application-tests] Launching instrumentation android.test.InstrumentationTestRunner on device emulator-5554 [2011-03-25 10:05:03 - application-tests] Collecting test information [2011-03-25 10:05:06 - application-tests] Test run failed: java.lang.VerifyError The test case is very simple: /** * Test case to test a null input parameter to the decode method. * */ public void testNullInputPerformCrcDecoding() { try { AppProtocolDecoder.performCrcDecoding(null); fail(""Expected IllegalArgumentException to be thrown...""); } catch (IllegalArgumentException expected) { assertTrue(expected instanceof IllegalArgumentException); } catch (ProtocolException ve) { fail(""Unexpected VisiProtocolException occured: "" + ve); } } AppProtocolDecoder.performCrcDecoding(null); is a static method. I've removed all the code from it so all it does literally is return null. The test always fails with the java.lang.VerifyError. This is under Android 2.3 using the Android JUnit Test runner in Eclipse. Any ideas? I've tried recreating the test project and all sorts of permutations of code modifications. Thanks in advance! EDIT (!): Logcat output: D/AndroidRuntime( 919): Shutting down VM W/dalvikvm( 919): threadid=1: thread exiting with uncaught exception (group=0x40015560) E/AndroidRuntime( 919): FATAL EXCEPTION: main E/AndroidRuntime( 919): java.lang.VerifyError: com.company.h1s.androidserver.test.protocol.AppProtocolDecoderTest E/AndroidRuntime( 919): at java.lang.Class.getDeclaredConstructors(Native Method) E/AndroidRuntime( 919): at java.lang.Class.getConstructors(Class.java:490) E/AndroidRuntime( 919): at android.test.suitebuilder.TestGrouping$TestCasePredicate.hasValidConstructor(TestGrouping.java:226) E/AndroidRuntime( 919): at android.test.suitebuilder.TestGrouping$TestCasePredicate.apply(TestGrouping.java:215) E/AndroidRuntime( 919): at android.test.suitebuilder.TestGrouping$TestCasePredicate.apply(TestGrouping.java:211) E/AndroidRuntime( 919): at android.test.suitebuilder.TestGrouping.select(TestGrouping.java:170) E/AndroidRuntime( 919): at android.test.suitebuilder.TestGrouping.selectTestClasses(TestGrouping.java:160) E/AndroidRuntime( 919): at android.test.suitebuilder.TestGrouping.testCaseClassesInPackage(TestGrouping.java:154) E/AndroidRuntime( 919): at android.test.suitebuilder.TestGrouping.addPackagesRecursive(TestGrouping.java:115) E/AndroidRuntime( 919): at android.test.suitebuilder.TestSuiteBuilder.includePackages(TestSuiteBuilder.java:103) E/AndroidRuntime( 919): at android.test.InstrumentationTestRunner.onCreate(InstrumentationTestRunner.java:360) E/AndroidRuntime( 919): at android.app.ActivityThread.handleBindApplication(ActivityThread.java:3210) E/AndroidRuntime( 919): at android.app.ActivityThread.access$2200(ActivityThread.java:117) E/AndroidRuntime( 919): at android.app.ActivityThread$H.handleMessage(ActivityThread.java:966) E/AndroidRuntime( 919): at android.os.Handler.dispatchMessage(Handler.java:99) E/AndroidRuntime( 919): at android.os.Looper.loop(Looper.java:123) E/AndroidRuntime( 919): at android.app.ActivityThread.main(ActivityThread.java:3647) E/AndroidRuntime( 919): at java.lang.reflect.Method.invokeNative(Native Method) E/AndroidRuntime( 919): at java.lang.reflect.Method.invoke(Method.java:507) E/AndroidRuntime( 919): at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:839) E/AndroidRuntime( 919): at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:597) E/AndroidRuntime( 919): at dalvik.system.NativeStart.main(Native Method) W/ActivityManager( 61): Error in app com.company.h1s.androidserver running instrumentation ComponentInfo{com.company.h1s.androidserver.test/android.test.InstrumentationTestRunner}: W/ActivityManager( 61): java.lang.VerifyError W/ActivityManager( 61): java.lang.VerifyError: com.company.h1s.androidserver.test.protocol.AppProtocolDecoderTest I/ActivityManager( 61): Force stopping package com.company.h1s.androidserver uid=10031 I/Process ( 61): Sending signal. PID: 919 SIG: 9 D/AndroidRuntime( 911): Shutting down VM I am using Maven for class dependencies. Here are the libraries I am including via Maven: /.m2/repository/com/google/android/android/2.2.1/android-2.2.1.jar /.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar /.m2/repository/org/apache/httpcomponents/httpclient/4.0.1/httpclient-4.0.1.jar /.m2/repository/org/apache/httpcomponents/httpcore/4.0.1/httpcore-4.0.1.jar /.m2/repository/commons-codec/commons-codec/1.3/commons-codec-1.3.jar /.m2/repository/org/khronos/opengl-api/gl1.1-android-2.1_r1/opengl-api-gl1.1-android-2.1_r1.jar /.m2/repository/xerces/xmlParserAPIs/2.6.2/xmlParserAPIs-2.6.2.jar /.m2/repository/xpp3/xpp3/1.1.4c/xpp3-1.1.4c.jar /.m2/repository/org/json/json/20080701/json-20080701.jar /.m2/repository/org/slf4j/slf4j-android/1.6.1-RC1/slf4j-android-1.6.1-RC1.jar /.m2/repository/org/slf4j/log4j-over-slf4j/1.6.1/log4j-over-slf4j-1.6.1.jar /.m2/repository/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar /.m2/repository/org/apache/mina/mina-core/2.0.0/mina-core-2.0.0.jar /.m2/repository/com/company/app-common/1.0-SNAPSHOT/app-common-1.0-SNAPSHOT.jar /.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jar /.m2/repository/com/company/app-data/1.0-SNAPSHOT/app-data-1.0-SNAPSHOT.jar /.m2/repository/org/springframework/spring/2.5.6/spring-2.5.6.jar Can you post the full class definition Also go into logcat and copy anything close that looks suspicious what does the constructor look like for your Test class What I ended up doing is simply removing the code that was using these libraries. In most cases it was pretty trivial to do so.  If you are using external libraries in your main project you should export them to your tests.  Are you using any external jars? This seems to be a common source of java.lang.VerifyError in my experience. As this answer suggests you may need to recompile them or run them through dx so that they contain dalvik bytecode. Edit - I haven't dealt with this myself and I'm not sure about the maven way to do this but at some point I think you need to use dx on your jars: dx --dex --output=some.dex.jar some.jar If possible you might try doing this manually on your jars to see if that's actually the problem and then try to integrate that into your maven build afterwards. Yes quite a few external libraries. I am using Maven for dependency management and I've posted above all the stuff being included. I've also put in the logcat output which is not terribly helpful."
684,A,"Is there a JUnit TestRunner for running groups of tests? I am currently using JUnit 4 and have a need to divide my tests into groups that can be run selectively in any combination. I know TestNG has a feature to annotate tests to assign them to groups but I can't migrate to TestNG right now. It seems this could easily be accomplished in JUnit with some custom annotations and a custom JUnit TestRunner. I've checked both the JUnit docs and searched the web but couldn't find such a thing. Is anyone aware of such a TestRunner? Update: Thanks for your replies regarding test suites. I should have addressed these in my original question. Here we go: I don't want to use test suites because they would require me to manually create and manage them which means touching ALL my tests and arranging them into suites manually (too much work and a maintenance nightmare). All I need to do is run all unit tests except a few that are really integration tests. So I want to annotate these and run all others. At other times I want to just run the integration tests. I also have the need to put a unit test into multiple groups which is not possible with suites. Hope this helps to clear things up. Update 2: If JUnit doesn't have this OOB I'm looking for an Open Source library that adds this to JUnit (annotations + custom JUnit Test Runner). Related: http://stackoverflow.com/questions/457276/junit4-test-suites/4952225#4952225 JUnit has no such runner at the moment. Addressing the underlying issue the need to get reasonable assurance from a test suite in a limited amount of time is our highest development priority for the next release. In the meantime implementing a Filter that works through annotations seems like it wouldn't be a big project although I'm biased. Thanks Kent you hit the nail on the head pointing out the underlying issue; great to hear that this will be addressed in the next JUnit release. You're also right that my ramp-up time writing such a Filter may be a bit longer than yours given that you co-wrote JUnit :) I'll look into it though. Here's a link to the Filter class JavaDoc: http://junit.sourceforge.net/javadoc/org/junit/runner/manipulation/Filter.html  my advice is simply ditch JUnit and use TestNG. Once you get used to TestNG Junit looks like Stone Age.  You can create suites although that puts all the configuration in the suite and not in annotations.  If you're using ANT or maven you can control which tests are run by filtering the tests by name. A bit awkward but it might work for you.  JUnit 3 allows you to create test-suites which can be run like any other test. Doesn't JUnit 4 have a similar concept?  First you are addressing two problems - unit tests (often in the same package as the unit under test) and integration tests. I usually keep my integration tests in a separate package something like com.example.project.tests. In eclipse my projects look like: project/ src/ com.example.project/ tsrc/ com.example.project/ com.example.project.tests/ Right-clicking on a package and selecting 'run' runs the tests in the package; doing the same on the source folder runs all the tests. You can acheive a similar effect although you expressed a disinterest in it by using the Suite runner. However this violates DRY - you have to keep copies of the test names up to date in the suite classes. However you can easily put the same test in multiple suites. @RunWith(Suite.class) @Suite.SuiteClasses( { TestAlpha.class TestBeta.class }) public class GreekLetterUnitTests { } Of course I really should be keeping these things automated. A good method for doing that is to use the Ant task. <target name=""tests.unit""> <junit> <batchtest> <fileset dir=""tsrc""> <include name=""**/Test*.java""/> <exclude name=""**/tests/*.java""/> </fileset> </batchtest> </junit> </target> <target name=""tests.integration""> <junit> <batchtest> <fileset dir=""tsrc""> <include name=""**/tests/Test*.java""/> </fileset> </batchtest> </junit> </target>  No there is no similar concept to TestNG groups unfortunately. It was planned for JUnit4 but for some unclear reason it was dropped from the planning.  TestNG has my vote. It's annotation based can run as Groups single Tests etc can be linked into Maven and can run all JUnit tests as part of it's test runs. I highly recommend it over JUnit.  Check out Spring's SpringJUnit4ClassRunner. I've used it to optionally run tests based on a System property using the IfProfileValue annotation. This: @IfProfileValue(name=""test-groups"" values={""unit-tests"" ""integration-tests""}) public void testWhichRunsForUnitOrIntegrationTestGroups() { // ... } Will run if the System property 'test-groups' is set to either 'unit-tests' or 'integration-tests'. Update: JUnitExt has @Category and @Prerequisite annotations and looks like it should do what you need. However I've never used it myself so I can't vouch for it. The SpringJUnit4ClassRunner would have worked but we're using an old version of Spring that doesn't have this. Until we update (it's not easy in a huge project) I can't use this method. Thanks both of these look promising. I'll check them out. Note: Spring `@IfProfileValue` only **skips/ignores** the test if the name and value doesn't match and it doesn't **not-run** it. If you want to implement JUnit Categories type of implementation with this you may need to end up extending `SpringJUnit4ClassRunner#runChild` to _not-run_ instead of _ignoring_. This mainly matters if its results are in jenkins."
685,A,"Checking for 2 expected values in Junit I have a java program which throws an exception with 2 different messages for 2 different scenarios and I want the Junit test case to check for equality for both of these messages. As an example - public void amethod() { // do some processing if(scenario1 == true) { throw new MySystemException(""An error occured due to case 1 being incorrect.""); } else if(scenario2 == true) { throw new MySystemException(""An error occured as case 2 could not be found""); } } Now the JUnit for this would be something like- public void testAMethod() { // do something assertEquals(""Expected"" ""Actual""); } As I understand in this above example if I use the Scenario1 exception message the junit will fail when an exception is thrown for Scenario2 and vice versa. I would like to know if there is any other way provided in Junit by which I can use this one test method and check for both the messages for the test to pass? Something like an OR if possible to provide the ""Expected"" value with both these expected message. I hope my query is clear enough. Thanks UPDATE Sorry for the delayed response had got caught up with some other urgent matter. Thank you all for the very nice suggestions it certainly has helped me to understand a bit better now. Eventually to keep it rather simple I decided to implement a somewhat similar solution suggested by Don Roby. So created a new test class which looks like - public void testAMethodScenario1() { // do the necessary assertEquals(""Expected Exception Message 1"" ""Actual""); } public void testAMethodScenario2() { // do the necessary assertEquals(""Expected Exception Message 2"" ""Actual""); } Thank you all again for your responses. JUnit 4 provides (Expected Exception.class) @Test(expected= MySystemException.class) public void empty() { // what ever you want } Google: Expected Exceptions JUnit for more info.  You seem to be asking two things here how to test an exception and how to assert that a value matches either of two possible expected values. To test for an exception you can either use a JUnit4 annotation: @Test(expected=MySystemException.class) public void testException() { amethod(); } or use a try-catch in your test: @Test public void testException() { try { amethod(); fail(""MySystemException expected""); } catch (MySystemException e) { // Success! } } And if you have only one message in the try-catch version you can assert that you got it with an AssertEquals in the catch block. The best testing would have separate tests for your two scenarios and expect the correct single message. Better code might in fact have distinct exceptions for the two situations. But the need for a more complex assertion than simple equality does come up anyway and there's an elegant solution for it in Hamcrest matchers. Using that for this situation you could write something like (untested - don't trust my syntax completely): @Test public void testException() { try { amethod(); fail(""MySystemException expected""); } catch (MySystemException e) { String expectedMessage1 = ""An error occured due to case 1 being incorrect.""; String expectedMessage2 = ""An error occured as case 2 could not be found""; assertThat(e.getMessage() anyOf(equalTo(expectedMessage1) equalTo(expectedMessage2))); } } I have implemented something like your suggestion here. As it is an existing codebase and hence certain restrictions to modify a lot. What I have done is instead of modifying the existing test class created a new test class with 2 methods with obviously 2 different inputs to satisfy these 2 exception conditions. Both throw exception and then I am checking in respective methods with assertEquals for the exception message and both the test's pass.  The declared types of exception thrown bya method are part of its API. If you really want to distinguish different failure modes you should declare a different exception type for each failure mode. So something like this: /** * Do something. * @throws MySystemException1 in case 1. * @throws MySystemException2 if Foo not found. */ public void amethod() { // do some processing if(scenario1 == true) { throw new MySystemException1(""Case 1.""); } else if(scenario2 == true) { throw new MySystemException2(""Foo not found""); } } +1 for more specific exceptions. People don't do this enough. Not sure i'd recommend those particular names though. :) Well I couldnt change that as it would have required me to change a lot many things and as this was an existing code I couldnt make lot of changes. @Tom - The exception names were just as an example. ""I couldnt change that as it would have required me to change a lot [of] things"". You can still refactor towards the better design.  @Rule solution in JUnit4:  public class ExceptionRule implements MethodRule { @Override public Statement apply(final Statement base final FrameworkMethod method Object target) { return new Statement() { @Override public void evaluate() throws Throwable { try { base.evaluate(); Assert.fail(); } catch (MySystemException e) { if(scenario1) assertEquals(""Expected error message1"" e1.getMessage(); if(scenario2) assertEquals(""Expected error message2""e1.getMessage(); } } }; } } In your testcase use the Rule:  @Rule public ExceptionRule rule = new ExceptionRule(); This is quite a different way from what I had imagined. Good food for thought though. I will see if I can implement this way. Thanks. @Swift: you're right it's not easy way. I wrote it just for fun:) If you've many such testcases Rule could be re-used.  I think you need to manually catch the exception (for each scenario) and individually check the message: try { // trigger scenario 1 fail(""An exception should have been thrown here !""); } catch (MySystemException e1) { assertEquals(""Wrong error message"" m1 e1.getMessage()); } try { // trigger scenario 2 fail(""An exception should have been thrown here !""); } catch (MySystemException e2) { assertEquals(""Wrong error message"" m2 e2.getMessage()); } Of course you can have these scenarios defined as enum constants and simply iterate through them and check each of them within a loop since the ""copy/paste design pattern"" is pretty obvious in the above code. :) @Don Roby: note ""// trigger scenario 1"" and ""// trigger scenario 2"" - the point is to control which exception gets thrown and then test for that exception. Don't know why my previous comment on retracting disappeared... But I retracted the downvote (had to do a fake edit on the question to enable) and now I understand what you're saying you get an upvote instead. -1: This test will certainly fail as it's got two assertions that cannot both pass. Can you please explain ? The two assertions occur one at a time each for its own scenario. Trigger S1 -> expect exception e1 with m1. If nothing failed so far go on and trigger S2 -> expect exception e2 with m2. Or am I missing something ?...  BDD Style Solution with Catch Exception @Test public void testAMethodScenario1() { //given scenario 1 when(foo).amethod(); then(caughtException()) .isInstanceOf(MySystemException.class) .hasMessage(""An error occured due to case 1 being incorrect.""); } @Test public void testAMethodScenario2() { //given scenario 2 when(foo).amethod(); then(caughtException()) .isInstanceOf(MySystemException.class) .hasMessage(""An error occured as case 2 could not be found""); } Source code https://gist.github.com/mariuszs/7490875 Dependencies com.googlecode.catch-exception:catch-exception:1.2.0  Can you predict which scenario will occur? If so Costi's answer is correct. If not because there's some randomness or whatever you can write: @Test public void testAmethodThrowsException() { try { amethod(); fail(""amethod() should have thrown an exception""); } catch (MySystemException e) { String msg = e.getMessage(); assertTrue(""bad message: "" + msg msg.equals(""An error occured due to case 1 being incorrect."") || msg.equals(""An error occured as case 2 could not be found"")); } } @Don Roby: Hamcrest really annoys me. I think it's because it's a fluent interface sort of thing and fluent interfaces really annoy me. But clearly yes @Swift-Tuttle should consider Hamcrest and see if he likes it. Well thats what I was thinking. I cant predict. The scenarios would be random. Will try to implement. Thanks. +1: This will work though I prefer Hamcrest."
686,A,"Junit Ant Task output stack trace I have a number of tests failing in the following JUnit Task.  <target name=""test-main"" depends=""build.modules"" description=""Main Integration/Unit tests""> <junit fork=""yes"" description=""Main Integration/Unit Tests"" showoutput=""true"" printsummary=""true"" outputtoformatters=""true""> <classpath refid=""test-main.runtime.classpath""/> <batchtest filtertrace=""false"" todir=""${basedir}""> <fileset dir=""${basedir}"" includes=""**/*Test.class"" excludes=""**/*MapSimulationTest.class""/> </batchtest> </junit> </target> How do I tell Junit to ouput the errors for each test so that I can look at the stack trace and debug the issues. The answer was to add the tag within the tag.  <target name=""test-main"" depends=""build.modules"" description=""Main Integration/Unit tests""> <junit fork=""yes"" description=""Main Integration/Unit Tests"" showoutput=""true"" printsummary=""true"" outputtoformatters=""true""> <classpath refid=""test-main.runtime.classpath""/> <batchtest filtertrace=""false""> <fileset dir=""${basedir}/out/test/common"" includes=""**/*Test.class"" excludes=""**/*MapSimulationTest.class""/> <fileset dir=""${basedir}/out/test/test-simulation"" includes=""**/*Test.class"" excludes=""**/*MapSimulationTest.class""/> </batchtest> <formatter type=""brief"" usefile=""false""/> </junit> </target>  You'll need to add the formatter task as a child of the batchtest task (NOT as the immediate child of the junit task) The syntax of formatter is: <formatter type=""plain"" usefile=""false""/> type can be one of plain brief xml or failure. usefile=""false"" asks Ant to send output to the console. Scroll down to the h4 on ""formatters"" at http://ant.apache.org/manual/Tasks/junit.html for more details. At least with ant 1.9.0 you can also add the formatter as a child of the junit task. That would be helpful if you have several batch tasks."
687,A,"Netbeans - Source classes don't have access to Test classes? I have a NetBeans project set up with a bunch of source classes and about 10 jUnit test classes in a separate Test folder. From within the Test files I can import any other test file or source class. However from within the normal source files NetBeans acts as if the Test classes don't exist. Autocomplete on them won't work and if I attempt to use them I get a compile error. I understand that normally it wouldn't make sense to use a Test class from the regular source but I have a good reason in this case. Part of my program accepts a class name as a string and creates and instance of it with reflection. One of my jUnit tests calls this method to test it and passes it the name of a Test class. This always fails because the normal program code can't find any of the classes from the Test folder. Could you post the stacktrace? Perhaps the real problem is a typo in the classname that you are passing in? I would have expected them to run in the same classpath at runtime so it should be possible to reference back to the test classes through reflection. I wonder if there is something else going on here. I ran into a very similar issue and eventually discovered that the mock objects implemented under Test folder simply weren't being compiled because I was never directly instantiating them. In other words my code would fail here: Class gatewayClass = Class.forName(System.getProperty(""com.ov.MockGateway"")); Even though I had test/com/ov/MockGateway.java set up correctly. Since I was not explicitly creating an instance of MockGateway NetBeans helpfully skipped compiling it. If I compiled the file manually by selecting it from the Projects view and pressing F9 then my test ran just fine. My work-around was to add the following code into my unit test forcing NetBeans to compile the mock object class: public void doNothing() { // Force the compiler to build MockGateway.class // by including a do-nothing reference to it. MockGateway mock = new MockGateway(); }  While running a test the classpath will include the source and test classes. They must and I don't think it would really be possible to prevent one from accessing the other at runtime even with class loaders. I certainly highly doubt that Netbeans is doing it even if it is theoretically possible. There are two possibilities that I could guess at. One is that the error is not what you think it is. If you post a stack trace we could help you with that. The other is that you are using the wrong class loader to load the class (i.e. not just using Class.forName()). Can you post the code snippit doing the class loading? +1 for being more helpful than my answer.  I'm not sure if this is going to be the majority opinion but here's my take: if your program is failing because you can't access the test classes from your source classes you're doing something wrong. I don't care if you think you have a good reason you don't. (Well I think you don't.) Maybe you need to move a test class into the project proper. Or in your case you might need to configure the classpath for testing... I can't say definitively how I'd recommend solving the situation without having access to your code. From the point of view of the source classes the test classes really shouldn't seem to exist unless the project is being tested. I guess I wasn't completely clear. This problem does occur during testing. I have a jUnit test which is testing a method to instantiate classes based on a class name passed as a string. The class I want it to instantiate during testing is another Test class which shouldn't be included in the regular source. Agreed. If your using reflection to access the class then you should have no specific reference to whatever class your loading in WHICH case you should not get a compile error you should get a runtime error instead. That is Agreed with David... I don't have a reference to it. In fact I get no error at compile time. I was just using that as an example to show that the Test classes can't be accessed even when hardcoding it. @takteek: I understand your situation. Your question wasn't phrased in the best way I think (titling it ""Source classes don't have access to test classes"" makes us think ""well duh what's wrong with that?""). I'd suggest focusing on the class loader specifically why it's not finding your test class during the test you're running."
688,A,"Is there a way to make Eclipse run a JUnit test multiple times until failure? We occasionally have bugs that appear once in every X runs. Before people check in stuff (where it is automatically JUnit'd) our devs need to pass JUnit locally via Eclipse. Is there some convenient way (built in or high-quality Plugin) to make Eclipse run the same test X times and stop if there's a failure? An alternative to just clicking Run X times? Note that I'm looking for something in the UI (e.g. right click and say ""Run X times"" instead of just ""Run""). This is not really relevant to your question but my curiosity is piqued - what kind of bugs are only appearing every so many runs? Is there some random element in the tests? Multiple threads maybe? I assume you've already tried to eliminate the randomness somehow. If you use Spring (or if the devs can add Spring locally) you can annotate the tests with @Repeat(number_of_runs). @Peter: Primarily things like race conditions and unclean states. We're testing one part of a system against a bunch of services that are outside our control. Have a look here: http://stackoverflow.com/questions/8805305/in-eclipse-how-do-i-run-a-junit-test-case-multiple-times/9057363#9057363 Here is a post I wrote that shows several ways of running the tests repeatedly with code examples: http://codehowtos.blogspot.com/2011/04/run-junit-test-repeatedly.html You can use the @Parametrized runner or use the special runner included in the post There is also a reference to a @Retry implementation  I don't believe there's a built in way for junit to do exactly what you're asking for. If multiple runs produce different result you should have a unit test testing that case. Wich might be as simple as running a for loop in the relevant test cases. Yea but we've got a whole test suite where multiple people keep adding tests... On the build server we do have a loop... I was hoping Eclipse had something.  If you really want to run a test class until failure you need your own runner. @RunWith(RunUntilFailure.class) public class YourClass { // .... } which could be implemented as follows... package com.example; import org.junit.internal.runners.*; import org.junit.runner.notification.*; import org.junit.runner.*; public class RunUntilFailure extends Runner { private TestClassRunner runner; public RunUntilFailure(Class<?> klass) throws InitializationError { this.runner = new TestClassRunner(klass); } @Override public Description getDescription() { Description description = Description.createSuiteDescription(""Run until failure""); description.addChild(runner.getDescription()); return description; } @Override public void run(RunNotifier notifier) { class L extends RunListener { boolean fail = false; public void testFailure(Failure failure) throws Exception { fail = true; } } L listener = new L(); notifier.addListener(listener); while (!listener.fail) runner.run(notifier); } } ...releasing untested code feeling TDD guilt :)  Based on @akuhn's answer here is what I came up with - rather than running forever this will run 50 times or until failure whichever comes first. package com.foo import org.junit.runner.Description; import org.junit.runner.Runner; import org.junit.runner.notification.Failure; import org.junit.runner.notification.RunListener; import org.junit.runner.notification.RunNotifier; import org.junit.runners.BlockJUnit4ClassRunner; import org.junit.runners.model.InitializationError; public class RunManyTimesUntilFailure extends Runner { private static final int MAX_RUN_COUNT = 50; private BlockJUnit4ClassRunner runner; @SuppressWarnings(""unchecked"") public RunManyTimesUntilFailure(final Class testClass) throws InitializationError { runner = new BlockJUnit4ClassRunner(testClass); } @Override public Description getDescription() { final Description description = Description.createSuiteDescription(""Run many times until failure""); description.addChild(runner.getDescription()); return description; } @Override public void run(final RunNotifier notifier) { class L extends RunListener { boolean shouldContinue = true; int runCount = 0; @Override public void testFailure(@SuppressWarnings(""unused"") final Failure failure) throws Exception { shouldContinue = false; } @Override public void testFinished(@SuppressWarnings(""unused"") Description description) throws Exception { runCount++; shouldContinue = (shouldContinue && runCount < MAX_RUN_COUNT); } } final L listener = new L(); notifier.addListener(listener); while (listener.shouldContinue) { runner.run(notifier); } } }  I know it doesn't answer the question directly but if a test isn't passing every time it is run it is a test smell known as Erratic Test. There are several possible causes for this (from xUnit Test Patterns): Interacting Tests Interacting Test Suites Lonely Test Resource Leakage Resource Optimism Unrepeatable Test Test Run War Nondeterministic Test The details of each of these is documented in Chapter 16 of xUnit Test Patterns.  If the for loop works then I agree with nos. If you need to repeat the entire setup-test-teardown then you can use a TestSuite: Right-click on the package containing the test to repeat Go to New and choose to create a JUnit test SUITE Make sure that only the test you want to repeat is selected and click through to finish. Edit the file to run it multiple times. In the file you just find the addTestSuite(YourTestClass.class) line and wrap that in a for loop. I'm pretty sure that you can use addTest instead of addTestSuite to get it to only run one test from that class if you just want to repeat a single test method. Yea I tried something like this. But I was hoping for something directly in the UI. is it a Junit 3 only solution ? I wouldn't be surprised if it's different in JUnit 4 (I haven't tried it). But the basic idea is to let Eclipse make the first instance of the test and then you replicate what they did multiple times so I would think there would be an equivalent JUnit 4 solution"
689,A,JUnit - testing a web site I'm very new to JUnit but I want to set up some tests which does the following.. Tests a range of server to server API calls - verifying the responses are correct - I can do that fine. Open a web page enter data onto it and verify what happens on submit - This I am struggling with. Is it even possible? I am thinking that I could call a web page using a server side http web request but I'm not sure how I can interact with the site itself i.e. enter data into the forms. Any thoughts? Thanks Steve I suggest you to try the Robot Framework. This is an open source testing framework developed by engineers in Nokia Siemens Networks. It is primarily built on python and the Selenium testing libraries. It also includes support for testing Java/J2EE server side code through Jython libraries. I personally use it in my work sometimes and writing a test case is just as easy as describing an end-to-end flow through the use of Keywords (most of required ones are already inbuilt). You could go ahead and give this a shot if you find Selenium a li'l tough to work with. The Robot framework provides a fairly simple abstraction over raw selenium coupled with the power to make Java/J2EE server-side calls too. Regards Nagendra U M  Have a look at Selenium it's a system to test web applications (and de facto websites) you can write all your tests in java. There is an ather project named Tellurium based on Selenium but Tellurium works with groovy and a DSL it might be easier to handle at first. How does this works ? First you create tests in java (Selenium) or groovy (Tellurium) Then you start your tests. It will work with your web browser. The application will interact with your browser to test every inch of your application (as you coded it) At the end it give you a report about yours tests just as JUnit do. Great thanks Colin (and Joachim) I will give Selenium a try.  You can also exploit the nature of the web. There's no real reason to render a form fill it out and submit it to test the form processing code. The display of the form is one HTTP request and the submission is another. It's perfectly reasonable to test form submission code by mocking up what a browser would send and asserting that it's handled correctly. You do need to make sure that the form rendering and submission test code are in sync but you don't necessarily need a full integration for this either. There are tools that allow testing without booting up a browser... one that springs to mind is HTMLUnit (and there are others). If you find that Selenium is a pain to write or the tests brittle or flakey look for simpler tools like this.  You could use Selenium for this. I suggest you use the version 2 which is currently in development and should have a beta available soon (alphas are already available). +1 for recommending version 2. It is waaaay better than previous releases.
690,A,"How to override/control the way the JVM gets the system date? How do you mock-up/trick the JVM to get a date other that the current system date? I have a set of tests in JUnit I don't want to change but instead I want to change a setting so that when the JVM retrieves the date it retrieves the date I want. Have you done something similar before? Thanks. Do you want to override System.currentTimeMillis()? A static native method? Not that one but maybe the way a Date object is initialized by default. You could have a DateFactory interface with two implementations one which always returns new Date() (this would be used in production) and a mock object which returns Date objects which are more appropriate for your unit tests. (This is assuming you can change your code to make it more amenable to unit testing.) I doubt I could introduce a DateFactory in my case but I'll analyze it.  You could write a TestRule for it. Something like this might work: import org.joda.time.DateTime; import org.joda.time.DateTimeUtils; import org.junit.rules.TestRule; import org.junit.runner.Description; import org.junit.runners.model.Statement; public class FixedDateTimeRule implements TestRule { private final DateTime fixedDate; public FixedDateTimeRule(DateTime fixedDate) { this.fixedDate = fixedDate; } /* * (non-Javadoc) * * @see org.junit.rules.TestRule#apply(org.junit.runners.model.Statement org.junit.runner.Description) */ @Override public Statement apply(final Statement base Description description) { return new Statement() { @Override public void evaluate() throws Throwable { DateTimeUtils.setCurrentMillisFixed(fixedDate.getMillis()); try { base.evaluate(); } finally { DateTimeUtils.setCurrentMillisSystem(); } } }; } } @Rule public FixedDateTimeRule fixedDateTime = new FixedDateTimeRule(new DateTime(...)) @Test public void someKindOfTestThatNeedsAFixedDateTime() { ... }  There are a couple of ways to do this: if you can rewrite code - as mentioned in some other answers basically you need a configurable NowProvider DateFactory or other strategy (I like injectable NowProvider.now() as a dropin replacement for System.currentTimeMillis myself) Powermock and some other toolkits let you use ClassLoader tricks to override static methods even system methods Powermock works great. Just used it to mock System.currentTimeMillis(). Thanks jayshao for the hint. Thanks jayshao Powermock provided me a very good option to handle this problem. Powermock? Mmm... seems like it's worth a try if I can override System.currentTimeMillis while running my unit tests then this may be the answer I'll let you know later about the outcome. Glad it worked out for you - I'm a little uneasy about classloader manipulations in unit tests in principle but there are a lot of cases where injecting data providers is just cruft IMHO. @jayshao - I could sympathise with this opinion but (a) you only really tend to want inject something as seemingly _weird_ as a wall-clock time provider when your code is genuinely time sensitive (e.g. testing a cache that has expiries) (b) you can always provide a constructor overload that doesn't expose that factory as a param that just defaults to use the normal system provider so usage is still natural for users of your class. So on balance I'd rather inject. I didnt know about Powermock. Great suggestion jayshao as it looks really good  I would rather try to hide that functionality behind a mockable interface so that the ""real"" implementation gets the system date while your mock implementation returns whatever date you have configured. Messing with the system date in unit tests doesn't sound a very nice thing to me - I would try to avoid it if I can. Unit tests should be context-independent as much as possible - that saves a lot of trouble in the long run. However if you can't modify the code you are testing you may have no other way. The question then is are you interested only in the date or the full timestamp? The latter case sounds pretty hopeless to me to achieve (in any other way than the above mock interface) as I guess simply resetting the date before running your tests is not enough - you need a fixed instant of time to be returned throughout your unit tests. Maybe your tests run fast enough to finish before the clock ticks maybe not - maybe they pass right now then start to fail randomly sometime later after adding the next test :-( If you only need the date though you might try changing the date in the @BeforeClass method then resetting it in @AfterClass although it is gross. The interface idea sounds good a change like this one may not conflict with some limitations I have to accept. I'll let you know later."
691,A,A need a recommendation for a book on test driven programming with jee6? Anybody knows a good book that can help me understand test driven programming and how it is done in jee6? Ive never really used that approach(or at least correctly). I am very curious abut this topic i would like to give it a try in my java web projects. So if any good literature recommendation please let me know. In this question you will find some good answers: http://stackoverflow.com/q/2728360/212952 You don't need a book for that to my opinon. You need to know how to create and run unit tests (for different programming languages and frameworks). The idea of test driven development (TDD) is fairly simple: read the requirements (no requirements -> no TDD) write a test that tests the requirements. The test will fail initially. write code until the test passes (and don't write any extra stuff!) Ok thanks for the advise i will have a look around the web i see if i find some examples of tests in jUnit or something. Do you know any good website with examples and related stuff?
692,A,JUnit: Tool to compare result reports I'd like to compare two or more JUnit test results reports. Say I have three test runs of the same testsuite and I want to get e.g. a HTML table which shows one test per row with a column for each test run informing about the status of the test evt. the first line of the failure reason. I doubt there's not such tool already but I haven't found googling. Thanks Ondra If it's a maven project try out the surefire plugin. It typically reports like how you described. I'm not sure how to get comparison of more reports - could you give an example please?  I've created such tool. http://ondra.zizka.cz/stranky/programovani/java/apps/JUnitDiff-junit-test-results-report-comparison.texy Case closed.
693,A,"Accessing the attributes of a model contained in a ModelAndView object from the context of a controller test I am new to Spring MVC and I'm in the process of learning how to test my controllers. I have a simple test: @Test public void shouldDoStuff() { request.setRequestURI(""/myCompany/123""); ModelAndView mav = controller.getSomeDatas(""123"" request); assertEquals(mav.getViewName() ""company""); assertTrue(mav.getModel().containsKey(""companyInfo"")); assertTrue(mav.getModel().containsKey(""rightNow"")); assertEquals(mav.getModel().get(""companyInfo"") ""123""); } Here's my controller action: @RequestMapping(value = ""/myCompany/{companyGuid}"" method = RequestMethod.GET) public ModelAndView getSomeDatas(@PathVariable(""companyGuid"") String myGuid HttpServletRequest request) { /*ModelAndView mav = new ModelAndView(""company""); mav.addObject(""companyInfo"" myGuid); mav.addObject(""rightNow"" (new Date()).toString()); return mav;*/ Map<String Object> myModel = new HashMap<String Object>(); myModel.put(""companyInfo"" myGuid); myModel.put(""rightNow"" (new Date()).toString()); return new ModelAndView(""company"" ""model"" myModel); } I have a breakpoint set on the first assert. In the Display window in Eclipse mav.getModel() returns exactly what I'd expect: mav.getModel() (org.springframework.ui.ModelMap) {model={rightNow=Fri Nov 05 13:30:57 CDT 2010 companyInfo=123}} However any attempt to access the values in that model fails. For example I assumed the following would work: mav.getModel().get(""companyInfo"") null mav.getModel().containsKey(""companyInfo"") (boolean) false But as you can see get(""companyInfo"") returns null and containsKey(""companyInfo"") returns false. When I swap out the commented section of the controller with the uncommented section my tests work just fine but then my jsp view breaks because I'm trying to access properties of the model by saying things like ${model.companyInfo} etc. So I need to know at least one of two things (but better if you can answer both): If I leave the controller as shown how can I access the attributes of the model in my test? If I swap out the commented section for the uncommented section how can I access the attributes of the model in my jsp view? Any help is appreciated. Well the obvious answer is that your controller is broken and the test is doing its job. Without seeing your controller code that's all I can suggest I've updated my question to include my controller code. Either way the fact that the object clearly contains the value I seek seems rather contradictory to the fact that I can't retrieve this value from it wouldn't you agree? @pedrofalcaocosta I'm giving your answer an up vote because it helped me find my answer but I think it's appropriate to answer my own question here: ((java.util.HashMap<StringObject>)mav.getModel().get(""model"")).get(""companyInfo"") Sorry by the incomplete answer but the update to your question make me a little confuse... I knew the problem was in 'model' name. Anyway... your're welcome!  Ok now its clear! Try: mav.getModel().get(""model""); mav.getModel().containsKey(""model""); You called your modelmap 'model' in your controller... In your jsp i would recommend using Jstl: <%@page contentType=""text/html; charset=utf-8"" pageEncoding=""UTF-8"" language=""java""%> <%@taglib prefix=""c"" uri=""http://java.sun.com/jsp/jstl/core""%> <!DOCTYPE html SYSTEM ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""> <html xmlns=""http://www.w3.org/1999/xhtml""> <body> ${model.companyInfo} </body> </html> thanks for the tip it was helpful but not quite there :) It did help me find the answer but I've posted the actual answer in hopes that it will help other developers experiencing the same problem.  For question 1 Model provides a method that returns the model attributes as a map. In your test you can do: Map<StringObject> modelMap = mav.getModel().asMap(); modelMap.get(""companyInfo""); Assuming you set companyInfo into the model it should be there. As for part2 of the question I think someone else answered that already.  You forgot to call the constructor of ModelAndView with the viewname and you forgot to add your objects to the model. I think you code should look something like this... @Test public void shouldDoStuff() { request.setRequestURI(""/myCompany/123""); // call the constructor with the name of your view ModelAndView mav = new ModelAndView(""viewName""); // add the objects to the model mav.addAllObjects(controller.getSomeDatas(""123"" request)); assertEquals(mav.getViewName() ""viewName""); assertTrue(mav.getModel().containsKey(""companyInfo"")); } If you need to add more than one object with custom keys use the addObject method instead;  mav.addObject(""key1"" 1); mav.addObject(""key2"" 2); I can see why this would work but what I don't understand is this: getSomeDatas() is supposed to return a ModelAndView object. The object it's returning clearly contains the values I've added to it as is shown above. Why can't I retrieve these values from it? Why should I create a new ModelAndView object in my test and add objects from the controller when my controller is supposed to RETURN this ModelAndView object with the values contained within?"
694,A,"Is there a version of JUnit assertThat which uses the Hamcrest 'describeMismatch' functionality? In every version of JUnit I have tried (up to 4.8.1) a failing assertThat will display an error message that looks like: expected: [describeTo] got: [String representation of object] In other words it will display the toString() of the object instead of the mismatch description from the Matcher. If I use the assertThat from org.hamcrest.MatcherAssert.assertThat then it will call 'describeMismatch' and display a more helpful error message. Am I using Junit incorrectly or is there currently no version of JUnit that will do what I want? Do most people use the Hamcrest assertThat then? Use the other version assertThat(String T Matcher<T>) and in the first argument write your own message that will give you a better description of the failure. You are welcome Jacob. Thanks Boris. That is one nice way to create a description. However I would prefer to use the build-in error message that the Matcher can generate via its ""describeMismatch"" method.  Short answer: no. As far as I can tell the most recent version of Hamcrest (1.2) has introduced type signatures which are incompatible with version 1.1 which JUnit currently depends on. I am not sure the extent of the damage (so to speak) created by the change in Hamcrest but it does not appear that the JUnit team are in any hurry to upgrade (see the open issue). I am not entirely sure I have solved my issue but I am planning to use MatcherAssert.assertThat(). This can require a specific release of JUnit (junit-dep-xxx I believe) which will not have classpath conflicts with Hamcrest. Otherwise you may receive NoSuchMethodErrors when assertThat() makes the call to describeMismatch(). Thanks. Do you use the Hamcrest assertThat then or what? Hey Jacob hopefully my edit answers that question :)"
695,A,"How to run all tests belonging to a certain Category in JUnit 4 JUnit 4.8 contains a nice new feature called ""Categories"" that allows you to group certain kinds of tests together. This is very useful e.g. to have separate test runs for slow and fast tests. I know the stuff mentioned in JUnit 4.8 release notes but would like to know how I can actually run all the tests annotated with certain category. The JUnit 4.8 release notes show an example suite definition where SuiteClasses annotation selects the tests from certain category to run like this: @RunWith(Categories.class) @IncludeCategory(SlowTests.class) @SuiteClasses( { A.class B.class }) // Note that Categories is a kind of Suite public class SlowTestSuite { // Will run A.b and B.c but not A.a } Does anyone know how I could run all the tests in SlowTests category? It seems that you must have the SuiteClasses annotation... Hi. I have a question that is related. feel free to chime in: http://stackoverflow.com/questions/15776718/using-junit-categories-vs-simply-organizing-logical-test-categories-in-separate To run categorized tests without specifying all of them explicily in @Suite.SuiteClasses annotation you can provide your own implementation of Suite. For example a org.junit.runners.ParentRunner can be extended. Instead of using an array of classes provided by @Suite.SuiteClasses new implementation should perform search for categorized tests in classpath. See this project as an example of such approach. Usage: @Categories(categoryClasses = {IntegrationTest.class SlowTest.class}) @BasePackage(name = ""some.package"") @RunWith(CategorizedSuite.class) public class CategorizedSuiteWithSpecifiedPackage { }  One downside to Kaitsu's solution is that Eclipse will run your tests twice and the SlowTests 3 times when running all the tests in a project. This is because the Eclipse will run all the tests then the AllTests suite then the SlowTestSuite. Here is a solution that involves creating subclasses of the Kaitsu solution test runners to skip the suites unless a certain system property is set. A shameful hack but all I have come up with so far. @RunWith(DevFilterClasspathSuite.class) public class AllTests {} . @RunWith(DevFilterCategories.class) @ExcludeCategory(SlowTest.class) @SuiteClasses(AllTests.class) public class FastTestSuite { } . public class DevFilterCategories extends Suite { private static final Logger logger = Logger .getLogger(DevFilterCategories.class.getName()); public DevFilterCategories(Class<?> suiteClass RunnerBuilder builder) throws InitializationError { super(suiteClass builder); try { filter(new CategoryFilter(getIncludedCategory(suiteClass) getExcludedCategory(suiteClass))); filter(new DevFilter()); } catch (NoTestsRemainException e) { logger.info(""skipped all tests""); } assertNoCategorizedDescendentsOfUncategorizeableParents(getDescription()); } private Class<?> getIncludedCategory(Class<?> klass) { IncludeCategory annotation= klass.getAnnotation(IncludeCategory.class); return annotation == null ? null : annotation.value(); } private Class<?> getExcludedCategory(Class<?> klass) { ExcludeCategory annotation= klass.getAnnotation(ExcludeCategory.class); return annotation == null ? null : annotation.value(); } private void assertNoCategorizedDescendentsOfUncategorizeableParents(Description description) throws InitializationError { if (!canHaveCategorizedChildren(description)) assertNoDescendantsHaveCategoryAnnotations(description); for (Description each : description.getChildren()) assertNoCategorizedDescendentsOfUncategorizeableParents(each); } private void assertNoDescendantsHaveCategoryAnnotations(Description description) throws InitializationError { for (Description each : description.getChildren()) { if (each.getAnnotation(Category.class) != null) throw new InitializationError(""Category annotations on Parameterized classes are not supported on individual methods.""); assertNoDescendantsHaveCategoryAnnotations(each); } } // If children have names like [0] our current magical category code can't determine their // parentage. private static boolean canHaveCategorizedChildren(Description description) { for (Description each : description.getChildren()) if (each.getTestClass() == null) return false; return true; } } . public class DevFilterClasspathSuite extends ClasspathSuite { private static final Logger logger = Logger .getLogger(DevFilterClasspathSuite.class.getName()); public DevFilterClasspathSuite(Class<?> suiteClass RunnerBuilder builder) throws InitializationError { super(suiteClass builder); try { filter(new DevFilter()); } catch (NoTestsRemainException e) { logger.info(""skipped all tests""); } } } . public class DevFilter extends Filter { private static final String RUN_DEV_UNIT_TESTS = ""run.dev.unit.tests""; @Override public boolean shouldRun(Description description) { return Boolean.getBoolean(RUN_DEV_UNIT_TESTS); } @Override public String describe() { return ""filter if ""+RUN_DEV_UNIT_TESTS+"" system property not present""; } } So in your FastTestSuite launcher just add -Drun.dev.unit.tests=true to the VM arguments. (Note that this solution references a fast test suite instead of a slow one.)  I found out one possible way to achieve what I want but I don't consider this to be the best possible solution as it relies on ClassPathSuite library that is not part of JUnit. I define the test suite for slow tests like this: @RunWith(Categories.class) @Categories.IncludeCategory(SlowTests.class) @Suite.SuiteClasses( { AllTests.class }) public class SlowTestSuite { } AllTests class is defined like this: @RunWith(ClasspathSuite.class) public class AllTests { } I had to use ClassPathSuite class from ClassPathSuite project here. It will find all the classes with tests. Thank you this helped me a lot. It's actually a quite reasonable sollution. Thanks for anwesering your own question since it's a really good one :-) For anyone wondering how to automate running a category of tests (with this exact setup) using Ant [this question](http://stackoverflow.com/questions/6226026/how-to-run-all-junit-tests-in-a-category-suite-with-ant) might be useful. As a compliment to my question http://stackoverflow.com/q/2698174/59470 and detailed explanation I added a blog entry: http://novyden.blogspot.com/2011/06/using-junit-4-categories-to-replace.html  Here are some of the main differences between TestNG and JUnit when it comes to groups (or categories like JUnit calls them): JUnit's are typed (annotations) while TestNG's are strings. I made this choice because I wanted to be able to use regular expressions when running tests for example ""run all the tests that belong to the group ""database*"". Also having to create a new annotation whenever you need to create a new category is annoying although it has the benefit that an IDE will tell you right away where this category is used (TestNG shows you this in its reports). TestNG separates very clearly your static model (the code of your tests) from the runtime model (which tests get run). If you want to run the groups ""front-end"" first and then ""servlets"" you can do this without having to recompile anything. Because JUnit defines groups in annotations and you need to specify these categories as parameters to the runner you usually have to recompile your code whenever you want to run a different set of categories which defeats the purpose in my opinion. We built our own categories support into our JUnit tests in a very similar way to JUnit the main difference being that instead of the @Categories.IncludeCategory annotation we made ours configurable via a system property. Why this was too hard for JUnit to do for us is anybody's guess.  I am not sure what exactly your problem is. Just add all the tests to a suite (or hirachy of suites). Then use the Categories Runner and Include/ExcludeCategory annotation to specify the categories you want to run. A good idea might be to have one suite containing all the tests and a couple of seperate suites referring to the first one specifying the different set of Categories you neeed. My problem is that I have thousands of tests and I don't want to manually add them to any suites. I just want that tests with certain category are run. It shouldn't be so hard for JUnit to find out which tests have certain annotation as it actually does that anyway when finding test methods.  Not a direct answer to your problem but maybe the general approach could be improved... Why are your tests slow? Maybe the set-up lasts long (database I/O etc.) maybe the tests are testing too much? If this is the case I would seperate the real unit-tests from the ""long-running"" ones which often indeed are integration tests. In my setups I have staging env where unit-tests are run often and integration-tests constantly but more rarely (e.g. after each commit in version control). I have never worked with grouping for unit tests because they should be loosely coupled alltogether. I only work with grouping and relationship of test-cases in integration-test setups (but with TestNG). But good to know that JUnit 4.8 introduced some grouping features. Thanks Manuel for your comments! I don't really need to separate unit tests but I use JUnit also for integration tests and want to separate them from unit tests. I've looked also at TestNG and it seems to make testing (and not just unit testing) nicer than JUnit. And it also has better documentation and a good book."
696,A,"Google's Android HelloWorldTest fail - receives null pointer on retrieving resource I'm using the HelloAndroidTest tutorial from Google: http://developer.android.com/resources/tutorials/testing/helloandroid_test.html. Here's the test class: package com.example.helloandroid.test; import com.example.helloandroid.HelloAndroid; import android.test.ActivityInstrumentationTestCase2; import android.widget.TextView; public class HelloAndroidTest extends ActivityInstrumentationTestCase2<HelloAndroid> { private HelloAndroid mActivity; private String resourceString; private TextView mView; public HelloAndroidTest() { super(""com.example.helloandroid"" HelloAndroid.class); } protected void setUp(TextView mView) throws Exception { super.setUp(); mActivity = this.getActivity(); mView = (TextView) mActivity .findViewById(com.example.helloandroid.R.id.textview); resourceString = mActivity .getString(com.example.helloandroid.R.string.hello); } public void testPreconditions() { assertNotNull(mView); // <== always null //System.out.println(""Resourse string: "" + resourceString); //assertNotNull(resourceString); // <== always null (when run) } public void testText() { assertEquals(resourceString (String) mView.getText()); } } Here's the HelloAndroid class: package com.example.helloandroid; import android.app.Activity; import android.os.Bundle; public class HelloAndroid extends Activity { /** Called when the activity is first created. */ @Override public void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.main); } } This is main.xml: <?xml version=""1.0"" encoding=""utf-8""?> <TextView android:id=""@+id/textview"" xmlns:android=""http://schemas.android.com/apk/res/android"" android:layout_width=""fill_parent"" android:layout_height=""fill_parent"" android:text=""@string/hello""/> And strings.xml: <?xml version=""1.0"" encoding=""utf-8""?> <resources> <string name=""hello"">Hello Android!</string> <string name=""app_name"">Hello Android</string> </resources> Both mView and resource string fail their respective notNull tests. This is pretty basic but it does require an activity to be successfully created and the resource pulled from the HelloAndroid project which is the functionality I need to get on with unit testing. Any ideas on how to fix this? so mActivity is not null just to prove it add assertnotNull(mActivity) Right in this case the assert not null fails on a null pointer adjusted the title. assertNotNull fails or do you receive a NPE (as title mentions)? I think I've got it. It looks like the activity needs to be created in the test method itself. Once I moved it there it works fine. The getActivity documentation actually states something to that effect which is what finally clued me in. In the words of the immortal MLK - Free at last free at last - free at last. Although that fixed it when I redid the HelloWorldTest it worked fine and didn't need this fix. I haven't had time to track what I did wrong in the first place.  Looking at your first code submission I don't think setUp takes any parameters therefore your overridden method with a parameter would never get called and therefore all your instance vars are null."
697,A,"Junit Dynamically Created Tests Not Working public class NewTest extends SeleneseTestCase { public static Test suite() throws Exception { TestSuite suite = new TestSuite(); TestSuite s = new TestSuite(""TestCase Name""); GeneratedTest t = new GeneratedTest(""testName""); t.setFailure(""TestCase Name: testName""); s.addTest(t); t = new GeneratedTest(""testAge""); s.addTest(t); suite.addTest(s); s = new TestSuite(""TestCase Name2""); t = new GeneratedTest(""testOOGABOOGA""); t.setFailure(""TestCase Name2: testOOGABOOGA""); s.addTest(t); suite.addTest(s); s = new TestSuite(""TestCase Name4""); t = new GeneratedTest(""testName""); t.setFailure(""TestCase Name4: testName""); s.addTest(t); t = new GeneratedTest(""testAge""); s.addTest(t); suite.addTest(s); s = new TestSuite(""TestCase Name3""); t = new GeneratedTest(""testName""); t.setFailure(""TestCase Name3: testName""); s.addTest(t); t = new GeneratedTest(""testAge""); s.addTest(t); suite.addTest(s); return suite; } } public class GeneratedTest extends TestCase { public String testFailMessage; public GeneratedTest(String name) { ((TestCase)this).setName(name); } public void runTest() { if (testFailMessage != null) { fail(testFailMessage); } } public void setFailure(String msg) { testFailMessage = msg; } } As you can see (or maybe you can't) i'm adding tests to junit at runtime. This is all fine and dandy except that it doesn't properly display them. Here see what I mean: click here for image As you can see tests with the same name don't even display that they've been run except for the last test with duplicate name and that test has the error messages from all the other tests with the same name. Is this simply just a flaw with the way that i'm doing it (junit3 style)? Would I have to change it to use junit4 parameterization to fix it? if you have all means to switch to junit4 (JDK5 or higher required) then you should do just that. Parametrized tests are definitely the answer in your case. I noticed something similar in Eclipse's test runner. For JUnit 3.8 style parametrized tests the names were not being displayed. Switching to JUnit 4 style solved the problem. While this isn't exactly your scenario I think it is something you'll have to live with until you can update the tests to JUnit 4. Eclipse does still run the tests which is the important thing."
698,A,Unit testing of GWT RequestFactory services without GWTTestCase Somewhere I don't remember where I spotted information that starting from GWT 2.1.1 it is possible to test ReqeustFactory services without GWTTestCase. If this is true please show me how. I found it myself browsing GWT's source code. The answer is hidden in InProcessRequestTransport javadoc: http://code.google.com/p/google-web-toolkit/source/browse/trunk/user/src/com/google/web/bindery/requestfactory/server/testing/InProcessRequestTransport.java Broken link.... @HDave The whole RequestFactory code was moved to com.google.web.bindery package the link is fixed now  The RequestFactorySource type can be used to instantiate RequestFactory instances in non-GWT runtimes. The previously-mentioned InProcessRequestTransport is used by GWT's own RequestFactoryJreSuite tests to avoid the need to fire up an entire GWT runtime environment. The RequestFactorySource type isn't limited to just testing. If you implement your own RequestTransport (perhaps based on a java.net.HttpUrlConnection or Apache HttpClient library) you can write console apps bulk-query apps or health probers using your production RequestFactory endpoints. This is a huge improvement over GWT`s old RPC system which only supports GWT-based clients. The RequestFactoryMagic type has been renamed to RequestFactorySource as of svn revision r9946.
699,A,"How to deal with relative path in Junits between Maven and Intellij I have a maven project with a module /myProject pom.xml /myModule pom.xml /foo bar.txt Consider a Junit in myModule which needs to open bar.txt with maven the basedir is the module directory. So to open the file bar.txt :  new File(""foo/bar.txt"") This works well when you execute mvn test BUT when you launch the same junit in intellij it fails because Intellij sets the basedir in the project directory not the module directory. Intellij tries to open myProject/foo/bar.txt instead of myProject/myModule/foo/bar.txt Is there a way to deal with that ? Thank you! This was my exact problem If you want to keep your code you can try to change the working directory in the run/debug configuration (first entry in the combo box giving access to what you want to run) Set this to your module root. But prefer the other suggested approach: ClassLoader.getSystemResourceAsStream(youPath) Or my preferred: getClass.getResource(youPath) or getClass.getResourceAsStream(youPath) A leading '/' in path indicates the working dir of your project while no '/' indicates a relative dir to current class. I use this last solution for my tests: I put my test data resources at the same package level as the test source or in a subdir to avoid too messy package. This way I can do a simple call without complicated path and without having to deal with working directory: project-root - module A - src - test - rootfile.txt - my-complicated-package-naming-root - mypackage - Test.java - testResource.xml I can get the files this way: final URL rootfile= Test.class.getResource(""/rootfile.txt""); final URL testResource= Test.class.getResource(""testResource.xml""); thanks that's a solution I set the junit working directory to $MODULE_DIR$ and it works as expected. Unfortunatly it's not possible to use this variable in default junit configuration ! What a shame... $MODULE_DIR$ isn't suggested in the default junit configuration but you can force the value in the ""working configuration"" field avec it works! Don't use the system class loader....... getClass().getResourceAsStream(youPath) would only work if the resources are in that class's JAR file (if it was packaged in a separate JAR) - you need to do use class.getClassloader() or Thread.currentThread().contextClassLoader() to reliably load from anywhere on the classpath @iangreen: Are you really sure it's the case - would You mind quoting docs?  a) Don't use Files use InputStreams. get your InputStream via ClassLoader.getSystemResourceAsStream(""foo/bar.xml"") Most APIs that deal with Files are happy with InputStreams as well. b) Don't use foo directories use directories both maven and your IDE know about (i.e. put them in src/main/resources or src/test/resources so they are on the Class Path) c) If you have an API that absolutely needs a File not an InputStream you can still do new File(ClassLoader.getSystemResource(""foo/bar.xml"").toURI()) be careful if you are using groovy in your project: ClassLoader.getSystemResourceAsStream can stop working (return always null) because intellij starts to use a command line wrapper / dynamic proxy. See to disable this feature http://www.jguru.com/faq/view.jsp?EID=1533847. I've lost so many hours trying to figure what was happening... Thanks I'm aware of those stuff but I need to retrieve a File which can't be in the classpath because its path is configured in some parameters. I really need a solution with relative path. I think you should to use getClass().getClassLoader().getClassLoader (or the thread context class loader) - not the system class loader. that's probably why it doesn't work sometimes.  Solution inspired by Guillaume : In Run->Edit configuration->Defaults->JUnit->Working directory set the value $MODULE_DIR$ and Intellij will set the relative path in all junits just like Maven. This is my approach Perfect! You have no idea how much time this has cost me and my team where is this documented? nowhere but now new version of Intellij suggests the `MODULE_DIR` value for the `Working directory` text field Thanks This helped! classloading is so much better than file... using filesystem is evil Sometimes when testing a component that scans a file hierarchy containing some very intricate test cases this is simply the only sane thing to do. This really saved me. Thank you @tbruyelle! This should be the chosen answer Using filesystem is evil? And where do you think the eventual file comes from??? In some cases e.g. when using a C.I. Server you might want to specify source config files to be used in your test cases that don't exist in your artifact. Please enlighten me how I am supposed to get to them not using the evil file system. Thank you. Be aware that this alters your default JUnit launch configuration. Therefore it won't alter any configurations that you already have created. If this solution doesn't work for you double check which configuration you are actually using and whether it has inherited this value correctly."
700,A,"Headless integration tests of an eclipse plugin with maven tycho I wrote some Junit tests for my eclipse plugin. If I start my test suite as a ""JUnit Plug-in Test"" from Eclipse everything is working fine. Now I want to run them from Maven Tycho. So I put the following packaging : ""eclipse-test-plugin"" in the pom.xml and the integration tests start with a ""mvn clean integration-test"". So I think my maven configuration is quite OK. But some tests are failing and I suspect the headless build can't detect the IMarkers my tests are trying to detect since IMarkers are UI components. Am I right? Any idea to get my tests based on IMarkers running with Tycho? Regards Xavier http://twitter.com/#!/xavier_seignard It would be good if you could provide the error you are getting from the failed tests. I have found that looking the `[timestamp].log` file under `target/work/configuration` folder is often a good starting point You need to tell test plugin that you want to run the test with a UI by default it will run with the headless runner. <plugin> <groupId>org.eclipse.tycho</groupId> <artifactId>tycho-surefire-plugin</artifactId> <configuration> <useUIHarness>true</useUIHarness> <useUIThread>true</useUIThread> </configuration> </plugin>"
701,A,"Junit4 : expected=Exception not working with SPRING I'm trying to use the @Test(expected = RuntimeException.class) annotation in order to test for an expected exception. My code is as follows: @Test(expected = RuntimeException.class) public void testSaveThrowsRuntimeException(){ User user = domain.save(null); } and my save method simple like this : public User save(User newUser) { if(newUser == null) { throw new RuntimeException(); } //saving code goes here } after debugging the code I found that code throwing the exception as expected but its getting eaten somewhere in between in spring framework classes. I tried the same with old way (try catch block) but still I am not able to catch that exception in test and test keeps throwing errors in runafter method of Junit : org.springframework.transaction.UnexpectedRollbackException: JTA transaction unexpectedly rolled back (maybe due to a timeout); nested exception is javax.transaction.RollbackException at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1031) at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709) at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678) at org.springframework.test.context.transaction.TransactionalTestExecutionListener$TransactionContext.endTransaction(TransactionalTestExecutionListener.java:504) at org.springframework.test.context.transaction.TransactionalTestExecutionListener.endTransaction(TransactionalTestExecutionListener.java:277) at org.springframework.test.context.transaction.TransactionalTestExecutionListener.afterTestMethod(TransactionalTestExecutionListener.java:170) at org.springframework.test.context.TestContextManager.afterTestMethod(TestContextManager.java:344) at org.springframework.test.context.junit4.SpringMethodRoadie.runAfters(SpringMethodRoadie.java:307) at org.springframework.test.context.junit4.SpringMethodRoadie$RunBeforesThenTestThenAfters.run(SpringMethodRoadie.java:338) at org.springframework.test.context.junit4.SpringMethodRoadie.runWithRepetitions(SpringMethodRoadie.java:217) at org.springframework.test.context.junit4.SpringMethodRoadie.runTest(SpringMethodRoadie.java:197) at org.springframework.test.context.junit4.SpringMethodRoadie.run(SpringMethodRoadie.java:143) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.invokeTestMethod(SpringJUnit4ClassRunner.java:142) at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51) at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44) at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27) at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37) at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) Caused by: javax.transaction.RollbackException at org.objectweb.jotm.TransactionImpl.commit(TransactionImpl.java:245) at org.objectweb.jotm.Current.commit(Current.java:488) at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028) ... 23 more And I am sure this is because of that RuntimeException I am throwing in save but not able catch it or pass the test with expected clause. anybody have any idea whats going wrong? If Spring will always catch the exception why do you want to test for it not doing so? Isn't this the expected behaviour (even if not what *you* were expecting) yes thats true but it throws error on console afterward. So I want catch it myself. Turned out that my first answer was wrong. Both @Test(expected=...) and @ExpectedException work but there is some incompability between the Spring TestContext and Junit 4.5. Using Junit 4.4 solved the problem for me. Finally.  Either you're running a unit test in which case Spring TX shouldn't come in to play or you're running some kind of integration test where you want to test what the save method does when your runtime exception is swallowed. I don't think anything is going wrong you just need to make sure you understand what it is you are trying to test. thank you Paul if you look at the stack trace I am running tests with SpringJUnit4ClassRunner so Spring TX comes in play. It works with @NotTransactional annotation. Well exactly. Either run it with plain old junit and not with SpringJUnit4ClassRunner and test for runtime exception or test for expected transactional behaviour.  Here's a work-around I found to with Junit 4.5 - separate the @Transactional and @ExpectedException into nested functions. I guess the problem is something to do with the aop stuff spring puts around a @Transactional method. @Test @ExpectedException(org.springframework.dao.DataIntegrityViolationException.class) public void Test10UniqueName() { DoTest10UniqueName(); } @Transactional public void DoTest10UniqueName() { final String NAME = ""NAME""; ProductCategoryDAO dao = DAOFactory.getProductCategoryDAO(); ProductCategory test1 = new ProductCategory(); test1.setName(NAME); ProductCategory test2 = new ProductCategory(); test2.setName(NAME); dao.save(test1); dao.save(test2); }"
702,A,"Junit (3.8.1) testing that an exception is thrown (works in unit test fails when added to a testSuite) I'm trying to test that I'm throwing an exception when appropriate. In my test class I have a method similar to the following: public void testParseException() { try { ClientEntitySingleton.getInstance(); fail(""should have thrown exception.""); } catch (RuntimeException re) { assertEquals( ""<exception message>"" re.getMessage()); } } This works fine (green bar) whenever I run that single unitTest class. However when I add that test to a testSuite I get a red bar Unit test failure reported on the exception. One more thing... it works in the testSuite if it's the first test in the suite. Actually I'm doing two of these tests and just figured out that if I make them the first two tests in the suite all is good but I get this failure if a ""regular"" test precedes it. So I have a work-around but no real answer. Any ideas? Heres'a stack trace of the ""failure"" java.lang.RuntimeException: ProcEntity client dn=""Xxxxxx/Xxxx/XXX"" is defined multiple times. at com.someco.someprod.clientEntityManagement.ClientEntitySingleton.addClientEntity(ClientEntitySingleton.java:247) at com.someco.someprod.clientEntityManagement.ClientEntitySingleton.startElement(ClientEntitySingleton.java:264) at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanStartElement(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XMLParser.parse(Unknown Source) at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at com.someco.someprod.clientEntityManagement.ClientEntitySingleton.parse(ClientEntitySingleton.java:216) at com.someco.someprod.clientEntityManagement.ClientEntitySingleton.reload(ClientEntitySingleton.java:303) at com.someco.someprod.clientEntityManagement.ClientEntitySingleton.setInputSourceProvider(ClientEntitySingleton.java:88) at com.someco.someprod.clientEntityManagement.test.TestClientBase.setUp(TestClientBase.java:17) at com.someco.someprod.clientEntityManagement.test.TestClientEntityDup.setUp(TestClientEntityDup.java:8) at junit.framework.TestCase.runBare(TestCase.java:125) at junit.framework.TestResult$1.protect(TestResult.java:106) at junit.framework.TestResult.runProtected(TestResult.java:124) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:118) at junit.framework.TestSuite.runTest(TestSuite.java:208) at junit.framework.TestSuite.run(TestSuite.java:203) at junit.framework.TestSuite.runTest(TestSuite.java:208) at junit.framework.TestSuite.run(TestSuite.java:203) at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) Without seeing the rest of the code it's difficult to tell but are there other tests that are using ClientEntitySingleton and calling its getInstance method? If you have a lazy init singleton then it wouldn't be being initialized multiple times. Have you tried forking the tests into a separate JVM and see if you still have the problem? added to the question In looking at that stack trace I don't see the call to ClientSingleton.getInstance(). It actually looks like the exception is occurring as a result of the setup method. Do you have multiple tests in that class that are order dependent? Your IDE and test suite runner may run them in different orders. If you have multiple tests try ignoring all but this one then re-enable them one by one until the problem occurs. That's it!!! When I run it stand-alone the call to getInstance() triggers the parsing. Once I have the singleton the call to set the inputprovider (that's done as part of the setup()) triggers the parsing so it's not inside my try/catch. Thanks!! No problem at all. I added code to the tearDown() method to clean out the singleton. That did the trick there is no more dependency on order and the test work correctly stand-alone and as part of a testSuite Obligatory link to http://misko.hevery.com/2008/08/17/singletons-are-pathological-liars/ I've considered that there is something residual about the singleton but it remains that in both cases the code is throwing the exception. When run alone it hits my catch clause and executes the assertEquals while when it's part of a testSuite it never makes it to the assertEquals. JUnit just catches the exception and reports the exception itself as a test failure. Is there a non RuntimeException being thrown when it's part of a test suite (again maybe an effect from the singleton)? Maybe initializing the singleton multiple times is actually causing an exception. Since you're only catching RuntimeException a non RuntimeException would show up in the test suite and reported by the test runner. It's the same RuntimeException that I'm throwing on purpose (and that I catch in the test when it's stand alone). In the testSuite JUnit gets to it before my catch clause and calls it a test failure. Very odd. Hmm..that sounds strange. Is there any way you could post the stack trace of the test failure?  I would suggest that you do not catch RuntimeException which has about a thousand subclasses. Based on the name of your class it sounds as if you are only expecting ParseException. This is what you should be catching in your test. Finally assuming that you are running these tests in an IDE (because of the references to red/green bars) you should examine the failure message of the case - JUnit should report what the expected message was versus what the actual message is. This will help you diagnose what is actually going on which should help you figure out why you might get one type of behavior when the test is run isolated vs run as a group. just added more detail re: the impact of the ordering of the tests in the testSuite. Actually it is throwing a RuntimeException (not a subclass). THe error I get is not a mismatch on the assertEquals but just a test failure that the exception was raised. It works just fine running the assertEquals when run alone. It only fails when it's made part of a testSuite I will try subclassing RuntimeException throwing the subclass and catching it explicitly (it was a bit ""lazy"" just throwing RuntimeException). I'm not sure why that should make a difference but I'm also not sure why I'm seeing the difference when I add it to the testSuite. So it's worth a shot.  Just realized that I never posted the ""answer"" to this problem. The ClientEntitySingleton was set up so that on initialization it would simply cache the name of the xml file to be loaded. It is loaded on first reference and it is reloaded if you change the file name once the singleton data is loaded. So as long as the failures occurred before a successful load the parse took place during the test case (1st access). Once I had successfully loaded an XML file any future changes to the source file property triggered an immediate parse of the XML file. Unfortunately I was setting the file name in the test setup method. (That's actually right there in the stack trace.) So if you think JUnit is not working with testing exceptions this isn't your confirmation."
703,A,"Using JUnit and TestNG together Do you think it's a good idea to use JUnit and TestNG together in one project? I need some features from TestNG but I also need JUnit specific extensions like DbUnit and XmlUnit. And if I use them together do you think I should put both test package trees in the same ""test"" folder in my Eclipse project? DBUnit works fine with TestNG but nevertheless you can run both JUnit 3 and TestNG tests at the same time: all you need to do is to put your JUnit 3 classes in a tag <test junit=""true""> </test> and all your other TestNG classes in a regular: <test> </test> Look for the string ""junit"" in the documentation for more details.  Why can't you use DBUnit with TestNG? As far as I can tell it doesn't have anything specific to TestNG. Just have to define the import in TestNG's before test and afterTest. I would be surprised if XMLUnit couldn't be used in a similar matter."
704,A,"Java: How to test methods that call System.exit()? I've got a few methods that should call System.exit() on certain inputs. Unfortunately testing these cases causes JUnit to terminate! Putting the method calls in a new Thread doesn't seem to help since System.exit() terminates the JVM not just the current thread. Are there any common patterns for dealing with this? For example can I subsitute a stub for System.exit()? [EDIT] The class in question is actually a command-line tool which I'm attempting to test inside JUnit. Maybe JUnit is simply not the right tool for the job? Suggestions for complementary regression testing tools are welcome (preferably something that integrates well with JUnit and EclEmma). I'm curious as to why a function would ever call System.exit()... If you're calling a function that exits the application. For example if the user tries to perform a task they are not authorized to perform more that x times in a row you force them out of the application. I still think that in that case there should be a nicer way to return from the application rather than System.exit(). If you're testing main() then it makes perfect sense to call System.exit(). We have a requirement that on error a batch process should exit with 1 and on success exit with 0. Create a mock-able class that wraps System.exit() I agree with EricSchaefer. But if you use a good mocking framework like Mockito a simple concrete class is enough no need for an interface and two implementations. Stopping test execution on System.exit() Problem: // do thing1 if(someCondition) { System.exit(1); } // do thing2 System.exit(0) A mocked Sytem.exit() will not terminate execution. This is bad if you want to test that thing2 is not executed. Solution: You should refactor this code as suggested by martin: // do thing1 if(someCondition) { return 1; } // do thing2 return 0; And do System.exit(status) in the calling function. This forces you to have all your System.exit()s in one place in or near main(). This is cleaner than calling System.exit() deep inside your logic. Code Wrapper: public class SystemExit { public void exit(int status) { System.exit(status); } } Main: public class Main { private final SystemExit systemExit; Main(SystemExit systemExit) { this.systemExit = systemExit; } public static void main(String[] args) { SystemExit aSystemExit = new SystemExit(); Main main = new Main(aSystemExit); main.executeAndExit(args); } void executeAndExit(String[] args) { int status = execute(args); systemExit.exit(status); } private int execute(String[] args) { System.out.println(""First argument:""); if (args.length == 0) { return 1; } System.out.println(args[0]); return 0; } } Test: public class MainTest { private Main main; private SystemExit systemExit; @Before public void setUp() { systemExit = mock(SystemExit.class); main = new Main(systemExit); } @Test public void executeCallsSystemExit() { String[] emptyArgs = {}; // test main.executeAndExit(emptyArgs); verify(systemExit).exit(1); } }  A quick look at the api shows that System.exit can throw an exception esp. if a securitymanager forbids the shutdown of the vm. Maybe a solution would be to install such a manager.  There is a minor problem with the SecurityManager solution. Some methods such as JFrame.exitOnClose also call SecurityManager.checkExit. In my application I didn't want that call to fail so I used Class[] stack = getClassContext(); if (stack[1] != JFrame.class && !okToExit) throw new ExitException(); super.checkExit(status);  There are environments where the returned exit code is used by the calling program (such as ERRORLEVEL in MS Batch). We have tests around the main methods that do this in our code and our approach has been to use a similar SecurityManager override as used in other tests here. Last night I put together a small JAR using Junit @Rule annotations to hide the security manager code as well as add expectations based on the expected return code. http://code.google.com/p/junitsystemrules/  The library System Rules has a JUnit rule called ExpectedSystemExit. With this rule you are able to test code that calls System.exit(...): public void MyTest { @Rule public final ExpectedSystemExit exit = ExpectedSystemExit.none(); @Test public void systemExitWithArbitraryStatusCode() { exit.expectSystemExit(); //the code under test which calls System.exit(...); } @Test public void systemExitWithSelectedStatusCode0() { exit.expectSystemExitWithStatus(0); //the code under test which calls System.exit(0); } } Wow this was exactly what I was looking for thanks! Perfect. Elegant. Didn't have to change a stitch of my original code or play around with the security manager. This should be the top answer!  I like some of the answers already given but I wanted to demonstrate a different technique that is often useful when getting legacy code under test. Given code like: public class Foo { public void bar(int i) { if (i < 0) { System.exit(i); } } } You can do a safe refactoring to create a method that wraps the System.exit call: public class Foo { public void bar(int i) { if (i < 0) { exit(i); } } void exit(int i) { System.exit(i); } } Then you can create a fake for your test that overrides exit: public class TestFoo extends TestCase { public void testShouldExitWithNegativeNumbers() { TestFoo foo = new TestFoo(); foo.bar(-1); assertTrue(foo.exitCalled); assertEquals(-1 foo.exitValue); } private class TestFoo extends Foo { boolean exitCalled; int exitValue; void exit(int i) { exitCalled = true; exitValue = i; } } This is a generic technique for substituting behavior for test cases and I use it all the time when refactoring legacy code. It not usually where I'm going to leave thing but an intermediate step to get the existing code under test. This tecniques does not stop the conrol flow when the exit() has been called. Use an Exception instead.  You can test System.exit(..) with replacing Runtime instance. E.g. with TestNG + Mockito: public class ConsoleTest { /** Original runtime. */ private Runtime originalRuntime; /** Mocked runtime. */ private Runtime spyRuntime; @BeforeMethod public void setUp() { originalRuntime = Runtime.getRuntime(); spyRuntime = spy(originalRuntime); // Replace original runtime with a spy (via reflection). Utils.setField(Runtime.class ""currentRuntime"" spyRuntime); } @AfterMethod public void tearDown() { // Recover original runtime. Utils.setField(Runtime.class ""currentRuntime"" originalRuntime); } @Test public void testSystemExit() { // Or anything you want as an answer. doNothing().when(spyRuntime).exit(anyInt()); System.exit(1); verify(spyRuntime).exit(1); } }  How about injecting an ""ExitManager"" into this Methods: public interface ExitManager { void exit(int exitCode); } public class ExitManagerImpl implements ExitManager { public void exit(int exitCode) { System.exit(exitCode); } } public class ExitManagerMock implements ExitManager { public bool exitWasCalled; public int exitCode; public void exit(int exitCode) { exitWasCalled = true; this.exitCode = exitCode; } } public class MethodsCallExit { public void CallsExit(ExitManager exitManager) { // whatever if (foo) { exitManager.exit(42); } // whatever } } The production code uses the ExitManagerImpl and the test code uses ExitManagerMock and can check if exit() was called and with which exit code. I really like this solution.  Calling System.exit() is a bad practice unless it's done inside a main(). These methods should be throwing an exception which ultimately is caught by your main() who then calls System.exit with the appropriate code. That doesn't answer the question though. What if the function being tested IS ultimately the main method? So calling System.exit() might be valid and ok from a design perspective. How do you write a test case for it? You shouldn't have to test the main method as the main method should just take any arguments pass them to a parser method and then kick start the application. There should be no logic in the main method to be tested. @Elie: In these types of questions there are two valid answers. One answering the question posed and one asking why the question was based. Both types of answers give a better understanding and especially both together.  You can use the java SecurityManager to prevent the current thread from shutting down the Java VM. The following code should do what you want: SecurityManager securityManager = new SecurityManager() { public void checkPermission(Permission permission) { if (""exitVM"".equals(permission.getName())) { throw new SecurityException(""System.exit attempted and blocked.""); } } }; System.setSecurityManager(securityManager); Hm. The System.exit docs say specifically that checkExit(int) will be called not checkPermission with name=""exitVM"". I wonder if I should override both? The permission name actually seems to be exitVM.(statuscode) i.e. exitVM.0 - at least in my recent test on OSX.  Indeed Derkeiler.com suggests: Why System.exit() ? Instead of terminating with System.exit(whateverValue) why not throw an unchecked exception? In normal use it will drift all the way out to the JVM's last-ditch catcher and shut your script down (unless you decide to catch it somewhere along the way which might be useful someday). In the JUnit scenario it will be caught by the JUnit framework which will report that such-and-such test failed and move smoothly along to the next. Prevent System.exit() to actually exit the JVM: Try modifying the TestCase to run with a security manager that prevents calling System.exit then catch the SecurityException. public class NoExitTestCase extends TestCase { protected static class ExitException extends SecurityException { public final int status; public ExitException(int status) { super(""There is no escape!""); this.status = status; } } private static class NoExitSecurityManager extends SecurityManager { @Override public void checkPermission(Permission perm) { // allow anything. } @Override public void checkPermission(Permission perm Object context) { // allow anything. } @Override public void checkExit(int status) { super.checkExit(status); throw new ExitException(status); } } @Override protected void setUp() throws Exception { super.setUp(); System.setSecurityManager(new NoExitSecurityManager()); } @Override protected void tearDown() throws Exception { System.setSecurityManager(null); // or save and restore original super.tearDown(); } public void testNoExit() throws Exception { System.out.println(""Printing works""); } public void testExit() throws Exception { try { System.exit(42); } catch (ExitException e) { assertEquals(""Exit status"" 42 e.status); } } } Update December 2012: Will proposes in the comments using System Rules a collection of JUnit(4.9+) rules for testing code which uses java.lang.System. This was initially mentioned by Stefan Birkner in his answer in December 2011. System.exit(…) Use the ExpectedSystemExit rule to verify that System.exit(…) is called. You could verify the exit status too. For instance: public void MyTest { @Rule public final ExpectedSystemExit exit = ExpectedSystemExit.none(); @Test public void noSystemExit() { //passes } @Test public void systemExitWithArbitraryStatusCode() { exit.expectSystemExit(); System.exit(0); } @Test public void systemExitWithSelectedStatusCode0() { exit.expectSystemExitWithStatus(0); System.exit(0); } } Make sure the tear down is executed properly otherwise your test will fail in a runner such as Eclipse because the JUnit application can't exit! :) Didn't like the first answer but the second is pretty cool--I hadn't messed with security managers and assumed they were more complicated than that. However how do you test the security manager/testing mechanism. Works like a charm. I don't like the solution using the security manager. Seems like a hack to me just to test it. Exactly what i was looking for nice answer. I like that solution but it doesn't work for me. I use Aaron M. Renn's getopt port which does a `System.getProperty(""gnu.posixly_correct"" null)` (Getopt.java line 615). This causes a security exception to be thrown whenever the `NoExitSecurityManager` is installed. I also tried the more elaborate `NoExitSecurityManager` from the SystemRules library mentioned in another answer but it also throws the exception. @bbuser the one from http://www.urbanophile.com/arenn/hacking/getopt/ then? Interesting. That would be best in a separate question (with a link to this question) in order to present what you have tried and the security exception it generates. @VonC Don't agree with the first answer. Some Java programs are written to be executed by scripts which expect 0=success 1=failure behaviour. AFAIK the Java spec does not force a JVM to return a non-zero error code if an uncaught exception occurs. Second answer looks good though. @DuncanJones re-reading http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-2.html#jvms-2.10  you are right (on the lack of fixed exit status on a JVM ending because of unchecked exception) If you're using junit 4.7 or above let a library deal with capturing your System.exit calls. System Rules - http://stefanbirkner.github.com/system-rules/ @Will interesting suggestion. I have included it in this (old) answer for more visibility. ""Instead of terminating with System.exit(whateverValue) why not throw an unchecked exception?"" -- because I'm using command line argument processing framework which calls `System.exit` whenever an invalid command line argument is supplied. This is fantastic -- thank you so much. @MichaelScheper right I have added the proper attribution in the answer. Nothing against @Will but this answer actually predates his comment so I think its author should receive the credit: http://stackoverflow.com/a/8658497/1450294 I wish I could vote this up twice.  You actually can mock or stub out the System.exit method in a JUnit test. For example using JMockit you could write (there are other ways as well): @Test public void mockSystemExit(@Mocked(""exit"") System mockSystem) { // Called by code under test: System.exit(); // will not exit the program }  For VonC's answer to run on JUnit 4 I've modified the code as follows protected static class ExitException extends SecurityException { private static final long serialVersionUID = -1982617086752946683L; public final int status; public ExitException(int status) { super(""There is no escape!""); this.status = status; } } private static class NoExitSecurityManager extends SecurityManager { @Override public void checkPermission(Permission perm) { // allow anything. } @Override public void checkPermission(Permission perm Object context) { // allow anything. } @Override public void checkExit(int status) { super.checkExit(status); throw new ExitException(status); } } private SecurityManager securityManager; @Before public void setUp() { securityManager = System.getSecurityManager(); System.setSecurityManager(new NoExitSecurityManager()); } @After public void tearDown() { System.setSecurityManager(securityManager); }  One trick we used in our code base was to have the call to System.exit() be encapsulated in a Runnable impl which the method in question used by default. To unit test we set a different mock Runnable. Something like this: private static final Runnable DEFAULT_ACTION = new Runnable(){ public void run(){ System.exit(0); } }; public void foo(){ this.foo(DEFAULT_ACTION); } /* package-visible only for unit testing */ void foo(Runnable action){ // ...some stuff... action.run(); } ...and the JUnit test method... public void testFoo(){ final AtomicBoolean actionWasCalled = new AtomicBoolean(false); fooObject.foo(new Runnable(){ public void run(){ actionWasCalled.set(true); } }); assertTrue(actionWasCalled.get()); } Simple and elegant. Wish I could vote it up twice. Is this what they call dependency injection? This example as written is sort of half-baked dependency injection - the dependency is passed to the package-visible foo method (by either the public foo method or the unit test) but the main class still hardcodes the default Runnable implementation.  Use Runtime.exec(String command) to start JVM in a separate process. How will you interact with the class under test if it is in a separate process from the unit test?"
705,A,"How to test a class that has private methods fields or inner classes How do I use JUnit to test a class that has internal private methods fields or nested classes? It seems bad to change the access modifier for a method just to be able to run a test. Best way to test a private method is not testing it directly I disagree. A (public) method which is long or difficult to comprehend has to be refactored. It would be folly not to test the small (private) methods that you get instead of only the public one. Not testing any methods just because it's visibility is stupid. Even unit test should be about smallest piece of code and if you test only public methods you will never now for sure where error occurs - that method or some other. For those who fall on this page and are looking for something more specific to **Android** I wrote a blog article on this topic that focuses on Android development : http://blog.octo.com/en/android-testing-testing-private-methods/ *Android testing has its own problematic regarding private / protected methods testing mostly due to the fact that code under test and test code are separated into 2 different applications. The article explains how we can achieve private / protected method testing on android and rules out a misconception of AndroidManifest package statements.* @Dainius so why you dont test snippets of pieces of your function instead of limiting yourself to the whole function? either your logic dont made sense or you are the hypocrit Check the article *[Testing Private Methods with JUnit and SuiteRunner](http://www.artima.com/suiterunner/privateP.html)*. why do you think I don't test important part of methods? But mostly a method should do only what is should do so that important part is all method (or function if you want to call it that). If your function do more than one think it's usually bad design (usually is not always there could be exception as all rules have exceptions). in case someBody else there also expecting that protected variable of parent class will be accessible in child class like me please try childObject.getClass().getSuperclass().getDeclaredField(""fieldName"") instead Possible duplicate / See also: http://stackoverflow.com/questions/2811141/is-it-bad-practice-to-use-reflection-in-unit-testing You need to test the class *functionality* not its *implementation*. Wanna test the private methods? Test the public methods that call them. If the functionality the class offers is tested thoroughly the internals of it have demonstrated to be correct and reliable; you don't need to test the internal conditions. The tests should maintain decoupling from the tested classes. sugar/boiler plate for the reflection approach: http://code.google.com/p/accessive/ has a `FieldAccessor` `MethodAccessor` `ClassAccessor` `ProxyAccessor` factory. JML has a spec_public comment annotation syntax that allows you to specify a method as public during tests: private /*@ spec_public @*/ int methodName(){ ... } This syntax is discussed at http://www.eecs.ucf.edu/~leavens/JML/jmlrefman/jmlrefman_2.html#SEC12. There also exists a program that translates JML specifications into JUnit tests. I'm not sure how well that works or what its capabilities are but it doesn't appear to be necessary since JML is a viable testing framework on its own.  As others have said... don't test private methods directly. Here are a few thoughts: keep all methods small and focused (easy to test easy to find what is wrong) use code coverage tools I like Cobertura (oh happy day looks like a new version is out!) Run the code coverage on the unit tests. If you see that methods are not fully tested add to the tests to get the coverage up. Aim for 100% code coverage but realize that you probably won't get it. Up for code coverage. No matter what kind of logic is in the private method you are still invoking those logic through a public method. A code coverage tool can show you which parts are covered by the test therefore you can see if your private method is tested.  Wooww takes some guts to add an answer here :) Today I pushed a library to help testing private methods and fields.It has been designed with Android in mind but can really be used for any Java project. If you got some code with private methods or fields or constructors you can use BoundBox. It does exactly what you are looking for. Here below is an example of a test that accesses 2 private fields of an Android activity to test it : @UiThreadTest public void testCompute() { // given boundBoxOfMainActivity = new BoundBoxOfMainActivity(getActivity()); // when boundBoxOfMainActivity.boundBox_getButtonMain().performClick(); // then assertEquals(""42"" boundBoxOfMainActivity.boundBox_getTextViewMain().getText()); } BoundBox makes it easy to test private/protected fields methods and constructors. You can even access stuff that is hidden by inheritance. Indeed BoundBox breaks encapsulation. It will give you access to all that through reflection BUT every thing is checked at compile time. Ideal for testing some legacy code. Use it carefully. ;) https://github.com/stephanenicolas/boundbox Just tried it BoundBox is a simple and elegant and solution! Thx ! Any help to advertise about the lib would be more than welcome ;)  Generally a unit test is intended to exercise the public interface of a class or unit. Therefore private methods are implementation detail that you would not expect to test explicitly. That's the best answer IMO or as it is usually said test behaviour not methods. Unit testing is not a replacement for source code metrics static code analysis tools and code reviews. If private methods are so complex that they need separates tests then it probably needs to be refactored not more tests thrown at it.  I have used reflection to do this in the past and in my opinion it was a big mistake. Strictly speaking you should not be writing unit tests that directly test private methods. What you should be testing is the public contract that the class has with other objects; you should never directly test an object's internals. If another developer wants to make a small internal change to the class which doesn't affect the classes public contract he/she then has to modify your reflection based test to ensure that it works. If you do this repeatedly throughout a project unit tests and then stop being a useful measurement of code health and start to become a hindrance to development and an annoyance to the development team. What I recommend doing instead is using a code coverage tool such as Cobertura to ensure that the unit tests you write provide decent coverage of the code in private methods. That way you indirectly test what the private methods are doing and maintain a higher level of agility. +1 to this. In my opinion it's the best answer to the question. By testing private methods you are testing the implementation. This defeats the purpose of unit testing which is to test the inputs/outputs of a class' contract. A test should *only* know enough about the implementation to mock the methods it calls on its dependencies. Nothing more. If you can not change your implementation without having to change a test - chances are that your test strategy is poor.  I am not sure whether this is a good technique but I developed the following pattern to unit test private methods: I don't modify the visibility of the method that I want to test and add an additional method. Instead I am adding an additional public method for every private method I want to test. I call this additional method Test-Port and denote them with the prefix t_. This Test-Port method then simply accesses the according private method. Additionally I add a boolian flag to the Test-Port method to decide whether I grant access to the private method through the Test-Port method from outside. This flag is then set globally in a static class where I place e.g. other global settings for the application. So I can switch the access to the private methods on and off in one place e.g. in the corresponding unit test.  First I'll throw this question out: why do your private members need isolated testing? Are they that complex providing such complicated behaviors as to require testing apart from public surface? It's unit testing not 'line-of-code' testing. Don't sweat the small stuff. If they are that big big enough that these private members are each a 'unit' large in complexity -- consider refactoring such private members out of this class. If refactoring is inappropriate or infeasible can you use the strategy pattern to replace access to these private member functions / member classes when under unit test? Under unit test the strategy would provide added validation but in release builds it would be simple pass-thru. Because often a particular piece of code from a public method is refactored into an internal private method and is really the critical piece of logic which you might have got wrong. You want to test this independently from the public method Even the shortest code sometimes without unit test is not correct. Just try to caluclate the difference between 2 geograhpical angles. 4 lines of code and most will not do it correct at first try. Such methods needs unit test because the form the base of a trustfull code. (Ans such usefull code can be public too; less usefull protected  I only test the public interface but I have been known to make specific private methods protected so I can either mock them out entirely or add in additional steps specific for unit testing purposes. A general case is to hook in flags I can set from the unit test to make certain methods intentionally cause an exception to be able to test fault paths; the exception triggering code is only in the test path in an overridden implementation of the protected method. I minimize the need for this though and I always document the precise reasons to avoid confusion.  Since you're using JUnit have you looked at junit-addons? It has the ability to ignore the java security model and access private methods and attributes.  To test legacy code with large and quirky classes it is often very helpful to be able to test the one private (or public) method I'm writing right now. I use the junitx.util.PrivateAccessor-package. Lots of helpful one-liners for accessing private methods and private fields. import junitx.util.PrivateAccessor; PrivateAccessor.setField(myObjectReference ""myCrucialButHardToReachPrivateField"" myNewValue); PrivateAccessor.invoke(myObjectReference ""privateMethodName"" java.lang.Class[] parameterTypes java.lang.Object[] args); Hope that was helpful :) +1 for `junitx.util.PrivateAccessor`  If you're trying to test existing code that you're reluctant or unable to change reflection is a good choice. If the class's design is still flexible and you've got a complicated private method that you'd like to test separately I suggest you pull it out into a separate class and test that class separately. This doesn't have to change the public interface of the original class it can internally create an instance of the helper class and call the helper method. If you want to test difficult error conditions coming from the helper method you can go a step further. Extract an interface from the helper class add a public getter and setter to the original class to inject the helper class (used through its interface) and then inject a mock version of the helper class into the original class to test how the original class responds to exceptions from the helper. This approach is also helpful if you want to test the original class without also testing the helper class.  I tend not to test private methods. There lies madness. Personally I believe you should only test your publicly exposed interfaces (and that includes protected and internal methods).  I'd use reflection since I don't like the idea of changing the access to a package on the declared method just for the sake of testing. However I usually just test the public methods which should also ensure the the private methods are working correctly. you can't use reflection to get private methods from outside the owner class the private modifier affects reflection also This is not true. You most certainly can as mentioned in Cem Catikkas's answer. you can't use reflection to get private methods from outside the owner class the private modifier affects reflection also [Here](http://www.codeproject.com/KB/cs/testnonpublicmembers.aspx)'s a good article on the question's subject @jmfsg The article you link to link to specifically says ""Testing private methods is a little more involved; but we can still do it using `System.Reflection`."" (Apparently you need `ReflectionPermission` but that's not normally a problem.)  As many above have suggested a good way is to test them via your public interfaces. If you do this it's a good idea to use a code coverage tool (like Emma) to see if your private methods are in fact being executed from your tests. You should not indirectly test! Not only touching via coverage ; test that the expected result is delivered!  The private methods are called by a public method so the inputs to your public methods should also test private methods that are called by those public methods. When a public method fails then that could be a failure in the private method.  You can turn off access restrictions for reflection so that private means nothing. The setAccessible(true) call does that. The only restriction is that a ClassLoader may disallow you from doing that. See Subverting Java Access Protection for Unit Testing (Ross Burton) for a way to do this in Java  What if your test classes are in the same package as the class that should be tested? But in a different directory of course src & classes for your source code test/src and test/classes for your test classes. And let classes and test/classes be in your classpath.  Just two examples of where I would want to test a private method: Decryption routines - I would not want to make them visible to anyone to see just for the sake of testing else anyone can use them to decrypt. But they are intrinsic to the code complicated and need to always work. (the obvious exception is reflection which can be used to view even private methods in most cases when SecurityManager is not configured to prevent this) Creating an SDK for community consumption. Here public takes on a wholly different meaning since this is code that the whole world may see (not just internal to my app). I put code into private methods if I don't want the SDK users to see it - I don't see this as code smell merely as how SDK programming works. But of course I still need to test my private methods and they are where the functionality of my SDK actually lives. I understand the idea of only testing the ""contract"". But I don't see one can advocate actually not testing code - ymmv. So my tradeoff involves complicating the JUnits with reflection rather than compromising my security & SDK. @ngreen that is all true but is not what the question is about. OP asks about testing private methods. Default is not the same as private; as stated you can easily see a default access method from a class just by declaring that class in the same package. With private access you require reflection which is a whole other ballgame. While you should never make a method public just to test it `private` is not the only alternative. No access modifier is `package private` and means that you can unit test it as long as your unit test lives in the same package. I was commenting on your answer point 1 in particular not the OP. There is no need to make a method private just because you don't want it to be public. @ngreen true thx - I was lazy with the word ""public"". I have updated the answer to include public protected default (and to make mention of reflection). The point I was trying to make is that there is good reason for some code to be secret however that shouldn't prevent us from testing it.  here is my generic function to test private fields: protected <F> F getPrivateField( String fieldName Object obj) throws NoSuchFieldException IllegalAccessException { Field field = obj.getClass().getDeclaredField( fieldName ); field.setAccessible( true ); return (F)field.get( obj ); }  When I have private methods in a class that is sufficiently complicated that I feel the need to test the private methods directly that is a code smell: my class is too complicated. My usual approach to addressing it is to tease out a new class that contains the interesting bits. Often this method and the fields it interacts with and maybe another method or two can be extracted in to a new class. The new class exposes these methods as 'public' so they're accessible for unit testing. The new and old classes are now both simpler than the original class which is great for me (I need to keep things simple or I get lost!). Note that I'm not suggesting that anyone create classes without using their brain! The point here is to use the forces of unit testing to help you find good new classes. I think that if you have method there is no need to create another class just to be able to test that method. I don't think that class design was question here.. So I assume that author did everything to have proper design before so introducing another class for one method you just increasing complexity. Of course when you have enough complex program there will be no obvious bugs.. But you agree that good OOD would expose (make public) only methods that are necessary for that class/object to work correctly? all other should be private/protectec. So in some private methods there will be some logic and IMO testing these methods will only improve quality of software. Of course I agree that if some piece of code is to complex it should be divided to separate methods/class. @Dainius: I don't suggest creating a new class *solely* so you can test that method. I do suggest that writing tests can help you improve your design: good designs are easy to test. Question was how to test private methods. You say that you would do new class for that (and add much more complexity) and after suggest to not create new class. So how test private methods? What you describe is an ongoing problem for anyone trying to do unit testing and far beyond the scope of this question. A short answer is ""Read _Working Effectively with Legacy Code_ by Michael Feathers and then remember that all code is legacy code 5 minutes after you write it. There is a bit of a chicken-and-egg problem here. Strictly speaking you should have code that you want to refactor under test first yet to test the code you first need to put it in a different class. That seems a bit suboptimal to me. @Danius: Adding a new class in most cases reduce the complexity. Just measure it. Testability in some cases fights against OOD. The modern approach is favor testability.  To answer your question I've developed dp4j; All you need is add dp4j.jar to your classpath (see instructions on website for eclipse). NB: it's the first release patches are welcome (there are known limitations to be addressed in next releases)!  Groovy has a bug/feature through which you can invoke private methods as if they were public. So if you're able to use Groovy in your project it's an option you can use in lieu of reflection. Check out this page for an example. That is a hack not a real solution. @givanse I disagree...this is one of the reasons testing Java code using Groovy is so powerful.  The best way to test a private method is via another public method. If this cannot be done then one of the following conditions is true: The private method is dead code There is a design smell near the class that you are testing The method that you are trying to test should not be private Disagree. It's totally valid to have an algorithm in a private method which needs more unit testing than is practical through a class's public interfaces. Hey Mr Shiny Yes but then you will have brittle tests. Also see number 2 in my reply. I don't understand why testing a private method creates code smell? Is not the point of unit testing to test each small part to isolate errors. could you explain what a brittle test is please? A brittle test is a test which fails too easily when there's a change in the code. Generally this happens when the test result is based on what the method does rather than on the expected outputs and side-effects given a set of inputs. Ideally a change to the code which does not change the results should not break the test. @Mr. Shiny and New: If your private method is complex enough to warrant independent unit testing then it's complex enough to have its own class. The class can be internal of course (i.e. not accessible from other packages). But isn't an internal class adding more complexity to something that maybe is complex to understand but are 4 lines of code extracted to a private method to avoid repetition? @sleske I don't agree. You can have a small private method which by design you needed it to be private. and I would rather test each small piece of code than test that private method through a public one. This way of testing will go towards a component test. Having more finely grained tests that cover smaller private methods mean when things do break your tests will tell you where. After all coding input and outputs are /assumptions/ and we test these assumptions remain true. If your internal ""brittle"" test breaks then the assumption about how it works has changed and code that uses it will probably be broken in subtle ways. ( Usually when a test starts failing you'll reach for the debugger to find out why having smaller sub-tests will make finding the place the fault occurs much faster ) The test-the-private-method camp is missing the point: it can take a great deal more work and time to test the private methods and it's not likely to save any/enough time when a test fails. If the test fails for some error in a private method like an NPE you'll still be taken right to the problem (the private method). If the data generated by a private method don't match you can easily see that in your assertion by debugging equals(). In some cases it may make sense to write tens of private method tests using reflections but the need should be clearly visible. @Pete I agree. My team tests private methods and I feel like I waste a lot of time ""fixing"" unit tests for private methods that start failing after I change around how the private internals of a class work without changing the class's contract. I look at the failing tests and it's now failing because private method f was not called 3 times or private method g returned some different result. Yet the public tests do not break. To me that is a waste and is not very helpful having those tests. 2 seconds on a debugger will show me where some public method fails... Further we never get close to 100% code coverage anyway so why not focus your time doing quality testing on the methods that clients will actually be using directly. @grinch Spot on. The only thing you would gain from testing private methods is debugging information and that's what debuggers are for. If your tests of the class's contract have full coverage then you have all the information you need. Private methods are an **implementation** detail. If you test them you will have to change your tests every time your implementation changes even if the contract doesn't. In the full life cycle of the software this is likely to cost a lot more than the benefit it gives. @AlexWien you're right. Coverage was not the right term. What I should have said was ""if your tests of the class's contract cover all meaningful inputs and all meaningful states"". This you rightly point out may prove infeasible to test via the public interface to the code or indirectly as you refer to it. If that is the case for some code you are testing I would be inclined to think either: (1) The contract is too generous (e.g. too many parameters in the method) or (2) The contract is too vague (e.g. method behaviour varies greatly with class state). I would consider both design smells. @ErikMadsen Full coverage does not mean correct result. A direct unit test can check for correct result. An indirect often not. A third way would be to temporarily make complex private methods public but only during the initial implementation period so that they can be easily tested without changing the design. Then when the class has evolved to the point where you can test reliably from the public API make the method private and focus your test case effort on the public API. This allows you to change the implementation without breaking tests and still alerts you if the private method breaks anything. I'd only do this for complex private methods and even then it might make better sense to split them up somehow. Almost all private methods should not be directly tested (to reduce maintenance costs). Exception: Scientific methods may have forms like f(a(b(c(x)d(y)))a(e(xz))b(f(xyz)z)) where abcde and f are horribly complicated expressions but are otherwise useless outside of this single formula. Some functions (think MD5) are hard to invert so it can be difficult (NP-Hard) to choose parameters that will fully cover behavior space. It is easier to test abcdef independently. Making them public or package-private lies that they are reusable. Solution: make them private but test them. @ErikMadsen I was thinking about instance methods. My point is that some methods are so complex to test by themselves that you are more likely to find mistakes by testing components. However these components are implementation details. Breaking them out clutters the design and so makes the software less readable. @Eponymous I'm a bit torn on this one. The object oriented purist in me would say the you should use an object oriented approach rather than static private methods allowing you to test via public instance methods. But then again in a scientific framework memory overhead is entirely likely to be a concern which I believe argues for the static approach. I need to simplify my public method -> I extract some code into private method. It has to be private. And I need to test it. i'm +1ing this answer because of the value of the comments. @Trumpi please add a note to this answer asking readers to go through all the comments. There's huge value in reading the comment debate. There is a major problem with private methods. They are inaccessible to subclasses (`extends ...`) making it very difficult to provide slightly different versions in a test scenario.  If you have somewhat of a legacy application and you're not allowed to change the visibility of your methods the best way to test private methods is to use reflection. Internally we're using helpers to get/set private and private static variables as well as invoke private and private static methods. The following patterns will let you do pretty much anything related to the private methods and fields. Of course you can't change private static final variables through reflection. Method method = targetClass.getDeclaredMethod(methodName argClasses); method.setAccessible(true); return method.invoke(targetObject argObjects); And for fields: Field field = targetClass.getDeclaredField(fieldName); field.setAccessible(true); field.set(object value); Notes: * targetClass.getDeclaredMethod(methodName argClasses) lets you look into private methods. The same thing applies for getDeclaredField. * The setAccessible(true) is required to play around with privates. Useful if you don't know the API perhaps but if you are having to test private methods in this manner there is something up with your design. As another poster says unit testing should test the class's contract: if the contract is too broad and instantiates too much of the system then the design should be addressed. True... Anything Reflective will fail miserably after obfuscation. Very useful. Using this it is important to keep in mind that it would fail badly if tests were run post obfuscation. The example code didn't work for me but this made thigs clearer: http://www.java2s.com/Tutorial/Java/0125__Reflection/Callaclassmethodwith2arguments.htm This is all great (I've been doing the same for a long time) but there is one glitch that I am trying to find the solution for. Imagine that u wanna test the exception outcome of the private method. Instead of the method invocation giving u an IllegalArgumentException for example you'll get null Class:java.lang.reflect.InvocationTargetException. Maybe there is a way of covering this but I am not familiar with it. A colegue of mine proposed using powermock for it. Much better than using reflection directly would be to use some library for it such as [Powermock](http://code.google.com/p/powermock/wiki/BypassEncapsulation). Thank you ""Rob"" for the link. My test was failing at the getDeclaredMethod() call because I wasn't passing it the parameter arguments. @despot: According to the [doc](http://docs.oracle.com/javase/6/docs/api/java/lang/reflect/InvocationTargetException.html) you should be able to retrieve the original exception via `getTargetException()` or `getCause()`. This is why Test Driven Design is helpful. It helps you figure out what needs to be exposed in order to validate behavior. Here is an example for **MyAdder.sum(int int)** --- `MyAdder object = new MyAdder(); Method method = MyAdder.class.getDeclaredMethod(""sum"" new Class[] { Integer.TYPE Integer.TYPE }); method.setAccessible(true); Object r = method.invoke(object new Object[] { 1 2 }); Assert.assertEquals(3l ((Integer) r).longValue());` shouldn't that be getClass().getDeclaredMethod() ? Agreed with this answer I have also passed from this problem and solved it by using reflection. ""play around with privates.""  From this article: Testing Private Methods with JUnit and SuiteRunner (Bill Venners) you basically have 4 options: Don't test private methods. Give the methods package access. Use a nested test class. Use reflection. An alternative to Bill Venners' suggestion of using a static nested class is to use an inner class as shown here: http://www.redirecttonull.com/?p=224 @user86614 Putting test code into production code is not a good idea.  A private method only be accessed within the same class. So there is no way to test a “private” method of a target class from any test class. A way out is that you can perform unit testing manually or can change your method from “private” to “protected”. And then A protected method can only be accessed within the same package where the class is defined. So testing a protected method of a target class means we need to define your test class in the same package as the target class. if all the above is Not suits your requirement Use the reflection to access the private method  EDIT: Having tried Cem Catikkas' solution using reflection I'd have to say his was a more elegant solution than I have described here. However if you're looking for an alternative to using reflection and have access to the source you're testing this will still be an option. There is possible merit in testing private methods of a class particularly with test driven development where you would like to design small tests before you write any code. Creating a test with access to private members and methods can test areas of code which are difficult to target specifically with access only to public methods. If a public method has several steps involved it can consist of several private methods which can then be tested individually. Advantages: can test to a finer granularity Disadvantages: test code must reside in the same file as source code which can be more difficult to maintain similarly with .class output files they must remain within the same package as declared in source code However if continuous testing requires this method it may be a signal that the private methods should be extracted which could be tested in the traditional public way. Here is a convoluted example of how this would work: // import statements and package declarations public class ClassToTest { private int decrement(int toDecrement) { toDecrement--; return toDecrement; } // constructor and rest of class public static class StaticInnerTest extends TestCase { public StaticInnerTest(){ super(); } public void testDecrement(){ int number = 10; ClassToTest toTest= new ClassToTest(); int decremented = toTest.decrement(number); assertEquals(9 decremented); } public static void main(String[] args) { junit.textui.TestRunner.run(StaticInnerTest.class); } } } Inner class would be compiled to ClassToTest$StaticInnerTest. See also: http://www.javaworld.com/javaworld/javatips/jw-javatip106.html  Testing private methods breaks the encapsulation of your class because every time you change the internal implementation you break client code (in this case the tests). So don't test private methods. unit test and src code are a pair. If you change the src maybe you have to change the unit test. That is the sense of junit test. They shall garuantee that all works as before. and it is fine if they break if you change the code.  Private methods are consumed by public ones otherwise they're dead code. That's why you test the public method asserting the expected results of the public method and thereby the private methods it consumes. Testing private methods should be tested by debugging before running your unit tests on public methods. They may also be debugged using test driven development debugging your unit tests until all your assertions are met. I personally believe it is better to create classes using TDD; creating the public method stubs then generating unit tests with all the assertions defined in advance so the expected outcome of the method is determined before you code it. This way you don't go down the wrong path of making the unit test assertions fit the results. Your class is then robust and meets requirements when all your unit tests pass.  In C# you could have used System.Reflection though in Java I don't know. Though I feel the urge to answer this anyway since if you ""feel you need to unit test private methods"" my guess is that there is something else which is wrong... I would seriously consider looking at my architecture again with fresh eyes....  If you want to test private methods of a legacy application where you can't change the code one option is jMockit which will allow you to create mocks to an object even when they're private to the class. As part of the jmockit library you have access to the Deencapsulation class which makes testing private methods easy: `Deencapsulation.invoke(instance ""privateMethod"" param1 param2);`"
706,A,"Trying to run trivial Android JUnit tests. Getting: ""Test run failed: No test results"" What am I missing? I have never used JUnit before and now I'm trying to set it up on an Android project. My project under test is fairly complex including some JNI but my test project at the moment is completely trivial. I have found many examples (that look totally different) online of how to make a test project but it seems that no matter which one I follow I get the same results. Here's my JUnit project code:  package com.mycompany.myproject.test; import android.test.AndroidTestCase; public class SimpleTestCaseExample extends AndroidTestCase { public void test_testOne() { fail(""Just Always Fail""); } } When I run I see the following in Logcat:  stdout INSTRUMENTATION_STATUS: numtests=2 stdout INSTRUMENTATION_STATUS: test=test_testOne stdout INSTRUMENTATION_STATUS_CODE: 0 stdout INSTRUMENTATION_STATUS: id=InstrumentationTestRunner stdout INSTRUMENTATION_STATUS: current=2 stdout INSTRUMENTATION_STATUS: class=com.mycompany.myproject.test.SimpleTestCaseExample stdout INSTRUMENTATION_STATUS: stream= stdout INSTRUMENTATION_STATUS: numtests=2 stdout INSTRUMENTATION_STATUS: test=testAndroidTestCaseSetupProperly stdout INSTRUMENTATION_STATUS_CODE: 1 stdout INSTRUMENTATION_STATUS: id=InstrumentationTestRunner stdout INSTRUMENTATION_STATUS: current=2 stdout INSTRUMENTATION_STATUS: class=com.mycompany.myproject.test.SimpleTestCaseExample stdout INSTRUMENTATION_STATUS: stream=. stdout INSTRUMENTATION_STATUS: numtests=2 stdout INSTRUMENTATION_STATUS: test=testAndroidTestCaseSetupProperly stdout INSTRUMENTATION_STATUS_CODE: 0 stdout INSTRUMENTATION_RESULT: stream= stdout Test results for InstrumentationTestRunner=.. stdout Time: 0.07 stdout OK (2 tests) stdout INSTRUMENTATION_CODE: -1 But I get the following in the Console:  Launching instrumentation android.test.InstrumentationTestRunner on device emulator-5554 Collecting test information Test run failed: No test results I have tried a variety of different things messing with the basic TestCase class or the TestSuite class or a variety of other options. I tried to just go for the most trivial example because I'm really still trying to learn how this works. Whatever I try I see this error. Any suggestions would be appreciated! If I'm missing some critical information please let me know and I'll update. OK I figured it out. And there is very little chance that anybody would have guessed what the problem is. I'm not sure what made me try it. I have some JNI code that prints error messages to stdout. That code is not running in my test project but I use the same emulator. For that reason I had a /data/local.prop that redirects stdout to logcat. It turns out that the test tools expect the output from the JUnit tests to appear on stdout. When the logcat redirect of stdout is on nothing ends up on stdout and the test system doesn't get the output and so it fails to run. I removed my local.prop redirect of stdout to logcat and rebooted the emulator and now it works. It never occurred to me that the test system relied upon reading stdout itself. Thanks for this answer never would have thought of that"
707,A,Removing redundant JUnit testsuite tests In our JUnit testsuite I have spotted a few tests that do not drive up coverage and thus should be removed (they take time but does not really add value to the test suite). I was wondering what tools exist that can spot redundant tests for me? I'd consider this a misuse of the code coverage metric. Just because a test doesn't increase the metric it is not necessarily redundant - it could test a specific execution path that consists of LOC that are covered by several other tests together but represents behaviour that none of the other tests cover. And remember: code behaviour is influenced very much by state but no test coverage tool I know measures the coverage of state space. Unless the runtime of your test suite is a serious problem there are much better things you can do with your time than eliminate possibly redundant tests. I tend to agree with you (may be misuse of coverage metric). But there are other metrics as well so I think this problem should be approached wider and look at more of the standard metrics like for instance also number of conditionals.  Some months ago I used Eclipse plugin Clover. It could show how many times each line of code be executed and highlight after unit tests. I think Clover could help you at this point. That is not really my point here:) I use Cobertura on Hudson and IntelliJ's own coverage tool during development. The latter also shows the number of times each line passes. But not from which tests! I want to know which tests that do not add to better coverage and since I am not autistic I cannot myself remember how many times the test suite passes each line in my codebase. Understood. I hope Clover added the very new feature during these months:)
708,A,"Automatic JUnit on SVN checkin I'm using IntelliJ-Idea to write Java daemons. We use JUnit to unit test our code and want to have automated runs on check-in to subversion. Are there any tools available that can automatically run unit tests and then send reports in email and to Hobbit? Not sure about Hobbit but TeamCity is another nice Continuous Integration server. It can be configured to run on Subversion commit and has a decent IntelliJ plugin. It'll send emails too.  I think you should look into continuous integration as others have suggested. I've personally tried CruiseControl and Hudson and prefer Hudson. You can just download it and run it to see if you like it configuration is a breeze just: Download hudson.war from http://hudson.dev.java.net. Run it in standalone mode (java -jar hudson.war). Point your browser to http://localhost:8080 to configure and use. It has built-in support for Maven and Ant if you use those but if not you can simply write your own batch file/shell script. If continuous integration is not an option you can look into the use of a Subversion post-commit hook (http://svnbook.red-bean.com/en/1.0/svn-book.html#svn-ch-5-sect-2.1). I have no experience with using hooks for the purpose you described so I really can't offer a lot of help here.  We use Hudson as our Continuous Integration server. You can set-up jobs to poll your SCM at specific intervals and run Ant tasks on them. We have ours set-up to run a build and testing target when new changes in SVN are detected and to then email those who are involved with the project if the build or testing targets fail. Your link is wrong. Hudsons looks like it fits. I'm going to evaluate CruiseControl and Hudson. Thank you @gunnarsteinn - Thanks for pointing out the broken link :)  We use TeamCity for all our java projects. Mostly we use junit with maven and TeamCity already have a maven2 runner so setting up a new build task is easy. You tell TeamCity where your svn trunk is and select a runner. If you already use IntelliJ there is a plugin for TeamCity so you can get instant feedback if your tests pass when doing a commit. And if you are realy lazy you can select remote run when doing a commit and select ""Only commit if tests pass"". And as a final note. TeamCity is not limited to just run java test we use it to run python unit tests as well.  We use CruiseControl to automatically build projects and run unit tests whenever a change is committed to subversion. It can be configured to send e-mail notifications if the build/unit tests fail."
709,A,"Query regarding assert in JUnit All I am having a hard time understanding the concept of assertXXX () in Junit. Currently I have a method A(String fileName) that is generating a xml file for a input filename ""XXX.XX"" . So my test case includes testing if the code is generating any file and not just XML file which is wrong i.e. method A should only generate xml files. My code is: testCreateFile() { String fileName = ""testFile.csv""; A(fileName); File fileObj = new File (fileName); assertFalse(fileObj.exists()); // Since I check if the file should not be created } If I do this I get an AssertionError and jUnit window shows 2 Failures. Do I have to handle this Exception? if A creates a file don't you want to assertTrue? In other words you expect to find the file after you run A. And then you assert that the content of the file is the expected xml... If an exception is a failure you don't need to handle the exception. @darkie15 you should do both. you can assert that something is true or that something is false that something is not null or that something is null. It depends on what your expectations are. Got it. Thanks !! @darkie15 very good. what was the issue? I don't feel I really answered a question but sometimes just bouncing an idea of someone can help a dev resolve the issue... Well my actual code does checking of the extension using fileName.indexOf("".xml"") . But for the above code posted A should not have created a .csv file and hence I expect assertFalse() to work fine. ah i see. What exactly is the question then? What errors are you getting? My question was that assertFalse () was giving me this AssertionError and if I use assertTrue()  there would be no errors. Basically I was finding it difficult to understand how to use assert functions. Should it be used to test valid information passed or invalid information passed."
710,A,Is it possible to test Android app using ordinary JRE Junit test? Is it possible test Android app using ordinary JRE Junit test? I mean by not extending AndroidTestCase and running the tests on a phone or emulator? I have design my app using MVP as far at is goes with Android and have a Presenter that basically knows nothing about the specific View in this case the specific Activity. So I was hoping to run regular JUnit tests on my Presenter. Is that possible or must I use AndroidTestCase and run the test on a phone or emulator? As it turns out it is possible I used this guide to get it working: https://sites.google.com/site/androiddevtesting/notepadsample
711,A,"Easy way of running the same junit test over and over? Like the title says I'm looking for some simple way to run JUnit 4.x tests several times in a row automatically using Eclipse. An example would be running the same test 10 times in a row and reporting back the result. We already have a complex way of doing this but I'm looking for a simple way of doing it so that I can be sorta sure that the flaky test I've been trying to fix stays fixed. An ideal solution would be an Eclipse plugin/setting/feature that I am unaware of. I'm very curious about why you would want to do this. I am running a big black box test have made a small change and want to see how that affected the stability of this previously flaky test. This is very close to my question: http://stackoverflow.com/questions/1835523/is-there-a-way-to-make-eclipse-run-a-junit-test-mulitple-times-until-failure It is indeed except that you want it to run until failure while I just want to run it a number of times which may affect the answers I get. Are you against TestNG because if not then you could just use @Test(invocationCount = 10) and that is all that there is to it. I wasn't ""against"" TestNG we just weren't using it in that project. Quite decent solution to this problem can be found [In this link](http://www.codeaffine.com/2013/04/10/running-junit-tests-repeatedly-without-loops/) The easiest (as in least amount of new code required) way to do this is to run the test as a parametrized test (annotate with an @RunWith(Parameterized.class) and add a method to provide 10 empty parameters). That way the framework will run the test 10 times. This test would need to be the only test in the class or better put all test methods should need to be run 10 times in the class. Here is an example: @RunWith(Parameterized.class) public class RunTenTimes { @Parameterized.Parameters public static List<Object[]> data() { return Arrays.asList(new Object[10][0]); } public RunTenTimes() { } @Test public void runsTenTimes() { System.out.println(""run""); } } EDIT: With the above it is possible to even do it with a parameterless constructor but I'm not sure if the framework authors intended that or if that will break in the future. EDIT (in response to comment): If you are implementing your own runner then you could have the runner run the test 10 times. If you are using a third party runner then with 4.7 you can use the new @Rule annotation and implement the MethodRule interface so that it takes the statement and executes it 10 times in a for loop. The current disadvantage of this approach is that @Before and @After get run only once. This will likely change in the next version of JUnit (the @Before will run after the @Rule) but regardless you will be acting on the same instance of the object (something that isn't true of the Parameterized runner). This assumes that whatever runner you are running the class with correctly recognizes the @Rule annotations. That is only the case if it is delegating to the JUnit runners. If you are running with a custom runner that does not recognize the @Rule annotation then you are really stuck with having to write your own runner that delegates appropriately to that Runner and runs it 10 times. Note that there are other ways to potentially solve this (such as the Theories runner) but they all require a runner. Unfortunately JUnit does not currently support layers of runners. That is a runner that chains other runners. Yes this is the solution that I would like to have though and which will be best for most people so I'm going to go ahead and accept the answer. Unfortunately I'm already running @RunWith with another runner but otherwise this would have been an ideal solution. Holy crap I think you may have fixed a gripe I have had about JUnit for a long time - thanks! :) Great answer I wish I could upvote more than one time!!!!  This works much easier for me. public class RepeatTests extends TestCase { public static Test suite() { TestSuite suite = new TestSuite(RepeatTests.class.getName()); for (int i = 0; i < 10; i++) { suite.addTestSuite(YourTest.class); } return suite; } }  I've found that Spring's repeat annotation is useful for that kind of thing: @Repeat(value = 10) http://static.springsource.org/spring/docs/2.5.6/reference/testing.html http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/test/annotation/Repeat.html Changing test frameworks is not what I would call an easy way of doing it.  Anything wrong with: @Test void itWorks() { // stuff } @Test void itWorksRepeatably() { for (int i = 0; i < 10; i++) { itWorks(); } } Unlike the case where you are testing each of an array of values you don't particularly care which run failed. No need to do in configuration or annotation what you can do in code. I'd like to run several tests as normal unit tests and get a trace and status for each one.  There's an Intermittent annotation in the tempus-fugit library which works with JUnit 4.7's @Rule to repeat a test several times or with @RunWith. For example @RunWith(IntermittentTestRunner.class) public class IntermittentTestRunnerTest { private static int testCounter = 0; @Test @Intermittent(repition = 99) public void annotatedTest() { testCounter++; } } After the test is run (with the IntermittentTestRunner in the @RunWith) testCounter would be equal to 99. Yeah it's the same problem here already using another runner and so can't use this one good idea though. Yeah I'm having the same issue with RunWith... as it goes I tweaked tempus-fugit to get round it a little you can use a @Rule rather than runner when you want to run repeatedly. You mark it up with @Repeating instead of intermittent. The rule version wont run @Before/@Afters though. See http://tempus-fugit.googlecode.com/svn/site/documentation/concurrency.html#JUnit_Integration (scroll down to load/soak testing) for more details."
712,A,"Is there any way to automatically execute JUnit testcases once with all logging enabled and once with all logging disabled to increase code-coverage and find subtle bugs? I've found a solution see my own answer below. Does anyone have a more elegant one? Assume the following class to be tested: public class Foo { private final Logger logger = LoggerFactory.getLogger(Foo.class); public void bar() { String param=[..]; if(logger.isInfoEnabled()) logger.info(""A message with parameter {}"" param); if(logger.isDebugEnabled()) { // some complicated preparation for the debug message logger.debug([the debug message]); } } } and the following test-class: public class FooTest { @Test public void bar() { Foo foo=new Foo(); foo.bar(); } } A code-coverage tool like e.g. Cobertura will correctly report that only some of the conditional branches have been checked. info and debug are either activated or deactivated for the logger. Besides looking bad in your coverage score this poses a real risk. What if there is some side effect caused by code inside if(logger.isDebugEnabled())? What if your code does only work if DEBUG is enabled and fails miserably if the log level is set to INFO? (This actually happened in one of our projects :p) So my conclusion is that code containing logger statements should always be tested once with all logging enabled and once with all logging disabled... Is there a way to do something like that with JUnit? I know how to globally enable or disable all my logging in Logback so the problem is: How can I execute the tests twice once with logging enabled once with logging disabled. p.s. I'm aware of this question but I don't think this is a duplicate. I'm less concerned about the absolute coverage values but about subtle hard-to-find bugs that might be contained inside of a if(logger.isDebugEnabled()). I've solved this problem by implementing a base class that test classes should extend if such functionality is desired. The article Writing a parameterized JUnit test contained the solution. See LoggingTestBase for the logging base class and LoggingTestBaseExampleTest for a simple example that's using it. Every contained test method is executed three times: 1. It's executed using the logging as defined in logback-test.xml as usual. This is supposed to help while writing/debugging the tests. 2. It's executed with all logging enabled and written to a file. This file is deleted after the test. 3. It's executed with all logging disabled. Yes LoggingTestBase needs documentation ;) You could configure logback programmatically without Joran. You'd have shorter code. Moreover if configuration were done programmaticaly you could restore the old configuration at the end of each test. Holler on logback-dev if you'd like to see sample code. Otherwise very nice work! Thanks I'll come back at you on the dev list. I wasn't aware that logback can be configured programmatically Joran aside and that configuration can be restored beside reconfiguration. Note it is not considered good practice to manage log levels for a typical application by writing out the configuration from the application. Logging frameworks are designed to manage this external to the application. They are unit-tests it's not an application. Also there are certainly cases were changing the loog-level configuration makes sense e.g. a '-v' commandline option of a console application.  I would recommend switching from JUnit to TestNG. TestNG has a lot of advanced features over JUnit. It enables you to run your tests multiple times with different configuration and I guess that's what you need Can JUnit and TestNG be combined? I'm more or less talking about Maven 2 integration here... all our tests are JUnit tests... Yes TestNG can run JUnit tests. I use it in maven2 projects all the time Converting a test from TestNG to JUnit is easy though time-consumnig  eqbridges suggestion of simply running the tests twice with different logging contexts seems the simplest. You don't have to remember to code the logic in every blessed test for one big advantage. The other is that you can see which logging level is to blame very easily. That being said there a couple of strategies if you just had to do this in one test run. For 3.8 I would put everything in suites and make two suites one for each logging level which sets the logging level before running the tests. This is functionally the same thing as running the whole test suite twice with different command line parameters except that you get it with one run. In JUnit 4.x a couple of additional options come to mind: One is a custom runner. Although I can't think off hand of everything you would have to do to make this work but a runner that actually runs the test twice and annotating the test with @RunWith your custom runner could work. The other is parameterized tests. Although you would actually have to set up every test to take parameters (this requires a constructor which takes the arguments) and then set the log level according to the parameter. EDIT: In response to your request for a how-to on the paramterized tests here is the javadoc on the runner to get you started and here is a more practical guide. Thanks for the ""parameterized tests"" hint. I found some documentation see my solution. Do you have any links to docu about parameterized tests? The JUnit FAQ was last updated in 2006 and I couldn't find anything like it... What's going on at junit.org?? The cookbook seems to be outdated too...  Have you tried simply maintaining two separate log configuration files? Each one would log at different levels from the root logger. All logging disabled: ... <root> <priority value=""OFF""/> <appender-ref ref=""LOCAL_CONSOLE""/> </root> ... All logging enabled: ... <root> <priority value=""ALL""/> <appender-ref ref=""LOCAL_CONSOLE""/> </root> ... Execution would specify different configurations on the classpath via a system parameter: -Dlog4j.configuration=path/to/logging-off.xml -Dlog4j.configuration=path/to/logging-on.xml I'd like to be able to simply execute 'mvn clean install' and have all tests executed. With my solution proposal in my own answer this is possible. Additionally the developer is only required to write a single ordinary test class as usual. @Huxi: note the log configuration file is not just for initializing the logging framework it's used for controlling the log levels (which is what you're trying to do). By doing two (or three) test runs you're accomplishing what you want without forcing developers to create 3 unit tests for this functionality (which is what you're proposing). You can control which log configuration is chosen by each run using different maven profiles. A script can take care of executing the runs in one command. I was thinking more in the lines of Maven 2 and Logback. Having two configs and initializing the logging framework isn't the problem. Executing the tests twice calling the respective logger configuration method beforehand is my problem. I should have stated that more clearly I think. you can specify system parameters in maven in a choosable profile. You want two profiles one with logging and one without. Getting them to both execute with one command is beyond my ken  If you feel you have too much logging if you turn everything on perhaps you could try to cut down the amount of logging. Its not very useful if it too much for the computer to procude never mind a human to read. I route all logging into a temp file so the output isn't cluttered with useless logging messages. The whole point of my question is that I'd like to be able to test both scenarios: all logging turned on and all logging turned off. We really had the situation that a class would work if run with debug enabled and wouldn't with only info enabled. I'd like to catch such problems in a general way."
713,A,"Separation of JUnit classes into special test package? I am learning the concepts of Test-Driven Development through reading the Craftsman articles (click Craftsman under By Topic) recommended in an answer to my previous question ""Sample project for learning JUnit and proper software engineering"". I love it so far! But now I want to sit down and try it myself. I have a question that I hope will need only a simple answer. How do you organize your JUnit test classes and your actual code? I'm talking mainly about the package structure but any other concepts of note would be helpful too. Do you put test classes in org.myname.project.test.* and normal code in org.myname.project.*? Do you put the test classes right alongside the normal classes? Do you prefer to prefix the class names with Test rather than suffix them? I know this seems like the kind of thing I shouldn't worry about so soon but I am a very organization-centric person. I'm almost the kind of person that spends more time figuring out methods to keep track of what to get done rather than actually getting things done. And I have a project that is currently neatly divided up into packages but the project became a mess. Instead of trying to refactor everything and write tests I want to start fresh tests first and all. But first I need to know where my tests go. edit: I totally forgot about Maven but it seems a majority of you are using it! In the past I had a specific use case where Maven completely broke down on me but Ant gave me the flexibility I needed so I ended up attached to Ant but I'm thinking maybe I was just taking the wrong approach. I think I'll give Maven another try because it sounds like it will go well with test-driven development. I was searching for this but I had to LOL. Not being a troll but you sound exactly like me with your ""organization-centricity"". ADT/ADHD is the word ;) Thanks for this thread! possible duplicate of [Where should I put my JUnit tests?](http://stackoverflow.com/questions/811827/where-should-i-put-my-junit-tests) I put my test classes in the same package as what they are testing but in a different source folder or project. Organizing my test code in this fashion allows me to easily compile and package it separately so that production jar files do not contain test code. It also allows the test code to access package private fields and methods.  I prefer putting the test classes into the same package as the project classes they test but in a different physical directory like: myproject/src/com/foo/Bar.java myproject/test/com/foo/BarTest.java In a Maven project it would look like this: myproject/src/main/java/com/foo/Bar.java myproject/src/test/java/com/foo/BarTest.java The main point in this is that my test classes can access (and test!) package-scope classes and members. As the above example shows my test classes have the name of the tested class plus Test as a suffix. This helps finding them quickly - it's not very funny to try searching among a couple of hundred test classes each of whose name starts with Test... Update inspired by @Ricket's comment: this way test classes (typically) show up right after their tested buddy in a project-wise alphabetic listing of class names. (Funny that I am benefiting from this day by day without having consciously realized how...) Update2: A lot of developers (including myself) like Maven but there seems to be at least as many who don't. IMHO it is very useful for ""mainstream"" Java projects (I would put about 90% of projects into this category... but the other 10% is still a sizeable minority). It is easy to use if one can accept the Maven conventions; however if not it makes life a miserable struggle. Maven seems to be difficult to comprehend for many people socialized on Ant as it apparently requires a very different way of thinking. (Myself having never used Ant can't compare the two.) One thing is for sure: it makes unit (and integration) testing a natural first-class step in the process which helps developers adopt this essential practice. Excellent conventions +1 Good point about using a suffix instead! I agree with the suffix note too. Also since the test classes are separated into a different physical folder there's no need to try and prefix with Test in some attempt to trick an alphabetical order sort into grouping and I think SomeClassTest reads better.  I use Maven. The structure that Maven promotes is:- src/main/java/org/myname/project/MyClass.java src/test/java/org/myname/project/TestMyClass.java i.e. a test class with Test prepended to the name of the class under test is in a parallel directory structure to the main test. One advantage of having the test classes in the same package (not necessarily directory though) is you can leverage package-scope methods to inspect or inject mock test objects. I though it was `Test.java` not `Test.java`"
714,A,"Junit test that creates other tests Normally I would have one junit test that shows up in my integration server of choice as one test that passes or fails (in this case I use teamcity). What I need for this specific test is the ability to loop through a directory structure testing that our data files can all be parsed without throwing an exception. Because we have 30000+ files that that 1-5 seconds each to parse this test will be run in its own suite. The problem is that I need a way to have one piece of code run as one junit test per file so that if 12 files out of 30000 files fail I can see which 12 failed not just that one failed threw a runtimeexception and stopped the test. I realize that this is not a true ""unit"" test way of doing things but this simulation is very important to make sure that our content providers are kept in check and do not check in invalid files. Any suggestions? Do I need to use BaseTestRunner? I'd write one test that read all the files either in a loop or some other means and collected all the failed files in a collection of some kind for reporting. Maybe a better solution would be a TestNG test with a DataProvider to pass along the list of file paths to read. TestNG will create and run one test for each file path parameter passed in. This is my backup option I'd prefer to have the test show up as a x tests where x is the number of files discovered in our simulation folder.  I think what you want is parameterized tests. It's available if you're using JUnit4 (or TestNG). Since you mention JUnit you'll want to look at the @RunWith(Parameterized.class) and @Parameters annotations' documentation. Thanks this works.  A Junit3 answer: Create a TestSuite that creates the instances of the TestCases that you need with each TestCase initialized according to your dynamic data. The suite will run as a whole within a single JVM instance but the individual TestCases are independent of each other (setUp tearDown get called the error handling is correct reporting gives what you asked for etc). The actual implementation can be a bit clumsy because TestCase conflates the Name of the test with the METHOD to be run but that can be worked around. We normally just combine the suite with the dynamic testcases in the same class and use the suite() method to get the TestSuite. Ant's JUnit task is smart enough to notice this for example. public class DynamicTest extends TestCase { String filename ; public DynamicTest ( String crntFile ) { super(""testMethod""); filename = crntFile ; } // This is gross but necessary if you want to be able to // distinguish which test failed - otherwise they all share // the name DynamicTest.testMethod. public String getName() { return this.getClass().getName() + "" : "" + filename ; } // Here's the actual test public void testMethod() { File f = new File( filename ) ; assertTrue( f.exists() ) ; } // Here's the magic public static TestSuite suite() { TestSuite s = new TestSuite() ; for ( String crntFile : getListOfFiles() ) { s.addTest( new DynamicTest(crntFile ) ) ; } return s ; } } You can of course separate the TestSuite from the TestCase if you prefer. The TestCase doesn't hold up well stand alone though so you'll need to have some care with your naming conventions if your tests are being auto-detected."
715,A,"How do I start unit testing? I know that unit testing is desirable and I am interested in doing unit testing. The only problem is I have no idea how or even where to start really. So my question is: How do I learn about and start unit testing? Specifically I frequently write Java code in NetBeans and C# code in Visual Studio and am interested in what tools to use and how to get my feet wet. Can anyone offer any advice for an absolute unit testing n00b? I know that there are a lot of somewhat similar questions out there but I am less interested in why and more interested in how. Any Video Tutorials? Start small. Unit testing (and automated testing in general) isn't a silver bullet doesn't always apply to every situation and can be a bit of a culture shock. That said if you're writing software that you're selling or that your company relies on I highly recommend adopting it. You'd be surprised how many professional development shops don't. First get comfortable the mechanics of creating and running unit tests with your development tools. Then start with a new (preferably small but not necessarily trivial) class or method that you want to test. Writing tests for existing code has its own challenges which is why you should start with either something brand new or something that you are going to rewrite. You should notice that making this class or method testable (and therefore reusable) has an impact on how you write the code. You should also find that thinking about how to test the code up front forces you to think about and refine the design now instead of some time down the road ""when there's more time"". Things as simple as ""What should be returned if a bad parameter is passed in?"". You should also feel a certain amount of confidence that the code behaves exactly the way you expect it to. If you see a benefit from this exercise then build on it and start applying it to other parts of your code. Over time you'll have confidence in more and more of your software as it becomes more provably correct. The hands on approach helped get my head around the subject better than a lot of the reading material and helped fill in the gaps of things I just didn't understand. Especially where TDD was concerned. It was counter-intuitive until I actually tried it.  Try to read on StackOverflow tag unit-testing :) Is Unit Testing worth the effort? How to make junior programmers write tests? What is unit testing? How do you know what to test when writing unit tests? Another entry point would be the tags junit and nunit There are lots of question dealing this. If you're searching books about Unit Testing try this thread: Good C# Unit testing book. There the famous Kent Beck book is mentioned ""Test Driven Development By Example"". It's worth reading! Good luck!  Check out The Art of Unit Testing by Roy Osherove it's a good book for beginners since it starts at the very beginning.  Find-a-bug-write-a-test The next time you find a bug in your code base before fixing it write a test. The test should fail. Then fix the bug. The test should pass. If the test doesn't pass there's either a bug in your test or a bug in your fix. A person will never find that bug in your code again. The unit tests will find it (and faster than a person can). This is definitely a small start but it gets you into testing. Once you've got the hang of it you'll probably start writing more tests and eventually get a knack for how code will fail and which tests you need (for example: a test for every business rule). Later in your progression you setup a continuous integration server which makes sure your codebase is always solid. I'm not sure this is the best way to start. Often bugs are because of complex interactions between far parts of code. Making a test for an interaction bug is quite difficult especially in a codebase without other tests  If you really want to understand unit testing (and get hooked) try it out it should only take a few hours! First I recommend downloading a unit testing framework such as NUnit (if you want to start with .NET / C#). Most of these frameworks have online documentation that provides a brief introduction like the NUnit Quick Start. Read that documentation then choose a fairly simple self-contained class for which you are responsible. If you can try to choose a class that: Has few or no dependencies on other classes - at least not on complex classes. Has some behavior: a simple container with a bunch of properties won't really show you much about unit testing. Try writing some tests to get good coverage on that class then compile and run the tests. Unit testing is simple to learn and hard to master (apologies for the cliché but it is appropriate here) so once you've done this start reading around: for example guerda has provided several excellent links in another answer to this question. +1 for get started (and the reference to my answer of course :D )  This Tutorial for writing JUnit tests in NetBeans should give you an idea how unit testing is done technically. NUnit for C# works pretty much the same. For an advanced view of how to integrate unit testing into you daily development the standard reference is Kent Beck's ""Test Driven Development By Example"". Here's a broad overview.  I would recommend reading Michael Feathers' ""Working Effectively with Legacy Code"". Old code often ends up being code that's hard to Unit Test. Feathers' book is a great guide in how to refactor your code to the point that unit tests are a snap to write. Not exactly an answer to the question you asked but might be a missing step between where you are and where you need to be to implement some of the answers others have given.  A good start is to buy a good book where you can read about unit-testing. I have a tips on a book called ""Software testing with visual studio team system 2008"" and it takes you trough the basics stuff and background to more higher levels of unit-testing and practises."
716,A,"How do I run JUnit tests during my Ant build script while omitting test classes from my resulting jar? I'm using the Hello World with Ant tutorial from the Ant manual to learn about Ant. The last part of the tutorial involves adding JUnit tests to the project. I've got everything working as described in the tutorial and am now going on to make some minor changes. One of the changes I would like to make is to run the tests during a typical build but not have the *Test.class files end up in the final .jar file for the application. This is because the eventual project I will be working on will be for a device with limited hard drive space and support for only a subset of the Java SDK so I would prefer to just omit these test files entirely from the jar. How do I do this? It would be easy enough to create two separate jars one for testing and one for deployment but this seems less than ideal. My current build.xml file is below. <property name=""src.dir"" value=""src""/> <property name=""build.dir"" value=""build""/> <property name=""classes.dir"" value=""${build.dir}/classes""/> <property name=""jar.dir"" value=""${build.dir}/jar""/> <property name=""lib.dir"" value=""lib""/> <property name=""report.dir"" value=""${build.dir}/junitreport""/> <property name=""main-class"" value=""oata.HelloWorld""/> <path id=""classpath""> <fileset dir=""${lib.dir}"" includes=""**/*.jar""/> <path location=""[LocalPath]/junit-4.8.2.jar""/> </path> <path id=""application"" location=""${jar.dir}/${ant.project.name}.jar""/> <target name=""clean""> <delete dir=""${build.dir}""/> </target> <target name=""compile""> <mkdir dir=""${classes.dir}""/> <javac srcdir=""${src.dir}"" destdir=""${classes.dir}"" classpathref=""classpath""/> <copy todir=""${classes.dir}""> <fileset dir=""${src.dir}"" excludes=""**/*.java""/> </copy> </target> <target name=""jar"" depends=""compile""> <mkdir dir=""${jar.dir}""/> <jar destfile=""${jar.dir}/${ant.project.name}.jar"" basedir=""${classes.dir}""> <manifest> <attribute name=""Main-Class"" value=""${main-class}""/> </manifest> </jar> </target> <target name=""junit"" depends=""jar""> <mkdir dir=""${report.dir}""/> <junit printsummary=""yes"" haltonfailure=""yes"" showoutput=""yes""> <classpath> <path refid=""classpath""/> <path refid=""application""/> </classpath> <formatter type=""xml""/> <batchtest fork=""yes""> <fileset dir=""${src.dir}"" includes=""*Test.java""/> </batchtest> </junit> </target> <target name=""junitreport"" depends=""junit""> <junitreport todir=""${report.dir}""> <fileset dir=""${report.dir}"" includes=""TEST-*.xml""/> <report todir=""${report.dir}""/> </junitreport> </target> <target name=""run"" depends=""junit""> <java fork=""true"" classname=""${main-class}""> <classpath> <path refid=""classpath""/> <path refid=""application""/> </classpath> </java> </target> <target name=""clean-build"" depends=""cleanjunit""/> <target name=""main"" depends=""cleanrun""/> One thing I have tried is modifying the jar command to exclude the *Test.class files ... <jar destfile=""${jar.dir}/${ant.project.name}.jar"" basedir=""${classes.dir}"" excludes=""**/*Test.class""> ... which successfully excludes the test classes but then when the tests are run via the junit target it fails with the following stack trace when run with -v: [LocalPath]\build.xml:44: Test HelloWorldTest failed at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.actOnTestResult(JUnitTask.java:1863) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:814) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1808) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:760) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:390) at org.apache.tools.ant.Target.performTasks(Target.java:411) at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1397) at org.apache.tools.ant.Project.executeTarget(Project.java:1366) at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41) at org.apache.tools.ant.Project.executeTargets(Project.java:1249) at org.apache.tools.ant.Main.runBuild(Main.java:801) at org.apache.tools.ant.Main.startAnt(Main.java:218) at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280) at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109) Try making your jar task DEPEND on the junit task so that it runs the tests after compilation but before jar. Remove the dependency for the junit task as well it should depend on the compile task not the jar task. Thanks that makes a lot more sense than the tutorial and fixes my issue. If you edit your answer to reflect this information I'll accept it. Can you change: <target name=""jar"" depends=""compile""> <mkdir dir=""${jar.dir}""/> <jar destfile=""${jar.dir}/${ant.project.name}.jar"" basedir=""${classes.dir}""> <manifest> <attribute name=""Main-Class"" value=""${main-class}""/> </manifest> </jar> </target> to: <target name=""jar"" depends=""junit""> <mkdir dir=""${jar.dir}""/> <jar destfile=""${jar.dir}/${ant.project.name}.jar""> <fileset dir=""${classes.dir}"" excludes=""**/*Test.class""/> <manifest> <attribute name=""Main-Class"" value=""${main-class}""/> </manifest> </jar> </target> <target name=""junit"" depends=""compile""> <mkdir dir=""${report.dir}""/> <junit printsummary=""yes"" haltonfailure=""yes"" showoutput=""yes""> <classpath> <path refid=""classpath""/> <path refid=""application""/> </classpath> <formatter type=""xml""/> <batchtest fork=""yes""> <fileset dir=""${src.dir}"" includes=""*Test.java""/> </batchtest> </junit> </target> That should exclude the Test classes I believe from the final JAR file. n.b The change in dependencies for each of the tasks. That does not work to exclude the test files from my .jar even after adding the missing asterisk (`**/*Test.class`). I edited my original post with something similar that I've tried. See the above on changing the depends attributes on the tasks.  Based on @Jon's advice I changed the junit target to run against the build/classes folder instead of the jar and updated the dependencies appropriately. My updated build.xml file is below: <project name=""HelloWorld"" basedir=""."" default=""main""> <property name=""src.dir"" value=""src""/> <property name=""build.dir"" value=""build""/> <property name=""classes.dir"" value=""${build.dir}/classes""/> <property name=""jar.dir"" value=""${build.dir}/jar""/> <property name=""lib.dir"" value=""lib""/> <property name=""report.dir"" value=""${build.dir}/junitreport""/> <property name=""main-class"" value=""oata.HelloWorld""/> <path id=""classpath""> <fileset dir=""${lib.dir}"" includes=""**/*.jar""/> <path location=""[LocalPath]/junit-4.8.2.jar""/> </path> <path id=""application"" location=""${jar.dir}/${ant.project.name}.jar""/> <target name=""clean""> <delete dir=""${build.dir}""/> </target> <target name=""compile""> <mkdir dir=""${classes.dir}""/> <javac srcdir=""${src.dir}"" destdir=""${classes.dir}"" classpathref=""classpath""/> <copy todir=""${classes.dir}""> <fileset dir=""${src.dir}"" excludes=""**/*.java""/> </copy> </target> <target name=""jar"" depends=""junit""> <mkdir dir=""${jar.dir}""/> <jar destfile=""${jar.dir}/${ant.project.name}.jar"" basedir=""${classes.dir}"" excludes=""**/*Test.class""> <manifest> <attribute name=""Main-Class"" value=""${main-class}""/> </manifest> </jar> </target> <target name=""junit"" depends=""compile""> <mkdir dir=""${report.dir}""/> <junit printsummary=""yes"" haltonfailure=""yes"" showoutput=""yes""> <classpath> <path refid=""classpath""/> <path location=""${classes.dir}""/> </classpath> <formatter type=""xml""/> <batchtest fork=""yes""> <fileset dir=""${src.dir}"" includes=""*Test.java""/> </batchtest> </junit> </target> <target name=""junitreport"" depends=""junit""> <junitreport todir=""${report.dir}""> <fileset dir=""${report.dir}"" includes=""TEST-*.xml""/> <report todir=""${report.dir}""/> </junitreport> </target> <target name=""run"" depends=""jar""> <java fork=""true"" classname=""${main-class}""> <classpath> <path refid=""classpath""/> <path refid=""application""/> </classpath> </java> </target> <target name=""clean-build"" depends=""cleanjar""/> <target name=""main"" depends=""cleanrun""/> </project>"
717,A,"ExpectedException in jUnit? Is there an equivalent to NUnit's ExpectedException or Assert.Throws<> in jUnit? If you need to test many exception cases for a specific type of test it's useful to write a generic method: private void expectException(Class<? extends Throwable> c Object testObject) { String err = ""Expected "" + c + "" for '"" + testObject + ""'.""; try { // insert a test using testObject which should throw an exception } catch (Throwable t) { if (c.isAssignableFrom(t.getClass())) return; fail(err + "" Got "" + t + "".""); } fail(""Suceeded. "" + err); } Then you can write one test per line improving clarity and conciseness: @Test public void myTest() { expectException(IllegalArgumentException.class ""foo""); expectException(IndexOutOfBoundsException.class ""bar""); expectException(ClassNotFoundException.class ""baz""); } @Tim: I clarified now the benefits of my approach. I hope it's clear now that I am not re-implementing what's already available. Do not reimplement yourself what is already available in the library: https://github.com/junit-team/junit/wiki/Exception-testing  junit4: @Test(expected = org.dom4j.DocumentException.class) void shouldThrowException() { getFile(null); } junit3: void testShouldThrowException() { try { getFile(null); fail(""Expected Exception DocumentException""); } catch(DocumentException e) {} } only available in jUnit4 I have updated this answer to include a way to do this in jUnit3 The good thing about the ""JUnit3"" approach is that you can then write one exception-test-case per line whereas you need five lines for each with the ""JUnit4"" approach. See my answer for more information: http://stackoverflow.com/a/15385613/974531 Actually the answer using `ExpectedException` by @Maciej is better: http://stackoverflow.com/a/4265441/53444  If you are using Groovy for your junit tests you can use shouldFail. Here is an example using junit3 style: void testShouldThrowException() { def message = shouldFail(DocumentException) { documentService.getFile(null) } assert message == 'Document could not be saved because it ate the homework.' }  You might also consider taking a look at the ExpectedException class which provides richer exception matching. https://github.com/junit-team/junit/wiki/Exception-testing Not only you can match the exception class but also you can apply custom matchers to its message."
718,A,Using AspectJ to replace third party objects with mocks in Unit Tests I'm writing a web services client using Spring-WS and the WebServiceTemplate class. Down in the bowls of the WebServiceTemplate class a WebServiceConnection is created. WebServiceConnection.send is used to actually send the message. What I'd like to do is intercept the call to WebServiceConnection.send and replace it with some logic that examines the object passed to WebServiceConnetion.send. It strikes me that this would be a good place to use Aspects. However I'm not sure how I can have the aspects run only when I'm executing the unit tests. I would also like to have a different aspects run based on what tests I'm executing. Anyone have any ideas on how to do this? You can use Runtime Weaving with AspectJ. You don't have to compile the aspects into yout app it is enought to include them when testing. Since there has to be a META-INF/aop.xml on the classpath when using AspectJ and since you have to start the JVM with -agent:myPath/aspectjweaver.jar you have your tools at hand to use AspectJ only when testing. Oh and if you use AspectJ to compile your app you can still use additional aspects when testing if you combine runtime weaving and compile time weaving. This isn't quite the solution I'd hoped for but it seems to work. Adds a bit of complexity to my build scripts but nothing I can't handle. Thanks
719,A,Reflection in unit tests for checking code coverage Here's the scenario. I have VO (Value Objects) or DTO objects that are just containers for data. When I take those and split them apart for saving into a DB that (for lots of reasons) doesn't map to the VO's elegantly I want to test to see if each field is successfully being created in the database and successfully read back in to rebuild the VO. Is there a way I can test that my tests cover every field in the VO? I had an idea about using reflection to iterate through the fields of the VO's as part of the solution but maybe you guys have solved the problem before? I want this test to fail when I add fields in the VO and don't remember to add checks for it in my tests. dev environment: Using JUnit Hibernate/Spring and Eclipse I would recommend cobertura for this task. You will get a complete code coverage report after you run your tests and if you use the cobertura-check ant task you can add checks for the coverage and stop the ant call with the property haltonfailure.  Keep it simple: write one test per VO/DTO: fill the VO/DTO with test data save it (optional: check everything has been correctly save at the database level using pure JDBC) load it check that the loaded VO/DTO and the original one matches Productive code will evolve and tests will need to be maintained as well. Making tests the simplest as possible even if they are repetitive is IMHO the best approach. Over-engineering the tests or testing framework itself to make tests generic (e.g. by reading fields with reflection and filling VO/DTO automatically) leads to several problems: time spent to write the test is higher bug might be introduced in the test themselves maintenance of the test is harder because they are more sophisticated tests are harder to evolve e.g. the generic code will maybe not work for new kinds of VO/DTO that differ slightly from the other and will be introduced later (it's just an example) tests can not be used easily as example of how the productive code works Test and productive code are very different in nature. In productive code you try to avoid duplication and maximize reuse. Productive code can be complicated because it is tested. On the other hand you should try to have tests as simple as possible and duplication is ok. If a duplicated portion is broken the test will fail anyway. When productive code change this may require several tests to be trivially changed. With the problem that tests are seen as boring piece of code. But I think that's the way they should be. If I however got your question wrong just let me know. I like your approach of keeping the tests simple and repetitive.  You could make it part of the validation of the VO. If the fields aren't set when you use a getter it can throw an exception. not quite what I mean. I want to make sure that All the fields that are in the VO are able to CRUD from the DB backend because it's not guaranteed I have to code it explicitly for each one. Basically I want to write a test case that makes sure I'm calling all the setters and getters that are in the VO in my test cases automatically I just don't know how to do it.
720,A,"How to check if a Java class contains JUnit4 tests? I have a Java class. How can I check if the class contains methods that are JUnit4 tests? Do I have to do an iteration on all methods using reflection or does JUnit4 supply such a check? Edit: since comments cannot contain code I placed my code based on the answer here: private static boolean containsUnitTests(Class<?> clazz) { List<FrameworkMethod> methods= new TestClass(clazz).getAnnotatedMethods(Test.class); for (FrameworkMethod eachTestMethod : methods) { List<Throwable> errors = new ArrayList<Throwable>(); eachTestMethod.validatePublicVoidNoArg(false errors); if (errors.isEmpty()) { return true; } else { throw ExceptionUtils.toUncheked(errors.get(0)); } } return false; } Use built-in JUnit 4 class org.junit.runners.model.FrameworkMethod to check methods. /** * Get all 'Public' 'Void'  non-static and no-argument methods * in given Class. * * @param clazz * @return Validate methods list */ static List<Method> getValidatePublicVoidNoArgMethods(Class clazz) { List<Method> result = new ArrayList<Method>(); List<FrameworkMethod> methods= new TestClass(clazz).getAnnotatedMethods(Test.class); for (FrameworkMethod eachTestMethod : methods){ List<Throwable> errors = new ArrayList<Throwable>(); eachTestMethod.validatePublicVoidNoArg(false errors); if (errors.isEmpty()) { result.add(eachTestMethod.getMethod()); } } return result; } @ohadshai: A logic problem on 'if (errors.isEmpty()) {return true;}'. What is following a method fails the validation? You may fix it easily:) Thanks! see how I used it in the question body.  JUnit is commonly configured using either an annotation based approach or by extending TestCase. In the latter case I would use reflection to look for implemented interfaces (object.getClass().getInterfaces()). In the former case I would iterate all methods looking for @Test annotations e.g. Object object; // The object to examine for (Method method : object.getClass().getDeclaredMethods()) { Annotation a = method.getAnnotation(Test.class); if (a != null) { // found JUnit test } }  Assuming that your question can be reformulated as ""How can I check if the class contains methods with org.junit.Test annotation?"" then use Method#isAnnotationPresent(). Here's a kickoff example: for (Method method : Foo.class.getDeclaredMethods()) { if (method.isAnnotationPresent(org.junit.Test.class)) { System.out.println(""Method "" + method + "" has junit @Test annotation.""); } }"
721,A,"JUnit - Using the Wrong Assert I quote this post from exubero's entry. I think this entry will benefit everyone who is doing a unit test: There are a large number of different methods beginning with assert defined in Junit's Assert class. Each of these methods has slightly different arguments and semantics about what they are asserting. The following shows some irregular uses of assertTrue: assertTrue(""Objects must be the same"" expected == actual); assertTrue(""Objects must be equal"" expected.equals(actual)); assertTrue(""Object must be null"" actual == null); assertTrue(""Object must not be null"" actual != null); Some unit testing experts pointed out that the above code could be better written as: assertSame(""Objects must be the same"" expected actual); assertEquals(""Objects must be equal"" expected actual); assertNull(""Object must be null"" actual); assertNotNull(""Object must not be null"" actual); One of the advantage of using the appropriate 'assertXXX()' will increase the readability of the unit test. Can anyone point out what other benefit of using the appropriate 'assertXXX()' ? This is not a blog. Please don't post rants here and then tack on a minor question at the end. Noted on that. fair enough. I edited the wordings in the post. I never meant this question as 'rant'. My observation poses an interesting debate that can help every one to better write a unit test. So I just would like to see what other think about it. It isn't really a debate is it? You should only use `assertTrue` for the specific case where testing a boolean method. What's the issue? Did you meet someone who was confused by assertTrue? In addition to what @Vadim daid above using the proper assert may guard against bugs created by cut-copy-paste of tests. As an example assertTrue(""Objects must not be the same"" expected != actual); Is then copied and modified to assertTrue(""Objects must not be the same"" newobject == actual); When the code changes and this test fails and the comment fools the next developer to ""fix"" the code in a way that introduces a new bug. If the cut-copy-paste-code was something like this: assertFalse(""Objects must be the same"" newobject == actual); The dis-congruity of the comment the assertion and the test case may be more noticeable. And yes I've seen this happen.  I'm not a Java developer and I don't know what JUnit outputs when an assertion fails. Many unit testing frameworks that I've been using output better error information when use something like assertEquals. Let me show you an example what I'm talking about: assertTrue(""Objects must be equal"" ""One"" == ""Two""); assertEquals(""Objects must be equal"" ""One"" ""Two""); In first case you can have an error output like this: Error: Expected true actual was false. Output for the second case: Error: Exected ""One"" actual was ""Two"". As you can see the second case gives better more meaningful information. Hey vadim thank you for pointing this one out. You're completely right. Using assertEquals() will give a better output when there is an error compare to assertTrue()."
722,A,"@Autowired annotation not able to inject bean in JUnit class my test class: public class myTest extends TestCase{ @Autowired BeanClass beanObject public void beanTest() { Classdata data = beanObject.getMethod(); } } I am getting a null pointer exception at line: Classdata data = beanObject.getMethod(); the beanObject.getMethod(); precisely gives nullpointer exception How should i make possible the autowiring of the field beanObject in my Junit class so that i can use the methods from the ""BeanClass"" class? Copied from Comments: in plain terms.. beanClass is an interface which has certain methods.. i have tagged that beanClass with @Service(""beanObject"") annotation..that banClass is implemented by beanClassImpl class which has the method implementations.. i need to use those implementations in my testClass to get the data to be compared.. for that i am doing @Autowired beanClass beanObject in my testClass.. m i going terribly wrong somewhere? Where is Spring supposed to get the `beanObject` from? Have you declared it somewhere? beanObject already is a beanClass objectwhere i have declared the beanClass as a @Service(""beanObject"") annotation. You probably need to decorate your tests with these annotations: @ContextConfiguration(locations = {/* your xml locations here */}) @RunWith(SpringJUnit4ClassRunner.class) Or if you use JUnit 3.x you should extend from AbstractJUnit38SpringContextTests Reference: TestContext support classes Update: The problem seems to be that the context file can't be found (see discussion in comments). in plain terms.. beanClass is an interface which has certain methods.. i have tagged that beanClass with @Service(""beanObject"") annotation..that banClass is implemented by beanClassImpl class which has the method implementations.. i need to use those implementations in my testClass to get the data to be compared.. for that i am doing @Autowired beanClass beanObject in my testClass.. m i going terribly wrong somewhere? More Updates: Don't annotate the interface annotate the implementing class. Annotating the interface with @Service has no effect! yes i already did add those two statements: @ContextConfiguration(locations = {spring-servlet.xml}) @RunWith(SpringJUnit4ClassRunner.class) but still the same.. null pointer exception also... my spring-servlet.xml file is directly under WEB-INF folder.. @user582811 OK then you should edit the question and show the relevant parts of `spring-servlet.xml` @user582811 under WEB-INF? And the test finds the context file? the test is not able to find the context file @user582811: thought so. try this: `locations = ""classpath:WEB-INF/spring-servlet.xml""` since the test was not able to find the context file...i tried putting the spring-servlet.xml directly under root folder that is src.and then the two statements @ContextConfiguration(locations = {""spring-servlet.xml"") @RunWith(SpringJUnit4ClassRunner.class) ..but in vain\ but if WEB-INF is inside src/main/webapp that won't help either then you'll have to use ""file:src/main/webapp/WEB-INF/spring-servlet.xml"" @user582811 use prefixes either classpath: or file:. Reference: http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/resources.html#resources-resourceloader i tried this:@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""classpath:WEB-INF/liiraWeb-servlet.xml""}) and public class myTest extends TestCase{ @Autowired BeanClass beanObject public void beanTest() { Classdata data = beanObject.getMethod(); } }..... still it gives nullpointer... m sorry m a newbie to springs... but m giving my efforts... and really appreciate ur help in it.. this autowiring thing does not seems to work in a junit class... also tried to manually autowire the bean like--->ApplicationContext context = new ClassPathXmlApplicationContext(""spring-servlet.xml""); context.getAutowireCapableBeanFactory().autowireBean(beanObject);.. this attempt too it is not able to find the context file!! @user582811 autowiring is not the problem but spring contexts usually don't reside under WEB-INF and WEB-INF is usually not on the class path during development thats what i did.. i relocated the spring-servlet.xml file under src folder outside of the web-inf folder and then tried autowiring.. still no progress! so what needs to be done to simply autowire a service object in plain terms.. beanClass is an interface which has certain methods.. i have tagged that beanClass with @Service(""beanObject"") annotation..that banClass is implemented by beanClassImpl class which has the method implementations.. i need to use those implementations in my testClass to get the data to be compared.. for that i am doing @Autowired beanClass beanObject in my testClass.. m i going terribly wrong somewhere? @alpesh003 see my updated answer yes i did that.. annotated the implementing class with @Service(""beanObject"")... and tried using beanClassImpl autowiring... but still the same.. nullpointerexception @alpesh003 yes obviously if the context file isn't found then the bean isn't wired either. Sorry I don't mean to be rude but the info I get from you is too vague and I actually have work to do :-) Thanx a lot... actually the run configurations were picking up JUnit 3 and also had to make some connection factory related settings in the spring-servlet.xml file... its solved... thanx for ur help and valuable time! :) @alpesh003 you're welcome. How about an accept checkmark?"
723,A,Is the test suite setup method executed once for each test or only once for all? I know the answer may differ for each test framework. But for the ones you know what should happen? In MSTest you have TestInitializeAttribute When run in a load test the method marked with this attribute will run once for every virtual user iteration in the test. If you need to do initialization operations once that apply to the entire test use the ClassInitializeAttribute. AssemblyInitializeAttribute is run once for all tests in all classes.  This naturally depends on the frameworks and for the concrete answers to this you should check the relevant documentation. Set up methods for tests or fixtures are useful but they should not be abused. If unit tests have complex set up methods you could argue they are more so integration tests and thus should be refactored. A complex test set up is a code smell. On the other hand set up methods used wisely can reduce duplication and make tests more readable and maintainable.  In NUnit you have TestFixtureSetUp which runs only once before all tests in the fixture run and SetUp which runs before each test method is run.  In junit4 you have annotations available to mark both kind of setup/teardown methods. Here is the summary: running setup before each test suite use @BeforeClass running tear down after each test suite use @AfterClass running setup before each test method in your suite use @Before running tear down after each test method in your suite use @After
724,A,"spring 3 autowiring and junit testing My code: @Component public class A { @Autowired private B b; public void method() {} } public interface X {...} @Component public class B implements X { ... } I want to test in isolation class A. Do I have to mock class B? If yes how? Because it is autowired and there is no setter where i could send the mocked object. This forum discussion makes sense to me. You can declare your private member b as a type of InterfaceB which is implemented by the class B (ie: service-oriented) then declare a MockB class would also implement the same interface. In your test environment application context you declare MockB class and your production application context you declare the normal B class and in either case the code for class A does not need to be changed since it will be auto-wired. With @Autowire you don't declare your beans in application context they're just here in the classpath. So this solution doesn't work. This solution works if you wire the bean byType (in 2.5) or declare a qualifier (3.0). In Spring 2.5 you can autowire to a bean declared in your XML Application Context configuration as can be read in documentation http://static.springsource.org/spring/docs/2.5.x/reference/beans.html#beans-factory-autowire. In Spring 3.0 they still allow you to do this with the @Qualifier annotation (documented at http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/beans.html#beans-autowired-annotation).  I want to test in isolation class A. You should absolutely mock B rather than instantiate and inject an instance of B. The point is to test A whether or not B works so you should not allow a potentially broken B interfere with the testing of A. That said I highly recommend Mockito. As mocking frameworks go it is extremely easy to use. You would write something like the following: @Test public void testA() { A a = new A(); B b = Mockito.mock(B.class); // create a mock of B Mockito.when(b.getMeaningOfLife()).thenReturn(42); // define mocked behavior of b ReflectionTestUtils.setField(a ""b"" b); // inject b into the B attribute of A a.method(); // call whatever asserts you need here } +1 for mocking getMeaningOfLife() to 42 :-). With the new version of Mockito I'd use the `@InjectMocks` annotation on the declaration of `A` and get rid of the reflection `setField(..)`  Here's an example of how I got my tests working with Spring 3.1 JUnit 4.7 and Mockito 1.9: FooService.java public class FooService { @Autowired private FooDAO fooDAO; public Foo find(Long id) { return fooDAO.findById(id); } } FooDAO.java public class FooDAO { public Foo findById(Long id) { /* implementation */ } } FooServiceTest.java @RunWith(MockitoJUnitRunner.class) public class FooServiceTest { @Mock private FooDAO mockFooDAO; @InjectMocks private FooService fooService = new FooService(); @Test public final void findAll() { Foo foo = new Foo(1L); when(mockFooDAO.findById(foo.getId()).thenReturn(foo); Foo found = fooService.findById(foo.getId()); assertEquals(foo found); } } If not using `MockitoJUnitRunner` important to remember in `FooServiceTest`: `@Before public void initMocks() { MockitoAnnotations.initMocks(this); }`  You can inject the field via reflection using Spring's ReflectionTestUtils.setField (or the junit extension PrivateAccessor) or you can create a mock application context and load that. Though for a simple unit (non-integration) test I favor using reflection for simplicity."
725,A,"How to mock object construction? Is there a way to mock object construction using JMock in Java? For example if I have a method as such: public Object createObject(String objectType) { if(objectType.equals(""Integer"") { return new Integer(); } else if (objectType.equals(""String"") { return new String(); } } ...is there a way to mock out the expectation of the object construction in a test method? I'd like to be able to place expectations that certain constructors are being called rather than having an extra bit of code to check the type (as it won't always be as convoluted and simple as my example). So instead of: assertTrue(a.createObject() instanceof Integer); I could have an expectation of the certain constructor being called. Just to make it a bit cleaner and express what is actually being tested in a more readable way. Please excuse the simple example the actual problem I'm working on is a bit more complicated but having the expectation would simplify it. For a bit more background: I have a simple factory method which creates wrapper objects. The objects being wrapped can require parameters which are difficult to obtain in a test class (it's pre-existing code) so it is difficult to construct them. Perhaps closer to what I'm actually looking for is: is there a way to mock an entire class (using CGLib) in one fell swoop without specifying every method to stub out? So the mock is being wrapped in a constructor so obviously methods can be called on it is JMock capable of dynamically mocking out each method? My guess is no as that would be pretty complicated. But knowing I'm barking up the wrong tree is valuable too :-) You're factory could have it's own factories for each type it might construct (an Integer factory a String factory etc) but you're right that's veering toward overly complicated. You're assert instanceof test is probably the best way to go. jmockit can do this. See my answer in http://stackoverflow.com/questions/22697#93675 Yes! JMockit RULES! http://stackoverflow.com/questions/190597/how-to-go-about-mocking-a-class-with-final-methods  Dependency Injection or Inversion of Control. Alternatively use the Abstract Factory design pattern for all the objects that you create. When you are in Unit Test mode inject an Testing Factory which will tell you what are you creating then include the assertion code in the Testing Factory to check the results (inversion of control). To leave your code as clean as possible create an internal protected interface implement the interface (your factory) with the production code as an internal class. Add a static variable type of your interface initialized to your default factory. Add static setter for the factory and you are done. In your test code (must be in the same package otherwise the internal interface must be public) create an anonymous or internal class with the assertion code and the test code. Then in your test initialize the target class assign (inject) the test factory and run the methods of your target class.  Alas I think I'm guilty of asking the wrong question. The simple factory I was trying to test looked something like: public Wrapper wrapObject(Object toWrap) { if(toWrap instanceof ClassA) { return new Wrapper((ClassA) toWrap); } else if (toWrap instanceof ClassB) { return new Wrapper((ClassB) toWrap); } // etc else { return null; } } I was asking the question how to find if ""new ClassAWrapper( )"" was called because the object toWrap was hard to obtain in an isolated test. And the wrapper (if it can even be called that) is kind of weird as it uses the same class to wrap different objects just uses different constructors[1]. I suspect that if I had asked the question a bit better I would have quickly received the answer: ""You should mock Object toWrap to match the instances you're testing for in different test methods and inspect the resulting Wrapper object to find the correct type is returned... and hope you're lucky enough that you don't have to mock out the world to create the different instances ;-)"" I now have an okay solution to the immediate problem thanks! [1] opening up the question of whether this should be refactored is well out of the scope of my current problem :-)  I hope there is none. Mocks are supposed to mock interfaces which have no constructors... just methods. Something seems to be amiss in your approach to testing here. Any reason why you need to test that explicit constructors are being called ? Asserting the type of returned object seems okay for testing factory implementations. Treat createObject as a blackbox.. examine what it returns but dont micromanage how it does it. No one likes that :) Update on the Update: Ouch! Desperate measures for desperate times eh? I'd be surprised if JMock allows that... as I said it works on interfaces.. not concrete types. So Either try and expend some effort on getting those pesky input objects 'instantiable' under the test harness. Go Bottom up in your approach. If that is infeasible manually test it out with breakpoints (I know it sucks). Then stick a ""Touch it at your own risk"" comment in a visible zone in the source file and move ahead. Fight another day. It is a simple factory method. The problem is that the factory creates wrappers for other objects (taking those in as a parameter during construction) and returns those. I'd like to be able to test that the constructor is being called rather than mock out or instantiate the object being wrapped. The problem with that is that the factory method takes an object which is hard to recreate in isolation - refactoring to allow that is pretty much out of the question for now. Although... mocking those is probably the answer...  Are you familiar with Dependency Injection? If no then you ceartanly would benefit from learning about that concept. I guess the good-old Inversion of Control Containers and the Dependency Injection pattern by Martin Fowler will serve as a good introduction. With Dependency Injection (DI) you would have a DI container object that is able to create all kinds of classes for you. Then your object would make use of the DI container to instanciate classes and you would mock the DI container to test that the class creates instances of expected classes.  The only thing I can think of is to have the create method on at factory object which you would than mock. But in terms of mocking a constructor call no. Mock objects presuppose the existence of the object whereas a constructor presuppose that the object doesn't exist. At least in java where allocation and initialization happen together."
726,A,"Play! + Siena + GAE + JUnit I am trying to get some basic unit tests up and running on the Play! framework using the Siena persistence library with GAE as the intended deployment target. I have the project configured properly and can deploy the app to GAE. I created a basic domain object: public class User extends Model { @Id(Generator.AUTO_INCREMENT) public Long id; @Column(""first_name"") public String firstName; @Column(""last_name"") public String lastName; @Column(""email"") public String email; public User(String firstName String lastName String email) { this.firstName = firstName; this.lastName = lastName; this.email = email; } public static Query<User> all() { return Model.all(User.class); } } and a basic unit test: public class BasicTest extends UnitTest { @Before public void setup() { Fixtures.deleteAll(); } @Test public void canCreateUser() { new User(""Jason""""Miesionczek""""atmospherian@gmail.com"").insert(); User user = User.all().fetch().get(0); assertNotNull(user); assertEquals(1User.all().count()); } } I understand that in Play! 1.0.3 Fixtures support for Siena is not there yet which should be fixed in 1.1 but in the mean time what should i use instead of Fixtures.deleteAll() to clear the test db before each test? Right now my second assertion fails because the database retains the previously inserted records. You would need to do a delete per table. E.g: Model.all(User.class).delete();  I don't know if it's already to late but I've added some Fixture support for Siena (tested with play 1.1 gae 1.4 siena 1.3 crudsiena 1.2). It supports deleteAll() and load(): SienaFixture.java http://pastie.org/1367878 I am pretty new to play and siena (1 week) so there is probably a better way of doing it (especially deleteAll()). Thanks this helped a lot"
727,A,"Running .class file with a .java file I have a ListTester.java file in which I've created with some unit tests in there to check the built in List class in Java. I have also been given a List.class file to have the junit tests check against to make sure they are correct. However i'm not sure how to make sure the .class file is utilized by my .java file when it runs the tests. How would I go about making it work? From what I was told I can put it in the same directory as my List12Tester.java file and it should use it automatically. Note that you can *not* run .java files. They are compiled to .class files (in this case by your IDE) and then executed by a JVM. The IDE tends to mask this process but it's important to understand to avoid the situation known as *""CLASSPATH hell""*. Just javac your List12Tester (make sure to also put junit in the classpath) and then run it from the console. IDE would be a better choice though... Use an IDE like Eclipse. Add the directory where the List12.class resides as a dependency of your project. And tell the people giving you class-files to document and package (in a jar) their code correctly. This has the effect of placing the .class file in the CLASSPATH. If you are running your app from a command line then you would need to put the .class file you are given and the one generated from compiling your .java class into the CLASSPATH. The JVM will then find them as necessary when they are referenced. I was using an IDE in this case Eclipse. Your answer helped me. I ended up sticking the .class file in the ""Bin"" directory which had the Tester file's class file and that seemed to do the trick. Thanks for the help."
728,A,JUnit Test Runner that creates tests just before running them I use JUnit 3.x TestRunner that intantiates all tests at once before running them. Is there a Test Runner available that would create each test (or at least each test suite's tests) just before running them? I can use JUnit 4.x runners but my tests are 3.x tests. We have some extensive test infrastructure code that helps us organize test environment and test data. Unfortunately some tests use constructor and some tests use setUp to drive it. The issue we are having is time-related. Rather than refactoring to move things from constructors to setUp it would be easier to have constructor to run just before setUp. Also I can't see any significant difference between unit test framework creating tests (execute constructors) JIT or lining them up before tests run. Could you please provide some more information why you need to do this? I'm not the only one who suspects there's something more to know. You are probably doing it wrong. Each unit test should be self-contained and not depend on any other test results. Otherwise when one of the tests break it will break all the tests that depend on it. So you will see a lot of errors without easy way to understand what is the actual cause. On the other hand if all unit tests are independent a broken test is extremely easy to debug and fix. EDIT: I am assuming the reason you ask the original question is because you have some dependencies in your test. If I am wrong please ignore this answer :)  In JUnit 3 you'd need to write your own TestSuite class that delayed instantiation of the tests in the suite.
729,A,Why is my test not running Strangest thing happened my test was running ok and now its not anymore I didn't change the code at all here is the exception : Class not found com.example.test java.lang.ClassNotFoundException: com.example.test at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.loadClass(RemoteTestRunner.java:693) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.loadClasses(RemoteTestRunner.java:429) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:452) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Have you tried to restart the IDE(Eclipse)? @Petar Minchev yes 2 hints that may work : - refresh your project (F5) - clean your project (Project -> Clean) Practices told me none of the above works. Check run config of the JUnit test. In the classpath tab resotre default entries once in case that is not the case already. In project properties check if both junit package and classes source folder is in the source package and there is an actual class compilation existring for the class under test in your expected target folder.  If you use maven to build your project then it puts all compiled java classes in folder like target/classes and Test classes under target/test-classes. So I guess eclipse won't able to find classes from target/test-classes. If you want to run JUnit classes from Eclipse use Eclipse->Project->clean and then you will have all classes under same target configured in eclipse.  I had similar issue and problem was because of broken build paths to external libraries (e.g. Selenium server). So first of all please open Build path (right click project -> Build Path -> Configure Build Path) and verify that all external libraries can be loaded successfully (there are messages like 'cannot load'). Most likely a library was renamed/moved while optimization or adding external tools like Ant. Hope this helps and it will save your time goggling around :) WBR Andrey  JVM is not able to find the class test. is the class name correct. I mean is it not com.example.Test? @GK it doesn't matter the package name I use this test to test my class didn't change it at all just changed my class and tested method for this class like several times up to now it just stopped working out of nothing Reinstalling Eclipse will help for sure:) Just backup your workspace and use it later again. Well before doing reinstall try chburd hints. Did you remove any references from the its buildpath. Or the resource to which the project's buildpath is referencing. If you are using eclipse then build your project once. @GK I just changed my class nothing else  I see that you are using Eclipse but are you also using some other external build/test tool like Ant or Maven? Sometimes Eclipse can get out of sync when also using other external tools. If you are using external tools do whatever is necessary to clean up generated artifacts (e.g. mvn clean). Then refresh the Eclipse project and do a clean build as suggested previously. Then try running your unit test again using Eclipse. Good luck and hope this helps.
730,A,SpringJunit4ClassRunner -- can I change the lifetime of the injected resources? By experiment I find that SpringJunit4ClassRunner treats the context and its beans as 'class scope' in the JUnit sense of scope. It inititializes my beans once for the entire set of tests in the class. Is there any way to use this mechanism and get these things to be 'test scope'? In short I wish that the context was being loaded as @Before instead of @BeforeClass. You can annotate test methods that dirty the Spring context with @DirtiesContext (documented here) causing the context to be re-loaded for subsequent tests in the class. I realise that this isn't quite what you're asking for but perhaps it will do what you need.
731,A,"Can JUnit Ant task report ignored tests? We're currently using JUnit 4.4 and Ant 1.7.1 for our builds. I have some test cases that are @Ignored. When I run them in Eclipse the JUnit test runner reports them as ignored. I would like to see them listed in the XML output from Ant (so I can report on them) but they do not seem to be there. Does anyone have this working? Is there a switch to turn them on? An upgrade I need to do? A fix for this issue has now been applied to the head of Ant core scheduled for release as part of the upcoming version of Ant 1.9.0. It should be possible to try this fix locally be replacing ant-junit.jar in your Ant distribution's lib directory with the version from the nightly builds or by running the full nightly Ant distribution or by building the Ant sources directly. Since the Ant team are currently voting on preparing a new release it may just be worth waiting for 1.9.0 to be officially packaged and pushed out for download.  Just tried Ant 1.9.0 with JUnit 4.11. If you use <junit printsummary=""on""> you'll get output like: [junit] Running com.example.IgnoredTest [junit] Tests run: 1 Failures: 0 Errors: 0 Skipped: 1 Time elapsed: 0.01 [junit] Running com.example.PassingTest [junit] Tests run: 1 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.01 [junit] Running com.example.FailingTest [junit] Tests run: 1 Failures: 1 Errors: 0 Skipped: 0 Time elapsed: 0.01 I think it'd be preferrable if we could get output like this with printsummary=off: [junit] Test com.example.IgnoredTest SKIPPED [junit] Test com.example.FailingTest FAILED but it seems the more verbose output above is the best we can do unless I'm missing some obscure trick with one of the junit task arguments.  It looks like this is a known Ant issue/bug. Per mc1arke's reply below this bug has been fixed in Ant 1.9.0. I have not verified this for myself as my current projects do not use Ant.  This thread talks about the same issue but it provides some additional information: you can get data on ignored tests when running the tests using maven surefire and hudson is able to display that data. http://jenkins.361315.n4.nabble.com/Is-it-possible-to-show-Ignore-JUnit-tests-td1565288.html"
732,A,"JUnit 4 compare Collections How would you succinctly assert the equality of collections elements specifically a Set in JUnit 4? check this SO post http://stackoverflow.com/questions/1086691/collectionassert-in-junit Are you trying to assert that two Sets are equal to each other (contain the same elements) or that two elements of the same Set are equal? I need to see that the elements of two Sets are equal If you want to check whether a List or Set contains a set of specific values (instead of comparing it with an already existing collection) often the toString method of collections is handy: String[] actualResult = calltestedmethod(); assertEquals(""[foo bar]"" Arrays.asList(actualResult).toString()); List otherResult = callothertestedmethod(); assertEquals(""[42 mice]"" otherResult.toString()); This is a bit shorter than first constructing the expected collection and comparing it with the actual collection.  Check this article. One example from there: @Test public void listEquality() { List<Integer> expected = new ArrayList<Integer>(); expected.add(5); List<Integer> actual = new ArrayList<Integer>(); actual.add(5); assertEquals(expected actual); } short but great Link explains really fast what you can do with Junit4-  A particularly interesting case is when you compare  java.util.Arrays$ArrayList<[[namevaluetype] [name1value1type1]]> and  java.util.Collections$UnmodifiableCollection<[[namevaluetype] [name1value1type1]]> So far the only solution I see is to change both of them into sets assertEquals(new HashSet<CustomAttribute>(customAttributes) new HashSet<CustomAttribute>(result.getCustomAttributes())); Or I could compare them element by element.  As an additional method that is array based ... you can consider using unordered array assertions in junitx . Although the Apache CollectionUtils example will work there is a pacakge of solid assertion extensions there as well : I think that the ArrayAssert.assertEquivalenceArrays(new Integer[]{123} new Integer[]{132}); approach will be much more readable and debuggable for you (all Collections support toArray() so it should be easy enough to use the ArrayAssert methods. Of course the downside here is that junitx is an additional jar file or maven entry...  <dependency org=""junit-addons"" name=""junit-addons"" rev=""1.4""/>  Apache commons to the rescue again. assertTrue(CollectionUtils.isEqualCollection(coll1 coll2)); Works like a charm. I don't know why but I found that with collections the following assertEquals(coll1 coll2) doesn't always work. In the case where it failed for me I had two collections backed by Sets. Neither hamcrest nor junit would say the collections were equal even though I knew for sure that they were. Using CollectionUtils it works perfectly. This is actually trivial the tricky part is to clearly indicate the difference to the caller  with hamcrest: assertThat(s1 is(s2)); with plain assert: assertEquals(s1 s2); NB:t the equals() method of the concrete set class is used  This can be done by rolling out your own version of Assert. public static void assertEquals(Collection expected Collection actual); public static void assertEquals(Collection expected Collection actual boolean ordered); Sample method call: int[] input = new int[] { 3 2 7 5}; int[] expected = new int[] { 2 3 5 7}; int[] actual = MySortModule(input); // sample class that sorts an integer array AssertHelper.assertEquals(expected actual); I have written a helper class which does that. You can visit this link to view the blog and download the source code.  You can just assert that the two Sets are equal to one another which invokes the Set equals() method. public class SimpleTest { private Set<String> setA; private Set<String> setB; @Before public void setUp() { setA = new HashSet<String>(); setA.add(""Testing...""); setB = new HashSet<String>(); setB.add(""Testing...""); } @Test public void testEqualSets() { assertEquals( setA setB ); } } This test will pass if the two Sets are the same size and contain the same elements. Works for Map too. This does not display very good results in the report. If your toStrings are clearly defined it is better but still not good (A small difference can end up with a page of text) @Giodude Do you have `equals` and `hashCode` implemented in the class that you're storing in your Hashtable? As you can see those are just strings and a long... I'm testing Avro to serialize and de-serialize a map and that's the result. I think there's gotta be something fishy going on with the way the strings are serialized and de-serialized that makes the test fail but I can't seem to find the problem. Uhm how come I get: java.lang.AssertionError: expected: java.util.Hashtable<{CompanyName=8PKQ9va3nW8pRWb4SjPF2DvdQDBmlZ Ric=sZwmXAdYKv Category=AvrIfd QuoteId=4342740204922826921}> but was: java.util.Hashtable<{CompanyName=8PKQ9va3nW8pRWb4SjPF2DvdQDBmlZ Ric=sZwmXAdYKv Category=AvrIfd QuoteId=4342740204922826921}>"
733,A,"moving from Java/Junit4 to Scala/ScalaTest I have some questions regarding ScalaTest: How can I ensure test-execution ORDER with ScalaTest can I continue to use Junit4's @Suite.SuiteClasses annotation or is there another way without using Junit at all? @RunWith(Suite.class) @Suite.SuiteClasses({ FirstTest.class SecondTest.class }) public class MyTestSuite { @BeforeClass public void setup() { public MyDB db = new DB(""config""); } } Can I just translate the above to Scala and I am ready to go? Another question is: How can I make the ""db"" field available to the Suite-Classes? In Java the field would be static so I could just reference it from all Suite-Classes. Maybe define a companion object which holds a reference? Do you want test execution order within a class or just to define the order in which the classes themselves are executed? Actually I would like to use both! In ScalaTest a suite can contain zero to many tests and zero to many nested suites. A suite's nested suites are provided by its nestedSuites method and the names of the tests to run are provided by its testNames method. The general answer is that for any built-in ScalaTest style trait the order of nested suite and test execution is determined by the order of they show up in the return value of nestedSuites and testNames. So if you're not otherwise getting the order you want then you can override either or both of those methods. However for test order it is probably much easier to simply use one of the traits in which tests are functions because those traits run tests in the order they appear in the source. (By contrast a Suite runs tests in alphabetical order by test name.) So I'd suggest you use a FunSuite for starters. For example: import org.scalatest.FunSuite class MySuite extends FunSuite { test(""this one will run first because it appears first"") { // ... } test(""this one will run second because it appears second"") { // ... } test(""this one will run third and so on"") { // ... } } As far as getting suites to run in order that's more rarely done because people like to use discovery to find new suites as they are written. But the way to do it is override nestedSuites. In 1.5 there the Suites class is a convenient way to do that: import org.scalatest.Suites class MySuites extends Suites( new MyFirstSuite new MySecondSuite new MyThirdSuite new MyFourthSuite ) Then you run MySuites and it will run its nested suites in the order you declared them."
734,A,Loop through array each element a JUnit test I have a JUnit 4 test that loops through an array of test data: public @Test void testAll() { final Object[][] sets = new Object[][] { // SET TYPE VALUE // --- valid sets // groups x(s(A1 B1 C1) G) x(s(A4 B4 D4) G) x(s(A8 B8 D8 C8) G) x(s(JJ B4 D4) G 4*3) x(s(A9 JJ D9) G 9*3) x(s(A2 B2 C2) G) x(s(A4 B4 JJ) G 4*3) x(s(A4 B4 C4 D4) G) // runs x(s(A1 A2 A3) R) x(s(B8 B9 B10) R) x(s(JJ C2 C3) R 6) x(s(D8 D9 JJ D11) R 38) x(s(D8 D9 JJ JJ) R 38) // sames x(s(A1 A1) S) x(s(B4 B4 B4) S) x(s(C8 C8) S) x(s(D3 D3) S) // doubt-cases assume group (TODO: verify this is correct) x(s(JJ JJ D4) G 4*3) x(s(A7 JJ JJ) G 7*3) x(s(JJ D9 JJ) G 9*3) x(s(JJ JJ JJ) G 1) // --- invalid sets x(s(B1 A2 A3) I) // not same colour x(s(D11 D12 JJ JJ) I) // last joker is 14 x(s(B1 B1 A1) I) // duplicate B1 x(s(A1 A2 A3 A5) I) // gap A4 x(s(JJ A1 JJ B1 C1) I) // one J replaces D1 then nothing left to replace x(s(A1 A2) I) // short x(s(B1) I) // shorter x(s(A5 A6) I) // short }; for (Object[] o : sets) { TileSet s = (TileSet) o[0]; Type t = (Type) o[1]; int v = (Integer) o[2]; System.out.println(s); assertEquals(t s.getType()); assertEquals(v s.getValue()); // test isValid though it's Too Simple To Break(R) if (t == Type.INVALID) assertFalse(s.isValid()); else assertTrue(s.isValid()); } } Because it's all in one test method the whole test stops as soon as one element in the array fails. Is there a way around that without making a method for each test item? Maybe something with reflection? If you need it to continue through assert failures how about building up a list of elements that fail as you're looping then asserting that the list is empty after the loop? If it's not empty print out the list of failures. @birryree how would you do this? wouldn't the looping stop once an element fails? I'm a beginner btw. Use JUnit 4's parameterized tests. They are a perfect fit for this type of problem although the documentation is quite lacking. Here are a few other samples on how to use them.: http://ourcraft.wordpress.com/2008/08/27/writing-a-parameterized-junit-test/ http://isagoksu.com/2009/development/agile-development/test-driven-development/using-junit-parameterized-annotation/ http://www.mkyong.com/unittest/junit-4-tutorial-6-parameterized-test/ This is exactly what I need. One thing though is there a way to show the toString() of the test data in the test results? It's inconvenient to have to keep looking them up by array index. you might want to look into either subclassing the Parameterized runner or providing your own implementation and overriding the `getName()` method inside the inner class `TestClassRunnerForParameters`  catch AssertionError and add the caught error to the errors list at the end check the list to be empty raise a compound AssertionError if not. I know but I have too much test data for that (what's currently there will be expanded some more). The errors list? Is that just a list I'll have to make or a built-in JUnit thing? @Bart you have to make it. BTW it is a good habit to instantiate test data in `setUp` and make small `test...` methods.
735,A,"Configure JNDI names with Open EJB I'm trying to (unit) test my EJB class without having to startup my websphere environment. Now I'm using Open EJB but there are some issues with resolving the JNDI Names for other EJBs that are used within my EJB... and there is no way for me to inject mocked classes from my test right now. Getting the InitialContext final Properties properties = new Properties(); properties.setProperty(Context.INITIAL_CONTEXT_FACTORY ""org.apache.openejb.client.LocalInitialContextFactory""); properties.setProperty(""log4j.category.OpenEJB.options "" ""debug""); properties.setProperty(""log4j.category.OpenEJB.startup "" ""debug""); properties.setProperty(""log4j.category.OpenEJB.startup.config "" ""debug""); properties.setProperty(""MyOwnDatasource.JdbcDriver "" ""com.ibm.as400.access.AS400JDBCDriver""); properties.setProperty(""MyOwnDataSource.JdbcUrl "" ""jdbc:as400:MYHOSTNAME;database name=MYDATABASE;libraries=MYDEFAULTTABLE""); ic = new InitialContext(properties); Inside my class under test there is a lookup for java:comp/env/ejb/PrefixEjbNameLocalHome and I can not set Open EJB to generate JNDI names in that format. Additional Property for JNDI name format I tried setting the formatting rule like this: properties.setProperty(""openejb.jndiname.format "" ""comp/env/ejb/{interfaceClass}""); Properties aren't used? Also the logging configuration isn't used. I'm only seeing INFO and WARN messages from Open EJB although I set log4j.category.OpenEJB.* and the like to DEBUG or TRACE. It's the ""java:"" part that is messing up your test case. Basically Context.INITIAL_CONTEXT_FACTORY and ""java:"" are mutually exclusive. The InitialContext class has a special understanding of ""java:"" or any ""foo:"" lookups and if they are at the beginning of the name it will not use INITIAL_CONTEXT_FACTORY you specified. A somewhat frustrating part of JNDI. If you lookup the name exactly as printed in the log it will work. So for example this log message: INFO - Jndi(name=WidgetBeanRemote) --> Ejb(deployment-id=WidgetBean) Then in code: Properties p = new Properties(); p.put(Context.INITIAL_CONTEXT_FACTORY ""org.apache.openejb.client.LocalInitialContextFactory""); // set any other properties you want Context context = new InitialContext(p); Object o = context.lookup(""WidgetBeanRemote""); wow thanks. I've been battling with this for some time. Especially to find a name that can be looked up both when testing with openEjb and deploying on Tomee."
736,A,"JUnit View in Eclipse on OSX Snow Leopard I recently purchased a mac and am trying to get junit tests working on Eclipse. I am using the latest Galileo but for some reason there is no option to ""run as junit"" on the tests. What am I doing wrong? There's no JUnit view panel either. However I went into the ""About Eclipse"" thing and it said JUnit is installed. I also port installed junit. I accidentally downloaded the wrong distribution which did not contain junit. Consider getting the Java EE edition - it has junit. thanks I installed JEE and it worked out :)  Are you in the right perspective? JUnits show up for me. Edit: You can try opening the runtime configuration menu (do Run>Run as...) and see if JUnit shows up. If not try running a software update and pull JUnit down.  Is your test project configured to use the correct version of junit? Check the java build path of the project. Also are you tests specified correctly? Using the @Test annotation for junit 4 etc? I just hit this problem a few minutes ago."
737,A,"JUnit in NetBeans I've coded a Swing application. Now I've got to test this big application using JUnit testing in NetBeans. I've learned some basics but I'm unable to figure out how to trigger the events automatically. If someone worked on it they can help. You can also redirect me to some sources at least. You know in a Swing application you'll have buttons etc. which generate events which are methods and I need to trigger the events. So is there a way to test the entire software? ""...I'm unable to figure out how to trigger the events automatically..."" - trigger what events? Do you mean running the JUnit tests? Those aren't events - you just tell the IDE to run the tests. Your expectations of how things ought to work aren't clear. Very sorry for the unclear explanation. You know in a swing you'll have buttons etc which generate events which are methods and I need to trigger the events. So I need a way to test the entire software. http://stackoverflow.com/questions/1480843/unit-testing-a-swing-component You should check uispec4j. Open source library licensed under Common Public License. Linked tutorial shows how to easily fire event and check its result.  A bit after the fact but hopefully helpful to someone. NB JUnit comes with NetBeans (as long as you opt it in) and using Jemmy along with JUnit (through NetBeans' Jelly) makes for testing Swing UI actually pretty simple. Here are some good links for getting setup: http://platform.netbeans.org/tutorials/nbm-test.html http://netbeans.dzone.com/articles/how-use-jemmy-junit-netbeans"
738,A,"Yet another Ant + JUnit classpath problem I'm developing an Eclipse SWT application using Eclipse. There are also some JUnit 4 tests which test some DAO's. But when I try to run the tests via an ant build all of the tests fail because the test classes aren't found. Google brought up about a million of people who all have the same problem but none of their solutions seem to work for me -.- . These are the contents of my build.xml file: <property name=""test.reports"" value=""./test/reports"" /> <property name=""classes"" value=""build"" /> <path id=""project.classpath""> <pathelement location=""${classes}"" /> </path> <target name=""testreport""> <mkdir dir=""${test.reports}"" /> <junit fork=""yes"" printsummary=""no"" haltonfailure=""no""> <batchtest fork=""yes"" todir=""${test.reports}"" > <fileset dir=""${classes}""> <include name=""**/Test*.class"" /> </fileset> </batchtest> <formatter type=""xml"" /> <classpath refid=""project.classpath"" /> </junit> <junitreport todir=""${test.reports}""> <fileset dir=""${test.reports}""> <include name=""TEST-*.xml"" /> </fileset> <report todir=""${test.reports}"" /> </junitreport> </target> The test classes are in the build-directory together with the application classes although they are in some subfolders according to their packages. Maybe this is important too: At first Ant complained that JUnit wasn't in its classpath but since I put it there (with the eclipse configuration editor) it complains about JUnit being in its classpath twice. WARNING: multiple versions of ant detected in path for junit [junit] jar:file:C:/Users/as df/Documents/eclipse/plugins/org.apache.ant_1.7.1.v20090120-1145/lib/ant.jar!/org/apache/tools/ant/Project.class [junit] and jar:file:/C:/Users/as%20df/Documents/eclipse/plugins/org.apache.ant_1.7.1.v20090120-1145/lib/ant.jar!/org/apache/tools/ant/Project.class I've tried specifying each and every subdirectory each and every class file I've tried filesets and filelists nothing seems to work. Thanks for your help I've been sitting for hours on this thing now... Would you mind to write the username ""as_df"" instead of ""as df"" (replacing blank by underscore)? This Ant build.xml works fine for me. Check out the properties to see if the directory structure matches yours; adjust as needed. <?xml version=""1.0"" encoding=""UTF-8""?> <project name=""xslt-converter"" basedir=""."" default=""package""> <property name=""version"" value=""1.6""/> <property name=""haltonfailure"" value=""no""/> <property name=""out"" value=""out""/> <property name=""production.src"" value=""src""/> <property name=""production.lib"" value=""lib""/> <property name=""production.resources"" value=""config""/> <property name=""production.classes"" value=""${out}/production/${ant.project.name}""/> <property name=""test.src"" value=""test""/> <property name=""test.lib"" value=""lib""/> <property name=""test.resources"" value=""config""/> <property name=""test.classes"" value=""${out}/test/${ant.project.name}""/> <property name=""exploded"" value=""out/exploded/${ant.project.name}""/> <property name=""exploded.classes"" value=""${exploded}/WEB-INF/classes""/> <property name=""exploded.lib"" value=""${exploded}/WEB-INF/lib""/> <property name=""reports.out"" value=""${out}/reports""/> <property name=""junit.out"" value=""${reports.out}/junit""/> <property name=""testng.out"" value=""${reports.out}/testng""/> <path id=""production.class.path""> <pathelement location=""${production.classes}""/> <pathelement location=""${production.resources}""/> <fileset dir=""${production.lib}""> <include name=""**/*.jar""/> <exclude name=""**/junit*.jar""/> <exclude name=""**/*test*.jar""/> </fileset> </path> <path id=""test.class.path""> <path refid=""production.class.path""/> <pathelement location=""${test.classes}""/> <pathelement location=""${test.resources}""/> <fileset dir=""${test.lib}""> <include name=""**/junit*.jar""/> <include name=""**/*test*.jar""/> </fileset> </path> <path id=""testng.class.path""> <fileset dir=""${test.lib}""> <include name=""**/testng*.jar""/> </fileset> </path> <available file=""${out}"" property=""outputExists""/> <target name=""clean"" description=""remove all generated artifacts"" if=""outputExists""> <delete dir=""${out}"" includeEmptyDirs=""true""/> <delete dir=""${reports.out}"" includeEmptyDirs=""true""/> </target> <target name=""create"" description=""create the output directories"" unless=""outputExists""> <mkdir dir=""${production.classes}""/> <mkdir dir=""${test.classes}""/> <mkdir dir=""${reports.out}""/> <mkdir dir=""${junit.out}""/> <mkdir dir=""${testng.out}""/> <mkdir dir=""${exploded.classes}""/> <mkdir dir=""${exploded.lib}""/> </target> <target name=""compile"" description=""compile all .java source files"" depends=""create""> <!-- Debug output <property name=""production.class.path"" refid=""production.class.path""/> <echo message=""${production.class.path}""/> --> <javac srcdir=""src"" destdir=""${out}/production/${ant.project.name}"" debug=""on"" source=""${version}""> <classpath refid=""production.class.path""/> <include name=""**/*.java""/> <exclude name=""**/*Test.java""/> </javac> <javac srcdir=""${test.src}"" destdir=""${out}/test/${ant.project.name}"" debug=""on"" source=""${version}""> <classpath refid=""test.class.path""/> <include name=""**/*Test.java""/> </javac> </target> <target name=""junit-test"" description=""run all junit tests"" depends=""compile""> <!-- Debug output <property name=""test.class.path"" refid=""test.class.path""/> <echo message=""${test.class.path}""/> --> <junit printsummary=""yes"" haltonfailure=""${haltonfailure}""> <classpath refid=""test.class.path""/> <formatter type=""xml""/> <batchtest fork=""yes"" todir=""${junit.out}""> <fileset dir=""${test.src}""> <include name=""**/*Test.java""/> </fileset> </batchtest> </junit> <junitreport todir=""${junit.out}""> <fileset dir=""${junit.out}""> <include name=""TEST-*.xml""/> </fileset> <report todir=""${junit.out}"" format=""frames""/> </junitreport> </target> <taskdef resource=""testngtasks"" classpathref=""testng.class.path""/> <target name=""testng-test"" description=""run all testng tests"" depends=""compile""> <!-- Debug output <property name=""test.class.path"" refid=""test.class.path""/> <echo message=""${test.class.path}""/> --> <testng classpathref=""test.class.path"" outputDir=""${testng.out}"" haltOnFailure=""${haltonfailure}"" verbose=""2"" parallel=""methods"" threadcount=""50""> <classfileset dir=""${out}/test/${ant.project.name}"" includes=""**/*.class""/> </testng> </target> <target name=""exploded"" description=""create exploded deployment"" depends=""testng-test""> <copy todir=""${exploded.classes}""> <fileset dir=""${production.classes}""/> </copy> <copy todir=""${exploded.lib}""> <fileset dir=""${production.lib}""/> </copy> </target> <target name=""package"" description=""create package file"" depends=""exploded""> <jar destfile=""${out}/${ant.project.name}.jar"" basedir=""${production.classes}"" includes=""**/*.class""/> </target> </project> Hm seems like that solved pat of the problem but now I get ""Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit""  I was getting this only when using the 'fork=true' option to the junit task. It was happening because my ANT_HOME had '..' in it (e.g. '/3rdparth/jboss/jboss-5/../tools'). Once I reduced that path the 'multiple versions of ant' warning went away.  I had the same problem 'multiple versions of ant detected in path for junit'. The problem went away when I renamed by Eclipse_Home directory and removed special characters from it. The path had '[1]' in it which was causing the problem."
739,A,"Can't load ResourceBundle during junit test I'm trying to write some Junit tests to test old classes in our app. The code is trying to load a ResourceBundle (for translations) but fails. I guess the problem is classpath related but I can't figure it out. The code is laid out in /src and my tests are in /test. The ResourceBundles are loaded given a base name relative to /src say ""foo/bar/baz"". My tests use the same classpath as the app itself so I don't understand why it won't find the bundles. Any ideas to what's wrong? Are your tests under a directory called `/test`? If you temporarily change the relative path your ResourceBundles to an absolute one does the problem go away? If you are using maven you typically want to put all kind of properties files and resource bundles in a separate source folder called src/main/resources/. If you want to override values in your unit tests then you can add a duplicate prop. file in the src/test/resources source folder.  Resources are not resolved from the source folder but from the class directory. Are your resource files copied to the output folder (bin target/classes etc.) ? If not your classes can't find them. D'uh! Of course my tests use another output directory than the app does. This is definately it."
740,A,"Run all tests in Junit 4 I want to be able to run all tests in a project programmatically. I know Eclipse has a ""Run as JUnit test"" configuration which somehow grabs all the tests in a project and run them. Is there any way for me to also grab the list of tests programmatically and run them? Or is there some good way to construct a test suite containing all the test cases without manually listing out every one (all 700+) of them? I've tried the ""New... -> Test Suite"" option in Eclipse but that seems to work only for JUnit 3 identifying tests by their extending from TestCase The test classes are JUnit 4 so their only distinguishing characteristic is the annotation no naming convention no subclassing from TestCase. Thanks in advance! See if this helps: http://burtbeckwith.com/blog/?p=52 Though it does not really solve your immediate problem I find it a very useful general practice to create suites and suites of suites e.g. for a package something like PackageFooSuite etc. and assemble these suites in one or more suites again like ModuleFooSuite and have one top-level suite like AllTestsSuite. That way it's easy to run both all tests in one step as well as submodule tests for the package I'm currently working on (and have the tests run quicker than if I would always run all of them): @RunWith(Suite.class) @Suite.SuiteClasses({ PackageFooSuite.class PackageBarSuite.class} ) public final class AllTestsSuite {} // or ModuleFooSuite and that in AllTests I tend to agree with you. Unfortunately the people who were working on this before me did not and left maybe 700 or more testcases which I am not keen on manually stringing together by hand @alexloh: Yeah quite understandable. Perhaps it's actually feasible to automate this without too much hassle like: for each package/directory create a suite class made of the names of all the files in the current package/directory. If you start from the bottom directory it might also be relatively simple to include the generated suites for a sub-package in the level above.  With Eclipse Indigo (possibly Helios as well) in the Run Configurations dialog box you now have the ability to Run all tests in a selected project package or source folder. Also a good reference from Eclipse is the article Java Unit testing with JUnit 4.x in Eclipse.  You can do this fairly easily from within maven using the surefire plugin: I usually clean/compile/install my projects from the command line before comparing them for eclipse usage (mvn eclipse:clean eclipse:eclipse) and you can define a test suite in your pom which lists all the tests you want to run en masse every time you run mvn install. You're not calling them programatically exactly but you can certainly call them en masse. Hmm... I don't use Maven but I'll try it out if it solves my problem. I'll tick this if I find that it works. Thanks!!  I also recommend using the JUnit Suite annotations. Follow the link for more detail. this sounds like manual listing OP explicitly wanted to avoid.  Of the top of my head using Spring: Implement a TypeFilter that matches classes with methods annotated with @Test (don't forget to consider the superclasses) Invoke classpath scanning on your top-most test package Invoke the JUnitRunner with the scan results More info on classpath scanning and custom type filters here  None of the other answers did it for me. I had 40k tests I needed to run so manually listing every class was not an option. I did it with ClasspathSuite. A test suite that runs all Junit4 and Junit3 test cases in the class path is as follows: import org.junit.extensions.cpsuite.ClasspathSuite; import org.junit.extensions.cpsuite.ClasspathSuite.*; import org.junit.runner.RunWith; import org.junit.runner.JUnitCore; import static org.junit.extensions.cpsuite.SuiteType.*; @RunWith(ClasspathSuite.class) @SuiteTypes({ JUNIT38_TEST_CLASSES TEST_CLASSES }) public class RunAllSuite { /* main method not needed but I use it to run the tests */ public static void main(String args[]) { JUnitCore.runClasses(RunAllSuite.class); } } I needed to run it from command line so this is what I did: Downloaded cp-1.2.6.jar Create the previously mentioned RunAllSuite Compile the class javac RunAllSuite.java -cp cpsuite-1.2.6.jar;junit-4.8.1.jar run it with target tests in the class path java -cp cpsuite-1.2.6.jar;junit-4.8.1.jar;path/to/runallsuite/folder;target/classes;target/test-classes RunAllSuite And that's it. With the RunAllSuite above anywhere in your code you can just do JUnitCore.runClasses(RunAllSuite.class) which runs all tests in class path. There are other config options as well which are explained in the ClasspathSuite home page. Note also that the class given above does not print anything. If that is needed you can do import org.junit.extensions.cpsuite.ClasspathSuite; import org.junit.extensions.cpsuite.ClasspathSuite.*; import org.junit.runner.RunWith; import org.junit.runner.JUnitCore; import org.junit.internal.TextListener; import static org.junit.extensions.cpsuite.SuiteType.*; @RunWith(ClasspathSuite.class) @SuiteTypes({ JUNIT38_TEST_CLASSES TEST_CLASSES }) public class RunAllSuite { public static void main(String args[]) { JUnitCore junit = new JUnitCore(); junit.addListener(new TextListener(System.out)); junit.run(RunAllSuite.class); } }"
741,A,"Lack of support base class in Junit4/Jmock2 We're finally migrating our unit test code base from JUnit 3 to JUnit 4. We also make heavy use of JMock 2. With JUnit 3 JMock provides a useful base class for your tests (MockObjectTestCase) which as well as itself being s subclass of Junit's TestCase it handles various housekeeping duties regarding the mock framework. It makes life pretty easy for the test class. Now with JUnit4 JMock provides no such support. Your test class has to manually create a Mockery object it has to remember to use the correct test runner annotation and must delegate all mock-related operations to the mockery. In short it puts far more responsibility on the test class than was needed for JUnit 3 tests. Now I appreciate that part of JUnit4's charm is there being no need to subclass something but this JMock situation seems like a step backwards and makes porting from 3 to 4 rather more work than should be necessary. Am I missing something? Is there actually a nice way to write my JUnit4/Jmock2 test classes without manually adding all that plumbing to every class? I could write my own support base class of course but it seems such an obvious omission from the JMock2 API I have to wonder if I've missed the point. Edit: here's the source code of what the optional support class would look like: @RunWith(JMock.class) public class JMockSupport { protected final Mockery mockery = new Mockery(); protected void checking(ExpectationBuilder expectations) { mockery.checking(expectations); } protected <T> T mock(Class<T> typeToMock) { return mockery.mock(typeToMock); } protected <T> T mock(Class<T> typeToMock String name) { return mockery.mock(typeToMock name); } protected Sequence sequence(String name) { return mockery.sequence(name); } protected void setDefaultResultForType(Class<?> type Object result) { mockery.setDefaultResultForType(type result); } protected void setImposteriser(Imposteriser imposteriser) { mockery.setImposteriser(imposteriser); } protected void setNamingScheme(MockObjectNamingScheme namingScheme) { mockery.setNamingScheme(namingScheme); } protected States states(String name) { return mockery.states(name); } } This contains all of the methods that the JUnit3 MockObjectTestCase class defined which just echo to the mockery. The @RunWith annotation is there also to avoid the possibility of forgetting to add it to your test class. There are also problems with having base classes. In previous versions I suffered from trying to combine base classes from different test frameworks. That's why we went to composition over inheritance. It'll be interesting to see what we can do with the new @Rule structure. I agree that composition is immeasurably more flexible but inheritance brings convenience. It's nice to able to pick. For the record I've implemented an @Rule context in the JMock respository  I've done this migration too and it is a pain. I can understand why they've binned the base class mechanism - I was trying to juggle JMock base classes with Spring JUnit-enabled base classes and that obvious doesn't work. Once I embarked on this migration one area I found for 'optimisation' was creating appropriate Expectation base classes encapsulating common operations on your mock objects rather than creating a new Expectation object (and instance) for every test. That will save you a little grief. Think you've commented the wrong message?  No. There is no such support. The test base class in JMock 1 caused a lot of problems because you can only extend a single class and so people couldn't use JMock with other test frameworks that also defined base classes. That's why we went with delegation rather than inheritance in JMock2. That said you might be able to use the MockObjectTestCase class from JMock2's JUnit3 support library as long as you annotate your class with @RunWith(JMock.class). But I haven't tried. There has been a request for an ""auto-mocking"" JUnit4 runner that will create the context and mock objects for you by automagical reflection. Some people like this others really don't like it. If you want this feature vote for the issue in the JMock JIRA. Oh hullo Nat didn't expect to find you here :) I've modified my question to include the source of the suggested base class. Note that it's entirely optional but does make life a little bit easier."
742,A,"Testing SOAP endpoint through junit throws SAXParseException (soapenv:Envelope) I've try to implement integration tests for a working application (spring hibernate soap cxf). I build an SOAP-XML by hand and handle it to my endpoint. Like this: private Source createRequest(String domainName String email String userId String password) { String content = """"; content += ""<soapenv:Envelope xmlns:soapenv=\""http://schemas.xmlsoap.org/soap/envelope/\"" xmlns:sup=\""[...]"">""; content += "" <soapenv:Header/>""; content += "" <soapenv:Body>""; content += "" <sup:RegisterRequest>""; content += "" <sup:domain>"" + domainName + ""</sup:domain>""; content += "" <sup:email>"" + email + ""</sup:email>""; content += "" <sup:userId>"" + userId + ""</sup:userId>""; content += "" <sup:password>"" + password + ""</sup:password>""; content += "" </sup:RegisterRequest>""; content += "" </soapenv:Body>""; content += ""</soapenv:Envelope>""; return new StringSource(content); } In my test I handle it to my endpoint like this: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""../../WEB-INF/applicationContext.xml"" ""../../WEB-INF/test.xml"" }) @Transactional public class TestRegisterEndpoint extends AbstractTransactionalJUnit4SpringContextTests { @Resource private RegisterEndpoint registerEndpoint; @Test public void registerUserFailsForUnexistantDomain() throws Exception { Source result = registerEndpoint.invoke(createRequest(""unexistant_domain"" ""test@test.de"" ""test@test.de"" ""myPassword1"")); } } But when I try to run the test I get an exception ""Cannot find the declaration of element 'soapenv:Envelope'."". org.springframework.oxm.jaxb.JaxbUnmarshallingFailureException: JAXB unmarshalling exception: null; nested exception is javax.xml.bind.UnmarshalException - with linked exception: [org.xml.sax.SAXParseException: cvc-elt.1: Cannot find the declaration of element 'soapenv:Envelope'.] at org.springframework.oxm.jaxb.JaxbUtils.convertJaxbException(JaxbUtils.java:75) [...] Caused by: javax.xml.bind.UnmarshalException - with linked exception: [org.xml.sax.SAXParseException: cvc-elt.1: Cannot find the declaration of element 'soapenv:Envelope'.] at javax.xml.bind.helpers.AbstractUnmarshallerImpl.createUnmarshalException(AbstractUnmarshallerImpl.java:315) [...] Caused by: org.xml.sax.SAXParseException: cvc-elt.1: Cannot find the declaration of element 'soapenv:Envelope'. at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source) I would guess I have to define the ""soapenv"" namespace somewhere where I havn't it. But if so I don't know where. Start of ""applicationContext.xml"": <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:tx=""http://www.springframework.org/schema/tx"" xmlns:p=""http://www.springframework.org/schema/p"" xmlns:aop=""http://www.springframework.org/schema/aop"" xsi:schemaLocation=""http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd""> Start of ""test.xml"": <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd""> How can I get rid of the exception? Ok I've found help elsewhere. The solution: because my endpoint extended ""PayloadEndpoint"" public interface RegisterEndpoint extends PayloadEndpoint { } I have to insert only the payload into the endpoint (sounds logical ;) ). If I do it like this: private Source createRequest(String domainName String email String userId String password) { String content = """"; content += ""<sup:RegisterRequest xmlns:sup=\""[...]\"">""; content += "" <sup:domain>"" + domainName + ""</sup:domain>""; content += "" <sup:email>"" + email + ""</sup:email>""; content += "" <sup:userId>"" + userId + ""</sup:userId>""; content += "" <sup:password>"" + password + ""</sup:password>""; content += ""</sup:PreregisterRequest>""; return new StringSource(content); } all is working. Are you sure you want to use string building to make an XML document? What if `password` contains an `<`? At least XML-escape the variables better yet use a DOM API to build the document. Yes you are right. But this is only the code used in the tests not in production code. I don't care about exceptional input at this place. All I care about here is readability."
743,A,"How do you assert that a certain exception is thrown in JUnit 4 tests? How can I use JUnit4 idiomatically to test that some code throws an exception? While I can certainly do something like this: @Test public void testFooThrowsIndexOutOfBoundsException() { boolean thrown = false; try { foo.doStuff(); } catch (IndexOutOfBoundsException e) { thrown = true; } assertTrue(thrown); } I recall that there is an annotation or an Assert.xyz or something that is far less kludgy and far more in-the-spirit of JUnit for these sorts of situations. The problem with any other approach but this is that they invariably end the test once the exception has been thrown. I on the other hand often still want to call `org.mockito.Mockito.verify` with various parameters to make sure that certain things happened (such that a logger service was called with the correct parameters) before the exception was thrown. You can see how to exceptions test in JUnit wiki page https://github.com/junit-team/junit/wiki/Exception-testing In my case I always get RuntimeException from db but messages differ. And exception need to be handled respectively. Here is how I tested it: @Test public void testThrowsExceptionWhenWrongSku() { // Given String articleSimpleSku = ""999-999""; int amountOfTransactions = 1; Exception exception = null; // When try { createNInboundTransactionsForSku(amountOfTransactions articleSimpleSku); } catch (RuntimeException e) { exception = e; } // Then shouldValidateThrowsExceptionWithMessage(exception MESSAGE_NON_EXISTENT_SKU); } private void shouldValidateThrowsExceptionWithMessage(final Exception e final String message) { assertNotNull(e); assertTrue(e.getMessage().contains(message)); }  Be careful using expected exception because it only asserts that the method threw that exception not a particular line of code in the test. I tend to use this for testing parameter validation because such methods are usually very simple but more complex tests might better be served with: try { methodThatShouldThrow(); fail( ""My method didn't throw when I expected it to"" ); } catch (MyException expectedException) { } Apply judgement. Maybe I'm old school but I still prefer this. It also gives me a place to test the exception itself: sometimes I have exceptions with getters for certain values or I might simply look for a particular value in the message (e.g. looking for ""xyz"" in the message ""unrecognized code 'xyz'""). I think NamshubWriter's approach gives you the best of both worlds. +1 useful in some scenarios where expected = xx doesn't match requirements. Using ExpectedException you could call N exception.expect per method to test like this exception.expect(IndexOutOfBoundsException.class); foo.doStuff1(); exception.expect(IndexOutOfBoundsException.class); foo.doStuff2(); exception.expect(IndexOutOfBoundsException.class); foo.doStuff3(); This is classic thank you. I dont like all the countless annotations coming with each new version of the framework. Simplest solution meeting the requirements thanks. @user1154664 Actually you can't. Using ExpectedException you can only test that one method throws an exception because when that method is called the test will stop executing because it threw the expected exception! Your first sentence just isn't true. When using `ExpectedException` the normal thing to do is to set the expectation immediately before the line that you expect to throw the exception. That way if an earlier line throws the exception it won't trigger the rule and the test will fail.  JUnit 4 has support for this: @Test(expected=IndexOutOfBoundsException.class) public void testIndexOutOfBoundsException() { ArrayList emptyList = new ArrayList(); Object o = emptyList.get(0); } I discovered that that what i described before happened because the test wasn't marked with this annotation @RunWith(value=BlockJUnit4ClassRunner.class) This piece of code will not work if you expect an exception only somewhere in your code and not a blanket like this one. @skaffman This wouldn't work with org.junit.experimental.theories.Theory runned by org.junit.experimental.theories.Theories @skaffman: Is there a way to do similar thing in `junit 3.8` expect method to throw exception? I don't use that annotation and don't have that problem... @raisercostin: Please delete your first comment. With all the upvotes it is highly misleading (because the annotation works exactly as advertised as you found out yourself in the next comment). @Thilo I deleted my comment but probably the response should also specify that that annotation is really needed since your test might be executed and annotation ignored.  BDD Style Solution: JUnit 4 + Catch Exception @Test public void testFooThrowsIndexOutOfBoundsException() { when(foo).doStuff(); then(caughtException()).isInstanceOf(IndexOutOfBoundsException.class); } Source code https://gist.github.com/mariuszs/23f4e1e1857c28449b61 Dependencies com.googlecode.catch-exception:catch-exception:1.2.0  If you can use JUnit 4.7 you can use the ExpectedException Rule @RunWith(JUnit4.class) public class FooTest { @Rule public ExpectedException exception = ExpectedException.none(); @Test public void doStuffThrowsIndexOutOfBoundsException() { Foo foo = new Foo(); exception.expect(IndexOutOfBoundsException.class); foo.doStuff(); } } This is much better than @Test(expected=IndexOutOfBoundsException.class) because the test will fail if IndexOutOfBoundsException is thrown before foo.doStuff() See this article for details +1 a better solution than the accepted answer. It's not always better since it gets applied to every test in that class. Using `@Test(expected=...)` is specific to that one method. Each approach is better in different situations. @skaffman - If I've understood this correctly it looks like the exception.expect is being applied only within one test not the whole class. If the exception we expect to be thrown is an checked exception should we add throws or try-catch or test this situation in another way? IMHO the answer from _rwoo_ below is a much better solution (http://code.google.com/p/catch-exception/) i.e. in this example when you `throw new NullPointerException();` after `foo.doStuff()` the test will **not** fail with an NPE. @Jason Thompson: ""should"" is nice but not bullet-proof: With verify-exception you can be sure that the test is okay even if someone else added some code after `doStuff()`: when the new code passes so does the test and more importantly: when the new code throws the test will fail. the given ""solution"" would hide this fact (and moreover verify-exception is more concise and thus less error-prone): @MartinTrummer No code should run after foo.doStuff() since the exception is thrown and the method is exited. Having code after an expected exception (with the exception of closing resources in a finally) is unhelpful anyway since it should never be executed if the exception is thrown. @MartinTrummer What I'm struggling to understand is how code can execute after an exception is thrown. Perhaps you can give an example. My understanding is that as soon as an exception is thrown the method exits with an exception. So if I had a function that looked like this: void myMethod { throw new IllegalStateException(); executeOtherMethod(); } As far as I know there is no way executeOtherMethod() would ever be called. So it's a non issue. This code doesn't change how java works but rather tells the test runner that the method will throw an exception and some characteristics of it. @Jason but that's not the code you have. you have `foo.doStuff(); bar.doStuff();` - and you expect `foo.doStuff();` to throw. so if it does throw `bar.doStuff()` is never executed (that's bad). but even worse is the case when the code changes and `foo.doStuff()` does not throw anymore. now it's possible that `bar.doStuff()` throws insteead- the test passes - and you don't notice that the new code actually broke the original expectations that `foo.doStuff();` throws. Really nice solution! @JasonThompson: usually in a testing environment the exception is swallowed and you assert it was thrown. The test doesn't exit in that case. As I commented on rwoo's answer I think the drawback with his library is that you can't proxy every object [for example classes that are final?]... but I'm not sure without trying it out. This is the best approach. There are two advantages here compared to skaffman's solution. Firstly the `ExpectedException` class has ways of matching the exception's message or even writing your own matcher that depends on the class of exception. Secondly you can set your expectation immediately before the line of code that you expect to throw the exception - which means your test will fail if the wrong line of code throws the exception; whereas there's no way to do that with skaffman's solution. Doesn't work when we extend TestCase the test fails although the exception is thrown as expected  I tried many of the methods here but they were either complicated or didn't quite meet my requirements. In fact one can write a helper method quite simply: public class ExceptionAssertions { public static void assertException(BlastContainer blastContainer ) { boolean caughtException = false; try { blastContainer.test(); } catch( Exception e ) { caughtException = true; } if( !caughtException ) { throw new AssertionFailedError(""exception expected to be thrown but was not""); } } public static interface BlastContainer { public void test() throws Exception; } } Use it like this: assertException(new BlastContainer() { @Override public void test() throws Exception { doSomethingThatShouldExceptHere(); } }); Zero dependencies: no need for mockito no need powermock; and works just fine with final classes.  JUnit has built-in support for this with an ""expected"" attribute  To solve the same problem I did set up a small project: http://code.google.com/p/catch-exception/ Using this little helper you would write verifyException(foo IndexOutOfBoundsException.class).doStuff(); This is less verbose than the ExpectedException rule of JUnit 4.7. In comparison to the solution provided by skaffman you can specify in which line of code you expect the exception. I hope this helps. you can also do this with [caughtException](http://goo.gl/Stkj6D) I thought about doing something like this as well but ultimately discovered that the true power of ExpectedException is that not only can you specify the expected exception but you can also specify certain properties of the exception such as the expected cause or expected message. My guess is that this solution has some of the same drawbacks as mocks? For example if `foo` is `final` it will fail because you can't proxy `foo`?  You can also do this: @Test public void testFooThrowsIndexOutOfBoundsException() { try { foo.doStuff(); assert false; } catch (IndexOutOfBoundsException e) { assert true; } } Whats up with the weird bracket style?  As answered before there are many ways of dealing with exceptions in JUnit. But with Java 8 there is another one: using Lambda Expressions. With Lambda Expressions we can achieve a syntax like this: @Test public void verifiesTypeAndMessage() { assertThrown(new DummyService()::someMethod) .isInstanceOf(RuntimeException.class) .hasMessage(""Runtime exception occurred"") .hasMessageStartingWith(""Runtime"") .hasMessageEndingWith(""occurred"") .hasMessageContaining(""exception"") .hasNoCause(); } assertThrown accepts a functional interface which instances can be created with lambda expressions method references or constructor references. assertThrown accepting that interface will expect and be ready to handle an exception. This is relatively simple yet powerful technique. Have a look at this blog post describing this technique: http://blog.codeleak.pl/2014/07/junit-testing-exception-with-java-8-and-lambda-expressions.html The source code can be found here: https://github.com/kolorobot/unit-testing-demo/tree/master/src/test/java/com/github/kolorobot/exceptions/java8  How about this: Catch a very general exception make sure it makes it out of the catch block then assert that the class of the exception is what you expect it to be. This assert will fail if a) the exception is of the wrong type (eg. if you got a Null Pointer instead) and b) the exception wasn't ever thrown. public void testFooThrowsIndexOutOfBoundsException() { Throwable e = null; try { foo.doStuff(); } catch (Throwable ex) { e = ex; } assertTrue(ex instanceof IndexOutOfBoundsException); } That's what you do in JUnit 3. Junit 4 does it better. Also you won't see what kind of Exception ex is in the test results when the day comes where the test fails.  Just make a Matcher that can be turned off and on like this: public class ExceptionMatcher extends BaseMatcher<Throwable> { private boolean active = true; private Class<? extends Throwable> throwable; public ExceptionMatcher(Class<? extends Throwable> throwable) { this.throwable = throwable; } public void on() { this.active = true; } public void off() { this.active = false; } @Override public boolean matches(Object object) { return active && throwable.isAssignableFrom(object.getClass()); } @Override public void describeTo(Description description) { description.appendText(""not the covered exception type""); } } To use it: add the public field ExpectedException exception = ExpectedException.none(); then in your code ExceptionMatcher exMatch = new ExceptionMatcher(MyException.class); exception.expect(exMatch); someObject.somethingThatThrowsMyException(); exMatch.off();"
744,A,"Intellij idea tests compilation takes too long (compared with Eclipse) When I ""junit"" some class compilation in Idea takes about 4-5 sec. In Eclipse it takes milliseconds. What should I do to speed up Intellij Idea compilation for tests? There is plugin in Idea plugin repository which solve exactly this problem!  I don't have IntelliJ in front of me but could it be that IntelliJ is set to do a clean compile before it runs the tests where Eclipse is set to do a make (I.e only compile changed classes) Just a thought check for module settings idea settings: nothing. I just started IntelliJ and I see a checkbox in the Run/Debug configurations which is Make so my original idea was wrong and make will only compile the changed classes. Some other things to check: 1) Are they using the same version of Java? This could make a difference. 2) Have you edited the eclipse ini file giving it more memory therefore running one ide with more memory than the other 3) Double check both run debug configurations for differences Debug / Run - no difference. Maybe eclipse compile classes in background but effictevly than idea (may be idea file cache play role) By default Eclipse does a Make when you save a file so this means that the classes are probably all compiled buy the time you start the tests but if you make sure both are compiled before running the tests is IntelliJ still slower? Can you check the version of Java both are using? They use same JRE Just to check. If you compile the IntelliJ project and in the Run/Debug un-check the Make before running tests. If it is still slower then it is not the compile. They are both using the same unit test package? Other than that I reckon its a question for the IntelliJ team on http://www.jetbrains.net/devnet/community/idea/ideacommunity  Eclipse compiles on the fly (on save) so it does not have to do anything but to start the tests. Idea does not. Before starting the tests it has to make (compile all changed classes of the whole project). It is also not possible to start a test in idea when there is any compile error elsewhere in the project. Idea does not have this feature. You can bind Make to Ctrl-S maybe that helps a little if you are a ""Save often save early"" guy ;-) Is it possible to enable this in idea ? I perss ""test all"" button hundred times."
745,A,"Junit vs TestNG At work we are currently still using Junit3 to run our tests. We have been considering switching over to Junit4 for new tests being written but I have been keeping an eye on TestNG for a while now. What experiences have you all had with either Junit4 or TestNG and which seems to work better for very large numbers of tests. Having flexibility in writing tests is also important to us since our functional tests cover a wide aspect and need to be written in a variety of ways to get results. Old tests will not be re-written as they do their job just fine. What I would like to see in new tests though is flexibility in the way the test can be written natural assertions grouping and easily distributed test executions. Any changes from 08 to now???? I've used both but I have to agree with Justin Standard that you shouldn't really consider rewriting your existing tests to any new format. Regardless of the decision it is pretty trivial to run both. TestNG strives to be much more configurable than JUnit but in the end they both work equally well. TestNG has a neat feature where you can mark tests as a particular group and then easily run all tests of a specific group or exclude tests of a particular group. Thus you can mark tests that run slowly as in the ""slow"" group and then ignore them when you want quick results. A suggestion from their documentation is to mark some subset as ""checkin"" tests which should be run whenever you check new files in. I never saw such a feature in JUnit but then again if you don't have it you don't REALLY miss it. For all its claims of high configuration I did run into a corner case the a couple weeks ago where I couldn't do what I wanted to do... I wish I could remember what it is but I wanted to bring it up so you know that it's not perfect. The biggest advantage TestNG has is annotations... which JUnit added in version 4 anyways. JUnit can do the grouping thing you are talking about by defining a test suite and then adding the tests in the desired group to that suite. You can then set up a target in your ant script that only runs that suite and set up your source control to run that target upon checkin. The biggest advantage TestNG has over JUnit is the ability to dynanmically generate test data for parameterized tests. Each test data element is a different ""test"" so it makes it really easy to create data-driven tests http://testng.org/doc/documentation-main.html#parameters Parametrized tests are easy done with Theories that are integrated in newer versions of Junit (but are experimental at the moment). TestNG groups can be done in JUnit 4.8 with Categories: http://kentbeck.github.com/junit/doc/ReleaseNotes4.8.html.  First I would say don't rewrite all your tests just to suit the latest fad. Junit3 works perfectly well and the introduction of annotations in 4 doesn't buy you very much (in my opinion). It is much more important that you guys write tests and it sounds like you do. Use whatever seems most natural and helps you get your work done. I can't comment on TestNG b/c I haven't used it. But I would recommend unitils a great wrapper for JUnit/TestNG/DBUnit/EasyMock regardless of which route you take. (It supports all the flavors mentioned above) Unitils doesn't look like it has been updated in a while; will it work with newer versions of JUnit/TestNG? Just for anyone finding this answer in 2011 I checked and the latest unitils version (3.2) has a release date of: 2011-09-29. So it *is* being actively maintained.  A couple of additions to Mike Stone's reply: 1) The most frequent thing I use TestNG's groups for is when I want to run a single test method in a test suite. I simply add this test to the group ""phil"" and then run this group. When I was using JUnit 3 I would comment out the entries for all methods but the one I wanted to run in the ""suite"" method but then would commonly forget to uncomment them before checkin. With the groups I no longer have this problem. 2) Depending on the complexity of the tests migrating tests from JUnit3 to TestNG can be done somewhat automatically with sed and creating a base class to replace TestCase that static imports all of the TestNG assert methods. I have info on my migration from JUnit to TestNG here and here. the problem with checking in changes you did not intended is actualy because you have to review what you are checking in. And if you have a big check in  that is not an excuse that is another problem: you should have many smaller checkins.  Your question seems two folded to me. On one had you would like to compare two test frameworks on the other hand you would like to implement tests easily have natural assertions etc... Ok firstly JUnit has been playing catchup with TestNG in terms of functionality they have bridged the gap some what with v4 but not well enough in my opinion. Things like annotations and dataproviders are still much better in TestNG. Also they are more flexible in terms of test execution since TestNG has test dependency grouping and ordering. JUnit still requires certain before/after methods to be static which limits what you can do prior to the running of tests TestNG never has this issue. TBH mostly the differences between the two frameworks don't mean much unless your focusing on integration/automation testing. JUnit from my experience is built from the ground up for unit testing and is now being pushed towards higher levels of testing which IMO makes it the wrong tool for the job. TestNG does well at unit testing and due to its robust dataproviding and great test execution abilities works even better at integration/automation test level. Now for what I believe is a separate issue how to write well structured readable and maintainable tests. Most of this I am sure you know but things like Factory Pattern Command Pattern and PageObjects (if your testing websites) are vital it is very important to have a layer of abstraction between what your testing (SUT) and what the actual test is (assertions of business logic). In order to have much nicer assertions you can use Hamcrest. Make use of javas inheritance/interfaces to reduce repetition and enforce commonality. Almost forgot also use the Test Data Builder Pattern this coupled with TestNG's dataprovider annotation is very useful.  TestNG's biggest draw cards for me include its support test groups and more importantly - test group dependencies (marking a test as being dependent of a group causes the tests to simply skip running when the dependent group fails). TestNG's other big draw cards for me include test parameters data providers annotation transformers and more than anything - the vibrant and responsive user community. Whilst on the surface one might not think all of TestNGs features above might not be needed once you start to understand the flexibility bring to your tests you'll wonder how you coped with JUnit. (disclaimer - I've not used JUnit 4.x at all so am unable to really comment on advances or new features there). I am using both JUnit4 and TestNG TestNG has better support for spring spring-test integration. Makes testing spring based application a lot easier.  I wanted to share the one I encountered today. I found built-in Parameterized runner is quite crude in Junit4 as compare to TestNG (I know each framework has its strengths but still). The Junit4 annotation @parameters is restricted to one set of parameters. I encountered this problem while testing the valid and invalid behavior for functionality in same test class. So the first public static annotated method that it finds will be used but it may find them in any order. This causes us to write different classes unnecessarily. However TestNG provides clean way to provide different kind of data providers for each and every method. So we can test the same unit of code with valid and invalid way in same test class putting the valid/invalid data separately. I will go with TestNG.  Let me enlighten everyone because all the other answers in this thread focus on the simplest and most obvious of TestNG features. We need to draw attention to what makes TestNG truly far more powerful: 1. JUnit still requires certain before/after methods to be static which limits what you can do prior to the running of tests TestNG never has this issue. 2. TestNG @Configuration methods can all take an optional argument to their annotated methods in the form of a ITestResult XmlTest Method or ITestContext. This allows you to pass things around that JUnit wouldn't provide you. JUnit only does this in listeners and it is limited in use. 3. TestNG comes with some pre-made report generation classes that you can copy and edit and make into your own beautiful test output with very little effort. Just copy the report class into your project and add a listener to run it. 4. TestNG has a handful of nice listeners that you can hook onto so you can do additional magic at certain phases during testing.  Cheers to all the above. Some other things I've personally found I like more in TestNG are: The @BeforeClass for TestNG takes place after class creation so you aren't constrained by only being able to call static methods of your class in it. Parallel and parameterized tests maybe I just don't have enough of a life... but I just get a kick writing one set of Selenium tests accepting a driver name as a parameter. Then defining 3 parallel test groups 1 each for the IE FF and Chrome drivers and watching the race! I originally did 4 but way too many of the pages I've worked on break the HtmlUnit driver for one reason or another. Yeah probably need to find that life. ;)  I like the neat and easy integration of TestNG with Guice.  About a year ago we had the same problem. I spent sometime considering which move was better and eventually we realized that TestNG has no 'killer features'. It's nice and has some features JUnit 4 doesn't have but we don't need them. We didn't want people to feel uncomfortable writing tests while getting to know TestNG because we wanted them to keep writing a lot of tests. Also JUnit is pretty much the de-facto standard in the Java world. There's no decent tool that doesn't support it from the box you can find a lot of help on the web and they added a lot of new features in the past year which shows it's alive. We decided to stick with JUnit and never looked back.  Also one more advantage of TestNG is supporting of parallel testing. In our era of multicores it's important i think. I also used both frameworks. But i using hamcrest for assertations. Hamcrest allows you easily write your own assert method. So instead of assertEquals(operation.getStatus() Operation.Status.Active); You can write assertThat(operation isActive()); That gives you opportunity to use higher level of abstraction in your tests. And this makes your tests more robust."
746,A,"Unable to get hudson to parse JUnit test output XML EDIT: This issue has been fixed by google in gtest 1.4.0; see the original bug report for more information. I've recently switched to gtest for my C++ testing framework and one great feature of it which I am presently unable to use is the ability to generate JUnit-style XML test reports which could then be read in by our hudson build server. The XML output generated by the gtest test suite all looks legit: <?xml version=""1.0"" encoding=""UTF-8""?> <testsuite tests=""370"" failures=""0"" disabled=""0"" errors=""0"" time=""45.61"" name=""AllTests""> <testsuite name=""application"" tests=""7"" failures=""0"" disabled=""0"" errors=""0"" time=""8.953""> <testcase name=""zero_tasks_on_bootup"" status=""run"" time=""0"" classname=""application"" /> ...etc. </testsuite> </testsuite> I've also tried adding a JUnitReport task to my ant build script which works fine and generates XML like so: <?xml version=""1.0"" encoding=""UTF-8""?> <testsuite tests=""370"" failures=""0"" disabled=""0"" errors=""0"" time=""45.61"" name=""AllTests""> <testsuite name=""application"" tests=""7"" failures=""0"" disabled=""0"" errors=""0"" time=""8.953""> <testcase name=""zero_tasks_on_bootup"" status=""run"" time=""0"" classname=""application"" /> ...etc. </testsuite> </testsuite> The problem is whenever I tell ant to publish the JUnit test results and then point it to either the raw test result XML or the compiled result generated in the ant JUnitReport task hudson always complains about finding no test results there. I'm not a java guy so I can't tell what's going on here and I can't find an example of how the JUnit XML ought to look like. Can someone help to point me in the right direction? Can you tell me how to display XML here? I have problems with markup that the pre and code tag combo don't fix. I can post an example if you can tell me how to make it show up. If I ""view source"" on yours I see that the magic characters are escaped. What am I doing wrong? I'll try again... Here's how I do it:  <target name=""junit"" depends=""compile-tests"" description=""run all unit tests""> <mkdir dir=""${reports}""/> <junit haltonfailure=""false""> <jvmarg value=""-Xms128m""/> <jvmarg value=""-Xmx128m""/> <classpath> <path refid=""project.classpath""/> </classpath> <formatter type=""xml""/> <batchtest fork=""yes"" todir=""${reports}""> <fileset dir=""${test}/classes""> <include name=""**/*Test*.class""/> </fileset> </batchtest> </junit> </target> <target name=""generate-reports"" depends=""junit"" description=""create JUnit test HTML reports""> <mkdir dir=""${reports}""/> <junitreport todir=""${reports}""> <fileset dir=""${reports}""> <include name=""TEST-*.xml""/> </fileset> <report format=""frames"" todir=""${reports}""/> </junitreport> </target>  I'm almost certain that this is not a problem parsing the XML but rather a problem finding the XML files. If you are using a relative path in the Hudson config make sure you are clear which directory it is relative to (I seem to remember it being non-obvious under certain circumstances). As for examples of what the JUnit XML files are supposed to look like good luck with that. It's not precisely specified anywhere. Different tools have differing dialects. That said Hudson does a good job of recognising all of them. I believe it was the developers of JUnitReport who first introduced the XML format so if you're using that that's about as canonical as you are going to get.  Edit: Google test has fixed this issue which is included in the gtest 1.4.0 release. See the original bug report for more info. Bah! I've finally found the cause of this problem -- it's because gtest produces one giant XML file for all test results and hudson expects one XML test report per class. I've written a perl script as a workaround for this issue. To use it you would make a target in your ant xml script which looks something like this: <target name=""runtests""> <exec executable=""wherever/${ant.project.name}Test"" failonerror=""false"" dir=""tests""> <arg value=""--gtest_output=xml:${build.dir}\reports\${ant.project.name}.xml""/> </exec> <!-- Workaround for broken gtest output --> <mkdir dir=""${build.dir}/reports/output""/> <exec executable=""perl"" failonerror=""false"" dir=""tests""> <arg value=""gtest-hudson.pl""/> <arg value=""${build.dir}/reports/${ant.project.name}.xml""/> <arg value=""${build.dir}/reports/output""/> </exec> </target> For some reason gtest also doesn't like the wrong style of slashes being passed to it from ant so I made my exec for windows only as my hudson is running on a windows server. Change to '/' for unix obviously. I've also filed an issue for this on the gtest page and also one on hudson's issue tracker so hopefully one of the two teams will pick up on the issue as I don't have enough time to jump in and make a patch myself.... though if this doesn't get fixed in the near future I might just have to. ;)"
747,A,"Running single test case in Junit eclipse Hi I am using junit for unit testing. I am using Eclipse. I have given 12 test cases that is 12 different functions. When I right click on the paticular function in Outline and give RunAS--->Junit Test it is executing all the test cases. How to overcome this ? I just want to run only one test case or one function !!! Any idea ? I know two ways of just running one 1) Click in the method name of the test you want to run. The method name will now become marked in a special color ( for me it's gray ). While the method name is marked right click and select ""Run as JUnit Test"". That should do it. 2) Run all your tests once. In the JUnit windows right click the test you want to run and select ""Run as JUnit Test"". This works for me both in Eclipse 3.5.1 and SpringSource Tool Suite 2.3.0 Hope this helps I will admit its a simple thing but not intuitive :) Solved my problem.  The following page has discussion on this issue specifically versions and changes to JUnit 4 etc. http://srivaths.blogspot.com/2009/04/run-single-junit-test-method-in-eclipse.html You'd need to give more information regarding versions etc if you need more help but as that discussion suggests in JUnit 4 it works if you don't extend junit.framework.TestCase."
748,A,"JUnit: Possible to 'expect' a wrapped exception? I know that one can define an 'expected' exception in JUnit doing: @Test(expect=MyException.class) public void someMethod() { ... } But what if there is always same exception thrown but with different 'nested' causes. Any suggestions? unimportant side-note: it is ""expected=..."" not ""expect=..."" I wrote a little JUnit extension for that purpose. A static helper function takes a function body and an array of expected exceptions: import static org.junit.Assert.assertTrue; import static org.junit.Assert.fail; import java.util.Arrays; public class AssertExt { public static interface Runnable { void run() throws Exception; } public static void assertExpectedExceptionCause( Runnable runnable @SuppressWarnings(""unchecked"") Class[] expectedExceptions ) { boolean thrown = false; try { runnable.run(); } catch( Throwable throwable ) { final Throwable cause = throwable.getCause(); if( null != cause ) { assertTrue( Arrays.asList( expectedExceptions ).contains( cause.getClass() ) ); thrown = true; } } if( !thrown ) { fail( ""Expected exception not thrown or thrown exception had no cause!"" ); } } } You can now check for expected nested exceptions like so: import static AssertExt.assertExpectedExceptionCause; import org.junit.Test; public class TestExample { @Test public void testExpectedExceptionCauses() { assertExpectedExceptionCause( new AssertExt.Runnable(){ public void run() throws Exception { throw new Exception( new NullPointerException() ); } } new Class[]{ NullPointerException.class } ); } } This saves you writing the same boiler plate code again and again. That would be nice if java had closures! As is try/catch/getCause() is probably less boiler plate code than crafting anonymous classes!  You could create a Matcher for exceptions. This works even when you are using another test runner like Arquillian's @RunWith(Arquillian.class) so you can't use the @RunWith(ExtendedTestRunner.class) approach suggested above. Here's a simple example: public class ExceptionMatcher extends BaseMatcher<Object> { private Class<? extends Throwable>[] classes; // @SafeVarargs // <-- Suppress warning in Java 7. This usage is safe. public ExceptionMatcher(Class<? extends Throwable>... classes) { this.classes = classes; } @Override public boolean matches(Object item) { for (Class<? extends Throwable> klass : classes) { if (! klass.isInstance(item)) { return false; } item = ((Throwable) item).getCause(); } return true; } @Override public void describeTo(Description descr) { descr.appendText(""unexpected exception""); } } Then use it with @Rule and ExpectedException like this: @Rule public ExpectedException thrown = ExpectedException.none(); @Test public void testSomething() { thrown.expect(new ExceptionMatcher(IllegalArgumentException.class IllegalStateException.class)); throw new IllegalArgumentException(""foo"" new IllegalStateException(""bar"")); } Added by Craig Ringer in 2012 edit: An enhanced and more reliable version: Basic usage unchanged from above Can pass optional 1st argument boolean rethrow to throw unmatched exception. That preserves the stack trace of the nested exceptions for easier debugging. Uses Apache Commons Lang ExceptionUtils to handle cause loops and to handle non-standard exception nesting used by some common exception classes. Self-describe includes accepted exceptions Self-describe on failure includes a the cause stack of the exception encountered Handle Java 7 warning. Remove the @SaveVarargs on older versions. Full code: import org.apache.commons.lang3.exception.ExceptionUtils; import org.hamcrest.BaseMatcher; import org.hamcrest.Description; public class ExceptionMatcher extends BaseMatcher<Object> { private Class<? extends Throwable>[] acceptedClasses; private Throwable[] nestedExceptions; private final boolean rethrow; @SafeVarargs public ExceptionMatcher(Class<? extends Throwable>... classes) { this(false classes); } @SafeVarargs public ExceptionMatcher(boolean rethrow Class<? extends Throwable>... classes) { this.rethrow = rethrow; this.acceptedClasses = classes; } @Override public boolean matches(Object item) { nestedExceptions = ExceptionUtils.getThrowables((Throwable)item); for (Class<? extends Throwable> acceptedClass : acceptedClasses) { for (Throwable nestedException : nestedExceptions) { if (acceptedClass.isInstance(nestedException)) { return true; } } } if (rethrow) { throw new AssertionError(buildDescription() (Throwable)item); } return false; } private String buildDescription() { StringBuilder sb = new StringBuilder(); sb.append(""Unexpected exception. Acceptable (possibly nested) exceptions are:""); for (Class<? extends Throwable> klass : acceptedClasses) { sb.append(""\n ""); sb.append(klass.toString()); } if (nestedExceptions != null) { sb.append(""\nNested exceptions found were:""); for (Throwable nestedException : nestedExceptions) { sb.append(""\n ""); sb.append(nestedException.getClass().toString()); } } return sb.toString(); } @Override public void describeTo(Description description) { description.appendText(buildDescription()); } } Typical output: java.lang.AssertionError: Expected: Unexpected exception. Acceptable (possibly nested) exceptions are: class some.application.Exception Nested exceptions found were: class javax.ejb.EJBTransactionRolledbackException class javax.persistence.NoResultException got: <javax.ejb.EJBTransactionRolledbackException: getSingleResult() did not retrieve any entities.> I've extended the example in the answer into something more complete. Excellent and very helpful answer - thanks. This approach is a lifesaver when working with Arquillian to test EJBs since they like to wrap every unchecked exception in an EJBException.  You could always do it manually: @Test public void someMethod() { try{ ... all your code } catch (Exception e){ // check your nested clauses if(e.getCause() instanceof FooException){ // pass } else { Assert.fail(""unexpected exception""); } }  As of JUnit 4.11 you can use the ExpectedException rule's expectCause() method: @Rule public ExpectedException expectedException = ExpectedException.none(); @Test public void throwsNestedException() throws Exception { expectedException.expectCause(is(instanceOf(SomeNestedException.class))); throw new ParentException(""foo"" new SomeNestedException(""bar"")); } because of the hamcrest generics hell line 6 has to look like this: `expectedException.expectCause(is(IsInstanceOf.instanceOf(SomeNestedException.class)));` but other than that it's an elegant solution.  The most concise syntax is provided by catch-exception: import static com.googlecode.catchexception.CatchException.*; catchException(myObj).doSomethingNasty(); assertTrue(caughtException().getCause() instanceof MyException);  If you're using the latest version of JUnit you can extend the default test runner to handle this for you (without having to wrap each of your methods in a try/catch block) ExtendedTestRunner.java - New test runner: public class ExtendedTestRunner extends BlockJUnit4ClassRunner { public ExtendedTestRunner( Class<?> clazz ) throws InitializationError { super( clazz ); } @Override protected Statement possiblyExpectingExceptions( FrameworkMethod method Object test Statement next ) { ExtendedTest annotation = method.getAnnotation( ExtendedTest.class ); return expectsCauseException( annotation ) ? new ExpectCauseException( next getExpectedCauseException( annotation ) ) : super.possiblyExpectingExceptions( method test next ); } @Override protected List<FrameworkMethod> computeTestMethods() { Set<FrameworkMethod> testMethods = new HashSet<FrameworkMethod>( super.computeTestMethods() ); testMethods.addAll( getTestClass().getAnnotatedMethods( ExtendedTest.class ) ); return testMethods; } @Override protected void validateTestMethods( List<Throwable> errors ) { super.validateTestMethods( errors ); validatePublicVoidNoArgMethods( ExtendedTest.class false errors ); } private Class<? extends Throwable> getExpectedCauseException( ExtendedTest annotation ) { if (annotation == null || annotation.expectedCause() == ExtendedTest.None.class) return null; else return annotation.expectedCause(); } private boolean expectsCauseException( ExtendedTest annotation) { return getExpectedCauseException(annotation) != null; } } ExtendedTest.java - annotation to mark test methods with: @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.METHOD}) public @interface ExtendedTest { /** * Default empty exception */ static class None extends Throwable { private static final long serialVersionUID= 1L; private None() { } } Class<? extends Throwable> expectedCause() default None.class; } ExpectCauseException.java - new JUnit Statement: public class ExpectCauseException extends Statement { private Statement fNext; private final Class<? extends Throwable> fExpected; public ExpectCauseException( Statement next Class<? extends Throwable> expected ) { fNext= next; fExpected= expected; } @Override public void evaluate() throws Exception { boolean complete = false; try { fNext.evaluate(); complete = true; } catch (Throwable e) { if ( e.getCause() == null || !fExpected.isAssignableFrom( e.getCause().getClass() ) ) { String message = ""Unexpected exception cause expected<"" + fExpected.getName() + ""> but was<"" + ( e.getCause() == null ? ""none"" : e.getCause().getClass().getName() ) + "">""; throw new Exception(message e); } } if (complete) throw new AssertionError( ""Expected exception cause: "" + fExpected.getName()); } } Usage: @RunWith( ExtendedTestRunner.class ) public class MyTests { @ExtendedTest( expectedCause = MyException.class ) public void someMethod() { throw new RuntimeException( new MyException() ); } } I love this solution! However sadly I'm having trouble getting it to compile in conjunction with Groovy JUnit 4 testing. Very nice but you should do `Set testMethods = new HashSet(super.computeTestMethods());` in `computeTestMethods()` to avoid duplicates. We ran into this when running tests on a class and on one method. Thanks @sjngm I've updated my answer That's the cleanest solution. Couple of edits though: ExtendedTestRunner needs to extend SpringJUnit4ClassRunner to properly support Spring context. Also computeTestMethods has incompatible return type (should be ArrayList).  You could wrap the testing code in a try / catch block catch the thrown exception check the internal cause log / assert / whatever and then rethrow the exception (if desired). Thanks that's the most obvious solution. You're welcome!"
749,A,"Mocking up WifiManager for Android Unit Testing I'm trying to implement some unit tests for a couple of classes that rely on WifiManager and the returned ScanResults. What I'd like to do is be able to control the ScanResults that I'm receiving in order to test a variety of different conditions. Unfortunately it's been quite difficult for me to successfully mock up WifiManager (though I suppose I can pass its constructor null references in my MockWifiManager). This will only be my first problem as once I have a MockWifiManager to play with (if this even works!) I will have to successfully create my test ScanResults which does not have a public constructor (Imagine it's created by some factory somewhere). Questions: With it not having a public constructor can I even extend it? Am I going about this all wrong? I often get asked questions about how to do a specific task but really they're trying to solve a different problem the wrong way maybe that's what I'm doing here? I'm very new to android so having to mock up all of this functionality has been trying to say the least. Thanks for your inputs! Edit: I'm having a hell of a time instantiating a MockWifiManager as well. The constructor for wifi manager is expecting an IWifiManager a type which does not appear to exist in the Android SDK. You could try to create the ScanResult instances by using reflection to access the private constructors. The code might look something like this:  try { Constructor<ScanResult> ctor = ScanResult.class.getDeclaredConstructor(null); ctor.setAccessible(true); ScanResult sr = ctor.newInstance(null); sr.BSSID = ""foo""; sr.SSID = ""bar""; // etc... } catch (SecurityException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (NoSuchMethodException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (IllegalArgumentException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (InstantiationException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (IllegalAccessException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (InvocationTargetException e) { // TODO Auto-generated catch block e.printStackTrace(); } For other ways of testing I most of times convert the information from instances like ScanResult and encapsulate only the information I need into my own objects. These I feed to the method doing the hard work. This makes testing easier as you can easily build these intermediate objects without relying on the real ScanResult objects. Every time I have to use reflection God kills a puppy. It may be the only way unfortunately thanks for the code snippet  Create an abstraction around WifiManager. Use this for your mocking. Mocking stuff you don't own is hard and brittle. If done right you should be able to switch the internals plus you'll end up with a better mockable API. For your testing you can stub/fake the manager to you hearts content. For production you'll pass in a concrete instance. With regards to your point about changing your code just to make it testable that is incorrect. Firstly you should mock roles not types as discussed in the paper below. Google for more info. Secondly creating an abstraction around third party code is a best practice as stated by the dependency inversion principle in SOLID. You should always depend on abstractions rather than concrete implementations whether you are unit testing or not. http://www.objectmentor.com/resources/articles/dip.pdf http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod Ah yes isolate the things that may change. I don't have any control over the Android API unless I protect myself with that abstraction. Good call! It seems odd to design around the need for testing. Are there other benefits to wrapping every system api that I may need to use? @Brian yes. Firstly you can swap the WifiManager in one place. What happens when version x comes out and there is a difference in the API? You'll have to change many areas in your codebase. I'm updating my answer with more info."
750,A,"Exclude individual JUnit Test methods without modifying the Test class? I'm currently re-using JUnit 4 tests from another project against my code. I obtain them directly from the other project's repository as part of my automated Ant build. This is great as it ensures I keep my code green against the very latest version of the tests. However there is a subset of tests that I never expect to pass on my code. But if I start adding @Ignore annotations to those tests I will have to maintain my own separate copy of the test implementation which I really don't want to do. Is there a way of excluding individual tests without modifying the Test source? Here's what I have looked at so far: As far as I can see the Ant JUnit task only allows you to exclude entire Test classes not individual test methods - so that's no good for me I need method granularity. I considered putting together a TestSuite that uses reflection to dynamically find and add all of the original tests then add code to explicitly remove the tests I don't want to run. But I ditched that idea when I noticed that the TestSuite API doesn't provide a method for removing tests. I can create my own Test classes that extend the original Test classes override the specific tests I don't want to run and annotate them with @Ignore. I then run JUnit on my subclasses. The downside here is that if new Test classes are added to the original project I won't pick them up automatically. I'll have to monitor for new Test classes as they are added to the original project. This is my best option so far but doesn't feel ideal. The only other option I can think of is to run the bad tests anyway and ignore the failures. However these tests take a while to run (and fail!) so I'd prefer to not run them at all. Additionally I can't see a way of telling the Ant task to ignore failures on specific test methods (again - I see how you can do it for individual Test classes but not methods). I feel like I'm missing an obvious trick! Any ideas? Thanks! If the unwanted tests are in specific classes/packages you could use a fileset exclude in Ant to exclude them during import. No unfortunately I need to exclude specific test methods. That is I want to run some tests in a given Test class but not others.  A possibility I can think of to achieve what you want with the stated constraints is to use bytecode modification. You could keep a list of classes and methods to ignore in a separate file and patch the bytecode of the test classes as you load them to remove this methods altogether. If I am not mistaken JUnit uses reflection to find the test methods to execute. A method rename operation would then allow you to remove these methods before JUnit finds them. Or the method can be modified to return immediately without performing any operation. A library like BCEL can be used to modify the classes when loaded. JUnit 4 uses annotations instead of method names to determine the tests to run. It would be still valid to modify the method to return immediately. But thanks I was thinking about JUnit3 Presumably bytecode modification could also be used to add @Ignore annotations to methods in compiled classes.  It doesn't help you now but TestNG supports this sort of ability.  OK this is a rather heavyweight solution but don't throw things at me if it sounds ridiculous. The core of Junit4 is the org.junit.runner.Runner class and its various subclasses most importantly org.junit.runners.Suite. These runners determine what the tests are for a given test class using things like @Test and @Ignore. It's quite easy to create custom implementations of a runner and normally you would hook them up by using the @RunWith annotation on your test classes but obviously that's not an option for you. However in theory you could write your own Ant task perhaps based upon the standard Ant Junit task which takes your custom test runner and uses it directly passing each test class to it in turn. Your runner implementation could use an external config file which specifies which test methods to ignore. It'd be quite a lot of work and you'd have to spend time digging around in the prehistoric Ant Junit codebase to find out how it works. The investment in time may be worth it however. It's just a shame that the Junit Ant task doesn't provide a mechanism to specify the test Runner that would be ideal. I would say implementing a runner is deceptively simple. I found myself copying and pasting code from other parts of JUnit to get a custom one to work properly and it was very brittle - pretty much guaranteed to break on a new release. All true. I never claimed it was elegant :)  If you want to run only a subset of the tests it sounds like that class has more than one responsibility and should be refactored down. Alternately the test class could be broken apart so that the original project had all the tests but on one or more classes(I'm guessing some of the tests are really integration tests and touch the database or network) and you could exclude the class(es) you didn't want. If you can't do any of that your option of overriding is probably best. Take the process of whenever you need to ignore some methods you extend that class and add it to your Ant exclude list. That way you can exclude what you can't pass and will still pull in all new tests (methods you didn't override and new test classes) without modifying your build.  If you can't touch the original test at all you are going to have some serious limitations. Your overriding sounds like the best bet but with a couple of changes: Build the Ant tests specifically excluding the super classes so that additional classes that you don't know about get run. You can use the @Rule annotation (new to JUnit 4.7) to know what test is being run and abort it (by returning an empty Statement implementation) rather than overriding specific methods giving you more flexibility in knowing whether or not to avoid the test. The only problem with this method is that you can't stop the @Before methods from running using this method which may be slow. If that is a problem (and you really can't touch the tests) then @Ignore in the overridden method is the only thing I can think of. If however you can touch those tests some additional options open up: You could run them with a custom runner by specifying the @RunWith tag on the class. This runner would just pass over execution to the standard runner (JUnit4.class) in that project but in your project (via a system property or some other mechanism) would inspect the test name and not run a test. This has the advantage of being the least intrusive but the most difficult to implement (runners are hairy beasts one of the stated goals of @Rule was to eliminate most of the need to make them). Another is to make an assumeThat statement on the test that would check some configuration setting that would be true if that test should run. That would actually involve injecting right into the test which is most likely a deal breaker in anything remotely labeled a ""separate project."""
751,A,"How do I configure Eclipse to run your tests automatically? I read this article: Configure your IDE to run your tests automatically http://eclipse.dzone.com/videos/configure-your-ide-run-your It's pretty easy to configure Eclipse IDE to run an Ant target every time a file is saved. MyProject -> Right-click : Properties -> Builders -> New -> Ant Builder The problem is that the builder has to rebuild the jar on every save which is very long. The JUnit tests run using the .classes in the jar. We already have JUnit configurations (EclipseIde .launch files which contains the whole classpath to run the tests). I wish I could create a builder that wraps those JUnit launch files. This would have the benefit of running the tests against Eclipse .classes (faster than rebuilding the jar). Also the test results are displayed in the JUnit view. Has anybody manage to do that? just curious: what's the major differences between the 3 solutions posted below? are any of them markedly better/worse than another? @Jason - CT-Eclipse is eclipse specific. Infinitest is open source and being actively developed. JUnit max is also eclipse specific but costs money. It is endorsed by Kent Beck though which may make it prefered by some. I have heard many developers rave about Infinitest though so I would recommend to try it first. http://www.junitloop.org/index.php/JUnitLoop is another one like this (haven't tried it yet myself)  I recently started using Infinitest and it seems to somehow ""know"" which parts of the code affect which test cases. So when you change some code it automatically re-runs the tests which are likely to break. If the test fails it marks an error at the spot where it failed the same way Eclipse would mark a coding error like calling a non-existent method or whatever. And it all happens in the background without you having to do anything.  You can use JUnit Max it is an eclipse plug in that will run all you tests every time you save. But it will do it without interrupting your work flow. The results are shown in the left corner of Eclipse and you can always go back to the last successful testrun. The plugin runs the tests that are most likely to fail first so that you get a response for your last saving as fast as possible. But it is still working :) I think it's dead - http://www.junitmax.com/junitmax/subscribe.html JUnit Max was revived.  You could try CT-Eclipse a continuous testing plugin for Eclipse. From the plugin page: With CT-Eclipse enabled as you edit your code Eclipse runs your tests quietly in the background and notifies you if any of them fail or cause errors. Does anyone know if CT-Eclipse can work in newer versions of Eclipse than 3.1 mentioned here: http://groups.csail.mit.edu/pag/continuoustesting/? The latest message posted to the mailing list (https://lists.csail.mit.edu/pipermail/continuous-testing-plugin-discuss/2009-April/000157.html) suggests moving to JUnit Max which is not supported any more. JUnit Max is revived. CT-Eclipse doesn't appear to have been touched since 2007. The project page shows no activity on this project in 6 years at this point. Probably best to look elsewhere. The JUnit Max page shows a $100/yr subscription?!? Perhaps Infinitest (http://infinitest.github.com/)..."
752,A,"Unit testing in Java - what is it? Can you explain in a few sentences: Why we need it / why they make our life easier ? How to unit-test [simple example in Java] ? When do not we need them / types of projects we can leave unit-testing out? useful links wikipedia That's a way more than ""a few sentences"" ;) Shorter answers are better as the question stated. C'mon the 'Benefits' section is not too long. And it has a lot of useful links inside ;)  This is how how your programming should be : Decide on an interface (not necessarily a java interface but how method looks like to everybody Write a test Code the implementation  Why we need it / why they make our life easier ? It allows you to check the expected behavior of the piece(s) of code you are testing serving as a contract that it must satisfy. It also allows you to safely re-factor code without breaking the functionality (contract) of it. It allows you to make sure that bug fixes stay fixed by implementing a Unit test after correcting a bug. It may serve as as a way to write decoupled code (if you have testing in mind while writing your code). How to unit-test [simple example in Java] ? Check out the JUnit website and the JUnit cookbook for details. There isn't much to writing JUnit test cases. Actually coming up with good test cases is surely harder than the actual implementation. When do not we need them / types of projects we can leave unit-testing out? Don't try to test every method in a class but rather focus on testing the functionality of a class. Beans for example you won't write tests for the getters and setters... Links JUnit - Unit testing EclEmma - test coverage tool link text - Wikipedia link to unit testing  What I would have written is already covered in many of the responses here but I thought I'd add this... The best article I ever read on when to use/not to use unit tests was on Steve Sanderson's blog. That is an excellent article covering the cost/benefit of unit testing on different parts of your code-base (i.e. a compelling argument against 100% coverage) @Dolbz I'll check it. Thank you.  One of the best books on the hows and whys of unit testing in Java is Pragmatic Unit Testing in Java with JUnit (Andy Hunt & Dave Thomas)  It's probably work reading the Wikipedia article on Unit Testing as this will answer most of your questions regarding why. The JUnit web site has resources for writing a Java unit test of which the Junit Cookbook should probably be your first stop. Personally I write unit tests to test the contract of a method i.e. the documentation for a particular function. This way you will enter into a cycle of increasing your test coverage and improving documentation. However you should try to avoid testing: Other people's code including the JDK Non-deterministic code such as java.util.Random JUnit is not the only unit testing framework available for Java so you should evaluate other frameworks like TestNG before diving into it. In addition to ""top-level"" frameworks you will also find quite a few projects covering specific areas such as: HTMLUnit for Web SIPUnit for SIP SwingUnit for GUI Code I think it's important to note that technically a unit test determines whether some small part (a unit) of a program is working properly. What most everyone here is talking about and the usual modern usage is an automated unit test which runs as part of a build or on demand.  Because that is your proof that the application actually works as intended. You'll also find regression bugs much easier. Testing becomes easier as you don't have to manually go through every possible application state. And finally you'll most likely find bugs you didn't even know existed even though you manually tested your code. Google for junit Unit tests should always be written as said it is your proof that the application works as intended. Some things cannot or can be hard to test for example a graphical user interface. This doesn't mean that the GUI shouldn't be tested it only means you should use other tools for it. See point 2. 'it is your proof that the application works as intended'. This is a very common misconception. All they prove is that a particular 'unit' works as intended (not the application). When integrating many units there is still scope for integration bugs where the behaviour over multiple units is not as expected despite individual units having 100% success. That's not to say that unit tests are not incredibly useful in themselves. good answer except for ""google it"" If you're going to move into unit testing it's also worth looking at mocking frameworks. Using junit is only part of the equation often you'll want to predetermine/emulate the behaviour of certain classes for the test e.g. for a test on a class that normally talks to a web service you may mock out the web service to return the results you want rather than making an actual call to a webserver. See this qestion: http://stackoverflow.com/questions/22697/whats-the-best-mock-framework-for-java and http://mockito.org/ for Mockito a nice mocking framework."
753,A,"How to parse xml file in my junit selenium test case using eclipse How to parse xml file in my junit selenium test case using eclipse I want to grab data from this file and insert it in forms using my selenium RC test case. Java has inbuilt machanism for dealing with XML processing. It has support for both DOM as well as SAX parsers. You may want to have a look at http://download.oracle.com/javaee/1.4/tutorial/doc/JAXPIntro.html  Take a look at dom4j and this is my approach. I implemented a selenium + Junit framework and all the data inputs are in XML format. So I setup the structure of the xml input. So If you are going on this direction I would advise you to identify what xml format/structure. Example: <seleniumtest> <!-- Selenium Test Parameters --> <Parameter name=""inputValue""> <value>myvalue</value> </Parameter> <Parameter name=""mypassword""> <value>password</value> </Parameter> <Parameter name=""users""> <value>user1</value> <value>user2</value> </Parameter> </seleniumtest> One you finalize on the structure you can start writing a parse and expose the data into rest of the test or just for a one test. Here is the start for your xml parser Create a class and have the constructor read in the xml file public class XMLFileParser { private static final Logger log = Logger.getLogger(XMLFileParser.class.getName()); private Document document; /** * Method will construct a Document Object from the given InputStream * @param readIn The input stream to read the document. */ public XMLFileParser(InputStream readIn) { SAXReader reader = new SAXReader(); try { document = reader.read(readIn); }catch (DocumentException ex) { log.error(""Error when attempting to parse input data file"" ex); } } /** * Write a method to iterate over the nodes to actually parse the xml file. */ public HashMap<String Object> parseInput(HashMap<String Object> params) { Element rootElement = document.getRootElement(); for(Iterator iter = rootElement.elementIterator(""Parameter""); iter.hasNext();) { // This where you should write your parse logic } return params; } It doesn't have to be a hashmap but thats what I wanted to do you can make this method to return anything that you wish based on your framework. Have Fun;  Have a look at http://dom4j.sourceforge.net/"
754,A,"Spring @Transactional: Rollback information reported but not actually performed I have annotated my test classes as follows: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""file:WebContent/WEB-INF/applicationContext.xml"" ""file:WebContent/WEB-INF/context-aspects.xml""}) @Transactional public class MyTest { } However when executing tests the test database is suddenly filled with values although @Transactional is enabled and I can read the following in the log: INFO: Began transaction (4): transaction manager [org.springframework.orm.hibernate3.HibernateTransactionManager@669aa3f3]; rollback [true] 07.04.2011 23:57:33 org.springframework.test.context.transaction.TransactionalTestExecutionListener endTransaction INFO: Rolled back transaction after test execution for test context ... Any ideas why the actual rollback after the test case is not performed? Update: If I am using HSQLDB I don't have these problems - so is it a problem of mysql? What database engine are you using? I am using MySQL InnoDB. In another project it works with the same database but I am unable to figure out where the differences in the configurations are. In case of an MySQL which type do you use: MyISAM or InnoDB? I could finally solve the problem. Hibernate was generating MyISAM tables which apparently have no Transaction support. This was due to a wrong hibernate dialect configured. I used org.hibernate.dialect.MySQL5Dialect but org.hibernate.dialect.MySQL5InnoDBDialect is required.  Update: If I am using HSQLDB I don't have these problems - so is it a problem of mysql? Yes you are right. Check that you use the right dialect (when using Hibernate: org.hibernate.dialect.MySQL5InnoDBDialect) and may you should monitor the statements that been send to the database. cool I found this answer 5min ago and just posted it ..."
755,A,"JUnit test timing in Eclipse I have JUnit 3 setup on Eclipse Helios. I created a test case to add 1 million users through a web service and test the result. I don't quite understand JUnite timing the result (on JUnit output): <?xml version=""1.0"" encoding=""UTF-8""?> <testrun name=""UserRegistrationTestCase"" project=""TestCases"" tests=""1"" started=""1"" failures=""0"" errors=""0"" ignored=""0""> <testsuite name=""testcases.UserRegistrationTestCase"" time=""8880.946""> <testcase name=""testCreateUser"" classname=""testcases.UserRegistrationTestCase"" time=""8880.946""/> </testsuite> </testrun> The time says on the JUnit display: 8880.946 s. What does that mean? Does that display in seconds (whereas in the XML the time=""8880.946"")? Is 8880.946 (in XML) in milliseconds? If not how would I configure Eclipse JUnit to allow time to display in milliseconds (The reason I ask is because I initially added 1000 users and the result came to 13.593s). The timing is always in seconds. Not sure how to change it and with that large a number why would you want to? If you want the time in milliseconds you can of course move the decimal 3 places to the right. With milliseconds we want to calculate ways to calculate `latency` in milliseconds.  This is the format in which the time is present in xml: seconds.ms So in your case it is 8880 seconds and 946 millis"
756,A,"IntelliJ IDEA with Junit 4.7 ""!!! JUnit version 3.8 or later expected:"" When I attempt to run the following test in IntelliJ IDEA I get the message: !!! JUnit version 3.8 or later expected: It should be noted that this is an Android project I am working on in IntelliJ IDEA 9. public class GameScoreUtilTest { @Test public void testCalculateResults() throws Exception { final Game game = new Game(); final Player player1 = new Player(); { final PlayedHole playedHole = new PlayedHole(); playedHole.setScore(1); game.getHoleScoreMap().put(player1 playedHole); } { final PlayedHole playedHole = new PlayedHole(); playedHole.setScore(3); game.getHoleScoreMap().put(player1 playedHole); } final GameResults gameResults = GameScoreUtil.calculateResults(game); assertEquals(4 gameResults.getScore()); } } The full stack trace looks like this... !!! JUnit version 3.8 or later expected: java.lang.RuntimeException: Stub! at junit.runner.BaseTestRunner.<init>(BaseTestRunner.java:5) at junit.textui.TestRunner.<init>(TestRunner.java:54) at junit.textui.TestRunner.<init>(TestRunner.java:48) at junit.textui.TestRunner.<init>(TestRunner.java:41) at com.intellij.rt.execution.junit.JUnitStarter.junitVersionChecks(JUnitStarter.java:152) at com.intellij.rt.execution.junit.JUnitStarter.canWorkWithJUnitVersion(JUnitStarter.java:136) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:49) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:110) Process finished with exit code -3 What version of intellij? Intellij 9 the latest patch. Thanks for asking this question. Almost 4 years later it and the answers below are still helpful. There are two thing I could imagine to happen If your IDE tries to start an Android Junit test that directly runs on the emulator you can't use Junit4. If you accidentally used the junit classes provided from the android jar they can't run on a normal jvm because there are only real compiled classes for the android dalvik vm.  This problem happens because Android Platform (android.jar) already contains JUnit classes. IDEA test runner loads these classes and sees that they are from the old JUnit while you are trying to use annotated tests which is a feature of the new JUnit therefore you get the error from the test runner. The solution is simple open the Project Structure | Modules | Dependencies and move the junit-4.7.jar up so that it comes before Android 1.6 Platform in the classpath. Now the test runner will be happy as it loads the new JUnit version. Just to make people aware this is still an issue with later versions of android. Issue to track the progress: http://youtrack.jetbrains.com/issue/IDEA-80559 Thanks for the wonderful solution. The issue was very annoying Thanks @CrazyCoder solved the problem in maven as well by placing the android provided dependency after the test dependencies I have a gradle android project that uses robolectric as described here: http://www.peterfriese.de/android-testing-with-robolectric/ and when I move the Android Platform away from 1st in the classpath I get the following exception: `Class not found: ""com.example.intellijgradletest.MainActivityTest""`"
757,A,"[Play Framework]: Performance testing with MySql I'm trying to implement play in my current job. And my manager asked me how would play framework deal with 1 millions records and MySql (I know that it is silly)...I've answered back that any problem JPA can address PLAY also can... But he want some report... So I thought on creating a test method with play UnitTest... a simple query from the database with one million records in some table. Question: Is there some way to simulate one millon record in mysql database ? PS: I know that maybe it could not be a PLAY related question... Something like this will help: MyController extends Controller { public static void doInsert() { for (int i=0;i<1000000;i++) { final SampleEntity entity = new SampleEntity(); entity.firstName = ""First Name ""+i; entity.anotherProperty = i; //etc entity.save(); } } } You may need to deal a bit with flushing the connection every so often (this is from memory may not be quite right) in order to avoid problems with Hibernate/JPA caching so many rows: if (i % 50000 == 0) { em().flush(); }  Insert a million records into a table. It will take a few minutes at most. You can't simulate real performance without real data."
758,A,"Injecting Mockito mocks into a Spring bean I would like to inject a Mockito mock object into a Spring (3+) bean for the purposes of unit testing with JUnit. My bean dependencies are currently injected by using the @Autowired annotation on private member fields. I have considered using ReflectionTestUtils.setField but the bean instance that I wish to inject is actually a proxy and hence does not declare the private member fields of the target class. I do not wish to create a public setter to the dependency as I will then be modifying my interface purely for the purposes of testing. I have followed some advice given by the Spring community but the mock does not get created and the auto-wiring fails: <bean id=""dao"" class=""org.mockito.Mockito"" factory-method=""mock""> <constructor-arg value=""com.package.Dao"" /> </bean> The error I currently encounter is as follows: ... Caused by: org...NoSuchBeanDefinitionException: No matching bean of type [com.package.Dao] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: { @org...Autowired(required=true) @org...Qualifier(value=dao) } at org...DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(D...y.java:901) at org...DefaultListableBeanFactory.doResolveDependency(D...y.java:770) If I set the constructor-arg value to something invalid no error occurs when starting the application context. I can do the following using Mockito -- <bean id=""stateMachine"" class=""org.mockito.Mockito"" factory-method=""mock""> <constructor-arg value=""com.abcd.StateMachine""/> </bean> With Regards A Thanks for the answer @Alexander. May I ask: does it wire-up correctly? If so which versions of Spring/Mockito are you using?  I have a very simple solution using Spring Java Config and Mockito: @Configuration public class TestConfig { @Mock BeanA beanA; @Mock BeanB beanB; public TestConfig() { MockitoAnnotations.initMocks(this); //This is a key } //You basically generate getters and add @Bean annotation everywhere @Bean public BeanA getBeanA() { return beanA; } @Bean public BeanB getBeanB() { return beanB; } } Thank you Piotr! This works beautifully - you deserve 10000 upvotes for this and I am sure they will sing songs about you in heaven.  Since 1.8.3 Mockito has @InjectMocks - this is incredibly useful. My JUnit tests are @RunWith the MockitoJUnitRunner and I build @Mock objects that satisfy all the dependencies for the class being tested which are all injected when the private member is annotated with @InjectMocks. I @RunWith the SpringJUnit4Runner for integration tests only now. I will note that it does not seem to be able to inject List in the same manner as Spring. It looks only for a Mock object that satisfies the List and will not inject a list of Mock objects. The workaround for me was to use a @Spy against a manually instantiated list and manually .add the mock object(s) to that list for unit testing. Maybe that was intentional because it certainly forced me to pay close attention to what was being mocked together. Yeah this is the best way. Springockito doesn't actually inject the mocks for whatever reason in my case.  Please take a look at this tiny little creature: https://bitbucket.org/kubek2k/springockito/wiki/Home This is a very clean approach - I like it! You had me at Springockito-annotations. Forget all the rest `springockito-annotations` is something you want to use. The nice thing is that you can have a non-complete XML configuration (omit the mocks) and annotations will fill in the blanks (the mocks). Then just `@Autowire` everything. Sadly Springockito doesn't let you easily grab the `WebApplicationContext` (you might do this if you're writing an integration test with `@WebAppConfiguation`) as documented by tihs issue: https://bitbucket.org/kubek2k/springockito/issue/12/plz-give-another-contextloader-that You've probably just got to call the `mock()` method yourself. Beware of https://bitbucket.org/kubek2k/springockito/issue/37/spring-test-context-caching-confuses Link is broken! @SandeepJindal Link works for me  If you're using spring >= 3.0 try using Springs @Configuration annotation to define part of the application context @Configuration @ImportResource(""com/blah/blurk/rest-of-config.xml"") public class DaoTestConfiguration { @Bean public ApplicationService applicationService() { return mock(ApplicationService.class); } } If you don't want to use the @ImportResource it can be done the other way around too: <beans> <!-- rest of your config --> <!-- the container recognize this as a Configuration and adds it's beans to the container --> <bean class=""com.package.DaoTestConfiguration""/> </beans> For more information have a look at http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/beans.html#beans-java Nice one. I used this when the test I am testing is @Autowired in the actual test case.  Below code works with autowiring - it is not shortest version but usefull when it should work only with standard spring/mockito jars. <bean id=""dao"" class=""org.springframework.aop.framework.ProxyFactoryBean""> <property name=""target""> <bean class=""org.mockito.Mockito"" factory-method=""mock""> <constructor-arg value=""com.package.Dao /> </bean> </property> <property name=""proxyInterfaces""> <value>com.package.Dao</value> </property> </bean>  <bean id=""mockDaoFactory"" name=""dao"" class=""com.package.test.MocksFactory""> <property name=""type"" value=""com.package.Dao"" /> </bean> this ^ works perfectly well if declared first/early in the XML file. Mockito 1.9.0/Spring 3.0.5  Update - new answer here: http://stackoverflow.com/a/19454282/411229. This answer only applies to those on Spring versions before 3.2. I've looked for a while for a more definitive solution to this. This blog post seems to cover all my needs and doesn't rely on ordering of bean declarations. All credit to Mattias Severson. http://www.jayway.com/2011/11/30/spring-integration-tests-part-i-creating-mock-objects/ Basically implement a FactoryBean package com.jayway.springmock; import org.mockito.Mockito; import org.springframework.beans.factory.FactoryBean; /** * A {@link FactoryBean} for creating mocked beans based on Mockito so that they * can be {@link @Autowired} into Spring test configurations. * * @author Mattias Severson Jayway * * @see FactoryBean * @see org.mockito.Mockito */ public class MockitoFactoryBean<T> implements FactoryBean<T> { private Class<T> classToBeMocked; /** * Creates a Mockito mock instance of the provided class. * @param classToBeMocked The class to be mocked. */ public MockitoFactoryBean(Class<T> classToBeMocked) { this.classToBeMocked = classToBeMocked; } @Override public T getObject() throws Exception { return Mockito.mock(classToBeMocked); } @Override public Class<?> getObjectType() { return classToBeMocked; } @Override public boolean isSingleton() { return true; } } Next update your spring config with the following: <beans...> <context:component-scan base-package=""com.jayway.example""/> <bean id=""someDependencyMock"" class=""com.jayway.springmock.MockitoFactoryBean""> <constructor-arg name=""classToBeMocked"" value=""com.jayway.example.SomeDependency"" /> </bean> </beans> Updated answer here: http://stackoverflow.com/a/19454282/411229  Update: There are now better cleaner solutions to this problem. Please consider the other answers first. I eventually found an answer to this by ronen on his blog. The problem I was having is due to the method Mockito.mock(Class c) declaring a return type of Object. Consequently Spring is unable to infer the bean type from the factory method return type. Ronen's solution is to create a FactoryBean implementation that returns mocks. The FactoryBean interface allows Spring to query the type of objects created by the factory bean. My mocked bean definition now looks like: <bean id=""mockDaoFactory"" name=""dao"" class=""com.package.test.MocksFactory""> <property name=""type"" value=""com.package.Dao"" /> </bean> Updated link to Ronen's Solution: http://narkisr.com/blog/2008/2647754885089732945 Link down :( so sad I don't understand that the factory method has return type Object ... But the amra's solution has a generic return type so that Spring should recognize it... But the amra's solution doesn't work for me Neither this solution spring doesn't infer the type of bean that is returned from the factoryBean hence No matching bean of type [ com.package.Dao ] ...  As of Spring 3.2 this is no longer an issue. Spring now supports Autowiring of the results of generic factory methods. See the section entitled ""Generic Factory Methods"" in this blog post: http://spring.io/blog/2012/11/07/spring-framework-3-2-rc1-new-testing-features/. The key point is: In Spring 3.2 generic return types for factory methods are now properly inferred and autowiring by type for mocks should work as expected. As a result custom work-arounds such as a MockitoFactoryBean EasyMockFactoryBean or Springockito are likely no longer necessary. Which means this should work out of the box: <bean id=""dao"" class=""org.mockito.Mockito"" factory-method=""mock""> <constructor-arg value=""com.package.Dao"" /> </bean>  The best way is: <bean id=""dao"" class=""org.mockito.Mockito"" factory-method=""mock""> <constructor-arg value=""com.package.Dao"" /> </bean> Update In the context file this mock must be listed before any autowired field depending on it is declared. I updated the code. Try it now! I get an error: ""Error creating bean with name 'mockito': bean definition is abstract"" @amra: spring dosn't infer the type of the object returned in this case... http://stackoverflow.com/q/6976421/306488 Don't know why this answer is upvoted so much the resulting bean cannot be autowired because it has the wrong type. Agree with @Ryan it works if the mock bean is defined before the dependent bean. I think this is because Spring unable to guess what the type of the mocked bean would be hence unless it is actually there Spring unable to determine that the mock has to be created beforehand It can be autowired if it is listed first in the context file (before any autowired fields that would depend on it are declared.) Yup as @Ryan said works for us when the mock bean is configured at the top of the context file (after any imports). See also http://stackoverflow.com/questions/16833063/spring-context-dirty-after-each-integration-test about spring 3.2 remedy. As of spring 3.2 the order of the beans no longer matters. See the section entitled ""Generic Factory Methods"" in this blog post: http://spring.io/blog/2012/11/07/spring-framework-3-2-rc1-new-testing-features/  Given: @Service public class MyService { @Autowired private MyDAO myDAO; // etc } You can have the class that is being tested loaded via autowiring mock the dependency with Mockito and then use Spring's ReflectionTestUtils to inject the mock into the class being tested. @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = { MvcConfiguration.class }) @RunWith(SpringJUnit4ClassRunner.class) public class MyServiceTest { @Autowired private MyService myService; private MyDAO myDAOMock; @Before public void before() { myDAOMock = Mockito.mock(MyDAO.class); ReflectionTestUtils.setField(myService ""myDAO"" myDAOMock); } // etc }  I found a similar answer as teabot to create a MockFactory that provides the mocks. I used the following example to create the mock factory (since the link to narkisr are dead): http://hg.randompage.org/java/src/407e78aa08a0/projects/bookmarking/backend/spring/src/test/java/org/randompage/bookmarking/backend/testUtils/MocksFactory.java <bean id=""someFacade"" class=""nl.package.test.MockFactory""> <property name=""type"" value=""nl.package.someFacade""/> </bean> This also helps to prevent that Spring wants to resolve the injections from the mocked bean.  Perhaps not the perfect solution but I tend not to use spring to do DI for unit tests. the dependencies for a single bean (the class under test) usually aren't overly complex so I just do the injection directly in the test code. I understand your approach. However I find myself in this situation on a large legacy code base that doesn't easily allow for this - yet. @Lars - agreed - the same could be said of the tests I am dealing with. I have found the Mockito/Spring combo to be very useful when I need to test code that depends heavily on Spring aspects/AOP (for instance when testing spring security rules). Although one is perfectly justified in claiming that such tests should be a integration test.  Posting a few examples based on the above approaches With Spring: @ContextConfiguration(locations = { ""classpath:context.xml"" }) @RunWith(SpringJUnit4ClassRunner.class) public class TestServiceTest { @InjectMocks private TestService testService; @Mock private TestService2 testService2; } Without Spring: @RunWith(MockitoJUnitRunner.class) public class TestServiceTest { @InjectMocks private TestService testService = new TestServiceImpl(); @Mock private TestService2 testService2; }  Today I found out that a spring context where I declared a before the Mockito beans was failing to load. After moving the AFTER the mocks the app context was loaded successfully. Take care :) There is something missing. 8-) You moved what after the mocks?  For the record all my tests correctly work by just making the fixture lazy-initialized e.g.: <bean id=""fixture"" class=""it.tidalwave.northernwind.rca.embeddedserver.impl.DefaultEmbeddedServer"" lazy-init=""true"" /> <!-- To solve Mockito + Spring problems --> <bean class=""it.tidalwave.messagebus.aspect.spring.MessageBusAdapterFactory"" /> <bean id=""applicationMessageBus"" class=""org.mockito.Mockito"" factory-method=""mock""> <constructor-arg value=""it.tidalwave.messagebus.MessageBus"" /> </bean> <bean class=""org.mockito.Mockito"" factory-method=""mock""> <constructor-arg value=""javax.servlet.ServletContext"" /> </bean> I suppose the rationale is the one Mattias explains here (at the bottom of the post) that a workaround is changing the order the beans are declared - lazy initialization is ""sort of"" having the fixture declared at the end.  Looks like the link to blog post you refer to in your answer has changed to http://narkisr.com/blog/2008/2647754885089732945  @InjectMocks private MyTestObject testObject @Mock private MyDependentObject mockedObject @Before public void setup() { MockitoAnnotations.initMocks(this); } This will inject any mocked objects in the test class so in this case it will inject mockedObject in testObject. This was mentioned above but here is the code. How do I stub a particular method of `mockedObject`? @Teinacher when(mockedObject.execute).thenReturn(objToReturn); You can put that either in the before method or inside your test method. FYI: This approach won't work if i want partial Autowiring and partial mocking in MyTestObject. This is exactly the type of solution I was looking for. Very simple and elegant and gets the job done. Not sure why there aren't more up votes."
759,A,Putting Spring integration tests in different classes and packages I am using AbstractTransactionalSpringContextTests to run spring integrations tests. The spring context is loaded just once and then all the tests are run. How do I do the same if I want my tests to be in many classes and packages. Of course the spring context should be loaded just once for all my tests (in all classes and packages) and not once per class or package. As the Javadocs for AbstractSingleSpringContextTests (one of the superclasses of AbstractTransactionalSpringContextTests) state: This class will cache contexts based on a context key: normally the config locations String array describing the Spring resource descriptors making up the context. Unless the setDirty() method is called by a test the context will not be reloaded even across different subclasses of this test. This is particularly beneficial if your context is slow to construct for example if you are using Hibernate and the time taken to load the mappings is an issue. Your context is cached so all other tests that run within the same classloader (i.e. - all your other tests that run during the same test run) will use the cached context. You don't need to do any extra setup - should be done for you already. If you're not sure about this or wish to troubleshoot simply turn on the logging for org.springframework and you should see plenty of helpful logging about when the context is being loaded which file how; etc.
760,A,"Why junit ComparisonFailure is not used by assertEquals(Object Object)? In Junit 4 do you see any drawback to throw a ComparisonFailure instead of an AssertionError when assertEquals(Object Object) fails ? assertEquals(Object Object) throws a ComparisonFailure if both expected and actual are String an AssertionError if either is not a String AssertionError message is already of the form ""expected:<""+ expected.toString() +""> but was <""+ actual.toString() (via String.valueOf see below junit-4.8.2 method invoked by Assert.assertEquals(Object Object) to build AssertionError message): static String format(Object expected Object actual) { ... String expectedString= String.valueOf(expected); String actualString= String.valueOf(actual); ... return formatted+""expected:<""+ expectedString +""> but was:<""+ actualString +"">""; ComparisonFailure provide far more readable way to spot the differences in dialog box of eclipse or Intellij IDEA (FEST-Assert throws this exception) [Update: question edited to focus on ComparisonFailure/AssertionError discussion.] I think you can certainly write your own substitute assertEquals method to do that without any significant problems if that works for you. However in the general case (from the point of view of the framework developers) is it a good idea I'm not sure. Often the failure objects won't have a toString implmentation at which point the failure message from the IDE will be very misleading. You would get the impression that the comparison was on reference identity when it may not have been. In other words it is valuable if the objects have a meaningful toString implementation otherwise it may not be. I agree it is more valuable when objects have a meaningful toString(). Otherwise the current message is already giving the impression that the comparison was on reference identity : ""java.lang.AssertionError: expected:<...@c2f1> but was:<...@20ba>""  We started with comparing strings because it was obvious how to make the error message more helpful. We never expanded ComparisonFailure to general objects because it wasn't clear how to do so in a general way. As other have suggested you're welcome to add special assertions if you can provide better error messages or move to Hamcrest which provides a general mechanism for adding useful failure messages. Regards Kent Thanks for mentioning Hamcrest I've never heard of it before. Looks interesting. http://code.google.com/p/hamcrest/  I agree with current JUnit implementation with two exception classes. Mostly because it gives us an ability to differentiate comparison problems (ComparisonFailure) and more ""severe"" type incompatibility problems (AssertionError). In general text message inside an Exception is just a helper for a human being and is not meant to be touched by any software tools. That's why type of thrown exception is the only indicator of the problem happened. In current junit it's not possible to differentiate between *non-string* comparison problems and incompatibility problems : both raise *AssertionError*. ComparisonFailure is raised only for String comparisons."
761,A,"Is there an automated way to make sure that all parts of code is unit tested? I have written JUnit tests for my class and would like it to tell me if there is any part of my code that is not unit tested. Is there a way to do this? Yes coverage tools like cobertura or emma. They create reports that show every line in the source code and whether it was executed or not (and aggregated statistics as well). Of course they can only show you if the code was run. There is no way to tell if the unit test contained assertions to confirm that the result was correct.  Your headline and your actual question differ. The tools mentioned in the other answers can tell you which part of the code were not tested (=not executed at all). Making ""make sure that all parts of code is unit tested"" is a different thing. The coverage tools can tell you whether all lines/instructions have been executed but they don't guarantee that everything is tested functionally (all constellations of data all execution paths etc.). This requires some brain power. In my opinion test coverage often gives a wrong feeling of safety. E.g. testing trivial getters increases coverage a lot but is rather useless.  You need some code coverage tools. See here (http://java-source.net/open-source/code-coverage) for some If you look at the first one I think it does what you need Cobertura is a free Java tool that calculates the percentage of code accessed by tests. It can be used to identify which parts of your Java program are lacking test coverage. It is based on jcoverage. Features of Cobertura: Can be executed from ant or from the command line.  If you are using IntelliJ then there is a button titled ""Run With Coverage""  If you use Eclipse you can also try EclEmma which shows you which lines of source were covered by your test. This is sometimes more useful than running a coverage tool like Cobertura because you can run a single test from inside Eclipse and then get immediate feedback on what was covered. Thank you. I like that it easily integrates into Eclipse"
762,A,"Unit testing several implementation of the same trait/interface I program mostly in scala and java using scalatest in scala and junit for unit testing. I would like to apply the very same tests to several implementations of the same interface/trait. The idea is to verify that the interface contract is enforced and to check Liskov substitution principle. For instance when testing implementations of lists tests could include: An instance should be empty if and only if and only if it has zero size. After calling clear the size sould be zero. Adding an element in the middle of a list will increment by one the index of rhs elements. etc. What are the best practices ? Contract tests are easy to do with JUnit 4 here's a video by Ben Rady. Great! Does this technique works also with Scalatest? Sorry no clue :) I just tried with scalatest and it seems to work. Thanks.  This sounds like it could be a job for shared tests. Shared tests are tests that are shared by different fixture objects. I.e. the same test code is run on different data. ScalaTest does have support for that. Search for ""shared tests"" in the documentation of your favorite style trait that represents tests as functions (Spec WordSpec FunSuite FlatSpec etc.). An example is the syntax for FlatSpec: it should behave like emptyList See Sharing Tests in the FlatSpec documentation  In Java/JUnit I generally handle this by having an abstract testcase from which tests for the specific test class inherit all the tests and have a setup method instantiating the implementation. I can't watch the video abyx posted right now but I suspect it's this general idea. Another interesting possibility if you don't mind introducing yet another testing framework would be to use JDave Specification classes. I haven't tried using either of these with Scalatest or with Scala traits and implementations but it should be possible to do something similar.  For Scala strongly consider ScalaCheck. All of those contracts are expressible as one-line specifications in ScalaCheck. When run ScalaCheck will generate a configurable number of sample inputs randomly and check that all of the specifications hold. It's about the most semantically dense way possible to create unit tests."
763,A,JUNIT Tests for OSB services Can we have Junit Test cases for testing Proxy Business Services in Oracle Service Bus? If yes can someone give me some pointers to the same. here http://cn.forums.oracle.com/forums/thread.jspa?threadID=1101583&start=0&tstart=0 a very long discussion I had with Alph he wrote his conclusions here http://wordpress.transentia.com.au/wordpress/2010/11/20/unit-testing-xquery-using-osbs-api-2/ this can be done by creating test suites to put and get messages from the queue and having your proxy do the transformation. The transformed output can then be compared with the expected out by simple string comparison.  JUnits can also be written for any type of proxy not only queue based proxies.
764,A,"Comparing arrays in JUnit assertions concise built-in way? Is there a concise built-in way to do equals assertions on two like-typed arrays in JUnit? By default (at least in JUnit 4) it seems to do an instance compare on the array object itself. EG doesn't work: int[] expectedResult = new int[] { 116800 116800 }; int[] result = new GraphixMask().sortedAreas(rectangles); assertEquals(expectedResult result); Of course I can do it manually with: assertEquals(expectedResult.length result.length); for (int i = 0; i < expectedResult.length; i++) assertEquals(""mismatch at "" + i expectedResult[i] result[i]); ..but is there a better way? Assert.assertArrayEquals(""message"" expectedResult result) Hm I don't see any 'assertArrayEquals' in my 'junit.framework.Assert'? @mBria upgrade your junit 4.8.1 is what I have and what appears to be the latest available via Maven (http://grepcode.com/search?query=junit+4.9&start=0&entity=type&n=). Is it only in 4.8.2 or 4.9? Got it. Andy's answer helped too thanks ya'll.  Use org.junit.Assert's method assertArrayEquals: import org.junit.Assert; ... Assert.assertArrayEquals( expectedResult result ); If this method is not available you may have accidentally imported the Assert class from junit.framework. @suat - You appear to be mistaken. Please check the online documentation at http://www.junit.org/apidocs/org/junit/Assert.html or the most recent release at http://www.junit.org/. If you consider your comment correct please provide a link. Upss I had thought to be using the latest version thanks for the pointer. But I can't revert the downvote as it says ""Your vote is now locked in unless this answer is edited"". @suat - You should now be able to revert the downvote. Thank you for taking a look. Assert.assertArrayEquals is deprecated now.  You can use Arrays.equals(..): assertTrue(Arrays.equals(expectedResult result)); What stinks about that though is you get NO data about what went wrong when it fails. Nice when you are on an older junit version (like on Android)"
765,A,GWT unit test for Activity and View Does anybody have a link for a tutorial on how to write JRE junit tests (extending TestCase and not GWTTestCase) that tests Activity and Views in GWT 2.1? Best regards Pich I have managed to test views which of course contains reference to the objects that calls GWT.create() using PowerMock. For Activities it is easy todo using Mockito for instance to mock the View.  Views can only be unit tested using GWTTestCase because they call (either explicitly or implicitly) GWT.create(). To test Activities use mock Views to avoid use of GWT.create(). what if activity contains reference to the objects such as GWT PlaceController which calls GWT.create() under the hood? (unfortunately PlaceController is a class so it cannot be mocked easily...) `Place Controller` has a constructor specifically for this: `PlaceController(EventBus eventBus PlaceController.Delegate delegate)` In this case you'll mock the `Delegate` instead of the `PlaceController`. It's worth noting as well that most mock frameworks (i.e. EasyMock) support mocking concrete classes (with limitations). Thanks you are right. I prefer to mock only interfaces so that's why I had this issue.
766,A,"Is there a problem-free way to run Scala 2.7.7 unit tests in Eclipse integrated nicely? I'm developing Scala code using Eclipse often when I run tests I get this error: No tests found with test runner 'JUnit 3'. Environment: Eclipse for Java Developers 3.5.1 Scala 2.7.7 JUnit 4.7 I'm currently writing my tests as JUnit3 tests and invoking them by right clicking on a package in the project explorer choosing Run As -> JUnit Test. (I was writing them as JUnit4 tests but ran into even more problems.) If I fire up eclipse the tests may not run unless I first open the source code file for the test. If I do open the source code file for the test it will run. However often then when I make any modification to the test file or any other source code file Eclipse will refuse to run my tests saying: ""No tests found with test runner 'JUnit 3'."" I just repeated this just now: Open eclipse Open the .scala file with some tests Invoke the tests by right clicking on the package for that file in the project explorer and choosing Run As -> JUnit Test It ran the tests One failed I changed a string literal in the failing test to fix it I then re-launced the test using the same method and I get the dreaded ""No tests found with test runner 'JUnit 3'."" I get this same method using other methods of launching the tests e.g. JUnit buttons or menus to re-run all or some tests To get the tests to run again I close and re-open Eclipse... So I end up relaunching Eclipse many many times a day. Note: I do often use XML literals in my tests I wonder if that has anything to do with it. 2nd Note: See my answer to this thread: http://stackoverflow.com/questions/1517642/what-is-the-current-state-of-the-scala-eclipse-plugin/1681156#1681156 where I described some of the other problems I'm having with Scala+Eclipse. Most of the problems are just minor annoyances but this test invocation problem is a real time waster would love to find a way around it! This Just works on trunk ... JUnit 3 and 4 unit test are detected and Run As => JUnit test does the right thing. Oh and BTW the best way to get my attention on this sort of thing is to file a bug or enhancement ticket in Trac rather than posting messages to StackOverflow (even though it worked this time ;-) Prior to trunk intercepting this behaviour the mechanism which the JDT used to search for annotations attempted to parse the searched source as if it were Java ... that sort of succeeded in some circumstances for some Scala sources but success here would essentially be accidental. Before you ask no there's absolutely no chance of a backport to the 2.7.x branch. Sounds like I will just have to be patient thanks. Thanks Miles yes I commented on the Trac issue: https://lampsvn.epfl.ch/trac/scala/ticket/1590. Having it work in the trunk is great but doesn't help me :) Certainly looking forward to 2.8 though. Any work arounds you can suggest on 2.7.7? Maybe based on your knowledge of the fix you might have some insight - it is odd that it works sometimes..."
767,A,"How to rewrite data-driven test suites of JUnit 3 in Junit 4? I am using data-driven test suites running JUnit 3 based on Rainsberger's JUnit Recipes. The purpose of these tests is to check whether a certain function is properly implemented related to a set of input-output pairs. Here is the definition of the test suite: public static Test suite() throws Exception { TestSuite suite = new TestSuite(); Calendar calendar = GregorianCalendar.getInstance(); calendar.set(2009 8 05 13 23); // 2009. 09. 05. 13:23 java.sql.Date date = new java.sql.Date(calendar.getTime().getTime()); suite.addTest(new DateFormatTestToString(date JtDateFormat.FormatType.YYYY_MON_DD ""2009-SEP-05"")); suite.addTest(new DateFormatTestToString(date JtDateFormat.FormatType.DD_MON_YYYY ""05/SEP/2009"")); return suite; } and the definition of the testing class: public class DateFormatTestToString extends TestCase { private java.sql.Date date; private JtDateFormat.FormatType dateFormat; private String expectedStringFormat; public DateFormatTestToString(java.sql.Date date JtDateFormat.FormatType dateFormat String expectedStringFormat) { super(""testGetString""); this.date = date; this.dateFormat = dateFormat; this.expectedStringFormat = expectedStringFormat; } public void testGetString() { String result = JtDateFormat.getString(date dateFormat); assertTrue( expectedStringFormat.equalsIgnoreCase(result)); } } How is it possible to test several input-output parameters of a method using JUnit 4? This question and the answers explained to me the distinction between JUnit 3 and 4 in this regard. This question and the answers describe the way to create test suite for a set of class but not for a method with a set of different parameters. Solution: Based on drscroogemcduck's answer this is the exact page what helped. the really simple way: you can always have a method: checkGetString(date dateFormat expectedValue) and then just have a method @Test testGetString: checkGetString(date1 '...' '...'); checkGetString(date2 '...' '...'); the nicer way: http://junit.sourceforge.net/javadoc_40/org/junit/runners/Parameterized.html or better junit theories: http://isagoksu.com/2009/development/agile-development/test-driven-development/using-junit-datapoints-and-theories/ checkGetString has all of its state inside the method and none in the class so it is independent. In this case yes but in general it is not necessary. For example I would like to test some database-related functions and I need a new connection for each test. With the ""really simple way"" how would you ensure that tests are running independently and setup is performed between checkGetString calls? The example of Parameterized smells for me: duplicate definition of Fibonacci parameters?"
768,A,"how to test w/ junit that warning was logged w/ log4j? I'm testing a method that logs warnings when something went wrong and returns null. something like: private static final Logger log = Logger.getLogger(Clazz.class.getName()); .... if (file == null || !file.exists()) { // if File not found log.warn(""File not found: ""+file.toString()); } else if (!file.canWrite()) { // if file is read only log.warn(""File is read-only: ""+file.toString()); } else { // all checks passed can return an working file. return file; } return null; i'd like to test with junit that a warning was issued in addition to returning null in all cases (e.g. file not found file is read-only). any ideas? thanks asaf :-) UPDATE My implementation of Aaron's answer (plus peter's remark): public class UnitTest { ... @BeforeClass public static void setUpOnce() { appenders = new Vector<Appender>(2); // 1. just a printout appender: appenders.add(new ConsoleAppender(new PatternLayout(""%d [%t] %-5p %c - %m%n""))); // 2. the appender to test against: writer = new StringWriter(); appenders.add(new WriterAppender(new PatternLayout(""%p %m%n"")writer)); } @Before public void setUp() { // Unit Under Test: unit = new TestUnit(); // setting test appenders: for (Appender appender : appenders) { TestUnit.log.addAppender(appender); } // saving additivity and turning it off: additivity = TestUnit.log.getAdditivity(); TestUnit.log.setAdditivity(false); } @After public void tearDown() { unit = null; for (Appender appender : appenders) { TestUnit.log.removeAppender(appender); } TestUnit.log.setAdditivity(additivity); } @Test public void testGetFile() { // start fresh: File file; writer.getBuffer().setLength(0); // 1. test null file: System.out.println("" 1. test null file.""); file = unit.getFile(null); assertNull(file); assertTrue(writer.toString() writer.toString().startsWith(""WARN File not found"")); writer.getBuffer().setLength(0); // 2. test empty file: System.out.println("" 2. test empty file.""); file = unit.getFile(""""); assertNull(file); assertTrue(writer.toString() writer.toString().startsWith(""WARN File not found"")); writer.getBuffer().setLength(0); } thanks guys @Bert: that's what happens when you synthesize an example... anyway good eyes! 10x There's a bug in the first if block such that if `file == null` is true the `file.toString()` in the log.warn will throw. just flexing those code-review ""muscles"" to keep them in shape. The examples in this post were very helpful but I found them little confusing. So I am adding a simplified version for the above with some minor changes. I am adding my appender to the root logger. This way and assuming additively is true by default I will not need to worry about losing my events due to logger hierarchy. Make sure this meet your log4j.properties file configuration. I am overriding append and not doAppend. Append in AppenderSkeleton deals with level filtering so I do not want to miss that. doAppend will call append if the level is right. public class TestLogger { @Test public void test() { TestAppender testAppender = new TestAppender(); Logger.getRootLogger().addAppender(testAppender); ClassUnderTest.logMessage(); LoggingEvent loggingEvent = testAppender.events.get(0); //asset equals 1 because log level is info change it to debug and //the test will fail assertTrue(""Unexpected empty log""testAppender.events.size()==1); assertEquals(""Unexpected log level""Level.INFOloggingEvent.getLevel()); assertEquals(""Unexpected log message"" loggingEvent.getMessage().toString() ""Hello Test""); } public static class TestAppender extends AppenderSkeleton{ public List<LoggingEvent> events = new ArrayList<LoggingEvent>(); public void close() {} public boolean requiresLayout() {return false;} @Override protected void append(LoggingEvent event) { events.add(event); } } public static class ClassUnderTest { private static final Logger LOGGER = Logger.getLogger(ClassUnderTest.class); public static void logMessage(){ LOGGER.info(""Hello Test""); LOGGER.debug(""Hello Test""); } } } log4j.properties log4j.rootCategory=INFO CONSOLE log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d %p [%c] - %m%n # un-comment this will fail the test #log4j.logger.com.haim.logging=DEBUG  In the setup of the unit test: Get the same logger Make it non-additive Add an appender which remembers the messages in a list: public class TestAppender extends AppenderSkeleton { public List<String> messages = new ArrayList<String>(); public void doAppend(LoggingEvent event) { messages.add( event.getMessage().toString() ); } } Add the appender to the logger Now you can call your code. After the test you will find all log messages in the list. Add the log level if you want (messages.add( event.getLevel() + "" "" + event.getMessage() );). In tearDown() remove the appender again and enable additivity. @Peter. Aaron: thanks a lot. I've used a combination of your answers. @Visage: comparing strings make test more readable IMHO You're right - but you should do the compare after the functionality has run. Storing LoggingEvents doesnt preculude you using their toString method in your Asserts while just storing their string representation may lose data. Even better to just make messages a list of LoggingEvents - that way you can process them however you want rather than dealing withe the string representation. There is nothing preventing `event` from changing after `doAppend()` was called. `toString()` will make sure you get a snapshot.  Instead of calling log4j directly use a protected method in your class. Something like: protected void log(String message Level level) { //delegates to log4j } Then create a subclass of the class under test that oevrrides this method so that you can verify it is being called as expected. class MyTest extends <class under test> { boolean somethingLogged = false; protected void log(String message Level level) { somethingLogged = true; } } and then assert based on somethingLogged. You can add conditional logic in the overriding method t test based on expected message/level. You could go further and record all the invocations and then search through the logged messages or check they were logged in the right order etc... 10x but i'd like to keep my test the less invasive i can. also your solution is very verbose and would require a central facility for logging and a lot of coding IMO. This is good as a last resort but it is better to use the existing Log4J facilities - that is more portable and reusable.  An alternative to Aaron's solution would be to configure a WriterAppender with an attached StringWriter. At the end of the test you can verify the contents of the log output string. This is a bit easier to implement (no need for custom code) however is less flexible with regards to checking the results as you only get the output as plain text. In some cases that may make it more difficult to verify the output than with Aaron's solution."
769,A,"How to write a jUnit test for a class that uses a network connection I would like to know what's the best approach to test the method ""pushEvent()"" in the following class with a jUnit test. My problem is that the private method ""callWebsite()"" always requires a connection to the network. How can I avoid this requirement or refactor my class that I can test it without a connection to the network? class MyClass { public String pushEvent (Event event) { //do something here String url = constructURL (event); //construct the website url String response = callWebsite (url); return response; } private String callWebsite (String url) { try { URL requestURL = new URL (url); HttpURLConnection connection = null; connection = (HttpURLConnection) requestURL.openConnection (); String responseMessage = responseParser.getResponseMessage (connection); return responseMessage; } catch (MalformedURLException e) { e.printStackTrace (); return e.getMessage (); } catch (IOException e) { e.printStackTrace (); return e.getMessage (); } } } Stubbing You'll need a test double (stub) to allow isolated easy unit testing. The following is non tested but demonstrates the idea. The use of Dependency Injection will allow you to inject at test time a test version of your HttpURLConnection. public class MyClass() { private IHttpURLConnection httpUrlConnection; public MyClass(IHttpURLConnection httpUrlConnection) { this.httpUrlConnection = httpUrlConnection; } public String pushEvent(Event event) { String url = constructURL(event); String response = callWebsite(url); return response; } } Then you create a stub (sometimes referred to as a mock object) to be the stand in for the concrete instance. class TestHttpURLConnection : IHttpURLConnection { /* Methods */ } You'll also construct a concrete version for your production code to use. class MyHttpURLConnection : IHttpURLConnection { /* Methods */ } Using your test class (an adapter) you are able to specifiy what should happen during your test. A mocking framework will enable you to do this with less code or you can manually wire this up. The end result of this for your test is that you'll set your expectations for your test for example in this case you may set OpenConnection to return a true boolean (This is just an example by the way). Your test will then assert that when this value is true the return value of your PushEvent method matches some expected result. I've not touched Java properly for a while but here are some recommended mocking frameworks as specified by StackOverflow members. I was just about to type something very similar. I think he needs to decouple his class from the url connection like you mentioned. then his unit test will test the interaction with a mock urlconnection. He also needs an integration test to test the integration with a concrete instance of the url connection. I should mention mocking is an overloaded term. Essentially it all boils down to using a fake object to enable isolated testing check the other mocking questions here on SO for a broader overview. If someone tells you to use mocking as I have you can do so with a fake/stub/mock/hand rolled object. +1 this is a more general applicable solution than the accepted answer. @Grundlefleck I agree but at the same time the accepted answer is a perfectly valid solution. In fact I'd recommend test extensions over mocking frameworks to begin when unit testing. Once test extensions become painful/and or tedious - it's time to move to a mocking framework. Lol and when mocking becomes tiring and costly what to do @Finglas ? :)  As an alternative to Finglas's helpful answer with respect to mocking consider a stubbed approach where we override the functionality of callWebsite(). This works quite well in the case where we aren't so interested in the logic of callWebsite as that of the other logic called within pushEvent(). One important thing to check is that callWebsite is calledwith the correct URL. So first change is to the method signature of callWebsite() to become: protected String callWebsite(String url){...} Now we create a stubbed class like this: class MyClassStub extends MyClass { private String callWebsiteUrl; public static final String RESPONSE = ""Response from callWebsite()""; protected String callWebsite(String url) { //don't actually call the website just hold onto the url it was going to use callWebsiteUrl = url; return RESPONSE; } public String getCallWebsiteUrl() { return callWebsiteUrl; } } And finally in our JUnit test: public class MyClassTest extends TestCase { private MyClass classUnderTest; protected void setUp() { classUnderTest = new MyClassStub(); } public void testPushEvent() { //could do with a more descriptive name //create some Event object 'event' here String response = classUnderTest.pushEvent(event); //possibly have other assertions here assertEquals(""http://some.url"" (MyClassStub)classUnderTest.getCallWebsiteUrl()); //finally check that the response from the callWebsite() hasn't been //modified before being returned back from pushEvent() assertEquals(MyClassStub.RESPONSE response); } }  Approaching things from a slightly different angle... I'd worry less about testing this specific class. The code in it is extremely simple and while a functional test to make sure it's working with a connection would be helpful a unit level test ""may"" not be necessary. Instead I'd focus on testing the methods it calls that appear to actually do something. Specifically... I'd test constructURL method from this line: String url = constructURL (event); making sure that it can construct a URL properly from different Events and throws Exceptions when it should (possibly on an invalid Event or null). And I'd test the method from the following line: String responseMessage = responseParser.getResponseMessage (connection); Possibly pulling out any ""get information out of the connection"" logic into one proc and leaving only ""parse said information"" in the original one: String responseMessage = responseParser.getResponseMessage(responseParser.getResponseFromConnection(connection)); or something along those lines. The idea being to put any ""must deal with external data sources"" code in one method and any code logic in separate methods that can be easily tested.  Possible solution: You can extend this class override callWebsite (you have to make it protected for this purpose) - and the override method write some stub method implementation.  Create an abstract class WebsiteCaller which would be a parent of ConcreteWebsiteCaller and WebsiteCallerStub. This class should have one method callWebsite (String url). Move your callWebsite method from MyClass to ConcreteWebsiteCaller. And MyClass will look like: class MyClass { private WebsiteCaller caller; public MyClass (WebsiteCaller caller) { this.caller = caller; } public String pushEvent (Event event) { //do something here String url = constructURL (event); //construct the website url String response = caller.callWebsite (url); return response; } } and implement method callWebsite in your WebsiteCallerStub in some way appropriate for testing. Then in your unit test do something like this: @Test public void testPushEvent() { MyClass mc = new MyClass (new WebsiteCallerStub()); mc.pushEvent (new Event(...)); } I think what you are demonstrating is an integration test as your test relies on an external resource. A better unit test would inject a mock/stub websitecaller interface. your integration test can test the websitecaller class exclusively as well as the websitecaller injected into Mycalss"
770,A,"Best practicies for JUnit and RMI Apps RMI Registry I'm looking to write a set of system integration tests for an RMI client application. I'd like to start the RMI server in JUnit 3.8.2 and then run those tests. Has anyone done this? I'm using something like this in a pojo with main:  import java.rmi.registry.*; .... //setup try { java.rmi.registry.LocateRegistry.createRegistry(1099); System.out.println(""RMI registry ready.""); this.StartMyServer(); // client set up MyClient m = null; m = (MyClient) Naming.lookup(""//MyHost/MyServer"" m); // tests here } catch (MyClientException me) { System.out.println(""MyClient fall down go boom""); } catch (MyServerException me) { System.out.println(""MyServer fall down go boom""); } catch (Exception e) { System.out.println(""Exception starting RMI registry:""); } What's the best way to turn this into a junit test so the registry is started only once and the tests only run if the registry actually starts up? I could like to avoid having a test class with one test method with all the test code from main there which is what I found myself doing before coming here. Also what should I watch out for when writing my tests? Any RMI testing gotchas? If you're using JDK 5 or higher I'd recommend upgrading to JUnit 4.4. Annotations are a great help. I'd also recommend reading this. I don't see why you'd write a separate main class to run tests. Isn't that what the TestRunner is supposed to do? I'd start the registry in a setup and shut it down in a teardown method. I didn't write the main the prior developer did. It's a manual ""smoke test"" that I am porting to junit"
771,A,"Comparing XMLs using XMLUnit RecursiveElementNameAndTextQualifier I am trying to compare 2 XML files using XMLUnit 1.2. I am using the RecursiveElementNameAndTextQualifier() qualifier. When changing the order of some entities order in my XML it causes XMLUnit to pass on some cases and fail on other cases. My XML file looks like this and I'm comparing it to a similar copy with a simple location swapping of one peer of attributes. <root> <ent> <value> <int>1</int> </value> <value> <int>2</int> </value> </ent> <ent> <value> <int>3</int> </value> <value> <int>4</int> </value> </ent> </root> When swapping int: 1 with int: 2  in one of the XML files the test fails. But when swapping int:4 with int:3 it keeps passing. Here is my testing code: public void testRecursiveElement() throws Exception { InputSource xml1 = new InputSource(""xml1.xml""); InputSource xml2 = new InputSource(""xml2.xml""); Diff myDiff = new Diff(xml1 xml2); myDiff.overrideElementQualifier(new RecursiveElementNameAndTextQualifier() ); assertXMLEqual(""Not similar"" myDiff true); } Can you advice what is the problem  and why does XMLUnit detect a diff in the first case and not in the second case. Is there any solution to this problem ? I've tried to reproduce the behavior you described but didn't succeed. When using the RecursiveElementNameAndTextQualifier the order of the elements doesn't seem to matter. So neither swapping int:1 and int:2 nor swapping int:3 and int:4 led to an assertion error in my tests. I don't know which behavior you'd expect but changing the RecursiveElementNameAndTextQualifier to a ElementNameAndTextQualifier led to assertion errors for both swapping cases."
772,A,"GWT JUnit test in NetBeans I have written application in GWT using NetBeans. Now I want to test my application with JUnit. I have never used JUnit before but I have basic concept of how it works. Now the question is how do I setup basic Unit test to test some of my GWT Widgets. I found THIS simple example but don't know how it can be ran in NetBeans. If you have the test file right click on it and select ""Test file"" or press Ctrl+F6. If you are using a Maven 2 project you can add the Surefire plugin to run all the test automatically when packaging."
773,A,"BeforeClass using Spring transactional tests I'm using the Spring transactional test classes to do unit testing of my DAO code. What I want to do is create my database once before all the tests run. I have a @BeforeClass annotated method but that runs before Spring loads up the application context and configures the jdbcTemplate thus I don't actually have a connection to the DB at that time. Is there a way to run my DB setup once after the context loads but before the tests start running? This thead asks the same question but the accepted solution seems to just be ""don't do that"". I'm inclined to say this just seems like it isn't doable. I have since moved to Spring 3 and am using the jdbc:embedded-database xml tag to set up my database before the test runs. I also created a custom test execution listener that wraps DBUnit operations for validating/resetting the database state after each test case. See http://blog.zenika.com/index.php?post/2010/02/05/Testing-SQL-queries-with-Spring-and-DbUnit-part-2 What was your solution? How did you make it work? I just set it up and BeforeClass annotated method runs after spring loads up the context..using testNG I'd second the advice that you should make each of your tests autonomous and therefore do all your setup with @Before rather than with @BeforeClass. If you wish to stick with your approach just use the @Before method and have a simple boolean check to see if the setup has already been completed. e.g.  if(!databaseSetup) { ...set up the database databaseSetup=true; } Not too fancy but it will work! See my answer here for an example spring transaction test with annotations using dbunit. Hope this helps! That's true - thanks for pointing it out. I suppose you could use a static member variable but that's just getting deeper into the problem. -1 because in fact this will NOT work because JUnit creates an instance of the test class for every method thus your ""databaseSetup"" variable will always be false when entering the ""@Before"" method and you will still end up reinitializing the database before every test method. If you make you databaseSetup boolean static this should work.  my solution a bit complicated but i needed it for an test framework :-) do not be afraid of the german javadocs the method names and bodies should be enough to get it FIRST create Annotation to mark Class or method for database work (create table and/or insert statements)   @Target({ElementType.TYPE ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) @Inherited @Documented public @interface SchemaImport { /** * Location der Schemadatei(en). Die Datei darf nur SQL Statements enthalten. * Wird keine Location gesetzt greift der Defaultwert. * @return String */ String[] locationsBefore() default {""input/schemas/before.sql""}; /** * Location der Schemadatei(en). Die Datei darf nur SQL Statements enthalten. * Wird keine Location gesetzt greift der Defaultwert. * @return String */ String[] locationsAfter() default {""input/schemas/after.sql""}; /** * Ein SchemaImport findet nur bei passender Umgebungsvariable statt mit diesem * Flag kann dieses Verhalten geändert werden. * @return boolean */ boolean override() default false; }  SECOND create listener which looks for the annotation AbstractTestExecutionListener is a Spring Framework Class from   <dependency> <groupId>org.springframework</groupId> <artifactId>org.springframework.test</artifactId> <version>2.5.6</version> </dependency>    public class SchemaImportTestExecutionListener extends AbstractTestExecutionListener implements ApplicationContextAware { /** * Standard LOG Definition. */ private static final Logger LOG = LoggerFactory.getLogger( SchemaImportTestExecutionListener.class); /** * Datasource Name - gemeint ist der Name der Datasource Bean bzw. die ID. */ private static final String DATASOURCE_NAME = ""dataSource""; /** * JDBC Template. */ private SimpleJdbcTemplate simpleJdbcTemplate; /** * Flag um festzustellen ob prepareTestInstance schon gerufen wurde. */ private boolean isAlreadyPrepared = false; /** * Standard Constructor laut API von konkreten Implementierungen für * TestexecutionListener erwartet es geht aber auch ohne. */ public SchemaImportTestExecutionListener() { } /** * Für jede Testklasse die mit der {@link SchemaImport} Annotation ausgezeichnet * ist wird ein entsprechender SchemaImport durchgeführt. * * Der SchemaImport findet pro Klasse exakt einmal statt. Diese Verhalten * entspricht der BeforeClass * Annotation von JUnit. * * Achtung mit Nutzung von Schemaimport auf Klassenebene ist kein * Rollback möglich stattdessen SchemaImport auf Methodenebene nutzen. * * @param testContext * @throws java.lang.Exception */ @Override public void prepareTestInstance(TestContext testContext) throws Exception { final SchemaImport annotation = AnnotationUtils.findAnnotation(testContext.getTestClass() SchemaImport.class); if ((annotation != null) && !isAlreadyPrepared && (isPropertyOrOverride(annotation))) { executeSchemaImports(testContext annotation.locationsBefore() true); isAlreadyPrepared = true; } } /** * Für jede Testmethode mit {@link SchemaImport} werden die angegebenen * Schema Dateien als SQL ausgeführt. * * @param testContext * @throws java.lang.Exception */ @Override public void beforeTestMethod(TestContext testContext) throws Exception { // nur für Methoden mit passender Annotation Schemaimport durchführen final SchemaImport annotation = AnnotationUtils.findAnnotation(testContext.getTestMethod() SchemaImport.class); if (annotation != null) { executeSchemaImports(testContext annotation.locationsBefore() true); } } @Override public void afterTestMethod(TestContext testContext) throws Exception { // nur für Methoden mit passender Annotation Schemaimport durchführen final SchemaImport annotation = AnnotationUtils.findAnnotation(testContext.getTestMethod() SchemaImport.class); if (annotation != null) { executeSchemaImports(testContext annotation.locationsAfter() false); } } /** * Prüfen ob passende Umgebungsvariable gesetzt wurde. Diese kann durch * entsprechendes Setzen des Flags an der Annotation überschrieben werden. * @return */ private boolean isPropertyOrOverride(SchemaImport annotation) { String prop = System.getProperty(TYPEnviroment.KEY_ENV); if (StringUtils.trimToEmpty(prop).equals(TYPEnviroment.EMBEDDED.getEnv())) { LOG.info(""Running SchemaImport Enviroment is set:'"" + prop + ""'""); return true; } else { if (annotation.override()) { LOG.warn( ""Running SchemaImport although Enviroment is set:'"" + prop + ""'""); return true; } else { LOG.warn( ""Not Running SchemaImport cause neither Environment or SchemaImport.override are set.""); return false; } } } /** * Hilfesmethode die eigentlichen SchemaImport kapselt. * * @param testContext * @param locations */ private void executeSchemaImports(TestContext testContext String[] locations boolean checkLocations) { // für jede Datei SchemaImport durchführen korrekte Reihenfolge // ist durch Entwickler zu gewährleisten if (locations.length > 0) { for (String location : locations) { if (StringUtils.trimToNull(location) != null) { if (isResourceExistant(location checkLocations)) { LOG.info(""Executing Schema Location: '"" + location + ""'""); SimpleJdbcTestUtils.executeSqlScript(getJdbcTemplate( testContext) new ClassPathResource(location) false); } else { LOG.warn( ""Schema Location '"" + location + ""' for SchemaImport not found.""); } } else { throw new RuntimeException(""SchemaImport with empty Locations in:'"" + testContext.getTestClass().getSimpleName() + ""'""); } } } } /** * * @param resource * @return */ private boolean isResourceExistant(String resource boolean checkLocations) { try { new ClassPathResource(resource).getInputStream(); return true; } catch (IOException ex) { if (checkLocations) { throw new RuntimeException(ex); } else { return false; } } } /** * Hilfsmethode um an ein JdbcTemplate heranzukommen. * * @param TestContext * @return SimpleJdbcTemplate */ private SimpleJdbcTemplate getJdbcTemplate(TestContext context) { if (this.simpleJdbcTemplate == null) { this.simpleJdbcTemplate = new SimpleJdbcTemplate(getDataSource( context)); } return this.simpleJdbcTemplate; } /** * Hilfsmethode um an die Datasource heranzukommen. * * @param testContext * @return DataSource */ private DataSource getDataSource(TestContext testContext) { return (DataSource) testContext.getApplicationContext().getBean( DATASOURCE_NAME DataSource.class); } /** {@inheritDoc} */ @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { throw new UnsupportedOperationException(""Not supported yet.""); } }  THIRD add the listener to the test execution   @ContextConfiguration(locations = {""classpath*:spring/persistence/*.xml""}) @Transactional @TestExecutionListeners({ TransactionalTestExecutionListener.class SchemaImportTestExecutionListener.class}) public abstract class AbstractAvHibernateTests extends AbstractAvTests { /** * SimpleJdbcTemplate für Subclasses verfügbar. */ @Autowired protected SimpleJdbcTemplate simpleJdbcTemplate; }  in use   @SchemaImport(locationsBefore={""schemas/spring-batch/2.0.0/schema-hsqldb.sql""}) public class FooTest extends AbstractAvHibernateTests { }  its important to note - beware of thread problems while using testNg parallel testing for this to work there should be some 'synchronized' markers for the getJdbcTemplate / dataSource Methods in the listener ps: the code for the test base class:   @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = {""classpath*:spring/*.xml""}) @TestExecutionListeners({ DependencyInjectionTestExecutionListener.class DirtiesContextTestExecutionListener.class LogDurationTestExecutionListener.class LogMethodNameTestExecutionListener.class}) public abstract class AbstractAvTests implements ApplicationContextAware { /** * Logger für Subclasses verfügbar. */ protected final Logger LOG = LoggerFactory.getLogger(getClass()); /** * {@link ApplicationContext} für Subclasses verfügbar. */ protected ApplicationContext applicationContext; /** {@inheritDoc } */ @Override public final void setApplicationContext(final ApplicationContext applicationContext) { this.applicationContext = applicationContext; } }  LogDurationTestExecutionListener and LogMethodNameTestExecutionListener are custom listeners not provided by spring but not needed for the schemaImport to work properly  Try using your old methods instead of the fancy annotations. @BeforeClass public static void beforeClass() { ApplicationContext context = new ClassPathXmlApplicationContext( ""applicationContext.xml""); [...] }  I don't know what unit testing framework you are using but for JUnit you can make your test class subclass AbstractTransactionalJUnit4SpringContextTests which has an executeSqlScript method this can either be run in a beforeclass or beforemethod method. My preference is to use BeforeMethod as this means that each of my unit tests are autonomous even if it means my unit tests run a bit slower. I wouldn't be able to put this in a @BeforeClass method because @BeforeClass methods must be static and are run before the application context is loaded. Ok it is possible with TestNg if that is any help?"
774,A,"Difference between setUp() and setUpBeforeClass() When unit testing with JUnit there are two similar methods setUp() and setUpBeforeClass(). What is the difference between these methods? Also what is the difference between tearDown() and tearDownAfterClass()? Here are the signatures: @BeforeClass public static void setUpBeforeClass() throws Exception { } @AfterClass public static void tearDownAfterClass() throws Exception { } @Before public void setUp() throws Exception { } @After public void tearDown() throws Exception { } The @BeforeClass and @AfterClass annotated methods will be run exactly once during your test run - at the very beginning and end of the test as a whole before anything else is run. In fact they're run before the test class is even constructed which is why they must be declared static. The @Before and @After methods will be run before and after every test case so will probably be run multiple times during a test run. So let's assume you had three tests in your class the order of method calls would be: setUpBeforeClass() (Test class first instance constructed and the following methods called on it) setUp() test1() tearDown() (Test class second instance constructed and the following methods called on it) setUp() test2() tearDown() (Test class third instance constructed and the following methods called on it) setUp() test3() tearDown() tearDownAfterClass()  Think of ""BeforeClass"" as a static initializer for your test case - use it for initializing static data - things that do not change across your test cases. You definitely want to be careful about static resources that are not thread safe. Finally use the ""AfterClass"" annotated method to clean up any setup you did in the ""BeforeClass"" annotated method (unless their self destruction is good enough). ""Before"" & ""After"" are for unit test specific initialization. I typically use these methods to initialize / re-initialize the mocks of my dependencies. Obviously this initialization is not specific to a unit test but general to all unit tests. BTW if you beginning to write unit test I would recommend this pot from my blog. It has pointers to other great material on unit testing as well : http://madhurtanwani.blogspot.com/search/label/mock  setUpBeforeClass is run before any method execution right after the constructor (run only once) setUp is run before each method execution tearDown is run after each method execution tearDownAfterClass is run after all other method executions is the last method to be executed. (run only once deconstructor)  From the Javadoc: Sometimes several tests need to share computationally expensive setup (like logging into a database). While this can compromise the independence of tests sometimes it is a necessary optimization. Annotating a public static void no-arg method with @BeforeClass causes it to be run once before any of the test methods in the class. The @BeforeClass methods of superclasses will be run before those the current class. The difference being that setUpBeforeClass is run before any of the tests and is run once; setUp is run once before each test (and is usually used to reset the testing state to a known-good value between tests)."
775,A,junit - share a fixture between testcase i wonder whether it is possible to have a fixture that can be shared between testcases for instance a hibernate session. Thank you! You want all your tests (not testcases) to share the same hibernate session ? Create it in your setUp() method only if it has not been already created and store it in a static member of your testcase class similar to a singleton implementation.
776,A,Maven-surefire-plugin and forked mode So I have some classes that rely on a jar file that has native methods in them. I am running into issues when mocking the objects in this jar file...so I have found a solution that works. Using forkedmode pertest seems to fix this issue. However there are 5 files affected by needing to be run in forkedmode...there are 130 other tests that do not need forking and the build time with cobertura and everything is VERY slow as it is forking for every test in that pom... So my question is...is there a way to specify which classes you want to run in forkedmode and run everything else normally? is there a way to specify which classes you want to run in forkedmode and run everything else normally? You can do this by specifying two <execution> elements with specific <configuration>: a default one for most tests (excluding those that need to be forked) with the forkMode set to once and a special one for the special tests (including only the special one) where the forkMode set to always. Here is a pom snippet showing how to do this: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <!-- Lock down plugin version for build reproducibility --> <version>2.6</version> <executions> <execution> <id>default-test</id><!-- here we configure the default execution --> <configuration> <forkMode>once</forkMode><!-- this is the default can be omitted --> <excludes> <exclude>**/somepackage/*Test.java</exclude> </excludes> </configuration> </execution> <execution> <id>special-test</id><!-- and here we configure the special execution --> <phase>test</phase> <goals> <goal>test</goal> </goals> <configuration> <forkMode>always</forkMode> <includes> <include>**/somepackage/*Test.java</include> </includes> </configuration> </execution> </executions> </plugin> See also 7.1.6. Setting Parameters for Goals Bound to Default Lifecycle @bmucklow You're welcome (and while maven can handle your use case very nicely it is already not that trivial IMHO). Glad it was helpful. yup that's the way to go Thanks worked perfectly! I'm still relatively new to Maven and editing pom files. Hi thank you I have been searching for this answer for quite a while but I do have one question how do I get maven to show me the combined executions at the moment I can only get the results for the default test.
777,A,How can I create stub Junit tests in Eclipse? Is there a simple way of creating stubs for Junit4 tests in Eclipse (Galileo)? You might also look into Fast Code Eclipse Plugin. Once you configure you can generate junit/testng test by selecting the class or any method. Also gives you way to navigate from a method to all the tests.  Right click on a class/file in the package explorer then new-> JUnit Test Case. Then a dialog opens and if you click the next button it allows you to check which methods to automatically create the stubs for. +1 thanks - that was what I was after.
778,A,"Binary output and testing for Eclipse plugins I am developing an Eclipse plugin and I use maven to coordinate my source structure. In order to compile the plugin I use the tycho extension for maven. However I was wondering how to execute unitests. I want to use the surefire plugin for testing as I additionally use a sonar server for source code quality management. Unitests are applyed if I use eclipse-test-plugin as package target. However I want to make use of the default surefire plugin for applying unitests. Now I figured out that the src/test/java that contains my unittest packages is read and compiled correctly but written into the wrong output folder. I need to have the tests in target/test-classes. However they are compiled to target/classes. As I am new to Eclipse plugin development and maven I could not find out how to write the tests to the correct output folder. I've already tried adding and and changing the build.properties of the eclipse-plugin project. It works also fine for other projects that aren't plugin projects and do not make use of tycho. Any help appreciated. Regards Florian Unlike standard maven projects the convention for eclipse plugins/OSGi bundles is to have tests reside in separate projects. This is because there is no such thing as a maven dependency scope ""test"" in OSGi. Thus keeping your tests inside the same project as your code under test would force you to mix up test code/dependencies an productive code/dependencies. As you mentioned Tycho provides a separate maven packaging type ""eclipse-test-plugin"" which you should use for dedicated test plugins/fragments. See https://docs.sonatype.org/display/TYCHO/PackagingTypes There is no support in Tycho for plain unit tests residing in the same project. Thank's for the answer. I figured out how to run the tests and it works well so I just can't add unit test stats to Sonar then but that's not that important."
779,A,"Solution for Parameterized Tests I'm familiar with the use of Parameterized tests in JUnit e.g: http://junit.org/apidocs/org/junit/runners/Parameterized.html but I wondered whether there were any alternative (possibly better) approaches to externally defined test data. I don't really want to hardcode my test data in my source file - I'd prefer to define it in XML or in some sort of other structured manner. Is there a particular framework or extension to a framework that provides me with a better solution than the standard JUnit approach. What specifically was the best approach you found to this? EDIT: I'm familiar with the FIT framework and the use of it in this case I want to avoid that if possible. If there is a JUnit extension or similar framework that provides a better solution than TestNG then please let me know. You can try Dependency Injection or Inversion of Control for this. The Spring Framework does this. ""Best"" is a matter of opinion. Google Guice also works quite well. How does this help me with parameterized testdata though? @ Dave W. Smith: point taken. I'm not sure Spring helps here sure it alleviates unit/integration testing but not in terms of parameterized data for tests. It seems a bit overkill to drop Spring into my application to achieve this also. Many thanks anyways.  So I found TestNG's approach to this which allows you to specify parameters for tests more info available here: http://testng.org/doc/documentation-main.html#parameters-testng-xml An example of this would be: @Parameters({ ""first-name"" }) @Test public void testSingleString(String firstName) { System.out.println(""Invoked testString "" + firstName); assert ""Cedric"".equals(firstName); } and: <suite name=""My suite""> <parameter name=""first-name"" value=""Cedric""/> <test name=""Simple example""> You can also use a datasource (such as Apache Derby) to specify this testdata I wonder how flexible a solution this is though.  There is nothing I see in the parameterized test example that requires you to store the data in the class itself. Use a method that pulls the data from an xml file and returns Object[][] and call that where in the example they use static code. If you want to switch the TestNG of course they have already written an XML parser for you. It looks like JUnitExt has one as well for JUnit 4 or you could write one of your own."
780,A,"another java.lang.ClassNotFoundException in ant's junit task I can't figure out why I am getting this exception from my ant build.xml file. I checked and everything is in the classpath. Why must this be so complicated?! I had trouble with Ant in the past and it seems it always is something related to the classpath. I am pointing to junit.jar using both ways: within eclipse: window->preferences->ant->runtime->Ant Home->Add External Jars and also within the build.xml script. This time Ant is not able to locate my test class in the junit task. Is there something wrong with the way I am pointing to this class? <target name=""init""> <property name=""sourceDir"" value=""src""/> <property name=""outputDir"" value=""build"" /> <property name=""junitLocation"" value=""C:\...\org.junit4_4.3.1\junit.jar"" /> </target> <target name=""clean"" depends=""init""> <delete dir=""${outputDir}"" /> </target> <target name=""prepare"" depends=""clean""> <mkdir dir=""${outputDir}"" /> </target> <target name=""compile"" depends=""prepare""> <javac srcdir=""${sourceDir}"" destdir=""${outputDir}"" classpath=""${junitLocation}""/> </target> <path id=""classpath""> <pathelement location=""${outputDir}"" /> <pathelement location=""${junitLocation}"" /> </path> <target name=""testApplication"" depends=""compile""> <echo>Running the junit tests...</echo> <junit fork=""yes"" haltonfailure=""yes""> <test name=""my.package.MyTest"" /> <formatter type=""plain"" usefile=""false"" /> <classpath refid=""classpath"" /> </junit> </target> I am always getting:  [junit] Testsuite: my.package.MyTest [junit] Tests run: 1 Failures: 0 Errors: 1 Time elapsed: 0 sec [junit] Caused an ERROR [junit] my.package.MyTest [junit] java.lang.ClassNotFoundException: my.package.MyTest [junit] at java.net.URLClassLoader$1.run(Unknown Source) [junit] at java.security.AccessController.doPrivileged(Native Method) [junit] at java.net.URLClassLoader.findClass(Unknown Source) [junit] at java.lang.ClassLoader.loadClass(Unknown Source) [junit] at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) [junit] at java.lang.ClassLoader.loadClass(Unknown Source) [junit] at java.lang.ClassLoader.loadClassInternal(Unknown Source) [junit] at java.lang.Class.forName0(Native Method) [junit] at java.lang.Class.forName(Unknown Source) BUILD FAILED Apparently Ant finds junit.jar and attempts to start the test but why can't it find my test class? I point to the folder with compiled test class. So I know that junit is on Ant's classpath at least but the ClassNotFound puzzles me. Any ideas perhaps? Many thanks! Just checking - you see ./build/my/package/MyTest.class? You're sure it builds properly? yep I can see the compiled class in the right directory no problem there Are you sure your test class is in the build folder? You're invoking junit in a separate JVM (fork=true) so it's possible that working folder would change during that invocation and with build being relative that may cause a problem. Run ant from command line (not from Eclipse) with -verbose or -debug switch to see the detailed classpath / working dir junit is being invoked with and post the results back here if you're still can't resolve this issue. I'll be danged! I think you are on to something! I changed the script to and it worked! Are there any particular disadvantages to set fork=no? It spawns a new JVM. Unless there's need for you to do so you may want to just leave it off so your tests run in-process with ant. Or if you do use fork set 'dir' attribute to explicitly specify your working folder. Forking is useful if your tests use a lot of memory; when fork=yes you can set the maxmemory which does a -Xmx on the forked JVM instance. Thanks a bunch! I did run ant with -verbose (from within eclipse). It is confirmed that with fork=yes junit fails. But with fork=no it works just fine. What should I be looking for in the ant verbose output? I concentrate only on what [junit] task spits out so a good starting point might be the classpath statements If you do need to keep fork set to yes set dir to your ""root"" folder (e.g. parent of `build`). That should work. Details are here: http://ant.apache.org/manual/OptionalTasks/junit.html  You don't need to add the JUnit jar to the classpath. If ant can't find it the <junit> task won't run. I suggest trying different ways of specifying the classpath as described in the ant doc at http://ant.apache.org/manual/using.html#path; specifically <classpath path=""${srcdir}"" /> might help. If not ChssPly76's suggestion of running ant -debug from the command line is the best bet. You'll want to bump up the buffer on your command prompt window though because ant's debugger is extremely verbose. Thanks for that. I don't use Ant 1.7 much :) Don't know why this was voted down. +1 to offset You're wrong about `` not running without junit.jar. Since Ant 1.7 it's possible to put junit.jar in classpath of `` task as long as `ant-junit.jar` is available to Ant. That said I do agree it's not necessarily the best approach"
781,A,"How do I write a JUnit test case to test threads and events I have a java code which works in one (main) thread. From the main thread i spawn a new thread in which I make a server call. After the server call is done I am doing some work in the new thread and after that the code joins the main thread. I am using eclipse Jobs to do the server call. I want to know how do I write a JUnit test case for this. I'm guessing that you may have done your mocking code and may want a simple integration test to ensure that that your server call works. One of the difficulties in testing threads comes from their very nature - they're concurrent. This means that you're force into writing JUnit test code that is forced to wait until your thread has finished its job before testing your code's results. This isn't a very good way of testing code and can be unreliable but usually means that you have some idea about whether you code is working. As an example your code may look something like: @Test public void myIntegrationTest() throws Exception { // Setup your test // call your threading code Results result = myServerClient.doThreadedCode(); // Wait for your code to complete sleep(5); // Test the results assertEquals(""some value""result.getSomeValue()); } private void sleep(int seconds) { try { TimeUnit.SECONDS.sleep(seconds); } catch (InterruptedException e) { e.printStackTrace(); } } I really don't like doing this and prefer mocks and agree with the other answers. But if you need to test your threads then this is one approach that I find works.  The resources provided by Abhijeet Kashnia may help but I am not sure what you are trying to achieve. You can do unit testing with mocks to verify your code that won't test concurrency but will provide coverage. You can write an integration test to verify that the threads are being created and joined in the fashion you expect.However this will not guarantee against concurrency problems. Most concurrent problems are caused by timing bugs which are not predictable and thus can't be tested for accurately.  You may need to restructure your code so that it can be easily tested. I can see several distinct areas for testing: Thread Management code: the code that launches the thread(s) and perhaps waits for results The ""worker"" code run in the thread The concurrency issues that may result when multiple threads are active Structure your implementation so that Your Thread Management code is agnostic as to the details of the Worker. Then you can use Mock Workers to enable testing of Thread Management - for example a Mock Worker that fails in certain ways allows you to test certain paths in the management code. Implement the Worker code so that it can be run in isolation. You can then unit test this independently using mocks for the server. For concurrency testing the links provided by Abhijeet Kashnia will help.  I suggest you use a mocking framework to confirm that the server call was indeed made. As for the thread unit testing: Unit testing multithreaded applications  This is what ConcurrentUnit was created for. The general usage is: Spawn some threads Have the main thread wait or sleep Perform assertions from within the worker threads (which via ConcurrentUnit are reported back to the main thread) Resume the main thread from one of the worker threads once all assertions are complete See the ConcurrentUnit page for more info."
782,A,"Passing command line arguments to JUnit in Eclipse All I am currently using JUnit 4 for writing test cases. I am fairly new to JUnit and finding it difficult to test my main class which takes arguments. I have specified the arguments to my JUnit test class by: 1 > Right click JUnit test class 2 > Goto Run As -> Run Configurations 3 > Select the Arguments tab and specify a value (I have entered an invalid argument i.e. the main class expects the command line argument to be converted to an int and I am passing a String value that cannot be converted to int) However the main class that I am testing if the command line argument cannot be converted to a int than I throw IllegalArgumentException. However the JUnit does not show the testMain() method as Error or Failure. I don't think my setup is right for the JUnit class. Can anyone please guide me where I am going wrong To test your class main method simply write something like: @Test(expected = IllegalArgumentException.class) public void testMainWithBadCommandLine() { YourClass.main(new String[] { ""NaN"" }); }  Change the main() method to something like this: public static void main(String[] args) { MyClass myclass = new MyClass(args); myclass.go(); } Move the code that was in main() to the new method go(). Now your test method can do this: public void myClassTest() { String[] args = new String[]{""one"" ""two""}; //for example MyClass classUnderTest = new MyClass(testArgs); classUnderTest.go(); }  Firstly the arguments should be in the program arguments section. Normally the launching point of the application that's the main method doesn't need to be tested if you design the app to be testable. Refactor the class  public static class ArgumentValidator { public static boolean nullOrEmpty(String [] args) { if(args == null || args.length == 0) { throw new IllegalArgumentException(msg); } //other methods like numeric validations } } You can now easily test the nullOrEmpty method using junit like  @Test(expected = IllegalArgumentException.class) public void testBadArgs() { ArgumentValidator.nullOrEmpty(null); } I think this is a better approach"
783,A,Using easymock repeated void method call I am new to easymock. I am trying to mock a service where one of the methods is a void method that will get called an unknown (and large) number of times. How do I specify that any number of calls is allowed? I know how to do it for methods that have a non-void return type. Thanks Call the void method of the mock. Afterwards use EasyMock.expectLastCall().anyTimes() Probably better to use EasyMock.expectLastCall().atLeastOnce() just to make sure the method is actually called. I tend to use for all expectations because it makes your tests less brittle to internal changes that don't effect behavior.
784,A,"How to jUnit test the result of code in another thread I have a process that runs in a thread (used as a realtime signal analysis process). I want to feed that thread process a known input and then test -- in jUnit -- that the output is correct. I have a callback listener that can notify me when the thread finishes processing the data and I can run assertions on the result successfully by registering the test case itself as a listener. When those assertions fail they do throw an exception. But that exception is not registered as a failure by jUnit presumably because they are happening outside of a test method. How do I structure my jUnit test so that the test fails correctly after the listener returns? Here's a simplified version of the code.  public class PitchDetectionTest extends TestCase implements EngineRunCompleteListener() { AudioData fixtureData; PitchDetectionEngine pitchEngine; public void setUp() { fixtureData = <stuff to load audio data>; } public void testCorrectPitch() { pitchEngine = new PitchEngine(fixtureData); pitchEngine.setCompletionListener(this); pitchEngine.start(); // delay termination long enough for the engine to complete try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } // gets called by the PitchEngine when it has finished processing all data in the // fixtureData. This is the only method defined by interface // EngineRunCompleteListener. public void notifyEngineRunComplete() { // The real code asserts things about the PitchEngine's results. When they fail // an exception is thrown that I can see in the console but this unit test still // shows as 'success' in the jUnit interface. What I want is to see // testCorrectPitch() fail. assertTrue(false); } } public class PitchEngine () { EngineRunCompleteListener completionListener(); Thread myThread; public void start() { // other stuff myThread = new Thread(this); myThread.start(); } public void run() { while (some condition) { // do stuff with the data } if (engineRunCompleteListener != null) { engineRunCompleteListener.notifyEngineRunComplete(); } } } this may be useful - http://code.google.com/p/awaitility/ You already have two threads running. Your junit thread and the process thread (started by myThread.start(). Off the top of my head I can think of at least two options that you have all of them involving moving the assertion away from notifyEngineRunComplete. For example: You can use join to wait for the process thread to finish and then do your assertions (Javadoc here). You can put your junit thread to sleep by waiting on a monitor object and then in your callback function notify this monitor. This way you'll know that the process has finished. You can use an Executor and a Future object. I think this would be the coolest solution if it works with your classes (Javadoc here). you can add a method to the engine class called `joinMe()` or something like this which performs the same functionality. This way the unit test thread can wait for the engine to finish without being aware of its internal thread join() is the solution we ended up using. I had to add a method to the engine class to return its thread which feels slightly icky because it means adding methods to a class for the pure purpose of testing them but it was otherwise very simple and it worked. Thank you for the suggestion. You should also consider using an atomicboolean to flag what happened in your other thread and assert its value in the main thread after the join occured.  if you must run the assertion code within the callback wrap the callback method in a try catch. catch any throwable and have a way of passing that exception back to the junit thread (some sort of shared thread state). the junit thread then just re-throws any returned throwable.  join() works if you only have one worker thread that you want to perform assertions from. If you have multiple threads that need to report assertions back to the main thread you'll need to use some other mechanism. Check out ConcurrentUnit for that. Not true you can collect your results and share them back to the main thread which can loop over them checking expected vs actual. You can also collect all the thread references and join against all of them. But I'll check out ConcurrentUnit too... @Antony - Sure but that requires a bit of boilerplate. Joining the worker thread from the main test thread avoids that or using something like ConcurrentUnit if you have multiple worker threads. But simply joining the thread will not report assertions from the child thread. I think you're right about that.  I want to feed that thread process a known input and then test -- in jUnit -- that the output is correct. I have a callback listener that can notify me when the thread finishes processing the data and I can run assertions on the result successfully by registering the test case itself as a listener. Rather than starting a separate thread for PitchEngine inside your unit test why not extract the //do stuff with the data logic in PitchEngine to another public method and simply invoke that in the unit test? I can't think of any reason to actually spawn the thread inside your unit test since it sounds like all you really care about (in this unit test) is testing the processing logic. This would be a great idea with the simplified version i've shown you here - sadly in this case running the ""do stuff"" in the engine involves hooking up a bunch of data processing objects and managing their interactions. It depends on several methods in PitchEngine and several more in its superclass and isn't easily extractable. I can see some approaches to improving the modularization there but they'd take more time than I have at the moment. Have you looked into replacing those other objects with mocks in your `PitchEngineTest`? Using mocks wouldn't help here; the test I'm trying to write is a full stack test of the pitch engine confirming that it is detecting the correct pitch given a sample audio waveform. I need all the actual code running."
785,A,"Why doesn't JUnit provide assertNotEquals methods? Does anybody know why JUnit 4 provides assertEquals(foobar) but not assertNotEqual(foobar) methods? It provides assertNotSame (corresponding to assertSame) and assertFalse (corresponding to assertTrue) so it seems strange that they didn't bother including assertNotEqual. By the way I know that JUnit-addons provides the methods I'm looking for. I'm just asking out of curiosity. I wonder same api of Assert is not too symmetric because for testing do objects refer to same it provides assertSame and assertNotSame. Of course it is not too long to write: assertFalse(foo.equals(bar)); With such a assertion only informative part of output is unfortunately name of test method so descriptive message should be formed separately: String msg = ""Expected <"" + foo + ""> to be unequal to <"" + bar +"">""; assertFalse(msg foo.equals(bar)); That is of course so tedious that it is better to roll your own assertNotEqual. Luckily in future it will maybe be part of the JUnit: JUnit issue 22 But this is less useful because JUnit can't generate a helpful failure message telling you for example the unequal values of foo and bar. The real failure reason is hidden and turned into a simple boolean. I totally agree. Especially assertFalse needs proper message argument to produce output to tell what really went wrong. I think this is usefull for the text present tests. Thnx Problem with the text is that it will be out of date as the code evolves.  I'm coming to this party pretty late but I have found that the form: static void assertTrue(java.lang.String message boolean condition) can be made to work for most 'not equals' cases. int status = doSomething() ; // expected to return 123 assertTrue(""doSomething() returned unexpected status"" status != 123 ) ; While this does work the problem is that if the assertion fails it will simply say ""Exepcted true but was false"" or some other unclear statement. What would be great is if it was Expected Not 123 but was 123.  It's better to use the Hamcrest for negative assertions rather than assertFalse as in the former the test report will show a diff for the assertion failure. If you use assertFalse you just get an assertion failure in the report. i.e. lost information on cause of the failure.  I'd suggest you use the newer assertThat() style asserts which can easily describe all kinds of negations and automatically build a description of what you expected and what you got if the assertion fails: assertThat(objectUnderTest is(not(someOtherObject))); assertThat(objectUnderTest not(someOtherObject)); assertThat(objectUnderTest not(equalTo(someOtherObject))); All three options are equivalent choose the one you find most readable. To use the simple names of the methods (and allow this tense syntax to work) you need these imports: import static org.junit.Assert.*; import static org.hamcrest.CoreMatchers.*; I appreciate the pointer to the alternate assertion syntax but pointing elsewhere doesn't answer *why* JUnit never provided `assertNotEquals()`. @seh: The way I read it the question was not about historical interest but about a way to formulate the assertion ""these two objects are not equal"" in a JUnit test. I answered that. Considering the ""why is/was there no `assertNotEqual`"" I'd say that's because it's a specialized assert that's not needed as often as `assertEquals` and therefore would be expressed via the generic `assertFalse`. also import static org.junit.Assert.assertThat; ""choose the one you find most readable"". People reading and writing unit tests are programmers. Do they really find this more readable than assertNotEqual(objectUnderTest someOtherObject) or assertFalse(objectUnderTest.equals(someOtherObject))? I'm not convinced by the fancy matcher APIs - it seems to be considerably harder for a programmer to explore/discover how to use them... @Joachim I agree that `assertThat` is more expressive than `assert*` but I don't think it's more expressive than the java expression you can put inside and out of the `assert*` expression in general (after all I can express anything in java code). It's a general problem I've started coming across with fluent-style APIs - every one is basically a new DSL you have to learn (when we all already know the Java one!). I suppose Hamcrest is ubiquitous enough now that it's reasonable to expect people to know it though. I'll have a play... @bacar: for some asserts it's basically a matter of style. But `assertThat` is a whole lot more expressive than the limited set of `assert*` methods available. Therefore you can express the exact constraints in a single line have it (almost) read like an english sentence *and* get a meaningful message when the assert fails. Granted that's not always a killer feature but when you've seen it in action a few times you'll see how much value it adds. These are the specific imports if you don't like * imports: `import static org.junit.Assert.assertThat;` `import static org.hamcrest.CoreMatchers.is;` `import static org.hamcrest.CoreMatchers.not;`  There is an assertNotEquals in JUnit 4.11: https://github.com/junit-team/junit/blob/master/doc/ReleaseNotes4.11.md#improvements-to-assert-and-assume  The obvious reason that people wanted assertNotEquals() was to compare builtins without having to convert them to full blown objects first: Verbose example: .... assertThat(1 not(equalTo(Integer.valueOf(winningBidderId)))); .... vs. assertNotEqual(1 winningBidderId); Sadly since Eclipse doesn't include JUnit 4.11 by default you must be verbose. Caveat I don't think the '1' needs to be wrapped in an Integer.valueOf() but since I'm newly returned from .NET don't count on my correctness.  Modulo API consistency why JUnit didn't provide assertNotEquals() is the same reason why JUnit never provided methods like assertStringMatchesTheRegex(regex str) vs. assertStringDoesntMatchTheRegex(regex str) assertStringBeginsWith(prefix str) vs. assertStringDoesntBeginWith(prefix str) i.e. there's no end to providing a specific assertion methods for the kinds of things you might want in your assertion logic! Far better to provide composable test primitives like equalTo(...) is(...) not(...) regex(...) and let the programmer piece those together instead for more readability and sanity. well for some reason assertEquals() exists. It didn't have to but it does. The question was about the lack of symmetry - why does assertEquals exist but not its counterpart?  I'd argue that the absence of assertNotEqual is indeed an asymmetry and makes JUnit a bit less learnable. Mind that this is a neat case when adding a method would diminish the complexity of the API at least for me: Symmetry helps ruling the bigger space. My guess is that the reason for the omission may be that there are too few people calling for the method. Yet I remember a time when even assertFalse did not exist; hence I have a positive expectation that the method might eventually be added given that it is not a difficult one; even though I acknowledge that there are numerous workarounds even elegant ones."
786,A,"JUnit java.lang.NoSuchMethodError: junit.framework.ComparisonFailure.getExpected()Ljava/lang/String I am getting the following exception from a test case that ran successfully before but now it throws this exception: java.lang.NoSuchMethodError: junit.framework.ComparisonFailure.getExpected()Ljava/lang/String; at org.eclipse.jdt.internal.junit4.runner.JUnit4TestListener.testFailure(JUnit4TestListener.java:63) at org.junit.runner.notification.RunNotifier$4.notifyListener(RunNotifier.java:100) at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:41) at org.junit.runner.notification.RunNotifier.fireTestFailure(RunNotifier.java:97) at org.junit.internal.runners.JUnit38ClassRunner$OldTestClassAdaptingListener.addError(JUnit38ClassRunner.java:41) at org.junit.internal.runners.JUnit38ClassRunner$OldTestClassAdaptingListener.addFailure(JUnit38ClassRunner.java:64) at junit.framework.TestResult.addFailure(TestResult.java:46) at junit.framework.TestResult.runProtected(TestResult.java:127) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:118) at junit.framework.TestSuite.runTest(TestSuite.java:208) at junit.framework.TestSuite.run(TestSuite.java:203) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Anyone know what is causing this? It seems like an internal issue with the JUnit runner. can you post the code at the line where it is failing? what version of junit are you using? It looks like others have seen this with 4.5. Unfortunately I cannot post the entire test method cause it contains some business logic. However there is nothing unusual about the assertions just simple assertEquals(). The stack trace doesn't give me a specific line from the test class that its failing on. The test ran fine before so I'm not sure why its throwing this exception. I have never seen this exception before. I am using JUnit Version 4.8.1 I only had the issue after I upgraded to the Kepler version of Eclipse. The test would run fine if I ran ""All"" tests. But when I ran a single method test it would fail with this error. I found this answer and thought I'd share it here to help someone else in the future. The long and short of it is upgrade to Junit 4.4 or better https://www.gotraveltech.com/confluence/display/COOK/My+Eclipse Scroll down to ""How do I run individual JUnit 4.x.x tests in MyEclipse?""  This can also occur if you're using JUnit 4 but import the Assert class from the old junit.framework package (rather than the new org.junit package) Check both your imports and your static imports - the culprit for me was import static junit.framework.Assert.assertEquals; thankyou! I did exactly the same thing :) See also: http://stackoverflow.com/questions/291003/differences-between-2-junit-assert-classes  The same error occurred to me when running my test class in eclipse but my solution was slightly different. My setup was that I had junit-4.10 jar in the classpath but my test class was using junit 3 and the exception only occurred if the assertEquals method failed. The problem was that eclipse was still using the junit 4 runner so I changed it by editing the configuration (selecting the ""Run Configuration"" in eclipse then selecting the test class and then changing the ""Test Runner"" in the dropdown from ""JUnit 4"" to ""JUnit 3"").  The getExpected() method on junit.framework.ComparisonFailure was only added in JUnit 3.8.2 (remember junit.framework package is from JUnit 3.8 whereas JUnit 4 uses org.junit). The method wasn't there in 3.8.1 which is the most common version of 3.x still out there. I think the method was added for easier migration to JUnit 4 tooling and occasionally this exception pops up on older code bases that use JUnit 3.8. The Eclipse JUnit 4 test runner would appear to switch back to calling the junit.framework.* code when running JUnit 3.8 tests. SO I'm guessing you still have JUnit 3.8.1 lurking about on your classpath and it's clashing with the Eclipse test runner. Either get rid of that JAR or ""upgrade"" it to 3.8.2. Thanks for the answer. If you do not see it in your project dependencies then one of the libraries might be using 3.8. If you cannot remove the library from there then try using the @RunWith(JUnit4.class) annotation.  I also had the problem but it was because I was using JUnit 4.1. I switched to 4.4 and the problem went away."
787,A,"Selecting specific tests to run in gradle I'm trying to fix our messy failing test runs and unfortunately I'm very new to gradle. We currently have testng junit and I'd like to add some spock tests to the mix as well. I'm not quite sure how gradle determines which tests to run when I type ""gradle test"". How can I prevent the testng &/or junit tests from running? How can I get gradle to start running my spock tests? $> gradle test -Dtest.single=YourTestClass  By default the test task runs all JUnit tests it can find which includes any Spock tests. To make it run TestNG tests instead configure the task as follows: test { useTestNG() } If you have both JUnit and TestNG tests you need two test tasks one for each test framework. To run a subset of tests use the -Dtest.single system property. For more information see the corresponding section in the Gradle User Guide. From the documentation: The testNamePattern will be used to form an include pattern of `**/testNamePattern*.class`. Hence `a/b/` selects all test classes below a package `a.b`. (Note that `a` doesn't necessarily have to be a root package.) To run Spock and TestNG but not JUnit tests you'll need two tasks (as explained above) and configure the JUnit task to just include Spock tests (by package or class naming convention). I looked at the documentation you referenced but it is quite confusing. For example what on earth does: ""gradle -Dtest.single=a/b/ test"" mean? I'm trying to prevent standard junit tests from running but run spock & testng tests. I'm not having any luck."
788,A,"Is there a way to prevent Maven Test from rebuilding the database? I've recently been asked to effectively sell my department on unit testing. I can't tell you how excited this makes me but I do have one concern. We're using JUnit with Spring and Maven and this means that each time mvn test is called it rebuilds the database. Obviously we can't integrate that with our production server -- it would kill valuable data. How do I prevent the rebuilding without telling maven to skip testing? The best I could figure was to assign the script to operate in a test database (line breaks added for readability): mvn test -Ddbunit.schema=<database>test -Djdbc.url=jdbc:mysql://localhost/<database>test? createDatabaseIfNotExist=true&amp; useUnicode=true&amp;characterEncoding=utf-8 I can't help but think there must be a better way. I'm especially interested in learning if there is an easy way to tell Maven to only run tests on particular classes without building anything else? mvn -Dtest=<test-name> test still rebuilds the database. ======= update ======= Bit of egg on my face here. I didn't realize that I was using the same variable in two places meaning that the POM was using a ""skip.test"" variable for both rebuilding the database and for running the tests... It's not a ""unit test"" if it touches a live production database Knowing how dbunit and Spring are being initialized for the unit tests would be helpful. Unit tests by definition only operate on a single component in the system. You should not be attempting to write unit tests which integrate with any external services (web DB etc.). The solution I have to this is to use a good mocking framework to stub out the behaviour of any dependencies your components have. This encourages good interface APIs since most mocking frameworks work best with simple interfaces. It would be best to create a Repository pattern interface for any interactions with your DB and then mock out the impl any time you are testing a class that interacts with it. You can then functionally test your Repository impl separately. This also has the added benefit of keeping your unit tests fast enough to remain part of your CI so that your feedback cycle is as fast as possible.  We're using JUnit with Spring and Maven and this means that each time mvn test is called it rebuilds the database. Maven doesn't do anything with databases by itself your code does. In any case it's very unusual to run tests (which are not unit tests) against a production database. How do I prevent the rebuilding without telling maven to skip testing? Hard to say without more details (you're not showing anything) but profiles might be a way to go.  Update: I guess that DBUnit does the rebuilding of the DB because it is told to do so in the test setup method. If you change your setup method you can eliminate the DB rebuild. Of course you should do it so that you get the DB reset when you need it and omit it when you don't. My first bet would be to use a system property to control this. You can set the property on the command line the same way you already do with jdbc.url et al. Then in the setup method you add an if to test for that property and do the DB reset if it is set. A test database completely separated from your production DB is definitely the best choice if you can have it. You can even use e.g. Derby an in-memory DB which can run embedded within the JVM. But in case you absolutely can't have a separate DB use at least a separate test schema inside that DB. In this scenario I would recommend you put your DB connection parameters into profiles within your pom the default being the test DB and a separate profile to contain the production settings. This way it can never happen that you accidentally run your tests against the production DB. In general however it is also important to understand that tests run against a DB are not really unit tests in the strict sense rather integration tests. If you have an existing set of such tests fine use them as much as you can. However you should try to move towards adding more real unit tests which test only a small isolated portion of your code at once (a method or class at most) ideally self contained (need no DB net config files etc.) so they can run fast - this is a very important point. If you have 5000 unit tests and each takes only 5 seconds to run that totals up to almost 7 hours so you obviously won't run them very often. If a test takes only 5 milliseconds you get the results in less than half a minute so you can afford to run all your tests before you commit your latest change - many times a day. That makes a huge difference in the speed of feedback you get from the tests. Hope this helps. See the update: I need mvn to build the classes but I can't simply call ""build and test without touching the (temporary/testing) database"""
789,A,Easy way to compare ArrayLists for equality using JUnit? What is an easy way to compare ArrayLists for equality using JUnit? Do I need to implement the equality interface? Or is there a simple JUnit method that makes it easier? I think this might be a slightly too easy answer (although it is correct). Testing ArrayLists for equals implies you have given thought to equality of the elements. If the elements are Integers that is all fine. But if they are instances of your own domain classes then you should be made aware of the pitfalls surrounding equality (and cloning). Please check out: http://www.artima.com/lejava/articles/equality.html for a good set of tips about implementing equality. On an aside: If you ever need to clone objects consider the use of copy constructors instead of implementing cloneable. Cloneable introduces a whole set of problems you might not expect.  You might want to check the documentation for List.equals. I'm new to Java thanks. So does this mean that if I have a List I will need to override equals for SomeClass ? It will compare the elements of the lists with `Object.equals`. By default this will be true if they are the same instance. If you want to allows different objects with the same internal data to match then they should provide `SomeClass` with an `equals` (and `hashCode`) method.  You need to do nothing special for list equality just use assertEquals. ArrayList and other lists implement equals() by checking that all objects in the corresponding positions of the lists are equal using the equals() method of the objects. So you might want to check that the objects in the list implement equals correctly. yep I noticed when I read the docs. The problem with this answer is that it won't report the contents of the list on failure. Try using assertThat(a is(b)); instead. starblue's warning about implementing equals still holds. See also [duplicate question 3236880](http://stackoverflow.com/questions/3236880/assert-list-in-junit): better answers and code samples there. Update from the future: this doesn't seem to work on arrays of primitive types and in either case assertEquals(Object[] Object[]) is deprecated. Use assertArrayEquals. @johncip That's good advice for arrays but the question was about `ArrayList`. Oops! I was looking at some array equality questions and this got mixed in somehow. Should have read more carefully. In that case you're right about arrayEquals obviously.
790,A,"Junit : add handling logic when exception is not expected I am trying to print the stack trace of the exception. However for negative test case only the unexpected exception is printed. I am using the @Rule ExpectedException to do the exception detection. I don't know how to add handling logic in case an unexpected exception is thrown. @Rule public ExpectedException thrown = ExpectedException.none(); @Test public void myTest() throws Exception { thrown.expect(MyException.class); thrown.expectMessage(""expected message""); } Franziga you might try catch-exception to satisfy your theoretical interest: @Test public void myTest() throws Exception { catchException(obj).doSomethingExceptional(); log(caughtException()); assertTrue(caughtException() instanceof MyException.class); assertEquals(""expected message"" caughtException().getMessage()); }  Can't you simply catch the exception within your test method and then print the stack trace? (and then even rethrow it if you want). I can do it but I just want to know if there is any other solution for that. @franziga what's your point - is it just theoretical interest or do you want to actually achieve something concrete? both :) Your workaround of course fulfills my requirement.  Did you try implementing WatchmanTest.failed?"
791,A,"NoClassDefFoundError when trying to unit test JSON parsing in Android I'm working on an Android app where some part of it gets JSON-formatted data from a web service. The basics of the class parsing the JSON data looks like this: public class JsonCourseParser implements CourseParser { public Course parseCourse(String courseData) { Course result; try { JSONObject jsonObject = new JSONObject(courseData); // parse stuff here and construct a proper Course object result = new Course(""foo"" ""bar""); } catch (JSONException e) { result = null; } return result; } } This builds just fine and it works when I call the parseCourse() method from within my Activity. When trying to test this very same function in a unit test however the unit test won't even launch. A NoClassDefFoundError is shown in the Failure Trace of the JUnit view in Eclipse: java.lang.NoClassDefFoundError: org/json/JSONException at JsonCourseParserTest.initClass(JsonCourseParserTest.java:15) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:236) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: java.lang.ClassNotFoundException: org.json.JSONException at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) ... 16 more My unit test looks like this: public class JsonCourseParserTest { private static CourseParser courseParser; @BeforeClass public static void initClass() { courseParser = new JsonCourseParser(); } @Test public void testParseCourse() { Object course = courseParser.parseCourse(DummyJsonConstants.TEP4165_JSON); assertNotNull(course); } } If I simply comment out the line with the JSONObject (and the belonging JSONException) in my JsonCourseParser class the test runs just fine. In case it might be relevant I have the following structure on my workspace: I have three different projects. One is the Android application project. The second is an Android Test project where I have unit tests that needs to interact with application context database views etc. The third is a normal project where I keep the ""clean"" unit tests i.e. those that only test utility classes POJO classes etc. The unit test above resides in the third project. Edit: I sadly don't have much more detail to add to the problem description. Is there really nobody that has any idea what might cause this issue? Any suggestions would be welcome! I had the same problem but I found a better way. The framework Roboelectric does the job. At first I just added the roboelectric.jar as external library (because I don't use Maven) to the build path of my Java JUnit test project and as ""class annotation"" I added @RunWith(RobolectricTestRunner.class). But then I got a NoClassDefFoundError: android/net/Uri. After I still added the android.jar itself used by the corresponding Android project (although the Android library with its android.jar was already a part of my build path) it works.  Edit: see updated answer at the end I've found the answer to this one. It doesn't solve my problem but at least it explains why there is a problem. First I made the faulty assumption that the JSONObject (imported from the org.json package) was included as a part of the JRE. It isn't - in an Android project this resides in android.jar (classic ""duh"" moment). This discovery boosted my self confidence a bit. This could easily be solved by adding a reference to the android.jar in my unit test project - or at least so I thought for a brief moment. Doing so only gave me another error when running my test: java.lang.RuntimeException: Stub! at org.json.JSONObject.<init>(JSONObject.java:8) ... At least this gave me something more to google for. What I found however wasn't really encouraging (yet another classic ""duh"" moment)... This blog describes the problem pretty well: Why Android isn’t ready for TDD and how I tried anyway. If you don't bother to read the whole thing the brief explanation is as follows: The problem here is that the android.jar supplied with the SDK is stubbed out with no implementation code. The solution that seems to be expected is that you should run your unit tests on the emulator or a real phone. When further doing some googling with this in mind I found several articles blogs and also questions here on SO regarding the issue. I'll add a few links here at the end for those that might be looking: android.jar In The SDK Only Has The API And Not The Implementation? Can I use android.os.* libraries in a standalone project? AndroidTesting Eclipse + Android + JUnit test references android.os class = NoClassDefFoundError (not sure why on earth I didn't find this one when I initially wrote my question I must have been completely lost...) And there are plenty more if you look around. There are several suggestions/solution/alternative approaches to unit testing in Android in many of those links but I won't bother to try to make a good answer based on that (as there is obviously way too much I still don't know about Android development). If anybody has any nice tips though I'll be glad to hear about them :) UPDATE: After experimenting a bit more I actually managed to find a working solution to this specific problem. The reason why I didn't try this in the first place was that I thought I had read somewhere that it would be problematic to include ""normal"" java libraries in my Android app. I was trying so many different things to get around my problem so I thought I'd just give this a try as well - and it actually worked! Here is how: I downloaded the source for the ""real"" org.json package from here: http://www.json.org/java/index.html Next I compiled the source code and packed it together in my own json.jar I added the newly created json.jar to the build path of my project (the main project of my Android application not the test project) No changes to my code no changes to my test only adding this library. And everything works both my Android app and my unit tests. I also tested stepping through the code in debug mode and when debugging the Android app the JSONObject in my JsonCourseParser is fetched from the Android SDK (android.jar) but when debugging my unit test it is fetched from my own json.jar. Not sure if this means that the json.jar isn't included when my app is built or if the runtime intelligently selects the correct library to use. Also not sure if this extra library might have any other effects on my final app. I guess only time will tell... thanks for your detailed analysis and description of the problem! And did the time tell you something for this problem? Building the jar from source worked for me! It seems really silly that Android doesn't include this in the SDK somewhere  I had this exact problem in my JSON tests. Your updated answer led me to try a simpler approach which worked for my project. It allows me to run JUnit tests using the 'real' org.json JAR on my non-Android classes from within Eclipse: Download org.json's json.jar from here (or wherever): http://mvnrepository.com/artifact/org.json/json Add a folder 'libs-test' to your project copy the json.jar file to it In Eclipse open: Run / Run Configurations select JUnit / YourTestClass On the Classpath tab remove ""Google APIs"" from Bootstrap entries Click ""Add JARs"" navigate to /libs-test/json.jar and add it. Click ""Close"" Run your tests"
792,A,"Can you or do you write Junit style unit tests without expliciting using 'assertEquals' exceptions fail test I noticed some unit tests like in the spring framework where you setup the object and the test but don't explicitly use the assert methods. Essentially you have an exception or not. Is this unit testing? Is this something to avoid? For example here are some tests from the Spring framework. No assert clauses just a test. public void testNeedsJoinPoint() { mockCollaborator.needsJoinPoint(""getAge""); mockControl.replay(); testBeanProxy.getAge(); mockControl.verify(); } I call this ""smoke testing"" and do it very often. It is like running your engine and assert that there is no smoke ie no exceptions are thrown. Personally I consider this good style (but hey I am also the guy that promotes dependent test :)  If you're concerned try breaking the code to make sure that the tests are effective.  Every exception let the test fail. So you can use your own exceptions to break your Unit test. If you read the source code a assertXXX will throw an Exception to the TestRunner. So Unit tests are build upon exceptions. Mocking is definitely nothing to avoid. It's good because it may encourage isolation of your tests.  The test you are showing is full of expectations but they are expressed in terms of the mock object. Some tests may be totally void of asserts and still be ok with me for instance a test that simply loads the spring context and (implicitly) asserts its validity. I really think the question should be if it is a good test. Sometimes it may be and sometimes it's just the best you can get. And it may often be a lot better than nothing."
793,A,"Spring 3+ How to create a TestSuite when JUnit is not recognizing it I'm using Spring 3.0.4 and JUnit 4.5. My test classes currently uses Spring's annotation test support with the following syntax: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration (locations = { ""classpath:configTest.xml"" }) @TransactionConfiguration (transactionManager = ""txManager"" defaultRollback = true) @Transactional public class MyAppTest extends TestCase { @Autowired @Qualifier(""myAppDAO"") private IAppDao appDAO; ... } I don't really need the line extends TestCase to run this test. It's not needed when running this test class by itself. I had to add extends TestCase so that I can add it in a TestSuite class: public static Test suite() { TestSuite suite = new TestSuite(""Test for app.dao""); //$JUnit-BEGIN$ suite.addTestSuite(MyAppTest.class); ... If I omit the extends TestCase my Test Suite will not run. Eclipse will flag suite.addTestSuite(MyAppTest.class) as error. How do I add a Spring 3+ test class to a Test Suite? I'm sure there's a better way. I've GOOGLED and read the docs. If you don't believe me I'm willing to send you all my bookmarks as proof. But in any case I would prefer a constructive answer. Thanks a lot. LOL. I googled again and saw my question on the Google results. I'm gonna search again :) Rod Johnson himself wrote ""Spring itself has an excellent unit test suite"" (http://www.theserverside.com/news/1363858/Introduction-to-the-Spring-Framework). Where is this excellent test suite documented? The url you gave doesnot work! You are right; JUnit4-style tests should not extend junit.framework.TestCase You can include a JUnit4 test as part of a JUnit3 suite this way: public static Test suite() { return new JUnit4TestAdapter(MyAppTest.class); } Usually you would add this method to the MyAppTest class. You could then add this test to your larger suite:  public class AllTests { public static Test suite() { TestSuite suite = new TestSuite(""AllTests""); suite.addTest(MyAppTest.suite()); ... return suite; } } You can create a JUnit4-style suite by creating a class annotated with Suite @RunWith(Suite.class) @SuiteClasses( { AccountTest.class MyAppTest.class }) public class SpringTests {} Note that AccountTest could be a JUnit4-style test or a JUnit3-style test. Thanks a lot. I tried your first suggestion and it worked! I was expecting Spring had its own test suite as written by Rod Johnson. But maybe he meant differently. Problem solved."
794,A,"Strategies to mock a webservice I'm implementing a client consuming a webservice. I want to reduce dependencies and decided to mock the webservice. I use mockito it has the advantage vs. EasyMock to be able to mock classes not just interfaces. But that's not the point. In my test I've got this code: // Mock the required objects Document mDocument = mock(Document.class); Element mRootElement = mock(Element.class); Element mGeonameElement = mock(Element.class); Element mLatElement = mock(Element.class); Element mLonElement = mock(Element.class); // record their behavior when(mDocument.getRootElement()).thenReturn(mRootElement); when(mRootElement.getChild(""geoname"")).thenReturn(mGeonameElement); when(mGeonameElement.getChild(""lat"")).thenReturn(mLatElement); when(mGeonameElement.getChild(""lon"")).thenReturn(mLonElement); // A_LOCATION_BEAN is a simple pojo for lat & lon don't care about it! when(mLatElement.getText()).thenReturn( Float.toString(A_LOCATION_BEAN.getLat())); when(mLonElement.getText()).thenReturn( Float.toString(A_LOCATION_BEAN.getLon())); // let it work! GeoLocationFetcher geoLocationFetcher = GeoLocationFetcher .getInstance(); LocationBean locationBean = geoLocationFetcher .extractGeoLocationFromXml(mDocument); // verify their behavior verify(mDocument).getRootElement(); verify(mRootElement).getChild(""geoname""); verify(mGeonameElement).getChild(""lat""); verify(mGeonameElement).getChild(""lon""); verify(mLatElement).getText(); verify(mLonElement).getText(); assertEquals(A_LOCATION_BEAN locationBean); What my code shows is that I ""micro-test"" the consuming object. It's like I would implement my productive code in my test. An example for the result xml is London on GeoNames. In my opinion it's far too granular. But how can I mock a webservice without giving everystep? Should I let the mock object just return a XML file? It's not about the code but the approach. I'm using JUnit 4.x and Mockito 1.7 The easiest option would be to mock the WebService client when(geoLocationFetcher.extractGeoLocationFromXml(anyString())) .thenReturn(""<location/>""); You can modify the code to read the response xml from the file system. Sample code can be found here: Mocking .NET WebServices with Mockito  I think the real problem here is that you have a singleton that calls and creates the web service so it is difficult to insert a mock one. You may have to add (possibly package level) access to the singleton class. For example if the constructor looks something like private GeoLocationFactory(WebService service) { ... } you can make the constructor package level and just create one with a mocked web service. Alternatively you can set the webservice by adding a setter method although I don't like mutable Singletons. Also in that case you have to remember to unset the webservice afterwards. If the webservice is created in a method you might have to make the GeoLocationFactory extensible to substitute the mock service. You may also look into remove the singleton itself. There are articles online and probably here on how to do that.  you really want to be mocking the results returned from the webservice to the code that will be using the result. In your example code above you seem to be mocking mDocument but you really want to pass in an instance of mDocument that has been returned from a mocked instance of your webservice and assert that the locationBean returned from the geoLocationFetcher matches the value of A_LOCATION_BEAN. usually I'd declare an interface and create a webservice wrapper that implements the interface and instantiates the webservice Thanks I got your point. So how would you ""mock a webservice""? So you just check the result your webservice consumer returns?"
795,A,"JUnit/HSQLDB: How to get around errors with Oracle syntax when testing using HSQLDB (no privilege and/or no DUAL object) I have DAO code which contains some JDBC with Oracle-specific syntax for example: select count(*) cnt from DUAL where exists (select null from "" + TABLE_NAME + "" where LOCATION = '"" + location + ""')"") I am running JUnit tests on this DAO method using an in-memory HSQLDB database. Apparently the DUAL table is Oracle specific and causes an error when I run the test: org.springframework.jdbc.BadSqlGrammarException: StatementCallback; bad SQL grammar [select count(*) cnt from DUAL where exists (select null from ESRL_OBSERVATIONS where LOCATION = '/path1')]; nested exception is java.sql.SQLException: user lacks privilege or object not found: DUAL Can anyone suggest anything I can do to get around this issue? I am using Hibernate to create the schema -- perhaps there's a setting I can make in my Hibernate properties which will enable support for Oracle style syntax? Thanks in advance for any suggestions etc. --James If you use Hibernate 3.6 with HSQLDB 2.0.1 or later you can use a connection property sql.syntax_ora=true on your connection URL. This enables the DUAL table together with some other Oracle specific syntax. You probably need a few more connection properties for behaviour that is not covered by the main property. See: http://hsqldb.org/doc/2.0/guide/management-chapt.html#mtc_compatibility_oracle Thanks a lot for this suggestion. Unfortunately I can't get it to work. My URL is 'jdbc:hsqldb:mem:testdb;sql.syntax_ora=true'. I am using Hibernate version 3.6.0 and HSQLDB version 2.0.1-rc3. Is there something else required that I may be missing? Something interesting is happening -- when I run the test now from an Ant build script then the test passes but if I run the test from within an Eclipse IDE (Run As -> JUnit Test) then I still get the original error. WTF? Nevermind -- I had an older version of the HSQLDB JAR file being pulled into the Eclipse build path from another Eclipse project which caused the error. Once that was rectified all went well. Thanks again for the expert advice.  The HSQL ""Oracle style syntax"" can also be enabled via a SQL command SET DATABASE SQL SYNTAX ORA TRUE See 12.30 . It's an alternative to set the property sql.syntax_ora=true as suggested in fredt's answer but in some cases it may be more practical (can be set via JDBC after the HSQL database has been started).  Create a table called DUAL with one column ""DUMMY"" in the HSQLDB database. Insert one row value 'X'. Thanks for this suggestion however I am not creating the tables manually or programmatically they are being generated in-memory by Hibernate when the tests launch. The tests run each time the code is built by Ant or Maven. @JamesAdams You may still create a table via custom script (using spring of course) ` `"
796,A,"Where can I find a complete Maven Cargo plugin example for EJB tests? For tests of some small JBoss enterprise apps I would like to use JUnit and the Maven Cargo plugin. (I know that there is also JSFUnit but first I would like to take a closer look at Cargo.) Is there a simple example available online which I could use as a reference for running a JUnit test which invokes a EJB operation using JBoss (4.2 or 5.1) using the Maven Cargo plugin? I have found some good introductions to the configuration but I get error messages in the EJB lookup so it would be helpful to see how it should be used. Here is the test code using InitialContext: public void testEcho() { assertEquals(""Echo Echo"" lookupEchoBeanRemote().Echo(""Echo"")); } private EchoBeanRemote lookupEchoBeanRemote() { try { Context c = new InitialContext(); return (EchoBeanRemote) c.lookup(""EchoBean/remote""); } catch (NamingException ne) { Logger.getLogger(getClass().getName()).log(Level.SEVERE ""exception caught"" ne); throw new RuntimeException(ne); } } Which gives this error: testEcho(de.betabeans.Echo2Test) Time elapsed: 0.885 sec <<< ERROR! java.lang.reflect.UndeclaredThrowableException at $Proxy3.Echo(Unknown Source) at de.betabeans.Echo2Test.testEcho(Echo2Test.java:17) Caused by: java.security.PrivilegedActionException: java.lang.reflect.InvocationTargetException at java.security.AccessController.doPrivileged(Native Method) at org.jboss.ejb3.security.client.SecurityActions.createSecurityContext(SecurityActions.java:657) at org.jboss.ejb3.security.client.SecurityClientInterceptor.invoke(SecurityClientInterceptor.java:59) at org.jboss.aop.joinpoint.MethodInvocation.invokeNext(MethodInvocation.java:102) at org.jboss.ejb3.remoting.IsLocalInterceptor.invoke(IsLocalInterceptor.java:74) at org.jboss.aop.joinpoint.MethodInvocation.invokeNext(MethodInvocation.java:102) at org.jboss.aspects.remoting.PojiProxy.invoke(PojiProxy.java:62) at $Proxy4.invoke(Unknown Source) at org.jboss.ejb3.proxy.impl.handler.session.SessionProxyInvocationHandlerBase.invoke(SessionProxyInvocationHandlerBase.java:207) at org.jboss.ejb3.proxy.impl.handler.session.SessionProxyInvocationHandlerBase.invoke(SessionProxyInvocationHandlerBase.java:164) ... 28 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.jboss.security.SecurityContextFactory.createSecurityContext(SecurityContextFactory.java:117) at org.jboss.security.SecurityContextFactory.createSecurityContext(SecurityContextFactory.java:76) at org.jboss.ejb3.security.client.SecurityActions$1.run(SecurityActions.java:662) ... 38 more Caused by: java.lang.ClassFormatError: Absent Code attribute in method that is not native or abstract in class file javax/security/jacc/PolicyContextException at java.lang.ClassLoader.defineClass1(Native Method) If I use the EJB annotation @EJB(beanInterface=EchoBeanRemote.classmappedName=""EchoBean/remote"") private EchoBeanRemote newSessionBean; public Echo3Test(String testName) { super(testName); } public void testEcho() { assertEquals(""Echo Echo"" newSessionBean.Echo(""Echo"")); } The test result is testEcho(de.betabeans.Echo3Test) Time elapsed: 0.001 sec <<< ERROR! java.lang.NullPointerException at de.betabeans.Echo3Test.testEcho(Echo3Test.java:20) jndi.properties is located in the EJB jar root folder and contains these lines: java.naming.factory.initial=org.jnp.interfaces.NamingContextFactory java.naming.factory.url.pkgs=org.jboss.naming:org.jnp.interfaces java.naming.provider.url=jnp://localhost:1099 ### The TimedSocketFactory connection timeout in milliseconds (0 == blocking) jnp.timeout=0 ### The TimedSocketFactory read timeout in milliseconds (0 == blocking) jnp.sotimeout=0 The bean source code is package de.betabeans; import javax.ejb.Remote; @Remote public interface EchoBeanRemote { String Echo(final String in); } package de.betabeans; import javax.ejb.Stateless; @Stateless public class EchoBean implements EchoBeanRemote { public String Echo(final String in) { return in + "" "" + in; } } I have also tested a web application which can call the EJB without problems - in both ways with InitialContext or an annotation. A warning which I received in the deployment of the web application was WARN [MappedReferenceMetaDataResolverDeployer] Unresolved references exist in JBossWebMetaData:[#web-app:AnnotatedEJBReferenceMetaData{name=de.betabeans.Echo3Servlet/echoBeanejb-ref-type=nulllink=nullignore-dependecy=falsemapped/jndi-name=EchoBean/remoteresolved-jndi-name=nullbeanInterface=interface de.betabeans.EchoBeanRemote} #web-app:AnnotatedEJBReferenceMetaData{name=NewServlet/newSessionBeanejb-ref-type=nulllink=nullignore-dependecy=falsemapped/jndi-name=NewSessionBean/remoteresolved-jndi-name=nullbeanInterface=interface de.betabeans.NewSessionBeanRemote}] 12:26:11770 INFO All tests performed with JBoss 5.1.0.GA on two different build systems. I have uploaded the complete Maven project now to http://www.mikejustin.com/download/JBossSimpleEJBApp-ejb-test.zip Please edit your question with these comments the commenting system is not ideal for code snippets. Also please provide the code of your lookup and JNDI settings. Your are right but after fixing it here the test still fails with the same error. I found some problem reports for UndeclaredThrowableException in JBoss which seem to indicate that the test is not using the correct version of the client jar files. I used 5.1.0.GA in the POM and this is also the installed server version. Comparing all Jar files will take some time :) I've updated my answer. If you will have problems with applyling my patch I will send you my entire sources. I've prepared version for Jboss 4.2.3. Do you run this test as regular jUnit test or by cargo? Can you place your entire project on the web (e.g. in code.google.com)? I run mvn install which runs the Cargo plugin and this starts JBoss 5.1 and deploys the app invokes the tests and shuts down JBoss again. I will upload the project later today. I get this exception: javax.naming.NoInitialContextException: Cannot instantiate class: org.jnp.interfaces.NamingContextFactory [Root exception is java.lang.ClassNotFoundException: org.jnp.interfaces.NamingContextFactory] This can be fixed by adding jbossall-client.jar to the POM but the I run into a classcast exception if I use the context lookup method. I have no jboss.xml configuration file and now try to configure the EJB there. If I use an @EJB annotation I get a null pointer exception. There are plenty of Cargo + JBoss samples on the web e.g. http://i-proving.ca/space/Technologies/Maven/Maven+Recipes/Deploy+to+JBoss+using+Cargo or http://www.vineetmanohar.com/2009/05/26/maven-cargo-jboss/. But there are many ways to use Cargo too so maybe you should paste your pom and the error here so that we could help. What is the problem actually? Uploaded the complete Maven project to http://www.mikejustin.com/download/JBossSimpleEJBApp-ejb-test.zip I'll check this. Just one thing why is `jndi.properties` located in the EJB jar root folder? You need to put it on the classpath of the client which means the unit test here (i.e. `src/tests/resources`). EDIT: after added sources First of all - my example works on JBoss 4.2.3.GA and Cargo 1.0 I did some refactoring of your code: pom.xml file  <groupId>de.betabeans</groupId> <artifactId>JBossSimpleEJBApp-ejb-test</artifactId> <packaging>ejb</packaging> <version>1.0-SNAPSHOT</version> <name>JBossSimpleEJBApp-ejb JEE5 EJB Test</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>javax.ejb</groupId> <artifactId>ejb-api</artifactId> <version>3.0</version> <scope>provided</scope> </dependency> <dependency> <groupId>jboss</groupId> <artifactId>jboss-ejb3</artifactId> <version>4.2.3.GA</version> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>3.8.1</version> <scope>test</scope> </dependency> <dependency> <groupId>org.jboss.client</groupId> <artifactId>jbossall-client</artifactId> <version>4.2.3.GA</version> <scope>test</scope> </dependency> </dependencies> <repositories> <repository> <releases> <enabled>true</enabled> </releases> <snapshots> <enabled>true</enabled> <updatePolicy>always</updatePolicy> </snapshots> <id>repository.jboss.com</id> <name>Jboss Repository for Maven</name> <url>http://repository.jboss.com/maven2/</url> <layout>default</layout> </repository> </repositories> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>2.0.2</version> <configuration> <source>1.5</source> <target>1.5</target> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-ejb-plugin</artifactId> <version>2.1</version> <configuration> <ejbVersion>3.0</ejbVersion> </configuration> </plugin> <plugin> <groupId>org.codehaus.cargo</groupId> <artifactId>cargo-maven2-plugin</artifactId> <version>1.0</version> <configuration> <container> <containerId>jboss42x</containerId> <home>${jboss.home}</home> <append>false</append> </container> <configuration> <type>existing</type> <home>${jboss.home}/server/default</home> <properties> <cargo.jboss.configuration>default</cargo.jboss.configuration> <cargo.rmi.port>1099</cargo.rmi.port> <cargo.logging>high</cargo.logging> </properties> </configuration> <wait>false</wait> </configuration> <executions> <execution> <id>start-container</id> <phase>pre-integration-test</phase> <goals> <goal>start</goal> </goals> </execution> <execution> <id>stop-container</id> <phase>post-integration-test</phase> <goals> <goal>stop</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <skip>true</skip> </configuration> <executions> <execution> <id>surefire-it</id> <phase>integration-test</phase> <goals> <goal>test</goal> </goals> <configuration> <skip>false</skip> </configuration> </execution> </executions> </plugin> </plugins> <finalName>JBossSimpleEJBApp-ejb-test</finalName> </build> I've changed your pom in following sections: dependecies (you use ejb-api from glasfish) repositories (I recomend to use JBoss Maven2 in layout default) version and containerId of cargo-maven2-plugin I've moved resources folder to test folder jndi.properties file must be use by test (not beans) and can be as follows or that you have: java.naming.factory.initial=org.jnp.interfaces.NamingContextFactory java.naming.factory.url.pkgs=org.jboss.naming:org.jnp.interfaces java.naming.provider.url=localhost The orher configuration files (jboss.xml MANIFEST.MF) aren't necessary. Configuration of server The biggest problem with jboss 4.2.3.GA server and ejb.jar file is that it doesn't work by default! Description of the problem and workaround you can find here. (That was the hardest thing. In Jboss 5.0 server this problem doesn't exist but in maven-cargo-plugin this container is as experimental one) That's all Below I paste some links to references If you'll have still problems I will send you my entire fixed project. EJB 3.0 Jboss tutorials (sourcecodes) Jboss 4.2.x configuration My original answer There are several problems with Cargo and JBoss. Main reason is that datasource configuration in pom doesn't work so you need to deploy separate datasource file. From JBoss perspective it has to be a file placed in main deploy directory (for Tomcat is located in META-INF folder). Second task is copy jdbc library before cargo run. Great resource for you is this post from Carlos Sanchez blog. Maybe you can use Selenium instead JSFUnit too :) To copy datasource file I use following configuration:  <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-resources-plugin</artifactId> <executions> <execution> <id>copy-ds-context</id> <goals> <goal>copy-resources</goal> </goals> <phase>pre-integration-test</phase> <configuration> <outputDirectory>${jboss.deploy-ds.dir}</outputDirectory> <resources> <resource> <directory>${basedir}/src/main/webresources/META-INF</directory> <filtering>true</filtering> <includes> <include>context-ds.xml</include> </includes> </resource> </resources> </configuration> </execution> </executions> </plugin> Many thanks for your answer which might be helpful in the future - in my current very basic example project there are no datasources involved. Works great - your help is very appreciated!"
797,A,"JUnit Exception Testing Edit: Not JUnit 4 available at this time. Hi there I have a question about ""smart"" exception testing with JUnit. At this time I do it like this: public void testGet() { SoundFileManager sfm = new SoundFileManager(); // Test adding a sound file and then getting it by id and name. try { SoundFile addedFile = sfm.addSoundfile(""E:\\Eclipse_Prj\\pSound\\data\\Adrenaline01.wav""); SoundFile sf = sfm.getSoundfile(addedFile.getID()); assertTrue(sf!=null); System.out.println(sf.toString()); sf = sfm.getSoundfileByName(""E:\\Eclipse_Prj\\pSound\\data\\Adrenaline01.wav""); assertTrue(sf!=null); System.out.println(sf.toString()); } catch (RapsManagerException e) { System.out.println(e.getMessage()); } // Test get with invalid id. try { sfm.getSoundfile(-100); fail(""Should have raised a RapsManagerException""); } catch (RapsManagerException e) { System.out.println(e.getMessage()); } // Test get by name with invalid name try { sfm.getSoundfileByName(new String()); fail(""Should have raised a RapsManagerException""); } catch (RapsManagerException e) { System.out.println(e.getMessage()); } } As you can see I need one try/catch block for each function that is supposed to throw an exception. It seems not to be a good way to do this - or is there no possibility to reduce the use of try/catch? I would suggest against doing System.out.println inside test. System.out and System.err tend to mixup and produce garbled test output. I suggest that you need to break up testGet into multiple separate tests. The individual try/catch blocks seem to be pretty independent of each other. You may also want to extract the common initialization logic into its own setup method. Once you have that you can use JUnit4's exception annotation support something like this: public class MyTest { private SoundManager sfm; @Before public void setup() { sfm = new SoundFileManager(); } @Test public void getByIdAndName() { // Test adding a sound file and then getting it by id and name. SoundFile addedFile = sfm.addSoundfile(""E:\\Eclipse_Prj\\pSound\\data\\Adrenaline01.wav""); SoundFile sf = sfm.getSoundfile(addedFile.getID()); assertTrue(sf!=null); System.out.println(sf.toString()); sf = sfm.getSoundfileByName(""E:\\Eclipse_Prj\\pSound\\data\\Adrenaline01.wav""); assertTrue(sf!=null); System.out.println(sf.toString()); } @Test(expected=RapsManagerException.class) public void getByInvalidId() { // Test get with invalid id. sfm.getSoundfile(-100); } @Test(expected=RapsManagerException.class) public void getByInvalidName() { // Test get with invalid id. sfm.getSoundfileByName(new String()); } } I like this one but i'm afraid of switching to JUnit4 in a running project. @InsertNickHere: Your caution is commendable but I suggest misplaced in this case. JUnit4 can run JUnit3-style tests without change allowing you to gradually migrate one test at a time. Also JUnit4 is now 4 years old it's really time you moved. I have to totally agree with you-in this case I have to get moved to JUnit 4. how to use this annotation to mention multiple expected exceptions? For e.g. a method might throw IO exception and Number format exception in the same execution. shall i write two different test methods? Got the answer for my doubt http://stackoverflow.com/questions/1410172/testing-for-multiple-exceptions-with-junit-4-annotations  The most concise syntax is provided by catch-exception: public void testGet() { SoundFileManager sfm = new SoundFileManager(); ... // setup sound file manager verifyException(sfm RapsManagerException.class) .getSoundfile(-100); verifyException(sfm RapsManagerException.class) .getSoundfileByName(new String()); }  With JUnit 4 you can use annotations instead. However you should separate your test into 3 distinct methods for this to work cleanly. Note that IMHO catching an exception in the first scenario should be a failure so I modified the catch block accordingly. public void testGet() { SoundFileManager sfm = new SoundFileManager(); // Test adding a sound file and then getting it by id and name. try { SoundFile addedFile = sfm.addSoundfile(""E:\\Eclipse_Prj\\pSound\\data\\Adrenaline01.wav""); SoundFile sf = sfm.getSoundfile(addedFile.getID()); assertTrue(sf!=null); System.out.println(sf.toString()); sf = sfm.getSoundfileByName(""E:\\Eclipse_Prj\\pSound\\data\\Adrenaline01.wav""); assertTrue(sf!=null); System.out.println(sf.toString()); } catch (RapsManagerException e) { fail(e.getMessage()); } } @Test(expected=RapsManagerException.class) public void testGetWithInvalidId() { SoundFileManager sfm = new SoundFileManager(); sfm.getSoundfile(-100); } @Test(expected=RapsManagerException.class) public void testGetWithInvalidName() { SoundFileManager sfm = new SoundFileManager(); sfm.getSoundfileByName(new String()); } As I said above I like the idea but I'm afaid of switching to JUnit4 in a running project. @InsertNickHere you should break up your test into 3 individual methods anyway with common setup. Then if you really want to minimize the exception handling code (and you have lots of duplicate code like the ones you show) you could extract the try/catch into a separate method and pass the actual method to be tested via an interface (but this is really overkill IMHO and it makes your test code more difficult to understand).  If you have an expected exception and you can't use an annotation to trap it you need to catch it and assert that you've got what you expected. For example: Throwable caught = null; try { somethingThatThrows(); } catch (Throwable t) { caught = t; } assertNotNull(caught); assertSame(FooException.class caught.getClass()); If you can use an annotation instead do that as it's much clearer. But that's not always possible (e.g. because you're testing a sequence of methods or because you're using JUnit 3). Sorry but I don't see any advantage of your version compared to mine. Maybe there is none with JUnit3 :( You're putting too much in a test case. You're putting too much in a `try`. You're printing messages out for manual reading rather than making test assertions fail! (You're embedding non-portable paths in your code but that's your business…) The paths are just for this example I will not post 100% original code here. But thanks for that advice anyway. :)"
798,A,"How do I set the working directory for the Ant 'junit' task? My Ant build includes a junit task that runs some tests. In order for the tests to work the value of the property that specifies the current working directory (user.dir) must be changed but I am unsure how to achieve this. The task in question currently looks like this: <junit printsummary=""withOutAndErr"" fork=""true"" haltonfailure=""yes"" showoutput=""true"" filtertrace=""false"" dir=""C:/workspace/obp-web""> <jvmarg value=""-Duser.dir=C:/workspace/obp-web""/> <classpath> <fileset dir=""${web.lib.dir}"" includes=""**/*.jar""/> <fileset dir=""${lib.dir}"" includes=""**/*.jar""/> </classpath> <batchtest fork=""no"" todir=""${web.build.dir}/testresults""> <formatter type=""xml""/> <zipfileset src=""${web.build.dir}/test-obp-web.jar""> <include name=""**/*Test.class""/> </zipfileset> </batchtest> </junit> Notice that I've tried to use both the ""dir"" attribute and the ""jvmarg"" task to change the working directory to C:/workspace/obp-web. However when I run Ant with verbose output turned on I see the following output which indicates that the working dir has not been set correctly: [junit] dir attribute ignored if running in the same VM [junit] Using System properties {java.runtime.name=Java(TM) SE Runtime Environment sun.boot.library.path=c:\jdk6\jre\bin java.vm.version=10.0-b23 ant.lib rary.dir=C:\java\apache-ant-1.7.0\lib java.vm.vendor=Sun Microsystems Inc. java.vendor.url=http://java.sun.com/ path.separator=; java.vm.name=Java HotSpot(T M) Client VM file.encoding.pkg=sun.io user.country=CA sun.java.launcher=SUN_STANDARD sun.os.patch.level=Service Pack 1 java.vm.specification.name=Java Virtual Machine Specification user.dir=c:\workspace\obp-ear java.runtime.version=1.6.0_07-b06 java.awt.graphicsenv=sun.awt.Win32GraphicsEnvironment java.endorse d.dirs=c:\jdk6\jre\lib\endorsed os.arch=x86 java.io.tmpdir=C:\Users\donal\AppData\Local\Temp\ line.separator= Try using a jvmarg: <junit fork=""yes""> <jvmarg value=""-Duser.dir=somedir""/> ... </junit> Note that fork must be true on both the junit tag and the batchtest tag as the batchtest tag overrides the value from junit. Jvmargs only work if junit forks a new JVM. I tried that but no luck. I've updated the original item with the info. @David That is true but we are instructing junit to fork a new JVM with that user.dir property not modifying the property in a running JVM You cannot change the current working directory in Java by setting user.dir. Changing that system property is considered a user error.  Same problem as you. I resolved it by making the batchtest fork to true : batchtest fork=""no"" .. to batchtest fork=""yes"" ..  Have you tried pathelement location? This worked for me.  <classpath> <!-- filesets etc. --> <pathelement location=""C:/workspace/obp-web"" /> </classpath>  Use the attribute ""dir"" (must also fork the vm): http://ant.apache.org/manual/Tasks/junit.html I tried that but no luck. I've updated the original item with the info. It worked for me. This worked also for me. Don't forget fork=""yes""."
799,A,"How to create unit tests easily in eclipse I want to create unit tests easily by just selecting method. Is there a tool in eclipse that does that. It should support templates. I should be able to create positive test as well as negative tests. To create a test case template: ""New"" -> ""JUnit Test Case"" -> Select ""Class under test"" -> Select ""Available methods"". I think the wizard is quite easy for you. I think OP wanted to select the method and create the unit test. I have written a plug-in for that. This was exactly what i was looking for the Netbeans function does exactly the same and when using maven its good that it creates the testcase in the correct folder @TheresaForster Which one you like? This answer or the one given by fastcodejava? Note that ""available methods"" is on the next page of the wizard (in Eclipse 4 at least)  You can use my plug-in to create tests easily. You can highlight the method and press Ctrl Alt Shift U and it will create the unit test for it. the plug-in is available at : http://www.3pintech.com/products/fast-code/. Hope this helps.  Check out this stackoverflow discussion - How to automatically generate junits? If you are starting new and its a java applicatin then Spring ROO looks very interesting too! Hope that helps.  Any unit test you could create by just pressing a button would not be worth anything. How is the tool to know what parameters to pass your method and what to expect back? Unless I'm misunderstanding your expectations. Close to that is something like FitNesse where you can set up tests then separately you set up a wiki page with your test data and it runs the tests with that data publishing the results as red/greens. If you would be happy to make test writing much faster I would suggest Mockito a mocking framework that lets you very easily mock the classes around the one you're testing so there's less setup/teardown and you know you're really testing that one class instead of a dependent of it. +1 for great comment. No tool can write the unit test you intend to create but the OP wanted to find the tool that will highlight the method and created stub test. You then have to fill the the test method body. the default tool that comes with eclipse does a pretty good job but one cannot highlight a method and create the test. Also in the test is already there the default tool will not work. First even automatically generated tests can be useful. Sometimes you simply forget to check something obvious - e.g. null pointer check. Second I think the question is meant to how to pregenerate tests which you will implement yourself. Something like if you need to check if your method throws exception accordingly tool may generate empty test method with @Test(expected=...) anotation automatically."
800,A,"Classpath trouble using JUnit with both Eclipse and Maven In a JUnit test I'm using this code to load in a test-specific config file: InputStream configFile = getClass().getResourceAsStream(""config.xml""); When I run the test through eclipse it requires the xml file to be in the same directory as the test file. When I build the project with maven it requires the xml to be in src/test/resources so that it gets copied into target/test-classes. How can I make them both work with just one file? Try adding the src/test/resources/ directory as a source folder in your Eclipse project. That way it should be on the classpath when Eclipse tries to run your unit test. Sadly this single saving comment took me a full day to find. Solved my problem of tests that get resource bundles running fine in Maven but failing when run in Eclipse. Thanks!  if you just need it at test time you can load it via the file system the context here should be the same for both cases: new FileInputStream(new File(""target/test-classes/your.file"")); not pretty but it works  I know this is an old question but I stumbled on it and found it odd that none of the above answers mention the Maven Eclipse Plugin. Run mvn eclipse:eclipse and it will add src/main/resources and src/test/resources and such to the ""Java Build Path"" in Eclipse for you. It has a bunch of stuff you can configure from there too (if you need to deviate from the conventions if you don't then it's already ready to go): Maven Eclipse Plugin - eclipse:eclipse Maven Eclipse Plugin - sourceIncludes/sourceExcludes In general if you're using Maven don't manually change stuff in Eclipse. Never commit your .classpath/.project and instead just use the Maven Eclipse Plugin to generate those files (and or update them). That way you have ONE configuration source for your project MAVEN and not manual settings that you end up having to tell others to use and remember yourself and so on. If it's a Maven project: check it out of source control run ""mvn eclipse:eclipse"" and it should work in Eclipse if it doesn't your POM needs work.  Place the config.xml file in src/test/resources and add src/test/resources as a source folder in Eclipse. The other issue is how getResourceAsStream(""config.xml"") works with packages. If the class that's calling this is in the com.mycompany.whatever package then getResourceAsStream is also expecting config.xml to be in the same path. However this is the same path in the classpath not the file system. You can either place file in the same directory structure under src/test/resources - src/test/resources/com/mycompany/whatever/config.xml - or you can add a leading ""/"" to the path - this makes getResourceAsStream load the file from the base of the classpath - so if you change it to getResourceAsStream(""/config.xml"") you can just put the file in src/test/resources/config.xml That's it! The / was the problem. Thanks!"
801,A,How to add a JUnit 4 test that doesn't extend from TestCase to a TestSuite? In JUnit 3 I simply called suite.addTestSuite( MyTest.class ) However if MyTest is a JUnit 4 test which does not extend TestCase this doesn't work. What should I do instead to create a suite of tests? Found the answer myself: here Like so: import org.junit.runner.RunWith; import org.junit.runners.Suite; @RunWith(Suite.class) @Suite.SuiteClasses({ TestCalculatorAddition.class TestCalculatorSubtraction.class TestCalculatorMultiplication.class TestCalculatorDivision.class }) public class CalculatorSuite { // the class remains completely empty // being used only as a holder for the above annotations }  For those with a large set of 3.8 style suites/tests that need to coexist with the new v4 style you can do the following: import org.junit.runner.RunWith; import org.junit.runners.Suite; @RunWith(Suite.class) @Suite.SuiteClasses({ // Add a JUnit 3 suite CalculatorSuite.class // JUnit 4 style tests TestCalculatorAddition.class TestCalculatorDivision.class }) public class CalculatorSuite { // A traditional JUnit 3 suite public static Test suite() { TestSuite suite = new TestSuite(); suite.addTestSuite(TestCalculatorSubtraction.class); return suite; } } I get an exception doing this `class xy contains itself as a SuiteClass`
802,A,"Junit to test concurrency I'm trying to test java.util.concurrent.ConcurrentLinkedQueue when accessed via multiple threads. Mentioned below is my Junit test using RepeatedTest to run in two concurrent threads. My questions is: is it correct to use RepeatedTest to test concurrency for example on ConcurrentLinkedQueue? The source code is mentioned below. Thanks import java.util.concurrent.ConcurrentLinkedQueue; import junit.extensions.ActiveTestSuite; import junit.extensions.RepeatedTest; import junit.extensions.TestSetup; import junit.framework.TestCase; public class TestNonBlockingConcurrentQueue extends TestCase{ private static ConcurrentLinkedQueue clq; public void testPut() throws Exception { int messageCounter = 0; for(;messageCounter <10000; messageCounter++){ clq.offer(messageCounter); } assertEquals(clq.size() messageCounter); } public void testGet() throws Exception { while(!clq.isEmpty()){ clq.poll(); } assertEquals(""Size should be zero"" clq.size() 0); } public static junit.framework.Test suite( ) { ActiveTestSuite ats = new ActiveTestSuite(); TestSetup setup = new TestSetup(ats) { protected void setUp() throws Exception { System.out.println(""Creating ConcurrentLinkedQueue..""); clq = new ConcurrentLinkedQueue(); } protected void tearDown( ) throws Exception { clq = null; } }; ats.addTest(new RepeatedTest(new TestNonBlockingConcurrentQueue(""testPut"") 2)); ats.addTest(new RepeatedTest(new TestNonBlockingConcurrentQueue(""testGet"") 2)); return setup; } public TestNonBlockingConcurrentQueue(String testName){ super(testName); } You can never really run tests to check concurrency problems. The fact that no problem shows up on a particular test machine (on a given OS with a certain number of cores or processors or even just other processes running at the same time) doesn't mean that there isn't a problem. Of course you can write tests for concurrency problems. You certainly can't write tests that are proof of the absence of bugs but you can still test for the presence of specific bugs. These tests can be complicated to write however. This is also true for non-concurrency problems. To quote 'Edsger W. Dijkstra': *Testing shows the presence not the absence of bugs.*. @Willi absolutely! It's just that concurrency problems tend to be harder to detect I think.  JUnitPerf uses RepeatedTest to test concurrent code so it seems reasonable to use it to do the same thing with your test above see: http://www.clarkware.com/software/JUnitPerf.html There are other methods for unit testing concurrent code although none of them can really verify that your code is thread safe: Concurrent Runner ConcJunit Also see: http://stackoverflow.com/questions/2035890/unit-testing-concurrent-code"
803,A,"is it possible to send JUnit test report by email? with ant it is possible to run JUnit tests and generate test reports in several formats: text HTML XML .. and it is recurrent to send test results by email to the responsible so my question: is it possible to use the xml file generated by Junit in order to send a summary(Html) of the test execution? or is there another better solution to send the results test execution by email? any help will be appreciated :) thanks for your helps. You could also convert the XML file into an HTML using XSL styles within junitreport task. http://ant.apache.org/manual/Tasks/junitreport.html And then use the inbuilt mail task to mail this HTML file <mail mailhost=""smtp.myisp.com"" mailport=""1025"" subject=""Test build""> <from address=""config@myisp.com""/> <replyto address=""me@myisp.com""/> <to address=""all@xyz.com""/> <message>The ${buildname} nightly build has completed</message> <attachments> <fileset dir=""dist""> <include name=""**/*.html""/> </fileset> </attachments> </mail>  Team City supports it out of the box and I found it easier to configure than Cruise Control.  Use the cruise control to build your project and run unit test. http://cruisecontrol.sourceforge.net/ its a very good tool.  Hudson is a very good build-server with support for email notifications of build and test results."
804,A,"JUnit 4.8.1 / Maven trivial test not working Although I tried out all suggestions I found I still can't get the most trivial JUnit test running. The error message basically repeats saying ""junit.framework.AssertionFailedError: No tests found in project002.trivial.TestClassTest"". You may inspect a snapshot of my IDE or download the zipped project. Here is my pom.xml: <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""> <modelVersion>4.0.0</modelVersion> <groupId>project002</groupId> <artifactId>project002</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>project002</name> <url>http://maven.apache.org</url> <properties> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> <repositories> <repository> <id>EclipseLink Repo</id> <url>http://www.eclipse.org/downloads/download.php?r=1&amp;nf=1&amp;file=/rt/eclipselink/maven.repo</url> </repository> </repositories> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.7</version> <scope>test</scope> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>2.3.1</version> <configuration> <source>1.6</source> <target>1.6</target> <encoding>UTF-8</encoding> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.5</version> <configuration> <junitArtifactName>org.junit:com.springsource.org.junit</junitArtifactName> <includes> <include>**/*Test.java</include> </includes> </configuration> </plugin> </plugins> </build> </project> Here is the error message: ------------------------------------------------------------------------------- Test set: project002.trivial.TestClassTest ------------------------------------------------------------------------------- Tests run: 1 Failures: 1 Errors: 0 Skipped: 0 Time elapsed: 0.078 sec <<< FAILURE! warning(junit.framework.TestSuite$1) Time elapsed: 0 sec <<< FAILURE! junit.framework.AssertionFailedError: No tests found in project002.trivial.TestClassTest at junit.framework.Assert.fail(Assert.java:47) at junit.framework.TestSuite$1.runTest(TestSuite.java:97) at junit.framework.TestCase.runBare(TestCase.java:134) at junit.framework.TestResult$1.protect(TestResult.java:110) at junit.framework.TestResult.runProtected(TestResult.java:128) at junit.framework.TestResult.run(TestResult.java:113) at junit.framework.TestCase.run(TestCase.java:124) at junit.framework.TestSuite.runTest(TestSuite.java:232) at junit.framework.TestSuite.run(TestSuite.java:227) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.maven.surefire.junit.JUnitTestSet.execute(JUnitTestSet.java:213) at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115) at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102) at org.apache.maven.surefire.Surefire.run(Surefire.java:180) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350) at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021) There's something wrong with your testcase. When you extend Testcase it will always be executed using Junit 3.x thus you need to prefix your test method with ""test"". If you want to use junit 4.x remove ""extends Testcase"" then annotate your test method with @Test. This one runs using Junit 3.x and it will not work because your test method is not prefixed with ""test"":- public class TestClassTest extends TestCase { @Test public void isThisReallyTrue() { assertTrue(true); } } This one will run in Junit 4.x:- public class TestClassTest { @Test public void isThisReallyTrue() { assertTrue(true); } } Thanks. It almost worked. isThisReallyTrue() was renamed to testIsThisReallyTrue() and since assertTrue() is a static method I had to adopt it to Assert.assertTrue(). Now it works fine! Hi Matt ... but it did *not* work for me without having the method name prefixed by ""test"" ... @erlord with JUnit 4 there is no longer any need to extend from TestCase or prefix your method names with ""test"" as long as you have @Test @erlord if you don't want to use Assert.assertTrue() do a ""import static"" on Assert. Then you can retain assertTrue() instead of Assert.assertTrue(). Hmm... @matt is right you don't really need to prefix with ""test"" if you already annotate the test method with @Test. That said you will find some folks (like me) prefer to prefix the test methods with ""test"" for readability purpose. Please if you could really be bothered downloading my testcase from http://drop.io/test4jerror20100710 and give it a try. I wonder where I go wrong. Thanks! I just repeated the test case and I am afraid I must confirm what I said before: isThisReallyTrue() is *not* recognized but changing it to testIsThisReallyTrue() actually works."
805,A,"No Such Method Error when creating JUnit test I've tried figuring out this problem for the last 2 days with no luck. I'm simply trying to create an annotation based JUnit test using the spring framework along with hibernate. My IDE is netbeans 6.5 and I'm using hibernate 3 spring 2.5.5 and JUnit 4.4. Here's the error I'm getting: Testcase: testFindContacts(com.mycontacts.data.dao.MyContactHibernateDaoTransactionTest): Caused an ERROR Failed to load ApplicationContext java.lang.IllegalStateException: Failed to load ApplicationContext at org.springframework.test.context.TestContext.getApplicationContext(TestContext.java:203) at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109) at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75) at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:255) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.createTest(SpringJUnit4ClassRunner.java:93) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.invokeTestMethod(SpringJUnit4ClassRunner.java:130) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sessionFactory' defined in class path resource [shared-context.xml]: Invocation of init method failed; nested exception is java.lang.NoSuchMethodError: org.objectweb.asm.ClassWriter.<init>(I)V at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1337) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:473) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:409) at java.security.AccessController.doPrivileged(Native Method) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:380) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:264) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:221) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:261) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:185) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164) at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:423) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:729) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:381) at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:84) at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:42) at org.springframework.test.context.TestContext.loadApplicationContext(TestContext.java:173) at org.springframework.test.context.TestContext.getApplicationContext(TestContext.java:199) Caused by: java.lang.NoSuchMethodError: org.objectweb.asm.ClassWriter.<init>(I)V at net.sf.cglib.core.DebuggingClassWriter.<init>(DebuggingClassWriter.java:47) at net.sf.cglib.core.DefaultGeneratorStrategy.getClassWriter(DefaultGeneratorStrategy.java:30) at net.sf.cglib.core.DefaultGeneratorStrategy.generate(DefaultGeneratorStrategy.java:24) at net.sf.cglib.core.AbstractClassGenerator.create(AbstractClassGenerator.java:216) at net.sf.cglib.core.KeyFactory$Generator.create(KeyFactory.java:144) at net.sf.cglib.core.KeyFactory.create(KeyFactory.java:116) at net.sf.cglib.core.KeyFactory.create(KeyFactory.java:108) at net.sf.cglib.core.KeyFactory.create(KeyFactory.java:104) at net.sf.cglib.proxy.Enhancer.<clinit>(Enhancer.java:69) at org.hibernate.proxy.pojo.cglib.CGLIBLazyInitializer.getProxyFactory(CGLIBLazyInitializer.java:117) at org.hibernate.proxy.pojo.cglib.CGLIBProxyFactory.postInstantiate(CGLIBProxyFactory.java:43) at org.hibernate.tuple.entity.PojoEntityTuplizer.buildProxyFactory(PojoEntityTuplizer.java:162) at org.hibernate.tuple.entity.AbstractEntityTuplizer.<init>(AbstractEntityTuplizer.java:135) at org.hibernate.tuple.entity.PojoEntityTuplizer.<init>(PojoEntityTuplizer.java:55) at org.hibernate.tuple.entity.EntityEntityModeToTuplizerMapping.<init>(EntityEntityModeToTuplizerMapping.java:56) at org.hibernate.tuple.entity.EntityMetamodel.<init>(EntityMetamodel.java:295) at org.hibernate.persister.entity.AbstractEntityPersister.<init>(AbstractEntityPersister.java:434) at org.hibernate.persister.entity.SingleTableEntityPersister.<init>(SingleTableEntityPersister.java:109) at org.hibernate.persister.PersisterFactory.createClassPersister(PersisterFactory.java:55) at org.hibernate.impl.SessionFactoryImpl.<init>(SessionFactoryImpl.java:226) at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:1294) at org.hibernate.cfg.AnnotationConfiguration.buildSessionFactory(AnnotationConfiguration.java:859) at org.springframework.orm.hibernate3.LocalSessionFactoryBean.newSessionFactory(LocalSessionFactoryBean.java:814) at org.springframework.orm.hibernate3.LocalSessionFactoryBean.buildSessionFactory(LocalSessionFactoryBean.java:732) at org.springframework.orm.hibernate3.AbstractSessionFactoryBean.afterPropertiesSet(AbstractSessionFactoryBean.java:211) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1368) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1334) It appears to be a configuration problem (class path) with netbeans 6.5. I copied the project over to eclipse and it works just fine. I'm getting this ERROR even if in Eclipse I tried with 1.5/3.1/4.0. I had a similar error using Spring 2.5 with Hibernate on NetBeans 6.5. One way I resolved the issue was downloading Spring 3.0 M2 and creating a NetBeans library I called Spring_3 using the jars from the distro. This library also had to include the antlr-runtime-3.0.jar. See: http://forum.springsource.org/showthread.php?t=65324 -Leo  I think you are picking up an incorrect version of asm.jar somewhere in the classpath you are using for your tests.  The java.lang.NoSuchMethodError always indicates that the version of a class that was on your compiler's classpath is different from the version of the class that is on your runtime classpath (had the method been missing at compile-time the compile would have failed.) In this case you had a different version of org.objectweb.asm.ClassWriter on your classpath at compile time than is on your runtime classpath. what do you mean by ""version is different"" ? is it missing method in the runtime class or whatever change in the runtime class ? I suppose that compiling something with a class that then gets a new static field (irrelevant thing) and is used in runtime doesn't throw this error right ? just to say it does not 'always' indicate that it's a different version but may also be that the same version is loaded twice. I had this problem the other day. To further narrow down where the problem comes from start your vm with `-class:verbose` which displays all classes loaded and from where  I would like to share my suggestion for anyone who comes across this JUnit Testing for Android; none of the above worked for me and this was a simple fix: In Eclipse>>project properties>>Android>> make sure that the project you are testing is referenced under Library. 'is library' can remain unchecked  Definitely you have different versions of your ClassWriter class at runtime than compile time."
806,A,"Why is this (trivial) unit test failing? This was taken nearly verbatim from IBM's Mastering Grails series. DateTagLib.groovy: class DateTagLib { def thisYear = { out << Calendar.getInstance().get(Calendar.YEAR) } } DateTagLibTests.groovy: class DateTagLibTests extends TagLibUnitTestCase { def dateTagLib protected void setUp() { super.setUp() dateTagLib = new DateTagLib() } void testThisYear() { String expected = Calendar.getInstance().get(Calendar.YEAR) assertEquals(""years do NOT match"" expected dateTagLib.thisYear()) } protected void tearDown() { super.tearDown() } } grails test-app DateTagLib output: ------------------------------------------------------- Running 1 unit test... Running test DateTagLibTests... testThisYear...FAILED Tests Completed in 359ms ... ------------------------------------------------------- Tests passed: 0 Tests failed: 1 ------------------------------------------------------- I tried matching the types (int/long/String) but I'm still banging my head against the wall. This test also fails: void testThisYear() { long expected = Calendar.getInstance().get(Calendar.YEAR) assertEquals(""years do NOT match"" expected (long) dateTagLib.thisYear()) } out << Calendar.getInstance().get(Calendar.YEAR) puts the result into out if you want to test this use def thisYear = { Calendar.getInstance().get(Calendar.YEAR) }  Try the following instead class DateTagLibTests extends TagLibUnitTestCase { void testThisYear() { String expected = Calendar.getInstance().get(Calendar.YEAR) tagLib.thisYear() assertEquals(""years do NOT match"" expected tagLib.out) } } Your original code has 2 problems: You should not instantiate DateTagLib explicitly. It is already available through a property of the test class named tagLib thisYear does not return the year value it writes it to out. Within a test you can access the content written to the output via tagLib.out Awesome thanks for the explanation! No problem BTW I don't know whether you're planning to use this taglib or if it's just for illustrative purposes but it seems fairly pointless to me. IMO it's almost as easy to use the following GSP code `${new Date()[Calendar.YEAR]}`"
807,A,"How to run test case in JUnit from command line ? I'm trying to run a JUnit test case from command line using this command: F:\>java org.junit.runner.JUnitCore org.junit4.9b2.junit.SimpleTest but I get this error: Exception in thread ""main"" java.lang.NoClassDefFoundError: org/junit/runner/JUnitCore Caused by: java.lang.ClassNotFoundException: org.junit.runner.JUnitCore at java.net.URLClassLoader$1.run(Unknown Source) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) Could not find the main class: org.junit.runner.JUnitCore. Program will exit. What is the problem? Obviously you need junit on the classpath :-) java -cp path/to/junit.jar:path/to/local/classes org.junit.runner.JUnitCore \ org.junit4.9b2.junit.SimpleTest (replace the : with ; on windows platforms) I already did .. this is the value of CLASSPATH .;C:\Program Files\Java\jre6\lib\ext\QTJava.zip;F:\junit\junit4.9b2;F:\junit\junit4.9b2\junit-4.9b2.jar;F:\junit\junit4.9b2\org\junit\samples How can i specify execute one method of the testcase class?thanks @janwen it doesn't seem like you can do that from the command line. But you should ask a separate question if you want to be sure. I know you can do it from Maven or Eclipse but not from the CLI thanks @SeanPatrickFloyd I am going to write shell script to do this. @janwen for a single test case see http://stackoverflow.com/questions/9288107/run-single-test-from-a-junit-class-using-command-line"
808,A,"How do I connect StaticListableBeanFactory with ClassPathXmlApplicationContext? In the setup of my test cases I have this code:  ApplicationContext context = new ClassPathXmlApplicationContext( ""spring/common.xml"" ); StaticListableBeanFactory testBeanFactory = new StaticListableBeanFactory(); How do I connect the two in such a way that tests can register beans in the testBeanFactory during setup and the rest of the application uses them instead of the ones defined in common.xml? Note: I need to mix a static (common.xml) and a dynamic configuration. I can't use XML for the latter because that would mean to write > 1000 XML files. You can use ConfigurableListableBeanFactory.registerSingleton() instead of StaticListableBeanFactory.addBean(): ApplicationContext context = new ClassPathXmlApplicationContext( ""spring/common.xml"" ); GenericApplicationContext child = new GenericApplicationContext(context); child.getBeanFactory().registerSingleton(""foo"" ...);  An alternative you might like to try is to have a Test.xml with the bean definitions that imports your common.xml: <import resource=""spring/common.xml""/> <bean id=""AnIdThatOverridesSomethingInCommon""/> You can only have one bean definition with a particular id - in the same file it's an XML validation error in different files Spring will override the definition. Edit: Just noticed that this is not suitable for your case - I'll leave it here for completeness. No downvote because of the edit :-) Thanks - this is actually what I use but I guess you wouldn't want to if you're trying to apply this to a load of existing tests."
809,A,Java Test Harness For JSON (de)serialiation I am wondering if anybody knows of some standard JSON unit tests that are used out there. I have a library that does some JSON serialization and deserialization to a specific data object which I would like to verify is correct. I have homespun several tests which all pass but I am suspicious there are many corner cases I missed. I assume that some library out there must have fairly thorough unit tests which I could modify. Does anybody know of any such tests? Download python so that you have the standard library source code. Their test code is in: Lib/json/tests  If you want to use a Java test framework for JSON try JSONassert with JUnit. It's very lightweight to use.
810,A,"How to unit test production routes in Apache Camel? Let's say I have my routes created in separate RouteBuilder class. It looks like: grab message from JMS queue do some transformation validation etc depending on validation results forward to specific JMS queue and save something in DB I'd like to unit test this route with no JMS broker and no DB. I know I can mock my Processor implementations but that's not enough. I don't want to change this route (let's suppose I got that class in jar file). As far as I know from Camel in Action (sec. 6.2.6) to be able to use mocks of endpoints and other stuff I need to change my route endpoint definitions (in book's example this is change of ""mina:tcp://miranda"" to ""mock:miranda"" etc). Is it possible to test the flow in complete isolation without changing route definitions? If I got my RouteBuilder as a separate class am I forced to somehow ""copy"" route definition and change it manually? Isn't it testing the wrong thing? I'm quite new to Camel and for me it'd be really cool to be able to have isolated unit test while deveoping routes. Just to be able to change something run small test observe result and so on. Assuming the RouteBuilder class has hardcoded endpoints then its a bit tougher to test. However if the RouteBuilder using the property placeholder for endpoint uris then you often will be able to use a different set of endpoint uris for unit tests. As explained in chapter 6 of the Camel book. If they are hardcoded then you can use the advice with feature in your unit test as shown here: http://camel.apache.org/advicewith.html In Camel 2.7 we made it possible to manipulate the route much easier so you can remove parts replace parts etc. Thats the weaving stuff that link talks about. For example to simulate sending a message to a database endpoint you can use that above and replace the to with another where you send it to a mock instead. In previous releases you can use the interceptSendToEndpoint trick which is also covered in the Camel book (section 6.3.3) Oh you can also replace components with mock component as shown on page 169. Now in Camel 2.8 onwards the mock component will no longer complain about uri parameters it doesnt know. That means its much easier to replace components with mocks on a per component level.  I have  <bean id=""properties"" class=""org.apache.camel.component.properties.PropertiesComponent""> <property name=""location"" value=""classpath:shop.properties""/> </bean> <route> <from uri=""direct://stock""/> <to uri=""{{stock.out}}""/> </route> in my spring file and then in the shop.properties on the test class path i have a stock.out=xxxx which is replaced at runtime so i can have to different routes one for runtime and one for test theres a better example in 6.1.6 unit testing in multiple environments"
811,A,Is classpath junit.jar and import org.junit statement in Eclipse independent? Are classpath junit.jar and import org.junit statement independent of each other in Eclipse? After adding junit.jar on Windows 7 (environment) classpath I am not able to benefit from importing org.junit.*; statement in Eclipse for Java. Eclipse informs that the import org.junit cannot be resolved. Is it normal behaviour? By using command line (cmd.exe) junit works fine: java org.junit.runner.JUnitCore org.junit.tests.AllTests However within Eclipse I cannot use Junit classes. The project build path is in charge in Eclipse. If junit.jar (of appropriate version) or the JUnit eclipse lib are on the build path you can import junit classes. If not not. The classpath environment variable is never a good idea.  Eclipse builds classpath to based upon what is called a 'build path' and invokes JVM with a -cp argument. JVM ignores CLASSPATH env variable if an explicit -cp jvm arg is passed to it so your Windows classpath setting is ignored. Solution: set up your project build path correctly ie. add your jars there.
812,A,Debugging maven junit tests with filtered resources? We are using filtered testResources in JUnit-tests that are usually executed by the maven surefire plugin. That is the pom contains a section  <build> <testResources> <testResource> <directory>src/test/resources</directory> <filtering>true</filtering> </testResource> </testResources> ... How can I run such JUnit-tests in the debugger? If I execute the tests in eclipse the tests fail since the test resources are not filtered. If the filtered test resources would be written somewhere into the target directory I could just use this as an additional source path - but this is not the case. If I try to run the maven build in eclipse with Debug As / maven test  the build does not stop in the breakpoints. Any other ideas? There are several options. First you can run the test from the command line specifying maven.surefire.debug. Per default surefire tests are run in a forked JVM which means that if you just debug the maven process you won't get any stops in the test breakpoints. That's probably what you are seeing now. See also http://maven.apache.org/plugins/maven-surefire-plugin/examples/debugging.html Othwerwise I would recommend to configure your project within the IDE as a maven project. If the project is configured as maven project the resource filtering will occur automatically prior to running the tests. That's at least how it works within Idea and I think Eclipse does the same with the right maven plugin installed. You can also run the maven build once from the command line and then manually add the target/test-classes directory to your IDE configuration. Works but is a little bit dodgy.  If I execute the tests in eclipse the tests fail since the test resources are not filtered. Use m2eclipse and resources will get filtered inside Eclipse.  I ran into a similar situation and have documented the complete process Debugging Junit test cases with maven
813,A,"How to run junit tests by category in maven Using junit 4.8 and the new @Category annotations is there a way to choose a subset of categories to run with maven's surefire plugin? For example I have: @Test public void a() { } @Category(SlowTests.class) @Test public void b() { } And I'd like to run all non-slow tests as in: (note that the -Dtest.categories was made up by me...). mvn test -Dtest.categories=!SlowTests // run non-slow tests mvn test -Dtest.categories=SlowTests // run only slow tests mvn test -Dtest.categories=SlowTestsFastTests // run only slow tests and fast tests mvn test // run all tests including non-categorized So the point is that I don't want to have to create test suites (maven just picks up all unit tests in the project which is very convenient) and I'd like maven to be able to pick the tests by category. I think I just made up the -Dtest.categories so I was wondering if there's a similar facility I can use? thanks See [Mixing testng and junit](http://stackoverflow.com/questions/9172820/mixing-testng-junit-with-maven-surefire-plugin-version-2-11-and-higher-for-integ) for more details. From maven-surefire-plugin version 2.11 categories are supported You can use mvn test -Dgroups=""com.myapp.FastTests com.myapp.SlowTests"" But ensure that you configure properly the maven surefire plugin <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.11</version> <dependencies> <dependency> <groupId>org.apache.maven.surefire</groupId> <artifactId>surefire-junit47</artifactId> <version>2.12.2</version> </dependency> </dependencies> </plugin> See docs in: https://maven.apache.org/surefire/maven-surefire-plugin/examples/junit.html  Based on this blog post - and simplifying - add this to your pom.xml: <profiles> <profile> <id>SlowTests</id> <properties> <testcase.groups>com.example.SlowTests</testcase.groups> </properties> </profile> <profile> <id>FastTests</id> <properties> <testcase.groups>com.example.FastTests</testcase.groups> </properties> </profile> </profiles> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.13</version> <dependencies> <dependency> <groupId>org.apache.maven.surefire</groupId> <artifactId>surefire-junit47</artifactId> <version>2.13</version> </dependency> </dependencies> <configuration> <groups>${testcase.groups}</groups> </configuration> </plugin> </plugins> </build> then at the command line mvn install -P SlowTests mvn install -P FastTests mvn install -P FastTestsSlowTests Version 2.13 of surefire plugin didn't work for me. I got this error: ""groups/excludedGroups require TestNG or JUnit48+ on project test classpath"" although I am using Junit 4.11. Downgrading surefire plugin to 2.12.2 resolved the error. This worked for me using version 2.17 of Surefire. I had to change the org.apache.maven.surefire:surefire-junit47:2.13 dependency to org.apache.maven.surefire:common-junit48:2.17.  I had a similar case where I want to run all test EXCEPT a given category (for instance because I have hundreds of legacy uncategorized tests and I can't / don't want to modify each of them) The maven surefire plugin allows to exclude categories for instance: <profiles> <profile> <id>NonSlowTests</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <excludedGroups>my.category.SlowTest</excludedGroups> </configuration> </plugin> </plugins> </build> </profile> </profiles>  Maven has since been updated and can use categories. An example from the Surefire documentation: <plugin> <artifactId>maven-surefire-plugin</artifactId> <version>2.11</version> <configuration> <groups>com.mycompany.SlowTests</groups> </configuration> </plugin> This will run any class with the annotation @Category(com.mycompany.SlowTests.class) Version 2.11 didn't work for me because Surefire kept ignoring the groups I specified. Upgrading to surefire plugin version 2.12.3 did the trick How does one do multiple configurations for different categories though? Ie the OP wanted to be able to specify a category on the command line but if the category is specified in the POM how do you specify one on the command line? Attention! Version 2.11 ignores groups! Provided example under http://maven.apache.org/surefire/maven-surefire-plugin/examples/junit.html#Using_JUnit_Categories is still incorrect!  Not exactly the same thing but using surefire plugin test classes can be chosen based on file name. You are not using Junit Categories though. An example for running just DAO tests. <executions> <execution> <id>test-dao</id> <phase>test</phase> <goals> <goal>test</goal> </goals> <configuration> <excludes> <exclude>none</exclude> </excludes> <includes> <include>**/com/proy/core/dao/**/*Test.java</include> </includes> </configuration> </execution> http://maven.apache.org/plugins/maven-surefire-plugin/examples/inclusion-exclusion.html  This answer is obsolete It looks like this is not supported see SUREFIRE-329 (even if this issue is/was not about JUnit 4.8 I'd recommend to vote/comment this one). So I'm afraid you'll have to rely on ""solutions"" like this one for now. Although at one time it was true THIS ANSWER IS OBSOLETE. See the below answer. suggest also deselcting this as the ""correct"" answer."
814,A,"How do I unit test a custom ant task? I am writing a custom ant task that extends Task. I am using the log() method in the task. What I want to do is use a unit test while deveoping the task but I don't know how to set up a context for the task to run in to initialise the task as if it were running in ant. This is the custom Task: public class CopyAndSetPropertiesForFiles extends Task { public void execute() throws BuildException { log(""CopyAndSetPropertiesForFiles begin execute()""); log(""CopyAndSetPropertiesForFiles end execute()""); } } This is the unit test code: CopyAndSetPropertiesForFiles task = new CopyAndSetPropertiesForFiles(); task.execute(); When the code is run as a test it gives a NullPointerException when it calls log. java.lang.NullPointerException at org.apache.tools.ant.Task.log(Task.java:346) at org.apache.tools.ant.Task.log(Task.java:334) at uk.co.tbp.ant.custom.CopyAndSetPropertiesForFiles.execute(CopyAndSetPropertiesForFiles.java:40) at uk.co.tbp.ant.custom.test.TestCopyAndSetPropertiesForFiles.testCopyAndSetPropertiesForFiles(TestCopyAndSetPropertiesForFiles.java:22) Does anybody know a way to provide a context or stubs or something similar to the task? Thanks Rob. Accepted answer from Abarax. I was able to call task.setProject(new Project()); The code now executes OK (except no logging appears in th console - at least I can exercise the code :-) ). Looking at the Ant source code these are the two relevent classes: ProjectComponent and Task You are calling the log method from Task: public void log(String msg) { log(msg Project.MSG_INFO); } Which calls: public void log(String msg int msgLevel) { if (getProject() != null) { getProject().log(this msg msgLevel); } else { super.log(msg msgLevel); } } Since you do not have project set it will call ""super.log(msg msgLevel)"" public void log(String msg int msgLevel) { if (getProject() != null) { getProject().log(msg msgLevel); } else { // 'reasonable' default if the component is used without // a Project ( for example as a standalone Bean ). // Most ant components can be used this way. if (msgLevel <= Project.MSG_INFO) { System.err.println(msg); } } } It looks like this may be your problem. Your task needs a project context.  Ant has a handy class called BuildFileTest that extends the JUnit TestCase class. You can use it to test the behaviour of individual targets in a build file. Using this would take care of all the annoying context. There's a Test The Task chapter in the Apache Ant Writing Tasks Tutorial that describes this.  Or better yet decouple the task object itself from the logic (lets call it TaskImpl) inside the task - so that you can pass in your own dependencies (e.g. the logger). Then instead of testing the task object you test TaskImpl -> which you can pass in the logger and any other weird bits and pieces it might need to do its job. Then unit testing is a matter of mocking the dependencies. Definitely do this! TDD is a design methodology when Ant provides the design you can't do much - especially if you inherit from a framework provided superclass not under your control. You want to test *only* the functionality you provide *not* Ants implementation of Task. Voting up!"
815,A,How do you change the layout of JUnit Reports in Hudson? So I'm setting up Hudson right now and couldn't be more pleased. However I need to display a table in the test results page as opposed to the graph it provides. Does anyone know how I would go about doing this? I guess you'd want to make a custom plugin out of the existing Junit functionality. You can pretty much copy the java files from: hudson/main/core/src/main/java/hudson/tasks/junit/ and resource files (jellys) from: hudson/main/core/src/main/resources/hudson/tasks/junit/ to your new plugin (unless you don't want to fork Hudson source). It seems that the files you'd like to fiddle around would be hudson/main/core/src/main/java/hudson/tasks/junit/History.java (where the graphs are created) and hudson/main/core/src/main/resources/hudson/tasks/junit/History/index.jelly (where the created graphs are shown). From History-class you can pretty easily get a grip on how to fiddle around with TestObjects.  What do you want to display in the table - just the results from the latest build or the same trend data that the default graph displays ? Either way I think you'd need to modify the Hudson code to do what you want - see the Hudson Wiki. I'm trying to replace the graph with a simple table. I know I'm going to have to modify the Hudson source I'm just not sure where I'd have to modify it.
816,A,"Does Maven surefire plugin run tests using multiple threads? I'm wondering if the Maven surefire plugin either runs tests multi-threaded by default (and if so can the number of threads be controlled? ) or if it runs tests from the Test classes in a random order or predictable order or if the order can dictated by some means. I haven't verified this yet (I'll do so tomorrow just looking for some heads up guidance and verification at this point) but it looks as if my various JUnit Test classes are getting the tests run in some intermixed order. Which makes it a real pain to orchestrate the creating of the test resources (which are quite hefty in my case). Its probably a classic problem I run my suite with the Eclipse JUnit runner and everything runs very linear and plays nice. I go to Maven cmd line and things seems to be stepping all over each other. By default Maven runs your tests in a separate (""forked"") process nothing more (this can be controlled using the forkMode optional parameter). If you are using TestNG or Junit 4.7+ (since SUREFIRE-555) it is possible to run tests in parallel (see the parallel and the threadCount optional parameters) but that's not a default. Now while I'm not sure if the surefire plugin behaves the same as JUnit it is possible to get some control by manually creating a TestSuite and specify the order in which tests are executed: TestSuite suite= new TestSuite(); suite.addTest(new MathTest(""testAdd"")); suite.addTest(new MathTest(""testDivideByZero"")); You are however strongly advised never to depend upon test execution order unit tests really should be indeed independent. P.S.: Just in case there is also this request SUREFIRE-321 (to run tests in alphabetical order) that you might want to vote for. Hi aware of avoiding interactive tests trying to get there... Voted for both issues. Looks like 555 is now part of release 2.5 http://repo1.maven.org/maven2/org/apache/maven/plugins/maven-surefire-plugin/2.5/. please make use of surefire plugin version 2.4.2 or above in case you dont wanna face problems related to OOM  JUnit runs tests in the order in which they appear in the .java file (not alphabetically). Maven-surefire runs them in a different order but not predictably (as far as I can tell). Ideally tests would be independent of one another but singletons and static context can complicate things. A helpful way to get new static contexts between executions of separate TestCase's (but not individual tests) is to set the forkMode variable in your pom.xml.. <forkMode>always</forkMode>  First of all your unit tests should be independent of each other. This is because the order of execution is not guaranteed even by JUnit so each test should set up and tear down its context (aka test fixture) independent of what happens before or after. The order of execution is definitely not random though in JUnit it tends to be the same (I would guess alphabetical order) but you should not build on it - it can change anytime and apparently in Surefire the order is different. Here is a good link on why interacting tests are not a good idea. Hi aware of avoiding interactive tests trying to get there... thanks for the advice and the link."
817,A,"How do I test local inner class methods in java? In many application I often have algorithms which make use of dedicated sub-algorithms (or simply well defined pieces of code). Till now when I wrote the main algorithm i created a private method for each sub-algorithm like in the example below (OldStyle): public class OldStyle { public int mainAlg() { int x = subAlg01(); int y = subAlg02(); int z = x * y; return z; } private int subAlg01() { return 3; } private int subAlg02() { return 5; } } This worked fine but I didn't like having a proliferation of methods (subAlg01 and subAlg02) which even if private were only used by one method (mainAlg). Recently I dicovered the use of local inner classes and now my example is (NewStyle): public class NewStyle { public int mainAlg() { class Nested { public int subAlg01() { return 3; } public int subAlg02() { return 5; } } Nested n = new Nested(); int x = n.subAlg01(); int y = n.subAlg02(); int z = x * y; return z; } } I like it very much but now I have the following problem: how do I test subAlg01 and subAlg02 using JUnit? By the way: I'm using eclipse. Thanks for you help. Edit: I try to explain better: I have let's say a sorting algorithm and I want to test it to be sure it runs as expected. This sorting algorithms is used by only method m of class X. I could make it a private method of class X but class X usually has nothing to do with sorting so why ""spoil"" class X with the sorting method? So I put it inside method m. Some time later I want to improve my sorting algorithm (I make it faster) but I want to be sure that it's behavior is as expected so I want to re-test it with the original tests. That's what I want to do maybe there is no solution I hope someone may help me. Edit after answer choice. I selected Rodney answer because his solution is the one I adopted: a standalone helper class helps me (it's a helper!) to have a clear view of what are the sub methods and it also gives me the ability to test them. I removed the Eclipse tag as this problem is not specific to it. You should avoid over complicating your code just because you don't like having a proliferation of methods In your case you can test the methods if they are methods of the outer class or if you absolutely need the inner class then place it outside the mainAlg() method so it is visible globally. public class NewStyle { public int mainAlg() { Nested n = new Nested(); int x = n.subAlg01(); int y = n.subAlg02(); int z = x * y; return z; } class Nested { public int subAlg01() { return 3; } public int subAlg02() { return 5; } } } this can then be called using new NewStyle().new Nested().subAlg01(); Thank for the reply. I also tried this solution but I like it less than mine since looking at the class I still don't have a clear view that the subAlgs are only part of the main algorithm and are not used by anyone else. Nevertheless it allows testing. new NewStyle().new Nested().subAlg01(); Are you sure? I've NEVER seen this... /ponder  Filippo I understand your frustration with the problem and with some of the answers. When I first started using JUnit many years ago I too wanted to test private code and I thought it silly of the gurus to say it was a bad idea. Turns out they were right (surprise!) but only after writing quite a few tests did I understand why. You might need to go through the same process but you will eventually come to the same conclusion ;-) Anyway in your situation I would make Nested into a proper standalone class possibly in a separate package to make obvious that it's a helper classes. Then I would write tests for it directly independent of any other tests. Then I'd write tests for NewStyle and focus only on the behaviour of NewStyle. (Quite probably I would also inject Nested into NewStyle rather than instantiating it within NewStyle -- i.e. make it an argument to NewStyle's constructor. Then when I write tests for NewStyle in the test I'd pass in an instance of Nested and carry on. If I felt particularly tricky I'd create an interface out of Nested and create a second implementation and test NewStyle with that too.) +1 because your solution is the same I have currently adopted (standalone helper class)  The issue comes when your inner class's behavior happens to be a core part of what a method does: Say that you have a method is supposed to pass a runnable to a third object which happens to be an inner class that really shouldn't be a separate entity: In this case proper unit testing pretty much requires exercising said inner class by mocking that third object and using argument capture to test it does what it should. This kinds of issues are very common in languages that have more support for functional programming where testing lambdas passed to third parties is pretty much a requirement. But as it was said before if your inner class is never used by an outside party then it's just an implementation detail and testing it separately is just not useful.  Actually you can test your local inner class. But it needs to implement an interface. This way you can create an instance of the local class via reflection and cast it to it's interface. Once you have the interface type you can test your implementing code without problems: The interface: public interface Algorithm { int subAlg01(); int subAlg02(); } The class: public class NewStyle { public int mainAlg() { class Nested implements Algorithm { public int subAlg01() { return 3; } public int subAlg02() { return 5; } } Nested n = new Nested(); int x = n.subAlg01(); int y = n.subAlg02(); int z = x * y; return z; } } The test: public class NewStyleTest { @Test public void testLocal() throws ClassNotFoundException NoSuchMethodException SecurityException InstantiationException IllegalAccessException IllegalArgumentException InvocationTargetException { Class<?> forName = Class.forName(""NewStyle$1Nested""); Constructor<?> declaredConstructor = forName .getDeclaredConstructor(NewStyle.class); declaredConstructor.setAccessible(true); Algorithm algorithm = (Algorithm) declaredConstructor .newInstance(new NewStyle()); assertEquals(algorithm.subAlg01() 3); assertEquals(algorithm.subAlg02() 5); } }  You should only test the public interface of classes not private members or private inner classes. Private members are meant to be implementation details used only by public methods of the class (directly or indirectly). So you can unit test these indirectly via their caller methods. If you feel you don't have enough granularity in those unit tests or that you can't sense (some of) the results you are interested in this probably signs a problem with your design: the class may be too big trying to do too much thus some of its functionality may need to be extracted into a separate class where it can then be unit tested directly. In the current example if the inner class itself contains a lot of code you may simply turn it into a top-level class then you can unit test its methods directly. (Btw your inner class should be static if it doesn't need to refer to the enclosing class instance.) Hi thank for the reply. I have already read many answers like your but it's not very clear to me the meaning. Please can you help me understand better? I have a piece of code which is used only once but I need to verify if it does what it's expected to do. Ho do I accomplish that? Testing seemed to me a good choice. @Filippo see my update. Testing is indeed a good choice - write tests that cover all the different usage scenarios you can come up with. A code coverage tool helps you spot untested code parts. @Péter about your suggestion on static: inner classes may be static local inner class no @Péter I can't put the static modifier in a local-inner-class (i.e. inside a method) Local classes are quite different from inner classes so you can't declare them static. Local classes are like anonymous classes except that they have a name which you can use multiple times inside the method. http://docstore.mik.ua/orelly/java-ent/jnut/ch03_11.htm @Péter if I turn the inner class into a top level class I have created a new class just for testing. Is it worthwhile? (remember I didn't like having private methods so I don't want to have an extra class!) @Filippo oops I didn't notice that was a local class instead of an inner class - my bad :-( @Filippo testability (or the lack thereof) is one of the most important properties of a design. A design which can't be unit tested properly is not a good design (usually - there are exceptions as always but these are rare). Of course it is up to you how you define ""properly"" :-) I tried to give good rules of thumb in my answer. In OOP it is desirable to have many smaller classes instead of a few big ones but again ""small"" and ""big"" is relative. Exactly as Peter saied the problem is that if you have a submethod that shorts something you should extract it to other class. Remember all the classes should have one and only one defined purpose. And thanks to the test is that you are realizing that you have a class that does two different things.  You cannot reach these classes from the outside so junit cannot test Them. You must have things public to test Them. Do you know if JMockit (or similar frameworks) can test methods within local inner classes? This is not correct. Mock frameworks such as JMockit can handle private inner classes (as well as private methods and members) just fine. JMockit most likely does naughty things with reflection then which junit doesn't.  Hm I know that languages like Groovy are often used to do unit testing on Java and are able to access private fields and methods transparently.....but I'm not sure about nested classes. I can understand why you might want to do this sort of thing but I kind of take the same position I take with testing getters and setters they should be tested as a side effect of testing your public methods and if they don't get tested that way then why are they there to begin with?"
818,A,"How do I run the test cases in a non-web-project I have imported into my Spring web project? This package has a class - HowToUseGeocell.java - with test cases defined. I have imported this entire project into a larger Java web project (Spring 3) in Eclipse controlled by a Maven task (gae:run). But I don't know how to run these test cases from inside the larger project. Could someone point me in the right direction please? Thanks. I have imported this entire project into a larger Java web project If this means that the sources and tests of the ""non web project"" have been copied respectively in the src/main/java and src/test/java trees of the ""web project"" then just run: mvn test If the import is about something else please explain what it means and what the project structure looks like."
819,A,Any way to get Eclipse to always use common VM args for running JUnit tests? Anyone know how to get Eclipse to use a set of default VM args? I'm getting fed up of constantly having to specify these manually every time I run a specific package class or method within my otherwise smoothly-running test suite. Using JUnit 4 Eclipse 3.5. Rationale: some tests are integration tests verifying operations with large input sets so it's tedious to have to keep putting in: -Xms256m -Xmx512m If there's something e.g. in Eclipse prefs where I can specify this once and for all I'd be very grateful! If you run your tests in a separate JRE (the default I think) then you can go to Installed JREs and specify default VM arguments for it there as in the screenshot below (the orange buttons indicate what I clicked to access the dialog below it). If you run your tests in the same JRE as the workspace then the solution is to edit your eclipse.ini file and put the arguments there. Looks promising I'll give it a go Yep working nicely thanks :-)  you can try in eclipse.ini file http://wiki.eclipse.org/Eclipse.ini
820,A,"Maven EAR module and EJB dependencies tests We are building our EAR & EJB projects with maven. It will build all the EJB projects and then they are used as the dependency for EAR so they are packed into the EAR file eventually. The problem is that each EJB project has junit tests that check the EJB. For now these tests are not very useful because they try to connect to application server (jboss) and execute methods from EJB interface. Is there any way I can build the EJBs build and deploy the EAR and then run all the tests from all of the EJBs against the application server ? For now I'm simulating AP in tests by initiation EJB-Implementation classes and manually ""injecting"" injections (someEJBImpl.em = EntityManager....) which is very annoying because we have a huge dependencies between them and I have to handle transactions by myself. Is there any other way of running EJB tests against real AP ? May be deploy EAR after each EJB module with subset of EJB modules that were already built ? But how ? May be set to run maven tests of all EJB modules as part of EAR tests ? How to do this ? For JBoss you could try the Maven Cargo plugin. I am currently testing it with JBoss 5.1 and still working on it: http://stackoverflow.com/questions/1707740/where-can-i-find-a-complete-maven-cargo-plugin-example-for-ejb-tests  This is not a simple problem and there's no easy answer. Hopefully these pointers will help. I think your best strategy is to separate your tests into the genuine unit tests - those that can run in isolation without a container and move the tests that require the container into integration tests. You can use Ejb3unit to maximise the tests that don't require a container to run. It helps mock some of the complicated dependencies. Ejb3unit has a Maven plugin see the documentation for details connecting to their Maven repository. Other mocking frameworks such as JMock can also help. You can mock classes as well as interfaces if you use a ClassImposteriser. For those tests that do need an EJB container you can configure these to run as integration tests it may make sense to move them to a separate project depending on the relationships between your EJB projects. It is possible to launch an embedded Jetty instance in your JUnit tests and programmatically add servlets to it. Of course Jetty isn't an EJB container You'll need an EJB container like OpenEJB. To configure OpenEJB into Jetty use a configuration like this: <plugin> <groupId>org.mortbay.jetty</groupId> <artifactId>maven-jetty-plugin</artifactId> <configuration> <scanIntervalSeconds>5</scanIntervalSeconds> <contextPath>/example</contextPath> <systemProperties> <systemProperty> <name>java.naming.factory.initial</name> <value>org.apache.openejb.client.LocalInitialContextFactory</value> </systemProperty> <systemProperty> <name>java.naming.factory.url.pkgs</name> <value>org.mortbay.naming</value> </systemProperty> </systemProperties> </configuration> </plugin> The dependency declarations for OpenEJB would be: <dependency> <groupId>org.apache.openejb</groupId> <artifactId>openejb-core</artifactId> <version>3.1</version> <scope>test</scope> </dependency> You can also use Selenium to help with the functional tests (assuming you got this far) here's a guide using Selenium Jetty and OpenEJB to do so."
821,A,"JUnit and Android? Is anyone using Junit and Android? Or is that just a worthy hope? Is there a tutorial anywhere? Android has great support for JUnit 3 From Testing Fundamentals on Android Developers: The Android testing framework an integral part of the development environment provides an architecture and powerful tools that help you test every aspect of your application at every level from unit to framework. The testing framework has these key features: Android test suites are based on JUnit. You can use plain JUnit to test a class that doesn't call the Android API or Android's JUnit extensions to test Android components. If you're new to Android testing you can start with general-purpose test case classes such as AndroidTestCase and then go on to use more sophisticated classes. The Android JUnit extensions provide component-specific test case classes. These classes provide helper methods for creating mock objects and methods that help you control the lifecycle of a component. Test suites are contained in test packages that are similar to main application packages so you don't need to learn a new set of tools or techniques for designing and building tests. The SDK tools for building and tests are available in Eclipse with ADT and also in command-line form for use with other IDES. These tools get information from the project of the application under test and use this information to automatically create the build files manifest file and directory structure for the test package. The SDK also provides monkeyrunner an API testing devices with Python programs and UI/Application Exerciser Monkey a command-line tool for stress-testing UIs by sending pseudo-random events to a device. This document describes the fundamentals of the Android testing framework including the structure of tests the APIs that you use to develop tests and the tools that you use to run tests and view results. The document assumes you have a basic knowledge of Android application programming and JUnit testing methodology. I'd love to hear some developers that are actually use this about their experience  ""Note that the Android testing API supports JUnit 3 code style but not JUnit 4."" (Source) If you want to use JUnit4 or have existing JUnit4 tests you can use JUnit4Android.  I have been using Roboelectric which is awesome because it does not launch the simulator making the run time for the tests very very quick. A sample project is provided as an example and can be found here at github  You can also use Robotium to drive the UI from within JUnit for more functional style testing."
822,A,"Unit Tests Architecture Question So I've started to layout unit tests for the following bit of code: public interface MyInterface { void MyInterfaceMethod1(); void MyInterfaceMethod2(); } public class MyImplementation1 implements MyInterface { void MyInterfaceMethod1() { // do something } void MyInterfaceMethod2() { // do something else } void SubRoutineP() { // other functionality specific to this implementation } } public class MyImplementation2 implements MyInterface { void MyInterfaceMethod1() { // do a 3rd thing } void MyInterfaceMethod2() { // do something completely different } void SubRoutineQ() { // other functionality specific to this implementation } } with several implementations and the expectation of more to come. My initial thought was to save myself time re-writing unit tests with something like this: public abstract class MyInterfaceTester { protected MyInterface m_object; @Setup public void setUp() { m_object = getTestedImplementation(); } public abstract MyInterface getTestedImplementation(); @Test public void testMyInterfaceMethod1() { // use m_object to run tests } @Test public void testMyInterfaceMethod2() { // use m_object to run tests } } which I could then subclass easily to test the implementation specific additional methods like so: public class MyImplementation1Tester extends MyInterfaceTester { public MyInterface getTestedImplementation() { return new MyImplementation1(); } @Test public void testSubRoutineP() { // use m_object to run tests } } and likewise for implmentation 2 onwards. So my question really is: is there any reason not to do this? JUnit seems to like it just fine and it serves my needs but I haven't really seen anything like it in any of the unit testing books and examples I've been reading. Is there some best practice I'm unwittingly violating? Am I setting myself up for heartache down the road? Is there simply a much better way out there I haven't considered? Thanks for any help. If you really are doing the same setup and tear down in each test class then what you're doing is fine but I find that in practice this is almost never the case. In fact most of the time having a setup method which instantiates test data as you have here is not even what you want. Instead within a test class you setup any infrastructure and each test method sets up its own instance of an object to test some aspect of it. True moving the getTestedImplementation() call to the first line of each test and putting the result in a local might be what I want down the line. Probably safer to start with a new object. Right now though there is no state in the implementation classes so they should only need to be created once.  While I support SLott 100% I would also consider JUnit parametrized tests instead of test class hierarchy for this: @RunWith(Parameterized.class) public class MyInterfaceTester { private MyInterface m_object; public void MyInterfaceTester(MyInterface object) { m_object = object; } @Parameters public static Collection<Object[]> data() { List<Object[]> list = new ArrayList<Object[]>(); list.add(new Object[]{new MyImplementation1()}); list.add(new Object[]{new MyImplementation2()}); return list; } @Test public void testMyInterfaceMethod1() { // use m_object to run tests } @Test public void testMyInterfaceMethod2() { // use m_object to run tests } } No need in test class hierarchy: just add new implementation by adding another list element in data method. Didn't know about this feature thanks for sharing. More info and a working example here: http://www.devx.com/Java/Article/31983/0/page/3 for anyone else interested.  is there any reason not to do this? No. Do it. Tests are classes for exactly this reason. I haven't really seen anything like it in any of the unit testing books and examples I've been reading. Keep reading. Introductions don't cover this. Is there some best practice I'm unwittingly violating? No. Am I setting myself up for heartache down the road? No. Some folks get nervous about ""brittle tests"". You can find some questions here looking for ways to make it so a change to the software doesn't also lead to changes to the tests. In the long run trying to create ""robust"" tests are silly. You want tests written so that every small change to the visible interface level of the software requires test rewriting. You want tests so that invisible internal changes do not require test rewriting. Use of classes and subclasses is orthogonal to those considerations. Is there simply a much better way out there I haven't considered? No. Object-orientation is the point. Tests are a class for exactly this reason."
823,A,"JUnit produces strange AssertionFailedError I am currently using JUnit 4.4 and Java 1.6.x. And after a recent code fix we started getting this AssertionFailedError in my JUnit tests on the method: UtilityTest.testParseDate(4t): Mon Jan 15 09:26:07 PST 2001 expected: ""Mon Jan 15 09:26:07 PST 2001"" but was: ""Mon Jan 15 09:26:07 PST 2001"" junit.framework.AssertionFailedError: UtilityTest.testParseDate(4t): Mon Jan 15 09:26:07 PST 2001 expected: but was: at UtilityTest.testParseDate(Unknown Source) As you can see the expected and actual appear identical and after several code inspections we can find no obvious error in the code. Test runs with actual data have also produced correct (expected) results. Has anyone seen this behavior before in JUnit and if so did you find the cause and/or a fix? I have seen the same thing in previous versions of Java and JUnit myself: always somewhat random when it occurs and usually the only fix ""that worked"" was to retype the chunk of code in from scratch. Weird yet that was tne only way to remove this error. I'm trying to find out something more ""concrete"" in the behavior this time. Thanks -Richard The test code is: Calendar cal = Calendar.getInstance(); Date today = new Date(); cal.set(2001 0 15 9 26 07); // Jan 15 2001 09:26:07 // format 4 = ddd mmm dd hh:mm:ss TTT yyyy (with gettime) assertEquals(""UtilityTest.testParseDate(4t): Mon Jan 15 09:26:07 PST 2001"" cal.getTime() Utility.parseDate(""Mon Jan 15 09:26:07 PST 2001"" today true)); Here's what parseDate looks like (just the method signature as the code was long): public static Date parseDate(String date Date today boolean gettime) { I think you may have it though - even though it does not DISPLAY the milliseconds they would be different. That would probably explain the apparent ""randomness"" of the test case passing and failing. Sometimes the instructions will happen in less than a millisecond in which case your test will work.  Can you post the code for UtilityTest.testParseDate()? Are you using assertEquals() on the date values or are you comparing them in another fashion? If so can you assert that the millisecond timestamps are equal instead of the dates themselves? Thanks for poking my brain to think about the milliseconds!  That was it - the milliseconds were off. A judicious application of cal.clear() fixed the problem."
824,A,"Testing Tapestry pages and components with JUnit I usually try to minimize testing with Selenium and maximize the usage of plain old back-end testing (JUnit mocking). With Tapestry I am finding it hard to test pages and components in the latter way due to the ""magic"" that occurs with the callback functions. Have you been able to solve this? Or are you just using Selenium for the whole web layer (pages components)? According to the Tapestry documentation using PageTester is the appropriate way to do unit testing of Pages and Components : http://tapestry.apache.org/tapestry5/guide/unit-testing-pages.html But this seems similar to HtmlUnit style web testing as the interaction happens through a web browser like interface and not through the interface of the Page or Component. Edit I just tried a simple unit test for pages and it works quite well : public class FooPageTest extends AbstractServiceTest{ @Autobuild @Inject private FooPage fooPage; @Test public void setupRender(){ fooPage.setupRender(); } } AbstractServiceTest provides a test runner which provides the Tapestry dependency injection to the unit test class. With Autobuild you get the @Inject dependencies of the FooPage satisfied and for the component injections and @Property annotated elements you will need to figure out something else."
825,A,GWT Junit ant warning: multiple versions of ant detected in path for junit Noob question: still learning GWT and I just set up my project to use GWTTestCase by running webAppCreator -junit. It set up everything nicely and I'm able to write and run test cases but invoking ant test.dev gives me this warning: [junit] WARNING: multiple versions of ant detected in path for junit [junit] jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class [junit] and jar:file:/home/webbtra/proj/myProject/trunk/lib/gwt-dev.jar!/org/apache/tools/ant/Project.class It doesn't seem to be breaking anything so I don't want to spend a ton of time on it. Just figured I'd post here to see if anyone has some insight on this. Thanks. -tjw I guess it is a bug. Your link requires login... can you copy/paste? @Travis I can't find right now but you may be interested in this link: http://stackoverflow.com/questions/2805574/yet-another-ant-junit-classpath-problem
826,A,"Include NUnit in my Open-Source Project Download? I'm considering two possibilities: include NUnit with the source code of an open-source project - to make it very easy for the potential contributors to run automated tests right away. I feel it is important to promote the ""tests-first"" culture in this project (or at least make it clear to everyone that tests matter). distribute the source code without it but then I'd have to describe in the readme how to make the tests run and include a link to the NUnit download page. The question: Is #1 legal? Which of the two options is normally preferred? I'd like to hear from those familiar with the license legalese and those familiar with the common practices. As far as the legality goes I'm not a lawyer and you should talk to one about it. Include your unit tests in your open source project. You do not have to redistribute the [n/j]Unit runtime libraries when you publish. Let me rephrase that. DO NOT redistribute the j/nUnit run times with your project. You can keep the unit tests in your project. The only time you would want to redistribute the unit frameworks is if the version was disappearing and you couldn't use a newer version. +1 I agree. Another good reason is that even if it's legal to bundle NUnit with your open source project there's a high likelihood that NUnit will change over time so you will actually be doing your users a disservice by giving them a cruddy older version instead of the latest and greatest. Just bundle in your unit tests and then include a link to where they can download NUnit. the question is not about ""bundling"" but about checking it in into a source code repository. This is aimed at developers/contributors rather than users  A lot of projects do this with external libraries so anyone who checks it out immediately has all the binaries in case they need additional libraries. I have seen some projects that include a full binary copy of the tools they require so you have the exact version the app needs plus another folder that just has the exact dlls or whatever the project needs. Do you have examples of such projects? I'd like to take a quick look at their source code. Thanks!  I'll try to answer the question myself. First of all it's important to separate our open-source product's binary distribution (aimed at users) from its source-code repository (which is for developers/contributors). The question really concerns the latter. I've looked at two open-source products as examples. NUnit binary: has a dependency on fit (included with license) source: has dependencies on NAnt (download it yourself) and log4net (included without a license) Moq binary: no dependencies source includes the following external dependencies: Silverlight testing tools - included without a licence Microsoft .NET framework reference assemblies - no license Castle - license included (Castle project uses the Apache license and based on my reading of it that's a requirement) So inclusion of third-party development/build/testing tools with the source code (not with production binaries) seems to be a common practice and the tools can be included with or without a license notice depending on the type of license and its requirements. You can include the license just in case."
827,A,"Another simple problem with integrating Junit and Ant I can't get my Junit tests to run from my build.xml script. I feel like I have tried everything. Here is my build.xml script.  <property file=""build.properties""/> <property name=""src.dir"" value=""src""/> <property name=""build.dir"" value=""classes""/> <property name=""web.dir"" value=""war""/> <property name=""test.dir"" value=""test""/> <path id=""build.classpath""> <fileset dir=""lib""> <include name=""*.jar""/> </fileset> <!-- servlet API classes: --> <fileset dir=""${appserver.lib}""> <include name=""servlet*.jar""/> </fileset> <pathelement path=""${build.dir}""/> <pathelement path=""${test.dir}""/> </path> <path id=""classpath.base""/> <path id=""classpath.test""> <pathelement location=""c:/ant/lib/junit.jar"" /> <pathelement location=""${build.dir}""/> <pathelement location=""${src.dir}""/> <pathelement location=""${test.dir}"" /> <path refid=""classpath.base"" /> </path> <target name=""build""> <mkdir dir=""${build.dir}""/> <javac destdir=""${build.dir}"" source=""1.5"" target=""1.5"" debug=""true"" deprecation=""false"" optimize=""false"" failonerror=""true""> <src path=""${src.dir}""/> <classpath refid=""build.classpath"" source=""1.5"" target=""1.5"" debug=""true"" deprecation=""false"" optimize=""false"" failonerror=""true""/> </javac> </target> <target name=""test""> <junit haltonfailure=""true""> <classpath refid=""classpath.test"" /> <classpath refid=""build.classpath""/> <formatter type=""brief"" usefile=""false"" /> <test name=""com.mmz.mvc.test.PracticeTest"" /> </junit> </target> I am getting the following error message from the console.  test: [junit] Testsuite: com.mmz.mvc.test.PracticeTest [junit] Tests run: 1 Failures: 0 Errors: 1 Time elapsed: 0 sec [junit] Null Test: Caused an ERROR [junit] com.mmz.mvc.test.PracticeTest [junit] java.lang.ClassNotFoundException: com.mmz.mvc.test.PracticeTest [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:248) [junit] at java.lang.Class.forName0(Native Method) [junit] at java.lang.Class.forName(Class.java:247) [junit] at org.eclipse.ant.internal.ui.antsupport.EclipseDefaultExecutor.executeTargets(EclipseDefaultExecutor.java:32) [junit] at org.eclipse.ant.internal.ui.antsupport.InternalAntRunner.run(InternalAntRunner.java:423) [junit] at org.eclipse.ant.internal.ui.antsupport.InternalAntRunner.main(InternalAntRunner.java:137) BUILD FAILED C:\Users\Eric\Documents\Java\mmz\WEB-INF\build.xml:44: Test com.mmz.mvc.test.PracticeTest failed I figured out how to get it to run my tests but now it is telling me that my test is successful even though it is supposed to fail I know it fails because I am testing assertTrue(""Example doesnt work""false); +1 for answering your own question"
828,A,"Waiting for all threads spawned by my code under test in JUnit test case How do I ensure in a JUnit test case that all the the threads spawned directly/indirectly by the method under test are done with there job so that I can assert the final result? @Test public void testMethod() { Result result=method();// may spawn multiple threads to set result.value Assert.assertTrue(result.getValue()==4); //should execute only after result.value is set to its final value. } I suggest you invoke the Thread#join on multiple threads instances in method()follow this wayall sub threads are completed.  The real question is how do you deal with this case in your non-test code? What does the API of method() guarantee to its callers about when result.value will be set? Bear that in mind strongly when writing tests - the purpose is to assert that the class and its methods behave as they advertise. Sometimes working out what the advertised interface is can be half of the challenge. In a situation like this I would strongly recommend that your Result object behave like a Future in that its get() method blocks until the result is available. (Alternatively give it a waitFor() method or similar). If your method doesn't provide any specific guarantees or blocking calls all you can really do in the test is to keep checking the value every x seconds in a loop putting a @Timeout on the test to ensure that the value is set in a ""reasonable"" time. But this is all a client would be able to do too so it's a very valid test; it highlights that the interface isn't very usable for clients and modifying it would be a nice idea. +1 your explanation is way better than mine :-)  One approach for dealing with this requires two steps: Block the main test thread while waiting for worker threads Have each worker thread signal the main thread when its work is completed unblocking the main thread when the last worker completes. Ideally you'll want the main thread to timeout and fail the test if all of the workers do not complete within the expected time frame. This is pretty straightforward using ConcurrentUnit: public class MyTest extends ConcurrentTestCase { @Test public void test() throws Throwable { int threads = 5; for (int i = 0; i < threads; i++) { new Thread(new Runnable() { public void run() { threadAssertEquals(1 1); // Perform assertions as needed resume(); } }).start(); } threadWait(1000 threads); // Wait for 5 resume calls } }"
829,A,"Direction with JUnit Testing I am trying to wrap my head around Junit testing and have read examples and what not but still finding it difficult to understand how and what to test. Below is the Class with methods that I am creating my test cases from (as well as my test case Class). import java.util.Iterator; /** * The Probability class understands the likelihood that something will happen. * <p> * (c) Copyright Fred George 2008. All right reserved. Adapted and used for * educational purposes with permission from Fred George. * </p> * * @author Fred George */ public class Probability { /** Value of receiver. */ private final double value; /** Cannot happen. */ private static final double IMPOSSIBLE_VALUE = 0.0; /** Will happen. */ private static final double CERTAIN_VALUE = 1.0; /** Instance that represents outcome that will happen. */ public static final Probability CERTAIN = new Probability(CERTAIN_VALUE); /** * Answer a new instance of the receiver with the specified value as the * likelihood that it occurs. * * @param valueAsFraction * value between 0.0 and 1.0 * @throws */ public Probability(final double valueAsFraction) { if (valueAsFraction < IMPOSSIBLE_VALUE || valueAsFraction > CERTAIN_VALUE) { throw new IllegalArgumentException(""Specified value of "" + valueAsFraction + "" is not between 0.0 and 1.0""); } value = valueAsFraction; } /** * Answer the liklihood that the receiver will occur and the specified other * Probability will occur. * * @return ""and"" of receiver and other Probability * @param other * Probability being and'ed to receiver */ public final Probability and(final Probability other) { return new Probability(this.value * other.value); } /** * Answer the value of the receiver as a scaled double between 0.0 * (impossible) to 1.0 (certain). * <p> * This method is modeled after those in Double Integer and the rest of * the wrapper classes. * * @return value of receiver as double between 0.0 and 1.0 */ public final double doubleValue() { return value; } /** * Answer true if the receiver has the same value as the other (assuming * other is a Probability). * * @return true if receiver's value equals other's value * @param other * Object (assumed to be Probability) to compare */ public final boolean equals(final Object other) { if (!(other instanceof Probability)) { return false; } return this.value == ((Probability) other).value; } /** * Answers with a hashcode for the receiver. * @return the hash */ public final int hashCode() { return (new Double(this.value)).hashCode(); } /** * Answer true if the combined likelihoods of the specified Collection of * Probabilities sums to certain (100%). * * @return true if combined likelihoods is 100% * @param probabilities * Collection of likelihoods to sum */ public static final boolean isTotalCertain(final java.util.Collection probabilities) { double sum = 0; for (Iterator i = probabilities.iterator(); i.hasNext();) { sum += ((Probability) i.next()).value; } return sum == CERTAIN_VALUE; } /** * Answer the liklihood that the receiver will not occur. * * @return ""not"" of receiver */ public final Probability not() { return new Probability(CERTAIN_VALUE - value); } /** * Answer the liklihood that the receiver will occur or the specified other * Probability will occur or both. * * @return ""or"" of receiver and other Probability * @param other * Probability being or'ed to receiver */ public final Probability or(final Probability other) { return this.not().and(other.not()).not(); // DeMorgan's Law } /** Multiplier from double to percentage. */ private static final int PERCENTAGE_MULTIPLIER = 100; /** * Answers a String representation of the receiver suitable for debugging. * * @return String representation of the receiver */ public final String toString() { int percentage = (int) (value * PERCENTAGE_MULTIPLIER); return percentage + ""%""; } } And here is what I've attempted for some of the test cases. I haven't tried them all but am stuck on the ""equals"" method. package edu.psu.ist.probability; import edu.psu.ist.decision.Decision; import junit.framework.TestCase; import junit.framework.*; public class ProbabilityTest extends TestCase { private Probability p1; private Probability p2; private Probability p3; private Decision d1; protected void setUp() { p1 = new Probability(.6); p2 = new Probability(.7); p3 = new Probability(.6); d1 = new Decision(""No decision made""); } public void testHashCode() { fail(""Not yet implemented""); } public void testProbability() { assertEquals(p1.doubleValue() .6); try{ p1 = p3; //get here bad fail(""Should raise an IllegalArgumentException""); }catch (IllegalArgumentException e){ //good! } } public void testAnd() { assertEquals((p1.and(p2)).doubleValue() .42); } public void testDoubleValue() { assertEquals(p1.doubleValue() .6); } public void testEqualsObject() { assertEquals(p1 p3); //assertEquals(p1 p2); assertTrue(!p1.equals(p2)); assertTrue(p1.equals(p3)); /*Probability p1 = new Probability (.7); Probability p2 = new Probability (.6); Decision d1 = new Decision(); boolean TRUE = p1.equals(p2); boolean FALSE = p1.equals(d1); try { p1.equals(p2); p1.equals(d1); p1.equals(null); } catch (NullPointerException ex){ // exception should be thrown } // assertEquals(""Return true if theses values are the same""p1.doubleValue() p2.doubleValue()); // assertEquals(""Return false if not equal"" p1.doubleValue() d1.equals(p1.doubleValue())); // assertNotSame(""Objects are not the same"" p1 d1); */ } public void testIsTotalCertain() { fail(""Not yet implemented""); } public void testNot() { fail(""Not yet implemented""); } public void testOr() { fail(""Not yet implemented""); } public void testToString() { fail(""Not yet implemented""); } } Maybe someone can shed some light that will help me understand this process more clearly. In your particular case the code seems straight forward for the most part. Try to focus on testing the behavior of the code. Here are some test scenarios for equals method. Pass in a non-Probability object String test = ""foo""; assertTrue(!p1.equals(test)); Should the test pass? Should the test expect an exception? Pass in null assertTrue(!p1.equals(null)); Should the test pass? Should the test expect an exception?  You've chosen a somewhat hairy first step comparing floating point numbers can be non-intuitive. You want to make sure you use the assertXXX methods with a delta: double x = 1.3; double y = 13.0 / 10.0; double acceptable_difference = 0.05; assertEquals(xy acceptable_difference); That should return true as you're unlikely to make your value match. In terms of writing your tests just think what you want to make sure of being careful to test the boundary conditions like if one Probability is 0. Speaking of floating point I bet you could find uses of not that get you below 0.0 if ever so slightly. That's something to take a look at."
830,A,Ant JUnit Task couldn't find junit/framework/TestCase.class I'm trying to get a JUnit 4.8.1 task to run in Ant 1.7.1. My IDE is Eclipse Helios. I've been banging my head against a brick wall for 2 days now and cannot figure this out. I'm sure from reading other posts its a classpath problem but I can't see where I'm going wrong. My JUnit4 test suite is defined as follows: package mypackage.tests; import org.junit.runner.RunWith; import org.junit.runners.Suite; /** * JUnit 4 Test Suite for the entire <code>mypackage</code> * package */ @RunWith(Suite.class) @Suite.SuiteClasses({ mypackage.tests.controller.AllTests.class }) public class AllTests { } ...simple enough but the test fails with a ClassNotFoundException java.lang.ClassNotFoundException: mypackage.tests.AllTests at java.net.URLClassLoader$1.run(Unknown Source) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Unknown Source) When I switch on the debug flag I get the following trace from Ant: (...) [junit] Couldn't find junit/framework/TestCase.class [junit] Found C:\eclipse\plugins\org.apache.ant_1.7.1.v20100518-1145\lib\ant-launcher.jar [junit] Found C:\eclipse\plugins\org.apache.ant_1.7.1.v20100518-1145\lib\ant.jar [junit] Found C:\eclipse\plugins\org.apache.ant_1.7.1.v20100518-1145\lib\ant-junit.jar fileset: Setup scanner in dir C:\eclipse\plugins with patternSet{ includes: [org.junit_4.8.1.v4_8_1_v20100427-1100/junit.jar] excludes: [] } Finding class junit.framework.Test Loaded from C:\eclipse\plugins\org.junit_4.8.1.v4_8_1_v20100427-1100\junit.jar junit/framework/Test.class (...) Clearly the JUnit jar is on the classpath and other classes such as Test.class are being picked up so why am I getting the 'couldn't find TestClass.class' message? I tried adding the JUnit.jar to the ant classpath in the Eclipse GUI and this has the following effect: (...) [junit] Found C:\eclipse\plugins\org.junit_4.8.1.v4_8_1_v20100427-1100\junit.jar [junit] Found C:\eclipse\plugins\org.apache.ant_1.7.1.v20100518-1145\lib\ant-launcher.jar [junit] Found C:\eclipse\plugins\org.apache.ant_1.7.1.v20100518-1145\lib\ant.jar [junit] Found C:\eclipse\plugins\org.apache.ant_1.7.1.v20100518-1145\lib\ant-junit.jar (...) [junit] WARNING: multiple versions of ant detected in path for junit [junit] jar:file:C:/eclipse/plugins/org.apache.ant_1.7.1.v20100518-1145/lib/ant.jar!/org/apache/tools/ant/Project.class [junit] and jar:file:/C:/eclipse/plugins/org.apache.ant_1.7.1.v20100518-1145/lib/ant.jar!/org/apache/tools/ant/Project.class (...) Can someone please please please help me to get this working! Oh I almost forgot... the test suite runs fine from within Eclipse. Thanks in advance! The error is caused by the fact that your test classes cannot be loaded. Did you put the package mypackage.tests.controller (i.e. the folder(s) containing the class files or the jar archive) on the classpath of your junit call? This can be done using a nested classpath structure. (http://ant.apache.org/manual/Tasks/junit.html). OMG I could weep. Two days for THAT!? This was precisely the problem. I couldn't figure out how to add the class files only so I included a task and added the jar file to the classpath.
831,A,"Selenium RC - disabling browser cookie Is it possible to disable a browser cookie using selenium RC specifically? If so what is the api call or sequence of calls to make this happen. There is a feature being tested where theres a need to verify the behavior when cookies are enabled or disabled. Thanks One thing i tried is creating a custom FF3 profile that has cookie turned off and one turned on. Each time i start selenium i point it to any one the profiles depending on which case being tested. Its an idea but Still haven't figured how to point selenium to start with these custom profiles. Another idea (I haven't tried that) would be to use a special proxy between the Selenium RC client and the tested web application. The proxy would be able to filter the cookies when asked to. There are some proxy implementations intended for development debugging and tracing roles. I am pretty sure you can find one with the feature to block cookies. EDIT: This solution has the advantage of being browser-independent.  As specified in the comment. If you are using FF you could specify the profile to be used. The way to do it it so specify the browserStartCommand (3rd argument of the DefaultSelenium constructor) to something similar to: *custom ""C:/Program Files/Mozilla Firefox/firefox.exe"" -no-remote -profile ""C:/Some/Path/To/Mozilla/Firefox/Profiles/selenium"" And this profile you could have the cookies disabled. that's not usable in selenium 2 webdriver  There's an easier way with using only a default profile if on Selenium 2.x. FirefoxProfile profile=new FirefoxProfile(); profile.setPreference(""network.cookie.cookieBehavior""2);  If you are going to be using Firefox there is a specific command to access the firefox template. You use -firefoxProfileTemplate ""path to the profile"" as described here. I would use the different profiles for cookies on and off as that way you can control it a lot better."
832,A,"how to write a junit testCase for add method? here is the add method for polynomial public Polynomilal add (Polynomial poly){ //getA()..etc getters for the coefficients of the polynomial. MyDouble aVal=getA().add(poly.getA()); MyDouble bVal=getB().add(poly.getB()); MyDouble cVal=getC().add(poly.getC()); Polynomial addedPoly=new Polynomial(aVal bVal cVal); return addedPoly; } and the test case for add method starts with public void testAdd() { ........ ........ } What specifically is your problem? You don't know how to test using JUnit or don't know what behaviour to test or how to test behaviour of add? As addition to david's answer I recommend you to use Custom Assertion pattern described here. Also I'd consider using parametrized tests for input and expected data as described here and using some JUnit specific examples like the following. Hope this helps!  Here's some of the basics... The general idea of a unit test is to compare ""What you want"" with ""What you got"". A simple assertion is like assertEquals(""it better work!"" 4 /* expected */ 2 + 2); If you know what aVal should be you can do assertEquals(""aVal should be this"" <what you expecte it to be> aVal); There's a special detail for ""double"" values because roundoff causes them to often be not exactly what you expect so you say: assertEquals(""some double value"" 1.555555d 1.0d + 5.0d / 9.0d .001); // within .001? ok! Anyway that's the gist of unit tests. Assertions of things you can see. Very handy stuff. (assertEquals and friends are all statically accessible from TestCase which most unit tests descend from.)"
833,A,"How to unit test a Spring MVC annotated controller? I am following a Spring 2.5 tutorial and trying at the same time updating the code/setup to Spring 3.0. In Spring 2.5 I had the HelloController (for reference): public class HelloController implements Controller { protected final Log logger = LogFactory.getLog(getClass()); public ModelAndView handleRequest(HttpServletRequest request HttpServletResponse response) throws ServletException IOException { logger.info(""Returning hello view""); return new ModelAndView(""hello.jsp""); } } And a JUnit test for the HelloController (for reference): public class HelloControllerTests extends TestCase { public void testHandleRequestView() throws Exception{ HelloController controller = new HelloController(); ModelAndView modelAndView = controller.handleRequest(null null); assertEquals(""hello"" modelAndView.getViewName()); } } But now I updated the controller to Spring 3.0 and it now uses annotations (I also added a message): @Controller public class HelloController { protected final Log logger = LogFactory.getLog(getClass()); @RequestMapping(""/hello"") public ModelAndView handleRequest() { logger.info(""Returning hello view""); return new ModelAndView(""hello"" ""message"" ""THIS IS A MESSAGE""); } } Knowing that I am using JUnit 4.9 can some one explain me how to unit test this last controller? possible duplicate of [How to unit test a Spring MVC controller using @PathVariable?](http://stackoverflow.com/questions/1401128/how-to-unit-test-a-spring-mvc-controller-using-pathvariable) @jackyesind Add the request URI at the beginning of each test like below. That should take care of the NPE. ... request.setRequestURI(""/annotationInYourController.do""); Object handler = handlerMapping.getHandler(request).getHandler(); ...  With mvc:annotation-driven you have to have 2 steps: first you resolve the request to handler using HandlerMapping then you can execute the method using that handler via HandlerAdapter. Something like: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(""yourContext.xml"") public class ControllerTest { @Autowired private RequestMappingHandlerAdapter handlerAdapter; @Autowired private RequestMappingHandlerMapping handlerMapping; @Test public void testController() throws Exception { MockHttpServletRequest request = new MockHttpServletRequest(); // request init here MockHttpServletResponse response = new MockHttpServletResponse(); Object handler = handlerMapping.getHandler(request).getHandler(); ModelAndView modelAndView = handlerAdapter.handle(request response handler); // modelAndView and/or response asserts here } } This works with Spring 3.1 but I guess some variant of this must exist for every version. Looking at the Spring 3.0 code I'd say DefaultAnnotationHandlerMapping and AnnotationMethodHandlerAdapter should do the trick. @I got null pointer exception in `Object handler = handlerMapping.getHandler(request).getHandler();` How would i solve  One advantage of annotation-based Spring MVC is that they can be tested in a straightforward manner like so: import org.junit.Test; import org.junit.Assert; import org.springframework.web.servlet.ModelAndView; public class HelloControllerTest { @Test public void testHelloController() { HelloController c= new HelloController(); ModelAndView mav= c.handleRequest(); Assert.assertEquals(""hello"" mav.getViewName()); ... } } Is there any problem with this approach? For more advanced integration testing there is a reference in Spring documentation to the org.springframework.mock.web. +1 Thank you so much Sasha. It works great. I couldn't imagine it was so simple. @AramKocharyan: in unit tests I would recommend against using Autowired and supply dependencies explicitly via constructor or setters. If you really want all the Spring goodness you may want to look into http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/beans.html#beans-java @SashaO yeah learnt that this week :P I'm just setting the autowired instance with a setter as you say worked well. This won't work if there are @Autowired components in HelloController I find this approach too white box. The fact that the view is called ""hello"" doesn't mean is doing what needs to do. We are unit testing our controllers using HttpClient. Just as if the test were the browser. @Rafael -- I can see why you would want to test View with the HttpClient (http://htmlunit.sourceforge.net/ is great for that BTW) but why would you want to test Controller? With Controller you want to test that it puts the right stuff into the model and invokes the correct view.  You can also look into other web testing frameworks that are independent of Spring like HtmlUnit or Selenium. You won't find any more robust strategy with JUnit alone other than what Sasha has described except you should definitely assert the model."
834,A,"Classpath problem for my Ant Build file I have been using this exact Build.xml file. I had problems in the beginning using the junit task with it but I figured out those problems a couple months ago. Recently I got the all to common error message when I ran my build file with the test task. test: [junit] Testsuite: com.mmz.mvc.test.AgentDAOTest [junit] Tests run: 1 Failures: 0 Errors: 1 Time elapsed: 0 sec [junit] Null Test: Caused an ERROR [junit] com.mmz.mvc.test.AgentDAOTest [junit] java.lang.ClassNotFoundException: com.mmz.mvc.test.AgentDAOTest [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:248) [junit] at java.lang.Class.forName0(Native Method) [junit] at java.lang.Class.forName(Class.java:247) [junit] at org.eclipse.ant.internal.ui.antsupport.EclipseDefaultExecutor.executeTargets(EclipseDefaultExecutor.java:32) [junit] at org.eclipse.ant.internal.ui.antsupport.InternalAntRunner.run(InternalAntRunner.java:423) [junit] at org.eclipse.ant.internal.ui.antsupport.InternalAntRunner.main(InternalAntRunner.java:137) BUILD FAILED C:\Users\myName\Documents\Java\mmz\WEB-INF\build.xml:45: Testcom.mmz.mvc.test.AgentDAOTest failed I know this problem is related to my classpath but I am not sure why this would all of a sudden break when it has been working for so long. My build follow looks like the following. <property file=""build.properties""/> <property name=""src.dir"" value=""src""/> <property name=""build.dir"" value=""classes""/> <property name=""web.dir"" value=""war""/>  <path id=""build.classpath""> <fileset dir=""lib""> <include name=""*.jar""/> </fileset> <fileset dir=""${appserver.lib}""> <include name=""servlet*.jar""/> </fileset> <pathelement path=""${build.dir}""/> <pathelement path=""${test.dir}""/> </path> <path id=""classpath.base""/> <path id=""classpath.test""> <pathelement location=""c:/ant/lib/junit.jar"" /> <pathelement location=""${build.dir}""/> <pathelement location=""${src.dir}""/> <pathelement location=""${test.dir}"" /> <pathelement location=""classes""/> <path refid=""classpath.base"" /> </path> <target name=""build""> <mkdir dir=""${build.dir}""/> <mkdir dir=""${test.dir}""/> <javac destdir=""${build.dir}"" source=""1.5"" target=""1.5"" debug=""true"" deprecation=""false"" optimize=""false"" failonerror=""true""> <src path=""${src.dir}""/> <classpath refid=""build.classpath""/> </javac> <javac destdir=""${build.dir}"" source=""1.5"" target=""1.5"" debug=""true"" deprecation=""false"" optimize=""false"" failonerror=""true""> <src path=""${test.dir}""/> <classpath refid=""build.classpath""/> </javac> </target> <target name=""test""> <junit haltonfailure=""true""> <classpath refid=""classpath.test"" /> <classpath refid=""build.classpath""/> <formatter type=""brief"" usefile=""false"" /> <test name=""com.mmz.mvc.test.AgentDAOTest""/> <test name=""com.mmz.mvc.test.AgentProfileDAOTest""/> <test name=""com.mmz.mvc.test.BuyerDAOTest""/> <test name=""com.mmz.mvc.test.BuyerSellerDAOTest""/> <test name=""com.mmz.mvc.test.BaseDAOTest""/> <test name=""com.mmz.mvc.test.MemberDAOTest""/> <test name=""com.mmz.mvc.test.SellerDAOTest""/> </junit> I am not very good with build files and I am not very good at understanding the how to setup classpaths and everything so If somebody could help I would appreciate it. Looks like a missing jar file containing the class ""com.mmz.mvc.test.AgentDAOTest"". Reading your build file suggests the jar used to be located in your ""lib"" directory. I'm assuming of course that this class is not a missing Java source file located under you ""src"" or ""${test.dir}"" directories..... What is really wierd is I have gone in and switched the order of the declared Tests inside of the test task and It will not give me a problem? Do you see any obvious problems with the build file. @CitadelCSAlum: If so does AgentDAOTest have a dependency on one of the other Test classes? Shouldnt be - but just checking."
835,A,JUnit for database code I've been trying to implement unit testing and currently have some code that does the following: query external database loading into a feed table query a view which is a delta of my feed and data tables updating data table to match feed table my unit testing strategy is this: I have a testing database that I am free to manipulate. in setUP() load some data into my testing db run my code using my testing db as the source inspect the data table checking for counts and the existence/non existence of certain records clear testing db loading in a different set of data run code again inspect data table again Obviously I have the data sets that I load into the source db set up such that I know certain records should be addeddeletedupdated etc. It seems like this is a bit cumbersome and there should be an easier way? any suggestions? DbUnit will meet your needs. One thing to watch out for is that they have switched to using SLF4J as their logging facade instead of JCL. You can configure SLF4J to forward the logging to JCL but be warned if you are using Maven DbUnit sucks in their Nop log provider by default so you will have to use an exclusion I blogged about this conflict recently.  If you are using Maven one option is to use the sql-maven-plugin. It allows you to run database initialization/population scripts during the maven build cycle.  I use DbUnit but also I work very hard to not to have to test against the DB. Tests that go against the database should only exist for the purpose of testing the database interface. So I have Mock Db Connections that I can set the data for use in all the rest of my tests.  Is it your intent to test the view which generates the deltas or to test that your code correctly adds deletes and updates in response to the view? If you want to test the view you could use a tool like DBUnit to populate your feed and data tables with various data whose delta you've manually calculated. Then for each test you would verify that the view returns a matching set. If you want to test how your code responds to diffs detected by the view I would try to abstract away database access. I imagine an java method to which you can pass a result set (or list of POJO/DTO's) and returns a list of parameter Object arrays (again or POJO's) to be added. Other methods would parse the diff list for items to be removed and updated. You could then create a mock result set or pojo's pass them to your code and verify the correct parameters are returned. All without touching a database. I think the key is to break your process into parts and test each of those as independently as possible.  Apart from the already suggested DBUnit you may want to look into Unitils. It uses DBUnit but provides more than that (quoting from the site): Automatic maintenance of databases with support for incremental repeatable and post processing scripts Automatically disable constraints and set sequences to a minimum value Support for Oracle Hsqldb MySql DB2 Postgresql MsSql and Derby Simplify test database connection setup Simple insertion of test data with DBUnit * Run tests in a transaction JPA entity manager creation and injection for hibernate toplink and * Hibernate SessionFactory creation and session Automatically test the mapping of JPA entities / hibernate mapped objects with the database
836,A,"Different Behavior using JUnit via Ant versus IntelliJ IDEA I'm experiencing different behavior when running some integration tests via IntelliJ IDEA 10 and using Ant from the command line. Specifically tests that call singletons fail via Ant and succeed via IDEA. I'm aware of the downfalls of using singletons but this is code that I've inherited and would like to create tests before removing them :) I've verified that both are using the same version of JUnit (4.8.1). Here's a snippet from the Ant build.xml file: <junit printsummary=""yes"" fork=""yes"" forkmode=""perTest"" haltonfailure=""no""> <classpath refid=""classpath.test""/> <formatter type=""xml""/> <batchtest todir=""${report.home}/tmp""> <fileset dir=""${test.home}""> <include name=""**/*Test.java""/> <exclude name=""**/*TransactionalTests.java""/> </fileset> </batchtest> </junit> Since I cannot find documentation on the default settings of JUnit in IDEA I'm not able to determine if/how they fork etc. when running tests. Any suggestions on getting the same behavior via Ant and IntelliJ is greatly appreciated. What do the tests look like? Do they use `@Before` to initialize stuff or self-initializing fields? Are the tests run in the same order? I have had issues with running tests in Maven and Eclipse where the tests were run in a different order. These lead to the state of Singleton objects being different in the two run environments. Here's an example of a test experiencing this behavior: https://gist.github.com/875322 IDEA runs all the tests in the single JVM instance while you are forking via Ant to run each test in its own JVM instance. Since you have singletons in your code results could be different. IDEA has no option to fork tests at the moment though this feature is planned. The order of execution also might be the case since you are using singletons. IDEA runs tests in alphabetical order and there is no way to change it. So to get identical behavior you need to tell Ant to run your tests in the same order if the order of tests is important (which is not a good idea). Also you have some tests excluded via Ant IDEA doesn't have such option. If your other tests depend on the excluded tests results will be different. I should have mentioned this earlier but I tried all combinations of forking in my Ant build. Reordering tests also did not help. The tests I have excluded are just base classes that other tests extend. Accepting your answer since you had good suggestions. At this point I've moved on and got around the test failures by making some minor code changes."
837,A,"Where can I find the value of JUNIT_CONTAINER in Eclipse? I need to find out in which path this following path is resolved: <classpathentry kind=""con"" path=""org.eclipse.jdt.junit.JUNIT_CONTAINER/4""/> It's no classpath variabl to be found unter Window->Preferences Java->Build Path->Classpath Variables. Where can I found the value von JUNIT_CONTAINER/4? Thanks A simple solution is this little JUnit test. It must be a test because Eclipse only sets the needed libraries to the classpath System Property: import static org.junit.Assert.assertTrue; import org.junit.Test; public class TestApp { @Test public void bla() { System.out.println(System.getProperty(""java.class.path"")); assertTrue(true); } }  A classpathentry of kind ""con"" means classpath container. From Java Class Paths help page: entry denoting a classpath container: an indirect reference to a structured set of project or libraries. Classpath containers are used to refer to a set of classpath entries that describe a complex library structure. Like classpath variables classpath containers (IClasspathContainer) are dynamically resolved. Classpath containers may be used by different projects causing their path entries to resolve to distinct values per project. They also provide meta information about the library that they represent (name kind description of library.) Classpath containers can be manipulated through JavaCore methods getClasspathContainer and setClasspathContainer. So in your case to be really sure about the resolved path you could query your own project through those calls like this ClassPathUtils case IClasspathEntry.CPE_CONTAINER: { final IClasspathContainer container; try { container = JavaCore.getClasspathContainer( entry.getPath() jproj ); } catch( JavaModelException e ) { Logger.getLogger().logError( e ); continue; } if( container != null ) { final IClasspathEntry[] containerEntries = container.getClasspathEntries(); for( int j = 0; j < containerEntries.length; j++ ) { resolved.add( containerEntries[ j ].getPath() ); } } }  Try Help -> About Eclipse Platform -> Configuration Details."
838,A,"How can I find out if code is running inside a JUnit test or not? In my code I need to do certain fixes only when it is run inside a JUnit test. How can I find out if code is running inside a JUnit test or not? Is there something like JUnit.isRunning() == true ? This is a terrible idea of course still better than not testing at all. Maybe... FWIW I often find it's useful to do *while developing*. Put a check for jUnit in the outer-most method of an API that throws a run-time exception at the end when being run in-container. This way you can run unit tests and get a valid result and also test the full stack easily from the UI but your transaction gets thrown out due to the RTE. I think I have a pretty valid use case for this: I want to send an email in production if something goes seriously wrong and I don't want that email if it's inside of a test case (the developer as opposed to the production support staff will see the failed results on his own). We have over a hundred JUnit tests and I don't want to go modify each one to call something like EmailService.dontSendEmailsToProductionTeam() I like to know if code is running within a unit test so I can do things like tell it to use a different S3 bucket. That way I can still test everything is wired up without having to mock out S3 entirely. I think it is useful as a safety constraint. Safety constraints seldom do any harm even if ""should never happen"". Lots of people on this thread say that it's a bad idea for code to run slightly differently when under JUnit. I generally agree but I think there are some exceptions. For example I am currently writing INTEGRATION (as opposed to Unit) tests for an app that connects to a DB. These acceptance tests often need to completely reinitialize the DB with specific test data. Obviously I don't want this EVER EVER be done on an actual production DB because that might completely erase valuable production data. The easiest way to garantee that this will never happen is to make it impossible for the code to connect to a production DB when it is running under JUnit. This in turn can be done if (for example) the Factory that generates a connection can tell that it's running under JUnit and in that case will return a null connection unless the database we are trying to connect to has a name that is known to be a test database (ex: ""testdatabase"").  I can find a justification for this which is when you want to provide code in a production class to help testing. This would be similar to the Java assert which only applies when the debug flag is set. Something like this: Object debugState() { // This is only meant to be called when testing if (!JUnit.isRunning()) { throw new IllegalStateException(""Not in a test!""); } // Now compute possibly costly debug information // not to be used in production Object state = ... }  If you're doing things differently because you're doing a unit test you're defeating the purpose of a unit test. A unit test is supposed to perform exactly like production would (except for the setup and teardown of any necessary data and such you need....but that is included in the JUnit test itself and not your code). However if you truly do have a good reason for this I'd look at the stack and see if JUnit is there. I know this sounds a bit against TDD but testing code for GoogleAppengine using Jersey/Jetty introduces more than one thread on my local machine which in turn causes the new threads not to have the right mock testing environment from Google initialised so I need to check in many points of my potential second thread if my AppEngine mock up env is properly initialised already which I do not want to do if I am in a production server. I'ts clearly a bit messy and would be better if I could mimic the AppEngine env more closely on my local machine (i.e. have only one thread). I think this should be your actual question. How do I get my unit tests to behave the same as the deployed environment. This might help you actually solve your issues without introducing code to actually check whether or not you are running a unit test. Why not test for GAE using the GAE development server? It is pretty close to the real thing and I believe they even have JUnit support now.  It might be a good idea if you want to programmatically decide which ""profile"" to run. Think of Spring Profiles for configuration. Inside an integration tests you might want to test against a different database. Here it the tested code that works public static boolean isJUnitTest() { StackTraceElement[] stackTrace = Thread.currentThread().getStackTrace(); List<StackTraceElement> list = Arrays.asList(stackTrace); for (StackTraceElement element : list) { if (element.getClassName().startsWith(""org.junit."")) { return true; } } return false; } We need to configure database environment differently for JUnit tests.  Your are really violating the idea of TDD if your code if doing something different for the Test. Why do you only need to change it inside the test?  First of all this a probably not a good idea. You should be unit testing the actual production code not slightly different code. If you really want to do this you could look at the stacktrace but since you are changing your program for this anyway you might just as well introduce a new static boolean field isUnitTesting in your code and have JUnit set this to true. Keep it simple. Yes you might for instance have a singleton object in your class containing the isUnitTesting field and set it to true in a JUnit @Before method. come to think of it this is a little like dependency injection. He should be injecting mock objects for the dependencies that are too difficult to test but I suppose that setting a boolean to true explicitly is still better then trying to have the class figure it out by itself. Guys you convinced and encouraged me to rephrase this question. http://stackoverflow.com/questions/2351293/google-appengine-local-junit-tests-jersey-framework-embedded-jetty  I agree with the previous answers it seems like a bad idea to mix testcode in production code. If you cannot perform the tests you want with JUnit and a mock framework you should probably rethink your design. Good mocking frameworks: http://easymock.org/ http://mockito.org/"
839,A,How to ignore a test within the JUnit test method itself We have a number of integration tests that fail when our staging server goes down for weekly maintenance. When the staging server is down we send a specific response that I could detect in my integration tests. When I get this response instead of failing the tests I'm wondering if it is possible to skip/ignore that test even though it has started running. This would keep our test reports a bit cleaner. Does anybody have suggestions? In your test you could test for this condition before performing the Assert.whatever. If the test fails just return from the method without running the rest of the code/asserts.  It's been awhile since I used JUnit but isn't there a way to Assume a condition is true? I think that has a different meaning than a pass/fail of the test. Your report should indicate that the test was not run. Edited to add: Assume class  The simplest solution would be to split your tests up into two suites: integration tests and pure unit tests. Then develop a script or some other automated means to determine if the server is up and simply skip the integration test suite if the server is down. But if grouping the tests into suites like this is not practical for some reason here's an alternative: You could create a custom Runner that skips tests if the server is unavailable. You could either program the runner to determine server availability on its own or determine it through some external process such as a script that executes before the test phase and sets a system property on the JVM that the runner can check (e.g. pass -Dcom.company.testrun.integration=false as a command line argument). You could enable your custom runner using the @RunWith annotation on your integration test classes and use the built-in runner for all other tests so they are not impacted. Alternatively you could use your runner for all tests and invent a new annotation (e.g. @IntegrationTest) that you use to decorate your integration test methods. Using the latter approach the runner would apply its skip logic only if the server is unavailable and the test method has the special annotation.
840,A,"Mocking EJB injection in tests Whenever I want to test a class which uses resource injection I end up including a constructor that will only be used within the test: public class A { @EJB B b; // Used in tests to inject EJB mock protected A(B b) { this.b = b; } public A() {} // Method that I wish to test public void foo() { b.bar(); } } Is there another way of mocking resource injection or this is the correct pattern to follow? you could use easy gloss to that effect it mocks the EJBs injection system. another way is to set the field using reflexion in your tests I sometime use something like this : public static void setPrivateField(Class<? extends Object> instanceFieldClass Object instance String fieldName Object fieldValue) throws Exception { Field setId = instanceFieldClass.getDeclaredField(fieldName); setId.setAccessible(true); setId.set(instance fieldValue); } Something like easy gloss was what I was looking for thank you!  It's certainly one way to do it although I'd rely on package access; don't provide a constructor injection point but simply have your test in the same package as the bean being tested. That way your test can just access the value directly (assuming it's not private): @Test public void EJBInjectionTest() { A a=new A(); a.b=new B() { // mock functionality here of course... }; assertNotNull(a.b); } I like this way because there is no need to add additional lines of code. Thanks!  Eliocs If type B where an interface then you wouldn't ""just"" bo doing it for test-cases; you'd be allowing for any alternative implementations of ""B's behaviour"" even if the need for it/them hasn't been dreamed-up yet. Yeah basically that's the only pattern to follow (AFAIK)... so (rightly or wrongly) you may as well make the best of it ;-) Cheers. Keith."
841,A,"Junit: Test Spring (auto)wiring with reflection? Is it possible to JUnit test if wiring by Spring is succesfully? I would like to do this by reflection. Like: get all beans with id *Controller and test if the fields *services are not null? Thank you! build your ApplicationContext either via XmlWebApplicationContext's constructor or via the spring JUnit test runner and make your test implement ApplicationContextAware use the methods of ApplicationContext to find and verify everything you need with the help of ReflectionUtils and ReflectionTestUtils. But have in mind that if injection fails the whole context initialization fails.  A better way is to annotate the setter methods with org.springframework.beans.factory.annotation.Required and add the required annotations post processor: <!-- This bean will cause an error if you forget to supply any properties annotated with @Required on the setter method; this is good for catching errors. --> <bean class=""org.springframework.beans.factory.annotation.RequiredAnnotationBeanPostProcessor"" /> If you want to verify that methods that match a certain pattern have the @Required annotation implement a compiler hook an AnnotationProcessor that causes a compiler failure if methods matching a certain pattern aren't annotated with @Required."
842,A,"How to do unit test for Exceptions? As you know exception is thrown at the condition of abnormal scenarios. So how to analog these exceptions? I feel it is challenge. For such code snippets: public String getServerName() { try { InetAddress addr = InetAddress.getLocalHost(); String hostname = addr.getHostName(); return hostname; } catch (Exception e) { e.printStackTrace(); return """"; } } Does anybody have good ideas? Out of curiosity why is this a CW? He's probably noticed the first comment to most questions is ""should be wiki"". This one probably has 6+ valid answers so CW seems reasonable. Other answers have addressed the general problem of how to write a unit test that checks that an exception is thrown. But I think your question is really asking about how to get the code to throw the exception in the first place. Take your code as an example. It would be very hard to cause your getServerName() to internally throw an exception in the context of a simple unit test. The problem is that in order for the exception to happen the code (typically) needs to be run on a machine whose networking is broken. Arranging for that to happen in a unit test is probably impossible ... you'd need to deliberately misconfigure the machine before running the test. So what is the answer? In some cases the simple answer is just to take the pragmatic decision and not go for total test coverage. Your method is a good example. It should be clear from code inspection what the method actually does. Testing it is not going to prove anything (except see below **). All you are doing is improve your test counts and test coverage numbers neither of which should be project goals. In other cases it may be sensible to separate out the low-level code where the exception is being generated and make it a separate class. Then to test the higher level code's handling of the exception you can replace the class with a mock class that will throw the desired exceptions. Here is your example given this ""treatment"". (This is a bit contrived ... ) public interface ILocalDetails { InetAddress getLocalHost() throws UnknownHostException; ... } public class LocalDetails implements ILocalDetails { public InetAddress getLocalHost() throws UnknownHostException { return InetAddress.getLocalHost(); } } public class SomeClass { private ILocalDetails local = new LocalDetails(); // or something ... ... public String getServerName() { try { InetAddress addr = local.getLocalHost(); return addr.getHostName(); } catch (Exception e) { e.printStackTrace(); return """"; } } } Now to unit test this you create a ""mock"" implementation of the ILocalDetails interface whose getLocalHost() method throws the exception you want under the appropriate conditions. Then you create a unit text for SomeClass.getServerName() arranging that the instance of SomeClass uses an instance of your ""mock"" class instead of the normal one. (The last bit could be done using a mocking framework by exposing a setter for the local attribute or by using the reflection APIs.) Obviously you would need to modify your code to make it testable like this. And there are limits to what you can do ... for example you now cannot create a unit test to make the real LocalDetails.getLocalHost() method to throw an exception. You need to make a case-by-case judgement as to whether it is worth the effort of doing this; i.e. does the benefit of the unit test outweigh the work (and extra code complexity) of making the class testable in this way. (The fact that there is a static method at the bottom of this is a large part of the problem.) ** There is a hypothetical point to this kind of testing. In your example the fact that the original code catches an exception and returns an empty string could be a bug ... depending on how the method's API is specified ... and a hypothetical unit test would pick it up. However in this case the bug is so blatant that you would spot it while writing the unit test! And assuming that you fix bugs as you find them the unit test becomes somewhat redundant. (You wouldn't expect someone to re-instate this particular bug ...) Right this is my wanted. Mock is a method to do this. but it needs to adjust the production code and pay additional efforts. Do you think this mock method could catch some bugs?  You can tell junit that the correct behavior is to get an exception. In JUnit 4 it goes something like: @Test(expected = MyExceptionClass.class) public void functionUnderTest() { … }  Many unit testing frameworks allow your tests to expect exceptions as part of the test. JUnit for example allows for this. @Test (expected=IndexOutOfBoundsException.class) public void elementAt() { int[] intArray = new int[10]; int i = intArray[20]; // Should throw IndexOutOfBoundsException }  Okay there are a few possible answers here. Testing for an exception itself is easty @Test public void TestForException(){ try{ DoSomething(); Fail(); }catch(Exception e) { Assert.That(e.msg Is(""Bad thing happened"")) } } Alternately you can use the Exception Annotation to note that you expect an exception to come out. Now as to you specific example Testing that something you are creating inside your method either via new or statically as you did when you have no way to interact with the object is tricky. You normally need to encapsulate that particular generator and then use some mocking to be able to override the behavior to generate the exception you expect. Isn't there a ) missing in this line? `Assert.That(e.msg Is(""Bad thing happened"")` my java unittesting experience is too little so didn't dare to directly edit it. Yes there is a missing trailing parenthesis on the end there."
843,A,"Selenium - Store hidden variable We are using Junit + Selenium to webtest our webpage. But we have run into a problem. I need to parse the value from a hidden field with Selenium. HTML of hidden field <input type=""hidden"" name=""secretId"" value=""123456""/> I use the following XPath //input[@name='secretId']/@value I need to scrape that hidden variable and store it using a XPath and use it further on down the script. How do I do this with Selenium? I have tried String secretId = selenium.getText(""//input[@name='secretId']/@value""); Returns empty string String secretId = selenium.getEval(""//input[@name='secretId']/@value""); Returns null The XPath is correct I have verified this with XPath Checker in Firefox Thanks? I have got the answer to get the value by using WebDriver: String secretId = driver.findElement(By.xpath(""//input[@name='secretId']"")).getText();  Found the answer String secretId = selenium.getValue(""//input[@name='secretId']""); How can I get the value by using WebDriver? I don't know about Java but this works in Python and might as well i Java: driver.find_element_by_name('something').value = 'some string' Yes the above code for Selenium RC works fine"
844,A,"Is it against best practice to throw Exception on most JUnit tests? Almost all of my JUnit tests are written with the following signature: public void testSomething() throws Exception My reasoning is that I can focus on what I'm testing rather than exception handling which JUnit appears to give me for free. But am I missing anything by doing this? Is it against best practice? Would I gain anything by explicitly catching specific exceptions in my test and then fail()'ing on them? JUnit doesn't fail a test if an exception occurs? @brian To clarify yes JUnit fails if an uncaught exception occurs but I meant in the context of me catching exceptions thrown by my code. The main benefit involves when you are testing some scenario that requires an Exception to be thrown (e.g. err-r handling) You can in JUnit4 use something like: @Test(expected=ArithmeticException.class) but some people find that to be harder to read/less intention revealing than an explicit try{} catch (Exception e) block and if you wanted to check state (say of some mock object or to see if the exception was thrown in the right place or logged or the like)  Do NOT catch and fail -- you will lose valuable information. Let all exceptions fly right on out. This means you need to add each checked exception your signature that can be thrown. However I'd advise you not to take the lazy way out and blindly use throws Exception as a matter of habit. This excuses you from ever even having to think about how your API really behaves with respect to exceptions. IMHO adding the full list of checked exceptions to a test method does not provide much benefit and in fact hampers the readability of your test signatures (especially if the SUT can throw a lot of exceptions).  If an exception is thrown and you're not expecting it the test should fail. If it's an unchecked exception I allow the exception to be thrown and have JUnit fail the test. If it's a checked exception you have a choice: either add the exception to the throws clause of the method signature or catch it inside the method. The compiler will force a choice on you because you can't run the code without either of these choices. Lately I've tended to not catch exceptions inside my tests. If it's supposed to throw an exception I mark it as such with the annotation. If it throws an unchecked exception I left JUnit fail the test for me. If it's a checked exception I add the throws clause to the method signature and left JUnit fail the test for me. Agreed that the test should fail but my question is more about whether I should catch the exception and explicitly fail the test or let it bubble up to the JUnit framework and let it fail the test for me.  In general if you are testing a case where you do not expect an Exception to occur then I would just let the test method throw Exception as you have illustrated since it will nicely differentiate between Failing test cases (they do not pass one of your assertions) and Error test cases (they cause an unexpected Exception). The JUnit TestRunners will catch the thrown Exception regardless so you don't have to worry about your entire test suite bailing out if an Exception is thrown. On the other hand if you are writing a test that is supposed to trigger an exception then you either want to use the @Test(expected=IllegalArgumentException.class) variant of the JUnit 4 annotation or the more common JUnit 3 idiom of: try { target.someMethodToTest(); fail(""Should have gotten an exception""); } catch (IllegalStateException ise) { //expected it's all good } This is the best answer. I'll add that I think the question here is one of style: catch-and-fail or throw? Normally best practice avoids ""throws Exception"". The reason is that it makes exception handling meaningless for the API user. But there is no such user here. So ""throws Exception"" is the right practice."
845,A,"Need help with developing a class for my JUnit test I have this JUnit test that I need help developing a Interface and Class for here is the test: Box b1 = new DefaultBox( ""abc"" ); Box b2 = new DefaultBox( ""def"" ); Box b3 = new DefaultBox( """" ); assertEquals(""abc"" b1.contents()); assertEquals(""[abc]"" b1.toString()); assertTrue(b1.equals(b1)); assertFalse(b1.equals(b2)); assertFalse(b1.equals(null)); assertEquals(""cba"" b1.flip().contents()); assertEquals("""" b3.flip().contents()); can anyone help me in developing a Default box class and a box interface to make these test pass? Any help would be most appreciated. Updates Ok I am trying to start a constuctor but i keep getting a run time error saying ""Implicit super constructor Box() is undefined. Must explicitly invoke another constructor"" Here is my class: import javax.swing.Box; public class DefaultBox extends Box{ public DefaultBox(String string) { } } my Junit test is: import static org.junit.Assert.*; import javax.swing.Box; public class question3_test { Box b1 = new DefaultBox( ""abc"" ); Box b2 = new DefaultBox( ""def"" ); Box b3 = new DefaultBox( """" ); public void testquestion3(){ assertEquals(""abc"" b1.contents()); assertEquals(""[abc]"" b1.toString()); assertTrue(b1.equals(b1)); assertFalse(b1.equals(b2)); assertFalse(b1.equals(null)); assertEquals(""cba"" b1.flip().contents()); assertEquals("""" b3.flip().contents()); } } I have tried to remove the ""extends Box"" but then that gives me a run time error on the Junit test. Can anyone guide me on how to remove this implicit super constructor error? What kind of help you need? Your question doesn't seem so complex. contents should return the string contained toString should put it in brackets equals should check if contents are equal flip should return a new DefaultBox with reversed content I suppose this is homework but the class that you have to implement it's quite trivial.. This is homework and I basically need help on how to construct this class. I am struggling on how to use TDD to construct actual classes any help would be much appreciated  Here is one possible skeleton. I choose not to provide full implementations since this is a homework problem interface Box { //put content and flip methods } public class DefaultBox implements Box { public DefaultBox(String str) { //Find out how to store this str as an internal field variable? } } I do appreciate everyone's help but I had a side questions about Eclipse. I created my classes and JUnit test but the JUnit test file is not letting me run it as a test all it says is Run Configuration. I have used this before and was able to select Run As JUnit test. Does anyone know how to activate this option. alpdog14: with the JUnit file open just click the green play button in the toolbar. It should recognize it as a JUnit test and launch the correct configuration and from then on you can use the Run As menu if you'd prefer.  I'm pretty sure that you don't want to use javax.swing.Box but rather a custom interface that you either got together with your assignment or have to write yourself. So remove the import of javax.swing.Box or replace it with the correct import. Also I'd like to suggest a different way to write your JUnit test: import static org.junit.Assert.*; public class DefaultBoxTest { Box b1 = new DefaultBox( ""abc"" ); Box b2 = new DefaultBox( ""def"" ); Box b3 = new DefaultBox( """" ); public void testContents(){ assertEquals(""abc"" b1.contents()); } public void testToString(){ assertEquals(""[abc]"" b1.toString()); } public void testEqualsItSelf(){ assertTrue(b1.equals(b1)); } public void testNotEqualsOther(){ assertFalse(b1.equals(b2)); } public void testNotEqualsNull(){ assertFalse(b1.equals(null)); } public void testFlip(){ assertEquals(""cba"" b1.flip().contents()); } public void testFlipEmpty(){ assertEquals("""" b3.flip().contents()); } } This tests effectively the same thing but it will tell you more precisely which parts work and which ones don't. Because with your test it will only tell you that the entire test failed and you'll have to find out which one failed. If you write it this way it will tell you exactly which assert failed.  Implicit super constructor Box() is undefined. Must explicitly invoke another constructor"". It is because DefaultBox calls a constructor that is not compatible with Box. In this case DefaultBox has a String constructor which because it is empty JVM will try to create a Box with no-arg constructor which it cant find. Try import javax.swing.Box; public class DefaultBox extends Box{ public DefaultBox(String string) { //call some version of the Box constructor that is suitable. Javax swing has only a int arg constructor so super(5); perhaps? } } Quite why you would have a homework on Swing Boxes is puzzling to me though :) alpdog14: your test variables are SUPPOSED to get highlighted in red. This is test-driven development. Of course Eclipse can't find the class; you haven't written it yet. Remove the import you do not want to use javax.swing.Box the teacher expects you to write an interface Box. actually eclipse wanted me to put in the javax.swing.Box without it my Junit test varibales get highlighted in red My guess is that your assignment wants you to create an interface/abstract class Box on your own and also write a DefaultBox that implements/extends Box."
846,A,"How to unit test a method with enum parameters? I'm using junit and EasyMock to do unit testing on a project I'm working on. However I've run into a problem. I have a good handful of methods that have a parameter that is an enumeration. I ran into the java.lang.NullPointerException when attempting to mock the enum and it seems enums just cannot be mocked. More information I found on it here: http://download.oracle.com/javase/tutorial/java/javaOO/classvars.html Is there any good way to unit test this method without mocking the enum?? Thanks! EDIT: Péter Török was right! I was completely looking over the fact that I could just plug in something for the enum. For example: public void methodName(String description Location buildingLocation) { where Location is my enum I can call the method as: methodName(""here is my description"" Location.DENVER); Can you show a sample unit test with the mentioned problem (SSCCE)? What does your enum contain that you need to mock it? Why can't you just simply use the available values themselves? Since enums are (supposed to be) stateless and immutable they should be readily available for unit testing you should have no problems in instantiating them they should hold no (mutable) global state and should have no external dependencies which make them hard to use in unit tests. Failing any of the above would be a sign of a design problem to me rather than a unit testing problem. Thanks! I think I've been on auto-pilot doing this unit test I just didn't even think about not doing a mock."
847,A,"Passing JUnit data between tests I just discovered when creating some CRUD tests that you can't set data in one test and have it read in another test (data is set back to its initialization between each test). All I'm trying to do is (C)reate an object with one test and (R)ead it with the next. Does JUnit have a way to do this or is it ideologically coded such that tests are not allowed to depend on each other? [Here is a solution i came up with and an explanation of the drawbacks of using static variables][1] [1]: http://stackoverflow.com/questions/17885221/how-to-save-non-static-properties-state-between-junit-test-methods-answer in this basic example the variable is changed in the test A and can be used in the test B public class BasicTest extends ActivityInstrumentationTestCase2 { public BasicTest() throws ClassNotFoundException { super(TARGET_PACKAGE_ID launcherActivityClass); } public static class MyClass { public static String myvar = null; public void set(String s) { myvar = s; } public String get() { return myvar; } } private MyClass sharedVar; @Override protected void setUp() throws Exception { sharedVar = new MyClass(); } public void test_A() { Log.d(S""run A""); sharedVar.set(""blah""); } public void test_B() { Log.d(S""run B""); Log.i(S""sharedVar is: "" + sharedVar.get()); } } output result is: run A run B sharedVar is: blah  JUnit promotes independent tests. One option would be to put the two logical tests into one @Test method. TestNG was partly created to allow these kinds of dependencies among tests. It enforces local declarations of test dependencies -- it runs tests in a valid order and does not run tests that depend on a failed test. See http://testng.org/doc/documentation-main.html#dependent-methods for examples. It really is ideal for unit tests to be independent in both state and order. JUnit supports the ideal. TestNG supports both the ideal and pragmatic exceptions. Cedric Beust the author of TestNG discusses the issues in more detail in the sources below. He confirmed the intent of JUnit with Beck and Gamma and found shortcomings with working around the JUnit approach with static members. * Beust's 2004 blog post http://beust.com/weblog/2004/02/08/junit-pain/ * The first several pages of Beust's book ""Next Generation Java Testing: TestNG and Advanced Concepts"" Addison-Wesley 2008. So it *is* ideology! I was afraid of that. I did think come to think of it that there were some new annotations having to do with intertest dependency but maybe I was reading about TestNG. Good point. In contrast TestNG allows you to tag tests with a test category like ""database"" or ""integration"" via the 'groups' attribute; provides @BeforeGroup/@AfterGroup setup/teardown methods; and supports running or excluding-from-running a set of groups. More detail on Beust's slides 6-11 here: http://qconsf.com/sf2007/file?path=/QConSF2007/slides/public/CedricBeust_TestNG.pdf. I agree with all that for unit tests. But CRUD tests are database access and therefore not unit tests. It's a shame that JUnit which is so flexible and pervasive should be limited in any way that restricts it to unit tests only. To make true CRUDy unit tests one would have to setup test state (database) and tear-it-down afterwords. Unfortunately this is very often complicated and error-prone which increases test development overhead while also increasing test failure noise. An example of a low error-prone approach to setting-up state would be running an SQL script whereas a high error-prone approach would be to use the same methods that are being tested. Unfortunately the former is almost always impractical or even impossible depending on the project so for the latter test dependency seems to address this issue.  How much processing time do these tests take? If not a lot then why sweat it. Sure you will create some object unnecessarily but how much does this cost you?  @ Test void testCreateObject ( ) { Object obj = unit . createObject ( ) ; } @ Test void testReadObject ( ) { Object obj = null ; try { obj = unit . createObject ( ) ; // this duplicates tests aleady done } catch ( Exception cause ) { assumeNoException ( cause ) ; } unit . readObject ( obj ) ; } Good point. It's more that I have to think up test data to ensure uniqueness in the 2 createObject()s but this may be the way to go. I guess part of me resists writing the same ""create"" code twice. I'm over-DRY methinks.  Well for unit tests your aim should be test the smallest isolated piece of codeusually method one by one. So testCreate() is a test case and testRead is anther. However there is nothing that stops you from creating a testCreateAndRead() to test the two functions together. But then if the test fails which code unit does the test fail at? You dont know. Those kind of tests are more like integration test which should be treated differently. If you really want to do it you can create a static class variable to store the object created by testCreate() then use it in testRead(). As I have no idea what version of Junit you talking about I just pick up the ancient one Junit 3.8: Utterly Urgly but works: public class Test extends TestCase{ static String stuff; public void testCreate(){ stuff = ""abc""; } public void testRead(){ assertEquals(stuff ""abc""); } } It *is* an integration test - duh CRUD means accessing the database. By the way good idea about the static variable if it works. I'll have to try it. That worked thanks! Can you guarantee this works ? Have you defined in what order JUnit would reliably execute these tests ?"
848,A,"How do I build and distribute my Java project? I used Eclipse to build a pretty simple Java project. It builds and runs in the IDE. I have a few unit tests I wrote using JUnit. They all build and pass in the IDE. My project is in the following path: /home/vg1890/workspace/project/ The main source is in: /home/vg1890/workspace/project/src And the tests are in: /home/vg1890/workspace/project/tests The package name is com.vg1890.stuff. When I type: echo $CLASSPATH at the command line nothing is returned (ubuntu). How can I build the entire project from outside the IDE? How do make it so that when I distribute the source to another computer that it will build and run (including the unit tests)? Thanks! This is exactly the kind of thing that Apache Ant is usually used for. Eclipse supports ant scripts quite well. Does the destination machine need to have Apache Ant as well in order to build the project? Yes - but it will be present on pretty much any machine used for Java development. Building from source or running unit tests isn't typically something non-developers do.  Further to Michael's answer note that many of the IDEs (IDEA NetBeans Eclipse) allow you to integrate their build mechanisms very tightly with your own Ant build scripts and can give you a hand starting them. This can be useful when starting out. However it is useful later to consider implementing your build mechanism so that it works cleanly (i.e. completely separated from your IDE). This is a very good idea these days where often IDEs have such in-built support for certain technologies (e.g. Spring Hibernate). It is otherwise too easy to release some JAR file which is missing some vital library or end up with a dependency that you didn't know existed.  I'd suggest to use Apache Maven to manage your build and distribute your sources (maven is widely used and like Ant lost of Java users/programmers have it installed on their computer). It's hard to define maven in one line (maven is much more than just a ""build tool"") so I've pasted below a more complete definition taken from the first chapter of this great book: Maven: the Definitive Guide. Maven is a project management tool which encompasses a project object model a set of standards a project lifecycle a dependency management system and logic for executing plugin goals at defined phases in a lifecycle. When you use Maven you describe your project using a well-defined project object model Maven can then apply cross-cutting logic from a set of shared (or custom) plugins. Once maven installed putting a complete build in place for your project (compilation of java sources and unit tests execution of tests packaging of the compiled classes) is a matter of 10 seconds (really). To get started with Eclipse have a look at the Guide to using Eclipse with Maven 2.X. really really *not*. OP already has a project structure that does not follow Maven's convention so putting a complete build in place is a matter of restructuring his project or diving into Maven's configuration - so forget about 10 seconds. Additionally the strengths of Maven probably won't really manifest with a very simple project. How much time do you need to move files from one directory to another one? I don't need more than 5 seconds to do it so 10 seconds for main sources and tests seem fair. I agree that it's easy to move an existing (simple) project into maven. Simple create a new java project with the maven tool and drag in your sources. BUT you will need to install the Eclipse integration first (there are two options - m2eclipse or IAM) AND both are not entirely bug free at this point."
849,A,"Ant+Junit works on first run and never again Hey guys I've been trying all day to get an ant file to automatically build my project. The file (appended below) was on a web page I found and is pretty thorough. My problem is that it works as long as I don't run the ""clean"" target. Once the I run the ""clean"" target the ""test"" target ceases to work. I get NoClassFound errors on all test classes. Even though it worked earlier. I'm working on a Macbook (10.5.8) using the latest version of eclipse. This error occurs running the file from eclipse and from a terminal using Ant version 1.7.1 I modified the file slightly to adapt it to my file structure which is as follows: src/ packageA.packageB/ ClassA.java ... ClassN.java unittests/ packageA.packageB/ AllClassTests.java ClassATest.java ... ClassNTest.java lib/ junit-4.7.jar The ant file is: <project name=""SampleJUnitTests"" default=""dist"" basedir="".""> <description> DataTypes Build File </description> <!-- set global properties for this build --> <property name=""project_name"" value=""DataTypes""/> <property name=""src"" location=""src""/> <property name=""build"" location=""bin""/> <property name=""dist"" location=""dist""/> <property name=""lib"" location=""lib""/> <property name=""reports"" location=""reports""/> <property name=""tests"" location=""unittests""/> <property name=""tmp"" location=""tmp_file""/> <!-- the names of various distributable files --> <property name=""jar_name"" value=""${project_name}.jar""/> <property name=""war_name"" value=""${project_name}.war""/> <!-- top level targets --> <target name=""compile"" depends=""init"" description=""compile the source code "" > <javac srcdir=""${src}"" destdir=""${build}""> <classpath> <fileset dir=""lib""> <include name=""**/*.jar""/> </fileset> </classpath> </javac> </target> <target name=""dist"" depends=""compile"" description=""generate the distributable files "" > <!-- Put everything in ${build} into the MyProject-${DSTAMP}.jar file --> <jar jarfile=""${dist}/${jar_name}"" basedir=""${build}""/> </target> <target name=""clean"" description=""clean up"" > <!-- Delete the ${build} and ${dist} directory trees --> <delete dir=""${build}""/> <delete dir=""${dist}""/> <delete dir=""${reports}""/> <delete dir=""${tmp}""/> </target> <target name=""run-tests"" depends=""compile"" description=""run your test suite"" > <junit printsummary=""yes"" haltonfailure=""no"" showoutput=""yes"" tempdir=""${tmp}""> <classpath> <pathelement path=""${build}""/> <fileset dir=""lib""> <include name=""**/*.jar""/> </fileset> </classpath> <batchtest fork=""yes"" todir=""${reports}/raw/""> <formatter type=""xml""/> <fileset dir=""${tests}""> <include name=""**/*.java""/> <exclude name=""**/All*Tests.java""/> </fileset> </batchtest> </junit> </target> <target name =""test"" depends=""run-tests""> <junitreport todir=""${reports}""> <fileset dir=""${reports}/raw/""> <include name=""TEST-*.xml""/> </fileset> <report format=""frames"" todir=""${reports}\html\""/> </junitreport> </target> <target name =""run"" depends="""" description=""if this project can be run run it"" > </target> <!-- supporting targets --> <target name=""init"" description=""initialize the build environment"" > <!-- Create the time stamp --> <tstamp/> <!-- Create directory structures --> <mkdir dir=""${build}""/> <mkdir dir=""${lib}""/> <mkdir dir=""${dist}/lib""/> <mkdir dir=""${reports}""/> <mkdir dir=""${reports}/raw/""/> <mkdir dir=""${reports}/html/""/> <mkdir dir=""${tmp}""/> </target> <target name=""all"" depends=""clean test""> </target> I give up and hope that someone can shine a light on my problem. Thanks a lot! You haven't got any Ant target which builds the test classes. You could do this as part of the compile target if you wanted: <target name=""compile"" depends=""init"" description=""compile the source code "" > <javac srcdir=""${src}"" destdir=""${build}""> <classpath> <fileset dir=""lib""> <include name=""**/*.jar""/> </fileset> </classpath> </javac> <javac srcdir=""${tests}"" destdir=""${build}""> <classpath> <pathelement path=""${build}""/> <fileset dir=""lib""> <include name=""**/*.jar""/> </fileset> </classpath> </javac> </target> Thanks! That was it. I hadn't noticed that when I modified the script their tests were mixed in with the source. :)"
850,A,"How to output more than one string in junit I have a list of messages which I compare with a given count. When the count fails then I want to output all messages found so I know which message is missing or superfluous. Currently I use:  import scala.collection.JavaConversions._ def Assert_Messages ( expected : Int actual : java.util.List [String]) { if (expected != 0 && actual.size == 0) { junit.framework.Assert.fail (""An expected error message was not reported."") } else if (expected != actual.size) { actual foreach (junit.framework.Assert.fail (_)) } // if } // Assert_Messages But this will only output the first message as junit.framework.Assert.fail does not return. Has anybody got an idea for me which does not involve an ugly StringBuffer? JUnit is set as the test must run on Android. Thanks for any help. I look forward to learning something new and nifty. You can try junit.framework.Assert.fail(actual.mkString(""; "")). Replace ""; "" with whatever you want to use as separator; if you want to be able to reconstruct the original sequence of strings be sure to escape characters that collide with your separating string e.g. with a backslash or something. Some side remarks: it may not matter much in this particular case but instead of checking someCollection.size == 0 it is good practice to test for someCollection.isEmpty instead. On a Scala List size is an O(n) operation if n is the size of your list; isEmpty is O(1). You don't need to explicitly write the scala. prefix when you import stuff from within the scala package (unless there are ambiguities) so import collection.JavaConversions._ will do nicely."
851,A,"Why isn't my @BeforeClass method running? I have the following code:  @BeforeClass public static void setUpOnce() throws InterruptedException { fail(""LOL""); } And various other methods that are either @Before @After @Test or @AfterClass methods. The test doesn't fail on start up as it seems it should. Can someone help me please? I have JUnit 4.5 The method is failing in an immediate call to setUp() which is annotated as @before. Class def is : public class myTests extends TestCase { Can you confirm what version of junit you have in your classpath? I have JUnit 4.5 in classpath do NOT extend TestCase AND use annotations at the same time! If you need to create a test suite with annotations use the RunWith annotation like: @RunWith(Suite.class) @Suite.SuiteClasses({ MyTests.class OtherTest.class }) public class AllTests { // empty } public class MyTests { // no extends here @BeforeClass public static void setUpOnce() throws InterruptedException { ... @Test ... (by convention: class names with uppercase letter) Yes this is a better option. Didn't know about these @Suite and @RunWith annotations. +1 and thanks for your inputs. thanks. I had a hard time to find it last week.. kind of just in time Thank you Carlos works fine now. Also thanks for all your help Vinegar. just had the same problem and this solved it for me. Thanks! @CarlosHeuberger i'm extending a class which has methods like `logins` and `sessions` and I need to run @BeforeClass annotated method also. Is that possible? @yashhy - sorry this should be asked as a question not as a comment.  the method must be static and not directly call fail (otherwise the other methods won't be executed). The following class shows all the standard JUnit 4 method types: public class Sample { @BeforeClass public static void beforeClass() { System.out.println(""@BeforeClass""); } @Before public void before() { System.out.println(""@Before""); } @Test public void test() { System.out.println(""@Test""); } @After public void after() { System.out.println(""@After""); } @AfterClass public static void afterClass() { System.out.println(""@AfterClass""); } } and the ouput is (not surprisingly): @BeforeClass @Before @Test @After @AfterClass You are correct thanks but did not start working once i changed to static. Show us the class definition. It might be the case that your class is a subclass of someother and that parent class may have some method annotated as @BeforeClass and that method of the parent class is ending up with some errors and causing the halt. Otherwise try to add a normal test method. The method is failing in an immediate call to setUp() which is annotated as @before. Class def is : public class myTests extends TestCase { Are you having any @Test methods? Yes as question states."
852,A,"Java: how to ""restart"" a static class? I have a static class (Foo) and a main class (Main) See Main.java: public class Main { public static void main(String[] args) { System.out.println(Foo.i); // 0 Foo.i++; System.out.println(Foo.i); // 1 // restart Foo here System.out.println(Foo.i); // 1 again...I need 0 } } See Foo.java: public class Foo { public static int i = 0; } Is there any way to restart or reset a static class? Note: I need this because I'm testing a static class with jUnit and I need to clean parameters before second test. EDIT ALMOST SOLUTION: Using StanMax answer I can to this: Main.java public class Main { public static void main(String[] args) throws Exception { test(); test(); } public static void test() throws Exception { System.out.println(""\ntest()""); MyClassLoader myClassLoader = new MyClassLoader(); Class<?> fooClass = myClassLoader.loadClass(Foo.class.getCanonicalName()); Object foo = fooClass.newInstance(); System.out.println(""Checking classloader: "" + foo.getClass().getClassLoader()); System.out.println(""GC called!""); System.gc(); } } MyClassLoader.java public class MyClassLoader { private URLClassLoader urlClassLoader; public MyClassLoader() { try { URL url = new File(System.getProperty(""user.dir"") + ""/bin/"").toURL(); URL[] urlArray = {url}; urlClassLoader = new URLClassLoader(urlArray null); } catch (Exception e) { } } public Class<?> loadClass(String name) { try { return (Class<?>) urlClassLoader.loadClass(name); } catch (Exception e) { } return null; } @Override protected void finalize() throws Throwable { System.out.println(""MyClassLoader - End.""); } } Foo.java public class Foo { public static int i = 0; static { System.out.println(""Foo - BEGIN ---------------------------------""); } public void finalize() throws Throwable { System.out.println(""Foo - End.""); } } OUTPUT test() Foo - BEGIN --------------------------------- Checking classloader: java.net.URLClassLoader@ec160c9 GC called! MyClassLoader - End. Foo - End. test() Foo - BEGIN --------------------------------- Checking classloader: java.net.URLClassLoader@ec3fb9b GC called! MyClassLoader - End. Foo - End. PROBLEM: if I do the cast bellow: Foo foo = (Foo) fooClass.newInstance(); I get error: java.lang.ClassCastException For this particular example just do `Foo.i = 0;` inside your `Main` method. @Adrian has the right answer. Don't do any of this. If necessary create a new interface between your test and the static class. The default implementation of the interface will provide the fields of the static class. For testing you can use EasyMock or just create a new instance of this interface that returns the values you want. Don't mess with the classloader or the garbage collector. You have the ClassCastException because the Foo class loaded by the URLClassLoader is different than the Foo class loaded by the ClassLoader that is running your code (they are not ==). Even if they have the exact same definition you cannot cast one to the other. The only way this sort of trick works is if Foo implements an interface FooInterface that is loaded by a ClassLoader that is shared between the two classloaders in which case you can cast to FooInterface. But that doesn't help you access static fields. NamshubWriter: ok I Did this....but the main problem is because I'm using a dynamic type....I'm getting ""fooClass cannot be resolved to a type"". Maybe something in Refletion API can help.... Avoid static. It is well known that static is not testable and should thus be avoided. For example avoiding static is one of the key motivations behind dependency injection. If you need one instance only at runtime use the singleton pattern instead. And create a new instance for each test run.  Only if you can unload class get it re-loaded as class static code gets executed when class is loaded. But you can just directly modify the value: Foo.i = 0; (or create equivalent method for doing it esp. if static member is not public) Wow Stax! But how can I unload a class? (thanks!) No easy way; only works if you actually control ClassLoader that was used to load the class (which you don't unless you actually constructed it). But if you do you just create a new ClassLoader load class using it. This will NOT affect existing instances of the ""old"" class just creates a new Class definition.  Create a static method that sets the class variables to their initial values then call it when you need it. I have a big class and don't want to refactor everthing to clean parameters...of couse if I can't reinitilize a Class I'll have to do this... Tks! That's the most efficient way."
853,A,"JUnitCore run() method is not implicitly calling setUp() before each test I am creating a custom test runner for JUnit test cases using JUnitCore's run(junit.framework.Test test) and passing in ClassName.suite(). My tests run however the results returned are null. It seems that objects are not being initialized in the setUp() method cause setUp() apparently is never called as it should even with the @Before annotation. The tests are successful if I instantiate each object required within each test method. This approach however is tedious and defeats the purpose of having a test class. Is this behavior normal? Are there better test runners out there for JUnit that reflect the same behavior as the test runner in Eclipse? Thanks. Here is the code for the runner: public class TestRunner { Result res = new Result(); String results = new String(); JUnitCore runner = new JUnitCore(); protected TestRunner() { } public String runReport(Test input) { System.out.println(input.toString()); res = runner.run(input); String testClass = ""Class: ""; String testFailCt = ""Failure Count: ""; String testFalures = ""Failures: ""; String testRunCt = ""Runs: ""; String testRunTm = ""Run Time: ""; String testSuccess = ""Success: ""; String newln = ""\n""; results += testClass + input.getClass() + newln; results += testFailCt + res.getFailureCount() + newln; results += testFalures + newln; List<Failure> failures = res.getFailures(); int i = 0; for (Failure x: failures) { i++; results += i +"": "" + x + newln; } results += testRunCt + res.getRunCount() + newln; results += testRunTm + res.getRunTime() + newln; results += testSuccess + res.wasSuccessful() + newln; return results; } } Here is how the runReport() method is being called from another class: runner.runReport(TestClassName.suite()); What should I pass into run() so that setUp() will be implicitly called before each test? I know passing in the suite will not do so. Therefore I just altered my test cases so that all necessary objects are instantiated within each test. could you show some code? It is easier to see what you mean or to see typos which sometimes are the cause of the problem. ... setUp() apparently is never called as it should even with the @Before annotation. JUnit version 4 has annotation support - I think junit.framework indicates that you are using Version 3. If you are running JUnit 4 tests using a JUnit 3 TestRunner you might find these articles of interest: JUnit Test Runner that creates tests just before running them JUnit FAQ - see ""Writing Tests"" para. 3. An early look at JUnit 4 Good luck! Thanks for the answer and for the links I will definitely look into it."
854,A,"Ant classpath and junit.jar I have the an build.xml that allows me to run junit tests. Here is the relevant part: <path id=""JUnit 4.libraryclasspath""> <pathelement location=""../../../../../eclipse/plugins/org.junit4_4.5.0.v20090824/junit.jar""/> <pathelement location=""../../../../../eclipse/plugins/org.hamcrest.core_1.1.0.v20090501071000.jar""/> </path> <path id=""MyProject.classpath""> <pathelement location=""bin""/> <path refid=""JUnit 4.libraryclasspath""/> </path> <target name=""run_unit_tests"" depends=""build""> <mkdir dir=""${junit.output.dir}""/> <junit printsummary=""yes"" haltonfailure=""no""> <classpath refid=""MyProject.classpath"" /> <formatter type=""xml""/> <batchtest todir=""${junit.output.dir}""> <fileset dir=""${src}""> <include name=""**/*Test*.java""/> </fileset> </batchtest> </junit> </target> If I replace the line: <pathelement location=""../../../../../eclipse/plugins/org.junit4_4.5.0.v20090824/junit.jar""/> with <pathelement location=""${eclipse.home}/plugins/org.junit4_4.5.0.v20090824/junit.jar""/> The change breaks the classpath. I get the following error: The <classpath> for <junit> must include junit.jar if not in Ant's own classpath As far as I understand the location attribute should hold the same value in both cases. So what can be the reason? As a side question this build file will not work on an environment with different junit version (the path will break). Is it possible to add a ""general"" path to junit.jar? Is eclipse.home expanded properly? You can always symlink your jar to another location if you are on linux. This way you could symlink it to a path that does not include the version. For example:  ln -s plugins/org.junit4_4.5.0.v20090824/junit.jar /wherever/junit.jar You may want to have a libraries folder outside of the eclipse subdirectories. You could also consider using ivy or maven to manage your dependencies. I'm not sure why your above example fails but the error message implies that junit.jar is not found. Are you sure that eclipse.home is set? You could echo it with:  <echo message=""${eclipse.home}"">  The JUnit library resides wherever you tell it to reside. Personally I would forget entirely about linking against the jar files shipped with Eclipse and instead download the jars directly. If I have a project say at path /project then I would try to put the dependency somewhere in that hierarchy like at /project/test/lib/junit.jar. If my ant build file is then /project/build.xml then it's as simple as adding ./test/lib/junit.jar to the JUnit classpath. Trying to reference an arbitrary location on your machine is fragile (remember Eclipse could be installed anywhere) particularly when using relative paths (since your project contents could also be stored anywhere)."
855,A,"JUnit Rule TemporaryFolder I'm creating a TemporaryFolder using the @Rule annotation in JUnit 4.7. I've tried to create a new folder that is a child of the temp folder using tempFolder.newFolder(""someFolder"") in the @Before (setup) method of my test. It seems as though the temporary folder gets initialized after the setup method runs meaning I can't use the temporary folder in the setup method. Is this correct (and predictable) behavior? This is a problem in Junit 4.7. If you upgrade a newer Junit (for example 4.8.1) all @Rule will have been run when you enter the @Before method:s. A related bug report is this: http://github.com/KentBeck/junit/issuesearch?state=closed&q=rule#issue/79 thanks. ill have to upgrade. I would not that even with JUnit 4.8.2 that on Windows 7 the TemporaryFolder rule doesn't work correctly. Looks like it's a permissions issue.  This works as well. EDIT if in the @Before method it looks like myfolder.create() needs called. And this is probably bad practice since the javadoc says not to call TemporaryFolder.create(). 2nd Edit Looks like you have to call the method to create the temp directories if you don't want them in the @Test methods. Also make sure you close any files you open in the temp directory or they won't be automatically deleted. <imports excluded> public class MyTest { @Rule public TemporaryFolder myfolder = new TemporaryFolder(); private File otherFolder; private File normalFolder; private File file; public void createDirs() throws Exception { File tempFolder = myfolder.newFolder(""folder""); File normalFolder = new File(tempFolder ""normal""); normalFolder.mkdir(); File file = new File(normalFolder ""file.txt""); PrintWriter out = new PrintWriter(file); out.println(""hello world""); out.flush(); out.close(); } @Test public void testSomething() { createDirs(); .... } } While this does create a new folder tempFolder is not actually in a temp location (it is in your working directory) because myFolder has not been set to a temporary location. It will not get cleaned up either. This leaves the directories in the $TEMP or %TEMP% directory so still looking for real answer."
856,A,jUnit: How to determine level of code coverage? How can i determine what percentage of my methods (and code) are covered by jUnit tests? I am assuming there is a more sophisticated way then simply counting ... and 1 and 2 and .. I specifically wonder how will such counting be handled when single method is covered by 'n' tests. this presentation points to several tools you can use for the purpose.  I would suggest to go for cobertura for code coverage. It gives detailed information and can give you line by line coverage as well as branch coverage.  I've used EclEmma very successfully to cover JUnit test runs. And its free.
857,A,What GUI should I run with JUnit(similar to NUnit gui) What GUI should use to run my JUnit tests and how exactly do I do that? My entire background is in .NET so I'm used to just firing up my NUnit gui and running my unit tests. If the lights are green I'm clean. Now I have to write some Java code and want to run something similar using JUnit. The JUnit documentation is nice and clear about adding the attributes necessary to create tests but its pretty lean on how to fire up a runner and see the results of those tests. I'm very sad that 'eclipse' seems to be the only answer. There are many test situations in which setting up a full development environment is overkill and non-helpful. If you want a standalone test runner (not the build-in IDE one) then for Junit3 you can use junit.textui.TestRunner %your_class% - command line based runner junit.swingui.TestRunner [%your_class%] - runner with user interface (swing-powered) For Junit4 the UI-powered runners were removed and so far I haven't found a convenient solution to run new Junit4 tests on old swing-powered runner without additional libraries. But you can use JUnit 4 Extensions that provides a workaround to use junit.swingui.TestRunner. More here  Why you need a GUI runner? Can't you just run the tests from the IDE itself? In .Net we have TestDriven.net in Java there must be something equivalent. You can check out IntelliJ IDEA it has the unit testing support built-in.  Eclipse is by-far the best I've used. Couple JUnit with a code coverage plug-in and Eclipse will probably be the best unit-tester.  JUnit stopped having graphical runners following the release of JUnit 4. If you do have an earlier version of JUnit you can use a graphical test runner by entering on the command line[1]:  java junit.swingui.TestRunner [optional TestClass] With the optional test class the specified tests will run straight away. Without it you can enter the class into the GUI. The benefits of running your tests this way is that you don't have the overhead of an entire IDE (if you're not already running one). However if you're already working in an IDE such as Eclipse the integration is excellent and is a lot less hassle to get the test running. If you do have JUnit 4 and really don't want to use an IDE to run the tests or want textual feedback you can run the text UI test runner. In a similar vein as earlier this can be done by entering on the command line[1]:  java junit.textui.TestRunner [TestClass] Though in this case the TestClass is not optional for obvious reasons. [1] assuming you're in the correct working directory and the classpath has been setup which may be out of scope for this answer  There's a standalone JUnit runner that has a UI but I recommend using one of the builtin test runners in the Java IDEs (Eclipse Netbeans and IntelliJ all have good ones). They all support JUnit and most support TestNG as well. What are names of some standalone JUnit runners? I've tried searching for it but come up short.
858,A,"Creating a assertClass() method in JUnit I'm creating a test platform for a protocol project based on Apache MINA. In MINA when you receive packets the messageReceived() method gets an Object. Ideally I'd like to use a JUnit method assertClass() however it doesn't exist. I'm playing around trying to work out what is the closest I can get. I'm trying to find something similar to instanceof. Currently I have: public void assertClass(String msg Class expected Object given) { if(!expected.isInstance(given)) Assert.fail(msg); } To call this: assertClass(""Packet type is correct"" SomePacket.class receivedPacket); This works without issue however in experimenting and playing with this my interest was peaked by the instanceof operator. if (receivedPacket instanceof SomePacket) { .. } How is instanceof able to use SomePacket to reference the object at hand? It's not an instance of an object its not a class what is it?! Once establishing what type SomePacket is at that point is it possible to extend my assertClass() to not have to include the SomePacket.class argument instead favouring SomePacket? SomePacket is a type like this:  SomePacket something = null; You cannot reference it directly at runtime in the way that you want. The class object is the only way to go that I know of.  The argument to instanceof is a class/interface name and it is implemented directly as a JVM instruction. As such it is pretty efficient. http://java.sun.com/docs/books/jvms/second_edition/html/Instructions2.doc6.html#instanceof One advantage of instanceof instead of comparing the classes is that instanceof will still work if the class has been proxied. It will also work if the object being checked is null. Thanks! I guess it is as I thought - I cannot duplicate a similar result and must call the .getClass() and compare it that way.  Take a look at the Hamcrest matchers now included in JUnit. What you want is something like: import static org.junit.Assert.assertThat; import static org.hamcrest.CoreMatchers.instanceOf; assertThat(receivedPacket instanceOf(SomePacket.class)); That does everything you need including a decent error message upon assertion failure. Answers a different question - but it's an answer :) Thanks Steve. it should be `assertThat(receivedPacket instanceOf(SomePacket.class));` (notice the `.class`) Corrected thanks  FYI you could check if(given == null || given.getClass() != expected) to check if the class of given is exactly the class expected not just assignment-compatible. I'd call this one assertClass and your original suggestion assertInstance or assertInstanceOf. In response to the comment: My answer was meant as a general tip not as an answer to your original question. I added an ""FYI"" to make that clearer ;) Depending on what exactly you want both ways may be reasonable. In the following example public class Base {} public interface Ifc {} public class A{} extends Base implements Ifc public class B{} extends Base the behavior of the methods above would be like this A a=new A(); B b=new B(); assertClass(A.class a); assertClass(Base.class a); // fails a.getClass() != Base.class assertClass(Ifc.class a); // fails a.getClass() != Ifc.class assertClass(B.class b); assertClass(Base.class b); // fails b.getClass() != Base.class assertClass(Ifc.class b); // fails b.getClass() != Ifc.class assertInstance(A.class a); assertInstance(Base.class a); // works assertInstance(Ifc.class a); // works assertInstance(B.class b); assertInstance(Base.class b); // works assertInstance(Ifc.class b); // fails B does not implement Ifc Most likely you'll want to check with your method. But if you want to validate the exact class e.g. to make sure that it's not proxied or that it's not an extending class my suggestion would be the way to do that. The accuracy of the name of the method isn't what I'm trying to get an answer to. Perhaps my list of questions at the end were not clear. I'm more interested in how instanceof can reference the object as it does. I'm beginning to think the JVM does something like this: if (o instanceof Obj) == if (o.getClass().equals(Obj.class)) ? I updated my answer. if (o instanceof Obj) != if (o.getClass().equals(Obj.class))! Normally (special ClassLoader handling aside) you can safely compare with (o != null && Obj.class == o.getClass()) btw.  assertThat(expected).isInstanceOf(given.class); Using FEST Assertions 2.0 is a Java library Fixtures for Easy Software Testing found here: https://github.com/alexruiz/fest-assert-2.x/wiki  Why don't you just use assertEquals? assertEquals(SomePacket.class given.getClass()); The question is more of an interest - how does instanceof reference the class directly as it does? FWIW I probably will end up having to use assertEquals(). The main problem is the potential for error. It is easy to forget to .getClass(). Writing multiple methods with one which takes an Object and calls .getClass() will clearly solve this niggle though."
859,A,"problem compiling a junit test class with ant I am having issues integrating junit with a working ant build.xml file. My test class is in the same directory as my source classes. As I am learning how to use ant I simply want to compile all the source and the test classes. I am using eclipse and the junit test classes work fine when execute through eclipse. Which means the classpath is set up correctly (at least from eclipse's point of view) with junit.jar and ant-junit-1.7.0.jar although I am not sure if the latter jar is absolutely necessary. My folder structure is: src/code/MyClass.java src/code/MyClassTest.java and the ant file contain only one target just to compile MyClass and MyClassTest I do not include any junit tasks at the moment and do not mind to have the build files in the same directory as well: <target name=""compile"" > <javac srcdir=""src/"" /> </target> Ant worked fine until I added MyClassTest.java (Junit with annotations) in my folder. The output is: [javac] C:\....\src\MyClassTest.java:3: package org.junit does not exist cannot find symbol My thinking is that somehow Ant can't find the junit libraries. Since I do not specify a classpath I was assuming that Ant would look at the same location as the source files to find to find what it needs...How can I tell Ant that the junit jars are right there? Any ideas are really appreciated. Regards You don't have to use the absolute path as you did in your comment. It can be relative but it is relative to the execution location of ant (or the basedir attribute on the project element). And don't put in a forward slash to start the path or it will look at root and treat it as an absolute path. Actually I didn't have any luck until I used fetch.xml per apache website to pull jar dependencies. Then it worked fine for me. This is what I did: On command line I went to ant home directory. I typed “ant -f fetch.xml -Ddest=system” (stores jars in Ant's lib directory) It would not run because it wanted me to have maven-artifact-ant-2.0.4-dep.jar in ant/lib folder. I did a google search for this jar and found it at jarfinder.com. I downloaded it and put it in the Ant Lib folder as that is where it was looking for this file. It then pulled all the files it needed and I was set. Then in command line I went back to my workspace folder and reset it to build.xml ant -buildfile build.xml I then did ant clean ant compile (build successful) and ant run and all worked - Build Successful. JUnit is built in with Eclipse but with ANT I tried several solutions from apache and ant documentation but this is what worked for me. (I am on Windows 7) -Erik  Yes you do have to specify a CLASSPATH. It's been a long time since I last used Eclipse but I believe you have to right click on the project root choose ""Properties"" and add the JUnit JAR to the CLASSPATH. Ant has to know the CLASSPATH when you use it to build. Have a look at their <path> stuff. Here's how I do it:  <path id=""production.class.path""> <pathelement location=""${production.classes}""/> <pathelement location=""${production.resources}""/> <fileset dir=""${production.lib}""> <include name=""**/*.jar""/> <exclude name=""**/junit*.jar""/> <exclude name=""**/*test*.jar""/> </fileset> </path> <path id=""test.class.path""> <path refid=""production.class.path""/> <pathelement location=""${test.classes}""/> <pathelement location=""${test.resources}""/> <fileset dir=""${test.lib}""> <include name=""**/junit*.jar""/> <include name=""**/*test*.jar""/> </fileset> </path>  Jars have to be specified by name not by directory. Use: <javac srcdir=""src/"" classpath=""pathtojar/junit.jar""/> Where ""pathtojar"" is the path containing the junit jar. Thanks Marco and Duffy. Even though according to the eclipse package explorer view the junit.jar is referenced right under the root project name I had to point to the jar directly via its absolute location.. I mean classpath=""/junit.jar"" just didn't work I had to say classpath=""C:\Users\Desktop\Work\Eclipse\plugins\org.junit4_4.3.1\junit.jar"" Thanks again!  You need to define a classpath and use it with the javac task. See: http://ant.apache.org/manual/using.html for some basics."
860,A,"Unit Testing an EJB I'm looking for a way to apply TDD to Session Beans. can anyone provide advices and links on how to unit test them ? how to use JUnit to do so ? P.S : I'm new to Test Driven Development and Session Beans. I'm using EJB v2. You don't say which version of EJB you're using. If it's EJB v3 check out Ejb3Unit. From the website: Ejb3Unit is a JUnit extention and can execute automated standalone junit tests for all EJB 3.0 conform Java EE projects. The out of container test approach leads to short build-test-cycles because no container deployment is necessary anymore. However I would advocate separating out functionality from the EJB-specifics. This will allow you to test complex functionality outside the container and without using frameworks such as the above. The majority of your tests would be testing POJOs (plain old Java objects) and relatively few would focus on your persistence framework testing. EDIT: So if you're using EJB v2 then obviously ignore the first point. The second point remains valid however.  I'm currently using apache openejb as an embedded container for unit tests. Although this is an EJB3 / JPA project it should work the same for EJB2. To bootstrap the container in your tests you just have to create an InitialContext object that you can later use to lookup EJBs and DataSources: Properties props = new Properties(); props.put(Context.INITIAL_CONTEXT_FACTORY ""org.apache.openejb.client.LocalInitialContextFactory""); // a DataSource named ""mysql"" props.put(""mysql"" ""new://Resource?type=DataSource""); props.put(""mysql.JdbcDriver"" ""com.mysql.jdbc.Driver""); props.put(""mysql.JdbcUrl"" ""jdbc:mysql://localhost:3306""); props.put(""mysql.JtaManaged"" ""true""); props.put(""mysql.DefaultAutoCommit"" ""false""); props.put(""mysql.UserName"" ""root""); props.put(""mysql.Password"" ""root""); Context context = new InitialContext(props); LocalInterface local = (LocalInterface)context.lookup(localInterfaceName + ""BeanLocal""); DataSource ds = (DataSource)context.lookup(""java:openejb/Resource/mysql""); Edit: There is some more documentation in the 'Testing Techniques' section at http://openejb.apache.org/3.0/index.html. can you provide me with a link to a good tutorial on this ? seems promising.  I'm assuming you are talking about EJB2.x Session Beans. For these kind of animals what I like to do is: Use the Session Bean as a wrapper that just delegates logic to a POJO that you can test easily outside a container. Outside container testing is better faster easier etc but won't cover things such as deployment descriptor validation - and/or - Use something like Cactus for in-container testing (check the Howto EJB documentation) - and/or - Build and deploy your EJB module with Cargo for integration testing. I tried to setup Cactus for use but I don't really get it ! I feel like it's too difficult to setup. I'm using Eclipse and NetBeans. +1 - For your last two suggestions I haven't thought about Cactus in years. :) Actually I didn't use Cactus for years :) Cactus is not that hard. Did you try google: http://www.google.com/search?q=ejb+testing+cactus ? There are plenty of simple samples.  Mockrunner can be used in conjunction with MockEJB to write tests for EJB based applications. Have a look at this http://mockrunner.sourceforge.net/examplesejb.html"
861,A,"maven test behind proxy (squid) I'm having a problem with maven behind a squid proxy server. I have a module of my system that depends external communication with a remote webservice. I have my maven proxy configurations under ~/.m2/settings.xml but apparently these information are been used just for dependencies downloads. When I run 'mvn test' these configurations aren't used on command line execution call. This is the instruction echoed on console: ${JAVA_HOME}/bin/java -jar /tmp/surefirebooter4156656684210940660.jar /tmp/surefire2646147996861949548tmp /tmp/surefire3498083351425809633tmp There's a way to pass arguments to JVM during tests and other maven method executions? []'s And Past have you simply tried setting http_proxy env variable? I have this variable set. But this is an 8nix environment sensitive context variable. Normally Java environment doesn't considers this variable to define -Dhttp.proxy* args. I want to use some definition as ~/.m2/settings.xml Based on comments left for this post I want to complement the answer: To fix some arguments on pom.xml we can configure -DforkMode=never directly on surefire plugin configuration. i.e. <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.5</version> <configuration> <argLine>-DforkMode=never</argLine> </configuration> </plugin> The solution based on -DforkMode was suggested by this post referenced by @johan-sjoberg on comments posted here Thanks for all by helping. And Past  Perhaps this can be of interest to you: How do I configure proxy settings for Java. Alternatively you can try these startup parameters: -Dhttp.proxyHost=url -Dhttp.proxyPort=port -Dhttp.proxyUser=user -Dhttp.proxyPassword=pass [EDIT] These properties can also be set in MAVEN_OPTS. This post describes how this would work for the test-profile. I have thios configurations on my JRE jvmargs defined on Eclipse environment. But MVN ignore this. The same applies to command line environment. How can I inject these jvmargs on mvn context independently of runtime environment. Until now dependencies are supported by ~/.m2/settings.xml. Each user have yours. And [this](http://maven.apache.org/settings.html#Proxies) is what you're using? Have you tried setting the proxy information in `MAVEN_OPTS`? Also have a look at [this](http://stackoverflow.com/questions/824019/maven-2-1-0-not-passing-on-system-properties-to-java-virtual-machine) issue. This is exactly the configuration defined on ~/.m2/settings.xml. I'll check MAVEN_OPTS and report here. Thanks by helping! I used the argument explained on post that you suggested and it's working! Using ""-DforkMode=never"" i.e. 'mvn test -DforkMode=never' Surefire doesn't fork the execution from parent jvm process. Thanks! PS: how can I check your comment as Right Response? Glad I could help. You can only accept post."
862,A,"How to create a database deadlock using jdbc and JUNIT I am trying to create a database deadlock and I am using JUnit. I have two concurrent tests running which are both updating the same row in a table over and over again in a loop. My idea is that you update say row A in Table A and then row B in Table B over and over again in one test. Then at the same time you update row B table B and then row A Table A over and over again. From my understanding this should eventually result in a deadlock. Here is the code For the first test. public static void testEditCC() { try{ int rows = 0; int counter = 0; int large=10000000; Connection c=DataBase.getConnection(); while(counter<large) { int pid = 87855; int cCode = 655; String newCountry=""Egypt""; int bpl = 0; stmt = c.createStatement(); rows = stmt.executeUpdate(""UPDATE main "" + //create lock on main table ""SET BPL=""+cCode+ ""WHERE ID=""+pid); rows = stmt.executeUpdate(""UPDATE BPL SET DESCRIPTION='SomeWhere' WHERE ID=602""); //create lock on bpl table counter++; } assertTrue(rows == 1); //rows = stmt.executeUpdate(""Insert into BPL (ID DESCRIPTION) VALUES (""+cCode+"" '""+newCountry+""')""); } catch(SQLException ex) { ex.printStackTrace(); //ex.getMessage(); } } And here is the code for the second test. public static void testEditCC() { try{ int rows = 0; int counter = 0; int large=10000000; Connection c=DataBase.getConnection(); while(counter<large) { int pid = 87855; int cCode = 655; String newCountry=""Jordan""; int bpl = 0; stmt = c.createStatement(); //stmt.close(); rows = stmt.executeUpdate(""UPDATE BPL SET DESCRIPTION='SomeWhere' WHERE ID=602""); //create lock on bpl table rows = stmt.executeUpdate(""UPDATE main "" + //create lock on main table ""SET BPL=""+cCode+ ""WHERE ID=""+pid); counter++; } assertTrue(rows == 1); //rows = stmt.executeUpdate(""Insert into BPL (ID DESCRIPTION) VALUES (""+cCode+"" '""+newCountry+""')""); } catch(SQLException ex) { ex.printStackTrace(); } } I am running these two separate JUnit tests at the same time and am connecting to an apache Derby database that I am running in network mode within Eclipse. Can anyone help me figure out why a deadlock is not occurring? Perhaps I am using JUnit wrong. How are you running two test methods at the same time? JUnit is executing test methods sequentially. I have two JUnit test cases and run one then switch over to the other one and run that one and it shows both of them running. I see. What transaction isolation level are you using? That is something that I didn't set. I am not sure what that means but I remember the Professor telling us to not change the timeout for the transactions. I am not sure if that is related. You should check the transaction isolation level as it determines whether or not the DB locks rows touched by a transaction. If the isolation level is too low no locking occurs so no deadlock either. Update: according to this page the default tx isolation level for Derby is read committed which should be OK. The page is worth reading btw as it explains tx isolation and its different levels and what problems it solves. Next question then: what is DataBase in your code? This seems to be a nonstandard way to get a connection. Update2: I think I got it. Quote from the API doc: Note: By default a Connection object is in auto-commit mode which means that it automatically commits changes after executing each statement. If auto-commit mode has been disabled the method commit must be called explicitly in order to commit changes; otherwise database changes will not be saved. In other words rows are not locked because your effective transactions last only for the lifetime of individual updates. You should switch off autocommit before starting to work with your connection: Connection c=DataBase.getConnection(); c.setAutoCommit(false); Ok so it must be the transactions I am doing are not the type that cause deadlocks. I am not sure what else to try so I will have to do some more reading. I have a class that I called 'DataBase' that has all my methods in it. One of them is getConnection. It just uses the DriverManager.getConnection(url) method from JDBC. @Isawpalmetto OK I think I got it - see my last update. There we go now I am getting them thanks a lot. Update: What I am getting is actually just a lock timeout. A deadlock is not happening and I am pretty sure it is not the code because I have tried various deadlock examples from the internet. It must be some settings with JUnit. @Isawpalmetto Or the DB. I guess the DB detects that the lock can not be acquired and interrupts the transaction after a certain amount of time passed. I observed the same some time ago. @Isawpalmetto And this is indeed the case with Derby - see http://db.apache.org/derby/docs/10.0/manuals/develop/develop75.html @Isawpalmetto One more note: in order to achieve a proper deadlock (where thread A and B wait for each other indefinitely) you should ensure that test 1 gets to lock row 1 and test 2 locks row 2 first. With your current setup it is IMHO unlikely: what probably happens is that one of the connections gets _both_ locks and then runs on happily while the other blocks. If you start both tests _exactly_ the same time you have _some_ chance of achieving deadlock. To increase your chances you could insert a sleep period (of e.g. 10 seconds) after the first `executeUpdate` in both tests. I actually changed the tests from the one that I had posted and was still having the same problem. Then I was thinking that maybe the timeOut lock I was getting was because the updates were happening too fast so that the other thread couldn't even get a lock on the row. I added the sleep period and now I am getting deadlocks. Thanks"
863,A,Verify XHTML is valid in a JUnit test? I have a Spring controller that returns XHTML what's the easiest way to setup a JUnit test that verifies that the XHTML is valid? I'd also like to verify certain elements are present. The only way is to parse the output. I've used dom4j in my jUnit tests. Then you can use Xpath or DOM to extract the elements you want and test them. If you're not already using a parser it can take a little messing around to get it going. But once you've got it it's very handy and you can write all sorts of great tests. If using parsers is new to you perhaps take a look at the dom4j quickstart guide.
864,A,"How to create a JUnit 4 test suite to pass when expected exceptions are thrown I am writing a JUnit 4 test suite which runs some tests that check if an exception has been thrown. On their own my tests look somewhat like this: @RunWith(value=BlockJUnit4ClassRunner.class) public class CreateCommandsExceptionsTest extends TestCase { Statistics statistics; @Before public void setUp(){ ... } ... @Test (expected=StatsArrayTooShortException.class) public void testStatsArrayTooShortException(){ ... } The tests run fine on their own but when I attempt to put them in a test suite they fail because the exception that I am testing for is being thrown. My test suite looks like: @RunWith(value=BlockJUnit4ClassRunner.class) public class UnitTestSuite { public static testSuite(){ TestSuite suite = new TestSuite(""Test for unitTests""); suite.addTestSuite(StatisticsTest.class); ... return suite; } } If anyone could tell me how I should be setting up my test suite so that I can pass a test when I catch an expected exception I would appreciate it. You are using the wrong syntax for Suites in Junit 4. This is the right way: @RunWith(Suite.class) @Suite.SuiteClasses({ JunitTest1.class JunitTest2.class }) public class JunitTest5 { } That works! Thank you! Could you recommend any good documentation for JUnit? I can't seem to find any http://junit.sourceforge.net/ - junit4 cookbook http://junit.org - main site for junit"
865,A,Randomize database row results I've been finding some unit tests that assume that the results of database queries are in a particular order yet the query being run doesn't include an order by clause. I would like to find more of these unit tests so that I can examine whether the test is at fault in its assumptions or the code is at fault in its lack of specifying an order. I'm using java junit spring hibernate dbunit jdbc and postgresql. One idea I had was to intercept the test queries somewhere and if a query does not include an order by clause then capture all the results and return them in a random order. Where would be the easiest place to intercept and check the query? Are there other simple ways of identifying such tests? You could take a look at extending Hibernate's EmptyInterceptor and specifically the onPrepareStatement method. If the sql query passed as the argument doesn't contain an order by clause you could try adding order by random() to it. That looks like it might work. Is my implementation able to be injected in a sessionFactory bean? If you configure your sessionFactory with Spring's LocalSessionFactoryBean I'd try making a test-specific config that adds the interceptor there (entityInterceptor property). Thanks got that working. Now to deal with the errors (eg filtering out select distinct queries)
866,A,"Different Singleton instances with JUnit tests I have a standalone singleton which successfully passes the test. But with a group of tests this fails since once a singleton is defined it does not allow to reset the instance. Any ideas about how to go about this? I assume you have a private static field within your singleton class to store the initialized instance. If you do not want to modify your code you can define a teardown method which run after every test and in this method you set this static field to null via reflection as seen here. -1 IMO this is making a bad situation worse +1 for giving an actual solution. I have no control over 3rd party code which is a singleton and have a need for solutions not advised on how it should be done.  I highly recommend moving away from Singletons as a design pattern and using Singleton as a scope (Dependency Injection). This would simply make your problem go away. But assuming you are stuck in the world of Singletons then you have a few options depending on if you are testing the Singleton or the dependency. If you are testing the dependant item then you can mock the Singleton using PowerMock and JMockIt. See my previous post about mocking Runtime.getRuntime for instructions on how to go about this. If you are testing the Singleton then you need to relax the rules on construction or give the Singleton a ""Reset"" method.  generally beware of singletons most often they are evil bad design and tend to represent big yucky global variables (which is bad for maintenance). still to get tests in place first you can do:   static setInstance(...){ //package visibility or in difficult cases you have to use public instance = ...; }  as said this is more a workaround. so get first tests place but then refactor away from singleton pattern.  Don't use a singleton. Specifically the only difference between a singleton and a global variable is that the singleton tries to enforce a single instance (by making the constructor private for example). Instead make the constructor public and write tests using new instances. In your actual program use getInstance() to get the canonical global instance (or use an IOC container). And remember that singletons are pathological liars. If you're still too comfortable with the idea of a Singleton instead of making the constructor public you can add a public (and static) factory method to create instances in a way that can't be used by accident e.g.: public static MyClass TEST_CreateInstance() { return new MyClass(); }  Spring provides the DirtiesContext annotation for this particular use case where you need new instances of the singleton beans for each testcase. It basically creates a new application context for each testcase/testclass which has this annotation applied.  Singleton instance needs to be passed to SUT by test itself - that way you create singleton (and destroy) for each test. Adopting IoC and mocking framework like Mockito would render this approach almost trivial.  You can add a method to destroy the singleton for example destroyMe(); where you deinitialize everything and set the instance of the singleton to null.  public void destroyMe(){ this.instance = null; //-- other stuff to turn it off. } I will leave synchronization problems though ;) But why do you need to re-initialize your singleton for each test? It should not differ based on the concept of the singleton."
867,A,"Prefix for testing methods in Unit: ""test"" vs ""should"" It is a common practice to prefix the tests method names in JUnit with ""test"". But in the last few years some people changed this to the prefix ""should"". If I want to test the customer creation in a database I would normally name the method ""testCustomerCreation"". However some people would name ""shouldCreateCustomer"". This is a lot of personal taste when I am the only person in the project or when everyone else in the project agrees with me. But when/where this is not the case some divergences or inconsistent mixes starts to shows up. I readed somewhere an article of a guy that named his methods like ""testShouldCreateCustomer"" and for this reason he decided to drop the ""test"" prefix. But in fact he wasn't prefixing with ""test"" he was using ""testShould"" and changed to ""should"". Obviously this did not convinced me. I am personally strongly inclined to stick to ""test"" prefix because the methods names normally starts with verbs in the infinitive form (""get"" ""set"" ""add"" ""remove"" ""clear"" ""send"" ""receive"" ""open"" ""close"" ""read"" ""write"" ""create"" ""list"" ""pop"" ""print"" etc so is ""test""). So prefixing a method name with ""should"" makes it sound really very strange for me looks wrong. So what is the real good reason to use ""should"" instead of ""test""? What are the great advantages and disadvantages? I'd spend less time worrying about silly prefixes and more time worrying about the rest of the test name being descriptive. I would say the 'test' prefix is simply a holdover from the pre annotation days when that was required. I would suggest you simply use meaningful names for your test cases (and that may mean with or without 'test'). I prefer to name the test method so that it is clear what is being tested. i.e. checkNullParameter() runSimpleQuery() runQueryWithBadParam() All the test cases are located in a test directory anyway and all actual tests are annotated so the 'test' prefix is rather redundant. @Benjamin: What about testQueryWithBadParam()? @Robin: I partially agree. Sometimes ""check"" or ""verify"" is better than ""test"" but ""runSimpleQuery"" is not exactly what you does. You don't simply run it you run and asserts that it runned correctly so would be like ""testSimpleQueryRun"". Equally we would have ""testQueryWithBadParam"". I know that this way it is very likely that every method will have the prefix test but it would not be simply a prefix it will be part of the description of what the method does. @Victor - I think @Test testThisThing() is redundant. The annotation already determines the methods purpose is to do a test. I don't think having the same prefix on every method that is already annotated to be a test is very useful whether that prefix is 'test' 'should' ... or anything else. runQueryWithBadParam() - You aren't saying what you expect here though. I'd personally prefer to see shouldThrowBadParamExceptionOnBadParam(). Should also encourages you to describe your expectation...... The annotation (expected=BadParamException) should be clear enough for the simple case. In other cases I let the code show what I expect as there might be several assertions made with regard to multiple bad parameters. For example null or an out of range value might both be contained within the same testcase (bad param). The assertions themselves tell what I am expecting and should be clear to anyone reading the code.  I prefer the test suffix. It's possible that you might have a method with a prefix of should in your project e.g. shouldBuy and your test would then be called testShouldBuy because shouldShouldBuy would just look very strange. I also use the MoreUnit Eclipse plugin which will automatically create a test method prefixed with test when I press Ctrl+U or jump to the test method when I press Ctrl+J. (Although you can configure which prefix it uses.) If you are not consistent with your naming automated tools such as MoreUnit won't be able to help you with your testing.  In the original JUnit test methods had to begin test. A lot of frameworks for other languages copied this convention. Even though it's no longer the case in JUnit and even though other frameworks may be different I think most programmers are still pretty familiar with methods named e.g. testX as being unit tests so I think it's good to stick to the test convention for that reason.  The 'Should' convention is aligned with the behaviour driven development style of testing. I personally really prefer to write tests in this style as it encourages you to write tests that read as specifications and are more aligned with the behaviour of the class or system that you are testing. Where possible I sometimes go one step further and give the test class even more context using it's name: class ANewlyCreatedAccount { shouldHaveAZeroBalance() {} shouldCalculateItsOwnInterest() {} } By naming your classes and thinking about them in this specification style this can give you a lot of guidance on which tests to write and in which order you should write the tests and make them green. Yes 'should' vs 'test' is just a prefix and it's important to be consistent but this question is also about the style and mindset of how you test your code and choose which tests to write. BDD has a ton of value so I suggest reading further and give this a try. Ok you are convincing me. But this means that all the people that simply name test methods that does not even looks like to specification of anything (like ""shouldSaveUserToDatabaseWhenPasswordIsChanged"") are people who simply does not know what they are doing? If the test method particularly has nothing to do with BDD (like a test that guards against a bug that occurred in very peculiar combination of conditions) should it still be prefixed with ""should""? +1 for mentioning that it's about behaviour rather than testing methods  Consistency is more important than being correct on naming issues. If there is any question on a project the technical member responsible for the project should outline the coding practices formally so that issues like this don't kill valuable project time. ""Consistency is more important than being correct"" - story of my company's 1.5 million line ERP software in one sentence. on _naming_ issues. There are certain things worth spending time discussing and finding best practices. Arguing over brace placement capitalization scheme and test prefixes don't fall into that group. Overgeneralizing and poorly applied advice might be more of your software's issue."
868,A,"How can I compare files in a JUnit test case? I want to implement JUnit on a small project I'm working on because I want to learn a little bit about it. The tutorials that I read all make reference to methods that have a particular output. In my case my output are files how can I do this? any simple example? any approach that could help me with this? The files are raw text files that are build by a void private method. How do the methods write to the file? If they get a stream you can just give them your own instead of one that points to a file and then compare it. Whith basic write buffer string inside a web crawler (I press enter but it wrote the message) the code is something like: txtUrlSpecial.write(bigText.charAt(j)); Use junitx.framework.FileAssert class from junit-addons project. Other links: API pom (searchable through http://search.maven.org). One of the methods: assertEquals(java.lang.String message java.io.Reader expected java.io.Reader actual)  If you can't control the method to put the output in a stream then I'd say you need to refactor your code so that the method receives a stream in the parameter (or in the constructor of its class). After that testing is pretty easy - you can just check the stream. Easily testable code usually equals good code. The problem is that actually it doesn't make one single file it makes between 3 and 5 files depending on different things. But because I'm really noob with junit I wanted to try it first with one single file to understand it completely. Thanks =D. @Saikios this is relevant for any number of files :)  You want to get a correct output file for a given set of inputs and setup a test to call your void method with those inputs and then compare your validated output file against whats produced by your method. You need to make sure that you have some way of specifying where your method will output to otherwise your test will be very brittle. private static final File dir = new File(""tmp""); @Before public void setUp() { Assert.assertTrue(""Unable to create "" + dir.getAbsoultePath() dir.exists() || dir.mkDirs()); } @Test public void testXYZ() { final File expected = new File(""xyz.txt""); final File output = new File(dir ""xyz.txt""); TestClass.xyz(output); Assert.assertEquals(FileUtils.readLines(expected) FileUtils.readLines(output)); } Uses commons-io FileUtils for convinience text file comparison. eclipse says that org.junit.internal.runners.TestClass is deprecated =( I like this but why nobody vote for your answer? :S You can always vote for it yourself You should only need to import `junit.framework.Assert` & `org.junit.Test` are you using JUnit 4? I guess the usage of this method must be limited to short files because the output is poorly readable. No information about the line number for example.  After your methods write the file in the unit-test you can read the file and verify whether it is written correctly. Another thing that makes sense is to have your methods split in one that retrieves that data and returns it to the methods that merely writes it to a file. Then you can verify whether the data returned by the first method is fine. And another plausible approach would be to pass an OutputStream to the method that writes the data. In the ""real code"" you can pass a FileOutputStream / FileWriter while in the test-code you can write a mock implementation of OutputStream and check what is being written to it.  Although your question may seem simplistic it does strike to the heart of unit testing one needs to write well formed code that is testable. This is why some experts advise that one should write the unit test first and then the implementing class. In your case I suggest you allow your method to execute and create the file(s) expected following which your unit test(s) can analyse that the files are formed correctly. Thanks I will have it in mind for next time to first make my unit test :D The piece of code that writes the whole file is probably too big and complex. JUnit may show if the code still works (no regression) and this is something. However really good Unit tests also help to find what is broken."
869,A,"Spring Transactional Parameterized Test and Autowiring Is there a way to get a class that extends AbstractTransactionalJUnit4SpringContexts to play nicely with JUnit's own @RunWith(Parameterized.class) so that fields marked as Autowired get wired in properly?  @RunWith(Parameterized.class) public class Foo extends AbstractTransactionalJUnit4SpringContextTests { @Autowired private Bar bar @Parameters public static Collection data() { // return parameters following pattern in // http://junit.org/apidocs/org/junit/runners/Parameterized.html } @Test public void someTest(){ bar.baz() //NullPointerException } } what is `AbstractTransactionalJUnit4SpringContexts` ? Sorry that was a typo. Should be fixed now as well as the "".class"" on Parameterized. See http://jira.springframework.org/browse/SPR-5292 There is a solution. Nice! Thanks that'll be a huge help. I have used this solution. Be careful they define their own Parameters annotation. When the code invokes getParametersMethod(final TestClass testClass) then testClass.getAnnotatedMethods(Parameters.class); will return and empty list. When you remove public static @interface Parameters {} it's starting to work as expected...  No you can't. The superclass has: @RunWith(SpringJUnit4ClassRunner.class) which assures that the tests are run within spring context. If you replace it you are losing this. What comes to my mind as an alternative is to extend SpringJunit4ClassRunner provide your custom functionality there and use it with @RunWith(..). Thus you will have the spring context + your additional functionality. It will call super.createTest(..) and then perform additional stuff on the test. That's disappointing... thanks anyway. @James Kingsbery see my update for an alternative Yeah I came to the same conclusion (must be a good idea then).  You can use a TestContextManager from Spring. In this example I'm using Theories instead of Parameterized. @RunWith(Theories.class) @ContextConfiguration(locations = ""classpath:/spring-context.xml"") public class SeleniumCase { @DataPoints public static WebDriver[] drivers() { return new WebDriver[] { firefoxDriver internetExplorerDriver }; } private TestContextManager testContextManager; @Autowired SomethingDao dao; private static FirefoxDriver firefoxDriver = new FirefoxDriver(); private static InternetExplorerDriver internetExplorerDriver = new InternetExplorerDriver(); @AfterClass public static void tearDown() { firefoxDriver.close(); internetExplorerDriver.close(); } @Before public void setUpStringContext() throws Exception { testContextManager = new TestContextManager(getClass()); testContextManager.prepareTestInstance(this); } @Theory public void testWork(WebDriver driver) { assertNotNull(driver); assertNotNull(dao); } } I found this solution here : How to do Parameterized/Theories tests with Spring this option wouldn't support things like @BeforeTransaction would it?  Inspired by Simon's solution you can use TestContextManager also with Parameterized runner: @RunWith(Parameterized.class) @ContextConfiguration(locations = ""classpath:/spring-context.xml"") public class MyTestClass { @Parameters public static Collection data() { // return parameters following pattern in // http://junit.org/apidocs/org/junit/runners/Parameterized.html } @Before public void setUp() throws Exception { new TestContextManager(getClass()).prepareTestInstance(this); } } Here is full example I am not sure about handling @Transactional in this case. I have tested that @Transactional annotations are not handled by TestContextManager  I've had to handle the transactions programmatically (see http://www.javathinking.com/2011/09/junit-parameterized-test-with-spring-autowiring-and-transactions/): @RunWith(Parameterized.class) @ContextConfiguration(locations = ""classpath*:/testContext.xml"") public class MyTest { @Autowired PlatformTransactionManager transactionManager; private TestContextManager testContextManager; public MyTest (... parameters for test) { // store parameters in instance variables } @Before public void setUpSpringContext() throws Exception { testContextManager = new TestContextManager(getClass()); testContextManager.prepareTestInstance(this); } @Parameterized.Parameters public static Collection<Object[]> generateData() throws Exception { ArrayList list = new ArrayList(); // add data for each test here return list; } @Test public void validDataShouldLoadFully() throws Exception { new TransactionTemplate(transactionManager).execute(new TransactionCallback() { public Object doInTransaction(TransactionStatus status) { status.setRollbackOnly(); try { ... do cool stuff here } catch (Exception e) { throw new RuntimeException(e); } return null; } }); }"
870,A,"More Matchers recorded than the expected - Easymock fails from Maven and not from Eclipse I'm having a strange problem with Easymock 3.0 and JUnit 4.8.2. The problem only occurs when executing the tests from Maven and not from Eclipse. This is the unit test (very simple): ... protected ValueExtractorRetriever mockedRetriever; ... @Before public void before() { mockedRetriever = createStrictMock(ValueExtractorRetriever.class); } @After public void after() { reset(mockedRetriever); } @Test public void testNullValueExtractor() { expect(mockedRetriever.retrieve(""PROP"")).andReturn(null).once(); replay(mockedRetriever); ValueExtractor retriever = mockedRetriever.retrieve(""PROP""); assertNull(retriever); assertTrue(true); } And I get: java.lang.IllegalStateException: 1 matchers expected 2 recorded. The weird thing is that I'm not even using an argument matcher. And that is the only method of the test! and to make it even worst it works from Eclipse and fails from Maven! I found a few links which didn't provide me with an answer: Another StackOverflow post Expected Exceptions in JUnit If I change the unit test and add one more method (which does use an argument matcher): @Test public void testIsBeforeDateOk() { expect(mockedRetriever.retrieve((String)anyObject())).andReturn(new PofExtractor()).anyTimes(); replay(this.mockedRetriever); FilterBuilder fb = new FilterBuilder(); assertNotNull(fb); CriteriaFilter cf = new CriteriaFilter(); assertNotNull(cf); cf.getValues().add(""2010-12-29T14:45:23""); cf.setType(CriteriaType.DATE); cf.setClause(Clause.IS_BEFORE_THE_DATE); CriteriaQueryClause clause = CriteriaQueryClause.fromValue(cf.getClause()); assertNotNull(clause); assertEquals(CriteriaQueryClause.IS_BEFORE_THE_DATE clause); clause.buildFilter(fb cf mockedRetriever); assertNotNull(fb); Filter[] filters = fb.getFilters(); assertNotNull(filters); assertEquals(filters.length 1); verify(mockedRetriever); logger.info(""OK""); } this last method passes the test but not the other one. How is this possible!?!?! Regards Nico More links: ""bartling.blogspot.com/2009/11/using-argument-matchers-in-easymock-and.html"" ""www.springone2gx.com/blog/scott_leberknight/2008/09/the_n_matchers_expected_m_recorded_problem_in_easymock"" ""stackoverflow.com/questions/4605997/3-matchers-expected-4-recorded"" Are you using m2eclipse? How does your pom look like? I am. The pom is quite big unfortunately as this is one test of many tests that work (which make this even more weird) from one of many maven modules. The most weird thing is that if I enable the maven-surefire-report and run the threads in parallel it works! but it depends on the environment (Hudson fails but it works in my dev box) Have you removed your complete local maven repository and tested it from the scratch on your dev-box ? I tried it and I'm experiencing the same problem! This is really weird! as it only happens in a few tests ... Any other ideas? I had a very similar problem and wrote my findings in the link below. http://www.flyingtomoon.com/2011/04/unclosed-record-state-problem-in.html (just updated) I believe the problem in on another test that affects your current test. The problem is on another test class and it affects you test. In order to find the place of the real problem I advice to disable the problematic tests one by one till you notify the failing test. Actually this is what I did. I disabled the failing tests one by one till I found the problematic test. I found a test that throws an exception and catches by ""@extected"" annotation without stopping the recording. > Thx a lot!!! I disabled all my tests and went thru all of them until I found one like that and that also lead me to another wrong test class which I fixed and now everything works. Thx for the updated post :)  I believe the first error message java.lang.IllegalStateException: 1 matchers expected 2 recorded. means your mockedRetriever methods called twice but test expects it was called once. So your Eclipse and Maven's configuration differs. And I have no reason to reset mock after test. Just keep in mind JUnit creates new class instance for every single test method. EDITED: What about the reason why the last test method passed the answer is: expect(mockedRetriever.retrieve((String)anyObject())).andReturn(new PofExtractor()).anyTimes(); But in your first test method it is: expect(mockedRetriever.retrieve(""PROP"")).andReturn(null).once(); as equivalent of: expect(mockedRetriever.retrieve(""PROP"")).andReturn(null); @Constantiner 1-> `mockedRetriever` is called only once as you can see in the method: `testNullValueExtractor`. I cannot see where 2 ""behaviours"" have been recorded ... 2-> The reset is there because originally I had more than one test and I reused the same mock and to be honest I didn't know that there were more than one instance being instantiated. Good to know! 3-> I don't understand what you say. The problem is that if I only leave (and remove the other one) the method: `testIsBeforeDateOk` it fails!! That is the problem!!!! 4-> Finally Maven created my eclipse project and not me. Ah sorry! I didn't have a look that you there is no class under test in your first code and you just calling mock object directly. What if you add `verify(mockedRetriever);` at the end of `testNullValueExtractor()` method in your first code sample? @Constantiner I added it and still it fails in ""record mode"". I'm pretty sure this is not a problem with the test itself but with something else. I'll try disabling the other tests and see if this works as this doesn't actually make any sense does it? Of course debugging single test is much more simple than a lot of tests at once. @Constantiner -> I finally found it! It was a problem as the one @Otuzbesli mentioned in his answer. Thx a lot!!! @Nico Congrats mate! :)"
871,A,"How to run test methods in specific order in JUnit4? I want to execute test methods which are annotated by @Test in specific order. For example: public class MyTest{ @Test public void test1(){} @Test public void test2(){} } I want to ensure to run test1() before test2() each time I run MyTest but I couldn't find annotation like @Test(order=xx). I think it's quite important feature for JUnit if author of JUnit doesn't want the order feature why? Thanks! They seem to me to be executed in the order they appear in the source file. You should never write tests that need to be executed in a specified order. That's really bad practice. Every test should be able to run independent. I think the tests when run individually are run in the order they appear in the file but if run as part of a suite they tend to be run in alphabetical order. I agree that tests should be independent but in my case I was testing that a cache is loaded once and then the same cache is called without loaded every other time after that until it is purged. I made sure they were in the order I wanted them to run in in the file and then I named them test1 test2 to ensure they ran in the suite correctly. It's not perfect but you could try it. @EJP this was almost universally true of java pre 7. Pre 7 most JVMs did this but it was never guaranteed. Java 7 JVMs can return the methods in a non-deterministic order. @Apfelsaft so what are test suits useful for? http://stackoverflow.com/questions/4649410/how-can-i-run-or-change-the-order-of-specific-test-methods-in-a-junit-test-cla Work around. Remove @Test from you test cases convert them as private functions then crate a single test case and call private functions in order. See the answer in Specifying an order to junit 4 tests. It doesn't look hard to implement some kind of test ordering or test dependency. +1 for providing a pointer to something that actually answers the question. While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.  Migration to TestNG seems the best way but I see no clear solution here for jUnit. Here is most readable solution / formatting I found for jUnit: @FixMethodOrder(MethodSorters.NAME_ASCENDING) public class SampleTest { @Test void callTimeDoesntMatter(){} @Test void stage1_prepareAndTest(){}; @Test void stage2_checkSomething(){}; @Test void stage2_checkSomethingElse(){}; @Test void stage3_thisDependsOnStage2(){}; } This ensures stage2 methods are called after stage1 ones and before stage3 ones.  Look at a JUnit report. JUnit is already organized by package. Each package has (or can have) TestSuite classes each of which in turn run multiple TestCases. Each TestCase can have multiple test methods of the form ""public void test*()"" each of which will actually become an instance of the TestCase class to which they belong. Each test method (TestCase instance) has a name and a pass/fail criteria. What my management requires is the concept of individual TestStep items each of which reports their own pass/fail criteria. Failure of any test step must not prevent the execution of subsequent test steps. In the past test developers in my position organized TestCase classes into packages that correspond to the part(s) of the product under test created a TestCase class for each test and made each test method a separate ""step"" in the test complete with its own pass/fail criteria in the JUnit output. Each TestCase is a standalone ""test"" but the individual methods or test ""steps"" within the TestCase must occur in a specific order. The TestCase methods were the steps of the TestCase and test designers got a separate pass/fail criterion per test step. Now the test steps are jumbled and the tests (of course) fail. For example: Class testStateChanges extends TestCase public void testCreateObjectPlacesTheObjectInStateA() public void testTransitionToStateBAndValidateStateB() public void testTransitionToStateCAndValidateStateC() public void testTryToDeleteObjectinStateCAndValidateObjectStillExists() public void testTransitionToStateAAndValidateStateA() public void testDeleteObjectInStateAAndObjectDoesNotExist() public void cleanupIfAnythingWentWrong() Each test method asserts and reports its own separate pass/fail criteria. Collapsing this into ""one big test method"" for the sake of ordering loses the pass/fail criteria granularity of each ""step"" in the JUnit summary report. ...and that upsets my managers. They are currently demanding another alternative. Can anyone explain how a JUnit with scrambled test method ordering would support separate pass/fail criteria of each sequential test step as exemplified above and required by my management? Regardless of the documentation I see this as a serious regression in the JUnit framework that is making life difficult for lots of test developers.  I think it's quite important feature for JUnit if author of JUnit doesn't want the order feature why? I'm not sure this there is a clean way to do this with JUnit to my knowledge JUnit assumes that all tests can be performed in an arbitrary order. From the FAQ: How do I use a test fixture? (...) The ordering of test-method invocations is not guaranteed so testOneItemCollection() might be executed before testEmptyCollection(). (...) Why is it so? Well I believe that making tests order dependent is a practice that the authors don't want to promote. Tests should be independent they shouldn't be coupled and violating this will make things harder to maintain will break the ability to run tests individually (obviously) etc. That being said if you really want to go in this direction consider using TestNG since it supports running tests methods in any arbitrary order natively (and things like specifying that methods depends on groups of methods). Cedric Beust explains how to do this in order of execution of tests in testng. I suspect that JUnit4 tests are in fact done in spec order. It is not guaranteed because that would (1) promote bad practices among testers and would (2) tie JUnit's hands somewhat. Either you have two independent tests or you only have one test and should code as such. @JonFreedman as I understand the question it's not a case of the tests being interdependent just of having a spec of things to test and wanting the results to appear in that order. I can understand not enforcing order for unit tests however when using JUnit to write integration tests it would be nice to be able to specify the order that tests are run. E.g. Run the login test first. @BrianD. login is probably a ""fixture"" instead of a test that must run before all the others. I will probably write a BeforeClass that logs in and then write the tests to execute in any order. I would probably do login as a Rule so that one field in the class can simultaneously declare the test to login as well as logout. Although this raises a second related question which is what if another Rule wants to be logged in before it runs? Now you have some order dependency again. The implication ""tests should be independent => tests should be ORDER independent"" is not true. Consider automated grading of student's homeworks. I want to test their solution for smaller inputs first and for larger inputs later. When the solution is failing for smaller inputs (for time/memory limit) then why should the tests run for larger inputs? >>>>> TL;DR and Ressurrecting mode on! -> JUnit is wrong! If I want to run the unit test in a class and assume the first succeeds to test another one (let's say for inclusion and edition of a value) it will allow the last to fail if the failure is actually in the first. Note JUnit does not run them in the order they are defined in the class. I have seen them run in different order for at least JUnit 4.10. @Pascal Wouldn't a TestSuite solve this problem? We used them to test databases where the individual tests were independent it also made sense to combine some tests together into a larger scenario such as (create table/insert data/delete data/drop table). This was before JUnit4 so my apologies if TestSuite has been deprecated or replaced with some newer feature. My question would be whether you would recommend TestSuites at all. Junit 4.11 has the feature now - see other answer by Özhan Düz  If the order is important you should make the order yourself. @Test public void test1() { ... } @Test public void test2() { test1(); ... } In particular you should list some or all possible order permutations to test if necessary. For example void test1(); void test2(); void test3(); @Test public void testOrder1() { test1(); test3(); } @Test(expected = Exception.class) public void testOrder2() { test2(); test3(); test1(); } @Test(expected = NullPointerException.class) public void testOrder3() { test3(); test1(); test2(); } Or a full test of all permutations: @Test public void testAllOrders() { for (Object[] sample: permute(1 2 3)) { for (Object index: sample) { switch (((Integer) index).intValue()) { case 1: test1(); break; case 2: test2(); break; case 3: test3(); break; } } } } Here permute() is a simple function which iterates all possible permuations into a Collection of array.  The (as yet unreleased) change https://github.com/junit-team/junit/pull/386 introduces a @SortMethodsWith. https://github.com/junit-team/junit/pull/293 at least made the order predictable without that (in Java 7 it can be quite random). Seems that #386 has been merged in 4.11.  What you want is perfectly reasonable when test cases are being run as a suite. Unfortunately no time to give a complete solution right now but have a look at class: org.junit.runners.Suite Which allows you to call test cases (from any test class) in a specific order. These might be used to create functional integration or system tests. This leaves your unit tests as they are without specific order (as recommended) whether you run them like that or not and then re-use the tests as part of a bigger picture. We re-use/inherit the same code for unit integration and system tests sometimes data driven sometimes commit driven and sometimes run as a suite.  use in the following way it wil work with junit 4. @FixMethodOrder(MethodSorters.NAME_ASCENDING) public class SampleTest { @Test public void testAcreate() { System.out.println(""first""); } @Test public void testBupdate() { System.out.println(""second""); } @Test public void testCdelete() { System.out.println(""third""); } }  I've read a few answers and agree its not best practice but the easiest way to order your tests - and the way that JUnit runs tests by default is by alphabetic name ascending. So just name your tests in the alphabetic order that you want. Also note the test name must begin with the word test. Just watch out for numbers test12 will run before test2 so: testA_MyFirstTest testC_ThirdTest testB_ATestThatRunsSecond  Junit 4.11 comes with @FixMethodOrder annotation. Instead of using custom solutions just upgrade your junit version and annotate test class with FixMethodOrder(MethodSorters.NAME_ASCENDING). Check the release notes for the details. Here is a sample: import org.junit.runners.MethodSorters; import org.junit.FixMethodOrder; import org.junit.Test; @FixMethodOrder(MethodSorters.NAME_ASCENDING) public class SampleTest { @Test public void firstTest() { System.out.println(""first""); } @Test public void secondTest() { System.out.println(""second""); } } It is better than nothing but it is not what I want. I want to have the tests run in their order of appearance in the class. You could write your own custom method runner but before that you should find a way to list exact order of declarations. As stated in the javadoc reflection api doesnt support this kind of things ""The elements in the array returned are not sorted and are not in any particular order."" +1 I needed this in order to verify that my tests were in fact order independent. I removed it after making sure this was the case. BTW. I have implemented the feature allowing to specify test execution order in JUnit 4: https://github.com/adko-pl/junit/commit/4421261dbdcaed8ff0a82f4d5229ac8ad6c97543 It adds: a new `@FixMethodOrder` value **METHOD_ORDER** and a new attribute to the `org.junit.Test` annotation **order**.  See my solution here: ""Junit and java 7."" In this article I describe how to run junit tests in order - ""just as in your source code"". Tests will be run in order as your test methods appears in class file. http://intellijava.blogspot.com/2012/05/junit-and-java-7.html But as Pascal Thivent said this is not a good practise. I had seen your blog post (in russian !) but this is way too complicated. @NicolasBarbulesco I have two blogs (rus and eng). It is too complicated because you sholdn't create tests with execution order dependency. My solution is workaround but real solution - is to remove that dependency."
872,A,"Re-running failed and not-run tests Let me describe a simple use-case: Running all tests in our project may take up to 10 minutes. Sometimes I see an obvious bug in my code after the first failed test so I want to stop running all tests fix the bug and re-run them. Unfortunately I can either re-run all tests from the beginning or re-run failed tests only. Is there a plugin for IDEA which allows me to re-run failed tests AND tests which weren't yet executed when I pressed ""STOP""? What if your 'fix' breaks a test that passed on the previous run? Yes it is possible but it can happen with ""Rerun failed"" button as well. Atlassian has the solution for your problem: Clover. But it is commercial.  This goes against the idea of a test suite. Normally you want to run all your tests specifically so you know you haven't broken anything somewhere unexpected. If you change the code and then run a subset of the tests the possibility exists that you broke something and one of the skipped tests would have failed. This is a case of not getting your cake and eating it too. If you find a bug in an early test by all means stop the suite. Fix the bug but then run the suite from the beginning."
873,A,SAXParseException when running Spring 3 JUnit I just upgraded to Spring 3 and attempted to run some JUnits to make sure everything was copacetic. It wasn't. I got a SAXParseException when loading the context... it complained: Failed to read schema document 'http://www.springframework.org/schema/beans/spring-beans-2.0.xsd' because 1) could not find the document; 2) the document could not be read; 3) the root element of the document is not . My suspicion is that it couldn't access the XSD due to proxy configuration. Is there a way to make it stop trying so hard to validate it and just run the darn thing? :) In all seriousness though I didn't have a problem when I was running under Spring 2 so I suspect that Spring 2 didn't bother to try to validate but Spring 3 does. I was hoping there was a way to bypass that functionality. One further clue... Since I couldn't run the JUnits I went ahead and fired up the server (this is a web app) and everything seemed to go swimmingly. So that makes me think that Spring is just fine with my context files as they are. Edit The same error occurs when using instead the Spring 3.0 schemas It could be because you are using xsd of spring 2. You could try using http://www.springframework.org/schema/beans/spring-beans-3.0.xsd. I do get the same error when using the 3.0 schema.  As it turns out my problem was a result of re-bundling the spring jars into a single jar. Just using the separate jars worked fine for me.
874,A,"Have JUnit fail tests that don't actually run an assertion My team is working on educating some of our developers about testing. They understand why to write tests and are on board that they should write tests but are falling a little short on writing good tests. I just saw a commit like this public void SomeTest{ @Test public void testSomething{ System.out.println(new mySomething.getData()); } So they were at least making sure their code gave them the expected output by looking. It will be a bit before we can really sell the idea of code reviews. In the mean time I was considering having JUnit fail any tests that do not have actual assertXXX or fail statements in them. I would then like to have that failure message say something like ""Your tests should use assertions and actually examine the output!"". I fully expect this to lead to calls like assertTrue(1 == 1);. We're working on the team buy in for proper testing and code reviews are there any technical mechanisms we can use to make life easier for the developers that already get it?? What about technical mechanisms to help the new guys understand? You can use some static code analyzer. I use PMD which includes a JUnit rule set. There are a lot of IDE plugins which will mark rule violations in the IDE. You can configure the rule sets to your needs. You will also profit from the other rule sets - which will warn you on code style / best practice violations (although you have to decide sometimes if the tool or you are the fool :-)).  I think you should consider organizational changes: mentoring training code reviews. The tools can only help you if you're using them in good faith with a base understanding of the goals. If one of these is missing they won't help you. Humans are just to intelligent to do dump things or work around metrics. I think your assessment is not correct that ""they"" are on board if they can't write a single useful test. Automatic tools are simply not the correct tools at this stage. You can't learn by being told by a program what to do next. I'm with you on getting people to buy in and get educated. I'm looking more for tools to help another developer and I monitor the the code so that when we spot a bad test we can flag it and go talk to the guys that are still learning."
875,A,"Grails Unit Tests: Why does this statement fail? I've developed in Java in the past and now I'm trying to learn Grails/Groovy using this slightly dated tutorial. import grails.test.* class DateTagLibTests extends TagLibUnitTestCase { def dateTagLib protected void setUp() { super.setUp() dateTagLib = new DateTagLib() } protected void tearDown() { super.tearDown() } void testThisYear() { String expected = Calendar.getInstance().get(Calendar.YEAR) // NOTE: This statement fails assertEquals(""the years dont match and I dont know why."" expected dateTagLib.thisYear()) } } DateTagLibTests.groovy (Note: this TagLibUnitTestCase is for Grails 1.2.1 and not the version used in the tutorial) For some reason the above test fails with: expected:<2010> but was:<2010> I've tried replacing the test above with the following alternate version of the test and the test passes just fine: void testThisYear() { String expected = Calendar.getInstance().get(Calendar.YEAR) String actual = dateTagLib.thisYear() // NOTE: The following two assertions work: assertEquals(""the years don\'t match"" expected actual) assertTrue(""the years don\'t match"" expected.equals(actual)) } These two versions of the test are basically the same thing right? Unless there's something new in Grails 1.2.1 or Groovy that I'm not understanding. They should be of the same type because the values are both the value returned by Calendar.getInstance().get(Calendar.YEAR) Duplicate My bad! @Victor Yes I'm very interested in that beta! But I'm afraid I've been shut out! yeah it just started today less than 12 hours old. You kind of missed the train but don't worry it'll go into open beta in 7 days :) The object returned from dateTagLib.thisYear() must not be a string. Try  assertEquals(""the years dont match and I dont know why."" expected dateTagLib.thisYear().toString()) In your working example Groovy is converting .thisYear() to a String for you. Print out dateTagLib.thisYear().class to be sure. cheers Lee Thanks Lee (This question's not an exact duplicate...but it's close though... :-D)"
876,A,"JUnit for getters and setters As it was highly recommended in one of the books I'm reading I've started to write tests for each of the class I'm creating. I'm not really sure what is a good practice to cover setters and getters or not. If yes I'm not sure how because to verify the setter method you need to call the getter for this variable and vice versa. So you will never know where the actual problem is. Where I went is just similar tests for setters and getters. If you practice TDD (Test-Driven Design) the setters and getters will emerge only as you have a need for them - you don't just automatically create them up front ""just in case"". In this case the need for the accessor will typically emerge in the course of an already-failing test for a less-rudimentary method. This means that your accessor will be tested not by a specific test for it but in the course of testing the broader method. That's better; it's silly to create accessors ""just in case"" and silly to write tests for methods that are as trivial as that. But it's nice to have them test-covered.  If your getters and setters are simply getting and setting a backing variable then writing unit tests for them does not provide much value as you are essentially testing that the language and compiler are doing what they are supposed to. Focus instead on testing that behavior works as expected. If you set property A and that means property B should now have a different value that might be worth a unit test.  General guideline for writing unit test is if your tested class contains logic. Even if you have a delegation logic (business layer calls dao layer) I would recommend writing tests for it. As for value objects (or POJOs to that matter) you can avoid unit testing. BUT if you do wish to test those object - use reflection. You can set a field with reflection and see what the getter returns. You can set the field with a setter and check the value using reflection again.  The philosophy of Test Driven Development says ""test everything that can possibly break"". Getters and setter are trivial to code and are often generated for you by your IDE. Hopefully the developers of your IDE have already tested that code so you won't have to. The only case i would bother testing getters is if they do more than just set/get. Write test for your business logic not tests for the sake of writing tests. ...and if they ""do more than just set/get"" they're not really setters/getters in the strictest sense. It's easier to work with code where only raw accessors have those names; you can look at the name and know ""this does nothing more than set/get"". Good point Carl  If you care about coverage metrics setters and getters can kill you. Regardless of that you should definitely test them especially if they do more than set or get. Also its OK to test set/get in one test. If the props are private then you have to do it that way unless there are side effects of a set. Or you can implicitly test them when testing other methods. For example if you are testing a DAO you can use the setters and getters when you write you testSave method... To make life easier you might be able to write a reflection based test utility to test setters and getters. Or just write simple tests its not hard or time consuming although it is boring... There are a lot of options here... Testing just to achieve high coverage metrics is sort of like getting paid per line of code.. imho... @alexander I didnt say you should test for metrics. I said setters/getters can kill metrics. Ill clarify above."
877,A,"JUnit TestCase object instantiation Is a new (or different) instance of TestCase object is used to run each test method in a JUnit test case? Or one instance is reused for all the tests? public class MyTest extends TestCase { public void testSomething() { ... } public void testSomethingElse() { ... } } While running this test how many instances of MyTest class is created? If possible provide a link to a document or source code where I can verify the behaviour. I couldn't find a clear answer in the JUnit docs about your question but the intent as anjanb wrote is that each test is independent of the others so a new TestCase instance could be created for each test to be run. If you have expensive test setup (""fixtures"") that you want to be shared across all test cases in a test class you can use the @BeforeClass annotation on a static method to achieve this result: http://junit.sourceforge.net/javadoc_40/org/junit/BeforeClass.html. Note however that a new instance may still be created for each test but that won't affect the static data your @BeforeTest method has initialized. @BeforeClass is not available in JUnit 3. An equivalent is described here: http://stackoverflow.com/questions/3023091/does-junit-3-have-something-analogous-to-beforeclass  If you are asking this because you are concerned about data being initialized and re-initialized in your constructor be aware that the prescribed way to initialize your test cases data is via setUp() and tearDown() exclusively.  Yes definitely. I found that data I stored in instance variables could not be accessed between tests due to this design.  There's one instance for each test run. Try public class MyTest extends TestCase { public MyTest() { System.out.println(""MyTest Constructor""); public void setUp() { System.out.println(""MyTest setUp""); public void tearDown() { System.out.println(""MyTest tearDown""); public void testSomething() { System.out.println(""MyTest testSomething""); public void testSomethingElse() { System.out.println(""MyTest testSomethingElse""); } The Sourcecode (including that to newer versions - your and my example is Junit 3) is on http://www.junit.org  Yes a separate instance is created. While running that test 2 instances of MyTest gets created. If you want a different behavior one option is to use a similar tool called TestNG(http://testng.org/doc/). Thanks for the quick response. Can you also please provide a link to a document or source code where I can verify this behaviour? You can easily verify it by providing a constructor and add a System.out.println to it."
878,A,Sample project for learning JUnit and proper software engineering I'm having a hard time making the connection between testing and code. I could ask numerous questions here about things like directory structure and naming of test classes and JUnit 3 vs 4 and so on but I'd rather find a sample project that does it right and learn by reading it. I'd like something not too complex so that I can understand it easily and JUnit 4 would be best (no reason to stick with 3 since I'm starting fresh right?). I'm on Windows 7 I use Eclipse and I'm planning/hoping to learn and use Hudson. I like Ant haven't had a good experience with Maven but that's cool too. Is there a sample project out there that fits this criteria? edit: Neither of these answers mentioned Hudson; I really like the Craftsman articles (and maybe it goes into continuous integration) but does anyone else have any other suggestions? For reference we use Hudson for CI and Maven for build. Another project that you may be interested in is Sonar - which runs code analysis metrics on your code (http://sonar.codehaus.org/). Sonar looks neat. I just installed the plugin to Hudson so whenever I start actually practicing these concepts it will be there to help me analyze my code. (I've had Hudson installed for a while but it currently has no projects yet) If you're looking for an easy example of project structure and convention I would recommend giving Maven another go. To use it with Eclipse install the M2Eclipse plugin and create a Maven project using one of the Maven archetypes. The archetypes build simple template projects including source and test structure and will run with Maven and Eclipse right away. Thanks I might reconsider it. I've heard that Maven is extremely popular so there must be something there... I wish I could accept both answers. Over the past few days I set up Maven and the M2Eclipse plugin and got it all hooked into my server running Hudson and Sonar and SVN and it's really quite an amazing setup. But the Craftsman articles from the other answer got me in the right mindset. Hehe no hard feelings. Glad it helped! Maven/Sonar/Hudson is definitely a good combination.  I found the Craftsman articles by Robert Martin (Uncle Bob) an excellent resource for learning effective unit testing. It focuses on Test Driven Development and walks you through the experience of a new coder learning how to test code. The first article can be found here and is called The Craftsman #1: Opening Disaster. Wow it's now 2:46am here I should've been asleep a long time ago but I couldn't stop reading them. Very nice answer!!
879,A,"ant junit task error i am trying to do the spring tutorials from the spring website. one of the tutorials include bulding an ant build file which when i build I keep getting this error BUILD FAILED build.xml:146: You must not specify nested elements when using refid When i click on the error it seems to be pointing at this location <target name=""tests"" depends=""buildbuildtests"" description=""Run tests""> <junit printsummary=""on"" fork=""false"" haltonfailure=""false"" failureproperty=""tests.failed"" showoutput=""true""> <classpath refid=""master-classpath""/> <formatter type=""brief"" usefile=""false""/> <batchtest> <fileset refid=""master-classpath""> <include name=""**/*Tests.*""/> </fileset> </batchtest> </junit> <fail if=""tests.failed""> tests.failed=${tests.failed} *********************************************************** *********************************************************** **** One or more tests failed! Check the output ... **** *********************************************************** *********************************************************** </fail> </target> any clues why this error is generated? I figured out the solution. <batchtest> <fileset dir=""master-classpath""> <include name=""**/*Tests.*""/> </fileset> </batchtest> I originally used refid=""master-classpath"" which is wrong. Did this really work in the end? It looks like you'd need to have a directory called `master-classpath` for it to do so."
880,A,Which PMD rules to activate for JUnit tests? I'm in the middle of setting up PMD as a tool in our team to support us writing better code. Basically I'm building Ant scripts and try to set up some rules for everyone to use. But right now I hit this problem: When I write JUnit tests I don't want to use the same rules I apply on our main source code. I don't care that much about String rules (like string dupliates or weird instantiations) in the junit tests. My questions is: Is that a fault on my side and should I start writing better JUnit tests? Should I provide a 2nd set of rules that disables some of the string/design/finalizers rules? Two things. Why are you trying to set up rules why not using the existing rules? (Special requirements?). And second yes of course Unit tests should have a good quality as well. Your Unit test test you production code so shouldn't they have at least the same quality as your production code? Ah I'm not creating new rules all by myself I'm only selecting from the existing rules. I don't need some rules in my project for example I disable the j2ee rules. And our existing codebase will result in thousands of warnings if I don't disable some of the rules. About the 2nd point I want a high quality in my unit tests but for example when I test an class that exports my objects in a text format I'm using `Strings` in my `assert` statements a lot to check if the data is exported correct and PMD will give lots of warnings about String optimizations. That's something I thought turning off wouldn't affect the unit test quality much.  The second option - I don't run PMD against my tests at all. I could and PMD provides some JUnit specific rules. I would definitely use a separate ruleset against the test code though. I expect more String literals and some thing specified instead of using conditionals/loops. After all I don't want to duplicate the code I am trying to test. I'll set up another ruleset configuration for my tests. There is some code duplication there and some string jumbling that I don't see as a code smell.
881,A,Test Cases: Mocking Database using Spring beans Our application has a service layer and a DAO layer written as Spring beans. While testing the Service Layer- I do not want to depend upon a real database so I am mocking that by creating a 'Mock' Impl for the DAO layer So when I am testing the Service layer- I chain the Service layer beans to the Mock DAO beans And in Production- will chain the Service layer to the 'real' DAO beans Is that a good idea ? Any alternate suggestion on how to mock the database layer ? Clarification:This question is about testing the Service Layer and not the DAO layer. While testing the service layer- I assume that either the DAO layer has already been tested or doesn't need testing. The main thing is- how do we test service layer- without being dependent upon the DAO implementation- hence I am mocking the DAO layer You are definitely on the right track. My mocking framework of choice is Mockito  This is a technique we've been using for many years now. Note that when it comes to mocking the DAO interfaces you have some choices: Create mock instances as real Java classes Use a dynamic mocking framework such as jMock (my preference) or EasyMock Dynamic mocking frameworks allow you to stub out a variety of circumstances (no data 1 row many rows exception throwing) without having to create complex classes to stub out the behavior you wish to test  As I understand the question it is explicitly dedicated to best practices regarding testing DAO layers as mocking a database seems nnot so straightforward as mocking the DAO layer when testing services. Personally I'd raise the question back if it's reasonable to really unit test a DAO layer in the classical unit testing meaning. If you design your DAO layer properly it does not do much more than mapping domain objects to queries. That said I alway propose to use an embedded database like H2 HSQL or the Java 6 embedded Derby to do things like this as mocking a datasource is really much more effort than simply raising an embedded database. Spring 3 will provide a nice builder pattern to create such databases on the fly. RC1 of it will also introduce a jdbc namespace to ease setup further. See this one for details. But even with current Spring 2.5 branch using an embedded database is just a matter of taking the databases JAR and setting up a DataSource accordingly. See clarification added to the Question. Thanks  That's a great way to use mocking to test the database. I don't think any alternative suggestion is necessary; I think you've got the right technique already!
882,A,Managing test data for Junit tests We are facing one problem in managing test data(xmls which is used to create mock objects). The data which we have currently has been evolved over a long period of time. Each time we add a new functionality or test case we add new data to test that functionality. Now the problem is when the business requirement changes the format( like length or format of a variable) or any change which the test data doesn't support  we need to change the entire test data which is 100s of MBs in size. Could anyone suggest a better method or process to overcome this problem? Any suggestion would be appreciated. Personally I would stay away from creating data for tests case anywhere other then within the test cases. Instead of creating test data create data generators that allow for the quick generation of objects within each test case or within each before block. This has two main advantages: It makes the tests much easier to read as the developer can see exactly what objects are being used and It should greatly cut down on the amount of test data that you need to manage. Reserve test data for things like functional and integration tests and use a tool like DBDeploy to manage that data. This data needs to be kept intentionally small. The use of DBDeploy and DBUnit allows for the database to be cleaned before each test or test suite. This should also limit the amount of data you need as it greatly increases data reuse. Thanks Chris. That was was a good insight.  While this is not a complete solution to your problem but would definitely help (esp in your case since you have 100s of MBs of data) -- Write tests based on behavior verification instead of data verification. Martin Fowler has a very good article here Thanks Mihir. Found the article useful.
883,A,"Cannot get DBUnit to import data for CollectionTable I have the following Entity:  @Entity @Table @NamedQuery(name = Constants.FINDALLFINDERNAME query = Constants.FINDALLQUERY) public class Customer extends AuditableEntity { ... @ElementCollection(fetch = FetchType.EAGER) @CollectionTable(name = Constants.PHONES joinColumns = @JoinColumn(name = Constants.CUSTOMER_ID)) @Column(name = Constants.CUSTOMER_PHONES nullable = true) private List<Phone> phones; I want to import the following data into my schema during a test:  <?xml version='1.0' encoding='UTF-8'?> <dataset> <CUSTOMER id='101' USERNAME=""user1"" FIRSTNAME=""Mick"" LASTNAME=""Knutson""/> <PHONES AREACODE=""415"" P_NUMBER=""5551212"" TYPE=""WORK"" CUST_ID=""101"" /> </dataset> But I get this error:  [EL Info]: 2011-01-31 10:57:56.945--ClientSession(30624226)--Communication failure detected when attempting to create transaction on database. Attempting to retry begin transaction. Error was: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.3.0.v20110129-r8902): org.eclipse.persistence.exceptions.DatabaseException Internal Exception: org.h2.jdbc.JdbcSQLException: The object is already closed [90007-148] Error Code: 90007. Dataset written org.dbunit.dataset.NoPrimaryKeyException: PHONES Here is my unit test:  ... public class CustomerTest { //-----------------------------------------------------------------------// // Attributes //-----------------------------------------------------------------------// private static EntityManagerFactory emf; private static EntityManager em; private static EntityTransaction tx; //-----------------------------------------------------------------------// // Lifecycle Methods //-----------------------------------------------------------------------// @BeforeClass public static void initEntityManager() throws Exception { emf = Persistence.createEntityManagerFactory(Constants.PERSISTENCEUNIT); em = emf.createEntityManager(); } @AfterClass public static void closeEntityManager() throws SQLException { if (em != null) em.close(); if (emf != null) emf.close(); } @Before public void initTransaction() throws Exception { tx = em.getTransaction(); seedData(); } @After public void afterTests() throws Exception { dumpData(); } //-----------------------------------------------------------------------// // Unit Tests //-----------------------------------------------------------------------// @Test public void test__Create__and__Read_SingleCustomer() throws Exception { // Creates an instance of Customer Customer customer = CustomerFixture.createSingleCustomer(); // Persists the Customer to the database tx.begin(); em.persist(customer); tx.commit(); tx.begin(); assertNotNull(""ID should not be null"" customer.getId()); // Retrieves a single Customer from the database TypedQuery<Customer> q = em.createNamedQuery( Constants.FINDALLFINDERNAME Customer.class); List<Customer> customers = q.getResultList(); assertThat(customers.size() is(4)); tx.commit(); } @Test public void test__DeleteCustomer() throws Exception { tx.begin(); // Uses Sting Based Criteria CriteriaBuilder cb = em.getCriteriaBuilder(); CriteriaQuery<Customer> c = cb.createQuery(Customer.class); Root<Customer> cust = c.from(Customer.class); c.select(cust) .where(cb.equal(cust.get(""username"") ""user1"")); Customer result = em.createQuery(c).getSingleResult(); em.remove(result); // Retrieves all the Customers from the database TypedQuery<Customer> q = em.createNamedQuery( Constants.FINDALLFINDERNAME Customer.class); List<Customer> customers = q.getResultList(); tx.commit(); assertThat(customers.size() is(3)); } //-----------------------------------------------------------------------// // DBUnit Helper Methods //-----------------------------------------------------------------------// protected void seedData() throws Exception { tx.begin(); Connection connection = em.unwrap(java.sql.Connection.class); try { IDatabaseConnection dbUnitCon = new DatabaseConnection(connection); dbUnitCon.getConfig().setProperty(DatabaseConfig.PROPERTY_DATATYPE_FACTORY new H2DataTypeFactory()); IDataSet dataSet = getDataSet(); DatabaseOperation.REFRESH.execute(dbUnitCon dataSet); } finally { tx.commit(); connection.close(); } } protected IDataSet getDataSet() throws Exception { return new FlatXmlDataSetBuilder().build(new FileInputStream(""./src/test/resources/dataset.xml"")); } protected void dumpData() throws Exception { tx.begin(); Connection connection = em.unwrap(java.sql.Connection.class); try { IDatabaseConnection dbUnitCon = new DatabaseConnection(connection); dbUnitCon.getConfig().setProperty(DatabaseConfig.PROPERTY_DATATYPE_FACTORY new H2DataTypeFactory()); IDataSet dataSet = dbUnitCon.createDataSet(); FlatXmlDataSet.write(dataSet new FileOutputStream(""./target/test-dataset_dump.xml"")); System.out.println(""Dataset written""); } finally { tx.commit(); connection.close(); } } } Can someone help me to understand how to import CollectionTables with DBUnit? I can across enter link description here and tried the solution but modified it a bit: The property:  // Set the property by passing the new IColumnFilter dbUnitCon.getConfig().setProperty( DatabaseConfig.PROPERTY_PRIMARY_KEY_FILTER new NullPrimaryKeyFilter(""ID"" ""ADDRESS_KEY"" ""P_NUMBER"" ""HOBBY_NAME"")); then the Filter Class:  class NullPrimaryKeyFilter implements IColumnFilter { private String[] keys = null; NullPrimaryKeyFilter(String... keys) { this.keys = keys; } public boolean accept(String tableName Column column) { for(String key: keys){ if(column.getColumnName().equalsIgnoreCase(key)){ return true; } } return false; } } Now I can use the filter for all my TABLES in my test."
884,A,Junit REST tests? I am using Struts2 Spring 2 and Junit 4.5. My main question is whether even to test REST calls? Because basically this are only calls to business logic and they don't have any logic theirs methods. I have tests for all DAO and BL classes rest xml and json handler so I don't know what should I even test in REST calls - result codes/response/...? Unit tests for the REST calls may not give you value but system tests exercising the entire system through REST will. Yes but I am using stress tests for this (JMeter).
885,A,"Relative path problem in ANT junit task I have setup an ant script as eclipse builder to automatically run all my tests like below: <project name=""auto-test"" default=""test""> <property name=""tst-dir"" location=""C:\STAF\services\custom\TopCoder\bin"" /> <path id=""classpath.base"" /> <path id=""classpath.test""> <pathelement location=""D:\eclipse\eclipse\plugins\org.junit4_4.3.1\junit.jar"" /> <pathelement location=""${tst-dir}"" /> <path refid=""classpath.base"" /> </path> <target name=""test"" description=""Run the tests""> <junit> <classpath refid=""classpath.test"" /> <formatter type=""brief"" usefile=""false"" /> <test name=""testDataGenerator.test.AllTests"" /> </junit> </target> </project> It was all good before I changed a test fixture file from absolute path to relative path: SAXReader reader = new SAXReader(); Document document = reader.read(new File(""."").getCanonicalPath()+""\\conf\\TestData.xml""); The ant task now try to open D:\eclipse\eclipse\conf\TestData.xml instead of C:\STAF\services\custom\TopCoder\conf\TestData.xml I've also try to run AllTests manually from Eclipse and it's all good. Has anyone met similar problem before? Thanks in advance. PS. ANT_HOME=D:\eclipse\eclipse\plugins\org.apache.ant_1.7.0.v200706080842 Follow up: I tried to run the ant script from command line and find below: C:\STAF\services\custom\TopCoder>ant -f c:\STAF\services\custom\TopCoder\task\build.xml the ant script works correctly. C:>ant -f c:\STAF\services\custom\TopCoder\task\build.xml the script will claim: [junit] C:\conf\TestData.xml (The system cannot find the path specified) I've also checked eclipse builder setting there seems nothing to change the path to D:\eclipse\eclipse. Java resolves relative paths against the current user directory which is typically the directory from where the java program was invoked. One way to overcome this issue is to define an environmental variable for your base path. Then you could easily use ""relative paths"" (meaning create absolute paths by concatenating the base path and the relative path).  Here is the solution I find: Just as kgiannakakis mentioned Ant also start executing its task from the location it was invoked so we just need to change the working directory setting of our custom eclipse builder. In the JRE tab choose ""Execution Environment"". Change the Working directory to your current workspace.  Looks like I've missed the karma but anyway... We do this:- Build.xml <project name=""whatever""> <property file=""build.${env.COMPUTERNAME}.properties""/> <property file=""build.properties""/> build.properties project.root=.. build.file.dir=${project.root}/buildfiles deploy.dir=${project.root}/deploy which of course you can override by creating your OWN build.computername.properties to allow for developer path differences etc"
886,A,"Does JUnit4 testclasses require a public no arg constructor? I have a test class written in JUnit4 syntax that can be run in eclipse with the ""run as junit test"" option without failing. When I run the same test via an ant target I get this error: java.lang.Exception: Test class should have public zero-argument constructor at org.junit.internal.runners.MethodValidator.validateNoArgConstructor(MethodValidator.java:54) at org.junit.internal.runners.MethodValidator.validateAllMethods(MethodValidator.java:39) at org.junit.internal.runners.TestClassRunner.validate(TestClassRunner.java:33) at org.junit.internal.runners.TestClassRunner.<init>(TestClassRunner.java:27) at org.junit.internal.runners.TestClassRunner.<init>(TestClassRunner.java:20) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:26) at junit.framework.JUnit4TestAdapter.<init>(JUnit4TestAdapter.java:24) at junit.framework.JUnit4TestAdapter.<init>(JUnit4TestAdapter.java:17) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:386) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768) Caused by: java.lang.NoSuchMethodException: dk.gensam.gaia.business.bonusregulering.TestBonusregulerAftale$Test1Reader.<init>() at java.lang.Class.getConstructor0(Class.java:2706) at java.lang.Class.getConstructor(Class.java:1657) at org.junit.internal.runners.MethodValidator.validateNoArgConstructor(MethodValidator.java:52) I have no public no arg constructor in the class but is this really necessary? This is my ant target <target name=""junit"" description=""Execute unit tests"" depends=""compile jar-test""> <delete dir=""tmp/rawtestoutput""/> <delete dir=""test-reports""/> <mkdir dir=""tmp/rawtestoutput""/> <junit printsummary=""true"" failureproperty=""junit.failure"" fork=""true""> <classpath refid=""class.path.test""/> <classpath refid=""class.path.model""/> <classpath refid=""class.path.gui""/> <classpath refid=""class.path.jfreereport""/> <classpath path=""tmp/${test.jar}""></classpath> <batchtest todir=""tmp/rawtestoutput""> <fileset dir=""${build}/test""> <include name=""**/*Test.class"" /> <include name=""**/Test*.class"" /> </fileset> </batchtest> </junit> <junitreport todir=""tmp""> <fileset dir=""tmp/rawtestoutput""/> <report todir=""test-reports""/> </junitreport> <fail if=""junit. failure"" message=""Unit test(s) failed. See reports!""/> </target> The test class have no constructors but it has an inner class with default modifier. It also have an anonymouse inner class. Both inner classes gives the ""Test class should have public zero-argument constructor error"". I am using Ant version 1.7.1 and JUnit 4.7 Instances of your test classes need to be made somehow. You can create a no-arg test which adds test instances created in some other way which can be useful for parameterising tests (or was in JUnit 3 anyway). But why would you suppress the synthetic no-arg constructor?  I believe you need a no-args constructor but if you don't declare any constructors Java will create a synthetic one for you. Are you sure the ant-task is not picking up another class; one that just happens to follow the naming convention you've set (*Test or Test*)?  Eclipse uses a different implementation to execute JUnit4 test cases - it has its own test runner. This happens to be different from the one used by the Ant - the default one available in the JUnit distribution and is the reason for the discrepancy noted in the execution behavior of the environments in Ant and Eclipse. Taking a look at the source code of JUnit 4.3.1 4.5 and 4.7 (especially that of the testrunners) reveals that test classes must have a public zero-arg constructor. Do note that the default runner in JUnit v4.7 is BlockJUnit4ClassRunner. You'll notice that the javadocs (what a pity!) contain the rules to be followed on what constitutes a well formed test class - a public zero-argument constructor is one of them. I was thinking the same as you. That it had something to do with junit3 vs junit4 but as far as I can see in the stacktrace it is actually a JUnit4TestAdapter that is involved. Well I did take a look at the sources of Ant 1.7.1. In JUnitTestRunner.java (line 386 which appears in the stack trace) a TestSuite instance is created only if the preconditions are met. Oops my bad line 396 creates the TestSuite whereas 386 creates the JUnit4TestAdapter. @Thomas which version of JUnit and Ant are you using? The reason is got to do with the presence of the TestRunner class itself in the stack trace. I am using TeamCity and the version bundled with it is 1.7.1. The Junit version is 4.7. This is the version I have specified in the classpath under my target. I have tried to run the same ant target in eclipse and the same version as above. Same errors. I believe I have the reason for this change in behavior. Eclipse has its own testrunner that is not as stringent as the one in JUnit.  Having a no-arg constructor allows test classes to be aggregated into suites:  TestSuite suite = new TestSuite(); suite.add(TestClass.class); ...  For older versions of junit if you have the word ""Test"" in your inner class this problem will occur. We ran into this problem while implementing the subclass-for-test pattern naming the subclass e.g. FooForTest. By renaming to FooSubclass the problem was solved. See the above comment from @Vineet Reynolds for more detail about affected junit versions and why this happens for ant but not eclipse. Hope that helps!  Thank you all for your time and your answers. I have now found a solution. Previously I thought the input for the batchtest part of my ant target should be .class files but it is also possible to use .java files. That solved the problem. Now it no longer complains about inner classes missing public constructors. <target name=""junit"" description=""Execute unit tests""> <delete dir=""tmp/rawtestoutput""/> <delete dir=""test-reports""/> <mkdir dir=""tmp/rawtestoutput""/> <junit printsummary=""on"" failureproperty=""junit.failure"" fork=""true""> <jvmarg value=""-Duser=TBA -Dpassword=ibber11""/> <classpath refid=""class.path.test""/> <classpath refid=""class.path.model""/> <classpath refid=""class.path.gui""/> <classpath refid=""class.path.jfreereport""/> <classpath path=""tmp/${test.jar}""/> <batchtest todir=""tmp/rawtestoutput""> <fileset dir=""src/test""> <include name=""**/*.java""/> <exclude name=""**/SessionHelper.java""/> <exclude name=""**/TestHelper.java""/> </fileset> </batchtest> <sysproperty key=""user"" value=""tba""/> <sysproperty key=""password"" value=""ibber11""/> </junit> <junitreport todir=""tmp""> <fileset dir=""tmp/rawtestoutput""/> <report todir=""test-reports""/> </junitreport> <fail if=""junit.failure"" message=""Unit test(s) failed. See reports!""/> </target> My only problem now is that I have to filter out the test classes without tests in order to avoid a ""No runnable methods""-error. That is the helper and util classes. There must be a more elegant solution than mine. One solution could be a naming convention as suggested [http://stackoverflow.com/questions/672466/junit-how-to-avoid-no-runnable-methods-in-test-utils-classes%5D%5Bhere%5D. The helper classes does not contain the @Test annotaion. It must be possible to utilize this in some way... Try exclude **/*Helper.java.  Could be because you are using Enclosed.class then the enclosed classes has to be static @RunWith(Enclosed.class) public class MyClassThatCointaintTestClasses { public static class Class1Test { @Test public void test1(){ } @Test public void test2(){ } } public static class Class2Test { @Test public void test21(){ } @Test public void test22(){ } } }  You must have a default constructor for the test case. Otherwise the runner doesn't know how to instantiate the class. Looks like you have a constructor with args. Java doesn't create default constructor if you already have a constructor. Generally you should avoid constructors in test cases. If you want do initialization write a init method and annotate it with @BeforeClass. The benefit is that the stack trace will be much cleaner if you have any errors. As you can see the constructor's stack trace is really confusing for most people.  Non-static inner classes have a hidden constructor that takes the outer class as argument. If your inner classes dont share state with the outer classes just make them static."
887,A,"AspectJ: How to replace an existing annotation Using AspectJ how do you replace an existing annotation? I have the following code declare @method : @Test * *(..) : @Test(timeout=10); Which generates the following error on every test method: ... already has an annotation of type org.junit.Test cannot add a second instance [Xlint:elementAlreadyAnnotated] Of course the error makes sense but what is the syntax to say ""remove the @Test annotation from all methods that have it. Then replace it with @Test(timeout=10)"" I doubt that you can do that with AspectJ. At least I could not find any relevant info in the current version of AspectJ in Action. What you can do is inject your own custom annotation next to the test annotation and write a custom JUnit Runner class (bound with the @RunWith annotation which you can again inject with your aspect) that gives your custom annotation precedence over the @Test annotation. Funny that's the same book I have open on my desk right now. lol. I like where you're going with this. I've written a few mixins so I'm somewhat comfortable with making classes do completely different things (like extend something else). One problem with your suggestion though: my test classes already use the @RunWith annotation. I wonder if this could work if I make all tests extend a parent class that uses a custom @RunWith as you mentioned The parent class approach will work (`@RunWith` is marked with `@Inherited`) but if the child classes contain the annotation the child annotation wins over the parent annotation.  I'm the AspectJ project lead. Under https://bugs.eclipse.org/bugs/show_bug.cgi?id=313026 we are looking at how to use declare annotation for: augmenting existing annotations (adding values to those that are already there) replacing them defining precedence (should your declare replace what is there?) We are also looking at a form of it that removes annotations: declare @remove_from_method: int mymethod(): @ToBeRemoved; But you can't do it yet... Thanks for the update. I've logged in added some votes on the bug. I would love that feature. It feels like creating a work-around is a fairly ugly process."
888,A,"How to organize projects? I've been working with Visual Studio for a long long time but now I'm been requested to work on a Java web project. We've decide to use Spring MVC as framework and we want to use Log4J (for logging obviously =P) and JUnit for unit testing. Now in the ""Microsoft way"" I will create a Solution and I'll add A web project and a Unit testing project; now that I'm usign Netbeans is it possible to do like that? Or how should I organize my projects? Thanks for sharing your experience! In Netbeans you specify what type of project you want to create say Java Web Application. Netbeans will then create the files and folders to support that project. Within the project view explorer you can see the 'Test Packages' node this is where you add java classes to support your unit testing. When you add a unit test Netbeans will add a reference to the correct JUnit library to your project (you can see this on project properties > libraries > compile test). For Spring MVC the same goes. You add a dependency in Netbeans either at project creation time or from the properties dialog afterwards. This is just tip of the iceberg. So I hope this information allows you to at least get started and you can return with more specific questions as you get further in.  Have you thought about using Maven as a way to manage your project? I've heard really good things about it. You can find a list of what Maven is exactly here. In short it has the following goals (I took these from the web site): Making the build process easy Providing a uniform build system Providing quality project information Providing guidelines for best practices development Allowing transparent migration to new features Maven is an excellent way to go. There is a learning curve but for a basic project you should be up and running in a day or so.  Try to use maven and there is a standard way for a project :)"
889,A,JUnit output in Maven reports I am using Maven for my project and have following question: I want to see the logging output of my JUnit tests (log4j System.out whatever) in test reports. Do you have any idea how to achieve this? Thanks ;-) how are you running your maven build? Just using command line: mvn test/install/package/etc. I believe you can redirect the output (System.out) of test using the maven surefire plugin configuration key redirectTestOutputToFile and then find the output in target/surefire-reports Check more about this in the plugin docs In one snipplet: <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <redirectTestOutputToFile>true</redirectTestOutputToFile> </configuration> </plugin> </plugins> </build>
890,A,"JUnit custom runner with Spring application context I am fairly new to Spring and am working with a suite of JUnit 4.7 integration tests for a web application. I have a number of working test cases of the form: import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""/META-INF/spring/testContext.xml"" }) public class myTest { @Test public void testCreate() { //execute tests .... } } My application has a number of external dependencies that I am testing all of which have beans that are initialized through the loading of testContext.xml. Some of these external dependencies require custom code to initialize and tear down the necessary resources. Rather than duplicate this code in every test class that requires it I would like to encapsulate it into a common location. My thought was to create a seperate context definition as well as a custom runner that extends SpringJUnit4ClassRunner and contains the @ContextConfiguration annotation and related custom code like so: import org.junit.runners.model.InitializationError; import org.junit.runners.model.Statement; import org.springframework.context.ApplicationContext; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; //load the context applicable to this runner @ContextConfiguration(locations = { ""/META-INF/spring/testContext.xml"" }) public class MyCustomRunner extends SpringJUnit4ClassRunner { public MyCustomRunner(Class<?> clazz) throws InitializationError { super(clazz); } @Override protected Statement withBeforeClasses(Statement statement) { // custom initialization code for resources loaded by testContext.xml ... return super.withBeforeClasses(statement); } @Override protected Statement withAfterClasses(Statement statement) { // custom cleanup code for resources loaded by testContext.xml .... return super.withAfterClasses(statement); } } I could then have each test class specify its applicable runner with: @RunWith(MyCustomRunner) When I do this my tests run and the proper withBeforeClasses and withAfterClasses methods are executed. However no applicationContext is provided back to the test class and all of my tests fail with: java.lang.IllegalArgumentException: Can not load an ApplicationContext with a NULL 'contextLoader'. Consider annotating your test class with @ContextConfiguration. The context only loads correctly if I specify the @ContextConfiguration annotation on each test class - ideally I would like this annotation to live with the handler code for the resources it is responsible for loading. Which leads me to my question - is it possible to load Spring context information from a custom runner class? Thanks! You can create a base test class - @ContextConfiguration can be inherited as well as @Before @After and so on: @ContextConfiguration(locations = { ""/META-INF/spring/testContext.xml"" }) public abstract class myBaseTest { @Before public void init() { // custom initialization code for resources loaded by testContext.xml } @After public void cleanup() { // custom cleanup code for resources loaded by testContext.xml } } @RunWith(SpringJUnit4ClassRunner.class) public class myTest extends myBaseTest { ... } Thanks axtavt this looks like it will work for me."
891,A,"How to simultaneously run all JUnit tests for a Eclipse Java project without Maven? I have a small Java project in Eclipse. I have a class of JUnit tests for each class in the project. I'm using JUnit 4 and this is not a maven project. Is there an easy way to tell Eclipse to run all tests in all test classes at once? Right click on a source folder then Run As… > JUnit Test. You could also create a parent unit test suite and list all of your tests/suites in it.  Select the source directory containing all test classes right-click select ""Run as..."" and select JUnit test."
892,A,"Testing this Activity I have an Activity I would like to automate testing on. The start up of the Activity is like this: Get data from Intent Bind to service and obtain some objects specific to that Activity's intent data Query these objects and print information to screen I have a basic grasp of testing Activities but given how this Activity depends quite heavily on a Service and other objects I'm unsure how to start testing it. Ideally the service would also be set up during testing to return only data pre-defined by me. I'd be careful with your verbiage choice of ""automate."" That implies other things than what I think it is you actually want. Why don't you just use the emulator to test your app? The testing will happen on emulators and actual devices. I want to automate the various things I normally do in the activity: sending text testing parsing etc. From what I recall Android doesn't yet have a way to ""auto-SMS."" Your app has to call the devices SMS program and at that point your activity loses focus leaving it up to the user to hit send. Testing your parsing functions could be done independently say using XML or an array of data. From the Android Documentation With instrumentation Android can automate UI testing by sending events to the application under test precisely control the start of an activity and monitor the state of the activity during its life cycle. I think should able to get something going with InstrumentationTestCase I have done some very basic testing using it"
893,A,"Turning IDENTITY_INSERT ON on a table to load it with DB Unit I try to load a table that have an identity column with DB Unit. I want to be able to set the id value myself (I don't want the database generate it for me). Here is a minimal definition of my table create table X ( id numeric(100) IDENTITY PRIMARY KEY NOT NULL ) To insert a line in X I execute the following SQL set INDENTITY_INSERT X ON insert into X(id) VALUES(666) No problem. But when I try to load this table with the following db unit XML dataset (RS_7_10_minimal_ini.xml) <dataset> <X id=""666""/> </dataset> using the following minimal JUnit (DBTestCase) test case : package lms.lp.functionnal_config; import java.io.FileInputStream; import org.dbunit.DBTestCase; import org.dbunit.PropertiesBasedJdbcDatabaseTester; import org.dbunit.dataset.IDataSet; import org.dbunit.dataset.xml.FlatXmlDataSetBuilder; import lms.DBUnitConfig; import org.junit.Test; public class SampleTest extends DBTestCase { public SampleTest(String name) { super( name ); System.setProperty( PropertiesBasedJdbcDatabaseTester.DBUNIT_DRIVER_CLASS DBUnitConfig.DBUNIT_DRIVER_CLASS ); System.setProperty( PropertiesBasedJdbcDatabaseTester.DBUNIT_CONNECTION_URL DBUnitConfig.DBUNIT_CONNECTION_URL ); System.setProperty( PropertiesBasedJdbcDatabaseTester.DBUNIT_USERNAME DBUnitConfig.DBUNIT_USERNAME ); System.setProperty( PropertiesBasedJdbcDatabaseTester.DBUNIT_PASSWORD DBUnitConfig.DBUNIT_PASSWORD ); } protected IDataSet getDataSet() throws Exception { return new FlatXmlDataSetBuilder().build(new FileInputStream(""src/test/resources/RS_7_10_minimal_ini.xml"")); } @Test public void testXXX() { // ... } } It fails with the following exception com.sybase.jdbc3.jdbc.SybSQLException: Explicit value specified for identity field in table 'X' when 'SET IDENTITY_INSERT' is OFF. It seems DB Unit does not turn identity ON before inserting a row for which the value of the identity column is specified. I already tried to execute myself on the connection retrieved from the JdbcDataBaseTester but no luck. Probably a new connection or not the same connection used to push the data into de DB. Any idea? Thanks a lot for your help all ! Octave Yes found the solution in the DBUnit FAQ actually Can I use DbUnit with IDENTITY or auto-increment columns? Many RDBMSes allow IDENTITY and auto-increment columns to be implicitly overwritten with client values. DbUnit can be used with these RDBMS natively. Some databases like MS SQL Server and Sybase need to explicitly activate client values writing. The way to activate this feature is vendor-specific. DbUnit provides this functionality for MS SQL Server with the InsertIdentityOperation class. Although it is written for the MS SQL Server is also works for Sybase. So I push my data set to db with  new InsertIndentityOperation(DatabaseOperation.CLEAN_INSERT).execute(connectioninitialDataSet); Et voilà. Thanks for your answer rawheiser. Is that a typo or did the name change to InsertIdentityOperation? In my `getSetUpOperation()` I used `return new InsertIdentityOperation(DatabaseOperation.CLEAN_INSERT);` You are a lifesaver :) thanks a lot...  Not familar enough with DBUnit to help you with the specifics; but I have used a table truncate and reseeding the identity value in similar situations. dbcc checkident"
894,A,"Organization of JUnit tests in projects What would you consider best practice for organizing JUnit tests in a project and why? For example do you keep your tests next to the classes they test? Do you put them in a separate but parallel package structure? Do you use a different organization strategy entirely? as Bill the Lizard said it helps to have a parallel structure so that 1) I can ship a src.zip or src.tar.gz and leave out the unit tests 2) At a version control system level you can put hooks on who changes source code and who changes only unit tests ""Disadvantage"" You cannot seal your JAR file if both the source and unit tests are in the same package (meaning you need to delete the unit tests before preparing your .JAR and sealing it)  I respect the Maven project structure even when I don't use maven on a project simply because I got used to it. The best practice is to use a separate source folder that respects the same package structure as your main source folder. Your test specific sources (utils that you code only to be used in tests) should be put there and if you intend to use them with app runtime code then move it into the main source folder. The idea is to decouple well just as you factorize efficiency by separating persistance and control for ex.  I put my tests in a seperate but similar/parallel package structure. This is how Maven likes things and it works well with IDEs too. I like it this way because I don't have my test code mixed up with my application code yet I can still access package-private things for the purpose of mocking and state-verification.  I use a separate but parallel package structure for several reasons. It keeps tests organized the same way as the application code. I can easily build just the application files for distribution. Test code still has access to my application code. It's not as cluttered as having test code mixed with application code.  Just use Maven. With maven you can create a default structure for your project: mvn archetype:create -DgroupId=com.yoyodyne -DartifactId=UberApp This will create Maven's standard directory layout containing space for unit tests as well as your main project. Using maven you can then run the unit tests without packaging it to a jar and you can build a jar that contains just your application. You can also have different classpaths and different dependencies for run test and compile time. I find it most disturbing to see so few people around here are actually using Maven (or at least Ant though I prefer Maven for the dependency handling.)"
895,A,use cactus and junit 4 at the same Is it possible to use cactus and Junit 4 at the same time? Because I know that current cactus release is based on Junit 3.8. Cactus looks like hasn't been maintained for a long timedoesn't even support annotation is there any better framework to replace cactus? Wikipedia has a link to a workaround. Note that a JUnit 4 suite can run JUnit 3.8 tests. This means you can keep your Cactus tests as JUnit 3.8 and your other tests as JUnit 4.
896,A,"Execute setup() once workaround causing TestSuit to fail I have 2 files: xxxxxTest.java [refer this] public class xxxxxTest extends TestCase { // Run setup only once public static Test suite() { TestSetup setup = new TestSetup(new TestSuite(xxxxxTest.class)) { protected void setUp() throws Exception { //Some init which i need only once } protected void tearDown() throws Exception { } }; return setup; } public void testMyFirstMethodTest() { assertNotNull(do stuff here); } } AllTests.java public class AllTests { public static Test suite() { TestSuite suite = new TestSuite(""Test for xxxxxx""); //$JUnit-BEGIN$ suite.addTestSuite(xxxxxTest.class); //$JUnit-END$ return suite; } } So my individual test(xxxxxTest.java) works fine exactly as I want.When i run my test suite (AllTests.java) it fails because the init in setup() i provided in xxxxxTest.java are not being executed. Any suggestions? UPDATE I tried @BeforeClass in JUnit 4. But it didn't help because in my ssetUp() method I start an embedded Jetty server (server.start()) the server works fine with the code I posted but when I do the same with @BeforeClass it does not work. Similar to manuel's point: do you -need- to use JUnit 3? Then a class-level static{} initializer might be your best bet. Otherwise I recommend using JUnit 4 which has a construct which would might enjoy: import org.junit.Assert; import org.junit.BeforeClass; import org.junit.Test; public class xxxxxTest { @BeforeClass public static void beforeClass() { //Some init which i need only once } @Test public void testMyFirstMethodTest() { Assert.assertNotNull("""");//do stuff here); } }  In rare cases I also hacked around with static when using JUnit3. In your case: give the static{} initializer a try maybe it works opposed to your static initialization. if possible upgrade to JUnit4 and use @BeforeClass annotation (it is run once for a test-class). Your other JUnit3 test-classes should be runnable with JUnit4 test-runner also. static{} initializer worked like a charm. Thanks!"
897,A,"Need help improving a tightly coupled design I have an in-house enterprise application (EJB2) that works with a certain BPM vendor. The current implementation of the in-house application involves pulling in an object that is only exposed by the vendor's API and making changes to it through the exposed methods in the API. I'm thinking that I need to somehow map an internal object to this external one but that seems too simple and I'm not quite sure of the best strategy to go about doing this. Can anyone shed some light on how they have handled such a situation in the past? I want to ""black box"" this vendor's software so I can replace it easily if needed. What would be the best approach from a design point of view to somehow map an internal object to this exposed API object? Keep in mind that my in-house app needs to talk to the API still so there is going to be some dependency between the two but I want to reduce it so I can also test in isolation from this software using junit. Thanks Jason I was looking for more confirmation that creating some type of facade was the way to go. I also was not sure why it wasn't done here so I thought I was missing something. Spring would be ideal and I hope to migrate it to that framework at some point in the future. Unless there is some particular constraint in the app the answers below depict the standard way of abstraction(using interface). I am sure you already knew this so I am curious if there is anything specific in your app that might make this difficult. Also not trying to piggy back here but if you wish to make really losely coupled you can use Spring IOC instead of ""new MyAPIEndpoint();"" Abstraction; implement a DAL which will provide the transition from internal to external and back. Then if you switched vendors your internals would remain valuable and you could change out the vendor specific code; assuming the vendors provide the same functionality and the data types related to each other.  I will be the black sheep here and advocate for the YAGNI principle. The problem is that if you do an abstraction layer now it will look so close to the third party API that it will just be a redundant layer. Since you don't know now what a hypothetical future second vendor's API will look like you don't know what differences you need to account for and any future port is likely to require a rework for those unforeseen differences anyway. If you need a test framework my recommendation is to make your own test implementation using the same API as the BPM vendor. Even better almost all reputable API providers provide some sort of sandbox mode for testing. If they don't you should ask for one. I agree I typically don't do this unless I have an immediate need for two implementations or a web service where we try to limit the client side code. But for testing and TDD style is is very helpful. The cool thing about the abstraction is that you can have a ProductionThirdPartyAPI implementation a test implementation and a test DevelopmentThirdPartyAPI so you can swap out development and production api layers that alone might be worth it. Karl I agree to an extent. What ends up happening though is that unless a formal layer exists between the vendor and the application the vendors implementation via types and API behavior begins to surface further upstream then it should. Once you do switch vendors you begin to realize how coupled you were to the vendors implementation; whereas an abstraction layer forces you to think about your applications needs first; abstracting away the vendor types and API oddities.  Create an interface for the service layer internally all your code can work with that. Then make a class that uses that interface and calls the third party api methods and as the api facade. i.e. interface IAPIEndpoint { MyDomainDataEntity getData(); } class MyAPIEndpoint : IAPIEndpoint { public MyDomainDataEntity getData() { MyDomainDataEntity dataEntity = new MyDomainDataEntity(); // Call the third party api and fill it return dataEntity; } } It is always a good idea to interface out third party apis so you don't get their funk invading your app domain and you can swap out as needed. You could make another class implementation that uses a different service entirely. To use it in code you just call IAPIEndpoint endpoint = new MyAPIEndpoint(); // or get it specific to the lang you are using. Making your stuff based on interfaces when it spans multiple implementations is the way to go. It works great for TDD as well so you can just swap out the interface to a local test one that can inspect your domain code entirely separate from the third party api. Yeh finding common ground is always tricky. Yes the interface describes what you app will use consistently across all implementations (classes using the interface or impl in java world). I tend to put only the common methods that are 'genericized' into the main interface. Then any specific api methods that are used by these methods in an implementation can do specific methods for the implementation. For instance here we have a matchup server and we call an interface for FindMatch() StartMatch() CloseMatch() etc. Those are in the interface. Then in a Gamecenter... implementation of that class we use the GameCenter calls to get a match start a match close a match and have many other methods that those calls use specific to GameCenter as private methods. Then we can easily swap with a Test implementation that locally runs and also another matchup server that is self-hosted. So our three third party or unique uses are all commonized by the interface with generic calls. HTH Interfaces are also very common in web service server/client setups where all the code might not be on the client and only the interface and container classes need to be present. Quick Java sample: http://www.java-examples.com/java-interface-example Thanks Ryan I think that makes sense. Let me ask a question to make sure I understand: I can use this approach and create the interface - in one impl I can call the third party api in another impl I can call a test impl of the api so I can test in isolation. Is that correct? My concern is how I'm going to handle the methods in the API object when I have to call them (mostly set/get but there are some methods)."
898,A,"How can I get Hudson to be able to access JUnit? I've got Hudson running on TOMCAT it can build my Netbeans project using the ant build.xml but it won't run any of my unit tests because of what I assume is a problem with the classpath: package org.junit does not exist [javac] import org.junit.After; [javac] ^ But I've got the junit-4.8.1.jar on the classpath in /etc/environment and I can successfuly run the junit tests from a console using java org.junit.runner.JUnitCore org.junit.tests.AllTests My CLASSPATH is set to /home/bedwyr/junit4.8.1/junit-4.8.1.jar:. Am I going wrong somewhere or is there anything else I need to set? [edit] What I did was to export/include (using the ide) all libraries (including Junit) hudson then reads all it needs from the subversion repo. I then ran into an issue with exposing hudson to the internet and pretty soon gave up on tomcat on ubuntu server (again to do with the tomcat security manager) - glassfish is a lot smoother and that's where I am now - apache front end with ajp_proxy to hudson on glassfish. Try to run the junit tests from the console with the same user account (environment) your TOMCAT is running. Aha! Just tried that and tomcat6 can't find the class. ... I thought /etc/environment was global? Why isn't the tomcat user able to see junit? Hudson executes things in its own environment as far as I'm aware -- it doesn't pull in your shell profile etc.. Your build/test shouldn't depend (too heavily) on the environment of the particular machine its running on. You can export environment variables in Hudson. There is a plugin for it. http://wiki.hudson-ci.org/display/HUDSON/Setenv+Plugin But solution suggested by Rob makes more sense. Here you don't have to worry where you run. Depending on environment variable is not a very good idea. Since it affects all programs you run.  The junit jar is on the classpath but it is not packed somewhere in the netbeans project folder. Solution: remove the auto-added junit lib from your netbeans project (test library) and then explicitely add the junit jar via right clicking 'test libraries' -> 'add jar/folder'  You need to properly set your classpath whether you're using Ant or Maven to perform your build. Using Ivy with Ant or switching your build to Maven will allow the build to automatically lookup dependencies and properly set the classpath. If neither of those is an option you need to do something similar to the following in Ant. <property file=""build.properties"" /> <property name=""junit-home"" location=""/etc/environment"" /> <path id=""test.compile.classpath""> <path refid=""compile.classpath"" /> <pathelement location=""${junit-home}/junit-4.8.1.jar"" /> <pathelement location=""${target}"" /> </path> The build.properties part will allow you to override the default junit-home if it is different for individuals and hudson. Another way to modify properties is when invoking Ant from Hudson there is an advanced section which allows you to pass a properties file or a properties form which would allow you to enter key value pairs in the following format: junit-home=/etc/environment"
899,A,"How to Replace The Call to A Private Method of The Class Being Tested Well I am right now testing legacy code. And I am somewhere near to pass this test but its stuck at the line having comments on it. Here is the snippet  new NonStrictExpectations(){ SASCustomerDataAssemblerBD assembleBd; CustomerTOs tos; CustomerSASTO to; Another rowTo; SelectionJobLogBD logBd; { SASCustomerDataAssemblerBD.getInstanceUsingEjbRef(); result = assembleBd; assembleBd.getData(); result = tos; .. .. //This line is not being invoked. //Instead the actual line of code is working. Which is //Collections.max(someCollection someComparator); //Hence I am stuck because getting null in ""to"" invoke(Collections.class ""max"" new ArrayList() new MaxDateComparator()); result = to; to.getSasDataRow(); result = rowTo; SelectionJobLogBD.getInstanceUsingEjbRef(); result = logBd; .. } }; new TaskSASCustomerReading().execute(); Whereas all the values of result are mocked up. Is `to` being instantiated somewhere else? @Feanor: Its a mockup instance. To clarify by 'mockup instance' you mean there is another call in this NonStrictExpectations record phase in which the result is assigned to `to`? @Feanor: No no I just updated the code to show `to`. Solved it in another way :). Mocked the original method -- only the method that calls Collections.max() under the hood.  new MockUp<TaskSASCustomerReading>() { @Mock // This original method is invoking Collections.max(). // Therefore I just mocked this one other methods are all original String findLatestSelectionDate(Collection customerTOs) { return """"; } }; new Expectations(){ SASCustomerDataAssemblerBD assembleBd; CustomerTOs tos; SelectionJobLogBD logBd; { try { SASCustomerDataAssemblerBD.getInstanceUsingEjbRef(); result = assembleBd; assembleBd.getData(); result = tos; SelectionJobLogBD.getInstanceUsingEjbRef(); result = logBd; }catch(Exception e){} } }; new TaskSASCustomerReading().execute(); None the less I was totally misunderstood the thing in the first place when I asked the question. In my original question I am in fact trying to invoke a method instead of replacing it. (P.S. Never work after hours. ;))"
900,A,"What is the best way to launch HSQLDB for unit testing when working with spring maven and hibernate? In my project I can successfully test database code. I'm using Spring Hibernate HSQLDB JUnit and Maven. The catch is that currently I have to launch HSQLDB manually prior to running the tests. What is the best way to automate the launching of HSQLDB with the technologies being used? Currently I am working on an application where a in-memory Database is needed for automated JUnit tests. Following article answered a lot of my questions: [http://tshikatshikaaa.blogspot.de/2012/09/junit-testing-spring-service-and-dao.html](http://tshikatshikaaa.blogspot.de/2012/09/junit-testing-spring-service-and-dao.html) I am assuming that with hsql you are referring to HSQLDB. Configure your database url for JDBC drivers (for hibernate etc) to embedded memory based version of HSQLDB: jdbc:hsqldb:mem:myunittests Then a inprocess version of HSQLDB automatically starts that stores stuff to memory. No need to start any external servers. Thanks just getting started with HSQLDB. The tutorial I was following was configured for using a server. Changing to use in memory solves my problem.  I myself use in-memory database of hsql for testing my DAO. As a result i need not be connected to any external db server or have any network connection. Use following settings jdbc.driverClassName=org.hsqldb.jdbc.JDBCDrive jdbc.url=jdbc:hsqldb:mem:DatabaseName Also include the <property name=""hibernateProperties""> <props> <prop key=""hibernate.dialect"">org.hibernate.dialect.HSQLDialect</prop> <prop key=""default_schema"">test</prop> <prop key=""hibernate.show_sql"">true</prop> <prop key=""hibernate.format_sql"">false</prop> <prop key=""hibernate.hbm2ddl.auto"">create</prop> </props> </property> This will allow you to use the in-memory database and will automatically create the database tables from hibernate objects before executing tests. Hope this will help you.  You can also run an Ant task <startdb>: https://forums.hibernate.org/viewtopic.php?f=6&t=984383&start=0  Use it in-process or in memory and it will get started from JDBC when establishing a connection.  With JUnit you can create a method that is executed before your tests using the following annotation: @Before The link to the JUnit docs about it is here: JUnit FAQ - Test Fixtures"
901,A,"How to get failure trace with Junit4 in my Junit test I use usually ""AssertEquals"" and when the test fails the trace is properly displayed in the Failure trace of JUnit/eclipse I would like to know how to get these trace to show it in a file? @Test public void testButtons() { SelectionButton().ButtonFile(); assertEquals(""selected button should be edit""FILE.EDITFile.getSelectedItem); } how could I print/redirect the assert failure trace in a file ? thanks as in redirecting to a file? I tried from eclipse to set an output file but xhen the test fails the assertion trace is only displayed in the failure trace view and not in the file. Laura The assertEquals method of JUnit throws an AssertionError without a message on error. If you want to log more information on the failure you will have to catch the AssertionError like in:  try{ assertEquals(true true); }catch (AssertionError ex) { //Do Something throw(ex); } Yes if you want to launch the application before each test you should use @Before. Yes it is allowed to use inheritance with test classes. @Before before each @Test and not once in the testClass right ? thanks Dave for answers. should I do something to to accept answers or vote via stackoverflow ? or could I for example create a class where I call launchAppli() and closeit() respectively in @BeforeClass and @AfterClass and I make my tests class extends this class ? it is working for me thanks. so we can use try/catch with assert? so I can use catch (AssertionError ex) for all assertions ? well you can do this but it is in general not good practice because if you do not throw the exception at the end of the catch part the test will falsely pass. so what are the good practice ? Normally your test is not supposed to fail right? When you write an assert(expected actual) (of whatever type) you expect the actual part to be the same as the expected part. If this is not the case the part of the code you are testing is wrong and you should look over there and not try to add more logging in your tests. Unit tests are not supposed to give you stacktraces or debug info Eclipse does this for you anyway to start with the debugging more easily. so the annotations @Before@After@AfterClass and @BeforeClass aren't always useful? but if I want to open open the appli(my software to test) and close it before and after all @Test in the same class I have to add @AfterClass and @BeforeClass right ? otherwise I use @after and @Before if I want to do it before and after each @Testright ? ok Dave so the junit test can have the following: @BeforeClass public static void setUpBeforeClass() throws Exception { startappli();} @Test public void test1(){try{clickButton(); assertEquals(truetrue);}catch(AssertionError ex){//something} @AfterClass public static void AfterTest(){CloseAppli();} sorry for the format so is it a good junit test ? and could I call the same methode @afterClass and @BeforeClass from the other tests ? Yes that's an acceptable test. You should not call a method annotated with AfterClass or BeforeClass yourself. If you want to call the method yourself why not drop the annotation? If you want the method to be called before every test defined in the class you should use @Before and @After instead. No those annotations are not always useful. I would even discourage using @BeforeClass because this makes your tests dependend of eachother. but I have to launch the appli before each test  so should I use @Before ? or may be call the function that launches the appli inside @Test ?"
902,A,how to do DAO(db) layer unit test? since the dao layer methods will be dependent on data in the database in complex systems some operations will depend on lots of tables in this way unit test are not repeatable and independent i'm wondering how good TDD layers do this? thx. A testing framework like DbUnit is exactly what you want. From their site: DbUnit is a JUnit extension (also usable with Ant) targeted at database-driven projects that among other things puts your database into a known state between test runs. This is an excellent way to avoid the myriad of problems that can occur when one test case corrupts the database and causes subsequent tests to fail or exacerbate the damage. DbUnit also supports a variety of RDBMS's but I might recommend something like HSQLDB which can be embedded into your project/tests so your unit tests aren't dependent on being able to connect to a database somewhere in your company's basement. :) Although on the other hand you will be testing using a different RDBMS than you would be using in a production environment...  IMHO you are on to something... If you are communicating with your Sql Server then you are not doing unit test but integration tests. If you do TDD then you realise this and start to put server communication into wrappers so you can stub and mock any test data instead of using a framework like DbUnit to control database state. I think that Your business logic should not be directly in touch with databases -- or webservices or other external resources. If it is odds are that you will never write anything but integration tests. It's true that tests like these are more like integration tests but regardless of what label you put on it there may still be a need for testing. As I mentioned in my response embedding an RDBMS like HSQLDB will take server communication out of the equation but are there better ways of making sure things like ORM's are worked as intended? Is there an alternative to something like DbUnit that will still give you accurate feedback on whether tweaking that Hibernate mapping broke something?
903,A,"How much unit testing is a good thing? (No ""related questions"" seem to nail this so here goes.) I work on production code. Arguing for anything that isn't visible to the user is hard to do sometimes. If sales can't see it it's an external cost to them and they'll argue against it unless there's a great reason not to. How much unit testing is a good thing? If you test every class every method your current release will take longer possibly much longer. If you test nothing maintenance in the future will take you longer possibly much longer as bugfixes and new functionality cause problems you didn't foresee and that the unit tests would have caught. How do you find a healthy justifiable balance? Edit: to answer a few questions that people reasonable have raised... Sales isn't running the process but they certainly have input and should have limited input in any group. They're the ones who pay the bills. If they're completely steering everything that would be unreasonable obviously. I'm certain there's no best answer but I'm curious as to what other people think is reasonable. I'm expecting both extremes (everything! nothing!) and a lot in the middle. No one gets to choose their manager and if a bad policy on unit testing is a make-or-break decision in someone staying with a company/project... you have a lot more career options than most of us friend. :-) Second Edit: ""Justifiable"" is an important word in there. If I want to have time budgeted/allowed for unit testing and don't want to have to sneak it in I'm going to need to justify the why. The top answer right now for me is ""test things that have broken before"" because I can always justify reactive policies. Any ideas on how to justify something proactive? Why is ""sales"" running your software development process? You need a development lead / manager that actually understands software development. @Nate - Jon Skeet has flexibility to choose senior management he likes. Some people are not so lucky :) This is a controversial topic. There really is no best answer because there are situations where testing is beneficial and there are situations where it is a waste of time. The only way for us to produce an acceptable answerable depends on how well you describe your *individual* situation. @DVK even so at a certain level a sales person is not sitting over your shoulder checking your algorithms and monitoring your build script similarly they won't be checking if you are writing unit tests just if you are productive. If you don't have a decent manager though you won't get the team doing it. On [**Podcast #41 of StackOverflow**](http://blog.stackoverflow.com/2009/02/podcast-41/) Jeff and Joel discuss about TDD coverage with Uncle Bob Martin. Was a great piece of advice. Read [the transcript](https://stackoverflow.fogbugz.com/default.asp?W29030) or [listen the podcast](http://blog.stackoverflow.com/2009/02/podcast-41/). I think it will be really useful to everyone interested in this question. Two suggestions for minimal unit testing that will provide the most ""bang for the buck"": Start by profiling your application to find the most commonly used parts - make sure those are unit tested. Keep moving outward to the less commonly used code. When a bug is fixed write a unit test that would have detected it. I might actually push for this one; when we find a bug budget time not only to fix it but to test it so it doesn't become a bug again. That seems easy to argue for; it might be reactive instead of proactive but it's very much a good start.  The purpose of developer testing is to speed up the development of completed software of an acceptable level of quality. Which leads to two caveats: it is perfectly possible to do it wrong so that it actually slows you down. So if you find it slows you down it is very likely the case that you are doing it wrong. your definition of 'acceptable quality' may differ from that of marketing. Ultimately they are right or at least have the final say. Software that works is a specialised niche market equivalent to high-end engineered hardware made from specialist expensive materials. If you are outside that market then customers will no more expect your software to work reliably than expect their shirt to stop a bullet.  What was adviced to me is this: Try as you think ; After a while evaluate yourself: If testing spend more time than you felt was reasonnable and you had too little return over investment test less. If your product was tested enough and you lost time test more. Loop as needed. Another algorithm: :-) Some testing is really easy and really useful. Always do this with high priority. Some testing is really hard to set up and rarely come useful (for example it could be duplicated by human testing that always happen in your process). Stop doing this it's losing your time. In between try to find a balance that may vary as time goes depending on the phases of your project ... UPDATED for the comment about proving the usefulness of some tests (the ones that you firmly believe in): I'm often telling to my younger collegues that we technical people (developpers and the like) have a lack in communication with our management. As you say for management costs that are not listed do not exist therefore they avoiding them cannot serve to justify another cost. I used to be frustrated about that also. But thinking about it that is the very essence of their job. If they would accept unnecessary costs without justification they would be poor managers ! It's not to say that they are right to negate us these activities that we know are useful. But we first have to make apparent the costs. Even more if we report the cost in an appropriate way the management will have to make the decision we want (or they would be bad managers ; note that the decision may still be prioritized ...). So I suggest to track the cost so that they are not hidden any more : In the place where you track the time you spend note separately the costs that come from the code being untested (if not available in the tool add it as a comment) Aggregate those costs on a dedicated report if the tool doesn't so that each week your manager reads that X% of your time was spend on that Each time you evaluate loads evaluate separately several options with or without automated testing showing the time spend on manual testing or automated testing is about the same (if you limit yourself to the most useful tests as explained earlier) while the latter is an asset against regressions. Link bugs to the original code. If the link is not in your process find a way to connect them : you need to show that the bug comes from having no automatic tests. Accumulate also a report of those links. To really impact the manager you could send them every week a spreadsheet up to date (but with the whole history not only for the week). SpreadSheet gives graphics that give immediate understanding and let the unbelieving manager get to the raw numbers... The problem that I have is proving that to an outside group; playing it by ear is fine but I need to justify my decision and it's *hard* to measure ""return on investment"" because I don't have the results of the choices I didn't take.  I always believe in not being extreme. In particular when time and energy are limited. You just can't test it all. Not every methods/functions need a unit test. The following might not need. (1) The one that is clearly not complex like just get/set little condition or loop. (2) The one that will be called by other method that have unit tests. With these two criteria I think you can cut a lot of those. Just a thought. You *always* believe in not being extreme? Oh the irony! Hahahaha .........  Test enough so that you can feel comfortable that a bad refactor will be caught by the tests. Usually its enough to test logic and plumbing/wiring code. If you have code that is essentially getter/setters why test them? regarding the sales guy's opinion that testing isnt needed - well if they know so much why dont they do the bloody coding? I've said the same thing to sales but honestly they're not going to ""get it"" unless I put some rationale behind it. That said sales has short-term incentive in most organizations I've been in. Developers have long-term incentives usually; there's not a cash bonus to get a release out faster but there's always a bonus to have to work less-hard in the long run if you're doing it more efficiently.  IMO if there is enough to give someone who inherits the code an idea so that they can start making changes whether that be fixing bugs or putting in enhancements without having to spend days reading the code to get it that's my suggestion. Thus don't test everything to death but do cover some common cases and a few edge cases just to see what happens if things don't go as laid out initially.  This I think is a fallacy: If you test every class every method your current release will take longer possibly much longer. Testing - especially Test First - improves our flow keeps us in the zone actually speeds us up. I get work done faster because I test. It is failing to test that slows us down. I don't test getters and setters; I think that's pointless - especially since they're auto-generated. But pretty much everything else - that's my practice and my advice. +1. It is an infuriating fallacy that has altogether too much traction. Agreed entirely. While writing tests may slow down the initial code (as in the first few hundred lines) I find that it speeds up everything and anything past that.  While it is possible to over test (point of diminishing returns) it's hard to do so. Testing (particularly testing early in the process) saves time. The longer a defect stays in a product the more it costs to fix. Test early test often and test as completely as is practical!  While unit testing is useful you should definitely have a system test plan for every release - this should include testing the normal use-cases of your application (for regression) AND the specific feature being worked on in more depth. Automated system testing is pretty much vital to avoid regressions - unit tests can all pass and your app will still be a crock of dung. But if you can't do automated system testing for all of your use-cases (most applications have complex use cases particularly where interacting with 3rd party systems and user interfaces) then you can run manual system testing. User interfaces create the main problems - most other things can be automated relatively easily. There are heaps of tools to auto-test user interfaces but they are notoriously brittle i.e. in every release the auto-tests need to be tweaked just to pass (assuming no new bugs).  I'd suggest picking up the book The Art of Unit Testing. Chapter 8 covers integrating unit testing into your organization. There's a great table (p. 232) that shows the results of a two-team trial (one using tests one without); the test team shaved two days off their overall release time (including integration testing and bug fixing) and had 1/6 the bugs found in production. Chapter 9 discusses test feasibility analysis for getting the most bang-for-the-buck with legacy code. If only for the empirical data that seems worth my cash. Thanks!  Start creating unit tests for the most problematic areas (ie sections of code that often breaks and causes a lot of communication between the sales team and developers). This will cause an immediate and visible impact by the sales team and other personnel. Then once you have credibility and they see the value start adding less problematic areas until you start to notice that the ROI just isn't there anymore. Sure full coverage is nice in theory but in practice it's often not necessary. Not to mention too costly.  The ""cost"" is paid during development when it is much more cost effective and the return is realized during ongoing maintenance when it is much harder and expensive to fix bugs. I generally always do unit testing on methods that: Read/write to the data store Perform business logic and Validate input Then for more complex methods I'll unit test those. For simple things like getter/setters or simple math stuff I don't test. During maintenance most legitimate bug reports get a unit test to insure that the specific bug will not happen again.  Automated unit testing brings a lot to the table. We've used it on several projects. If someone breaks the build everyone immediately knows who did it and they fix it. It's also built into the later versions of Visual Studio. Look into Test Driven Development It should save you a lot of time and doesn't produce a significant amount of overhead. Hope this helps! If so mark it. We're Java and using Hudson hourly to pull code from version control run the build and tests on the build and email us if anything is awry. It's a *great* addition.  For unit testing my company has adopted a fairly good strategy: we have a tiered application (Data Layer Service Layer/Business Objects Presentation layer). Our service layer is the ONLY way to interact with the database (via methods in the data layer). Our goal is to have at least a basic unit test in place for each method in the service layer. It's worked well for us - we don't always thoroughly check every code path (especially in complex methods) but every method has it's most common code path(s) verified. Our objects are not unit tested except incidentally via the service layer tests. They also tend to be 'dumb' objects - most have no methods except those required (such as Equals() and GetHastCode()). Any tests on the presentation layer? Not other than manual. We do fairly rigorous smoke testing on each fix though.  How much unit testing is a good thing : Unit Testing is not static that once you have done and your job is complete It will go on through out life of product until you do not stop further development on your product Basically Unit Testing should be done each time : 1) you do a fix 2) New Release 3) Or you find a new Issue I have not mentioned development period as during this period your unit level test are evolved. Basic thing here is not Quantity (How much) but coverage of your unit test For example : For your application you fond a issue an particular function X You do a fix for X If no other module is touched you can do unit testing applicable for module X  Now this is point how much unit testing for X cover So Your unit Test must check: 1) Each interface 2) All input/out operations 3) Logical checks 4) Application specific results"
904,A,"How to inject ServletContext for JUnit tests with Spring? I want to unit test a RESTful interface written with Apache CXF. I use a ServletContext to load some resources so I have: @Context private ServletContext servletContext; If I deploy this on Glassfish the ServletContext is injected and it works like expected. But I don't know how to inject the ServletContext in my service class so that I can test it with a JUnit test. I use Spring 3.0 JUnit 4 CXF 2.2.3 and Maven. See http://stackoverflow.com/questions/2665773/spring-i-wish-to-create-a-junit-test-for-a-web-application-webapplicationconte Probably you want to read resources with servletContext.getResourceAsStream or something like that for this I've used Mockito like this:  @BeforeClass void setupContext() { ctx = mock(ServletContext.class); when(ctx.getResourceAsStream(anyString())).thenAnswer(new Answer<InputStream>() { String path = MyTestClass.class.getProtectionDomain().getCodeSource().getLocation().getPath() + ""../../src/main/webapp""; @Override public InputStream answer(InvocationOnMock invocation) throws Throwable { Object[] args = invocation.getArguments(); String relativePath = (String) args[0]; InputStream is = new FileInputStream(path + relativePath); return is; } }); }  In your unit test you are probably going to want to create an instance of a MockServletContext: http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/mock/web/MockServletContext.html You can then pass this instance to your service object through a setter method."
905,A,"Accessing Activity before it starts I am using the InstrumentationTestCase class in order to unit test some things within an activity. I need to be able to check the SharedPreferences's contents and edit them before this activity is launched. I cannot use the setUp method to create the Activity and access it's SharedPreferences object to edit it and then close that activity before finishing the setUp method because it apparently is locking the tests processing. I also cannot access the SharedPreferences after I have launched the activity inside the test because as soon as the Activity is launched it will already change the SharedPreferences object and act according to it before I had the chance to get it's reference. I apparently cannot access the SharedPreferences before either because I have no Activity object... and as soon as I do it is already executing code and being launched... So my question is is there any way to access the SharedPreferences (and any other Activity information) of this Activity before I have the Activity actually created through an Intent? I cannot change it to an ActivityInstrumentationTestCase2 because my test uses a second activity in it's process so I can't just change to this class and use it's setUp() method to access the SharedPreferences. Well... To tell you frankly.. I am not able to visualize your scenario. But is checking for info in application is doable ? Create a class which extends android.app.Application and specify class name in Manifests child application element. Sample Code: import android.app.Application; public class MyApplication extends Application { @Override public void onCreate() { super.onCreate(); //try and access activity info here. } } When your application is launched first class method to execture is onCreate of your application and has all the lifecyle events of that of any activity.. You must define extended application class in manifest by: <application android:name="".MyApplication"" android:label=""@string/application_name""> I hope this ca give you some overview. That still does not solve the problem because i am unit testing the activity. If i would have an application to unit test this application would also have to do this SharedPreferences access at startup or launch the Activity that is currently doing it. And so when testing as soon as i would launch the Application I would have already missed the SharedPreferences. Unless there is a way to access an Application that was not launched. Thank you though.  I haven't tried it but if you set the mode to MODE_WORLD_READABLE and possibly MODE_WORLD_WRITEABLE instead of MODE_PRIVATE I would think you could access the shared preferences from another application before the activity under test starts. You could probably also use a different activity or service within the apk or another apk that establishes a shared user ID and has the same certificate to do the access without changing the access mode. No you can't. You can't access private data from another app. Falmarri you are are wrong. Please see the documentation of MODE_WORLD_READABLE which can be used to make data non-private. Also please take time to understand what a shared user ID does with respect to private data at the underlying unix level.  I found the best simpler way to do this through the instrumentation only without having to edit the application's architecture or any of the access attributes. I achieved it through this: Instrumentation instrumentation = getInstrumentation(); instrumentation.getTargetContext().getSharedPreferences(..); This way I can access the SharedPreferences before any Activity is launched by the instrumentation. Thanks for all the help hints and other alternatives anyway. Could you explain where you did this at? It has been a while since I have done this but I was doing it in the tests class that extends InstrumentationTestCase inside one of the methods that would run the test and before that method would launch the activity. Alternatively I think you can also perform this on the setUp method of your test class."
906,A,"Where to place a supplementary classes used in JUnit? My unit tests need supplementary classes mostly as factories for creating objects of classes under test. For example: // EmployeeTest.java public class EmployeeTest { @Test public void testName() { Employee emp = EmployeeTestFactory.get(); // static method assert(emp.getName() instanceof String); } } // SalaryTest.java public class SalaryTest { @Test public void testAlwaysPositive() { Employee emp = EmployeeTestFactory.get(); // static method assert(emp.getSalary() > 0); } } As you see I need to work with an instance of class Employee in both unit tests. I would like to create a simple factory which will create such objects on demand e.g.: public class EmployeeTestFactory { public static Employee get() { // create it make persistent fill with data // and return } } Is it a correct approach? If yes where should I place this class? Right next to unit tests? Or maybe this factory is a sort of ""resource""? Yes right next to the unit tests is perfectly fine. You can make a package .helper for the sake structure.  For me this a question of test data handling. There are several approaches to handle test data: Build them inside the junit classes. pros: you see what you have in the junit class. cons: You can't reuse them. Create Factories which creates the testdata. pros: Corse grained reuse is guaranteed cons: Fine grained reuse is not possible in a good structured way (when you need testdata with distinct attribute values) you can only access your testdata inside the the project when you build with maven. The builder en.wikipedia.org/wiki/Builder_pattern and the object mother pattern martinfowler.com/bliki/ObjectMother.html are useful in this case. Here is a comparsion: geekswithblogs.net/Podwysocki/archive/2008/01/08/118362.aspx Create Factories in an own project. pros: you can use your testdata in the other projects cons: you have an additional project to manage My Favorite: I like the most to have an own project. When the domain objects have interfaces you can build jaxb implementations of them and hold the testdata in xml. For readability and reuse the best approach for me. You must then only say which xml testdata set to load. If this is overkill I like the builder pattern with the fluent interface:  Employee.build.name(""Bob"").age(32).street(""teststreet""); Your Case: I would hold them under the same package name like the domain classes and add the package testdata at the end. I wouldn't name the factory method only get. Give it a name which is meaningful. If you need an example how to use an inmemory database with spring hibernate and junit here is an example: http://little-tdd-project.origo.ethz.ch/ Most of all I liked the idea about JAXB thanks!"
907,A,"Trouble getting unit testing of RPC on GWT I am trying to get RPC testing using GWT. I am using the default StockWatcher project that is mentioned here I download the project I import it everything works fine. I then run junitcreator in the StockWatcher project: /Users/stephen/Work/gwt/gwt-mac-1.6.4/junitCreator -junit /Users/stephen/Applications/eclipse/plugins/org.junit_3.8.2.v20080602-1318/junit.jar -module stockwatcher -eclipse StockWatcher com.google.gwt.sample.stockwatcher.StockWatcherTest this creates the StockWatcherTest.java in the appropriate test directory and gives me some hosted and web mode launch files. I then also added junit.jar to the classpath for this project. I then modify StockWatcherTest.java to test whether I am capable of making a asynchronous request to the server. Everything looks fine but when I try to run StockWatcherTest.java in hosted mode I get the following error: Starting HTTP on port 0 HTTP listening on port 49569 The development shell servlet received a request for 'greet' in module 'com.google.gwt.sample.stockwatcher.StockWatcher.JUnit.gwt.xml' [WARN] Resource not found: greet; (could a file be missing from the public path or a tag misconfigured in module com.google.gwt.sample.stockwatcher.StockWatcher.JUnit.gwt.xml ?) com.google.gwt.user.client.rpc.StatusCodeException: Cannot find resource 'greet' in the public path of module 'com.google.gwt.sample.stockwatcher.StockWatcher.JUnit' Here is my StockWatcherTest.java class package com.google.gwt.sample.stockwatcher.client; import com.google.gwt.core.client.GWT; import com.google.gwt.junit.client.GWTTestCase; import com.google.gwt.user.client.rpc.AsyncCallback; /** * GWT JUnit tests must extend GWTTestCase. */ public class StockWatcherTest extends GWTTestCase { /** * Must refer to a valid module that sources this class. */ public String getModuleName() { return ""com.google.gwt.sample.stockwatcher.StockWatcher""; } /** * Add as many tests as you like. */ public void testSimple() { GreetingServiceAsync greetingService = GWT.create(GreetingService.class); greetingService.greetServer(""Bob"" new AsyncCallback<String>() { public void onFailure(Throwable caught) { // Show the RPC error message to the user System.out.println(caught); fail(""big time failure""); finishTest(); } public void onSuccess(String result) { System.out.println(""success biatch""); assertTrue(true); } }); delayTestFinish(1000); } } Here is com/google/gwt/sample/stockwatcher/StockWatcher.gwt.xml <?xml version=""1.0"" encoding=""UTF-8""?> <!DOCTYPE module PUBLIC ""-//Google Inc.//DTD Google Web Toolkit 1.6.2//EN"" ""http://google-web-toolkit.googlecode.com/svn/tags/1.6.2/distro-source/core/src/gwt-module.dtd""> <module rename-to='stockwatcher'> <!-- Inherit the core Web Toolkit stuff. --> <inherits name='com.google.gwt.user.User'/> <!-- Inherit the default GWT style sheet. You can change --> <!-- the theme of your GWT application by uncommenting --> <!-- any one of the following lines. --> <inherits name='com.google.gwt.user.theme.standard.Standard'/> <!-- <inherits name='com.google.gwt.user.theme.chrome.Chrome'/> --> <!-- <inherits name='com.google.gwt.user.theme.dark.Dark'/> --> <!-- Other module inherits --> <!-- Specify the app entry point class. --> <entry-point class='com.google.gwt.sample.stockwatcher.client.StockWatcher'/> </module> and here is web.xml in my generated war <?xml version=""1.0"" encoding=""UTF-8""?> <!DOCTYPE web-app PUBLIC ""-//Sun Microsystems Inc.//DTD Web Application 2.3//EN"" ""http://java.sun.com/dtd/web-app_2_3.dtd""> <web-app> <!-- Default page to serve --> <welcome-file-list> <welcome-file>StockWatcher.html</welcome-file> </welcome-file-list> <!-- Servlets --> <servlet> <servlet-name>greetServlet</servlet-name> <servlet-class>com.google.gwt.sample.stockwatcher.server.GreetingServiceImpl</servlet-class> </servlet> <servlet-mapping> <servlet-name>greetServlet</servlet-name> <url-pattern>/stockwatcher/greet</url-pattern> </servlet-mapping> </web-app> So what am I doing wrong? Any help is appreciated. Thank you. 1-you need to add ""finishTest();"" at the end of the ""onSuccess"" method. 2-And to resolve the exeption you got : add in your StockWatcher.gwt.xml the path to your servlet greet. servlet path='/greet' class='com.google.gwt.sample.stockwatcher.server.GreetingServiceImpl'/  i made some simple tests for the stock watcher. you can see them at: http://tayek.com/StockWatcher.zip  Another solution is using GWT SyncProxy (support both sync & async) to test GWT RPC services in JRE See the post at http://www.gdevelop.com/w/blog/2010/01/10/testing-gwt-rpc-services/ for details"
908,A,"JUnit unable to find tests in Eclipse I have a strange issue with JUnit 4 tests in Eclipse 3.5 that I couldn't solve - any hints gratefully received! Initially: I had a test suite working properly with 100+ tests all configured with JUnit 4 annotations. I'd run these typically by right clicking on my source folder and selecting ""Run as JUnit test"". All worked perfectly. Now: When I try to run the test messages all I get is an error ""No tests found with test runner 'JUnit 4'"". Any idea what is happening? I simply can't work out what could have changed to make this fail. My guess is that it is some configuration issue based on the build path or class path? Additional info: I'm using the following plugins: EGit Subversion client (Subclipse) Counterclockwise for Clojure ASM Framework Findbugs Metrics plugin Additional info: doesn't seem to be a problem with JUnit itself as tests still run fine in other projects. Really does seem to be project configuration related but I can't work out what dependency could be missing.... funniest answer for this question. Change the test name to start with ""test"" For example @Test public void checkForTwoStringTest(){....} Change to @Test public void testCheckForTwoString This did work like a miracle.... :-)  My first couple of thoughts (without seeing some example test code): http://stackoverflow.com/questions/2332832/no-tests-found-with-test-runner-junit-4 A lot of suggestions state to restart Eclipse and clean project. In my experience sometimes eclipse gets stuck in a mode where it thinks I am trying to do a hybrid run between JUnit 3 and 4 so naming the '@Test' method with the old verbiage of naming it 'test...' sometimes works. Also if you have any other plugins i.e. m2eclipse or other that you have recently added this could effect your situation as well. If anything you have recently added uses its own version of JUnit this could cause issues. Some sample test code might help with further investigation. Thanks - I did check that thread but none of the tips there seemed to work (restart eclipse or clean project didn't work I'm not using Maven). Also most of the tests are already named something like ""@Test public void testXXX()"" so probably isn't the 3/4 confusion issue. Hmm... try getting rid of the 'test' keyword and running just one test case individually does that work? Also since it seems to be a project configuration issue do you have the same 'working directory' between the 2 projects for your JUnit configuration (arguements tab)? I believe it defaults to ${workspace}. I have ${workspace_loc:ProjectName} as the working directory (which is the default). All the code is then under the src subdirectory. Could that be an issue?  I never found out the real cause of this issue but an upgrade to Eclipse 3.6 finally solved it. I'm having this problem again in Juno (4.2.2) with m2e. I find I need to restart in order to clear it up.  Had this issue too doing a ""Project --> Clean"" and then trying to run the unit tests as usual already did it!  I solved a problem like this differently. None of these solutions worked for me. My project is a Google Web App project a Maven Project and a JREbel project... so any of these could have been messing with my setup. I also found that I could not run any classes with a main() method using Run As -> Java Application. That resulted in a ClassNotFoundException. My solution was to create a new Web Application Project and copy in all the source and set it up again. Now both Java Application and JUnit Test work as expected"
909,A,"@Before and @After exclusion I'm writing some tests for my DAO and because a lot of the tests use a test object that is being saved to my database I've created a setup() and teardown() method with the annotations @Before and @After respectively to avoid redundant code but one of the tests actually one that doesn't really need the test object calls a method in the DAO that contains the call getCurrentSession().clear() (it's a method that uses ScrollableResults to get data from the db in batches and to avoid the memory to fill up it calls flush() and clear() every 50 rows). This creates a problem because the clear() actually removes the test object that is created in setup() from the session so when teardown() is called I get an error:  org.hibernate.NonUniqueObjectException: a different object with the same identifier value was already associated with the session: [nl.ru.cmbi.pdbeter.core.model.domain.PDBEntry#395] Is there a way to tell JUnit to not use the setup() and teardown() on this test or is it better to put all the tests that don't actually need the setup() and teardown() in a new test class? JUnit will run all methods annotated with @Before and @After for each test so you will need to split your tests into two classes.  First of all yes it makes a lot of sense to isolate tests that don't need the @Before or @After behavior in a separate test. Secondly you may want to take a look at the Spring Framework's support for running unit tests within a database transaction that is automatically rolled back at the end of each test so that you do not have to ever worry about some tests affecting the state of an external resource or ask questions about which tests are being run in each order etc. Combine this with an in-memory database (like HSQL) and you won't even need to worry about having a running database somewhere to run against making your build much more portable. Thanks for the quick answer! I will create a new test class then. And I'm already running the tests in transactions using spring ;)  Define your own BlockJUnit4ClassRunner: public class WithoutBeforeAfter extends BlockJUnit4ClassRunner { public WithoutBeforeAfter(Class<?> klass) throws InitializationError { super(klass); } @Override protected Statement withBefores(FrameworkMethod method Object target Statement statement) { if(method.getName().equals(""methodNameIgnore@Before"")){ return statement; }else{ return super.withBefores(method target statement); } } @Override protected Statement withAfters(FrameworkMethod method Object target Statement statement) { if(method.getName().equals(""methodNameIgnore@After"")){ return statement; }else{ return super.withAfters(method target statement); } } } Then use it in your test case: @RunWith(value=WithoutBeforeAfter.class) At least it is available in JUnit 4.8."
910,A,"Alternatives to backdooring java access when unit testing I'm trying to unit test a class with a number of private methods. Each of the private methods can be rather extensive. I can either make the method package scoped (which causes a warning) or I can use the code below to test it: Method method = instance.getClass().getDeclaredMethod(""methodName""); method.setAccessible(true); Object object = method.invoke(instance); assertNotNull(object); The class is not a ""God Object"" and most of its methods touch all of its fields. Any suggestions on how this can be handled better? KLE is right. However if you're working with legacy code and you have to deal with dependencies in private methods one last-ditch option is JMockit. I haven't used it but read about it in The Art of Unit Testing. It's supposed to be able to swap calls from the original class to your fake class. Use this for breaking dependencies on other objects so you can test the public methods not for testing private methods. And use it as a safety net on the way towards refactoring to a decoupled design. +1 nice addition.  I'm curious what kind of warning you are getting when you package scope methods but anyway a way around this kind of issue is to make the test a static inner class of the object. This can have tradeoffs; you may need to exclude the class from being packaged with the code if deployment size is an important issue and you need to be careful about accidentally introducing dependencies from the testing framework into the production code. Another potential option is to have a static inner class that helps expose the methods you need and the test would use it as a pass-through to the private methods. The downside here is that this class essentially exposes the private methods to anyone who wants to use the class so you have to be careful to clearly express that this class is for testing purposes only.  Testing private methods may also be a testing-smell. My reference is the excellent book http://www.manning.com/rainsberger/ You are supposed to test behaviors instead of method : the granularity is a bit different. Example 1 : to test a Pile how do you test push and pop without referencing each other? But testing the global behavior is possible. This reminds us that even for testing objects are the right granularity not methods. Example 2 : when you want to test the interaction between several objects testing method by method is clearly not correct you want to test a global behavior. If a method is not public it cannot be called by the outside world and it's behaviour is less strictly defined. But more than everything if you test a private method you will not be able to refactor your code later. So testing should be done on public code only. All of your private methods are either not used and should be removed from your code base or called by a public method (or multiple public methods) and your inputs to the public methods should be designed to test all of the private methods called by the public method. +1 for ""testing smell"". If you have some many private methods you should probably split the class into smaller classes with each getting a subset of the private methods. Then (some of) these methods can become public methods.  This is probably a stretch if you're not already using it... but Groovy is really great for violating access restrictions for unit testing... you can just reach in and call the methods as though they were public without additional reflection.  You may consider using reflection."
911,A,How to capture a list of specific type with mockito Is there a way to capture a list of specific type using mockitos ArgumentCaptore. This doesn't work: ArgumentCaptor<ArrayList<SomeType> argument = ArgumentCaptor.forClass(ArrayList.class); I find that it's a terrible idea to use concrete list implementation here (`ArrayList`). You can always use `List` interface and if you want represent the fact that it's covariant then you can use `extends`: `ArgumentCaptor>` The nested generics-problem can be avoided with the @Captor annotation: @RunWith(MockitoJUnitRunner.class) public class Test{ @Mock private Service service; @Captor private ArgumentCaptor<ArrayList<SomeType>> captor; @Test public void shouldDoStuffWithListValues() { //... verify(service).doStuff(captor.capture())); } } I prefer using `MockitoAnnotations.initMocks(this)` in the `@Before` method rather than using a runner that excludes the ability to use another runner. However +1 thanks for pointing out the annotation.  Yeah this is a general generics problem not mockito-specific. There is no class object for ArrayList<SomeType> and thus you can't type-safely pass such an object to a method requiring a Class<ArrayList<SomeType>>. You can cast the object to the right type: Class<ArrayList<SomeType>> listClass = (Class<ArrayList<SomeType>>)(Class)ArrayList.class; ArgumentCaptor<ArrayList<SomeType> argument = ArgumentCaptor.forClass(listClass); This will give some warnings about unsafe casts and of course your ArgumentCaptor can't really differentiate between ArrayList<SomeType> and ArrayList<AnotherType> without maybe inspecting the elements. The example you showed can be simplified based on the fact that java makes type inference for the static method calls: `ArgumentCaptor> argument = ArgumentCaptor.forClass((Class) List.class);`
912,A,"how to integration test a DAO built with spring + iBatis I asked a question title of which might have been misleading so I'm going to try to ask the question again with much detailed stuff. (i know question seems long but please bear with me) What I'm trying to do: I simply want to write a test case for my DAO and make it work. I know my DAO's work fine inside the container (app server) but when calling DAO from test case..it does not work. I think because its outside of the container. Stuff in my spring-for-iBatis.xml <bean id=""IbatisDataSourceOracle"" class=""org.springframework.jndi.JndiObjectFactoryBean""> <property name=""jndiName"" value=""jdbc/RSRC/my/db/oltp""/> </bean> <bean id=""MapClient"" class=""org.springframework.orm.ibatis.SqlMapClientFactoryBean""> <property name=""configLocation"" value=""classpath:sql-map-config-oracle.xml""/> <property name=""dataSource"" ref=""IbatisDataSourceOracle""/> </bean> Stuff in my sql-map-config-oracle.xml <sqlMapConfig> <settings enhancementEnabled=""true"" useStatementNamespaces=""true"" /> <transactionManager type=""JDBC""> <dataSource type=""JNDI""> <property name=""DataSource"" value=""jdbc/RSRC/my/db/oltp""/> </dataSource> </transactionManager> <sqlMap resource=""mymapping.xml""/> </sqlMapConfig> my Abstract class: public abstract MyAbstract { public SqlMapClientTemplate getSqlTempl() SQLException{ public static final String ORCL = ""jdbc/RSRC/PIH/eiv/oltp""; try { ApplicationInitializer.getApplicationContext().getBean(""MapClient""); SqlMapClient scl = (SqlMapClient) ApplicationInitializer.getApplicationContext().getBean(""MapClient""); DataSource dsc = (DataSource) MyServiceLocator.getInstance().getDataSource(ORCL); return new SqlMapClientTemplate (dsc scl); } catch (NamingException e) { log.error(ne.getMessage() e); throw new SQLException(""some error here: "" + e.getMessage()); } } } my DAO: public class MyDAO extends MyAbstract{ public AnObject getSomething(String id) { HashMap myMap = new HashMap(); myMap.put(""id"" id); try { setSqlMapClientTemplate(getSqlTempl()); } catch (SQLException ne) { log.error (ne.getMessage() ne); } getSqlMapClientTemplate().queryForList(""mymapping.someproc"" myMap); return AnObject ((List)myMap.get(""firstresult"").get(0)); } } Mytests public class MyDAOTests extends TestCase { public void testMyDAO () { MyDAO myd = new MyDAO(); AnObject ano = myd.getSomething(""15""); assertEquals(""1500"" ano.getContentId()); } } I've tried to present the whole problem in this code snippet. The test fails because it is not able to get the connection to the database...since it is outside the container. I know the design can be fixed to make better use of dependency injections. Can you show me based on this snippet what improvements could be made so that the tests would work? I've been struggling with this and would really appreciate some help. PS: I had to make use of setSqlMapClientTemplate() because I want call to my DAO to be just simple MyDAO myd = new MyDAO() I do not want to make interface for each one of my DAO. Why on earth does your Abstract class have the JNDI name hard-wired into it? Just inject it using Spring. This seems like an application written for Spring by someone who did not actually understand Spring. I feel bad for you. I feel bad for myself...for not knowing spring well enough to correct this code myself. fault is mine... how would you correct it? :) There are lots of problems here. First I count three citations of the JNDI lookup string in your small example. DRY would tell you to write it once and refer to it if possible. Second I don't appreciate your DAO very much. Is this really what you're writing or is this just an example? I don't think this is the Spring idiom. There's no interface. How will you do declarative transactions without one? I'd recommend looking at the Spring docs for iBatis more carefully. Third I'd recommend using JUnit 4.4 or better yet the TestNG idiom - annotations. Also check out the Spring @ContextConfiguration to inject the beans you need in setUp. Fourth your DAOs cannot work because you need a JNDI lookup service running and you can't get one without the container. The answer is to have a DriverManager data source for your tests. UPDATE: Here's an idea to try: Use the Spring idiom for iBatis. If legacy prevents you from doing this perhaps Spring isn't your answer. Once you do this all you have to do is override the data source app context to use DriverManager instead of JNDI for your test. **First**: understand that is bad but what is 1 place I should keep it? you said inject is using spring. what exactly does that mean? Of the three citings am I using spring injection somewhere? how would i do that? **Second**: I am changing legacy code. there is lot of code that is calling the DAO methods. If I do follow the spring idiom exactly then I will have to go back and change the way DAO's are called. **Fourth**: instead of MyServiceLocator I can use DriverManager datasource but what about .getBean(""MapClient"");? how will I get that without the container? and thanks a lot Oh no don't look at Rose India. Not a good source for anything. ""I am changing legacy code"" - then maybe Spring isn't the answer for your situation. What problem are you trying to solve? In the legacy code I am converting JDBC code to be in iBatis. If I take out the JNDI name from my Abstract class then I wont be using MyServiceLocator. But then how Do I get the DataSource in MyAbstract class? I read chapter 11 of spring but they dont show how to get the JNDI name as a bean and have a datasource from it.. you provide a setter - setDataSource(DataSource) - and allow it to be injected by the context (xml file) the whole point then is that your DAO is not tied to a specific datasource or JNDI string making it possible/easy to supply different values mock objects etc for testing. This is the main theme of Spring - to have dependencies injected INTO a class rather than a class going out and finding what it needs @matt: your comment helped me finding this link: http://www.roseindia.net/spring/springpart3.shtml this looks like what I should do. Ok so this would remove hard coded JNDI string in MyAbstract class. Still one problem is coming from the test case (outside container) how do I get MapClient bean (2nd line of try block in getSqlTempl())"
913,A,"How to make @BeforeClass run prior Spring TestContext loads up? it should be piece of cake for programmers using testNG. I have this scenario  @ContextConfiguration(locations={""customer-form-portlet.xml"" ""classpath:META-INF2/base-spring.xml"" }) public class BaseTestCase extends AbstractTestNGSpringContextTests { ... @BeforeClass public void setUpClass() throws Exception { But I'd need the spring context to be load up after @BeforeClass. I I came up with overriding AbstractTestNGSpringContextTests methods : @BeforeClass(alwaysRun = true) protected void springTestContextBeforeTestClass() throws Exception { this.testContextManager.beforeTestClass(); } @BeforeClass(alwaysRun = true dependsOnMethods = ""springTestContextBeforeTestClass"") protected void springTestContextPrepareTestInstance() throws Exception { this.testContextManager.prepareTestInstance(this); } and make my method @BeforeClass(alwaysRun = true dependsOnMethods = ""setUpClass"") protected void springTestContextPrepareTestClass() throws Exception { } But then I get : Caused by: org.testng.TestNGException: org.springframework.test.context.testng.AbstractTestNGSpringContextTests.springTestContextPrepareTestInstance() is not allowed to depend on protected void org.springframework.test.context.testng.AbstractTestNGSpringContextTests.springTestContextBeforeTestClass() throws java.lang.Exception Make it public also doesn't help. Could please anybody mention here if it can be done in a working manner :-) I know that I could load the testContext manually but that wouldn't be so fancy. It works like this but TestContextManager is not visible so I can't call prepareTestInstance() method on it : @Override @BeforeClass(alwaysRun = true dependsOnMethods = ""setUpClass"") public void springTestContextPrepareTestInstance() throws Exception { } Well I created custom DependencyInjectionTestExecutionListener and I have overriden injectDependencies() method and done my init code in there @TestExecutionListeners( inheritListeners = false listeners = {DITestExecutionListener.class DirtiesContextTestExecutionListener.class}) @ContextConfiguration(locations= ""customer-form-portlet.xml"") public class BaseTestCase extends AbstractTestNGSpringContextTests { AND public class DITestExecutionListener extends DependencyInjectionTestExecutionListener { protected void injectDependencies(final TestContext testContext) throws Exception { INITSTUFF(); Object bean = testContext.getTestInstance(); AutowireCapableBeanFactory beanFactory = testContext.getApplicationContext().getAutowireCapableBeanFactory(); beanFactory.autowireBeanProperties(bean AutowireCapableBeanFactory.AUTOWIRE_NO false); beanFactory.initializeBean(bean testContext.getTestClass().getName()); testContext.removeAttribute(REINJECT_DEPENDENCIES_ATTRIBUTE); }"
914,A,"How to unit test a ResponseBody or ResponseEntity sent by a spring mvc Controller? When I do junit tests I do something like this to test spring mvc controllers : request.setRequestURI(""/projects/""+idProject+""/modify""); ModelAndView mv = handlerAdapter.handle(request response controller); where controller tested is like : @RequestMapping(value = ""{id}/modify"") public String content(ModelMap model @PathVariable(""id"") Project object) { But I don't find how to get the ResponseBody answer of request handlers defined like this : @RequestMapping(""/management/search"") public @ResponseBody ArrayList<SearchData> search(@RequestParam(""q"")) { .... .... ArrayList<SearchData> datas = ....; return datas; } looking around I found another solution that in my case is working: http://stackoverflow.com/questions/9138555/spring-framework-test-restful-web-service-controller-offline-i-e-no-server-n It seems very easy to implement and it fits nicely with my test code. Your unit test only needs to verify the contents of the return value of the method: ArrayList<SearchData> results = controller.search(""value""); assertThat(results ...) The @ResponseBody annotation is irrelevant. This is one of the big benefits of annotated controllers - your unit tests can focus on the business logic not the framework mechanics. With pre-annotation controllers half of your test code is spent constructing mock requests responses and associated gubbins like that. It's a distraction. Testing that your code's annotations integrate properly with the framework is the job of integration and/or functional tests. Also for a ResponseEntity result you can just call getBody to get the results. i.e. ResponseEntity results = controller.search(""value""); MyObject obj = results.getBody(); assertThat( obj ...) I hope it's not bad form to link to my own blog. In February I wrote entries directly related to testing controllers. http://digitaljoel.nerd-herders.com/2011/02/05/mock-testing-spring-mvc-controller/ and http://digitaljoel.nerd-herders.com/2011/02/05/using-mockito-to-test-spring-mvc-ajax-interaction/ with the latter testing a method that returns a ResponseEntity. Ok thank you for your answer skaffman you're right I just need to verify the contents of the return value. Thank you digitaljoel I'll have a look at your blog it could be very interesting for me :) I disagree with this answer. Testing the annotations request parameters etc. is very important. The author of the question was on the right track when he was using `HandlerAdapter` @egervari: I didn't say it wasn't important I said it wasn't the job of unit tests as my last sentence says."
915,A,How can I mock static methods in Java 1.4? I'm using JUnit with Mockito. PowerMock can mock static methods but it doesn't seem to be possible to use it with Java 1.4 specially since it needs annotations. Is there any other alternative? Thanks. One alternative that someone (if not you) need to consider is upgrading your target Java platform. Java 1.5 / 5.0 came out in 2004. If you are migrating wait until you have migrated. Use a simply but ugly approach until then. We are migrating but in the meantime we're stuck with 1.4. Try jMockit and double check if you can refactor the code (or is it legacy code?). Mockito says about mocking static Methods Mockito prefers object orientation and dependency injection over static procedural code that is hard to understand & change. (Source) Check also this related post especially Jon's answer. Unfortunately it is legacy code I can't change for the moment. But I agree the long term goal is to not use static methods at all.  Personally I prefer minimizing my use of mocks. If the static method is in your code I'd modify it to make it more unit test friendly. Maybe it shouldn't be static. Or if it has to be maybe you could use a setup method that determines how the static method behaves. If the static method is not in your code you're probably out of luck. I personally favor the use of mocks so to be able to make true unit tests. I agree it shouldn't be static but unfortunately I can't change it for the moment.  (Shameless self promotion here) There is a project PowerMock-Legacy that lets you use PowerMock in Java 1.4. It is a bit verbose and not all functionality is supported but may worth a try. Looks good. Even though I'd like to not need to mock static methods that's probably the best answer to the given question.
916,A,"UrlMappingsTests - Could not load class in test type 'integration'? I'm trying to create a UrlMappingsTest for my grails project and I'm getting the following exception: java.lang.RuntimeException: Could not load class in test type 'integration' at gant.Gant$_dispatch_closure5.doCall(Gant.groovy:391) at gant.Gant$_dispatch_closure7.doCall(Gant.groovy:415) at gant.Gant$_dispatch_closure7.doCall(Gant.groovy) at gant.Gant.withBuildListeners(Gant.groovy:427) at gant.Gant.this$2$withBuildListeners(Gant.groovy) at gant.Gant$this$2$withBuildListeners.callCurrent(Unknown Source) at gant.Gant.dispatch(Gant.groovy:415) at gant.Gant.this$2$dispatch(Gant.groovy) at gant.Gant.invokeMethod(Gant.groovy) at gant.Gant.executeTargets(Gant.groovy:590) at gant.Gant.executeTargets(Gant.groovy:589) My class is pretty simple and I tried to follow the example. What's going on? import grails.test.GrailsUrlMappingsTestCase class UrlMappingTests extends GrailsUrlMappingsTestCase { // static mappings = UrlMappings void testForwardingUrls(){ assertForwardUrlMapping( ""/rest/users/stefan/files"" controller: ""file"" action: ""allFiles"" ); } } With the static line uncommented I get  [groovyc] You attempted to reference a variable in the binding or an instance variable from a static context. [groovyc] You misspelled a classname or statically imported field. Please check the spelling. [groovyc] You attempted to use a method 'UrlMappings' but left out brackets in a place not allowed by the grammar. [groovyc] @ line 7 column 21. [groovyc] static mappings = UrlMappings [groovyc] ^ If you look at the source for GrailsUrlMappingsTestCase you'll see that (as you maybe suspect) static mappings = UrlMappings isn't needed since it does that by default if you don't specify a static mappings variable. Do you get a nested exception under the java.lang.RuntimeException: Could not load class in test type 'integration' when you leave the line commented out? Post the full stacktrace it should work. What version of grails are you using? Hm okay I'm testing mine on 1.3.6 and it's working. Could you post the full stacktrace when you have it commented out? I get a class not found when I run without. I'm using 1.3.7 I started a new project and this fails in 1.3.7. I suspect it's a regression.  I've logged a ticket on the grails bugtracker for this."
917,A,Weird problem with doing tests with junit I'm writing a little library for movies for myself. It's partly for learning TDD. Now I have a problem I can't solve. The code is in here https://github.com/hasanen/MovieLibrary/blob/master/movielibrary-core/src/test/java/net/pieceofcode/movielibrary/service/MovieLibraryServiceITC.java The problem is that when I run the whole class (right click above class name in eclipse) the second test fails because removing doesn't succeed. But when right clicking the method (getMovieGenres_getAllGenresAndRemoveOne_returnsTwoGenreAndIdsAreDifferent) and choosing Run as Junit Test it works. I don't necessarily need the fix but at least some advice on how to find why junit is acting like this. The movieLibraryService variable is annotated @Resource - what is injecting the value into that variable? Sorry I didn't fully understand this one. I'm using https://github.com/hasanen/MovieLibrary/blob/master/movielibrary-core/src/test/java/net/pieceofcode/movielibrary/ITCBase.java at the base of all integration test. So if I understanded your question right the answer is spring? :D Yup that's Spring alright. IMHO the problem is your test isn't real unit test but integration one. So while testing your service you're testing all the layers it uses. I recommend yo to use mocks for lower layers dependencies (EasyMock or something) and use integration tests only for your repository layer. This way you can avoid persistence layer influences while testing service layer.  Something outside your test class (likely a superclass) is creating movieLibraryService and it's not being recreated as often as it needs to be for independent testing. If you add the line movieLibraryService = new MovieLibraryService(); at the top of your testSetUp() method this service will be properly reset before the running of each test method and they will likely work properly. As it is I suspect you're getting a failure on the assertions about size as the size is becoming 6 instead of 3. Alternatively you could add a teardown method (annotated with @After) which removes the contents of the movie library so that it always starts empty.  From the way you explain the problem the problem appears to be in the setUp class. The setUp class runs before every test case invocation. This is the general sequence. 1- Add three movies. 2- Test if three movies exists. 3- Add three movies 4- remove movie item # 1. Since sequence 1-4 works the problem is sequence 3. Either sequence 3 swallows some exception or mutates the underlying object. (may be changes the sequence.) Without knowing how addMovie changes the underlying object its hard to tell. Thanks I solve the problem. I didn't consider that when the second test the id's of genres wasn't 013 as those were in first test. So I made new method which gives me genre by title so I don't need to hard code the id when I'm testing removing. That may solve this instance of the issue but it's not the best tactic. See my answer. I agree with @Don.. Please create a new instance in the setup method.
918,A,"Hamcrest & JUnit & Eclipse: Error messages wrong way round I'm currently running Hamcrest 1.3RC on top of JUnit 4 on top of Eclipse Helios and there's just one thing that bothers me about Hamcrest: The error messages are the wrong way around. Instead of ""Expected: < expectedvalue > but was: < actualvalue >"" I get ""Expected: < actualvalue>  but was: < expectedvalue >"". I mean it's not a big thing but come on ^^ Has really noone of the Hamcrest developers who are doing such a great job in every other way noticed this? Or is this an error unique to my environment? Just tell me if you've got it too or don't have it or better even you know a way to fix this bug. I tried it with both Hamcrest 1.2 and 1.3RC but neither did it correctly. TIA for any kind of hint. Some code to illustrate the issue (names are partly german I hope it doesn't matter): Produkt p2 = pdao.getProdukt(""Kekse""); assertNotNull(p2); assertEquals(p2.getName() ""Kekse""); assertThat(p2.getPreis().doubleValue() closeTo(2.57 0.01)); assertEquals(p2.getFuellmenge() 200); assertEquals(p2.getFuelleinheit() ""G""); assertEquals(p2.isUeber18() false); assertEquals(p2.isAktiv() true); [EDIT2] Using Hamcrest exclusively solved the problem. I'm gonna avoid the assertEquals(......) thing from now on in favor of the assertThat(... is(...)). Sorry I didn't respond. I expected to be notified by e-mail about comments but that option counts just for the answers apparently. I'm gonna include some code. I agree with David Harkness. Most likely you're using the parameter order of the JUnit assert methods and Hamcrest reversed it. How about some code that enables us to reproduce the problem? Read the API docs: http://www.junit.org/apidocs/org/junit/Assert.html All the JUnit assertXxx methods have expectedValue first actualValue second. You're simply calling the method with the parameters in the wrong order. Try assertEquals(""Kekse"" p2.getName()); and you'll be fine. That's good advice in general though: read the documentation before using an API ;)  I use Hamcrest for both Java and PHP and do not have this issue. I suspect that you're passing the expected value before the actual value which is the old xUnit way of asserting things. Hamcrest opts for a more readable structure. Here is the simplified declaration for MatcherAssert.assertThat(): void assertThat(T actual Matcher<T> matcher) Pass the actual value followed by a matcher relating it to the expected value. You can optionally pass a more descriptive message before the actual value. void assertThat(String reason T actual Matcher<T> matcher) Here are a few examples: assertThat(add(2 4) is(6)); assertThat($fruit->hasSeeds() is(true)); assertThat($fruit->getColor() containsString('red')); Always include source code in your question. It increases your chances of being answered and--more importantly--answered correctly. ;) thx for answering I'm about to include some code now. OK I followed your advice on using assertThat(is()) in favor of assertEquals and also included some code. Well it worked. The issue is and was actually only with the JUnit assertEquals Hamcrest having nothing to do with it which is why I'm going to skip the assertEquals() methods from now on."
919,A,"Does JUnit have some weird 30 character length limit on the name of your class? I have a class that I made for some unit tests. Everything was going swimmingly until I changed the name of the class to match the class that I was testing suffixed with TestCase. All of a sudden every time I tried to run the test case in Eclipse I get a ""There is no input configuration for this type"". Someone then suggested that there is a 30 character length limit on the name of the class. I had a look at the class name and it was 32 characters long. I then deleted two characters off the end and tried again and everything worked. I put them back and it stopped working. Is there an explanation for this? EDIT: In response to some of the comments. It is Galileo using Windows XP JUnit 4.4. EDIT 2: Sorry guys. I guess I was wrong. The pattern seems to be that JUnit/Eclipse does not like my class name being TestCase. As soon as I take the TestCase part away it works. It works with a massively long string short strings and everything in between. The name can be anything like ABCTestCase it just CANNOT be for some reason TestCase. Oracle legacy haunt...! :-O Hmmm. Does it run outside of Eclipse? Maybe it is an issue in the JUnit Eclipse plugin instead of in JUnit itself. Just tried a class with a 153 character name (FQN of 178 characters) with no issue in eclipse. Which version of Eclipse? What OS are you on? On Windows there is a maximum path length of 260 so if you have a deeply nested class structure that could be a problem: http://msdn.microsoft.com/en-us/library/aa365247(VS.85).aspx#maximum_path_length Just completed my answer with the different known limits plus a theory on where the process might be over those limits. By default I believe the JUnit runners are set to look for *Test files so it will filter out TestCase. People often use *TestCase as a base class without any tests of its own. Not sure if that's what you're running into. If so it's configurable in the runner.  I am not sure this is linked at all to some kind of OS length limitation but rather to: some hard-coded parametrization based on the name of the class. or to some problem with the source folder. Indeed you message looks like: From QuickTip: JUnit: The input type of the launch configuration does not exist If you are getting the above error message in Eclipse IDE while running your test case just make sure that you have the test class as a part of the “eclipse source folder” definition. Easiest way. Right click on the folder –> Build Path –>Use as source folder There could be a length limit issue (you can see some projects undergoing a refactoring ""to be under the file length limit"" (org.eclipse.jdt.core.tests.performance). But that is strange considering the length limit on Windows are: 32767 characters for the maximum command line length for the CreateProcess function. This limitation comes from the UNICODE_STRING structure. CreateProcess is the core function for creating processes so if you are talking directly to Win32 then that's the only limit you have to worry about. But if you are reaching CreateProcess by some other means then the path you travel through may have other limits. 8192 character command line length limit imposed by CMD.EXE. 2048 length due to the INTERNET_MAX_URL_LENGTH (around 2048) command line length limit imposed by the ShellExecute/Ex functions. (If you are running on Windows 95 then the limit is only MAX_PATH.) 32767 characters for the maximum size of your environment (includes the all the variable names plus all the values) Maybe the total length of the javac command does exceed one of those limits and fail to compile one of those JUnit Java classes meaning it can no longer be executed (and trigger the above error message) Can't be. It works if the class name is under 30 characters and not when over 30 characters. The above error is what I am seeing yes. But I also did a Google and the link is the first one I came across. It is most definitely on the build bath. @uriDium: ok (I did not find that link right away since you did not copy *exactly* the error message). May be some length limit related to the Build Path then? Sorry about the exact message :( I was trying to type it in from memory. It seems that way. There seems to be people that are coping with quite a long name. The build path might be quite lengthy at that point. I will keep investigating.  Turns out that it was because I was extending TestCase which makes the JUnit runner think it is still version 3. Even if you tell it to use version 4.  Check your run Configuration under Run -> Run... Your Test has a Configuration there. Check the ""Test class"" field."
920,A,"JUnit theory for hashCode/equals contract The following class serve as generic tester for equals/hashCode contract. It is a part of a home grown testing framework. What do you think about? How can I (strong) test this class? It is a good use of Junit theories? The class: @Ignore @RunWith(Theories.class) public abstract class ObjectTest { // For any non-null reference value x x.equals(x) should return true @Theory public void equalsIsReflexive(Object x) { assumeThat(x is(not(equalTo(null)))); assertThat(x.equals(x) is(true)); } // For any non-null reference values x and y x.equals(y) // should return true if and only if y.equals(x) returns true. @Theory public void equalsIsSymmetric(Object x Object y) { assumeThat(x is(not(equalTo(null)))); assumeThat(y is(not(equalTo(null)))); assumeThat(y.equals(x) is(true)); assertThat(x.equals(y) is(true)); } // For any non-null reference values x y and z if x.equals(y) // returns true and y.equals(z) returns true then x.equals(z) // should return true. @Theory public void equalsIsTransitive(Object x Object y Object z) { assumeThat(x is(not(equalTo(null)))); assumeThat(y is(not(equalTo(null)))); assumeThat(z is(not(equalTo(null)))); assumeThat(x.equals(y) && y.equals(z) is(true)); assertThat(z.equals(x) is(true)); } // For any non-null reference values x and y multiple invocations // of x.equals(y) consistently return true or consistently return // false provided no information used in equals comparisons on // the objects is modified. @Theory public void equalsIsConsistent(Object x Object y) { assumeThat(x is(not(equalTo(null)))); boolean alwaysTheSame = x.equals(y); for (int i = 0; i < 30; i++) { assertThat(x.equals(y) is(alwaysTheSame)); } } // For any non-null reference value x x.equals(null) should // return false. @Theory public void equalsReturnFalseOnNull(Object x) { assumeThat(x is(not(equalTo(null)))); assertThat(x.equals(null) is(false)); } // Whenever it is invoked on the same object more than once // the hashCode() method must consistently return the same // integer. @Theory public void hashCodeIsSelfConsistent(Object x) { assumeThat(x is(not(equalTo(null)))); int alwaysTheSame = x.hashCode(); for (int i = 0; i < 30; i++) { assertThat(x.hashCode() is(alwaysTheSame)); } } // If two objects are equal according to the equals(Object) method // then calling the hashCode method on each of the two objects // must produce the same integer result. @Theory public void hashCodeIsConsistentWithEquals(Object x Object y) { assumeThat(x is(not(equalTo(null)))); assumeThat(x.equals(y) is(true)); assertThat(x.hashCode() is(equalTo(y.hashCode()))); } // Test that x.equals(y) where x and y are the same datapoint // instance works. User must provide datapoints that are not equal. @Theory public void equalsWorks(Object x Object y) { assumeThat(x is(not(equalTo(null)))); assumeThat(x == y is(true)); assertThat(x.equals(y) is(true)); } // Test that x.equals(y) where x and y are the same datapoint instance // works. User must provide datapoints that are not equal. @Theory public void notEqualsWorks(Object x Object y) { assumeThat(x is(not(equalTo(null)))); assumeThat(x != y is(true)); assertThat(x.equals(y) is(false)); } } usage: import org.junit.experimental.theories.DataPoint; public class ObjectTestTest extends ObjectTest { @DataPoint public static String a = ""a""; @DataPoint public static String b = ""b""; @DataPoint public static String nullString = null; @DataPoint public static String emptyString = """"; } If I'm reading this correctly shouldn't the last statement in your equalsIsSymmetric method be assertThat not assumeThat? Yes thanks very much :) So you're going for a home grown solution but do you know of some open source library to do these kinds of common testing? (I also suggest comparable and serializable.) I would be interested in using such a framework. there is no such framework (as I can see). I can contribute this code to an open source project (see below in the Frank's answer) @ivo: I've integrated this class in dollar: http://bitbucket.org/dfa/dollar/src/tip/src/test/java/com/humaorie/dollar/integration/ObjectTest.java Joshua Bloch lays out the contract for hash code and equals in chapter 3 of ""Effective Java"". Looks like you covered a great deal of it. Check the document to see if I missed anything. also the javadoc for Object is very detailed  The notEqualsWorks(Object x Object y) theory is false: two distinct instances may still be logically equal according to their equals method; you're assuming instances are logically different if they're different references. Using your own example above the two distinct datapoints below (a != a2) are nevertheless equal but fail the notEqualsWorks test: @DataPoint public static String a = ""a""; @DataPoint public static String a2 = new String(""a""); true but you should note that the theory has the following requirement: ""User must provide datapoints that are not equal"".  Maybe I'm missing something but the equalsIsSymmetric test is in fact only correctly tested if you have to DataPoints which have the same values (e.g. String a = ""a""; String a2 = ""a"";) Otherwise this test is only done when the 2 parameters are one instance (i.e. equalsIsSymmetric(a a);). In fact you test again if equals obey the 'reflective' requirement instead of the symmetric requirement. yes but in the current setup it is not able to create an 'x' and a 'y' for which holds x != y and x.equals(y) because the notEqualsWorks test will fail in that case. So the equalsIsSymmetric test is only performed for x and y where x == y. for this reason the test has `assumeThat(y.equals(x) is(true))` yeah. Assuming the above setup JUnit will execute: equalsIsSymmetric(a a) and equalsIsSymmetric(b b). Right?  One thing to consider: testing an object's conformance to the equals contract should involve instances of other types. In particular problems are likely to appear with instances of a subclass or superclass. Joshua Bloch gives an excellent explanation of the related pitfalls in Effective Java (I'm reusing duffymo's link so he should get credit for it) -- see the section under Transitivity involving the Point and ColorPoint classes. True your implementation doesn't prevent someone from writing a test that involves instances of a subclass but because ObjectTest is a generic class it gives the impression that all data points should come from a single class (the class being tested). It might be better to remove the type parameter altogether. Just food for thought. indeed! Thanks I'm removing the type parameter T.  The equalsWorks(Object x Object y) method is doing the very same test as equalsIsReflexive(Object x). It should be removed. I also think that notEqualsWorks(Object x Object y) should be removed since it prevents one to do the other theories with data points that are equal even thought the whole testing is about having such objects. Without such data points the reflexivity is the only thing that is tested."
921,A,"In Java how can I validate a thrown exception with JUnit? When writing unit tests for a Java API there may be circumstances where you want to perform more detailed validation of an exception. I.e. more than is offered by the @test annotation offered by JUnit. For example consider an class that should catch an exception from some other Interface wrap that exception and throw the wrapped exception. You may want to verify: The exact method call that throws the wrapped exception. That the wrapper exception has the original exception as its cause. The message of the wrapper exception. The main point here is that you want to be perf additional validation of an exception in a unit test (not a debate about whether you should verify things like the exception message). What's a good approach for this? I would appreciate if you were able to check the best fitting answer as the correct one. I made a helper similar to the other posted ones: public class ExpectExceptionsExecutor { private ExpectExceptionsExecutor() { } public static void execute(ExpectExceptionsTemplate e) { Class<? extends Throwable> aClass = e.getExpectedException(); try { Method method = ExpectExceptionsTemplate.class.getMethod(""doInttemplate""); method.invoke(e); } catch (NoSuchMethodException e1) { throw new RuntimeException(); } catch (InvocationTargetException e1) { Throwable throwable = e1.getTargetException(); if (!aClass.isAssignableFrom(throwable.getClass())) { // assert false fail(""Exception isn't the one expected""); } else { assertTrue(""Exception captured "" true); return; } ; } catch (IllegalAccessException e1) { throw new RuntimeException(); } fail(""No exception has been thrown""); } } And the template the client should implement public interface ExpectExceptionsTemplate<T extends Throwable> { /** * Specify the type of exception that doInttemplate is expected to throw * @return */ Class<T> getExpectedException(); /** * Execute risky code inside this method * TODO specify expected exception using an annotation */ public void doInttemplate(); } And the client code would be something like this: @Test public void myTest() throws Exception { ExpectExceptionsExecutor.execute(new ExpectExceptionsTemplate() { @Override public Class getExpectedException() { return IllegalArgumentException.class; } @Override public void doInttemplate() { riskyMethod.doSomething(null); } }); } It looks really verbose but if you use an IDE with good autocompletion you will only need to write the type of exception and the actual code under test. (the rest will be done by the IDE :D)  Looking at the proposed answers you can really feel the pain of not having closures in Java. IMHO the most readable solution is ye good old try catch. @Test public void test() { ... ... try { ... fail(""No exception caught :(""); } catch (RuntimeException ex) { assertEquals(Whatever.class ex.getCause().getClass()); assertEquals(""Message"" ex.getMessage()); } }  @akuhn: Even without closures we can get a more readable solution (using catch-exception): import static com.googlecode.catchexception.CatchException.*; public void test() { ... ... catchException(nastyBoy).doNastyStuff(); assertTrue(caughtException() instanceof WhateverException); assertEquals(""Message"" caughtException().getMessage()); } whoa that just about blew up my mind!  i did something very simple testBla(){ try { someFailingMethod() fail(); //method provided by junit } catch(Exception e) { //do nothing } } true did just that just remembered it wrong.. Why don't you do the following? try { someFailingMethod(); fail(); } catch(Exception e) { //nothing } It's more readable Anything more complex is pretty pointless.  As provided in your answer it's a good approach. In addition to this: You could wrap the function expectException into a new Annotation called ExpectedException. An annotated method would look like this: @Test @ExpectedException(class=WrapperException.class message=""Exception Message"" causeException) public void testAnExceptionWrappingFunction() { //whatever you test } This way would be more readable but it's exactly the same approach. Another reason is: I like Annotations :) in this way you must extend the test runner in order to take in account the @ExpectedException That is definitely a good answer. Very readable which IMO is one of the properties of well written code Edison: I haven't written the Annotation yet ;-) +1 This is a very good answer. The only point it doesn't satisfy from the question is 'The exact method call that throws the wrapped exception.' I'm also interested to see the code behind the annotation.  In JUnit 4 it can be easily done using ExpectedException rule. Here is example from javadocs: // These tests all pass. public static class HasExpectedException { @Rule public ExpectedException thrown = ExpectedException.none(); @Test public void throwsNothing() { // no exception expected none thrown: passes. } @Test public void throwsNullPointerException() { thrown.expect(NullPointerException.class); throw new NullPointerException(); } @Test public void throwsNullPointerExceptionWithMessage() { thrown.expect(NullPointerException.class); thrown.expectMessage(""happened?""); thrown.expectMessage(startsWith(""What"")); throw new NullPointerException(""What happened?""); } } I changed the accepted answer to this one because I felt it would take too long to rise to the top and this is probably the most up-to-date. Note I don't work on Java on a day-to-day basis at the moment so am not sure how widely this new approach is being adopted.  The following helper method (adapted from this blog post) does the trick: /** * Run a test body expecting an exception of the * given class and with the given message. * * @param test To be executed and is expected to throw the exception. * @param expectedException The type of the expected exception. * @param expectedMessage If not null should be the message of the expected exception. * @param expectedCause If not null should be the same as the cause of the received exception. */ public static void expectException( Runnable test Class<? extends Throwable> expectedException String expectedMessage Throwable expectedCause) { try { test.run(); } catch (Exception ex) { assertSame(expectedException ex.getClass()); if (expectedMessage != null) { assertEquals(expectedMessage ex.getMessage()); } if (expectedCause != null) { assertSame(expectedCause ex.getCause()); } return; } fail(""Didn't find expected exception of type "" + expectedException.getName()); } The test code can then invoke this as follows: TestHelper.expectException( new Runnable() { public void run() { classInstanceBeingTested.methodThatThrows(); } } WrapperException.class ""Exception Message"" causeException ); Is there any way to compress the in-line class / method in Java?  Until this post I've done my exception validation by doing this: try { myObject.doThings(); fail(""Should've thrown SomeException!""); } catch (SomeException e) { assertEquals(""something"" e.getSomething()); } I spent a few moments thinking about the issue though and came up with the following (Java5 JUnit 3.x): // Functor interface for exception assertion. public interface AssertionContainer<T extends Throwable> { void invoke() throws T; void validate(T throwable); Class<T> getType(); } // Actual assertion method. public <T extends Throwable> void assertThrowsException(AssertionContainer<T> functor) { try { functor.invoke(); fail(""Should've thrown ""+functor.getType()+""!""); } catch (Throwable exc) { assertSame(""Thrown exception was of the wrong type! Expected ""+functor.getClass()+"" actual ""+exc.getType() exc.getClass() functor.getType()); functor.validate((T) exc); } } // Example implementation for servlet I used to actually test this. It was an inner class actually. AssertionContainer<ServletException> functor = new AssertionContainer<ServletException>() { public void invoke() throws ServletException { servlet.getRequiredParameter(request ""some_param""); } public void validate(ServletException e) { assertEquals(""Parameter \""some_param\"" wasn't found!"" e.getMessage()); } public Class<ServletException> getType() { return ServletException.class; } } // And this is how it's used. assertThrowsException(functor); Looking at these two I can't decide which one I like more. I guess this is one of those issues where achieving a goal (in my case the assertion method with functor parameter) isn't worth it in the long run since it's just a lot easier to do those 6+ of code to assert the try..catch block. Then again maybe my 10 minute result of problem solving at friday evening just isn't the most intelligent way to do this. +1 I like this. It gives you a standard set of validation along with a mechanism to extend that validation on a per-test basis. I think it might be more readable if the functor class was defined inline. I thought this a bit more some of the code can be gotten rid of with abstract classes which implement fe. the type and validate methods so that one can just write assertThrowsException(new ThrowNPE() { public void invoke() {String s = null;s.charAt(null);}}); Still seems a bit heavy to me but then again this may help asserting complex exceptions while the other approach is better for simpler cases.  For JUNIT 3.x public void test(){ boolean thrown = false; try{ mightThrowEx(); } catch ( Surprise expected ){ thrown = true; assertEquals( ""message"" expected.getMessage()); } assertTrue(thrown ); } Sweet.......!!!"
922,A,"How can i do unit test hashCode function? How can i test this hashCode function? public int hashCode(){ int result = 17 + hashDouble(re); result = 31 * result + hashDouble(im); return result; } Whenever I override equals and hash code I write unit tests that follow Joshua Bloch's recommendations in ""Effective Java"" Chapter 3. I make sure that equals and hash code are reflexive symmetric and transitive. I also make sure that ""not equals"" works properly for all the data members. When I check the call to equals I also make sure that the hashCode behaves as it should. Like this: @Test public void testEquals_Symmetric() { Person x = new Person(""Foo Bar""); // equals and hashCode check name field value Person y = new Person(""Foo Bar""); Assert.assertTrue(x.equals(y) && y.equals(x)); Assert.assertTrue(x.hashCode() == y.hashCode()); } On top of this you could add that it would be reasonable to test that modifications to non-key fields do not cause a modified hashCode to be generated. Also that modifications to key fields do cause modified hashCodes.  hashCode is overrided so as to make instances with same fields identical for HashSet/HashMap etc. So Junit test should assert that two different instances with same values return identical hashCode. Thank you very much. One test is useful but testing millions of values would be more useful and still take less than a second.  Create many (millions of) reproduceably random objects and add all the hashCodes to a Set and check you get almost and many unqiue values as the number of generate ids. To make them reproduceable random use a fixed random seed. Additionally check you can add these Items to a HashSet and find them again. (Using a differnt object with the same values) Make sure your equals() matches your hashCode() behaviour. I would also check that your fields are all final. I've downvoted this for the simple reason that adding randomness to a unit test is a Bad Thing - since the first thing you want to know is *why* the test failed - which is very difficult if its inputs are random. Plus you may need to run billions of objects through your hashcode method to get confidence and that may mean your unit test takes a very long time to run which is also a bad thing. No reason given for the down votes. An explaination would be helpful. @Visage How does using non-random data tell you why a test fails all you need is reproduceability to help diagnose a failed test. You cannot achieve the level of proof you claim with any realistic test driven development. However you can say when the test fails that you have a problem. @Visage billions isn't that many but millions would find most bugs. Just a single test can find a bugs surprising often. I don't think it's a bad answer. If the randomness is used to build up a statistical profile why is that bad? And the length of time it takes to run shouldn't be the deciding factor in whether or not to write a test. It's possible to divide your tests into fast ones that you run every time and longer-running tests that are at your discretion and don't need to be run unless changes are made. Peter's answer doesn't deserve a down vote in my opinion. @Visage: in some cases a random approach makes sense e.g. when something is not supposed to fail or even crash. I write this from personal experience. However I agree that this is not the way to test whether or not some functionality is working and/or creating proper results. @Visage I would agree that its a bad thing to write unit tests which give a false sense of security. ;) @duffymo I assume visage imagined random to mean unreproduceable will edit.  I don't think there's a need to unit-test a hashcode method. Especially if it is generated by either your IDE or a HashCodeBuilder (apache commons) Reading the code is possibly the best check to ensure it makes sense. i.e. its hard to find pathelogical cases by trial and error.  Guess this is a duplicate but here are two links: How should one unit test the hashCode-equals contract? http://jkingdon2000.blogspot.com/2006/11/testing-equals-and-hashcode.html  When you write a mathematical function in general (like hash code) you test some examples in your tests until you are convinced that the function works as expected. How many examples that are depends on your function. For a hash code function I'd think you test at least that two distinct objects that are considered equal have the same hash code. Like assertNotSame(obj1 obj2); // don't cheat assertEquals(obj1.hashcode() obj2.hashcode()); Further you should test that two different values have different hash codes to avoid implementing hashcode() like return 1;."
923,A,"How can I bind a DataSource to an InitialContext for JUnit testing? I'm trying to run JUnit tests on database ""worker"" classes that do a jndi lookup on an InitialContext to get a DataSource. The worker classes are usually running on a Glassfish v3 App Server which has the appropriate jdbc resources defined. The code runs just fine when deployed on the App Server but doesn't run from the JUnit testing environment because obviously it can't find the jndi resources. So I tried to setup an InitialContext in the test class that binds a datasource to the appropriate context but it doesn't work. Here is the code I have in the test @BeforeClass public static void setUpClass() throws Exception { try { // Create initial context System.setProperty(Context.INITIAL_CONTEXT_FACTORY ""org.apache.naming.java.javaURLContextFactory""); System.setProperty(Context.URL_PKG_PREFIXES ""org.apache.naming""); InitialContext ic = new InitialContext(); ic.createSubcontext(""java:""); ic.createSubcontext(""java:/comp""); ic.createSubcontext(""java:/comp/env""); ic.createSubcontext(""java:/comp/env/jdbc""); // Construct DataSource SQLServerConnectionPoolDataSource testDS = new SQLServerConnectionPoolDataSource(); testDS.setServerName(""sqlserveraddress""); testDS.setPortNumber(1433); testDS.setDatabaseName(""dbname""); testDS.setUser(""username""); testDS.setPassword(""password""); ic.bind(""java:/comp/env/jdbc/TestDS"" testDS); DataWorker dw = DataWorker.getInstance(); } catch (NamingException ex) { Logger.getLogger(TitleTest.class.getName()).log(Level.SEVERE null ex); } } Then the DataWorker class has a method with the following code more or less InitialContext ic = null; DataSource ds = null; Connection c = null; PreparedStatement ps = null; ResultSet rs = null; String sql = ""SELECT column FROM table""; try{ ic = new InitialContext(); ds = (DataSource) ic.lookup(""jdbc/TestDS""); c = ds.getConnection(); ps = c.prepareStatement(sql); // Setup the Prepared Statement rs = ps.executeQuery(); if(rs.next){ //Process Results } }catch(NamingException e){ throw new RuntimeException(e); }finally{ //Close the ResultSet PreparedStatement Connection InitialContext } If I change the ic.createSubContext(""java:/comp/env/jdbc""); ic.bind(""java:/comp/env/jdbc/TestDS""testDS); lines to ic.createSubContext(""jdbc""); ic.bind(""jdbc/TestDS""testDS); The worker class is able to find the DataSource but fails giving an error saying that ""username failed to login to the server"". If I pass the DataSource that I create in the JUnit method directly into the worker it can connect and run queries. So I would like to know how to bind a DataSource that can be looked up by the worker class without being in the Web Container. This might be a bit late. The link here is exactly what you need http://blogs.oracle.com/randystuph/entry/injecting_jndi_datasources_for_junit Whilst this may theoretically answer the question [it would be preferable](http://meta.stackexchange.com/q/8259) to include the essential parts of the answer here and provide the link for reference. This gives `java.lang.ClassNotFoundException: org.apache.naming.java.javaURLContextFactory` Better late than never... this is exactly what I needed as well!  When I last tried something like this a few years ago I finally gave up and refactored: at that point you could NOT create a DataSource outside of a container. Maybe you can now maybe someone's mocked something up. Still that smells... You shouldn't have ANY ""business logic"" code directly dependent on DataSources or JNDI lookups or such. That's all plumbing to be wired together outside your code. How flexible is your design? If your code under test is directly dependent on a DataSource (or even obtains its own Connection) refactor it. Injecting a Connection will let you can test all you like with plain old JDBC even using an in-memory implementation and save you from having to prop up a lot of unnecessary (for the test anyway) infrastructure to do it."
924,A,What’s the strategy to recover data during DAO unit test? When I test DAO module in JUnit an obvious problem is: how to recover testing data in database? For instance a record should be deleted in both test methods testA() and testB() that means precondition of both test methods need an existing record to be deleted. Then my strategy is inserting the record in setUp() method to recover data. What’s your better solution? Or your practical idea in such case? Thanks Bozho is correct of course but just to add a bit of detail: If possible unit tests set up their data before manipulating it and then clean up after themselves. So ideally you would not be trampling on existing data (perhaps copied from production) for testing but setting some up as part of the test; that's practically the only way you can be assured that your test will be testing what you intended.  A simple solution is to roll back the transaction after the test (for example in tearDown()). That way the tests can make all the changes they like but they won't change the database (don't forget to turn autoCommit off for the connection). There is a drawback though: If a test fails you can't peek at the database to figure out why. Therefore most of my tests clean the database before they run and they use autoCommit so I can see the last state where it failed run a fixed SQL query against the data etc. 'roll back' solution is cool!  I'd make a method called createRecord(). It may be a test-method as well. And whenever you need to create a record call that method from your other test methods.  Maybe DBUnit can help you out. It allows to have a TEST database in a predefined state before executing each test. Once it set upped it's really easy to test database driven applications.
925,A,"Android Eclipse Plugin: Instrumentation Test Runner not specified I'm getting this error when trying to run unit tests from Eclipse with an Android Project. The list of Instrumentation Test Runners is empty in the Android preferences. [2009-06-17 23:57:51 - MyApp] ERROR: Application does not specify a android.test.InstrumentationTestRunner instrumentation or does not declare uses-library android.test.runner Google-fu failing me. It's also annoyingly decided that because I tried to run a unit test once that's what I always want to do... Grr The problem is when you created the project you would have had a AVD so these configuration would be missing. My suggested way is first create the AVD and then create the android project :). If you would have already created the project and if does not have much code you have written I would suggest to delete it and create a new one.  One thing I noticed in this discussion that might be tripping some people up is that you need to make sure the ""instrumentation"" element in your manifest is a child of ""manifest"" and not of ""application."" (The examples here are correct but this easy to mix up.) http://developer.android.com/guide/topics/manifest/instrumentation-element.html If you put your instrumentation stuff inside application it won't be picked up and your choices in the Eclipse ADT plugin for instrumentation runner may be blank. (But no error is thrown or shown etc.) and uses-library needs to be a child of application  In the Run Configuration you may have Android JUnit Test if there are any new launch configuration entries inside this you delete it and then run your application it will run. NOTE - This is likely to be the solution if you tried to run the test case before adding the correct lines to the manifest as described in the answer from Josef. If you have done this delete the configuration (which will be complaining that no instrumentation test runner has been specified in its header) and then run it as an Android Junit Test again and it will create a valid configuration picking up the correct stuff that you have added to the manifest (see Josef's answer for this). Thanks worked for me  Just do a right click on your test class from eclipse IDE and click on ""Run As"". After this select ""run Configuration"" which will launch a Confiuration Window in eclipse and you need to click on the radio button next to the ""Instrumentation Runner"" and select the configured Instrumentation Runner from the drop down. Now click on apply and then click on Run . I think this will solve your problem. Thanks Smruti That's the right answer when you want to run one test class alone and you already have define instrumentation and uses-library in your manifest.  Besides ensuring that the below items are declared in the manifest of your test app check in the Run Configuration that the ""Instrumentation runner"" field is set to ""com.google.android.apps.common.testing.testrunner.GoogleInstrumentationTestRunner"". This what I ran into when figuring out why I test wouldn't run. Manifest: <instrumentation android:name=""android.test.InstrumentationTestRunner"" android:targetPackage=""your.package"" android:label=""your tests label"" /> and... <uses-library android:name=""android.test.runner"" />  You're probably missing the following in your AndroidManifest.xml: <instrumentation android:name=""android.test.InstrumentationTestRunner"" android:targetPackage=""your.package"" android:label=""your tests label"" /> and <uses-library android:name=""android.test.runner"" /> thanks! also the wrong package name can be the cause I got this error after I changed my package name in XML. Once I ran a clean (and android updated my run configurations) it fixed it. I'd upvote this twice if I could. Note to self: uses-library goes within the application element.  It's not in your code it's just eclipse is a little buggy. In your run configurations it could be trying to run a jUnit test but select Run Application and that error will go away."
926,A,"NetBeans 6.9 and JUnit 4.8.2 package visibility problem I recently upgraded from NetBeans 6.7.1 to NetBeans 6.9 and my old JUnit tests are showing ""cannot find symbol"" errors in the NetBeans editor around the import statements. Everything builds correctly on these unit tests and I can still run/debug the unit tests without any issues. However auto-complete within the editor doesn't work at all for the classes it can't find. This is making it difficult to create new JUnit tests. I can import classes from another module within the suite correctly. Java API classes also import without issue. This seems like a dependency issue but I don't know how to fix it. Here's the hypothetical NetBeans project structure I'm running on: MyCodeSuite - MyNetBeansModule1 - - Source Packages - - - com.company.module1.foo - - - - DoSomething1 - - - - DoSomethingElse1 - - - - ClassInQuestion - - Unit Test Packages - - - com.company.module1.foo - - - - ClassInQuestionTest - MyNetBeansModule2 - - Source Packages - - - com.company.module2.foo - - - - DoSomething2 - - - - DoSomethingElse2 ClassInQuestionTest.java: package com.company.module1.foo; import com.company.module1.foo.DoSomething1; // this is where the editor starts showing errors; these errors are propagated throughout the code anywhere ""DoSomething1"" shows up import com.company.module2.foo.DoSomething2; // the editor doesn't complain for this one // These imports all work correctly import java.util.List; import org.junit.After; import org.junit.Before; import org.junit.Test; import static org.junit.Assert.*; I've duplicated this for new unit tests as well as existing unit tests. It doesn't matter what class I'm importing from thecom.company.module1.foopackage. The editor shows errors for any class within that package and within that same module. Also note that I'm using the ""Create JUnit Tests"" context menu item within NetBeans 6.9 to get going on new unit tests. This is also how the original unit tests classes (under NB 6.7.1) were created. Yeah I've had that problem popup before too. Never been able to figure it out since it'll seemingly randomly afflict projects. @george-pauley agreed on no actual error the editor was just displaying errors. I use the following schemas in my NB 7.0.1 freeform Java project.xml: www.netbeans.org/ns/project/1.xsd www.netbeans.org/ns/freeform-project/1.xsd www.netbeans.org/ns/freeform-project/2.xsd www.netbeans.org/ns/freeform-project-java/3.xsd None of these support <test-dependency> directly. My fix to @stever's original problem via the NB GUI: Project Properties==>Java Sources Classpath==>Java Platform for all ""Source Package Folder""s I set my ""Java Platform"" to a valid ""JDK 1.6"" (even though I'm building with a 32-bit JDK 7u1). This eliminates my JUnit ""cannot find symbol"" errors but does muddy up my {project}/nbproject directory slightly. In addition to modifying ""project.xml"" it created the following 4 new files: genfiles.properties jdk.xml nbjdk.properties nbjdk.xml  This is driving me nuts! It is worth noting that there is no actual error the tests will run fine. It's just the editor that's messed up.  So you are importing from the same package as your unit test is in? There is a setting at Tools -> Options -> Editor -> Hints -> Imports -> Import From The Same Package That might be related. You could also check the options at Hints -> Dependency Scanning. Hello Jorn I found the options you suggested and tried them to no avail. Any other ideas?  One of my co-workers figured it out. Your Unit Tests needs a dependency on the package that it is testing. Unfortunately you cannot do this via the NetBeans GUI. But you CAN do this by directly editing the project.xml file directly. Here is an example the bold part is what I added. ... unit org.netbeans.libs.junit4 com.mycompany.mypackage sigh I can't make this format correctly. You need to add a section that points to the package you want to test. This was it. Fantastic. Caveats: you must not include the tag under and you must include . This was the case with at least my particular migration and module."
927,A,"How to test that a method should take more than X seconds to finish(with JUnit)? Basically I need the opposite behaviour of the @Test(timeout=X) annotation. The problem I want to solve is to detect in some way that the method never ends (as a right behaviour). I am assuming that if the method didn't stop after X seconds I am sure ""it will never end"". Thanks! You could try this: @Test public void methodDoesNotReturnFor5Seconds() throws Exception { Thread t = new Thread(new Runnable() { public void run() { methodUnderTest(); } }); t.start(); t.join(5000); assertTrue(t.isAlive()); // possibly do something to shut down methodUnderTest }"
928,A,"com iplanet ias JAR I've been struggling with this for past couple of days. I am trying to test a DAO outside the container but while running the test case I am getting the error: Error creating bean with name 'SqlMapClient' defined in class path resource [applicationContext.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: com/iplanet/ias/admin/common/ASException I'm using NB to run the tests. people have been saying that I need the above class to be in the run time class path of the test case. However I am absolutely unable to find where actually that jar is...?? I've included all that jars that are on my containers classpath + jars in my projects lib folder to the runtime classpath of the Unit test. Still I get the same error. Also googling for this JAR didnt work either. Maybe someone out there knows where to get this freakin jar from. And hopefully that fixed my problems. This doesn't directly answer your question but I have two advices. First now that I know that you are using Spring I'd suggest to stop using your own ServiceLocator to lookup the JNDI datasource as you mentioned in a previous question. Instead you should use Spring facilities for that and then inject the datasource into yours beans. To get a JDNI datasource use Spring's JndiObjectFactoryBean something like that: <bean id=""dataSource"" class=""org.springframework.jndi.JndiObjectFactoryBean""> <property name=""jndiName""> <value>java:/comp/env/jdbc/myDS</value> </property> </bean> Then when running outside the container (typically when running tests) my advice would be to not use a JNDI datasource. Instead you should use Spring facilities to provide a datasource to your DAOs in another way (e.g. using a DriverManagerDataSource you don't need a real connection pool when running tests). This would allow you to run your tests without having to start iPlanet which makes sense for testing (and you don't want to test iPlanet's connection pool you want to test your DAOs). So create an applicationContext-test.xml to be used during testing with another configuration for the data access. Below a sample configuration for the DriverManagerDataSource: <bean id=""dataSource"" class=""org.springframework.jdbc.datasource.DriverManagerDataSource""> <property name=""driverClassName"" value=""...""/> <property name=""url"" value=""...""/> <property name=""username"" value=""...""/> <property name=""password"" value=""...""/> </bean> This is really the recommended approach (check the chapter Data access using JDBC for more details on the different options). PS: I have no idea from where com/iplanet/ias/admin/common/ASException comes from but it is obviously one of iPlanet itself. If you really want to find out search in all jars of your iPlanet install especially the one referenced in its startup script. But I think that' you'll face JNDI issues after that be warned. when working outside the container I did stop using the JNDI name. instead I use what you suggested. As mentioned in this question: stackoverflow.com/questions/1716636/… The xml snippet you gave above wont be enough. I will still need SqlMapClientFactoryBean (because my DAO uses getSqlMapClientTemplate().queryForXXX(). If I don't have SqlMapClientFactoryBean then I get an error saying 'No sqlMapClient Found'. And the error comes when instantiating THAT bean. I did not explicitly install iplanet. I think i'll install it now to actually find the damn thing. Of course you need `SqlMapClientFactoryBean` I was just covering the datasource part. I'm gonna check the other question because something is confusing me I don't get how a iPlanet exception can be thrown if you're not doing anything with iPlanet... exactly!! and Im trying to do everything outside the container. whatever iplanet is..it has to be related to the appserver..but appserver is not even in the picture here. Please let me know if you want me to post something else (code wise). I'll accept your answer here. we can move to the other question. thanks"
929,A,"Forcing FileNotFoundException I'm writing a test for a piece of code that has an IOException catch in it that I'm trying to cover. The try/catch looks something like this: try { oos = new ObjectOutputStream(new FileOutputStream(cacheFileName)); } catch (IOException e) { LOGGER.error(""Bad news!"" e); } finally { The easiest way seems to make FileOutputStream throw a FileNotFoundException but perhaps I'm going about this all the wrong way. Anyone out there have any tips? From your comment: Yes I suppose the question should really have been ""How do I create a file that does not exist on both Linux and Windows?"" On windows I can use 'new File(""X:/"")' where X: is a drive letter that does not exist. On Linux this does not work because that is a valid file name. Look at java.io.File.createTempFile. Use it to create the file and then delete it. Probably pass it something like: File tempFile; tempFile = createTempFile(getClass().getName() Long.toString(System.currentTimeMillis()); tempFile.delete(); That should give you a unique name in a platform indendent manner that you can safely use without (much) fear of it existing. IF you want to absolutely ensure the file doesn't get created by another test: As a refinement instead of deleting the temp file grab an exclusive lock on it using RandomAccessFile (in rw mode) then getChannel().lock(). Be sure to unlock() and delete in a finally block at the end of your test. @Kevin Isn't that platform dependent though? On Unix file locking is advisory while on Windows it is mandatory. Thanks. I had tried that earlier but it seemed like the FileOutputStream was auto-creating the file. Turns out I had a ""helper"" method used in the test that auto-created the file. File.createTempFile(""prefix"" ""suffix"") will always create an unique file (for the JVM execution) no matter how many times it's called with the same arguments. So you don't need to worry about specifying an unique name when invoking that method. That is what I figured... I just couldn't be 100% sure given the javadoc.  I hope this is what you meant. if(new File(cachedFile).exists()) { oos = new ObjectOutputStream(new FileOutputStream(cacheFileName)); //do your code here } else { throw new FileNotFoundException(""File doesn't exist!""); } This make no sense. FileOutputStream create a file. To check if the file exists make no sense if the op want create a file.  Two easy ways would be either set cacheFileName to a non-existent file or set the specified file to read-only access. -John  There are two parts to any test: getting it to happen and measuring that you got the correct result. Fault Injection The easiest answer is the one that's already been mentioned which is to set cacheFileName to a file that will never exist. This is likely the most practical answer in this situation. However to cause an arbitrary condition such as an IOException what you really want is Fault Injection. This forces faults in your code without forcing you to instrument your source code. Here are a few methods for doing this: Mock objects You could use a factory method to create an overridden ObjectOutputStream or FileOutputStream. In test code the implementation would throw an IOException when you wanted to and in production code would not modify the normal behavior. Dependency Injection In order to get your Mock Object in the right place you could use a framework such as Spring or Seam to ""inject"" the appropriate object into the class that's doing the work. You can see that these frameworks even have a priority for objects that will be injected so that during unit testing you can override the production objects with test objects. Aspect Oriented Programming Instead of changing the structure of your code at all you can use AOP to inject the fault in the right place. For instance using AspectJ you could define a Pointcut where you wanted the exception to be thrown from and have the Advice throw the desired exception. There are other answers to fault injection on Java; for instance a product called AProbe pioneered what could be called AOP in C long ago and they also have a Java product. Validation Getting the exception thrown is a good start but you also have to validate that you got the right result. Assuming that the code sample you have there is correct you want to validate that you logged that exception. Someone above mentioned using a Mock object for your logger which is a viable option. You can also use AOP here to catch the call to the logger. I assume that the logger is log4j; to solve a similar problem I implemented my own log4j appender which captures log4j output: I specifically capture only ERROR and FATAL which are likely to be the interesting log messages in such a case. The appender is referenced in log4j.xml and is activated during the test run to capture error log output. This is essentially a mock object but I didn't have to restructure all my code that got a log4j Logger.  As the code is currently written you could try to mock out the error() call on the LOGGER object and check to see if it gets called when you expect an IOException. Your desire to test may have uncovered a fundamental problem with the code as it's written. An error is occurring but there's no boolean or flag value (set the filename to a special pattern) that provides other sections of code to determine if writing to the file was successful. If this is contained in a function maybe you could return a boolean or set an object level variable.  A FileNotFoundException would obviously trigger the catch. The javadoc states the cases where it will be thrown. You should also consider that the ObjectOutputStream constructor can throw an IOException so may want to cover this case in your tests.  You could set cacheFileName to an invalid name or to one you know doesn't exist. Yes I suppose the question should really have been ""How do I create a file that does not exist on both Linux and Windows?"" On windows I can use 'new File(""X:/"")' where X: is a drive letter that does not exist. On Linux this does not work because that is a valid file name. Use a character that is invalid for files like * or ?.  I'm writing a test for a piece of code that has an IOException catch in it that I'm trying to cover. I'm not entirely sure I understand your goal but if you want to test if the exception is thrown you can tell the test you expect it to throw the exception: @Test(expected=IOException.class) Your test will then fail if the exception is not thrown and succeed if it is thrown (like if the cacheFileName file does not exist).  cacheFileName = ""thisFileShouldNeverExistAndIfItDoesYouAreScrewingUpMyTests""; Sure you could take steps and jump thru hoops to programatically make sure that the file name will never ever ever exist or you could use a String that will never exist in 99.99999% of cases."
930,A,How perform Junit tests with Struts - Ibatis im using Struts 1.2.x and Ibatis 2.x version for development so i finish yesterday and now i want to perform test this is my first time trying to work with JUnit I already make test but in JavaApp not running on server so how can I simulate or generate mocks with server behavior and wich mocks are recommended for Struts and Ibatis built-in Environment? for example how can i set accerts for login screen? I know about StrutsTestCase im using it and about Cactus are for containers and mocking for non containers scenaries i want to view a demo using struts and ibatis cause I dont know how to retrieve data from mapping. Thanks sorry about my English Thanks in advance! Have you looked at HttpUnit? I tried it for Servlet testing completely different from Struts Actions and such - I know but it had some decent tutorials. You also might want to look at StrutsTestCase for JUnit. That project should be a sufficient start for unit testing struts. It also mentions Cactus which is a framework for testing web applications on the server side. Actually it all depends on how far you want to go with testing. You probably should have started with writing a test first Test Driven Design you know ;) It just works. Not that I do it all the time... I know about StrutsTestCase im using it and about Cactus are for containers and mocking for non containers scenaries i want to view a demo using struts and ibatis cause I dont know how to retrieve data from mapping. Thanks
931,A,Running JUnit Tests in Parallel in IntelliJ IDEA I have a large suite of tests that takes about half an hour to run and would love to be able to the test classes in parallel. Is there a way to do that with IntelliJ IDEA 9? Not a dupe question (as this Q is for IntelliJ) - but there is some discussion here: http://stackoverflow.com/questions/423627/running-junit-tests-in-parallel IDEA will understand parallel JUnit tests only since version 10. There is a tracker issue which you can vote for and watch for progress: http://youtrack.jetbrains.net/issue/IDEA-47103 We plan to add it in IDEA 10 but the priority would depend on the number of votes. Can you offer a source for this? IDEA 10 will be released by the end of 2010. The source is IDEA developer I talked to. I've added tracker issue link to my reply. Wonderful! When do you expect to release IDEA 10?
932,A,"Mockito: Injecting Mocks Throughout Control Flow I'm still learning mockito and right now I'm learning how to inject mocks. I have an object under test with a particular method that depends on other objects. Those objects in turn depend on other objects. I want to mock certain things and have those mocks be used everywhere during execution--throughout the control flow of the method. For example assume there are classes like: public class GroceryStore { public double inventoryValue = 0.0; private shelf = new Shelf(5); public void takeInventory() { for(Item item : shelf) { inventoryValue += item.price(); } } } public class Shelf extends ArrayList<Item> { private ProductManager manager = new ProductManager(); public Shelf(int aisleNumber){ super(manager.getShelfContents(aisleNumber); } } public class ProductManager { private Apple apple; public void setApple(Apple newApple) { apple = newApple; } public Collection<Item> getShelfContents(int aisleNumber) { return Arrays.asList(apple apple apple apple apple); } } I need to write test code with portions along the lines of: .... @Mock private Apple apple; ... when(apple.price()).thenReturn(10.0); ... ... @InjectMocks private GroceryStore store = new GroceryStore(); ... @Test public void testTakeInventory() { store.takeInventory(); assertEquals(50.0 store.inventoryValue); } Whenever apple.price() is called I want my mock apple to be the one used. Is this possible? EDIT: Important note... the class that contains the object I want to mock does have a setter for that object. However I don't really have a handle to that class at the level I'm testing. So following the example although ProductManager has a setter for Apple I don't have a way of getting the ProductManager from the GroceryStore object. @Alois: Yes. I'm using Spring. This exact object is being loaded through spring. Is there a way to inject my mock object during testing instead? I think you must create a factory for Apple and then mock the factory @Alois: something along those lines may be right but . . . how do I get ProductManager to use the factory (from within my unit test of GroceryStore)? with a setter in ProductManager to define factory. Are you using any DI (dependency injection) framework ? spring or guice for example The problem is you create objects you depend on by ""new"" instead of injecting it. Inject ProductManager into Shelf (e.g. in constructor) and inject Shelf into GroceryStore. Then in test use mocks. If you want to use @InjectMocks you have to inject by setter methods. By constructor it could look like this: public class GroceryStore { public double inventoryValue = 0.0; private shelf; public GroceryStore(Shelf shelf) { this.shelf = shelf; } public void takeInventory() { for(Item item : shelf) { inventoryValue += item.price(); } } } public class Shelf extends ArrayList<Item> { private ProductManager manager; public Shelf(int aisleNumber ProductManager manager) { super(manager.getShelfContents(aisleNumber); this.manager = manager; } } public class ProductManager { private Apple apple; public void setApple(Apple newApple) { apple = newApple; } public Collection<Item> getShelfContents(int aisleNumber) { return Arrays.asList(apple apple apple apple apple); } } Then you can test it mocking all the objects you depend on: @Mock private Apple apple; ... when(apple.price()).thenReturn(10.0); @InjectMocks private ProductManager manager = new ProductManager(); private Shelf shelf = new Shelf(5 manager); private GroceryStore store = new GroceryStore(shelf); //Then you can test your store. You are welcome. It is generally good practice to inject dependencies instead of creating them by ""new"". It makes your code more testable. Misko Hevery has nice guide about it: http://misko.hevery.com/code-reviewers-guide/flaw-constructor-does-real-work/ I forgot I had this question open! :) The basic answer turned out to be ""No you cannot inject mocks all the way down the control flow of a method call."" Meaning you cannot create a mock and have it apply everywhere automatically. You have to manually ""install"" it where you want it so to speak. It would be much more convenient to demand ""anywhere you see an apple replace it with my mock!"" But that can't be done. You are correct the only solution is to change the code and inject the mocks ""manually."" Thanks for taking the time to respond. @gmale: Actually you can do that (ie mock all instances of a given class independently of who creates them where and when) but it requires a more capable mocking tool such as JMockit (my own) or PowerMockito."
933,A,"Is there any way to replace dynamic methods? Let's say we have an interface which has two methods: public interface MyInterface { public SomeType first(); public SomeType second(); } This interface is implemented by MyInterfaceImpl. Inside the implementation first() calls second() to retrieve some results. I want to build a unit test which would assert things coming out of first() based on what comes out of second() similar to: 1 public class MyInterfaceTest { 2 private MyInterface impl = new MyInterfaceImpl(); 4 @Test 5 public void testFirst() { 6 // modify behaviour of .second() 7 impl.first(); 8 assertSomething(...); 10 // modify behaviour of .second() 11 impl.first(); 12 assertSomethingElse(...); 13 } 14 } Is there an easy way to create a mock on the line 2 so that all calls to selected methods (e.g. first()) would be invoked directly (delegated to MyInterfaceImpl) whereas some other methods (e.g. second()) replaced with mock counterparts? This is actually very easily doable with PowerMock for static methods but I need something similar for dynamic ones. Solutions based on MyInterface mock = EasyMock.createMock(MyInterface.class); MyInterface real = new MyInterfaceImpl(); EasyMock.expect(mock.first()).andReturn(real.first()).anyTimes(); EasyMock.expect(mock.second()).andReturn(_somethingCustom_).anyTimes(); are not good enough especially for interfaces having lots of methods (lots of boilerplate). I need the forwarding behaviour as real actually depends on other mocks. I would expect something like this to be handled by a framework and not by my own class. Is this achievable? How about good old subclassing? I mean something like private MyInterface impl = new MyInterfaceImpl(){ public final MyInterface mock = EasyMock.createMock(MyInterface.class); @override //only the method you need to mock public SomeType second(){ return mock.second(); } } @Test public void testFirst() { // modify behaviour of .second() EasyMock.expect(impl.mock.second()).andReturn(""What I want"").anyTimes(); impl.first(); assertSomething(...); // modify behaviour of .second() EasyMock.expect(impl.mock.second()).andReturn(""Now I want something else"").anyTimes(); impl.first(); assertSomethingElse(...); } You are not testing the exact class you want to test but an anonymous subclass. But we can assume subclassing works ok in Java.;-) I have actually implemented something along these lines. Will accept this answer as I don't think EasyMock can do what I wanted anyway :(  Possibly you could use a Dynamic Proxy. Very interesting advice. However it isn't applicable here - when inside first() second() is called based on `this` which cannot be faked/proxied (unless MyImpl would call second() dynamically which doesn't make sense). But thanks for suggestion anyway because of it I now have learned some fun stuff about dynamic proxies :)  It sounds like you should quite possibly have separate interfaces for first() and second() if the implementation of first() has to call second(). You could then split the implementation too and mock out second() while testing first(). Without a more concrete example of what first() and second() are it's tricky to say for sure. Using EasyMock on the implementation class to mock out only second() call might work but you don't seem to want to do that anyway. This may require telling EasyMock to pass calls to first() through to the normal implementation - I'm not sure. Another option might be to subclass the implementation within the test class (as a nested class) allowing you to override just second() for the purposes of testing. It's pretty ugly though. Personally I don't like faking out part of a class just to test the rest. I'd much rather fake out all a class's dependencies. Unfortunately splitting first() and second() is not an option - our design is centric to business logic type and the idea is to keep all the functions that do work for that type of work under the same roof. Thanks for the answer though. @mindas: I'd encourage you to at least revisit that decision. An account authenticator shouldn't be the same type as an account report generator for example. If you're bound by ugly design decisions you shouldn't be surprised when the testing (and implementation) become ugly too :( (That's assuming it really *is* an ugly design of course. I'm speculating but it sounds like it...) This gets a bit offtopic but are you really suggesting methods shouldn't be using other public methods of the same class? E.g. in Java String.equalsIgnoreCase calls String.regionMatches - and although these are different functions they are not _that_ different to be placed separately. Maybe it's a bad example in the sense that String is not an interface but what I wanted point out that my case is quite similar so first() and second() do a very close thing."
934,A,"Unit testing DAO am I doing it right? I'm starting out unit testing and reviewing Java web programming. The thing is I don't know if I'm doing things right. I'm building a mini blog. I'm using EasyMock for the mock objects. Here's my code: The test case The PostDAO The Post Bean I'd really appreciate your comments and suggestions on how I could improve my code and become a better programmer. Thanks in advance! The pastebin links are dead. without the links this question looses sense. Hard to follow without any code... Some thought from beginnig of your test: public void testPostDAO() { try { new PostDAO(null); fail(""Expected IllegalArgumentException""); } catch (IllegalArgumentException ex) {} new PostDAO(connectionMock); } Following changes are not mandatory and someone can said they are wrong but it's my opinion: Split normal case and error -- that are two different behaviours of your unit under test Rename it to something more clear - to specify what you are doing -- e.g. testPostDAOWithNullArg Your test failes if no exception raised how about if WeryBadStange exception will be raised? You will not catch it and will not get to fail line. If the VeryBadStrangeException is thrown doesn't it either have to be checked or it will be thrown from the test and trigger a failure ? Yeah really you will see that something failed :) But I prefer to check it -- something like -- fail(""Expected Exception1 but got Exception2"") - in my unit testing framework it gives me failed test and a better understanding of the problem than if i just got strange exception message and error test. Yes. Checking that explicit exception is thrown is definitely a good idea  Not related to your unit testing but I'd consider your exception handling in createPost(..) bad practise. The exception isn't completely swallowed but returning a Post object as if everything went okay is not a good idea. See Error Hiding You're right man thanks for pointing that out :)  What are you testing here? IMHO the only thing important here is your SQL command as there's no specific logic to test in your DAO. Testing init code like ""new PostDAO(null)"" is useless when it is so simple. I would rather hsqldb in memory mode to test against a real database and have less mock code. Is there any reason why you could not use JUnit 4 test style using annotation rather than extending TestCase as in JUnit 3 ? 1. I'm testing the whole class. I'm confused how do I know if I have to test a component of my program or not? 3. Ok I'll dive into hsqldb next :) 4. Actually all the tutorials I'm reading teaches JUnit 3. Are there a lot of compelling reasons to switch to JUnit 4? BTW could you point me to the right direction on how to unit test with hsqldb? :) The questions I ask myself are: Who much confidence I get from the tests? What are they not catching? In PostDAOTest if you mistyped the name of a database column in the code and also the test the test would pass but the code would have a bug. Mocks can be useful but they are only as good as the expectations you set on them. A database is complicated enough that it would be very easy to get those expectations wrong. Use an in-memory database like hsqldb for your DAO tests and mocks for classes that talk to the DAOs.  I think it's generally ok (I'd introduce a String constant for your SQL to avoid repeating it btw). Of course the one thing you're not testing with this is the actual interaction with the database (since this is being mocked out). So I would expect a corresponding test (or set of?) that actually interact with the database and insert/rollback as appropriate. Otherwise you're testing something that (at the moment) is quite trivial. A note on test organisation. The below tests 2 things (null construction fails and normal construction succeeds). I would split this into two tests otherwise if the first section fails you never test the second section (in more complex scenarios this makes fault diagnosis more difficult since you may not have as much evidence as you require) public void testPostDAO() { try { new PostDAO(null); fail(""Expected IllegalArgumentException""); } catch (IllegalArgumentException ex) {} new PostDAO(connectionMock); } Some people object to testing new PostDAO(null) due to its triviality. I disagree. One of the reasons you write tests is to ensure behaviour doesn't change unless you expect it to. So the above is good - I'd just split it into two explicit tests. >>I'd introduce a String constant And then what this test will test? So you misstype ""insart"" and ...? Introduce a string constant to avoid repeating it and having inconsistent/misleading values ? Seems fairly obvious and normal good practise to me If you're going to downvote answers can you please indicate why ? I don't think there's anything particularly wrong/contentious/off-topic in the above is there? I'm voting Brian's answer up. There is nothing in it to merit a down vote in my opinion.  There is not point in your Unit Test at all. What you are testing? What logic do you have? What it proves? I can see that your tests prove nothing. Miss typed insert will pass your test. Invalid arguments too. I think you should use integration or functional testing here. Use real database. This will make your tests much lighter plus this will test your code and what is more important it will prove that your code correct. If you are about codecoverage every thing is ok here. Just run functional and unit tests in single session. For example we have special cantegory ""Integration"" that we mark tests that access to the database. So fo regualar work we just disable tests with this category. And only run whole sute when needed. Also our CI server runs whole set. Makes sence? Hi mike how do I know if I have to test a part of my program or not?  Here are some thoughts for a mini-code review. Take them for what they're worth just don't take offense: You should be using JUnit 4.x idiom. No need to extend TestCase. Use the ""@Test"" annotations. Mocking the connection for the DAO is ridiculous. Use a real connection connect to the database and run real queries. Without that your DAO test is worthless. You're missing one of the most important considerations when testing DAOs: the data. Your set up as written isn't helping. What you really want is to seed a database with test data run your tests and then roll the whole thing back so it's as if you were never there. One of the biggest problems with testing databases is making sure your test data is available. Doing things as one transaction is the best way to accomplish it. You're testing the ""happy path"" but you aren't trying any edge conditions at all. Each one should be a separate test call. I can't see your schema but if your table prohibits null values for any of the parameters or enforces a unique constraint I'd write separate tests for each to demonstrate that fact and to prove that it's working properly. What's the right approach for handling them? Throw an exception? You should think about it. Your error handling is poor. Printing a message to the console isn't helpful. You should at least log it using log4j. Your createPost method seems off-base. You pass two parameters and return a Post. Usually with ORM solutions like Hibernate you'll have objects in place already. DAOs should not be creating Connections. They should be passed in by a service layer that knows about units of work and transactions. Speaking of which you have no commit/rollback logic. Thank god because it doesn't belong in a DAO but I'll bet you haven't thought about it. It looks like you mean ""id"" to be a primary key but I see nothing to actually pull it out of the database after your INSERT and populate the object. It won't happen on its own. You'd learn a lot by looking at Spring's JDBC support. I'd recommend it. They've done this better than you ever will. 10. Do you think I should try and dive into Spring? It's actually my real goal to learn it. I'm just doing the mini blog as practice. Special thanks to you duffymo for your patience and your help. Thank you :) Yes I can recommend Spring whole-heartedly. Don't worry about swallowing the whole thing at once. You can do it in pieces. Start with the JDBC support and work your way out. duffymo if we use real connection to DB is this still Unit test? I though it becomes Integration Test doesn't it? You have to use a real connection if you're testing the DAO in my opinion. Anything else is ludicrous. The link is dead What link is dead? I see two and both of them worked. Besides this is almost four years old. SO link is ""dead"". Btw. I don't agree with the post's attitude but it makes many good points"
935,A,"Wicket Spring and Hibernate - Testing with Unitils - Error: Table not found in statement [select relname from pg_class] I've been following a tutorial and a sample application namely 5 Days of Wicket - Writing the tests: http://www.mysticcoders.com/blog/2009/03/10/5-days-of-wicket-writing-the-tests/ I've set up my own little project with a simple shoutbox that saves messages to a database. I then wanted to set up a couple of tests that would make sure that if a message is stored in the database the retrieved object would contain the exact same data. Upon running mvn test all my tests fail. The exception has been pasted in the first code box underneath. I've noticed that even though my unitils.properties says to use the 'hdqldb'-dialect this message is still output in the console window when starting the tests: INFO - Dialect - Using dialect: org.hibernate.dialect.PostgreSQLDialect. I've added the entire dump from the console as well at the bottom of this post (which goes on for miles and miles :-)). Upon running mvn test all my tests fail and the exception is:  Caused by: java.sql.SQLException: Table not found in statement [select relname from pg_class] at org.hsqldb.jdbc.Util.sqlException(Unknown Source) at org.hsqldb.jdbc.jdbcStatement.fetchResult(Unknown Source) at org.hsqldb.jdbc.jdbcStatement.executeQuery(Unknown Source) at org.apache.commons.dbcp.DelegatingStatement.executeQuery(DelegatingStatement.java:188) at org.hibernate.tool.hbm2ddl.DatabaseMetadata.initSequences(DatabaseMetadata.java:151) at org.hibernate.tool.hbm2ddl.DatabaseMetadata.(DatabaseMetadata.java:69) at org.hibernate.tool.hbm2ddl.DatabaseMetadata.(DatabaseMetadata.java:62) at org.springframework.orm.hibernate3.LocalSessionFactoryBean$3.doInHibernate(LocalSessionFactoryBean.java:958) at org.springframework.orm.hibernate3.HibernateTemplate.doExecute(HibernateTemplate.java:419) ... 49 more I've set up my unitils.properties file like so:  database.driverClassName=org.hsqldb.jdbcDriver database.url=jdbc:hsqldb:mem:PUBLIC database.userName=sa database.password= database.dialect=hsqldb database.schemaNames=PUBLIC My abstract IntegrationTest class:  @SpringApplicationContext({""/com/upbeat/shoutbox/spring/applicationContext.xml"" ""applicationContext-test.xml""}) public abstract class AbstractIntegrationTest extends UnitilsJUnit4 { private ApplicationContext applicationContext; } applicationContext-test.xml:  <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:tx=""http://www.springframework.org/schema/tx"" xsi:schemaLocation="" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd""> <bean id=""dataSource"" class=""org.unitils.database.UnitilsDataSourceFactoryBean""/> </beans> and finally one of the test classes:  package com.upbeat.shoutbox.web; import org.apache.wicket.spring.injection.annot.test.AnnotApplicationContextMock; import org.apache.wicket.util.tester.WicketTester; import org.junit.Before; import org.junit.Test; import org.unitils.spring.annotation.SpringBeanByType; import com.upbeat.shoutbox.HomePage; import com.upbeat.shoutbox.integrations.AbstractIntegrationTest; import com.upbeat.shoutbox.persistence.ShoutItemDao; import com.upbeat.shoutbox.services.ShoutService; public class TestHomePage extends AbstractIntegrationTest { @SpringBeanByType private ShoutService svc; @SpringBeanByType private ShoutItemDao dao; protected WicketTester tester; @Before public void setUp() { AnnotApplicationContextMock appctx = new AnnotApplicationContextMock(); appctx.putBean(""shoutItemDao"" dao); appctx.putBean(""shoutService"" svc); tester = new WicketTester(); } @Test public void testRenderMyPage() { //start and render the test page tester.startPage(HomePage.class); //assert rendered page class tester.assertRenderedPage(HomePage.class); //assert rendered label component tester.assertLabel(""message"" ""If you see this message wicket is properly configured and running""); } } Dump from console when running mvn test:  [INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Building shoutbox [INFO] task-segment: [test] [INFO] ------------------------------------------------------------------------ [INFO] [resources:resources {execution: default-resources}] [WARNING] File encoding has not been set using platform encoding Cp1252 i.e. build is platform dependent! [WARNING] Using platform encoding (Cp1252 actually) to copy filtered resources i.e. build is platform dependent! [INFO] Copying 3 resources [INFO] Copying 4 resources [INFO] [compiler:compile {execution: default-compile}] [INFO] Nothing to compile - all classes are up to date [INFO] [resources:testResources {execution: default-testResources}] [WARNING] File encoding has not been set using platform encoding Cp1252 i.e. build is platform dependent! [WARNING] Using platform encoding (Cp1252 actually) to copy filtered resources i.e. build is platform dependent! [INFO] Copying 2 resources [INFO] [compiler:testCompile {execution: default-testCompile}] [INFO] Nothing to compile - all classes are up to date [INFO] [surefire:test {execution: default-test}] [INFO] Surefire report directory: F:\Projects\shoutbox\target\surefire-reports INFO - ConfigurationLoader - Loaded main configuration file unitils-default.properties from classpath. INFO - ConfigurationLoader - Loaded custom configuration file unitils.properties from classpath. INFO - ConfigurationLoader - No local configuration file unitils-local.properties found. ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.upbeat.shoutbox.web.TestViewShoutsPage Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 0.02 sec INFO - Version - Hibernate Annotations 3.4.0.GA INFO - Environment - Hibernate 3.3.0.SP1 INFO - Environment - hibernate.properties not found INFO - Environment - Bytecode provider name : javassist INFO - Environment - using JDK 1.4 java.sql.Timestamp handling INFO - Version - Hibernate Commons Annotations 3.1.0.GA INFO - AnnotationBinder - Binding entity from annotated class: com.upbeat.shoutbox.models.ShoutItem INFO - QueryBinder - Binding Named query: item.getById => from ShoutItem item where item.id = :id INFO - QueryBinder - Binding Named query: item.find => from ShoutItem item order by item.timestamp desc INFO - QueryBinder - Binding Named query: item.count => select count(item) from ShoutItem item INFO - EntityBinder - Bind entity com.upbeat.shoutbox.models.ShoutItem on table SHOUT_ITEMS INFO - AnnotationConfiguration - Hibernate Validator not found: ignoring INFO - notationSessionFactoryBean - Building new Hibernate SessionFactory INFO - earchEventListenerRegister - Unable to find org.hibernate.search.event.FullTextIndexEventListener on the classpath. Hibernate Search is not enabled. INFO - ConnectionProviderFactory - Initializing connection provider: org.springframework.orm.hibernate3.LocalDataSourceConnectionProvider INFO - SettingsFactory - RDBMS: HSQL Database Engine version: 1.8.0 INFO - SettingsFactory - JDBC driver: HSQL Database Engine Driver version: 1.8.0 INFO - Dialect - Using dialect: org.hibernate.dialect.PostgreSQLDialect INFO - TransactionFactoryFactory - Transaction strategy: org.springframework.orm.hibernate3.SpringTransactionFactory INFO - actionManagerLookupFactory - No TransactionManagerLookup configured (in JTA environment use of read-write or transactional second-level cache is not recommended) INFO - SettingsFactory - Automatic flush during beforeCompletion(): disabled INFO - SettingsFactory - Automatic session close at end of transaction: disabled INFO - SettingsFactory - JDBC batch size: 1000 INFO - SettingsFactory - JDBC batch updates for versioned data: disabled INFO - SettingsFactory - Scrollable result sets: enabled INFO - SettingsFactory - JDBC3 getGeneratedKeys(): disabled INFO - SettingsFactory - Connection release mode: auto INFO - SettingsFactory - Default batch fetch size: 1 INFO - SettingsFactory - Generate SQL with comments: disabled INFO - SettingsFactory - Order SQL updates by primary key: disabled INFO - SettingsFactory - Order SQL inserts for batching: disabled INFO - SettingsFactory - Query translator: org.hibernate.hql.ast.ASTQueryTranslatorFactory INFO - ASTQueryTranslatorFactory - Using ASTQueryTranslatorFactory INFO - SettingsFactory - Query language substitutions: {} INFO - SettingsFactory - JPA-QL strict compliance: disabled INFO - SettingsFactory - Second-level cache: enabled INFO - SettingsFactory - Query cache: enabled INFO - SettingsFactory - Cache region factory : org.hibernate.cache.impl.bridge.RegionFactoryCacheProviderBridge INFO - FactoryCacheProviderBridge - Cache provider: org.hibernate.cache.HashtableCacheProvider INFO - SettingsFactory - Optimize cache for minimal puts: disabled INFO - SettingsFactory - Structured second-level cache entries: disabled INFO - SettingsFactory - Query cache factory: org.hibernate.cache.StandardQueryCacheFactory INFO - SettingsFactory - Echoing all SQL to stdout INFO - SettingsFactory - Statistics: disabled INFO - SettingsFactory - Deleted entity synthetic identifier rollback: disabled INFO - SettingsFactory - Default entity-mode: pojo INFO - SettingsFactory - Named query checking : enabled INFO - SessionFactoryImpl - building session factory INFO - essionFactoryObjectFactory - Not binding factory to JNDI no JNDI name configured INFO - UpdateTimestampsCache - starting update timestamps cache at region: org.hibernate.cache.UpdateTimestampsCache INFO - StandardQueryCache - starting query cache at region: org.hibernate.cache.StandardQueryCache INFO - notationSessionFactoryBean - Updating database schema for Hibernate SessionFactory INFO - Dialect - Using dialect: org.hibernate.dialect.PostgreSQLDialect INFO - XmlBeanDefinitionReader - Loading XML bean definitions from class path resource [org/springframework/jdbc/support/sql-error-codes.xml] INFO - SQLErrorCodesFactory - SQLErrorCodes loaded: [DB2 Derby H2 HSQL Informix MS-SQL MySQL Oracle PostgreSQL Sybase] INFO - DefaultListableBeanFactory - Destroying singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@3e0ebb: defining beans [propertyConfigurerdataSourcesessionFactoryshoutServiceshoutItemDaowicketApplicationorg.springframework.aop.config.internalAutoProxyCreatororg.springframework.transaction.annotation.AnnotationTransactionAttributeSource#0org.springframework.transaction.interceptor.TransactionInterceptor#0org.springframework.transaction.config.internalTransactionAdvisortransactionManager]; root of factory hierarchy INFO - sPathXmlApplicationContext - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@a8e586: display name [org.springframework.context.support.ClassPathXmlApplicationContext@a8e586]; startup date [Tue May 04 18:19:58 CEST 2010]; root of context hierarchy INFO - XmlBeanDefinitionReader - Loading XML bean definitions from class path resource [com/upbeat/shoutbox/spring/applicationContext.xml] INFO - XmlBeanDefinitionReader - Loading XML bean definitions from class path resource [applicationContext-test.xml] INFO - DefaultListableBeanFactory - Overriding bean definition for bean 'dataSource': replacing [Generic bean: class [org.apache.commons.dbcp.BasicDataSource]; scope=singleton; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=close; defined in class path resource [com/upbeat/shoutbox/spring/applicationContext.xml]] with [Generic bean: class [org.unitils.database.UnitilsDataSourceFactoryBean]; scope=singleton; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null; defined in class path resource [applicationContext-test.xml]] INFO - sPathXmlApplicationContext - Bean factory for application context [org.springframework.context.support.ClassPathXmlApplicationContext@a8e586]: org.springframework.beans.factory.support.DefaultListableBeanFactory@5dfaf1 INFO - pertyPlaceholderConfigurer - Loading properties file from class path resource [application.properties] INFO - DefaultListableBeanFactory - Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@5dfaf1: defining beans [propertyConfigurerdataSourcesessionFactoryshoutServiceshoutItemDaowicketApplicationorg.springframework.aop.config.internalAutoProxyCreatororg.springframework.transaction.annotation.AnnotationTransactionAttributeSource#0org.springframework.transaction.interceptor.TransactionInterceptor#0org.springframework.transaction.config.internalTransactionAdvisortransactionManager]; root of factory hierarchy INFO - AnnotationBinder - Binding entity from annotated class: com.upbeat.shoutbox.models.ShoutItem INFO - QueryBinder - Binding Named query: item.getById => from ShoutItem item where item.id = :id INFO - QueryBinder - Binding Named query: item.find => from ShoutItem item order by item.timestamp desc INFO - QueryBinder - Binding Named query: item.count => select count(item) from ShoutItem item INFO - EntityBinder - Bind entity com.upbeat.shoutbox.models.ShoutItem on table SHOUT_ITEMS INFO - AnnotationConfiguration - Hibernate Validator not found: ignoring INFO - notationSessionFactoryBean - Building new Hibernate SessionFactory INFO - earchEventListenerRegister - Unable to find org.hibernate.search.event.FullTextIndexEventListener on the classpath. Hibernate Search is not enabled. INFO - ConnectionProviderFactory - Initializing connection provider: org.springframework.orm.hibernate3.LocalDataSourceConnectionProvider INFO - SettingsFactory - RDBMS: HSQL Database Engine version: 1.8.0 INFO - SettingsFactory - JDBC driver: HSQL Database Engine Driver version: 1.8.0 INFO - Dialect - Using dialect: org.hibernate.dialect.PostgreSQLDialect INFO - TransactionFactoryFactory - Transaction strategy: org.springframework.orm.hibernate3.SpringTransactionFactory INFO - actionManagerLookupFactory - No TransactionManagerLookup configured (in JTA environment use of read-write or transactional second-level cache is not recommended) INFO - SettingsFactory - Automatic flush during beforeCompletion(): disabled INFO - SettingsFactory - Automatic session close at end of transaction: disabled INFO - SettingsFactory - JDBC batch size: 1000 INFO - SettingsFactory - JDBC batch updates for versioned data: disabled INFO - SettingsFactory - Scrollable result sets: enabled INFO - SettingsFactory - JDBC3 getGeneratedKeys(): disabled INFO - SettingsFactory - Connection release mode: auto INFO - SettingsFactory - Default batch fetch size: 1 INFO - SettingsFactory - Generate SQL with comments: disabled INFO - SettingsFactory - Order SQL updates by primary key: disabled INFO - SettingsFactory - Order SQL inserts for batching: disabled INFO - SettingsFactory - Query translator: org.hibernate.hql.ast.ASTQueryTranslatorFactory INFO - ASTQueryTranslatorFactory - Using ASTQueryTranslatorFactory INFO - SettingsFactory - Query language substitutions: {} INFO - SettingsFactory - JPA-QL strict compliance: disabled INFO - SettingsFactory - Second-level cache: enabled INFO - SettingsFactory - Query cache: enabled INFO - SettingsFactory - Cache region factory : org.hibernate.cache.impl.bridge.RegionFactoryCacheProviderBridge INFO - FactoryCacheProviderBridge - Cache provider: org.hibernate.cache.HashtableCacheProvider INFO - SettingsFactory - Optimize cache for minimal puts: disabled INFO - SettingsFactory - Structured second-level cache entries: disabled INFO - SettingsFactory - Query cache factory: org.hibernate.cache.StandardQueryCacheFactory INFO - SettingsFactory - Echoing all SQL to stdout INFO - SettingsFactory - Statistics: disabled INFO - SettingsFactory - Deleted entity synthetic identifier rollback: disabled INFO - SettingsFactory - Default entity-mode: pojo INFO - SettingsFactory - Named query checking : enabled INFO - SessionFactoryImpl - building session factory INFO - essionFactoryObjectFactory - Not binding factory to JNDI no JNDI name configured INFO - UpdateTimestampsCache - starting update timestamps cache at region: org.hibernate.cache.UpdateTimestampsCache INFO - StandardQueryCache - starting query cache at region: org.hibernate.cache.StandardQueryCache INFO - notationSessionFactoryBean - Updating database schema for Hibernate SessionFactory INFO - Dialect - Using dialect: org.hibernate.dialect.PostgreSQLDialect INFO - DefaultListableBeanFactory - Destroying singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@5dfaf1: defining beans [propertyConfigurerdataSourcesessionFactoryshoutServiceshoutItemDaowicketApplicationorg.springframework.aop.config.internalAutoProxyCreatororg.springframework.transaction.annotation.AnnotationTransactionAttributeSource#0org.springframework.transaction.interceptor.TransactionInterceptor#0org.springframework.transaction.config.internalTransactionAdvisortransactionManager]; root of factory hierarchy Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 1.34 sec <<< FAILURE! Running com.upbeat.shoutbox.integrations.ShoutItemIntegrationTest Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 0 sec <<< FAILURE! Running com.upbeat.shoutbox.mocks.ShoutServiceTest Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 0.01 sec <<< FAILURE! Results : Tests in error: initializationError(com.upbeat.shoutbox.web.TestViewShoutsPage) testRenderMyPage(com.upbeat.shoutbox.web.TestHomePage) initializationError(com.upbeat.shoutbox.integrations.ShoutItemIntegrationTest) initializationError(com.upbeat.shoutbox.mocks.ShoutServiceTest) Tests run: 4 Failures: 0 Errors: 4 Skipped: 0 [INFO] ------------------------------------------------------------------------ [ERROR] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] There are test failures. Please refer to F:\Projects\shoutbox\target\surefire-reports for the individual test results. [INFO] ------------------------------------------------------------------------ [INFO] For more information run Maven with the -e switch [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3 seconds [INFO] Finished at: Tue May 04 18:19:58 CEST 2010 [INFO] Final Memory: 13M/31M [INFO] ------------------------------------------------------------------------ Any help is greatly appreciated. While applicationContext-test.xml overrides the default datasource to supply a Unitils datasource the applicationContext.xml still declares the following property (that gets replaced during Maven filtering): <prop key=""hibernate.dialect"">${hibernate.dialect}</prop> So if you want to run integration tests against an HSQLDB database (Unitils seems to be configured to do so) my understanding is that you are supposed to use the filter filters-LOCAL.properties that is triggered when using the LOCAL profile and declares: hibernate.dialect=org.hibernate.dialect.HSQLDialect You can check the target/classes/applicationContext.xml to confirm this but it seems obvious that you're not getting the above value but PostgreSQLDialect. The odd part is that the pom.xml of mysticpaste declares the LOCAL profile as activeByDefault (at least the version in the project repository): <profile> <id>LOCAL</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <env>LOCAL</env> </properties> </profile> It should just work. So the question are: what does mvn help:active-profiles say? do you have the this LOCAL profile (and the LOCAL filter)? do you have the latest version of this pom.xml? @John To be clear using a profile to switch from one dialect to another is just one way to go nothing forces you to use this solution (even if I think profiles are nice). But the fact is that you need to change the value of the `hibernate.dialect` property depending on the environment. @Pascal Using profiles seems like a clever choice as I do have several environments (development testing pre-production and production). When deploying to the production server is it enough to use the -P parameter for the deploy command only or would I need to set the profile containing my production settings to activeByDefault before sending it off to tomcat? @John: Dealing with environments is indeed a perfect use case for profiles. Regarding your question my recommendation would be to not change the `activeByDefault` use this for the profile that you use the most frequently (very likely a development profile) don't change your pom to release to another environment. Instead use `-P` when you want another profile (this will disable any profile active by default). Just in case have a look at the [Introduction to Build Profiles](http://maven.apache.org/guides/introduction/introduction-to-profiles.html). I have removed the filters that mysticpaste uses in their pom so I'm not using it. I did this based on an earlier question where it was suggested that I keep away from using filters. Instead of these filters I'm using 1 single application.properties file (which contains ""hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect""). I'll create the LOCAL filter as you suggested with the appropriate HSQLDialect set :-) The suggestion you received here on SO is just one opinion and doesn't necessarily reflect all opinions :) I personally share the point of view expressed by the author of this Wicket series in [this comment](http://www.mysticcoders.com/blog/2009/03/09/5-days-of-wicket-day-1/#comment-1313) and prefer profiles over `PropertyPlaceholderConfigurer`. Thanks a lot Pascal for the help with the original problem and the note on the use of profiles. Your suggestion of making a profile with the appropriate dialect seems to have done the trick!"
936,A,"Does JUnit 3 have something analogous to @BeforeClass? I'm forced to use JUnit 3 for a particular test suite. I understand setUp() and tearDown() serve the function of @Before and @After but is there an analogue of @BeforeClass and @AfterClass for things that should happen once before the tests start and once after all tests are run? No this is a new feature in JUnit4.  OK I should have searched SO better. http://stackoverflow.com/questions/1646586/class-teardown-in-junit-3 public static Test suite() { return new TestSetup(new TestSuite(YourTestClass.class)) { protected void setUp() throws Exception { System.out.println("" Global setUp ""); } protected void tearDown() throws Exception { System.out.println("" Global tearDown ""); } }; }"
937,A,"How do I create more verbose result messaging for a Jmeter Junit Sampler I've noticed that when I run a Junit sampler with Jmeter the messaging is pretty bare. If an exception occurs Jmeter will report it but everything else that happens in my tests has to get outputted using Log4J or another logging tool. Is there a way just to log result back to Jmeter? Do they have an API for this? You can append the assertion results to a log file. Check on ""Append assertion errors"" and ""Append runtime exceptions"" Add listener ""View Results in Table"" Specifya file name and check on ""Errors"" in ""Write results to file/ Read from file"" section. YOu could also click ""Configure"" button to add more information to the output file.  Have you looked into the simple data writer as an option? You could also do a BeanShell script to parse and write results to a file."
938,A,Can JUnit be used as an alternative to browser testing in Spring? I'm building a Spring web app and up until now all of my testing is using a browser. This involves starting the server opening a browser window and checking to see if accessing any of the pages causes an error. This is starting to get repetitive and doesn't seem to be the most efficient way to do this. Since the Junit jar file is already in my project could it be used as an alternative to this browser testing and if so any tips on how to get started making JUnit simulate the act of opening a browser to access the web app? Take a look at Selenium. It allows you to script functional tests using JUnit or TestNG and execute them in a browser automatically.  I suggest you to try the Robot Framework. This is an open source testing framework developed by engineers in Nokia Siemens Networks. It is primarily built on python and the Selenium testing libraries. It also includes support for testing Java/J2EE server side code through Jython libraries. I personally use it in my work sometimes and writing a test case is just as easy as describing an end-to-end flow through the use of Keywords (most of required ones are already inbuilt). You could go ahead and give this a shot if you find Selenium a li'l tough to work with. The Robot framework provides a fairly simple abstraction over raw selenium coupled with the power to make Java/J2EE server-side calls too. Regards Nagendra U M  You can use the HTMLUnit extension to script to drive the web site from JUnit. I used a while back and worked fine for thi site I was doing then. see http://htmlunit.sourceforge.net/
939,A,How can I add source code to my dependency libraries in Maven? For instance I have included into my dependencies junit-addons : junit-addons. But in the maven repository there isn't any source code. And I know it exists (I have downloaded it). How can I modify the dependencies in order to use libraries from my local project instead from the maven repository (I would omit the junit-addons from the respository and use a local JAR and its source code instead). Note: I use m2eclipse. I use free version of Artifactory repository. I created a jar file {artifactId}-{version}-sources.jar and uploaded to the repository into the same group-id as binary jar file. Then in my pom I added dependency: <dependency> <groupId>mygroupid</groupId> <artifactId>artifact</artifactId> <version>1.0</version> <classifier>source</classifier> </dependency> During maven build phase source jar was downloaded to my local repository. I use netbeans 7.0 and it automatically managed everything for me. For example right click on method and choosing go toSource correctly brings me to source code in the jar.  How can I modify the dependencies in order to use libraries from my local project instead from the maven repository You can't. If a sources JAR isn't available in the central repository just put the sources somewhere on your file system in a folder JAR or zip (you could install:install-file a sources archive in your local repository following Maven's conventions) and setup Eclipse to use them (right-click on the JAR under Maven Dependencies in the Package Explorer select Java Source Attachment and setup the Location path). Great reply. Thanks for your help. The source attachment will be lost any time you use `mvn eclipse:eclipse` though. (I think) @sfussenegger With `eclipse:eclipse` certainly. But the OP is using m2eclipse and this setting isn't lost upon modifications of the pom.xml to my knowledge.  After you have downloaded it into your local repository you could make a copy of it. Give it a new artifactId (e.g. libraryName-myVersion) and add dependencies in the pom. Make sure you change the folder names jar names pom names and the artifactId itself in the pom. Store everything in your local repository. Now you can use your hacked version of your dependency. But to be honest I do not thing this is a good idea to do. But maybe it helps/could not be avoided in your case.  You could use the install-file mojo to locally install artifacts into your local maven repository. If you want to share this artifact with others (say your team or another workstation) you could use your own repository manager (e.g. Nexus) and configure it as a mirror for any repository e.g. central. Nexus will fetch (and cache) artifacts from central. Additionally you may upload just about any artifact (like junit-addons sources) to your nexus installation. In order to do configure a mirror you'll have to edit ${user.home}/.m2/settings.xml <settings> <!-- SNIP --> <mirrors> <mirror> <id>nexus-releases</id> <mirrorOf>*</mirrorOf> <!-- replace nexus.example.com with the location of your nexus installation --> <url>http://nexus.example.com/releases</url> </mirror> </mirrors> <profiles> <profile> <id>nexus</id> <repositories> <repository> <id>central</id> <url>http://central</url> <releases><enabled>true</enabled></releases> <snapshots><enabled>false</enabled></snapshots> </repository> </repositories> <id>nexus</id> <pluginRepositories> <pluginRepository> <id>central</id> <url>http://central</url> <releases><enabled>true</enabled></releases> <snapshots><enabled>false</enabled></snapshots> </pluginRepository> </profile> </profiles> </settings>  I've solved the problem in a very straight forward way: I have copied into the folder ${user.home}/.m2/repository/{group-name}/{artifactId}/{version}/ the source file following MAVEN standard: {artifactId}-{version}-sources.jar and it works as a charm! Eclipse checks the local repository and finds the sources. I don't know if this is the MAVEN way though.
940,A,"JUnit Best Practice: Different Fixtures for each @Test I understand that there are @Before and @BeforeClass which are used to define fixtures for the @Test's. But what should I use if I need different fixtures for each @Test? Should I define the fixture in the @Test? Should I create a test class for each @Test? I am asking for the best practices here since both solutions aren't clean in my opinion. With the first solution I would test the initialization code. And with the second solution I would break the ""one test class for each class"" pattern. Tips: Forget the one test class per class pattern it has little merit. Switch to one test class per usage perspective. In one perspective you might have multiple cases: upper boundary lower boundary etc. Create different @Tests for those in the same class. Remember that JUnit will create an instance of the test class for each @Test so each test will get a distinct fixture (set up by the same @Before methods). If you need a dissimilar fixture you need a different test class because you are in a different perspective (see 1.) There is nothing wrong with tweaking the fixture for a particular test but you should try to keep the test clean so it tells a story. This story should be particularly clear when the test fails hence the different well named @Test for each case (see 1.)  If you are positive that your fixture is unique to single test then it belongs to @Test method. This is not typical though. It could be that some part of it is unique or you didn't parametrize/extracted it right but typically you will share a lot of the same data between tests. Ultimately fixture is part of the test. Placing fixture in @Before was adopted as xUnit pattern because tests always: prepare test data/mocks perform operations with SUT validate/assert state/behavior destroy test data/mocks and steps 1 (@Before) and 4 (@After) are reused a lot (at least partially) in related tests. Since xUnit is very serious about test independence it offers fixture methods to guarantee that they always run and test data created/destroyed properly.  I would suggest to create a separate Class based on the different fixtures you need. If you have two different fixtures you need just create two different classes (give them a convenient name). But i would think a second time about that in particular about the difference in the fixtures and why are the different. May be you are on the way to a kind of integration test instead of unit test?"
941,A,"Defining jUnit Test cases Correctly I am new to Unit Testing and therefore wanted to do some practical exercise to get familiar with the jUnit framework. I created a program that implements a String multiplier public String multiply(String number1 String number2) In order to test the multiplier method I created a test suite consisting of the following test cases (with all the needed integer parsing etc) public class MultiplierTest { @Test public void testMultiply() { Multiplier multiplier = new Multiplier(); // Test for 2 positive integers assertEquals(""Result"" 5 multiplier.multiply(""5"" ""1"")); // Test for 1 positive integer and 0 assertEquals(""Result"" 0 multiplier.multiply(""5"" ""0"")); // Test for 1 positive and 1 negative integer assertEquals(""Result"" -1 multiplier.multiply(""-1"" ""1"")); // Test for 2 negative integers assertEquals(""Result"" 10 multiplier.multiply(""-5"" ""-2"")); // Test for 1 positive integer and 1 non number assertEquals(""Result""  multiplier.multiply(""x"" ""1"")); // Test for 1 positive integer and 1 empty field assertEquals(""Result""  multiplier.multiply(""5"" """")); // Test for 2 empty fields assertEquals(""Result""  multiplier.multiply("""" """")); } } In a similar fashion I can create test cases involving boundary cases (considering numbers are int values) or even imaginary values. 1) But what should be the expected value for the last 3 test cases above? (a special number indicating error?) 2) What additional test cases did I miss? 3) Is assertEquals() method enough for testing the multiplier method or do I need other methods like assertTrue() assertFalse() assertSame() etc 4) Is this the RIGHT way to go about developing test cases? How am I ""exactly"" benefiting from this exercise? 5)What should be the ideal way to test the multiplier method? I am pretty clueless here. If anyone can help answer these queries I'd greatly appreciate it. Thank you. Just a little thing - all your tests have ""Result"" as the string. That string should be informative - in the first one for instance you might instead say ""5*1 != 5"". 1) But what should be the expected value for the last 3 test cases above? (a special number indicating error?) As explained by the other answerers it depends on the interface contract of your multiplier. You should think through how you (or its clients in general) are supposed to use it what should happen in case of specific errors or extreme cases etc. In Java the convention is to throw an exception in such cases. 2) What additional test cases did I miss? A couple of cases which come to my mind: // test commutativity assertEquals(""0"" multiplier.multiply(""0"" ""5"")); assertEquals(""-1"" multiplier.multiply(""1"" ""-1"")); assertEquals(""149645"" multiplier.multiply(""173"" ""865"")); assertEquals(""149645"" multiplier.multiply(""865"" ""173"")); // test some more unusual cases of multiplying with 0 assertEquals(""0"" multiplier.multiply(""-5"" ""0"")); assertEquals(""0"" multiplier.multiply(""0"" ""-0"")); // test with numbers starting with '+' assertEquals(""368"" multiplier.multiply(""+23"" ""+16"")); assertEquals(""-368"" multiplier.multiply(""-23"" ""+16"")); // test multiplying huge values without overflow assertEquals(""18446744073709551616"" multiplier.multiply(""4294967296"" ""4294967296"")); assertEquals(""18446744073709551616"" multiplier.multiply(""-4294967296"" ""-4294967296"")); 3) Is assertEquals() method enough for testing the multiplier method or do I need other methods like assertTrue() assertFalse() assertSame() etc In this case all you need is to compare two values for equality. In other tests you may need different kinds of asserts. 4) Is this the RIGHT way to go about developing test cases? How am I ""exactly"" benefiting from this exercise? There is no single ""right"" way to unit testing. What comes closest is probably test driven development which is recommended by many (including myself) if you write your code from scratch. Your benefit from this exercise is probably that you got familiar with JUnit and tried the ""tester hat"" on for a while. 5)What should be the ideal way to test the multiplier method? How is this question different from the previous one? Thank you for the pointers. It makes sense now. @Epitaph purists insist that one should test only one single thing in any test method. I am not that picky. I strive to test a single coherent use case in a test method including possibly several asserts. In your specific case however note that when a method called throws an exception the execution of the calling test method is terminated there so no further calls will be executed. In other words you must put each of your exception tests into its own separate test method. Can I have one method containing multiple assertEquals() for similar test cases? For example having 1 method testArgumentMultiply() for test cases involving illegal inputs like characters special symbol strings empty string. Since all of these inputs will result in the same NumberFormatException?  You should consider adding test cases that verify your exceptions are working fine using the @Expected annotation. Basically write a case that you know should generate a exception and then see that the test passes. A good way to determine if you have missed any cases in complex methods is to run them through a code coverage tool. Run all your tests and then look at the code coverage result. If there are parts of your code that are not visited by your test cases you are probably missing some. You can find a good guide here. can you recommend some code coverage tools? There is a good list here: http://java-source.net/open-source/code-coverage. I suggest you try http://codecover.org/ I personally use Clover from Atlassian but that is not a free tool.  If you are really testing edge conditions and you are expecting string representation of numbers perhaps you can test passing strings to the function method under test @Test(expected= NumberFormatException.class) public void test() { multiplier.multiply(""a"" ""b""); } I disagree with cletus on his implementation as his test case expects an all encompassing IllegalArgumentException I think it is better to test for specific sub classes than using a parent exception. I believe the `IllegalArgumentException` was supposed to be an example only. I concede that point but fact is expecting a parent exception can make you catch lot of other unexpected exceptions.  Firstly your code is in error because you have a class but no functions. I'm assuming all these tests are in one function? If so I would advise against it. Generally you want one test to test one thing so: public class MultiplierTests { @Test public void testSimpleMultiple() { assertEquals(...); } ... } Secondly you're passing an int as the result here: assertEquals(""Result"" 5 multiplier.multiply(""5"" ""1"")); but Multiplier.multiply() returns a String? So how you test this depends on what the result is. If passed in an empty string does it throw an exception? If so you can define your @Test annotation to say it expects an exception to be thrown: @Test(expected = IllegalArgumentException.class) public void test() { multiplier.multiply("""" ""5""); } In your IllegalArgumentException example what should go in the field of expected value if I am using the assertEquals() method with 3 parameters as below @Test(expected=NumberFormatException.class) public void testMultiply() { assertEquals(""4 * a"" ? tester.multiply(""3"" ""a""); } @Epitaph in the `IllegalArgumentException` example don't use `assertEquals()`. Just call the method. If no exception is thrown the test will fail. If an exception other than `IllegalArgumentException` is thrown the test will fail. It will only pass if `IllegalArgumentException` is thrown. Thanks. I tried putting in ""new NumberFormatException()"" for the expected value and it worked. Is that fine too? @Epitaph you use the appropriate exception. I just used `IllegalArgumentException` as an example in my post because i didn't know what exception would be thrown or if one would be thrown at all. @cletus No I wasn't talking about the different Exceptions but about using either just the method or the assert assertEquals() with Exception object as the value for expected parameter. It seems both are fine. Again thanks for the input. @cletus Hope you don't mind me asking another doubt. Suppose I am testing for special characters as the input. Do I need to make test cases for all possible cases like 1) special character in first parameter and a regular character in other 2) special character in second parameter 3) special characters in both parameters 4) more than one special characters ? When is enough?"
942,A,"Need an example for using Junit in Intellij Idea Maybe it's just me but I cannot understand the documentation regarding Junit test integration in Intellij Idea. What I am looking for is a simple tutorial example such as: Here is a method calculating 2+2 and here is a test class testing it to be 4. This is the checkbox you set to make it go. If there is already such a thing on the web or inside the Intellij Idea help please refer me to it. I use Idea 7.0.4 and would like to use JUnit 3.8 or 4.*. TIA. Here is a small sample of how I use intellij with junit public class MathTest { // Press Ctrl-Shift-F10 here to run all tests in class @Test public void twoPlusTwo() { // Press Ctrl-Shift-F10 here to run only twoPlusTwo test assertThat( 2+2 is( 4 ) ); } @Test public void twoPlusThree() { // Press Ctrl-Shift-F10 here to run only twoPlusThree test assertThat( 2+3 is( 5 ) ); } } Once you run the test once it will show up at the top of the screen as a run ""configuration"". Then you can either hit the green triangle to rerun the test or use the debug triangle to run in debug mode with breakpoints enabled."
943,A,"Bringing unit testing to an existing project I'm working on an existing Java EE project with various maven modules that are developed in Eclipse bundled together and deployed on JBoss using Java 1.6. I have the opportunity to prepare any framework and document how unit testing should be brought to the project. Can you offer any advice on... JUnit is where I expect to start is this still the defacto choice for the Java dev? Any mocking frameworks worth setting as standard? JMock? Any rules that should be set - code coverage or making sure it's unit rather than integration tests. Any tools to generate fancy looking outputs for Project Managers to fawn over? Anything else? Thanks in advance. EasyMock is the best known mocking framework(look at google searches :P). the name is not as cool as jMock however it is better. However Mockito allows you to skip one step in creating mocks(you dont need to replay(mock) after setting it up). This is a complex question so just a few notes about our practice at $work: JUnit is indeed still the standard. Most documentation and literature treats JUnit. Mockito seems to be the new star in Java mocking although we still use JMock and think it's fine for our needs. We use the EclEmma Eclipse plugin for checking our test coverage and like it.  If you haven't done so already read Working Effectively with Legacy Code by Michael Feathers. existing code is per defintion legacy.  I've been retrofitting unit tests to a C++ project and it is not pleasant. First thing I did was to identify where most of the 'action' occurs. Then use that to start putting unit tests on the functions that can be test easily. Then once you have the easier ones you can start looking at expanding the coverage virally - attack the functions that have fewer dependancies run through them a few times in a debugger seeing what values are passed in and then write unit tests with those values to make sure you don't break anything. Don't expect a quick fix - it's taken 3 weeks (6hr days 5 days a week) to get 20% coverage but the code spends 80% of the time in that code so I think it has been time well spent and has uncovered quite a few bugs.  Regarding test coverage I think that when you're bringing in unit testing to an existing project it's too early to start setting coverage expectations. You should start by ensuring that you actually can integrate the test framework and get reports from the coverage tools. Once you've done that you can start monitoring coverage and then you can consider targets.  Any tools to generate fancy looking outputs for Project Managers to fawn over? Be careful. A fancy tool for displaying metrics on unit test counts coverage code quality metrics line counts check-in counts and so on can be dangerous in the hands of some project managers. A project manager (who is not in touch with the realities of software development) can get obsessed with the metrics and fail to realize that: they don't give the real picture of the project's health and progress and they can give a completely false picture of the performance of individual team members. You can get silly situations where a manager gives the developers the message that they should (for example) try to achieve maximal unit test coverage for code where this is simply not warranted. Time is spent on pointless work the important work doesn't get done and deadlines are missed. Any rules that should be set - code coverage or making sure it's unit rather than integration tests. Code coverage is more important for parts of the code that are likely to be fragile / buggy. Don't mandate any benchmark coverage level. Unit tests versus integration tests depends on the nature and complexity of the system you are building. Adding lots of unit level tests after the fact is probably a waste of time. It should only be done for class identified as being problematic / needing maintenance work. Adding integration level tests after the fact is useful especially if the projects original developers are no longer around. A decent integration test suite helps to increase your confidence that some change does not break important system functionality. But this needs to be done judiciously. A test suite that tests the N-th degree of a website's look and feel can be a nightmare to maintain ... and impediment to progress.  Concerning the unit testing framework there are mainly two of them : jUnit and TestNG. Both have theuir advantages and both are equally performant. The main dvantage of jUnit is (to my mind) its default incoproration of an Eclipse plugin allowing easy tests calling. Concerning the mocking framework I don't find them to be a required part of your testing approach. Of course they're useful but they solve a specific purpose : testing a behaviour (as opposite to testing an interface - what jUnit allows. With mocking frameworks you're able to test how a specific class implements a specific interface. Will you need it ? Obviously. Will you need it first ? I don't know. Concerning the rules the only one I've found to be useful is simple (as always) : ""always test code that broke at least once."". Consider your bug tracker. Each time a bug is encountered there must be a unit test ensuring there is no regression. It's to my mind the faster way to have quality code. Concerning the fancy- and efficient - output I can recommend you enough to install a continous integration server (Hudson obviously). It will run all your test suite each time code is commited to ensure there are no side effects. it will generate graphs shoiwing the number of test run and so on. it also can integrate code coverage tools and graphs. This continuous integration server will really become fast your testing buddy. This isn't really the purpose of a mocking framework. The true purpose is to be able to test ClassA which uses InterfaceB and to mock B to test how A behaves when B does this or that - to fully isolate the test of A from any implementations of B (otherwise you would be testing both at the same time and then it's not a *unit* test)."
944,A,"Testing console based applications/programs - Java All I have written a PhoneBook application in Java that is command line based. The application basically asks for some details of user like Name Age Address and phone numbers and stores them in a file. Other operations involve looking up PhoneBook by name phone number etc. All the details are entered through console. I am trying to write JUnit test cases for each of the functionalities that I have implemented but not able to figure out how to redirect System.in in the implementation code to something in my JUnit test methods that would supply those values when my actual code stops for user input? Example: My implementation code has: BufferedReader is = new BufferedReader (new InputStreamReader(System.in)); System.out.println(""Please enter your name:""); String name = is.readLine(); // My test cases stop at this line. How can I pass command line values i.e. redirect System.in to my test based values? Hope it makes sense You can find some sample code to do what you describe here: http://illegalargumentexception.blogspot.com/2010/09/java-systemconsole-ides-and-testing.html But it is largely just a way to implement what oxbow_lakes describes in his answer. The library System Rules provides the rule TextFromStandardInputStream for simulating input in JUnit tests. public class YourAppTest { @Rule public TextFromStandardInputStream systemInMock = emptyStandardInputStream(); @Test public void test() { systemInMock.provideText(""name\nsomething else\n""); YourApp.main(); //assertSomething } } For details have a look at the System Rules documentation. Very very useful library! Used it here: https://github.com/binwiederhier/syncany/blob/7ee6174877a646c78e2f5faa36115d01bf6cc9ec/syncany-cli/src/test/java/org/syncany/tests/cli/InitCommandTest.java#L137  System.setIn(new BufferedInputStream(new FileInputStream(""input.txt""))); @darkie15: It's just an example. @darkie15: You could also work with `PipedInputStream` and `PipedOutputStream` or so. Can you please elaborate your answer? Why would I use a file ""input.txt"" ??  Why not write your application to take a Reader as input? That way you can easily replace an InputStreamReader(System.in) with a FileReader(testFile) public class Processor { void processInput(Reader r){ ... } } And then two instances: Processor live = new Processor(new InputStreamReader(System.in)); Processor test = new Processor(new FileReader(""C:/tmp/tests.txt""); Getting used to coding to an interface will bring great benefits in almost every aspect of your programs! Note also that a Reader is the idiomatic way to process character-based input in Java programs. InputStreams should be reserved for raw byte-level processing. I did not understand you. How does the code posted above works in my example? So if I have to pass command line values in my test method I have to use the file `tests.txt` ? If that is the case I am looking to pass values through my test method rather than creating files to just pass values. The two `Reader`'s have the same interface. You use a `FileReader` for your unit tests because it requires no user input (you prepare text files for the purpose of testing different inputs). You use `InputStreamReader(System.in)` for when you execute the program live so it'll still ask for user input directly.  I suggest you to separate the code into three parts: Read input (like name in your example) Do what you need to do with that input Print the results You do not need to test reading input and printing results as that's Java code that is already tested by people writing Java. The only thing you need to test is the thing you are doing whatever that is. Unit tests are named like that because they tests units of code in isolation. You don't test the whole program you test small pieces that are self-contained and have a well-defined function. In unit tests you should not rely on input/output operations. You should provide inputs and expected outputs directly in the unit test. It is sometimes convenient to use File reading operations to supply input or output (e.g. if the amount of data is huge) but as a general rule the more you go into input/output in your unit tests the more complex they become and you are more likely not to do unit but integration tests. In your case you use name somehow. If that is the only parameter then make a method - let's call it nameConsumer - that takes that name does something and returns its result. In your unit tests do something like this: @Test public void testNameConsumer() { // Prepare inputs String name = ""Jon""; String result = nameConsumer(name); assertEquals(""Doe"" result); } Move your println and readLine calls to other methods and use around nameConsumer but not in your unit tests. Read more about this here: http://haacked.com/archive/2008/07/22/unit-test-boundaries.aspx C# example but still: http://dotnet.dzone.com/news/unit-testing-file-io Keep it simple it pays off."
945,A,"Displaying junit fail messages through maven2 I have a Java project that is built through maven2. We use JUnits and occasionally have test failures. I'm wondering if there is a way to display more information when the test fails. When I run the tests through IntelliJ I get something like ""Expected: 3 Actual: 10."" Is there a way to get this same data through maven? Have you tried: Maven Surefire Report Plugin? And of course the test results are available in target/surefire-reports but the format is far from human-readable. Also your CI server should display test failures in a decent format.  You can configure the maven surefire plugin to output more information on the console using the useFile option. Your configuration would look like this then: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <useFile>false</useFile> </configuration> </plugin> If you need this for specific invocations of your build you can also set this parameter via the command line like so: mvn test -Dsurefire.useFile=false Thanks that worked perfectly!"
946,A,"Does TestNG has runner like SpringJUnit4ClassRunner When I write tests in JUnit (in Spring context) I usualy do it like this: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(""classpath:testContext.xml"") public class SimpleTest { @Test public void testMethod() { // execute test logic... } } How can I do the same with TestNG? I'll add more details. With AbstractTestNGSpringContextTests it works but not in a way I want to. I have some test ... @ContextConfiguration(locations = { ""classpath:applicationContextForTests.xml"" }) public class ExampleTest extends AbstractTestNGSpringContextTests { private Boolean someField; @Autowired private Boolean someBoolean; @Test public void testMethod() { System.out.println(someField); Assert.assertTrue(someField); } @Test public void testMethodWithInjected() { System.out.println(someBoolean); Assert.assertTrue(someBoolean); } // setters&getters } and descriptor ... <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd""> <bean id=""exampleTest"" class=""pl.michalmech.ExampleTest""> <property name=""someField""> <ref bean=""someBoolean""/> </property> </bean> <bean id=""someBoolean"" class=""java.lang.Boolean""> <constructor-arg type=""java.lang.String"" value=""true""/> </bean> </beans> The results are ... null true Tests run: 2 Failures: 1 Errors: 0 Skipped: 0 Time elapsed: 0.599 sec <<< FAILURE! Results : Failed tests: testMethod(pl.michalmech.ExampleTest) That's why I asked about runner. possible duplicate of http://stackoverflow.com/questions/2608528/spring-dependency-injection-with-testng Looks like but quite different. http://stackoverflow.com/questions/2608528/spring-dependency-injection-with-testng TestNG does not have runners like JUnit. Sorry. I really did search first. I tried extend by `AbstractTestNGSpringContextTests` byt it didn't work. I'll try one more time maybe I messd up something. Elaborate on ""didn't work""."
947,A,"JUnit and junit.framework.TestSuite - No runnable methods I've made some unit tests (in test class). The tutorial I've read said that I should make a TestSuite for the unittests. Odd is that when I'm running the unit test directly (selecting the test class - Run as jUnit test) everything is working fine altough when I'm trying the same thing with the test suite there's always an exception: java.lang.Exception: No runnable methods. Here is the code of the test suite: import junit.framework.Test; import junit.framework.TestSuite; public class AllTests { public static Test suite() { TestSuite suite = new TestSuite(""Test suite for com.xxx.yyyy.test""); //$JUnit-BEGIN$ suite.addTestSuite(TestCase.class); //$JUnit-END$ return suite; } } Any ideas why this isn't working ? For sure it won't work since you're not telling the test suite what are your test classes. But I'm wondering why you're not using the ""classical way"" for building Test suites which is ant using jUnit's ant tasks.  I'm not experienced in ant - so I'm not using it for testing it right now. Searching the internet it seems like I'm mixing up the old jUnit 3.8 and jUnit 4.0 behavior. Trying now a way to use the ""new behavior"" edited: now it works: AllTest changed to: import org.junit.runner.RunWith; import org.junit.runners.Suite; import org.junit.runners.Suite.SuiteClasses; @RunWith(value=Suite.class) @SuiteClasses(value={TestCase.class}) public class AllTests { } TestCase changed to: import static org.junit.Assert.assertTrue; import org.junit.Test; public class TestCase { @Test public void test1 { assertTrue (tmp.getTermin().equals(soll)); } }  Took me a bit too to figure it out but I think this solves your problem: You're doing a suite.addTestSuite(TestCase.class) while you should've done a suite.addTest(TestCase.class). You can also add a testsuite to a testsuite to create a whole hierarchy of testsuites. In that case you'll have to use suite.addTest(). But note that you then use .suite() and not .class: suite.addTest(MyTestSuite.suite())!"
948,A,"Best way to unit test Collection? I'm just wondering how folks unit test and assert that the ""expected"" collection is the same/similar as the ""actual"" collection (order is not important). To perform this assertion I wrote my simple assert API:- public void assertCollection(Collection<?> expectedCollection Collection<?> actualCollection) { assertNotNull(expectedCollection); assertNotNull(actualCollection); assertEquals(expectedCollection.size() actualCollection.size()); assertTrue(expectedCollection.containsAll(actualCollection)); assertTrue(actualCollection.containsAll(expectedCollection)); } Well it works. It's pretty simple if I'm asserting just bunch of Integers or Strings. It can also be pretty painful if I'm trying to assert a collection of Hibernate domains say for example. The collection.containsAll(..) relies on the equals(..) to perform the check but I always override the equals(..) in my Hibernate domains to check only the business keys (which is the best practice stated in the Hibernate website) and not all the fields of that domain. Sure it makes sense to check just against the business keys but there are times I really want to make sure all the fields are correct not just the business keys (for example new data entry record). So in this case I can't mess around with the domain.equals(..) and it almost seems like I need to implement some comparators for just unit testing purposes instead of relying on collection.containsAll(..). Are there some testing libraries I could leverage here? How do you test your collection? Thanks. If the equals method doesn't check all the fields you can use the Unitils http://unitils.org/ ReflectionAssert class. Calling ReflectionAssert.assertReflectionEquals(expectedCollectionactualCollection) will compare each element reflectively field by field (and this doesn't just apply for collections it will work for any object). this seems like a good way for my collection assertion. If A extends B I assume it will take account of fields from both A and B is that correct? I can't find it in the API documentation but I suppose I can test it out. Is there a way to specify a rule to assert only the fields from A and not B? okay this is pretty awesome... just read up the documentation and it seems like I should be using ReflectionAssert.assertLenientEquals(..) since that doesn't take account of the item order. Thanks much. @limc. You're welcome. I'm not sure on a way to just check the subclass fields (it may be possible I've just never tried).  I couldn't get the last part of jasonmp85's answer to work as is. I included the imports I used because some junit jars include old hamcrest stuff for convenience. This works for me but the assert loop definitely isn't as nice as if hasItems(..) worked as written in jason's answer. import org.hamcrest.Matcher; import org.hamcrest.beans.SamePropertyValuesAs; import org.hamcrest.collection.IsCollectionWithSize; import static org.hamcrest.CoreMatchers.hasItem; import static org.hamcrest.MatcherAssert.assertThat; ... /* * Tests that a contains every element in b (using introspection * to compare bean properties) and that a has the same size as b. */ @Test public void testBeans() { Collection<Foo> a = doSomething(); Collection<Foo> b = expectedAnswer; Collection<Matcher<Foo>> bBeanMatchers = new LinkedList<Matcher<Foo>>(); // create a matcher that checks for the property values of each Foo for(Foo foo: B) bBeanMatchers.add(new SamePropertyValuesAs(foo)); // check that each matcher matches something in the list for (Matcher<Foo> mf : bBeanMatchers) assertThat(a hasItem(mf)); // check that list sizes match assertThat(a IsCollectionWithSize.hasSize(b.size())); } ... Don't need the separate for loops this is more concise: `for (final Foo expectedFoo : b) { assertThat(a hasItem(new SamePropertyValuesAs(expectedFoo))); }`  I'm not sure what version of JUnit you're using but recent ones have an assertThat method which takes a Hamcrest Matcher as an argument. They're composable so you can build up complex assertions about a collection. For instance if you wanted to assert that a collection A contained every element in collection B you could write: import static org.junit.Assert.*; import static org.junit.matchers.JUnitMatchers.*; import static org.hamcrest.core.IsCollectionContaining.*; import static org.hamcrest.collection.IsCollectionWithSize.*; import org.hamcrest.beans.SamePropertyValuesAs; public class CollectionTests { /* * Tests that a contains every element in b (using the equals() * method of each element) and that a has the same size as b. */ @Test public void test() { Collection<Foo> a = doSomething(); Collection<Foo> b = expectedAnswer; assertThat(a both(hasItems(b)).and(hasSize(b.size()))); } /* * Tests that a contains every element in b (using introspection * to compare bean properties) and that a has the same size as b. */ @Test public void testBeans() { Collection<Foo> a = doSomething(); Collection<Foo> b = expectedAnswer; Collection<Matcher<Foo>> bBeanMatchers = new LinkedList<Matcher<Foo>>(); // create a matcher that checks for the property values of each Foo for(Foo foo: B) bBeanMatchers.add(new SamePropertyValuesAs(foo)); assertThat(a both(hasItems(bBeanMatchers)).and(hasSize(b.size()))) } } The first test just uses the equalTo() matcher on every object (which will delegate to your equals implementation). If that's not strong enough you can use the second case which will use getters and setters to compare every element. Finally you can even write your own matchers. The Hamcrest package doesn't come with a matcher for matching by field (as opposed to matching bean properties) but it's trivial to write a FieldMatcher (and indeed is a good exercise). The Matchers are a bit odd at first but if you follow their example of making new Matchers have a static method that returns the matcher you can do a bunch of import statics and your code basically reads like an English sentence (""assert that a both has the items in b and has the same size as b""). You can build up a pretty impressive DSL with these things and make your test code a lot more elegant. Thanks for the info. I guess I never realized these actually exist. :) But I don't think it will work for me in my current scenario. I just read up the documentation and it seems like equalTo() tests object equality using Object.equals and in my case I don't want to futz around with my equals(..) if possible. But I'll keep this useful reference in mind for future use. Right the equalTo() matcher uses equals() but the SamePropertyValuesAs matches all the getters and setters of two JavaBeans. However if you need to match private fields or something you'll have to roll your own if you go this route. @jasonmp85 Does this code still work for you? I'm getting `and (org.hamcrest.Matcher>>>) in CombinableMatcher cannot be applied to (org.hamcrest.Matcher>>)` with Hamcrest 1.2 I upvoted for introducing me to `SamePropertyValueAs` but the `testBeans()` method is broken with junit4.11 and hamcrest1.3. The both..and is borked on the `and`; the hasItems(..) is borked on `assertThat` The simple test() method above doesn't compile for me (with Hamcrest 1.3) I get `error: reference to hasItems is ambiguous both method hasItems(T#1...) in IsCollectionContaining and method hasItems(T#2...) in JUnitMatchers match`?"
949,A,"Force JUnit to run one test case at a time I have a problematic situation with some quite advanced unit tests (using PowerMock for mocking and JUnit 4.5). Without going into too much detail the first test case of a test class will always succeed but any following test cases in the same test class fails. However if I select to only run test case 5 out of 10 for example it will pass. So all tests pass when being run individually. Is there any way to force JUnit to run one test case at a time? I call JUnit from an ant-script. I am aware of the problem of dependant test cases but I can't pinpoint why this is. There are no saved variables across the test cases so nothing to do at @Before annotation. That's why I'm looking for an emergency solution like forcing JUnit to run tests individually. How exactly does a test fail i.e. what do you test for? Are you using JUnit < 4.x? No it's JUnit 4.5. Please edit this information into your question so that everyone can read it. ""There are no saved variables across the test cases"" are you sure about that? What about static variables? I am aware of the problem of dependant test cases but I can't pinpoint why this is. There are no saved variables across the test cases so nothing to do at @Before annotation. That's why I'm looking for an emergency solution like forcing JUnit to run tests individually. The @Before statement is harmless because it is called for every test case. The @Before**Class** is dangerous because it has to be static.  Your description shows me that your unit tests depend each other. That is strongly not recommended in unit tests. Unit test must be independent and isolated. You have to be able to execute them alone all of them (in which order it does not matter). I know that does not help you. The problem will be in your @BeforeClass or @Before statements. There will be dependencies. So refactor them and try to isolate the problem. Probably your mocks are created in your @BeforeClass. Consider to put it into the @Before statement. So there's no instance that last longer than a test case. Okay it's strongly not recommended. Not allowed? By whom?  You should check your whole codebase that there are no static variables which refer to mutable state. Ideally the program should have no static mutable state (or at least they should be documented like I did here). Also you should be very careful about cleaning up what you write if the tests write to the file system or database. Otherwise running the tests may leak some side-effects which makes it hard to make the tests independent and repeatable. Maven and Ant contain a ""forkmode"" parameter for running JUnit tests which specifies whether each test class gets its own JVM or all tests are run in the same JVM. But they do not have an option for running each test method in its own JVM.  I am aware of all the recommendations but to finally answer your question here is a simple way to achieve what you want. Just put this code inside your test case: Lock sequential = new ReentrantLock(); @Override protected void setUp() throws Exception { super.setUp(); sequential.lock(); } @Override protected void tearDown() throws Exception { sequential.unlock(); super.tearDown(); } With this no test can start until the lock is acquired and only one lock can be acquired at a time.  It sounds to me that perhaps it isn't that you are not setting up or tearing down your tests properly (although additional setup/teardown may be part of the solution) but that perhaps you have shared state in your code that you are not aware of. If an early test is setting a static / singleton / shared variable that you are unaware of the later tests will fail if they are not expecting this. Even with Mocks this is very possible. You need to find this cause. I agree with the other answers in that your tests have exposed a problem that should not be solved by trying to run the tests differently.  Your problem is not that JUnit runs all the tests at once you problem is that you don't see why a test fails. Solutions: Add more asserts to the tests to make sure that every variable actually contains what you think Download an IDE from the Internet and use the built-in debugger to look at the various variables Dump the state of your objects just before the point where the test fails. Use the ""message"" part of the asserts to output more information why it fails (see below) Disable all but a handful of tests (in JUnit 3: replace all strings ""void test"" with ""void dtest"" in your source; in JUnit 4: Replace ""@Test"" with ""//D@TEST""). Example: assertEquals(list.toString() 5 list.size());  It seems that your test cases are dependent that is: the execution of case-X affects the execution of case-Y. Such a testing system should be avoided (for instance: there's no guarantee on the order at which JUnit will run your cases). You should refactor your cases to make them independent of each other. Many times the use of @Before and @After methods can help you untangle such dependencies. +1 I wouldnt say ""should"" be avoided but ""must"" be avoided to avoid just this scenario I am aware of the problem of dependant test cases but I can't pinpoint why this is. There are no saved variables across the test cases so nothing to do at @Before annotation. That's why I'm looking for an emergency solution like forcing JUnit to run tests individually. Dependency has many faces: Databases files system properties etc. You should reduce your suite to two conflicting tests. Start commenting code in one test until they succeed. Then uncomment and do the same in the second test. This will give you some insights regarding the cause of your problem. ""There are no saved variables across the test cases"" are you sure about that? What about static variables?  Excuse me if I dont answer your question directly but isn't your problem exactly what TestCase.setUp() and TestCase.tearDown() are supposed to solve? These are methods that the JUnit framework will always call before and after each test case and are typically used to ensure you begin each test case in the same state. See also the JavaDoc for TestCase.  Congratulations. You have found a bug. ;-) If the tests ""shouldn't"" effect each other then you may have uncovered a situation where your code can enter a broken state. Try adding asserts and logging to figure out where the code goes wrong. You may even need to run the tests in a debugger and check your code's internal values after the first test."
950,A,"Maven test isn't picking up JUnit 4 Tests unless class ends with Test on a multi-module project Apache Maven 3.0 (r1004208; 2010-10-04 12:50:56+0100) running mvn test ignores any JUnit 4 tests unless the name of the class is *Test. Having just a single dependency to junit-4.8.2 and target/source configured to be 1.6 That's the standard configuration in the maven surefire plugin. By default the Surefire Plugin will automatically include all test classes with the following wildcard patterns: ""*/Test.java"" - includes all of its subdirectories and all java filenames that start with ""Test"". ""**/*Test.java"" - includes all of its subdirectories and all java filenames that end with ""Test"". ""**/*TestCase.java"" - includes all of its subdirectories and all java filenames that end with ""TestCase"". Source: Inclusions and Exclusions of Tests (this article also shows how you can add additional test class patterns)."
951,A,"How do you get FlexUnit output in JUnit format? I am setting up FlexUnit to run from the command line and want to capture the results in JUnit format so that I can pull them into Hudson. What are my options? Do you use ANT or Maven? Actually we are using Make. We built our own build system to deal with the limitations in FlexBuilder. I use ANT to produce my JUnit style FlexUnit reports. I haven't worked with Make before so I can't directly help you with the syntax for that. However in case it helps this is strait for my project's ANT build file: <target name=""test""> <echo>Executing FlexUnit tests...</echo> <!-- Execute TestRunner.swf as FlexUnit tests and publish reports --> <flexunit workingDir=""${bin.loc}"" toDir=""${report.loc}"" haltonfailure=""false"" verbose=""true"" localTrusted=""true""> <source dir=""${main.src.loc}"" /> <testSource dir=""${test.src.loc}""> <include name=""**/*Test.as"" /> </testSource> <library dir=""${lib.loc}""/> </flexunit> <echo>Testing Complete</echo> <echo>Generating test reports...</echo> <!-- Generate readable JUnit-style reports --> <junitreport todir=""${report.loc}""> <fileset dir=""${report.loc}""> <include name=""TEST-*.xml"" /> </fileset> <report format=""frames"" todir=""${report.loc}/html"" /> </junitreport> <copy todir=""./test-reports""> <fileset dir=""${report.loc}""/> </copy> <echo>Generation complete</echo> </target> As you can see I'm using flexUnitTasks to run the tests and the junitreport task to generate the reports."
952,A,Maven: use specific test-classes of project A in project B I have projects A and B where B requires A. Inside project A I have a utility-class UC that should only be available for JUnit-tests and therefore resides in src/test/java of project A. As long as I write tests in A I have access to UC. However if I run Maven and want it to execute the tests in B I get compiler errors since UC is not accessible in B. Obviously Eclipse includes all classes in all source folders when it compiles something (i.e. it knows about UC when I write tests in B) while Maven removes all test-classes in the final version of A. My question is this: what do I need to do to have UC accessible in B when I run its tests with Maven? Please understand that I'm new to Maven and I think that similar questions have been asked. However I can't convert what is written there into my problem and fix it. I hope it's clear what I'm trying to do... The maven-jar-plugin page - http://maven.apache.org/plugins/maven-jar-plugin/usage.html - mentions two ways. The simple approach is creating a test-jar artifact and then refer to that. (snippets blatantly copied from official page) <project> ... <build> <plugins> ... <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <version>2.4</version> <executions> <execution> <goals> <goal>test-jar</goal> </goals> </execution> </executions> </plugin> ... </plugins> </build> ... </project> and then refer to it with the test-jar type and test scope in the projects that need it: <project> ... <dependencies> <dependency> <groupId>groupId</groupId> <artifactId>artifactId</artifactId> <type>test-jar</type> <version>version</version> <scope>test</scope> </dependency> </dependencies> ... </project> If you need to do this a lot you should most likely consider moving your test code to separate projects. @sjngm I rolled back when I found that the instructions given in your answer differed from the official Maven documentation. Classification is different from type. Thanks for leaving my answer alone and adding your own answer ;)  After looking some more I finally found a solution: http://www.waltercedric.com/java-j2ee-mainmenu-53/361-maven-build-system/1349-maven-reusing-test-classes-across-multi-modules-projects.html 1 I've seen this pattern occasionally on other questions so I guess I just didn't understand it that way... Oh well. *eyeroll* 1 That original link stopped working. I found it again on archive.org (don't mind the awkward layout). Precisely what I was looking for thanks for the great link!  I have always found test-jars awkward. They're kind of peculiar because they only get used once freshly deployed to the repository. Otherwise other projects do not see the changes. This is why I recommend creating just normal projects which you can use to put your test helpers into and then refer to them by using a test scope dependency.
953,A,"Running ""pure"" JUnit 4 tests using ant We have migrated to both JUnit 4 and ant 1.7 The tests runs fine in eclipse but the annotations are ignored when running the tests using ant. According to the Ant junit task documentation: It also works with JUnit 4.0 including ""pure"" JUnit 4 tests using only annotations and no JUnit4TestAdapter. But the documentation doesn't elaborate on how it should be configured. Is there any special setting required for the junit task? Am I missing something? We have both Tests that extends TestCase (i.e. 3.8 style) and ""pure"" Junit 4 tests could that be the problem? This is the relevant part of my generic ant script... not sure if that'll help you or not..  <junit fork=""true"" forkmode=""once"" haltonfailure=""false"" haltonerror=""false"" failureproperty=""tests.failures"" errorproperty=""tests.errors"" includeantruntime=""true"" showoutput=""true"" printsummary=""true""> <classpath> <path refid=""path-id.test.classpath.run""/> </classpath> <formatter type=""xml""/> <batchtest fork=""yes"" todir=""${dir.build.testresults}""> <fileset dir=""${dir.src.tests}""> <include name=""**/*Test.java""/> </fileset> </batchtest> </junit>  This happened to me and it was because I was both using annotations and extending TestCase. public class TestXXX extends TestCase { @Test public void testSimpleValidCase() { // this was running } @Test public void simpleValidCase() { // this wasn't running } } When you extend TestCase you are assuming JUnit3 style so JUnit4 annotations are ignored. The solution is to stop extending TestCase.  You can finally only find and execute tests with the skipNonTests parameter added in ant 1.9.3+! This is the code snippet from the accepted answer above (except for the new skipNonTests parameter and getting rid of the ""Test"" in the filename requirement): <junit printsummary=""yes"" haltonfailure=""yes""> <formatter type=""xml""/> <classpath refid=""path.test""/> <batchtest skipNonTests=""true"" fork=""yes"" todir=""${dir.report.unittests.xml}""> <fileset dir=""src""> <include name=""**/*.java""/> </fileset> </batchtest> </junit> Yeah it's great that they finally added this. Now if they could just fix everything-else-in-Ant Ant might actually become a nice tool. Haha.  What I ended up doing was adding an Ant to one of my definitions that is used by the task>. Et voila.  Apply this annotation to the other classes org.junit.Ignore  Verify your classpath definition... this solved my problem. <path id=""classpath"" description=""Classpath do Projeto""> <fileset dir=""${LIB}""> <include name=""**/*.jar"" /> <exclude name=""**/.SVN/*.*""/> </fileset> </path>  I am using pure JUnit4 tests with Ant. Here is the interesting part of my build file: <junit printsummary=""yes"" haltonfailure=""yes""> <formatter type=""xml""/> <classpath refid=""path.test""/> <batchtest fork=""yes"" todir=""${dir.report.unittests.xml}""> <fileset dir=""src""> <include name=""**/*Test*.java""/> </fileset> </batchtest> </junit> Make sure you have the latest version of the junit.jar file in the lib directory of Ant. As far as I know the required version is delivered with ant 1.7 or higher versions... So there's no way to avoid having to use a naming pattern for test classes (like ""*Test.java"") even though JUnit 4 doesn't require that? @jonik You don't have to use a naming pattern but it will help you avoid running JUnit against any additional classes you might have which don't contain tests that are in the same package as your test classes. Classes like this will give errors in the JUnit output. What if you have utility classes which happen to have Test at the start but which aren't test classes? Usually ""using JUnit 4"" means ""the @Test annotation marks tests not the class name"". It should be possible to include all .java files and have it only run the ones with tests in them. @Trejkaz There is a new parameter added that will filter out non-test classes for you. It is in an [answer below.](http://stackoverflow.com/questions/635481/running-pure-junit-4-tests-using-ant/25148876#25148876)  Ant ships with a version of JUnit 3 by default. JUnit 3 has no support for test annotations. To use the JUnit 4 annotations from the junit task make sure that you provide the location of a JUnit 4 jar in a nested classpath element of the junit task (see this entry in the ant FAQ). <junit showoutput=""yes"" fork=""true""> <classpath> <!-- The location of the JUnit version that you want to use --> <pathelement location=""lib/junit-4.9b1.jar""/> </classpath> <formatter type=""plain"" usefile=""false"" /> <batchtest> <fileset dir=""${tests.dir}""/> </batchtest> </junit> This is a preferable solution to overwriting the ant-junit.jar in ANT_HOME/lib as it means you can keep your JUnit jar in source control alongside your code making upgrades to later versions straightforward. Note that whilst I haven't specified any include pattern in my fileset above this does mean that the junit task will attempt to run JUnit against all the classes in that directory structure which might result in a number of classes being included that don't contain any tests depending on how you have structured your source files. I have exactly the same problem in Debian: ant 1.8.0 junit 4.10 the classpath to the junit-4.10.jar added as part of the junit task and yet the annotation @Task is not recognized only renaming the methods to start with test* works ... @MenelaosPerdikeas Did you mean `@Task`? I would expect you to be using `@Test` as the annotation. Assuming you are using `@Test` try running ant with the -v flag and the output of the junit task will show a classpath. You can then ensure that the junit-4.10.jar classpath that you added to the task is present. I found -v really helpful when trying to diagnose this problem. I had exactly this `` element except for the `fork=""true""` option. Adding that option solved it for me."
954,A,"Using a Jetty Server with JUnit tests I've been trying to test my web app by starting a Jetty server in the BeforeClass method of my JUnit test case and then using a HttpClient to form request to the server. I get the server to start without any issues but I keep getting 404's when I try to make a request. The configurtaion of my server is like the following:  public void start() throws Exception { if (server == null) { server = new Server(PORT); server.setStopAtShutdown(true); wac = new WebAppContext(); wac.setContextPath(""/app""); wac.setResourceBase(""war""); wac.setClassLoader(this.getClass().getClassLoader()); server.addHandler(wac); server.start(); } } Is there something wrong with my config? The server is running and I can see that I am hitting it it just can't find any resources. This is a complete Junit test class that uses jetty: package test.server; import java.io.BufferedReader; import java.io.InputStreamReader; import org.apache.http.HttpResponse; import org.apache.http.client.HttpClient; import org.apache.http.client.methods.HttpGet; import org.apache.http.impl.client.DefaultHttpClient; import org.junit.After; import org.junit.Before; import org.junit.Test; import org.mortbay.jetty.Server; import org.mortbay.jetty.webapp.WebAppContext; public class MockPortalTest { private Server server; @Before public void startServer() throws Exception { server = new Server(8080); server.setStopAtShutdown(true); WebAppContext webAppContext = new WebAppContext(); webAppContext.setContextPath(""/app""); webAppContext.setResourceBase(""src/main/webapp""); webAppContext.setClassLoader(getClass().getClassLoader()); server.addHandler(webAppContext); server.start(); } @Test public void shouldBePreAuthenticated() throws Exception { String userId = ""invalid""; HttpClient client = new DefaultHttpClient(); HttpGet mockRequest = new HttpGet(""http://localhost:8080/app""); mockRequest.setHeader(""http-user""userId); HttpResponse mockResponse = client.execute(mockRequest); BufferedReader rd = new BufferedReader (new InputStreamReader(mockResponse.getEntity().getContent())); // DO YOUR ASSERTIONS } @After public void shutdownServer() throws Exception { server.stop(); } }  You probably need to define this servlet as a resource in your deployment descriptor (web.xml or jetty-web.xml). Just a guess; a 404 indicates that your code isn't running at all and therefore isn't the problem. The problem is occurring before your code even has a chance to execute. ahaha yes I know notice that wac.getDescriptor returns null.  I started with the code snippet in the first post and then started to fight my way through. The code below finally worked for me: Server server = new Server(8080); server.setStopAtShutdown(true); WebAppContext webAppContext = new WebAppContext(); webAppContext.setContextPath(""/app""); webAppContext.setResourceBase(""src/main/webapp""); webAppContext.setClassLoader(getClass().getClassLoader()); server.addHandler(webAppContext); URL url = new URL(""http://localhost:8080/app/some_call""); URLConnection connection = url.openConnection(); List<String> lines = IOUtils.readLines(connection.getInputStream()); System.out.println(lines.get(0)); POM looks like this: <dependency> <groupId>org.mortbay.jetty</groupId> <artifactId>jetty</artifactId> <version>${jetty.version}</version> </dependency> <dependency> <groupId>org.mortbay.jetty</groupId> <artifactId>jetty-util</artifactId> <version>${jetty.version}</version> </dependency> <jetty.version>6.1.25</jetty.version> -- Oliver I had to add `server.start();` after `server.addHandler(webAppContext);`.  I think you forgot to set: wac.setWar(""/path/to/war/file""); Or better use the constructor: wac = new WebAppContext(""/path/to/war/file"" ""/app""); I set the resource base and the class loader so the path to the war gets set automatically. If I inspect the WebAppContext during runtime this is at least the behavior that I see."
955,A,"Meaning of epsilon argument of assertEquals for double values I have a question about junit assertEquals to test double values. Reading API doc I can see: @Deprecated public static void assertEquals(double expected double actual) Deprecated. Use assertEquals(double expected double actual double epsilon) instead What does epsilon value mean? (Epsilon is a letter in the Greek alphabet right?). Can Someone explain to me how to use it? you should accept one of these answers by clicking the check mark. I guess he never came back to check the answer.. Epsilon is the value that the 2 numbers can be off by. So it will assert to true as long as Math.abs(expected - actual) < epsilon Right thanks for example.  The thing is that two double may not be exactly equal due to precision issues inherent to floating point numbers. With this delta value you can control the evaluation of equality based on a error factor. Also some floating-point values can have special values like NAN and -Infinity/+Infinity which can influence results. If you really intend to compare that two doubles are exactly equal it is best compare them as an long representation Assert.assertEquals(Double.doubleToLongBits(expected) Double.doubleToLongBits(result)); Or Assert.assertEquals(0 Double.compareTo(expected result)); Which can take these nuances into account. I have not delved into the Assert method in question but I can only assume the previous was deprecated for this kind of issues and the new one does take them into account.  Which version of Junit is this? I've only ever seen delta not epsilon - but that's a side issue! From the JUnit javadoc: delta - the maximum delta between expected and actual for which both numbers are still considered equal. It's probably overkill but I typically use a really small number e.g. private static final double DELTA = 1e-15; @Test public void testDelta(){ assertEquals(123.456 123.456 DELTA); } Thanks for including the sample code.  Epsilon is a difference between expected and actual values which you can accept thinking they are equal. You can set .1 for example.  Note that if you're not doing math there's nothing wrong with asserting exact floating point values. For instance: public interface Foo { double getDefaultValue(); } public class FooImpl implements Foo { public double getDefaultValue() { return Double.MIN_VALUE; } } In this case you want to make sure it's really MIN_VALUE not zero or -MIN_VALUE or MIN_NORMAL or some other very small value. You can say double defaultValue = new FooImpl().getDefaultValue(); assertEquals(Double.MIN_VALUE defaultValue); but this will get you a deprecation warning. To avoid that you can call assertEquals(Object Object) instead: // really you just need one cast because of autoboxing but let's be clear assertEquals((Object)Double.MIN_VALUE (Object)defaultValue); And if you really want to look clever: assertEquals( Double.doubleToLongBits(Double.MIN_VALUE) Double.doubleToLongBits(defaultValue) ); Or you can just use Hamcrest fluent-style assertions: // equivalent to assertEquals((Object)Double.MIN_VALUE (Object)defaultValue); assertThat(defaultValue is(Double.MIN_VALUE)); If the value you're checking does come from doing some math though use the epsilon. If you want to check for an exactly equals set epsilon to 0.0 -- the Object variant is not required.  Floating point calculations are not exact - there is often round-off errors and errors due to representation. (For example 0.1 cannot be exactly represented in binary floating point.) Because of this directly comparing two floating point values for equality is usually not a good idea because they can be different by a small amount depending upon how they were computed. The ""delta"" as it's called in the JUnit javadocs describes the amount of difference you can tolerate in the values for them to be still considered equal. The size of this value is entirely dependent upon the values you're comparing. When comparing doubles I typically use the expected value divided by 10^6. Mdma very good explanation thanks"
956,A,"Testing code which calls native methods I have a class like this: public final class Foo { public native int getBar(); public String toString() { return ""Bar: "" + getBar(); } } Please note that getBar() is implemented with JNI and that the class is final. I want to write a junit test to test the toString() method. For this I need to mock the getBar() method and then run the original toString() method to check the output. My first thought was that this must be impossible but then I found PowerMock which supports testing final classes and native methods according to the feature list. But so far I had no success with it. The best thing I managed was mocking the complete class but then the test tested the mocked toString() method instead of the real one which doesn't make much sense. So how can I use PowerMock to test this toString() method from above? I prefer using PowerMock with Mockito but if this is not possible I have no problem with using EasyMock instead. Or use JMockit with dynamic partial mocking: import org.junit.*; import mockit.*; public class FooTest { @Test public void testToString(final Foo foo) { new Expectations(foo) {{ foo.getBar(); result = 42; }}; assertEquals(""Bar: 42"" foo.toString()); } }  Or use Strategy Pattern:  public final class Foo { public IBarStrategy barStrategy; ...... } interface IBarStrategy{ int getBar(); } When unit test inject a mock IBarStrategy instance then you could test class Foo. Dependency injection is a nice thing for code re-use and modularity but if that's not needed then why should I increase the complexity and loosen the API of my classes just for testability? But that's not the point here. So let's say I can't change the test subject so I can't extract the native methods into an interface.  Found it. The way I was doing it was correct. The only thing I missed was telling the mock object to call the original method when toString was called(). So it works like this: @RunWith(PowerMockRunner.class) @PrepareForTest({ Foo.class }) public class FooTest { @Test public void testToString() throws Exception { Foo foo = mock(Foo.class); when(foo.getBar()).thenReturn(42); when(foo.toString()).thenCallRealMethod(); assertEquals(""Bar: 42"" foo.toString()); } }"
957,A,"JUnit Command Line Testing This has been asked before but was not clarified to the point where I get it. Similar to the one or two other threads I've seen on this subject I'm working on a chat client with command line inputs for logging in/off disconnecting etc. and I am unsure how to simulate this in a JUnit test case. Other responses indicated that I should try changing the System.in to a separate InputStream but...then what? tl;dr: I have a method in my actual code for parsing command line input and need a JUnit way of testing that these were entered and appropriately processed. Cheers gents. EDIT: It seems I misunderstood the question. I usually use the term ""command line input"" to refer to command line arguments given to the process to start with rather than interactive console input. However... Handing your real code either a different InputStream or possibly even a Reader or Scanner would indeed help - anything to separate the ""getting input"" part from the console. You can then fake the input all in one go pretty easily using a String as input in your test code and then either converting it to bytes and wrapping those bytes in a ByteArrayInputStream or wrapping the string directly in StringReader. The downside of this is that there's no easy way of making this ""pause"" after one command in order to check the results. You may want to alter the design somewhat so that the part which reads the input is separated from the part which handles the input. The reading part could be a very simple loop on the order of: String line; while ((line = reader.readLine()) != null) { handleInput(line); } You could then potentially leave that part untested by unit tests or write some relatively primitive tests - but you can then test handleInput extensively as it's now separated from the input source. Original answer If you've extracted the parsing code from the code which really starts the application it's easy: run that code and check the results. This will be easiest if you have some sort of class encapsulating the options of course. For example your main method might look like this: public static void main(String[] args) { Options options = Options.parse(args); // Use options here } Then you can just test Options.parse very easily. I'm probably just having an epic brainfart but I don't think this really answers my question. I can start ""Welcome please enter commands:"" loop inside my JUnit test case but how do I then enter commands from aforementioned test case which has no main method? @Nick: Ah I see. By ""command line input"" I thought you mean command line arguments... whereas you really mean interactive console input. Will edit. I think separating the handling from the reading is going to be my tack. Having a method that takes a string argument so I can manually pass it whatever I want in JUnit will make my life easy. Thanks."
958,A,"JUnit best-practices - should I split methods for reuse? I have the following JUnit test: @Test public final void testDivisors() { testDivisorsAux(1 new int[] { 1 }); testDivisorsAux(6 new int[] { 1 2 3 6 }); testDivisorsAux(0 new int[] { }); ... } private final void testDivisorsAux(final int number final int[] expected) { List<Integer> divisors = Util.divisors(number); assertSame(divisors.size() expected.length); for (int i : expected) { assertTrue(divisors.contains(i)); } } The method that I'm testing is quite simple it just receives a number and returns a List with its divisors. I don't want to repeat the test code many times so I've created an auxiliar method testDivisorsAux. Everything works fine I'm just wondering... is this a bad practice? Should I write the test in a different way? Maybe keep all the code within the ""@Test method""? PMD is telling me that JUnit tests should include assert() or fail() (for the first method) and JUnit 4 tests that execute tests should use the @Test annotation (for the second one). I know that PMD is using just a regular expression (well actually it's XPath) to determine which rules I'm breaking... so I'm inclined to think that it's simply a ""false positive"" warning. But anyway I would like to know if I'm doing something wrong. (Appart from writing tests 4 times longer than the method being tested :) While I was searching for questions similar to this one I've found something called parametrized tests... but it seems it's something oriented to much bigger scenarios isn't it? Personally I think it is good practise to do this. There is a small danger of going too far and building masses of code to do your test in which case you should rethink the design of the stuff you're testing. You don't really want to have to write tests to test your tests but the example you give seems fine and it means your test cases are a lot simpler. I have not used PMD and personally I would try to configure it to ingore these warnings. If they worry you you might be able to change your code to get rid of them. My guess is that the 2nd warning was because your helper method starts with the word test. In junit3 all methods that start with test are tests. Why not rename the method testDivisorsAux to be assertDivisors - maybe having a method that starts with assert will help with the 1st warning also. Thanks a lot for your answer @Adam. I was not really worried by the PMD warnings but anyway good trick to avoid them! :) Not only a good trick I think the names you propose are also more descriptive than mine..."
959,A,"Maven2 junit timeout annotation doesn't work We have a bunch of tests in a maven2 project and build with cruisecontrol. However the build regularly hangs because the annotation of the test with a timeout @Test(timeout = 5000) is ignored. I tried and run maven locally reproducing the fact that the timeout is ignored. Is there a way to activate the timeout for the tests again? I currently use a workaround in setting a timeout in cruisecontrol. However this simply means that the whole build is canceled and not just the tests that should fail. Remark: It seems in maven 1 there was a property to activate the junit timeout which is missing in maven 2. Try to set the forkedProcessTimeoutInSeconds parameter of the Maven Surefire Plugin: Kill the forked test process after a certain number of seconds. If set to 0 wait forever for the process never timing out This seems to be an equivalent of the maven.junit.timeout test property in Maven 1 and will ""activate"" a timeout at the Maven level (different from the annotation). @roesslerj I'm not aware of all the details and I'm going to state the obvious but I guess the surefire plugin doesn't implement support for this. You can always try to raise an issue http://jira.codehaus.org/browse/SUREFIRE. Thanks for the input. Any ideas why the annotation isn't working?"
960,A,"JUnit confusion: use 'extend Testcase' or '@Test'? I've found the proper use (or at least the documentation) of JUnit very confusing. This question serves both as a future reference and as a real question. If I've understood correctly there are two main approaches to create and run a JUnit test: Approach A (JUnit 3-style): create a class that extends TestCase and start test methods with the word test. When running the class as a JUnit Test (in Eclipse) all methods starting with the word test are automatically run. import junit.framework.TestCase; public class DummyTestA extends TestCase { public void testSum() { int a = 5; int b = 10; int result = a + b; assertEquals(15 result); } } Approach B (JUnit 4-style): create a 'normal' class and prepend a @Test annotation to the method. Note that you do NOT have to start the method with the word test. import org.junit.*; import static org.junit.Assert.*; public class DummyTestB { @Test public void Sum() { int a = 5; int b = 10; int result = a + b; assertEquals(15 result); } } Mixing the two seems not to be a good idea see e.g. this stackoverflow question: Now my questions(s): What is the preferred approach or when would you use one instead of the other? Approach B allows for testing for exceptions by extending the @Test annotation like in @Test(expected = ArithmeticException.class). But how do you test for exceptions when using approach A? When using approach A you can group a number of test classes in a test suite like this: TestSuite suite = new TestSuite(""All tests""); suite.addTestSuite(DummyTestA.class); suite.addTestSuite(DummyTestAbis.class); But this can't be used with approach B (since each testclass should subclass TestCase). What is the proper way to group tests for approach B? Edit: I've added the JUnit versions to both approaches You should use JUnit 4. It's better. Much frameworks have started to deprecate the JUnit 3.8 support. This is from the Spring 3.0 reference documentation: [Warning] Legacy JUnit 3.8 class hierarchy is deprecated In general you should always try to use the latest stable release of a framework when you start something new.  The distinction is rather easy: extending TestCase is the way unit tests were written in JUnit 3 (of course it's still supported in JUnit 4) using the @Test annotation is the way introduced by JUnit 4 Generally you should choose the annotation path unless compatibility with JUnit 3 (and/or a Java version earlier than Java 5) is needed for several reasons: The @Test annotaton is more explicit and is easier to support in tools (for example it's easy to search for all tests this way) Multiple methods can be annotated with @Before/@BeforeClass and @After/@AfterClass providing more flexibility Integrated support for testing for expected exceptions using expected= Support for the @Ignored annotation To test for expected exceptions in a JUnit 3 TestCase you'd have to make the text explicit. public void testMyException() { try { objectUnderTest.myMethod(EVIL_ARGUMENT); fail(""myMethod did not throw an Exception!""); } catch (MyException e) { // ok! // check for properties of exception here if desired } } @Yishai: that's true but most of the time I'm already content if the method throws the correct type of Exception on problematic input. Helpful and thorough answer but I don't fully understand ""check for message of exception"". Checking against a hardcoded string is going to be a maintenance nightmare. You must have meant ""check for the properties of your specific exception type"". @thSoft: it's not often that it's used but occasionally I want to make sure that the exception method mentions the offending field for example. Then a simple `assertTrue(e.getMessage().contains(""foo""))` could be useful. Even in JUnit4 this is an important idiom when you have to check the message or some other property of the exception (such as the cause). The `expected` method only checks for type.  There is an unanswered part to your question and that is ""What is the proper way to group tests for approach B?"" The official answer is that you annotate a class with an @RunWith(Suite.class) and then use the @Suite.SuiteClasses annotation to list the classes. This is how the JUnit developers do it (listing every class in a suite manually). In many ways this approach is an improvement in that it is trivial and intuitive to add before suite and after suite behaviors (just add an @BeforeClass and @AfterClass method to the the class annotated with the @RunWith - much better than the old TestFixture). However it does have a step backwards in that annotations don't allow you to dynamically create the list of classes and working around that problem gets a bit ugly. You have to subclass the Suite class and dynamically create the array of classes in the subclass and pass it to the Suite constructor but this is an incomplete solution in that other subclasses of Suite (such as Categories) don't work with it and essentially do not support dynamic Test class collection. +1 for this. After embarking on a task to write a dynamic solution to adding Tests to a TestSuite I've had to extend TestCase in each of my Tests. This in turn has broken previously working unit Tests which used JUnit4 annotations to define expected exceptions. My search for a way to dynamically populate a Test Suite has led me to this thread and specifically your answer which I believe is one of the few remaining desirable reasons to continue with JUnit 3.  The ""preferred"" approach would be to use annotations which have been introduced since Junit 4. They make a lot of things easier (see your second question) You can use a simple try/catch block for that:  public void testForException() { try { Integer.parseInt(""just a string""); fail(""Exception should have been thrown""); } catch (final Exception e) { // expected } }  I have a preference for JUnit 4 (Annotation approach) because I find it more flexible. If you want to build test suite in JUnit 4 you have to create a class grouping all tests like this: import org.junit.runner.RunWith; import org.junit.runners.Suite; import org.junit.runners.Suite.SuiteClasses; @RunWith(Suite.class) @SuiteClasses({ Test1.class Test2.class Test3.class Test4.class })public class TestSuite { /* empty class */ }"
961,A,"junit suite tests in phases: All @Before then all @Test then all @After I'd like a junit runner that executes all @Before methods then all @Test methods then all @After methods. This is how my System-Tests work. The @Before methods are run to setup the test data and scenarios. The application is started. Then the @Test methods are run with the application running in the background. Those @Test methods can change data or respond to the application. Then the framework waits for the application to finish up. Afterward the @After methods are run to verify the test results. I already use junit annotations assertion methods and other various bits. But I just can't figure out how to use junits runners to execute test methods in this way. I couldn't make heads nor tails of the ""Computer"" interface in junit 4.8 or figure out how to apply Rules to this. Wow thanks I forgot to ask my question. edited... Is there a question there somewhere? This isn't what JUnit does. JUnit has a design philosophy that emphasizes independent unit tests. As such it isn't a natural framework for system tests. What you want to do fits nicely into TestNG (which as a design goal tries to straddle both unit and system tests). In JUnit the @Before and @After are run before and after each test. You can shoehorn this kind of thing into JUnit using a Suite which references all of your tests and is responsible for all setup and teardown so the Suite's @BeforeClass and @AfterClass methods get run before and after the suite which if you organize it correctly could be all of your system tests. There are lot of organizational challenges in the code when it gets large with the JUnit approach so I would suggest you consider and alternative framework if this is the bulk of what you want to do. Good point I've left the realm of junit. Its time to admit it to myself and move on. (for my non-unit tests at least).  I think you can solve this by making only one actual test method that just calls are your actual tests which you do not declare as ssuch. Kinda like: @Before public void beforeTest(){} @After public void afterTest(){} @Test public void test(){ test1(); test2(); test3(); } public void test1(){} public void test2(){} public void test3(){}"
962,A,junit: Best practice to develop test case? Technology: Junit latest version Application is business oriented Some people use hard coded data for test case some use properties files and some xml files. As per my knowledge xml is better than other two. Is there some better approach in use in industry. Please suggest best practice to develop test cases. I don't think there are best practices. I suggest that you use the one that works best for your particular problem space and the kind of testing you need to perform. If the tests you need to code essentially involve calling methods with a large number of different inputs then a data driven approach (using properties files XML or something else) is a good idea. If not it is a bad idea. The one thing to watch is spending too much time creating complicated infrastructure so that you can code your tests neatly.  I'd try to keep the tests fast and simple. The faster the tests run the more tests you can add to your build. The disadvantage of xml: parsing is quite expensive reading the values from the DOM too. For tabular data I'd use flat files in some sort of CSV format. For key/value data a simple properties file is absolut sufficient. With JUnit we're on unit testing level we want to know if the public interface are implemented according to the specs and if they behave in a defined way for all possible input. Therefore I usually hardcode the test values in the test methods because they usually don't change (no need to edit the values outside of the test classes)  It is important that the mapping between data representation in the test and data passed to the function being tested is as transparent as possible. Hard-coded data is totally ok if the data are few and easy to observe right in the source. The fewer windows you need to keep open to understand a test case the better. XML is best for nested tree-like data but it's a bit wordy. YAML may also be good for this. For flat data properties and just line-organized files are ok. There's no single format which is superior to all others in all ways. Choose the easiest and most natural for a particular test suite / subject area. Investing some effort in handling the most natural format does pay when you need to quickly produce more and more test cases and then again when you investigate a regression. E.g in our project (quite large) we had to invent several data representations and write (simple) custom parsers just to make writing and reading data for test cases a breeze.
963,A,"Get name of currently executing test in JUnit 4 In JUnit 3 I could get the name of the currently running test like this: public class MyTest extends TestCase { public void testSomething() { System.out.println(""Current test is "" + getName()); ... } } which would print ""Current test is testSomething"". Is there any out-of-the-box or simple way to do this in JUnit 4? Background: Obviously I don't want to just print the name of the test. I want to load test-specific data that is stored in a resource with the same name as the test. You know convention over configuration and all that. Thanks! What does the above code give you in JUnit 4? JUnit 3 tests extend TestCase where getName() is defined. JUnit 4 tests do not extend a base class so there is no getName() method at all. I have a similar problem where I want to set the test name since I'm using the Parametrized runner that only gives me numbered test cases. JUnit 4.7.x - 4.8.x The following approach will print method names for all tests in a class: @Rule public MethodRule watchman = new TestWatchman() { public void starting(FrameworkMethod method) { System.out.println(""Starting test: "" + method.getName()); } }; JUnit 4.9.x onwards Since JUnit 4.9 the TestWatchman class has been deprecated in favour of the TestWatcher class which has a similar invocation: @Rule public TestRule watcher = new TestWatcher() { protected void starting(Description description) { System.out.println(""Starting test: "" + description.getMethodName()); } }; Not working in Eclips 4 @takacsot That's surprising. Can you please post a fresh question about this and ping me the link here?  JUnit 4.7 added this feature it seems. Looks like this will get you the method name: import org.junit.Rule; public class NameRuleTest { @Rule public TestName name = new TestName(); @Test public void testA() { assertEquals(""testA"" name.getMethodName()); } @Test public void testB() { assertEquals(""testB"" name.getMethodName()); } } import org.junit.Rule; Also note that TestName is not available in @before :( See: http://old.nabble.com/@Rule-TestName-not-available-in-@Before-methods...-td25198691.html Apparently newer versions of JUnit execute `@Rule` before `@Before` - I'm new to JUnit and was depending on `TestName` in my `@Before` without any difficulties. There are [more efficient ways of doing this available](http://stackoverflow.com/a/13987909/474189). Github links no longer work :( If you are using parameterized tests ""name.getMethodName()"" will return {testA[0] testA[1] etc} thus I use some like : assertTrue(name.getMethodName().matches(""testA(\\[\\d\\])?""));  @ClassRule public static TestRule watchman = new TestWatcher() { @Override protected void starting( final Description description ) { String mN = description.getMethodName(); if ( mN == null ) { mN = ""setUpBeforeClass..""; } final String s = StringTools.toString( ""starting..JUnit-Test: %s.%s"" description.getClassName() mN ); System.err.println( s ); } };  Try this instead: public class MyTest { @Rule public TestName testName = new TestName(); @Rule public TestWatcher testWatcher = new TestWatcher() { @Override protected void starting(final Description description) { String methodName = description.getMethodName(); String className = description.getClassName(); className = className.substring(className.lastIndexOf('.') + 1); System.err.println(""Starting JUnit-test: "" + className + "" "" + methodName); } }; @Test public void testA() { assertEquals(""testA"" testName.getMethodName()); } @Test public void testB() { assertEquals(""testB"" testName.getMethodName()); } } The output looks like this: Starting JUnit-test: MyTest testA Starting JUnit-test: MyTest testB NOTE: This DOES NOT work if your test is a subclass of TestCase! The test runs but the @Rule code just never runs. God bless you for your NOTE at the very of the example.  Consider using SLF4J (Simple Logging Facade for Java) provides some neat improvements using parameterized messages. Combining SLF4J with JUnit 4 rule implementations can provide more efficient test class logging techniques. import org.junit.Rule; import org.junit.Test; import org.junit.rules.MethodRule; import org.junit.rules.TestWatchman; import org.junit.runners.model.FrameworkMethod; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class LoggingTest { @Rule public MethodRule watchman = new TestWatchman() { public void starting(FrameworkMethod method) { logger.info(""{} being run..."" method.getName()); } }; final Logger logger = LoggerFactory.getLogger(LoggingTest.class); @Test public void testA() { } @Test public void testB() { } }  JUnit 4 does not have any out-of-the-box mechanism for a test case to get it’s own name (including during setup and teardown). Is there an not-out-of-the-box mechanism out there other than inspecting the stack? Not the case given the answers below! maybe assign the correct answer to someone else?  I'd suggest you decouple the test method name from your test data set. I would model a DataLoaderFactory class which loads/caches the sets of test data from your resources and then in your test case cam call some interface method which returns a set of test data for the test case. Having the test data tied to the test method name assumes the test data can only be used once where in most case i'd suggest that the same test data in uses in multiple tests to verify various aspects of your business logic.  Most likely Bill the Lizard is right. In case it still doesn't work out you could extract the test name from the stack dump of the current thread (Thread.currentThread().getStackTrace()) -- rather hacky but maybe the end justifies the means :)  String testName = null; StackTraceElement[] trace = Thread.currentThread().getStackTrace(); for (int i = trace.length - 1; i > 0; --i) { StackTraceElement ste = trace[i]; try { Class<?> cls = Class.forName(ste.getClassName()); Method method = cls.getDeclaredMethod(ste.getMethodName()); Test annotation = method.getAnnotation(Test.class); if (annotation != null) { testName = ste.getClassName() + ""."" + ste.getMethodName(); break; } } catch (ClassNotFoundException e) { } catch (NoSuchMethodException e) { } catch (SecurityException e) { } } All of which is pointless given @FroMage's answer though.  A convoluted way is to create your own Runner by subclassing org.junit.runners.BlockJUnit4ClassRunner. You can then do something like this: public class NameAwareRunner extends BlockJUnit4ClassRunner { public NameAwareRunner(Class<?> aClass) throws InitializationError { super(aClass); } @Override protected Statement methodBlock(FrameworkMethod frameworkMethod) { System.err.println(frameworkMethod.getName()); return super.methodBlock(frameworkMethod); } } Then for each test class you'll need to add a @RunWith(NameAwareRunner.class) annotation. Alternatively you could put that annotation on a Test superclass if you don't want to remember it every time. This of course limits your selection of runners but that may be acceptable. Also it may take a little bit of kung fu to get the current test name out of the Runner and into your framework but this at least gets you the name. Conceptually at least this idea seems rather straightforward to me. My point being: I wouldn't call it convoluted. ""on a Test superclass ..."" - Please no more of the horrible inheritance based design patterns. This is so JUnit3!"
964,A,"How to test my servlet using JUnit I have created a web system using java servlets and have now been told to do some JUNIT testing on it! I really dont have a clue how you would test a servlet with junit tests could some one give me an example of a test? This is an exmaple of my code that allows a user to register this is submitted from my main page via Ajax...  public void doPost(HttpServletRequest request HttpServletResponse response) throws ServletExceptionIOException{ //get params String userName=request.getParameter(""username""); String passwrd=request.getParameter(""password""); String name=request.getParameter(""name""); try { // Load the database driver Class.forName(""com.mysql.jdbc.Driver""); //pass reg details to datamanager dataManager = new DataManager(); //store result as string String result = dataManager.register(userName passwrd name); //set response to html + no cache response.setContentType(""text/html""); response.setHeader(""Cache-Control"" ""no-cache""); //send response with register result response.getWriter().write(result); }catch(Exception e){ System.out.println(""Exception is ;""+e); } } HOw would i go about testing this? with junit? my datamanager is just a basic bit of code that submits it to the database! Use Selenium for webbased unit tests. There's a Firefox plugin called Selenium IDE which can record actions on the webpage and export to JUnit testcases which uses Selenium RC to run the test server. Thanks for this looks good but it doesn really test the methods/servlet code does it not directly? or am i wrong. It does by firing HTTP requests programmatically.  Here's another alternative using OpenBrace's ObMimic library of Servlet API test-doubles (disclosure: I'm its developer). package com.openbrace.experiments.examplecode.stackoverflow5434419; import static org.junit.Assert.*; import com.openbrace.experiments.examplecode.stackoverflow5434419.YourServlet; import com.openbrace.obmimic.mimic.servlet.ServletConfigMimic; import com.openbrace.obmimic.mimic.servlet.http.HttpServletRequestMimic; import com.openbrace.obmimic.mimic.servlet.http.HttpServletResponseMimic; import com.openbrace.obmimic.substate.servlet.RequestParameters; import org.junit.Before; import org.junit.Test; import javax.servlet.ServletException; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; /** * Example tests for {@link YourServlet#doPost(HttpServletRequest * HttpServletResponse)}. * * @author Mike Kaufman OpenBrace Limited */ public class YourServletTest { /** The servlet to be tested by this instance's test. */ private YourServlet servlet; /** The ""mimic"" request to be used in this instance's test. */ private HttpServletRequestMimic request; /** The ""mimic"" response to be used in this instance's test. */ private HttpServletResponseMimic response; /** * Create an initialized servlet and a request and response for this * instance's test. * * @throws ServletException if the servlet's init method throws such an * exception. */ @Before public void setUp() throws ServletException { /* * Note that for the simple servlet and tests involved: * - We don't need anything particular in the servlet's ServletConfig. * - The ServletContext isn't relevant so ObMimic can be left to use * its default ServletContext for everything. */ servlet = new YourServlet(); servlet.init(new ServletConfigMimic()); request = new HttpServletRequestMimic(); response = new HttpServletResponseMimic(); } /** * Test the doPost method with example argument values. * * @throws ServletException if the servlet throws such an exception. * @throws IOException if the servlet throws such an exception. */ @Test public void testYourServletDoPostWithExampleArguments() throws ServletException IOException { // Configure the request. In this case all we need are the three // request parameters. RequestParameters parameters = request.getMimicState().getRequestParameters(); parameters.set(""username"" ""mike""); parameters.set(""password"" ""xyz#zyx""); parameters.set(""name"" ""Mike""); // Run the ""doPost"". servlet.doPost(request response); // Check the response's Content-Type Cache-Control header and // body content. assertEquals(""text/html; charset=ISO-8859-1"" response.getMimicState().getContentType()); assertArrayEquals(new String[] { ""no-cache"" } response.getMimicState().getHeaders().getValues(""Cache-Control"")); assertEquals(""...expected result from dataManager.register..."" response.getMimicState().getBodyContentAsString()); } } Notes: Each ""mimic"" has a ""mimicState"" object for its logical state. This provides a clear distinction between the Servlet API methods and the configuration and inspection of the mimic's internal state. You might be surprised that the check of Content-Type includes ""charset=ISO-8859-1"". However for the given ""doPost"" code this is as per the Servlet API Javadoc and the HttpServletResponse's own getContentType method and the actual Content-Type header produced on e.g. Glassfish 3. You might not realise this if using normal mock objects and your own expectations of the API's behaviour. In this case it probably doesn't matter but in more complex cases this is the sort of unanticipated API behaviour that can make a bit of a mockery of mocks! I've used response.getMimicState().getContentType() as the simplest way to check Content-Type and illustrate the above point but you could indeed check for ""text/html"" on its own if you wanted (using response.getMimicState().getContentTypeMimeType()). Checking the Content-Type header the same way as for the Cache-Control header also works. For this example the response content is checked as character data (with this using the Writer's encoding). We could also check that the response's Writer was used rather than its OutputStream (using response.getMimicState().isWritingCharacterContent()) but I've taken it that we're only concerned with the resulting output and don't care what API calls produced it (though that could be checked too...). It's also possible to retrieve the response's body content as bytes examine the detailed state of the Writer/OutputStream etc. There are full details of ObMimic and a free download at the OpenBrace website. Or you can contact me if you have any questions (contact details are on the website).   public class WishServletTest { WishServlet wishServlet; HttpServletRequest mockhttpServletRequest; HttpServletResponse mockhttpServletResponse; @Before public void setUp(){ wishServlet=new WishServlet(); mockhttpServletRequest=createNiceMock(HttpServletRequest.class); mockhttpServletResponse=createNiceMock(HttpServletResponse.class); } @Test public void testService()throws Exception{ File file= new File(""Sample.txt""); File.createTempFile(""ashok""""txt""); expect(mockhttpServletRequest.getParameter(""username"")).andReturn(""ashok""); expect(mockhttpServletResponse.getWriter()).andReturn(new PrintWriter(file)); replay(mockhttpServletRequest); replay(mockhttpServletResponse); wishServlet.doGet(mockhttpServletRequest mockhttpServletResponse); FileReader fileReader=new FileReader(file); int count = 0; String str = """"; while ( (count=fileReader.read())!=-1){ str=str+(char)count; } Assert.assertTrue(str.trim().equals(""Helloashok"")); verify(mockhttpServletRequest); verify(mockhttpServletResponse); } } An explanation of this code would have been helpful  First off in a real application you would never get database connection info in a servlet; you would configure it in your app server. There are ways however of testing Servlets without having a container running. One is to use mock objects. Spring provides a set of very useful mocks for things like HttpServletRequest HttpServletResponse HttpServletSession etc: http://static.springsource.org/spring/docs/3.0.x/api/org/springframework/mock/web/package-summary.html Using these mocks you could test things like What happens if username is not in the request? What happens if username is in the request? etc You could then do stuff like: import static org.junit.Assert.assertEquals; import java.io.IOException; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import org.junit.Before; import org.junit.Test; import org.springframework.mock.web.MockHttpServletRequest; import org.springframework.mock.web.MockHttpServletResponse; public class MyServletTest { private MyServlet servlet; private MockHttpServletRequest request; private MockHttpServletResponse response; @Before public void setUp() { servlet = new MyServlet(); request = new MockHttpServletRequest(); response = new MockHttpServletResponse(); } @Test public void correctUsernameInRequest() throws ServletException IOException { request.addParameter(""username"" ""scott""); request.addParameter(""password"" ""tiger""); servlet.doPost(request response); assertEquals(""text/html"" response.getContentType()); // ... etc } } perfect thanks this line wasn't needed i forgot to delete! ""// Load the database driver Class.forName(""com.mysql.jdbc.Driver""); "" @Paul Croarkin: This is a spot on example!! Very Helpful.  First you should probably refactor this a bit so that the DataManager is not created in the doPost code.. you should try Dependency Injection to get an instance. (See the Guice video for a nice intro to DI.). If you're being told to start unit testing everything then DI is a must-have. Once your dependencies are injected you can test your class in isolation. To actually test the servlet there are other older threads that have discussed this.. try here and here. Ok thanks for your comments are you saying that the DataManager should be created within a method within that servlet? i watched that video and didnt really understand it :( very new to java and have never done any kind of testing. Take a look at that Guice video (at least the beginning) - it does a good job of explaining why you never want to instantiate a new object in a class that you plan on unit testing.  I find Selenium tests more useful with integration or functional (end-to-end) testing. I am working with trying to use org.springframework.mock.web but I am not very far along. I am attaching a sample controller with a jMock test suite. First the Controller: package com.company.admin.web; import javax.validation.Valid; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.validation.BindingResult; import org.springframework.validation.ObjectError; import org.springframework.web.bind.annotation.ModelAttribute; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.SessionAttributes; import org.springframework.web.bind.support.SessionStatus; import com.company.admin.domain.PaymentDetail; import com.company.admin.service.PaymentSearchService; import com.company.admin.service.UserRequestAuditTrail; import com.company.admin.web.form.SearchCriteria; /** * Controls the interactions regarding to the refunds. * * @author slgelma * */ @Controller @SessionAttributes({""user"" ""authorization""}) public class SearchTransactionController { public static final String SEARCH_TRANSACTION_PAGE = ""searchtransaction""; private PaymentSearchService searchService; //private Validator searchCriteriaValidator; private UserRequestAuditTrail notifications; @Autowired public void setSearchService(PaymentSearchService searchService) { this.searchService = searchService; } @Autowired public void setNotifications(UserRequestAuditTrail notifications) { this.notifications = notifications; } @RequestMapping(value=""/"" + SEARCH_TRANSACTION_PAGE) public String setUpTransactionSearch(Model model) { SearchCriteria searchCriteria = new SearchCriteria(); model.addAttribute(""searchCriteria"" searchCriteria); notifications.transferTo(SEARCH_TRANSACTION_PAGE); return SEARCH_TRANSACTION_PAGE; } @RequestMapping(value=""/"" + SEARCH_TRANSACTION_PAGE method=RequestMethod.POST params=""cancel"") public String cancelSearch() { notifications.redirectTo(HomeController.HOME_PAGE); return ""redirect:/"" + HomeController.HOME_PAGE; } @RequestMapping(value=""/"" + SEARCH_TRANSACTION_PAGE method=RequestMethod.POST params=""execute"") public String executeSearch( @ModelAttribute(""searchCriteria"") @Valid SearchCriteria searchCriteria BindingResult result Model model SessionStatus status) { //searchCriteriaValidator.validate(criteria result); if (result.hasErrors()) { notifications.transferTo(SEARCH_TRANSACTION_PAGE); return SEARCH_TRANSACTION_PAGE; } else { PaymentDetail payment = searchService.getAuthorizationFor(searchCriteria.geteWiseTransactionId()); if (payment == null) { ObjectError error = new ObjectError( ""eWiseTransactionId"" ""Transaction not found""); result.addError(error); model.addAttribute(""searchCriteria"" searchCriteria); notifications.transferTo(SEARCH_TRANSACTION_PAGE); return SEARCH_TRANSACTION_PAGE; } else { model.addAttribute(""authorization"" payment); notifications.redirectTo(PaymentDetailController.PAYMENT_DETAIL_PAGE); return ""redirect:/"" + PaymentDetailController.PAYMENT_DETAIL_PAGE; } } } } Next the test:  package test.unit.com.company.admin.web; import static org.hamcrest.Matchers.containsString; import static org.hamcrest.Matchers.equalTo; import static org.junit.Assert.assertThat; import org.jmock.Expectations; import org.jmock.Mockery; import org.jmock.integration.junit4.JMock; import org.jmock.integration.junit4.JUnit4Mockery; import org.junit.Before; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.ui.Model; import org.springframework.validation.BindingResult; import org.springframework.validation.ObjectError; import org.springframework.web.bind.support.SessionStatus; import com.company.admin.domain.PaymentDetail; import com.company.admin.service.PaymentSearchService; import com.company.admin.service.UserRequestAuditTrail; import com.company.admin.web.HomeController; import com.company.admin.web.PaymentDetailController; import com.company.admin.web.SearchTransactionController; import com.company.admin.web.form.SearchCriteria; /** * Tests the behavior of the SearchTransactionController. * @author slgelma * */ @RunWith(JMock.class) public class SearchTransactionControllerTest { private final Mockery context = new JUnit4Mockery(); private final SearchTransactionController controller = new SearchTransactionController(); private final PaymentSearchService searchService = context.mock(PaymentSearchService.class); private final UserRequestAuditTrail notifications = context.mock(UserRequestAuditTrail.class); private final Model model = context.mock(Model.class); /** * @throws java.lang.Exception */ @Before public void setUp() throws Exception { controller.setSearchService(searchService); controller.setNotifications(notifications); } @Test public void setUpTheSearchForm() { final String target = SearchTransactionController.SEARCH_TRANSACTION_PAGE; context.checking(new Expectations() {{ oneOf(model).addAttribute( with(any(String.class)) with(any(Object.class))); oneOf(notifications).transferTo(with(any(String.class))); }}); String nextPage = controller.setUpTransactionSearch(model); assertThat(""Controller is not requesting the correct form"" target equalTo(nextPage)); } @Test public void cancelSearchTest() { final String target = HomeController.HOME_PAGE; context.checking(new Expectations(){{ never(model).addAttribute(with(any(String.class)) with(any(Object.class))); oneOf(notifications).redirectTo(with(any(String.class))); }}); String nextPage = controller.cancelSearch(); assertThat(""Controller is not requesting the correct form"" nextPage containsString(target)); } @Test public void executeSearchWithNullTransaction() { final String target = SearchTransactionController.SEARCH_TRANSACTION_PAGE; final SearchCriteria searchCriteria = new SearchCriteria(); searchCriteria.seteWiseTransactionId(null); final BindingResult result = context.mock(BindingResult.class); final SessionStatus status = context.mock(SessionStatus.class); context.checking(new Expectations() {{ allowing(result).hasErrors(); will(returnValue(true)); never(model).addAttribute(with(any(String.class)) with(any(Object.class))); never(searchService).getAuthorizationFor(searchCriteria.geteWiseTransactionId()); oneOf(notifications).transferTo(with(any(String.class))); }}); String nextPage = controller.executeSearch(searchCriteria result model status); assertThat(""Controller is not requesting the correct form"" target equalTo(nextPage)); } @Test public void executeSearchWithEmptyTransaction() { final String target = SearchTransactionController.SEARCH_TRANSACTION_PAGE; final SearchCriteria searchCriteria = new SearchCriteria(); searchCriteria.seteWiseTransactionId(""""); final BindingResult result = context.mock(BindingResult.class); final SessionStatus status = context.mock(SessionStatus.class); context.checking(new Expectations() {{ allowing(result).hasErrors(); will(returnValue(true)); never(model).addAttribute(with(any(String.class)) with(any(Object.class))); never(searchService).getAuthorizationFor(searchCriteria.geteWiseTransactionId()); oneOf(notifications).transferTo(with(any(String.class))); }}); String nextPage = controller.executeSearch(searchCriteria result model status); assertThat(""Controller is not requesting the correct form"" target equalTo(nextPage)); } @Test public void executeSearchWithTransactionNotFound() { final String target = SearchTransactionController.SEARCH_TRANSACTION_PAGE; final String badTransactionId = ""badboy""; final PaymentDetail transactionNotFound = null; final SearchCriteria searchCriteria = new SearchCriteria(); searchCriteria.seteWiseTransactionId(badTransactionId); final BindingResult result = context.mock(BindingResult.class); final SessionStatus status = context.mock(SessionStatus.class); context.checking(new Expectations() {{ allowing(result).hasErrors(); will(returnValue(false)); atLeast(1).of(model).addAttribute(with(any(String.class)) with(any(Object.class))); oneOf(searchService).getAuthorizationFor(with(any(String.class))); will(returnValue(transactionNotFound)); oneOf(result).addError(with(any(ObjectError.class))); oneOf(notifications).transferTo(with(any(String.class))); }}); String nextPage = controller.executeSearch(searchCriteria result model status); assertThat(""Controller is not requesting the correct form"" target equalTo(nextPage)); } @Test public void executeSearchWithTransactionFound() { final String target = PaymentDetailController.PAYMENT_DETAIL_PAGE; final String goodTransactionId = ""100000010""; final PaymentDetail transactionFound = context.mock(PaymentDetail.class); final SearchCriteria searchCriteria = new SearchCriteria(); searchCriteria.seteWiseTransactionId(goodTransactionId); final BindingResult result = context.mock(BindingResult.class); final SessionStatus status = context.mock(SessionStatus.class); context.checking(new Expectations() {{ allowing(result).hasErrors(); will(returnValue(false)); atLeast(1).of(model).addAttribute(with(any(String.class)) with(any(Object.class))); oneOf(searchService).getAuthorizationFor(with(any(String.class))); will(returnValue(transactionFound)); oneOf(notifications).redirectTo(with(any(String.class))); }}); String nextPage = controller.executeSearch(searchCriteria result model status); assertThat(""Controller is not requesting the correct form"" nextPage containsString(target)); } } I hope this might help.  EDIT: Cactus is now a dead project: http://attic.apache.org/projects/jakarta-cactus.html You may want to look at cactus. http://jakarta.apache.org/cactus/ Project Description Cactus is a simple test framework for unit testing server-side java code (Servlets EJBs Tag Libs Filters ...). The intent of Cactus is to lower the cost of writing tests for server-side code. It uses JUnit and extends it. Cactus implements an in-container strategy meaning that tests are executed inside the container.  You can do this using Mockito to have the mock return the correct params verify they were indeed called (optionally specify number of times) write the 'result' and verify it's correct. import static org.junit.Assert.*; import static org.mockito.Mockito.*; import java.io.*; import javax.servlet.http.*; import org.apache.commons.io.FileUtils; import org.junit.Test; public class TestMyServlet{ @Test public void testServlet() throws Exception { HttpServletRequest request = mock(HttpServletRequest.class); HttpServletResponse response = mock(HttpServletResponse.class); when(request.getParameter(""username"")).thenReturn(""me""); when(request.getParameter(""password"")).thenReturn(""secret""); PrintWriter writer = new PrintWriter(""somefile.txt""); when(response.getWriter()).thenReturn(writer); new MyServlet().doPost(request response); verify(request atLeast(1)).getParameter(""username""); // only if you want to verify username was called... writer.flush(); // it may not have been flushed yet... assertTrue(FileUtils.readFileToString(new File(""somefile.txt"") ""UTF-8"") .contains(""My Expected String"")); } thanks a lot bro. I've spent a week trying to figure out exactly how to mock out servlet request/response and for some reason this post just made it all snap into focus. appreciated! This way how do you insure ""Cache-Control"" being set on response? Instead of printing to an actual file on disk you could use a StringWriter (as a parameter to the constructor of PrintWriter). You would then assertTrue(stringWriter.toString().contains(""My Expected String"")); This way the test will read/write memory instead of disk.  Just incase the above answers are no longer working in the newer versions of Mockito instead of using mock() and when() Mockito.mock() and Mockito.when() should be used"
965,A,"Eclipse - debugger doesn't stop at breakpoint I am trying to trouble shoot a JUnit. In the source code I have set break point in two places: 1) in a line where a static member is initialized 2) the first line of one of the test cases. The debugger stops in the static field initializing line. But it doesn't stop in the test case. No matter where I set the break point in the test case the debugger doesn't stop there. I know for sure that the test case is executed as I can see the log messages that I have added appear in the log. Any help would be greatly appreciated. I am using Eclipse Galileo and JUnit4 launcher. Can you also add information about which version of JUnit and the JRE you are using? I believe that is helpful in this context. The JDK in use is more important by the way (there are some messed JDKs that could cause this). Please ignore the query on the version of JUnit. Sorry I should have included that. JDK 1.6 update 14. Hmm try downgrading to JDK 5 or a lower version of JDK 6 (definitely not 13) or switch to parallel scavenging GC. Happened to me once when I had unchecked ""Run > Build automatically"" and forgot to re-check it.  Is your code compiled with the -g option turned on to generate the debug information in the .class file? That's required of course. Yes it is compiled with -g option.  Fix could be as simple as clicking run/skip all breakpoints. Worked for me. it took me three days to spot that I'd somehow clicked that Thanks...must have clicked that toolbar icon by accident somehow :-P it saved my time thanks Thanks... no idea when that got clicked  Make sure you declare the package at the top. In my groovy code this stops at breakpoints: package Pkg1 import java.awt.event.ItemEvent; isMule = false class LineItem { // Structure defining individual DB rows public String ACCOUNT_CODE public String ACCOUNT_DESC ... This does not stop at breakpoints: import java.awt.event.ItemEvent; isMule = false class LineItem { // Structure defining individual DB rows public String ACCOUNT_CODE public String ACCOUNT_DESC ...  To remove the breakpoints: Debug your class as a junit test When your debugger stops click the ""breakpoints"" tab next to ""variables"" and ""expressions"" At the top right of the breakpoint tab click the button with two 'X' Stop the test replace your breakpoint and run the debugger again  Usually when this happens to me (rare but it does) means that the code being executed is different than the code in the editor. It will happen from time to time for Eclipse that the built classes and the code in editor are out of sync. When that happens I get all sort of weird debugger behavior (debugging empty lines skipping lines of codes etc). Restarting Eclipse clean all projects and rebuild everything usually clears things up. I had also the Maven plugins (older versions... had not had it for a while now) that had a tendency to do that too. Otherwise it might be a bug maybe the one Vineet stated Hope this helps Yea what you've stated happens more in IDEs that do not automatically recompile code. Oracle JDeveloper (from personal experience) and to a lesser extent (Netbeans) have been found wanting. Good advice. Not said enough. Sometimes we over-think the problem and it's just the system out of synch. Such as it is with rebooting a computer for trouble shooting simple issues.  Yes I have been resolved this issue by removing all debug points from project and clean up then try.  This could be related to one of the bugs in JDK 6 Update 14 as indicated in the release notes for JDK 6 update 15. If this indeed turns out to be the issue you should move to a higher version of the JDK (that's no guarantee though since fixes have been released against 6u16 6u18 and 7b1). The best bet is to use -XX:+UseParallelGC flag. Increasing the size of the minimum and maximum heap size to delay the first GC bring temporary relief. By the way use this bug report in Eclipse to track how others have been faring. You're welcome. u16 seems to be the release where this was fixed to a good enough extent despite u15's release notes. I upgraded to JDK 1.6 update 16. Now it stops at all the break points that I have set. Thanks a lot for your help. Thanks @Vineet - I was pulling my hair out over this one :-)  Make sure under Run > Debug Configurations that 'Stop in main' is selected if applicable to your situation. Please be sure to read previous responses before posting. The problem was their jvm version .. and was solved three years ago. Best not to revive old threads unless your response contributes something significant that was not already covered in previous answers. Thanks. @Answerer your answer solved my problem. It's strange that ""Stop in main"" was unset. It appears that his answer did contribute something significant that was not already covered in previous answers :) Note this is an option for Java applications but not Android applications. thank you..very direct answer very helpful  For JDK7 run->Debug Configurations check ""Keep JUnit running after a test run when debugging"".  One additional comment regarding Vineet Reynolds answer. I found out that I had to set -XX:+UseParallelGC in eclipse.ini I setup the vm arguments as follow -vmargs -Dosgi.requiredJavaVersion=1.7 -Xms512m -Xmx1024m -XX:+UseParallelGC -XX:PermSize=256M -XX:MaxPermSize=512M that solved the issue."
966,A,"Compare scala.xml.Elem object in unit test I have two scala.xml.Elem objects (actual expected). I am using JUnit 4 but have also included XMLUnit 1.3. Is there any easy way to compare the two objects for equality ignoring attribute order and insignificant whitespace in the XML? I tried XMLUnit.assertXMLEqual() but it complains that the types are scala.xml.Elem. I know that I can use equals or == but I would like the benefit of having the assertion print the two values when they are not equal. If I use assertTrue(actual.equals(expected)) and they are not equal the only output will be ""assertion failed"". The earlier answers were helpful to me though I found that sometimes I wanted to check a larger chunk of XML and the failure comparison showing both chunks of XML was a bit hard to read. This method will try to recurse down into child elements first to compare those so if a deeply nested element is incorrect it will show a much more concise error. Depending on your XML this might not give you enough context to work out where it's actually failing but I find it useful. /** Check that the XMLs are the same ignoring empty text nodes (whitespace). */ private def assertEqual(actual: xml.Node expected: xml.Node) { def recurse(actual: xml.Node expected: xml.Node) { // depth-first checks to get specific failures for ((actualChild expectedChild) <- actual.child zip expected.child) { recurse(actualChild expectedChild) } actual should be (expected) } recurse(scala.xml.Utility.trim(actual) scala.xml.Utility.trim(expected)) }  Use the version of assertTrue that allows passing custom messages public static void assertTrue(java.lang.String message boolean condition) and (for example) diff to produce the string with the descendand nodes that aren't equal scala> val xml1 = <person><name>john</name><lastname>smith</lastname></person> xml1: scala.xml.Elem = <person><name>john</name><lastname>smith</lastname></person> scala> val xml2 = <person><name>jane</name><lastname>smith</lastname></person> xml2: scala.xml.Elem = <person><name>jane</name><lastname>smith</lastname></person> scala> assert(xml1 == xml2 xml1.child diff xml2.child mkString("" "")) java.lang.AssertionError: assertion failed: <name>john</name> at scala.Predef$.assert(Predef.scala:91) at .<init>(<console>:8) at .<clinit>(<console>) I had considered using assertTrue. I did not know about `diff`. I'll try that. Thanks.  If you want to compare to XML Elem objects ignoring whitespaces you can remove the whitespaces from them with scala.xml.Utility.trim method. scala> val a = <foo>bar</foo> a: scala.xml.Elem = <foo>bar</foo> scala> val b = <foo> bar </foo> b: scala.xml.Elem = <foo> bar </foo> scala> a == b res8: Boolean = false scala> import scala.xml.Utility.trim import scala.xml.Utility.trim scala> trim(a) == trim(b) res9: Boolean = true Scala does not care about the order of the attributes if you use XML literals: scala> val a = <foo first=""1"" second=""2"" /> a: scala.xml.Elem = <foo first=""1"" second=""2""></foo> scala> val b = <foo second=""1"" first=""1"" /> b: scala.xml.Elem = <foo first=""1"" second=""1""></foo> scala> a == b res22: Boolean = true I would recommend ScalaTest for unit testing there you have the ShouldMatchers: // Scala repl started with scalatest-1.2.jar in the classpath scala> val a = <foo>bar</foo> a: scala.xml.Elem = <foo>bar</foo> scala> val b = <foo>bar</foo> b: scala.xml.Elem = <foo>bar</foo> scala> a should equal(b) scala> val b = <foo>bar2</foo> b: scala.xml.Elem = <foo>bar2</foo> scala> a should equal(b) org.scalatest.TestFailedException: <foo>bar</foo> did not equal <foo>bar2</foo> at org.scalatest.matchers.Matchers$class.newTestFailedException(Matchers.scala:148) at org.scalatest.matchers.ShouldMatchers$.newTestFailedException(ShouldMatchers.scala:2329) at org.scalatest.matchers.ShouldMatchers$ShouldMethodHelper$.shouldMatcher(ShouldMatchers.scala:871) at org.scalatest.matchers.ShouldMatchers$SeqShouldWrapper.should(ShouldMatchers.scala:1724) at .<init>(<console>:15) at .<clinit>(<console>) at RequestResult$.<init>(<console>:9) at RequestResult$.<clinit>(<console>) at RequestResult$scala_repl_result(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.Delega... I was going to post a followup question about trimming whitespace but your answer covers that. Thanks."
967,A,"What java version is needed for JUnit 4.8 I am trying to run JUnit test with a 1.5 JRE but get the error Message: java.lang.UnsupportedClassVersionError: Bad version number in .class file When I switch to JRE 1.6 (actually a JDK but that shouldn't matter right?) everythings works fine. So the questions are: Do we really need Java 6 for the current JUnit version? what is the newest JUnit version that works with Java 5? A spot check of a few classes shows that the JUnit 4.8.2 jar file was compiled with java 5 or with the java 6 compiler with the -target option set so that it makes java5-compatible class files. A far more likelier explanation is that you've accidentally compiled the test you wanted to run with a Java 6 JDK. What does: javap -verbose -classpath YOUR_TOP_DIRECTORY com.yourname.YourTest say at the top of its output for ""major version""? If it says 50 then the problem is that you've compiled your test class for java 6 only. You need to either use a JDK 5 compiler or pass the option -target 1.5 to your compiler. The culprint was another library I used.  The current .jar file for JUnit 4.8.2 was compiled with/for Java 5 (class version number is 49.0). I remember an earlier version accidentally being built with Java 6 (and without the -target switch) but that was simply a mistake.  The pom.xml of Junit says jdk 1.5. http://github.com/KentBeck/junit/blob/master/pom.xml"
968,A,"Can I write a test without any assert in it? I'd like to know if it is ""ok"" to write a test without any ""assert"" in it. So the test would fail only when an exception / error has occured. Eg: like a test which has a simple select query to ensure that the database configuration is right. So when I change some db-configuration I re-run this test and check if the configuration is right. ? Thanks! @Kyle Rozendo: JUnit can (and typically *is*) used for more than unit testing. You added some ""unit-testing"" tag to this question but I'm really not sure that a test that verifies some ""db-configuration"" (the OP wrote *so when I change some db-configuration I re-run this test and check if the configuration is right*) is really a unit-test. Nowhere did the OP mention unit-testing and I wouldn't call such a test a ""unit test"". @Web - ""Unit testing is a software verification and validation method in which a programmer tests if individual units of source code are fit for use."" I Disagree :) This kind of test is sometimes called a ""smoke test"". If smoke comes out it's failed the test. Testing is a really subjective discussion. Some people will say no you should always have AAA syntax. Personally I've written tests that do things very similar to what your talking about so I'd say sure go ahead - if it helps you build a more stable app then why not. For example in NUnit i consider [ExpectedException typeof(XXXX)] to be logically equivalent to an Assert. Also in some tests you might not assert anything but expect a particular order of execution via Mocks and Expects. thanks ! Yes. I'm reminded of seeing some JMock tutorial without asserts but Expectations. Will explore on that !  As @Kyle noted your test case is valid. In fact the opposite would also be a valid: when you write a test case to confirm that a certain call with specific parameter(s) results in an exception.  It is perfectly valid to make sure a unit test runs without encountering an exception. As per Matt B's suggestion be sure to document what the test is actually testing to be clear and precise. just be sure document what the test is actually testing to be clear thanks ! @matt Yes I would ! :) (The example I gave defenitely requires that )  It is surely acceptable to write a unit test that doesn't have any assertions. You could do this for: Testing a case that ends without an exception. In this case if you can it's nice to dress the test with the specific type of the exception as in [ExpectedException(MyException)]. Testing a feature is there. Even there isn't a possibility that the test may generate an exception you may want to make this test fail if someone decides to remove that feature. If the test uses a method and the method is removed the test will simply fail to build.  Sure you can do that. It is also perfectly fine to write a test without assertions where the expected outcome is an exeption. I know testng will let you specify an exception that should be thrown and the test will fail if the expected exception isn't thrown."
969,A,"Junit test methods What are the most oftenly used test methods I should start with in order to get familiar with unit testing? There are just a lot of them but I guess there are some like common or something. I meant Junit methods like AssertTrue() etc. What do you mean ""test methods""? Like AssertTrue() for instance. AssertTrue() gets you alot! :) assertEquals is the most commonly used test method. assertEquals( ""string1"" ""string1"" ); //would fail assertEquals( expectedValue actualValue ); //would pass if expectedValue.equals( actualValue ) You can also add a comment that is printed if the assertion fails: assertEquals( ""method result should be 7"" 7 thing.methodThatShouldReturn7() ); //would pass if 7 == thing.methodThatShouldReturn7() See the Assert class javadoc for more details and once you are comfortable with assertEquals you can look at the other assert options available to you. You really want to use the latter method that gives you an error string; it'll help when things go wrong. Also note how the message is worded as an expectation rather than telling some wrong value. That way of wording helps a lot in figuring out what happened  @Before and @After annotations (equivalent to what setUp() and tearDown() are for).  There are just a few patterns to learn with multiple implementing methods for different types and an optional initial message argument. assertEquals() assertTrue() and assertFalse() assertNull() and assertNotNull() assertSame() and assertNotSame() fail() assertArrayEquals() assertThat() At a minimum you'll need to learn all the patterns but the last -- these are all needed for different situations.  I would also recommend knowing about fail() and exception handling within JUnit. The brief suggestion is to always throw exceptions from the test method unless testing for that specific exception. Catching exceptions and failing works but you lose quite a bit of information on reports. A good article about it is here: http://www.exubero.com/junit/antipatterns.html.  setUp() and tearDown() they are called before and after each case."
970,A,"write eclipse junit plugin test where do i start on writing plugin test? I have written some toy plugins and would like to start on doing TDD with my plugins. from the plugin test how do you interact with eclipse? for example programmatically post a event to do some actions? What kind of functionality would you like to test? As a matter of fact JUnit plugin tests are just plain JUnit tests there is nothing Eclipse-specific about them. The only difference is that if you run you JUnit test as a JUnit plugin test they will run inside an instance of Eclipse. If your plugin are RCP (Rich Client Platform) plugins with SWT you could use SWTBot. Those test can be encapsulated into JUnit one: If your plugins are OSGi-based you must be aware that OSGi bundle runs it's own class loader and therefore the classes appear not to be in the same package. See ""Is OSGi the enemy of JUnit tests?"" Make your test plugin a fragment. One problem is that other plugins cannot access classes defined in fragments (as Patrick Paulin points out in a more detailed discussion about fragments in unit tests). Another problem is that plugin.xml in a fragment is ignored. And therefore you test plugin cannot contribute From Patrick's article: A fragments looks much like a plug-in from the outside. It is represented as a separate project in your workspace it contains a manifest that describes its contents it is built and deployed as a jar. What makes a fragment different is that it contributes it’s resources at runtime to a single host plug-in. The classes in the fragment are therefore loaded by the host plug-in’s classloader. By placing our unit tests in fragments we can provide our tests access to the non-public methods of the classes under test. Also because the source code and test code are actually part of the same plug-in there are no issues related to non-exported packages. Test classes will have access to all packages in the plug-in whether they are exported or not. The main disadvantage to this fragment based approach is that it is difficult to aggregate unit tests into a master test suite. While it’s easy to create a test suite that includes the tests within a fragment it’s not so easy to create a suite that includes the tests in multiple fragments. If your plugins need just some simple testing a JUnit test suite is enough Create a new test case BookTest in the package test.yourpackage Right click on the package and choose ""New > JUnit Test Case""."
971,A,"Assert statement causing JUnit tests to stop In the class I am testing there are a few assert statements that check for various conditions. One of the methods is GetNames(string id){ assert(! id.Equals("""")); // Causes all junit tests to stop ... } and there is an assert statement to check if id is not blank. In my unit tests I have one test where I pass it a blank string and a few others. The problem is that when the assert statement gets executed in the class the JUnit tests stop running after that test case. How can I change/setup the unit tests so that if there is an assertion failure in the class the tests do not stop. I have to enable the assertions in order for the code to run properly since there are cases where variables are incremented in the assertions. Unfortunately I cannot change any of the code. I am using the JUnit plugin in eclipse. I do not have any code in the setup and teardown sections. If you have assert statements in the code being tested and assertions are enabled then those statements will be causing an AssertionError to be thrown. You can catch that in your test code and ignore it: try { xxx.GetNames(""""); } catch (AssertionError e) { // this is expected - ignore it } Thanks. I just realized the code which is causing the tests to stop actually calls ""system.exit(1)"". Is there a way to still run the unit tests after system.exit has been called? No it causes the VM to terminate. What is the code that you're testing? Any library code that calls `System.exit` is broken. Actually you could write your own `SecurityManager` that prohibits a call to `System.exit`. See for instance http://stackoverflow.com/questions/309396/java-how-to-test-methods-that-call-system-exit If the intent is to verify that the exception is thrown you need to call `fail()` after calling `GetNames`. If the intention is to ignore the assertion failure then whatever `GetNames` does might not have been completed so it would be questionable to do that in a test (where you wanted to know the intended state of your objects before you make assertions)"
972,A,"Configuring IntelliJ IDEA for unit testing with JUnit I feel like an idiot for asking this but I've wasted way too much time on this already. I'm an old Eclipse user which makes starting a java project with JUnit obvious. Just start writing a test class and it offers to add any libraries and imports that the file needs. I decided to try out IntelliJ this morning so downloaded the trial. The JUnit plugin is installed. I make a new Java project and I want to write a test case. How do I add the junit.jar to my project? (I actually want to add it to every java project now and forever more - is there a way of doing that?). you could accept one as the answer and I would prefer the one from @CrazyCoder... If you already have a test class but missing the JUnit library dependency please refer to Configuring Libraries for Unit Testing documentation section. Pressing Alt+Enter on the red code should give you an intention action to add the missing jar. However IDEA offers much more. If you don't have a test class yet and want to create one for any of the source classes see instructions below. You can use the Create Test intention action by pressing Alt+Enter while standing on the name of your class inside the editor or by using Ctrl+Shift+T keyboard shortcut. A dialog appears where you select what testing framework to use and press Fix button for the first time to add the required library jars to the module dependencies. You can also select methods to create the test stubs for. You can find more details in the Testing help section of the on-line documentation. Ah k thank you. The link to ""Testing help section"" led me to putting the cursor on the unresolved @Test in my test file and now my life is complete! Doesn't cmd-shift-T mean I have to start with the class under test instead of the test itself? Updated my reply to cover your case. First link does not work. Here's the correct one: http://site2.jetbrains.com/idea/webhelp/configuring-libraries-for-unit-testing.html I've updated the link it's now http://www.jetbrains.com/idea/webhelp/configuring-testing-libraries.html. @Bob +1 for pointing at the flow breach. But is making MyClass first then immediately create the corresponding MyClassTest with Ctrl+Shift+T; is that necessarily that bad? Making MyClassTest implies that there will be a MyClass class one test later right?  Press Ctrl+Shift+T in the code editor. It should solve your problems. Mac OS: ⌘ Cmd+Shift+T"
973,A,"How do I use a JUnit Parameterized runner with a varargs constructor? I wrote a mockup example to illustrate this without exposing anything confidential. It's a ""dummy"" example which does nothing but the problem occurs in the test initialiser. @RunWith(Parameterized.class) public class ExampleParamTest { int ordinal; List<String> strings; public ExampleParamTest(int ordinal String... strings) { this.ordinal = ordinal; if (strings.length == 0) { this.strings = null; } else { this.strings = Arrays.asList(strings); } } @Parameters public static Collection<Object[]> data() { return Arrays.asList(new Object[][] { {0 ""hello"" ""goodbye""} {1 ""farewell""} }); } @Test public void doTest() { Assert.assertTrue(true); } } Basically I have a test constructor which accepts multiple arguments for a local list variable and I want to populate this through an array initialiser. The test method will handle the local list variable correctly - I have removed this logic to simplify the test. When I write this my IDE has no complaints about syntax and the test class builds without any compile errors. However when I run it I get: doTest[0]: java.lang.IllegalArgumentException: wrong number of arguments at java.lang.reflect.Constructor.newInstance(Unknown Source) doTest[1]: java.lang.IllegalArgumentException: argument type mismatch at java.lang.reflect.Constructor.newInstance(Unknown Source) What exactly has gone wrong here and how do I correctly use this pattern? What is doTest and what data does it hold? In this example doTest is just a dummy test to allow this testsuite to execute in JUnit. The problem is not in the test code as you will see when you try to run this test as it stands; despite the fact that it has an almost empty test body it still fails to run due to the aforementioned initialisation error. I added part of the stacktrace that I see when I run your test to make it clear that the error is happening when the varargs constructor is invoked. If that's different from what you're seeing feel free to change it or revert it. Can't test it right now but I guess if you invoke a method or a constructor with variable arguments you have to invoke it with an array instead of a variable list of values. If I'm right then this should work: @Parameters public static Collection<Object[]> data() { return Arrays.asList(new Object[][] { {0 new String[]{""hello"" ""goodbye""}} {1 new String[]{""farewell""}} }); } Some explanation On source code level we can write test = ExampleParamTest(0 ""one"" ""two""); The compiler will convert this to an array of Strings. JUnit uses the reflection and invocation API and from this perspective the constructors signature is public ExampleParamTest(int i String[] strings); So to invoke the constructor - and that's what JUnit is doing internally - you have to pass an integer and a String array. I tried it it works. You're right this did work. But it's not universally true. Normally I could write something like: myTest = new ExampleParamTest(0 ""hello"" ""goodbye""); This is the entire point of variable argument methods. Why then does this not work in my example?"
974,A,"How can I use the Parameterized JUnit test runner with a field that's injected using Spring? I'm using Spring to inject the path to a directory into my unit tests. Inside this directory are a number of files that should be used to generate test data for parameterized test cases using the Parameterized test runner. Unfortunately the test runner requires that the method that provides the parameters be static. This doesn't work for my situation because the directory can only be injected into a non-static field. Any ideas how I can get around this? Gishu: sets static fields no? Maybe not in JUnit because the order or execution would have the @Parameters method called before Spring. But in general? Can you change to another mechanism to inject a string into a test-fixture ? I mean the bottleneck here seems to be that Spring can't set non-static fields/ I assume you are using JUnit 4.X since you mentioned the Parameterized test runner. This implies you aren't using @RunWith(SpringJUnit4ClassRunner). Not a problem just listing my assumptions. The following uses Spring to get the test files directory from the XML file. It doesn't inject it but the data is still available to your test. And in a static method no less. The only disadvantage I see is that it may mean your Spring config is getting parsed/configured multiple times. You could load just a smaller file with test specific info if need be. @RunWith(Parameterized.class) public class MyTest { @Parameters public static Collection<Object[]> data() { ApplicationContext ctx = new ClassPathXmlApplicationContext(""/jeanne/jeanne.xml""); String dir = ctx.getBean(""testFilesDirectory"" String.class); // write java code to link files in test directory to the array return Arrays.asList(new Object[][] { { 1 } }); } // rest of test class } Good idea. I forget completely about just getting the `ApplicationContext` manually. It makes the code a bit uglier but it makes the test output infinitely better to be able to used `Parameterized`.  Remember that Spring inject using @Autowired but also with setter. So instead of using @Autowired use the setter: private static String directory; public void setDirectory(String directory) { this.directory = directory; } public static String getDirectory() { return directory; }  You can use a TestContextManager from Spring. In this example I'm using Theories instead of Parameterized. @RunWith(Theories.class) @ContextConfiguration(locations = ""classpath:/spring-context.xml"") public class SeleniumCase { @DataPoints public static WebDriver[] drivers() { return new WebDriver[] { firefoxDriver internetExplorerDriver }; } private TestContextManager testContextManager; @Autowired SomethingDao dao; private static FirefoxDriver firefoxDriver = new FirefoxDriver(); private static InternetExplorerDriver internetExplorerDriver = new InternetExplorerDriver(); @AfterClass public static void tearDown() { firefoxDriver.close(); internetExplorerDriver.close(); } @Before public void setUpStringContext() throws Exception { testContextManager = new TestContextManager(getClass()); testContextManager.prepareTestInstance(this); } @Theory public void testWork(WebDriver driver) { assertNotNull(driver); assertNotNull(dao); } } I found this solution here : How to do Parameterized/Theories tests with Spring  I use the following solution with the Parameterized.class without any problem: http://bmocanu.ro/coding/320/combining-junit-theoriesparameterized-tests-with-spring/ @ContextConfiguration(value = ""classpath:test-context.xml"") public abstract class AbstractJunitTest extends AbstractJUnit4SpringContextTests { private static TestContextManager testContextManager = null; private static DAOFactory daoFactory = null; @Before public void initApplicationContext() throws Exception { if (testContextManager == null) { testContextManager = new TestContextManager(getClass()); testContextManager.prepareTestInstance(this); daoFactory = (DAOFactory)applicationContext.getBean(""daoFactory""); } } protected DAOFactory getDaoFactory() throws Exception { return daoFactory; } } @RunWith(Parameterized.class) public class SomeTestClass extends AbstractJunitTest { ... }"
975,A,"JUnit test on URLConnection use EasyMock? Hey have been trying to work this out for last day or so but hitting brick wall. Trying to unit test this bit of code. But not sure if need to use EasyMock or not?? Seem few examples online but seem to be using older techniques. public boolean verifyConnection(final String url) { boolean result; final int timeout = getConnectionTimeout(); if (timeout < 0) { log.info(""No need to verify connection to client. Supplied timeout = {}"" timeout); result = true; } else { try { log.debug(""URL: {} Timeout: {} "" url timeout); final URL targetUrl = new URL(url); final HttpURLConnection connection = (HttpURLConnection) targetUrl.openConnection(); connection.setConnectTimeout(timeout); connection.connect(); result = true; } catch (ConnectException e) { log.warn(""Could not connect to client supplied url: "" + url e); result = false; } catch (MalformedURLException e) { log.error(""Malformed client supplied url: "" + url e); result = false; } catch (IOException e) { log.warn(""Could not connect to client supplied url: "" + url e); result = false; } } return result; } It just take's in a url checks its valid and returns T or F. Do you want to test this method or do you want to test something else and need to mock the invovation of this method? If you want to mock this method I'd recommend passing in the URL rather than the String. Don't have your method create the URL it needs; let the client create the URL for you and pass it in. That way your test can substitute a mock if it needs to. It's almost a dependency injection idea - your method should be given its dependencies and not create them on its own. The call to ""new"" is the dead giveaway. It's not a drastic change. You could overload the method and have two signatures: one that accepts a URL string and another that accepts the URL itself. Have the first method create the URL and call the second. That way you can test it and still have the method with the String signature in your API for convenience.  Trying to set up mock implementation of the HttpURLConnection. Like public class MockHttpURLConnection extends HttpURLConnection {' then added method to class to override ' protected HttpURLConnection createHttpURLConnection(URL url) throws IOException { return (HttpURLConnection) url.openConnection(); } So test looking something like this: @Test public void testGetContentOk() throws Exception { String url = ""http://localhost""; MockHttpURLConnection mockConnection = new MockHttpURLConnection(); TestableWebClient client = new TestableWebClient(); client.setHttpURLConnection(mockConnection); boolean result = client.verify(url); assertEquals(true result); } @Test public void testDoesNotGetContentOk() throws Exception { String url = ""http://1.2.3.4""; MockHttpURLConnection mockConnection = new MockHttpURLConnection(); TestableWebClient client = new TestableWebClient(); client.setHttpURLConnection(mockConnection); boolean result = client.verify(url); assertEquals(false result); } /** * An inner private class that extends WebClient and allows us * to override the createHttpURLConnection method. */ private class TestableWebClient extends WebClient1 { private HttpURLConnection connection; /** * Setter method for the HttpURLConnection. * * @param connection */ public void setHttpURLConnection(HttpURLConnection connection) { this.connection = connection; } /** * A method that we overwrite to create the URL connection. */ @Override public HttpURLConnection createHttpURLConnection(URL url) throws IOException { return this.connection; } } First part passed but is getting true for false dummy test thanks for feedback back so far best site I have found for help. So let me know if think on right track Dave don't add an answer as a response to your question.  I have always observed that Mocking Can be avoided as much as possible because it can lead to difficult to maintain JUnit tests and defeat the whole purpose. My suggestion would be to create a temporary server on your local machine from a JUnit itself. At the beginning of JUnit you can create a server(not more than 10-15 lines of coding required) using Java sockets and then in your code pass the URL for the local server. This way you are reducing mocking and ensuring maximum code coverage. Something like this - public class SimpleServer extends Thread { public void run() { try { serverSocket = new ServerSocket(port); while (true) { Socket s = serverSocket.accept(); } } catch (IOException e) { e.printStackTrace(); } finally { serverSocket = null; } } }"
976,A,Grouping JUnit tests Is there any way to group tests in JUnit so that I can run only some groups? Or is it possible to annotate some tests and then globally disable them? I'm using JUnit 4 I can't use TestNG. edit: @RunWith and @SuiteClasses works great. But is it possible to annotate like this only some tests in test class? Or do I have to annotate whole test class? Related: http://stackoverflow.com/questions/457276/junit4-test-suites/4952225#4952225 `@RunWith` and `@SuiteClasses` is not great at all look at http://stackoverflow.com/questions/18894951 JUnit 4.8 supports grouping: public interface SlowTests {} public interface IntegrationTests extends SlowTests {} public interface PerformanceTests extends SlowTests {} And then... public class AccountTest { @Test @Category(IntegrationTests.class) public void thisTestWillTakeSomeTime() { ... } @Test @Category(IntegrationTests.class) public void thisTestWillTakeEvenLonger() { ... } @Test public void thisOneIsRealFast() { ... } } And lastly @RunWith(Categories.class) @ExcludeCategory(SlowTests.class) @SuiteClasses( { AccountTest.class ClientTest.class }) public class UnitTestSuite {} Taken from here: http://weblogs.java.net/blog/johnsmart/archive/2010/04/25/grouping-tests-using-junit-categories-0 Also Arquillian itself supports grouping: https://github.com/weld/core/blob/master/tests-arquillian/src/test/java/org/jboss/weld/tests/Categories.java  You can create test Suite objects that contain groups of tests. Alternatively your IDE (like Eclipse) may have support for running all the tests contained in a given package.  Do you want to group tests inside a test class or do you want to group test classes? I am going to assume the latter. It depends on how you are running your tests. If you run them by Maven it is possible to specify exactly what tests you want to include. See the Maven surefire documentation for this. More generally though what I do is that I have a tree of test suites. A test suite in JUnit 4 looks something like:  @RunWith(Suite.class) @SuiteClasses(SomeUnitTest1.class SomeUnitTest2.class) public class UnitTestsSuite { } So maybe I have a FunctionTestsSuite and a UnitTestsSuite and then an AllTestsSuite which includes the other two. If you run them in Eclipse you get a very nice hierarchical view. The problem with this approach is that it's kind of tedious if you want to slice tests in more than one different way. But it's still possible (you can for example have one set of suites that slice based on module then another slicing on the type of test). Had to use syntax: @SuiteClasses({SomeUnitTest1.class SomeUnitTest2.class})  To handle the globally disabling them JUnit (4.5+) has two ways One is to use the new method assumeThat. If you put that in the @BeforeClass (or the @Before) of a test class and if the condition fails it will ignore the test. In the condition you can put a system property or something else that can be globally set on or off. The other alternative is to create a custom runner which understands the global property and delegates to the appropriate runner. This approach is a lot more brittle (since the JUnit4 internal runners are unstable and can be changed from release to release) but it has the advantage of being able to be inherited down a class hierarchy and be overridden in a subclass. It is also the only realistic way to do this if you have to support legacy JUnit38 classes. Here is some code to do the custom Runner. Regarding what getAppropriateRunnerForClass might do the way I implemented it was to have a separate annotation that tells the custom runner what to run with. The only alternative was some very brittle copy paste from the JUnit code. private class CustomRunner implements Runner private Runner runner; public CustomRunner(Class<?> klass RunnerBuilder builder) throws Throwable { if (!isRunCustomTests()) { runner = new IgnoredClassRunner(klass); } else { runner = getAppropriateRunnerForClass(klass builder); } public Description getDescription() { return runner.getDescription(); } public void run(RunNotifier notifier) { runner.run(notifier); } } EDIT: The @RunWith tag only works for a whole class. One way to work around that limiation is to move the test methods into a static inner class and annotate that. That way you have the advantage of the annotation with the organization of the class. But doing that won't help with any @Before or @BeforeClass tags you will have to recreate those in the inner class. It can call the outer class's method but it would have to have its own method as a hook.
977,A,"TDD for Java Command-line program and nested readLine() Basically I'm writing a Java Command-line application which accept parameter from user by readLine. Ex if the user press ""1"" it will ask which book the user wants to check out the user can press the number of the book by ""1"" or ""2"" ... So the application accepts user parameter two times. And I'm trying to use TDD to test the application alongside as well. Here is the problem. If I do something like this to simulate user input System.setIn(new ByteArrayInputStream(PRESS_TWO.getBytes())); // set the first option Program.main(new String[]{}); // run the program System.setIn(new ByteArrayInputStream(PRESS_ONE.getBytes())); // set the second option The first step that waits for user to choose option is ok but it will go right through the second step without waiting for the second input. How can I simulate this in JUnit? Thanks This is how I read input  try { i1 = Integer.parseInt(reader.readLine()); } catch (Exception e) { System.out.println(""Enter a valid integer!!""); } if (i1 == 1) { System.out.println("" 1. Book1 ""); System.out.println("" 2. eBook2 ""); System.out.println("" 3. Book3 ""); } else if (i1 == 2) { System.out.println("" Which one do you want?: ""); int i2 = 0; try { i2 = Integer.parseInt(reader.readLine()); } catch (Exception e) { // Do you know what numbers are!!! System.out.println(""Enter a valid integer!!""); } } One of the great things about test driven development is that it can highlight design issues. I personally would take the difficulty you're having with this as a sign you might have a problem with your design. You might consider breaking up your logic. First have an input controller. Test that thoroughly. Then perhaps you need a menu view. Then perhaps a book retrieval controller that takes a book number. Each of these is far easier to test and in making the program easier to test you've now also made it more modular and each piece is more reusable. +1 Marvo nice. Marvo if you had that as an answer I'd upvote it fo sho. Thanks I decided to change my design to State Pattern which is much more easier to test. It depends on how your code reads the input but I would assume that once the end of the first ByteArrayInputStream is reached your code sees end-of-file and terminates. Try to combine the two streams into one (separated by a newline). I just added the code how I read input. Note that `main` will run to completion (or until an exception is thrown) even before the second stream is set. The first `reader.readLine()` will succeed but the second one will probably return `null` since it sees that the end of the (first) stream has been reached (what is the type of `reader` by the way?). When `Integer.parseInt()` receives `null` it will throw an exception which will get caught and `main()` will terminate normally. Can I do something to pause it and accept the second parameter? I'm looking at SequenceInputStream but it's not working. Not unless you restructure the code and split up the method; coroutines (methods that can ""leave"" at a certain point and resume execution from the same point later) do not exist in Java. You might want to look at @Marvo's suggestions. But is there a reason you can't just give it all the user input in one stream?"
978,A,How to use JUnit and Hamcrest together? I can't understand how JUnit 4.8 should work with Hamcrest matchers. There are some matchers defined inside junit-4.8.jar in org.hamcrest.CoreMatchers. At the same time there are some other matchers in hamcrest-all-1.1.jar in org.hamcrest.Matchers. So where to go? Shall I explicitly include hamcrest JAR into the project and ignore matchers provided by JUnit? In particular I'm interested in empty() matcher and can't find it in any of these jars. I need something else? :) And a philosophical question: why JUnit included org.hamcrest package into its own distribution instead of encouraging us to use original hamcrest library? Also if JUnit 4.1.1 + Hamcrest 1.3 + Mockito 1.9.5 are being used make sure mockito-all is not used. It contains Hamcrest core classes. Use mockito-core instead. The below config works :  <dependency> <groupId>org.hamcrest</groupId> <artifactId>hamcrest-all</artifactId> <version>1.3</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> <version>1.9.5</version> <scope>test</scope> <exclusions> <exclusion> <artifactId>hamcrest-core</artifactId> <groupId>org.hamcrest</groupId> </exclusion> </exclusions> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.1.1</version> <scope>test</scope> <exclusions> <exclusion> <artifactId>hamcrest-core</artifactId> <groupId>org.hamcrest</groupId> </exclusion> </exclusions> </dependency>  why JUnit included org.hamcrest package into its own distribution instead of encouraging us to use original hamcrest library? I would guess that's because they wanted the assertThat to be part of JUnit. So that means the Assert class has to import the org.hamcrest.Matcher interface and it can't do that unless JUnit either depends on Hamcrest or includes (at least part of) Hamcrest. And I guess including part of it was easier so that JUnit would be usable without any dependencies.  If you're using a Hamcrest with a version greater or equal than 1.2 then you should use the junit-dep.jar. This jar has no Hamcrest classes and therefore you avoid classloading problems. Since JUnit 4.11 the junit.jar itself has no Hamcrest classes. There is no need for junit-dep.jar anymore.  Not exactly answering your question but you should definitely try FEST-Assert fluent assertions API. It's competing with Hamcrest but has a much easier API with only one static import required. Here is the code provided by cpater using FEST: package com.test; import java.util.ArrayList; import java.util.List; import org.junit.Test; import static org.fest.assertions.Assertions.assertThat; public class EmptyTest { @Test public void testIsEmpty() { List myList = new ArrayList(); assertThat(myList).isEmpty(); } } EDIT: Maven coordinates: <dependency> <groupId>org.easytesting</groupId> <artifactId>fest-assert</artifactId> <version>1.4</version> <scope>test</scope> </dependency> indeed looks interesting! I just swapped my assertion library. I was quite satisfied with hamcrest but because of the junit problematic inclusion stuff and some hard to write test (with collection and generics) I'm know in love with FEST! Thanks for sharing. Bump - FEST is now up to 2.0x. https://github.com/alexruiz/fest-assert-2.x/wiki  junit provides new check assert methods named assertThat() which uses Matchers and should provide a more readable testcode and better failure messages. To use this there are some core matchers included in junit. You can start with these for basic tests. If you want to use more matchers you can write them by yourself or use the hamcrest lib. The following example demonstrates how to use the empty matcher on an ArrayList: package com.test; import static org.hamcrest.Matchers.empty; import static org.hamcrest.Matchers.is; import static org.junit.Assert.assertThat; import java.util.ArrayList; import java.util.List; import org.junit.Test; public class EmptyTest { @Test public void testIsEmpty() { List myList = new ArrayList(); assertThat(myList is(empty())); } } (I included the hamcrest-all.jar in my buildpath) where exactly `org.hamcrest.Matchers.empty()` is located? Could you please give a link to the JAR file? You can find all here: http://code.google.com/p/hamcrest/ and the download of hamcrest-all.jar here: http://code.google.com/p/hamcrest/downloads/detail?name=hamcrest-all-1.2.jar&can=2&q= Looks like hamcrest 1.2 [is not](http://code.google.com/p/hamcrest/issues/detail?id=12) in Maven Central repository. That's the problem I'm facing :( Ok in repo is only 1.1 and there is no empty Matcher. You can download the 1.2 jar and install it in your local repo: http://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html Hamcrest 1.3 has now been released and is in maven central. A relevant comment from Stefan Birkner in the answers [If you're using a Hamcrest with a version greater or equal than 1.2 then you should use the junit-dep.jar. This jar has no hamcrest classes and therefore you avoid classloading problems.](http://stackoverflow.com/a/7353554/759452) I thoroughly recommend using org.hamcrest.MatcherAssert.assertThat rather than the JUnit version as it will allow more descriptive error messages.
979,A,"How to test Java app operating directly on external API After comming from Ruby world I'm having little problems doing TDD in Java. The biggest issue is when I have application that is just communicating with external API. Say I want to just fetch some data from Google Calendar or 5 tweets from some Twitter user and display it. In Ruby I don't have any problems because I can monkey-patch the API library in tests directly but I have no such option in Java. If I think about this in terms of MVC my model objects are directly accessing the API through some library. The question is is this bad design? Should I always wrap any API library in some interface so I can mock/stub it in Java? Because when I think about this the only purpose of that interface would be to simulate (please don't kill me for saying this) the monkey-patch. Meaning that any time I use any external resource I have to wrap each layer in interface that can be stubbed out. # do I have to abstract everything just to do this in Java? Twitter.stub!(:search) Now you might say that I should always abstract away the interface so I can change the underlying layer to anything else. But if I'm writing twitter app I'm not going to change it to RSS reader. Yes I can add for example Facebook and then it would make sense to have interface. But when there is no other resource that can be substituted for the one I'm using than I still have to wrap everything in interfaces to make it testable. Am I missing something or is this just a way to test in the Java world? I understand that what you want are Mock objects. As you described it one of the ways one can generate ""test versions"" of objects is by implementing a common interface and using it. However what you are missing is to simply extend the class (provided that it is not declared final) and override the methods that you want to mock. (NB: the possibility of doing that is the reason why it is considered bad form for a library to declare its classes final - it can make testing considerably harder.) There is a number of Java libraries that aim in facilitating the use of Mock objects - you can look at Mockito or EasyMock. Yes I can extend it but that requires to have another layer that will pass the ""API"" object (mocked version in tests) to the model. But I guess there's no way around that. Well in that case you could either use a factory method to create the object or you could replace the private field in your model by using reflection.  Wrapping the external API is the way I would do this. So as you already said you would have an interface and two classes: the real one and the dummy implementation. Yes it may seem unreasonable from the perspective of some services indeed being specific like Twitter. But this way your build process doesn't depend on external resources. Depending on external libraries isn't all that bad but having your tests depend on actual data present or not present out there on the web can mess up the build process. The easiest way is to wrap the API service with your interface/class pair and use that throughout your code. I am not advocating against use of mock objects for testing. I just think that what I described is the simplest thing that could possibly work in this scenario. Well external services should have *published APIs* which are contracts not meant to be broken without good reason. Yes. In a perfect world. In reality this might not be the case. And even if the reason is good enough the problem stay nevertheless. But why go to the trouble of creating a wrapper when it adds no value? Just like in Ruby in Java we can also provide mock/stub implementations for *any* class when writing unit tests. I definitely wouldn't say that it doesn't ""add value"" since it does exactly what is needed. Now the other concern you pointed out is the extra effort however you neglected the fact that my answer is available from plain Java without any extra dependencies. Why would I go through the extra effort of obtaining an extra dependency and worrying about its compatibility licence... stuff.. if I can simply do this. Would you wrap say the Apache Commons Email API? It already wraps the more complex Java Mail API and is very easy to use (merely instantiate `SimpleEmail` set a few properties and call `send`). Clearly no extra value in yet another layer of indirection over that. Regarding the extra effort of using a mocking library why not if it saves more than it costs? (I can guarantee the extra effort in writing mocks/stubs by hand is significantly larger for any realistic number of unit tests.) Oh yes.. and wrapping is not useful for testing. It also decreases coupling which is particularly important for external services. It is wrapper that is being used though the rest of the code. So if (read: when) the service provider changes something the only diff you have to make would hopefully have to be applied just to the wrapper. Depending on external services is the worst possible coupling I've seen so far. You do make a point here! But please note that Apache Commons Email API is just a library while Twitter API or say Adwords API are also external services. I would most certainly wrap them.  You can ""monkey-patch"" an API in Java. The Java language itself does not provide specific means to do it but the JVM and the standard libraries do. In Ruby developers can use the Mocha library for that. In Java you can use the JMockit library (which I created because of limitations in older mocking tools). Here is an example JMockit test equivalent to the test_should_calculate_value_of_unshipped_orders test available in Mocha documentation:  @Test public void shouldCalculateValueOfUnshippedOrders() { final Order anOrder = new Order(); final List<Order> orders = asList(anOrder new Order() new Order()); new NonStrictExpectations(Order.class) {{ Order.findAll(); result = orders; anOrder.getTotalCost(); result = 10; }}; assertEquals(30 Order.unshippedValue()); }  Mockito is more handy and like your ruby mocks.  Using interfaces is just generally good practice in Java. Some languages have multiple inheritance others have duck typing Java has interfaces. It's a key feature of the language it lets me use different aspects of a class in different contexts and different implementations of the same contract without changing client code. So interfaces are a concept you should embrace in general and then you would reap the benefits in situations like this where you could substitute your services by mock objects. One of the most important books about Java best practices is Effective Java by Joshua Bloch. I would highly suggest you to read it. In this context the most important part is Item 52: Refer to objects by their interfaces. Quote: More generally you should favor the use of interfaces rather than classes to refer to objects. If appropriate interface types exist then parameters return values variables and fields should all be declared using interface types. The only time you really need to refer to an object’s class is when you’re creating it with a constructor. And if you take things even further (e.g. when using dependency injection) you aren't even calling the constructor. One of the key problems of switching languages is that you have to switch the way of thinking too. You can't program language x effectively while thinking in language y. You can't program C effectively without using pointers Ruby not without duck typing and Java not without Interfaces. Normally I don't make my service classes implement a separate interface simply because of the fact that said interfaces tend to never get a second implementation. (I don't count ""mock"" implementations because that is only *one* particular implementation strategy for mocking - and a poor one.) To me separate interfaces are mainly meant to support the plugability of alternate implementations (real ones in production code that is) for stable abstractions. Would the downvoter care to explain? Sure (sorry for the delay)! I don't think that ""Item 52"" in Effective Java says that developers should *create* separate Java interfaces for every class as you seem to imply. Note the part which says ""If appropriate interface types exist ..."". When they *don't* exist it's perfectly fine to use the implicit class interface. *Separate* interfaces should not be abused. No not for every class (obviously not for entities and other value holders also not for helper classes) but I do suggest that all service classes should be backed by interfaces (because you don't want tight coupling in services and you want the possibility to pass in mocks or proxies and frankly: because that's what interfaces were made for)."
980,A,"Parameterized jUnit test without changing runner Is there a clean way to run parameterized jUnit 4 tests without changing the runner i.e. without using @RunWith(Parameterized.class)? I have unit tests which require a special runner already and I can't replace this one with Parameterized. Maybe there is some kind of ""runner chaining"" so I could both runners at the same time? (Just a wild guess...) I have released a framework with a couple of runners that are able to enforce parameterization on the test-class while allowing you to chain an arbitrary 3rd-party runner for the actual test-execution. The framework is CallbackParams - (http://callbackparams.org) - and these are the runners: CallbackParamsRunner BddRunner By using the framework annotation ... @WrappedRunner ... you can specify an arbitrary 3rd-party runner in this manner: @RunWith(CallbackParamsRunner.class) // or @RunWith(BddRunner.class) @WrappedRunner(YourSpecialRunner.class) public class YourTest { ... Parameterized tests with CallbackParams differ considerably from the traditional approach to test-parameterization however. The reasons are explained in this tutorial article with BddRunner explained near the end of the tutorial article. For your first CallbackParams test you would probably prefer BddRunner since it requires less boiler-plate stuff but when you start reusing parameter values between different test-classes you are probably better off with CallbackParamsRunner which demands stronger type-checking. Also - with BddRunner you must not have any @Test-methods. Instead you must use the framework annotations @Given @When and @Then. That requirement sometimes clash with those of the third-party runner but it usually works out quite well. Good Luck!  org.junit.runners.Parameterized is created by org.junit.internal.builders.AnnotatedBuilder by reflect mechanism. Maybe you could extend Parameterized as your own Runner: @RunWith(MyParameterized.class). Thanks good idea! Well the special runner I need is not written by me and I have to use it as is. But I can apply your solution the other way around: I'll extend the special runner such that it has the same functionality as Parameterized. Let's see if JUnit's license allows some copy and paste. ;-)"
981,A,"Extending Junit4 or test case? We've got a simple webservice for handling queries about our data. I'd like to make a set of asserts/case extentions that would provide high level methods for testing various aspects of the response. For instance I could write assertResultCountMinimum(int). The method would take care of building the query executing it and unpacking the response to validate the data. I'd also like the to I want to make sure I have the right idea in my head about how to go about this. First create a test case class of my own with the right setup and teardown methods. For our purposes MyTestCase. Then provide a series of classes that extend Assert with the new assert methods. The end user of these classes would extend MyTestCase and would use the asserts that I've created. This is the pattern I think I see in jWebUnit. I feel like I'm mixing and matching junit 3 and 4 concepts. I'd love to have just junit 4 concepts. But I can't seem to line up in my head the proper way to build this. Also the assert methods that belong to Junit's Assert class are all static. Some of my asserts would require requerying the webservice. This makes me think I should really just provide the asserts as a series of helper functions inside of MytestCase. The later gets the job done but doesn't feel right. Any insight musings requests for clarification much appreciated. Follow up edit: As Jeanne suggests below I'm creating a super class with all of my asserts & setup/teardown methods. In reality my asserts are actually helper functions which wrap around the basic junit 4 asserts which I import into my super class. Any test of mine will just extend this super class. One caveat that I'm considering is making the super class abstract since there shouldn't be any instance of the super class. I end up doing roughly the same thing. If there's a logic to JUnit4's making the asserts into static calls I'm not seeing it. :( Marc I use two patterns in JUnit 4. For ""utility type"" assertions I made a static class. For example ReflectionAssertions. Then I use a static import to use those assertions in my JUnit 4 test. For local type assertions that are only used in one class I make them regular methods in the JUnit 4 test class itself. For example assertCallingMyBusinessMethodWithNullBlowsUp(). These don't have much reuse value. I don't consider this mixing concepts because the later group aren't reusable outside my test. If I had reusable assertions that made webservice calls (and therefore needed state) I would create a superclass that did not extend TestCase and use that. My superclass would have the state and @Before methods for setup. As such it is part of the test. Jeanne Sounds like I'm in the second case of local type assertions. It also sounds like I am using the same pattern you. Thanks for the reply."
982,A,"Java Junit - Capture the standard input / Output for use in a unit test I'm writing integration tests using JUnit to automate the testing of a console based application. The application is homework but this part isn't the homework. I want to automate these tests to be more productive -- I don't want to have to go back and retest already tested parts of the application. (Standard reasons to use Unit tests) Anyway I can't figure out or find an article on capturing the output so that I can do assertEquals on it nor providing automated input. I don't care if the output/input goes to the console/output pane. I only need to have the test execute and verify the the output is what is expected given the input. Anyone have an article or code to help out with this. @dfa I don't agree. It's similar indeed but different enough. ...granted the answer is the same... The other thread now has a better answer. It involves the use of the jUnit StandardOutputStreamLog System Rule. There are also System Rules for stderr and stdin. Here is the solution in place of ByteArrayOutputStream. It does not add anything to the idea of System.setOut. Rather I want to share the implementation that is better than capturing everything into ByteArrayOutputStream. I prefer to capture only selected information and let all log messages to appear in the console as they are logged rather than capturing everything into a balckbox (of which size?) for later processing. /** * Once started std output is redirected to this thread. * Thread redirects all data to the former system.out and * captures some strings.*/ static abstract class OutputCaputre extends Thread { // overrdie these methods for System.err PrintStream getDownstream() { return System.out;} void restoreDownstream() { System.setOut(downstream);} // will be called for every line in the log protected abstract void userFilter(String line); final PrintStream downstream; public final PipedInputStream pis; private final PipedOutputStream pos; OutputCaputre() throws IOException { downstream = getDownstream(); pos = new PipedOutputStream(); pis = new PipedInputStream(pos); System.setOut(new PrintStream(pos)); start(); } public void run() { try { BufferedReader br = new BufferedReader(new InputStreamReader(pis)); // once output is resotred we must terminate while (true) { String line = br.readLine(); if (line == null) { return; } downstream.println(line); userFilter(line); } } catch (IOException e) { e.printStackTrace(); } } public void terminate() throws InterruptedException IOException { restoreDownstream(); // switch back to std pos.close(); // there will be no more data - signal that join(); // and wait until capture completes } }; Here is an example of using the class: OutputCaputre outputCapture = new OutputCaputre() { protected void userFilter(String line) { downstream.println(""Capture: "" + line); } }; System.out.println(""do you see me captured?""); // here is your test outputCapture.terminate(); // finally stop capturing  Use System.setOut() (and System.setErr()) to redirect the output to an arbitrary printstream - which can be one that you read from programmatically. For example: final ByteArrayOutputStream myOut = new ByteArrayOutputStream(); System.setOut(new PrintStream(myOut)); // test stuff here... final String standardOutput = myOut.toString(); So simply going `PrintStream _out = System.out;` Won't work? It would - i.e. you'd have a reference to the existing output stream - but you can't *read* anything from it as there are no appropriate methods to do so on the general `PrintStream` interface. The technique involves setting the output to a specific printstream you know how to read from.  The System class has methods setIn() setOut() and setErr() that allow you to set the standard input output and error streams e.g. to a ByteArrayOutputStream that you can inspect at will."
983,A,"Longest running unit tests? How can we find the junit tests in our suite that take the longest amount of time to run? The default output of the junitreport ant task is helpful but our suite has thousands of tests organized into many smaller suites so it gets tedious and the worst offenders are always changing. We use luntbuild but ideally it would be something we could just run from ant. Just out of interest why do you want to know? One of my most useful jUnit tests takes a long time - not because the code is inefficient but because it tests with a very exhaustive set of conditions (three for() loops deep) We have a lot of poorly written tests. Some do a lot of unnecessary set-up. Some tests use Thread.sleep(). A few really need a long time to run. Our suite takes a half hour to run and I'm looking for the easy targets to speed it up. JUnitReport works on the xml files produced by the JUnit task. You could write a task that would read the test durations out of the same xml files (TEST-*.xml). But you can also take a shortcut and just read the summary file created by JUnitReport (TESTS-TestSuites.xml) which has all the information in the single file. A quick way to do this is to use a bit of xsl to just show the slowest tests: <xsl:output method=""text""/> <xsl:template match=""/""> <xsl:text> </xsl:text> <xsl:for-each select=""testsuites/testsuite""> <xsl:sort select=""@time"" data-type=""number"" order=""descending"" /> <xsl:value-of select=""@name""/> : <xsl:value-of select=""@time""/> <xsl:text> </xsl:text> </xsl:for-each> </xsl:template> To run from Ant you do this: <target name=""show.slow.tests""> <xslt in=""target/tests-results/TESTS-TestSuites.xml"" out=""target/slow.txt"" style=""slow.xsl""/> </target> Then you can just look at the first X lines to find the X slowest tests: jfredrick$ head target/slow.txt ForcingBuildShouldNotLockProjectInQueuedStateTest : 11.581 CruiseControlControllerTest : 7.335 AntBuilderTest : 6.512 Maven2BuilderTest : 4.412 CompositeBuilderTest : 2.222 ModificationSetTest : 2.05 NantBuilderTest : 2.04 CruiseControlConfigTest : 1.747 ProjectTest : 1.743 BuildLoopMonitorTest : 0.913  Use TeamCity. They have great reports and version 4.0 even orders your tests so the most flaky tests run first.  If you launch your tests on your build server using cruise control it is one of the top level options to sort by run time."
984,A,"Where do I configure log4j in a JUnit test class? Looking at the last JUnit test case I wrote I called log4j's BasicConfigurator.configure() method inside the class constructor. That worked fine for running just that single class from Eclipse's ""run as JUnit test case"" command. But I realize it's incorrect: I'm pretty sure our main test suite runs all of these classes from one process and therefore log4j configuration should be happening higher up somewhere. But I still need to run a test case by itself some times in which case I want log4j configured. Where should I put the configuration call so that it gets run when the test case runs standalone but not when the test case is run as part of a larger suite? Here is some code which does that (just search for ""isConfigured"").  I use system properties in jog4j.xml: ... <param name=""File"" value=""${catalina.home}/logs/root.log""/> ... and start tests with: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.16</version> <configuration> <systemProperties> <property> <name>catalina.home</name> <value>${project.build.directory}</value> </property> </systemProperties> </configuration> </plugin>  The LogManager class determines which log4j config to use in a static block which runs when the class is loaded. There are three options intended for end-users: Do not configure log4j; use default configuration. Allow log4j to scan the classpath for a log4j config file during your test. Specify the path to the configuration file manually yourself and override the classpath search. You can specify the location of the configuration file directly by using the following argument to java: -Dlog4j.configuration=<path to properties file> in your test runner configuration. See also the online documentation. How will it know it's a ""log4j configuration file?"" What is the filename? (log4j.xml?) @Chad: I've edited my answer to address your question. Please see the link to the static block to see exactly how this is implemented. Please fix the grammar in this answer `The LogManager class implements determines` thats good idea @DavidWilliams fixed how embarrassing!  You may want to look into to Simple Logging Facade for Java (SLF4J). It is a facade that wraps around Log4j that doesn't require an initial setup call like Log4j. It is also fairly easy to switch out Log4j for Slf4j as the API differences are minimal.  I generally just put a log4j.xml file into src/test/resources and let log4j find it by itself: no code required the default log4j initialisation will pick it up. (I typically want to set my own loggers to 'DEBUG' anyway) For standard building testing I would set Log4j to warning or even error. If the tests succeeds(also negative tests) there should be no logging which gets the users attention."
985,A,Run all the junit tests that are in a given package in Netbeans? We have a bunch junit tests in our current project. Essentially we are looking for a way to run all the test in a given package. Currently in Netbeans I see you can run all the tests or a single test but no way to run a a sub-set of tests. Is this built into Netbeans? Or is there another way we can do this? In JUnit this is achieved through TestSuite. You can check for more information here (look at the code not at the pictures). At least in the Eclipse IDE there is a functionality that lets you add a new TestSuite select which tests it is to include and then have the IDE generate it for you. I haven't seen such thing in Netbeans but you should check for any additional JUnit plugins.  Just set up junit test groups like documented e.g. on this question and then run the test group via your ant or maven build.
986,A,"JUnit mocks which tool should i use? I come from the PHP testing world and i'm starting testing in Java. I've seen several tools to mock the SUT in JUnit like Mockito SevenMock ClassMock etc. I really appreciate any recommendation of which one should i use. Thanks in advance! Mockito seems to be most often used edit: Comparison between Mockito vs JMockit - why is Mockito voted better than JMockit? Easymock vs Mockito: Design vs Maintainability? http://www.dahliabock.com/blog/2009/08/21/mocking-mockito-vs-easymock-example/  I've used Mockito quite a lot. http://mockito.org/ I have not used EasyMock so cant say much about it. Using Mockito is straightforward but the test classes that you intend to test should also be in a testable decoupled state. Since with mockito your stubbing and injecting the dependencies you may have to change your production class to be in this state - where the dependencies are injected. In mockito the good thing is stubbing with 'if-when-then' if your production class is dependent on an object and you wish to mock this final ClassToMock mockObject = mock(ClassToMock.class); when(mockObject.getTestMethod()).thenReturn(""Test""); Now this mockObject can now be injected when initializing your intended class. final ClassToTest test = new ClassToTest(mockObject); Mockito uses reflection to create these mock objects. However if you have a dependency and if the intended object is declared final then mocking will fail. When you starting using Mockito you will see how it fits. Another useful method in Mockito is verify where you can verify certain operations in your mock objects. Have a peep at mockito. However there are limitations in mock objects in some cases it will be hard to create mock objects perhaps external/third party code.  I've been using JMock for a while. I personally like that the resulting code is easy to read and you can easily differentiate between allowances and expectations: context.checking(new Expectations() {{ // allowances ignoring(parserState); allowing(factory).create(); will(returnValue(object)); // expectations one(service).addSth(with(any(Integer.class)) with(sth)); }}); Other powerful features are: Sequences: invocations in sequence. Argument matchers: hamcrest library States: constraint invocations when a condition is true."
987,A,"Which UnitTest framework to learn for Java now? Although I have programmed with Java for roughly 3 years + now (not day-to-day but at least I understand the fundamentals) haven't really come into the field of Unit Testing... My work is now more Testing / Problem Analysis oriented so I reckon a good Java Unit Testing Framework will be quite helpful to this role. Obviously there is no formal rule of which framework to go with in my Team. So just wondering as a beginner which framework is a good one to get started quickly? Junit? TestNG? or something else? Edit: it seems JUnit has more printed books as references compared to TestNG on Amazon. But I don't know their popularity / trend in Java Industry as for now. We have ""Next Generation Testing"" book for TestNg which is quite good. I would go for JUnit. Why?........... The newer versions of JUnit are rather feature rich. Almost everything that's available in TestNG is doable. So choosing a library is down to personal preference. Hi @Robert Harvey Because JUnit is is popular among developers and it is already used in several projects. When choosing a methodology we should consider which one is good for the entire team not just one person. In that context I think it is better to go for JUnit.  Junit is famous and best unit test tools I think you and me both know Junit is so important so that Java developer changes their coding pattern as junit contribution You can learn some Junit as basic unit test technology as well as learn testng as second unit test technology. The following link maybe be useful for you! http://www.javarmi.com/2010/08/junit-4-tutorial-and-example-get-starting/  Junit is the best place to start. Junit 4.x has many rich features like annotations etc as well its get good support from popular frameworks like Spring.  I would learn TestNG with Unitils. Covered all my needs all the time. Add XMLUnit and DBUnit and you should be settled for a long time. I looked at the page. Seems pretty cool. Unitils works with jUnit too +1 for mentioning Unitils. Interesting project I didn't know about before.  Both do their jobs equally well so there is no right or wrong. You can use both in combination with numerous frameworks to help support your development. If you need to work for a client you can learn the one that you are expected to use. If not the best idea is to try them both out and find out what works best for you.  I've used both and found that for unit testing only jUnit works very well but in terms of System testing I would recommend TestNG. TestNG offers a few extra features (like parametric testing) and is highly flexible. For example in jUnit one test failing in a suite usually means you have to re-run your entire suite but in TestNG you can simply rerun the test that failed. Heres a really good article outlining both (its a bit biased towards TestNG though) http://www.ibm.com/developerworks/java/library/j-cq08296/index.html all you said is also true in jUnit I didn't think jUnit support parametric testing for objects  I'm the creator of TestNG and author of the ""Next Generation Testing"" book so here are a few (obviously biased) thoughts. For functional (or end to end / integration / system whatever term you prefer) testing TestNG offers a few features that JUnit doesn't have and that users have found very useful: Groups. Once you have compiled your tests you can just ask TestNG to run all the ""front-end"" tests or ""fast"" ""slow"" ""database"" etc... Support for multithreaded testing. This has two different meanings: 1) You can tell TestNG to run all your tests concurrently in thread pools that you define (one line change in an XML file). This is very configurable so that tests that are not multithreaded can still be run in single threads. 2) You can tell TestNG to invoke your test methods n times from p threads. This gives you a good idea of how thread safe the code you are testing is. Test dependencies and deterministic ordering. This is very useful for integration testing where it's very common to see test methods that need prior test methods to succeed before they can even run. TestNG is very popular in the Selenium community for this specific reason (""don't bother testing this page of my web site if the login test failed""). Another immediate advantage of test dependencies is that TestNG can tell you ""1 method failed 99 methods skipped"" which is a much more accurate result than ""100 methods failed"". TestNG has many many more features and I'm happy to answer questions here or on our mailing-list (http://groups.google.com/group/testng-users ). +1 for making my life easier in the past several years. :) You didn't mention data providers which encourage and facilitate running the same test with different values. Late to the party here but I also want to say the TestNG documentation is generally better for a ""beginner"".  To be perfectly honest I JUnit is way more popular than TestNG at least here where I work and live. I know this might be a strange argument but if I were you I'd scan through some job adds and see which framework does the industry in your area/country favour (for example I'm going for JUnit because of that I mean sure I could learn TestNG it might turn out to be better but what good would it do to me if all the employers require JUnit knowledge?).  JUnit is very well supported in many IDE's including Eclipse and JUnit 4 with annotations is quite nice to work with. Unless you have specific needs then go for tool support i.e. jUnit. TestNG is also supported by all the major IDEs. As well as jUnit? I think the TestNG plugin works just as well as the JUnit plugin. I haven't noticed any problems with it.  TestNG with Unitils is really great. It covers almost all the requirements. Unitils support lot of assertion utils Spring Hibernate mocking frameworks dbutils etc.  Although it may seem to be a petty criterion all things being equals it looks like job trends heavily favour JUnit. http://www.indeed.com/jobtrends?q=TestNG%2C+JUnit&l= oh quite a huge gap as in Australia... never knew this :)  To me it is just a matter of taste. I've personnaly used jUnit daily for more than 5 years now and I'm very happy with it especially with the last version and its @Rule feature that prevents my team from writting the same code over and over. It integrates well with all the tools I use daily : IDE build continuous intégration..."
988,A,"How do You Unit Test a Custom Assert? I'm writing my own JUnit Assert? How do I test it? I know how to feed it something that will pass and something that will make it fail but how do I write a JUnit test for those things? The custom assert will look something like: public static void assertSomething() { if (!something()) { fail(""Expected something but ...""); } } How can I catch that fail? fail() throws a junit.framework.AssertionFailedError which you could catch in a unit test of your assertion method if you like. Example: @Test(expected = AssertionFailedError.class) public void testMyAssertFails() { assertSomething(""valueThatWillFail""); } @Test public void testMyAssertPasses() { assertSomething(""valueThatPasses""); //if you reach this line no failure was thrown }"
989,A,"assert vs. JUnit Assertions Today I saw a JUnit test case with a java assertion instead of the JUnit assertions - What are the best practices in this respect? This may not apply if you exclusively use stuff that's shiny and new but assert was not introduced into Java until 1.4SE. Therefore if you must work in an environment with older technology you may lean towards JUnit for compatibility reasons.  I prefer JUnit assertions as they offer a richer API than the built-in assert statement and more importantly do not need to be explicitly enabled unlike assert which requires the -ea JVM argument.  When a test fails you get more infomation. assertEquals(1 2); results in java.lang.AssertionError: expected:<1> but was:<2> vs assert(1 == 2); results in java.lang.AssertionError you can get even more info if you add the message argument to assertEquals  In JUnit4 the exception (actually Error) thrown by a JUnit assert is is the same the error thrown in by the java assert keyword (AssertionError) so it is exactly the same as assertTrue and other than the stack trace you couldn't tell the difference. That being said asserts have to run with a special flag in the JVM causing many test to appear to pass just because someone forgot to configure the system with that flag when the JUnit tests were run - not good. In general because of this I would argue that using the JUnit assertTrue is the better practice because it guarantees the test is run ensures consistency (you sometimes use assertThat or other asserts that are not a java keyword) and if the behavior of JUnit asserts should change in the future (such as hooking into some kind of filter or other future JUnit feature) your code will be able to leverage that. The real purpose of the assert keyword in java is to be able to turn it off without runtime penalty. That doesn't apply to unit tests.  I'd say use JUnit asserts in test cases and use java's assert in the code. In other words real code shall never have JUnit dependencies as obvious and if it's a test it should use the JUnit variations of it never the assert.  I would say if you are using JUnit you should use the JUnit assertions. assertTrue() is basically the same as assert Otherwise why even use JUnit? You would use JUnit for the test running framework. Asserts are really a small part of the value JUnit gives you. JUnit without `Assert` would require more boilerplate. `assert` without JUnit would require you to write a whole framework. I was more saying if you're going to use the tool USE THE TOOL. regular old assert statements seem a tad silly in JUnit test cases. They belong in with the actual code in my opinion. @CheesePls ""regular old assert statements"" are in fact more recent than JUnit asserts.  when Java didn't have assert people bitched and moaned and nagged Sun into adding it. once Java had it nobody uses it. Very true. I think it is because they are disabled by default. Yep. I use the same Assert class that I used before assert was added. Why? No one can turn off my checks. When Java didn't have assert people bitched and moaned and nagged Sun into adding it. Once Java had it nobody uses it as they fucked it up making them optional with -ea."
990,A,"How to catch AssertPathsEqual and print error to console? How could I catch this AssertionFailedError and print to the console an message that says: ""Expected text value '55555' but was '55556' at /xpathResult[1]/result[2]/field[1]/field[1]/field[6]/text()[1] for [testFileName] and [testName]"" I'm not exactly sure how to catch the values out from the assertion without having to ""hack"" and manually parse the diff output string. I would rather have an elegant method rather than a ""hack"" method. junit.framework.AssertionFailedError: org.custommonkey.xmlunit.Diff [different] Expected text value '55555' but was '55556' - comparing <field ...>55555</field> at /xpathResult[1]/result[2]/field[1]/field[1]/field[6]/text()[1] to <field ...>55556</field> at /xpathResult[1]/result[2]/field[1]/field[1]/field[6]/text()[1] at junit.framework.Assert.fail(Assert.java:47) at org.custommonkey.xmlunit.XMLAssert.assertXMLEqual(XMLAssert.java:125) at org.custommonkey.xmlunit.XMLAssert.assertXMLEqual(XMLAssert.java:113) at org.custommonkey.xmlunit.XMLAssert.assertXpathEquality(XMLAssert.java:582) at org.custommonkey.xmlunit.XMLAssert.assertXpathsEqual(XMLAssert.java:453) at org.custommonkey.xmlunit.XMLAssert.assertXpathsEqual(XMLAssert.java:435) at org.custommonkey.xmlunit.XMLTestCase.assertXpathsEqual(XMLTestCase.java:454) at tst.PatrolTests.compareXMLEqualityToLastTest(PatrolTests.java:353) at tst.PatrolTests.doPlate(PatrolTests.java:141) at tst.PatrolTests.testPlate(PatrolTests.java:117) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at junit.framework.TestCase.runTest(TestCase.java:168) at junit.framework.TestCase.runBare(TestCase.java:134) at junit.framework.TestResult$1.protect(TestResult.java:110) at junit.framework.TestResult.runProtected(TestResult.java:128) at junit.framework.TestResult.run(TestResult.java:113) at junit.framework.TestCase.run(TestCase.java:124) at junit.framework.TestSuite.runTest(TestSuite.java:232) at junit.framework.TestSuite.run(TestSuite.java:227) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:79) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Am I going to have to write my own DiffDescriber class that parses the Diff text output? You can catch the AssertionError to get the detailed error message to the console. You can try something like this :  try{ // say you are comparing FileReader objects that // refer to XML documents assertXMLEqual(filereader1 filereader2); } catch (final SAXException e) { // show faiulre message } catch (final IOException e) { // show failure message } catch (final AssertionError e) { System.err.println(e.toString()); } But I think rather than catching AssertionError it is better to get more details about assertion failure by making use of the class ""Diff"" (from org.custommonkey.xmlunit). You can use this class to check if XML documents being compared are similar (or identical) and to get more details about the difference. You can also use the DetailedDiff class to get a detailed diff. Check Example4 and Example5 from the XMLUnit documentation. These examples show the usage of Diff and DetailedDiff. Thanks for the reference to examples. What I need here though is a method of getting Diff properties. When there is a diff I want to know where in the file the diff was detected and what kind of difference it was. With those examples all I would be able to do is parse the ugly diff output string in order to get those property values. Is there no other way? As I understand what you want is a way to get the context of the diff to detect where the XML files differ. I tried a bit in that direction but couldn't find a way. One approach would be to get the XML nodes which differ by using the method ""getAllDifferences"" in the DetailedDiff class and then doing a comparison of them.Say:Diff d = new Diff(file1 file2); DetailedDiff nd = new DetailedDiff(d); for(Object o:nd.getAllDifferences()) { Difference f = (Difference) o; Node c = f.getControlNodeDetail().getNode(); Node t = f.getTestNodeDetail().getNode(); .."
991,A,"How do I make Eclipse recognize JUnit tests when creating a suite? When I use Eclipse to create a JUnit test suite it does not detect any existing tests and warns ""No test classes selected."" I started from the test class package (test/com/.../package) and the package is selected. There are several JUnit tests there also created through the same version of Eclipse but there is no way to select them. I am using JUnit 4. Thanks! The wizard for creating a suite currently only works with JUnit 3 tests see corresponding bugzilla entry. How long has JUnit 4 been around? Well that explains it thanks!  A Suite that works for me is : import org.junit.runner.RunWith; import org.junit.runners.Suite; import org.junit.runners.Suite.SuiteClasses; ... @RunWith(Suite.class) @SuiteClasses( { MyTest.class }) public class SeleniumSuite { ... } This helps if you want to run just a subset of tests defined in a package. What are you tests called? Try re-factoring them so they are called either Test*.java or *Test.java. They're the normal *Test.java that Eclipse creates.  In Eclipse you can simply right-click on the project / package you want to run tests in and select Run as > Junit Test - you can avoid needing to programmatically create a test suite class completely. Well if you never want to automate them which is the whole point.... Huh? I'm referring to creating a `TestSuite` class which programmatically lists which tests to include. Eclipse can execute all your tests in a project without one and so can Ant/Maven etc. Can you please explain how you are trying to invoke the tests from Eclipse? No `run as` executes classes/applications/etc. My point is that in Eclipse you don't need to bother with the `TestSuite` creation - Eclipse's test runner can run a series of unit tests without one (just by being told through the GUI which to run). The point of creating the suite is so that the tests can be automated. If you have to sit there and select ""run as"" then you're not a nightly/hourly batch process you're a person. Right but typically you don't automate your tests by launching them in Eclipse. You do so with Ant or Maven both of which have features which can handle running all tests in a given folder/package/etc negating the need to manually create a TestSuite. Eclipse has the same type of feature which was my only point. Maybe I misunderstood - ""run as"" creates source code? I'll give it a try. This is the most practical answer. Thanks."
992,A,"Mocking grails method that uses a findAll Generating a MissingMethodException def retrieveEatenFood(String token String addedDate) { def consumer = Consumer.findByMobileToken(token) if(consumer != null) { def efList = [] def list = consumer.findAll(""from EatenFood as ef where date(ef.dateAdded) = date(:da)""[da:sdf_long.parse(addedDate)]) list.each{ def eatenList = [:] eatenList.put(""foodType""it.food.name) eatenList.put(""sequenceNumber""it.sequenceNumber) eatenList.put(""eatenDate"" it.eatenDate) eatenList.put(""DateAdded""it.dateAdded) efList.add(eatenList); } return efList; } } Trying to mock the above method but the findAll keep generating an exception. This issue it works! Now i need to write the test for it and i keep getting this exception. Can anyone help me please! groovy.lang.MissingMethodException: No signature of method: carrotdev.Consumer.findAll() is applicable for argument types: (java.lang.String java.util.LinkedHashMap) values: [from EatenFood as ef where date(ef.dateAdded) = date(:da) [da:Sun Feb 13 01:51:47 AST 2011]] Possible solutions: findAll(groovy.lang.Closure) find(groovy.lang.Closure) at carrotdev.ConsumerService.retrieveEatenFood(ConsumerService.groovy:146) at carrotdev.ConsumerService$retrieveEatenFood.call(Unknown Source) at carrotdev.ConsumerServiceTests.testEatenFoodRetrievedSucessfully(ConsumerServiceTests.groovy:359) I would move the query to the Consumer domain class with a descriptive name e.g. static List<EatenFood> findAllEatenByDate(String date) { consumer.findAll( ""from EatenFood as ef where date(ef.dateAdded) = date(:da)"" [da:sdf_long.parse(addedDate)]) } Then the call is simply def list = Consumer.findAllEatenByDate(addedDate) and you can mock that easily with def foods = [new EatenFood(...) new EatenFood(...) ...] Consumer.metaClass.static.findAllEatenByDate = { String date - > foods } Be sure to test the finder method in your Consumer integration test. thanks a million! Man that ""metaClass.static"" is a savior! Thanks!"
993,A,"JUnit tool classes package As I'm currently adding more unit test to our code I'm stuck because of a simple problem. I need to write some boilerplate code to make it easy to write test classes. For example to connect to the database without the configuration of the Resin server using JNDI injection. My problem is: where can I put boilerplate code that is only related to tests? Of course it needs to be in my test directory but is it good practice to create a package call it ""tool"" in my package tree as this package will have no equivalent in the package tree of the project source? Why or why not? Edit: Let's take an example: I need to create a class to inject a database connection into my test classes. Let's call it JNDIDatabaseInjector. This class will obviously only be used for test purposes as the database connection used in production is defined in the application server configuration. In which package should I best put my JNDIDatabaseInjector class? I put all of my for-release packages in a src Source Folder (in Eclipse). All of the junits go in the test-src Source Folder. This makes it simple to separate the test code and release only the packages that are to be released. So it might look like this: MyProject - src + com.mycompany.tool - test-src - com.mycompany.tool TestBed.java As you are setting up you might want to look into mockito to create your boilerplates. Ok I will look in it. Thank you! My question is probably unclear. I will try to reformulate... @Traroth please try a Google search for ""mockito database injector."" Or try http://stackoverflow.com/questions/22697/whats-the-best-mock-framework-for-java . We use mockito to inject db connections at work. Would that address your question?  This is the structure I would recommend for maven based projects: . |-- src/ | |-- main/ | | `-- java/ | | `-- com/ | | `-- example/ | | |-- bar/ | | | `-- Bar.java | | `-- foo/ | | `-- Foo.java | `-- test/ | |-- java/ | | `-- com/ | | `-- example/ | | |-- bar/ | | | `-- BarTest.java | | |-- foo/ | | | `-- FooTest.java | | |-- util/ | | | `-- Util1.java | | |-- AbstractTest.java | | `-- Util2.java | `-- resources/ | `-- com/ | `-- example/ | |-- bar/ | | `-- BarTest-context.xml | `-- foo/ | `-- FooTest-context.xml `-- pom.xml As you can see I put Util1.java in util subdirectory but IMHO first common parent package (com.example in this case) is a better choice. I would also advise you creating base abstract AbstractTest class along with utilities to share common testing boilerplate. Also note where I put testing resources (Spring context configuration in this example). This way you can access some test data (XML SQL etc.) easily just by issuing: getClass().getResource(""someFile.txt"") If you are in com.example.foo.FooTest the statement above will load file from src/test/resources/com/example/foo/someFile.txt automatically. This directory layout prevents from sibling dependencies between packages (only child -> parent or parent -> child). Don't know why but somehow it feels better. The real problem comes when you want to share testing utilities between maven artifacts - one of the biggest flaws of maven's convention-over-configuration approach. Nice. That's exactly what I looked for. Thank you!"
994,A,"JUnit - what are its strengths and weaknesses I'm wondering what are the strengths and weaknesses of using JUnit for code testing? For example I don't believe it is good for testing concurrency. Any input would be appreciated. Thanks Testing concurrent code is hard no matter what you choose. TestNG has a mechanism for creating multiple threads and running them in a concurrency test. I believe it'd be possible to create a bunch of FutureTasks in a JUnit test to exercise a method. It's just not supported in JUnit 4. The strengths? Here are a few: Separates test code from production code. Makes it possible to run a suite of tests for an application all at once. Support in Ant Maven and IDEs like IntelliJ and Eclipse. Has become part of the thinking in Test Driven Development and agile techniques. Excellent. thanks very much for that duffmo. May I ask if there are some weaknesses with JUnit? Thanks very much again. It's not TestNG == ""Next Generation"". I think they have some catching up to do. I disagree that JUnit 4 should try to support every feature in TestNG in order to ""catch up"". JUnit has extension points like Runners and Rules that should allow third-party developers provide extra functionality. That allows the core functionality to be simpler and easier for beginners to learn.  Just a note for people searching concurrency - there is a unit test framework designed to test concurrency (something-Unit). A Google search is not finding it for me but I know it exists read about it recently.  JUnit has two strengths: It works well enough. Moreover like Perl it makes the easy things easy and the hard things possible. We have test runners that fit into the JUnit framework but actually do static analysis using the ASM bytecode library. It's ubiquitous. Everyone knows it every tool supports it you can find most answers on the web. Testing concurrent code is possible in JUnit but it doesn't provide any particular support for it. You have to manage your threads and arrange interleaving scenarios yourself. That said I believe there are tools out there and if you adopt JUnit you'll likely find that is the expected scenario from test helpers like hamcrest matchers."
995,A,GWTTestcase accessing javascript object defined in an external javascript file fails I have defined a GWT module that includes an external javascript file using tag. I have written a GWTTestCase that returns the above described module's name. When my testcase accesses a javascript object I see the following exception Caused by: com.google.gwt.core.client.JavaScriptException: (null): null Any idea on how to fix this? Am I right in assuming that the scripts included in the gwt module definition file will be available when executing the GWTTestCase? When I invoke the page after running the module in hosted mode using firebug I can see that all the external javascript files are included properly and the javascript object defined in them are available I have fixed it myself. Apparently when accessing such objects it should be referenced using $wnd variable. Example: test.js defined object test. In order to access it from GWT one should use $wnd.test Hope this answers saves somebody else' time.
996,A,"spring - @ContextConfiguration fail to load config file in src/test/resources I've tried to load the spring config file in src/test/resources classpath with the following abstract class: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={""classpath:/applicationContext.xml""}) public class BaseIntegrationTests { } I have the applicationContext.xml file in src/test/resources but spring cant load it. Thank you. Please please use *code* blocks to set aside pieces of code. I think you don't need `/` just use `classpath:applicationContext.xml` This suit my needs: http://stackoverflow.com/questions/10385452/location-of-spring-context-xml I think i have a simillar problem I found out that my application-context.xml was not on target/test-classes/ neighter on src/test/resources  Try by using * so that it can search to your classpath @ContextConfiguration(locations={""classpath*:applicationContext.xml""})  your application context must be included in classpath and put * : @ContextConfiguration(locations = { ""classpath:*/application-context.xml"" })  You seem to be using maven and trying to run the tests from within eclipse. Check the buil folder (target/test-classes/) for applicationContext.xml. If it is not there you'd have to build first.  There is a reported bug with using the spring-test dependency (includes SpringJUnit4ClassRunner) with versions of JUnit > 4.4. If you are using a version of JUnit newer than 4.4 trying moving that down to 4.4 and see if it solves your problem.  To be precise it's the content of the test output directory (target/test-classes) that is on the class path not src/test/resources. But resources under src/test/resources are copied to the test output directory by the resources:testResources goal (which is bound by default to the process-test-resources phase). Having that said your code looks fine and resources for the test source code should have been copied either by your IDE or by Maven when running tests and should thus be available on the class path. So there must be something else wrong. I can see that your class is a base class for integration tests. Did you configure anything fancy in your pom? Can you show it? process-test-resources helped. Fixed the pom. Thank you! my resource is on the test-classes directory and i can load the file with `Myclass.class.getClassLoader.getResource(""/my/path"")`. but doesn't work with `@ContextConfiguration(location = ""classpath:/my/path"")`"
997,A,Junit report as PDF preferably with Maven Hey all Looking for feedback if anyone has a good way to when you finalize the code run one last Junit run and create a PDF-version of the junit report. You can use http://junitpdfreport.sourceforge.net Unfortunately maven is not supported. You have to configure it with AntRun plugin.  Maven PDF plugin with Surefire report: http://maven.apache.org/plugins/maven-pdf-plugin/surefire-report.html I'm trying this out but now running into the infamous 'surefire tests that fail during site/report generation but work fine outside' problem.
998,A,"Code generating JUnit based on Abstract Syntax tree walk Assuming I have the following class and method: package generation; class HelloWorld { public boolean isEven(int val) { if ( (val % 2) == 0) return true; else return false; } } Assume I want to generated the following JUnit test: package generation; import static org.junit.Assert.assertFalse; import static org.junit.Assert.assertTrue; import org.junit.Test; public class HelloWorldTest { @Test public void testIsEven() { HelloWorld h = new HelloWorld(); assertTrue(h.isEven(2)); assertFalse(h.isEven(1)); } } Given the following method of tree walking a Java Syntax Tree: How can I use the java Eclipse Abtract Syntax Tree in a project outside Eclipse? (ie not an eclipse plugin) How would you code generate the Unit test case given the class example up the top? I hope you realise the folly of generating the test from the code?! What about nested branches or expressions in your `if` statement which do not use literals? How should the JUnit be created? @davmac - I assume you're saying that TDD is the 'correct' approach. I contend that this would be useful in a legacy code environment. @Ira - Thanks - have dropped that in. I'm interested in the simplest thing that could possibly work. I'm aware the complexity of this could explode quite quickly - so just want to see the simplest case. Perhaps I misunderstood but it seems this isn't really a question it's an interesting idea and you'd like someone else to implement it for you. @hawkeye: I think you mean ""val % 2"" not ""a % 2"" @hawkeye I'm saying that generating a test from existing code will only test that the code does what it does. Not that it does what it is supposed to do. @hawkeye to be clear: generated tests will test implementation details as opposed to conformance to the specification or interface. If there are bugs in the implementation a test might be generated which passes but for which a correct implementation of the tested code a test failure would occur. @davmac - I hear you on that. I'm suggesting that the primary risk for legacy code broken or not - is whether it ceases its existing behaviour or not (not whether it is or is not correct because once it has lasted in production for a few years - making changes to it is actually the greater risk.) People work around broken code because the cost of change can be so high. @hawkeye - I really am not seeing it. There is no point having such tests which only ""test"" code that will never be changed. I.e. if you can't afford to change the behaviour then you mustn't change the code; and in that case the test is pointless. @davmac I was making a distinction between ""using junit tests to test against changes you've intentionally introducing to the code under the tests"" and ""using junit to protect against tangential changes not intended to change the code under the test but end up doing so anyway"" - ie insurance for legacy code. (Imagine a codebase of 10M + lines and 70+ coders with 10+ projects running concurrently in a very conservative company.) You need a lot more than just a parse tree. (This mistake is repeated by practically everybody that hasn't built a serious program analysis tool; you need a lot more machinery to do anything really interesting. It is why compilers aren't trivial). The ""simplest"" case requires parsing the code of the method to be tested into ASTs name/type resolving everything so you know the meaning of all the symbols (you have to know that val in an integer) and determining the control flows through the code and the predicates that control them. With that information you can essentially enumerate valid control-flow paths picking up information about the predicates along the path for each one forming in essence a conjunction of all the conditions along that path. (In your example if .. val%2 ... return true; is one path controlled by val%2==true). You get to worry about modelling how side effects in the path affect the various predicates. And you'd like to range information on integers (and sizes of strings and arrays etc.). Then for each path you need to generate a set of input arguments that makes the path predicate true; given that this predicate could be pretty complicated you'll likely need some kind of SAT solver. With solutuions to the path predicate you now need to generate ASTs corresponding to to tests (e.g. set up variables to enable the method arguments to satisy the predicate; for simple integer equations you can likely just generate expressions for the arguments as in your example). Finally assemble the test calls into an AST for a method insert into an AST representing a unit test case method and prettyprint the result. Well that wasn't so hard :-} Our DMS Software Reengineering Toolkit has a Java front end that will parse Java produce ASTs enumerate control flow paths through a method [this isn't so easy: consider exceptions] compute range constraints on integer variables and give you the general ability to climb around the ASTs to extract/construct what you want. Doesn't include a SAT solver yet but we've thought about it. that sounds pretty awesome. Let me know if you've got any positions open :)  You might want to skip the awkward hardly well-defined part of your questions and look into property based testing. I believe Agitar had a tool that did that. It just looked at method signatures and fired in edge case values. I'm interested in the simplest case of code analysis->code generation."
999,A,"How to distribute junit tests to another machine The build.xml we have today have many targets to compile and run unit tests. The build.xml refers to many property files with relative to itself. All this works fine when build and test is done on same machine. It is pretty simple to do build get all jars and any test input files to a build_home(another machine). How to run junit on the new location? Should I create small build.xml files to running the tests? (There is no way to create ant build.xml dynamically) Any solutions? ( GridGain is possible solution. Not tried yet. ) Edit: Mode details on why this more complicated: The source code is around 3G doing clearcase update and build takes considerable time (40 minute) against real test time of junit testing - 60 minutes. We have many machines to run the tests -- loading Clearcase on all systems not possible. Try Cruise Control. It's a great way to offload your build and unit test to another machine. Thanks. How does cruise control run the tests? we use hudson so I could use similar technique. We do that. It works fine. The difference is doing the build on hudson running the tests on another. The way I see now is to have the junit run code in another ant file say junit.xml copy over along with ""jars"" and ""testdata"". Run tests using junit.xml ( from JoseK ) Hudwon works equally well. Why not configure it to run your tests as part of your build?  I understand your question as you want to only run the Junit tests on another machine without actually building on it? You can run something on the lines below as build scripts from Cruise control and probably Hudson too If you're using the task via ant then follow the same principles as a standard build. You can check out the code to all target machines from source control. Externalize all root directories to a build.properties. These are properties which have to be set on each machine like so. #Overall Project Name project.name=myapp # Top Level Root directory of the new working project toplevel.project.dir=D:/proj/eComm # Root directory of the source code root.project.dir=D:/proj/eComm/Construction # JDK home directory jdk.home=C:/jdk1.5.0_11 build.properties will also have some static properties defined relative to the above. These need not be changed by any user on any local machine. ear.dist.dir = ${root.project.dir}/target src.dir = ${root.project.dir}/src test.src.dir = ${root.project.dir}/test Ensure your build.xml only refers to any further sub directories via these properties without any hardcoded values in it. My junit are in a separate file which is imported into the build.xml by <import file=""${root.project.dir.buildscripts.dir}/junit.xml""/> and some part of junit.xml is shown below <target name=""run.junit"" depends=""clean.junit junit.info prepare.junit"" description=""Compiles and runs all JUnit Tests in the 'test' directory and produces a report of all failures""> <junit printsummary=""yes"" fork=""true"" haltonfailure=""no"" showoutput=""yes"" maxmemory=""512m""> <jvmarg line=""${junit.jvm.arg}""/> <classpath> <fileset dir=""${src.dir}""> <include name=""**/*.*""/> </fileset> <fileset dir=""${ear.dist.dir}/build/classes""> <include name=""**/*.*""/> </fileset> <pathelement path=""${test.run.path}""/> </classpath> <formatter type=""xml""/> <batchtest fork=""true"" todir=""${ear.dist.dir}/build/junit""> <fileset dir=""${test.src.dir}"" includes=""${test.pattern.include}""/> </batchtest> </junit> </target> You almost said what I want is not possible with out changes in build.xmls. Accepted answer as it has enough explanation about how to structure the build files. Thanks!"
1000,A,"Is it possible to synchronize JUnit test cases with the classes under tests within Eclipe? I know it is possible to create a TestCase or TestSuite with the wizard within JUnit but how does one sync the code after the class under the test has been modified such as method signature modification or newly added methods. I would like my TestCases to be able to sync up (remove or add) those changed methods/parameters/method signatures within my TestCases. I tried searching on Google to no avail maybe there is an eclipse plugin for that? Tests which have a 1-1 relationship with the structure of the production code are a test smell. It's much better for the tests to be written as a specification of the system's behaviour (~one test per behaviour) instead of having tests generated based on the production code (~one test per method). Quoted from http://blog.daveastels.com/files/BDD_Intro.pdf When you realize that it's all about specifying behaviour and not writing tests your point of view shifts. Suddenly the idea of having a Test class for each of your production classes is ridiculously limiting. And the thought of testing each of your methods with its own test method (in a 1-1 relationship) will be laughable. Thx for the reply. A short while after posting this question I realized that writing tests based on behaviour is the way to go and therefore that is what I am doing. I do not really need unit test synchronizing now :) I fully support that point of view (unit testing the behaviour). Adding up this comes up easier to read tests since the method name contains the behaviour it tests.  If you change method signatures using the automated refactoring then the test cases - and all other code that calls the method - will be automatically updated. For newly added methods I don't know of a way to have the test class automatically updated.  As has been mentioned before refactorings such as renames or moves will be automatically reflected in the test cases as long as you refactor using the Eclipse tooling and not by manually renaming for example. As for new methods it's impossible to generate tests automatically. Of course there are some exceptions for auto-generated code where you control the generation and where you could generate test cases as well but for ""normal"" manual code the best you could do was to provide stubs (empty methods) and what's the use in that? A better approach is to track code coverage using a tool such as Cobertura or Emma which happens to have a nice Eclipse plugin that allows you to see inside your source code what code is being covered by tests and which isn't. This then is your report of where you need more testing. Yeah I see I thought there was maybe something beyond code coverage but I think that will do anyways :) Thanks a lot for the clear answer"
1001,A,"Why does heap space run out only when running JUnit tests? When running JUnit tests I always seem to run into this error: eclipse outOfMemoryError: heap space I have monitored Eclipse with JConsole and heap memory peaks at about 150MB. I have set heap memory to 1GB. I am using the following arguments when starting Eclipse:  -vm ""C:\Program Files\Java\jre1.5.0_08\bin\javaw.exe"" -vmargs -Xmx1024M -XX:MaxPermSize=128M -Dcom.sun.management.jmxremote.port=8999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false Does anyone know what may be causing this issue? It happens only when running JUnit tests. May you please provide further details? Which version of Eclipse? Which version of JUnit? Any mock-objects framework? A code snippet which rises the exception? Any framework behind the curtain? I've just released a plugin for Eclipse that will automatically set the heap size on JUnit launchers for you. You can get it from http://code.google.com/p/junitlaunchfixer/ It works with Eclipse Europa Ganymede and Galileo. FYI: junitlaunchfixer is not available at preset.  You probably have a memory leak in your JUnit tests. A common gotcha is this: Junit will create a new instance of a TestCase class for every test method in it And all instance variables will be kept around until JUnit terminates. That means: if you have a TestCase class with 50 test methods and an instance variable that is initialized with a 1MB object graph in your setUp() method then that TestCase class will require 50MB heap space.  I found the solution to my problem - it may help others ;) When I was increasing the heap size I was increasing the heap size of eclipse application not of my program (which I executed through eclipse) What I had to do is modify the execution commands before running my program.  Junit tests are run in a different vm as the Eclipse IDE. So it is that vm that is out of memory and not the Eclipse one. You can change the settings of the test vm in the run configurations of the test. You go to the run configurations and then under arguments you can set the vm arguments.  Further to @Thijs Wouters response to fix this issue in eclipse I did the following: Added a new Run configuration under JUnit (Run>Run configuration>JUnit>New) Within the arguments tab set VM arguments to ""-Xms64m -Xmx256m"" or higher if needs be"
1002,A,"Writing data to System.in In our application we expect user input within a Thread as follows : BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); I want to pass that part in my unit test so that I can resume the thread to execute the rest of the code. How can I write something into System.in from junit? I didn't think you could but apparently you can according to Justin. However wouldn't it be better to refactor the tested code to get the inputstream inserted from outside? @Bart if you are testing an app that takes input from `System.in` eventually you will want to test the module that takes the data from `System.in`. @Justin Isn't this testing Java and not your code? If you make the InputStream injected you are still testing all of your code and depending on Java to provide a correct System.in. @Justin: The two answers suggesting redirecting `System.in` are not really doing a complete job of that though since System.in no longer reads from standard in which is what the application will be using. You're really not testing anything except for maybe whether the System class functions properly which I assure you has been tested by Sun. @Jacob no. You are testing how your code handles various inputs from `System.in`. You can automate the tests instead of having users enter in test data through a console. @Mark The application will be using the inputstream that `System.in provides`. The application shouldn't care where that data is coming from. When run in a production setting it will be coming from a keyboard but in the test it will be coming from a text file or some hard coded strings. You are testing the ability of your application to handle different types of inputs. Instead of the suggestions above (edit: I noticed that Bart left this idea in a comment as well) I would suggest making your class more unit testable by making the class accept the input source as a constructor parameter or similar (inject the dependency). A class shouldn't be so coupled to System.in anyway. If your class is constructed from a Reader you can just do this: class SomeUnit { private final BufferedReader br; public SomeUnit(Reader r) { br = new BufferedReader(r); } //... } //in your real code: SomeUnit unit = new SomeUnit(new InputStreamReader(System.in)); //in your JUnit test (e.g.): SomeUnit unit = new SomeUnit(new StringReader(""here's the input\nline 2""));  What you want to do is use the method setIn() from System. This will let you pass data into System.in from junit. +1 for being the simplest answer @Erick thanks. I try to be complete yet concise. Thanks... that was what i was looking for. @user glad I could help. This works when I run the JUnit tests individually but when I run the entire class my program seems to stall at the points where the setIn() should be working. Does anyone else have this issue and if so any ideas on how to get around it? @decal I'm not sure what the problem might be sorry.  Replace it for the duration of your test: String data = ""the text you want to send""; InputStream testInput = new ByteArrayInputStream( data.getBytes(""UTF-8"") ); InputStream old = System.in; try { System.setIn( testInput ); ... } finally { System.setIn( old ); } Small detail: since the bufferedreader is constructed without a charset shouldn't you call `getBytes()` without a charset as well (or of course add a charset to the bufferedreader)?"
1003,A,Running all tests from a @Category using Maven I want to run only a subset of my unit tests the ones defined by a specific @Category. So I read several SO questions such as this one (which is exactly what I am looking for) and also this one. The solution of my problem seems to be provided by the ClasspathSuite project. So I started to write the NewTest and OldTest interfaces that will define my test categories. Then I created the AllTests suite: @RunWith(ClasspathSuite.class) public class AllTests { } After that I created a AllNewTests suite: @RunWith(Categories.class) @IncludeCategory(NewTest.class) @SuiteClasses( { AllTests.class }) public class AllNewTests { } Finally I create two JUnit classes one per category: @Category(NewTest.class) public class SomeNewTests { // some tests... } @Category(OldTest.class) public class SomeOldTests { // some tests... } Now if I run AllTests Eclipse launches all the tests of my project while Maven fails as no test are found: mvn test -Dtest=AllTests ... ------------------------------------------------------- T E S T S ------------------------------------------------------- Running my.company.AllTests Tests run: 0 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.093 sec There are no tests to run. If I run AllNewTests (which is the correct thing to do right?) in Eclipse everything is fine (i.e. it only run the tests annoted with @Category(NewTest.class)) but Maven returns an error: mvn test -Dtest=AllNewTests ... ------------------------------------------------------- T E S T S ------------------------------------------------------- Running my.company.AllNewTests Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 0.125 sec <<< FAILURE! Results : Tests in error: initializationError(my.company.AllNewTests) Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ The exception thrown is the following: org.junit.runner.manipulation.NoTestsRemainException at org.junit.runners.ParentRunner.filter(ParentRunner.java:256) at org.junit.experimental.categories.Categories.<init>(Categories.java:142) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:35) at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:24) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:29) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:24) at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:33) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:146) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:97) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:103) at $Proxy0.invoke(Unknown Source) at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:145) at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:70) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:69) My question is what I did wrong? Technical details: Java 6 Maven 3.0.2 JUnit 4.8.1 Surefire plugin 2.7.1 cpsuite-1.2.5 After reading some blog posts and stackoverflow questions I finally was able to make this work with the surefire plugin as user1034382 answered. In my case with version 2.17 of maven-surefire-plugin. Just to add my two cents the more up-to-date explanation can be found here: Using JUnit Categories to group tests But you may run with the following surefire plugin problem: [ERROR] java.lang.RuntimeException: Unable to load category: That can be fixed with this other stackoverflow question/answer: Where should I put interface class for Junit @Category? My answer is just to gather all these info here and avoid googling/reading to many diferent solutions. Al least this worked to me.  As an update: as of Surefire plugin v2.11 JUnit 4.8+ style categories are now supported. The release notes for Surefire v2.11 mention the new feature. The surefire:test goal can is configured using groups. Have you actually succeeded in doing this ? I'm can't seem to make it work. Surefire (2.11 / 2.12) seem to ignore the groups. It simply runs all tests. @JanGoyvaerts This was a bug and is now fixed. In fact to run the categories in 2.11 and 2.12 you have to specify a dependency on the junit 47 provider (surefire-junit47). This is fixed in 2.13-SNAPSHOT and should work without the explicit dependency. I've made it work in the end as you said. BUT ... there's a big performance problem with the provider. It buffers the logs (that's not too bad) and eats all memory and cpu when the tests last too long. So I switched to the naming convention - as it was meant to be originally. That works without a problem. I've signaled the problem.  I have solved my problem by creating my own JUnit Runner that extends the Suite. The idea is close to the principle by the Classpath Suite project i.e. looking for the classes existing in the classpath and keep only the ones that are annotated with a given annotation (@NewTest for example). If you are interested you can read the full story on my blog. This method is now completely obsolete with the evolutions made in surefire plugin (see user1034382 answer).
1004,A,"JUnit: pause for user input I'm learning JUnit. Since my app includes graphical output I want the ability to eyeball the output and manually pass or fail the test based on what I see. It should wait for me for a while then fail if it times out. Is there a way to do this within JUnit (or its extensions) or should I just throw up a dialog box and assertTrue on the output? It seems like it might be a common problem with an existing solution. Edit: If I shouldn't be using JUnit for this what should I be using? I want to manually verify the build every so often and unit test automatically and it'd be great if the two testing frameworks got along. I recommend using your unit tests for the Models if using MVC or any utility method (i.e. with Swing it's common to have color mapping methods). If you have a good set of unit tests on things like model behavior if you have a UI bug it'll help narrow your search. Visual based unit tests are very difficult at a company I worked at they had tried these visual tests but slight differences in video cards could produce failed tests. In the end this is where a good Q/A team is required.  Take a look at FEST-Swing. It provides an easy way to automatically test your GUIs. The other thing you'll want to do is separate your the code which does the bulk of the work from your gui code as much as possible. You can then write unit tests on this work code without having to deal with the user interface. You'll also find that you'll run these tests much more frequently as they can run quickly.  Unit tests shouldn't require human intervention. If you need a user to take an action then I think you're doing it wrong. If you need a human to verify things then don't do this as part of your unit tests. Just make it a required step for your test department to carry out when QA'ing builds. (this still works of your QA department is just you.)  Manually accepting/rejecting a test defeats the purpose of using an automated test framework. JUnit is not made for this kind of stuff. Unless you find a way to create and inject a mockup of the object representing your output device you should consider alternatives (don't really know any there sorry). I once wrote automated tests for a video decoding component. I dumped the decoded data to a file using some other decoder as a reference and then compared the output of my decoder to that using the PSNR of each pair of images. This is not 100% self contained (needs external files as resources) but was automated at least and worked fine for me. I'll look into a way to automatically eyeball the output. Not ideal might work for obvious regressions. I have some similar problem now and I can understand now the question: - I have to create watermarked images - I want to create an unit test however its not trivial to check if an image is really watermarked - I want to use temporaryfolder and I want to cleanup all files after the test So in this case it makes sense to say that for example if the test is running with a special parameter then before the end of the test the test prints the temp folder to the console and lets me check the results before cleanup. Then you can have a manual check if you want.  Although you could probably code that that is not what JUnit is about. It is about automated tests not guided manual tests. Generally that ""does it look right"" test is regarded as an integration test as it is something that is very hard to automate correctly in a way that doesn't break for trivial changes all the time. Take a look at Abbot to give you a more robust way to test your GUI. Or FEST (http://code.google.com/p/fest/). It's not the GUI I want to test; it's graphical output. Thanks for the pointers for when I *do* get to testing the GUI though!"
1005,A,"Validating number of failing Junits in ANT I would like to run a JUnit regression Test Suite from within an ANT build script. The test suite has a known number of failures which we are working towards fixing. I want the build to fail if the number of failing tests increases (or changes at all if that's easier to code) - this value can be hard coded in the ANT script. I'm looking for details on how to implement this. What's the point of running unit tests which are known to fail? As you may know this is against the best practice which is to keep 100% of unit tests passing all the time - and if a test fails either fix it right away or disable/remove it if it can't be fixed. Good point but this is actually a regression test suite which contains 'known' failures. In time I hope these will be fixed but at the moment we've just got around to documenting them and want to make sure we don't introduce more failures. Even though I'm a bit late to the party: Another option would be to use JUnit 4 and annotate the failing tests with @Ignore. That way they will not run and not count as failures but the number of ignored tests will still be printed reminding you there is work left to do. If a test fails for the first time you'll get a regular failure; then you can decide whether to fix or to ignore it. More information: http://www.devx.com/Java/Article/31983/1954 Thanks for your response. I hadn't encountered that annotation. Unfortunately because I am using parametrized tests I only want to ignore certain tests for certain parameters - I don't think JUnit is flexible enough to do this. Yes apparently you cannot ""@Ignore"" just some parameters. You could comment out the failing values and create a placeholder test that is ""@Ignored"" so you remember the failing values. That way you don't need to count tests...  The junit task creates and XML file with the failing tests. There's no reason you couldn't use XMLTask to count the number of failures and have a conditional fail if that value is too high. thanks I will take a look"
1006,A,"How do I run JUnit tests from inside my java application? Is it possible to run JUnit tests from inside my java application? Are there test frameworks I can use (such as JUnit.jar?) or am I force to find the test files invoke the methods and track the exceptions myself? The reason why I am asking is my application requires a lot of work to start launch (lots of dependencies and configurations etc) and using an external testing tool (like JUnit Ant task) would require a lot of work to set up. It is easier to start the application and then inside the application run my tests. Is there an easy test framework that runs tests and output results from inside a java application or am I forced to write my own framework? You can mock your resources or whatever running test in java application makes no sense to me .. take a look at the jmock.org The reason why I am asking is my application requires a lot of work to start launch (lots of dependencies and configurations etc) and using an external testing tool (like JUnit Ant task) would require a lot of work to set up. You need to remove these dependencies from the code you are testing. The dependencies and configurations are precisely what you are trying to avoid when writing a test framework. For each test you should be targeting the smallest testable part of an application. For example if you require a database connection to execute some process in a class you are trying to test - decouple the database handling object from your class pass it in via a constructor or setter method and in your test use a tool like JMock (or write a stub class) to build a fake database handling object. This way you are making sure the tests are not dependent on a particular database configuration and you are only testing the small portion of code you are interested in not the entire database handling layer as well. It might seem like a lot of work at first but this kind of refactoring is exactly what your test framework should be fleshing out. You might find it useful to get a book on software testing as a reference for decoupling your dependencies. It will pay off a lot more than trying to bootstrap JUnit from inside your running application.  Yes you can. I was doing it couple of times to run diagnostic/smoke tests in production systems. This is a snippet of key part of the code invoking JUnit: JUnitCore junit = new JUnitCore(); Result result = junit.run(testClasses); DON'T use JUnit.main inside your application it invokes System.exit after tests are finished and thus it may stop JVM process. You may want to capture JUnit's ""regular"" console output (the dots and simple report). This can be easily done by registering TextListener (this class provides this simple report). Please also be aware of several complications using this kind of method: Testing of any ""test framework"" including so small one although is quite simple may be confusing. For example if you want to test if your ""test framework"" return failure result when one of the tests fails you could (should?) create sample JUnit test that always fails and execute that test with the ""test framework"". In this case failing test case is actually test data and shouldn't be executed as ""normal"" JUnit. For an example of such tests you can refer to JUnit's internal test cases. If you want to prepare / display your custom report you should rather register your own RunListener because Result returned by JUnit doesn't contain (directly) information about passed tests and test method (it is only ""hardcoded"" as a part of test Description).  As documented in the JUnit FAQ: public static void main(String args[]) { org.junit.runner.JUnitCore.main(""junitfaq.SimpleTest""); }  According to the JUnit API JUnitCore has several methods to execute tests inside Java. Thanks to Tomislav Nakic-Alfirevic for pointing it out. http://junit.sourceforge.net/javadoc/org/junit/runner/JUnitCore.html"
1007,A,"How do you get the python Google App Engine development server (dev_server.py) running for unit test in GWT? So I have a GWT client which interacts with a Python Google App Engine server. The client makes request to server resources the server responds in JSON. It is simple no RPC or anything like that. I am using Eclipse to develop my GWT code. I have GWTTestCase test that I would like to run. Unfortunately I have no idea how to actually get the google app engine server running per test. I had the bright idea below of trying to start the app engine server from the command line but of course this does not work as Process and ProcessBuilder are not classes that the GWT Dev kit actually contains. package com.google.gwt.sample.quizzer.client; import java.io.IOException; import java.lang.ProcessBuilder; import java.lang.Process; import com.google.gwt.junit.client.GWTTestCase; public class QuizzerTest extends GWTTestCase { public String getModuleName() { return ""com.google.gwt.sample.quizzer.Quizzer""; } public void gwtSetUp(){ ProcessBuilder pb = new ProcessBuilder(""dev_appserver.py"" ""--clear_datastore"" ""--port=9000"" ""server_python""); try { p = pb.start(); } catch (IOException e) { System.out.println(""Something happened when starting the app server!""); } public void gwtTearDown(){ p.destroy(); } public void testSimple() { //NOTE: do some actual network testing from the GWT client to GAE here assertTrue(true);} } I get the following errors when compiling this file: [ERROR] Line 21: No source code is available for type java.lang.Process; did you forget to inherit a required module? [ERROR] Line 30: No source code is available for type java.lang.ProcessBuilder; did you forget to inherit a required module? As you can see below I basically want it to be the case that per test it: Starts a datastore-empty instance of my GAE server runs the test across the network against this server instance. Stop the server Of course report the result of the test back to me. Does anyone have a good way of doing this? Partial solutions are welcome! Hacks are fine as well. Maybe some progress on this problem could be made by editing the "".launch"" config file? The only important criteria is that I would like to ""unit test"" portions of my GWT code against my actual GAE Python server. Thank you. Yes that is precisely the case. I'm confused. Are you writing your frontend in Java with GWT and your backend in Python? I should be explicit. Yes that is precisely the case. Frontend (client) is written in GWT backend (server) is written in Python GAE. I would recommend creating an Ant target for this - take a look at this page for the full ant build file for GWT. Then as the first line of the testing target add an execution task to start the server. Look here for exec docs. Then set up that ant task in your IDE. This way you get the server running before your tests irrespective of where you run the tests from and it can be integrated into your build process if you want."
1008,A,"Unit-test framework for bean comparison Is there any good framework for comparing whole objects? now i do assertEquals(""ha@gmail.com"" obj.email); assertEquals(""5"" obj.shop); if bad email is returned i never get to know if it had the right shop i would like to get a list of incorrect fields. my object is simple ;) Duplicate: http://stackoverflow.com/questions/1411612/how-to-test-for-equality-of-complex-object-graphs I would go for hamcrest matchers. They allow me to write code like this: assertThat(obj hasProperty(""email"" equalTo(""ha@gmail.com""))); meaningful errors do sound cool and extendable also. I will look into it thx :) I personally don't like hamcrest. for me it looks like useless text. why not just assertEquals(obj.getEmail()""ha@gmail.com""); its much shorter! for me it looks like writing extra code for fun (plus static imports!). there are so many reasons why hamcrest is not a useless text =) First of all your *intention* is much cleaner then you get a meaningfull error when it fails instead of a generic ""AssertionError"" and above all it is extendable!  Going with the 1 test 1 assert line of thinking if you has each assert in its own test you would know if one or both of these had failed by the fact that there were 1 or 2 failing tests. @Test public void TestEmail() { obj = GetTestObject(); assertEquals(""ha@gmail.com"" obj.email); } @Test public void TestShop() { obj = GetTestObject(); assertEquals(""5"" obj.shop); } obviously you need to move the setup of the object into a method ot have it performed in the test set up method and have it a class variable. If you actually want to test if all properties are set in a single test: @Test public void TestAllProperties() { obj = GetTestObject(); bool testResult=true; string failureString; if ""ha@gmail.com"".equals( obj.email) == false { testResult=false; failureString+=""email was different""; } if ""5"".equals( obj.shop) == false { testResult=false; failureString+=""shop was different""; } assertTrue(testResultfailurestring); } but I'm not sure what this gives you really. But if you really want to compare the whole objects equality then override the equals method (not forgetting getHashCode too) and do your equality checking in there. after all that is what it is for... If you want a list of incorrect fields you could make the equals method populate an internal list which you could query if the equality check failed to get the list of failed fields the last time equality was checked. Don't really think that's a good idea though.  If you want to test object equality then surely you need to implement equals() and test using assertEquals() but otherwise Sam is correct one assertion per test  You can also implement the comparable interface. http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Comparable.html  this has helped me with that http://stackoverflow.com/questions/1411612/how-to-test-for-equality-of-complex-object-graphs"
1009,A,"How to use Junit to test asynchronous processes How do you test methods that fire asynchronous processes with Junit? I don't know how to make my test wait for the process to end (it is not exactly a unit test it is more like an integration test as it involves several classes and not just one) You could try JAT (Java Asynchronous Test): https://bitbucket.org/csolar/jat JAT has 1 watcher and hasn't been updated in 1.5 years. Awaitility was updated just 1 month ago and is on version 1.6 at the time of this writing. I'm not affiliated with either project but if I was going to invest in an addition to my project I'd give more credence to Awaitility at this time. IMHO it's bad practice to have unit tests create or wait on threads etc. You'd like these tests to run in split seconds. That's why I'd like to propose a 2-step approach to testing async processes. Test that your async process is submitted properly. You can mock the object that accepts your async requests and make sure that the submitted job has correct properties etc. Test that your async callbacks are doing the right things. Here you can mock out the originally submitted job and assume it's initialized properly and verify that your callbacks are correct. Sure. But sometimes you need to test code that is specifically supposed to manage threads. Wrap that code in a mockable interface :D I'm writing a messaging framework it three layers an application layer a data access layer and the transport layer. Application layer handles the state of the signaling data access layer handles addressing. These are independent of network technology so I have my unit tests for them. I also have integration tests for the whole thing running on a memory based transport layer. How can I test the network based transport layer? I can run it on loopback having only milliseconds of latency but that still is async. For those of us that use Junit or TestNG to do integration testing (and not just unit testing) or user acceptance testing (e.g. w/ Cucumber) waiting for an async completion and verifying the result is absolutely necessary. This answer is given in a unit test perspective.  An alternative is to use the CountDownLatch class. public class DatabaseTest { /** Data limit */ private static int DATA_LIMIT = 5; /** Countdown latch */ private CountDownLatch lock = new CountDownLatch(1); /** Received data */ private List<Data> receiveddata; @Test public void testDataRetrieval() throws Exception { Database db = new MockDatabaseImpl(); db.getData(DATA_LIMIT new DataCallback() { @Override public void onSuccess(List<Data> data) { receiveddata = data; lock.countDown(); } }); lock.await(2000 TimeUnit.MILLISECONDS); assertNotNull(receiveddata); assertEquals(DATA_LIMIT receiveddata.size()); } } NOTE you can't just used syncronized with a regular object as a lock as fast callbacks can release the lock before the lock's wait method is called. See this blog post by Joe Walnes. EDIT Removed syncronized blocks around CountDownLatch thanks to comments from @jtahlborn and @Ring please don't follow this example it is incorrect. you should _not_ be synchronizing on a CountDownLatch as it handles the thread-safety internally. It was good advice up until the synchronized part that ate up probably close to 3-4 hours of debugging time. http://stackoverflow.com/questions/11007551/synchronize-is-blocking-when-i-try-to-countdown#11007569 Apologies for the error. I've edited the answer appropriately. This simple demonstration of testing asynchronous code with CountDownLatch may be useful. http://razshahriar.com/2014/08/testing-asynchronous-code-in-java-with-countdownlatch/ Nice decription of CountDownLatch @Raz  You can try using the Awaitility framework. It makes it easy to test the systems you're talking about.  How about calling SomeObject.wait and notifyAll as described here OR using Robotiums Solo.waitForCondition(...) method OR use a class i wrote to do this (see comments and test class for how to use)  Start the process off and wait for the result using a Future.  One method I've found pretty useful for testing asynchronous methods is injecting an Executor instance in the object-to-test's constructor. In production the executor instance is configured to run asynchronously while in test it can be mocked to run synchronously. So suppose I'm trying to test the asynchronous method Foo#doAsync(Callback c) class Foo { private final Executor executor; public Foo(Executor executor) { this.executor = executor; } public void doAsync(Callback c) { executor.execute(new Runnable() { @Override public void run() { // Do stuff here c.onComplete(data); } }); } } In production I would construct Foo with an Executors.newSingleThreadExecutor() Executor instance while in test I would probably construct it with a synchronous executor that does the following -- class SynchronousExecutor implements Executor { @Override public void execute(Runnable r) { r.run(); } } Now my JUnit test of the asynchronous method is pretty clean -- @Test public void testDoAsync() { Executor executor = new SynchronousExecutor(); Foo objectToTest = new Foo(executor); Callback callback = mock(Callback.class); objectToTest.doAsync(callback); // Verify that Callback#onComplete was called using Mockito. verify(callback).onComplete(any(Data.class)); // Assert that we got back the data that we expected. assertEquals(expectedData callback.getData()); }  If you use a CompletableFuture (introduced in Java 8) you can make your test finish as soon as it's done rather than waiting a pre-set amount of time. Your test would look something like this: CompletableFuture<String> future = new CompletableFuture<>(); executorService.submit(new Runnable() { @Override public void run() { future.complete(""Hello World!""); } }); assertEquals(""Hello World!"" future.get()); ... and if you're stuck with java-less-than-eight try guavas [SettableFuture](http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/SettableFuture.html) which does pretty much the same thing"
1010,A,"Spring - unit tests I followed the instructions at ""Unit Testing with Junit Spring and Hibernate – Part 2"". I created my simple test class: package com.bontade.tests.unit.models; import org.hibernate.SessionFactory; import org.springframework.orm.hibernate3.SessionFactoryUtils; import org.springframework.test.AbstractTransactionalDataSourceSpringContextTests; import com.bontade.mvc.models.User; import com.bontade.mvc.models.dao.implementations.UserDAOImplementation; public class UserDAOTest extends AbstractTransactionalDataSourceSpringContextTests { private UserDAOImplementation userDAO; private SessionFactory sessionFactory; /* public UserDAOTest() { setDependencyCheck(false); setAutowireMode(AUTOWIRE_BY_NAME); } */ public void setUserDAO(UserDAOImplementation userDAO) { this.userDAO = userDAO; } public void setSessionFactory(SessionFactory sessionFactory) { this.sessionFactory = sessionFactory; } protected String[] getConfigLocations() { // classpath*:/src/test/java/com/bontade/tests/unit/models/test-spring- config.xml return new String[] { ""classpath*:/**/test-spring-config.xml"" }; } public void testSave() { String query = ""SELECT COUNT(*) FROM User WHERE name = 'Mark'""; int count = this.jdbcTemplate.queryForInt(query); assertEquals(""A user already exists in the DB"" 0 count); User user = new User(); user.setId(null); user.setName(""Mark""); user.setGender(""Man""); user.setSurname(""Surname""); user.setPassword(""X""); userDAO.saveUser(user); // flush the session so we can get the record using JDBC template SessionFactoryUtils.getSession(sessionFactory false).flush(); count = jdbcTemplate.queryForInt(query); assertEquals(""User was not found in the DB"" 1 count); } } My configuration file: <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd""> <bean id=""dataSource"" class=""org.springframework.jdbc.datasource.DriverManagerDataSource""> <property name=""driverClassName"" value=""org.h2.Driver"" /> <property name=""url"" value=""jdbc:h2:tcp://localhost/~/bontade_database_h2"" /> <property name=""username"" value=""ab"" /> <property name=""password"" value=""ab"" /> </bean> <bean id=""sessionFactory"" class=""org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean""> <property name=""dataSource"" ref=""dataSource"" /> <property name=""annotatedClasses""> <list> <value>com.bontade.mvc.models.User</value> </list> </property> </bean> <bean id=""transactionManager"" class=""org.springframework.orm.hibernate3.HibernateTransactionManager""> <property name=""sessionFactory"" ref=""sessionFactory"" /> </bean> <bean id=""userDAO"" class=""com.bontade.mvc.models.dao.implementations.UserDAOImplementation""> <property name=""sessionFactory"" ref=""sessionFactory"" /> <property name=""transactionTemplate""> <bean class=""org.springframework.transaction.support.TransactionTemplate""> <property name=""transactionManager"" ref=""transactionManager"" /> </bean> </property> </bean> Both files are placed inside the src/test/java directory in the com.bontade.tests.unit.models package. UserDAO and UserDAOImplementation are placed in the src/main/java directory. I'm getting error like this: ------------------------------------------------------------------------------- Test set: com.bontade.tests.unit.models.UserDAOTest ------------------------------------------------------------------------------- Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 0.297 sec <<< FAILURE! testSave(com.bontade.tests.unit.models.UserDAOTest) Time elapsed: 0.202 sec <<< ERROR! org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'com.bontade.tests.unit.models.UserDAOTest': Unsatisfied dependency expressed through bean property 'dataSource': Set this property value or disable dependency checking for this bean. at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.checkDependencies(AbstractAutowireCapableBeanFactory.java:1184) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1006) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireBeanProperties(AbstractAutowireCapableBeanFactory.java:329) at org.springframework.test.AbstractDependencyInjectionSpringContextTests.injectDependencies(AbstractDependencyInjectionSpringContextTests.java:205) at org.springframework.test.AbstractDependencyInjectionSpringContextTests.prepareTestInstance(AbstractDependencyInjectionSpringContextTests.java:180) at org.springframework.test.AbstractSingleSpringContextTests.setUp(AbstractSingleSpringContextTests.java:100) at junit.framework.TestCase.runBare(TestCase.java:125) at org.springframework.test.ConditionalTestCase.runBare(ConditionalTestCase.java:76) at junit.framework.TestResult$1.protect(TestResult.java:106) at junit.framework.TestResult.runProtected(TestResult.java:124) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:118) at junit.framework.TestSuite.runTest(TestSuite.java:208) at junit.framework.TestSuite.run(TestSuite.java:203) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.maven.surefire.junit.JUnitTestSet.execute(JUnitTestSet.java:207) at org.apache.maven.surefire.junit.JUnit3Provider.executeTestSet(JUnit3Provider.java:107) at org.apache.maven.surefire.junit.JUnit3Provider.invoke(JUnit3Provider.java:79) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:103) at $Proxy0.invoke(Unknown Source) at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:145) at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:87) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:69) Does anybody know the reason why? UPDATE: After reading @Ritesh's clue I moved ""test-spring-config.xml"" into the src/test/resources directory and now I see a file in the target/test-classes folder but I got a new error: ------------------------------------------------------------------------------- Test set: com.bontade.tests.unit.models.UserDAOTest ------------------------------------------------------------------------------- Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 0.572 sec <<< FAILURE! testSave(com.bontade.tests.unit.models.UserDAOTest) Time elapsed: 0.474 sec <<< ERROR! org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sessionFactory' defined in file [/home/adam/workspace/CarRentalMaven2XSLTJSPTiles2Tomcat/target/test-classes/test-spring-config.xml]: Invocation of init method failed; nested exception is java.lang.ClassFormatError: Absent Code attribute in method that is not native or abstract in class file javax/persistence/InheritanceType at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1338) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:473) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:409) at java.security.AccessController.doPrivileged(Native Method) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:380) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:264) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:261) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:185) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164) at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:423) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:728) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:380) at org.springframework.test.AbstractSingleSpringContextTests.createApplicationContext(AbstractSingleSpringContextTests.java:213) at org.springframework.test.AbstractSingleSpringContextTests.loadContextLocations(AbstractSingleSpringContextTests.java:189) at org.springframework.test.AbstractSingleSpringContextTests.loadContext(AbstractSingleSpringContextTests.java:169) at org.springframework.test.AbstractSpringContextTests.getContext(AbstractSpringContextTests.java:140) at org.springframework.test.AbstractSingleSpringContextTests.setUp(AbstractSingleSpringContextTests.java:98) at junit.framework.TestCase.runBare(TestCase.java:132) at org.springframework.test.ConditionalTestCase.runBare(ConditionalTestCase.java:76) at junit.framework.TestResult$1.protect(TestResult.java:110) at junit.framework.TestResult.runProtected(TestResult.java:128) at junit.framework.TestResult.run(TestResult.java:113) at junit.framework.TestCase.run(TestCase.java:124) at junit.framework.TestSuite.runTest(TestSuite.java:243) at junit.framework.TestSuite.run(TestSuite.java:238) at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83) at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:35) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:146) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:97) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:103) at $Proxy0.invoke(Unknown Source) at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:145) at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:87) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:69) Caused by: java.lang.ClassFormatError: Absent Code attribute in method that is not native or abstract in class file javax/persistence/InheritanceType at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632) at java.lang.ClassLoader.defineClass(ClassLoader.java:616) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141) at java.net.URLClassLoader.defineClass(URLClassLoader.java:283) at java.net.URLClassLoader.access$000(URLClassLoader.java:58) at java.net.URLClassLoader$1.run(URLClassLoader.java:197) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) at org.hibernate.cfg.InheritanceState.extractInheritanceType(InheritanceState.java:51) at org.hibernate.cfg.InheritanceState.<init>(InheritanceState.java:21) at org.hibernate.cfg.AnnotationBinder.buildInheritanceStates(AnnotationBinder.java:2295) at org.hibernate.cfg.AnnotationConfiguration.processArtifactsOfType(AnnotationConfiguration.java:540) at org.hibernate.cfg.AnnotationConfiguration.secondPassCompile(AnnotationConfiguration.java:291) at org.hibernate.cfg.Configuration.buildMappings(Configuration.java:1162) at org.springframework.orm.hibernate3.LocalSessionFactoryBean.buildSessionFactory(LocalSessionFactoryBean.java:673) at org.springframework.orm.hibernate3.AbstractSessionFactoryBean.afterPropertiesSet(AbstractSessionFactoryBean.java:211) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1369) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1335) ... 38 more This is what my maven console generates: ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.bontade.tests.unit.models.UserDAOTest SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/home/adam/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/home/adam/.m2/repository/org/slf4j/slf4j-simple/1.6.1/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/home/adam/.m2/repository/org/slf4j/slf4j-nop/1.5.3/slf4j-nop-1.5.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/home/adam/.m2/repository/org/slf4j/slf4j-jdk14/1.5.6/slf4j-jdk14-1.5.6.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. log4j:WARN No appenders could be found for logger (com.bontade.tests.unit.models.UserDAOTest). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. Tests run: 1 Failures: 0 Errors: 1 Skipped: 0 Time elapsed: 0.572 sec <<< FAILURE! Does it mean that I don't have the required jars in my classpath? Can you try with: return new String[] { ""classpath:test-spring-config.xml"" }; @up then I got error: `org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from class path resource [test-spring-config.xml]; nested exception is java.io.FileNotFoundException: class path resource [test-spring-config.xml] cannot be opened because it does not exist ...` you need to put slf4j or log4j properties in classpath. do you see test-spring-config.xml in target/test-classes folder? @up there wasn't test-spring-config.xml in target/test-classes folder so I moved the file into src/test/resources directory but I got a new error shown over in update of my question The solution was to use org.springframework.orm.hibernate3.LocalSessionFactoryBean instead of org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean"
1011,A,"Automatically add properties when running JUnit in Eclipse In order to run my unit tests on my Eclipse I need to set some properties for the VM. Thus when I first run my JUnit test I go in ""Open Run Dialog"" then in my JUnit configuration for this test I go in ""Arguments"" tab and put everything I need in the ""VM arguments"" text area. Is there a way to automatically add a set of properties when I run my JUnit so I will be able to only right-click on the test class and click on ""Run as > Junit Test"" to run a test? Technical information: Eclipse 3.3.2 JUnit 4 Java 5 Edit regarding response from Aaron Digulla: These properties are used in Spring configuration files*. Thus I can't use the idea given by Aaron as Spring will be initialized before the test is run. In addition to that I just need to know if I can achieve that in a easy way in Eclipse. Thus the solution must not have any impact on the compilation of the application outside Eclipse as my application will finally be compiled (and tested) by Maven2. * few ""unit"" tests indeed need my Spring configuration to be run. Ok I know that it is not real unit tests ;o) Edit 2: In fact I was indeed starting the Spring configuration by a test unit. Thus before starting Spring I check the System properties and if my properties are not set then I give them the required value... However I am a little disappointed that Eclipse can't do that for me automatically... Just to be explicit regarding spring tests. What OP is talking about when using spring is ""RunWith(SpringJUnit4ClassRunner.class) and ContextConfiguration"" will start spring before the regular junit Before and BeforeClass are hit so setting system properties in those will not work. Agreed used method in this way in one of my junit tests and it worked  @BeforeClass public static void setupProperties() { System.setProperty(""catalina.base"" ""C:\\sam-tomcat-7.0.42""); } I know this solution but it didn't respect my requirements. I wanted to have a solution only for Eclipse. Using this it will have an impact on the ""default"" build by Maven (ie outside the IDE). In addition it works only for one class (or you have to make all your tests extends the class that defines this `@BeforeClass`).  My solution is to create an abstract test base class for all tests in a project which extends TestCase. It has to be abstract so the automatic unit test finder will not consider it. In static code block of this class I set all properties I need. This ensures that the code runs once and only once and that it runs before any test in my project. [EDIT] You say that Spring is initialized before the tests run. This is a bug in your project: It must be the tests who initialize Spring. Otherwise you will always run into the problem that you have to test something outside of your control. Therefore I suggest to move the Spring init code into a place where you can call it at the point in time when the environment is ready. Alternatively check that the environment is correctly set up in setUp() and throw an error if a property is missing. This way you will at least know why the tests would fail later. But I still prefer to have total control when which subsystem comes to life. Anything else just begs for disaster. I can't use your idea as explained in my Edit (I don't downvote it as it is not a bad idea however)... I agree with static code block init. There is another alternative: @BeforeClass on a method where you can init your system variables.  When i want to set some properties entries for my junit test i implement the following protected void setUp() throws Exception { super.setUp(); System.setProperty(""Property1"" ""value1""); System.setProperty(""Property2"" ""value2""); } The properties are set before the test methode is called EDIT: You also can read the properties from a file and at thes to the System properties Same remark as for Aaron Digulla answer. It will not work if the properties are used by Spring... You should initialize all values and objects inside the setup method. Otherwise update the testcases.  You could try this - go to  Window->Preferences->Java->Installed JREs ans select the JVM in use than put a ""Default VM"" prameter like  -DrunningInEclipse Than you can check from within your TestCase:  System.getProperty(""runningInEclipse"") != null That works! Great! Fantastic! Thanks.  I never understood why the launch configurations have a way to define environment variables but the only way of adding a system property seems to be to add vm arguments. The way I've worked around this is to have tests (or an abstract tests base class) test for the presence of required properties if they aren't there then I load them from a .properties file on the classpath. This works as I can still override them or specify them from ANT or Maven but can also 'right click' -> Run As -> Junit Test the individual test files. edit: here is an example of getting Spring to optionally load a properties file in the same manner as described above: <bean id=""placeholderConfig"" class=""org.springframework.beans.factory.config.PropertyPlaceholderConfigurer""> <property name=""location"" value=""database.properties""/> <property name=""ignoreResourceNotFound"" value=""true"" /> <property name=""systemPropertiesMode""> <util:constant static-field=""org.springframework.beans.factory.config.PropertyPlaceholderConfigurer.SYSTEM_PROPERTIES_MODE_OVERRIDE"" /> </property> </bean>"
1012,A,Java: How do I ignore certain elements when comparing XML? I have an XML message like so: <root> <elementA>something</elementA> <elementB>something else</elementB> <elementC>yet another thing</elementC> </root> I want to compare a message of this type produced by a method under test to an expected message but I don't care about elementA. So I'd like the above message to be considered equal to: <root> <elementA>something different</elementA> <elementB>something else</elementB> <elementC>yet another thing</elementC> </root> I'm using the latest version of XMLUnit. I'm imagining that the answer involves creating a custom DifferenceListener; I just don't want to reinvent the wheel if there's something ready to use out there. Suggestions that use a library other than XMLUnit are welcome. I would use XSLT and the identity transform to filter out elements I want to ignore and compare the results. See XSL: how to copy a tree but removing some nodes ? earlier on SO. That's also a valid approach to the problem. Thanks for the suggestion.  I wound up implementing a DifferenceListener that takes a list of node names (with namespaces) to ignore textual differences for: public class IgnoreNamedElementsDifferenceListener implements DifferenceListener { private Set<String> blackList = new HashSet<String>(); public IgnoreNamedElementsDifferenceListener(String ... elementNames) { for (String name : elementNames) { blackList.add(name); } } public int differenceFound(Difference difference) { if (difference.getId() == DifferenceConstants.TEXT_VALUE_ID) { if (blackList.contains(difference.getControlNodeDetail().getNode().getParentNode().getNodeName())) { return DifferenceListener.RETURN_IGNORE_DIFFERENCE_NODES_IDENTICAL; } } return DifferenceListener.RETURN_ACCEPT_DIFFERENCE; } public void skippedComparison(Node node Node node1) { } } I'm surprised it's this complex - disappointing if xmlunit does not have something built in for ignoring specific elements?
1013,A,"How do I programmatically run all the JUnit tests in my Java application? From Eclipse I can easily run all the JUnit tests in my application. I would like to be able to run the tests on target systems from the application jar without Eclipse (or Ant or Maven or any other development tool). I can see how to run a specific test or suite from the command line. I could manually create a suite listing all the tests in my application but that seems error prone - I'm sure at some point I'll create a test and forget to add it to the suite. The Eclipse JUnit plugin has a wizard to create a test suite but for some reason it doesn't ""see"" my test classes. It may be looking for JUnit 3 tests not JUnit 4 annotated tests. I could write a tool that would automatically create the suite by scanning the source files. Or I could write code so the application would scan it's own jar file for tests (either by naming convention or by looking for the @Test annotation). It seems like there should be an easier way. What am I missing? According to a recent thread on the JUnit mailing list ClasspathSuite can collect and run all JUnit tests on the classpath. It is not precisely what you want since it is a class-level annotation but the source is available so you may be able to extend its internal discovery mechanism. Thanks that looks like it would do what I want but I will probably stick with my own solution for since it's simpler.  I ran into a minor problem with my last solution. If I ran ""all tests"" from Eclipse they ran twice because they ran the individual tests AND the suite. I could have worked around that but then I realized there was a simpler solution: package suneido; import java.io.IOException; import java.util.ArrayList; import java.util.Enumeration; import java.util.jar.JarEntry; import java.util.jar.JarFile; public class RunAllTests { public static void run(String jarfile) { String[] tests = findTests(jarfile); org.junit.runner.JUnitCore.main(tests); } private static String[] findTests(String jarfile) { ArrayList<String> tests = new ArrayList<String>(); try { JarFile jf = new JarFile(jarfile); for (Enumeration<JarEntry> e = jf.entries(); e.hasMoreElements();) { String name = e.nextElement().getName(); if (name.startsWith(""suneido/"") && name.endsWith(""Test.class"") && !name.contains(""$"")) tests.add(name.replaceAll(""/"" ""."") .substring(0 name.length() - 6)); } jf.close(); } catch (IOException e) { throw new RuntimeException(e); } return tests.toArray(new String[0]); } public static void main(String[] args) { run(""jsuneido.jar""); } } Why not edit your old post then multiple answer are just confusing?  Based on http://burtbeckwith.com/blog/?p=52 I came up with the following. It seems to work well. I can run it from within my code with: org.junit.runner.JUnitCore.main(""suneido.AllTestsSuite""); One weak point is that it relies on a naming convention (""Test"" suffix) to identify tests. Another weak point is that the name of the jar file is hard coded. package suneido; import java.io.IOException; import java.lang.reflect.Modifier; import java.util.*; import java.util.jar.JarEntry; import java.util.jar.JarFile; import org.junit.runner.RunWith; import org.junit.runners.Suite; import org.junit.runners.model.InitializationError; /** * Discovers all JUnit tests in a jar file and runs them in a suite. */ @RunWith(AllTestsSuite.AllTestsRunner.class) public final class AllTestsSuite { private final static String JARFILE = ""jsuneido.jar""; private AllTestsSuite() { } public static class AllTestsRunner extends Suite { public AllTestsRunner(final Class<?> clazz) throws InitializationError { super(clazz findClasses()); } private static Class<?>[] findClasses() { List<String> classFiles = new ArrayList<String>(); findClasses(classFiles); List<Class<?>> classes = convertToClasses(classFiles); return classes.toArray(new Class[classes.size()]); } private static void findClasses(final List<String> classFiles) { JarFile jf; try { jf = new JarFile(JARFILE); for (Enumeration<JarEntry> e = jf.entries(); e.hasMoreElements();) { String name = e.nextElement().getName(); if (name.startsWith(""suneido/"") && name.endsWith(""Test.class"") && !name.contains(""$"")) classFiles.add(name.replaceAll(""/"" ""."") .substring(0 name.length() - 6)); } jf.close(); } catch (IOException e) { throw new RuntimeException(e); } } private static List<Class<?>> convertToClasses( final List<String> classFiles) { List<Class<?>> classes = new ArrayList<Class<?>>(); for (String name : classFiles) { Class<?> c; try { c = Class.forName(name); } catch (ClassNotFoundException e) { throw new AssertionError(e); } if (!Modifier.isAbstract(c.getModifiers())) { classes.add(c); } } return classes; } } } Could you possibly explain why you use the check for isAbstract and not other options such as isInterface? Are abstract parents automatically loaded when the children are used for the first time?  You also could use ANT which has built-in task. Write ANT script and run it on target machine. ANT could create report as result. Yes but I don't want to install that kind of tool on the target system. (As I said in my original question.)  I have not tried this as of yet but came across this blog recently: http://burtbeckwith.com/blog/?p=52 The author provides a class that discovers all your junits and runs them so if you slot this in to your project it may provide the capability required? Hope this helps. I had discovered that post before. The problem is that it scans the source files. On my target systems I don't have the source. However I might be able to adapt his solution to scan the jar file instead.  using Class JUnitCore JUnitCore is a facade for running tests. It supports running JUnit 4 tests JUnit 3.8.x tests and mixtures. To run tests from the command line run java org.junit.runner.JUnitCore TestClass1 TestClass2 .... For one-shot test runs use the static method runClasses(Class[]). If you want to add special listeners create an instance of JUnitCore first and use it to run the tests. That doesn't solve my problem. As I said I know how to run specific test classes from the command line. But what I want is to run ALL the tests in my application without explicitly listing each one.  Get the Java project and pass the project JUnitLaunchShortcut jUnitLaunchShortcut = new JUnitLaunchShortcut(); jUnitLaunchShortcut.launch(""Pass the Java Project containing JUnits Classes"" ""run"");"
1014,A,"Compare Date objects with different levels of precision I have a JUnit test that fails because the milliseconds are different. In this case I don't care about the milliseconds. How can I change the precision of the assert to ignore milliseconds (or any precision I would like it set to)? Example of a failing assert that I would like to pass: Date dateOne = new Date(); dateOne.setTime(61202516585000L); Date dateTwo = new Date(); dateTwo.setTime(61202516585123L); assertEquals(dateOne dateTwo); If you were using Joda you could use Fest Joda Time. could you provide more information as how this should be implemented? Else this should be converted to a comment.  Yet another workaround I'd do it like this: assertTrue(""Dates aren't close enough to each other!"" (date2.getTime() - date1.getTime()) < 1000); +1 for comparing variance but does not account for absolute variance (e.g. what if date1 is after date2?) I usually take similar approach just wrapping that with Math.abs()  If you have Apache commons-lang on your classpath you can use DateUtils.truncate to round the dates to some field. assertEquals(DateUtils.truncate(date1Calendar.SECOND) DateUtils.truncate(date2Calendar.SECOND)); this is the solution I was looking for :) Thanks this saved me a ton of time! Why not use DateUtils.round?  Using JUnit 4 you could also implement a matcher for testing dates according to your chosen precision. In this example the matcher takes a string format expression as a parameter. The code is not any shorter for this example. However the matcher class may be reused; and if you give it a describing name you can document the intention with the test in an elegant way. import static org.junit.Assert.assertThat; // further imports from org.junit. and org.hamcrest. @Test public void testAddEventsToBaby() { Date referenceDate = new Date(); // Do something.. Date testDate = new Date(); //assertThat(referenceDate equalTo(testDate)); // Test on equal could fail; it is a race condition assertThat(referenceDate sameCalendarDay(testDate ""yyyy MM dd"")); } public static Matcher<Date> sameCalendarDay(final Object testValue final String dateFormat){ final SimpleDateFormat formatter = new SimpleDateFormat(dateFormat); return new BaseMatcher<Date>() { protected Object theTestValue = testValue; public boolean matches(Object theExpected) { return formatter.format(theExpected).equals(formatter.format(theTestValue)); } public void describeTo(Description description) { description.appendText(theTestValue.toString()); } }; }  I don't know if there is support in JUnit but one way to do it: import java.text.SimpleDateFormat; import java.util.Date; public class Example { private static SimpleDateFormat formatter = new SimpleDateFormat(""dd MMM yyyy HH:mm:ss""); private static boolean assertEqualDates(Date date1 Date date2) { String d1 = formatter.format(date1); String d2 = formatter.format(date2); return d1.equals(d2); } public static void main(String[] args) { Date date1 = new Date(); Date date2 = new Date(); if (assertEqualDates(date1date2)) { System.out.println(""true!""); } } } If you call the method `assertEqualDates` then I'd make its return type `void` and make the last line `assertEquals(d1 d2)`. This way it would behave the same as all the JUnit `assert*` methods. Agreed. I wanted to run the code and didn't have JUnit at hand. Be wary of global date formatters. They are not thread-safe. It's not a problem with this code but it's a bad habit to have. This doesn't handle the case where the two Date objects have a sub-second difference but they cross the second threshold.  Use a DateFormat object with a format that shows only the parts you want to match and do an assertEquals() on the resulting Strings. You can also easily wrap that in your own assertDatesAlmostEqual() method.  Instead of using new Date directly you can create a small collaborator which you can mock out in your test: public class DateBuilder { public java.util.Date now() { return new java.util.Date(); } } Create a DateBuilder member and change calls from new Date to dateBuilder.now() import java.util.Date; public class Demo { DateBuilder dateBuilder = new DateBuilder(); public void run() throws InterruptedException { Date dateOne = dateBuilder.now(); Thread.sleep(10); Date dateTwo = dateBuilder.now(); System.out.println(""Dates are the same: "" + dateOne.equals(dateTwo)); } public static void main(String[] args) throws InterruptedException { new Demo().run(); } } The main method will produce: Dates are the same: false In the test you can inject a stub of DateBuilder and let it return any value you like. For example with Mockito or an anonymous class which overrides now(): public class DemoTest { @org.junit.Test public void testMockito() throws Exception { DateBuilder stub = org.mockito.Mockito.mock(DateBuilder.class); org.mockito.Mockito.when(stub.now()).thenReturn(new java.util.Date(42)); Demo demo = new Demo(); demo.dateBuilder = stub; demo.run(); } @org.junit.Test public void testAnonymousClass() throws Exception { Demo demo = new Demo(); demo.dateBuilder = new DateBuilder() { @Override public Date now() { return new Date(42); } }; demo.run(); } }  You could do something like this: assertTrue((date1.getTime()/1000) == (date2.getTime()/1000)); No String comparisons needed. I think you meant ""/"" versus ""%""? This gets messy regarding arbitrary precision IMHO. Good point though. Whoops! Good catch. I don't think precision is an issue though. Date.getTime() always returns a long of ms since the epoch. This will fail if one value is 3.999 seconds and the other 4.000. Inother words sometimes it will tolerate a difference up to a seconds sometimes it will fail for a 2 ms difference.  i cast the objects to java.util.Date and compare assertEquals((Date)timestamp1(Date)timestamp2);  Just compare the date parts you're interested in comparing: Date dateOne = new Date(); dateOne.setTime(61202516585000L); Date dateTwo = new Date(); dateTwo.setTime(61202516585123L); assertEquals(dateOne.getMonth() dateTwo.getMonth()); assertEquals(dateOne.getDate() dateTwo.getDate()); assertEquals(dateOne.getYear() dateTwo.getYear()); // alternative to testing with deprecated methods in Date class Calendar calOne = Calendar.getInstance(); Calendar calTwo = Calendar.getInstance(); calOne.setTime(dateOne); calTwo.setTime(dateTwo); assertEquals(calOne.get(Calendar.MONTH) calTwo.get(Calendar.MONTH)); assertEquals(calOne.get(Calendar.DATE) calTwo.get(Calendar.DATE)); assertEquals(calOne.get(Calendar.YEAR) calTwo.get(Calendar.YEAR)); I like this approach a lot better then using a date formatter. Only problem is that the specific getter fields in Date are deprecated. Better to use a Calendar to do the same thing. Ah good point to note that those methods are deprecated. I've updated my answer with the alternative code to convert and compare Calendar objects instead.  In JUnit you can program two assert methods like this: public class MyTest { @Test public void test() { ... assertEqualDates(expectedDateObject resultDate); // somewhat more confortable: assertEqualDates(""01/01/2012"" anotherResultDate); } private static final String DATE_PATTERN = ""dd/MM/yyyy""; private static void assertEqualDates(String expected Date value) { DateFormat formatter = new SimpleDateFormat(DATE_PATTERN); String strValue = formatter.format(value); assertEquals(expected strValue); } private static void assertEqualDates(Date expected Date value) { DateFormat formatter = new SimpleDateFormat(DATE_PATTERN); String strExpected = formatter.format(expected); String strValue = formatter.format(value); assertEquals(strExpected strValue); } }  Something like this might work: assertEquals(new SimpleDateFormat(""dd MMM yyyy"").format(dateOne) new SimpleDateFormat(""dd MMM yyyy"").format(dateTwo));  This is actually a harder problem than it appears because of the boundary cases where the variance that you don't care about crosses a threshold for a value you are checking. e.g. the millisecond difference is less than a second but the two timestamps cross the second threshold or the minute threshold or the hour threshold. This makes any DateFormat approach inherently error-prone. Instead I would suggest comparing the actual millisecond timestamps and provide a variance delta indicating what you consider an acceptable difference between the two date objects. An overly verbose example follows: public static void assertDateSimilar(Date expected Date actual long allowableVariance) { long variance = Math.abs(allowableVariance); long millis = expected.getTime(); long lowerBound = millis - allowableVariance; long upperBound = millis + allowableVariance; DateFormat df = DateFormat.getDateTimeInstance(); boolean within = lowerBound <= actual.getTime() && actual.getTime() <= upperBound; assertTrue(MessageFormat.format(""Expected {0} with variance of {1} but received {2}"" df.format(expected) allowableVariance df.format(actual)) within); }"
1015,A,"Running JUnit Tests on a Restlet Router Using Restlet I have created a router for my Java application. From using curl I know that each of the different GET POST & DELETE requests work for each of the URIs and return the correct JSON response. I'm wanting to set-up JUnit tests for each of the URI's to make the testing process easier. However I'm not to sure the best way to make the request to each of the URIs in order to get the JSON response which I can then compare to make sure the results are as expected. Any thoughts on how to do this? I had a similar question http://stackoverflow.com/questions/2165561/ways-to-test-restful-services . rest-client should work quite good for your scenario. It's close but not quite what I'm after. It would be nice if I could set up test-suites etc. Also leads to the problem of all members of the team needing to have access to that UI. You could just use a Restlet Client to make requests then check each response and its representation. For example: Client client = new Client(Protocol.HTTP); Request request = new Request(Method.GET resourceRef); Response response = client.handle(request); assert response.getStatus().getCode() == 200; assert response.isEntityAvailable(); assert response.getEntity().getMediaType().equals(MediaType.TEXT_HTML); // Representation.getText() empties the InputStream so we need to store the text in a variable String text = response.getEntity().getText(); assert text.contains(""search string""); assert text.contains(""another search string""); I'm actually not that familiar with JUnit assert or unit testing in general so I apologize if there's something off with my example. Hopefully it still illustrates a possible approach to testing. Good luck! My pleasure glad to help! BTW I recommend you try Groovy for this sort of thing — it makes the tests more concise. It's especially great that it has getter and setter shortcuts and == means value equality. So instead of response.getEntity().getMediaType().equals(MediaType.TEXT_HTML) you can write response.entity.mediaType == MediaType.TEXT_HTML. HTH! That was great. With assert it's assertTrue(...) for anyone else using the example but perfect apart from that. Thanks  Unit testing a ServerResource // Code under test public class MyServerResource extends ServerResource { @Get public String getResource() { // ...... } } // Test code @Autowired private SpringBeanRouter router; @Autowired private MyServerResource myServerResource; String resourceUri = ""/project/1234""; Request request = new Request(Method.GET resourceUri); Response response = new Response(request); router.handle(request response); assertEquals(200 response.getStatus().getCode()); assertTrue(response.isEntityAvailable()); assertEquals(MediaType.TEXT_PLAIN response.getEntity().getMediaType()); String responseString = response.getEntityAsText(); assertNotNull(responseString); where the router and the resource are @Autowired in my test class. The relevant declarations in the Spring application context looks like <bean name=""router"" class=""org.restlet.ext.spring.SpringBeanRouter"" /> <bean id=""myApplication"" class=""com.example.MyApplication""> <property name=""root"" ref=""router"" /> </bean> <bean name=""/project/{project_id}"" class=""com.example.MyServerResource"" scope=""prototype"" autowire=""byName"" /> And the myApplication is similar to public class MyApplication extends Application { }  Based on the answer of Avi Flax i rewrite this code to java and run it with junitparams a library that allows pass parametrized tests. The code looks like: @RunWith(JUnitParamsRunner.class) public class RestServicesAreUpTest { @Test @Parameters({ ""http://url:port/path/api/rest/1 200 true"" ""http://url:port/path/api/rest/2 200 true"" }) public void restServicesAreUp(String uri int responseCode boolean responseAvaliable) { Client client = new Client(Protocol.HTTP); Request request = new Request(Method.GET uri); Response response = client.handle(request); assertEquals(responseCode response.getStatus().getCode()); assertEquals(responseAvaliable response.isEntityAvailable()); assertEquals(MediaType.APPLICATION_JSON response.getEntity() .getMediaType()); } } how can i add challenge request in junit?  I got the answer for challenge response settings in REST junit test case @Test public void test() { String url =""http://localhost:8190/project/user/status""; Client client = new Client(Protocol.HTTP); ChallengeResponse challengeResponse = new ChallengeResponse(ChallengeScheme.HTTP_BASIC""user"" ""f399b0a660f684b2c5a6b4c054f22d89""); Request request = new Request(Method.GET url); request.setChallengeResponse(challengeResponse); Response response = client.handle(request); System.out.println(""request""+response.getStatus().getCode()); System.out.println(""request test::""+response.getEntityAsText()); }"
1016,A,selenium 2 and junit Do most people use there selenium (in my case selenium 2 with web driver) tests with JUnit? I personally do not and was wondering if I am in the minority. I tend to write all my own classes and exception handling and have everything write to a database for reporting on results. The only problem I see with your approach is that the more code you write yourself instead of relying on existing third party libraries the more maintenance you will have. If you have the option of using third party your should (in most cases) integrate it instead of writing something yourself. I guess I just like my GUI automation to be more robust using page or domain design patterns to map the site under test to code. I see JUnit as a unit test framework and do not look at GUI automation in that light  If you want a testing framework but feel like JUnit is too restrictive you might want to consider using TestNG. It is designed for integration testing and has features that make GUI automation easier.
1017,A,"MouseEvent weirdness I'm trying to create a MouseEvent with certain modifiers for UnitTesting. I'm using J2SE and the following code fails to pass: public void testMouseEventProblem() { MouseEvent event = new MouseEvent(new JPanel() 1 System.currentTimeMillis() InputEvent.CTRL_DOWN_MASK | InputEvent.ALT_DOWN_MASK 11 0 false); assertEquals(InputEvent.CTRL_DOWN_MASK | InputEvent.ALT_DOWN_MASK event.getModifiers()); } It's complaining saying ""expected 640 but was 10"" What's going on? To expand on Uri's answer you probably want to say something like assert(InputEvent.CTRL_DOWN_MAsK & event.getModifiers() != 0); assert(InputEvent.ALT_DOWN_MAsK & event.getModifiers() != 0); This will check that both modifiers are pressed without regard to the rest of the string which it seems is something else.  It gets internally converted to ALT_MASK and CTRL_MASK constants (8 + 2) It happens in java.awt.event.InputEvent:405 in JDK 6 /** * Returns the modifier mask for this event. */ public int getModifiers() { return modifiers & (JDK_1_3_MODIFIERS | HIGH_MODIFIERS); } Try getModifiersEx():442: public int getModifiersEx() { return modifiers & ~JDK_1_3_MODIFIERS; } As ALT_DOWN_MASK and friends are extended modifiers introduced after Java 1.3 Proof: public class MouseEvt { public static void main(String[] args) { MouseEvent event = new MouseEvent(new JPanel() 1 System .currentTimeMillis() InputEvent.CTRL_DOWN_MASK | InputEvent.ALT_DOWN_MASK 1 1 0 false); System.out.printf(""%d - %d%n"" InputEvent.CTRL_DOWN_MASK | InputEvent.ALT_DOWN_MASK event.getModifiersEx()); } } Returns 640 - 640  I don't think you should use assertEquals here. You are checking that at least one of two specific bits are set in a value that could be anything so you probably want to separate asserts. Let me clarify this: You are getting a number that consists of a bunch of bits that are set (the modifiers) but you only care about two specific bits. With assertEquals you are essentially saying that you want two specific bits sets while the others are zero. What you could do is assert that getModifiers() & ( MASK1 | MASK2 ) is greater than zero since at least one of the two bits must be on for that to happen That being said something about those numbers looks funky are you sure you are using the correct mask values? But it would still be nice to know why getModifiers( ) returns a different quantity (I'm not a Java guy just curious). But you are correct the OP should be ANDing to test specific bits. To check for the bits being set on you could do: assertEquals(InputEvent.CTRL_DOWN_MASK | InputEvent.ALT_DOWN_MASK (InputEvent.CTRL_DOWN_MASK | InputEvent.ALT_DOWN_MASK) & event.getModifiers());"
1018,A,"NPE in StrutsTestCase after enabling Tiles I developed some JUnit tests that extend org.apache.struts2.StrutsTestCase. I used the tutorial on struts.apache.org as my starting point. Everything was working fine until I modified my simple web application to use Tiles. I have Tiles working fine in the app but now my Action test cases have stopped working. I'm getting NullPointerException at org.apache.struts2.views.tiles.TilesResult.doExecute when I run the following line of code: ActionProxy proxy = getActionProxy(""/displaytag.action""); The log shows the Struts 2 Action is executing succesfully until it tries to hand it off to TilesResult.doExecute. I suspect it is because the tests run outside of the container and the tiles.xml is only referenced in the web.xml and therefore my StrutsTestCase tests don't know where to find the definitions in tiles.xml. Is this making sense? I'm using Struts 2.2.1.1 and the tiles related jars (v. 2.0.6) included in the Struts distribution. I'll include a code snippet from my StrutsTestCase but please note everything runs successfully when I run the app from the browser in Tomcat it only fails when I run the StrutsTestCase outside of Tomcat. And the test cases ran successfully before I added Tiles. public class TagActionTest extends StrutsTestCase { static Logger logger = Logger.getLogger(TagActionTest.class); public void testCreateTagFail() throws Exception { logger.debug(""Entering testCreateTagFail()""); try { request.setParameter(""name"" """"); ActionProxy proxy = getActionProxy(""/createtag.action""); TagAction tagAction = (TagAction) proxy.getAction(); proxy.execute(); assertTrue(""Problem There were no errors present in fieldErrors but there should have been one error present"" tagAction.getFieldErrors().size() == 1); assertTrue(""Problem field 'name' not present in fieldErrors but it should have been"" tagAction.getFieldErrors().containsKey(""name"") ); } catch (Exception e) { logger.debug(""Error running testCreateTagFail()""); e.printStackTrace(); assertTrue(""Error running testCreateTagFail()"" false); } } Partial stack trace: java.lang.NullPointerException at org.apache.struts2.views.tiles.TilesResult.doExecute(TilesResult.java:105) at org.apache.struts2.dispatcher.StrutsResultSupport.execute(StrutsResultSupport.java:186) at com.opensymphony.xwork2.DefaultActionInvocation.executeResult(DefaultActionInvocation.java:373) Lastly can anyone explain what the deal is with StrutsTestCase? There's a tutorial page for using it with Struts 2 on struts.apache.org but the SourceForge page for it hasn't been updated since Struts 1.3 Also what's the difference between StrutsTestCase and MockStrutsTestCase I stumbled accross the same issue. Did you manage to underestand and solve it Justin? @wild_oscar unfortunately no. good luck @Justin found any solution ? It is trying to display the jsp page. So disable by adding ExecuteResult(false) in the code. So add the below line proxy.setExecuteResult(false); before proxy.execute()  I imagine you're initialising tiles with a listener: <listener> <listener-class>org.apache.struts2.tiles.StrutsTilesListener</listener-class> </listener> You need to initialise that Listener in your tests. I found a few others with the same issue [1]. The code below is in your class that extends StrutsSpringTestCase. You need to override the setupBeforeInitDispatcher. In the code snippet below the override sets the applicationContext attribute (also needed if you're using spring) and initialises Tiles (inside the if(tilesApplication) segment where tilesApplication is a boolean so you can toggle this code on an off based on your whether or not your application runs with tiles ):  /** Overrides the previous in order to skip applicationContext assignment: context is @autowired * @see org.apache.struts2.StrutsSpringTestCase#setupBeforeInitDispatcher() **/ @Override protected void setupBeforeInitDispatcher() throws Exception { //init context servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE applicationContext); if(tilesApplication){ servletContext.addInitParameter(BasicTilesContainer.DEFINITIONS_CONFIG ""WEB-INF/tiles.xml""); final StrutsTilesListener tilesListener = new StrutsTilesListener(); final ServletContextEvent event = new ServletContextEvent(servletContext); tilesListener.contextInitialized(event); } } [1] See http://depressedprogrammer.wordpress.com/2007/06/18/unit-testing-struts-2-actions-spring-junit/ thanks for the answer. i've since moved on from Struts2 to Spring MVC and and therefore can't easily confirm this solution in my old project. since this is the only answer this question has received (and it looks like it should work) i'm going to go ahead and accept it. many thanks! I had this same error traced it back to a TilesResult NPE and this fixed my issue. Thanks! I'm having the same issue but I'm not using Spring & hence I'm extending `StrutsJUnit4TestCase` - what will be the solution in my case ?"
1019,A,"Comparing text files w/ Junit I am comparing text files in junit using:  public static void assertReaders(BufferedReader expected BufferedReader actual) throws IOException { String line; while ((line = expected.readLine()) != null) { assertEquals(line actual.readLine()); } assertNull(""Actual had more lines then the expected."" actual.readLine()); assertNull(""Expected had more lines then the actual."" expected.readLine()); } Is this a good way to compare text files? What is preferred? If expected has more lines than actual you'll fail an assertEquals before getting to the assertNull later. It's fairly easy to fix though: public static void assertReaders(BufferedReader expected BufferedReader actual) throws IOException { String expectedLine; while ((expectedLine = expected.readLine()) != null) { String actualLine = actual.readLine(); assertNotNull(""Expected had more lines then the actual."" actualLine); assertEquals(expectedLine actualLine); } assertNull(""Actual had more lines then the expected."" actual.readLine()); } I like that your answer doesn't rely on any third party libraries but this code won't compile. The scope of the variable 'actual' is limited to the while-loop so the final assertNull line won't compile. @buzz3791: No the scope of `actualLine` is limited to the while loop. The scope of `actual` is the whole method. Oops thanks for point out my mistake.  Here is a more exhaustive list of File comparator's in various 3rd-party Java libraries: org.apache.commons.io.FileUtils org.dbunit.util.FileAsserts org.fest.assertions.FileAssert junitx.framework.FileAssert org.springframework.batch.test.AssertFile org.netbeans.junit.NbTestCase  FileUtils sure is a good one. Here's yet another simple approach for checking if the files are exactly the same. assertEquals(FileUtils.checksumCRC32(file1) FileUtils.checksumCRC32(file2)); While the assertEquals() does provide a little more feedback than the assertTrue() the result of checksumCRC32() is a long. So that may not be intrisically helpful. +1 I guess this could come in handy for really large files (when you only care about whether the files differ not what the difference is)  I'd suggest using Assert.assertThat and a hamcrest matcher (junit 4.5 or later - perhaps even 4.4). I'd end up with something like: assertThat(fileUnderTest containsExactText(expectedFile)); where my matcher is: class FileMatcher { static Matcher<File> containsExactText(File expectedFile){ return new TypeSafeMatcher<File>(){ String failure; public boolean matchesSafely(File underTest){ //create readers for each/convert to strings //Your implementation here something like: String line; while ((line = expected.readLine()) != null) { Matcher<?> equalsMatcher = CoreMatchers.equalTo(line); String actualLine = actual.readLine(); if (!equalsMatcher.matches(actualLine){ failure = equalsMatcher.describeFailure(actualLine); return false; } } //record failures for uneven lines } public String describeFailure(File underTest); return failure; } } } } Matcher pros: Composition and reuse Use in normal code as well as test Collections Used in mock framework(s) Can be used a general predicate function Really nice log-ability Can be combined with other matchers and descriptions and failure descriptions are accurate and precise Cons: Well it's pretty obvious right? This is way more verbose than assert or junitx (for this particular case) You'll probably need to include the hamcrest libs to get the most benefit  junit-addons has nice support for it: FileAssert It gives you exceptions like: junitx.framework.ComparisonFailure: aa Line [3] expected: [b] but was:[a]  Here's one simple approach for checking if the files are exactly the same: assertEquals(""The files differ!"" FileUtils.readFileToString(file1 ""utf-8"") FileUtils.readFileToString(file2 ""utf-8"")); Where file1 and file2 are File instances and FileUtils is from Apache Commons IO. Not much own code for you to maintain which is always a plus. :) And very easy if you already happen to use Apache Commons in your project. But no nice detailed error messages like in mark's solution. Edit: Heh looking closer at the FileUtils API there's an even simpler way: assertTrue(""The files differ!"" FileUtils.contentEquals(file1 file2)); As a bonus this version works for all files not just text. The assertTrue form is concise but relatively useless when it fails. At least the assertEquals method will show you a few characters where they are different **Update**: Nowadays I'd recommend [Google Guava](http://code.google.com/p/guava-libraries/) over Commons IO for reading the files as string: `Files.toString(file1 Charset.forName(""UTF-8""));` There isn't much difference in a case like this but overall Guava is a cleaner better documented and actively maintained library."
1020,A,Is there a way to separate long running (e.g. stress tests) out so they're not run by default in Maven 2? We've had an ongoing need here that I can't figure out how to address using the stock Maven 2 tools and documentation. Some of our developers have some very long running JUnit tests (usually stress tests) that under no circumstances should be run as a regular part of the build process / nightly build. Of course we can use the surefire plugin's exclusion mechanism and just punt them from the build but ideally we'd love something that would allow the developer to run them at will through Maven 2. Another option is to have the stress test detect it is running in maven and run only once or twice. i.e. turn into a regular functional test. This way you can check the code is still good but not run for a long time.  Adding to krosenvold's answer to ensure no unexpected behavior make sure you also have a default profile that is active by default that excludes the integration or stresstests you want to run in your special profile. <profile> <id>normal</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <excludes> <exclude>**/**/*IntTest.java</exclude> </excludes> </configuration> </plugin> </plugins> </build> <activation> <activeByDefault>true</activeByDefault> </activation> </profile> You will need to create a profile like this simply listing the surefire-plugin outside of a profile will override the profile should it be selected with: mvn -P integrationtest clean install  Use an integration test plugin such as the Super Helpful Integration Test Thingy to separate Integration Tests (long running systemic) from Unit Test (purists say 30 seconds max for all true unit tests to run). Make two Java packages for your unit tests versus integration tests. Then do not bind this plugin to a phase (the normal maven lifecycle) and only run it when it is explicitly called as a target like so: mvn shitty:clean shitty:install shitty:test <plugins> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>shitty-maven-plugin</artifactId> </plugin> </plugins> This way your normal developers will not be impacted and you'll be able to run integration tests on demand.  Normally you would add a profile to your maven configuration that runs a different set of tests: run this with mvn -Pintegrationtest install  <profile> <id>integrationtest</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <argLine>-client -Xmx896m -XX:MaxPermSize=192m</argLine> <forkMode>once</forkMode> <includes> <include>**/**/*Test.java</include> <include>**/**/*IntTest.java</include> </includes> <excludes> <exclude>**/**/*SeleniumTest.java</exclude> </excludes> </configuration> </plugin> </plugins> </build> <activation> <property> <name>integrationtest</name> </property> </activation> </profile>
1021,A,When is the right time to call robot.cleanUp() I'm using JUnit and FEST for Swing integration testing of our app that I start and stop multiple times in the test case. Should @after include a call to robot.cleanUp()? The general rule is as follows: each initialization site of a FrameFixture should have a corresponding cleanup site. Sepcifically if your @Before method initializes a FrameFixture (as in: w = new FrameFixture<MyWindow>() where w is a field of the test class) then you should define a @After method that will release the resources via w.cleanUp(). See the sample in: http://docs.codehaus.org/display/FEST/Getting+Started.
1022,A,What are good practices for unit testing Java EE/Web applications What are the best ways to test servlet oriented web applications. With possibly jdbc backed backends. Front-end = Struts2 and some servlets Back-end = hibernate some basic jdbc possible duplicate of [Unit-testing servlets](http://stackoverflow.com/questions/53532/unit-testing-servlets) Most important thing : design for testability. It means trying to have small independent components (which is almost always a good thing not only for testability) and test components separately. Inversion of Control (with or without an IoC framework like Spring) can help you a lot in that area. Once you have small components it's pretty easy to test them with jUnit easymock or any other standard test utilities. Testing the frontend is the most tedious and boring task. Selenium can help you a bit but there is no silver bullets that I know of.  use mocks easymock is one of the best mock frameworks(or you could use jmock if you like the name better). spring-mock has nice support for mocking servlet classes. you could also do integration testing with dbunit(to tests your sqls and hqls) but its slow and take much longer to write and maintain. i think you should not do it unless you had some problems with people writing bad queries or database schema is changed a lot.
1023,A,"Spring: JUnit-Test: applicationContext is not loaded I've got the following Test-class but it doesn't matter what I set as ""ContextConfiguration-locations"" - it never set my UserService. And it doesn't throw any error when I set a non-existing file in the locations-property... what am I doing wrong? @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = { ""classpath:applicationContextTest.xml"" }) public class BasicPersistenceTest { @Autowired private UserService userService; @Test public void testUserLogin() throws Exception { User result = userService.getUser(""stefan"" ""walkner""); Assert.assertNotNull(result); } @Test public void testLogin() { User user = userService.getUser(""stefan"" ""walkner""); Assert.assertNull(user); } public UserService getUserService() { return userService; } public void setUserService(UserService userService) { this.userService = userService; } } Spring-Version: 2.5.6.SEC01 JUnit-Version: 4.5 JDK: 1.5 When using @Autowired you don't need a setter for the property and I don't see why you want to have a public getter for it either. I removed getter/setter and included the setup()-method. bizzarely it seems like the setup()-method is never called... I put a Assert.assertNotNull(null) in there and a debug-breakpoint but it never gets there... Did you annotate the setup() with the @Before annotation? And you don't get any Spring injection errors? How is your UserService bean declared can you post that too? If you use @Autowired Spring does not need any method access to the property it can access the private directly. I havn't test it yet but if you really want to upgrade to spring 3.0 you can use the ehcache-spring-annotations framework. Note that http://code.google.com/p/ehcache-spring-annotations/ also works with Spring 2.5  I don't know why your test does not show any exceptions but Spring 2.5 is not compatible with JUnit 4.5. Either move to the Spring 3 milestones or downgrade JUnit to 4.4 . This bit me too"
1024,A,"Asserting exceptions in Java how? This might be a conceptually stupid question but it also might not and since I am still a student I think I should I have no problem asking. Imagine you have a method that if given certain conditions it will throw an NumberFormatException. I want to write a Unit Test to see if the exception is being correctly thorwn. How can I achieve this? P.S. I am using JUnit to write the Unit Tests. Thanks. Assuming you are using JUnit 4 call the method in your test in a way that causes it to throw the exception and use the JUnit annotation @Test(expected = NumberFormatException.class) If the exception is thrown the test will pass. +1 - Fast boy. @duffymo: Thanks! :)  As other posters suggested if you are using JUnit4 then you can use the annotation: @Test(expected=NumberFormatException.class); However if you are using an older version of JUnit or if you want to do multiple ""Exception"" assertions in the same test method then the standard idiom is: try { formatNumber(""notAnumber""); fail(""Expected NumberFormatException""); catch(NumberFormatException e) { // no-op (pass) } This is a good idiom for when you want to test things about the exception such as its message not just its existence. +1 - Good point about the older version. +1 - An answer to this question is not complete without the second example. In practice I need to default to this approach more often than not for the reasons mentioned. The problem with doing this is that I've seen a lot of people mess up the boilerplate. (ie: forgetting to call fail or catching too general of an exception). Personally I prefer to create a method that takes a Runnable or Callable (for checked exceptions) and asserts that it throws a given exception. (ie: assertException(NumberFormatException.class new Callable() { ... formatNumber(""notAnumber""); });. It's just as much boilerplate but ""feels safer"" I generally check the message and maybe other fields of the caught exception. @Michael D: Would you care to elaborate your comment and maybe add a answer of your own. I think like your idea.  You can do this:  @Test(expected=IndexOutOfBoundsException.class) public void testIndexOutOfBoundsException() { ArrayList emptyList = new ArrayList(); Object o = emptyList.get(0); }  A solution that is not bound to a particular JUnit version is provided by catch-exception which has been made to overcome some disadvantages that are inherent in the JUnit mechanisms.  If you can use JUnit 4.7 you can use the ExpectedException Rule @RunWith(JUnit4.class) public class FooTest { @Rule public ExpectedException exception = ExpectedException.none(); @Test public void doStuffThrowsIndexOutOfBoundsException() { Foo foo = new Foo(); exception.expect(IndexOutOfBoundsException.class); exception.expectMessage(""happened?""); exception.expectMessage(startsWith(""What"")); foo.doStuff(); } } This is much better than @Test(expected=IndexOutOfBoundsException.class) because the test will fail if IndexOutOfBoundsException is thrown before foo.doStuff() See this article and the ExpectedException JavaDoc for details For in depth reasoning and explanation I would like to recommend this article: http://monkeyisland.pl/2010/07/26/expected-exception-in-tests/ +1 from me. +1 - I didn't know this existed. The link to the [Expected Exception doc is broken](http://junit-team.github.io/junit/javadoc/4.10/org/junit/rules/ExpectedException.htmlhttp://junit-team.github.io/junit/javadoc/4.10/org/junit/rules/ExpectedException.html)  Use @Test(expected=IOException.class) http://junit.sourceforge.net/doc/faq/faq.htm#tests_7 This is fine if you have one expected exception. An alternative strategy is to add an Assert.fail() at the end of the test method. If an exception isn't thrown then the test will fail accordingly. e.g. @Test public void testIOExceptionThrown() { ftp.write(); // will throw IOException fail(); }  @Test(expected=""java.lang.NumberFormatException"") This annotation will do the trick."
1025,A,"Testing ServiceLocator using JUnit This is a follow up question to my previous question. I am trying to write test case for my ServiceLocator class but it gives me the following error: com/iplanet/ias/admin/common/ASException java.lang.NoClassDefFoundError: com/iplanet/ias/admin/common/ASException at java.lang.ClassLoader.defineClass1(Native Method) My test case: public void testServiceLocator () throws UivException NamingException { DataSource ds = ServiceLocator.getInstance().getDataSource(""jdbc/RSRC/my/mydb""); //I have not put any assert statements because i know above line is failing } The above code fails on getInstance() method which looks like this: static public ServiceLocator getInstance() throws UivException { try { if (me == null) { synchronized(ServiceLocator.class) { me = new ServiceLocator(); } } return me; } catch (UivException e) { throw new UivException(ErrorCode.SERVICE_LOCATOR_ERROR ErrorCode.SERVICE_LOCATOR_LOOKUP_ERROR e.getMessage()); } } I know this ServiceLocator works fine because when I test my application from the front end there are no issues. Only reason I am writing this test case is because I want to test my DAO. And for me to test my DAO ServiceLocator has to work (from JUnit). I don't have a clue what to make of the error message. Does anyone want to suggest something I can try that will hopefully work? EDIT: ServiceLocator constructor private ServiceLocator() throws UivException { try { ic = new InitialContext(); // System.out.println(""Created the Initial Context""); cache = Collections.synchronizedMap(new HashMap()); } catch (NamingException ne) { throw new UivException(ErrorCode.SERVICE_LOCATOR_ERROR 0 ne.getMessage()); } catch (NullPointerException e) { throw new UivException(ErrorCode.SERVICE_LOCATOR_ERROR 0 e.getMessage()); } } I'm curious why you're writing a service locator. Is it part of an assignment? In the real world Dependency Injection is the clearly superior choice using either Guice or Spring. not it isnt an assignment. We have old legacy code that is still using JDBC (Compared to ORM). I'm trying to move this legacy code to be more 'real world worthy'. You have any links about spring injections that you would like to share?  Actually the error is pretty clear: java.lang.NoClassDefFoundError: com/iplanet/ias/admin/common/ASException indicates that the definition of ASException cannot be found at runtime. When running in the application server this class is provided by iPlanet Application Server and the code just runs fine. To run this code outside the application server context you'll have to provide it ""manually"" by putting it to the classpath. So you have to add the right iPlanet JAR to the classpath (this is the tricky part you'll have to find which one). Additionally I can see taht you're using the InitialContext non-arg constructor so you're using iPlanet's environment settings when running inside the application server. Outside iPlanet (e.g. for a unit test) you'll have to provide the right JNDI properties for iPlanet yourself. To do so you'll have to put a jndi.properties file on the classpath with (at least) the initial context factory and the provider URL. Something like this: java.naming.factory.initial=... java.naming.provider.url=... Check your iPlanet documentation for the values.  Well is com.iplanet.ias.admin.common.ASException on the classpath when you invoke your tests? Are you relying on the app server to have the JDBC driver's libraries in it's classpath or are you deploying it yourself? I am relying on the app server. since appserver already has all the settings I just wanted to write the tests and get along with it. com.iplanet stuff seems to be Sun App server's admin properties. Thats probably needed to get the JDBC resource from the app server. ...but the Junit tests are not 'deployed' on the app server... but if your unit tests call code that tries to load the JDBC driver then the JDBC driver needs to be on the classpath when executing the unit tests."
1026,A,"Is Unit Testing worth the effort in a large and old (5yr) codebase? I've just joined a team which has been working in a main-always mode for the last 5 yrs (java maven based project). Consequently plans to leverage unit testing have always been in the pipeline never materialising (so far). A Great dev team has ensured that code quality is generally good and there aren't structural code issues but there's just no culture of writing jnuit tests. But I having seen the benefits of unit testing am a lone warrior here pushing for adoption of auto-testing. The team layout is such that a separate testing team does manual testing of functionality before rolling out the code and a change management team is a checkgate for change approvals and builds (no continuous integration either so far). Following are the hurdles: Because the code base is hugeand some of the original developers have left the team any additional unit tests may be too little too late. Add to it that I may be the only one pushing for unit testing. Though my manager has been supportive of the idea he doesnt want the change team to be bogged down by additional time required for the tests to run. I'm of the opinion that a stand-alone CI tool can be used to start off with and the change team must alter their scripts to skip tests as and when they are added. What would you do in my shoes? P.S.: I'm aware of a similar question on stackoverflow  but in this one the aim is to convince the different stakeholders and the best path to takel; not a technology comparison. I just discovered that there is a separate mini-project with only test cases in it. These are a very small number of tests and presumably a part of an ancient effort to cross testing out of the pending list. Besides they offer no code guarantee because they dont run everytime the code is compiled. Fresh test writing efforts are not helped by the fact that the code has not been written with automated tests in mind - parts have no scope to insert test/mock objects and a large mock framework may need to be evoloved. But I suppose the consensus so far is to persist with writing new tests? It would be worth writing tests to test specific functionality that you are working on and you want to remain stable. With a large existing application that has no test coverage at the moment you wouldn't want to sit down and write tests all in one go in order to get 100% unit test coverage and you never will. You'd just single out specific functionality for testing. I don't think there's such a thing as too little or too late; unit tests for only a tiny bit of very important functionality is better than none at all and some unit tests now are better than no unit tests ever. Problem is I want the rest of the team to start writing tests too and possibly to see its value. Trying to find the path of least resistance. You could lead by example start off by writing a few tests and then as they help track down regressions boast about the time they've saved you. ;)  I would not write tests just to write tests. What would be the business payoff? Your business has likely already suffered the effects of not having unit tests and now the issues have been addressed. If I did add tests I would only add them to the sections of code that would cause the most unpleasant results. For example I'd consider adding testing to any role/security related code.  Sounds like you have a pretty good handle on the situation. Two things: Don't expect to be able to fully unit test a 5 year old project. Assuming 5 developers working for 5 years you're in the neighborhood of 50000 programmer-hours. Assuming that tests take as much time to write as code you'd be doing pretty good to get 2-3% coverage in a year. So: Test new code and write tests for modifications of older code. Get time / take time to set up CI of some kind. Gradually you'll build up a nice battery of tests. Since you're apparently not drowning in bugs start slow gain momentum. Yes lead by example. Mandating the writing of tests can work on a new team (I've done it and been thanked later by people who took the habit with them to other teams) but on an existing team you'll likely only make them NOT want to test even more. So... start writing them yourself. Focus on bugs (write the failing test THEN fix the bug) and on new code especially any tricky code. The first time your suite catches a regression bug will be the moment you start making converts. @Varun - the best way to get someone test infected is to have a unit test save their ass a few times. The only way that can happen is if they actually write some tests :P A lot of developers unfamiliar with unit testing don't do it because they just don't know what to do. Give them a few examples of what good tests looks like and how easy they are to write and they will come around. ""Assuming that tests take as much time to write as code"" The main reason the team never wrote code was because of the notion that they'd rather spend that time writing new code. And that's the notion I need to battle. I could show them the spreadsheets the statistics on how test writing actually saves long run time but nobody's interested in the long run. Any suggestions how to persuade these devs to start writing unit tests NOW? We got our first compilation error in the code base today. I see a BIG opportunity. A CI tool at the very least.  It will be a massive effort and returns would be small. Rather automate and expand your current testing to ensure system stability (I assume there is some testing at system level). I think you should focus on parts of the system that still see lots of change or that you plan on changing in the near future. Unit testing is good but can be difficult to retrofit into an existing system. If you just focus on unit testing for the next year or more you are going to lose forward momentum.  IMHO unit tests or any other tests for that matter (except for functional tests) should be run frequently against parts of the application that are critical and volatile at the same time. In other words unit tests should not be written for the sake of writing tests but rather to ensure tha the development/maintenance engineers do not trip over certain conditions in code. That said you might also want to take a look at integration tests that run in a CI server more so because they're closer to the functional tests and also because the code is mature. From my observations identifying unit tests to write in a mature application is far difficult and has lesser RoI in the shorter term than integration tests. Integration tests in your case will ensure that the various tiers of the application continue to be held together with the intentions of the original developers. Unit tests on the other hand being a bit more localized in nature will ensure that any new patch/bug fix in a particular area does cause side effects in that location.  Unit testing is advantageous even in an old project for example by establishing unit tests you can be sure that the problem is not in existing code but in new code (even if you have a QA process bugs do still get through on occassion). Additionally they prevent changes to old code from introducing subtle differences in behavior. Also unit tests document the intended behavior of the huge codebase. Now adopting large changes are never easy. The first and most straightforward thing is to mandate unit testing of all new code so that the effort and workload of unit testing the old codebase doesn't continue to pile up. Then the second part is to take the initiative and create some unit tests for the old code. If you do that then you should be able to convince other members of your team to help with the effort in creating unit tests for existing code. If necessary come up with fun incentives like promising a pizza party for the team that creates the largest number of unit tests for the existing code base. Also remind your teammates that this isn't something where they need to drop everything that they are doing... if they only create one unit test for an existing component each day on top of their other work then eventually you will get a unit test for all the existing components in your codebase."
1027,A,"How do I load a properties file with JUnit / Ant? I'm using Ant 1.8 JUnit 4.8.2. I'm trying to load a properties file and having some problems. The properties file is at the root of my source directory and I load that into the classpath as well as explicitly loading the properties file in the classpath. Below is my ant build.xml file. Here is how I'm trying to load the properties … private void loadTestProperties() { try { Properties prop = new Properties(); InputStream in = getClass().getResourceAsStream(""db.properties""); prop.load(in); in.close(); } catch (Exception e) { fail(""Failed to load properties: "" + e.getMessage()); } // try } // loadTestProperties It always fails with null (properties were not loaded). <project name=""leads-testing"" default=""build"" basedir="".""> <property name=""tst-dir"" location=""/Users/davea/Documents/workspace-sts-2.6.0.SR1/infinitiusa_leads_testing/test"" /> <property name=""db-props-file"" location=""/Users/davea/Documents/workspace-sts-2.6.0.SR1/infinitiusa_leads_testing/test/db.properties"" /> <property name=""TALK"" value=""true"" /> <path id=""classpath.base""> </path> <path id=""classpath.test""> <pathelement location=""lib/junit-4.8.2.jar"" /> <pathelement location=""lib/selenium-java-client-driver.jar"" /> <pathelement location=""lib/classes12.jar"" /> <pathelement location=""${tst-dir}"" /> <pathelement location=""${db-props-file}"" /> <path refid=""classpath.base"" /> </path> <target name=""compile-test""> <javac srcdir=""${tst-dir}"" verbose=""${TALK}"" > <classpath refid=""classpath.test""/> </javac> </target> <target name=""clean-compile-test""> <delete verbose=""${TALK}""> <fileset dir=""${tst-dir}"" includes=""**/*.class"" /> </delete> </target> <target name=""test"" depends=""compile-test""> <junit> <classpath refid=""classpath.test"" /> <formatter type=""brief"" usefile=""false"" /> <test name=""com.criticalmass.infinitiusa.tests.InfinitiConfigOldG25Base"" /> </junit> </target> <target name=""all"" depends=""test"" /> <target name=""clean"" depends=""clean-compile-test"" /> </project> Anyone know the correct way to load the properties file? Thanks - Dave InputStream in = getClass().getResourceAsStream(""db.properties""); Try ""/db.properties"" instead. See Class.getResourceAsStream() For Ant file paths are resolved relative to the working directory. So if running from the project root the file would be in src/${db-props-file}.  Attempting to load a resource from getClass().getResourceAsStream() will cause db.properties to be looked for based on the package name of the class i.e. in a directory (in the classpath) like com/criticalmass/infinitiusa/.... Instead to load from the root of the classpath try something like InputStream in = Thread.currentThread().getContextClassLoader().getResourceAsStream(""db.properties""); This sounds right to me. I think the preferred way of acessing the ContextClassLoader is through the current class. So (I think) this would be even better: getClass().getContextClassLoader().getResourceAsStream(""db.properties""); Winner! Gweebz why do you say one way is preferred over another?"
1028,A,"JUnit tests run in eclipse return errors even though I have expected annotation (This is a follow up question to this one.) I'm having a problem with JUnit4 tests in eclipse. I'm trying to use the @Test(expected=...) annotation to unit test exception throwing. The problem is that when I run the tests they come back as errors instead of passing even though I'm expecting the exceptions to be thrown. I'm using eclipse 3.4.0 and JUnit 4.3.1. Here's the code: Class to test: public class IPAddress { private byte[] octets; private IPAddress() { octets = new byte[4]; } public IPAddress(String addr) throws InvalidIPAddressException { this(); if(addr == null || !isValid(addr)) throw new InvalidIPAddressException(addr); String strOctets[] = addr.split(""\\.""); for (int i = 0; i < strOctets.length; i++) octets[i] = Byte.parseByte(strOctets[i]); } public static boolean isValid(String addr) { String strOctets[] = addr.split(""\\.""); if (strOctets.length != 4) return false; for (int i = 0; i < strOctets.length; i++) { try { int num = Integer.parseInt(strOctets[i]); if (num < 0 || num > 255) return false; } catch (NumberFormatException e) { return false; } } return true; } public byte[] getOctets() { return octets; } } Exception: public class InvalidIPAddressException extends Exception { public InvalidIPAddressException(String addr) { super(""\"""" + addr + ""\"" is not a valid IP address""); } } Test case: public class IPAddressTest extends TestCase { @Test(expected=InvalidIPAddressException.class) public void testNullParameter() throws InvalidIPAddressException { @SuppressWarnings(""unused"") IPAddress addr = new IPAddress(null); fail(""InvalidIPAddressException not thrown.""); } @Test(expected=InvalidIPAddressException.class) public void testHostnameParameter() throws InvalidIPAddressException { @SuppressWarnings(""unused"") IPAddress addr = new IPAddress(""http://www.google.com""); fail(""InvalidIPAddressException not thrown.""); } @Test public void testValidIPAddress() throws InvalidIPAddressException { IPAddress addr = new IPAddress(""127.0.0.1""); byte[] octets = addr.getOctets(); assertTrue(octets[0] == 127); assertTrue(octets[1] == 0); assertTrue(octets[2] == 0); assertTrue(octets[3] == 1); } public static void main(String[] args) { TestRunner.run(IPAddressTest.class); } } It looks like you're mixing up JUnit 3 and 4 code. TestRunner is part of JUnit 3 so doesn't look at the annotations at all. Try removing extends TestCase and changing TestRunner.run(IPAddressTest.class); to JUnitCore.runClasses(IPAddressTest.class); (you'll need to add the imports as required). That fixed it. Thanks! Also if you're using eclipse you can use the built in testrunner - right click on the test class in package explorer and choose Run As -> JUnit test. The JUnitCore.runClasses method doesn't actually give any output so this may be a better way.  Likely because when you try to instantiate the InvalidIPAddressException the constructor throw a NullPointerException when it tries to concatenate the string in it. public InvalidIPAddressException(String addr) { super(""\"""" + addr + ""\"" is not a valid IP address""); // NPE while concatenate }"
1029,A,"Android JUnit Testing ... How to Expect an Exception Im attempting to write some tests using the built in android Junit testing framework. I am running into a problem with a test where I am expecting an exception to be thrown. In JUnit the annotation for the test method would be: @Test(expected = ArithmeticException.class) However in android this test fails with an ArithmeticException. I understand that the Android implementation is only a subset of JUnit 3 and doesnt even allow the annotation @Test (must be @SmallTest @MediumTest or @LargeTest and none of those allow for the 'expected=..' parameter) but this seems like a fairly significant test and seems like the android testing framework would be seriously lacking if it did not have this feature. Note : I tested this by adding the JUnit jar to the project and by adding and the annotations to my test methods. It makes sense to me why the annotations would be completely ignored because the android framework (runner?) is not looking for that annotation and just ignores it. Basically I'm just looking for the 'right' way to do this within the framework. The standard junit 3 idiom for this sort of test was: public void testThatMethodThrowsException() { try { doSomethingThatShouldThrow(); Assert.fail(""Should have thrown Arithmetic exception""); } catch(ArithmeticException e) { //success } }"
1030,A,"How do I test exceptions in a parameterized test? In JUnit4 you can write parameterized unit tests by providing parameters collection in one method which will be passed to the constructor of the test and testing in another method. If I have a parameter for which I expect an exception to be thrown how do I specify that? Gabriel please look at TestWatcher rule (since JUnit 4.9). Here is the sample code quoted from http://junit-team.github.io/junit/javadoc/4.11/org/junit/rules/TestWatcher.html: public static class WatchmanTest { private static String watchedLog; @Rule public TestWatcher watchman= new TestWatcher() { @Override protected void failed(Throwable e Description description) { watchedLog+= description + ""\n""; } @Override protected void succeeded(Description description) { watchedLog+= description + "" "" + ""success!\n""; } }; @Test public void fails() { fail(); } @Test public void succeeds() { } } Another approach would be to use http://junit-team.github.io/junit/javadoc/4.11/org/junit/rules/ErrorCollector.html from JUnit 4.7: @Rule public ExpectedException thrown = ExpectedException.none(); @Test public void testCollectingErrors() { thrown.handleAssertionErrors(); thrown.expect(MultipleFailureException.class); // or #expectMessage()/#expectCause() collector.checkThat(""a"" equalTo(""b"")); //... }  if (parameter == EXCEPTION_EXPECTED) { try { method(parameter); fail(""didn't throw an exception!""); } catch (ExpectedException ee) { // Test succeded! } } I hoped for a more reasonable solution - e.g. something like multiple @Parameters methods with optional expected annotation argument but I guess something like that is not a part of the framework. Thank you anyway.  In contrast to what other suggest I would not introduce any kind of logic to tests - even simple ifs! What you should have are two testing methods: first one takes valid parameters (and expects some output) second takes invalid parameters (and expects exceptions) Not sure if JUnit with its constructor-based parametrized testing is able to do this. Probably you would have to create two test classes for this. Go with JUnit Params or TestNG which offer much more convenient solution.  If you used catch-exception instead of the corresponding annotations and rules of JUnit4 then your code would look like this: catchException(obj).method(parameter); if (parameter != EXCEPTION_EXPECTED) { assert caughtException() instanceof ExpectedException; } // more assertions"
1031,A,"Passing JUnit command line parameters in eclipse I have recently been using junit in eclipse and I am still learning. I know how to pass command line parameters in eclipse but how do I pass them to a test case in Junit? Also how do I access them? You cannot pass command line arguments to the JUnit test because no main method is run. You will need to use system properties and access these in your test case. Select your test class in the Package Explorer. Right click and select Run As -> Open Run Dialog In the run dialog there is an Arguments tab where you can specify program and VM arguments. You should be able to enter your system property parameters here. Alternatively with the desired project as your current one from the main menu select Run -> Run Configurations to access the Arguments tab. It seems that with JUnit4 and Eclipse your only option is system properties. I have edited my answer to reflect this. But if I use eclipse I do not need a main method to run my test. How do I access them without setting them in system properties? This is another option: http://stackoverflow.com/questions/14820175/how-to-pass-an-argument-to-a-android-junit-test/14821971#14821971  I will skip passing as somebody has already replied with that. To access you use: System.getProperty(""propert.name.here""); (returns String) Not only does this just bloat the code but it doesn't even address the actual question. (Who voted this up anyway?) This snippet of code shows how to get at the system property that @Mark suggested be used as a way to pass an argument from the Eclipse Junit launch configuration into the JUnit test. In my case I'm interested in doing that with a port number for example.  Probably you have figured this out but when compiled and if using ANT or MVN you can pass arguments to your JUNIT or TestNG from inside the POM.XML file.  <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.4.3</version> <configuration> <forkMode>${test.junit.forkMode}</forkMode> <skip>${test.junit.skip}</skip> <argLine>${test.junit.argLine}</argLine> <jvm>${jdk.compiler.path}/binjava</jvm> </configuration> </plugin>"
1032,A,"setUp/tearDown (@Before/@After) why we need them in JUnit? I believe that we are all know that setUp (@Before) will execute before any test method and tearDown(@After) will execute after test method. Also we know that Junit will create one instance of Test per test method. my question is that can we just move setUp method content to class Constructor and remove setUp method? is there any specific reason to keep setUp method? I think some reason should like the following: If you move @Before contents to Constructor That's fine but the @After contents where you get to move? The differences of Constructor and @Before/@After is that Constructor should be used to instance some for class @Before/@After is for preparing test case resources.  This (old) JUnit best practices article puts it like this: Do not use the test-case constructor to set up a test case Setting up a test case in the constructor is not a good idea. Consider: public class SomeTest extends TestCase public SomeTest (String testName) { super (testName); // Perform test set-up } } Imagine that while performing the setup the setup code throws an IllegalStateException. In response JUnit would throw an AssertionFailedError indicating that the test case could not be instantiated. Here is an example of the resulting stack trace: junit.framework.AssertionFailedError: Cannot instantiate test case: test1 at junit.framework.Assert.fail(Assert.java:143) at junit.framework.TestSuite.runTest(TestSuite.java:178) at junit.framework.TestCase.runBare(TestCase.java:129) at junit.framework.TestResult.protect(TestResult.java:100) at junit.framework.TestResult.runProtected(TestResult.java:117) at junit.framework.TestResult.run(TestResult.java:103) at junit.framework.TestCase.run(TestCase.java:120) at junit.framework.TestSuite.run(TestSuite.java Compiled Code) at junit.ui.TestRunner2.run(TestRunner.java:429) This stack trace proves rather uninformative; it only indicates that the test case could not be instantiated. It doesn't detail the original error's location or place of origin. This lack of information makes it hard to deduce the exception's underlying cause. Instead of setting up the data in the constructor perform test setup by overriding setUp(). Any exception thrown within setUp() is reported correctly. Compare this stack trace with the previous example: java.lang.IllegalStateException: Oops at bp.DTC.setUp(DTC.java:34) at junit.framework.TestCase.runBare(TestCase.java:127) at junit.framework.TestResult.protect(TestResult.java:100) at junit.framework.TestResult.runProtected(TestResult.java:117) at junit.framework.TestResult.run(TestResult.java:103) ... This stack trace is much more informative; it shows which exception was thrown (IllegalStateException) and from where. That makes it far easier to explain the test setup's failure. @Pascal: Thanks yes I moved my initialization code to setup() and the error message changed from: ""junit.framework.AssertionFailedError: Cannot instantiate test case"" to: ""Error creating bean with name 'transactionManager' defined in class path resource [net/ezswitch/registration/ComponentContext.xml]: Cannot resolve reference to bean 'sessionFactory' while setting bean property 'sessionFactory'""  Here are 3 good reasons why. In summary: Some situations may prefer to defer setting up test fixtures as long as possible to just before the test case executes. Some test cases may be part of a deep test case inheritance hierarchy. It may be preferable to defer setting up test fixtures until the full hierarchy of constructors has completed. You get better diagnostics if setup code fails in setUp() rather than if it fails in the constructor. 1. Defer setting up fixtures until just before test case Design for Usability http://www.artima.com/weblogs/viewpost.jsp?thread=70189 ... And as Elliotte Rusty Harold put it if you're going to create a new TestCase instance for each test method ""why the hell bother with a setUp() method?"" You can just use the TestCase constructor. I've heard Bruce Eckel point out that there is one subtle difference between creating your fixture in setUp() versus creating it in the TestCase constructor. JUnit creates all the TestCase instances up front and then for each instance calls setup() the test method and tearDown(). In other words the subtle difference is that constructors are all invoked in batch up front whereas the setUp() method is called right before each test method. But this seems to be not that useful a difference in practice. 2. Defer setting up fixtures until after all test cases are instantiated ETutorial's Java Extreme Programming - 4.6 Set Up and Tear Down http://etutorials.org/Programming/Java+extreme+programming/Chapter+4.+JUnit/4.6+Set+Up+and+Tear+Down/ You may be wondering why you should write a setUp( ) method instead of simply initializing fields in a test case's constructor. After all since a new instance of the test case is created for each of its test methods the constructor is always called before setUp( ). In a vast majority of cases you can use the constructor instead of setUp( ) without any side effects. In cases where your test case is part of a deeper inheritance hierarchy you may wish to postpone object initialization until instances of derived [test] classes are fully constructed. This is a good technical reason why you might want to use setUp( ) instead of a constructor for initialization. Using setUp( ) and tearDown( ) is also good for documentation purposes simply because it may make the code easier to read. 3. Better diagnostics in case of setup failure JUnit best practices (JavaWorld) http://www.javaworld.com/jw-12-2000/jw-1221-junit.html Setting up a test case in the constructor is not a good idea. ... Imagine [in code where setup is done in the test case constructor] that while performing the setup the setup code throws an IllegalStateException. In response JUnit would throw an AssertionFailedError indicating that the test case could not be instantiated. ... This stack trace [of an exception thrown in setup code in the test case constructor] proves rather uninformative; it only indicates that the test case could not be instantiated. Instead of setting up the data in the constructor perform test setup by overriding setUp(). Any exception thrown within setUp() is reported correctly. ... This stack trace [of an exception thrown in setUp() method instead of the test case constructor] is much more informative; it shows which exception was thrown (IllegalStateException) and from where. That makes it far easier to explain the test setup's failure. I do not quite agree with the statement: ""In cases where your test case is part of a deeper inheritance hierarchy you may wish to postpone object initialization until instances of derived [test] classes are fully constructed."" I don't agree with the statement: ""In cases where your test case is part of a deeper inheritance hierarchy ... derived [test] classes are fully constructed."" Is this not the same as saying that in cases where a class is part of a deeper inheritance hierarchy we may want to postpone object initialization until fields of derived classes are fully constructed? i.e. Replace ""class"" for ""test case"" and it sounds ridiculous. The constructor of a super class is always called first. It doesn't know anything about the fields/methods of derived classes. Can't a superclass constructor do anything?  A custom runner such as SpringJUnit4ClassRunner may need to run some codes between the constructor and @Before method. In this case the runner may inject some dependency which the @Before methods needs. But dependency injection can only be run after the object is constructed.  The reason you need this is that for many tests you often need to initialize state before each test so that the tests can all make assumptions about the start state they're running in. Suppose your test class wraps say database access. After each test you'd want to remove whatever changes your tests have made to the db - if you didn't do that each test runs against a slightly modified database. Additionally any given test may see a different set of changes if some subset of the previous tests have failed. For example suppose test1 does an insert test2 checks that you're accurately reading the table size. Day 1 test1 fails and 0 is correct. Day 2 test1 succeeds and 1 is correct? BTW junit also supports @BeforeClass if you want to do a global setup and setup and teardowns are optional. I don't think the op is disputing the fact that setup code is useful. Rather the question is why don't we just use the class constructor to do what `setUp` does question.  At work we've discovered something rather interesting which answers your question. When you run a test suite especially a large set of tests (200+) JUnit starts to use a LOT of memory this is because ALL the tests are instanced before any actual test method is run. We ran into a ""memory leak"" because of this because we used Spring to wire in some JPA EntiryManager objects for our database tests this became A LOT of objects and a lot of memory and about half way through the tests we were getting OutOfMemory exceptions. IMHO best practise is to use setUp and tearDown to inject your dependencies and null out any and all class references this will make your tests run faster and save you a lot of head ache! Hope you learn from our mistakes :) good point. thanks for sharing."
1033,A,"Junit to test Java EE Application Client I've created an Java EE application on Glassfish and it's deployed and run successfully. However when I try to use junit to test the Application Client module It says javax.naming.NoInitialContextException: Need to specify class name in environment or system property or as an applet parameter or in an application resource file: java.naming.factory.initial I tried with many ways: Hashtable env=new Hashtable(); env.put(""java.naming.factory.initial""""com.sun.enterprise.naming.SerialInitContextFactory""); Context c = new InitialContext(env); Object remote = c.lookup(""java:comp/env/BookStore""); and it says cant not find class com.sun.enterprise.naming.SerialInitContextFactory I'm looking for this issue for 2 days but I've got no hope :( Thanks in advance I have to assume that you seen the EJB FAQ for Glassfish. It answers the multitude of questions folks have on how to connect applications to Glassfish. You also don't mention which version of GF you're using and that can be important since they use different jars and such. I'm using glassfish 3. When I set the J2EE Application as main project and run it with F6. It's fine. However I want to run the Junit test package instead. It says can not find jndi. This is obvious because maybe It ignore some steps to deploy the app. But I dont know how to fix it to run junit  Please ensure jar file containing com.sun.enterprise.naming.SerialInitContextFactory is in your classpath. The jar is provided by Glassfish."
1034,A,"service layer testing in spring mvc using easymock Service Interface: public List<UserAccount> getUserAccounts(); public List<UserAccount> getUserAccounts(ResultsetOptions resultsetOptions List<SortOption> sortOptions); Service Implementation: public List<UserAccount> getUserAccounts() { return getUserAccounts(null null); } public List<UserAccount> getUserAccounts(ResultsetOptions resultsetOptions List<SortOption> sortOptions) { return getUserAccountDAO().getUserAccounts(resultsetOptions sortOptions); } How can I test this using easymock or any other viable testing methodology? sample code will be appreciated. For the easy mock passing objects as parameters very confusing. Some one clearly explain whats the best approach to test the service layer? testing service interface will be considered as unit test or integration test? Asking question on Friday evening not a good idea since no one looks this in weekend and in Monday they will be busy with new questions. I'm sure some testing gurus surely know the answer and explanation for this and will help me out. Here you go assuming you are using JUnit 4 with annotations: import static org.easymock.EasyMock.createStrictMock; import static org.easymock.EasyMock.expect; import static org.easymock.EasyMock.replay; import static org.easymock.EasyMock.verify; public class UserAccountServiceTest private UserAccountServiceImpl service; private UserAccountDAO mockDao; /** * setUp overrides the default We will use it to instantiate our required * objects so that we get a clean copy for each test. */ @Before public void setUp() { service = new UserAccountServiceImpl(); mockDao = createStrictMock(UserAccountDAO.class); service.setUserAccountDAO(mockDao); } /** * This method will test the ""rosy"" scenario of passing a valid * arguments and retrieveing the useraccounts. */ @Test public void testGetUserAccounts() { // fill in the values that you may want to return as results List<UserAccount> results = new User(); /* You may decide to pass the real objects for ResultsetOptions & SortOptions */ expect(mockDao.getUserAccounts(null null) .andReturn(results); replay(mockDao); assertNotNull(service.getUserAccounts(null null)); verify(mockDao); } } You might also find this article useful especially if you are using JUnit 3. Refer this for a quick help on JUnit 4. Hope that helps. For a simple getter it may not - however - if you have conditions and if/else path in the service method then it would. One other thing the whole purpose of unit testing is to unit test a single unit (Service in this case) not Service and DAO (That is called integration testing).As to passing null I chose the simplest test. But as put in the comment you should create those objects (ResultSetOptions SortOptions) too. If they are not ready (i.e. only interfaces and no implementation) then you may have to use mock implementation for them too. @kneethan - it would also be helpful if you upvote - so that right ansewr/content moves up. Thanks. Thanks for your response so whenever we encounter some object parameters which are not relevant to unit test then we make them as null in the expect call ? Is it really make sense to write unit test case for these two methods? for me its nothing but delegating to DAO layer still we need to test? You could also choose to not mock the DAO but the layer underneath the DAO to make the tests more flexible: http://www.baeldung.com/2011/10/02/testing-the-service-layer/"
1035,A,"writing junit test to cover cross version serialization does any of you have any info/idea on how can I test my application for cross version compatibility. I am testing these object's serialization in current ""HEAD"" version. but I need to wait for my integration testing to see if I broke anything cross version. I am guessing I will need to store the objects serialized from last released version somewhere on my disk I can imagine how this kind of system might work but I wonder if someone already did it. While looking for ways to unit test serialization I stumbled on this question and on interesting blog posts on the same subject. I'll post the links here in case others find it useful... Here is Bob Lee's (Google Guice creator) take on unit testing serialization evolution: http://blog.crazybob.org/2006/01/unit-testing-serialization-evolution.html http://blog.crazybob.org/2006/01/unit-testing-serialization-evolution_13.html  Never did it but I'd use the following approach: Have both versions of the class under test NOT in the normal classpath. Use a custom classloader to load one version serialize one or more instances into a String or to a file or whatever use another instance of a custom classloader to load the other version. deserialize the stuff. assert all the required properties. For checking out the two versions I'd consider ant. ADD ONS FOR INTERFACING WITH svn exist. Probably for other systems aswell. Also a CI system might help (teamcity does). Sorry I don't get your second point. Sounds good some issues that you might be able to find creative soutions also: 1. junit tests will work only if both projects are checkedout in my workspace and in s specific location 2. I will need to write some mapping from project and its predecessor"
1036,A,JUnit assertArrayEquals I've written a method that returns an int[]. I want to create a test that ensures that when I run my method passing say 10 to the method that it returns an int[]{01357}. How can I do this? Maybe I'm just missing the proper syntax or should be using a different assert method...? If you are using JUnit4 you could use the org.junit.Assert class: Assert.assertArrayEquals(new int[]{01357} someMethod(10));
1037,A,"Hibernate SessionFactory creation failure when running JUnit tests only I'm using Eclipse and Hibernate to develope a J2EE web app. When running/debugging the web app my hibernate-related code works just fine. But if I run a junit test that tests some of my hibernate-related code I get an ""Initial SessionFactory creation failed"" error. The specific error message is as follows: Initial SessionFactory creation failed.java.lang.NoSuchMethodError: org.hibernate.persister.collection.OneToManyPersister.(Lorg/hibernate/mapping/Collection;Lorg/hibernate/cache/access/CollectionRegionAccessStrategy;Lorg/hibernate/cfg/Configuration;Lorg/hibernate/engine/SessionFactoryImplementor;)V Any comments or solutions are greatly appreciated! It would be good to show how you're initializing the sessionFactory for your test. Sounds like you are running your unit tests against a different version of the Hibernate library than you are compiling it against. I think you're right. I will check the classpath more carefully."
1038,A,"JUnit AntClassNotFoundException I'm getting ClassNotFoundException when running ""ant test"" target. All path and libraries exists including junit-4.8.1.jar located in lib folder. <project name=""progs"" default=""test"" basedir="".""> <!--++++++++++ Properties ++++++++++--> <property name=""src-main"" location=""PC_Serie_3""/> <property name=""src-test"" location=""PC_Serie_3_Tests""/> <property name=""target"" location=""target""/> <property name=""target-classes"" location=""target/PC_Serie_3""/> <property name=""target-test-classes"" location=""target/PC_Serie_3_Tests""/> <property name=""target-test-reports"" location=""target/test-reports""/> <path id=""test.extlibs.class.path""> <fileset dir=""lib""> <include name=""**/*.jar"" /> </fileset> </path> <!--++++++++++ Targets ++++++++++--> <target name=""init"" description =""Creates the target folders""> <mkdir dir=""${target-classes}""/> <mkdir dir=""${target-test-classes}""/> <mkdir dir=""${target-test-reports}""/> </target> <target name=""clean"" description=""Removes the target folders"" > <delete includeEmptyDirs=""true"" failonerror=""false"" verbose=""true"" > <fileset dir=""${target}"" defaultexcludes=""false""/> </delete> </target> <target name=""compile-main"" depends=""init"" description=""Compiles the main source"" > <javac debug=""true"" srcdir=""${src-main}"" destdir=""${target-classes}"" includeantruntime=""false""> </javac> </target> <target name=""compile-test"" depends=""compile-main"" description=""Compiles the test source"" > <javac debug=""true"" debugLevel=""source"" srcdir=""${src-test}"" destdir=""${target-test-classes}"" includeantruntime=""true""> <classpath> <pathelement location=""${target-classes}""/> <path refid=""test.extlibs.class.path"" /> </classpath> </javac> </target> <target name=""test"" depends=""compile-test"" description=""Runs the tests""> <echo>Running the junit tests...</echo> <junit printsummary=""yes"" haltonfailure=""true"" showoutput=""true"" > <classpath> <pathelement location=""${src-main}""/> <pathelement location=""${target-classes}""/> <pathelement location=""${target-test-classes}""/> <path refid=""test.extlibs.class.path"" /> </classpath> <batchtest fork=""yes"" todir=""${target-test-reports}"" > <fileset dir=""${src-test}""> <include name=""**/*Test*.java""/> </fileset> <formatter type=""xml""/> <formatter type=""plain"" usefile=""false"" /> </batchtest> </junit> </target> <target name=""package"" depends=""test"" description=""Packages the main classes into a jar"" > <buildnumber /> <jar jarfile=""${target}/library.jar"" basedir=""${target-classes}""/> </target> <!-- USAGE: ant deploy-app --> <target name=""deploy-lib"" depends=""package""> </target> </project> Console Output: test:  [echo] Running the junit tests... [junit] Running src.DocumentDBTests.DocumentDBv1Test [junit] Testsuite: src.DocumentDBTests.DocumentDBv1Test [junit] Tests run: 1 Failures: 0 Errors: 1 Time elapsed: 0 sec [junit] Tests run: 1 Failures: 0 Errors: 1 Time elapsed: 0 sec [junit] [junit] Caused an ERROR [junit] src.DocumentDBTests.DocumentDBv1Test [junit] java.lang.ClassNotFoundException: src.DocumentDBTests.DocumentDBv1Te st [junit] at java.net.URLClassLoader$1.run(URLClassLoader.java:202) [junit] at java.security.AccessController.doPrivileged(Native Method) [junit] at java.net.URLClassLoader.findClass(URLClassLoader.java:190) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:307) [junit] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:248) [junit] at java.lang.Class.forName0(Native Method) [junit] at java.lang.Class.forName(Class.java:169) [junit] I have tried with fork=""no"" but problem remains. I think the problem is that you are pointing junit to the source files and not the complied class files. Also the fileset is including *Test*.java when it should be including *Test*.class Try replacing your batchtest element with this: <batchtest fork=""yes"" todir=""${target-test-reports}"" > <fileset dir=""${target-test-classes}""> <include name=""**/*Test*.class""/> </fileset> <formatter type=""xml""/> <formatter type=""plain"" usefile=""false"" /> </batchtest> You were right.Thanks for your help this has been driving me mad all day!"
1039,A,"junit and ant issue. Cannot start test When I am running a junit test from ant I always get: D:\metrike>ant test Buildfile: build.xml init: compile: test: [junit] Running jmt.test.TestCodeBase [junit] Testsuite: jmt.test.TestCodeBase [junit] Tests run: 1 Failures: 1 Errors: 0 Time elapsed: 0046 sec [junit] Tests run: 1 Failures: 1 Errors: 0 Time elapsed: 0046 sec [junit] [junit] Testcase: warning(junit.framework.TestSuite$1): FAILED [junit] No tests found in jmt.test.TestCodeBase [junit] junit.framework.AssertionFailedError: No tests found in jmt.test.TestCodeBase [junit] [junit] [junit] Test jmt.test.TestCodeBase FAILED This is the ant file: <target name=""test"" depends=""compile""> <mkdir dir=""target/test-results""/> <junit haltonfailure=""no"" printsummary=""on""> <classpath > <pathelement location=""target/classes""/> <pathelement location=""Libraries/junit3.8.1/junit.jar""/> </classpath> <formatter type=""brief"" usefile=""false""/> <formatter type=""xml"" /> <batchtest todir=""target/test-results"" > <fileset dir=""target/classes"" includes=""**/TestCodeBase.class""/> </batchtest> </junit> </target> But when I manually run the test junit test works: D:\metrike>cd target D:\metrike\target>cd classes D:\metrike\target\classes>java jmt.test.TestCodeBase fatsource.jar eclapsed : 2297 ms over all : 2297 ms contains 3073 classes and 3700 referred classes 35968 referred methods 22351 referred fields Memory usage: 21326 KB Post gc-memory usage: 19506 KB contains 3073 classes and 3700 referred classes 35968 referred methods 22351 referred fields Can someone please tell me what I am doing wrong? I have been trying to fix this for a whole day but I cannot find the solution. Please post sample code of jmt.test.TestCodeBase esp. class definition and one of the test methods. I looks like you use public static int main() instead of JUnit convention public void testFoo() methods. If you're using JUnit4 your test methods should have the @Test annotation. JUnit tests usually cannot be run just with java <Testclass> thx for help. Forget to extend TestCase  It seems your test class is not actually a JUnit test class. When you run it manually you are not running it as a test but as a regular Java application. The class has a main method right? To run as a JUnit 3 (which you seem to be using) test the class needs to extend TestCase and have one or more public void methods whose names start with 'test'. For testing I would try running the class as a JUnit test in the IDE. thx for help. Forget to extend TestCase  1) Does jmt.test.TestCodeBase extend TestCase (junit.framework.TestCase)? If not it will need to to be picked up by the junit ant task. 2) Is the class written as a junit TestCase or is it just called from the main method? See this link for an example of writing simple tests in Junit3 style. For Junit4 just add @Test above the methods. 3) Are the test methods in Junit3 style (every method starts with test) or Junit4 style (every method has a @Test above it)? If Junit3 you should be good to go. If Junit4 you need to include the Junit4 test library in your ant classpath rather than using junit3.8.1. thx for help. Forget to extend TestCase"
1040,A,"How do I test with DBUnit with plain JDBC and HSQLDB without facing a NoSuchTableException? I am trying to use DBUnit with plain JDBC and HSQLDB and can't quite get it to work -- even though I've used DBUnit with Hibernate earlier with great success. Here's the code: import java.sql.PreparedStatement; import org.dbunit.IDatabaseTester; import org.dbunit.JdbcDatabaseTester; import org.dbunit.dataset.IDataSet; import org.dbunit.dataset.xml.XmlDataSet; import org.junit.Test; public class DummyTest { @Test public void testDBUnit() throws Exception { IDatabaseTester databaseTester = new JdbcDatabaseTester(""org.hsqldb.jdbcDriver"" ""jdbc:hsqldb:mem"" ""sa"" """"); IDataSet dataSet = new XmlDataSet(getClass().getResourceAsStream(""dataset.xml"")); databaseTester.setDataSet(dataSet); databaseTester.onSetup(); PreparedStatement pst = databaseTester.getConnection().getConnection().prepareStatement(""select * from mytable""); } } And this is the dataset.xml in question: <dataset> <table name=""mytable""> <column>itemnumber</column> <column>something</column> <column>other</column> <row> <value>1234abcd</value> <value>something1</value> <value>else1</value> </row> </table> </dataset> This test gives me a NoSuchTableException: org.dbunit.dataset.NoSuchTableException: mytable at org.dbunit.database.DatabaseDataSet.getTableMetaData(DatabaseDataSet.java:282) at org.dbunit.operation.DeleteAllOperation.execute(DeleteAllOperation.java:109) at org.dbunit.operation.CompositeOperation.execute(CompositeOperation.java:79) at org.dbunit.AbstractDatabaseTester.executeOperation(AbstractDatabaseTester.java:190) at org.dbunit.AbstractDatabaseTester.onSetup(AbstractDatabaseTester.java:103) at DummyTest.testDBUnit(DummyTest.java:18) If I remove the databaseTester.onSetup() line I get an SQLException instead: java.sql.SQLException: Table not found in statement [select * from mytable] at org.hsqldb.jdbc.Util.throwError(Unknown Source) at org.hsqldb.jdbc.jdbcPreparedStatement.<init>(Unknown Source) at org.hsqldb.jdbc.jdbcConnection.prepareStatement(Unknown Source) at DummyTest.testDBUnit(DummyTest.java:19) The dataset in itself is working since I can access it like it should: ITable table = dataSet.getTable(""mytable""); String firstCol = table.getTableMetaData().getColumns()[0]; String tName = table.getTableMetaData().getTableName(); What am I missing here? EDIT: As @mlk points out DBUnit doesn't create tables. If I insert the following before adding the dataset everything goes smoothly: PreparedStatement pp = databaseTester.getConnection().getConnection().prepareStatement( ""create table mytable ( itemnumber varchar(255) NOT NULL primary key "" + "" something varchar(255) other varchar(255) )""); pp.executeUpdate(); I posted a followup question as Is there any way for DBUnit to automatically create tables from a dataset or dtd? dbUnit does not create tables. Nor could it with the limited information given in the XML file. Hibernate I believe can create the tables. This is one of the reasons I stopped using in-memory databases and instead got the DBA to give each developer their own database. Every developer then keeps the database up to date using the same scripts which are later ran on live. This adds a small overhead (all developers need to keep their databases up to date) but means you don't need to mess about building the database for each run and you can be sure that the queries ran in test work in live. The second reason was speed. I found creating the in memory-database took a lot longer than simply connecting to an existing database. The third reason was the tear down is none-destructive (start up wipes the database). This means I can run the SQL under test on the database to help work out why a test is failing. I have now switched to a local Oracle XE instance running in a VM. The reason for this is that we can continue developing when not connected to the internal network. With an in-memory database I can run unit tests anywhere without having to switch up any configuration and without having to start up a DB server. Primarily they run on various dev boxes and on the CI server. That's a huge advantage in my book. Yes it is. I personally found it to be significantly slower however this may have changed now. The time taken to start a local VM'ed Oracle XE instance is a short once-per-day thing.  In case you do create your tables upfront like suggested here and still get a NoSuchTableException then there is something wrong with the schema. Before you now turn crazy fiddling with it in all sorts of weird and wonderful ways try setting the schema parameter to PUBLIC when you create the IDatabaseConnection like so: IDatabaseConnection databaseConnection = new HsqldbConnection(sqlConnection ""PUBLIC""); It took me some stepping through the DbUnit code with the debugger but this seems to do the trick."
1041,A,Hibernate Unit tests - Reset schema I'm testing the CRUD operations of my DAOs in JUnit tests. When i execute the single test Hibernate always resets the schema and populates the DB in a known state. But when i execute multiple tests in a row Hibernate resets the schema once and then the data is accumulated during the execution of the tests. This is an unexpected behavior so I'd like to add in the @Before method of the tests a function that explicitly resets the schema to avoid the pesistence of side data created by previous tests during the execution chain. Any tips? Thanks First of all what you're doing is not unit testing it's integration testiong. Tips: Use transactions and roll the back. Consider using DBUnit (not sure how helpful it would be in your case). Here's some relevant code from org.springframework.orm.hibernate3.LocalSessionFactoryBean. ... Connection con = session.connection(); Dialect dialect = Dialect.getDialect(getConfiguration().getProperties()); String[] sql = getConfiguration().generateSchemaCreationScript(dialect); executeSchemaScript(con sql);  You could begin a transaction before and rollback the transaction each @Test method something like this: public class HibernateIntegrationTest { protected static SessionFactory factory; protected Session session; @BeforeClass public static void createSessionFactory() { AnnotationConfiguration config = new AnnotationConfiguration(); // ... config.configure(); factory = config.buildSessionFactory(); } @AfterClass public static void closeSessionFactory() { if (factory != null) { factory.close(); } } @Before public void beginTransaction() { session = factory.getCurrentSession(); session.beginTransaction(); } @After public void rollbackTransaction() { if (session.getTransaction().isActive()) { session.getTransaction().rollback(); } if (session.isOpen()) { session.close(); } } } And extend this class in your tests classes. public class DemoTest extends HibernateIntegrationTest { @Test public void createMyEntity() { MyEntity e = new MyEntity(); //... session.save(e); assertNotNull(e.getId()); } } Note that this is not the cleanest way to do this the code above is more for demonstration purposes. Shouldn't most code be using transactions already? If so and if (as I understand) you can't generally nest transactions--how do you use something like this?
1042,A,"Choose order to execute JUnit tests I wanted to choose the order to execute the JUnit tests. I have 4 classes with several test methods in it my goal is to execute for instance method Y of class A then method X from class B and finally method Z from class A. Would you help please? If the previous answer is not satisfying I have noticed with the Sun JVM JUnit always seems to execute unit tests in the order of which they are defined. Obviously this is not a good idea to rely on this. Just FYI for future readers this seems to be the case with JVMs before Java 7 but since Java 7 the order is non-deterministically random. But JUnit 4.11+ has limited ability to define the order using the FixMethodOrder annotation.  The JUnit answer to that question is to create one test method like this:  @Test public void testAll() { classA.y(); classB.x(); classA.z(); } That is obviously an unsatisfying answer in certain cases (where setup and teardown matter) but the JUnit view of unit testing is that if tests are not independant you are doing something wrong. If the above doesn't meet your needs have a look at TestNG.  Create a TestSuite and call the test methods in the desired order. @Yishai is right in that JUnit is designed so each test is independent. So if you are calling test methods that can be run independently then there should be no problem with creating a TestSuite to cover a scenario for a specific calling-order.  In general you can't specify the order that separate unit tests run in (though you could specify priorities in TestNG and have a different priority for each test). However unit tests should be able to be run in isolation so the order of the tests should not matter. This is a bad practice. If you need the tests to be in a specific order you should be rethinking your design. If you post specifics as to why you need the order I'm sure we can offer suggestions. +1 although I don't think this advice is worded quite strongly enough. Unit tests should be able to run in isolation; that's fundamental. If yours aren't you're doing something wrong. Ok... I'm new at this... So the thing is: I'm using OpenKM (a Document Management System) and I have a class that tests Folders Manipulation i.e. tests the creation renaming and deletion a folder. So you're telling me that the creation renaming and deletion must be all on the same method instead of being seperated and dependent on the previous method? Generally you would have a setup and tear down method that would put the folders in a known state. For example the setup method could create a folder (and tear down would delete it if it still exists). Your delete and rename tests could operate on that folder. The test for create could verify that the folder was created properly in the setup method or it could create an entirely new folder (or subfolder of the folder created in the setup method).  This might be interesting to you: JExample A different approach to testing with interdepentent tests.  From version 4.11 you can specify execution order using annotations and ordering by method name: import org.junit.Test; import org.junit.FixMethodOrder; import org.junit.runners.MethodSorters; @FixMethodOrder(MethodSorters.NAME_ASCENDING) public class MyTest { @Test public void test1Create() { System.out.println(""first""); } @Test public void test2Update() { System.out.println(""second""); } } See JUnit 4.11 Release Notes  The general remark/idea that testing can be done in any arbitrary order is too strong. It really depends on what you are testing. For example I am testing a server where we have a changePassword action. I think it is obvious that the order of tests is critical. After changePassword the old password does not work anymore and before it does. I don't want to revert the server state after each test too much work. I can do it one time after all tests have been completed."
1043,A,"Inject sql data in a Spring junit test with an annotation im using Junit with spring-test and i would like to have a classic transactionnal test with this annotation: @Injectdata(""classpath:src/test/mydata.sql"") @Test public void myTest throws Exception { // ... } This data will be injected with the jdbcspring template in the same transaction & those datas will be available for only this test. Actually im injecting data this way : @Test public void myTest throws Exception { jdbcTemplate.update(""my sql query); } I know that Unitils framework do the samething but with a dataset dbunit file. I have found the solution by creating one myself. First create the listener used by Spring test: public class InjectDataTestExecutionListener extends DependencyInjectionTestExecutionListener { private static JdbcTemplate jdbcTemplate; private static DataSource datasource ; private static String ENCODING=""UTF-8""; @Override /** * Execute un éventuel script SQL indiqué via l'annotation {@link SqlFileLocation} * avant l'execution d'un test. */ public void beforeTestMethod(TestContext testContext) throws Exception { super.beforeTestClass(testContext); Method MyMethdo = testContext.getTestMethod(); SqlFileLocation dsLocation = MyMethdo.getAnnotation(SqlFileLocation.class); if (dsLocation!=null){ executeSqlScript(testContextdsLocation.value()); } } /** * Execute un script sur un chemin d'accès au fichier. * @param testContext le context du test * @param sqlResourcePath le chemin du fichier Sql * @throws DataAccessException en cas d'erreur d'accès au fichier */ private void executeSqlScript(TestContext testContext String sqlResourcePath) throws DataAccessException { JdbcTemplate jdbcTemplate = getJdbCTemplate(getDatasource(testContext)); Resource resource = testContext.getApplicationContext().getResource(sqlResourcePath); executeSqlScript(jdbcTemplate new EncodedResource(resourceENCODING)); } private DataSource getDatasource(TestContext testContext) { if (datasource==null){ datasource = testContext.getApplicationContext().getBean(DataSource.class); } return datasource; } private JdbcTemplate getJdbCTemplate(DataSource datasource) { if (jdbcTemplate==null){ jdbcTemplate = new JdbcTemplate(datasource); } return jdbcTemplate; } /** * Execute une resource via un jdbcTemplate donné. * @throws DataAccessException enc as de pb d'acces au fichier. */ private static void executeSqlScript(JdbcTemplate simpleJdbcTemplate EncodedResource resource) throws DataAccessException { List<String> statements = new LinkedList<String>(); try { LineNumberReader lnr = new LineNumberReader(resource.getReader()); String script = JdbcTestUtils.readScript(lnr); char delimiter = ';'; if (!JdbcTestUtils.containsSqlScriptDelimiters(script delimiter)) { delimiter = '\n'; } JdbcTestUtils.splitSqlScript(script delimiter statements); for (String statement : statements) { try { simpleJdbcTemplate.update(statement); } catch (DataAccessException ex) { throw ex; } } } catch (IOException ex) { throw new DataAccessResourceFailureException(""Impossible d'ouvrir le script depuis "" + resource ex); } } } Than on your class test add: @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations={/* ... */}) @Transactionnal @TestExecutionListeners({DependencyInjectionTestExecutionListener.classDirtiesContextTestExecutionListener.classTransactionalTestExecutionListener.class InjectDataTestExecutionListener.class }) The TRICK was to add ALL listeners normally added automatically by Spring if you dont add listeners. Avoiding that lead to weird errors. This is not documented but ive found that without listener with a transactionnal spring test those 3 listeners are added automatically by Spring (thanks the debug mode!) And finally you cand us this cool annotation like this : @SqlFileLocation(""classpath:sql/myfil.sql"") @Test public void testGetAll() throws Exception {/*...*/} You can even use relative paths or absolute path. And naturally the insertion will be like others inserts automatically rollback at the end."
1044,A,JUnit: @Before only for some test methods? I have some common set up code that I've factored out to a method marked with @Before. However it is not necessary for all this code to run for every single test. Is there a way to mark it so the @Before method only runs before certain tests? Just move out the tests that don't need the setup code into a separate test class. If you have some other code common to the tests that would be helpful to keep move that out into a helper class. +1 in other words don't use @Before if you don't want the behavior of @Before :)  Or use TestNG. It gives you finer grained control over tests.
1045,A,Junit - program verification vs whitebox fuzzing? I understand that program verification is a branch of computer engineering - but that it's practical application to real world code bases is limited by combinatorial explosion. I also understand that as part of designing your software change for a modification to an existing Java framework it's helpful to think about whitebox boundary and blackbox tests for your algorithm in advance. (Some people call this hammock driven development - thinking before you code.) Assuming you take this thinking and embed it in junit style tests I'm assuming that the Computer Science name for the contents is strictly 'whitebox testing/fuzzing' and not sufficient to comprise 'program verification'. So my question is - junit tests - whitebox fuzzing or program verification? Program verification is done proving mathematical properties on a mathematical model which is related to your application (it can be derived from the formal semantic of the programming language or by hand like writing behavioral types that models your web service). Take a look at pi-calculus to understand what I mean. Of course junit has nothing to do with formal program verification.
1046,A,"JUnit @Rule to pass parameter to test I'd like to create @Rule to be able to do something like this @Test public void testValidationDefault(int i) throws Throwable {..} Where i is parameter passed to the test by @Rule. However I do get java.lang.Exception: Method testValidationDefault should have no parameters is there any way to bypass it and set the i parameter in the @Rule? Is `@Rule` a JUnit provided annotation? Where is it supposed to be put? yes it is..... it can't be done you can't pass parameters to test method even using @Rule. See my response for a way to pass parameters albeit without using `@Rule`.  It should be noted that it is no longer true that you can't pass parameters directly to a test method. It can now be done using Theories and @DataPoints/@DataPoint. For example: @RunWith(Theories.class) public class TestDataPoints { @DataPoints public static int [] data() { return new int [] {2 3 5 7}; } public int add(int a int b) { return a + b; } @Theory public void testTheory(int a int b) { System.out.println(String.format(""a=%d b=%d"" a b)); assertEquals(a+b add(a b)); } } Output:  a=2 b=2 a=2 b=3 a=2 b=5 a=2 b=7 a=3 b=2 a=3 b=3 a=3 b=5 a=3 b=7 a=5 b=2 a=5 b=3 a=5 b=5 a=5 b=7 a=7 b=2 a=7 b=3 a=7 b=5 a=7 b=7 With the test passing. This doesn't allow to have several DataPoints so this functionality is pretty useless in 99% of cases. Still much more complicated and cumbersome than DataProviders in TestNG. Though it looks to be pretty obvious that this feature is very helpful.. You can have multiple `@DataPoints`. If they are of different types then they get handled automatically if they are of the same type you can use a [parameter supplier](http://blog.schauderhaft.de/2010/02/07/junit-theories/). Slightly cumbersome to set up but relatively straightforward to use once you do so.  I use @Parameters and @RunWith(value = Parameterized.class) for passing values to tests. An example can be found here. I did not know about the @Rule annotation but after reading this post I think it serves another purpose than passing parameters to the tests: If in your test class you create a field pointing to an object implementing the MethodRule interface and you mark this to be processed as a rule by adding the @Rule implementation then JUnit will call back on your instance for every test it will run allowing you to add additional behavior around your test execution. An additonal example of how to use @Rule is here. I hope this helps. That last url you gave cannot be found.  As IAdapter said you can't pass an argument using Rules but you can do something similar. Implement a Rule that holds all your parameter values and evaluates the test once for every parameter value and offers the values through a method so the test can pull them from the rule. Consider a Rule like this (pseudo code): public class ParameterRule extends MethodRule{ private int parameterIndex = 0; private List<String> parameters; public ParameterRule(List<String> someParameters){ parameters = someParameters; } public String getParameter(){ return parameters.get(parameterIndex); } public Statement apply(Statement st ...){ return new Statement{ public void evaluate(){ for (int i = 0; i < parameters.size(); i++){ int parameterIndex = i; st.evaluate() } } } } } You should be able to use this in a Test like this:  public classs SomeTest{ @Rule ParameterRule rule = new ParameterRule(ArrayList<String>(""a""""b""""c"")); public void someTest(){ String s = rule.getParameter() // do some test based on s } }  recently i started zohhak project. it lets you write tests with parameters (but it's a runner not a rule): @TestWith({ ""25 USD 7"" ""38 GBP 2"" ""null 0"" }) public void testMethod(Money money int anotherParameter) { ... }"
1047,A,"Is there any handy code coverage tool to be used with JUnit? Is there handy code coverage tool to be used with JUnit? +1 good question! There are many and google is your friend. NoUnit Coverlipse (I would recommend this but its an eclipse plugin) Codecover  Cobetura http://cobertura.sourceforge.net/  I would imagine most code coverage tools can be used pretty easily with JUnit. Previously I've used EMMA which is good. If you're an Eclipse user there's an Eclipse plugin called EclEmma to integrate things. If you're using Eclipse EclEmma is definitely good and very simple to use. EclEmma is very nice but users should read the [FAQ about coverage of exceptions](http://www.eclemma.org/faq.html#trouble02) (in a nutshell exception code may show up as non-covered even whan it is actually executed). If you find an apparently non-covered block you can just run it in the debugger to see if it actually gets executed. @DavidHarkness: Not particularly - it's just a while since I've configured one directly. The build systems I use now are somewhat different and may or may not be using EMMA behind the scenes. By ""previously"" are you implying that you have a new favorite code coverage tool?  Here are my preferences in that order: EclEmma Clover And here is a bunch of open source tools and with comparison. Hope that helps."
1048,A,"GWT Junit - error='no compilation unit for that type was seen' I'm trying to run a GWT unit test in a sample app. I ran  cmd /c /java/gwt-windows-1.6.4/webAppCreator.cmd -out gwttasks com.gwttasks.GwtTasks Copied in junit-4.5.jar into a lib directory and added that to the classpath. Ran:  cmd /c /java/gwt-windows-1.6.4/junitCreator.cmd -junit lib/junit-4.5.jar -module com.gwttasks.GwtTasks -eclipse GwtTasks com.gwt tasks.unit.GwtJunit When I try to run any of the generated cmd file (such as GwtJunit-hosted.cmd) or any of the launch files I get the following error. All the web pages I've seen say to add the test source to the classpath but it's already there so that's not the problem. Anyone else seen this?  com.google.gwt.junit.JUnitFatalLaunchException: The test class 'com.gwttasks.unit.GwtJunit' was not found in module 'com.gwttasks.GwtTasks'; no compilation unit for that type was seen at com.google.gwt.junit.JUnitShell.checkTestClassInCurrentModule(JUnitShell.java:390) at com.google.gwt.junit.JUnitShell.runTestImpl(JUnitShell.java:626) ... The answer could be found here : http://raibledesigns.com/rd/entry/testing_gwt_applications  I just want to add that I had the same problem because I did the (very silly) mistake to not put the GWTTestCase class into the ""client"" directory but into another one. No wonder it wasn't found ;)  In netbeans I added the src/java and test to the class path and debugging worked!!!!! THANKS"
1049,A,JTestcase for data in xml files with JUnit? I looked upon some years old code which uses JTestcase for separating data from Test case(JTestCase basically helps to manage data in xml files). So JTestCase is integrated with JUnit test cases code. Jar available for JTestcase is 2006 version so i guess there is no new release for same. This makes me think that probably JTestCase is old thing to use otherwise they would have provided new version. Please tell me if there is some new technology in place of JTestCase and if not what are negatives of Jtestcase(like one can be performance considering fact that it allows use of xml files which is itself in trade off with better organization of complex data). I couldn't find Maven artifact for JTestcase. Please let me know if it is available on any site. Which site is good dependable source to find maven artifact. Currently i see https://repository.sonatype.org/index.html#welcome for same purpose. That seems like a lot of effort to go to just to write a unit test. I think the negative of using JTestCase is doubling the amount of code you have to write. I wouldn't use this framework unless you could see some significant benefit to putting your test data into XML. I think doing this for tests would make it harder to maintain in most cases!
1050,A,"Maven/Surefire not finding unit tests I tried creating a test project with maven and the unit testing worked fine. However when trying to do the same for a j2ee project the unit tests cannot be found by surefire. The tests are in the correct directory (src/test/java) and they are being compiled. The junit test is at src/test/java/unit/TestAddition.java The surefire plugin config in pom.xml does not include/exclude any files. <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>2.8</version> </plugin> and junit testng <!-- Testing dependencies --> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> <dependency> <groupId>org.testng</groupId> <artifactId>testng</artifactId> <version>6.0.1</version> <scope>test</scope> </dependency> Here's the output of ""mvn clean test"" [DEBUG] (s) reportFormat = brief [DEBUG] (s) reportsDirectory = c:\workspace\JAdaptivLatest\target\surefire-reports [DEBUG] (s) runOrder = filesystem [DEBUG] (s) session = org.apache.maven.execution.MavenSession@1cf662f [DEBUG] (s) skip = false [DEBUG] (s) skipTests = false [DEBUG] (s) testClassesDirectory = c:\workspace\JAdaptivLatest\target\test-classes [DEBUG] (s) testFailureIgnore = false [DEBUG] (s) testNGArtifactName = org.testng:testng [DEBUG] (s) testSourceDirectory = c:\workspace\JAdaptivLatest\src\test\java [DEBUG] (s) trimStackTrace = true [DEBUG] (s) useFile = true [DEBUG] (s) useManifestOnlyJar = true [DEBUG] (s) useSystemClassLoader = true [DEBUG] (s) useUnlimitedThreads = false [DEBUG] (s) workingDirectory = c:\workspace\JAdaptivLatest [DEBUG] -- end configuration -- [INFO] Surefire report directory: c:\workspace\JAdaptivLatest\target\surefire-reports [DEBUG] dummy:dummy:jar:1.0 (selected for null) [DEBUG] org.apache.maven.surefire:surefire-booter:jar:2.8:compile (selected for compile) [DEBUG] org.apache.maven.surefire:surefire-api:jar:2.8:compile (selected for compile) [DEBUG] Adding to surefire booter test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-booter\2.8\surefire-booter-2.8.jar Scope: compile [DEBUG] Adding to surefire booter test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-api\2.8\surefire-api-2.8.jar Scope: compile [DEBUG] Setting system property [user.dir]=[c:\workspace\JAdaptivLatest] [DEBUG] Setting system property [localRepository]=[C:\Users\Croydon.IVSTEL1\.m2\repository] [DEBUG] Setting system property [basedir]=[c:\workspace\JAdaptivLatest] [DEBUG] Using JVM: C:\Program Files (x86)\Java\jdk1.6.0_24\jre\bin\java [DEBUG] dummy:dummy:jar:1.0 (selected for null) [DEBUG] org.apache.maven.surefire:surefire-testng:jar:2.8:test (selected for test) [DEBUG] org.apache.maven:maven-artifact:jar:2.0:test (selected for test) [DEBUG] org.codehaus.plexus:plexus-utils:jar:1.0.4:test (selected for test) [DEBUG] org.apache.maven.surefire:surefire-testng-utils:jar:2.8:test (selected for test) [DEBUG] org.testng:testng:jar:jdk15:5.7:test (selected for test) [DEBUG] junit:junit:jar:3.8.1:test (selected for test) [DEBUG] org.apache.maven.surefire:surefire-api:jar:2.8:test (selected for test) [DEBUG] org.testng:testng:jar:jdk15:5.7:test (selected for test) [DEBUG] junit:junit:jar:3.8.1:test (selected for test) [DEBUG] org.apache.maven.surefire:surefire-api:jar:2.8:test (selected for test) [DEBUG] Adding to surefire test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-testng\2.8\surefire-testng-2.8.jar Scope: test [DEBUG] Adding to surefire test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\maven-artifact\2.0\maven-artifact-2.0.jar Scope: test [DEBUG] Adding to surefire test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\codehaus\plexus\plexus-utils\1.0.4\plexus-utils-1.0.4.jar Scope: test [DEBUG] Adding to surefire test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-testng-utils\2.8\surefire-testng-utils-2.8 .jar Scope: test [DEBUG] Adding to surefire test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-api\2.8\surefire-api-2.8.jar Scope: test [DEBUG] test classpath classpath: [DEBUG] c:\workspace\JAdaptivLatest\target\test-classes [DEBUG] c:\workspace\JAdaptivLatest\target\classes [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\junit\junit\4.8.2\junit-4.8.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\testng\testng\6.0.1\testng-6.0.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\beanshell\bsh\2.0b4\bsh-2.0b4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\com\beust\jcommander\1.12\jcommander-1.12.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\yaml\snakeyaml\1.6\snakeyaml-1.6.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\server\selenium-server\1.0.3\selenium-server-1.0.3-standalone.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\server\selenium-server-coreless\1.0.3\selenium-server-coreless-1.0.3.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\ant\ant\1.7.1\ant-1.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\ant\ant-launcher\1.7.1\ant-launcher-1.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\servlet\servlet-api\2.4\servlet-api-2.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\bouncycastle\bcprov-jdk15\135\bcprov-jdk15-135.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\core\selenium-core\1.0.1\selenium-core-1.0.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\client-drivers\selenium-java-client-driver\1.0.2\selenium-java-client-driver-1.0.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\uncommons\reportng\1.1.2\reportng-1.1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\testng\testng\5.0\testng-5.0-jdk15.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\ant\ant\1.6.5\ant-1.6.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\velocity\velocity\1.4\velocity-1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\velocity\velocity-dep\1.5\velocity-dep-1.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\jtidy\jtidy\4aug2000r7-dev\jtidy-4aug2000r7-dev.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\easymock\easymock\3.0\easymock-3.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\cglib\cglib-nodep\2.2\cglib-nodep-2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\objenesis\objenesis\1.2\objenesis-1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\log4j\log4j\1.2.16\log4j-1.2.16.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\jdom\jdom\1.1\jdom-1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\com\jcraft\jsch\0.1.42\jsch-0.1.42.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\snmp4j\snmp4j\1.8.1\snmp4j-1.8.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\extremecomponents\extremecomponents\1.0.1\extremecomponents-1.0.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-collections\commons-collections\3.0\commons-collections-3.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-logging\commons-logging\1.0.4\commons-logging-1.0.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\tinyradius\tinyradius\1.0\tinyradius-1.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-beanutils\commons-beanutils\1.8.3\commons-beanutils-1.8.3.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\commons\commons-email\1.2\commons-email-1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\mail\mail\1.4.1\mail-1.4.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\activation\activation\1.1\activation-1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-lang\commons-lang\2.5\commons-lang-2.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-net\commons-net\2.2\commons-net-2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-pool\commons-pool\1.5.5\commons-pool-1.5.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-dbcp\commons-dbcp\1.4\commons-dbcp-1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-fileupload\commons-fileupload\1.2.2\commons-fileupload-1.2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\directwebremoting\dwr\3.0.M1\dwr-3.0.M1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\mortbay\jetty\cometd\6.1.4\cometd-6.1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\mortbay\jetty\jetty-util\6.1.4\jetty-util-6.1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javolution\javolution\5.5.1\javolution-5.5.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\net\java\dev\eval\eval\0.5\eval-0.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\javaee-api\6.0\javaee-api-6.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\transaction\jta\1.1\jta-1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\slf4j\slf4j-log4j12\1.6.1\slf4j-log4j12-1.6.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\slf4j\slf4j-api\1.6.1\slf4j-api-1.6.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\aspectj\aspectjtools\1.6.10\aspectjtools-1.6.10.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xalan\xalan\2.7.1\xalan-2.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xalan\serializer\2.7.1\serializer-2.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\tomcat\tomcat-dbcp\7.0.5\tomcat-dbcp-7.0.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\cglib\cglib\2.2\cglib-2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\asm\asm\3.1\asm-3.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-core\3.3.2.GA\hibernate-core-3.3.2.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\antlr\antlr\2.7.6\antlr-2.7.6.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\dom4j\dom4j\1.6.1\dom4j-1.6.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-annotations\3.4.0.GA\hibernate-annotations-3.4.0.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-commons-annotations\3.3.0.ga\hibernate-commons-annotations-3.3.0.ga.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-entitymanager\3.4.0.GA\hibernate-entitymanager-3.4.0.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javassist\javassist\3.4.GA\javassist-3.4.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-validator\3.1.0.GA\hibernate-validator-3.1.0.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\persistence\persistence-api\1.0\persistence-api-1.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\ejb3-persistence\1.0.2.GA\ejb3-persistence-1.0.2.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\mysql\mysql-connector-java\5.1.14\mysql-connector-java-5.1.14.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-core\2.2.1.1\struts2-core-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\xwork\xwork-core\2.2.1.1\xwork-core-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\freemarker\freemarker\2.3.16\freemarker-2.3.16.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\ognl\ognl\3.0\ognl-3.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-io\commons-io\1.3.2\commons-io-1.3.2.jar [DEBUG] C:\Program Files (x86)\Java\jdk1.6.0_24\jre\..\lib\tools.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-dojo-plugin\2.2.1.1\struts2-dojo-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-json-plugin\2.2.1.1\struts2-json-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-spring-plugin\2.2.1.1\struts2-spring-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-junit-plugin\2.2.1.1\struts2-junit-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-testng-plugin\2.2.1.1\struts2-testng-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\taglibs\standard\1.1.2\standard-1.1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-core\3.0.5.RELEASE\spring-core-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-asm\3.0.5.RELEASE\spring-asm-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-expression\3.0.5.RELEASE\spring-expression-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-beans\3.0.5.RELEASE\spring-beans-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-aop\3.0.5.RELEASE\spring-aop-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\aopalliance\aopalliance\1.0\aopalliance-1.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-context\3.0.5.RELEASE\spring-context-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-context-support\3.0.5.RELEASE\spring-context-support-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-tx\3.0.5.RELEASE\spring-tx-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-jdbc\3.0.5.RELEASE\spring-jdbc-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-orm\3.0.5.RELEASE\spring-orm-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-oxm\3.0.5.RELEASE\spring-oxm-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-web\3.0.5.RELEASE\spring-web-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-webmvc\3.0.5.RELEASE\spring-webmvc-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-webmvc-portlet\3.0.5.RELEASE\spring-webmvc-portlet-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-test\3.0.5.RELEASE\spring-test-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-testng-utils\2.8\surefire-testng-utils-2.8.jar [DEBUG] provider classpath classpath: [DEBUG] c:\workspace\JAdaptivLatest\target\test-classes [DEBUG] c:\workspace\JAdaptivLatest\target\classes [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\junit\junit\4.8.2\junit-4.8.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\testng\testng\6.0.1\testng-6.0.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\beanshell\bsh\2.0b4\bsh-2.0b4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\com\beust\jcommander\1.12\jcommander-1.12.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\yaml\snakeyaml\1.6\snakeyaml-1.6.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\server\selenium-server\1.0.3\selenium-server-1.0.3-standalone.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\server\selenium-server-coreless\1.0.3\selenium-server-coreless-1.0.3.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\ant\ant\1.7.1\ant-1.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\ant\ant-launcher\1.7.1\ant-launcher-1.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\servlet\servlet-api\2.4\servlet-api-2.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\bouncycastle\bcprov-jdk15\135\bcprov-jdk15-135.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\core\selenium-core\1.0.1\selenium-core-1.0.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\seleniumhq\selenium\client-drivers\selenium-java-client-driver\1.0.2\selenium-java-client-driver-1.0.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\uncommons\reportng\1.1.2\reportng-1.1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\testng\testng\5.0\testng-5.0-jdk15.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\ant\ant\1.6.5\ant-1.6.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\velocity\velocity\1.4\velocity-1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\velocity\velocity-dep\1.5\velocity-dep-1.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\jtidy\jtidy\4aug2000r7-dev\jtidy-4aug2000r7-dev.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\easymock\easymock\3.0\easymock-3.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\cglib\cglib-nodep\2.2\cglib-nodep-2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\objenesis\objenesis\1.2\objenesis-1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\log4j\log4j\1.2.16\log4j-1.2.16.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\jdom\jdom\1.1\jdom-1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\com\jcraft\jsch\0.1.42\jsch-0.1.42.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\snmp4j\snmp4j\1.8.1\snmp4j-1.8.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\extremecomponents\extremecomponents\1.0.1\extremecomponents-1.0.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-collections\commons-collections\3.0\commons-collections-3.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-logging\commons-logging\1.0.4\commons-logging-1.0.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\tinyradius\tinyradius\1.0\tinyradius-1.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-beanutils\commons-beanutils\1.8.3\commons-beanutils-1.8.3.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\commons\commons-email\1.2\commons-email-1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\mail\mail\1.4.1\mail-1.4.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\activation\activation\1.1\activation-1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-lang\commons-lang\2.5\commons-lang-2.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-net\commons-net\2.2\commons-net-2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-pool\commons-pool\1.5.5\commons-pool-1.5.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-dbcp\commons-dbcp\1.4\commons-dbcp-1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-fileupload\commons-fileupload\1.2.2\commons-fileupload-1.2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\directwebremoting\dwr\3.0.M1\dwr-3.0.M1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\mortbay\jetty\cometd\6.1.4\cometd-6.1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\mortbay\jetty\jetty-util\6.1.4\jetty-util-6.1.4.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javolution\javolution\5.5.1\javolution-5.5.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\net\java\dev\eval\eval\0.5\eval-0.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\javaee-api\6.0\javaee-api-6.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\transaction\jta\1.1\jta-1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\slf4j\slf4j-log4j12\1.6.1\slf4j-log4j12-1.6.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\slf4j\slf4j-api\1.6.1\slf4j-api-1.6.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\aspectj\aspectjtools\1.6.10\aspectjtools-1.6.10.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xalan\xalan\2.7.1\xalan-2.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xalan\serializer\2.7.1\serializer-2.7.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\tomcat\tomcat-dbcp\7.0.5\tomcat-dbcp-7.0.5.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\cglib\cglib\2.2\cglib-2.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\asm\asm\3.1\asm-3.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-core\3.3.2.GA\hibernate-core-3.3.2.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\antlr\antlr\2.7.6\antlr-2.7.6.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\dom4j\dom4j\1.6.1\dom4j-1.6.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-annotations\3.4.0.GA\hibernate-annotations-3.4.0.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-commons-annotations\3.3.0.ga\hibernate-commons-annotations-3.3.0.ga.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-entitymanager\3.4.0.GA\hibernate-entitymanager-3.4.0.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javassist\javassist\3.4.GA\javassist-3.4.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\hibernate-validator\3.1.0.GA\hibernate-validator-3.1.0.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\javax\persistence\persistence-api\1.0\persistence-api-1.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\hibernate\ejb3-persistence\1.0.2.GA\ejb3-persistence-1.0.2.GA.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\mysql\mysql-connector-java\5.1.14\mysql-connector-java-5.1.14.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-core\2.2.1.1\struts2-core-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\xwork\xwork-core\2.2.1.1\xwork-core-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\freemarker\freemarker\2.3.16\freemarker-2.3.16.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\ognl\ognl\3.0\ognl-3.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\commons-io\commons-io\1.3.2\commons-io-1.3.2.jar [DEBUG] C:\Program Files (x86)\Java\jdk1.6.0_24\jre\..\lib\tools.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-dojo-plugin\2.2.1.1\struts2-dojo-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-json-plugin\2.2.1.1\struts2-json-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-spring-plugin\2.2.1.1\struts2-spring-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-junit-plugin\2.2.1.1\struts2-junit-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\struts\struts2-testng-plugin\2.2.1.1\struts2-testng-plugin-2.2.1.1.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\taglibs\standard\1.1.2\standard-1.1.2.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-core\3.0.5.RELEASE\spring-core-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-asm\3.0.5.RELEASE\spring-asm-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-expression\3.0.5.RELEASE\spring-expression-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-beans\3.0.5.RELEASE\spring-beans-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-aop\3.0.5.RELEASE\spring-aop-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\aopalliance\aopalliance\1.0\aopalliance-1.0.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-context\3.0.5.RELEASE\spring-context-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-context-support\3.0.5.RELEASE\spring-context-support-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-tx\3.0.5.RELEASE\spring-tx-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-jdbc\3.0.5.RELEASE\spring-jdbc-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-orm\3.0.5.RELEASE\spring-orm-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-oxm\3.0.5.RELEASE\spring-oxm-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-web\3.0.5.RELEASE\spring-web-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-webmvc\3.0.5.RELEASE\spring-webmvc-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-webmvc-portlet\3.0.5.RELEASE\spring-webmvc-portlet-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\springframework\spring-test\3.0.5.RELEASE\spring-test-3.0.5.RELEASE.jar [DEBUG] C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-testng-utils\2.8\surefire-testng-utils-2.8.jar Forking command line: cmd.exe /X /C """"C:\Program Files (x86)\Java\jdk1.6.0_24\jre\bin\java"" -jar c:\workspace\JAdaptivLatest\target\surefire\surefirebooter39500 41630399073451.jar c:\workspace\JAdaptivLatest\target\surefire\surefire6724894077046045670tmp c:\workspace\JAdaptivLatest\target\surefire\surefire55036177478980 48974tmp"" ------------------------------------------------------- T E S T S ------------------------------------------------------- Running TestSuite Tests run: 0 Failures: 0 Errors: 0 Skipped: 0 Time elapsed: 0.193 sec There are no tests to run. Results : Tests run: 0 Failures: 0 Errors: 0 Skipped: 0 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 19.072s [INFO] Finished at: Mon Apr 04 14:21:23 EST 2011 [INFO] Final Memory: 14M/53M [INFO] ------------------------------------------------------------------------ PasteBin link of the log. Surefire has a few problems when you tell it to run both TestNG and JUnit tests. There is a trick to make this work though but I can't remember what it is right now. Try to search around you'll probably find the answer on SO somewhere. I removed all the tests in my project with the excetion of one Junit test i.e. TestAddition. I even removed the testNG dependecy from the pom file. yet Surefire cannot detect any tests. :(  As Cedric said above Surefire has some problems with TestNG and JUnit tests in the same project. I found that when running ""mvn test -X"" Surefire was using the testng plugin instead of junit even though the actual test was a JUnit one. [DEBUG] Adding to surefire test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-testng\2.8\surefire-testng-2.8.jar Scope:test I checked the dependency hierarchy and didn't find any other plugin requiring testng. Then I found the spring testng dependency. <dependency> <groupId>org.apache.struts</groupId> <artifactId>struts2-testng-plugin</artifactId> <version>${org.apache.struts.version}</version> <scope>test</scope> </dependency> After disabling it [DEBUG] Adding to surefire test classpath: C:\Users\Croydon.IVSTEL1\.m2\repository\org\apache\maven\surefire\surefire-junit4\2.8\surefire-junit4-2.8.jar Scope: test Surefire uses the junit plugin and the tests are detected."
1051,A,Multiple JUnit tests using FEST I've written several JUnit test methods to test my Java Swing GUI (using FEST). My class is in the following format: public class Tests { @BeforeClass public static void beforeClass() throws Exception { ... } @Before public void before() throws Exception { ... } @After public void after() throws Exception { ... } @AfterClass public static void afterClass() throws Exception { ... } @Test public void testA() { ... } @Test public void testB() { ... } @Test public void testC() { ... } } When I run this class in Eclipse it only runs the testA test and not testB or testC. If I comment out testA testB runs - so it seems to just run the first test each time. JUnit in Eclipse shows 'Runs: 2/3' after it has completed the first test but doesn't progress any further or show any errors. Is there something I'm doing wrong? I'm using JUnit 4.8.1 and FEST 1.2. I suspect that you have a side effect in testA that is blocking the code in testB from running. It's hard to say what is happening without the details in testA and testB but I can give you some general guidance. You are off to a good start by commenting out testA. When you do this do both testB and testC run? Try drilling down into testA and slowly commenting out the details in what's underneath testA until you tests run both testA and testB. Once you've done this you at least know where the problem is. For example if you code looks something like: public void testA() { doX(); doY(); doZ(); // Assert stuff ... } comment out everything after doX() and see what happens. If that works then uncomment doY() and see what happens. If it didn't work then drill down into doX() and start commenting out and re-running your tests again. This should help you to pinpoint the details of where things are going wrong. If you are still stumped when you figure this out come back and ask a question around those details. Good luck finding your bug! Thanks for your help. After commenting out all the other methods and finding it still just ran the first one each time I had a look at the beforeClass() and before() methods. It seems it was a problem with how I'm using FEST. I had window = new FrameFixture(frame) in the before() method instead of the beforeClass() method. After changing this it seems to be working OK.  Seems your code snippet is OK. You may check if each @Test method is independent to others each test method shouldn't share same context. Ensure @Before and @After have correct behaviors.
1052,A,"JUnit test for System.out.println() I need to write JUnit tests for an old application that's poorly designed and is writing a lot of error messages to standard output. When the getResponse(String request) method behaves correctly it returns a XML response: @BeforeClass public static void setUpClass() throws Exception { Properties queries = loadPropertiesFile(""requests.properties""); Properties responses = loadPropertiesFile(""responses.properties""); instance = new ResponseGenerator(queries responses); } @Test public void testGetResponse() { String request = ""<some>request</some>""; String expResult = ""<some>response</some>""; String result = instance.getResponse(request); assertEquals(expResult result); } But when it gets malformed XML or does not understand request it returns null and writes some stuff to standard output. Is there any way to assert console output in JUnit? To catch cases like: System.out.println(""match found: "" + strExpr); System.out.println(""xml not well formed: "" + e.getMessage()); Related to but not a duplicate of http://stackoverflow.com/questions/3381801/how-do-i-unit-test-saving-file-to-the-disk Instead of redirecting System.out I would refactor the class that uses System.out.println() by passing a PrintStream as a collaborator and then using System.out in production and a Test Spy in the test. In Production ConsoleWriter writer = new ConsoleWriter(System.out)); In the Test ByteArrayOutputStream outSpy = new ByteArrayOutputStream(); ConsoleWriter writer = new ConsoleWriter(new PrintStream(outSpy)); writer.printSomething(); assertThat(outSpy.toString() is(""expected output"")); Discussion This way the class under test becomes testable by a simple refactoring without having the need for indirect redirection of the standard output or obscure interception with a system rule.  using ByteArrayOutputStream and System.setXXX is simple: private final ByteArrayOutputStream outContent = new ByteArrayOutputStream(); private final ByteArrayOutputStream errContent = new ByteArrayOutputStream(); @Before public void setUpStreams() { System.setOut(new PrintStream(outContent)); System.setErr(new PrintStream(errContent)); } @After public void cleanUpStreams() { System.setOut(null); System.setErr(null); } sample test cases: @Test public void out() { System.out.print(""hello""); assertEquals(""hello"" outContent.toString()); } @Test public void err() { System.err.print(""hello again""); assertEquals(""hello again"" errContent.toString()); } I used this code to test the command line option (asserting that -version outputs the version string etc etc) That was the sort of thing I contemplated typing in but never got around to it. +1. Thx Furthemore I have used JUnitMatchers to test for responses: assertThat(result containsString(""You don't want to redirect the system.out stream because that redirects for the ENTIRE JVM. Anything else running on the JVM can get messed up. There are better ways to test input/output. Look into stubs/mocks.  You can set the System.out print stream via setOut() (and for in and err). Can you redirect this to a print stream that records to a string and then inspect that ? That would appear to be the simplest mechanism. (I would advocate at some stage convert the app to some logging framework - but I suspect you already are aware of this!) +1 this is the way: check my example below That was something that came to my mind but I couldn't believe there is no standard JUnit way to do that. Thanks Brain. But the credits got to dfa for the actual effort.  I know this is an old thread but there is a nice library to do this: System Rules Example from the docs: public void MyTest { @Rule public final StandardOutputStreamLog log = new StandardOutputStreamLog(); @Test public void overrideProperty() { System.out.print(""hello world""); assertEquals(""hello world"" log.getLog()); } } It will also allow you to trap System.exit(-1) and other things that a command line tool would need to be tested for. An excellent library +1. very nice library +1  @dfa answer is great so I took it a step farther to make it possible to test blocks of ouput. First I created TestHelper with a method captureOutput that accepts the annoymous class CaptureTest. The captureOutput method does the work of setting and tearing down the output streams. When the implementation of CaptureOutput's test method is called it has access to the output generate for the test block. Source for TestHelper: public class TestHelper { public static void captureOutput( CaptureTest test ) throws Exception { ByteArrayOutputStream outContent = new ByteArrayOutputStream(); ByteArrayOutputStream errContent = new ByteArrayOutputStream(); System.setOut(new PrintStream(outContent)); System.setErr(new PrintStream(errContent)); test.test( outContent errContent ); System.setOut(new PrintStream(new FileOutputStream(FileDescriptor.out))); System.setErr(new PrintStream(new FileOutputStream(FileDescriptor.out))); } } abstract class CaptureTest { public abstract void test( ByteArrayOutputStream outContent ByteArrayOutputStream errContent ) throws Exception; } Note that TestHelper and CaptureTest are defined in the same file. Then in your test you can import the static captureOutput. Here is an example using JUnit: // imports for junit import static package.to.TestHelper.*; public class SimpleTest { @Test public void testOutput() throws Exception { captureOutput( new CaptureTest() { @Override public void test(ByteArrayOutputStream outContent ByteArrayOutputStream errContent) throws Exception { // code that writes to System.out assertEquals( ""the expected output\n"" outContent.toString() ); } }); }"
1053,A,"Ehcache hangs in test I am in the process of rewriting a bottle neck in the code of the project I am on and in doing so I am creating a top level item that contains a self populating Ehcache. I am attempting to write a test to make sure that the basic call chain is established but when the test executes it hands when retrieving the item from the cache. Here are the Setup and the test for reference mocking is being done with Mockito: @Before public void SetUp() { testCache = new Cache(getTestCacheConfiguration()); recordingFactory = new EntryCreationRecordingCache(); service = new Service<Request Response>(testCache recordingFactory); } @Test public void retrievesResultsFromSuppliedCache() { ResultType resultType = mock(ResultType.class); Response expectedResponse = mock(Response.class); addToExpectedResults(resultType expectedResponse); Request request = mock(Request.class); when(request.getResultType()).thenReturn(resultType); assertThat(service.getResponse(request) sameInstance(expectedResponse)); assertTrue(recordingFactory.requestList.contains(request)); } private void addToExpectedResults(ResultType resultType Response response) { recordingFactory.responseMap.put(resultType response); } private CacheConfiguration getTestCacheConfiguration() { CacheConfiguration cacheConfiguration = new CacheConfiguration(""TEST_CACHE"" 10); cacheConfiguration.setLoggingEnabled(false); return cacheConfiguration; } private class EntryCreationRecordingCache extends ResponseFactory{ public final Map<ResultType Response> responseMap = new ConcurrentHashMap<ResultType Response>(); public final List<Request> requestList = new ArrayList<Request>(); @Override protected Map<ResultType Response> generateResponse(Request request) { requestList.add(request); return responseMap; } } Here is the ServiceClass public class Service<K extends Request V extends Response> { private Ehcache cache; public Service(Ehcache cache ResponseFactory factory) { this.cache = new SelfPopulatingCache(cache factory); } @SuppressWarnings(""unchecked"") public V getResponse(K request) { ResultType resultType = request.getResultType(); Element cacheEntry = cache.get(request); V response = null; if(cacheEntry != null){ Map<ResultType Response> resultTypeMap = (Map<ResultType Response>) cacheEntry.getValue(); try{ response = (V) resultTypeMap.get(resultType); }catch(NullPointerException e){ throw new RuntimeException(""Result type not found for Result Type: "" + resultType); }catch(ClassCastException e){ throw new RuntimeException(""Incorrect Response Type for Result Type: "" + resultType); } } return response; } } And here is the ResponseFactory: public abstract class ResponseFactory implements CacheEntryFactory{ @Override public final Object createEntry(Object request) throws Exception { return generateResponse((Request)request); } protected abstract Map<ResultTypeResponse> generateResponse(Request request); } I also had a problem with EHCache hanging although only in a hello-world example. Adding this to the end fixed it (the application ends normally). CacheManager.getInstance().removeAllCaches(); http://stackoverflow.com/a/20731502/2736496  After wrestling with it for a while I discovered that the cache wasn't being initialized. Creating a CacheManager and adding the cache to it resolved the problem."
1054,A,How to change JUnit transformation in Maven2 I have Java project built with Maven2. There is used JUnit framework for testing ( and Selenium but it is irrelevant ). I would like to add screenshot ( I have it ) into result of tests. Here on SO I found similar question which solves it but with Ant. I would like to know if there is any option how to manage it with Maven2 instead of Ant or if there is other solution how to add screenshot into test results. For testing in Maven I am using Maven surefire plugin and Maven surefire report plugin Thanks a lot. Unfortunately I don't think it is easily feasible without patching the maven surefire report plugin. The report plugin takes the surefire XML report and generates the maven HTML report using the Doxia library. The class generating the report is here: http://maven.apache.org/plugins/maven-surefire-report-plugin/xref/org/apache/maven/plugins/surefire/report/SurefireReportGenerator.html Patching this class to alter to report output is almost straightforward; however it requires maintaining a custom version of the surefire report plugin.
1055,A,"Ordering unit tests in Eclipse's JUnit view The JUnit view in Eclipse seems to order the tests randomly. How can I order them by class name? This drives me nuts in Eclipse and I wish someone would add this as an option to the plugin. I wish the accepted answer actually fixed the problem! As Gary said in the comments: it would be nice if Unit Runner could be told to go ahead and order them by class name. Hmm maybe I should look into the source code... I did look but there's no hint of a functionality to sort these names. I would suggest a change request to the JUnit plugin but I don't think that there are lot of people using this thing so: DIY. I would like to see the solution if you modify the plugin code. Yeah probably the best solution is to modify the plugin code. Wish I had the time. I know this problem :)  I was also searching for a solution for this and I found a kind of crack from the below URL. I don't know whether it works for you or not but it worked for me in Spring Tool Suite 2.5.2. http://osdir.com/ml/java.junit.user/2002-10/msg00077.html  mark wrote: it orders them base on execution time maybe you should sort your methods? source/sort members mark is right. But you cannot sort your unit test. It's not allowed to speculate about the order of execution. Unit tests have to be built independently and it's random how they are called by the UnitRunner. In most cases the test methods are sorted alphabetically. The classes are random. Try to use a TestSuite to order your tests. Well I'm trying to avoid using TestSuite. I appreciate unit test independence and I maintain that when creating them but it would be nice if Unit Runner could be told to go ahead and order them by class name. Hmm maybe I should look into the source code...  One thing that one might do is using the schema of JUnit 3.x. We used a test suite that was called AllTests where you add the tests to it in a specific order. And for every package we got another AllTests. Giving those test suites a name being the same as the package enables one to easily build a hierarchy that should be valued by the junit plugin. I really dislike how it is even presenting the test methods inside the Junit viewer. It should be in the very same order as they are specified in the TestCase class. I order those methods in the way of importance and features. So the upmost failing method is to correct first and then the more special one in the later part of the test case. That is really annoying that the test runner is scrambling those. I will take a look at it myself and if I find a solution I will update this answer. Update: My problem with the ordering of method names within a TestCase is related to this one: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7023180 (Thanks Oracle!). So in the end oracle changed the ordering of the methods within a class.getMethods or class.getDeclaredMethods call. Now the methods are random and can change between different runs of the JVM. It seams to be related to optimizations of compare or even is an attempt to compress method name - who knows... . So whats left. First one can use: @FixMethodOrder (from javacodegeeks.com): @FixMethodOrder(MethodSorters.DEFAULT) – deterministic order based on an internal comparator @FixMethodOrder(MethodSorters.NAME_ASCENDING) – ascending order of method names @FixMethodOrder(MethodSorters.JVM) – pre 4.11 way of depending on reflection based order Well that is stupid but explains why people start using test1TestName schema. Update2: I use ASM since Javassist also produces random sorted methods on getMethods(). They use Maps internally. With ASM I just use a Visitor. package org.junit.runners.model; import java.io.IOException; import java.io.InputStream; import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List; import org.objectweb.asm.ClassReader; import org.objectweb.asm.ClassVisitor; import org.objectweb.asm.MethodVisitor; import org.objectweb.asm.Opcodes; import com.flirtbox.ioc.OrderTest; /** * @author Martin Kersten */ public class TestClassUtil { public static class MyClassVisitor extends ClassVisitor { private final List<String> names; public MyClassVisitor(List<String> names) { super(Opcodes.ASM4); this.names = names; } @Override public MethodVisitor visitMethod(int access String name String desc String signature String[] exceptions) { names.add(name); return super.visitMethod(access name desc signature exceptions); } } private static List<String> getMethodNamesInCorrectOrder(Class<?> clazz) throws IOException { InputStream in = OrderTest.class.getResourceAsStream(""/"" + clazz.getName().replace('.' '/') + "".class""); ClassReader classReader=new ClassReader(in); List<String> methodNames = new ArrayList<>(); classReader.accept(new MyClassVisitor(methodNames) 0); return methodNames; } public static void sort(Class<?> fClass List<FrameworkMethod> list) { try { final List<String> names = getMethodNamesInCorrectOrder(fClass); Collections.sort(list new Comparator<FrameworkMethod>() { @Override public int compare(FrameworkMethod methodA FrameworkMethod methodB) { int indexA = names.indexOf(methodA.getName()); int indexB = names.indexOf(methodB.getName()); if(indexA == -1) indexA = names.size(); if(indexB == -1) indexB = names.size(); return indexA - indexB; } }); } catch (IOException e) { throw new RuntimeException(""Could not optain the method names of "" + fClass.getName() + "" in correct order"" e); } } } Just put this in your src/test/java folder in the package org.junit.runners.model. Now copy the org.junit.runners.model.TestClass of the junit 4.5 lib to the same package and alter its constructor by adding the sorting routine.  public TestClass(Class<?> klass) { fClass= klass; if (klass != null && klass.getConstructors().length > 1) throw new IllegalArgumentException( ""Test class can only have one constructor""); for (Class<?> eachClass : getSuperClasses(fClass)) for (Method eachMethod : eachClass.getDeclaredMethods()) addToAnnotationLists(new FrameworkMethod(eachMethod)); //New Part for(List<FrameworkMethod> list : fMethodsForAnnotations.values()) { TestClassUtil.sort(fClass list); } //Remove once you have verified the class is really picked up System.out.println(""New TestClass for "" + klass.getName()); } Here you go. Now you have nicely sorted methods in the order they are declared within the java file. If you wonder the class path is usually set that way that everything in your src (target or bin) folder is considered first by the classloader. So while defining the very same package and the same class you can 'override' every class / interface in any library you use. Thats the trick! Update3 I was able to get a tree view of every package and every class in the right order to. The idea is to subclass ParentRunner and then add all classes to it that you identify as being public and having methods annotated with test. Add a getName() method returning only the package name of the class your suite runner is representing (so you see the tree as a package tree without the suite's class name). Inspect subdirectories if you find a certain suite class (I use AllTests for all suite classes). If you do not find a suite class in a subdirectory check all of its subdirectories this way you dont miss a package containing tests if the parent directory is not containing a suite. That was it. The suite class I add everywhere is: @RunWith(MySuiteRunner.class) public class AllTests { } That's it. It should you give enough to start and extend on this one. The suite runner is only using reflection but I sort the test classes and suits of the subdirectories alphabetically and suits of subdirectories (which represent the packages they are in) are sorted upmost.  If you really need hard dependency between your JUnit test try JExample extension JExample introduces producer-consumer relationships to unit-testing. A producer is a test method that yields its unit under test as return value. A consumer is a test method that depends on one or more producers and their return values. You can install it in Eclipse for Junit4.4 or 4.5. import jexample.Depends; @Test @Depends(""#testEmpty"") public Stack<Integer> testPush(Stack<Integer> $) { $.push(42); assertFalse($.isEmpty()); return $; } As mentioned in this IBM article ""In pursuit of code quality: JUnit 4 vs. TestNG"": One thing the JUnit framework tries to achieve is test isolation. On the downside this makes it very difficult to specify an order for test-case execution which is essential to any kind of dependent testing. Developers have used different techniques to get around this like specifying test cases in alphabetical order or relying heavily on fixtures (@Before @After) to properly set things up. These workarounds are fine for tests that succeed but for tests that fail they have an inconvenient consequence: every subsequent dependent test also fails. In some situations this can lead to large test suites reporting unnecessary failures So beware: if you retain any solution for ordering your JUnit tests the way you want... you need to think if that solution support a ""skip"" feature in order to allow other tests to proceed even if one of them fails. Yeah just the results. And only because I might want to look for a specific test result after the unit tests have ran. I'm not sure why Eclipse can't order the test results as they're being run. Just use a SortedSet. I think he just want to see the _results_ in a specific order. ... yes hence my answer: since you can not sort the results you can try to order the tests."
1056,A,"Is it possible for e JUnit test to tell if it's running in Eclipse (rather than ant) I have a test that compares a large blob of expected XML with the actual XML received. If the XML is significantly different the actual XML is written to disk for analysis and the test fails. I would prefer to use assertEquals so that I can compare the XML more easily in Eclipse - but this could lead to very large JUnit and CruiseControl logs. Is there a way I can change a JUnit test behaviour depending on whether it's running through Eclipse or through Ant. I don't understand why the assertEquals makes it easier to to compare in Eclipse? Surely any diff tool will do? Yes - you can test if certain osgi properties are set (System.getProperty(""osgi.instance.area"") for instance). They will be empty if junit is started through ant outside of eclipse. It is also empty from inside Eclipse Juno. :(  Usually the system proeprties are different in different environments. Try to look for a system property which is only set by eclipse or ant. BTW: The output in eclipse is the same its just that the console for eclipse renders the output in a more readable form. Personally I wouldn't worry about the size of the logs. Generally you don't need to keep them very long and disk space is cheap. This would be an interesting comment for the question but it is definitely not a useful answer.  With Java 1.6+ it looks like the result of System.console() makes a difference between running for Eclipse or from a terminal: boolean isRealTerminal() { // Java 1.6+ return System.console() != null; }  Here are 2 solutions. Use system properties boolean isEclipse() { return System.getProperty(""java.class.path"").contains(""eclipse""); } Use stacktrace boolean isEclipse() { Throwable t = new Throwable(); StackTraceElement[] trace = t.getStackTrace(); return trace[trace.length - 1].getClassName().startsWith(""org.eclipse""); } The ""Use system properties"" approach worked for me thanks! @AlexR Brilliant never thought of instantiating an exception to determine the context of my current call. This is ripe for abuse but seems like there maybe cases where it is perfect. Thanks for this."
1057,A,Running ANT with a SeleneseTestCase results in java.lang.NoClassDefFoundError Ok so I'm trying to figure out why junit is failing during my ANT build when I try to run a class that extends a SeleneseTestCase. I can run the test manually without an issue. It's only when I try building and running it with ANT I get this problem. I'm using Eclipse on the Mac with the latest version of: selenium-java-client-driver-test.jar selenium-java-client-driver.jar selenium-server-standalone-2.0a7.jar These are referenced in my global settings under the ANT Global Settings and are imported properly in my SeleneseTestCase classes (I can run the testSuite.RegressionTest manually with no issues as well as the test class itself). So somewhere I'm missing the linking between ANT and Selenium. Any assistance or further things to try would be greatly welcomed. And if you're ever in San Diego (drinks are on me!!! dead serious :)) Here is my Console Output: [junit] Testsuite: testSuite.RegressionTest [junit] Tests run: 1 Failures: 0 Errors: 1 Time elapsed: 0 sec [junit] Null Test: Caused an ERROR [junit] null [junit] java.lang.reflect.InvocationTargetException [junit] Caused by: java.lang.NoClassDefFoundError: com/thoughtworks/selenium/SeleneseTestCase [junit] at java.lang.ClassLoader.defineClass1(Native Method) [junit] at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632) [junit] at java.lang.ClassLoader.defineClass(ClassLoader.java:616) [junit] at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141) [junit] at java.net.URLClassLoader.defineClass(URLClassLoader.java:283) [junit] at java.net.URLClassLoader.access$000(URLClassLoader.java:58) [junit] at java.net.URLClassLoader$1.run(URLClassLoader.java:197) [junit] at java.security.AccessController.doPrivileged(Native Method) [junit] at java.net.URLClassLoader.findClass(URLClassLoader.java:190) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:307) [junit] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:248) [junit] at testSuite.RegressionTest.suite(Unknown Source) [junit] Caused by: java.lang.ClassNotFoundException: com.thoughtworks.selenium.SeleneseTestCase [junit] at java.net.URLClassLoader$1.run(URLClassLoader.java:202) [junit] at java.security.AccessController.doPrivileged(Native Method) [junit] at java.net.URLClassLoader.findClass(URLClassLoader.java:190) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:307) [junit] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:248) [junit] Test testSuite.RegressionTest FAILED UPDATE: You can see that the Selenium .jar files are loaded... [javac] [search path for class files: /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/jsfd.jar/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/classes.jar ...[REMOVED IRRELEVANT LOADED CLASSES]... /Applications/eclipse/plugins/junit-4.8.2.jar/Applications/eclipse/plugins/selenium-java-client-driver-tests.jar/Applications/eclipse/plugins/selenium-java-client-driver.jar/Applications/eclipse/plugins/selenium-server-standalone-2.0a7.jar] ...[REMOVED IRRELEVANT LOADED CLASSES]... Have you run ANT with the verbose/debug on you'll see what CLASSPATH it really loads. @ karianna I've updated question with the part where it's loading the path to classes. From this output I'm assuming that the .jar files pertaining to selenium are being loaded properly. It doesn't look like it's selenium but rather ant that is giving you a headache. What version of ant are you using? This definitely sounds like a classpath issue that could be generated from ant. I have spent many a wasted hour trying to figure out why my builds weren't working only to find out that there was some other classpath variable getting loaded when my task ran. I've even had the misfortune of loading an older version of ant (hidden deep within Weblogic) when I was explicitly calling a newer version. That was 4 wasted hours of my life... My suggestion is to create a classpath variable in your ant script that explicitly names the jars you want to use and run your ant task with '-v' (for verbose) to see what is really loaded at runtime. The old saying goes 'if it smells like a duck walks like a duck and poops like a duck then you've probably got a duck' I found my issue resolved it self after I rebooted my Mac. I guess something with Eclipse (and the built in ANT v1.7.0.v200803061910) didn't recognize the classpaths and JARs I added until a reboot. I figured it was a classpath issue but a reboot come on!!!
1058,A,JUnit - should I assign null to resources in tearDown that were instantiated in setUp? I am reading one book about JUnit now and writer advises nulling resources in tearDown method. Why? Isn't this GC's job? Can it seriously make any harm? Lets think of example like this: public class SomeTest extends TestCase { Vector vector; List<Object> list; protected void setUp() { vector = new Vector(); list = new ArrayList<Object>(); } // messing with resources // adding deleting testing whatever protected void tearDown() { vector = null; list = null; } } What do you think? Is that code in tearDown necessary? It depends what you consider a resource. Whilst heap space is a resource you can probably get away with the GC cleaning up after you (YMMV). Things that might cause issues are Closables like database connections / open files and streams etc. which should always be closed after use to prevent nasties in long running code. I once had a situation that an integration test for some hibernate code didn't cleanup properly and resulted in some really strange errors. It took many hours to find and angered me so badly that I'll never make the same mistake again. If SomeTest was part of a very large TestSuite with many TestCases and somes TestCases in that TestSuite create objects in setUp() that take up a lot of memory and do not null out the references in tearDown() then yes you could run low on memory while running the suite. Note that the answer is different for JUnit 4.x style tests. Yes :) Your test case should be fine. I generally try to avoid @Before and @After cases - just do them where needed. I see this is the thing. Author should have written so :) So for example I posted above it's useless right ?  Yes this can indeed be necessary. You see JUnit will actually create a separate instance of the Test class for each test method and the Junit3 test runner (not so with JUnit4) will keep these instances around until the entire test suite has finished. Therefore if your (JUnit3) test class has fields that take up a lot of memory you can easily run out of heap space when you have a large number of test methods. Of course if those collections in your example code only ever contain a handful of short strings it doesn't matter. Interesting I didn't know that each method uses own Test instance :O Thanx ... It's the same for JUnit 4. Ok ... thanx everyone for help but this is most interesting answer for me so I mark it as accepted. JUnit4 does indeed create a different object for each test method but it will not keep these instances around until the entire test suite has finished. See my response to http://stackoverflow.com/questions/3655944/is-it-really-necessary-to-nullify-objects-in-junit-teardown-methods
1059,A,"Maven 3 and JUnit 4 compilation problem: package org.junit does not exist I am trying to build a simple Java project with Maven. In my pom-file I declare JUnit 4.8.2 as the only dependency. Still Maven insists on using JUnit version 3.8.1. How do I fix it? The problem manifests itself in a compilation failure: ""package org.junit does not exist"". This is because of the import statement in my source code. The correct package name in JUnit 4.* is org.junit.* while in version 3.* it is junit.framework.* I think I have found documentation on the root of the problem on http://maven.apache.org/plugins/maven-surefire-plugin/examples/junit.html but the advice there seems to be meant for Maven experts. I did not understand what to do. I haven't worked with Maven 3 but my first guess would be that your test class is in src/main/java and your POM defines junit as a test dependency (that is the value of the ""scope"" element is ""test""). I can't say for sure without seeing your POM and knowing the structure of your project though. Can you paste your pom.xml? Maven does not insist on JUnit that one who does must be you...it sounds like you didn't located your Test classes into the correct location src/test/java ...and of course the pom would be extremly helpful. I had the same problem. All i did was - From the pom.xml file i deleted the dependency for junit 3.8 and added a new dependency for junit 4.8. Then i did maven clean and maven install. It did the trick. To verify  after maven install i went project->properties-build path->maven dependencies and saw that now the junit 3.8 jar is gone ! instead junit 4.8 jar is listed. cool!!. Now my test runs like a charm.. Hope this helps somehow..  I had a similar problem of Eclipse compiling my code just fine but Maven failed when compiling the tests every time despite the fact JUnit was in my list of dependencies and the tests were in /src/test/java/. In my case I had the wrong version of JUnit in my list of dependencies. I wrote JUnit4 tests (with annotations) but had JUnit 3.8.x as my dependency. Between version 3.8.x and 4 of JUnit they changed the package name from junit.framework to org.junit which is why Maven still breaks compiling using a JUnit jar. I'm still not entirely sure why Eclipse successfully compiled. It must have its own copy of JUnit4 somewhere in the classpath. Hope this alternative solution is useful to people. I reached this solution after following Arthur's link above.  my problem was a line inside my pom.xml i had the line <sourceDirectory>${basedir}/src</sourceDirectory> removing this line made maven use regular structure folders which solves my issue  How did you declare the version? <version>4.8.2</version> Be aware of the meaning from this declaration explained here (see NOTES): When declaring a ""normal"" version such as 3.8.2 for Junit internally this is represented as ""allow anything but prefer 3.8.2."" This means that when a conflict is detected Maven is allowed to use the conflict algorithms to choose the best version. If you specify [3.8.2] it means that only 3.8.2 will be used and nothing else. To force using the version 4.8.2 try <version>[4.8.2]</version> As you do not have any other dependencies in your project there shouldn't be any conflicts that cause your problem. The first declaration should work for you if you are able to get this version from a repository. Do you inherit dependencies from a parent pom? Wouldn't `[4.8.2)` be better to force **at least** that version? @Joachim Sauer Using version ranges currently also resolves SNAPSHOT versions ([link](http://jira.codehaus.org/browse/MNG-3092)) - and it's very controversal discussed if that is a good concept. If you do not mind to get a SNAPSHOT version your suggestion is quite usable.  Just to have an answer with the complete solution to help the visitors: All you need to do is add the junit dependency to pom.xml. Don't forget the <scope>test</scope> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.11</version> <scope>test</scope> </dependency>  Add this dependency to your pom.xml file: http://mvnrepository.com/artifact/junit/junit-dep/4.8.2 This is a link-only answer and is more suited to a comment than an answer (unless you flesh it out with code examples).  @Dennis Roberts: You were absolutely right: My test class was located in src/main/java. Also the value of the ""scope"" element in the POM for JUnit was ""test"" although that is how it is supposed to be. The problem was that I had been sloppy when creating the test class in Eclipse resulting in it being created in src/main/java insted of src/test/java. This became easier to see in Eclipse's Project Explorer view after running ""mvn eclipse:eclipse"" but your comment was what made me see it first. Thanks. I moved some files from test to main and my junit Assert started throwing error. Deleting the test resolved the issue.Thanks! i moved one Test file from main to test. Then maven compile success with junit test. Thanks a lot"
1060,A,"How to obtain test case name in JUnit 4 at runtime? I want to do some logging while executing my JUnit test. In JUnit 3.x it was always easy to obtain the name of the currently running test case no matter how the test case was instantiated: public void testFoo() throws Exception() { String testName = this.getName(); // [...] do some stuff } In JUnit 4 things seem to be not so easy. Does anyone know a solution to this? Is there any option to reflect into the current Runner instance? Seems similar to: http://stackoverflow.com/questions/473401/get-name-of-currently-executing-test-in-junit-4 In JUnit 4.7 you can also get the name of the currently executed thest method. May be nice when logging. Taken from JUnit 4.7 Release Notes (read them here at github) : public class NameRuleTest { @Rule public TestName name = new TestName(); @Test public void testA() { assertEquals(""testA"" name.getMethodName()); } @Test public void testB() { assertEquals(""testB"" name.getMethodName()); } }  I know this is old but here is a useful (non-junit) method that I put at the top of all my tests. public static void printTestName(){ final StackTraceElement[] ste = new Throwable().getStackTrace(); int buffer = 35 - ste[1].getMethodName().length(); System.out.println(""*******************************************************""); System.out.println(""* TEST: "" + ste[1].getMethodName() + getBuffer(buffer) + ""*""); System.out.println(""*******************************************************""); } private static String getBuffer(int offset){ StringBuilder buffer = new StringBuilder(""""); for(int i = 1; i < offset; i++){ buffer.append("" ""); } return buffer.toString(); }  What's wrong with: @Test public void foo() throws Exception() { String testName = this.getName(); // [...] do some stuff } ? In JUnit 4 the test classes no longer extend a common framework class. So there's no inherited method getName any more. OK. I still don't get what is the `this.getName() about. How is it different than this.getClass().getName()? (I've found another answer for you) In JUnit 3 TestCase.getName() returns the name of the test method. When you run a JUnit3 test case multiple instances of your test class are created each with a different name. See http://junit.sourceforge.net/doc/cookstour/cookstour.htm  public class FooTest { @Rule final public TestRule traceTestWatcher = new TestWatcher() { @Override protected void starting(Description d) { System.out.println(d); } }; @Test public void testBar() { ... } @Test public void testBaz() { ... } }  OK. I've found another approach somewhere on the Internet:  @RunWith(Interceptors.class) public class NameTest { @Interceptor public TestName name = new TestName(); @Test public void funnyName() { assertEquals(""funnyName"" name.getMethodName()); } } That seems exactly in the direction I was looking for. I'll give it a try tomorrow. Unfortunately this solution is only going to be released in the upcoming final version of JUnit 4.7 . Meanwhile I'm forced to stick with an earlier version that's included in the customer's framework architecture. However this is the best answer to the question. Where is this Interceptors class? I don't see it in the 4.11 Javadoc @Peter The answer using the Rule annotation is what is in junit now."
1061,A,"How do I verify a method call and ignore the return value in EasyMock? I'm getting frustrated trying to do a simple thing - I want to invoke a method on a mock object and NOT check its return value. I just want to check that it was invoked with the correct parameters. Example: MyInterface mockObject = createMock(MyInterface.class); SomeObject param = new SomeObject(); /* the write object is not void and returns an instance of FooOjbect. * I want to ignore everything to do with FooObject - I do not care what * it is because I do not store its value. How do I do this? */ mockObject.write(param); replay(mockObject); someOtherObjectThatCallsAboveMockObject.process(mockObject); verify(mockObject); So are there any EasyMock experts out there? I'm not concerned about the design of the underlying method that I'm calling and not storing the return value because the actually implementation is coming from a third-party networking library (Apache Mina) and I have no control over the API. EDIT: Conclusion reached some time later I dumped EasyMock because it wasn't easy and went for Mockito. I changed to Mockito too. It's far easier and more comfortable I think. Instead of mockObject.write(param) write EasyMock.expect( mockObject.write(param) ).andReturn( /* return value here */ ); You still need to return a value for the code to be correct but you can ignore the value further on in the test. Can you provide an example of how I ignore the value later on? I have scoured the documentation and I don't see any good examples. @Elijah By ""ignoring the value"" it means your actual class does not depend on the value coming out of it. If you're actual class is not acting on this return value it means it's ignoring it. You have to provide a return value period. ... ignore in your test - never reference again. Are you having a problem with your real code? Does andReturn(null) cause problems? You're just trying to make the return value enough so that the code runs the way you desire it to. I misunderstood the meaning of ""expect."" I was assuming that it was expecting a return value of a certain instance and checking against it. But it worked as you suggested. Thanks! +1"
1062,A,"Selenium assertText for all text in a table? I need to assert that each row in table contains a certain text string either through selenium IDE or a Java test case. What's the best way to do this? Here's my current test: Command assertText Target //table[@id='myTable']//tbody//tr[not(@style)]/td[1] Value myValue I need to test the first column of every row but this only tests the first row. Is there an easy way to test every row? I haven't used selenium IDE only the java API so here how I'd do it in java (or the basic idea at least) int numRows = selenium.getXpathCount(""table[@id='myTable']//tbody//"" + ""tr[not(@style)]/td[1]"").intValue(); String[] values = new String[numRows]; for (int i = 0; i < numRows; i++) { values[i] = selenium.getText(""table[@id='myTable']//tbody//"" + ""tr[not(@style)]["" + i + ""]/td[1]""); } This also illustrates my other issue: how to select xpath elements by index AND attribute. :P"
1063,A,"How to use JUnit with eclipse I want to use JUnit as test framework with Selenium using eclipse as IDE. I am using Java for Selenium. Which jar file needed to add as library? Yes I have got 4.11 and 4.10 (.jar) for using with eclipse. 1) junit-4.11-SNAPSHOT-20120416-1530.jar 2) junit-4.10.jar Create a project. Add jUnit jar in its build path. Write a java class that extends TestCase. Write a method that strats with ""test"" word like ""testXX()"" in the created class. Run the class as jUnit. Yes got it. thanks Harry Joy  JUnit - Tutorial Yes the link mentioned above is helpful to learn JUnit"
1064,A,"Unit test helper methods? I have classes which previously had massive methods so I subdivided the work of this method into 'helper' methods. These helper methods are declared private to enforce encapsulation - however I want to unit test the big public methods. Is it good to unit test the helper methods too as if one of them fail the public method that calls it will also fail and this way we can identify why it failed? Also in order to test these using a mock object I would need to change their visibility from private to protected is this desirable? You unit test those helper methods by writing unit tests that exercise those portions of your code. You defined a public API in this class for a reason right? Test that. If it works the class works. Use code coverage tools to help you know whether a sufficient portion of that class is being tested and if it's not write more unit tests to exercise the public API and hit those corners that aren't being covered.  If your class really is that big then it sounds like you should be breaking out helper objects not just helper methods (although extracting methods is often a step along the way). Once you've done that your old class becomes simpler and easier to test (perhaps with mocks perhaps not) and you can test the methods on the new supporting classes directly. My preference is to test through the public API of an object. If that's too hard then it's a hint that the object should be broken up. ""If that's too hard then it's a hint that the object should be broken up."" Take your upvote.  You basically have 2 options: Increase the scope of the helper methods from private to default. You can then test these methods (assuming the test classes are in the same package as the test subject). This improves the testability of the class but you sacrifice some encapsulation Leave everything the way it is. This will prevent you from writing very fine-grained tests but doesn't require you to sacrifice any encapsulation. Personally I would choose (2) because you shouldn't really need to test private methods. The class should be tested through it's public interface (which in turn will call the private methods. Testing private methods can result in brittle tests i.e. tests that fail when only the internal behavior of a class changes. There is a third option (which I'm reluctant to mention): use reflection (or some other voodoo) to invoke private methods within your test class. This has the disadvantages of (1) and also the disadvantages intrinsic to reflective code (e.g. bypasses type checking and is hard to read) could also use a subclass defined in the test case which calls the protected method so test classes dont need to be in the same pacakge as implementation  One way is to omit private and put the tests in the same package. Then the tests can call the internal methods but no one else (= outside the package) can. Also failing internal methods should produce error messages which make it easy to fix the issue. When you put the code into production you'll see less than the tests and you'll be under a lot of pressure to fix the issues fast. So one minute spent here will save you one hour later with your boss sitting in your neck. Instead of makeing them package private I prefer to make helper methods protected and test them from an anonymous subclass in the JUnit test. @rsp: That's a fantastic solution that I've never thought of before. It should be its own answer so I can upvote it! :)  I'm pretty shocked at some of the answers here. In essence some people are saying ""Don't test the private code because that violates the TDD paradigm"" Test the damn code. Do whatever you need to in order to make sure that it works exactly as it should. Personally I would make the methods protected or default write the tests run the tests and upon success revert back to private. At this point I would comment out the relevant tests and leave an instruction block above them: /** Sorry about this but I inherited a mess... * if you need to test these methods expose them in source and un-comment the following * lines */ But absolutely never let rigid adherence to a development methodology get in the way of improving code. Your suggestion is basically just as dogmatic - you MUST test every method individually because otherwise it violates the TDD paradigm. Nonsense. A helper method can just be a common piece of code - and your code coverage tools can verify that the unit tests for the public methods covered all of it if need be. Amen @M1EK I just had to post an answer as nobody else had even mentioned code coverage! Original Poster expressed a desire to test those methods individually. ""...test the helper methods too as if one of them fail the public method that calls it will also fail - but this way we can identify why it failed.""  This is one of those cases where I would say go ahead and break the rules. If you were designing the class from scratch you definitely don't want helper methods to be unit tested on their own but... since you are refactoring an existing class it's acceptable to bend the rules in order to make sure you don't break anything. Making them protected will let you test the helpers themselves to make sure they still have the behavior you are expecting as you pull the logic out of the big hairball method as well as allow you to stub them out and return fixed responses so that you can make sure the big method you are refactoring behaves as expected for certain results of the helper methods. But you're not done yet at that point. Splitting the method up isn't really getting to the root of your problem. Now that you have the method broken up and a set of (slightly unorthodox) tests that show you exactly what all of the logic does you are in a good position to re-examine the whole class and try to figure out why the method was so big in the first place. Most likely your whole class also needs to be broken up into smaller units with discrete responsibilities that will then be easier to test without bending any rules.  As Don and Dror say making the methods public so you can create unit tests for them breaks encapsulation. You then tie yourself to a particular implementation. By making them public you declare to the world that these methods are part of the published interface and therefore their specifications are locked. Personally I'd go for a more pragmatic solution: Keep them private and don't write unit tests. If you get to a case where a public method fails and you can't figure out why but you think it might be a problem in one of your private methods then temporarily make them public write the unit test debug and when you're done make them private again and comment out the unit test.  This smells like you have the wrong problem. What you've described is akin to creating a sub-""unit test"" which leads me to believe that your unit tests are really testing a unit after all. Which is not a criticism of what you are trying to do: going from ""where we are today"" to ""somewhere else that's measurably better"" is a winning move. However it is a suggestion that you step back a bit to evaluate where you are - understanding how your current situation differs from some Platonic ideal could help to show new possibilities. There are plenty of suggestions here about scoping your helper methods. Another possibility would be to review the implementation to determine if there are helper classes lurking in your current implementation. Creating a new class and a suite of tests to exercise it is always acceptable. Note that this approach insulates you from the refactoring: you can change the implementation without changing your test suite (because the unit tests for the helper object continue to pass even when the helper object is no longer part of your production implementation) and you get a clean packaging of the implementation and the tests for it (usecase: you decide that bozo-sort is the wrong implementation and should no longer be used. If the bozo-sort implementation is isolated then you simply remove it and its tests. But when the tests of the bozo-sort implementation are tangled with all of the other tests there's more thinking involved). It may also help to review why you have unit tests for your code. If one of the reasons is ""to make refactoring safe"" then you don't want to be writing tests that lock you into an implementation.  If you want to test Helper methods you can change them from private but you might consider this. You should not unit test private details of your implementation mainly because it might change due to refactoring and ""break"" your test."
1065,A,"Setup and Tear Down of Complex Database State With Hibernate / Spring / JUnit I have a class I'm unit testing that requires fairly extensive database setup before the individual test methods can run. This setup takes a long time: for reasons hopefully not relevant to the question at hand I need to populate the DB programatically instead of from an SQL dump. The issue I have is with the tear-down. How can I easily rollback all the changes made in the db setup phase? I'm currently using Hibernate + Spring Transactional Testing support such that my individual test methods are wrapped in transactions. One solution would be to do the db setup within each test method such that the db setup would be rolled back automatically. However the test methods would take forever to run since each method would need to re-prep the database. Any other ideas? Basically I'm looking for a way to run my db setup run my individual tests (each wrapped in a transaction which gets rolled-back after execution) and then roll-back the initial db setup. Any ideas on making this working in a Hibernate / Spring / Junit fashion? Is there a Hibernate ""drop all tables"" equivalent command? Is there a reason that you have to have a connection to the database to run your unit tests? It sounds like it might be easier to refactor your class so that you can mock the interaction with the database. You can mock classes (with some exceptions) as well as interfaces with EasyMock (www.easymock.org). If your class relies on a complex pre-existing state in a connected database it would probably be easier to write faster executing tests using mocks. We don't know what the size of your project is or how often your tests are run but execution time might be something to think about especially in a large project.  Hibernate has a neat little feature that is heavily under-documented and unknown. You can execute an SQL script during the SessionFactory creation right after the database schema generation to import data in a fresh database. You just need to add a file named import.sql in your classpath root and set either create or create-drop as your hibernate.hbm2ddl.auto property. http://in.relation.to/Bloggers/RotterdamJBugAndHibernatesImportsql  You may want to look at @AfterClass annotation for Junit 4. This annotation will run when the tests are done. http://cwiki.apache.org/DIRxDEV/junit4-primer.html  Are you stuck with a specific database vendor? If not you could use an in-memory database such as HSQLDB. When you are done with the tests you just throw away the state. This is only appropriate if the tables can be empty at the start of the test suite (before your programmatic setup that is). You still need to create tables but if everything is neatly mapped using Hibernate you can use the hbm2ddl to generate your tables. All you have to do is add the following to your test session factory definition: <session-factory> ... <property name=""hibernate.hbm2ddl.auto"">create</property> ... </session-factory> If this solution seems applicable I can elaborate on it. Sometimes I forget about the simplest options. Bonus points to you. So by accepting this as the solution does that mean you opted for the db setup/tear down per individual test? I actually went with setting up the db once for a set of tests defined in the same class using a static @BeforeClass Junit method. But it would be easy enough to do on per-test basis with a @Before JUnit method. @Brian - Can you share how you set up a database from within a static method with no access to an application context or getClass()?  One solution that you may want to consider is to use a ""manual"" rollback or compensating transaction in db tear down. I suppose (and if it's not then it should be a trivial add-on to your Hibernate entities) all your entities have datetime create attribute indicating when they were INSERTed into the table. Your db setup method should record time before everything else. Then you have rather simple procedure for db tear down to delete all entities that were created after time recored in db setup. Of course this won't work for updates in db setup... But if you have limited number of updates then consider saving pristine image for this type of data and restore it during db tear down.  If you're working with relatively small database and with a DBMS that can do backups/exports of it relatively fast (like MS SQL Server) you can consider creating a database backup before the tests and then restore it when all testing is complete. This enables you to set-up a development/testing database and use it as a starting state for all your tests. I did it with native JDBC executing ''backup database'' and ''restore database'' T-SQL in-between tests and it worked reasonably well. However this approach is dependent on having the DBMS server on your local machine (for reasonable speed) you having sufficient privileges (which than should not be a problem) and the total size of database not exceeding a few tens on MB - at least in my experience.  DNUnit should help you in this regard. You can create separate data sets for each individual test case if you wish.  DBUnit will help a lot with this. You could theoretically turn off autocommits on JDBC but it will get hairy. The most obvious solution is to use DBUnit to set your data to a known state before you run the tests. IF for some reason you need your data back after the tests are run you could look at @AfterClass on a suite that runs all of your tests but it is generally considered a better practice to set up your tests and then run them so that if the test fails it is not just because it didn't have a prestine environment due to a failure to clean up an different test. You ensure that each test sets up its environment directly. wouldn't using DBUnit be conceptually equivalent to SQL scripting that was ruled out in the question? I didn't mention it specifically in the question but I am familiar with DBUnit and I have been avoiding it because I didn't want to have to maintain a ton of DBUnit test fixtures when it was easier to generate the DB programatically. However I may bite the bullet lacking a better solution."
1066,A,"What are ways to test methods that depend on static methods? I need to test some security related classes that depend on Spring Security. This code makes use of SecurityContextHolder.getContext() which is static. How can I test the calling method without setting up an entire security context? We are using JUnit 4 with Mockito. Mockito was pretty explicit in it's FAQ that static methods where not supported. Is there an alternative? An answer for the Spring Security case would be nice but I am looking for a solution to the more general problem. You can refer to the following issue and inject org.springframework.security.core.context.SecurityContextHolderStrategy instance which functionality is available since Spring Security 3.0. Nice! More ""Spring-way"" answer than mine.  Maybe refactoring code so it accepts some interface instead of getContext()? You'll need impl which will delegate all work to context though. UPDATE: Code will look like interface SecurityContext { void foo(); } class SpringSecurityContext implements SecurityContext { public void foo() { // call spring static method here } } class TestSecurityContext implements SecurityContext { public void foo() { // test case logic here } } class SecurityContextClient { private final SecurityContext context; public SecurityContextClient(SecurityContext context) { this.context = context; } void useSecurity() { context.foo(); } } this is meant to be a helper class for Spring Security so this would be the class that does the getting of the context; even in your case I have just moved the testing problem to another class. Maybe you can provide a concrete example to make your point stronger? @Andrew White I've updated an answer hope it clarifies my point.  You should be able to simply call SecurityContextHolder.setContext() with a mocked SecurityContext in your setupt code. SecurityContextHolder just seems to be a thin wrapper around a ThreadLocal so it should work fine.  Have a look at PowerMock it will allow you to mock out static method constructors and do all sorts of other crazy things you wouldn't normally be able to do with java. It integrates with most mocking libraries including mockito (look here http://code.google.com/p/powermock/wiki/MockitoUsage13 for an example). In general I've found this to be a very useful library to have in your testing toolbox (when coding java). The only caveat is that since this library plays around with your bytecode if you have other libraries that do bytecode instrumentation/manipulation you can run into trouble but you won't know until you try. this works exactly like I intended; I only wish I could vote this answer up more."
1067,A,"JUnit test ends with network connection error I'm running some JUnit 4 tests in eclipse for my Java project which I know have functioned two days ago without problems. Today though I get errors: Could not connect to: : 40212 java.net.SocketException: Network is unreachable at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333) at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366) at java.net.Socket.connect(Socket.java:525) at java.net.Socket.connect(Socket.java:475) at java.net.Socket.<init>(Socket.java:372) at java.net.Socket.<init>(Socket.java:186) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.connect(RemoteTestRunner.java:570) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:381) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Neither the test nor the code do something with networks. Also the eclipse dialog for installing new software can't connect to the update sites and neither can the SVN plug-in connect to the repository. The SVN repository is reachable I've checked with the cli-svn program. Proxies in eclipse are disabled. So my question is why does JUnit want to connect to the internet in the first place (how should offline working even be possible..) and can I do something to find the source of the problems? (My plan B is to remove eclipse and re-install it again) If it's a unit test why is it connecting to the Internet? The answer to the question ""why would Eclipse want to connect to the internet"" is that Eclipse doesn't connect to the internet at all however Eclipse has a special JUnit test listener which communicates with the JVM running the JUnit tests. This test listener opens up a socket on localhost (I'm not 100% sure on which side the server-socket is but that probably doesn't matter). I think that you have some networking trouble on your end preventing the Eclipse process from opening a socket on localhost. I've experienced similar trouble after bringing up/down a VPN and after coming back from a suspend/resume cycle. I'd try a reboot first. If that doesn't work I'd try running from a different workspace (run .../eclipse.exe -data path/to/new/workspace). Remember that there is no point in uninstalling Eclipse. Just unpack a new Eclipse somewhere and try running from there instead. If you do find out what's causing this I'd be interested to hear about it. +1 for the hints. You could be right about the suspend/resume cycle. I just don't have time to put more effort in this see my comment above.  I suspect Junit is launched as a separate process on the same machine (note the references to RemoteTestRunner above) and then Eclipse will talk to it via a socket. So is there some issue related to your TCP stack on your machine. Are you out of networking resources ? Is something hogging these ? netstat may be of use here. Or maybe a simple reboot ? I suffered from the same problem today. Are you by any chance using Debian? If so then this is related to bugs http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=560238 and http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=560056 and can be fixed by setting net.ipv6.bindv6only=1 in /etc/sysctl.d/bindv6only.conf I couldn't solve the problem I tried using another eclipse installation I removed and re-installed some Java packages tried different workspaces but nothing helped. I now use eclipse on a virtual machine (it's ugly I know but I need to get some work done). You did answer my question about why is it using network though. @Confusion: many thanks for researching&sharing this. One correction: you should set it to 0 rather than 1. Executable fix: """""""""""""""""""""""""" $ sudo sed -i 's/net.ipv6.bindv6only\ =\ 1/net.ipv6.bindv6only\ =\ 0/' \ /etc/sysctl.d/bindv6only.conf && sudo invoke-rc.d procps restart """""""""""""""""""""""""" Also: do you want to raise your entry as a proper answer? To me this would be the winning one.  Have you tried looking at the ""Run Configurations"" for the test that you're trying to execute? (i.e. right click the test and choose ""Run As > Run Configurations"") Perhaps there are some settings that have gone a little haywire. Perhaps there is something mis-configured with the ""Test runner"" setting in the ""Test"" tab?  on my windows 7 x64 machine i used a separate firewall blocking javaw.exe for some weird reason after unblocking it it worked again. didn't think the firewall would block machine internal socket communications too"
1068,A,How to count the number of open editor instances in an Eclipse RCP Application? I'm beginning to use the swtbot to test my reccent eclipse rcp projekt. A specific editor is opened multiple times in my application and want to count how often the editor is opened. How can i do that using swtbot? Thanks! :-) From the example of this book on SWTBot (p 34) it should be _bot.editors().size() But that is for all the opened editors so you may need to filter that list (based on the title of the editor for instance) to compute the right number of instances. Thanks... that did it.
1069,A,"Junit with new Date() What would the junit test be when i have the following method: @Override public void saveLastSuccesfullLogin(final User user) { gebruiker.setLastLogin(new Date()); storeUser(user); } submethode storeUser: @Override public void storeUser(final User user) { EntityManager em = emf.createEntityManager(); em.getTransaction().begin(); em.merge(user); em.getTransaction().commit(); em.close(); } The problem i have is the date being set for the entity user and then stored. Im using junit and easymock. Try pulling the new Date() into a method with default access specifier like below @Override public void saveLastSuccesfullLogin(final User user) { gebruiker.setLastLogin(getDate()); storeUser(user); } Date getDate() { return new Date(); } In your test class override the class as below using a mock or stubbed date. <ClassUnderTest> classUnderTest = new <ClassUnderTest> () { @Override Date getDate() { return mockDate; } } In this way you can assert the date value easily as it is going to be stubbed out. instead of overriding with anonymous class I would use [Mockito's partial mock](http://blog.javabien.net/2009/06/21/mockitos-partial-mocks-testing-real-objects-just-got-easier/). So it would be ClassUnderTest t = spy(new ClassUnderTest()); when(t.getDate()).thenReturn(DATE_MOCK); @SauliusŠimčikas Thanks for pointing out  What's the problem with the Date? That you don't know what it is to assert later? A few alternatives: Pass the date into the method Create a factory to get the current date/time so you can mock it out Assert the date within a threshold of correctness  There is also a more ""enterprise"" approach that may be used where Dependency Injection is available (like in EJB Spring etc.). You can define an interface for example TimeService and add e method that returns the current date.  public interface TimeService { Date getCurrentDate(); } You can implement this to return new Date() and use it like this:  gebruiker.setLastLogin(timeService.getCurrentTime()); This will obviously be very easy to test because you can mock the TimeService. Using EasyMock (just an example) this might be:  Date relevantDateForTest = ... expect(timeService.getCurrentTime()).andReturn(relevantDateForTest); replay(timeService); Using the TimeService throughout the entire code and never using new Date() is a pretty good practice and has other advantages as well. I found it helpful in a number of occasions including manual functional testing of features that would activate in the future. Going even further the system time may be retrieved from an external system thus making it consistent across clusters etc. I wrote a more detailed [post here](http://larselrund.com/2011/08/05/junit-with-new-date/) for those that wish to read more about using the separate service for retrieving the current date."
1070,A,"How to default the working directory for JUnit launch configurations in Eclipse? Our Java code (not the test code) reads files from the current directory which means the working directory needs to be set properly whenever we run the code. When launching a JUnit test from within Eclipse a launch configuration automatically gets created. The problem is that the working directory in that launch configuration is always by default the root project directory which is always wrong the test fails I have to open the launch configuration dialog change the working directory and relaunch the test. This is very annoying. The same thing happens when I run a single test method. I've already considered these: Changing the current directory from the test code - not possible by design. When opening a file pass a parent directory parameter - too difficult since this would affect lots of places. Use the Copy launch configuration feature of Eclipse to create new launch configurations from existing ones that already have a correct working directory set. This doesn't really makes sense here since I would like to launch a test or a test method quickly just by invoking ""run this test / method as JUnit test"". All in all it looks like it's responsibility of Eclipse not the code. Is there a way to set the default working directory for all future newly created JUnit launch configurations? With Eclipse 3 you can set you working directory. Go to you run/debug configuration -> Arguments tab. In the Working directory select ""other"" and enter the root of your test  As far as I can tell there's no way of changing the default working directory for future JUnit launch configurations in Eclipse 3.4. This issue has also been reported as a bug in the Eclipse bug database. However it is possible in IDEA. There's the Edit Defaults button for setting all kinds of defaults for each launch config type separately. It might be possible for moreunit to work around this also. I've request a feature request for that here - https://sourceforge.net/tracker/?group_id=156007&atid=798059.  I haven't found a possibility to do this but what you can do is to use: getClass().getResourceAsStream(filename); getClass().getClassLoader().getResourceAsStream(filename); This methods locates a resource on the classpath. The first one is relative to the location of the class the second one is relative to any classpath ""root entry"". You can then for example add the project root directory to the classpath. This does not work however if you want to write to a file as well.  If your tests depend on the current working directory I think it is responsability of your tests to setup correctly that working directory and to configure your classes to be tested to point that directory. If you have a superclass for most of your tests write a constant within it. Or: if you have a superclass for most of your tests write a @Before setup method. Or: if you have not a superclass for most of your tests write a constant in some class of the testing codebase.  This is a subjective answer: I believe you're doing your tests wrong you shouldn't be loading the files from the JUnit using relative or complete paths but instead have them as resources in the project (add them to the build path) and load them as resources in the JUnit tests. This way if something changes on the filesystem someone uses a different filesystem or IDE but still has them in the build path (as source folders) you're not going to have a problem. I'm unsure if this is what you mean but if you really want to change it go to the Run Configuration options -> Your JUnit -> Arguments tab at the bottom (Eclipse 3.4) you see Working Directory click 'other' and change it from there."
1071,A,"Blanking fields with Reflection I am writing a Junit test framework to test web services. There is a requirement for the input values to come from many different sources eg an earlier web service call or literals within the class. To achieve this I have constructors that accept the different inputs in different ways; all simple so far. The problem is the webservices also need to be exercised with a full data load and a mandatory fields only payload. Rather then litter the (in some cases verrry long) tests with if statements deciding whether to set a value or not I have written an annotation @Optional. Adding this annotation causes it to be nulled by the following code:  /** * Find all of the fields annotated with optional and null them * @throws IllegalAccessException * @throws IllegalArgumentException */ private void blankOptionalFields() throws IllegalAccessException{ for(Field field: this.getClass().getDeclaredFields()){ Annotation optionalAnnotation = field.getAnnotation(Optional.class); if(!(field.isSynthetic()) && optionalAnnotation instanceof Optional){ field.setAccessible(true); try{ field.set(this null); } catch(IllegalArgumentException e){ logger.debug(""Tried to set a scalar field to null!"" e); } } } } So two things: 1: Although this works it somehow feels fragile/dangerous there must be a better approach? 2: If this is not a carzy approach what is the best way to go about setting the scalar values to appropiate values? How about defining an interface containing just the one method that blanks out optional attributes? You can test an object for implementing the interface and call the method directly. This handles specific exceptions more elegantly than trying to create a catch all situation using reflection: interface Blankable { /** @return true if all optional fields are successfully blanked. **/ public boolean blankOptionalFields(); } and use like: if (obj implements Blankable) { if (!((Blankable) obj).blankOptionalFields()) { logger.debug(""Could not blank optional fields for "" + obj); } } I was blinded to this solution by a reflection. It seems logical to me not sure why I didn't think of this first.  I would refactor the tests splitting out the initialization code from the actual test code. For that matter you could put the actual test code (the code that invokes the web service) into a method that is shared between multiple test methods. As an semi-related comment: I would think of ""unit"" tests as exercising the service methods stand-alone while ""integration"" tests would exercise it as an actual web service. I like the idea of splitting the initialization code out but unfortunately the actual test code can't be shared. Fair point about the ""unit"" tests  I'm not enamored with this approach because you're mixing test code in with your production code. If you know which fields are mandatory ahead of time is it possible to just loop through those fields at set them without a complicated if structure? There is no production code involved its all test code. I can loop through the fields and set them but set them from what source? I'll have to write the setting code for each input"
