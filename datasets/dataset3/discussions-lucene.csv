0,A,"Digg-like search result ranking with Lucene / Solr? I'm using Solr for search. I have documents that have an integer field ""popularity"". I want to rank results by a combination of normal fulltext search relevance and popularity. It's kinda like search in digg - result ranking is based on the search relevance as well as how many digs a posting has. I don't have any specific ranking algorithm in mind. But is this something that can be done with solr? Solr's FunctionQuery is exactly what you need: http://wiki.apache.org/solr/FunctionQuery would be good if you could add an example your self based on the original question :)"
1,A,"Associating multivalued fields within a document? Say I have a Lucene index of Customers. Each Customer has the products they've ordered. Let's say these two fellas represent two documents in my index: Name: John Smith Product: Chicken Sandwich Price: $10 Product: Dodge Challenger Price: $35000 Name: John Q. Public Product: Chicken Sandwich Price: $15 Product: Audi TT Price: $35000 Given that my index is Customer-centric rather than order-centric my documents would presumably look like this: <add> <doc> <field name=""Name"">John Smith</field> <field name=""Product"">Chicken Sandwich</field> <field name=""Price"">10</field> <field name=""Product"">Dodge Challenger</field> <field name=""Price"">35000</field> </doc> <doc> <field name=""Name"">John Q. Public</field> ... Which would end up munging together all of the prices and products as multivalued fields and losing their relative associations. How would I get this into my index--and how would I query it--such that a search for ""Every customer that has paid more than $12 for a chicken sandwich"" would return only John Q. Public? It sounds like the queries you need would need to have each separate transaction as a document. So for the example you gave the data would look like: <add> <doc> <field name=""Name"">John Smith</field> <field name=""Product"">Chicken Sandwich</field> <field name=""Price"">10</field> </doc> <doc> <field name=""Name"">John Smith</field> <field name=""Product"">Dodge Challenger</field> <field name=""Price"">35000</field> </doc> <doc> <field name=""Name"">John Q. Public</field> Given the above schema here's a possible query syntax for your example: Product:""Chicken Sandwich"" AND Price:[00000000 TO 00001200] For further information the Lucene documentation does a better job than I ever could! The only fly in this ointment is the data type of the price field. In terms of writing least code if you store it as a zero padded string (probably in cents if I understand the American money system!) that's the most straightforward approach. However the best performing approach (which may be important depending upon how many transactions you might have) is to use a numeric field (again with cents). The trouble come when using the Lucene query parser: it doesn't understand this field type. Anyway numeric field issues definitely come under the heading of a different question. Good luck! I'm not worried about the speed/index size here more curious what to do when I get duplicate customers back. For example a search for all the ""John""'s in your example would return 4 records not 2 and I'd rather not have to hand that de-duplication logic to my java project. Hmm I see what you mean. In that case your original schema was probably correct. My query is probably correct as well (does that mean I'd get a half correct answer??). The name would contain the information you require. How were you thinking about updating the index? I guess you'd need to read the existing entry delete the document and add it again. (Multiple values per field are OK in Lucene). Virtually all of my querying is customer-centric. As such converting my index to order-centric would end up giving me massively duplicated customer data. Is there is a standard way of culling out the duplicate customers? Nothing of what I've read (Filters Collectors Collation) seem to fit... If a customer ordered again you'd just update his record with all his orders. At any rate I suppose I need to keep digging. Thanks for the help! If you aren't ""store""ing the data (i.e. just using Lucene to index the data) then don't worry about it. Duplicating the data won't make much difference. It's all about the term query lookups. Try it on a sample of your data and see what the difference in index size is. I doubt you'll find it too worrying (unless you're very short of disk space)."
2,A,"Java Lucene integration with .Net I've got nutch and lucene setup to crawl and index some sites and I'd like to use a .net website instead of the JSP site that comes with nutch. Can anyone recommend some solutions? I've seen solutions where there was an app running on the index server which the .Net site used remoting to connect to. Speed is a consideration obviously so can this still perform well? Edit: could NHibernate.Search work for this? Edit: We ended up going with Solr index servers being used by our ASP.net site with the solrnet library. Instead of using Lucene you could use Solr to index with nutch (see here) then you can connect very easily to Solr using one of the two libraries available: SolrSharp and SolrNet. Hadoop is java-only AFAIK and I don't know its interoperability with other platforms... I'm running everything on debian anyways even asp.net I'm looking at hadoop compatibility too looks really good will it be able to take my lucene indexes? Haven't tried but it should... trying it is the only way to be sure :)  Instead of using Solr I wrote a java based indexer that runs in a cron job and a java based web service for querying. I actually didn't index pages so much as different types of data that the .net site uses to build the pages. So there's actually 4 different indexes each with a different document structure that can all be queried in about the same way (say: users posts messages photos). By defining an XSD for the web service responses I was able to both generate classes in .net and java to store a representation of the documents. The web service basically runs the query on the right index and fills out the response xml from the hits. The .net client parses that back into objects. There's also a json interface for any client side JavaScript.  Why not switch from java lucene to the dot net version. Sure it's an investment but it's mostly a class substitution exercise. The last thing you need is more layers that add no value other than just being glue. Less glue and more stuff is what you should aim for... lucene.net has no Hadoop provider which is why we're on solr now  In case it wasn't totally clear from the other answers Lucene.NET and Lucene (Java) use the same index format so you should be able continue to use your existing (Java-based) mechanisms for indexing and then use Lucene.NET inside your .NET web application to query the index. From the Lucene.NET incubator site: In addition to the APIs and classes port to C# the algorithm of Java Lucene is ported to C# Lucene. This means an index created with Java Lucene is back-and-forth compatible with the C# Lucene; both at reading writing and updating. In fact a Lucene index can be concurrently searched and updated using Java Lucene and C# Lucene processes how about using it with hadoop? How do you want to combine Lucene with Hadoop? Index data that's already in Hadoop? Store a distributed lucene index in Hadoop? The latter would probably require a special version of lucene in order to distribute/query but maybe someone's tried to do it but probably in java.  Got here by searching for a comparison between SolrNet and SolrSharp just thought I'd leave here my impressions. It seems like SolarSharp is a dead project (wasn't updated for a long time) so the only option is SolarNet. I hope this will help someone I would have left a comment to the accepted answer but I don't have enough reputation yet :)  I'm also working on this. http://today.java.net/pub/a/today/2006/02/16/introduction-to-nutch-2.html It seems you can submit your query to nutch and get the rss results back. edit: Got this working today in a windows form as a proof of concept. Two textboxes(searchurl and query) one for the server url and one for the query. One datagrid view. private void Form1_Load(object sender EventArgs e) { searchurl.Text = ""http://localhost:8080/opensearch?query=""; } private void search_Click(object sender EventArgs e) { string uri; uri = searchurl.Text.ToString() + query.Text.ToString(); Console.WriteLine(uri); XmlDocument myXMLDocument = new XmlDocument(); myXMLDocument.Load(uri); DataSet ds = new DataSet(); ds.ReadXml(new XmlNodeReader(myXMLDocument)); SearchResultsGridView1.DataSource = ds; SearchResultsGridView1.DataMember = ""item""; } And it seems our division is probably going with windows search server express. well done We're starting to use Solr for this"
3,A,"How to get total match count in Solr/lucene I have a problem that i want to get total count of matched text in solr. but when i want to perform search using solr i have to set max rows parameter. can anybody explain how i could get the total matched count using solr efficiently? You can get the total result count independently from max rows defined through the numFound attribute in the Solr response. @Ahsan: like I said use the numFound attribute. i think that if we search some term like ""Contents:risk"" the numFound will tell us how many times this term exists in all the lucene indexed documents right? but can we get the total count of lucene indexed documents having the search term no matter one indexed document contains search term more than once.. @Ahsan: numFound counts documents not occurrences. can we the total count of documents where that search term exists.. thank you very much for ur responses"
4,A,How can I search on a list of values using Solr/Lucene? Given the following query: (field:value1 OR field:value2 OR field:value3 OR ... OR field:value50) Can this be broken down into something less verbose? Basically I have hundreds of category IDs and I need to search for items under large groups of category IDs (20-50 at a time). In MySQL I'd just use field IN(value1 value2 value3) rather than (field = value1 OR field = value2 etc...). Is there a simpler way for Solr/Lucene? http://stackoverflow.com/questions/2533815/solr-range-query-for-specefic-id-like-solr-selectqx1-2-5-11-64589 Use field:(value1 value2 value3) or if your default operator is AND then use field:(value1 OR value2 OR value3) Perfect thanks!
5,A,"How to match against subsets of a search string in SOLR/lucene I've got an unusual situation. Normally when you search a text index you are searching for a small number of keywords against documents with a larger number of terms. For example you might search for ""quick brown"" and expect to match ""the quick brown fox jumps over the lazy dog"". I have the situation where I have lots of small phrases in my document store and I wish to match them against a larger query phrase. For example if I have a query: ""the quick brown fox jumps over the lazy dog"" and the documents ""quick brown"" ""fox over"" ""lazy dog"" I'd like to find the documents that have a phrase that occurs in the query. In this case ""quick brown"" and ""lazy dog"" (but not ""fox over"" because although the tokens match it's not a phrase in the search string). Is this sort of query possible with SOLR/lucene? only setting mm parameter will not satisfy your needs since ""the quick brown fox jumps over the lazy dog"" will match all three documents ""quick brown"" ""fox over"" ""lazy dog"" and as you said: I'd like to find the documents that have a phrase that occurs in the query. In this case ""quick brown"" and ""lazy dog"" (but not ""fox over"" because although the tokens match it's not a phrase in the search string).  Sounds like you want the DisMax ""minimum match"" parameter. I wrote a blog article on the concept here a little while: http://blog.websolr.com/post/1299174416. There's also the Solr wiki on minimum match. The ""minimum match"" concept is applied against all the ""optional"" terms in your query -- terms that aren't explicitly specified using +/- whether they are ""+mandatory"" or ""-prohibited"". By default the minimum match is 100% meaning that 100% of the optional terms must be present. In other words all of your terms are considered mandatory. This is why your longer query isn't currently matching documents containing shorter fragments of that phrase. The other keywords in the longer search phrase are treated as mandatory. If you drop the minimum match down to 1 then only one of your optional terms will be considered mandatory. In some ways this is the opposite of the default of 100%. It's like your query of quick brown fox… is turned into quick OR brown OR fox OR … and so on. If you set your minimum match to 2 then your search phrase will get broken up into groups of two terms. A search for quick brown fox turns into (quick brown) OR (brown fox) OR (quick fox) … and so on. (Excuse my psuedo-query there I trust you see the point.) The minimum match parameter also supports percentages -- say 20% -- and some even more complex expressions. So there's a fair amount of tweakability.  It sounds like you want to use ShingleFilter in your analysis so that you index word bigrams: so add ShingleFilterFactory at both query and index time. At index time your documents are then indexed as such: ""quick brown"" -> quick_brown ""fox over"" -> fox_over ""lazy dog"" -> lazy_dog At query time your query becomes: ""the quick brown fox jumps over the lazy dog"" -> ""the_quick quick_brown brown_fox fox_jumps jumps_over over_the the_lazy lazy_dog"" This is still no good by default it will form a phrase query. So in your query analyzer only add PositionFilterFactory after the ShingleFilterFactory. This ""flattens"" the positions in the query so that the queryparser treats the output as synonyms which will yield a booleanquery with these subs (all SHOULD clauses so its basically an OR query): BooleanQuery: the_quick OR quick_brown OR brown_fox OR ... this should be the most performant way as then its really just a booleanquery of termqueries. well if you have a document with 3 tokens what i described will index two bigrams (quick brown fox -> quick_brown brown_fox). So it should generally work fine as well? Since you are only indexing bigrams it could return some false positives (imagine a query like ""brown fox quick brown dog"" this will match the 3-token document even though it doesnt actually contain ""quick brown fox"" but does contain both word bigrams) but its likely this will be rare? Thanks Robert. I'm still digesting your answer but seems reasonable. It seems to assume a fixed single size. Can be it adapted if you also have documents with 3 or more tokens?"
6,A,How to index forum discussions for search? For a discussion forum does it work better to index each entry inside a discussion thread as a separate lucene document or simple concat all entries within a discussion into one big block of text and index a whole discussion thread as a single lucene document? If you concatenate all entries within a discussion you run into the error where you cannot pin point the exact entry you want to retrieve. Lucene should be able to quickly index and search each entry (post/thread/whatever). Mashing them all together just seems overkill. With your suggestion the problem comes about when you want to display search results. You could potentially get 20 entries for the same discussion thread in your search result because all entries simply included the same word. Can you see what I mean? True it could return multiple entries but you still have control over what is ultimately displayed and you should be able to distinctly select singular instances of each entry.  If you decide to index them separately you can use Solr which is about to support search result collapsing: http://www.lucidimagination.com/blog/2010/09/16/2446/  Depends on what kind of search capabilities you are looking for. For eg if you want the users to be able to search for keywords that occurred in threads on some particular date then you must index all entries as separate documents with a date (as a NumericField searchable using a NumericRangeFilter). Indexing every entry as a separate document will also enable you to score each entry using the Lucene scorers which will help in retrieving the most relevant entries (and not threads) as a response to a query. Additionally you can also add the thread topic as a separate field to each entry-document (at the cost of little more space). Concatenating all entries is not a good idea if you want to point the user to the exact entry of interest. As to your concern(comment on Ryan's answer) on returning multiple entries from the same thread you can add a thread id to each entry while indexing. Then at the time of displaying results you can display only the entry for each thread id (the entry with the highest score could be displayed along with the thread topic)  I will prefer to index each entry separately. It will make the design more flexible as your system should have some kind of topic entity to group the entries in the same thread. And another issue to index with concatenation is it would need to re-index once new entry is posted which has performance impact.
7,A,"In Lucene how can a TokenFilter emit more than one term? I'm working with Lucene 3.2. How can I use a TokenFilter that doesn't just filter/modify a term but can also insert other terms into the stream? For example I want a filter that take as input ""tv42lcd"" and insert into the stream the words ""tv42lcd"" ""tv"" ""42"" ""lcd"". I'm aware that I could do this by implementing my own Tokenizer. But I rather still use the provided StandardTokenizer. How would your tokenizer know where to split the terms? e.g. why not ""tv42lcd"" ""tv"" ""42"" ""lc"" ""d"" or ""tv4"" ""2l"" ""cd""? This is just an example. But in my application I split based on the numeric chars. You can always mix default with custom: use StandardTokenizer logic where possible then wrap its output and add custom tokenization on the top. You can achieve this by extending but it's almost always better to use composition."
8,A,"Lucene: Indexsearcher: java.lang.UnsupportedOperationException I get the following error message java.lang.UnsupportedOperationException with Lucene search method: topDocs = searcher.search(booleanQuery null 100); when I'm trying to use the following implementation of MB25 Okapi Search (http://nlp.uned.es/~jperezi/Lucene-BM25)  booleanQuery.add(new BM25BooleanQuery(current_tags[i] ""tags"" new StandardAnalyzer()) BooleanClause.Occur.SHOULD); searcher = new IndexSearcher(INDEX_DIR); topDocs = searcher.search(booleanQuery null 100); I'm using an old version of Lucene: Lucene 2.4.1 (I cannot upgrade!) Can you help me to understand why I get such error ? thanks java.lang.UnsupportedOperationException at org.apache.lucene.search.Query.createWeight(Query.java:88) at org.apache.lucene.search.BooleanQuery$BooleanWeight.(BooleanQuery.java:185) at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:360) at org.apache.lucene.search.Query.weight(Query.java:95) at org.apache.lucene.search.Searcher.createWeight(Searcher.java:185) at org.apache.lucene.search.Searcher.search(Searcher.java:136) at NVoting.(NVoting.java:159) at Main.main(Main.java:8) Update BooleanQuery.java weights.add(c.getQuery().createWeight(searcher)); Query.java Weight weight = query.createWeight(searcher); The full stack trace will tell you where the exception was thrown and you can use that to investigate why. There might even be a note why this exception is deliberately thrown. If you look at Query API here method you are calling at org.apache.lucene.search.Query.createWeight(Query.java:88) takes Searcher object as parameter while you are passing a Query object. Hope this direct you on the right way. But then shouldn't I get a compiling error ? Anyway I've checked the code you mentioned and I've added the source to the question. It is correct the searcher is always passed (and not the query).  Here is what is happening: BM25BooleanQuery does not support the full Query API which includes createWeight(). While you can use most Lucene Query types as atoms in building a BooleanQuery this is not the case for BM25BooleanQuery. A BM25BooleanQuery has to stand on its own using BooleanTermQuery-s as its building blocks. If you stick to the instructions under ""How to use it"" in the URL you cited the queries should work. AFAIK there is no current solution to combining BM25 queries in Lucene into a larger query. Please see LUCENE-2091 and LUCENE-2392 for the state of the art on BM25 in Lucene. Sounds like a reasonable option - you can combine the resulting ScoreDoc[] s; This way you do not have to combine the queries. You will probably need to wrap this in a little code that indicates the source query of a specific document (unless you do not need this distinction). Thanks a lot I'm actually combining BM25BooleanQuery with MatchAllDocsQuery because I want to get all documents as result and not only the relevant ones. Maybe I can get such output differently you know how ? Maybe I can just combine the outputs of 2 different queries and not the queries themselves Not sure I got your last sentence. I actually need to remove from MatchAllDocsQuery results the documents returned by BM25BooleanQuery to avoid duplicates. I guess I can use doc ids for that... Also very important: my assumption is that if a doc is not returned by BM25 then its score is 0. (of course I haven't set any limit to results). Could you confirm this ? thanks One more thing: if I just pass the string ""tag1 tag2 tag3"" are the tags considered individually or not ? Or should I explicitly use BooleanTermQuery instead ? thanks @user680406 - I do not completely get your intention. Do you want to only keep documents NOT matching the query (i.e. a negative match?) You do need ids for this but better store separate ids. Do not use Lucene's doc ids as they are liable to change. About your second question - it depends on your analyzer. I guess the second option is better."
9,A,"What is best and most active open source .Net search technology? I'm trying to decide on an open source search/indexing technology for a .Net project. It seems like the standard out there for Java projects is Lucene but as far as .Net is concerned the Lucene.Net project seems to be pretty inactive. Is this still the best option out there? Or are there other viable alternatives? +1: I'm really interested in this. I tried going down the route of using SQL Server's full text indexing processor. It works well with searching binary files like pdf doc etc; but it is dog slow when searching regular columns. 6 seconds to search a 7000 row table is in my opinion unacceptable. For now I'm just using LIKE 'value%' simple searches which return extremely fast. While they were no 'full blown' releases (i.e. full documentation web site updates) of Lucene.Net for quite some time there are still fresh commits to its SVN repository. The latest release (2.3.2) for example was tagged in 07/24/09 (see here). Since the development is still active I would use it for new full-text-search projects. I kind of figured this was going to be the answer. Lucene.Net it is then. Thanks everybody!  If you don't really insist on .Net you can give Sphinx a try. Open source and available for all platforms (Windows / Linux).  SQLite has FTS3 (Full Text Search 3) that may do what you want it to do. I don't have direct experience with it but I believe it was developed explicitly to do what Lucene does at least in the simple case. I don't believe you can alter the tokenizer or anything (without modifying source code anyway) but it's an option. We use SQLite FTS in our product and it is very good and much faster than Lucene.NET for our specific cases.  I used to use DotLucene but ran into a number of problems. a major one was the fact that it required full trust to run. I have since moved to using SearchAroo: http://www.searcharoo.net/ it uses an XML data store and i have found its performance to be VERY similar to dot lucene. if you are looking for another option i'd definitely take a look.  I know this isn't open-source but it is a free and very comprehensive offering from Microsoft: Microsoft Search Server 2008 Express Out-of-the-box relevancy. Localized interface. Extensible search experience. No preset document limits. Continuous propagation indexing. Out-of-the-box indexing connectors Content summaries. Hit highlighting. Best bets and definitions. Query correction. Duplicate collapsing. Filter by property. Filter by language. Sort by date. E-mail/RSS alerts Interesting- I didn't know MS did a product like this. However the DB size limit is easily reached if you're going to use this for a search index. It's also not primarily designed for text-indexing and while text-indexing may work it'll perform rather poorly compared to something like lucene. ms search ... yuck!  After having used Lucene.Net in a couple projects I'd also like to add the suggestion of compiling the Java version of lucene into .net code with IKVM.NET. It works wonderfully and you never have to worry about being out-of-date with respect to the Java version. You also have the option of compiling all the extra libraries and using them as well (I'm using the GIS search stuff in one project). +1 for this obvious but easily overlooked option given Lucene.NET I haven't thought about this myself yet; did you encounter any obstacles that could make this difficult for non Java shops or is your experience with using IKVM for a project that size as smooth as it sounds like? @Mikos - pretty nifty idea; in case this turns out to be feasible with a project the size of Lucene it could be a nice precedence for this approach - or is this approach commonplace already and I've just been missing out? +1 I didn't even know this was available. Thanks. Have you thought of creating a codeplex project for this? Maybe setup a periodic build @Steffen - it really is that easy. The only problem I've run into was that the new version (3.0.2) of lucene uses a class that isn't available in the current version of IKVM's JVM. I ended up using SimpleFSLockFactory instead of NativeFSLockFactory.  Although its not .net i would recommend using Solr as its built on lucene and will be simple to integrate given the fact it returns XML/HTTP and JSON  Have a look at www.searcharoo.net. It has a crawler and features like work stemming indexing office documents/PDFs. The author is very active on the codeproject articles and responds to questions pretty quickly.  As I understand you need ""just"" a full-text index on your existing database and SQL Server full-text search in principle worked for you but your current implementation/setup is too slow. If I were you I wouldn't go for a completely different approach (just think about the mess to keep an external index in sync with your database or join query results from both etc.). Try to fix the performance issue with SQL Server as nobody would seriously assume that 6sec for searching 7k rows is the final word for a enterprise class solution that is used for some of the largest databases around... Maybe try to ask a new question about common pitfalls with this feature (I'm not an expert on this) and you might end up with a simple fix instead of a complete rebuild of your search architecture ;)  lucene.net will necessarily lag the java one since it is a port. I also don't like how the lucene port is a straight copy although it does make it easier on the docs I suppose. Something to consider is using Solr if you don't need super tight (binary) integration. I have used it before with good success. It is still powered by Lucene but I think it is better since it has some better features. You can use it from .net via an HTTP endpoint. One question to ask yourself is what you really need/want in a search solution. There are a lot of ways to go about implementing search and not all solutions work for every situation.  Lucene.net is implemented in nHibernate so if you also are looking for an O/R mapper the combination may be worth a deeper check. We currently develop a prototype and configuring Lucene is done in a bunch of minutes (we use fluent nhibernate). I am giving nHibernate a try as well. Thanks for the info."
10,A,"Cannot implement try/catch/finally when variable is final I have a problem. I am using a variable which has to be final because I am using it in a anonymous inner class. try { final IndexSearcher searcher = new IndexSearcher(index.getDirectory()true); searcher.search(query new HitCollector() { public void collect(int doc float score) { try { resultWorker.add(new ProcessDocument(searcher.doc(doc))); } catch (CorruptIndexException e) { log.error(""Corrupt index found during search"" e); } catch (IOException e) { log.error(""Error during search"" e); } } }); } catch (CorruptIndexException e) { log.error(""Corrupt index found during search"" e); } catch (IOException e) { log.error(""Error during search"" e); } finally { if(searcher != null) searcher.close(); } Problem is that I get a compiler error saying searcher cannot be resolved And if I move the searcher up like so: final IndexSearcher searcher; try { searcher = new IndexSearcher(index.getDirectory()true); Then I get compile error saying searcher may not be initialized. How can I fix this? PS: I cannot use Lombok @Cleanup because the field has to be final for the anonymous inner class to work try { // if new IndexSearcher throws searcher will not be initialized and doesn't need a close. The catch below takes care of reporting the error. final IndexSearcher searcher = new IndexSearcher(index.getDirectory()true); try { searcher.search(query new HitCollector() { public void collect(int doc float score) { try { resultWorker.add(new ProcessDocument(searcher.doc(doc))); } catch (CorruptIndexException e) { log.error(""Corrupt index found during search"" e); } catch (IOException e) { log.error(""Error during search"" e); } } }); } finally { searcher.close(); } } catch (CorruptIndexException e) { log.error(""Corrupt index found during search"" e); } catch (IOException e) { log.error(""Error during search"" e); } finally { } So here you are not closing the searcher in the catch? Because the new IndexSearcher throws CorruptIndex and IOException. Shouldn't saercher ble closed then? No I close it in the inner finally which will be executed always and only if `new IndexSearcher` succeeds. If the `new` throws searcher will never be initialized and doesn't need to be closed. Thanks. Seems to work Yeah generally you don't want to have `catch` and `finally` sharing the same `try` block.  It is a bit ugly but I think this will do the trick; IndexSearcher searcher = null; try { searcher = new IndexSearcher(index.getDirectory() true); final IndexSearcher finalSearcher = searcher; and replace searcher with finalSearcher in the anonymous inner class. Not sure if I like this hack :-) You don't have to *like* it :-)  put the body of the try{} block in another method:  IndexSearch searcher = openSearcher(); try { doSearch(searcher query resultWorker); } finally { searcher.close(); } private void doSearch(final IndexSearcher searcher Query query final ResultWorker resultWorker) { searcher.search(new HitCollector() { public void collect(int doc float score) { resultWorker.add(new ProcessDocument(searcher.doc(doc)); } }); }"
11,A,"Indexing and searching MySQL with solr I have set up Solr and am trying to index a simple 2 column 2 row table (MySQL 'test_tb' tabe within database 'test_db') with (first column) unique id (in the mysql of type int) and (second column) some text. I keep getting the error: WARNING: Error creating document : SolrInputDocument[{ID_F=ID_F(1.0)={1}}] org.apache.solr.common.SolrException: Document [null] missing required field: id What does this mean? Please help. schema.xml <fields> <field name=""ID_F"" type=""string"" indexed=""true"" stored=""true"" required=""false""/> <field name=""COLA"" type=""string"" indexed=""true"" stored=""true"" required=""false""/>  </fields> data-config.xml <dataConfig> <dataSource type=""JdbcDataSource"" driver=""com.mysql.jdbc.Driver"" url=""jdbc:mysql://localhost/test_db"" user=""root"" password=""root_pwd""/> <document name=""doc""> <entity name=""test_tb"" query=""select ID_FCOLA from test_tb""> <field column=""ID_F"" name=""ID_F"" /> <field column=""COLA"" name=""COLA"" /> </entity> </document> </dataConfig> In your schema you probably defined a required field named ""id"". Either you rename ID_F to id <field column=""ID_F"" name=""id"" />or change the value in your schema."
12,A,"Why size of lucene index increased if i index the same data? I implemented Hibernate search in my application i.e. based on Lucene. Whenever i indexes the database the size of the lucene indexes increase. But the result of the query return same no of results every time. Why the size of lucene increases each time if i index the same data everytime? FullTextSession fullTextSession = Search.getFullTextSession(getSession()); org.hibernate.Transaction tx = fullTextSession.beginTransaction(); Criteria criteria = fullTextSession.createCriteria(getPersistentClass()) .setResultTransformer(CriteriaSpecification.DISTINCT_ROOT_ENTITY) .setCacheMode(CacheMode.IGNORE) .setFetchSize(pageSize) .setFlushMode(FlushMode.MANUAL); int i = 0; List<ProdAttrAssociationVO> results = null; do { criteria = criteria.setFirstResult(i) .setMaxResults(pageSize); results = criteria.list(); for (ProdAttrAssociationVO entity : results) { fullTextSession.delete(entity); fullTextSession.index(entity); } // flush the index changes to disk so we don't hold until a commit if (i % batchSize == 0) { fullTextSession.flushToIndexes(); fullTextSession.clear(); } i += pageSize; } while (results.size() > 0); System.out.println(""ProdAttrAssociation Indexing Completed""); tx.commit(); I don't know anything about Hibernate but generally in Lucene deleted documents stay on the index until it is optimized. That could explain why you're seeing the index only growing. Try to run optimize() on the index. Not sure how you do it from Hibernate (I see it's a method on SearchFactory). Hope this helps."
13,A,Lucene-based database search engine I am planing to add search feature in my web application. I am using Struts 2 framwork for the application and the items that will be searched are strored in a Relational database. In order to achieve a full text search engine I have following doubts : For database based search engine should I use just lucene or some other utility based on Luncene like Solr luSql Compass etc. In case of Solr can it be embeded in to the web applcation rather than deploying it as a seaparate WAR. Is solr used to search relational databases or just file systems You really should tell us what technologies you're using for you web app. I am using Struts 2 framework look at hibernate search. i am pretty sure it fits ur needs as the data that is stored in DB is what will be indexed and hence made available for search. as far as I remember Solr stores the index on filesystem (though there might be a plugin to store the index in DB). u am also look at www.elasticsearch.com. it is from the creator of compass. i think there will be limited support provided for compass. Hibernate search will only work smoothly if he is already using hibernate for persistence right?  You are saying that your data is in database but you are not mentioning whether you are using a ORM solution. If you do I also suggest you have a look at Hibernate Search. Hibernate is a requirement but you can work against the classic Session or use JPA. The biggest advantage in an webapp is probably that you always work with managed entities and don't have to write boiler plate code to convert from and to Lucene Documents. Solr has its place as well though. It really depends on your requirements and architecture.  Depends on the level at which you want to operate. Since you are already inside a Java app it's pretty easy to use the Lucene API index your data and then use it again to search. Anything that you can do with Solr you could do with Lucene but probably with much more logic and glue code. Yes have a look at http://wiki.apache.org/solr/Solrj#EmbeddedSolrServer It can do both and much more http://lucene.apache.org/solr/features.html#Detailed+Features But is solr used to search relational databases or just file systems ?
14,A,"Why do we use Filters while searching I am working with Lucene.My work is to Query nd perform search on it. I want to know the use of Filters. I think your question is same as http://stackoverflow.com/questions/1271234/how-do-i-filter-my-lucene-search-results Use of filters in any place is about same. You write a predicate (set of rules that item should have) and use it to efficiently remove elements from collection where predicate did not apply. Personally I use Google Guava but code should be about the same. For an example lets assume that you have a collection of numbers and only need those what are over 4.  ArrayList<Integer> ints = Lists.newArrayList(3 6 8 3 4 6); System.out.println(ints); System.out.println(Collections2.filter(ints new Predicate<Integer>() { @Override public boolean apply(Integer i) {return i > 4 ? true : false; }})); This would print out: [3 6 8 3 4 6] [6 8 6] Thanks for the reply. But i Wanted to know in context of Lucene search.Further more i want to know about QueryFilter  Filters are different from queries in that filters can be cached. Basically when you use a filter Lucene stores a bitmap where bit i is 1 if the ith document matches the filter and 0 otherwise. If you do a search for everything that matches a query and a filter it will get the results of the query and bitwise-AND it with the cached filter. This can improve performance in some circumstances. Basically if you have one or two conditions that need to apply to almost every search (e.g. hide all documents which are ""high security"") then you might want to look into filters. Otherwise just doing normal queries should perform better since filters can take up a lot of memory."
15,A,"How to query SOLR for empty fields? I have a large solr index and I have noticed some fields are not updated correctly (the index is dynamic). This has resulted in some fields having an empty ""id"" field. I have tried these queries but they didn't work:  id:'' id:NULL id:null id:"""" id: id:['' TO *] Is there a way to query empty fields? Thanks If you have a large index you should use a default value  <field ... default=""EMPTY"" /> and then query for this default value. This is much more efficient than q=-id:["""" TO *]  According to SolrQuerySyntax you can use q=-id:[* TO *]. This worked for me in Solr 4.0 with an int field.  If you are using SolrSharp it does not support negative queries. You need to change QueryParameter.cs (Create a new parameter) private bool _negativeQuery = false; public QueryParameter(string field string value ParameterJoin parameterJoin = ParameterJoin.AND bool negativeQuery = false) { this._field = field; this._value = value.Trim(); this._parameterJoin = parameterJoin; this._negativeQuery = negativeQuery; } public bool NegativeQuery { get { return _negativeQuery; } set { _negativeQuery = value; } } And in QueryParameterCollection.cs class the ToString() override looks if the Negative parameter is true arQ[x] = (qp.NegativeQuery ? ""-("" : ""("") + qp.ToString() + "")"" + (qp.Boost != 1 ? ""^"" + qp.Boost.ToString() : """"); When you call the parameter creator if it's a negative value. Simple change the propertie List<QueryParameter> QueryParameters = new List<QueryParameter>(); QueryParameters.Add(new QueryParameter(""PartnerList"" ""[* TO *]"" ParameterJoin.AND true));  Try this: ?q=-id:["""" TO *] Even though the SolrQuerySyntax page says -id:[* TO *] only -id:["""" TO *] worked for me on solr 1.4."
16,A,"Help with Search Engine Architecture .NET C# I'm trying to create a search engine for all literature (books articles etc) music and videos relating to a particular spiritual group. When a keyword is entered I want to display a link to all the PDF articles where the keyword appears and also all the music files and video files which are tagged with the keyword in question. The user should be able to filter it with information such as author/artist place date/time etc. When the user clicks on one of the results links (book names for instance) they are taken to another page where snippets from that book everywhere the keyword is found are displayed. I thought of using the Lucene library (or Searcharoo) to implement my PDF search but I also need a database to tag all the other information so that results can be filtered by author/artist information etc. So I was thinking of having tables for Text Music and Videos and a field containing the path to the file for each. When a keyword is entered I need to search the DB for music and video files and also need to search the PDF's and when a filter is applied the music and video search is easy but limiting the text search based on the filters is getting confusing. Is my approach correct? Are there better ways to do this? Since the search content is limited only to the spiritual group there is not an infinite number of items to search. I'd say about 100-500 books and 1000-5000 songs. Lucene is a great way to get up and running quickly without too much effort along with several areas for extending the indexing and searching functionality to better suit your needs. It also has several built-in analyzers for common file types such as HTML/XML PDF MS Word Documents etc. It provides the ability to use a variety of Fields and they don't necessarily have to be uniform across all Documents (in other words music files might have different attributes than text-based content such as artist title length etc.) which is great for storing different types of content. Not knowing the exact implementation of what you're working on this may or may not be feasible but for tagging and other related features you might also consider using a database such as MySQL or SQL Server side-by-side with the Lucene index. Use the Lucene index for full-text search then once you have a result set go to the database to extract all the relational content. Our company has done this before and it's actually not as big of a headache as it sounds. NOTE: If you decide to go this route BE CAREFUL as the ""unique id"" provided by Lucene is highly volatile (it changes everytime the index is optimized) so you will want to store the actual id (the primary key in the database) as a separate field on the Document. Another added benefit if you are set on using C#.NET there is a port called Lucene.Net which is written entirely in C#. The down-side here is that you're a few months behind on all the latest features but if you really need them you can always check out the Java source and implement the required updates manually. If you're using files you can probably categorize them properly by file extension then depending on the extension of each file you just need to write code to properly build the Lucene Document that you want to add to your index. Hope that helps! so say I have a bunch of files in a folder that I want searched. How do I add the attributes (artist etc) to each file?  If you definitely want to go the database route then you should use SQL Server with Full Text Search enabled. You can use this with Express versions too. You can then store and search the contents of PDFs very easily (so long as you install the free Adobe PDF iFilter). Thanks. I think this might work. Is there a way to get ""snippets"" of text everywhere the keyword is found in a document using a sql query? Sorry not that I know of easily (but I've never tried).  You could try using MS Search Server Express Edition one of the major benefits is that it is free. http://www.microsoft.com/enterprisesearch/en/us/search-server-express.aspx#none Thanks for the tip. I looked into it briefly but I feel the full text search might be easier.  Yes there is a better approach. Try Solr and in particular check out facets. It will save you a lot of trouble. Correct Solr provides a web service interface to a Lucene search index. And yes facets can be used for filtering but also tells you your metadata about your search objects. Not sure what to make about ""installing software on client machines"" as Solr is a server-based implementation. No client-side stuff involved other than the application that exposes the search. Multi-faceted search is a good option if the user wants to do things like filter search results by author content type file size etc. However to my knowledge Solr can only be installed as a web-service so it would take a little longer to get up and running and installing the software on client machines could become a configuration nightmare. By ""client-side"" I just meant if this was meant to be built as an ""off the shelf"" application (e.g. users could build their own index). Based on the question it doesn't appear that way but if it's a possibility for the future then it's certainly something worth considering."
17,A,"Lucene.net returns the correct number of query hits but not the correct documents I'm new to Lucene and trying to sort this out. I'm indexing like this:  Directory dir = FSDirectory.Open(new System.IO.DirectoryInfo(dirIndexDir)); //Create the indexWriter IndexWriter writer = new IndexWriter(dir new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_29) true IndexWriter.MaxFieldLength.UNLIMITED); Document doc = new Document(); doc.Add(new Field(""keyform_type"" entry.keyForm.type Field.Store.YES Field.Index.NOT_ANALYZED)); doc.Add(new Field(""keyform_lang"" entry.keyForm.lang Field.Store.YES Field.Index.NOT_ANALYZED)); doc.Add(new Field(""keyform_dial"" entry.keyForm.dial Field.Store.YES Field.Index.NOT_ANALYZED)); doc.Add(new Field(""keyform_reg"" entry.keyForm.reg Field.Store.YES Field.Index.NOT_ANALYZED)); doc.Add(new Field(""keyform_term"" entry.keyForm.term.Value Field.Store.YES Field.Index.ANALYZED)); if(entry.refForm.type!=null) doc.Add(new Field(""refform_type"" entry.refForm.type Field.Store.YES Field.Index.NOT_ANALYZED)); if(entry.refForm.lang!=null) doc.Add(new Field(""refform_lang"" entry.refForm.lang Field.Store.YES Field.Index.NOT_ANALYZED)); if (entry.refForm.dial != null) doc.Add(new Field(""refform_dial"" entry.refForm.dial Field.Store.YES Field.Index.NOT_ANALYZED)); if(entry.refForm.reg!=null) doc.Add(new Field(""refform_reg"" entry.refForm.reg Field.Store.YES Field.Index.NOT_ANALYZED)); if(entry.refForm.term.Value!=null) doc.Add(new Field(""refform_term"" entry.refForm.term.Value Field.Store.YES Field.Index.ANALYZED)); doc.Add(new Field(""pos"" entry.pos Field.Store.YES Field.Index.NOT_ANALYZED)); for (int s = 0; s < entry.subject.Count; s++) { doc.Add(new Field(""subject_""+s entry.subject[s] Field.Store.YES Field.Index.NOT_ANALYZED)); } for (int g = 0; g < entry.sense.gloss.Count; g++) { doc.Add(new Field(""gloss_""+g entry.sense.gloss[g] Field.Store.YES Field.Index.ANALYZED)); } if (entry.signature.action != null) doc.Add(new Field(""action"" entry.signature.action Field.Store.YES Field.Index.NOT_ANALYZED)); if (entry.signature.source != null) doc.Add(new Field(""source"" entry.signature.source Field.Store.YES Field.Index.NOT_ANALYZED)); if(entry.signature.date==0) doc.Add(new Field(""date"" entry.signature.date.ToString() Field.Store.YES Field.Index.NOT_ANALYZED)); //Add the doc writer.AddDocument(doc); writer.Close(); I then query using this code:  //Doesn't matter what term is same result string term=""workers""; Directory dir = FSDirectory.Open(new System.IO.DirectoryInfo(luceneDir)); IndexSearcher searcher = new IndexSearcher(dir true); List<string> b=new List<string>(); b.Add(""keyform_gloss""); b.Add(""keyform_term""); b.Add(""refform_term""); b.Add(""refform_gloss""); for (int i = 0; i < nMaxDupes; i++) b.Add(""gloss_"" + i.ToString()); MultiFieldQueryParser mfqp = new MultiFieldQueryParser(Lucene.Net.Util.Version.LUCENE_29 b.ToArray() new StandardAnalyzer()); Query q = mfqp.Parse(term); TopDocs td = searcher.Search(q 300); for (int i = 0; i < td.totalHits; i++) { //Generate a dictionaryEntry for each hit Document doc = searcher.Doc(i); //Access the document fields blah } No matter what the value of term is Lucene returns the first X documents in the index where X = the number of documents that actually match term. When I browse the index using LUKE an identical hand-typed query (keyform_term:term gloss_0:term etc) returns both the correct number of results and the correct documents matching those results. The C# code above however always returns the first X documents which don't necessarily contain the search term in any of the searched fields. They're not even close. What am I doing wrong? I know the index is good because I can search it in LUKE so it has to be something in the query... Thanks! The line: Document doc = searcher.Doc(i); should be Document doc = searcher.Doc(td.scoreDocs[i].doc); or the correct C# syntax equivalent (I'm a Java guy sorry) Ohhh...Thanks! :) Just click the check mark to mark it the correct answer. ;) That's it exactly thanks! I'd vote the answer up if I could."
18,A,"Is there a HTML analyzer/tokenizer for Lucene? I wanted to index text from html in Lucene what is the best way to achieve this ? Is there any good Contrib module that can do this in Lucene ? EDIT Finally ended up using Jericho Parser. It doesn't create DOM and is easy to use. @DavidJames Congratulations you gave life to a year and half old thread :) Just google for ""Jericho Parser"" it solves the issue you defined anyway :) Also if you like the question you can favorite it and upvote :) This question is interesting. Converting to plain text before tokenizing could be a problem if it loses the natural delimiters that HTML tags provide. For example: ` New York City of Chicago` should not be tokenized as `New York City` `of` `Chicago` but that could happen it you convert it to `New York City of Chicago` first. Converting tag breaks to `\n` would probably help but you have to be careful: do you do it with ` `? Probably. ``? Probably not. Do you have the source for the tokenizer? I would recommend using Jsoup HTMP parser for extracting the text and then use lucene.It worked well for me.  I'm assuming that you don't actually want to index the HTML tags. If that's the case you can first extract text from HTML using Apache Tika. Then you can index the text in Lucene. Any better fast light weight tool than Tika ? That does only text extraction part ? @mbonaci @bajafresh4life : If I wanted *only* extraction of text and not the DOM and all other hassles would it still be TIKA ? You can try [htmlparser](http://htmlparser.sourceforge.net/). Or use the same library that Tika uses [TagSoup](http://home.ccil.org/~cowan/XML/tagsoup/). But if you ask me I'd still use Tika. @mbonaci : You'd still use Tika? Why ?? Reasons ? There are tons of examples out there; it's not heavy-weight; I usually need more than HTML parser or the need for other parsers arises later; it's reliable and OS; the both products are ASF so they will always work well together...  You might also want to take a look at /Lucene-3.0.3/src/demo which has an HTML parser example. That was the first thing I saw but believe me when I say I need more than that to work with... :)"
19,A,Is Solr available for .Net? I want to learn Solr. May I know some good tutorial/links for it? Also is Solr available for .NET? Have you tried the SOLR home site (http://lucene.apache.org/solr/)? Remember search engines are your friends! Lazarus FYI google brought me here. If you just want to replicate the Lucene.NET database between several machines (in a master-slave disposition) so you can try Lucene Steroids available at http://bitbucket.org/guibv/lucene.steroids. This code is based on Solr shell scripts and uses cwRsync to sync files. You get a lot more from using Solr than just replicating. Think of e.g. creating facets.  If you mean running the Solr server on .Net instead of Java then no there is no port. I've been trying to run it with IKVM here but it's low-priority to me so I can't put much time on it. It'd be great if someone can help out with this. If you mean using/connecting to Solr from a .Net application then yes you can use SolrNet or SolrSharp for that. I blogged about this not long ago. UPDATE: I made significant progress with Solr + IKVM. We use an older version of SolrNet and it is very easy to follow and integrate.. I'd recommend it. there seem to be a C# API for interoperation with Solr.. http://www.codeplex.com/solrsharp @Michael: yes I already linked to SolrSharp. Does either SolrSharp or SolrNet (or both) not use the RESTful interface to Solr but rather some direct interop? @soandos: Solr's interface is not RESTful. SolrNet only uses HTTP at the moment. There is an open issue to connect to SolrIKVM if you're interested in working on this: https://code.google.com/p/solrnet/issues/detail?id=69 Strange I got an error while doing a `Commit()` and the error log told me the `waitFlush` wasn't supported anymore in the latest version of Solr which was called in the background. Maybe an error on my side will check it soonish. I see my error now. I used the Nuget Prerelease version of the libraries (`0.4.0-beta2`) thinking it would be kind of up-to-date but it isn't. Now I've downloaded the `0.4.0.2002.zip` and referenced these assemblies and it works like a charm. @Jan_V : That's not correct. I'm the author of SolrNet and I'm still maintaining it. See https://github.com/mausch/SolrNet/blob/master/Documentation/README.md#downloads It appears these client connectors (SolrNet SolrSharp and MicroSolr) don't work anymore. They aren't updated for the newest version of Solr so some calls don't work anymore.  Yes it is there's at least one open source project on GitHub. https://github.com/mausch/SolrNet  Jeff Rodenburg created a C# API for interoperation with Solr. The API supports: Adding updating and deleting documents from a solr index. Configuration support for multiple solr instances. Flags for Read/Write modes in support of solr replication. Search queries that return strongly-typed objects Support for facets and more ... http://www.codeplex.com/solrsharp I already linked SolrSharp in my answer
20,A,"How to build a conceptual search engine? I would like to build an internal search engine (I have a very large collection of thousands of XML files) that is able to map queries to concepts. For example if I search for ""big cats"" I would want highly ranked results to return documents with ""large cats"" as well. But I may also be interested in having it return ""huge animals"" albeit at a much lower relevancy score. I'm currently reading through the Natural Language Processing in Python book and it seems WordNet has some word mappings that might prove useful though I'm not sure how to integrate that into a search engine. Could I use Lucene to do this? How? From further research it seems ""latent semantic analysis"" is relevant to what I'm looking for but I'm not sure how to implement it. Any advice on how to get this done? First  write a piece of python code that will return you pineapple  orange  papaya when you input apple. By focusing on ""is"" relation of semantic network. Then continue with has a relationship and so on. I think at the end  you might get a fairly sufficient piece of code for a school project.  I'm not sure how to integrate that into a search engine. Could I use Lucene to do this? How? Step 1. Stop. Step 2. Get something to work. Step 3. By then you'll understand more about Python and Lucene and other tools and ways you might integrate them. Don't start by trying to solve integration problems. Software can always be integrated. That's what an Operating System does. It integrates software. Sometimes you want ""tighter"" integration but that's never the first problem to solve. The first problem to solve is to get your search or concept thing or whatever it is to work as a dumb-old command-line application. Or pair of applications knit together by passing files around or knit together with OS pipes or something. Later you can try and figure out how to make the user experience seamless. But don't start with integration and don't stall because of integration questions. Set integration aside and get something to work. Good point on starting simple. In this case though potential customers for the app I'm building already have ""normal"" search engines. I have reason to believe that a more intelligent engine could add tangible value which is why I'd like to know if it is a feasible problem to attack before I jump in to make a ""me-too"" product. @DevX: Please slow down. A ""more intelligent engine"" is one thing. Build that first. Integration is the least of your worries. Save that for last after you get the ""more intelligent engine"" working. I'll repeat this because you don't seem to be reading it: integration can be left for last after you get some experience with the tools and solve the basic problem. +1 for the step 2.  This is an incredibly hard problem and it can't be solved in a way that would always produce adequate results. I'd suggest to stick to some very simple principles instead so that the results are at least predictable. I think you need 2 things: some basic morphology engine plus a dictionary of synonyms. Whenever a search query arrives for each word you Look for a literal match ""Normalize/canonicalze"" the word using the morphology engine i.e. make it singular first form etc and look for matches Look for synonyms of the word Then repeat for all combinations of the input words i.e. ""big cats"" ""big cat"" ""huge cats"" huge cat"" etc. In fact you need to store your index data in canonical form too (singluar first form etc) along with the literal form. As for concepts such as cats are also animals - this is where it gets tricky. It never really worked because otherwise Google would have been returning conceptual matches already but it's not doing that. Great idea on using the normalization first. NLTK Library/WordNet can be used to do this for sure. I wouldn't dismiss the conceptual tagging as impractical because Google hasn't done it yet though. Google deals with very open ended queries on billions of pages. Doing conceptual searches for them opens up a can of worms and besides users are generally only interested in the top 10 answers. That is general searchers want high accuracy. For my app though breadth is an important characteristic of the quality of the result. I don't want to miss out anything potentially relevant.  First I agree with most of the advice here about starting slow and first building bits and pieces of this grand plan devising a minimal first product and continuing from there. Second if you want to use some Wordnet functionality with Lucene there is a contrib package for interfacing Lucene queries with Wordnet. I have no idea whether it was ported over to pylucene. Good luck and be careful out there."
21,A,"Lucene .Net- What is a good method for creating index that is more complicated that key/value? I'm starting a project in which we are trying to index the contents of XML documents with Lucene .Net. In the little documentation I have found it seems that indexes can only consist of fileds with a single string value. The data that I am attempting to index is slightly more complicated than simple key value pairs. Here is an example of an xml document I would want to generate an index from:  <descriptor> <asset guid=""2AA7C8F9-2CB1-4A81-9421-C09F1D85939E"" generated-date=""2011-07-30"" generated-by=""hw/AutoMfg"" generated-with=""PMS""> <!-- information about where the asset can be used --> <target> <localization>en-us</localization> <localization>es-us</localization> <environment>desktop</environment> <environment>mobile</environment> </target> <!-- all contents of an asset must have the same version --> <version-information> <version-number source=""content"">9.1.123.4</version-number> <version-number source=""manufacturing"">9.1.123.4</version-number> <release-label>9.1</release-label> </version-information> <!-- catalog information about the primary role of the asset --> <role> <namespace>parent.type.family.some.thing</namespace> <mime-type>text/html</mime-type> <hwid>abc1234</hwid> </role> </asset> </descriptor> So I could see create fields named after the child elements of 'descriptor' but what about the child nodes there within? How can this data be indexed? Should I create a delimited string to represent the values of each fields? eg field: ""Target"" Value:""localization: en-us;es-us environment: desktop;mobile | ... Do I need to flatten my data out like in my example above to index it? Thanks! Take a look at Digester + Lucene. The .NET port of Digester is NDigester  Kind of tricky to give specific advice -- so much of it revolves around what you want to retrieve and how rather than the shape of the data. In any case I would start with Simone Chiaretta's excellent little series on lucene.net (1 2 3 4 5). One concept that will help alot is the fact that you can index the same field multiple times for a given document so you'll probably make something like: Target-Localization:en-us Target-Localization:es-us Target-Environment:desktop Target-Environment:mobile Lucene is fundamentally flat but capable of being deep while being flat in new and interesting ways."
22,A,"adapting text search for graph/molecule comparison algorithms I'm looking for a text search engine for a non-traditional sort of text search and I want advice on which tool (Lucene Sphinx Xapian or something else) is most appropriate for me plus pointers on where to get started. I have molecules represented as graphs (atoms and bond). I have a way to enumerate all subgraphs of up to size k. Being technical the inputs are SMILES and the output is canonical SMARTS and the number of times each subgraph/SMARTS occurs. For example if the input molecule is ""CCO"" then the canonical results are {""C"": 2 ""O"": 1 ""CC"": 1 ""OC"": 1 ""CCO"": 1} and if the molecule is ""SCO"" then the canonical results are {""C"": 1 ""S"": 1 ""O"": 1 ""CS"": 1 ""OC"": 1 ""SCO"": 1}. These are tiny examples. For real molecule I got around 500 ""words"" which look like ""CC(C)O"" ""CCCOCC"" ""cn"" and ""cccc(c)O"". Looking at molecules as a collections of characteristic strings plus counts means I should be able to use a text search tool to do comparisons at the text level with the hopes that they are meaningful at the chemistry level. For examples I can use cosine similarity perhaps with tf-idf weight and find similar molecules by looking for similar subpatterns. With the ""CCO"" and ""SCO"" examples above the cosine similarity is (2*1+1*1+1*1)/sqrt(2*2+1*1+1*1+1*1+1*1)/sqrt(6*(1*1)) = 4/sqrt(8*6) = 0.58. For another example if I want to find molecules which contain a ""CCS"" substructure then I can do a fast inverted index search based on the counts (the molecules must have at least 2 ""C""s at least 1 ""CS"" and so on) before tackling the NP subgraph isomorphism problem. That is text-based methods can act as a filter to reject obvious mismatches. I'm trying to figure out the text solutions which exist but it's a bit daunting. I don't need stop words I don't need stemming I don't care about word order; I don't need quite a number of the features which exist. I do need the ability to keep word vectors since it's important to know if ""C"" appears 2 time or 3. Which text search engine is most appropriate for me? It looks like Lucene especially with the work in Mahout. Can you recommend which parts of the documentation to look at or relevant tutorials? The ones I've found are meant for full-text searches with stemming and the other features I don't need. I have words W_i with repeat counts n_i and i < ~500. I want to do cosine similarity between them as per the linked definition. I think what I'm looking for is standard in the document search world and the chemistry doesn't matter but I'll update with an example. See also http://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python . What does ""similarity"" mean to you? E.g. should ""C=C"" be ""similar"" to ""C-C""? is ""N+"" similar to ""N""? Is ""cco"" similar to ""c(c)o"" etc? Perhaps if you gave a few example searches and the results they should find it would help us to know more about what you want (since we are not chemists). EDIT: I may have understood this better now. You want to compare graphs represented as strings. The strings have ""words"" which may repeat. You may use Lucene in which case I second the suggestion to use Solr. Basically each Solr document will consist of a single field; The field will contain the string which I suggest you unroll: write C C instead of C:2. If you use a space to separate the words you can use a WhiteSpaceAnalyzer. If you use another separator you may need to write a custom analyzer which is not so hard to do. Is this a good idea? I am not sure. Here's why: Lucene (and Solr) do not use cosine similarity as such but rather Lucene Similarity which mixes cosine TF/IDF and boolean scoring with some specific modifications. This works well for most textual use-cases but may be different than what you need. Do you need to compare hits from different searches? If you do it is hard to do using Solr as it normalized every search to a maximal value of 1. I suggest you do try Solr for a small sample of your database. If Solr works for you fine. If not shingling and min-hashes are probably the way to go. Mining of Massive Datasets by Rajaraman and Ullman is a recent free book about these subjects. I suggest you read it. It covers search for similar strings in mountains of data. I guess the differentiator is: Do you need a relatively large intersection? If so use shingling and min-hashes. If not maybe Solr is enough. String matching and sequence alignment? How so? My ""documents"" contain ""words"" which cam be repeated. Given a query document and a target document collection I want to find the 10 nearest in the collection based on (say) cosine similarity. Alignment algorithms implies order which my data doesn't have. Needleman–Wunsch Aho–Corasick and other string match algorithms just aren't applicable at least not as far as I can tell. (BTW I did work in bioinformatics for a bit so I know some of the places when they can be used.) I started reading that book the other day and it's very helpful. I will try with Solr and see what happens. I also came across gensim at http://nlp.fi.muni.cz/projekty/gensim/index.html . I have edited my answer to better address your documents and words.  Hmm... don't really know what are SMARTS or how chemical similarity actually work. If you want to use lucene first consider using solr. Since your data is in graphs you can take a look at neo4j with the solr component. Also would this problem be more closely related to document near duplicates? For helping with that there are a number of algorithms LSH Spotsigs shingling and simhash. Wish I could be of more help. Each compound has about 200-600 ""words"" selected from a vocabulary of about 200000 words. Thanks for the book recommendation! Ok I see what you mean well Solr is pretty easy to use. It is another layer on top of lucene. Do you know how many fields you may have per chemical? Use the Keyword tokenizer so that each of the input into a field that get indexed does not get tokenized and just don't filter indexing process with stemming or other special features. I do recommend that you get the book published by Packt. I think that is perhaps the only book avail on enterprise uses of the search engine. I want to see if text search can replace or simplify graph search. With 50 million molecules that's about 150 million atoms and as many bonds. I don't see how a generic graph db like neo4j can approach the capabilities of specialized chemistry search engines. But doing a cosine similarity search of 50 million documents each containing at most 1000 words (all unique) should be easy. I'm looking for a tool for that task.  Don't use lucene. Or Solr. The internal models are antiquated and cobbled together; although they do a good job. Find an engine with the minimal criteria (if you want to map inside a text engine) BM25F fully supported. If I were after it and I wanted scalability and performance and low cost support community frankly I'd go with SQL Server and cubes.Licensing with SQL Server could be a complete blocker. Good luck. I have no idea of why BM25F would be appropriate for what I'm doing. Why would it be better than cosine similarity? A friend suggested Xapian which has BM25 support but it doesn't appear to be that widely used. I use Macs and other unix variants so a Windows-only solution won't work."
23,A,"LockObtainFailedException updating Lucene search index using solr I've googled this a lot. Most of these issues are caused by a lock being left around after a JVM crash. This is not my case. I have an index with multiple readers and writers. I'm am trying to do a mass index update (delete and add -- that's how lucene does updates). I'm using solr's embedded server (org.apache.solr.client.solrj.embedded.EmbeddedSolrServer). Other writers are using the remote non-streaming server (org.apache.solr.client.solrj.impl.CommonsHttpSolrServer). I kick off this mass update it runs fine for a while then dies with a Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/.../lucene-ff783c5d8800fd9722a95494d07d7e37-write.lock I've adjusted my lock timeouts in solrconfig.xml <writeLockTimeout>20000</writeLockTimeout> <commitLockTimeout>10000</commitLockTimeout> I'm about to start reading the lucene code to figure this out. Any help so I don't have to do this would be great! EDIT: All my updates go through the following code (Scala): val req = new UpdateRequest req.setAction(AbstractUpdateRequest.ACTION.COMMIT false false) req.add(docs) val rsp = req.process(solrServer) solrServer is an instance of org.apache.solr.client.solrj.impl.CommonsHttpSolrServer org.apache.solr.client.solrj.impl.StreamingUpdateSolrServer or org.apache.solr.client.solrj.embedded.EmbeddedSolrServer. ANOTHER EDIT: I stopped using EmbeddedSolrServer and it works now. I have two separate processes that update the solr search index: 1) Servlet 2) Command line tool The command line tool was using the EmbeddedSolrServer and it would eventually crash with the LockObtainFailedException. When I started using StreamingUpdateSolrServer the problems went away. I'm still a little confused that the EmbeddedSolrServer would work at all. Can someone explain this. I thought that it would play nice with the Servlet process and they would wait while the other is writing. I'll double check. I think I'm updating 256 at a time in the mass-update process. How are you doing the update? You know that the writer keeps the lock as long as it's alive so if you want to swap writers you'll have to close the first then open the second right? Isn't this a horribly primitive locking technique for something as advanced as Lucene? Wouldn't it be better to use NoLockFactory and then implement something using just about any kind of proper concurrency mechanism: synchronised method or even an AtomicBoolean token?  >> But you have multiple Solr servers writing to the same location right? No wrong. Solr is using the Lucene libraries and it is stated in ""Lucene in Action"" * that there can only be one process/thread writing to the index at a time. That is why the writer takes a lock. Your concurrent processes that are trying to write could perhaps check for the org.apache.lucene.store.LockObtainFailedException exception when instantiating the writer. You could for instance put the process that instantiates writer2 in a waiting loop to wait until the active writing process finishes and issues writer1.close(); which will then release the lock and make the Lucene index available for writing again. Alternatively you could have multiple Lucene indexes (in different locations) being written to concurrently and when doing a search you would need to search through all of them. *  ""In order to enforce a single writer at a time which means an IndexWriter or an IndexReader doing deletions or changing norms Lucene uses a file-based lock: If the lock file (write.lock by default) exists in your index directory a writer currently has the index open. Any attempt to create another writer on the same index will hit a LockObtainFailedException. This is a vital protection mechanism because if two writers are accidentally created on a single index it will very quickly lead to index corruption."" Section 2.11.3 Lucene in Action Second Edition Michael McCandless Erik Hatcher and Otis Gospodnetić 2010  I'm assuming that you're doing something like: writer1.writeSomeStuff(); writer2.writeSomeStuff(); // this one doesn't write The reason this won't work is because the writer stays open unless you close it. So writer1 writes and holds on to the lock even after it's done writing. (Once a writer gets a lock it never releases until it's destroyed.) writer2 can't get the lock since writer1 is still holding onto it so it throws a LockObtainFailedException. If you want to use two writers you'd need to do something like: writer1.writeSomeStuff(); writer1.close(); writer2.open(); writer2.writeSomeStuff(); writer2.close(); Since you can only have one writer open at a time this pretty much negates any benefit you would get from using multiple writers. (It's actually much worse to open and close them all the time since you'll be constantly paying a warmup penalty.) So the answer to what I suspect is your underlying question is: don't use multiple writers. Use a single writer with multiple threads accessing it (IndexWriter is thread safe). If you're connecting to Solr via REST or some other HTTP API a single Solr writer should be able to handle many requests. I'm not sure what your use case is but another possible answer is to see Solr's Recommendations for managing multiple indices. Particularly the ability to hot-swap cores might be of interest. @three_cups_of_java: But you have multiple Solr servers writing to the same location right? My point is that this can't work - you should just share a single server. Okay I think I'm figuring it out. I just assumed I could have a solr server running in a servlet container and another solr server running in a separate process. I'll investigate this and get back to you. Thanks I'll have a look at how I'm using the writers. I am using a MultiCore setup. I've got a several webapps that write to the index and a separate process that is trying to update every record in the index (there are over 1 million documents in the index). I'm not sure your answer works for me since I'm using org.apache.solr.client.solrj.SolrServer for all my updates. (See my edited question.)"
24,A,"Problem with Lucene- search not indexing numeric values? I am using Lucene in PHP (using the Zend Framework implementation). I am having a problem that I cannot search on a field which contains a number. Here is the data in the index:  ts | contents --------------+----------------- 1236917100 | dog cat gerbil 1236630752 | cow pig goat 1235680249 | lion tiger bear nonnumeric | bass goby trout My problem: A query for ""ts:1236630752"" returns no hits. However a query for ""ts:nonnumeric"" returns a hit. I am storing ""ts"" as a keyword field which according to documentation ""is not tokenized but is indexed and stored. Useful for non-text fields e.g. date or url."" I have tried treating it as a ""text"" field but the behavior is the same except that a query for ""ts:*"" returns nothing when ts is text. I'm using Zend Framework 1.7 (just downloaded the latest 3 days ago) and PHP 5.2.9. Here is my code: <?php //========================================================= // Initializes Zend Framework (Zend_Loader). //========================================================= set_include_path(realpath('../library') . PATH_SEPARATOR . get_include_path()); require_once('Zend/Loader.php'); Zend_Loader::registerAutoload(); //========================================================= // Delete existing index and create a new one //========================================================= define('SEARCH_INDEX' 'test_search_index'); if(file_exists(SEARCH_INDEX)) foreach(scandir(SEARCH_INDEX) as $file) if(!is_dir($file)) unlink(SEARCH_INDEX . ""/$file""); $index = Zend_Search_Lucene::create(SEARCH_INDEX); //========================================================= // Create this data in index: // ts | contents // --------------+----------------- // 1236917100 | dog cat gerbil // 1236630752 | cow pig goat // 1235680249 | lion tiger bear // nonnumeric | bass goby trout //========================================================= function add_to_index($index $ts $contents) { $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Keyword('ts' $ts)); $doc->addField(Zend_Search_Lucene_Field::Text('contents' $contents)); $index->addDocument($doc); } add_to_index($index '1236917100' 'dog cat gerbil'); add_to_index($index '1236630752' 'cow pig goat'); add_to_index($index '1235680249' 'lion tiger bear'); add_to_index($index 'nonnumeric' 'bass goby trout'); //========================================================= // Run some test queries and output results //========================================================= echo '<html><body><pre>'; function run_query($index $query) { echo ""Running query: $query\n""; $hits = $index->find($query); echo 'Got ' . count($hits) . "" hits.\n""; foreach($hits as $hit) echo "" ts='$hit->ts' contents='$hit->contents'\n""; echo ""\n""; } run_query($index 'pig'); //1 hit run_query($index 'ts:1236630752'); //0 hits run_query($index '1236630752'); //0 hits run_query($index 'ts:pig'); //0 hits run_query($index 'contents:pig'); //1 hits run_query($index 'ts:[1236630700 TO 1236630800]'); //0 hits (range query) run_query($index 'ts:*'); //4 hits if ts is keyword 1 hit otherwise run_query($index 'nonnumeric'); //1 hits run_query($index 'ts:nonnumeric'); //1 hits run_query($index 'trout'); //1 hits Output  Running query: pig Got 1 hits. ts='1236630752' contents='cow pig goat' Running query: ts:1236630752 Got 0 hits. Running query: 1236630752 Got 0 hits. Running query: ts:pig Got 0 hits. Running query: contents:pig Got 1 hits. ts='1236630752' contents='cow pig goat' Running query: ts:[1236630700 TO 1236630800] Got 0 hits. Running query: ts:* Got 4 hits. ts='1236917100' contents='dog cat gerbil' ts='1236630752' contents='cow pig goat' ts='1235680249' contents='lion tiger bear' ts='nonnumeric' contents='bass goby trout' Running query: nonnumeric Got 1 hits. ts='nonnumeric' contents='bass goby trout' Running query: ts:nonnumeric Got 1 hits. ts='nonnumeric' contents='bass goby trout' Running query: trout Got 1 hits. ts='nonnumeric' contents='bass goby trout' Maybe you could try with Sphinx engine. I am running a site with more than 1 million regs  and sphinx its incredibly fast! The php api its also very easy to use. http://www.sphinxsearch.com/powered.html  I'm used to using Lucene under Java so I can't tell if your code is correct but it seems like the field is being tokanized in a manner that is stripping out anything exept [a-zA-Z]. It may help shed light on the situation to use an index explorer tool like http://www.getopt.org/luke/ to see exactly what is in the index.  The find() method tokenizes the query and with the default Analzer your numbers will be pretty much ignored. If you want to search for a number you have to construct the query or use an alternate analyzer that includes numeric values.. http://framework.zend.com/manual/en/zend.search.lucene.searching.html It is important to note that the query parser uses the standard analyzer to tokenize separate parts of query string. Thus all transformations which are applied to indexed text are also applied to query strings. The standard analyzer may transform the query string to lower case for case-insensitivity remove stop-words and stem among other transformations. The API method doesn't transform or filter input terms in any way. It's therefore more suitable for computer generated or untokenized fields. Note that newer versions of Zend Search Lucene include an alphanumeric analyzer; you just have to set it as default. Make sure to include this near the beginning of your indexing script as well as before you run $index->find(): Zend_Search_Lucene_Analysis_Analyzer::setDefault(new Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum_CaseInsensitive());  I was able to get text and numbers pretty readily by using Zend/Search/Lucene/Analysis/Analyzer/Common/TextNum.php as the default (use ::setDefault(...) as described above. My problem is that I was trying to index a large set of software and hardware wtih a long history and many version numbers. Zend Search Lucene was not tokenizing ""words"" like ""1.5.3"" or anything with a dot (IP addresses e.g.) underscore or hyphen. I first made a copy of TextNum.php renamed TextNumSSC.php (SSC is our application name) and tried editing the RegEx: do { if (! preg_match('/[a-zA-Z0-9.-_]+/' $this->_input $match PREG_OFFSET_CAPTURE $this->_position)) { // It covers both cases a) there are no matches (preg_match(...) === 0) // b) error occured (preg_match(...) === FALSE) return null; } Still no luck. Then I installed http://codefury.net/projects/StandardAnalyzer/ in the way instructed outside the Zend directory structure changed the RegEx to '/[a-zA-Z0-9.-_]+/' and now it works. Not sure the root cause of this but couldn't find anything on SO or web to address this dot issue."
25,A,Using solr for indexing different types of data I'm considering the use of Apache solr for indexing data in a new project. The data is made of different independent types which means there are for example botanicals animals cars computers to index. Should I be using different indexes for each of the types or does it make more sense to use only one index? How does using many indexes affect performance? Or is there any other possibility to achieve this? Thanks. Both are legitimate approaches but there are tradeoffs. First how big is your dataset? If it is large enough that you may want to partition it across multiple servers it probably makes sense to have different indexes. Second how important is performance - indexing it all together will likely result in worse performance but the degree depends on how much data there is and how complex the queries can get. Third do you have the need to query for multiple data types in the same search? If so indexing everything together can be a convenient way to allow this. Technically this could be achieved with separate indexes but getting the most relevant results for the query could be a challenge (not that it isn't already) Fourth a single index with a single schema and configuration can simplify the life of whoever will be deploying and maintaining the system. One other thing to consider is IDs - do the all of the different objects have a unique identifier across all types? If not you probably will need to generate this if you want to index them together. Thanks for your answer. I guess I really have to stick with multiple indexes since the generation of unique identifiers in one index would be a mess in my case. I played around with solr index distribution and using shards but they apparently were made for speeding up queries on huge datasets. I think five or even more cores isn't the way of use it is supposed to be. So my current thoughts are going towards just using Lucene without solr. I have a question. We have closer to 10 apps(approx 10000 rows of data per app with 10 columns; one or two columns will be big txt fields) and we also want to index all of our documents from shared drives this may be like 5000 word/pdf docs). We want to create a global search where you can search for anything you want and the results can be categorized by facets(apps) or modified date range filter etc. We will also use this search in each of these individual applications where user can search by txt and other fields like modified date modified user etc. Which of the two approach is better? From the research I have done it looks like people have a lot more than 10 cores and they are managing them (I don't know how well). Here is a [link](http://lucene.472066.n3.nabble.com/shareSchema-quot-true-quot-location-of-schema-xml-td3297392.html)
26,A,"How do I delete old documents from Lucene/Lucene.NET What is the idiomatic way to delete old documents from a Lucene Index? I have a date field (YYYYMMddhhmmss) on all of the documents and I'd like to remove anything more than a day old (for example). Should I perform a filtered search or enumerate through the IndexReader's documents? I'm sure the question is the same regardless of which platform Lucene is running on. Thanks! You could try using low-level APIs of Lucene. Get Term Enumerator from index with the term ""YYYY"". Iterate of the term enumerator to get terms. If the term's text doesn't with current date (or previous date) call IndexReader.deleteDocuments(term) with that term. Since you are not using Query object you will not get search related exception.  Searching for YYYYMMdd* should work as currently dates are stored as text strings. Once you have the results you could use IndexReader.delete to remove the docs you're not interested in. That seems to me the best way to achieve this. One problem I see with that approach is that I'll get a ""TooManyClauses"" exception when there are more than old 1024 documents. This really depends on your implementation. I will need to know the specifics but as a general rule you could either remove this warning for those searches since they are maintenance only anyway (by setting a higher max clauses count) or make more specific searches (YYMMddhh* etc.). Again all depends on your environment and implementation. I ended up doing a slight variation of this by using a MatchAllDocsQuery and a RangeFilter. Seems to be working OK so far..."
27,A,"Finding relevant keyword from a webpage On a side I have a Java CMS providing a Set of keywords On the other side I have a JavaScript Bookmarklet to curate a Webpage Is there a clever way to cross/match my set of keywords with the webpage content ? John Resign explain in some articles how to compress then search in a Dictionary terms but it seems really complicated. In fact I'm looking for a clever Java or JavaScript algorithm to match a Set of String in a text efficiently. My question is very close to this one: Effective search on a small text But I want to do it on 1 text instead of a database of texts Indexing with lucene works great but I don't know how to match efficiently indexed terms with all my keywords. You could use java like this: Set<String> keywords = new TreeSet<String>(Arrays.asList(""keyword1"" ""keyword2"")); String content = ""your doc here with keyword1 etc""; Set<String> contentWords = new TreeSet<String>(Arrays.asList(content.split("" ""))); contentWords.retainAll(keywords); // now contentWords contains only words from keywords in this case just ""keyword1"" if you want to get all the words from content that are not keywords use this instead: contentWords.removeAll(keywords); Using TreeSet should make it perform pretty well. This code which compiles and runs has been simplified for illustration. You would have to load your keywords from a DB etc and load your content from wherever etc. It's a little bit too straight forward ? You do not remove stop word in multilang case insensitive. retainAll() algorithm will do 2 loops  We have done some proof of concept based on Lucene indexed Set. It handle all language/text issues It is fast enought But a good answer needs a large relevant content. So results are sometimes stranges. And it's server side..."
28,A,Will CompassGps roll back if the transaction is rolled back? If CompassGps mirroring is used with Hibernate what happens if the database transaction rolls back? Will the changes to the index be rolled back (assuming file store for the index)? It seems like if the Lucene index is stored on the disk it won't be automatically rolled back unless Compass is smart enough to handle that for you. FWIW I am using the Searchable plugin with Grails. There wouldn't be anything to roll back on the Compass/Lucene end. Compass registers event listeners and acts on inserts deletes and updates to update the Lucene index. If a transaction rolled back then Compass wouldn't get those events. So Compass is only notified when/if a transaction commits successfully? There's not a lot of technical detail here but this section of the docs describes the integration: http://www.compass-project.org/docs/2.1.4/reference/html/gps-hibernate.html
29,A,How to Move Lucene Index results into SQL Server Database I have a little over 1 million records in my lucene database and would like to move them into a new database so I can more easily do advanced searching and join it with my existing tables etc. I have done some searching and haven't found a good/fast way to take my existing lucene database files and move them into a sql database. Any help would be appreciated or pointing me in the right direction. Details: My sql database is Microsoft SQL Server Management Studio. My application which creates the lucene database is a web scraper writing in c# EDIT: I am using Lucene.net How complex is the lucene db? Number of tables? As far as I know there are 12 fields which are inserted into the lucene database. The number of tables I need to create depends on how many I need to create. Not the answer you're looking for but I'd just like to point out that an index and a relational database are two vastly different things. Unless you're storing all the data in the index as well I really don't think what you're trying to do is possible. I understand that they are very different things but wondering if there is a way to structure a request of some sort to move my current Lucene index into a database structure @bvandrunen: are you storing the data in the index as well? I know with Lucene you have the ability to do that (although I would advise against it). If you do have all the data stored in the index as well then you can do what you want if not then what you're asking for is impossible.  Putting your Lucene index in DB negates the purpose of indexing. The main advantage of Lucene is extremely fast relevant searches over huge amount of text. Instead of putting index into the DB you might as well just use MSSQL Server full text search instead. I think you should consider your requirements once again and either switch to MSSQL full text search or use standard Lucene searching mechanisms.
30,A,"Storing words with apostrophe in Lucene index I've a company field in Lucene Index. One of the company names indexed is : Moody's When user types in any of the following keywordsI want this company to come up in search results. 1.Moo 2.Mood 3.Moodys 4.Moody's How should I store this index in Lucene and what type of Lucene Query should I use to get this behaviour? Thanks. The answer depends on the way you want the rest of your index to be built: 1. Should pluralization be represented? i.e. are ""Apple"" and ""Apples"" distinct? 2. Do you want to keep apostrophes or can they be obliterated? 3. Does a company name appear isolated or inside a larger field? thanks for ur comments...my response 1.No 2.I want Lucene not to keep apostrophes 3.Company name may appear isolted as well as in a larger field Based on your clarifications I want to divide your question into two and answer each in turn: How do I index words with apostrophes as equivalent to similar words without an apostrophe? e.g. mapping Moodys and Moody's to the same index term. How do I implement auto-complete search in Lucene - i.e. given an index find documents using word prefixes e.g. map Moo to Moodys ? 1 is relatively easy - Use a StandardToeknizer to create a token combining the apostrophe and s with the previous word then a StandardFilter to remove the apostrophe and s. This will convert Moody's to Moody. A StandardAnalyzer does this and much more (lowercasing and stop word removal) which may be more than you need. Using a stemmer should take both Moodys and Moody to the same token. Try SnowBallFilter for this. 2 is harder: Lucene's PrefixQuery to which Alan alluded will only work when the company name is the first word in a field. You need something like the answer to this question about auto-complete in Lucene.  The StandardAnalyser should work for 3 and 4 however won't work for 1 and 2. Without writing your own (complex) text analyser I would think about how you're expecting company names to be searched for. For example basic lucene search syntax means that you could find ""Moody's"" if you search using wildcards: ""Moo*"" and ""Mood*"". Therefore you might want to consider appending an ""*"" to the search term before submitting to lucene however this might cause some confusion if the user isn't aware of this wildcard addition under the hood."
31,A,"ConstantScoreRangeQuery in Lucene How is Lucene's ConstantScoreRangeQuery better than the old RangeQuery? In what case should you use still use RangeQuery? According to the RangeQuery documentation in your link A ConstantScoreRangeQuery: is faster than RangeQuery. does not cause a BooleanQuery.TooManyClauses exception if the range of values is large. does not influence scoring based on the scarcity of individual terms that may match. Suppose you are interested in scarcer terms being scored higher (say you are looking in a range of hours but want the scarcer hours to be scored higher - maybe you are looking for a ""slow"" period of the day to run a backup process). In that case the older RangeQuery seems preferable. The next generation will be the TrieRangeQuery currently in the contrib section. It will probably part of the Lucene 2.9 core. It provides faster range queries than both other methods."
32,A,"Does the latest Jackrabbit snapshot work properly with Lucene 3? I work on a Java web based app that uses both Jackrabbit and Hibernate Search. The problem I was facing was that Jackrabbit had a heavy dependency on Lucene 2 but Search requires Lucene 3 to work. I managed to do a bit of ""not so nice trickery"" to get this working (i.e. I had to find a way to have both Lucene 2 and 3 JARs in my WEB-INF/lib dir without any class conflicts). Although it all works fine I want to get rid of Lucene 2 completely but Jackrabbit has been holding this up for me. I came across the following link the other day and I see that the Jackrabbit guys have managed to upgrade to be compatible with Lucene 3 in a ""sandbox"" branch. My question is has anyone out there used this sandbox version? I have not had a chance to play with it yet so I was wondering if anyone can confirm that it does in fact work with Lucene 3 and if so did you face any issues when upgrading from the last stable Jackrabbit release to this sandbox / snapshot? This will help me greatly in my decision to upgrade to this now or to wait until they have another stable release. I recently merged the sandbox branch into trunk. See JCR-2415 for details. So the latest Jackrabbit trunk is based on Lucene 3.0.3 now. All of the test cases including integration tests pass. However since we are still very early in the release cycle there might still be some issues. Excellent news! I'm still sceptical to use a snapshot in our production environment though any ideas around when the next stable release will be?"
33,A,"smart search by first/last name I have to build a search facility capable of searching members by their first name/last name and may be some other search parameters (i.e. address). The search should provide a list of match candidates so that the user can select whatever he/she seems the ""correct"" match. The search should be smart enough so that the ""correct"" result would be among the first few items on the list. The search should also be tolerant to typos and misspellings and may be even be aware of name shortcuts i.e. Bob vs. Robert or Bill vs. William. I started investigating Lucene and the family (like elastic search) as a tool for the job. While it has an impressive array of features addressing similar problems for the full text search I am not so sure how to use them for my task - up to the point that maybe Lucene is not the right tool here at all. What do you guys think - how can I harness Elastic Search to solve my problem? Or should I look elsewhere? Lucene supports edit distance queries so that your search query will tolerate some typos you define this as the allowed edit distance for a term. for instance: name:johnni~0.8 would return ""johnny"" Also Solr provides a wide array of ready made search filters and analyzers you can use for search. In your case I would probably chain several filter factories together: TrimFilterFactory - trim the query LowerCaseFilterFactory - to get rid of case differences ISOLatin1AccentFilterFactory - to remove accents from letters (most people don't search with the accent anyway) PhoneticFilterFactory - for matching sounds like queries like: kris -> chris look at the documentation under the link it is pretty straight forward how to set up a new solr instance with an Analyzer that uses all the above filters. I used something similar for searching city names and it worked fairly well.  Lucene can be made tolerant of typos and misspellings and can use synonyms. As for The search should be smart enough so that the ""correct"" result would be among the first few items on the list Are there any search engines which don't try to do this? Well - yes and no. i.e. google is giving you suggestions even you yourself are not sure what are you looking for because you are looking for something (not sure what) relevant to your search terms. In my case I will know that I found my record (document) as soon as I see it. And I am pretty sure I will need only one @mfeingold: unfortunately if your criterion is ""will it match what I (mfeingold) expect?"" I don't think anyone here can answer since we have no clue what you'd expect. Why not just try it? It's ridiculously easy to set up Solr.  As far as Bob/Robert goes that can be done with synonyms but you need to get the synonym data from some reliable source. In addition to what @Asaf mentioned you might try to use N-gram indexing to deal with spelling variants. See the CJKAnalyzer for an example of how to do that."
34,A,"High level explanation of Similarity Class for Lucene? Do you know where I can find a high level explanation of Lucene Similarity Class algorithm. I will like to understand it without having to decipher all the math and terms involved with searching and indexing. Think of each document and search term as a vector whose coordinates represent some measure of how important each word in the entire corpus of documents is to that particular document or search term. Similarity tells your the distance between two different vectors. Say your corpus is normalized to ignore some terms then a document consisting only of those terms would be located at the origin of a graph of all of your documents in the vector space defined by your corpus. Each document that contains some other terms then represents a point in the space whose coordinates are defined by the importance of that term in the document relative to that term in the corpus. Two documents (or a document and search) whose coordinates put their ""points"" closer together are more similar than those with coordinates that put their ""points"" further apart.  Lucene's built-in Similarity is a fairly standard ""Inverse Document Frequency"" scoring algorithm. The Wikipedia article is brief but covers the basics. The book Lucene in Action breaks down the Lucene formula in more detail; it doesn't mirror the current Lucene formula perfectly but all of the main concepts are explained. Primarily the score varies with number of times that term occurs in the current document (the term frequency) and inversely with the number of times a term occurs in all documents (the document frequency). The other factors in the formula are secondary adjusting the score in attempt to make scores from different queries fairly comparable to each other. -1 The book says less than the JavaDocs so don't bother buying.  How was mentioned by erickson in Lucene is Cosine similarity Term Frequency-Inverse document frequency (TF-IDF). Imagine that you have two bags of terms in the query and in the document. This measurement only match exactly terms and after in the context include their semantically weights. Terms with very frequetly occurence has smaller weight (importancy) because you could them find it in lot of documents. But the serious problem what I see that Cosine similarity TF-IDF is not so robust on more inconsistent data where you need to compute similarity betweens the query and the document more robust e.g. misspeling typographical and phonetical errors. Because the words must have exact match."
35,A,"How to do partial word searches in Lucene.NET? I have a relatively small index containing around 4000 locations. Among other things I'm using it to populate an autocomplete field on a search form. My index contains documents with a Location field containing values like Ohio Dayton Ohio Dublin Ohio Columbus Ohio I want to be able to type in ""ohi"" and have all of these results appear and right now nothing shows up until I type the full word ""ohio"". I'm using Lucene.NET v2.3.2.1 and the relevant portion of my code is as follows for setting up my query.... BooleanQuery keywords = new BooleanQuery(); QueryParser parser = new QueryParser(""location"" new StandardAnalyzer()); parser.SetAllowLeadingWildcard(true); keywords.Add(parser.Parse(""\""*"" + location + ""*\"""") BooleanClause.Occur.SHOULD); luceneQuery.Add(keywords BooleanClause.Occur.MUST); In short I'd like to get this working like a LIKE clause similar to SELECT * from Location where Name LIKE '%ohi%' Can I do this with Lucene? Yes this can be done. But leading wildcard can result in slow queries. Check the documentation. Also if you are indexing the entire string (eg. ""Dayton Ohio"") as single token most of the queries will degenerate to leading prefix queries. Using a tokenizer like StandardAnalyzer (which I suppose you are already doing) will lessen the requirement for leading wildcard. If you don't want leading prefixes for performance reasons you can try out indexing ngrams. That way there will not be any leading wildcard queries. The ngram (assuming only of length 4) tokenizer will create tokens for ""Dayton Ohio"" as ""dayt"" ""ayto"" ""yton"" and so on. Thanks for the response. I'm not too worried about the slow queries yet as I'd like to see it work first before I decide if it's too slow or not. My location list should stay steady at around 4000 documents so I'm not too worried about it getting any bigger. When you say ""Yes this can be done."" could you elaborate a little more? I thought that the code I displayed above should be doing what I'm expecting but it's not. Any ideas on what I'm doing wrong?  it's more a matter of populating your index with partial words in the first place. your analyzer needs to put in the partial keywords into the index as it analyzes (and hopefully weight them lower then full keywords as it does). lucene index lookup trees work from left to right. if you want to search in the middle of a keyword you have break it up as you analyze. the problem is that partial keywords will explode your index sizes usually. people usually use really creative analyzers that break up words in root words (that take off prefixes and suffixes). get down in to deep into understand lucene. it's good stuff. :-)  Try this query: parser.Parse(query.Keywords.ToLower() + ""*"") That did the trick! You had just what I needed. /GBT: werd!!! This answer does not reflect what the final code should look like. I'm at a loss on where to put this? What type is ""query""? A final sample would be great. In Java at least you should trim spaces as the query ""Test*"" will compile while ""Test *"" won't @thinkzig: I know it's an old question but could you maybe edit this answer to show how the final code looks like (where should this line be added to)? Thanks!"
36,A,"How can I use Lucene for personal name (first name last name) search? I'm writing a search feature for a database of NFL players. The user enters a search string like ""Jason Campbell"" or ""Campbell"" or ""Jason"". I'm having trouble getting the appropriate results. Which Analyzer should I use when indexing? Which Query when querying? Should I distinguish between first name and last name or just index the full name string? I'd like the following behavior: Query: ""Jason Campbell"" -> Result: exact match for 1 player Jason Campbell Query: ""Campbell"" -> Result: all players with Campbell in their name Query: ""Jason"" -> Result: all players with Jason in their name Query: ""Cambel"" [misspelled] -> Result: all players with Campbell in their name you might be interested in this blog post of mine for doing name search: http://www.opensourceconnections.com/2013/08/21/name-search-in-solr/ StandardAnalyzer should work fine for all above queries. Your first query should be enclosed in double-quotes for an exact match your last query would require a fuzzy query. For example you could set Cambell~0.5 and you could get Campbell as match(with the numeric value after the tilde indicating the fuzziness). BTW I would suggest using Solr which provides features for spell-check and auto-suggest so you wouldn't have to reinvent the wheel. This is similar to Google's ""did you mean..."" Which Query implementation would you use? I'm having a tough time getting TermQuery to match an exact phrase. (You can programmatically set FuzzyQuery's fuzziness factory no need for tilde notation.) As the term suggests (no pun intended) a TermQuery is a for a term you should look at a query based on the case. If you want to match ""John Smith"" *exactly* then use PhraseQuery. If you want to Johnson Smith"" when the user types John Smith you should look at FuzzyQuery i basically ended up using something like you suggested. thanks. first try an exact match using either TermQuery or PhraseQuery depending on how many terms are in the query. Same approach for fuzzy query because it takes single terms as its input so you need to build up phrases using BooleanQuery. Thanks this helped."
37,A,"Choosing a stand-alone search server with custom ranking function spartial search I'm looking into the different options for choosing a search server for a project I'm involved in. The search server is used to power results on a dating website built in Rails in which the search provides all the 'matchmaking'-magic. Typical queries would involve ranking documents/results using an expression (in pseudo-code): Order by ranking: +50 if has_image attribute is true +10 if has_boost attribute is true +50 if latitude/longitude is within 40 miles from [point] +20 if latitude/longitude is within 80 miles [point] -(distance from attribute 'age' to 30) Filter by: Attribute 'age' between 25 and 35 Attribute 'sex' equals 'male' Per default I'm not needing the full-text features of most of the search servers out there and I do not need the full documents to be retrieved - just a unique ID. The nature of the project yields for a search-server with the following properties: Spartial ranking Ranking of results based on a custom function Attribute filters Scalable and fast Free I've found Sphinx Solr and ElasticSearch but all of these are (as far as I see) built and optimized for full-text searching with both ES and Solr built on Lucene and I don't know what would perform best for filter/attribute heavy searching. My questions: Which of these servers would you prefer and why? Have I missed any other obvious choices? Don't know about the others but Solr can do all of this: Spatial ranking You'll need a nightly build of Solr (the latest stable release as of this writing Solr 1.4.1 doesn't include this feature) as far as I know this is a pretty stable feature in trunk. Ranking of results based on a custom function Solr has lots of function queries to do boosting. Attribute filters This is a common search feature. Scalable and fast Lots of big websites are using Solr evidence of its scalability and speed. Free Solr is Apache licensed a very permissive license.  ElasticSearch has all these feature as well. Geographic distance/bounding box/polygon and custom score scripts in various languages are supported: http://www.elasticsearch.com/docs/elasticsearch/rest_api/query_dsl/ You will have no problem with performance of filters or other query types we're doing heavy filtering on our queries with 100+ attributes in some cases and it is fast. Another thing to take into account is integration with your data store. ES has a nice River feature for this but it's not compatible with all data stores but similar can be achieved via post commit hooks. Also social sites benefit from (near) realtime search and ElasticSearch has a 1 second default. It also is much more clean to configure and scale than Solr. This is my opinion after a month long eval of each app. It also does a really good job of adapting to your data model. Hope this helps. Paul  I think that while you could use a search engine like Solr or ES to power this I think that the ""business rules"" that you've defined mean that you are going to end up doing post processing. I think that the filtering and basic search is pretty easily done in your search engine but I am guessing that the ordering logic is going to end up pretty custom and complex and trying to push that into your search queries may be like putting a round peg in a square hole... You may be better off querying for results and then using your own post processor rules library to handle the ordering.  You aren't talking about a search engine. You're talking about a database. in SQL filtering is standard SELECT stuff; the ranking can be done by a somewhat crufty expression involving lots of CASE and then ORDER BY. To do the spatial parts of the query you will need a database with geospatial features. The only scalable fast free relational database with geospatial features is PostgreSQL."
38,A,"opening lucene index stored in hdfs How to read a lucene index directory stored over HDFS i.e. How to get IndexReader for the index stored over HDFS. The IndexReader is to opened in a map task. Something like: IndexReader reader = IndexReader.open(""hdfs/path/to/index/directory""); Thanks Akhil I think the Katta project might be what you are looking for. I haven't used it myself but been researching these kind of solutions recently and this seems to fit the bill. It's a distributed version of lucene using sharded indexes. http://katta.sourceforge.net/  Check out this blog article which Indexes and Searches the files on HDFS. http://www.drdobbs.com/article/print?articleID=226300241  If you want to open a Lucene index that's stored in HDFS for the purpose of searching you're out of luck. AFAIK there is no implementation of Directory for HDFS that allows for search operations. One reason this is the case is because HDFS is optimized for sequential reads of large blocks not small random reads which Lucene incurs. In the Nutch project there is an implementation of HDFSDirectory which you can use to create an IndexReader but only delete operations work. Nutch only uses HDFSDirectory to perform document deduplication. There is indeed a Directory implementation http://hadoop.apache.org/common/docs/r0.18.3/api/org/apache/hadoop/contrib/index/lucene/FileSystemDirectory.html to open Lucene directory over a general FS System But unfortunately I am getting this AbstractMethodError http://stackoverflow.com/questions/2763038/java-abstractmethoderror when I use IndexReader.open(new FileSystemDirectory(.....)). in a map task Didn't realize FileSystemDirectory existed. I would be wary of it though. It doesn't seem like it's actively maintained."
39,A,"Solr: Populate Separate Fields from a Tokenizer I've created a custom Tokenizer in Solr that looks for named entities. I would like to be able to use this information to populate separate fields within the lucene/solr document. As an example I want to populate a multivalued field called ""locations"" with all the location names that were extracted from the text. To extract locations the text is first tokenized to separate the words and to determine which tokens are locations. After this process I would like to emit the tokens for the tokenizer but also populate the field ""locations"" with all the location names that were extracted from the text. From the research that I've done there is no way to access the SolrDocument object from the Tokenizer or the TokenizerFactory so there is no way to populate fields from here. The solution that I've come up with so far is to create a custom UpdateRequestProcessorFactory that processes the text and extracts the fields and then the Tokenizer processes the text AGAIN to get the tokens. I would like to find a way to be able to do this work and only process the text once. Here's an idea I think would work in lucene but I have no idea if it's possible in solr. You could tokenize the string outside the typical tokenstream chain as you suggest then manually add the tokens to the document using the NOT_ANALYZED option. You have to add each token separately with document.add(...) which lucene will treat as a single field for searching.  The way I do it is less elegant that what it looks like you are shooting for: I preprocess the documents using a named entity recognizer and save all of the entities in a separate file. Then when I am publishing to Solr I just read the entities from this file and populate the entity fields (different for people locations and organizations). This could be simplified but since I had already done the parsing for other work it was easier to just reuse what already existed."
40,A,"How to get reliable docid from Lucene 3.0.3? I would like to get the int docid of a Document I just added to a Lucene index so that I can stick it into a Filter to update a standing query. My documents have a unique external id so I thought that doing a TermDocs enumeration on the unique id would return the correct document like this: protected int getDocId(IndexReader reader String idField Document doc) throws IOException { String id = doc.get(idField); TermDocs termDocs = reader.termDocs(new Term(idField id)); int docid = -1; while (termDocs.next()) { docid = termDocs.doc(); Document aDoc = reader.document(docid); String docIdString = aDoc.get(idField); System.out.println(docIdString + "": "" + docid); } return docid; } Unfortunately this loops and loops returning the same docIdString and increasing docids. What is the recommended way to get the docids for newly-added documents so that I could use them in a Filter immediately after the documents are commited? While Xodarap's answer below is far better than my code above the code actually worked -- the problem was that I had duplicate external ids due to iterative debugging. The doc Id of a document is not the same as the value in your id field. The doc ID is an internal Lucene identifier which you probably shouldn't access. Your field is just a field - you can call it ""ID"" but Lucene won't do anything special with it. Why are you trying to manually update the filter? When you commit merges can happen etc. so the IDs before will not be the same as the IDs afterwards. (Which is just an example of the general point that you shouldn't rely on Lucene IDs for anything.) So you don't need to just add that one document to the filter you need to update the whole thing. To update a cached filter just run a query for ""foo"" and use your filter with a CachingWrapperFilter. EDIT: Because your id field is just a field you do a search for it like you would anything else: TopDocs results = searcher.Search(new TermQuery(new Term(""MyIDField"" Id)) 1); int internalId = results.scoreDocs[0].doc; However like I said I think you want to ignore internal IDs. So I would build a filter from a query: BooleanQuery filterQuery = new BooleanQuery(); // or get existing query from cache filterQuery.Add(new TermQuery(new Term(""MyIdField"" Id)) BooleanClause.Occur.SHOULD); // add more sub queries for each ID you want in the filter here Filter myFilter = new CachingWrapperFilter(new QueryWrapperFilter(filterQuery)); I have potentially thousands of standing queries that I would like to evaluate when new docs are added. I don't want to re-run them on the entire collection since I should be able to do incremental updates on the results from each query that I have cached elsewhere. A CachingWrapperfilter doesn't quite feel right: it'll cache the old results but not the new docs. It seems like docids should have adequate duration for my purposes but maybe I am misreading their definition. @Gene Golovchinsky: Lucene IDs [are not persistent](http://wiki.apache.org/lucene-java/LuceneFAQ#When_is_it_possible_for_document_IDs_to_change.3F). Can you try to manually tell when they change? Sure. But you're taking on a lot of work for minimal gain. I would advise you to be really sure that filtering your way is critical - if you just use caching filters it will work 99.9999% of the time. Premature optimization is the root of all evil. thanks for your suggestions! I am not actually trying to persist the docids. I am persisting external ids per my sample code above and at a given point in time -- between any commit() or optimize() calls -- I would like to convert the external ids to their corresponding docids build a filter & use it. The problem I am having is that I am not getting expected results from the conversion step. Perhaps I am not using the right method of converting external ids to docids. @Gene Golovchinsky: I've added some code that might help. As I said the issue you were having before is that Lucene's doc ID != value of your id field so you can't enumerate by switching their values. Thanks for your help!"
41,A,Word Co-occurrence - find the co-occurence of a term in a set of n-grams How would I go about writing a co-occurence class in something like Java that takes a file full of n-grams and calculates word co-occurence for a given input term. Are there any librarys or packages which work with Lucene (indexes) or something like a map-reduce over the n-gram list in Hadoop..? Thanks. Anyone have any ideas... Thinking Solr/Lucene might be best approach for this but not sure how... ...sounds dumb but: is it really needed? If the corpus is a few millions of tokens or the ngrams a few millions a simple java or python program will do. I did something like that once just perl and mysql and displayed it on a webpage @GatoVolador do you have any example? Ok so assuming you want to find the co-occurrence of two different words in a file of ngrams.... Here's pseudo code-ish Java: // Co-occurrence matrix Hashmap<StringHashMap<StringInteger>> map = new HashMap(); // List of ngrams ArrayList<ArrayList<String>> ngrams = ..... // assume we've loaded them into here already // build the matrix for(ArrayList<String> ngram:ngrams){ // Calculate word co-occurrence in ngram for all words // result is an map strings-> count // words in alphabetical order Hashmap<String<ArrayList<String>Integer> wordCoocurrence = cooccurrence(ngram) // assume we have this // then just join this with original } // and just query with words in alphabetic order Doing a count like this would probably be pretty with Pig but you're probably more familiar with that than me
42,A,"How can I use lucene's shingleanalyzerwrapper + standardanalyzer + indexreader? I hope you can help me with this problem. What I intend to do: Given a right text I want to count the frequencies for every stemmized token ngrams without the stopwords(in other words the stopwords are already removed). This is the situation: I am indexing some texts with IndexWriter using ShingleAnalyzerWrapper + StandardAnalyzer and when I add a document to IndexWriter(like this: indexwriter.addDocument(doc analyzer); where analyzer is again ShingleAnalyzerWrapper + StandardAnalyzer ). But the problem is: When I get the term frequencies and the terms the stopwords seem to be substituted by underlines. This is the input: String text = ""to i want to to i want to linked""; String text2 = ""super by by hard easy ""; This is the output: term:|freq:6 term: _|freq:2 term:_ hard|freq:1 term:_ i|freq:2 term:_ link|freq:1 term:easy|freq:1 term:hard|freq:1 term:hard easy|freq:1 term:i|freq:2 term:i want|freq:2 term:link|freq:1 term:super|freq:1 term:super _|freq:1 term:want|freq:2 term:want _|freq:2 If anything was unclear please ask me so I try to make myself more clear Thanks for the help please see http://www.lucidimagination.com/search/document/e5681676403a007b/can_i_omit_shinglefilter_s_filler_tokens for some solutions. In this case it seems like you probably want to disable position increments on your stopfilter as you don't want to introduce a ""hole"" where the stopword was you want to pretend like they never existed. thats exactly what I wanted thank you very much. The link gives a page not found; this appears to be the same discussion though: http://www.gossamer-threads.com/lists/lucene/java-user/123704?do=post_view_threaded#123704"
43,A,"how to use ""name in (name1name2)"" in lucene query How do I use ""where field in (value1value2value3)"" in lucene query? I'm using lucene query in dotCMS. If I understand your question correctly this query would be equivalent: field:value1 OR field:value2 OR field:value3"
44,A,"Getting the Doc ID in Lucene In lucene I can do the following doc.GetField(""mycustomfield"").StringValue(); This retrieves the value of a column in an index's document. My question for the same 'doc' is there a way to get the Doc. Id ? Luke displays it hence there must be a way to figure this out. I need it to delete documents on updates. I scoured the docs but have not found the term to use in GetField or if there already is another method. The inner Lucene id is not set in stone. A better way to delete documents is to store a unique id as one of the document's fields and delete using its value. As in Lucene 3.0 Hits class is deprecated can some one suggest how to get the doc id in further versions? Thanks. I suspect the reason you're having trouble finding any documentation on determining the id of a particular Lucene Document is because they are not truly ""id""s. In other words they are not necessarily meant to be looked up and stored for later use. In fact if you do you will not get the results you were hoping for as the IDs will change when the index is optimized. Instead think of the IDs as the current ""offset"" of a particular document from the start of the index which will change when deleted documents are physically removed from the index files. Now with that said the proper way to look up the ""id"" of a document is:  QueryParser parser = new QueryParser(...); IndexSearcher searcher = new IndexSearcher(...); Hits hits = searcher.Search(parser.Parse(...); for (int i = 0; i < hits.Length(); i++) { int id = hits.Id(i); // do stuff }  Turns out you have to do this: var hits = searcher.Search(query); var result = hits.Id(0); As opposed to var results = hits.Doc(i); var docid = results.<...> //there's nothing I could find there to do this"
45,A,"Keyword search engine that returns statistics instead of hits First post on StackOverflow but I've always looked to this site as a great source of shared knowledge and I'm excited to see what comes up from this question. As I feel I have now reached the limits of what I can do with SQL indexes statistics and full-text search I'm currently looking for a search library that can provide us with the functionality we need. I'm not averse to writing it myself (and open-sourcing it if I can get the boss's approval) but I would prefer to find something open-source that already exists natch. What we're after is a search engine that can provide statistics about the results that are matched when a user searches for a specific keyword. Let's say for example that we were talking about a database of products in an online shop. We need to be able to return statistics about how many products there are that match a given set of keywords (and also be able to filter this result set by price category etc) as well as the total number of products in stock (assuming that this is stored in a field in the product table). All the search engines that I have found return the top n results and if you want statistics about the size of the result set you need to enumerate the whole set. Even if you didn't you still would need to do so to retrieve the total number of products in stock. Is there anything anyone knows of that is capable of this functionality? As I say I'm happy to get my hands dirty and either build it myself or modify the functionality of something like Lucene but I have not been able to find anything appropriate on Google. Thanks in advance guys! Something to keep in mind here is that ""enumerating all results"" can mean very different things - select count(*) is very different from doing all the joins etc. required to actually get each object. This is true in Lucene as well as relational databases. So I wouldn't worry about the mere fact that the documentation says ""we enumerate all results."" It's been my experience that the standard faceting of Solr scales to what 99% of people need. If you are in that 1% (i.e. you have a huge database) then I can suggest some ways of guessing the results which can be quicker. But Solr will probably work for you. I notice here that you are the first to mention Solr Faceting so even though Yuval F's explanation was fuller I'm going to mark this as the answer. Thanks very much for your help.  As I feel I have now reached the limits of what I can do with SQL indexes Are you sure? I ask because if you are using MySQL you might want to look into the full text search functionality of PostgreSQL. Especially when you combine it with the btree_gin and the trigram modules and the extremely decent explain functionality that allows you to extract reasonable row estimates from highly complex queries.  You might take a look at Solr which is a faceted search engine built on top of Lucene. Solr will count lots of different things for you in addition to doing full-text search. It is good at handling combinations of structured and full-text data. +1 for Solr - Solr has built in facets which are similar to the statistics you are looking for. It is also relatively easy to set up import an SQL database and see if this suits your needs. Try http://www.slideshare.net/erikhatcher/rapid-prototyping-with-solr-4312681 or http://lucene.apache.org/solr/tutorial.html @Mike Great to hear! Glad this turned out to be useful advice This looks like just what I need - I clearly didn't give Solr enough credit - I'll give it some more attention. Thanks! Just wanted to post an update in this thread as I have now prototyped a Solr implementation and it's provided us with huge (honestly HUGE) performance improvements. This was fantastic advice. For reference I used the stats feature of Solr to provide the functionality I needed and was able to talk to the Solr server I now have running using SolrNet."
46,A,"Adding custom Analyzers to Luke Luke the wonderful Lucene index viewer is now hosted under Google code. As a default it supports using several Lucene Analyzers out of the box. However I would like to use it to view an index I built using my own custom Analyzer Let's call it MyAnalyzer. Can you please tell me how to add MyAnalyzer to Luke along with the default analyzers? Googling and some examination of the lukeall jar gave me no clue. Just put the jar with your custom analyzer in the classpath. If you extend Analyzer it must work here is the relevant code from luke (v1.0): // populate analyzers try { Class[] an = ClassFinder.getInstantiableSubclasses(Analyzer.class); if (an == null || an.length == 0) { analyzers = defaultAnalyzers; } else { HashSet uniq = new HashSet(Arrays.asList(an)); analyzers = (Class[])uniq.toArray(new Class[uniq.size()]); } Object cbType = find(""cbType""); populateAnalyzers(cbType); } catch (Exception e) { e.printStackTrace(); } Now it works. The problems I had stemmed from improper creation of a Jar archive for the custom analyzer. Once I had the proper Jar including all of the directories and dependencies and the proper classpath it does work. Thanks again. Thanks zehrer. I am accepting this as this seems like the way to go. It still does not work for me but I will take another shot at it."
47,A,"Jackrabbit Running Queries against UUID I am using Jackrabbit and I am trying to query for an existing node which has a UUID. My code is shown below. The problem is that UUID for referenceNode is of the form ""'90be246a-a17c-445e-a5ad-81b064de0bee'"" and it seems that the XPATH engine used in Jackrabbit (Lucene) has problems dealing with hyphens. If I run query2 everything is fine and referenceNode is printed. If I run query1 (with the UUID) inside Eclipse nothing is returned. HOWEVER if I run query1 inside Jackrabbit Viewer the query runs fine. It seems like I have to escape the hyphens in my queryString but I tried adding double-backslashes and I get the same result. What is the proper way to run queries against UUID's?  // Set up Nodes rootNode = session.getRootNode(); Node referenceNode = rootNode.addNode(""referenceNode""); Node referencingNode = rootNode.addNode(""referencingNode""); referenceNode.addMixin(""mix:referenceable""); referencingNode.setProperty(""pointer"" new ReferenceValue(referenceNode)); // Query String uuid = referenceNode.getUUID(); QueryManager qm = ws.getQueryManager(); String queryString1 = ""//*[@jcr:uuid='""+uuid+""']""; String queryString2 = ""//referenceNode""; Query q = qm.createQuery(queryString1 Query.XPATH); QueryResult result = q.execute(); NodeIterator it = result.getNodes(); while(it.hasNext()) { Node node = it.nextNode(); System.out.println( node.getName()); } The problem might be that the node is not saved yet. As written in the search documentation ""Node names and property values are indexed as soon as the data is saved or as soon as the transaction is committed."" In this case I guess you could use Session.getNodeByIdentifier(String id) instead of using a query. It should be much faster as well. Thank you. I added a simple session.save() after the setup and before the query and it worked."
48,A,Lucene for Blackberry OS? Does anyone know if and where I can get a port of Lucene or a similar library that allows full text searching on Blackberry? Thanks You might be able to get an older version of lucene running. This user reported success: http://archives.devshed.com/forums/java-118/mobile-lucene-918481.html  The answer is no - there is no way to get lucene on blackberry.  CLucene can do that. CLucene is a pure cross-platform C++ port of Lucene currently in the process of conforming to Lucene 2.3.2. It executes faster and works lighter. Definitely suitable also for Blackberry applications. See: http://sourceforge.net/projects/clucene http://clucene.git.sourceforge.net/git/gitweb.cgi?p=clucene/clucene;a=summary I wouldn't know never tried it myself but figured C++ can run on anything. If it can't run as-is on blackberry I'm sure a Java wrapper is a good way to go. You can't use C++ to develop for blackberry right?
49,A,"Do WHERE clauses exist in Lucene(.Net)? I'm building an ASP.NET MVC site where I want to use Lucene.Net for full-text search. My site will be divided into certain categories and I want to allow users to search inside a specific category or inside all categories. To accomplish this I plan to create a term in all documents in my index that contains the name of the category that they're in. When querying the index I would need to execute a query that contains a WHERE clause if the user only wants results from one category. Does such WHERE clause functionality exist in Lucene/Lucene.Net? How do I restrict searches to only return results from a limited subset of documents in the index (e.g. for privacy reasons)? What is the best way to approach this? Thanks for the link. The FAQ there says: ""Just before calling `IndexSearcher.search()` add a clause to the query to exclude documents in categories not permitted for this search."" How do I add a clause to the query? Take a look here to see how to use the QueryFilter class - http://stackoverflow.com/questions/1307782/lucene-net-combine-multiple-filters-and-no-search-terms To implement a custom filter: http://stackoverflow.com/questions/1079934/how-do-you-implement-a-custom-filter-with-lucene-net"
50,A,Lucene 3: Where is StandardAnalyzer? I am working on Lucene 3.x (source code). To start with I downloaded latest source code from SVN stable code 3.0.2 from: http://www.apache.org/dyn/closer.cgi/lucene/java/ The second one has source files for package org.apache.lucene.analysis.standard however the first one does not have any such files (not even the package). Somewhere in issue list I found StandardAnalyzer is deprecated from 3.x Anyone lucene developer here enlighten me with reasons for such differences in source code? It looks like they are moving the StandardAnalyzer (and the related stuff) under org.apache.lucene.modules.analysis.standard.* I don't know the reason though. You can find the StandardAnalyzer in the SVN Trunk here: http://svn.apache.org/viewvc/lucene/dev/trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ The reason is that Lucene and Solr's development efforts have merged and as a result the common parts are being moved: See here: http://asserttrue.blogspot.com/2010/04/lucene-and-solr-development-merged.html Link returns 404 - not found.
51,A,"How do I sort Lucene results by field value using a HitCollector? I'm using the following code to execute a query in Lucene.Net var collector = new GroupingHitCollector(searcher.GetIndexReader()); searcher.Search(myQuery collector); resultsCount = collector.Hits.Count; How do I sort these search results based on a field? Update Thanks for your answer. I had tried using TopFieldDocCollector but I got an error saying ""value is too small or too large"" when i passed 5000 as numHits argument value. Please suggest a valid value to pass. The search.Searcher.search method will accept a search.Sort parameter which can be constructed as simply as: new Sort(""my_sort_field"") However there are some limitations on which fields can be sorted on - they need to be indexed but not tokenized and the values convertible to Strings Floats or Integers. Lucene in Action covers all of the details as well as sorting by multiple fields and so on. One key point: the field to be sorted must be indexed. ...and not tokenized - beat me to it :) Need to store the value?  What you're looking for is probably TopFieldDocCollector. Use it instead of the GroupingHitCollector (what is that?) or inside it. Comment on this if you need more info. I'll be happy to help. thanks for your answer......... I had tried using TopFieldDocCollector but i got an error saying ""value is too small or too large"" when i passed 5000 as numHits argument value...please suggest a valid value to pass...  In the original (Java) version of Lucene there is no hard restriction on the size of the the TopFieldDocCollector results. Any number greater than zero is accepted. Although memory constraints and performance degradation create a practical limit that depends on your environment 5000 hits is trivial and shouldn't pose a problem outside of a mobile device. Perhaps in porting Lucene TopFieldDocCollector was modified to use something other than Lucene's ""heap"" implementation (called PriorityQueue extended by FieldSortedHitQueue)—something that imposes an unreasonably small limit on the results size. If so you might want to look at the source code for TopFieldDocCollector and implement your own similar hit collector using a better heap implementation. I have to ask however why are you trying to collect 5000 results? No user in an interactive application is going to want to see that many. I figure that users willing to look at 200 results are rare but double it to 400 just as factor of safety. Depending on the application limiting the result size can hamper malicious screen scrapers and mitigate denial-of-service attacks too. ""No user in an interactive application is going to want to see that many"" is a bit presumptuous. Users scream and complain even when we truncate to 200000 for legitimate technical reasons..."
52,A,"ASP.NET Lucene Performance Improvements question I have coded up an ASP.NET website and running on win'08 (remotely hosted). The application queries 11 very large Lucene indexes (each ~100GB). I open IndexSearchers on Page_load() and keep them open for the duration of the user session. My questions: The queries take a ~5 seconds to complete - understandable these are very large indexes - but users want faster responses. I was curious to squeeze out better performance. ( I did look over the Apache Lucene website and try some of the ideas over there). Interested in if & how you tweaked it further especially ones from asp.net perspective. One ideas was to use Solr instead of querying Lucene directly. But that seems counter-intuitive introducing another abstraction in between and might add to the latency. Is it worth the headache in porting to Solr? Can anyone share some metrics on what improvement you got following a switch to Solr if it has been worth it. Are there some key things that could be done in Solr that could be replicated to speed up response times? Some questions / ideas: Are you hitting all 11 indexes for a single request? Can you reorganize the indexes so that you hit only 1 index (i.e. sharding) ? Have you run a profile of the application (using dotTrace or similar tool)? Where is the time spent? Lucene.Net? If most of the time is spent on Lucene.Net then if you migrate to Solr the latency should be negligible (compared to the rest of the spent time). Plus Solr can be easily distributed to increase performance. I'm not all too familiar with Lucene (I use Solr) but if you're searching 11 indexes per request can you run those searches in parallel (e.g. with TPL) ? Cannot reduce/collapse them - each index serves a different purpose. For example: one index is a list of Publications one in a set of experts one is a set of affiliations/institutes and so on....each needs to be query-able separately especially w.r.t context. Queries not necessarily sequentially several cases when results from multiple indexes are combined....your thoughts on multi-core Solr? I did profile my webapp and the max. latency is overwhelmingly in the Lucene queries. Does multi-core Solr offer significant performance +up? BTW I know you as the creator of Solrnet so very happy & honored to hear from someone of your caliber and talent. Muchas Gracias! Can you somehow reduce the number of indexes? Do all of the searches have to be performed sequentially? With TPL you can create future tasks with continuations and parallelize that. Or you could try pooling your IndexSearchers to take advantage of the warm up. If you migrate to Solr you would represent each of your 11 indexes as a core. In addition to that you can *shard* out (distribute) some or all of your indexes to other boxes. I think you will get better performance and also benefit from the additional flexibility however I'd recommend running some tests to see if it's worth the migration for *your* particular task i.e. if you get 4s instead of 5s you might decide it's not worth the migration. The queries to the indexes are context dependant. I hold the 11 searchers open for the duration of the user session. I am guessing this is inefficient but this was a rush-job. I want to re-engineer; If I understand correctly you would recommend migrating to Solr (presumably multicore). Thanks for responding!  The biggest thing is removing the search from the web tier and isolating it to it's own tier (a search tier). That way you have a dedicated box with dedicated resources that have the indexes loaded and ""warmed up"" in cache instead of having each user have a copy of it's own index reader. Could you clarify on what ""isolating to a search tier"" means? Should I use a separate physical machine? Or something else? @GalaticJello's suggestion is exactly what Solr is :) ""Are there some key things that could be done in Solr that could be replicated to speed up response times?"" ... and those key things are listed above if you would like to replicate them."
53,A,How do I tell lucene to search a complete document? I have lucene running and I query it via Solr. The indexes are built I have a document that contains lots of words now how to I tell lucene that it has to search the index for the document i provide what would be the query syntax? Maybe you can use MoreLikeThis? Document ids are quite volatile.
54,A,"PhraseQuery: are the terms strictly searched in their order? Are terms searched with a PhraseQuery in Lucene strictly matched in their order in the sentence ? For example if I have ""A B C"" and the doc contains ""A C B"" is PhraseQuery returning a hit ? thanks Yes order matters. So in your example the query ""A B C"" would NOT match a doc containing ""A C B"". Ok so do you suggest me to parse the strings and concatenate TermQueries with field and term ?  Not if you use slop in your PhraseQuery. Sets the number of other words permitted between words in query phrase. If zero then this is an exact phrase search. For larger values this works like a WITHIN or NEAR operator. The slop is in fact an edit-distance where the units correspond to moves of terms in the query phrase out of position. For example to switch the order of two words requires two moves (the first move places the words atop one another) so to permit re-orderings of phrases the slop must be at least two. More exact matches are scored higher than sloppier matches thus search results are sorted by exactness. The slop is zero by default requiring exact matches."
55,A,"Is there any Lucene highlighter that does not require the original text - but rather can work on term positions etc I have been reading the new 2nd edition of the Lucene in Action and they give an example of doing highlighting but unfortunately it requires the original text so it can get the position of terms etc. The highlighter is the official one in contrib so that implies its the sponsorted or official highlighter. Does anyone know of another highlighter that does not require the original text but works using the term positions (sorry if i got the terminology wrong) ??? I don't understand what your question is really - what would you be highlighting if not the text? Some highlighters need the original text and then analyze the query and proceed to highlight. What i meant was that the highlighter would build the text fragment from the index without having access to the original text. Both the standard highlighter and FastVectorHighlighter can use the index if you store the terms. (FVH can only use the index in fact). You can see an example of this on page 274 of Lucene in Action. The relevant code line is: TokenStream stream = TokenSources.getAnyTokenStream(searcher.getIndexReader() sd.doc ""title"" doc analyzer); That will get the token stream from the index. Thinking a bit more i think my original q is flawed because not all terms are stored (eg stop words) so its not possible to build an accurate original fragment for highlighting purposes. Is this a correct assumption ? If you analyze your text in a way which removes stop words then yes stop words will be removed. What I do is have two copy fields one which is indexed but not stored the other stored but not indexed. The indexed one is stemmed etc. The stored one just uses a whitespace tokenizer. This actually takes the same amount of space as a stored+indexed field and will get over the issue you described about stop words being removed. which basically amounts to storing the original text in full form along with the analyzed form - thanks for the tips..."
56,A,"How can I create PhraseQuery for multiple fields with custom Analyzer? I would like to parse user request ""Hello world!"" by my custom analyzer and search throw ""title"" ""description"" fields by using PhraseQuery I found crazy solution of my problem but it looks not optimized Try MultiFieldQueryParser. You can specify list of fields for which the query is to be created.  If you are using own custom analyzer and using that analyzer to parse query according to your need you must see jFlex(Use to change lucene grammar). Note: Same custom analyzer need to be used on both side while creating index and while searching query. You need to use SpanNearQuery while searching for phrase like ""Hello world!"" SpanNearQuery spanNear = new SpanNearQuery(span 0 true);"
57,A,"How to retrieve results by date range and sort using SOLR with ColdFusion 9.0.1? I'm using ColdFusion 9.0.1 and the integrated SOLR full text search engine. I have dates stored in my SQL Server database as datetime fields for upcoming events. I took these records and inserted them into a SOLR collection with the custom3 and custom4 fields being the dateStart and dateEnd dates respectively. Users want to query the collection against a date range and sort by closest date to now. First question: How do we set the datatype for the custom1-4 fields? Or can we? Based on this post Optimizing Solr for Sorting the field should be set to either tdate or date rather than string for best performance. Or does SOLR automatically make the field have the correct datatype based on this post Sort by date in Solr/Lucene performance problems? Second question: How would the search criteria be structured to pull records? How about between May 1 2011 and July 31 2011 for example? I don't tell too many people this but for you I believe it's time to ditch CFINDEX/CFSEARCH and start using Solr directly. CF's implementation is built for indexing a large block of text with some attributes not a query. If you start using Solr directly you can create your own schema and have far more granular control of how your search works. Yes it's going to take longer to implement but you will love the results. Filtering by date is just the beginning. Here's a quick overview of the steps: Create a new index using the CFAdmin. This is the easy way to create all the files you need. Modify the schema. The schema is in [cfroot]/solr/multicore/[your index name]/conf/ The top half of the schema is <types>. This defines all the datatypes you could use. The bottom half is the <fields> and this is where you're going to be making most of your changes. It's pretty straightforward just like a table. Create a field for each ""column"" you want to include. ""indexed"" means that you want to make that field searchable. ""stored"" means that you want the exact data stored so that you can use it to display results. Because I'm using CF9's ORM I don't store much beyond the primary key and I use loadEntityByPK() on my results page. After modifying the schema you need to restart the solr service/daemon. Use http://cfsolrlib.riaforge.org/ to index your data (the add method is a 'insert or modify' style method) and to perform the search. To do a search check out this example. It shows how to sort and filter by date. I didn't test it so the format of the dates might be wrong but you'll get the idea. http://pastebin.com/eBBYkvCW Sorry this is answer is so general I hope I can get you going down the right path here :) Thank you for the great advice Shannon! I will give your approach a shot and report back. Looks like a winner. When using CFSolrLib I keep getting ""The SOLR server did not respond"". Obviously I'm not setting the path to SOLR correctly. Working on my local maching I can pull up http://localhost:8983/solr and see the current list of collections. So I have the following settings: - I'm using WAMP so my web root is at D:\wamp\www - ColdFusion is located at D:\ColdFusion9 make it host=""http://localhost"" the path should be ""/solr/yourinstancename"" I updated my path to ""/solr/solr-test"". ""solr-test"" is the name of the collection. I can query the collection by navigating to localhost:8983/solr/solr-test/select?q= in my browser but I still receive ""The SOLR server did not respond"" error message when calling the resetIndex or add functions in CFSolrLib? When I output the host port and path by calling the get functions the values are all correct. Any other suggestions? Thanks. In your Step 2 you mention modifying the schema at [cfroot]/solr/multicore/[your index name]/conf/. I can't find a path like that however I did find it at [cfroot]/collections/[my index name]/conf/ Is this the path you meant or am I not creating my index correctly in CFAdmin? Newer Version There's a newer version of cfsolrlib here: https://github.com/iotashan/cfsolrlib Just noticed that the newer version of cfsolrlib only works with SOLR 3.1 or higher."
58,A,"Lucene IndexSearcher locks index causing IOException when rebuilding I've learned from reading the available documentation that an IndexSearcher instance should be shared across searches for optimal performance and that a new instance must be created in order to load any changes made to the index. This implies that the index is writable (using IndexWriter) after having created an instance of IndexSearcher that points to the same directory. However this is not the behaviour I see in my implementation of Lucene.Net. I'm using FSDirectory. RAMDirectory is not a viable option. The IndexSearcher locks one of the index files (in my implementation it's the _1.cfs file) making the index non-updatable during the lifetime of the IndexSearcher instance. Is this a known behaviour? Can't I rebuild the index from scratch while using an IndexSearcher instance created prior to rebuilding? Is it only possible to to modifications to the index but not to rebuild it? Here is how I create the IndexSearcher instance: // Create FSDirectory var directory = FSDirectory.GetDirectory(storagePath false); // Create IndexReader var reader = IndexReader.Open(directory); // I get the same behaviour regardless of whether I close the directory or not. directory.Close(); // Create IndexSearcher var searcher = new IndexSearcher(reader); // Closing the reader will cause ""object reference not set..."" when searching. //reader.Close(); Here is how I create the IndexWriter: var directory = FSDirectory.GetDirectory(storagePath true); var indexWriter = new IndexWriter(directory new StandardAnalyzer() true); I'm using Lucene.Net version 2.0. Edit: Upgrading to Lucene.Net 2.1 (thx KenE) and slightly modifying the way I create my IndexWriter fixed the problem: var directory = FSDirectory.GetDirectory(storagePath false); var indexWriter = new IndexWriter(directory new StandardAnalyzer() true); The latest version of Lucene.Net (2.1) appears to support opening an IndexWriter with create=true even when there are open readers: http://incubator.apache.org/lucene.net/docs/2.1/Lucene.Net.Index.IndexWriter.html Earlier versions are not clear as to whether they support this or not. I would try using 2.1. Thanks a bunch for answering. I will try it out and see if that version does what is promised."
59,A,"Java: from Lucene Hits to original objects I'd like to implement a filter/search feature in my application using Lucene. Querying Lucene index gives me a Hits instance which is nothing more than a list of Documents matching my criteria. Since I generate the indexed Documents from my objects which is the best way to find the original object related to a specific Lucene Document? A better description of my situation: Three model classes for now: Folder (can have other Folders or Lists as children) List (can have Tasks as children) and Task (can have other Tasks as children). They are all DefaultMutableTreeNode subclasses. I'll add the Tag entity in the future. Each Task has a text a start date a due date some boolean flags. They are displayed in a JTree. The hole tree is saved in an XML file. I'd like to do things like these: search Tasks with Google-like queries. Find all Tasks that start today. Filter Tasks by Tag. From your comments I think I understand a little better what you are doing. Can you describe what fields you have ""indexed"" with Lucene? Can you describe the UI you provide a little more? I assume you have a tree displayed but that by entering text in a field the user can get a list of leaf nodes that have some match in their label. Is that accurate? Do the matches have to be exact? Do you use features of Lucene like stemming and tokenization? Add a ""stored"" field that contains an object identifier. For each hit lookup the original object via the identifier. Without knowing more context it's hard to be more specific. Hardly. Lucene is an information retrieval system. Its data structures are different than those used to efficient lookup a record by key. I'm not sure to what kind of ""Tree"" you are referring but if you mean a `java.util.TreeMap` rather than walking the whole tree you'll get a O(log n) lookup (or O(1) lookup if you switch to a `HashMap`). Similar story if you use a B-Tree on disk. Lucene offers many features not available from a simple tree on its own: tokenization stemming relevance ranking etc. Perhaps you are using one or the other incorrectly if the difference isn't apparent. I'd like to use Lucene to find objects in the Tree (a DefaultTreeModel) so I could avoid to walk through it. I'm trying to say that this would be useless if I had to walk the tree anyway to get the objects corresponding to the Documents returned by Lucene. Yes this is the easy way to do this. I guess you could serialize your objects into documents and then recreate them but this sounds like a bad design. Since my objects are stored in a Tree I should walk the hole tree to find the object I'm looking for. This would make Lucene useless.  You can't not with vanilla Lucene. You said yourself that you converted your objects into Documents and then stored the Documents in Lucene how would you imagine that process would be reversible? If you want to store and retrieve your own objects in Lucene I strongly recommend that you use Compass instead. Compass is to Lucene what Hibernate is to JDBC - you define a mapping between your objects and Lucene documents Compass takes care of the conversion. Hibernate Search is to information retrieval what Hibernate is to relational databases. I haven't examined Hibernate Search in depth but I have looked at Compass and I believe it made a fundamental design mistake by implementing a JDBC-based `Directory` instead of an `IndexReader`. I really discourage the use of Compass. Compass can use whatever Lucene directory you choose the JDBC-based one is just one option. You can also use RAM directories and FileSystem directories. If that's the basis on which you've been recommending against Compass you've been doing so on the wrong information. And Hibernate Search is for indexing Hibernate databases it is *not* a general indexing mechanism. Lucene (and Compass) are. If using other directories how is atomicity preserved between the index and the stored entities? By using Transactions. Hibernate Search has the same problem to solve and solves it in the same way. Sounds interesting I'll look into it!"
60,A,"Hibernate Search Error Can someone help i'm trying a simple example with hibernate search but getting the following error: Exception in thread ""main"" org.hibernate.HibernateException: could not init listeners at org.hibernate.event.EventListeners.initializeListeners(EventListeners.java:205) at org.hibernate.cfg.Configuration.getInitializedEventListeners(Configuration.java:1396) at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:1385) at FirstExample.main(FirstExample.java:32) Caused by: java.lang.NullPointerException at java.io.Reader.<init>(Reader.java:61) at java.io.InputStreamReader.<init>(InputStreamReader.java:55) at org.hibernate.search.util.HibernateSearchResourceLoader.getLines(HibernateSearchResourceLoader.java:52) at org.apache.solr.analysis.StopFilterFactory.inform(StopFilterFactory.java:53) at org.hibernate.search.impl.SolrAnalyzerBuilder.buildAnalyzer(SolrAnalyzerBuilder.java:79) at org.hibernate.search.impl.InitContext.buildAnalyzer(InitContext.java:185) at org.hibernate.search.impl.InitContext.initLazyAnalyzers(InitContext.java:155) at org.hibernate.search.impl.SearchFactoryImpl.initDocumentBuilders(SearchFactoryImpl.java:541) at org.hibernate.search.impl.SearchFactoryImpl.<init>(SearchFactoryImpl.java:171) at org.hibernate.search.event.ContextHolder.getOrBuildSearchFactory(ContextHolder.java:60) at org.hibernate.search.event.FullTextIndexEventListener.initialize(FullTextIndexEventListener.java:122) at org.hibernate.event.EventListeners$1.processListener(EventListeners.java:198) at org.hibernate.event.EventListeners.processListeners(EventListeners.java:181) at org.hibernate.event.EventListeners.initializeListeners(EventListeners.java:194) ... 3 more My Item Class: @Indexed @AnalyzerDef(name=""customanalyzer"" tokenizer = @TokenizerDef(factory = StandardTokenizerFactory.class) filters = { @TokenFilterDef(factory = ISOLatin1AccentFilterFactory.class) @TokenFilterDef(factory = LowerCaseFilterFactory.class) @TokenFilterDef(factory = StopFilterFactory.class params = { @Parameter(name=""words"" value= ""org/hibernate/search/test/analyzer/solr/stoplist.properties"" ) @Parameter(name=""ignoreCase"" value=""true"") }) }) public class Item { @DocumentId private Integer id; @Field @Analyzer(definition = ""customanalyzer"") private String title; @Field @Analyzer(definition = ""customanalyzer"") private String description; @Field(index=Index.UN_TOKENIZED store=Store.YES) private String ean; private String imageURL; //public getters and setters } Program code: Session session = null; Transaction tx = null; SessionFactory sessionFactory = new Configuration().configure().buildSessionFactory(); session =sessionFactory.openSession(); FullTextSession ftem = org.hibernate.search.Search.getFullTextSession(session); ftem.beginTransaction(); List <Item> AllItem=session.createQuery(""From Item"").list(); for (Item item : AllItem) { ftem.index(item); } ftem.getTransaction().commit(); String searchQuery = ""title:Batman""; QueryParser parser = new QueryParser( ""title"" ftem.getSearchFactory().getAnalyzer(""customanalyzer"") ); org.apache.lucene.search.Query luceneQuery; try { luceneQuery = parser.parse(searchQuery); } catch (ParseException e) { throw new RuntimeException(""Unable to parse query: "" + searchQuery e); } org.hibernate.Query query = ftem.createFullTextQuery( luceneQuery Item.class); query.setFirstResult(20).setMaxResults(20); List results = query.list(); It looks like Hibernate Search can't find org/hibernate/search/test/analyzer/solr/stoplist.properties in the classpath. Make sure it's there. Do u know which jar should i place because on my class path i've almost all the needed jars such as solr-solrjsolr-coresolr-common @Noor: This file is not a part of Hibernate Search. It's a custom list of stopwords that should be provided by you."
61,A,Java Lucene: Use spans to get number of matches in a document How can use spans object to get all matches in a document for a spanNearQuery I got it upto here but not sure how to proceed  for(int i =0; i < splitwords.length ; i++) { sQuery[i] = new SpanTermQuery(new Term(fieldsplitwords[i])); } SpanQuery queryCount = new SpanNearQuery(sQuery 0 true); int numspans = 0; Spans span = queryCount.getSpans(reader); int docId; while(span.next()) { numspans++; docId = span.doc(); System.out.println(span.end() - span.start()); } Would I be able to get all matches(the count of matches) in current document? This will give you a hashtable containing the number of matches for each doc ID: Hashtable<Integer Integer> hits = new Hashtable<Integer Integer>(); while (spans.next() == true) { int docID = spans.doc(); int hit = hits.get(docID) != null ? hits.get(docID) : 0; hit++; hits.put(docID hit); }
62,A,dose mysql consume memory & cpu very much? currently i have a project using solrnow i want to add some featureso i'm thinking is need add mysql to my project solution as i use a vpsso i must consider memory & cpu consume? so my question is dose mysql cost memory & cpu to much ? also i was thinking is solr can provide the same functionthen i can reduce dependence software used in my project Better asked at serverfault but memory consumption (which you can limit) & CPU consumption depend entirely on what you're going to do with it. The footprint for an empty idle MySQL instance can be quite small. Can you please repeat the last sentence? The memory consumption of MySQL depends completely on your configuration. Giving MySQL more memory means faster queries because MySQL can cache more data in RAM. CPU use depends on what you want to do.
63,A,Fluent NHibernate + Lucene Search (NHibernate.Search) I'm using Fluent NHibernate and I would like to implement NHibernate.Search with Lucene but I can't find any examples on how to do that with Fluent NHibernate. It appears there are two steps. (According to Castle) Set the Hibernate properties in the configuration: hibernate.search.default.directory_provider hibernate.search.default.indexBase hibernate.search.analyzer Initializing the Event Listeners to index persisted objcts configuration.SetListener(ListenerType.PostUpdate new FullTextIndexEventListener()); configuration.SetListener(ListenerType.PostInsert new FullTextIndexEventListener()); configuration.SetListener(ListenerType.PostDelete new FullTextIndexEventListener()); I figured out how to add properties to the Fluent NHibernate Source Configuration but I cannot find where the Event Listeners are setup. I'm working on a Fluent API for Lucene which removes the need for attributes and integrated nicely with FNH Its still very pre-alpha contributions welcome! Are you still working on this plugin?  If you're using the Fluent Configuration API then you just need to use the ExposeConfiguration method to get at the NHibernate Configuration instance. Fluently.Configure() .Database(...) .Mappings(...) .ExposeConfiguration(cfg => { cfg.SetListener(...); cfg.SetListener(...); }) .BuildSessionFactory(); This is correct but when I tried this I ran into other problems with the Fluent NHibernate and the NHibernate.Search assemblies using two different versions of NHibernate. So this is correct - but it actually doesn't work. Yet. :) @rmontgomery429 It's a solution for OSS you should checkout from the trunk and build it on your own.
64,A,Cost comparison using Solr I plan to build something like pricegrabber.com/google product search. Assume I already have the data available in a huge table. I plan to submit this all to Solr. This solves the problem of search. However I am not sure how to do comparison. I can do a group by query(on UPC/SKU) for the products returned by Solr on the DB. However I dont want to do that. I want to somehow get product comparison data returned to me along with search from Solr itself. How do you think should my schema be? Do you think this use-case can be solved all by Solr/Sphinx? You need 'result grouping' or 'field collapsing' support to properly handle it. In Solr the feature is not available in any release version and is still under development. If you are willing to use an unreleased version of Solr then get the details here. Sphinx supports result grouping and I had used it a long time ago in a similar project. You can get more details here. An alternative strategy could be to preprocess your data so that only a single record per UPC/SKU gets inserted in the index. Each record can have a separate field containing the ids of all the items with the same UPC/SKU. Doing a database GROUP BY on the products returned by Solr may not be enough. For example if products A and B have the same UPC and a certain query matches A but not B then you will not get both A and B in your result set.
65,A,"Lucene Tag Searching problems with C# escape problems? I am using lucene 2.9.2 (.NET doesnt have a lucene 3) ""tag:C#"" Gets me the same results as ""tag:c"". How do i allow 'C#' to be a searchword? i tried changing Field.Index.ANALYZED to Field.Index.NOT_ANALYZED but that gave me no results. I assuming i need to escape each tag how might i do that? The problem isn't the query its the query analyzer you are using which is removing the ""#"" from both the query and (if you are using the same analyzer for insertion - which you should be) and the field. You will need to find an analyzer that preserves special characters like that or write a custom one. Edit: Check out KeywordAnalyzer - it might just do the trick: ""Tokenizes"" the entire stream as a single token. This is useful for data like zip codes ids and some product names. I figured it out i am using WhiteSpaceAnalyzer  According to the Java Documentation for Lucene 2.9.2 '#' is not a special character which needs escaping in the Query. Can you check out (i.e. by opening the index with Luke) how the value 'C#' is actually stored in the index? gah i dont have java installed on this machine. I'm sure one of my VMs does. I'll check it out later. I never knew about luke +1"
66,A,"Lucene: Am I correctly parsing the strings ? Terms or Phrases? I'm new to Lucene. If I use description = new TermQuery(new Term(""description"" ""my string"")); I ask Lucene to consider ""my string"" as unique word right ? I actually need to consider each word should I use PhraseQuery instead ? Or is it correct ? thanks Lucene Term Consructor says Term: public Term(String fldString txt) Constructs a Term with the given field and text. It means the field shall be description and ""my string"" would be content of the term. You are just constructing a term it doesnt say anything about the search results(Uniqueness or whatever) If you need to consider each worduse a Booleanquery and add the required conditions there @Patrick dont index fields into multiple terms if you plan to doask yourself what will be the field namewill it be same for each term? my suggestion would be to index the whole Field as a single Data Store(no tokenization) as during indexation the Analyzer won't be called for these fields with the same Analyzer then build the search termQueryParser can then find the terms you are finding using BooleanQuery @Narayan uhm I'm currently passing a query to the booleanQuery.add method. Does it make sense ? booleanQuery.add(new QueryParser(org.apache.lucene.util.Version.LUCENE_40 ""tags"" new WhitespaceAnalyzer(org.apache.lucene.util.Version.LUCENE_40)).parse(phrase[i]); Does it make sense ? By the way I don't know Data Store... In the indexing process I'm using a WhiteSpaceAnalyzer so I guess I need to tokenize the query strings as well. (This is the indexing code: document.add(new Field(""description"" flickrDoc.getDescription() Field.Store.YES Field.Index.ANALYZED)); @Narayan I actually need to code it and I cannot use Luke. I wrote before the code line I'm using to parse the string and pass it to the booleanQuery... however I'm not sure it is doing its job correctly. What do you think ? booleanQuery.add(new QueryParser(org.apache.lucene.util.Version.LUCENE_40 ""tags"" new WhitespaceAnalyzer(org.apache.lucene.util.Version.LUCENE_40)).parse(""tag1 tag2 tag3""); @Patrick since u say u tokenize while indexing open the index in Luke try to search with the whitespaceanalyzer and while searching use the same analyzer it should work with booleanquery use a MultiFieldQueryParser since you index with tokenized terms @Narayan ok. I thought I need to use a PhraseQuery and not a BooleanQuery though. If I use a BooleanQuery I should parse the string by myself right ? @Patrick yes you need to pass ""my String"" to build a BooleanQuery e.g.; **TermQuery tq= new TermQuery(new Term(""Description"" desc)); BooleanQuery bq = new BooleanQuery().add(tqBooleanClause.Occur.SHOULD);** @Narayan But in this way the content of Description is still indexed as term and not a phrase. @Patrick the above comments shows you how to build a BooleanQUery it doesnot index them as you presume in your last comment you should parse the term and index them as a whole(_""my String""_ corresponging to field _Description_) and then use a Boolean Query to find them. In My experience PhraseQuery has the problem with searchingit doesnt work with wildcards and it doesn't find the string match even if the documents contain them. try itluke debug itpost the results i would be glad to see them @Narayan yeah I wrongly wrote indexed instead of added. What I meant is that you are telling me how to build a boolean query based on multiple terms but what I'm asking is how to split my sentences into multiple terms (to pass to the booleanQuery)"
67,A,"NullPointerException in solr multicore I'm configuring my solr for two cores and have got most of it working but I'm getting this cryptic error. First off here's my solr.xml: <?xml version='1.0' encoding='UTF-8'?> <solr persistent=""true""> <cores adminPath=""/admin/cores""> <core name=""cars"" dataDir=""/var/lib/solr/data/cars"" config=""/etc/solr/home_cars/conf/solrconfig.xml"" schema=""/etc/solr/home_cars/conf/schema.xml"" instanceDir=""home_cars"" /> <core name=""industrial"" dataDir=""/var/lib/solr/data/industrial"" config=""/etc/solr/home_industrial/conf/solrconfig.xml"" schema=""/etc/solr/home_industrial/conf/schema.xml"" instanceDir=""home_industrial"" /> </cores> </solr> All of this seems fine. I believe I've set the proper permissions for all the locations but still I get this error in catalina.out: INFO: user.dir=/var/lib/tomcat6 Aug 8 2010 2:03:27 PM org.apache.solr.common.SolrException log SEVERE: java.lang.NullPointerException at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:173) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1317) at org.apache.solr.core.QuerySenderListener.newSearcher(QuerySenderListener.java:52) at org.apache.solr.core.SolrCore$3.call(SolrCore.java:1147) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:636) Some [info] logs and then this one: SEVERE: java.lang.NullPointerException at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:173) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1317) at org.apache.solr.core.QuerySenderListener.newSearcher(QuerySenderListener.java:52) at org.apache.solr.core.SolrCore$3.call(SolrCore.java:1147) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:636) I'm not a top cat in solr java or tomcat (or much else for that matter hehe). Any help is will be greatly appreciated! It seems like this happens because QueryElevationComponent's config doesn't exist. Try this: http://wiki.apache.org/solr/QueryElevationComponent You are correct and awesome! A few obvious error messages later and everything working just fine. Thank you Aillyn!  It happened to me that I copied the previous solrconfig.xml so the elevator.xml path reference was wrong. If you started from the template search for: <searchComponent name=""elevator"" class=""solr.QueryElevationComponent"" > <!-- pick a fieldType to analyze queries --> <str name=""queryFieldType"">string</str> <str name=""config-file"">elevate.xml</str> </searchComponent> And change it to: <searchComponent name=""elevator"" class=""solr.QueryElevationComponent"" > <!-- pick a fieldType to analyze queries --> <str name=""queryFieldType"">string</str> <str name=""config-file"">../../conf/elevate.xml</str> </searchComponent> Assuming you're using all the default directories. Thanks - I just ran into exactly the same problem and that fixed it :)"
68,A,"Lucene Query and Analyzer combination I have a quick question. Say I've got a string ""this string has some text in it"". I want lucene to be able to find it using following searches: ""string has"" ""has this"" ""text this"" Which combination of Analyzer and Query should I use for that? Lucene 3.0.3. You probably want to use solr instead of lucene since it will not require you to know things like this The standard analyzer and query parser will work."
69,A,"Complex Full text search using PlayFramework Search / Hibernate Search Suppose there are only two type of model objects. Tag Article Article can have a variable number of tags as well as a large text field containing the body of the article. How do I perform an efficient full text search for articles matching a set of tags that I define? For example out of 1 million articles what's the best way to efficiently query (with count and pagination support) for articles that 1) match body:business* and 2) are tag with ""America"" ""Economy"" and NOT tagged with ""Asia""? I am able to efficiently do 1) (using HQL or plain old SQL) and 2) (Using lucene query) separately but not both of them together. Anybody got some ideas? As in I want to filter articles that are both tagged with America (which is something that has to be done in the database) and articles which contains the world ""business"" (which should be done in the search engine). I could separately find articles using each methods and then take intersection of the results but that seems like a really inefficient approach. By using them together I meant if there was some way to implement this more efficiently than intersecting the results. I'm not sure to understand what you mean by ""both of them together""... SQL DB search and lucene query are 2 different mechanisms using different indexing data so they work differently and return different results. What do you mean by using them together? Yes it's not quite easy to realize the intersection and it wouldn't take into account the pertinence of both sets of results. If there is a way to mix both I don't know :)... The only way I know is to store all search information in one single indexing engine and there is nothing against indexing some DB data with lucene for example as it has been created for that. Thus you could search for everything using lucene (or another engine such as elastic search) Have you looked at the Elastic Search module? It is a very powerful module and the module owner has done a lot of work documenting his work."
70,A,Get number of pending changes on IndexWriter We have home grown search service based on Lucene. One particular question I'm faced some time ago was getting number of pending changes on IndexWriter. If the pending changes counter is zero there is no need to commit to the index reopen IndexReader IndexSearcher and so on. Also we have some application level logic that is linked to IndexReader.commit() call and it's better not to call it if there is no actual changes in commit point. I have access to all the places where methods IndexReader.updateDocument() and IndexReader.remove() are called so I simply can write my own counter of pending changes. But I'm intrested may be there is already exists one in the Lucene API itself? API check doesn't give me enough information on the topic. You can use IndexWriter.numRAMDocs to get number of documents added but I think there is no public API to get the current count of buffered deletes. IndexWriter.ramSizeInBytes might also be useful here. It tells you how much RAM is currently in-use so this will increase as you add or delete docs (but decrease when a flush takes place). Note that IndexReader.isCurrent is only usable if you commit the changes from the IndexWriter ie if you have pending changes but have not committed (or closed) the writer then IndexReader.isCurrent will still return true.  See IndexReader.isCurrent().
71,A,"Optimizing Solr for Sorting I'm using Solr for a realtime search index. My dataset is about 60M large documents. Instead of sorting by relevance I need to sort by time. Currently I'm using the sort flag in the query to sort by time. This works fine for specific searches but when searches return large numbers of results Solr has to take all of the resulting documents and sort them by time before returning. This is slow and there has to be a better way. What is the better way? Warning: Wild suggestion not based on prior experience or known facts. :) Perform a query without sorting and rows=0 to get the number of matches. Disable faceting etc. to improve performance - we only need the total number of matches. Based on the number of matches from Step #1 the distribution of your data and the count/offset of the results that you need fire another query which sorts by date and also adds a filter on the date like fq=date:[NOW()-xDAY TO *] where x is the estimated time period in days during which we will find the required number of matching documents. If the number of results from Step #2 is less than what you need then relax the filter a bit and fire another query. For starters you can use the following to estimate x: If you are uniformly adding n documents a day to the index of size N documents and a specific query matched d documents in Step #1 then to get the top r results you can use x = (N*r*1.2)/(d*n). If you have to relax your filter too often in Step #3 then slowly increase the value 1.2 in the formula as required. This might actually work. The penalty for performing the search twice will probably add about 200ms to the request but it might be worth it to avoid sorting huge datasets. I'll try this out. I think there must be a better way though. Yeah I agree that this looks a bit flaky. Another option (the warning in my answer still applies) may be to maintain more than one index - one with data for last week another with data for last month another for last year etc. (Choose the time periods based on your requirements and data/query distribution). When you get a new query fire it on the smallest index and check if you get the required number of results. If not fire it on successively larger index till you get the required number of results.  Obvious first question: what's type of your time field? If it's string then sorting is obviously very slow. tdate is even faster than date. Another point: do you have enough memory for Solr? If it starts swapping then performance is immediately awful. And third one: if you have older Lucene then date is just string which is very slow. I've got Solr on a dedicated box with 30GB of RAM allocated. That said a query for ""show me all items page 1 100 rows per page"" will swap (I assume) when Solr tries sorting by date since my index is about 120GB. I think this is the root of my problem. I am using Solr 1.4.1 the latest which I assume comes with the latest Lucene. I am using date. Maybe tdate will speed things up? Solr 1.4.1 uses tdate for date fields but with a precision step of 0. I bumped it up to a precision step of 4 (the recommended default) but I'm not seeing a speed increase. I think the problem is just swapping when running giant datasets.  I found the answer. If you want to sort by time and not relevance use fq= instead of q= for all of your filters. This way Solr doesn't waste time figuring out the weighted value of the documents matching q=. It turns out that Solr was spending too much time weighting not sorting. Additionally you can speed sorting up by pre-warming your sort fields in the newSearcher and firstSearcher event listeners in solrconfig.xml. This will ensure that sorts are done via cache."
72,A,"Counting sentences: Database (like h2) vs. Lucene vs.? I am doing some linguistic research that depends on being able to query a corpus of 100 million sentences. The information I need from that corpus is along the lines: how many sentences had ""john"" as first word ""went"" as second word and ""hospital"" as the fifth word...etc So I just need the count and don't need to actually retrieve the sentences. The idea I had was to split these sentences into words and store them into a database where the columns would be the positions (word-1 word-2 word-3..etc) and the sentences would be the rows. So it looks like: Word1 Word2 Word3 Word4 Word5 .... Congress approved a new bill John went to school ..... And my purpose will then be fulfilled by calling something like COUNT(SELECT * where Word1=John and Word4=school). But I am wondering: Can this be better achieved using Lucene (or some other tool)? The program I am writing (in Java) will be doing tens of thosands of such queries on that 100 million sentece corpus. So speed of look-up is important. Thanks for any advice Anas I suggest you read Search Engine versus DBMS. From what I gather you do need a database rather than a full text search library. In any case I suggest you preprocess your text and replace every word/token with a number using a dictionary. This replaces every sentence with an array of word codes. I would then store every word place in a separate database column simplifying counts and making them quicker. For example: A boy and a girl drank milk translates into: 120 530 14 120 619 447 253 (I chose arbitrary word codes) leading to store a row 120 530 14 120 619 447 253 0 0 0 0 0 0 0 .... (until the number of words you allocate per a sentence is exhausted). This is a somewhat sparse matrix so maybe this question will help. Thank you very much for the link I found it quite helpful. And actually I took away from it that search indexer would be faster than DB so I've decided to go with that God willing. Thanks again.  Or you can done it by the hand using only only java by List triple = new ArrayList(3); for (String word: inputFileWords) { if (triple.size == 3) { resultFile.println(StringUtils.join("" "" triple)); triple.remove(0); } triple.add(line); } then sort this file and sum all duplicate lines (manually or from some command line utility) it will be fast as possible. I am afraid this won't work for my purpose: I don't just want the duplicate lines I want the count of lines that satisfy certain properties (e.g. how many lines have ""car"" as 2nd word and ""crashed"" as 3rd word). So simply collapsing the lines won't do. Plus I need to be able to access the account in a reasonably fast manner since my code will be doing tens of thousands of such queries. oh sorry i'd just misunderstood your case. In this case using an DB will be an optimal choice. And for example Postgres have some features for easy indexing and quering arrays it can help you. But for 100 million rows it may be no so fast as you wish.  Look at Apache Hadoop and Map Reduce. It's developed for things like this. MapReduce seems designed for cluster computing I will be doing this thing on my personal notebook (the corpus is only a couple of GBs in size).  I suppose you already have infrastructure to create tokens from a given sentence. You can create a lucene document with one field for each word in the sentence. You can name the fields as field1 field2 and so on. Since lucene doesn't have a schema like DB you can define as many fields on the fly as you wish. You can add an additional identifier field if you want to identify which sentences matched a query. While searching your typical lucene query will be +field1:John +field4:school Since you are not bothered about the speed of retrieval you can write a custom Collector which will ignore scores. (That will return results significantly faster as well.) Since you don't plan to retrive back the matching sentences or words you should only index these fields and not store. That should push performance up by a notch. Ok thanks for the tip on the Collector.  Assuming that the queries are as simple as you have indicated a simple SQL db (Postgres MySQL possibly H2) would be perfect for this. That was the original idea but the worry I had (part of the reason why I posted this question) is whether the counting would be a bit slow with 100 million rows. I mean if it will take 10 seconds to count the rows that satisfy the select statement that would be too slow.  Lucene span queries can implement positional search. Use SpanFirst to find a word in the first N positions of a document and combine it with SpanNot to rule out the first N-1. Your example query would like this: <BooleanQuery: +(+spanFirst(john 1) +spanFirst(went 2)) +spanNot(spanFirst(hospital 5) spanFirst(hospital 4))> Lucene also of course allows getting the total hit count of a search result without iterating all the docs."
73,A,"How to submit a form from a html file to a java file? I'm achieving kinda web search engine (using Lucene library) what I have so far is a html file with a little form in within an input text to inter keyword and a submit button to send the form the point is that the code I got is a .java file (that needs to other .jar files) and I'm a newbie in .jsp and how html and java interconnected My question is clear is: how to submit a form from a html file to a java file and how does java recieve data from the html file?? I remember that in php we do for example $_GET['keyword'] but no idea in java. Thanks for your support Regards. I don't wish to be rude but this is a very big question just a step short of ""How do I write a computer program?"" It's not something that can be answered in a couple of paragrapsh. I think you need to get a book on JSP and servlets. You need something to be able to run Java Servlets and Java Server Pages. For an open source implementation of the specifications check Tomcat e.g.: http://tomcat.apache.org You can then implement your own servlet and overwrite the appropriate methods. You might want to start here to get an idea: http://java.sun.com/developer/onlineTraining/Programming/BasicJava1/servlet.html  A small question with a large answer! The main thing to understand is that there is a big chunk missing from your current picture: in between the HTML and the Java you will need a web server. And more than just a web server a web server that knows how to run Java programs - this is called a servlet engine. Happily there are many of these easily available. You should get hold of either Tomcat or Jetty; both are very good (if you're going for Jetty i would suggest version 6 rather than version 7 - long story). Once you've got your web server you will need to give it your HTML and your Java so it can serve the HTML to clients and run the Java. You do this by packaging them in something called a 'web archive' or WAR which has a particular layout and a configuration file called web.xml. It might help you to look at a very minimal example WAR. A key thing to understand here is how the servlet engine will call your Java code. There is a standard framework for this in which you write your code as something called a servlet. Oracle has a tutorial about these but it's rather dense; servlets are really much more simple than it might suggest. The example WAR i mention above has a very simple example of one with the configuration needed to make it work. You will find the documentation for the javax.servlet.http package indispensable - that's where most of the useful API that can be used by servlets is to be found (there's also important stuff in the parent javax.servlet package). Having installed your servlet engine written your servlet and packaged it as a WAR you will need to figure out how to start your servlet engine and feed it your WAR. There i'm afraid i'm going to leave you to read the documentation. I think everyone does it slightly differently! Anyway that should get you going. This is really the most basic slice through the world of Java web programming. As well as servlets there is something called JSP you should know about. After that there is a huge and fractious space of things called web frameworks which aim to make writing web applications simpler. Everyone has their favourite - mine is something called Stripes which is very simple but really gets things done. After that you might also want to look at more sophisticated frameworks like JSF or Wicket and after that perhaps 'full-stack' frameworks like Spring or Seam or other more focused server-side technologies such as Enterprise JavaBeans. Or you might well just want to stick with servlets!  Create a servlet class and map it on a certain URL pattern e.g. /servleturl. Let the HTML <form> action point to that URL. <form action=""servleturl"" method=""post""> <input type=""text"" name=""foo"" /> <input type=""text"" name=""bar"" /> <input type=""submit""> </form> In the servlet class override the doPost() method and gather the submitted values by request.getParameter(). String foo = request.getParameter(""foo""); String bar = request.getParameter(""bar""); // ... Then in the same servlet method just import instantiate and/or invoke that Java program according to its documentation. For example YourProgram program = new YourProgram(); program.process(foo bar); // ..."
74,A,"Ordering results by relevance using Solr search I'm new to Solr search and trying to get a grasp on how to handle ordering of results. I'm using Ruby on Rails together with the Sunspot gem to interface with Solr. I have an Article model that has the following fields that are indexed: text Title text AuthorNames integer NumberOfReviews I'd like to be able to perform a search on Solr where: Exact title matches are returned before anything else Positive weighting is given proportionally to Articles with a larger NumberOfReviews Ideally I'd also like to be able to do something like Google where near misses and typos are also found to a certain extent and alternative searches are suggested when it seems like the user might have made a mistake though I'm not sure this is possible. Can anyone help or point me in the right direction? Thanks in advance! ""Weighting"" is usually called ""boosting"" in Solr/Lucene jargon. Take a look at dismax queries and its parameters there's a lot of tuning you can do there. For example you can use the bf parameter to boost articles with larger NumberOfReviews. Mistake checking alternative suggestions: see SpellCheckComponent"
75,A,"How do we create a simple search engine using Lucene Solr or Nutch? Our company has thousands of PDF documents. How do we create a simple search engine using Lucene Solr or Nutch? We'll provide a basic Java/JSP web page were people can type in words and perform basic and/or queries then show them the document links of all matching PDF's. I think you want a system to manage your PDF file. Please try to use dspace system. Dspace is a digital library it supports Lucene based on. www.dspace.org. Sorry I have a mistake http://www.dspace.org/.  I have had good luck with lucene but it is not click install and search it does require a bit of work. If you need something that yo can download and install and be searching within 10 minutes look at the free Ominifind Yahoo Edition http://omnifind.ibm.yahoo.net/ it uses Lucene but is packaged such that it is configured and ready to run upon install a much easier way to try Lucene.  None of the projects in the Lucene family can natively process PDFs but there are utilities you can drop in and well written examples on how to roll your own. Lucene will do pretty much whatever you need it to do but there is overhead in terms of your time as Tony said above. Thousands of documents really isn't that many so you might be able to get away with a lighter weight alternative. That said I would still recommend looking at Solr - it's much much easier to set up than Lucene has support for backups replication etc. as well as a nifty JSON interface which would fit your use case very well: http://wiki.apache.org/solr/SolJSON Solr 1.4 will parse PDFs and MS Word documents.  If you've a Linux server you could use Beagle to index them and then just use the search functionality that comes with it. It has an (experimental) web search interface and it can be hooked into the FireFox search box as well. It automatically indexes files as they're included and I'd suspect that you'll find it much more efficient to enhance or fix beagle than to write your own search interface to Lucene.  Take a look at eprints. It includes a workflow for adding new documents automatically indexes and thumbnails PDF's and has fairly comprehensive full text search functionality. It can also be easily customised and branded. Why re-invent the wheel. Again. Again.... lmmfao.. mod +1 for being right and funny at the same time.  Nutch + Lucene + Pdf plugin enabled in Nutch is your solution. Nutch allows you to parse pdfs by enabling the pdf plugin. Lucene will allow you to index the crawled and parsed data and Nutch has servelet which gives you a search interface. We use the same for our internal lans.  Having the (imho) distinct advantage of being on a Mac I use SearchLight on a somewhat older G5. nice web interface to spotlight the Mac OS' built-in indexing service.  Answering such a broad question in this forum will be tough. I'd recommend you check out the book Lucene in Action which covers the basics of indexing and searching in a quite readable fashion. Given your application it sounds like Nutch and Solr probably will not be necessary. Since all of your documents are available locally Nutch probably won't be helpful. Solr may help you manage a cluster of searchers if you have a high query load but Lucene is highly performant and handles large document sets in a very scalable manner. The one area that might consume a lot of your effort is the use of PDF. It's possible to index PDF documents and there are Lucene contributions to facilitate the extraction of raw text from PDFs but depending on the document the quality of results can vary. Often the context of a keyword in a PDF document is unclear because of formatting instructions and that can make it hard to do proximity searches or show the context of a hit.  Google Search Appliance http://www.google.com/enterprise/gsa/ Why the downvotes? I don't understand the down votes either. A GSA is just what you need. Not only will it index all of your PDF's it will also index your entire intranet and it will provide much better search results than Lucene will. +1 downvotes were rather unfair. Except for the implication that the OP may be looking for a ""free"" solution GSA is a worthy consideration for this type of application... The downvotes where kind of hard. But I think the commenter could give little bit more info then just an url.  A great free search technology you might look at is the IBM Yahoo! free search. I'm not sure whether they followed through on plans to use Lucene under the covers but it remains one of the really great east to use free search technologies. It handles up to 500K documents I believe and it supports PDF and other non-text formats as well. Graphic user interface; easy to customize search results and basic search analytics. Basic thesaurus and powerful API so you can do pretty much whatever you want if the out of the box results are not to your liking. We've suggested this to a number of clients where there were fewer than half a million documents and they love it."
76,A,"Using NHibernate to Index Large Amounts of Data in Lucene.Net We are using Nhibernate as our data access layer. We have a table of 1.7 million records which we need to index one by one through Lucene for our search. As we run the console app we wrote to build our index it starts off fast but as it goes through the items it progressively gets slower and slower. Our First iteration was to just index them all. The second iteration was to index them by category. Now we are selecting subsets by category and then breaking them into ""pages"" of 100. We still have a degredation in performance. I turned on sql profiler and as it iterates the items it is calling the sql server for each item one by one for the images even though lazy loading is set to not for the image. This is a commerce site and we are indexing catalog items (products). Off each catalog item are 0 to many images (stored in a seperate table. Here is our Mapping: public class ItemMap : ClassMap<Item> { public ItemMap() { Table(""Products""); Id(x => x.Id ""ProductId"").GeneratedBy.GuidComb(); Map(x => x.Model); Map(x => x.Description); Map(x => x.Created); Map(x => x.Modified); Map(x => x.IsActive); Map(x => x.PurchaseUrl).CustomType<UriType>(); Component(x => x.Identifier m => { m.Map(x => x.Upc); m.Map(x => x.Asin); m.Map(x => x.Isbn); m.Map(x => x.Tid); }); Component(x => x.Price m => { m.Map(x => x.Currency); m.Map(x => x.Amount ""Price""); m.Map(x => x.Shipping); }); References(x => x.Brand ""BrandId""); References(x => x.Category ""CategoryId""); References(x => x.Supplier ""SupplierId""); References(x => x.Provider ""ProviderId""); HasMany(x => x.Images) .Table(""ProductImages"") .KeyColumn(""ProductId"") .Not.LazyLoad(); // TODO: Add variants } } And here is the root logic of the indexing app. public void IndexProducts() { Console.WriteLine(""--- Begin Indexing Products ---""); Console.WriteLine(); var categories = categoryRepository.GetAll().ToList(); Console.WriteLine(String.Format(""--- {0} Categories found ---"" categories.Count)); categories.Add(null); foreach (var category in categories) { string categoryName = ""\""None\""""; if (category != null) categoryName = category.Name; Console.WriteLine(String.Format(""--- Begin Indexing Category ({0}) ---"" categoryName)); var categoryItems = from p in catalogRepository.GetList(new ActiveProductsByCategoryQuery(category)) select p; int count = categoryItems.Count(); int pageSize = 100; int currentPage = 0; int offest = currentPage * pageSize; int current = 1; Console.WriteLine(String.Format(""Indexing {0} Products..."" count)); while (offest < count) { var products = (from p in categoryItems select p).Skip(offest).Take(pageSize); foreach (var item in products) { indexer.UpdateContent(item); UpdateCounter(current count); current++; } currentPage++; offest = currentPage * pageSize; } Console.WriteLine(); Console.WriteLine(String.Format(""--- End Indexing Category ({0}) ---"" categoryName)); Console.WriteLine(); } Console.WriteLine(""--- End Indexing Products ---""); Console.WriteLine(); } FYI the count is 26552 for the category in question. The first query it runs is this: exec sp_executesql N'SELECT TOP 100 ProductId100_1_ Upc100_1_ Asin100_1_ Isbn100_1_ Tid100_1_ Currency100_1_ Price100_1_ Shipping100_1_ Model100_1_ Descrip10_100_1_ Created100_1_ Modified100_1_ IsActive100_1_ Purchas14_100_1_ BrandId100_1_ CategoryId100_1_ SupplierId100_1_ ProviderId100_1_ CategoryId103_0_ Name103_0_ ShortName103_0_ Created103_0_ Modified103_0_ ShortId103_0_ DisplayO7_103_0_ IsActive103_0_ ParentCa9_103_0_ FROM (SELECT this_.ProductId as ProductId100_1_ this_.Upc as Upc100_1_ this_.Asin as Asin100_1_ this_.Isbn as Isbn100_1_ this_.Tid as Tid100_1_ this_.Currency as Currency100_1_ this_.Price as Price100_1_ this_.Shipping as Shipping100_1_ this_.Model as Model100_1_ this_.Description as Descrip10_100_1_ this_.Created as Created100_1_ this_.Modified as Modified100_1_ this_.IsActive as IsActive100_1_ this_.PurchaseUrl as Purchas14_100_1_ this_.BrandId as BrandId100_1_ this_.CategoryId as CategoryId100_1_ this_.SupplierId as SupplierId100_1_ this_.ProviderId as ProviderId100_1_ category1_.CategoryId as CategoryId103_0_ category1_.Name as Name103_0_ category1_.ShortName as ShortName103_0_ category1_.Created as Created103_0_ category1_.Modified as Modified103_0_ category1_.ShortId as ShortId103_0_ category1_.DisplayOrder as DisplayO7_103_0_ category1_.IsActive as IsActive103_0_ category1_.ParentCategoryId as ParentCa9_103_0_ ROW_NUMBER() OVER(ORDER BY CURRENT_TIMESTAMP) as __hibernate_sort_row FROM Products this_ left outer join Categories category1_ on this_.CategoryId=category1_.CategoryId WHERE (this_.IsActive = @p0 and (1=0 or (this_.CategoryId is not null and category1_.CategoryId = @p1)))) as query WHERE query.__hibernate_sort_row > 500 ORDER BY query.__hibernate_sort_row'N'@p0 bit@p1 uniqueidentifier'@p0=1@p1='A988FD8C-DD93-4119-8F84-0AF3656DAEDD' Then for each product it executes exec sp_executesql N'SELECT images0_.ProductId as ProductId1_ images0_.ImageId as ImageId1_ images0_.ImageId as ImageId98_0_ images0_.Description as Descript2_98_0_ images0_.Url as Url98_0_ images0_.Created as Created98_0_ images0_.Modified as Modified98_0_ images0_.ProductId as ProductId98_0_ FROM ProductImages images0_ WHERE images0_.ProductId=@p0'N'@p0 uniqueidentifier'@p0='487EA053-4DD5-4EBA-AA36-95B30C42F0CD' Which is fine. The problem is the first 2000 or so are really fast but the longer it runs through the category the slower it gets and the more memory it consumes - even though it's indexing the same number of products. GC is working because the memory usage drops but overall it climbs as the processor works. Is there anything we can do to speed up the indexer? Why is it steadily decreasing in performance? I don't think it is nhibernate or the queries because it starts out so fast. We're really at a loss here. Thanks After digging and running the sql 2008 Database Engine Tuning advisor and more research the data collection does not appear to be the issue. The problem (we think) is that Lucene isn't handling creating a large index very efficiently. The file keeps getting larger and large and the indexer runs slower and slower. You can optimize the search index once in a while. Are you using same session for all calls? If that's the case it will cache loaded entities and loop through them to check if they need flushing when Flush is called (which depends on your FlushMode). Either use a new session for every page of items or change FlushMode. You can specify when using criterias that specific properties should be prefetched using a sql join which may speed up reading of data. I usually trust the critiera apis more than Linq-to-NHibernate just because I actually decide what's done for every call.  Ayende had a post about getting this done (using a Stateless Session and a custom IList implementation) just a couple weeks ago. http://ayende.com/Blog/archive/2010/06/27/nhibernate-streaming-large-result-sets.aspx This sounds like just what you need at least for speeding up the record retrieval and minimizing memory use.  We ended up moving to Solr for our indexing. We couldn't ever get it to index efficiently which is probably due to the implementation. For reference: http://lucene.apache.org/solr/ http://code.google.com/p/solrnet/"
77,A,Is there Pattern to manage : Taxonomy + search (Lucene) + Permission I want to develop my own website administration. Here is my problem I want to use these technologies : * search : LUCENE/SOLR * users' permission : ACEGI (SPRING SECURITY) * taxonomy (I do not not if there is technologies for this) So I do not have experience in these technologies. I would like to know if somebody understand my situation in order to help me...I think there is a logical coherence between theses elements but I do not why...Do I need to develop these functionalities as if they were totally independant ? I've had some success in building web sites using Grails. Grails is the Java-world equivalent to the Rails framework and is built on top of Spring Hibernate and Groovy. It also interfaces smoothly with existing Java code because it runs in the same virtual machine and is basically Java. In addition Grails has a rich collection of plugins including ones for search and authentication/security. I am not sure what taxonomy you're referring to but that may be something you can model using Grails Domain classes. I would like to construct my little 'CMS'... What I call taxonomy is the fact that : if I create a new topic so it will create a new page in my website and if I create one new sub-topic so it will create another new page related to it parent. Does a pattern exist for this ? Ok you understand well my case. But what happen is that first I will use SPRING ROO. Then I am looking for a modular architecture (maybe using **nodes** such as Drupal) that will let me have a flexible interaction between the content it structure (taxonomy) and the full-text search engine. This is quite complex and you can give me a very good reason to use GRAILS...but I would prefer to invest myself on SPRING ROO (for other reasons...). tkx @nzaero If I understand you correctly you're talking about views on objects. Grails makes that pretty easy to do. The topic-subtopic relationship is also a simple thing to model in grails assuming it is a has-many relationship.
78,A,Lucene.net index directory usage in java lucene Lucene.net is a direct port of Lucene for java so it stands to reason that i could use the index directory created by Lucene.net directly from Lucene in java is this assumption correct? From the Lucene.Net site: In addition to the APIs and classes port to C# the algorithm of Java Lucene is ported to C# Lucene. This means an index created with Java Lucene is back-and-forth compatible with the C# Lucene; both at reading writing and updating. In fact a Lucene index can be concurrently searched and updated using Java Lucene and C# Lucene processes. However as Thilo points out in the comments this compatibility is version-dependant; the index format may change between releases. Be careful to match the versions though. Lucene improve their index format from time to time and then the old version cannot read the updated index files anymore.
79,A,Inverted index in Lucene I want to know which class in Lucene generates the inverted index? Thanks Lets break down some lucene fundamentals An index contains a sequence of documents. A document is a sequence of fields. A field is a named sequence of terms. A term is a string so Field when added to documents if they are inverted they are indexed note that fields can be both indexed and stored So invert(indexed) operation happens at the field level and yeah Field is a class which is where i think inversion happens  The inverted index is created in a class named FreqProxTermsWriter based on information retrieved from documents such as term frequency document frequency term position etc. Exactly in *appendPosting* function  An inverted index is the structure of the data files that Lucene uses. There's not really any particular class that makes it inverted. The classes in the org.apache.lucene.index package manage the files that ultimately make the data structure an inverted index. I know thatbut *lucene* first read each document as set of fields and process fields with same name together after that write each new field in a buffer. I want to know what does it do after this??
80,A,Katta in production environment According to the website Katta is a scalable failure tolerant distributed indexed data storage. I would like to know if it is ready to be deployed into production environment. Anyone already using it and has advices? Any pitfalls? Recommendations? Testimonials? Please share. Any answer would be greatly appreciated. http://katta.sourceforge.net/about/powered-by-katta has few. I also would like to hear first hand experience though. We have tried using katta and for what its worth - found it very stable relatively easy to manage (as compared to managing plain vanilla lucene) Only pitfall I can think of is lack of realtime updates - when we tested it (about 9-10 months back) update meant updating index using a separate process (hadoop job or what have you...) and replacing the live index this was a deal-breaker for us. If you are looking into distributed lucene you should really tryout ElasticSearch or Solandra
81,A,"How can I perform a fuzzy search for all words provided in a Lucene.net search I am trying to teach myself Lucene.Net to implement on my site. I understand how to do almost everything I need except for one issue. I am trying to figure out how to allow a fuzzy search for all search terms in a search string. So for example if I have a document with the string The big red fox I am trying to get bag fix to match it. The problem is it seems like in order to perform fuzzy searches I have to add ~ to every search term the user enters. I am unsure of the best way to go about this. Right now I am attempting this by string queryString = ""bag rad""; queryString = queryString.Replace(""~"" string.Empty).Replace("" "" ""~ "") + ""~""; The first replace is due to Lucene.Net throwing an exception if the search string has a ~ already apparently it can't handle ~~ in a phrase. This method works but it seems like it will get messy if I start adding fuzzy weight values. Is there a better way to default all words to allow for fuzzyness? You might want to index your documents as bi-grams or tri-grams. Take a look at the CJKAnalyzer to see how they do it. You will want to download the source and look at the source. [Here's](http://code.google.com/p/bzreader/source/browse/trunk/Snowball.NET/Extra/CJKTokenizer.cs?r=33) the equivalent Lucene.NET code. Thanks. I think I get the core concept of the CJKAnalyzer but I think I need to study up on Lucene a lot more before I try and just copy the java code over to C# and write my own tokenizers and analyzers"
82,A,"How Lucene scores results in a RegexQuery? I can see how two values when doing a regular/fuzzy full text search can be compared to determine which one is ""better"" (i.e. one value contains more keywords than the other one contains less non-keywords than the other). However how Lucene computes the score when doing regex queries using RegexQuery? It is a boolean query - a field's value is either compatible with the regex or not. Lucene can't take keywords from my regex query and do its usual magic... This is just a wild guess but one possible metric could be the number of backtracking steps the regex engine needs to take to match your search strings. Of course these values also depend mightily on the quality of your regex but when comparing several matches the one that was ""easier to match"" could be considered a better match than the one that the regex engine had to go through contortions for. Thanks for the answer! Would be an interesting metric indeed.  There are two passes. In the first it generates a list of all terms which match the regex. In the second it finds all documents with terms matching that regex. The major code you want to look at is in MultiTermQuery: public Query rewrite(IndexReader reader) throws IOException { FilteredTermEnum enumerator = getEnum(reader); BooleanQuery query = new BooleanQuery(); try { do { Term t = enumerator.term(); if (t != null) { TermQuery tq = new TermQuery(t); // found a match tq.setBoost(getBoost() * enumerator.difference()); // set the boost query.add(tq false false); // add to query } } while (enumerator.next()); } finally { enumerator.close(); } return query; } Two things: The boolean query is instantiated with coord on. So the standard coord scoring applies (i.e. the more terms you get the better). The boost of the term query is given by enumerator.difference(). However as of 3.0.1 this just returns 1:  @Override public final float difference() { // TODO: adjust difference based on distance of searchTerm.text() and term().text() return 1.0f; } So at some point this will return the distance (probably levenstein) between the terms. But for now it does nothing. @maayank: Lucene matches terms not strings. So your regex .*(dog|cat).* would match any single term matching that regex which would presumably be just the terms ""dog"" and ""cat"" and maybe something like ""hotdog"". I am not sure how exactly they would calculate distance here but I can guess it would be along the lines of ""consider each token of the regex as a literal (whether it was intended as a literal or not) and then calculate the distance."" Like the code says though this is just speculation; for now the distance always = 1 :-) In 2 you meant the distance between each matched term to its own matched fields' values right? How would it play out for example if there was a distance function with the regex "".*(dog|cat}.*"" and the value ""my dog and cat are happy""? you could always not analyze/not tokenize the values while indexing to get whole values regex. What do you mean by each token of the regex? the strings 'dog' and 'cat' from inside the regex query? It would seem problematic to me since how it would parse a token out of something like ""[^ABC]*\w[0-9]""? Thank you very much for the answer and comments! @maayank: good point about not tokenizing. I agree that regexs which have ""generic"" stuff in them like [^A] would be hard to find the distance of which is probably why the lucene devs haven't implemented it yet. My theory (consider every character as being in the alphabet and then calc the distance like that) is obviously pretty flawed; but I really don't know of a better solution. Maybe this would be a good question to ask as a general regex question? Tim's answer is also a good one."
83,A,if passing a stored document to MoreLikeThisHandler does mlt.mindf=1 exlude the document passed in? In Solr I am using the MoreLikeThis handler. I am passing in the unique id of a document that already exists in the index in order to find related documents. Does specifying mlt.mindf=1 include the previously mentioned document? If I want to be sure it exists in at least one document other than the one I pass in should I set the value of mlt.mindf=2 instead? mindf specifies the minimum document frequency that is the minimum number of documents that must include a term for that term to be counted. For more info see MoreLikeThis  That's a fun project your'e doing :D And your'e correct. You should set the value to 2 if you want to make sure that there is at-least one other document with the same term. Tip: If you're automatically finding related documentsi.e the relation(term) is not specified by a user make sure you search for good keywords to find related documents by filtering out unwanted words(use any of the available filters to do this). You could also suggest some words(maybe 56..) and use javascript to make something like the Wonder-wheel. Have fun :)
84,A,"Creating demo UI ontop of Solr I'm looking into some example UI on top of Solr that show of the functionality available in a demo like e.g. drill down faceted search. I found Blacklight which looks intensively interesting. Is there any other software that is worth researching or is Blacklight definitive the way to go? Thanks. Have you looked at using the Velocity templating built into Solr? You can find more about ""Solritas"" here: http://wiki.apache.org/solr/Solritas I am about to put together a demo Solr site for a presentation and am going down the Solritas route. You get faceting clustering and more! And no extra server to run. Interesting! I will look into that."
85,A,"Why are document stores like Lucene / Solr not included in NoSQL conversations? All of us have come across the recent hype of no-SQL solutions lately. MongoDB CouchDB BigTable Cassandra and others have been listed as no-SQL options. Here's an example: http://architects.dzone.com/articles/what-nosql-store-should-i-use However three years ago a co-worker and I were using Lucene.NET as what seem to fit the description of no-SQL. We did not use it just for user-inputted search queries; we used it to make a few reindexed RDBMS table data extremely performant. We implemented our own .NET sort-of-equivalent-to-Solr service to manage these indexes and make them callable. When I left the company the team switched to Solr itself. (For those not in the know Solr is a web service that wraps Lucene with REST-callable queries and index dumps.) What I don't understand is why is Solr not counted in the typical lists of no-SQL solution options? Am I missing something here? I assume that there are technical reasons why Solr is not comparable to the likes of CouchDB etc. and in fact I understand that CouchDB uses Lucene as its data store (yes?) but what disqualifies Solr? I'm not asking as some kind of Solr fanboy or anything I just don't understand why Solr and the like don't fit the definition of no-SQL and if Solr technically does fit the definition then what about it likely makes people pooh-pooh it? I'm asking because I'm having difficulty determining whether I should continue using Lucene-based solutions (like Solr) for solutions that I build or if I should really do more research with these other options. I once listened to an interview with Ursula K. LeGuin about fiction writing. The interviewer asked her about authors who work in different genre of writing. What makes one author a romance writer and another a mystery writer and another a science fiction writer etc. LeGuin responded by explaining: genre is about marketing not about content. It was an eye-opening statement. I think the same applies to technology solutions. The NoSQL movement is attracting attention because it's full of marketing energy right now. NoSQL data stores like Hadoop CouchDB MongoDB have commercial ventures backing them pushing their solutions as new and innovative and exciting so they can grow their business. The term ""NoSQL"" is a marketing brand that helps them to explain their value. You're right that Lucene/Solr is technically very similar to a NoSQL document store: it's a denormalized bag of documents (their term) with fields that aren't necessarily consistent across the collection of documents. It's indexed in a sophisticated way to allow you to search across all fields or by specific fields. But that's not the genre Lucene uses to explain its value. They don't have the same mission to grow a market and a business since they're managed by the Apache Foundation. They're happy to focus on the use case of fulltext search even though the technology could be used in other ways. They're following a tenet of software success: do one thing and do it well. Good thoughts thumbs-up. But CouchDB is an Apache project like Solr is and Solr is used by a lot of commercial scenarios such as CNET. So with your logic regarding commercial ventures vs. Apache other than the up-front messaging (i.e. ""faced searching"" rather than ""indexed column/value stores"") I still don't see why Solr isn't treated the same in the no-SQL space. CouchDB is supported by commercial enterprises Couchio and Cloudant. Damien Katz is the primary architect of CouchDB and he's the founder and CEO of Couchio. He just happens to grant his code to the Apache Foundation. RavenDB uses Lucene extensively IIRC  After doing more Google-searching I think this document sums it up pretty well: http://www.lucidimagination.com/blog/2010/04/30/nosql-lucene-and-solr/ Case in point Lucene/Solr is NoSql and could be considered one of NoSql's more mature ""forefathers"". It just does not get the NoSql hype it deserves because it didn't invent the term ""no-SQL"" and its users don't use the term so the hype machine overlooked it. Check out MUMPS for a *real* NoSQL forefather! http://en.wikipedia.org/wiki/MUMPS As NoSQL is usually interpreted as ""Not Only SQL"" MUMPS came in an era where it could not complement SQL at all. However kudos for the reference and ""nostolgia"" (although this is way before my time). Here's another document-oriented database that dates back to 1989: http://en.wikipedia.org/wiki/Lotus_Notes#Database I'm sure it's no coincidence that Damien Katz also worked on Lotus Notes at IBM. Berkeley DB and ESENT could be also considered NoSQL. Yes do check out MUMPS: http://thedailywtf.com/Articles/A_Case_of_the_MUMPS.aspx  I think that stimpy77 is partly right on the NoSQL being a branding thing. But also NoSQL means that it's a data storage platform that is simpler/easier then SQL based solutions. And I think while Solr/Lucene share some aspects (they store data) it really misses the mark to think that Solr/Lucene could be used as primary data storage for anything that has relationships. Sure lots of documents can be thrown into it and powerful search pull them back. But as soon as you want relationships then others such as CouchDB and others do much better that have a query syntax of some kind. Search is a bandaid solution in that case. Think about the use case ""find all documents tagged with word 'car'"". If I have some structures in my data then it's easy for me to get the document for tag car and pull everybody back. Versus relying on a search query that includes fq=tag:'car'. Search is more and more powerful the fewer relationships you have but the more relationships the better a datastore like CouchDB and brethren are. Thats why you still see CouchDB and friends paired with Solr and vice versa! Let each one do what it does best. Of course that isn't to say you can't leverage storing your source data in Solr that can be a powerful tool to use! ""[NoSQL is] a data storage platform that is simpler/easier then SQL based solutions"" Uh not hardly. It's just DIFFERENT. Especially when you get into distributed systems lack of consistency and non-ACID storage ""simple"" and ""easy"" are some of the first things you lose. ""I think that stimpy77 is partly right on the NoSQL being a branding thing."" I think this credit goes to Bill Karwin. Thanks tho. Regarding your point several of the opinioned definitions of ""NoSQL"" are that it specifically deemphasizes relational integrity. Does BigTable support relational data? Does Cassandra? Granted relational is nice but it is surely not part of the definition most people agree on of NoSQL. Solr on the other hand does support ""faceted searching"" which is sort of an abstract approach to many-to-many-to-many-to-many kinds of filtering. Filtering is not relational data but it can assist in subqueried virtual joins.  The main differences between a no sql and solr in operational wise are the following in my opinion. Solr requires an intermediate data store (database or XML files) whereas nosql itself a straight data store. You cannot do a constant writes to solr (solr 4.0 seems to bring that support) and you can only index at the max of every 2 mins and 200 records (which is very slow for high throughput writes and you are forced for an intermediate storage). You are require to change / define the schema when you alter what is stored in document. NoSQL has no such definitions. Solr indexes has performance implication when its index size grows whereas NoSQL is optimized for it (or claims to be :) ) Solr has underlying lucene search algorithms bundled but in NoSQL you need to build them This applies to the magnificent faceted search or blazing fast document search provided by solr. When people mark someone's answer down I wish they could say why. This answer has 5 points and I consider some correct and others not. But I'm no expect so would like it confirmed which are right and which are wrong. Point 1: Solr (Lucene) *is* a primary data store. There is nothing intermediate. If you want to you can use it as a system of record. Most people don't because its strength is in search. Point 2: There are people that are constantly indexing and doing commits once a second or even faster. Where did 2 minutes / 200 records come from? Point 3: True in this way it is not as flexible as other software. Point 4: Because Solr is designed around search it must have data in RAM for good performance. OS disk cache rules. Low RAM makes it slow. Point 5: Yes. Solr does search does it well. @HankCa : Thanks for your point Hello @Elyagrag Please find my responses below Appreciate your inputs here. Point 1: My point here was that you need to rebuild the whole document again and then submit that for processing which qualifies for an intermediate build (I agree you can avoid the store)My experiences with Solr was always to have a external data store which would help to re-index the data in case of the corrupt index. Point2:For performance reasons generally the commits are delayed and I agree you can change it to commit at very lower intervals but that affects performance on high transactions sites. Another thing to add to my point 1 is that when you need to add a field (which happens always) you need to reindex and thats is where you would need intermediate data store. see http://blog.michaelhamrah.com/2011/11/solr-improving-performance-and-other-considerations/  I think that the most relevant characteristic of solr/lucene that drops from the nosql list it's because until recently making lucene work as a real-time system was a pain. The usual workflow for any performant application was to index the incremental updates in batchs and updating the index every 5 minutes for example."
86,A,"Lucene Multilingual text field I have looked at this question - Indexing multilingual words in lucene and it confirmed some of my suspicions. I have an entity with a number of fields I wish to index. One of these fields can be one of several languages and I need to use different analyzers for each language. Am I best to implement this as different fields in the same index or as different indexes for each language? I am guessing that the trade off is between the overhead of running multiple indexes and the suckiness of cluttering up a single index. Any advice appreciated. I should never need to perform multi lingual searches. Will you ever need to search multiple languages at the same time? If so you can't use multiple indexes. One additional idea that you didn't mention: you can make each language a non-stored non-indexed field. Then you can copy all the (analyzed) data to a single stored+indexed field and it will behave as though you're searching a single field. (This is analogous to Solr's ""Copy fields"" - I'm not sure how hard it would be to do in hibernate.) If you keep them in separate indexes you should note that you won't be able to search across languages easily (or arguably at all). So if you want to allow queries like ""english:foo dutch:foo"" you'll need them in the same index. From a performance standpoint it would depend on how much data is shared. If the documents are disjoint (i.e. no document has two languages in it) then there probably won't be that much of a difference between having it in one index vs. two. The more data they share the more memory Lucene will duplicate so it will become better to have one index. My guess is that this is only an issue if you have a lot of stored data but YMMV. Well the problem from my point of view is this: If I have a Field called `Description` and it can be in language `A` or `B` and they both use different analyzers if I run both analyzers against the field to create `DescriptionA` and `DescriptionB` I will end up with badly tokenized indexes from whichever language is not being used. @Finbarr: So the problem you are trying to solve is ""how do I *identify* which language the text is in?"""
87,A,"The field value is 1 or true in solr search results I have one field that is indexed as string in Solr's schema.xml which is from a boolean(tinyint) column in mysql database. In query I search against this field using 1. But without any change this query cannot return correct results as it did. After I used true instead of 1 it worked again. Now it goes wrong again but with true no problem with 1. What's the exact problem here? Do I need change the field type in schema.yml to integer? Thank you in advance. Since it's a string field we can't possibly know how you indexed it. It could be ""true"" / ""false"" or ""1"" / ""0"" or ""on"" / ""off"" etc. Or even a mix of these maybe you have some documents with ""true"" and some with ""1"". If it's semantically a boolean field I recommend using the boolean fieldType e.g.: <field name=""inStock"" type=""boolean"" indexed=""true"" stored=""true"" /> for this to work you need the boolean fieldType declared (it comes declared in the default schema): <fieldType name=""boolean"" class=""solr.BoolField"" sortMissingLast=""true"" omitNorms=""true""/> Remember to rebuild the index after this change."
88,A,"Exclude footer header and navigation from Lucene indexing? I'm working on an old large site made ages ago by another developer. He has used Lucene as the engine for the site search indexing all pages in their full HTML form: (Some parts omitted for clarity) $this->index = Zend_Search_Lucene::open($path); $html = file_get_contents($document[""path""]); $doc = Zend_Search_Lucene_Document_Html::loadHTML($html); $doc->addField(Zend_Search_Lucene_Field::Text('url' $document[""path""])); $this->index->addDocument($doc); The problem is that the site navigation footer and header get indexed as well - doing a search for ""copyright"" returns every single page. Is there some switch I could flip when indexing full HTML content? I have no prior experience with Lucene but indexing the whole page seems pretty useless if there is no way to exclude the elements present on every page. Or should I just rewrite the search to just index the content from the database instead of cycling it through HTTP? Referring to Delve inside the Lucene indexing mechanism I would say that it is prudent to just store the textual information in lucene rather than the entire HTML. If you have direct database access storing the column data as ""Fields"" will provide you with a much fine grained control over search (For example performing boolean and range queries in those fields). So it seems using fields straight from the database is clearly the way to go. Thanks for the advice and the very enlightening link!"
89,A,Too many open files Error on Lucene The project I'm working on is indexing a certain number of data (with long texts) and comparing them with list of words per interval (about 15 to 30 minutes). After some time say 35th round while starting to index new set of data on 36th round this error occurred:  [ERROR] (2011-06-01 10:08:59169) org.demo.service.LuceneService.countDocsInIndex(?:?) : Exception on countDocsInIndex: java.io.FileNotFoundException: /usr/share/demo/index/tag/data/_z.tvd (Too many open files) at java.io.RandomAccessFile.open(Native Method) at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233) at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:69) at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:90) at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.<init>(NIOFSDirectory.java:91) at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:78) at org.apache.lucene.index.TermVectorsReader.<init>(TermVectorsReader.java:81) at org.apache.lucene.index.SegmentReader$CoreReaders.openDocStores(SegmentReader.java:299) at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:580) at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:556) at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:113) at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:29) at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:81) at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:736) at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:75) at org.apache.lucene.index.IndexReader.open(IndexReader.java:428) at org.apache.lucene.index.IndexReader.open(IndexReader.java:274) at org.demo.service.LuceneService.countDocsInIndex(Unknown Source) at org.demo.processing.worker.DataFilterWorker.indexTweets(Unknown Source) at org.demo.processing.worker.DataFilterWorker.processTweets(Unknown Source) at org.demo.processing.worker.DataFilterWorker.run(Unknown Source) at java.lang.Thread.run(Thread.java:636) I've already tried setting maximum number of open files by:  ulimit -n <number> But after some time when the interval has about 1050 rows of long texts the same error occurs. But it only occurred once. Should I follow the advice of modifying Lucene IndexWriter's mergeFactor from (Too many open files) - SOLR or is this an issue on the amount of data being indexed? I've also read that it's a choice between batch indexing or interactive indexing. How would one determine if indexing is interactive just by frequent updates? Should I categorize this project under interactive indexing then? UPDATE: I'm adding snippet of my IndexWriter:  writer = new IndexWriter(dir new StandardAnalyzer(Version.LUCENE_30) IndexWriter.MaxFieldLength.UNLIMITED); Seems like maxMerge (? or field length...) is already set to unlimited. You need to double check if ulimit value has actually been persisted and set to a proper value (whatever maximum is). It is very likely that your app is not closing index readers/writers properly. I've seen many stories like this in the Lucene mailing list and it was almost always the user app which was to blame not the Lucene itself.  I already used the ulimit but error still shows. Then I inspected the customized core adapters for lucene functions. Turns out there's too many IndexWriter.open directory that is LEFT OPEN. Should note that after processing will always call on closing the directory opened. Hi I'm new to Lucene. Do you mean invoking `writer.close()` or `writer.getDirectory().close()`? both writer and writer's directory should be closed after use. There are some open directory access on mine that's why I had the too many open files error. =] Interestingly I have done all that is said in the suggestions here and i still end up with the same error.  Use compound index to reduce file count. When this flag is set lucene will write a segment as single .cfs file instead of multiple files. This will reduce the number of files significantly. IndexWriter.setUseCompoundFile(true)
90,A,"Moving Lucene index to another server I am in the process of moving my Lucene indexing offline to be done by a conusmer of a JMS queue. I have it all working as it should. It creates the index correctly and I'm able to rsync the index files to the new box. The question is what is the best approach to have Lucene use the new index? How are others reinitializing their IndexWriters to use the new index files? Thanks! I am supposing you have all files that are indexed in new box right? Why can't you just open the index normally? (What other options are there?) For others who come after me here is what I've done. The goal was to have my JMS consumer generate my lucene index and then have each of my servers pull that new index when it's complete. Here are the steps I took: Created a crontab to generate my new index nightly Created a crontab to call a script updateLuceneIndex.sh to update the new index on each server !/bin/sh TIME=date +%s; rsync -av tomcat@consumer1.*.com:/home/tomcat/lucene /home/tomcat/lucene echo $TIME mv -f /home/tomcat/lucene/lucene /home/tomcat/lucene/$TIME As you see above it uses the timestamp as the directory name so the new index will show up in a folder named like /home/tomcat/lucene/1300291879 The server code will grab a list of the directories in the /home/tomcat/lucene directory. It sorts them based on the name of the new directory and grabs the last one (the newest) The server than deletes all old indexes except for the last 2 (in case one is corrupt) I create a new IndexReader pointing to the new directory I'm not sure if this is thread safe. I imagine if someone tries to search right when I am switching they'll get an error but I currently don't have that many searches to make that a likely scenario. However as our traffic grows it will definitely be a spot where things could break. If anyone knows a better way please advise. Here's that code: public void initialize(File newIndexDirectory) throws CorruptIndexException IOException { try { File path = (newIndexDirectory == null) ? new File(indexDirectory) : newIndexDirectory; Directory index = new SimpleFSDirectory(path); searcher = new IndexSearcher(index); logger.debug(""Successfully initialized index at: "" + path.getAbsolutePath()); currentIndexFile = path; } catch (Exception e) { logger.warn(""Lucene index is corrupt""); } } Following a year using your approach how has this worked?"
91,A,"Need Lucene query optimization advice Am working on web based Job search application using Lucene.User on my site can search for jobs which are within a radius of 100 miles from say ""BostonMA"" or any other location. Also I need to show the search results sorted by ""relevance""(ie. Score returned by lucene) in descending order. I'm using a 3rd party API to fetch all the cities within given radius of a city.This API returns me around 864 cities within 100 miles radius of ""BostonMA"". I'm building the city/state Lucene query using the following logic which is part of my ""BuildNearestCitiesQuery"" method. Here nearestCities is a hashtable returned by the above API.It contains 864 cities with CityName ass key and StateCode as value. And finalQuery is a Lucene BooleanQuery object which contains other search criteria entered by the user like:skillskeywordsetc. foreach (string city in nearestCities.Keys) { BooleanQuery tempFinalQuery = finalQuery; cityStateQuery = new BooleanQuery(); queryCity = queryParserCity.Parse(city); queryState = queryParserState.Parse(((string[])nearestCities[city])[1]); cityStateQuery.Add(queryCity BooleanClause.Occur.MUST); //must is like an AND cityStateQuery.Add(queryState BooleanClause.Occur.MUST); } nearestCityQuery.Add(cityStateQuery BooleanClause.Occur.SHOULD); //should is like an OR finalQuery.Add(nearestCityQuery BooleanClause.Occur.MUST); I then input finalQuery object to Lucene's Search method to get all the jobs within 100 miles radius.: searcher.Search(finalQuery collector); I found out this BuildNearestCitiesQuery method takes a whopping 29 seconds on an average to execute which obviously is unacceptable by any standards of a website.I also found out that the statements involving ""Parse"" take a considerable amount of time to execute as compared to other statements. A job for a given location is a dynamic attribute in the sense that a city could have 2 jobs(meeting a particular search criteria) todaybut zero job for the same search criteria after 3 days.SoI cannot use any ""Caching"" over here. Is there any way I can optimize this logic?or for that matter my whole approach/algorithm towards finding all jobs within 100 miles using Lucene? FYIhere is how my indexing in Lucene looks like: doc.Add(new Field(""jobId"" job.JobID.ToString().Trim() Field.Store.YES Field.Index.UN_TOKENIZED)); doc.Add(new Field(""title"" job.JobTitle.Trim() Field.Store.YES Field.Index.TOKENIZED)); doc.Add(new Field(""description"" job.JobDescription.Trim() Field.Store.NO Field.Index.TOKENIZED)); doc.Add(new Field(""city"" job.City.Trim() Field.Store.YES Field.Index.TOKENIZED  Field.TermVector.YES)); doc.Add(new Field(""state"" job.StateCode.Trim() Field.Store.YES Field.Index.TOKENIZED Field.TermVector.YES)); doc.Add(new Field(""citystate"" job.City.Trim() + "" "" + job.StateCode.Trim() Field.Store.YES Field.Index.UN_TOKENIZED  Field.TermVector.YES)); doc.Add(new Field(""datePosted"" jobPostedDateTime Field.Store.YES Field.Index.UN_TOKENIZED)); doc.Add(new Field(""company"" job.HiringCoName.Trim() Field.Store.YES Field.Index.TOKENIZED)); doc.Add(new Field(""jobType"" job.JobTypeID.ToString() Field.Store.NO Field.Index.UN_TOKENIZEDField.TermVector.YES)); doc.Add(new Field(""sector"" job.SectorID.ToString() Field.Store.NO Field.Index.UN_TOKENIZED Field.TermVector.YES)); doc.Add(new Field(""showAllJobs"" ""yy"" Field.Store.NO Field.Index.UN_TOKENIZED)); Thanks a ton for reading!I would really appreciate your help on this. Janis Apart from tempFinalQuery being unused and an unnecessary map lookup to get the state there doesn't seem to be anything too egregious in the code you post. Apart from the formatting... If all the time is taken in the Parse methods posting their code here would make sense.  Not quite sure if I completely understand your code but when it comes to geospatial search a filter approach might be more appropriate. Maybe this link can give you some ideas - http://sujitpal.blogspot.com/2008/02/spatial-search-with-lucene.html Maybe you can use Filters for other parts of your query as well. To be honest your query looks quite complex. --Hardy Could you please have a look at this and comment??Thanks. http://stackoverflow.com/questions/1052086/spatialquery-for-location-based-search-using-lucene  I might have missed the point of your question but do you have the possibility of storing latitude and longitude for zip codes? If that is an option you could then compute the distance between two coordinates providing a much more straightforward scoring metric. Could you please have a look at this and comment??Thanks. http://stackoverflow.com/questions/1052086/spatialquery-for-location-based-search-using-lucene  I'd suggest: storing the latitude and longitude of locations as they come in when a user enters a city and distance turn that into a lat/lon value and degrees do a single simple lookup based on numerical distance lat/lon comparisons You can see an example of how this works in the Geo::Distance Perl module. Take a look at the closest method in the source which implements this lookup via simple SQL.  I believe the best approach is to move the the nearest city determination into a search filter. I would also reconsider how you have the field setup; consider creating one term that has city+state so that would simplify the query.  Agree with the others here that this smells too much. Also doing a textual search on city names is not always that reliable. There is often a bit of subjectivity between place names (particularly areas within a city which might in themselves be large). Doing a geo spatial query is the way to go. Not knowing the rest of your set up it is hard to advise. You do have Spatial support built into Fluent to NHibernate and SQL Server 2008 for example. You could then do a search very quickly and efficiently. However your challenge is to get this working within Lucene. You could possibly do a ""first pass"" query using spatial support in SQL Server and then run those results through Lucene? The other major benefit of doing spatial queries is that you can then easily sort your results by distance which is a win for your customers."
92,A,Java Lucene: Search for terms that include non-alphanumeric characters I need to be able to return results using termDocs and Term's. I am not returning any results when I use standard analyser any ideas on other analysers avaliable to perform all same operations as standard analyser and return results using terms like (example term- #define):  analyser = new StandardAnalyser(Version.LUCENE_30); reader = IndexReader.open(FSDirectory.open(IndexDir) true); TermDocs td = reader.termDocs(); QueryParser parserContents = new QueryParser(Version.LUCENE_30fieldanalyser); query = parserContents.parse(searchTerm); docs = search.search(query 100000); ScoreDoc[] documents = docs.scoreDocs; for(ScoreDoc match : documents) { td.seek(new Term(fieldw)); td.skipTo(match.doc); hits = td.freq(); } However I do get results when I am trying to use queryparser and not termdocs. The hits are always zero in above context for terms like #define(special character #). Would be helpful if you could show a full chunk of code that's having the difficulty? How are you getting the reader above? With which query terms? Are you escaping correctly as outlined in http://lucene.apache.org/java/2_4_0/queryparsersyntax.html#Escaping%20Special%20Characters? @Femi: Updated what I have to get the reader but I think it's more of tokenizing while indexing or something like that is affecting the special characters like we have no escaping for # The StandardAnalyzer does a lot of pre-processing of tokens (it uses a stop list removes non-alpha characters lower-cases etc.) so that probably accounts for what you're seeing in your search results. Try analyzing same field with the SimpleAnalyzer or maybe even the WhitespaceAnalyzer to see what you get. That might give you enough experience with the results to know whether one of these analyzers is adequate or how to build your own that specifies the exact tokenizing operations you need. You might also want to add more than one field with the same values which were processed with different analyzers. That way for example you could search for stemmed and unstemmed text for text with or without the stop words removed with or without the special characters included etc.
93,A,Solr paging performance I have read (http://old.nabble.com/using-q%3D---adding-fq%3D-to26753938.html#a26805204): FWIW: limiting the number of rows per request to 50 but not limiting the start doesn't make much sense -- the same amount of work is needed to handle start=0&rows=5050 and start=5000&rows=50. Than he completes: There are very few use cases for allowing people to iterate through all the rows that also require sorting. Is that right? Is that true just for sorted results? How many pages of 10 rows each do you recommend to allow the user to iterate? Does Solr 1.4 suffer the same limitation? start=0&rows=5050 and start=5000&rows=50 Depends how you jump to start=5000. If you scroll through all results from 0 to 4999 ignoring them all and then continue scrolling from 5000 to 5050 then yes same amount of work is done here. Best thing to do is to limit the rows fetched from database itself by using something like ROWNUM in Oracle. . iterate through all the rows that also require sorting Few but yes there are use cases that have this requirement. Examples would be CSV/Excel/PDF exports. This question is about Solr not about a RDBMS. Doesn't work the same way.  Yes that's true also for Solr 1.4. That does not mean that start=0&rows=5050 has the same performance as start=5000&rows=50 since the former has to return 5050 documents while the latter only 50. Less data to transfer -> faster. Solr doesn't have any way to get ALL results in a single page since it doesn't make much sense. As a comparison you can't fetch the whole Google index in a single query. Nobody really needs to do that. The page size of your application should be user-definable (i.e. the user might choose to see 10 25 50 or 100 results at once). The default page size depends on what kind of data you're paging and how relevant the results really are. For example when searching on Google you usually don't look beyond the first few results so 10 elements are enough. eBay on the other hand is more about browsing the results so it shows 50 results per page by default and it doesn't even offer 10 results per page. You also have to take scrolling into account. Users would probably get lost when trying to browse through a 200-result page not to mention that it takes considerably longer to load. Ur solution is accepted so I voted for you :-)
94,A,"How to do a full text search in Cocoa? I need something like Lucene to do an optimized full text search in Cocoa. I am working on an Iphone app to search through a database. Anybody has any luck with other databases. Any help is appreciated. So far I can only find this. http://github.com/tcurdt/lucenekit/tree/master According to your comment: ""..I am not sure if I explained myself properly. I am looking to build an Iphone app that can do searches by passing the search phrases to a remote server..."" You could just use the Java (or .NET) version of Lucene server side and pass the search query to the server from the phone in objective c  The official Lucene is written based on Java if I'm not mistaken and I haven't played with the ObjC port you've mentioned. It's a Mac OS X project and a quick look shows some dependancies that may be difficult to port over to the iPhone. Depending on the complexity of your searching needs I see a few possible paths for you: For simple searching: the built-in SQLite supports indexes. This will be fast. Wait for SDK 3.0 and CoreData. Try your luck porting the above project. looks like SQLLite is an easy way out. I am not sure if I explained myself properly. I am looking to build an Iphone app that can do searches by passing the search phrases to a remote server. Can this be done using standard architecture?"
95,A,"adding documents to an existing index in lucene I would like to ask of how to add new documents to an existing lucene index. in the source code below I just change the paramater of IndexWriter into false. IndexWriter indexWriter = new IndexWriter( FSDirectory.open(indexDir) new SimpleAnalyzer() false IndexWriter.MaxFieldLength.LIMITED); because false means that the index will still be open and not close. also to add new document I should use indexWriter.addDocument(doc) but my question is how exactly can I add new documents to an existing lucene index. I am a bit loss in finding out where to put a new path directory containing new Documents in lucene class so that lucene can index those new documents and add it into existing indexes. any help would be appreciated though. thanks. import org.apache.lucene.analysis.SimpleAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.store.FSDirectory; import java.io.File; import java.io.FileReader; import java.io.IOException; public class testlucene1 { public static void main(String[] args) throws Exception { File indexDir = new File(""C:/Users/Raden/Documents/lucene/LuceneHibernate/adi""); File dataDir = new File(""C:/Users/Raden/Documents/lucene/LuceneHibernate/adi""); String suffix = ""txt""; testlucene1 indexer = new testlucene1(); int numIndex = indexer.index(indexDir dataDir suffix); System.out.println(""Total files indexed "" + numIndex); } private int index(File indexDir File dataDir String suffix) throws Exception { IndexWriter indexWriter = new IndexWriter( FSDirectory.open(indexDir) new SimpleAnalyzer() false IndexWriter.MaxFieldLength.LIMITED); indexWriter.setUseCompoundFile(false); indexDirectory(indexWriter dataDir suffix); int numIndexed = indexWriter.maxDoc(); indexWriter.optimize(); indexWriter.close(); return numIndexed; } private void indexDirectory(IndexWriter indexWriter File dataDir String suffix) throws IOException { File[] files = dataDir.listFiles(); for (int i = 0; i < files.length; i++) { File f = files[i]; if (f.isDirectory()) { indexDirectory(indexWriter f suffix); } else { indexFileWithIndexWriter(indexWriter f suffix); } } } private void indexFileWithIndexWriter(IndexWriter indexWriter File f String suffix) throws IOException { if (f.isHidden() || f.isDirectory() || !f.canRead() || !f.exists()) { return; } if (suffix != null && !f.getName().endsWith(suffix)) { return; } System.out.println(""Indexing file "" + f.getCanonicalPath()); Document doc = new Document(); doc.add(new Field(""contents"" new FileReader(f))); doc.add(new Field(""filename"" f.getCanonicalPath() Field.Store.YES Field.Index.ANALYZED)); indexWriter.addDocument(doc); } } also to add new document I should use .... but my question is how exactly can I add new documents to an existing lucene index can you please clarify what you mean? you know how to add documents to an index as you stated but then you ask how to... add new documents? allrightit was my mistake.I didn't fully understand the source code.but upon reading your comment I just realize it.thanks for the tips then. :-)  When you instantiate a new IndexWriter you will not create a new index (unless you explicitly tell lucene to force a new one). So your code will work regardless of whether the index already exists. oh yes.I just realize it.thanks though. :-) yes I know.but I was trying to add new documents to an existing index.what do you reckon I should do to achieve this? :-) I don't understand your question then. You create an indexwriter that looks at an existing index in the exact same way that you create an indexwriter which makes a new index. So your code will work regardless of whether `indexDir` has stuff in it or not."
96,A,"Spelling correction for data normalization in Java I am looking for a Java library to do some initial spell checking / data normalization on user generated text content imagine the interests entered in a Facebook profile. This text will be tokenized at some point (before or after spell correction whatever works better) and some of it used as keys to search for (exact match). It would be nice to cut down misspellings and the like to produce more matches. It would be even better if the correction would perform well on tokens longer than just one word e.g. ""trinking coffee"" would become ""drinking coffee"" and not ""thinking coffee"". I found the following Java libraries for doing spelling correction: JAZZY does not seem to be under active development. Also the dictionary-distance based approach seems inadequate because of the use of non-standard language in social network profiles and multi-word tokens. APACHE LUCENE seems to have a statistical spell checker that should be much more suited. Question here would how to create a good dictionary? (We are not using Lucene otherwise so there is no existing index.) Any suggestions are welcome! You can hit the Gutenberg project or the Internet Archive for lots and lots of corpus. Also I think that the Wiktionary could help you. You can even make a direct download.  http://code.google.com/p/google-api-spelling-java is a good Java spell checking library but I agree with Thomas Jung that may not be the answer to your problem. Thanks for the link anyways interesting API!  With regards to populating a Lucene index as the basis of a spell checker this is a good way to solve the problem. Lucene has an out the box SpellChecker you can use. There are plenty of word dictionaries available on the net that you can download and use as the basis for your lucene index. I would suggest supplementing these with a number of domain specific texts as well e.g. if your users are medics then maybe supplement the dictionary with source texts from medical thesis and publications. Thanks I think building a Lucene index will be my second try after I tried if Jazzy works ""good enough"". Both links in this post seem to be broken  What you want to implement is not spelling corrector but a fuzzy search. Peter Norvig's essay is a good starting point to build a fuzzy search from candidates checked against a dictionary. Alternatively have a look at BK-Trees. An n-gram index (used by Lucene) produces better results for longer words. The approach to produce candidates up to a given edit distance will probably work good enough for words found in normal text but will not work good enough for names addresses and scientific texts. It will increase you index size though. If you have the texts indexed you have your text corpus (your dictionary). Only what is in your data can be found anyway. You need not use an external dictionary. A good resource is Introduction to Information Retrieval - Dictionaries and tolerant retrieval . There is a short description of context sensitive spelling correction. Thank you for your insightful comment and the interesting book link. You are right what I really want is fuzzy search. However I will see how/if spell checking works for my particular application (maybe its good enough right now) and revisit the ideas you mentioned later. Thanks a lot!  Try Peter Norvig's spell checker. I was referring to the problem of selecting the right corpus (to get the right frequencies e.g. not the frequencies from English literature but the ones suitable for variable-quality social network data). If I understand Norvig's code correctly it only takes edit distances for single words up to 2 into account. That means it will work surprisingly good for single words and not at all for multiple word tokens. I really like Norvig's little spell checker thats awesome work! However the question boils down to selecting the right text corpus (just like with the more advanced LUCENE). Taking frequencies from the freely available work of Shakespeare won't help correcting social network profiles. So you're saying that ""trinking"" instead of ""drinking"" isn't addressed? I'll have to re-read Norvig's article and perhaps implement it for myself because I thought it could help."
97,A,"Frequencies of lucene unigrams and bigrams i am storing in lucene index ngrams up to level 3. When I am reading the index and calculating scoring of terms and ngrams I am obtaining results like this TERM FREQUENCY.... TFIDF minority 25 16.512926 minority report 24 16.179296 report 27 13.559037 cruise 12 11.440491 tom cruise 7 8.737819 So if we look at the example of ""tom cruise"" together as bigram it occurs 7 times. And from this we see that ""cruise"" occurs alone 5 times. So I dont want this duplication of frequency because ""cruise"" alone has scored better than ""tom cruise"" which is not true since it is contained inside. Sorry if i explain bad i dont know how to call this type of scoring if someone know to explain this technical words please edit. Thank you I believe I answered a similar question you asked a while ago. IIUC you want the more important terms to stand out and you feel that ""tom cruise"" is more important than ""cruise"". This looks like a problem in your model of the data. TFIDF seems to be wrong for what you want. You can try building a language model as described in Peter Norvig's ""Beautiful Data"" chapter. The gist is: Calculate a probability per each unigram bigram and trigram (you will need smoothing or back-off as explained in the paper). Choose your terms by probability rather than TFIDF. A Language Model Approach to Keyphrase Extraction seems to do similar stuff. Some alternatives are Kea (which uses TFIDF as one feature among several) and Peter Turney's Keyphrase extraction work. Thank you a lot for the tips. I went for the Kea however seems to be offering more domain specific controlled vocabularies but from Kea page i read about Maui that does same stuff with some additional features. http://code.google.com/p/maui-indexer/ I see that the results i am getting are very good! However i will dig now try to see exactly the details of the algorithm and scoring calculations..Thanx!"
98,A,"Getting Error with hibernate configuration file when adding lucene I am trying to add lucene to my application but i am running into error due to my configuration file: <?xml version=""1.0"" encoding=""UTF-8""?> <!DOCTYPE hibernate-configuration PUBLIC ""-//Hibernate/Hibernate Configuration DTD 3.0//EN"" ""http://hibernate.sourceforge.net/hibernate-configuration-3.0.dtd""> <hibernate-configuration> <session-factory> <property name=""hibernate.search.default.directory_provider""> org.hibernate.search.store.FSDirectoryProvider </property> <property name=""hibernate.search.default.indexBase"">./lucene/indexes</property> <property name=""hibernate.search.default.batch.merge_factor"">10</property> <property name=""hibernate.search.default.batch.max_buffered_docs"">10</property> <event type=""post-update""> <listener class=""org.hibernate.search.event.FullTextIndexEventListener""/> </event> <event type=""post-insert""> <listener class=""org.hibernate.search.event.FullTextIndexEventListener""/> </event> <event type=""post-delete""> <listener class=""org.hibernate.search.event.FullTextIndexEventListener""/> </event> <event type=""post-collection-recreate""> <listener class=""org.hibernate.search.event.FullTextIndexEventListener""/> </event> <event type=""post-collection-remove""> <listener class=""org.hibernate.search.event.FullTextIndexEventListener""/> </event> <event type=""post-collection-update""> <listener class=""org.hibernate.search.event.FullTextIndexEventListener""/> </event> <property name=""hibernate.connection.driver_class"">org.postgresql.Driver</property> <property name=""hibernate.connection.url"">jdbc:postgresql://localhost/postgres</property> <property name=""hibernate.connection.username"">postgres</property> <property name=""hibernate.connection.password"">noor</property> <property name=""hibernate.connection.pool_size"">10</property> <property name=""show_sql"">true</property> <property name=""hibernate.dialect"">org.hibernate.dialect.PostgreSQLDialect</property> <property name=""hibernate.hbm2ddl.auto"">update</property> <property name=""current_session_context_class"">thread</property> <property name=""hibernate.search.default.indexBase""> /users/application/indexes </property> <mapping resource=""com/BiddingSystem/Models/Users.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/PersonalUser.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/BusinessUser.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/BusinessContactNumbers.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Attribute.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/AttributeOption.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Category.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Item.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Auction.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Picture.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Administrator.hbm.xml""/> </session-factory> </hibernate-configuration> stack trace: org.hibernate.MappingException: invalid configuration at org.hibernate.cfg.Configuration.doConfigure(Configuration.java:2211) at org.hibernate.cfg.Configuration.configure(Configuration.java:2128) at org.hibernate.cfg.Configuration.configure(Configuration.java:2107) at com.BiddingSystem.server.ServiceImpl.<init>(ServiceImpl.java:58) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at java.lang.Class.newInstance0(Class.java:355) at java.lang.Class.newInstance(Class.java:308) at org.mortbay.jetty.servlet.Holder.newInstance(Holder.java:153) at org.mortbay.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:339) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:463) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:362) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:729) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:324) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505) at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:843) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:647) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380) at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:395) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:488) Caused by: org.xml.sax.SAXParseException: The content of element type ""session-factory"" must match ""(property*mapping*(class-cache|collection-cache)*event*listener*)"". at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:195) at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:131) at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:384) at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:318) at com.sun.org.apache.xerces.internal.impl.dtd.XMLDTDValidator.handleEndElement(XMLDTDValidator.java:2017) at com.sun.org.apache.xerces.internal.impl.dtd.XMLDTDValidator.endElement(XMLDTDValidator.java:901) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanEndElement(XMLDocumentFragmentScannerImpl.java:1782) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2938) at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:648) at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:511) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:808) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737) at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119) at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205) at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522) at org.dom4j.io.SAXReader.read(SAXReader.java:465) at org.hibernate.cfg.Configuration.doConfigure(Configuration.java:2208) ... 28 more Try placing all the <event> tags at the bottom. The order of tags is sometimes important (if defined so in the xsd) Thanks it works"
99,A,"Lucene: Can I run a query against few specific docs of the collection only? Can I run a query against few specific docs of the collection only ? Can I filter the built collection according to documents fields content ? For example I would like to query over documents having field2 = ""abc"". thanks Sure -- use a Filter. See http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/search/QueryWrapperFilter.html The code will look something like: QueryParser qp = ... Filter filter = new QueryWrapperFilter(qp.parse(""field2:abc"")); // pass filter to searcher.search()"
100,A,"What analyzer should I use for a URL in lucene.net? I'm having problems getting a simple URL to tokenize properly so that you can search it as expected. I'm indexing ""http://news.bbc.co.uk/sport1/hi/football/internationals/8196322.stm"" with the StandardAnalyzer and it is tokenizing the string as the following (debug output): (http04type=<ALPHANUM>) (news.bbc.co.uk721type=<HOST>) (sport1/hi2231type=<NUM>) (football3240type=<ALPHANUM>) (internationals/8196322.stm4167type=<NUM>) In general it looks good http itself then the hostname but the issue seems to come with the forward slashes. Surely it should consider them as seperate words? What do I need to do to correct this? Thanks P.S. I'm using Lucene.NET but I really don't think it makes much of a difference with regards to the answers. You should parse the URL yourself (I imagine there's at least one .Net class that can parse a URL string and tease out the different elements) then add those elements (such as the host or whatever else you're interested in filtering on) as Keywords; don't Analyze them at all.  The StandardAnalyzer which uses the StandardTokenizer doesn't tokenize urls (although it recognised emails and treats them as one token). What you are seeing is it's default behaviour - splitting on various punctuation characters. The simplest solution might be to use a write a custom Analyzer and supply a UrlTokenizer that extends/modifies the code in StandardTokenizer to tokenize URLs. Something like: public class MyAnalyzer extends Analyzer { public MyAnalyzer() { super(); } public TokenStream tokenStream(String fieldName Reader reader) { TokenStream result = new MyUrlTokenizer(reader); result = new LowerCaseFilter(result); result = new StopFilter(result); result = new SynonymFilter(result); return result; } } Where the URLTokenizer splits on / - _ and whatever else you want. Nutch may also have some relevant code but I don't know if there's a .NET version. Note that if you have a distinct fieldName for urls then you can modify the above code the use the StandardTokenizer by default else use the UrlTokenizer. e.g. public TokenStream tokenStream(String fieldName Reader reader) { TokenStream result = null; if (fieldName.equals(""url"")) { result = new MyUrlTokenizer(reader); } else { result = new StandardTokenizer(reader); } I know this is Java - but same principle in theory for .NET You can probably just copy it and edit it to add the additional tokens you need. BTW - i should have mentioned if any of your analyzers are doing any expensive initialisation (like hige lists of stop words) you should use the reusableTokenStream method. Thanks Joel. I ended up creating a tokenizer which inherited from CharTokenizer as this seemed simpler and did what I required. Thanks for the information I've look at the StandardTokenizer and I really don't understand half of it! I don't need or want all of the code handed to me on a plate but a nudge in the right direction of how to create a customer tokenizer based on those stop characters would be amazing. Thanks."
101,A,"Lucene.NET - Find documents that do not contain a specified field Let's say I have 2 instance of a class called 'Animal'. Animal has 3 fields: Name Age and Type The name field is nullable so before I insert an instance of Animal as a Lucene indexed document I check if Animal.Name == null and if it does I do not insert it as a field in my document. If I were to retrieve all animals I would see that the Name field does not exist and I can set its value to null. However there may be situations where I want to say ""Get me all animals that do not have a name specified yet."" In this situation I want to retrieve all Lucene.NET documents from my animal index that do not contain the Name field. Is there an easy way to do this with Lucene.NET? I want to stay away from having to perform some sort of hack to check if my name field has a value of 'null'. I believe you can do this with Solr but not with Lucene directly so it's not possible with Lucene.Net. Here's two workarounds which are not that bad: For items with a NULL value in the field add a custom string like __NULL__ or similar instead of omitting the field. This would be searchable. For items with a NULL value in the field add a field which will not be present on the items with a value. Eg. EMPTY_FIELD = ""no"". This can be used in a filter. Hope this helps you a bit on the way. Your solution of just performing a hack with a reserved keyword will work. Now I just need to figure out why special characters are not properly handled when I perform a search. I created another post to find out why I am getting such strange behavior with special characters. http://stackoverflow.com/questions/2732987/lucene-and-special-characters"
102,A,"Lucene SpanQuery - what is it good for? Can someone explain or provide a link to an explanation of what a SpanQuery is and what are typical use cases for it? The javadoc is very laconic and keeps mentioning the concept of ""Span"" which I'm not quite sure I get. Also I'm interested in the SpanScorer in the highlighter and what it does exactly. Span? That reminds me of a Monty Python... ""Span lovely Span wonderful Span!"" Spans provide a proximity search feature to Lucene. They are used to find multiple terms near each other without requiring the terms to appear in a specified order. You can specify the terms that you want to find and how close they must be. You can combine these span queries with each other or with other types of Lucene queries. How is the different to a query with a slop? This is useful http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/  Found this all about the SpanQuery  The javadocs you linked to are for a class in the "" org.apache.lucene.search.spans "" package. if you had clicked on the ""package"" link on those javadocs you would have been taken to... http://lucene.apache.org/java/2_4_1/api/org/apache/lucene/search/spans/package-summary.html ...where the concept of Spans and what a Span is are explained in depth.  A span query is a query that returns infomation about where in a document each match took place. You use the getSpans() method to get the locations. The following deck of slides (unfortunately in Powerpoint) contain an example: http://www.cnlp.org/apachecon2005/AdvancedLucene.ppt"
103,A,Can a Lucene index be stored in an RDBMS If I don't have access to the file system but do have access to a MySQL instance can I store the lucene index in a mysql database. I was able to find the DbDirectory and thought that it might do the trick. However it looks like it works with a Berkeley DB rather than an RDBMS. I dont believe you can it would defeat the purpose of Lucene. If your indexing does not take to long you could consider a RAMDirectory which I believe stores it in memory. The purpose of Lucene is to search fast and efficiently effectively storing a database in a database is not efficient or logical. The purpose of Lucene is searching. Why would storing the Lucene indexes in a database defeat that purpose?  There are some contribution that stores lucene index in more simple datastores (from perspective of data model). For example BerkleyDB and Cassandra. So technically it is possible to write implementation of Directory which would store index in Jdbc. There is one in Compass framework.
104,A,"How to increase position offsets in a lucene index to correspond to tags? I am using Lucene 3.0.3. In preparation to using SpanQuery and PhraseQuery I would like to mark paragraph boundaries in my index in a way that will discourage these queries from matching across paragraph boundaries. I understand that I need to increment position by some suitably large value in the PositionIncrementAttribute when processing text to mark paragraph boundaries. Let's assume that in the source document my paragraph boundaries are marked by <p>...</p> pairs. How do I set up my token stream to detect the tags? Also I don't actually want to index the tags themselves. For the purposes of indexing I would rather increment the position of the next legitimate token rather than emitting a token that corresponds to the tag since I don't want it to affect search. The easiest way to add gaps (= PositionIncrement > 1) is to provide a custom TokenStream. You do not need to change your Analyzer for that. However HTML parsing should be done upstream (i.e. you should segment and clean your input text accordingly before feeding it to Lucene). Here is a full working example (imports omitted): public class GapTest { public static void main(String[] args) throws Exception { Directory dir = FSDirectory.open(new File(""/tmp/lucene"")); IndexWriter iw = new IndexWriter(dir new SimpleAnalyzer() true MaxFieldLength.UNLIMITED); Document doc = new Document(); doc.add(new Field(""body"" ""A B C"" Store.YES Index.ANALYZED TermVector.YES)); doc.add(new Field(""body"" new PositionIncrementTokenStream(10))); doc.add(new Field(""body"" ""D E F"" Store.YES Index.ANALYZED TermVector.YES)); System.out.println(doc); iw.addDocument(doc); iw.close(); IndexSearcher is = new IndexSearcher(dir); QueryParser qp = new QueryParser(Version.LUCENE_30 ""body"" new SimpleAnalyzer()); for (String q : new String[] { ""\""A B C\"""" ""\""A B C D\"""" ""\""A B C D\"""" ""\""A B C D\""~10"" ""\""A B C D E F\""~10"" ""\""A B C D F E\""~10"" ""\""A B C D F E\""~11"" }) { Query query = qp.parse(q); TopDocs docs = is.search(query 10); System.out.println(docs.totalHits + ""\t"" + q); } is.close(); } /** * A gaps-only TokenStream (uses {@link PositionIncrementAttribute} * * @author Christian Kohlschuetter */ private static final class PositionIncrementTokenStream extends TokenStream { private boolean first = true; private PositionIncrementAttribute attribute; private final int positionIncrement; public PositionIncrementTokenStream(final int positionIncrement) { super(); this.positionIncrement = positionIncrement; attribute = addAttribute(PositionIncrementAttribute.class); } @Override public boolean incrementToken() throws IOException { if (first) { first = false; attribute.setPositionIncrement(positionIncrement); return true; } else { return false; } } @Override public void reset() throws IOException { super.reset(); first = true; } } } Thanks @Christian. This is interesting. I hadn't thought of this approach of segmenting the text first and then introducing the gaps. This could certainly be used to build up a more principled document parser. I had taken a more brute-force approach and created a variant of the `StandardTokenizer` based on a modified jflex file into which I inserted a rule to detect paragraphs: `MARKUP = (""<"" {ALPHANUM}+ {ANY}* "">"" | """") ` and `ANY = ({ALPHANUM} | {WHITESPACE} | [\""='&;])`. Of course my approach is more brittle but it seems to work well on my TREC documents. @Gene. Yup tokenizing at HTML level might work at a first glance but opens up a bunch of problems that will be revealed only when it's too late ;-) @Gene. What might be useful here is to use [boilerpipe](http://code.google.com/p/boilerpipe/) with a KeepEverythingExtractor. This already provides you some segmentation that can be used as input for the suggested solution above. By the way a simpler way to introduce this gap without a custom tokenstream is to implement Analyzer.getPositionIncrementGap() to return the gap that you want... its designed specifically for this purpose. @Robert Thanks! When I build a document parser that preserves sentence/paragraph structure I will definitely use this approach to delimit to the various pieces. @Robert what pattern should I use to have getPositionIncrementGap() return different values depending on the text being added? Calls to Document.add() just buffer the fields without any processing so the context that might determine gap size is lost."
105,A,"how to achieve pagination in lucene? Wondering how to achieve pagination in Lucene as it does not inherently support pagination. I basically need to search for 'top 10 entries' (based on some parameter) then 'next 10 entries' and so on. And at the same time I don't want Lucene to hog memory. Any piece of advice would be appreciated. Thanks in advance. You will need to apply your own paging mechanism something similar to that below.  IList<Document> luceneDocuments = new List<Document>(); IndexReader indexReader = new IndexReader(directory); Searcher searcher = new IndexSearcher(indexReader); TopDocs results = searcher.Search(""Your Query"" null skipRecords + takeRecords); ScoreDoc[] scoreDocs = results.scoreDocs; for (int i = skipRecords; i < results.totalHits; i++) { if (i > (skipRecords + takeRecords) - 1) { break; } luceneDocuments.Add(searcher.Doc(scoreDocs[i].doc)); } You will find that iterating the scoreDocs array will be lightweight as the data contained within the index is not really used until the searcher.Doc method is called. Please note that this example was written against a slightly modified version of Lucene.NET 2.3.2 but the basic principal should work against any recent version of Lucene. I agree results in Lucene are not as heavy as the results when querying a database so you can easily implement custom pagination methods without having to deal with performance issues The problem here is when you search for large data set with higher pagination number search is getting slower. It is like you search for a thing then omit a part of the search.  Another version of loop continuing with Kane's code snippet; .................... ScoreDoc[] scoreDocs = results.scoreDocs; int pageIndex = [User Value]; int pageSize = [Configured Value]; int startIndex = (pageIndex - 1) * pageSize; int endIndex = pageIndex * pageSize; endIndex = results.totalHits < endIndex? results.totalHits:endIndex; for (int i = startIndex ; i < endIndex ; i++) { luceneDocuments.Add(searcher.Doc(scoreDocs[i].doc)); }"
106,A,"Solr search for lots of values I'm using Solr to search for a long list of IDs like so: ID:(""4d0dbdd9-d6e1-b3a4-490a-6a9d98e276be"" ""4954d037-f2ee-8c54-c14e-fa705af9a316"" ""0795e3d5-1676-a3d4-2103-45ce37a4fb2c"" ""3e4c790f-5924-37b4-9d41-bca2781892ec"" ""ae30e57e-1012-d354-15fb-5f77834f23a9"" ""7bdf6790-de0c-ae04-3539-4cce5c3fa1ff"" ""b350840f-6e53-9da4-f5c2-dc5029fa4b64"" ""fd01eb56-bc4c-a444-89aa-dc92fdfd3242"" ""4afb2c66-cec9-8b84-8988-dc52964795c2"" ""73882c65-1c5b-b3c4-0ded-cf561be07021"" ""5712422c-12f8-ece4-0510-8f9d25055dd9""...etc This works up to a point but above a certain size fails with the message: too many boolean clauses. You can increase the limit in solrconfig.xml but this will only take it so far - and I expect the limit is there for a reason: <maxBooleanClauses>1024</maxBooleanClauses> I could split the query into several little ones but that would prevent me then sorting the results. There must be a more appropriate way of doing this? IMHO the real problem here is the situation that led you to need to do this. I agree but sadly it's not something I can change. I didn't create the system I just have to fix it. I somewhat have to agree with Mauricio this sounds like a SQL type query... Or a NoSQL solution versus something a full text fuzzy matching search engine is really going to rock at... You should be using a Lucene filter instead of building up the huge boolean query. Try using FieldCacheTermsFilter and pass that filter in to your Searcher. FieldCacheTermsFilter will translate your UID's to a Lucene DocIdSet and it'll do it fast since it's doing it via the FieldCache. +1 that is the way Can I use FieldCacheTermsFilter through the http query api? How?"
107,A,Zend_Search_Lucene - Can't create directory '/data/users_index' I have a problem creating an index with Zend_Search_Lucene. Now everything works fine on my local machine so I guess there is just an issue with file permissions on the webserver. Here is how I'm trying to create index in controller: $index = Zend_Search_Lucene::create('/data/users_index'); Of course the data directory has permissions set to 0777. Here is the directory listing: public_html public 0755 css 0755 js 0755 data 0777 Yet I'm getting this error: Can't create directory '/data/users_index'. $index = Zend_Search_Lucene::create('public/data/users_index'); ?? That doesn't work either I already tried that (it works on my local machine too of course but not on the server online). This does not provide an answer to the question. To critique or request clarification from an author leave a comment below their post.  Edit/Update: After further reading and seeing your structure I'd give it a shot and try using an ABSOLUTE path rather than a relative to ensure its writing to the write location. Sorry I missed that portion earlier. It's obviously not the best practice but it would atleast narrow down whether or not its a permission/finding issue. So change it to something like $index = Zend_Search_Lucene::create('/path/to/public_html/public/data/users_index'); Although you really should put that outside of the public HTML folder. There's no reason that the public should have access to your Lucene Index Files. For example mine are stored here: '../application/models/lucene/articles/index' If you are on a Linux/Unix machine you are going to have to CHMOD the folder or CHOWN/CHGRP so that the web server has write access. If you have access to the server you could simply run: chmod -R 770 /path/to/your/data/users_index If you are not the admin of the server however you should probably ask the server admin to make sure this is the proper permissions to be applied to this folder every admin has his/her own quirks about how they want folder permissions setup; what group they should be in; who gets to change it; etc. If you are on a Windows machine you are going to have to right click on the folder and grant permissions to the IUSR_XXXXX account and give them read/write access to that folder. (Replace XXX with whatever your machines name is) In most ZF applications you have an APPLICATION_PATH defined - use it - APPLICATION_PATH.'/models/lucene/articles/index' Actually I changed it to $index = Zend_Search_Lucene::create('data/users_index'); and it works. Weird. On my local machine it doesn't matter if I put slash in the beginning or not...
108,A,"Lucene PorterStemmer question Given the following code: Dim stemmer As New Lucene.Net.Analysis.PorterStemmer() Response.Write(stemmer.Stem(""mattress table"") & ""<br />"") // Outputs: mattress t Response.Write(stemmer.Stem(""mattress"") & ""<br />"") // Outputs: mattress Response.Write(stemmer.Stem(""table"") & ""<br />"") // Outputs: tabl Could someone explain why the PorterStemmer produces different results when there is a space in the word? I was expecting 'mattress table' to be stemmed to 'mattress tabl'. Also this is further confusing by the following code: Dim parser As Lucene.Net.QueryParsers.QueryParser = New Lucene.Net.QueryParsers.QueryParser(""MyField"" New PorterStemmerAnalyzer) Dim q As Lucene.Net.Search.Query = parser.Parse(""mattress table"") Response.Write(q.ToString & ""<br />"") // Outputs: MyField:mattress MyField: tabl q = parser.Parse(""""""mattress table"""""") Response.Write(q.ToString & ""<br />"") // Outputs My Field:""mattress tabl"" Could someone explain why I am getting different results from the QueryParser() and the Stem() function for the same word(s) using the same Analyzer? Thanks Kyle The query parser tokenizes it first into two tokens. Porter considers it all as one ""word"" and so only stems the last portion.  PorterStemmerAnalyzer is composed of series of tokenizers and filters. PorterStemmer is one of the filters to the tokenstream generated. If you want to verify that try changing the case of the query. QueryParser output will be in the lowercase due to LowerCaseFilter on tokenstream. Some sample code for custom analyzer can be checked here. This will give you a peek inside an Analyzer."
109,A,"How to get whole index in ZEND Lucene? Hi I am looking for a way to get the whole index if my query is nothing. My lucene is a picture of my database without some unwanted content like expired annonces... So I would like to use only lucene to get annonces and for that I need a way to get the whole index. Any ideas? thanks! I tend to use a field which is named after the index such as: $oDoc->addField( Zend_Search_Lucene_Field::keyword( 'index' 'availability' ) ); Then the term query will return all fields. It's not pretty but it works fine.  This is not the answer but something wich works for me: Using an indexKey like is_indexed always true. I am adding ""is_indexed:1"" to my query and it works... If you have something else let me know!"
110,A,"Solr Sunspot non-indexed field Solr (via Lucene) supports different ways to indicate the way a field is indexed in a document: indexed tokenized stored... I'm looking for a way to have fields that are stored in Solr but are not indexed. Is there a way to achieve that in Sunspot? Sunspot's configuration DSL supports an option of :stored => true for many of its default types. For the example of the stored string it would be much simpler than my first example: searchable do string :name :stored => true end This generates a field name of name_ss corresponding to the following dynamicField already present in Sunspot's standard schema: <dynamicField name=""*_ss"" stored=""true"" type=""string"" multiValued=""false"" indexed=""true""/> You can also create your own custom field or dynamicField in your schema.xml to be stored but not indexed and then use the Sunspot 1.2 :as option to specify a corresponding field name. For example a more verbose version of the above. In your schema: <dynamicField name=""*_stored_string"" type=""string"" indexed=""false"" stored=""true"" /> And in your model: searchable do string :name :as => 'name_stored_string' end  You can try : http://localhost:8983/solr/admin/luke?numTerms=0 And read with xpath or regex those fields with schema attribute value: <str name=""I"">Indexed</str> <str name=""T"">Tokenized</str> <str name=""S"">Stored</str> You will get something like: <lst name=""field""> <str name=""type"">stringGeneralType</str> <str name=""schema"">--SM---------</str> </lst> That will tell me if a field is stored/indexed or not. But is not a way to tell Solr about the field storage attributes. Or am I missing something here?"
111,A,"Configuring Compass with Annotated Hibernate I'm using Hibernate for a Java-based Web Application and want to add full-text search via Compass. Compass is supposed to support that but fails to provide any useful Getting Started guide. I could figure out that I have to annotate my Entities with @Searchable and the various @SearchableXXX variations and accessing Compass in my service code via HibernateHelper.getCompass(sessionFactory). I end up with a HibernateException saying ""Compass event listeners not configured please check the reference documentation and the application's hibernate.cfg.xml"". The reference documentation again hints and hibernate.cfg.xml while I configure Hibernate with Spring's AnnotationSessionFactoryBean. For that case the documetantation mentions: ""If Hibernate Annotations or Hibernate EntityManager (JPA) are used just dropping Compass jar file to the classpath will enable it (make sure you don't have Hibernate Search in the classpath as it uses the same event class name)."" That doesn't work for me. Any ideas whats I'm missing or a good resource for getting started? There are two modes for Compass to integrate with Hibernate. The embedded mode basically means that you need to drop the Compass jar into the application (and possibly need to configure event listeners depending on the Hibernate version) then you need to at least at a property configuration in your Hibernate cfg file that configures Compass index location. Here is a link link for more information. The other option is configuring Compass to integrate with Hibernate externally by creating a CompassGps and a Hibernate device (initialized with Hibernate SessionFactory). The Hibernate device will automatically register the relevant listeners with Hibernate. More information can be found link here. I had that exact link in my question already and it doesn't look like it changed much since I posted the question. Didn't really help. A full example of the Hibernate-with-Annoations setup would be useful.  I'm wondering why you chose Compass to go Hibernate. We looked at Compass and Hibernate-Search and we chose the latter as it has excellent integration. You can query the test index in exactly the same way you do an SQL database with HQL or Critera. If you were using iBatis or JDBC then Compass would of course be the better fit. Hibernate search is a better fit with JTA. Is it me or is Compass suffering from a distinct lack of activity? That's what we ended up with anyway. Therefore marking as the accepted answer...  The best resource to review would be to check the petclinic example provided with the compass distribution (with dependencies). If by default the listener is not configured then you will have to set the EventListener."
112,A,"How to get frequently occuring phrases with Lucene I would like to get some frequently occurring phrases with Lucene. I am getting some information from txt files and am losing a lot of context for not having information for phrases eg. ""information retrieval"" is indexed as two separate words. What is the way to get the phrases like this? I can not find anything useful on internet all the advises links hints especially examples are appreciated! EDIT: I store my documents just by title and content  Document doc = new Document(); doc.add(new Field(""name"" f.getName() Field.Store.YES Field.Index.NOT_ANALYZED)); doc.add(new Field(""text"" fReader Field.TermVector.WITH_POSITIONS_OFFSETS)); Because for what I am doing the most important is the content of the file. Titles are too often not descriptive at all (eg i have many pdf academic papers whose titles are codes and numbers) I desperately need to index top occuring phrases from text contents just now i see how much this simple ""bag of words"" approach is not efficient. Well the problem of losing the context for phrases can be solved by using PhraseQuery. An index by default contains positional information of terms as long as you did not create pure Boolean fields by indexing with the omitTermFreqAndPositions option. PhraseQuery uses this information to locate documents where terms are within a certain distance of one another. For example suppose a field contained the phrase “the quick brown fox jumped over the lazy dog”. Without knowing the exact phrase you can still find this document by searching for documents with fields having quick and fox near each other. Sure a plain TermQuery would do the trick to locate this document knowing either of those words but in this case we only want documents that have phrases where the words are either exactly side by side (quick fox) or have one word in between (quick [irrelevant] fox). The maximum allowable positional distance between terms to be considered a match is called slop. Distance is the number of positional moves of terms to reconstruct the phrase in order. Check out Lucene's JavaDoc for PhraseQuery See this example code which demonstrates how to work with various Query Objects: You can also try to combine various query types with the help of the BooleanQuery class. And regarding the frequency of phrases I suppose Lucene's scoring considers the frequency of the terms occurring in the documents.  Is it possible for you to post any code that you have written? Basically a lot depends on the way you create your fields and store documents in lucene. Lets consider a case where I have got two fields: ID and Comments; and in my ID field I allow values like this 'finding nemo' i.e. strings with space. Whereas 'Comments' is a free flow text field i.e. I allow anything and everything which my keyboard allows and what lucene can understand. Now in real life scenario it does not make sense to make my ID:'finding nemo' as two different searchable string. Whereas I want to index everything in Comments. So what I will do is I will create a document (org.apache.lucene.document.Document) object to take care of this... Something like this Document doc = new Document(); doc.add(new Field(""comments""""Finding nemo was a very tough job for a clown fish ..."" Field.Store.YES Field.Index.ANALYZED)); doc.add(new Field(""id"" ""finding nemo"" Field.Store.YES Field.Index.NOT_ANALYZED)); So essentially I have created two fields: comments: Where I have preferred to analyze it by using Field.Index.ANALYZED id: Where I directed lucene to store it but do not analyze it Field.Index.NOT_ANALYZED This is how you customize lucene for Default Tokenizer and analyser. Otherwise you can write your own Tokenizer and analyzers. Link(s) http://darksleep.com/lucene/ Hope this will help you... :) Thank you for reply Favonius! I have edited my postso you can see how i Index docs. If I understand what you are saying using only the information from title will not be appropriate for my case..? :( @Julia: Actually my answer is partially correct. I have misunderstood the n-grams problems as a simple indexing problem :o . Although considering only the 'id' ('title' in your case) might not be appropriate... which I think you have already recognized...  Julia It seems what you are looking for is n-grams specifically Bigrams (also called collocations). Here's a chapter about finding collocations (PDF) from Manning and Schutze's Foundations of Statistical Natural Language Processing. In order to do this with Lucene I suggest using Solr with ShingleFilterFactory. Please see this discussion for details. Yes exactly what i need is ngrams.... I was hoping i will not have to go too much into NLP :/ ..but can I ask you please before I go into this book chapter if i use tools you recommended me(and if i manage anyways) ngrams are found during the search time not during the index time? Can I obtain as the end result one index with indexed all together terms and frequent ngrams? Because I am doing some concept matching with ontology and it would be the best solution to have it that way(if possible ofcourse) Thanx! @Julia: I think you can apply the ShingleFilterFactory during indexing. And maybe you can use Luke (http://wiki.apache.org/solr/LukeRequestHandler) for viewing the results. Hope you now have enough to get you going. +1 for correctly recognizing the problem... :)"
113,A,"Solr: Boosting-down phonetic variations? I'm trying to search by two fields each having its own boost factor and include phonetic variations but results with these variations should always be ranked lower in the results. The problem is currently that results with phonetic variation in the field with the higher boost are preferred over results with exact match in the field with lower boost. In schema.xml I have a field named ""text"" containing two other searchable fields (""title"" and ""description"" implemented via copyField) each with its boost factor (defined in a dismax SearchHandler in solrconfig.xml). This field has a solr.PhoneticFilterFactory filter with DoubleMetaphone on both ""index"" and ""query"" analyzers. As I understand this phonetic variations of each word are added to the query and to the index. My question is how can I tell solr to give a separate boost factor (e.g. 0.3) to the phonetic variations? One possible solution is to create two more fields: Suppose your original fields are named ""title"" and ""description"". Create ""title_phonetic"" and ""description_phonetic"" copy_fields and only add the phonetic variations to these fields. Next use the dismax parser to give different boosts to these fields. this seems to work thanks!"
114,A,"Sort by date in Solr/Lucene performance problems We have set up an Solr index containing 36 million documents (~1K-2K each) and we try to query a maximum of 100 documents matching a single simple keyword. This works pretty fast as we had hoped for. However if we now add ""&sort=createDate+desc"" to the query (thus asking for the top 100 'new' documents matching the query) it runs for a long very long time and finally results in an OutOfMemoryException. From what I've understood from the manual this is caused by the fact that Lucene needs to load all the distinct values for this field (createDate) into memory (the FieldCache afaik) before it can execute the query. As the createDate field contains date and time the number of distinct values is pretty large. Also important to mention is that we frequently update the index. Perhaps someone can provide some insights and directions on how we can tune Lucene / Solr or change our approach in such a way that query times become acceptable? Your input will be much appreciated! Thanks. The problem is Lucene stores numbers as Strings. There are some utilities which split the date into YYYY MM DD and put them in different fields. That gives much better results. Newer version of Lucene (2.9 onwards) support Numeric fields and the performance improvements are significant (couple of orders of magnitude IIRC.) Check this article about the Numeric quries. Thanks for your input Sashikant! Indeed upgrading to Solr 1.4 (which implements Lucene 2.9) made a great difference. The main advantage of it for us is that it maintains the FieldCache per segment and does not need to reload it after a commit for segments that haven't changed.  You can sort the results by index order instead. The sort specification for descending by document number is: new SortField(null SortField.DOC true) You should also partition the index directories by the date field. All matching documents are examined by Lucene when collecting the top N results. The partitioning will split the examined set. You don't need to examine the older partitions if you have N results in the newest partition.  Try converting you Date type data into String type (such as milliseconds)."
115,A,Index strategy for tagged documents where tags can change often In addition to text content my documents have tags which can be searched too. The problem now is that the tags change quite often and every time a tag gets added or removed I have to call UpdateDocument which is quite slow when done for hundreds of documents. Are there any well performing strategies for storing tags that change often and need to be searched with Lucene? I have been thinking about keeping the tags in separate documents to keep them smaller but I can't figure out how to quickly search for tags AND content. Store [tag UID] pairs in a relational database. Every time a tag is added or updated it is added and updated in this table in the database. When performing a Lucene search that includes both tag data (stored in a database) and content (indexed in Lucene) you will need to merge the results together. One way you can do this is to: Make a database query to pull up all the UID's for the tag in question Translate all the UID's to Lucene doc ID's and set a bit in a BitSet for every matching Lucene doc ID Create a Filter that wraps the BitSet and pass that filter in to your search. We implemented this approach in our system and it works well. You might need to put a cache in front of the database for performance reasons though. The particulars of step (3) will vary depending on which version of Lucene you're using. Thanks! Putting a cache in front of the tags in the database should work. +1 for this approach.
116,A,"Is it possible to obtain real time search results sorted by frequently updating field with Lucene 3.0 in Java Consider following assumptions: I have Java 5.0 Web Application for which I'm considering to use Lucene 3.0 for full-text searching There will be more than 1000K Lucene documents each with 100 words (average) New documents must be searchable just after they are created (real time search) Lucene documents have frequently updating integer field named quality Where to find code examples (simple but as complete as possible) of near real time search of Lucene 3.0? Is it possible to obtain query results sorted by one of document fields (quality) which may be updated frequently (for already indexed document)? Such updating of document field will have to trigger Lucene index rebuilding? What is performance of such rebuilding? How to done it efficiently - I need some examples / documentation of complete solution. If however index rebuilding is not necessarily needed in this case - how to sort search results efficiently? There may be queries returning lots of documents (>50K) so I consider it unefficient to obtain them unsorted from Lucene and then sort them by quality field and finally divide sorted list to pages for pagination. Is Lucene 3.0 my best choice within Java or should I consider some other frameworks/solutions? Maybe full text search provided by SQL Server itself (I'm using PostgreSQL 8.3)? Near Real Time Search is available in Lucene since 2.9. Lucid Imagination has an article about this capability (before 2.9 release). The basic idea is you can now get an IndexReader from IndexWriter. If you refresh this IndexReader at regular interval you get most up to the date changes from the IndexWriter. Update: I haven't seen any code but here is the broad idea. All the nw document will be written to an IndexWriter preferably created with RAMDirectory which will will not be closed frequently. (To persist this in-memory index you may have to flush it to disk ocassionally.) You will have some indexes on the disk on which individual IndexReaders will be created. A MultiReader and a Searcher can be created on top of these Readers. One of the Reader will be from the in-memory index. At regular interval (say a few seconds) you will remove current Reader from the MultiReader get the new Reader from IndexWriter and construct the MultiReader/Searcher with new set of Readers. According to the article from Lucid Imagination (linked above) they have tried writing 50 documents per second without heavy slowdown. Thanks for your update. It gives me overview of complexity of using near real-time searching in Lucene itself. It is as skaffman said: ""The Lucene API is capable of everything you're asking but it won't be easy. It's a fairly low-level API and making it do complicated things is quite an exercise in itself"". Right now I'm looking into Compass as it promises to do the dirty job for me ;-). The real-time capabilites were added in Lucene 2.9. If Compass has previous versions of Lucene you probably won't see the real time goodies. Where do I find code examples for that? How and when exacly I must refesh IndexReader? How long will it take (performance)? Can I perform searches while IndexReader is refreshing?  The Lucene API is capable of everything you're asking but it won't be easy. It's a fairly low-level API and making it do complicated things is quite an exercise in itself. I can highly recommend Compass which is a search/indexing framework built on top of Lucene. As well as a much friendlier API it provides functionality such as object/XML/JSON mapping to Lucene indexes as well as fully transactional behaviour. It should have no trouble with your requirements such as realtime sorting of transactionally-updated documents. Compass 2.2.0 is built upon Lucene 2.4.1 but a Lucene 3.0-based version is in the works. It's sufficiently abstracted from the Lucene API that the transition should be seamless though. Where do I find simplest possible example of adding some objects (resources/documents etc.) to Compass and then searching then with specified sort order? I tried it myself based on documentation (it was not very helpful) and one of examples from Compass distribution but I failed. I don't know how to start and where to learn from... The Compass forum is pretty good I've received good help there in the past. Compass seems interesting so I will give it a try."
117,A,"Best practices for seaching for alternate forms of a word with Lucene I have a site which is searchable using Lucene. I've noticed from logs that users sometimes don't find what they're looking for because they enter a singular term but only the plural version of that term is used on the site. I would like the search to find uses of other forms of a word as well. This is a problem that I'm sure has been solved many times over so what are the best practices for this? Please note: this site only has English content. Some approaches I've thought of: Look up the word in some kind of thesaurus file to determine alternate forms of a given word. Some examples: Searches for ""car"" also add ""cars"" to the query. Searches for ""carry"" also add ""carries"" and ""carried"" to the query. Searches for ""small"" also add ""smaller"" and ""smallest"" to the query. Searches for ""can"" also add ""can't"" ""cannot"" ""cans"" and ""canned"" to the query. And it should work in reverse (i.e. search for ""carries"" should add ""carry"" and ""carried""). Drawbacks: Doesn't work for many new technical words unless the dictionary/thesaurus is updated frequently. I'm not sure about the performance of searching the thesaurus file. Generate the alternate forms algorithmically based on some heuristics. Some examples: If the word ends in ""s"" or ""es"" or ""ed"" or ""er"" or ""est"" drop the suffix If the word ends in ""ies"" or ""ied"" or ""ier"" or ""iest"" convert to ""y"" If the word ends in ""y"" convert to ""ies"" ""ied"" ""ier"" and ""iest"" Try adding ""s"" ""es"" ""er"" and ""est"" to the word. Drawbacks: Generates lots of non-words for most inputs. Feels like a hack. Looks like something you'd find on TheDailyWTF.com. :) Something much more sophisticated? I'm thinking of doing some kind of combination of the first two approaches but I'm not sure where to find a thesaurus file (or what it's called as ""thesaurus"" isn't quite right but neither is ""dictionary""). Stemming is a pretty standard way to address this issue. I've found that the Porter stemmer is way to aggressive for standard keyword search. It ends up conflating words together that have different meanings. Try the KStemmer algorithm.  Word stemming works OK for English however for languages where word stemming is nearly impossible (like mine) option #1 is viable. I know of at least one such implementation for my language (Icelandic) for Lucene that seems to work very well.  Consider including the PorterStemFilter in your analysis pipeline. Be sure to perform the same analysis on queries that is used when building the index. I've also used the Lancaster stemming algorithm with good results. Using the PorterStemFilter as a guide it is easy to integrate with Lucene.  If you're working in a specialised field (I did this with horticulture) or with a language that does't play nicely with normal stemming methods you could use the query logging to create a manual stemming table. Just create a word -> stem mapping for all the mismatches you can think of / people are searching for then when indexing or searching replace any word that occurs in the table with the appropriate stem. Thanks to query caching this is a pretty cheap solution.  Some of those look like pretty neat ideas. Personally I would just add some tags to the query (query transformation) to make it fuzzy or you can use the builtin FuzzyQuery which uses Levenshtein edit distances which would help for mispellings. Using fuzzy search 'query tags' Levenshtein is also used. Consider a search for 'car'. If you change the query to 'car~' it will find 'car' and 'cars' and so on. There are other transformations to the query that should handle almost everything you need."
118,A,"Lucene ""Or Queries"" I am new in Lucene I am trying to make a search something like this  content=""some thext"" and (id =""A"" or id=""B"" or id=""c"") I am really lost with that could you help me Thank you. BooleanQuery mainQuery = new BooleanQuery(); TermQuery contentFilter = new TermQuery(new Term(""content"" ""some text"")); mainQuery.add(contentFilter BooleanClause.Occur.MUST); BooleanQuery idFilter = new BooleanQuery(); idFilter.setMinimumNumberShouldMatch(1); idFilter.add(new TermQuery(new Term(""id"" A)) BooleanClause.Occur.SHOULD); idFilter.add(new TermQuery(new Term(""id"" B)) BooleanClause.Occur.SHOULD); idFilter.add(new TermQuery(new Term(""id"" C)) BooleanClause.Occur.SHOULD); mainQuery.Add(idFilter BooleanClause.Occur.MUST); Looks like `moduleFilter` should be changed to `idFilter`. Anyway +1 for answer. An explanation I found useful (from the Occur javadocs): For a BooleanQuery with no Occur.MUST clauses one or more Occur.SHOULD clauses must match a document for the BooleanQuery to match.  I believe the ""Grouping"" section in the Query Parser Syntax documentation provides the answer: (jakarta OR apache) AND website I suspect you should make your operators (and or) upper case. As well I don't think you can use the equals operator (use a colon instead). content:""some thext"" AND (id:""A"" OR id:""B"" OR id:""c"") Thank you... it was so successful your help"
119,A,"What is the proper dependency entry in pom.xml to use the Snowball analyzer with Lucene 2.4.0? I'm trying to swap in the SnowballAnalyzer for StandardAnalyzer on my Maven 2 project. I'm currently using  <dependency> <groupId>org.apache.lucene</groupId> <artifactId>lucene-contrib</artifactId> <version>2.4.0</version> <scope>compile</scope> </dependency> but I keep getting the following error: Missing: ---------- 1) org.apache.lucene:lucene-contrib:jar:2.4.0 I swear that says ""porn.xml"". From the repos it looks like you should be including this one: <dependency> <groupId>org.apache.lucene</groupId> <artifactId>lucene-snowball</artifactId> <version>2.4.0</version> </dependency> At least from what I can see from http://repo1.maven.org/maven2/ that looks like the proper dependency ?"
120,A,"get cosine similarity between two documents in lucene i have built an index in Lucene. I want without specifying a query just to get a score (cosine similarity or another distance?) between two documents in the index. For example i am getting from previously opened IndexReader ir the documents with ids 2 and 4. Document d1 = ir.document(2); Document d2 = ir.document(4); How can i get the cosine similarity between these two documents? Thank you When indexing there's an option to store term frequency vectors. During runtime look up the term frequency vectors for both documents using IndexReader.getTermFreqVector() and look up document frequency data for each term using IndexReader.docFreq(). That will give you all the components necessary to calculate the cosine similarity between the two docs. An easier way might be to submit doc A as a query (adding all words to the query as OR terms boosting each by term frequency) and look for doc B in the result set. Yes ok for the first i use the termfreqvector to get what i want but i wanted to check how much faster would it be the to get similarity from lucene. For the second part of your answer i checked in the javadoc that there is not an obvious way to get similarity score. Ok i can look for doc B in the result set but the only i can get is its position in the TopDocs not the exact similarity score between these two document vectors that i want.  you can find better solution @ http://darakpanand.wordpress.com/2013/06/01/document-comparison-by-cosine-methodology-using-lucene/#more-53 . following are the steps java code which builds term vector from content with the help of Lucene(check:http://lucene.apache.org/core/). By using commons-math.jar library cosine calculation between two documents is done. Try writing something more. Don't put only links.  As Julia points out Sujit Pal's example is very useful but the Lucene 4 API has substantial changes. Here is a version rewritten for Lucene 4. import java.io.IOException; import java.util.*; import org.apache.commons.math3.linear.*; import org.apache.lucene.analysis.Analyzer; import org.apache.lucene.analysis.core.SimpleAnalyzer; import org.apache.lucene.document.*; import org.apache.lucene.document.Field.Store; import org.apache.lucene.index.*; import org.apache.lucene.store.*; import org.apache.lucene.util.*; public class CosineDocumentSimilarity { public static final String CONTENT = ""Content""; private final Set<String> terms = new HashSet<>(); private final RealVector v1; private final RealVector v2; CosineDocumentSimilarity(String s1 String s2) throws IOException { Directory directory = createIndex(s1 s2); IndexReader reader = DirectoryReader.open(directory); Map<String Integer> f1 = getTermFrequencies(reader 0); Map<String Integer> f2 = getTermFrequencies(reader 1); reader.close(); v1 = toRealVector(f1); v2 = toRealVector(f2); } Directory createIndex(String s1 String s2) throws IOException { Directory directory = new RAMDirectory(); Analyzer analyzer = new SimpleAnalyzer(Version.LUCENE_CURRENT); IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_CURRENT analyzer); IndexWriter writer = new IndexWriter(directory iwc); addDocument(writer s1); addDocument(writer s2); writer.close(); return directory; } /* Indexed tokenized stored. */ public static final FieldType TYPE_STORED = new FieldType(); static { TYPE_STORED.setIndexed(true); TYPE_STORED.setTokenized(true); TYPE_STORED.setStored(true); TYPE_STORED.setStoreTermVectors(true); TYPE_STORED.setStoreTermVectorPositions(true); TYPE_STORED.freeze(); } void addDocument(IndexWriter writer String content) throws IOException { Document doc = new Document(); Field field = new Field(CONTENT content TYPE_STORED); doc.add(field); writer.addDocument(doc); } double getCosineSimilarity() { return (v1.dotProduct(v2)) / (v1.getNorm() * v2.getNorm()); } public static double getCosineSimilarity(String s1 String s2) throws IOException { return new CosineDocumentSimilarity(s1 s2).getCosineSimilarity(); } Map<String Integer> getTermFrequencies(IndexReader reader int docId) throws IOException { Terms vector = reader.getTermVector(docId CONTENT); TermsEnum termsEnum = null; termsEnum = vector.iterator(termsEnum); Map<String Integer> frequencies = new HashMap<>(); BytesRef text = null; while ((text = termsEnum.next()) != null) { String term = text.utf8ToString(); int freq = (int) termsEnum.totalTermFreq(); frequencies.put(term freq); terms.add(term); } return frequencies; } RealVector toRealVector(Map<String Integer> map) { RealVector vector = new ArrayRealVector(terms.size()); int i = 0; for (String term : terms) { int value = map.containsKey(term) ? map.get(term) : 0; vector.setEntry(i++ value); } return (RealVector) vector.mapDivide(vector.getL1Norm()); } } Has VecTextField been taken from [this](http://stackoverflow.com/questions/11945728/how-to-use-termvector-lucene-4-0) question? @o_nix - yes you are right. Thanks. Now fixed yes i see  I have been check this . Sujit Pal's result not true No his results may be right - we don't know - he simply has not given enough information to repeat his experiment. @tiendv how did you get Sujit Pal's documents? He does not provide a link to their contents on his web page? He just lists their titles? If you just used the document titles you will get a big difference because those document titles are very different. Because he said that : Document#0: Mitral valve surgery - minimally invasive (31825) Document#1: Mitral valve surgery - open (31835) Document#2: Laryngectomy (31706) Document#3: Shaken baby syndrome (30 then he said cosim(01)=0.9672563304581603 cosim(02)=0.7496880467405691 cosim(03)=0.23968406019250035 so i thought that true but i'm wrong I'm test this with Sujit Pal example : Document#0: Mitral valve surgery - minimally invasive (31825) Document#1: Mitral valve surgery - open (31835) Document#2: Laryngectomy (31706) but it got difference reuslt ! can you explain why Thanks  It's a very good solution by Mark Butler however the calculations of the tf/idf weights are WRONG! Term-Frequency (tf): how much this term appeared in this document (NOT all documents as in the code with termsEnum.totalTermFreq()). Document Frequency (df): the total number of documents that this term appeared in. Inverse Document Frequency: idf = log(N/df) where N is the total number of documents. Tf/idf weight = tf * idf for a given term and a given document. I was hoping for an efficient calculation using Lucene! I'm unable to find a an efficient calculation for the correct if/idf weights. EDIT: I made this code to calculate the weights as tf/idf weights and not as pure term-frequency. It works pretty well but I wonder if there's a more efficient way. import java.io.IOException; import java.util.HashMap; import java.util.HashSet; import java.util.Map; import java.util.Set; import org.apache.commons.math3.linear.ArrayRealVector; import org.apache.commons.math3.linear.RealVector; import org.apache.lucene.analysis.Analyzer; import org.apache.lucene.analysis.core.SimpleAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.document.FieldType; import org.apache.lucene.index.DirectoryReader; import org.apache.lucene.index.DocsEnum; import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.index.IndexWriterConfig; import org.apache.lucene.index.Term; import org.apache.lucene.index.Terms; import org.apache.lucene.index.TermsEnum; import org.apache.lucene.search.DocIdSetIterator; import org.apache.lucene.store.Directory; import org.apache.lucene.store.RAMDirectory; import org.apache.lucene.util.BytesRef; import org.apache.lucene.util.Version; public class CosineSimeTest { public static void main(String[] args) { try { CosineSimeTest cosSim = new CosineSimeTest( ""This is good"" ""This is good"" ); System.out.println( cosSim.getCosineSimilarity() ); } catch (IOException e) { e.printStackTrace(); } } public static final String CONTENT = ""Content""; public static final int N = 2;//Total number of documents private final Set<String> terms = new HashSet<>(); private final RealVector v1; private final RealVector v2; CosineSimeTest(String s1 String s2) throws IOException { Directory directory = createIndex(s1 s2); IndexReader reader = DirectoryReader.open(directory); Map<String Double> f1 = getWieghts(reader 0); Map<String Double> f2 = getWieghts(reader 1); reader.close(); v1 = toRealVector(f1); System.out.println( ""V1: "" +v1 ); v2 = toRealVector(f2); System.out.println( ""V2: "" +v2 ); } Directory createIndex(String s1 String s2) throws IOException { Directory directory = new RAMDirectory(); Analyzer analyzer = new SimpleAnalyzer(Version.LUCENE_CURRENT); IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_CURRENT analyzer); IndexWriter writer = new IndexWriter(directory iwc); addDocument(writer s1); addDocument(writer s2); writer.close(); return directory; } /* Indexed tokenized stored. */ public static final FieldType TYPE_STORED = new FieldType(); static { TYPE_STORED.setIndexed(true); TYPE_STORED.setTokenized(true); TYPE_STORED.setStored(true); TYPE_STORED.setStoreTermVectors(true); TYPE_STORED.setStoreTermVectorPositions(true); TYPE_STORED.freeze(); } void addDocument(IndexWriter writer String content) throws IOException { Document doc = new Document(); Field field = new Field(CONTENT content TYPE_STORED); doc.add(field); writer.addDocument(doc); } double getCosineSimilarity() { double dotProduct = v1.dotProduct(v2); System.out.println( ""Dot: "" + dotProduct); System.out.println( ""V1_norm: "" + v1.getNorm() + "" V2_norm: "" + v2.getNorm() ); double normalization = (v1.getNorm() * v2.getNorm()); System.out.println( ""Norm: "" + normalization); return dotProduct / normalization; } Map<String Double> getWieghts(IndexReader reader int docId) throws IOException { Terms vector = reader.getTermVector(docId CONTENT); Map<String Integer> docFrequencies = new HashMap<>(); Map<String Integer> termFrequencies = new HashMap<>(); Map<String Double> tf_Idf_Weights = new HashMap<>(); TermsEnum termsEnum = null; DocsEnum docsEnum = null; termsEnum = vector.iterator(termsEnum); BytesRef text = null; while ((text = termsEnum.next()) != null) { String term = text.utf8ToString(); int docFreq = termsEnum.docFreq(); docFrequencies.put(term reader.docFreq( new Term( CONTENT term ) )); docsEnum = termsEnum.docs(null null); while (docsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) { termFrequencies.put(term docsEnum.freq()); } terms.add(term); } for ( String term : docFrequencies.keySet() ) { int tf = termFrequencies.get(term); int df = docFrequencies.get(term); double idf = ( 1 + Math.log(N) - Math.log(df) ); double w = tf * idf; tf_Idf_Weights.put(term w); //System.out.printf(""Term: %s - tf: %d df: %d idf: %f w: %f\n"" term tf df idf w); } System.out.println( ""Printing docFrequencies:"" ); printMap(docFrequencies); System.out.println( ""Printing termFrequencies:"" ); printMap(termFrequencies); System.out.println( ""Printing if/idf weights:"" ); printMapDouble(tf_Idf_Weights); return tf_Idf_Weights; } RealVector toRealVector(Map<String Double> map) { RealVector vector = new ArrayRealVector(terms.size()); int i = 0; double value = 0; for (String term : terms) { if ( map.containsKey(term) ) { value = map.get(term); } else { value = 0; } vector.setEntry(i++ value); } return vector; } public static void printMap(Map<String Integer> map) { for ( String key : map.keySet() ) { System.out.println( ""Term: "" + key + "" value: "" + map.get(key) ); } } public static void printMapDouble(Map<String Double> map) { for ( String key : map.keySet() ) { System.out.println( ""Term: "" + key + "" value: "" + map.get(key) ); } } } Thanks for your feedback but as I understand it you do not need to calculate TF-IDF to calculate cosine similarity. You could calculate a similarity metric using TF-IDF if you want but that was not the aim of the code above. Specifically I am using the algorithm above to test how well some automatic extraction code works against some human generated answers on a per document basis. TF-IDF would not help in that case which is why I did not use it. Also I am happy to work with you optimising your code and I can see a few basic things you could do but it would be better if you posted it under a new question as this one did not mention TF-IDF? You could always cite this question? See http://stackoverflow.com/questions/6255835/cosine-similarity-and-tf-idf  I know question has been answered but for the people who might come here in future nice example of the solution can be found here: http://sujitpal.blogspot.ch/2011/10/computing-document-similarity-using.html  If you don't need to store documents to Lucene and just want to calculate similarity between two docs here's the faster code (Scala from my blog http://chepurnoy.org/blog/2014/03/faster-cosine-similarity-between-two-dicuments-with-scala-and-lucene/ ) def extractTerms(content: String): Map[String Int] = { val analyzer = new StopAnalyzer(Version.LUCENE_46) val ts = new EnglishMinimalStemFilter(analyzer.tokenStream(""c"" content)) val charTermAttribute = ts.addAttribute(classOf[CharTermAttribute]) val m = scala.collection.mutable.Map[String Int]() ts.reset() while (ts.incrementToken()) { val term = charTermAttribute.toString val newCount = m.get(term).map(_ + 1).getOrElse(1) m += term -> newCount } m.toMap } def similarity(t1: Map[String Int] t2: Map[String Int]): Double = { //word t1 freq t2 freq val m = scala.collection.mutable.HashMap[String (Int Int)]() val sum1 = t1.foldLeft(0d) {case (sum (word freq)) => m += word ->(freq 0) sum + freq } val sum2 = t2.foldLeft(0d) {case (sum (word freq)) => m.get(word) match { case Some((freq1 _)) => m += word ->(freq1 freq) case None => m += word ->(0 freq) } sum + freq } val (p1 p2 p3) = m.foldLeft((0d 0d 0d)) {case ((s1 s2 s3) e) => val fs = e._2 val f1 = fs._1 / sum1 val f2 = fs._2 / sum2 (s1 + f1 * f2 s2 + f1 * f1 s3 + f2 * f2) } val cos = p1 / (Math.sqrt(p2) * Math.sqrt(p3)) cos } So to calculate similarity between text1 and text2 just call similarity(extractTerms(text1) extractTerms(text2))"
121,A,"How to acces a file under WEB-INF from a Java web application Do you have any idea how to access files in WEB-INF/index folder from my application? I'm using OpenCMS for my application and I want to open a Lucene search index (with the help of Lucene IndexReader class) located at WEB-INF/index folder. Lucene jar is stored in WEB-INF/lib folder. If you want to lookup a file on the RFS (real file system) under the WEB-INF folder you can get the path via: String filepath = pageContext.getServletContext().getRealPath(""/"") + ""WEB-INF"" + java.io.File.separator + ""index""; and then use the common java file methods to read it.  What do you intend to do with those index files? OpenCms uses Lucene for its search engine. It is possible to use Lucene from within OpenCms for other purposes without any hassle if the index files are in the ""correct"" folder."
122,A,"Nested prohibit/require operators in Lucene search queries I am using Lucene for Java and need to figure out what the engine does when I execute some obscure queries. Take the following query: +(foo -bar) If I use QueryParser to parse the input I get a BooleanQuery object that looks like this: org.apache.lucene.search.BooleanQuery: org.apache.lucene.search.BooleanClause(required=true prohibited=false): org.apache.lucene.search.BooleanQuery: org.apache.lucene.search.BooleanClause(required=false prohibited=false): org.apache.lucene.search.TermQuery: foo org.apache.lucene.search.BooleanClause(required=false prohibited=true): org.apache.lucene.search.TermQuery: bar What does Lucene look for? Is it documents that MUST contain 'foo' but CANNOT contain 'bar'? What if I search for: -(foo +bar) Are those documents that CANNOT contain 'foo' and CANNOT contain 'bar'? Or perhaps ones that CANNOT contain 'foo' but MUST contain 'bar'? If it helps any here is what I used to peek into the QueryParser results: QueryParser parser = new QueryParser(""contents"" new StandardAnalyzer()); Query query = parser.parse(text); debug(query 0); public static void debug(Object o int depth) { for(int i=0; i<depth; i++) System.out.print(""\t""); System.out.print(o.getClass().getName()); if(o instanceof BooleanQuery) { System.out.println("":""); for(BooleanClause clause : ((BooleanQuery)o).getClauses()) { debug(clause depth + 1); } } else if(o instanceof BooleanClause) { BooleanClause clause = (BooleanClause)o; System.out.println(""(required="" + clause.isRequired() + "" prohibited="" + clause.isProhibited() + ""):""); debug(clause.getQuery() depth + 1); } else if(o instanceof TermQuery) { TermQuery term = (TermQuery)o; System.out.println("": "" + term.getTerm().text()); } else { throw new IllegalArgumentException(""Unknown object type""); } } By default Lucene assumes an OR relationship between terms so the first query is equivalent to +(foo OR -bar) which will match documents which contain (in the default field) ""foo"" or don't contain ""bar"" In the second query the ""+"" makes ""bar"" required which makes the optional ""foo"" redundant so it can be reduced to ""-bar"" which matches all documents that don't contain ""bar"" Thanks that makes sense!  Luke http://www.getopt.org/luke/ is very useful to understand what queries do This tool didn't help me with the question I posted but it provides a lot of valuable diagnostics information that I know will be useful in the future. Thanks for this I forwarded it to my team."
123,A,What's the most efficient way to retrieve all matching documents from a query in Lucene unsorted? I am looking to perform a query for the purposes of maintaining internal integrity; for example removing all traces of a particular field/value from the index. Therefore it's important that I find all matching documents (not just the top n docs) but the order they are returned in is irrelevant. According to the docs it looks like I need to use the Searcher.Search( Query Collector ) method but there's no built in Collector class that does what I need. Should I derive my own Collector for this purpose? What do I need to keep in mind when doing that? @Rodrigo Could you be a bit more specific? I read over that thread but it appears to have to do with permission checks. Can you explain how that is relevant to my question? Keep this in mind if you want to return ALL results: http://forums.alfresco.com/en/viewtopic.php?t=13381 Turns out this was a lot easier than I expected. I just used the example implementation at http://lucene.apache.org/java/2_9_0/api/core/org/apache/lucene/search/Collector.html and recorded the doc numbers passed to the Collect() method in a List exposing this as a public Docs property. I then simply iterate this property passing the number back to the Searcher to get the proper Document: var searcher = new IndexSearcher( reader ); var collector = new IntegralCollector(); // my custom Collector searcher.Search( query collector ); var result = new Document[ collector.Docs.Count ]; for ( int i = 0; i < collector.Docs.Count; i++ ) result[ i ] = searcher.Doc( collector.Docs[ i ] ); searcher.Close(); // this is probably not needed reader.Close(); So far it seems to be working fine in preliminary tests. Update: Here's the code for IntegralCollector: internal class IntegralCollector: Lucene.Net.Search.Collector { private int _docBase; private List<int> _docs = new List<int>(); public List<int> Docs { get { return _docs; } } public override bool AcceptsDocsOutOfOrder() { return true; } public override void Collect( int doc ) { _docs.Add( _docBase + doc ); } public override void SetNextReader( Lucene.Net.Index.IndexReader reader int docBase ) { _docBase = docBase; } public override void SetScorer( Lucene.Net.Search.Scorer scorer ) { } } Just remember to use the docBase value passed to your `SetNextReader` since the document id passed to `Collect` is specific to the current reader (from `SetNextReader`). You'll need to use (docBase+doc) when calculating ids to use with the topmost reader the one used when opening your `IndexSearcher`. Also don't forget about `IndexWriter.DeleteDocuments(Query)` if you want to remove matching documents. @Simon - Thanks I figured that out myself when I started getting wonky results. Also deletion was just an example I actually do need to retrieve the documents in my real application.  No need to write a hit collector if you're just looking to get all the Document objects in the index. Just loop from 0 to maxDoc() and call reader.document() on each doc id making sure to skip documents that are already deleted: for (int i=0; i<reader.maxDoc(); i++) { if (reader.isDeleted(i)) continue; results[i] = reader.document(i); } Thanks but I am interested in actually performing a query not just getting all the documents in the index.
124,A,"Apache Solr - are the documents itself stored internally apart from the index? I have been trying to research how solr works when documents like doc or pdf are submitted to it. I want to know if I submit pdfs to solr does it end up storing the pdf file also along with the index generated after parsing the pdf file? Thanks -Keshav Solr (Lucene) doesn't ""end up store the PDF file"" itself. However it can store the text contents of the PDF extracted from the PDF using a text-extractor such as Tika (if indeed the field is marked as stored in the schema). If you wish to store the PDF file in its entirety you will need to convert the PDF into (for example) Base64 representation and persist the base64 string as a ""Stored"" field. So when you access the doc you convert back from Base64 to PDF. Or save the pdf to the filesystem and save its location in a ""Stored"" field. Mikos Thanks for your response! You mentioned that text contents of the PDF can be stored. But is the text storage necessary for the index search to work? Not necessary for the searching. But if you need highlighting (snippets) then you will need to store."
125,A,"lucene.net combine multiple filters and no search terms How can I do a Filter across multiple fields in Lucene.Net? On one field I simply do: TermQuery tool = new TermQuery(new Term(""Tool"" ""Nail"")); Filter f = new QueryFilter(tool); If I now wanted to add a nail length to the filter how can I do that? Also I want the user to be a able to do a search with no search term (i.e. by just choosing a category) how can I do that? I think you're asking two questions... Question 1: Adding an additional filter Remember QueryFilter accepts any query (not just TermQuery). Therefore you can create a BooleanQuery of the criteria you wish to filter on. TermQuery toolQuery = new TermQuery(new Term(""Tool"" ""Nail"")); TermQuery nailLengthQuery = new TermQuery(new Term(""NailLength"" ""3 inches"")); BooleanQuery filterQuery = new BooleanQuery(); filterQuery.add(toolQuery BooleanClause.Occur.MUST); filterQuery.add(nailLengthQuery BooleanClause.Occur.MUST); Filter f = new QueryFilter(filterQuery); Question 2: Searching without a search term If the user provides no search term you can search using a MatchAllDocsQuery query."
126,A,pyLucene Installation I am on Ubuntu 10.04 Python 2.6.5 & having some trouble installing pyLucene. Here's what I have done so far installed these packages - sudo apt-get install ant sudo apt-get install sun-java6-jdk sudo update-java-alternatives -s java-1.5.0-sun sudo apt-get install gcc sudo apt-get install g++ sudo apt-get install gcj sudo apt-get install python-dev After that got the source code of pyLucene from http://www.apache.org/dyn/closer.cgi/lucene/pylucene/. I got version pylucene-3.0.1-1. Untared it. Since JCC needs to be built first; went to JCC dir & then typed python setup.py install. got an error about some setuptools patch. it told me to do this - sudo patch -d /usr/lib/python2.6/dist-packages -Nup0 < /home/code/python/lucene/pylucene-3.0.1-1/jcc/jcc/patches/patch.43.0.6c11 and I did. but I got this on the terminal - patching file setuptools/extension.py patching file setuptools/command/build_ext.py Hunk #1 FAILED at 85. Hunk #2 succeeded at 177 (offset 7 lines). Hunk #3 succeeded at 259 (offset 7 lines). 1 out of 3 hunks FAILED -- saving rejects to file setuptools/command/build_ext.py.rej Now my build fails - i.e. when I do sudo python setup.py build I get - ... error: command 'gcc' failed with exit status 1 What version do you need? There's a PPA with 2.9.2-1 here: https://launchpad.net/~owenmorris/+archive/ppa as mentioned in the question - @inception has used the version of pylucene 3.0.1 @MovieYoda: The version he's tried might not be the version he *needs*.  I think the problem may be that the patch is for setuptools 0.6-11 and you have an earlier version of setuptools (prob 0.6-10) installed from your distro's packages. I have had a similar problem that was resolved by removing the version of setuptools installed by the distro and installing your own downloaded from http://pypi.python.org/pypi/setuptools I have had no problems yet but there is a very small chance that this will interfere with some other programs on your system. I think this may be wrong! I think with the newer setuptools you don't need the patch at all. or none of the above and install JCC direct from your package manager Actually you do need the patch even with version 0.6.11c
127,A,"Lucene.Net Underscores causing token split I've scripted a MsSqlServer databases tablesviews and stored procedures into a directory structure that I am then indexing with Lucene.net. Most of my table view and procedure names contain underscores. I use the StandardAnalyzer. If I query for a table named tIr_InvoiceBtnWtn01 for example I recieve hits back for tIr and for InvoiceBtnWtn01 rather than for just tIr_InvoiceBtnWtn01. I think the issue is the tokenizer is splitting on _ (underscore) since it is punctuation. Is there a (simple) way to remove underscores from the punctuation list or is there another analyzer that I should be using for sql and programming languages? I'm trying the StopAnalyzer and the WhitespaceAnalyzer now. So for it looks like the WhitespaceAnalyzer may be the way to go. Yes the StandardAnalyzer splits on underscore. WhitespaceAnalyzer does not. Note that you can use a PerFieldAnalyzerWrapper to use different analyzers for each field - you might want to keep some of the standard analyzer's functionality for everything except table/column name. WhitespaceAnalyzer only does whitespace splitting though. It won't lowercase your tokens for example. So you might want to make your own analyzer which combines WhitespaceTokenizer and LowercaseFilter or look into LowercaseTokenizer. EDIT: Simple custom analyzer (in C# but you can translate it to Java pretty easily): // Chains together standard tokenizer standard filter and lowercase filter class MyAnalyzer : Analyzer { public override TokenStream TokenStream(string fieldName System.IO.TextReader reader) { StandardTokenizer baseTokenizer = new StandardTokenizer(Lucene.Net.Util.Version.LUCENE_29 reader); StandardFilter standardFilter = new StandardFilter(baseTokenizer); LowerCaseFilter lcFilter = new LowerCaseFilter(standardFilter); return lcFilter; } } I think I will want lowercase tokens. I'm assuming there is not a ""non-source compile"" way of combining Whitespace and lowercase. What is the difference between using LowercaseFilter and lowercaseTokenizer? @automatic: I have added an example of how to chain filters/tokenizers together. In general Solr is intended to be the ""easy to use"" version of lucene so yes there is not a way of doing this which doesn't require writing code if you use only lucene. But that is quasi-intentional. @automatic: Also LowercaseTokenizer is LowercaseFilter + LetterTokenizer; looking at LetterTokenizer though it will split at underscore too. So that is not what you want. Sorry."
128,A,"Lucene insensitive whitespace analyzer? I am using lucene for searching and with tags i use the whitespace analyzer. It looks like its stored properly. With standard analyzer my 'C#' search will yield results for C C++. Every analyzer i tried (i havent tried all) does this except for whitespace analyzer. This is fine except if i search c# i get no results (i'm using a lowercase C instead of uppercase). This is annoying if i search a title such as ""Lucene insensitive whitespace analyzer?"" when it happens to be ""Lucene Insensitive Whitespace analyzer?"". (Note the first 3 words start with upper and the last doesnt compared to my search with one upper and all lower). How do i make an insensitive whitespace analyzer? Note: WhitespaceAnalyzer is sealed. Try using LowerCaseFilter in conjunction with WhitespaceTokenizer: http://lucene.apache.org/java/3_0_0/api/core/org/apache/lucene/analysis/LowerCaseFilter.html http://lucene.apache.org/java/3_0_0/api/core/org/apache/lucene/analysis/WhitespaceTokenizer.html I tried to write an analyzer. Do you happen to know a tutorial? i'll try this once i find a good tutorial. I have plenty of time to fix this problem See http://lucene.apache.org/java/3_0_0/api/core/org/apache/lucene/analysis/package-summary.html for more info on how to write your own anayzer I couldnt figured it out. I end up making all text uppercase in my add function and did it again in my search function. It works perfectly now links not working anymore..."
129,A,Why doesnt' Lucene remove docs? I'm using Lucene.NET 2.3.1 with a MultiSearcher. For testing purposes I'm indexing a database with about 10000 rows. I have two indexes and I select randomly which to insert each row in. This works correctly but as Lucene doesn't have update capabilities I have to test if the row exists (I have an Id field) and then delete it. I have a List and a List and each is created with this code: IndexModifier mod = new IndexModifier(path new StandardAnalyzer() create); m_Modifiers.Add(mod); m_Readers.Add(IndexReader.Open(path)); m_Searchers.Add(new IndexSearcher(path)); Now the delete code: Hits results = m_Searcher.Search(new TermQuery(t)); for (int i = 0; i < results.Length(); i++) { DocId = results .Id(i); Index = m_Searcher.SubSearcher(DocId); DocId = m_Searcher.SubDoc(DocId); m_Modifiers[Index].DeleteDocument(DocId); } The search is correct and I'm getting results when the row exists. SubSearcher returns always 0 or 1 if Index is 0 SubDoc returns the same ID passed and if it's 1 then it returns around the number passed minus 5000 times the number of times I have indexed the DB. It seems as if it wasn't deleting anything. Each time I index the database I optimize and close the indices and Luke says it has no pending deletions. What could be the problem? I am not sure what's the end goal of this activity so pardon if the following solution doesn't meet your requirements. First if you want to delete documents you can use IndexReader which you have already created. IndexModifier is not required. Second you need not find the subsearcher ID and document ID in that subsearcher. You can as well use the top-level MultiReader. I would write the equivalent java code as follows. IndexReader[] readers = new IndexReader[size]; // Initialize readers MultiReader multiReader = new MultiReader(readers); IndexSearcher searcher = new IndexSearcher(multiReader); Hits results = searcher.search(new TermQuery(t)); for (int i = 0; i < results.length(); i++) { int docID = results.id(i); multiReader.deleteDocument(docID); } multiReader.commit(); // Check if this throws an exception. multiReader.close(); searcher.close(); Thanks at last I solved it myself. My problem was really a field which should be UN_TOKENIZED and was TOKENIZED. If I remember correctly in order to delete with the multiReader I had to close my Modifier and as most deletes will be done just as part of a document update I prefer not to close it and therefore delete it directly with the Modifier. Am I right or is there a better solution?
130,A,is it possible to use lucene(on linux) and asp.net(on windows) at the same time? I want to start a new project I need performance as well as a neat and robust GUI about the performance I have around 2 millions documents which I like to index'em by the help of lucene installed on linux due to its performance and security. and about GUI I'd like to have flexible and professional look website and since I'm experienced with .net I'd like to retrieve the lucene's result and show it in my own way. I've heard about some RESTful services available inside the lucene but I don't have any clue according to that and how to connect these two together. how can I connect asp.net to lucene? regards. Lucene has been ported to .Net. http://incubator.apache.org/lucene.net/ We use it for our website to index various things and these indexes are in the millions too. If you're wanting to use Linux in the belief that it's more efficient then it's not a matter of the choice of OS but of accessing a remote Lucene system. I'm not getting into a 'is linux more efficient than windows' discussion. This isn't the place and it will only start a flame war. Yes we Lucene.net it installed on a Windows system. yeah I know lucene.net exists but I want to run lucene under linux as a safe and fast engine for my indexing and distill its result into asp.net and create whatever I want  say a hierarchical search engine or an enterprise document manager etc... I do believe in power of asp.net to build such those things but I don't believe in MS windows performance. can you explain more about your situation ? have you installed lucene.net on MS windows server? and does it have the performance just like linux? just for your record right now we are using index server and asp.net  One option: Install Solr on Linux. Solr is a nice search server built based on Lucene that supports REST-like XML and JSON APIs. ASP can parse JSON and from there you can build your own front end in ASP.net. No. You just install Solr. It will bring Lucene along. you mean I have to install lucene then solr on top of that?
131,A,"custom code browser using Lucene/swish-e I am working on a C++ project which has a huge code base and multiple components. I want to create a rich code browser for it which will give a visual studio like experience. I am thinking of an Adobe AIR app with Lucene or Swish-e as the backend text indexer. I have never used either of the two. If you have used one or both of them can you please tell me if Lucene/Swish-e is suitable for this kind of application? Can I configure it to make it language aware for C++ Flex etc? Are there existing open source solutions for this problem I can take a look at? Lucene in Action 2nd ed. contains a chapter on a product called krugle which uses Lucene for searching source code. I have never used swish-e so I can't compare them but that book talks about the benefits and detriments of using Lucene. You can also try their search here for what that's worth. To summarize their chapter: Pros: Lucene scales very big and is very fast. Essentially once you have the index it's all gravy. Cons: The standard analyzers throw out words like ""if"" and ""for"" and punctuation marks like ""{"" which are important in source code searching. Also code doesn't tokenize easily (e.g. ""GetDatabaseInstance"" should tokenize to three words not one). So they had to write a lot of their own analyzers. Thanks. Will look into it."
132,A,"Encrypted Compressible Cross Platform File system in a file We wish to make a desktop application that searches a locally packaged text database that will be a few GB in size. We are thinking of using lucene. So basically the user will search for a few words and the local lucene database will give back a result. However we want to prevent the user from taking a full text dump of the lucene index as the text database is valuable and proprietary. A web application is not the solution here as the Customer would like for this desktop application to work in areas where the internet is not available. How do we encrypt lucene's database so that only the client application can access lucene's index and a prying user can't take a full text dump of the index? One way of doing this we thought was if the lucene index could be stored on an encrypted file system within a file (something like truecrypt). So the desktop application would ""mount"" the file containing the lucene indexes. And this needs to be cross platform (Linux Windows)...We would be using Qt or Java to write the desktop application. Is there an easier/better way to do this? [This is for a client. Yes yes conceptually this is bad thing :-) but this is how they want it. Basically the point is that only the Desktop application should be able to access the lucene index and no one else. Someone pointed that this is essentially DRM. Yeah it resembles DRM] I didn't mean ""this is evil just like DRM"". I meant ""this is broken just like DRM"". True-crypt sounds like a solid plan to me. You can mount volumes and encrypt them in all sorts of crazy overkill ways and access them just as any other file. No it isn't entirely secure but it should work well enough.  Why not building an index that contains only the data that user can access and ship that index with the desktop app?  One-way hash function. You don't store the plaintext you store hashes. When you want to search for a term you push the term through the function and then search for the hash. If there's a match in the database return thumbs up. Are you willing to entertain false positives in order to save space? Bloom filter. If the one way hash function is known to all the database can be copied and will be still useful to anybody else. Please read question again :-) Since the user can search the database xe is making a partial copy with every search (a.k.a. ""search results""); thus the database can be copied in parts anyway and then stitching them together: ""Hey Joe open this app and search for everything that begins with 'a' store results; search for everything that begins with 'b' store results; bring me the results when you've run out of letters."" The data is accessible thus protection of the actual database is irrelevant. So add a dash of salt and bake a copy for each customer. If my original comment had not been edited allusions to this being a forty-year-old problem whose solution is demonstrated in your /etc/passwd file would have made this evident.  How do we encrypt lucene's database so that only the client application can access lucene's index and a prying user can't take a full text dump of the index? You don't. The user would have the key and the encrypted data so they could access everything. You can bury the key in an obfuscated file but that only adds a slight delay. It certainly will not keep out prying users. You need to rethink. Actually the key would come from one of those standard hardware based software protection USB dongles. @Sidharth: regardless of *where* the key is stored the user has access to both the encrypted data and the key so you've already lost. It is for this very reason that copy-protection and DRM don't work. (For a reason why a USB dongle would be useless see [this](http://usbsnoop.sourceforge.net/)) @Sidharth: a HW dongle might help (when it works - it mostly has a serial-to-USB kludge inside oozing all kinds of happiness and joy) but then the whole application must be cryptographically secure otherwise a cracker will just find the place where the HW code is checked and either patch it with ""allow always"" or dump the decryption key - and you're back to square 1. We want to make it tough but not impossible. So yes you could do a memory dump and get the decryption key out (whether you are using a software key or a Hardware based dongle). Is there a way to make this tough for hackers ... we don't want to make it impossible. The point is the client is distributing a database of thousands of valuable text files. He wants customers to be able to search them offline. Searching is fine but indiscriminate ripping of data is not.  Technically there is little you can do. Lucene is written in Java and Java code can always be decompiled or run in a debugger to get the key which you need to store somewhere (probably in the license key which you sell the user). Your only option is the law (or the contract with the user). The text data is copyrighted so you can sue the user if they use it in any way that is outside the scope of the license agreement. Or you can write your own text indexing system. Or buy a commercial one which meets your needs. [EDIT] If you want to use an encrypted index just implement your own FSDirectory. Check the source for SimpleFSDirectory for an example. In that case see my edit for a solution. Yes people could do that... but we want to make it tough for people not necessarily impossible. I suggest to make it more simple for people to use your application so they have no incentive to get the data out. Spend a lot of money to make your legitimate users happy instead of making them mad with copy protection (which will only drive them to the next cracker for a version of your app without copy protection and without any hazzle). Its not our application... its for a client. They want it like this. We can't get into a moral debate with them. Too often questions like this get mixed up with philosophy :-). We want to know how to do it and not if its good for world hunger :-)  The problem here is that you're trying to both provide the user with data and deny it from em at the same time. This is basically the DRM problem under a different name - the attacker (user) is in full control of the application's environment (hardware and OS). No security is possible in such situation only obfuscation and illusion of security. While you can make it harder for the user to get to the unencrypted data you can never prevent it - because that would mean breaking your app. Probably the closest thing is to provide a sealed hardware box but IMHO that would make it unusable. Note that making a half-assed illusion of security might be sufficient from a legal standpoint (e.g. DMCA's anti-circumvention clauses) - but that's outside SO's scope. ( for the DRM version see e.g. this question: http://stackoverflow.com/questions/1790190/is-it-possible-to-protect-from-downloading-a-video-from-a-site/ )"
133,A,"Tokenizing Twitter Posts in Lucene My question in a nutshell: Does anyone know of a TwitterAnalyzer or TwitterTokenizer for Lucene? More detailed version: I want to index a number of tweets in Lucene and keep the terms like @user or #hashtag intact. StandardTokenizer does not work because it discards the punctuation (but it does other useful stuff like keeping domain names email addresses or recognizing acronyms). How can I have an analyzer which does everything StandardTokenizer does but does not touch terms like @user and #hashtag? My current solution is to preprocess the tweet text before feeding it into the analyzer and replace the characters by other alphanumeric strings. For example String newText = newText.replaceAll(""#"" ""hashtag""); newText = newText.replaceAll(""@"" ""addresstag""); Unfortunately this method breaks legitimate email addresses but I can live with that. Does that approach make sense? Thanks in advance! Amaç what does your final solution looks like? if you need a solution for solr this could help: https://issues.apache.org/jira/browse/SOLR-2059 and something like ""# => ALPHA"" ""@ => ALPHA"" A tutorial on twitter specific tokenizer which is a modified version of ark-tweet-nlp API can be found at http://preciselyconcise.com/apis_and_installations/tweet_pos_tagger.php This API is capable of identifying emoticons hashtagsinterjections etc present in a tweet  The StandardTokenizer and StandardAnalyzer basically pass your tokens through a StandardFilter (which removes all kinds of characters from your standard tokens like 's at ends of words) followed by a Lowercase filter (to lowercase your words) and finally by a StopFilter. That last one removes insignificant words like ""as"" ""in"" ""for"" etc. What you could easily do to get started is implement your own analyzer that performs the same as the StandardAnalyzer but uses a WhitespaceTokenizer as the first item that processes the input stream. For more details one the inner workings of the analyzers you can have a look over here Thanks. I already tried implementing my own Analyzer by using WhitespaceTokenizer instead of StandardTokenizer. But that leaves host names email addresses and some other stuff unrecognized and tokenized erroneously. I would like to process a stream with my custom TwitterTokenizer (which handles @s and #s does nothing else) then feed the resulting stream into a StandardTokenizer and go on from there. However as far as I understand an Analyzer can have only one Tokenizer at the beginning of the chain. Another approach could be to use PerFieldAnalyzerWrapper and make a second pass through the content to explicitely look for hash tags and user references and put them in a separate field of your document (e.g. 'tags' and 'replies'). The analyzers for those field then only return tokens for occurences of #tag and @user respectively. Yeah that makes sense. Thanks!  There's a Twitter-specific tokenizer here: https://github.com/brendano/ark-tweet-nlp/blob/master/src/cmu/arktweetnlp/Twokenize.java  The Twitter API can be told to return all Tweets Bios etc with the ""entities"" (hashtags userIds urls etc) already parsed out of the content into collections. https://dev.twitter.com/docs/entities So aren't you just looking for a way to re-do something that the folks at Twitter have already done for you?"
134,A,"PHP Zend Framework: Internal Server Error 500 on localhost when trying to run ""Lucene.php"" on browser I am running windows 7 and I have a fresh install of Zend framework. On a new php file called 'search.php' I am trying to build a search application using Zend_Search_Lucene with the first line being: <?php require_once ""C:...\Zend\Search\Lucene.php"";?> What I see on the browser when I run this code is internal server error 500..I then discovered that I get this error whenever I try to run some of the files in Zend library by itself and this is what caused the error I mentioned..i.e ERROR 500 on localhost/Zend/Search/Lucene.php localhost/Zend.../blabla.php.. Some of the files however did not display this 500 server error when run on the browser. ie: localhost/Zend/ProgressBar.php shows an empty page which is fine since I assume there are not any 'echo'es in the code. This is actually what I expected when I ran lucene.php on the browser... Can somebody experienced let me know how this can happen? Why do I get internal server error instead of exception? How do I check if my search application that uses ""Lucene.php"" file runs properly regardless of this internal 500 server error? thanks. I have finally solved the problem :) After looking at the error traces the internal server error is because the code <?php require_once ""C:...\Zend\Search\Lucene.php"";?> tries to access a particular code inside ""Lucene.php"" that contains a relative path to the Zend library folder =require_once('Zend\Search\Document\...'); and the server does not know the path to the file. What needed to be fixed was actually my php.ini file on the include_path i added ;C:\php5\pear;C:\Server\www\.....\ZendFramework\library..Now it shows empty page instead of internal server error. +1 @Arend: The error reporting function is really useful! thanks  Zend's code relies on their autoloader. You get an error since you don't initialize it then in Zend_Search_Lucene it tries to instanciate a nonexistent class. This should do the trick: require_once 'Zend/Loader.php'; Zend_Loader::registerAutoload(); self::$zendLoaded = true; Loader by itself just defines the class does nothing else. No wonder it works (shows an empty white page with http 200). Enable display_errors and try again. I added your code to my php file and it still throws the same internal server error..However like I said Loader.php by itself runs fine on the browser (shows white empty page)  Try to turn on error reporting: on the fly ini_set('display_errors' 1); error_reporting(E_ALL); in php.ini (probably different for php and cli) error_reporting = E_ALL display_errors = 1 For more information see: http://php.net/manual/en/errorfunc.configuration.php http://php.net/manual/en/function.error-reporting.php  As I cannot comment answers I re-use Maerlyn's code : require_once 'Zend/Loader.php'; Zend_Loader::registerAutoload(); self::$zendLoaded = true; cannot work as self is not defined her. According to the ZF documentation if your path is set correctly these 2 lines should be enough."
135,A,"Search term suggestions This question has been asked in various ways before but I'm wondering if people who have experience with automatic search term suggestion could offer advice on the most useful and efficient approaches. Here's the scenario: I'm just starting on a website for a book that is a dictionary of terms (roughly 1000 entries with 300 word explanations on average) many of which are fairly obscure and it is likely that many visitors to the site would not know how to spell the words. The publisher wants to make full-text search available for every entry. So I'm hoping to implement a search engine with spelling correction. The main site will probably be done in a PHP framework (or possibly Django) with a MySQL database. Can anyone with experience in this area give advice on the following: With a set corpus of this nature should I be using something like Lucene or Sphinx for the search engine? As far as I can tell neither of these has a built-in suggestion function. So it seems I will need to integrate one or more of the following. What are the advantages / disadvantages of: Suggestion requests through Google's search API A phonetic comparison algorithm like metaphone() in PHP A spell checking system like Aspell A simpler spelling script such as Peter Norvig's A Levenshtein function I'm concerned about the specificity of my corpus and don't want Google to start suggesting things that have nothing to do with this book. I'm also not sure whether I should try to use both a metaphone comparison and a Levenshtein comparison or some other combination of techniques to capture both typos and attempts at phonetic spelling. Look at http://wiki.apache.org/lucene-java/SpellChecker  You might want to consider Apache Solr which is a web service encapsulation of Lucene and runs in a J2EE container like Tomcat. You'll get term suggestion spell check porting stemming and much more. It's really very nice. See here for a full listing of its features relating to queries. There are Django and PHP libraries for Solr. I wouldn't recommend using Google Suggest for such a specialised corpus anyway and with Solr you won't need it. Hope this helps. Thanks for some reason I hadn't come across Solr yet. Looks really useful. Also thanks for confirming my suspicions about Google Suggest. Tomcat is not a J2EE container it is just a Servlet container although Tomcat does support some of the J2EE specification. BTW for those just dipping their toes into this like me this is from a forum thread on installing Solr on a shared host (WebFaction): ""The bad news is that Solr is relatively memory hungry (Django plus Solr sent my memory usage up to 204 MB) and therefore unable to run reliably on pretty much all of the shared hosting options."" +1 for Solr. You will get plenty much all you are looking for and then some."
136,A,Enabling soundex/metaphone for non-English characters I've been studying soundex metaphone and other string search techniques the past few days and in my understanding both algorithms work well in handling non-English words transliterated to English. However the requirement that I have would be for such search to work in the original untransliterated languages accomodating alphabets such as German Norwegian and even Cyrilic alphabets. Are there any search algorithms capable of handling these alphabets completely? Or am I better off using third party full-text-search libraries such as Lucene? Consequently the question then becomes 'does Lucene handle non-English alphabets?' If your use-case is just textual search in non-English languages you may not need soundex. You do need Lucene with a proper Analyzer as ire_and_curses said. If you want to handle different writing variants of the same word you will need a phonetic matching algorithm. Can you say more about your use-case? I'm not an expert in this area but your requirements seem quite difficult to me. Soundex was specifically designed for English sounds as well as characters. I don't think it will perform well for non-English languages. See for example the responses to this related question. Double-Metaphone is an attempt to deal with much more complex variations than Soundex or Metaphone and was designed to handle irregularities in a range of languages. It might be sufficient for your needs. There is a list of library implementations on the linked page. Support for other languages in Lucene is based on the concept of Analyzers. Lucene comes with a set of analyzers for different languages (although I couldn't find the default list) but the quality may be quite variable. Looks like Lucene + analyzers is what I'm really looking for thanks. :) what about ChineseJapaneseArabic and Indian languages?  There are some good references on Wikipedia starting from the Soundex article. I don't know whether there are existing libraries designed to handle such a wide variety of languages. The references there all point towards algorithms that handle Anglicized spellings of european names. I haven't seen any that actually handles the special characters as-is -- unless I understand them wrongly. I thought that maybe some of the Soundex variants were designed to work better with other languages but they are kind of focused on English or anglicized spellings. It wouldn't be hard to write something LIKE Soundex for each of those languages but you'd probably need the help of a linguist if you're not a native speaker.
137,A,"Lucene RangeQuery doesn't filter appropriately I'm using RangeQuery to get all the documents which have amount between say 0 to 2. When i execute the query Lucene gives me documents which have amount greater than 2 also. What am I missing here? Here is my code: Term lowerTerm = new Term(""amount"" minAmount); Term upperTerm = new Term(""amount"" maxAmount); RangeQuery amountQuery = new RangeQuery(lowerTerm upperTerm true); finalQuery.Add(amountQuery BooleanClause.Occur.MUST); and here is what goes into my index: doc.Add(new Field(""amount"" amount.ToString() Field.Store.YES Field.Index.UN_TOKENIZED Field.TermVector.YES)); UPDATE: Like @basZero said in his comment starting with Lucene 2.9 you can add numeric fields to your documents. Just remember to use NumericRangeQuery instead of RangeQuery when you search. Original answer Lucene treats numbers as words so their order is alphabetic: 0 1 12 123 2 22 That means that for Lucene 12 is between 0 and 2. If you want to do a proper numerical range you need to index the numbers zero-padded then do a range search of [0000 TO 0002]. (The amount of padding you need depends on the expected range of values). If you have negative numbers just add another zero for non-negative numbers. (EDIT: WRONG WRONG WRONG. See update) If your numbers include a fraction part leave it as is and zero-pad the integer part only. Example: -00002.12 -00001 000000 000001 000003.1415 000022 UPDATE: Negative numbers are a bit tricky since -1 comes before -2 alphabetically. This article gives a complete explanation about dealing with negative numbers and numbers in general in Lucene. Basically you have to ""encode"" numbers into something that makes the order of the items make sense. could you please let me know how do i use rangequery for decimal numbers?thanks! thanks a lot!...i already got this working ... For decimals (I assume you mean decimals with a fractional component) you need to scale them up eg. by multiplying by a million and removing any remainder: 1.2 -> 1200000. The amount you multiply by depends on how many decimal places you need to be accurate to. ...and of course you still need to zero-pad them as itsadok says. I should have said 1.2 -> 0001200000 No need to scale up. Alphabetically 1.234 comes before 1.3. Hi isadok As you already know lat/long can be decimal/negative numbers. is below the correct way of indexing a city's lat and long positions in Lucene? string paddedLatitude = PadNumber(latitude); string paddedLongitude = PadNumber(longitude); doc.Add(new Field(""latitude"" paddedLatitude Field.Store.YES Field.Index.UN_TOKENIZED)); doc.Add(new Field(""longitude"" paddedLongitude Field.Store.YES Field.Index.UN_TOKENIZED)); example: paddedLongitude-->""0041.811846"" paddedLongitude-->""-087.820628"" Thanks. That looks about OK. Please post this as a question and I promise to answer. Use Lucene 2.9.x and you can add numbers to the index.  I created a PHP function that convert numerics to lucene/solr range searchables. 0.5 is converted to 10000000000.5 -0.5 is converted to 09999999999.5 function luceneNumeric($numeric) { $negative = $numeric < 0; $numeric = $negative ? 10000000000 + $numeric : $numeric; $parts = explode('.' str_replace('' '.' $numeric)); $lucene = $negative ? 0 : 1; $lucene .= str_pad($parts[0] 10 '0' STR_PAD_LEFT); $lucene .= isset($parts[1]) ? '.' . $parts[1] : ''; return $lucene; } It seems to work hope this helps someone!"
138,A,"Using Hibernate Search (Lucene) I Need to Be Able to Search a Code With or Without Dashes This is really the same as it would be for a social security #. If I have a code with this format: WHO-S-09-0003 I want to be able to do: query = qb.keyword().onFields(""key"").matching(""WHOS090003"").createQuery(); I tried using a WhitespaceAnalyzer. actually you can implement your own method like this one: private String specialCharacters(String keyword) { String [] specialChars = {""-""""!""""?""}; for(int i = 0; i < specialChars.length; i++ ) if(keyword.indexOf(specialChars[i]) > -1) keyword = keyword.replace(specialChars[i] ""\\""+specialChars[i]); return keyword; } as you know lucene has special chars so if you want escape special chars than you should insert before that char double backslashes...  Using StandardAnalyzer or WhitespaceAnalyzer both have the same problem. They will index 'WHO-S-09-0003' as is which means that when you do a search it will only work if you have hyphens in the search term. One solution to your problem would be to implement your own TokenFilter which detects the format of your codes and removes the hyphens during indexing. You can use AnayzerDef to build a chain of toekn filters and an overall custom analyzer. Of course you will have to use the same analyzer when searching but the Hibernate Search query DSL will take care of that."
139,A,"Lucene: Wildcards are missing from index i am building a search index that contains special names - containing ! and ? and & and + and ... I have to tread the following searches different: me & you me + you But whatever i do (did try with queryparser escaping before indexing escaped it manually tried different indexers...) - if i check the search index with Luke they do not show up (question marks and @-symbols and the like show up) The logic behind is that i am doing partial searches for a live suggestion (and the fields are not that large) so i split it up into ""m"" and ""me"" and ""+"" and ""y"" and ""yo"" and ""you"" and then index it (that way it is way faster than a wildcard query search (and the index size is not a big problem). So what i would need is to also have this special wildcard characters be inserted into the index. This is my code: using System; using System.Collections.Generic; using System.IO; using System.Linq; using System.Text; using Lucene.Net.Analysis; using Lucene.Net.Util; namespace AnalyzerSpike { public class CustomAnalyzer : Analyzer { public override TokenStream TokenStream(string fieldName TextReader reader) { return new ASCIIFoldingFilter(new LowerCaseFilter(new CustomCharTokenizer(reader))); } } public class CustomCharTokenizer : CharTokenizer { public CustomCharTokenizer(TextReader input) : base(input) { } public CustomCharTokenizer(AttributeSource source TextReader input) : base(source input) { } public CustomCharTokenizer(AttributeFactory factory TextReader input) : base(factory input) { } protected override bool IsTokenChar(char c) { return c != ' '; } } } The code to create the index: private void InitIndex(string path Analyzer analyzer) { var writer = new IndexWriter(path analyzer true); //some multiline textbox that contains one item per line: var all = new List<string>(txtAllAvailable.Text.Replace(""\r"""""").Split('\n')); foreach (var item in all) { writer.AddDocument(GetDocument(item)); } writer.Optimize(); writer.Close(); } private static Document GetDocument(string name) { var doc = new Document(); doc.Add(new Field( ""name"" DeNormalizeName(name) Field.Store.YES Field.Index.ANALYZED)); doc.Add(new Field( ""raw_name"" name Field.Store.YES Field.Index.NOT_ANALYZED)); return doc; } (Code is with Lucene.net in version 1.9.x (EDIT: sorry - was 2.9.x) but is compatible with Lucene from Java) Thx Are you sure you mean 1.9.*? You mention asciifoldingfilter which sounds like a 2.9 variant. I should add that the gist of what you've described sounds fine so I suspect there's an issue in code we're not seeing in your analyzer. Are you deriving it from another class and not overriding all the methods you need to for instance? sorry - you are totally right - its 2.9.x version - thx! I updated my question and included all the relevant code (denormalize code is just a custom method to add up all the spaces for faster search. Finally had the time to look into it again. And it was some stupid mistake in my denormalice method that did filter out single character parts (as it was in the beginning) and thus it did filter out the plus sign if surrounded by spaces :-/ Thx for your help though Moleski! private static string DeNormalizeName(string name) { string answer = string.Empty; var wordsOnly = Regex.Replace(name ""[^\\w0-9 ]+"" string.Empty); var filterText = (name != wordsOnly) ? name + "" "" + wordsOnly : name; foreach (var subName in filterText.Split(' ')) { if (subName.Length >= 1) { for (var j = 1; j <= subName.Length; j++) { answer += subName.Substring(0 j) + "" ""; } } } return answer.TrimEnd(); } I update my answer - but this is no code that is specific to Lucene - it is just for my special case (and it is just a spike - so don't expect perfect code). If you want plain search - then forget about my denormalization method. It is just used in my special case to speed up live suggestions as a wildcard search is quite expensive for small words - and i want to show suggestions beginning with the first letter or second. I'm learning how to do this myself  do you think you could post code for your denormalize method? I'm not clear what that does at that point. Thanks."
140,A,In Lucene using a Standard Analyzer I want to make fields with spaces and special characters searchable In Lucene using a Standard Analyzer I want to make fields with spaces and special characters(underscore!@#....) searchable. I set IndexField to NOT_ANALYZED_NO_NORMS and Field.Store.YES When I look at my index in LUKE the fields are as I expected a value such as: 'SKU Number' yet when I search for 'SKU' or 'SKU*' nothing comes up. What am I missing? Searching for 'SKU' won't work because you indexed with NOT_ANALYZED; 'SKU Number' is the entire indexed term. If you want words split by whitespace that's what ANALYZED is for. Now doing a prefix search 'SKU*' would work except by default the lucene QueryParser lowercases expanded terms. Set lowercaseExpandedTerms on the parser to False.  Field.Store.YES does not influence the search behaviour on this field. And I'd set IndexField to simply NOT_ANALYZED. Try to perform search in Luke on full field text 'SKU Number' using KeywordAnalyzer analyzer.
141,A,"lucene vs solr scoring Can some one explain (or quote a reference) to compare the scoring mechanism used by SOLR and LUCENE in simpler words. Is there any difference in them; I am not that good at solr/lucene but my finding showed as if they are different. P.S: i just tries a simple query like ""+Contents:risk"" and didn't use any filter other stuff. Lucene uses concepts from the Vector space model to compute the score of documents. In summary queries and documents can be seen as vectors. To compute the score of a document for a particular query Lucene calculates how near each document's vector are from the query's vector. The more a document is near the query in VSM the higher the score. You can have more details by looking at Lucene's Similarity class and Lucene's Scoring document. thanks for such a great short description...  The actual formula can be found in the Similarity javadocs. Here's a summary of the parameters involved and a brief description of what they mean. Solr uses Lucene under the hood and by default Solr uses the default Lucene similarity algorithm. great thanks for those references;"
142,A,"Faceting with Solr using ""string"" fields ""text"" fields and ""copy"" fields I have a problem with Solr and Faceting and wondering if anyone knows of the fix. I have a work around for it at the minute however i really want to work out why my query isn't working. Here is my Schema simplified to make it easier to follow: <fields> <field name=""uniqueid"" type=""string"" indexed=""true"" required=""true""/> <!-- Indexed and Stored Field ---> <field name=""recordtype"" type=""text"" indexed=""true"" stored=""true""/> <!-- Facet Version of fields --> <field name=""frecordtype"" type=""string"" indexed=""true"" stored=""false""/> </fields> <!-- Copy fields for facet searches --> <copyField source=""recordtype"" dest=""frecordtype""/> As you can see I have a case insensitive field called recordtype and it's copied to a case sensitive field frecordtype which does not tokenize the text. This is because solr returns the indexed value rather than the stored value in the faceting results. When i try the following query: http://localhost:8080 /solr /select ?version=2.2 &facet.field=%7b!ex%3dfrecordtype%7dfrecordtype &facet=on &fq=%7b!tag%3dfrecordtype%7dfrecordtype%3aLarge%20Record &f1=*%2cscore &rows=20 &start=0 &qt=standard &q=text%3a%25 I don't get any results however the facteting still shows there is 1 record. <result name=""response"" numFound=""0"" start=""0"" /> <lst name=""facet_counts""> <lst name=""facet_queries"" /> <lst name=""facet_fields""> <lst name=""frecordtype""> <int name=""Large Record"">1</int> <int name=""Small Record"">12</int> <int name=""Other"">1</int> </lst> </lst> <lst name=""facet_dates"" /> </lst> However if i change the fitler query (line 7 only) to be on the ""recordtype"" insted of frecordtype: http://localhost:8080 /solr /select ?version=2.2 &facet.field=%7b!ex%3dfrecordtype%7dfrecordtype &facet=on &fq=%7b!tag%3dfrecordtype%7drecordtype%3aLarge%20Record &f1=*%2cscore &rows=20 &start=0 &qt=standard &q=text%3a%25 I get the 1 result back that i want. <result name=""response"" numFound=""1"" start=""0"" /> <lst name=""facet_counts""> <lst name=""facet_queries"" /> <lst name=""facet_fields""> <lst name=""frecordtype""> <int name=""Large Record"">1</int> <int name=""Small Record"">12</int> <int name=""Other"">1</int> </lst> </lst> <lst name=""facet_dates"" /> </lst> So my question is is there something i need to do in order to get the first version of the query to return the results i want? Perhaps it's something to do with URL Encoding or something? Any hints from some solr guru's or otherwise would be very grateful. NOTE: This isn't necessary a faceting question as the faceting is actually working. It's more a query question in that I can't perform a query on a ""string"" field even though the case and spacing is exactly the same as the indexed version. EDIT: For more information on faceting you can check out these blog post's on it: http://www.craftyfella.com/2010/01/faceting-and-multifaceting-syntax-in.html http://wiki.apache.org/solr/SimpleFacetParameters#facet.limit Thanks Dave Arrhhh Sorted this out... You need quotes around values with spaces :) You need quotes around the values E.g. frecordtype:""Large Record"" works frecordtype:Large Record This will search for Large in the frecordtype which will bring back nothing.. then Record across the default field in solr."
143,A,"Querying Solr without specifying field names I'm new to using Solr and I must be missing something. I didn't touch much in the example schema yet and I imported some sample data. I also set up LocalSolr and that seems to be working well. My issue is just with querying Solr in general. I have a document where the name field is set to tom. I keep looking at the config files and I just can't figure out where I'm going awry. A bunch of fields are indexed and stored and I can see the values in the admin but I can't get querying to work properly. I've tried various queries (http://server.com/solr/select/?q=value) and here are the results: **Query:** ?q=tom **Result:** No results **Query:** q=\*:\* **Result:** 10 docs returned **Query:** ?q=*:tom **Result:** No results **Query:** ?q=name:tom **Result:** 1 result (the doc with name : tom) I want to get the first case (?q=tom) working. Any input on what might be going wrong and how I can correct it would be appreciated. Well despite of setting a default search field is quite usefull i don't understand why don't you just use the solr query syntax: ......./?q=name:tom or ......./?q=:&fq=name:tom ok miss read the title :-(  Going through the solr tutorial is definitely worth your time: http://lucene.apache.org/solr/tutorial.html My guess is that the ""name"" field is not indexed so you can't search on it. You'd need to change your schema to make it indexed. Also make sure that your XML actually lines up with the schema. So if you are adding a field named ""name"" in the xml but the schema doesn't know about it then Solr will just ignore that field (ie it won't be ""stored"" or ""indexed""). Good luck Field is definitely indexed. Also if it wasn't I couldn't do some of the queries I listed right? Something like name:tom wouldn't work if I understand the docs. yea Mauricio has it right that you need to specify the defaultSearchField in the solrconfig. Also if you are using DisMax (which would allow ?q=tom to cause a search in multiple fields at the same time) there is another setting called ""qf""  It seems that a DisMax parser is the right thing to use for this end. Related stackoverflow thread here. On second thought; using copyField to a consolidated field may be the simpler option.  Set <defaultSearchField> to name in your schema.xml The <defaultSearchField> Is used by Solr when parsing queries to identify which field name should be searched in queries where an explicit field name has not been used. You might also want to check out (e)dismax instead. Nice! This was exactly what I needed. I didn't see this option before and now having a bunch of copyfields pointing to a large ""text"" field makes sense. Thanks so much!  I just came across to a similar problem... Namely I have defined multiple fields (that did not exist in the schema.xml) to describe my documents and want to search/query on the multiple fields of the document not only one of them (like the ""name"" in the above mentioned example). In order to achieve this I have created a new field (""compoundfield"") where I then put/copyField my defined fields (just like the ""text"" field on the schema.xml document that comes with Solr distribution). This results in something like this: coumpoundfield definition: <field name=""compoundfield"" type=""text_general"" indexed=""true"" stored=""false"" multiValued=""true""/> defaultSearchField: <!-- field for the QueryParser to use when an explicit fieldname is absent --> <defaultSearchField>compoundfield</defaultSearchField> <!-- SolrQueryParser configuration: defaultOperator=""AND|OR"" --> <solrQueryParser defaultOperator=""OR""/> <!-- copyField commands copy one field to another at the time a document is added to the index. It's used either to index the same field differently or to add multiple fields to the same field for easier/faster searching. --> <!-- ADDED Fields --> <copyField source=""field1"" dest=""compoundfield""/> <copyField source=""field2"" dest=""compoundfield""/> <copyField source=""field3"" dest=""compoundfield""/> This works fine for me but I am not sure if this is the best way to make such a ""multiple field"" search... Cheers!"
144,A,"Why does a Lucene exact match only score 0.4? I have a document that contains a title and a description. One of the item description contains ""Shipping is free with Amazon Prime"" among other words. I do a search with lucene for ""Shipping is free with Amazon Prime"" on the fields title and description using perl Lucene my $analyzer = new Lucene::Analysis::SimpleAnalyzer(); my @fields = ('title' 'description'); my $parser = new Lucene::MultiFieldQueryParser(\@fields $analyzer); I get a score of 0.4 only. My guess is that I get 0 for title (no match) and 0.8 for description (exact match) for an average of 0.4. How can I do a match on the title and/or description that would give me a score of 0.8 or more in this case? First you need to look at some Lucene scoring theory. Next explain() explains how a query got its score. I believe that Plucene has explain as well. Third why does the score have to be 0.8 or more? Lucene scores are relative and are valid in the context of a specific query. Their main use is to order hits. Unless you need the score for another purpose and as long as the relative order stays the same I would not care about the absolute score. I was trying to keep the best matches only above some threshold"
145,A,solr search for documents where a field doesn't exist How do I search for those document in a SOLR index which do not contain a specified field? this has been asked before but I can't find it... -field:[* TO *] In SolrNet use a negated SolrHasValueQuery @KyleMaxwell of course doing it at index-time is better... if not I don't think there's a better way to do it at query-time. That's insanely expensive. There has to be a better way but I haven't looked yet. At minimum you should create an empty field marker (e.g. NULL) and index that. Note this only works for fields that are indexed. Doing this for non-indexed fields will returns all records.
146,A,Best way to reuse a Runnable I have a class that implements Runnable and am currently using an Executor as my thread pool to run tasks (indexing documents into Lucene). executor.execute(new LuceneDocIndexer(doc writer)); My issue is that my Runnable class creates many Lucene Field objects and I would rather reuse them then create new ones every call. What's the best way to reuse these objects (Field objects are not thread safe so I cannot simple make them static) - should I create my own ThreadFactory? I notice that after a while the program starts to degrade drastically and the only thing I can think of is it's GC overhead. I am currently trying to profile the project to be sure this is even an issue - but for now lets just assume it is. @stevendick - yes exactly why I mentioned it IN THE QUESTION. Making the flyweight pattern thread-safe is often difficult. I have indexed millions of documents using Lucene with simple code that does not reuse Field objects. It goes very smoothly. Lucene is probably not the problem. I have no prior experience in Lucene; but is it possible to use a pool of `Field` objects? Each `Field` object is different (say 6 per document) so using one generic object pool would not work. I also don't want to keep track of 6 pools of objects that would need to be the same size as my thread pool - so if I ever change the thread pool size I have to change the object pool size - doesn't sound very maintainable. The GC should be optimized for dealing with small allocations that go away quickly since it's such a common use case. I'm not familiar with Lucene but if Field objects are small enough and short-lived enough chances are they're allocated on the stack by the JIT and never even reach the GC. WHAT do you want to reuse ? The Field objects or the Runnables ? You need to profile so that you can identify the problem rather than guessing that it's the large amount of Field objects causing the problem. Otherwise you might do a whole lot of work and see no measurable improvement. In terms of your question have you considered if the flyweight pattern would fit? For now I have decided to just use a simple Producer->Consumer model. I pass a BlockingQueue to each indexer rather then a document to index and then have the main driver of the program add new documents to that queue. The Indexers then feed off that [bounded] queue and reuse the Field objects and share the thread-safe IndexWriter. I did find a place where I was possibly not calling HttpMethod.releaseConnection() so that could have caused my memory issues (uncertain).  A Runnable object is reusable. It is thread object which is not. Best way ? it is your way :-) I think it is more a lucene question than a runnable question. I think you have that backwards. @Gandalf: This poster is absolutely right. It's your problem that your implementation of `Runnable` is not thread-safe. If it were you could have definitely passed it twice ( or more ) to the executor.  Your question asks how to reuse a Runnable so I am going to ignore the other details adn simply answer that question. If you are using a ThreadPoolExecutor you can use the [ThreadPoolExecutor#afterExecute][1] method to return the Runnable object to a pool/queue of 'cached' Runnables. [1]: http://java.sun.com/javase/6/docs/api/java/util/concurrent/ThreadPoolExecutor.html#afterExecute(java.lang.Runnable java.lang.Throwable) It's awesome that you're the only one who actually answered the question and didn't start talking about the circumstances instead. +1!  You might want to do some more benchmarking to nail down what's causing your slowdowns. I'm willing to bet that your problem is not related to the creation of Field instances. Field doesn't have any finalizers and they're not designed to be pooled.
147,A,"java.lang.IncompatibleClassChangeError BM25BooleanQuery Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Implementing class at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632) at java.lang.ClassLoader.defineClass(ClassLoader.java:616) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141) at java.net.URLClassLoader.defineClass(URLClassLoader.java:283) at java.net.URLClassLoader.access$000(URLClassLoader.java:58) at java.net.URLClassLoader$1.run(URLClassLoader.java:197) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) at NVoting.<init>(NVoting.java:143) at Main.main(Main.java:8) on this line: booleanQuery.add(new BM25BooleanQuery(current_tags[i] ""tags"" new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_31)) BooleanClause.Occur.SHOULD); I'm using an implementation of BM25 Okapi retrieval system: http://nlp.uned.es/~jperezi/Lucene-BM25 Can you help me to fix the issue ? I'm using Lucene 3.1 I've also tried Lucene 2.9.4 with no luck. thanks This may be helpful http://stackoverflow.com/questions/1980452/what-causes-java-lang-incompatibleclasschangeerror It means you are trying to use two libraries which were not compiled together and have a breaking change between them i.e. they are incompatible. I'm not a Java programmer so take this answer with caution but here's the facts as I see them: The last update date of the BM25 implementation is January 2009 (or perhaps December 2009) Lucene 3.1 was released in March this year IncompatibleClassChangeError indicates an incompatible change in a dependent class So I think you'll either need to use the same version of Lucene as the BM25 implementation used when it was built or rebuild it from source (if that's a possibility). Good luck  Either you use different third-party libraries for compilation and execution or some of your third-party libraries are not compatible. The error may happen if you use one version of a library for compilation and another one (with slightly different API) for execution. Double check all your library versions (do they fit?) and you classpaths."
148,A,How to get list of all search keyword in Lucene? I need the list of all search keyword(term) i.e. indexed in lucene index. I googled for it. but i didn't get the solution. Is it possible to get the list of all search term? You probably need the function IndexReader.terms() that gives you an enumeration of all the terms. I highly recommend using the Luke interactive Lucene front-end for exploring Lucene indexes.
149,A,Different lucene search results using different search space size I have an application that uses lucene for searching. The search space are in the thousands. Searching against these thousands I get only a few results around 20 (which is ok and expected). However when I reduce my search space to just those 20 entries (i.e. I indexed only those 20 entries and disregard everything else...so that development would be easier) I get the same 20 results but in different order (and scoring). I tried disabling the norm factors via Field#setOmitNorms(true) but I still get different results? What could be causing the difference in the scoring? Thanks Scoring depends on all the documents in the index: In general the idea behind the Vector Space Model (VSM) is the more times a query term appears in a document relative to the number of times the term appears in all the documents in the collection the more relevant that document is to the query. Source: Apache Lucene - Scoring Sorry I'm not an expert either. Did you have a look at http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/Similarity.html ? I'm not sure I understand that. So if I searched for a person given the name 'Mark' against all the person in my search space I'll be getting 'Mark Anthony' 'Markos' and 'Mark'. But if I limit the search space to those 3 only (just limit the indexing to just those 3) I will get 'Mark' 'Mark Anthony' and 'Markos'. How come their sorting will change given the same relevant documents but different 'noise' documents?  Please see the scoring documentation in Lucene's Similarity API. My bet is on the difference in idf between the two cases (both numDocs and docFreq are different). In order to know for sure use the explain() function to debug the scores. Edit: A code fragment for getting explanations: TopDocs hits = searcher.search(query searchFilter max); ScoreDoc[] scoreDocs = hits.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) { String explanation = searcher.explain(query scoreDoc.doc).toString(); Log.debug(explanation); } Please see my edit for an example. I didn't had much time to go back at my problem but this suggestion seems to point to the right direction. Thanks. Pardon but where can I get the int (second parameter) of explain() ?
150,A,"java lucene custom analyzer and tokenizer creating problem in termvector offsets? i got a problem about the lucene termvector offsets that is when i analyzed a field with my custom analyzer it will give the invalid offsets for termvector but it is fine with standard analyzer here is my analyzer code public class AttachmentNameAnalyzer extends Analyzer { private boolean stemmTokens; private String name; public AttachmentNameAnalyzer(boolean stemmTokens String name) { super(); this.stemmTokens = stemmTokens; this.name = name; } @Override public TokenStream tokenStream(String fieldName Reader reader) { TokenStream stream = new AttachmentNameTokenizer(reader); if (stemmTokens) stream = new SnowballFilter(stream name); return stream; } @Override public TokenStream reusableTokenStream(String fieldName Reader reader) throws IOException { TokenStream stream = (TokenStream) getPreviousTokenStream(); if (stream == null) { stream = new AttachmentNameTokenizer(reader); if (stemmTokens) stream = new SnowballFilter(stream name); setPreviousTokenStream(stream); } else if (stream instanceof Tokenizer) { ( (Tokenizer) stream ).reset(reader); } return stream; } } whats wrong with this ""Help required"" Since this code does nothing related to term offsets you should post one that does. E.g. your AttachmentNameTokenizer? ok the tokenizer code is here i have added the results Looks totally innocent so far. More code exact examples of input+output (with broken offsets) required :) I'd go to lucene user mailing list with that though. i just found that there is some problem with snowball filter not sure what exactly the problem is still in search of solution !!! the problem it with the analyzer as i posted the code for analyzer earlier actually the token stream is need to be rest for every new entry of text that is to be tokenized.  public TokenStream reusableTokenStream(String fieldName Reader reader) throws IOException { TokenStream stream = (TokenStream) getPreviousTokenStream(); if (stream == null) { stream = new AttachmentNameTokenizer(reader); if (stemmTokens) stream = new SnowballFilter(stream name); setPreviousTokenStream(stream); // ---------------> problem was here } else if (stream instanceof Tokenizer) { ( (Tokenizer) stream ).reset(reader); } return stream; } every time when i sets the previous token stream the next coming text field the has to be separately tokenized it always starts with end offset of last token stream that make the term vector offset wrong for new stream it now it works fine like this ublic TokenStream reusableTokenStream(String fieldName Reader reader) throws IOException { TokenStream stream = (TokenStream) getPreviousTokenStream(); if (stream == null) { stream = new AttachmentNameTokenizer(reader); if (stemmTokens) stream = new SnowballFilter(stream name); } else if (stream instanceof Tokenizer) { ( (Tokenizer) stream ).reset(reader); } return stream; }  Which version of Lucene are you using ? I'm looking at the super class code for 3x branch and behavior changes with each version. You may want to check the code for public final boolean incrementToken() where offset is calculated. I also see this: /** * <p> * As of Lucene 3.1 the char based API ({@link #isTokenChar(char)} and * {@link #normalize(char)}) has been depreciated in favor of a Unicode 4.0 * compatible int based API to support codepoints instead of UTF-16 code * units. Subclasses of {@link CharTokenizer} must not override the char based * methods if a {@link Version} >= 3.1 is passed to the constructor. * <p> * <p> * NOTE: This method will be marked <i>abstract</i> in Lucene 4.0. * </p> */ btw you can rewrite the switch statement like @Override protected boolean isTokenChar(int c) { switch(c) { case '': case '.': case '-': case '_': case ' ': return false; default: return true; } } i am using version 3.0"
151,A,"How do I setup Lucene so that I can search ignoring whitespace characters? For example a list of part numbers includes: JRB-1000 JRB 1000 JRB1000 JRB100-0 -JRB1000 If a user searches on 'JRB1000' or 'JRB 1000' I would like to return a match for all the part numbers above. Write a custom Analyzer that either splits these into several tokens (JRB 1000; relatively easy and forgiving to users) or concatenates them into a single token (JRB1000; hard but precise). Implementing your own Analyzer amounts to overriding the tokenStream argument in an existing one and perhaps writing a custom TokenFilter class. Apply your new Analyzer on both documents being indexed and queries. (Links are for the Java version but .NET should be similar.) I've decided to use a lookup table in my database rather than use Lucene for this. I've accepted your answer larsmans as it actually answered my question thanks for your help. The code for the custom analyzer and tokenizer is available on github if anyone else needs it: http://gist.github.com/624076 If your analyzer just removes spaces and dashes and then uses what remains as the tokens it may suffice. Yes part number is a separate field. I have managed to get this mostly working with a custom analyzer and tokenizer that removes the spaces and dashes and uses the result as the token. This works when searching for JRB1000 however it is not working when searching for 'JRB 1000' despite passing the custom analyzer in to the QueryParser. I'm beginning to think that Lucene may not be the right tool for the job here if all it is doing is stripping the spaces and dashes from the index and query I could quite easily do this by adding a lookup table to my database. Have you checked whether the contents of your index are correct? You can do that using Luke (http://www.getopt.org/luke/). That's a Java tool but it *should* work for Lucene.Net as the index format is identical. ""Removing spaces"" means either default behavior (which doesn't work) or treating everything as one token. It's the cases `JRB1000` -> `JRB 1000` and vice versa that cause the trouble here. (Unless the part number is a separate field?) Yep have used Luke to verify that the index is OK."
152,A,How to get the next term out of a Lucene index? I'm starting from a Lucene index which someone else created. I'd like to find all of the words that follow a given word. I've extracted the term (org.apache.lucene.index.Term) of interest from the index and I can find the documents which contain that term: segmentTermDocs = segmentReader.termDocs(term); while (segmentTermDocs.next) { doc = segmentReader.document(segmentTermDocs.doc); ... } Is there a way for me to locate the positions of the term in the document and extract the terms which follow it? Since indexing the n-grams isn't an option in your situation some brute force will be required. You could enumerate the IndexReader's terms and termPositions but that would likely be excrutiatingly slow. A faster approach would be implement a divide-and-conquer search algorithm by enumerating the terms and using a MultiPhraseQuery to check a group at once. Split all the potential terms into reasonably sized groups (say 1000) and run a MultiPhraseQuery search with each chunk and your prefix word. If there are any hits recursively call on sub-groups until you reach a single term. Thanks for the ideas! This is for generating a report so performance isn't really an issue. I ended up doing a brute-force search creating PhraseQuerys composed of the term of interest and every other term in the index. Those queries which had hits indicated the terms which followed the term of interest.  Here's Grant Ingersoll's paper: Accessing words around a positional match in Lucene.
153,A,How to sort Search Results based on a Field Value in Lucene-3.0.2? I have googled a lot and also searched in stackoverflow.com about how to sort search results based on a Field Value in Lucene 3.0.2 but not found any useful data. I'm getting the search results from the index based on the user query but not able to sort the results based on field like id or date. I have pasted my code here for searching lucene index- http://pastie.org/1033974. Please help me to solve this problem. If you provide me some example code or links where i can find that will be better. Thanks The IndexSearcher class has a couple of search methods that takes a Sort Object that you have to use. A Sort object is basically a wrapper around one or more SortField objects which hold details on what field to sort on and how. Note that a field must be indexed to be used for sorting. Thanks Pascal Dimassimo..!!! I will see how i can work around with those..!!!
154,A,"Faster search platform/indexing engine than Apache Solr? Apache Solr came up in a discussion today it seems it has the unique ability of being known to be the best out there yet I still heard some people complain especially when used in conjunction with 3rd party apps like RoR apps. Can anyone disprove this? Better but less known options than Apache Solr for searching/indexing? Or at least something faster than Lucene. Thank you. If some Ruby / Solr gem is slow how about just replacing it with another gem that isn't? I've used Sunspot a ruby gem for Apache Solr and have not faced any problems yet.  Try Sphinx. There's something of a religious war between the Sphinx and Solr camps. Being a clojure developer I really appreciate the ease of integration with lucene / solr. Both are fast. Unless this discussion yielded any empirical evidence that Solr is slower than some other indexing engine in a specific context use whatever's easiest for you IMHO. Otherwise you're just prematurely optimising your project based on FUD. Thank you will investigate this option as well.Apparently Lucene is a must for this project so CLucene sounds like the closest improvement unfortunately but thank you either way. Solr is essentially just a web service wrapped around Lucene. If you're using Solr you're using Lucene. I like Sphinx too! +1 for ""Use whatever's easiest for you IMHO. Otherwise you're just prematurely optimising your project based on FUD.""  Endeca is extremely fast and powerful but also very expensive so of no use if you're looking for a ""free"" solution. Ballpark figure?  Maybe have a look at CLucene - Lucene ported to C++. Thanks for the suggestion I'll investigate this option."
155,A,"Why is Lucene sometimes not matching InChIKeys? I have indexed my database using Hibernate Search. I use a custom analyzer both for indexing and for querying. I have a field called inchikey that should not get tokenized. Example values are: BBBAWACESCACAP-UHFFFAOYSA-N KEZLDSPIRVZOKZ-AUWJEWJLSA-N When I look into my index with Luke I can confirm that they are not tokenized as required. However when I try to search them using the web app some inchikeys are found and others are not. Curiously for these inchikeys the search DOES work when I search without the last hyphen as so: BBBAWACESCACAP-UHFFFAOYSA N I have not been able to find a common element in the inchikeys that are not found. Any idea what is going on here? I use a MultiFieldQueryParser to search over the different fields in the database:  String[] searchfields = Compound.getSearchfields(); MultiFieldQueryParser parser = new MultiFieldQueryParser(Version.LUCENE_29 Compound.getSearchfields() new ChemicalNameAnalyzer()); //Disable the following if search performance is too slow parser.setAllowLeadingWildcard(true); FullTextQuery fullTextQuery = fullTextSession.createFullTextQuery(parser.parse(""searchterms"") Compound.class); List<Compound> hits = fullTextQuery.list(); More details about our setup have been posted here by Tim and I. Some background information: ""The InChIKey sometimes referred to as a hashed InChI is a fixed length (25 character) condensed digital representation of the InChI that is not human-understandable. The InChIKey specification facilitates web searches for chemical compounds since these were problematic with the full-length InChI."" It turns out the last entries in the input file are not being indexed correctly. These ARE being tokenized. In fact it seems they are indexed twice: once without being tokenized and once with. When I search I cannot find the un-tokenized. I have not yet found the reason but I think it perhaps has to do with our parser ending while Lucene is still indexing the last entries and as a result Lucene reverting to the default analyzer (StandardAnalyzer). When I find the culprit I will report back here. Adding @Analyzer(impl = ChemicalNameAnalyzer.class) to the fields solves the problem but what I want is my original setup with the default analyzer defined once in config like so: <property name=""hibernate.search.analyzer"">path.to.ChemicalNameAnalyzer</property>"
156,A,"Lucene: termFreqVector is always null? for any document the termFreqVector is always null. I'm sure the documents are in the collection and the field exist. So where is the problem ? for (int i = 0; i < reader.numDocs(); i++){ TermFreqVector tfv = reader.getTermFreqVector(i ""tags""); thanks Are you sure you're indexing with your fields with Field.TermVector.YES? Here's a working example: Directory directory = new RAMDirectory(); Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_30); MaxFieldLength mlf = MaxFieldLength.UNLIMITED; IndexWriter writer = new IndexWriter(directory analyzer true mlf); Document doc = new Document(); doc.add(new Field(""tags"" ""foo bar"" Field.Store.NO Field.Index.ANALYZED Field.TermVector.YES)); writer.addDocument(doc); writer.close(); IndexReader reader = IndexReader.open(directory); for (int i = 0; i < reader.numDocs(); i++) { TermFreqVector tfv = reader.getTermFreqVector(i ""tags""); System.out.println(tfv); } you rock thanks @user680406 no problem :) @user680406: You should never use numDocs() to access the documents in the index but rather maxDoc() instead. Once the index has deletions you will not reach out all docs."
157,A,How do I drop an index using Lucandra? I am using Lucandra and want to drop an entire index. The IndexReader and IndexWriters don't have all methods implemented so even iterating through a call to deleteDocument(int docNum) isn't possible. Has anyone run up against this and either figured out how to either hack the Cassandra keyspace or made additions to the Lucandra code or figured out how to construct an iterator to delete all docs? The current version of lucandra doesn't store documents from 1-N so the deleteDocument(int) doesn't work. What I've done is index a field with the same term in all documents so you can match all documents then delete them with deleteDocuments(Term) call. Another option (if you only have 1 index per keyspace) is to truncate the cassandra CFs The next version of lucandra(in development) does store documents 1-N fashion. Jake is the version that stores documents in the 1-N fashion the .7 Cassandra version on Git? hey Jake thanks I've read your code and came to a similar conclusion. My concern is it seems that all docs might get read into memory at once. My team and I are using Cassandra for our app's database as well and have created tools that will allow us to iterate through the key space column families. I was thinking I could just remove entries in Cassandra in the Terminfo and Document column families that start with the index name. I'd be happy to open source this tool. What do you think?
158,A,"Lucene trouble with indexing recurring events I'm trying to come up with a way to query dates in Lucene. Basically I have an event that has a start date end date and can also occur regularly. The way I tried to go about it was to create an index field in Lucene that would list all the possible dates separated by a comma (or empty space would be enough really) and than apply range search to it. The dates were indexed like this: Event A starting on 31-10-09: ""20091031"" Event B starting on 31-10-09 and lasting for 2 days: ""20091031 20091101 20091102"" Event C recurring every Saturday for next 3 Saturdays: ""20091031 20091107 20091114"" That however didn't work because if I was looking for events between 20091030 and 20091101 it should list events A B and C but because B and C had some occurrences outside of the required range it did not find them. Any idea how to solve it? Thanks You haven't shown the code that creates the Document (or at least the Fields in that Document) nor the code you use to query so it's unlikely you're going to get any good answer. I don't think that the code is relevant to my question as it is more of a ""how would you do it?"" than ""where is an error in my code?"" kind of thing. A possible way to do this is to create a separate document per each occurrence of every event. Both event B and C will then have three documents each each of them having a date field and an event name field. A simple range search could then find the events. A separate question is whether to do this in Lucene at all. Please see Search Engine versus DBMS for a discussion of related issues. That's actually a good idea. Thanks! As for the Lucene/DBMS decision it is too late for this kind of decision at this stage of the project. However my search algorithm is more complicated than just range search and even though I'm sure I could implement weighting of different attributes etc. in RDBMS it just seems like too much of a hassle."
159,A,How can to group lucene's results? My application indexes discussion threads. Each entry in the discussion is indexed as a separate Lucene document with a common_id field which can be used to group search hits into one discussion. Currently when the search is performed if a thread has 3 entries then 3 separate hits are returned. Even though this is correct from the users point of view the same entry is appearing in the search multiple times. Is there a way to tell lucene to group it's search results by the common_id field before returning them? Am please see my answer to this similar question.  There is nothing built into Lucene that collapses results based on a field. You will need to implement that yourself. However they've recently built this feature into Solr. See http://www.lucidimagination.com/blog/2010/09/16/2446/
160,A,Ranking using Geographic Location in Solr What is the correct way to implement a custom ranking algorithm for Solr/Lucene? I read about Zvents implementing a Distance Weighting ranking system for documents which correspond to events in a specific geographic area (http://www.lucidimagination.com/Community/Hear-from-the-Experts/Articles/Zvents). I would like to do something similar: I index ads in different cities and I would like to boost the relevance of nearest ads given a specific location. Local Lucene is a project meant to add local search to Lucene. Basically you add spatial coordinates to the index fields. You then have to decide based on your index structure whether it is better to first search according to textual matches and then filter by geographic location or the other way around. Lucene in Action has an example of a spatial result filter. The forthcoming second edition will probably have more in that direction. See also the LocalSolr wiki page.
161,A,"Lucene Fuzzy Match on Phrase instead of Single Word I'm trying to do a fuzzy match on the Phrase ""Grand Prarie"" (deliberately misspelled) using Apache Lucene. Part of my issue is that the ~ operator only does fuzzy matches on single word terms and behaves as a proximity match for phrases. Is there a way to do a fuzzy match on a phrase with lucene? Lucene 3.0 has ComplexPhraseQueryParser that supports fuzzy phrase query. This is in the contrib package.  There's no direct support for a fuzzy phrase but you can simulate it by explicitly enumerating the fuzzy terms and then adding them to a MultiPhraseQuery. The resulting query would look like: <MultiPhraseQuery: ""grand (prarie prairie)""> Could you elaborate a bit more on this? I'm not using Lucene directly but rather through Solr. I may very well have to just get around to reading lucene in action. I wouldn't mind getting a better understanding of how the two work together and becoming comfortable with it at a more fundamental level. For now in Solr I'm achieving something that's effective enough for me using the solr.PhoneticFilterFactory analyzer. +1. The way to go  Came across this through Google and felt solutions where not what I was after. In my case solution was to simply repeat the search sequence against the solr API. So for example if I was looking for: title_t to include match for ""dog~"" and ""cat~"" I added some manual code to generate query as: ((title_t:dog~) and (title_t:cat~)) It might just be what above queries are about however links seems dead."
162,A,"How do i delete/update a doc with lucene? I am creating a tagging system for my site I got the basics of adding a document to lucene but i can seem to figure out how to delete a document or update one when the user changes the tags of something. I found pages that say use the document index and i need to optimize before effect but how do i get the document index? Also i seen another that said to use IndexWriter to delete but i couldnt figure out how to do it with that either. I am using C# asp.net and i dont have java install on that machine What version of Lucene are you using? The IndexWriter class has an update method that lets you update (BTW an update under the hood is really a delete followed by an add). You will need to have some identifier (such as as document id) which lets you update. When you index the document add a unique document identifier such as a URL a counter etc. Then the ""Term"" will be the ID of the document you wish to update. For example using URL you can update thus: IndexWriter writer = ... writer.update(new Term(""id""""http://somedomain.org/somedoc.htm"") doc); Incubating-Apache-Lucene.Net-2.0-004-11Mar07.bin.zip. So perhaps lucene 2.0. Yikes i just realized the date. I found an svn tag using Lucene.Net_2_9_1 Just so i am clear. Doc is the new document filled with the data i want. The term is the id of the old document i want to update/replace? -edit- update looks like a DeleteAdd. doc doesnt need to hold the same id or term as the older one. yes to your first point. Unsure what you mean by the 2nd it is very advisable to have an id term for your doc (much like having a primary key for a db table). Perfect i just tested it. It does create a doc even if the term doesnt exist. My id will be the same id as the media or doc id (which is a PK in my db)  You need an IndexReader to delete a document I'm not sure about the .net version but the Java and C++ versions of the Lucene API have an IndexModifier class that hides the differences between IndexReader and IndexWriter classes and just uses the appropriate one as you call addDocument() and removeDocument(). Also there is no concept of updating a document in Lucene you have to remove it an them re-add it again. In order to do this you will need to make sure that every document has a unique stored id in the index. great to know about updates. I dont see removeDocument nor IndexModifier (maybe .net is using an older version of lucene). I see a DeleteDocument in IndexReader. It accepts 'int docNum'. I have no idea what to do with it. Theres no docNum or docId in Document the docnum is the enumerator key for example IndexReader rdr = IndexReader.Open(@""Myindex""); int N = rdr.MaxDoc(); for(int n = 0; n< N; n++) { Document doc = rdr.Document(n); //do something with this doc }"
163,A,"Hyphens in Lucene I'm playing around with Lucene and noticed that the use of a hyphen (e.g. ""semi-final"") will result in two words (""semi"" and ""final"" in the index. How is this supposed to match if the users searches for ""semifinal"" in one word? Edit: I'm just playing around with the StandardTokenizer class actually maybe that is why? Am I missing a filter? Thanks! (Edit) My code looks like this:  StandardAnalyzer sa = new StandardAnalyzer(); TokenStream ts = sa.TokenStream(""field"" new StringReader(""semi-final"")); while (ts.IncrementToken()) { string t = ts.ToString(); Console.WriteLine(""Token: "" + t); } You can write your own tokenizer which will produce for words with hyphen all possible combinations of tokens like that: semifinal semi final You will need to set proper token offsets to tell lucene that semi and semifinal actually start at the same place in document. Is Lucene OK with having several terms with the same offset? Do all the searches handle this correctly? Having offset for terms makes sense for phrase search. Phrase search handles it correctly. Term search just ignores offsets and therefore document containing 'semi-final' will get more score than document containing 'semifinal' for search query 'semifinal OR final' because it contains 2 of searched terms while latter contains just one. Does anyone have some example code as to what this custom analyzer or tokenizer would look like? I understand the concept but not the implementation. Thanks!  I would recommend you use the WordDelimiterFilter from Solr (you can use it in just your Lucene application as a TokenFilter added to your analyzer just go get the java file for this filter from Solr and add it to your application). This filter is designed to handle cases just like this: http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.WordDelimiterFilterFactory  This is the explanation for the tokenizer in lucene - Splits words at punctuation characters removing punctuation. However a dot that's not followed by whitespace is considered part of a token. - Splits words at hyphens unless there's a number in the token in which case the whole token is interpreted as a product number and is not split. - Recognizes email addresses and internet hostnames as one token. Found here this explains why it would be splitting your word. This is probably the hardest thing to correct human error. If an individual types in semifinal this is theoretically not the same as searching semi-final. So if you were to have numerous words that could be typed in different ways ex: St-Constant Saint Constant Saint-Constant your stuck with the task of having both st and saint as well as a hyphen or non hyphenated to veriy. your tokens would be huge and each word would need to be compared to see if they matched. Im still looking to see if there is a good way of approaching this otherwise if you don't have a lot of words you wish to use then have all the possibilities stored and tested or have a loop that splits the word starting at the first letter and moves through each letter splitting the string in half to form two words testing the whole way through to see if it matches. but again whose to say you only have 2 words. if you are verifying more then two words then you have the problem of splitting the word in multiple sections example saint-jean-sur-richelieu if i come up with anything else I will let you know. I added the code. I read that also I'm just wondering what to use instead then. Thanks for your effort but the other answer provided me with the info I needed.  The rule (for the classic analyzer) is from is written in jflex: // floating point serial model numbers ip addresses etc. // every other segment must have at least one digit NUM = ({ALPHANUM} {P} {HAS_DIGIT} | {HAS_DIGIT} {P} {ALPHANUM} | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+ | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+ | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+ | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+) // punctuation P = (""_""|""-""|""/""|"".""|"""")  If you're looking for a port of the WordDelimiterFilter then I advise a google of WordDelimiter.cs I found such a port here: http://osdir.com/ml/attachments/txt9jqypXvbSE.txt I then created a very basic WordDelimiterAnalyzer: public class WordDelimiterAnalyzer: Analyzer { #region Overrides of Analyzer public override TokenStream TokenStream(string fieldName TextReader reader) { TokenStream result = new WhitespaceTokenizer(reader); result = new StandardFilter(result); result = new LowerCaseFilter(result); result = new StopFilter(true result StopAnalyzer.ENGLISH_STOP_WORDS_SET); result = new WordDelimiterFilter(result 1 1 1 1 0); return result; } #endregion } I said it was basic :) If anyone has an implementation I would be keen to see it!"
164,A,"How do I estimate the size of a Lucene index? Is there a known math formula that I can use to estimate the size of a new Lucene index? I know how many fields I want to have indexed and the size of each field. And I know how many items will be indexed. So once these are processed by Lucene how does it translate into bytes? Here is the lucene index format documentation. The major file is the compound index (.cfs file). If you have term statistics you can probably get an estimate for the .cfs file size Note that this varies greatly based on the Analyzer you use and on the field types you define. link is broken.. Thanks Scottie. Fixed it - till the next version... http://lucene.apache.org/java/3_4_0/fileformats.html (visited 11/2/2011) http://lucene.apache.org/core/3_6_1/fileformats.html as of right now.  I think it has to also do with the frequency of each term (i.e. an index of 10000 copies of the sames terms should be much smaller than an index of 10000 wholly unique terms). Also there's probably a small dependency on whether you're using Term Vectors or not and certainly whether you're storing fields or not. Can you provide more details? Can you analyze the term frequency of your source data?  The index stores each ""token"" or text field etc. only once...so the size is dependent on the nature of the material being indexed. Add to that whatever is being stored as well. One good approach might be to take a sample and index it and use that to extrapolate out for the complete source collection. However the ratio of index size to source size decreases over time as well as the words are already there in the index so you might want to make the sample a decent percentage of the original."
165,A,"Symfony with Zend Lucene and related models (with foreign keys) Well I was developing an application usin Symfony 1.4 and Doctrine when I realized a major drawback on my Zend Lucene implementation. I have a model called Publication that is related (via foreign key relations) with a few other models (subjects genres languages authors etc.) and I'm getting they're names when adding a new document to the index (using the Jobeet tutorial way) so that I can search for publications with a given subject genre language author etc... The problem is if for some reason I decide to alter the name of one of those related models the Zend Lucene index will not get updated. The only two solutions I could come up with were: Re-index all the publications regularly to guarantee that any changes made to the related models gets updated on the index (however this solution doesn't allow the index to be updated in real time) Get all the publications that are related with a given model and re-index them after it gets updated (using the save() postSave() postUpdate() or whatever you can come up with on Doctrine). --> This solution seemed great... It will only rebuild the index for the publications that are linked to the updated model right? Well if you have something like a thousand (1000) publications linked to it will take a few minutes to update (yeah I tested it) and on a user form it will timeout because it takes over 30 seconds (and even if it don't it would be bad to have a user looking at the screen for a few minutes awaiting for the page to finish to load). So what I want to know is if there's another solution? Is there a way to update an index on the fly based on a change on a related model without hanging the whole pahe? Maybe putting the task to run on the background or something? Is there such a way? If there's no way to do this with Lucene is there any way to use Full-Text Search with MySQL (with InnoDB tables) without using Zend Lucene that doesn't have such a drawback? If there's such a tool I'd glady refactor my code to accommodate a different library. Could you please help me with this? Thanks in advance! As far as I'm aware Zend_Search_Lucene is no longer updated. Its performance is also really bad when you get to big indexes (10k + documents). Remember that Zend Lucene is a php port of real Lucene. Sphinx or solr (solr being built on the original lucene library) will perform massively better and give better results. Zend_Search_Lucene as far as I know is still maintained along with the rest of the Zend Framework. However I agree that the orginal Java implementation made by Apache is faster (mainly because even if it's not machine code Java is faster than PHP that is interpreted at runtime). Also Solr is a stand-alone server for Lucene so it is faster than running it inside your own application. And finnaly Sphinx is an unrelated project written in C/C++ making it extremely fast (and can be used on other languages using bindings and stuff). Well... I'm answering my self. After thinking about it for some time I ended with a compromisse solution. On my model I already have a one-to-one relationship with a table that is used only for storing meta-information about a publication so I ended up inserting a new column called reindex (that is a ""boolean""). This way everytime I update an entity related with a publication (something that in production will happen very seldom but I want to be prepared for it) it will mark every publication that is related to it as needing reindexing. Than I have a task that can be run on cron job or Task Scheduler that will only reindex the publications that are marked as needing it. This way I can set this task to run a few times a week at late hours to keep the Index consistent. It's not a perfect solution but is the best I can came up with using only PHP and Zend Lucene.  A Lucene Document cannot be updated. You can only delete hits and re-add them back in. For that reason my original solution is not valid. I was looking at alternatives for you and there is one that caught my eye: http://www.sphinxsearch.com/ It seems that Sphinx is very fast at indexing but slower to perform searches. Might be something worth taking a look at. From what I have been reading the PHP implementation of Lucene is not very fast and this is normal as a behavior. There are ways to improve the speed of indexing large quantities of data which mainly involve increasing RAM in order to let Lucene write larger doc sizes into memory before dumping out the files. Well I ended up with the solution that I wrote below. It's not perfect but I find it ""good enough"". However it would be really cool if Lucene could improve a little bit in terms of updating only a single term of document and maybe even provide some kind of Real Time indexing. As referenced above sphinx is much faster to search than zend lucene. Orders of magnitude faster to both index and search. And how can I do that. As far as I know I can't only update one term on a ""document"" indexed on Zend Lucene. If such thing is possible I'd gladly try it. Any idea how to do it? You are right. I was just thinking about it but did not look up if it was possible. A Lucene Document cannot be updated."
166,A,"How do I index and search text files in Lucene 3.0.2? I am newbie in Lucene and I'm having some problems creating simple code to query a text file collection. I tried this example but is incompatible with the new version of Lucene. UDPATE: This is my new code but it still doesn't work yet. I suggest you look into Solr @ http://lucene.apache.org/solr/ rather than working with lucene api or http://elasticsearch.org  Lucene is a quite big topic with a lot of classes and methods to cover and you normally cannot use it without understanding at least some basic concepts. If you need a quickly available service use Solr instead. If you need full control of Lucene go on reading. I will cover some core Lucene concepts and classes that represent them. (For information on how to read text files in memory read for example this article). Whatever you are going to do in Lucene - indexing or searching - you need an analyzer. The goal of analyzer is to tokenize (break into words) and stem (get base of a word) your input text. It also throws out the most frequent words like ""a"" ""the"" etc. You can find analyzers for more then 20 languages or you can use SnowballAnalyzer and pass language as a parameter. To create instance of SnowballAnalyzer for English you this: Analyzer analyzer = new SnowballAnalyzer(Version.LUCENE_30 ""English""); If you are going to index texts in different languages and want to select analyzer automatically you can use tika's LanguageIdentifier. You need to store your index somewhere. There's 2 major possibilities for this: in-memory index which is easy-to-try and disk index which is the most widespread one. Use any of the next 2 lines: Directory directory = new RAMDirectory(); // RAM index storage Directory directory = FSDirectory.open(new File(""/path/to/index"")); // disk index storage When you want to add update or delete document you need IndexWriter: IndexWriter writer = new IndexWriter(directory analyzer true new IndexWriter.MaxFieldLength(25000)); Any document (text file in your case) is a set of fields. To create document which will hold information about your file use this: Document doc = new Document(); String title = nameOfYourFile; doc.add(new Field(""title"" title Field.Store.YES Field.Index.ANALYZED)); // adding title field String content = contentsOfYourFile; doc.add(new Field(""content"" content Field.Store.YES Field.Index.ANALYZED)); // adding content field writer.addDocument(doc); // writing new document to the index Field constructor takes field's name it's text and at least 2 more parameters. First is a flag that show whether Lucene must store this field. If it equals Field.Store.YES you will have possibility to get all your text back from the index otherwise only index information about it will be stored. Second parameter shows whether Lucene must index this field or not. Use Field.Index.ANALYZED for any field you are going to search on. Normally you use both parameters as shown above. Don't forget to close your IndexWriter after the job is done: writer.close(); Searching is a bit tricky. You will need several classes: Query and QueryParser to make Lucene query from the string IndexSearcher for actual searching TopScoreDocCollector to store results (it is passed to IndexSearcher as a parameter) and ScoreDoc to iterate through results. Next snippet shows how this all is composed: IndexSearcher searcher = new IndexSearcher(directory); QueryParser parser = new QueryParser(Version.LUCENE_30 ""content"" analyzer); Query query = parser.parse(""terms to search""); TopScoreDocCollector collector = TopScoreDocCollector.create(HOW_MANY_RESULTS_TO_COLLECT true); searcher.search(query collector); ScoreDoc[] hits = collector.topDocs().scoreDocs; // `i` is just a number of document in Lucene. Note that this number may change after document deletion for (int i = 0; i < hits.length; i++) { Document hitDoc = searcher.doc(hits[i].doc); // getting actual document System.out.println(""Title: "" + hitDoc.get(""title"")); System.out.println(""Content: "" + hitDoc.get(""content"")); System.out.println(); } Note second argument to the QueryParser constructor - it is default field i.e. field that will be searched if no qualifier was given. For example if your query is ""title:term"" Lucene will search for a word ""term"" in field ""title"" of all docs but if your query is just ""term"" if will search in default field in this case - ""contents"". For more info see Lucene Query Syntax. QueryParser also takes analyzer as a last argument. This must be same analyzer as you used to index your text. The last thing you must know is a TopScoreDocCollector.create first parameter. It is just a number that represents how many results you want to collect. For example if it is equal 100 Lucene will collect only first (by score) 100 results and drop the rest. This is just an act of optimization - you collect best results and if you're not satisfied with it you repeat search with a larger number. Finally don't forget to close searcher and directory to not loose system resources: searcher.close(); directory.close(); EDIT: Also see IndexFiles demo class from Lucene 3.0 sources. i am tried this http://pastebin.com/HqrbBPtp but without success... In the line 80 you have: `QueryParser parser = new QueryParser(Version.LUCENE_30 ""computer"" analyzer);` i.e. you set second parameter (default field) to ""computer"" and then you search without qualifier. Lucene tries to use default _field_ ""computer"" to find _term_ ""computer"" and since your document doesn't have such field Lucene fails. Use `QueryParser parser = new QueryParser(Version.LUCENE_30 ""content"" analyzer);` or search with qualifier: `Query query = parser.parse(""content:computer"");`.  package org.test; import java.io.BufferedReader; import java.io.File; import java.io.FileReader; import java.io.IOException; import org.apache.lucene.queryParser.*; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.Query; import org.apache.lucene.search.ScoreDoc; import org.apache.lucene.search.TopScoreDocCollector; import org.apache.lucene.store.Directory; import org.apache.lucene.store.FSDirectory; import org.apache.lucene.store.LockObtainFailedException; import org.apache.lucene.analysis.standard.StandardAnalyzer; import org.apache.lucene.index.CorruptIndexException; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.store.RAMDirectory; import org.apache.lucene.util.Version; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; public class LuceneSimple { private static void addDoc(IndexWriter w String value) throws IOException { Document doc = new Document(); doc.add(new Field(""title"" value Field.Store.YES Field.Index.ANALYZED)); w.addDocument(doc); } public static void main(String[] args) throws CorruptIndexException LockObtainFailedException IOException ParseException { File dir = new File(""F:/tmp/dir""); StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_30); Directory index = new RAMDirectory(); //Directory index = FSDirectory.open(new File(""lucDirHello"") ); IndexWriter w = new IndexWriter(index analyzer true IndexWriter.MaxFieldLength.UNLIMITED); w.setRAMBufferSizeMB(200); System.out.println(index.getClass() + "" RamBuff:"" + w.getRAMBufferSizeMB() ); addDoc(w ""Lucene in Action""); addDoc(w ""Lucene for Dummies""); addDoc(w ""Managing Gigabytes""); addDoc(w ""The Art of Computer Science""); addDoc(w ""Computer Science ! what is that ?""); Long N = 0l; for( File f : dir.listFiles() ){ BufferedReader br = new BufferedReader( new FileReader(f) ); String line = null; while( ( line = br.readLine() ) != null ){ if( line.length() < 140 ) continue; addDoc(w line); ++N; } br.close(); } w.close(); // 2. query String querystr = ""Computer""; Query q = new QueryParser( Version.LUCENE_30 ""title"" analyzer ).parse(querystr); //search int hitsPerPage = 10; IndexSearcher searcher = new IndexSearcher(index true); TopScoreDocCollector collector = TopScoreDocCollector.create(hitsPerPage true); searcher.search(q collector); ScoreDoc[] hits = collector.topDocs().scoreDocs; System.out.println(""Found "" + hits.length + "" hits.""); for(int i=0;i<hits.length;++i) { int docId = hits[i].doc; Document d = searcher.doc(docId); System.out.println((i + 1) + "". "" + d.get(""title"")); } searcher.close(); } } you can set ""File dir"" variable to the directory you need to index."
167,A,"What makes a good autowarming query in Solr and how do they work? This question is a follow up to this question about infrequent isolated read timeouts in a solr installation. As a possible problem missing / bad autowarming queries for new searchers were found. Now I am confused about how good autowarming queries should ""look like"". I read up but couldnt find any good documentation on this. Should they hit a lot of documents in the index? Or should they have matches in all distinct fields that exist in the index? Wouldnt just *:* be the best autowarming query or why not? The example solr config has theese sample queries in it: <lst><str name=""q"">solr</str> <str name=""start"">0</str> <str name=""rows"">10</str></lst> <lst><str name=""q"">rocks</str> <str name=""start"">0</str> <str name=""rows"">10</str></lst> I changed them to: <lst><str name=""q"">george</str> <str name=""start"">0</str> <str name=""rows"">10</str></lst> Why? Because the index holds film entities with fields for titles and actors. Those are the most searched ones. And george appears in titles and actors. I don't really know whether this makes sense. So my question is: What would be good autowarming quries for my index and why? What makes a good autowarming query? This is an example document from the index. The index has about 70000 documents and they all look like this (only different values of course): example document:  <doc> <arr name=""actor""><str>Tommy Lee Jones</str><str>Will Smith</str><str>Rip Torn</str> <str>Lara Flynn Boyle</str><str>Johnny Knoxville</str><str>Rosario Dawson</str><str>Tony Shalhoub</str> <str>Patrick Warburton</str><str>Jack Kehler</str><str>David Cross</str><str>Colombe Jacobsen-Derstine</str> <str>Peter Spellos</str><str>Michael Rivkin</str><str>Michael Bailey Smith</str><str>Lenny Venito</str> <str>Howard Spiegel</str><str>Alpheus Merchant</str><str>Jay Johnston</str><str>Joel McKinnon Miller</str> <str>Derek Cecil</str></arr> <arr name=""affiliate""><str>amazon</str></arr> <arr name=""aka_title""><str>Men in Black II</str><str>MIB 2</str><str>MIIB</str> <str>Men in Black 2</str><str>Men in black II (Hombres de negro II)</str><str>Hombres de negro II</str><str>Hommes en noir II</str></arr> <bool name=""blockbuster"">false</bool> <arr name=""country""><str>US</str></arr> <str name=""description"">Agent J (Will Smith) muss die Erde wieder vor einigem Abschaum bewahren denn in Gestalt des verführerischen Dessous-Models Serleena (Lara Flynn Boyle) will ein Alien den Planeten unterjochen. Dabei benötigt J die Hilfe seines alten Partners Agent K (Tommy Lee Jones). Der wurde aber bei seiner ""Entlassung"" geblitzdingst und so muß J seine Erinnerung erst mal etwas auffrischen bevor es auf die Jagd gehen kann.</str> <arr name=""director""><str>Barry Sonnenfeld</str></arr> <int name=""film_id"">120912</int> <arr name=""genre""><str>Action</str><str>Komödie</str><str>Science Fiction</str></arr> <str name=""id"">120912</str> <str name=""image_url"">/media/search/filmcovers/105x/kf/false/F6Q1XW.jpg</str> <int name=""imdb_id"">120912</int> <date name=""last_modified"">2011-03-01T18:51:35.903Z</date> <str name=""locale_title"">Men in Black II</str> <int name=""malus"">3238</int> <int name=""parent_id"">0</int> <arr name=""product_dvd""><str>amazon</str></arr> <arr name=""product_type""><str>dvd</str></arr> <int name=""rating"">49</int> <str name=""sort_title"">meninblack</str> <int name=""type"">1</int> <str name=""url"">/film/Men-in-Black-II-Barry-Sonnenfeld-Tommy-Lee-Jones-F6Q1XW/</str> <int name=""year"">2002</int> </doc> Most queries are exact match queries on actor fields with some filters in place. Example: INFO: [] webapp=/solr path=/select/ params={facet=true&sort=score+asc+malus+asc+year+desc&hl.simple.pre=starthl&hl=true&version=2.2&fl=*score&facet.query=year:[1900+TO+1950]&facet.query=year:[1951+TO+1980]&facet.query=year:[1981+TO+1990]&facet.query=year:[1991+TO+2000]&facet.query=year:[2001+TO+2011]&bf=div(sub(10000malus)100)^10&hl.simple.post=endhl&facet.field=genre&facet.field=country&facet.field=blockbuster&facet.field=affiliate&facet.field=product_type&qs=5&qt=dismax&hl.fragsize=200&mm=2&facet.mincount=1&qf=actor^0.1&f.blockbuster.facet.mincount=0&f.genre.facet.limit=20&hl.fl=actor&wt=json&f.affiliate.facet.mincount=1&f.country.facet.limit=20&rows=10&pf=actor^5&start=0&q=""Josi+Kleinpeter""&ps=3} hits=1 status=0 QTime=4 There are 2 types of warming. Query cache warming and document cache warming (There's also filters but those are similar to queries). Query cache warming can be done through a setting which will just re-run X number of recent queries before the index was reloaded. Document cache warming is different. The goal of document cache warming is to get a large quantity of your most frequently accessed documents into the document caches so they don't have to be read from disk. So your queries should focus on this. You need to try and figure out what your most frequently searched documents are and load those. Preferably with a minimal number of queries. This has nothing to do with the actual content of the fields. EDIT: To clarify. When warming document caches your primary interest is the documents that turn up in search RESULTS most often regardless of how they are queried. Personally I'd run searches for things like: Loading by country if most of your searches are for US films. Loading by year if most of your searches are for more recent films. Loading by genre if you have a short list of heavily searched genres. A last possibility is to load them all. Your documents look small. 70000 of them is nothing in terms of server memory nowadays. If your document cache is large enough and you have enough memory available go for it. As a side note some of your biggest benefit will be from your document cache. A query cache is only beneficial for repeated queries which can be disappointingly low. You almost always benefit from a large document cache. The OP might be using a field and/or filter cache too which would also be good to warm up. If you have an enum facet for example it will cache a bitmap of docs matching that filter so you'd want to warm all of them. In this case how you query is important not just the results. @Xodarap - I believe that Filter caches can be warmed implicitly. They run as subset of the most recently run queries before the index was reloaded. This is on a hot-swap though. On a cold start yes the queries matter."
168,A,"Syncing Lucene.net indexes across multiple app servers we are designing the search architecture for a corporate web application. We'll be using Lucene.net for this. The indexes will not be big (about 100000 documents) but the search service must be always up and always be up to date. There will be new documents added to the index all the time and concurrent searches. Since we must have high availability for the search system we have 2 application servers which expose a WCF service to perform searches and indexing (a copy of the service is running in each server). The server then uses lucene.net API to access the indexes. The problem is what would be the best solution to keep the indexes synced all the time? We have considered several options: Using one server for indexing and having the 2nd server access the indexes via SMB: no can do because we have a single point of failure situation; Indexing to both servers essentially writing every index twice: probably lousy performance and possibility of desync if eg. server 1 indexes OK and server 2 runs out of disk space or whatever; Using SOLR or KATTA to wrap access to the indexes: nope we cannot have tomcat or similar running on the servers we only have IIS. Storing the index in database: I found this can be done with the java version of Lucene (JdbcDirectory module) but I couldn't find anything similar for Lucene.net. Even if it meant a small performance hit we'd go for this option because it'd cleanly solve the concurrency and syncing problem with mininum development. Using Lucene.net DistributedSearch contrib module: I couldn't file a single link with documentation about this. I don't even know by looking at the code what this code does but it seems to me that it actually splits the index across multiple machines which is not what we want. rsync and friends copying the indexes back and forth between the 2 servers: this feels hackish and error-prone to us and if the indexes grow big might take a while and during this period we would be returning either corrupt or inconsistent data to clients so we'd have to develop some ad hoc locking policy which we don't want to. I understand this is a complex problem but I'm sure lots of people have faced it before. Any help is welcome! It seems that the best solution would be to index the documents on both servers into their own copy of the index. If you are worried about the indexing succeeding on one server and failing on the other then you'll need to keep track of the success/failure for each server so that you can re-try the failed documents once the problem is resolved. This tracking would be done outside of Lucene in whatever system you are using to present the documents to be indexed to Lucene. Depending on how critical the completeness of the index is to you you may also have to remove the failed server from whatever load balancer you are using until the problem has been fixed and indexing has reprocessed any outstanding documents. I checked the same thing once. It didn't seem worth the effort as there is a bunch of DB transaction related stuff that is not trivial to port to .Net. There were also complaints of reduced speed using the JDBCDirectory stuff. The source is in the Compass project - http://svn.compass-project.org/svn/compass/trunk/src/main/src/org/apache/lucene/store/jdbc/ After some thinking this is what I see as the most viable solution: when an indexing/deindexing request is received insert a row in a shared db table that works as a queue. Implement a simple win32 service that runs in both app servers and polls the queue every X seconds indexing the content locally. When the content is succesfully indexed the service marks the item as processed otherwise it keeps trying. Sean this is currently our candidate option. I agree with you and itsadok that it seems the sanest choice. I'm also trying to find the sources for JdbcDirectory to see if a port to .NET+SQL server would be feasible. Will keep the question open for a while to see if new approaches come up will accept this answer otherwise.  in the java world we solved this problem by putting a MQ in front of the index(es). The insert was only complete when the bean pulled from the queue was successful otherwise it just rolled back any action it took marked on the doc as pending and it was tried again later  I know that this is an old question but I just came across it and wanted to give my 2 cents for anyone else looking for advise on a multi-server implementation. Why not keep index files on a shared NAS folder? How is it different from storing index in a database that you were contemplating? A database can be replicated for high availability and so can be a NAS! I would configure the two app servers that you have behind a load balancer. Any index request that comes in will index documents in a machine specific folder on the NAS. That is there will be as many indexes on the NAS as your app servers. When a search request comes in you will do a multi-index search using Lucene. Lucene has constructs (MultiSearcher) built-in to do this and the performance is still excellent. I haven't verified if this is true or not but the following answer says ""one of the major Lucene recommendations is to not use networked file systems"": http://stackoverflow.com/a/8562566/1145177 The Lucerne FAQ mentions ""Use a local filesystem. Remote filesystems are typically quite a bit slower for searching. If the index must be remote try to mount the remote filesystem as a readonly mount"": http://wiki.apache.org/lucene-java/ImproveSearchingSpeed  +1 for Sean Carpenter's answer. Indexing on both servers seems like the sanest and safest choice. If the documents you're indexing are complex (Word/PDF and the sorts) you could perform some preprocessing on a single server and then give that to the indexing servers to save some processing time. A solution I've used before involves creating an index chunk on one server then rsyncing it over to the search servers and merging the chunk into each index using IndexWriter.AddIndexesNoOptimize. You can create a new chunk every 5 minutes or whenever it gets to a certain size. If you don't have to have absolutely up-to-date indexes this might be a solution for you.  The way we keep our load-balanced servers in sync each with their own copy of Lucene is to have a task on some other server that runs every 5 minutes commanding each load-balanced server to update their index to a certain timestamp. For instance the task sends a timestamp of '12/1/2013 12:35:02.423' to all the load-balanced servers (the task is submitting the timestamp via querystring to a webpage on each load-balanced website) then each server uses that timestamp to query the database for all updates that have occurred since the last update through to that timestamp and updates their local Lucene index. Each server also stores the timestamp in the db so it knows when each server was last updated. So if a server goes offline when it comes back online the next time it receives a timestamp command it'll grab all the updates it missed while it was offline."
169,A,How get the offset of term in Lucene? I want to get the offset of one term in the Lucene . How can i get it ? I vectored my content as Field.TermVector.WITH_POSITIONS_OFFSETS Is there any method in Lucene that give me offset of the term in one Document ? Try this: TermPositionVector vector = (TermPositionVector) reader.getTermFreqVector(docId myfield); See http://lucene.apache.org/core/3_0_3/api/core/org/apache/lucene/index/TermPositionVector.html to get the info you want.
170,A,"Why does my Lucene.net search fail when performing a fuzzy search on multiple words in the search query? In my application I have a Company with the name field of This is a test which is correctly being indexed by Lucene.Net. For reference my MultiFieldQueryParser has its default operator set to QueryParser.Operator.AND. My search passes when I search for this test~ and this tst~. However my search fails when I attempt to search for this~ test~ thas~ test~ thas test~ and other variations. This whole purpose is to allow the user to misspell their search a bit so if the user searches for Jon Doe it will still show results for John Doe allowing users to not remember exact spelling of things they entered in the database. Unfortunately it seems like it is only allowing fuzzy searches on the last term in the search phrase. Am I doing something wrong or do I need to use a whole separate Analyzer in order to do this? I recently had to implement something similar on my project. I ended up splitting-up phrase into multiple segments and constructing the query manually. var input = ""This is a test""; var fieldName = ""yourField""; var minimumSimilarity = 0.5f; var prefixLength = 3; var query = new BooleanQuery(); var segments = input.Split(new[] {"" ""} StringSplitOptions.RemoveEmptyEntries); foreach (var segment in segments) { var term = new Term(fieldName segment); var fuzzyQuery = new FuzzyQuery(term minimumSimilarity prefixLength); query.Add(fuzzyQuery BooleanClause.Occur.SHOULD); } Very primitive I know but appears to work. Note: this has only been tested against Lucene.net v2.3.1.3 Note: This works best if you use the WhitespaceAnalyzer at index-time many other analyzers will lower-case your data so a search for ""This"" wont match anything. In my case there was no requirement for the search to be case-sensitive so I generate search `Term`s using `StandardAnalyzer.TokenStream(string fieldName TextReader reader)` For Java it will be `query.add(new BooleanClause(fuzzyQuery Occur.SHOULD))` on the last line"
171,A,"Performing Lucene Search query ""Contains"" I am doing search for documents that contains the text entered by user It works fine if the searchText don't have any special characters in it. Below is how i create my QueryParser. : //analyzer is an StandardAnalyzer() QueryParser parser = new QueryParser(""Text"" analyzer); parser.SetAllowLeadingWildcard(true); return parser.Parse(string.Format(""*{0}*"" searchText)); I get the following error if the search text contains any special character in it : suppose say search text is ""bed ["" Cannot parse '*bed [*': Encountered ""<EOF>"" at line 1 column 7. How can i make my query parser not fail if there are any special characters in search text and also i don't want to ignore the special characters. Lucene supports escaping special characters that are part of the query syntax. The current list special characters are && || ! ( ) { } [ ] ^ "" ~ * ? : \ To escape these character use the \ before the character. For example to search for (1+1):2 use the query: (1+1)\:2  Try using: QueryParser parser = new QueryParser(""Text"" analyzer); parser.SetAllowLeadingWildcard(true); var escapedSearchText = QueryParser.Escape(searchText); return parser.Parse(string.Format(""*{0}*"" escapedSearchText)); i.e. escape the search text before building your query. Hope this helps I'm not entirely sure what you're after here but I suspect that you're using the standard analyser for index/query. If that's the case then yes: these sorts of characters will be silently ignored (or rather skipped) by the analyser. another thing i want to ask is after doing QueryParser.Escape will it ignore the special character while querying . I mean will it still search for ""bed ["" or the QueryParser.Escape will make it to search for only ""bed"". yes i am using StandardAnalyser whne tested i saw that its ignoring  is there any work around so i can make it work with special characters and not ignore them I think you just need to look at the analysers available and make your best choice. It could be that the simple analyser is a better fit for your use case... hard to tell really. You might even need to write your own..."
172,A,Get highest frequency terms from Lucene index i need to extract terms with highest frequencies from several lucene indexes to use them for some semantic analysis. So I want to get maybe top 30 most occuring terms(still did not decide on threshold i will analyze results) and their per-index counts. I am aware that I might lose some precision because of potentionally dropped duplicates but for now lets say i am ok with that. So for the proposed solutions (needless to say maybe) speed is not important since I would do static analysis I would put accent on simplicity of implementation because im not so skilled with Lucene and cant wrap my mind around some concepts of it.. I can not find any code samples from something similar so all concrete advices (code pseudocode links to code samples...) Appreciate all the advices! Thank you! A very simple way would be to use Luke. On the 'Overview' tab there is a 'Show top terms' button that can be used for what you need. That's the way. And you can copy-paste or even export the results. +1 if you want to run your own code Luke's code shows you how to....  Have a look at this: http://sujitpal.blogspot.com/2009/02/summarization-with-lucene.html The class in this page hascomputeTopTermQuery method which you should be easily able to retrofit for going over multiple indexes. Thanx! Exactly what I needed! Hi mindas! I using lucene 4.4 so it don't have terms() method ? help me please! @Thangnv you might want to open a separate thread for this or email Lucene Java mailing list. I don't have Lucene 4.4 running on hand. And the time has been on short supply these days ;-(
173,A,"Ehcache Search and Lucene Comparison There is an existing ehcache and lucene comparison sometime back and the answer is they can't be compared directly. However in EhCache 2.4 it now has search feature. We are thinking to migrate our currently Lucene solution into EhCache Search. One of the benefit I see is when EhCache is integrated with Terracotta it can be become distributed cache and index easily. Any other concerns I should take account before the migration? Could anyone share their experience about EhCache Search as cache and index solution? Thanks. Update: After a quick test it seems like EhCache Search does not allow cache to be persisted into disk. I got the below error if I try to set diskPersistent=""true"" on my ehcache.xml. Which mean the indexed cache need to be rebuilt everytime. I see this is one of the disadvantage. Search attributes not supported by this store type: net.sf.ehcache.store.compound.impl.DiskPersistentStore What if you use ehcache **in front** of Lucene? Let lucene do the indexing and you can cache and distribute the results with ehcache. Here are list of limitation: Can't persist searchable cache into disk. Custom AttributeExtractor only support certain types which means you can't use search attribute with parameterize T like net.sf.ehcache.search.Attribute.eq(T) net.sf.ehcache.search.Attribute.between(T T) etc but only net.sf.ehcache.search.Attribute.ilike(String) which may cause slower performance and less search constraint. I have to edit the EhCache source to make it support my custom type as I wish to use Attribute.eq(T). Thanks."
174,A,"Lucene query syntax I want to write a Lucene query which is the equivalent of the following SQL where age = 25 and name in (""tom"" ""dick"" ""harry"") The best I've come up with so far is: (age:25 name:tom) OR (age:25 name:dick) OR (age:25 name:harry) Is there a more succinct way to write this? Thanks Don Does this work? age:25 AND (name:tom OR name:dick OR name:harry) I understand this may not be what you're looking for. I didn't know if the purpose of your question was to factor out the age:25 clause or if it was to eliminate the name: prefixes. If you make name your QueryParser's default field you could reduce this down to: age:25 AND (tom OR dick OR harry)  age:25 AND name:(tom OR dick OR harry) alternatively +age:25 +name:(tom OR dick OR harry)  It's not much more succinct but you can try: (age:25) AND (name:tom OR name:dick OR name:harry)"
175,A,"Lucene - How to discard numeric terms in indexing? Using StandardAnalyzer my Lucene contain numeric terms too(i.e. ""200""). So number of term in my index is too big. Does anyone know if exists an Analyzer or Tokenizer that discards numeric terms? If not is there any easy way to get it? Antonio I suggest you try using SimpleAnalyzer. If this does not work for you you may have to write your own analyzer."
176,A,"Lucene QueryParser interprets 'AND OR' as a command? I am calling Lucene using the following code (PyLucene to be precise): analyzer = StandardAnalyzer(Version.LUCENE_30) queryparser = QueryParser(Version.LUCENE_30 ""text"" analyzer) query = queryparser.parse(queryparser.escape(querytext)) But consider if this is the content of querytext: querytext = ""THE FOOD WAS HONESTLY NOT WORTH THE PRICE. MUCH TOO PRICY WOULD NOT GO BACK AND OR RECOMMEND IT"" In that case the ""AND OR"" trips up the queryparser even though I am use queryparser.escape. How do I avoid the following error message?  Java stacktrace: org.apache.lucene.queryParser.ParseException: Cannot parse 'THE FOOD WAS HONESTLY NOT WORTH THE PRICE. MUCH TOO PRICY WOULD NOT GO BACK AND OR RECOMMEND IT': Encountered "" <OR> ""OR """" at line 1 column 80. Was expecting one of: <NOT> ... ""+"" ... ""-"" ... ""("" ... ""*"" ... <QUOTED> ... <TERM> ... <PREFIXTERM> ... <WILDTERM> ... ""["" ... ""{"" ... <NUMBER> ... <TERM> ... ""*"" ... at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:187) .... at org.apache.lucene.queryParser.QueryParser.generateParseException(QueryParser.java:1759) at org.apache.lucene.queryParser.QueryParser.jj_consume_token(QueryParser.java:1641) at org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1268) at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:1207) at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:1167) at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:182) I realise I'm rather late to the party here but putting quotes round the search string is a better option: querytext = ""\""THE FOOD WAS ... \""""  It's not just OR it's AND OR. I use the following workaround: query = queryparser.parse(queryparser.escape(querytext.replace(""AND OR"" ""AND or"")))  queryparser.parse only escapes special characters (as shown in this page) and leaves ""AND OR"" unchanged so it would not work in your case. Since presumably you also used StandardAnalyzer to analyze your text the terms in your index are already in lowercase. So you can change the whole query string to lowercase before giving it to the queryparser. Lowercase ""and"" and ""or"" are not considered operators so ""and or"" would not trip the queryparser."
177,A,"A few Lucene questions I've been using Zend and need a search. The Zend docs aren't great so I had a couple questions that are easy to answer but not directly obvious. I'm using Lucene to search an SQL database How do I associate the index of my item with the text of that item. So if they search and find the item how do I get its index returned? As far as I can tell you can only return the text of the search. When I add an item to the document that holds all the data but the document is created already is it simply a open('document') $doc = new Doc() $doc->addDocument() commit()? I understand that I update the Lucene document every time that I add something to the database. In optimizing should I reoptimize every time that I add something? Is that inefficient? Should I do it once a week? Sorry to ask what seems like obvious questions and thanks for your help in advance. 'Index and thou shalt retrieve' - You have to index what you want to be returned eventually. That is if you want to be able to return record id 1389 when searching for its text ""Flux Capacitator"" you should store a document having the text in one field and the id in another field. The id field does not have to be indexed but it has to be stored so you can get it back. What you are looking for is an 'update document' action. Lucene does not really have them. You should delete the document first and then add a new document containing the updated information. Now go back to item 1 take the id field you added there and make it indexed (say as Keyword) because you will need to use it as a unique identifier of the document in order to delete it. Great question. This is very much dependent on your use case. Do you have a daily ""dead time"" when your site/database is relatively idle? That would be the time to optimize. Do you have no such time? You can forgo optimizing and take a small (say 5-10%) performance penalty which could also be mitigated using the Merge Factor. I hope this make sense. If it does not please ask in the comments. Think I got it! Thanks! Actually I do have a question. Suppose I have a test to search that contains the word ""football"". If I search for ""foot"" it doesn't seem to come up. Is there any way to do this? Try using ""foot*"".  point 3) is addressed in Lucene 2.9 as NRT(NearRealtimeSearch) implemented by means of SegmentReader + internal RamDirectory usage check OtisGospodnetic wiki entry"
178,A,"Indexing large DB's with Lucene/PHP Afternoon chaps Trying to index a 1.7million row table with the Zend port of Lucene. On small tests of a few thousand rows its worked perfectly but as soon as I try and up the rows to a few tens of thousands it times out. Obviously I could increase the time php allows the script to run but seeing as 360 seconds gets me ~10000 rows I'd hate to think how many seconds it'd take to do 1.7million. I've also tried making the script run a few thousand refresh and then run the next few thousand but doing this clears the index each time. Any ideas guys? Thanks :) A few 10 thousand rows is still microscopic if you're timing out something really wrong is going on please post your SQL and maybe even your server info (cpu ram). I'm sorry to say it because the developer of Zend_Search_Lucene is a friend and he has worked really hard it but unfortunately it's not suitable to create indexes on data sets of any nontrivial size. Use Apache Solr to create indexes. I have tested that Solr runs more than 300x faster than Zend for creating indexes. You could use Zend_Search_Lucene to issue queries against the index you created with Apache Solr. Of course you could also use the PHP PECL Solr extension which I would recommend. Bill thanks for the heads up I've moved over to using Solr now and have got 1.7million rows indexing in roughly 8 minutes; doesn't that make you feel warm inside! I have run into one problem though... http://stackoverflow.com/questions/2668279/using-solr-and-zends-lucene-port-together but it was definately the right move thank you!  Some info for you all - posting as an answer so I can use the code styles. $sql = ""SELECT id company psearch FROM businesses""; $result = $db->query($sql); // Run SQL $feeds = array(); $x = 0; while ( $record = $result->fetch_assoc() ) { $feeds[$x]['id'] = $record['id']; $feeds[$x]['company'] = $record['company']; $feeds[$x]['psearch'] = $record['psearch']; $x++; } //grab each feed foreach($feeds as $feed) { $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::UnIndexed('id' $feed[""id""])); $doc->addField(Zend_Search_Lucene_Field::Text('company' $feed[""company""])); $doc->addField(Zend_Search_Lucene_Field::Text('psearch' $feed[""psearch""])); $doc->addField(Zend_Search_Lucene_Field::UnIndexed('link' 'http://www.google.com')); //echo ""Adding: "". $feed[""company""] .""-"".$feed['pcode'].""\n""; $index->addDocument($doc); } $index->commit(); (I've used google.com as a temp link) The server its running on is a local install of Ubuntu 8.10 3Gb RAM and a Dual Pentium 3.2GHz chip. Why are you double looping? Seems like you could get by with a single loop just fine. i'd like to share with you some performance tips : 1- you are getting into 2 loops  each of them has 1.7 million record ......... 2- Zend_Db_Table_Abstract::setDefaultMetadataCache($cache); which will notably speed up your querying  Try speeding it up by selecting only the fields you require from that table. If this is something to run as a cronjob or a worker then it must be running from the CLI and for that I don't see why changing the timeout would be a bad thing. You only have to build the index once. After that new records or updates to them are only small updates to your Lucene database. See http://framework.zend.com/manual/en/zend.search.lucene.index-creation.html for updating documents and the index. Thanks for the quick reply Htbaa :) I've just got rid of 2 fields so its now only 3 (id company name and postcode). I'm just running some tests and I'll see how that improves things. Ramping up the run time and letting it take as long as it takes is an option although its a pain for testing ;) I need to look into updating Lucene index's as I was unaware that you actually could? Thanks again!"
179,A,"Is it possible to create Java classes from JRuby and use them in Java? I'm trying to extend Lucene's Analyzer from JRuby and use it from java. A simple analyzer would look like: class MyAnalyzer < Java::OrgApacheLuceneAnalysis::Analyzer def TokenStream (file_name reader) result = StandardTokenizer.new(Version::LUCENE_CURRENT reader) result = LowerCaseFilter.new(result) result = LengthFilter.new(result 3 50) result = StopFilter.new(result StandardAnalyzer.STOP_WORDS_SET) result = PorterStemFilter.new(result) result end end Then I compile it: jrubyc -c /home/camilo/trunk/utils/target/dependency/lucene-core-3.0.2.jar --javac MyAnalyzer.rb and package it as a jar. Now when trying to use MyAnalyzer back in java MyAnalyzer is a descendent of org.jruby.RubyObject and not of org.apache.lucene.analysis.Analyzer. Is there a way to make Java treat MyAnalyzer as an Analyzer instead of a RubyObject? Or is this way outside the scope of what JRuby can do now? JRuby version: jruby 1.6.0 (ruby 1.8.7 patchlevel 330) From what I understand from you are trying to do I’m guessing you are trying to create a JRuby class that extends a Java class (with a scripting engine) and hand back that class to Java. Your Ruby class probably looks like this: require 'java' require 'lucene-core.jar' java_import 'org.apache.lucene.analysis.Analyzer' java_import 'org.apache.lucene.analysis.standard.StandardTokenizer' java_import 'org.apache.lucene.util.Version' java_import 'org.apache.lucene.analysis.TokenStream' java_import 'java.io.Reader' class MyAnalyzer < Analyzer def tokenStream(file_name reader) result = StandardTokenizer.new(Version::LUCENE_CURRENT reader) # ... end end You can then use this class in Java as follows: import javax.script.ScriptEngine; import javax.script.ScriptEngineManager; import javax.script.ScriptException; import java.io.FileReader; import java.io.FileNotFoundException; import java.io.Reader; import org.apache.lucene.analysis.Analyzer; public class RunMyAnalyzer { public static void main(String[] args) throws ScriptException FileNotFoundException { String filename = ""my-analyzer.rb""; ScriptEngineManager manager = new ScriptEngineManager(); ScriptEngine engine = manager.getEngineByName(""jruby""); Reader reader = new FileReader(filename); engine.eval(reader); // Instantiate the JRuby class and cast the result of eval. Analyzer analyzer = (Analyzer) engine.eval(""MyAnalyzer.new""); // You can then use this analyzer like a Lucene Analyzer } } You then compile and run with: $ javac -cp .:lucene-core.jar:$JRUBY_HOME/lib/jruby.jar RunMyAnalyzer.java $ java -cp .:lucene-core.jar:$JRUBY_HOME/lib/jruby.jar RunMyAnalyzer The key here is that JRuby produces a proxy class that can then be casted into Analyzer the Java superclass."
180,A,"solr: find last/highest unique key & range search i have an lucene index with an ""uniqueKey"" <uniqueKey>ID</uniqueKey> As far as i know this key have to be ""text"" (not int or long). <field name=""ID"" type=""string"" indexed=""true"" stored=""true""/> An small application used the lucene index in order to search only in those records which are added since the last run of that application. To reach that goal i'm trying the following. Load the last ID (from flatfile) into the variable $oldID get the current (last/highest) ID from Solr/lucene into the variable $currentID execute an range search between $oldID and $currentID Save the $currentID into a flat file for next Search/for next run Unfortunately I notice a problem: A.) how to find the highest ID? or B.) how i handle the unique key as an digit not text/char I tried something like that: http://localhost:8080/solr/select/?defType=func&q=max(ID0)&fq=ID:[$oldID+TO+$currentID]&fl=ID ...which returns strange thinks: ""999999"" as the highest value. That's not correct because the highest ID is 1043725. I think this is because ID is an Text-Field C.) maybe there is any other way to search only at the last added recoreds? Thanks for any kind of help! Solr's uniqueKey field can support any of the data type classes that Solr supports. While the underlying Lucene index itself treats all stored/indexed data as text Solr manages the translation to treat the indexed data according to data types. The string field-type restriction you're referring to is if you have enabled the QueryElevationComponent in solr config. If you have not enabled that feature you can make your uniqueKey a long to solve your issue. that's an really interesting information - i never heard about QueryElevationComponent before. By reading that -> http://wiki.apache.org/solr/QueryElevationComponent i have one more question to understand if it is an god idea (uncritical) to disable QueryElevationComponent. Just to understand that feature: there is no effekt if i doesn't use it explicitly. That's an prepared solar feature active by default to boost special kind of results. So if i don't use that feature in my application i can disable it without any side effects!? Thank you for your answers. Sorting and working with the primary key (ID) now works correct. :-) Query elevation allows search results to be manipulated by directly superseding calculated/scored results with specific index entries -- think of it as an override for certain search use cases. By default it's disabled. At any rate if you're not using it you can disable it without side effects."
181,A,solr balanced indexing I have two Solr server. The databases every day large amounts of data changes will happen. How I know and automate the problem that until solr0 adding/commiting/warmuping while just solr1 serve and if it is completed then solr1 adding/commiting/warmuping and serves the solr0. You might want to consider replication. Set solr0 as your master and solr1 as a slave. Reindex on solr0 and once it is finished and committed solr1 will start pulling the new index. If the reindexing load is so significant that solr0 can't serve queries while it is reindexing you could use something like varnish in front of the Solr servers and mark solr0 as inactive wile indexing. All of this can be automated by writing scripts that query the Solr DIH and replication status endpoints. @eMber only the index is replicated not any of the caches. If you had a script handling load balancer juggling for the active solr core you could move cache warming to that script to ensure that traffic doesn't go to any core that isn't warmed after indexing/replication. The warmup stage is executed in such a way that the most popular searches chassing again. If solr0 graduated the commitand/warmup stage and replicate to solr1 the fresh cashs stock is be replicated from solr0 to solr1?
182,A,"My Lucene queries only ever find one hit I'm getting started with Lucene.Net (stuck on version 2.3.1). I add sample documents with this:  Dim indexWriter = New IndexWriter(indexDir New Standard.StandardAnalyzer() True) Dim doc = Document() doc.Add(New Field(""Title"" ""foo"" Field.Store.YES Field.Index.TOKENIZED Field.TermVector.NO)) doc.Add(New Field(""Date"" DateTime.UtcNow.ToString Field.Store.YES Field.Index.TOKENIZED Field.TermVector.NO)) indexWriter.AddDocument(doc) indexWriter.Close() I search for documents matching ""foo"" with this: Dim searcher = New IndexSearcher(indexDir) Dim parser = New QueryParser(""Title"" New StandardAnalyzer()) Dim Query = parser.Parse(""foo"") Dim hits = searcher.Search(Query) Console.WriteLine(""Number of hits = "" + hits.Length.ToString) No matter how many times I run this I only ever get one result. Any ideas? Check how many documents are in your index using Luke. Could very well be something in your document add routine. I suspect you are overwriting (recreating) the index in your write loop. Make sure your Index creation code is outside of the write loop  Mikos is right about recreating the index your problem is here: Dim indexWriter = New IndexWriter(indexDir New Standard.StandardAnalyzer() True) Because you are passing true you're recreating the index each time - need to check for existence and create IF NEEDED. I ran into this a while ago here's how I got around it:  If _writer Is Nothing Then Dim create As Boolean = Not System.IO.Directory.Exists(path) OrElse System.IO.Directory.GetFiles(path).Length = 0 _directory = FSDirectory.GetDirectory(path _lockFactory) _writer = New IndexWriter(_directory _analyzer create) End If Where path is the path to your index. Not sure if this is the best approach but it is working for me (using lucene.net 2.3 also). Also you should avoid creating the writer every time if you can - lucene isn't going to like if you get in a situation where you have > 1 writer open on a particular index"
183,A,"Solr / lucene search - how easy to use - which one? I am creating a social site and for search want to try solr or lucene as I have very indepth searches required. Platform is PHP codeignitor and MySQL. However my php developers have 0 experience outside of PHP/MySQL. So before i make them implement this I need to know: 1) How easy or how much time would it normally take to setup and get it implemented? 2) Is there coding involved or is it ready out of the box? ( I know there will be some to link it with my system objects) 3) Which one to use out of the two? Check the Zend_Search_Lucene Component if interested in Lucene + PHP (http://framework.zend.com/manual/en/zend.search.lucene.overview.html) For your use I would suggest Solr. To use Lucene you will need in depth Java knowledge where as with Solr you don't necessarily need this. Solr will be ready out of the box but you will need to do some configuration to ""describe"" your search index. You need to configure it so that it understands what your documents look like what fields within that document to search on how to search them etc. This does have a learning curve. However it's not overly difficult. The time this takes is greatly affected by how complex you want your searches to be. For simple searches I would think a developer should be able to insert documents and perform searches within a week of starting with Solr. Depending on how in depth your searches are a developer could spend weeks or months learning and fiddling to tweak things. However the bulk of the work should be doable within a few weeks of concentrated effort. For what it's worth the wiki and mailing lists for Solr are great resources. AND the developers themselves are very responsive. EDIT: The coding involved with Solr would be on the PHP side. You need to write something to put your data into the XML format that Solr needs to insert documents into it's index as all of this is done via XML over HTTP. Great. Thanks.."
184,A,"Lucene query: bla~* (match words that start with something fuzzy) how? In the Lucene query syntax I'd like to combine * and ~ in a valid query similar to: bla~* //invalid query Meaning: Please match words that begin with ""bla"" or something similar to ""bla"". Update: What I do now works for small input is use the following (snippet of SOLR schema): <fieldtype name=""text_ngrams"" class=""solr.TextField""> <analyzer type=""index""> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""1"" catenateNumbers=""1"" catenateAll=""0"" splitOnCaseChange=""0""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.EdgeNGramFilterFactory"" minGramSize=""2"" maxGramSize=""15"" side=""front""/> </analyzer> <analyzer type=""query""> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""0"" catenateNumbers=""0"" catenateAll=""0"" splitOnCaseChange=""0""/> <filter class=""solr.LowerCaseFilterFactory""/> </analyzer> In case you don't use SOLR this does the following. Indextime: Index data by creating a field containing all prefixes of my (short) input. Searchtime: only use the ~ operator as prefixes are explicitly present in the index. in the development trunk of lucene (not yet a release) there is code to support use cases like this via AutomatonQuery. Warning: the APIs might/will change before its released but it gives you the idea. Here is an example for your case: // a term representative of the query containing the field. // the term text is not so important and only used for toString() and such Term term = new Term(""yourfield"" ""bla~*""); // builds a DFA that accepts all strings within an edit distance of 2 from ""bla"" Automaton fuzzy = new LevenshteinAutomata(""bla"").toAutomaton(2); // concatenate this DFA with another DFA equivalent to the ""*"" operator Automaton fuzzyPrefix = BasicOperations.concatenate(fuzzy BasicAutomata.makeAnyString()); // build a query search with it to get results. AutomatonQuery query = new AutomatonQuery(term fuzzyPrefix); I just came back to this question and saw your answer again. Have you tried it? What I'm doing now (works for small input) is to generate all prefixes of my input and put the prefixes in the index. Then I only need to use the ~ operator and get functionality like ~* Your workaround is just fine for small inputs... but as you hinted will be a problem for large inputs: you will add a huge amount of terms and postings for all these prefixes... this will make the pre-Lucene 4.0 fuzzy query even slower as it does a linear scan of all terms. Is there a Lucene query syntax that gives you access to the Automaton query through Solr without coding?  It's for an address search service where I want to suggest addresses based on partially typed and possibly mistyped streetnames/citynames/etc (any combination). (think ajax users typing partial street addresses in a text field) For this case the suggested query expansion is perhaps not so feasible as the partial string (street address) may become longer than ""short"" :) Normalization One possibility I can think of is to use string ""normalization"" instead of fuzzy searches and simply combine that with wildcard queries. A street address of ""miklabraut 42 101 reykjavík"" would become ""miklabrat 42 101 rekavik"" when normalized. So building index like this: 1) build the index with records containing ""normalized"" versions of street names city names etc with one street address per document (1 or several fields). And search the index like this: 2) Normalize inputstrings (e.g. mikl reyk) used to form the queries (i.e. mik rek). 3) use the wildcard op to perform the search (i.e. mik* AND rek*) leaving the fuzzy part out. That would fly provided the normalization algorithm is good enough :)  You mean you wish to combine a wildcard and fuzzy query? You could use a boolean query with an OR condition to combine for example: BooleanQuery bq = new BooleanQuery(); Query q1 = //here goes your wildcard query bq.Add(q1 BooleanClause...) Query q2 = //here goes your fuzzy query bq.Add(q2 BooleanClause...) yup this is not what I want :) This is the second time I hear of SimMetrics maybe it's time to give it a spin :) thank you Ok thanks for clarifying one approach I have used (for matching names of proteins etc.) using string distances like Smith-Waterman Jaro-Winkler etc. A tool like SimMetrics might be of some help http://www.dcs.shef.ac.uk/~sam/simmetrics.html I do not believe this would accomplish what the OP is asking as it would basically become ""bar~ OR bar*"" which is not the same thing as ""bar~*"" and would not find (for example) ""brafoo"".  I do not believe Lucene supports anything like this nor do I believe it has a trivial solution. ""Fuzzy"" searches do not operate on a fixed number of characters. bla~ may for example match blah and so it must consider the entire term. What you could do is implement a query expansion algorithm that took the query bla~* and converted it into a series of OR queries bla* OR blb* OR blc OR .... etc. But that is really only viable if the string is very short or if you can narrow the expansion based on some rules. Alternatively if the length of the prefix is fixed you could add a field with the substrings and perform the fuzzy search on that. That would give you what you want but will only work if your use case is sufficiently narrow. You don't specify exactly why you need this perhaps doing so will elicit other solutions. One scenario I can think of is dealing with different form of words. E.g. finding car and cars. This is easy in English as there are word stemmers available. In other languages it can be quite difficult to implement word stemmers if not impossible. In this scenario you can however (assuming you have access to a good dictionary) look up the search term and expand the search programmatically to search for all forms of the word. E.g. a search for cars is translated into car OR cars. This has been applied successfully for my language in at least one search engine but is obviously non-trivial to implement. Althoug fuzzy searches don't operate on a fixed number of characters for my case simply using ~ wont work (to big diff in char count). I want to match e.g. Sunla to Sundlaugarvegur. of course if i could tell lucene to only match on the first x characters of each word in the index using ~ would work... You would need to go beyond Lucene here use a string comparison algorithm like Levenstein Jaro-Winkler etc. (qv. below)"
185,A,how can I use Solr to do real-time search now we use deltaImport to update data from db to index. but we have some information need a real-time search or near real-time search. what should I do if I use solr to solve this? sorry I will do that right now to generate near real-time-search i would update the data in small packages and also update the index in small packages every minute (index update needs only some seconds - depending on the size of new data) don't forget to optimize the index regularly  This post could be useful for you: Solr and Near Real-Time Search  You should take a look at Solr 3.3 with RankingAlgorithm 1.2. It supports NRT and can update 10000 docs / sec. You can search concurrently during the updates. You can get more information from here: http://solr-ra.tgels.org/wiki/en/Near_Real_Time_Search_ver_3.x
186,A,Solr for indexing application log files I am thinking of using solr to index the log files generated by applications and allow the support staff to serach the log for trouble shooting. Any body ever did this kind of thing using solr? I met some folks who are doing this and hope to have more details to publish soon. Is Solr the best fit for this problem? You need the faceting the full-text search (because your logs aren't at all regular) and you don't need the set operations of a regular RDBMS? Rackspace uses Hadoop and Solr to index terabytes of log data: http://highscalability.com/how-rackspace-now-uses-mapreduce-and-hadoop-query-terabytes-data
187,A,"How does (carrot) clustering work in solr? i have running Lucene/Solr 4 for testing different features also ""clustering"". Currently 1 million documents are indexed. Every document has the following fields: ID (unique Key) Example1: 10245 Example2: 24974 TOPIC (Keywords of the document) Example1: ""disaster/japan/nuclear power station"" Example2: ""world/japan/nuclear power"" HEADLINE (1 line of text): Example1: ""explosion at nuclear power plant in japan"" Example2: ""news about japans nuclear power plant"" TEXT (the full text): ""In the Japanese nuclear power plant in Fukushima..."" All the fields are indexed and stored exapt TEXT which is only indexed not stored. I use the following specific configuration:  <str name=""carrot.title"">TOPIC</str> <str name=""carrot.snippet"">HEADLINE</str> If you looking the example you see that the TOPIC is different but japan is the same. Is it possible to configure solr/carrot in that way that example1 and example2 will be in one cluster? Because of the matching ""japan""?! Further there could be an 3rd TOPIC like ""news/nuclear power"" no ""japan"" inside but HEADLINE and TEXT are using the words: japans power plant. What solr/carrot configuration is relevant in order to receive those 3 news in one cluster? Thank you! Carrot2 is designed to cluster natural / unstructured text and such algorithms will very rarely produce results that a human would find perfect. Unfortunately such algorithms are also hard to ""debug"" -- the clusters they produce depend on many factors such as the frequencies with which words occur in your documents. In your specific example the word Japan may not have been chosen to form a cluster because it's too frequent -- it appears in all of the documents you quoted. Here are a few tips you may want to try to tweak the clusters: Try separating keywords with a period followed by a space rather than a slash e.g. ""disaster. japan. nuclear power station"". If you do that Carrot2 will treat word sequences such as ""nuclear power station"" as phrases rather than individual words. Try a different Carrot2 clustering algorithm e.g. STC. If there is a chance to get your full story text field stored (or maybe part of it such as the first paragraph) use the HEADLINE for carrot.title and the full text / excerpt for carrot.snippet. Play with the specific settings of Carrot2 algorithms. The best tool for this would be Carrot2 Clustering Workbench. Here's how to connect it to Solr: http://wiki.apache.org/solr/ClusteringComponent#Tuning_Carrot2_clustering Thank you there are a lot of interesting ideas I will try."
188,A,"searchable plugin ignores objects id I am using 0.5.5.1 grails searchable plugin. Search works on most of my objects and fields. However I have a class with String id and it consists of a Number Dash Number like 1-1 1-2 .. and so on. I cannot search this object by id. My guess its due to dash in it it might be ignored by searchable analyzer? Not sure.. Any ideas suggestions? i would first suggest that you download Luke http://code.google.com/p/luke/ and take a look at what exactly is going into the index. The default index location is ""${user.home}/.grails/projects/${app.name}/searchable-index/${grails.env}"" it is quite possible that the dashes are getting removed when the index is being created based on the analyzer you are using to created the index. I also believe you might need to do some character escaping when executing the query of the dash is included in the index You only need one ""\"". You are using too many You are looking at Location.search the \ there needs to be escaped you will see that actual debug statement shows only one \ in +1\-1 Thanks for your response. I downloaded luke and my index does show the id being indexed correctly with 1-1. I am trying escaping the - does not seem to work for now... still testing Hmm getting somewhere its only the ID field that is not being searched with a dash in it. If I put a dash in another field and search for it I will get the results back.. This has to be something within the plugin... what does the query look like? what i usually do is get the query working in Luke first then go back to my code and correct the problem. I suspect you are using the StandardAnalyzer if you use the WhitespaceAnalyzer and create your query like this stop\-gap with the backslash it will stay in the query Running query in Luke directly for: 1-1 using: org.apache.lucene.analysis.standard.StandardAnalyzer or org.apache.lucene.analysis.WhitespaceAnalyzer returns results but not with org.apache.lucene.analysis.SimpleAnalyzer... Weird because I believe by default Grails plugin is using Standard as well you can change the analyzer in the configuration file Searchable.groovy Tried that I put: compassSettings = [ 'compass.engine.analyzer.whitespace.type': 'org.apache.lucene.analysis.WhitespaceAnalyzer' ] and searching with Parent.search(params.qanalyzer: 'whitespace') still not returning the results for 1-1 while searching by other fields returns results You have to put a ""\"" before the hypen in the query for it to work I do but it still does not return the results something is really off with my setup as I dont see my new entries being indexed. Only those that are there on start up... Let's do one at a time Force a complete rebuild of your index to get the new index based on the new configuration. Call indexAll action on searchableController then retry test i just re-read your question and I would suggest creating a field with a different name than ""id"" to solve you problem. also...please accept the answer if you found it useful Run indexAll results compass.CompassGpsUtils Finished Searchable Plugin bulk index 884 ms. Running: 1. Location.search(""1\\\\-1"" analyzer: 'whitespace') debug: search.DefaultSearchMethod query: [+1\-1 +(alias:Location)] [0] hits took [1] millis 2. Location.search(""1-1"" analyzer: 'whitespace') debug: search.DefaultSearchMethod query: [+1-1 +(alias:Location)] [0] hits took [1] millis Same 0 results with or without escape \ As a comparison running: Location.search(""motors"" analyzer: 'whitespace') debug: search.DefaultSearchMethod query: [+motors +(alias:Location)] [1] hits took [0] millis Thank you for the time you spent going back and forth it was very useful. @Micor  any luck to solve this issue."
189,A,"Apache Lucene: Is Relevance Score Always Between 0 and 1? Greetings I have the following Apache Lucene snippet that's giving me some nice results: int numHits=100; int resultsPerPage=100; IndexSearcher searcher=new IndexSearcher(reader); TopScoreDocCollector collector=TopScoreDocCollector.create(numHitstrue); Query q=parser.parse(queryString); searcher.search(qcollector); ScoreDoc[] hits=collector.topDocs(0*resultsPerPageresultsPerPage).scoreDocs; Results r=new Results(); r.length=hits.length; for(int i=0;i<hits.length;i++){ Document doc=searcher.doc(hits[i].doc); double distanceKm=getGreatCircleDistance(lucene2double(doc.get(""lat"")) lucene2double(doc.get(""lng"")) Double.parseDouble(userLat) Double.parseDouble(userLng)); double newRelevance=((1/distanceKm)*Math.log(hits[i].score)/Math.log(2))*(0-1); System.out.println(hits[i].doc+""\t""+hits[i].score+""\t""+doc.get(""content"")+""\t""+""Km=""+distanceKm+""\trlvnc=""+String.valueOf(newRelevance)); } What I want to know is hits[i].score always between 0 and 1? It seems that way but I can't be sure. I've even checked the Lucene documentation (class ScoreDocs) to no avail. You'll see I'm calculating the log of the ""newRelevance"" value which is based on hits[i].score. I need hits[i].score to be between 0 and 1 because if it is below zero I'll get an error; above 1 and the sign will change from negative to positive. I hope some Lucene expert out there can offer me some insight. Many thanks FWIW [cosine similarity](http://en.wikipedia.org/wiki/Cosine_similarity) is always in [01]. Lucene uses a modified form of this which may deviate in complex ways from the theory. The scores are between 1 and 0 but the top score does not have to be 1. Scores are always relative to one another and a direct comparison should not really be made between scores of two different queries.  I believe that Lucene scores are always normalised i.e. the top-scoring hits get 1 (or near to it). The values should then always be between 0 and 1. By extension this means that the scores have no objective meaning i.e. they cannot be compared with anything other than other hits from the same result set. Disclaimer: I am not a Lucene Scientist. This is based only on my observations of Lucene in action though I've never seen this actually documented so I may have got completely the wrong end of the stick. Thanks for the reply. That's what I thought. I'd like to firm it up with something official though... This is a critical part of my app!  Yes the score will always be between 0 and 1. When Lucene calculates the score it finds individual scores for term hits within fields etc... and totals them. If the highest ranked hit has a total greater than 1 all of the document scores are normalised to be between 0 and 1 with the highest ranked document having a score of 1. If however no document's total was greater than 1 no normalisation occurs and the scores are returned as-is. This is why sometimes the top document has a score of 1 and other times has a score lower than 1. EDIT: Having done a bit more research the answer is most likely no. In the version of Lucene I am familiar with (v2.3.2) searches pass through the Hits object whose GetMoreDocs() method normalises scores if any of them are greater than 1. In later versions it appears to be that this is not the case as the Hits class is no longer used. Whether your scores will be between 0 and 1 will depend on which version of Lucene you are using and which mechanism is being used to search. To quote from the Lucene mailing list: The score is an arbitrary number > 0. It's not normalized to anything it should only be used to e.g. sort the results I'm using Lucene 2.9.2. I hope it's between 0 and 1. If the relevance can go over 1 I'll have to look at using something other than logarithms. Here's a hopefully better link to the same mailing thread: http://www.lucidimagination.com/search/document/ea3f9d1167fce259/range_score_in_lucene#3969a7cd3b7b7681 . Basically you are trying to combine the distance with the score which is a tough problem. I guess you can try custom sorting with some weights and see how it works."
190,A,"How to use a Lucene Analyzer to tokenize a String? Is there a simple way I could use any subclass of Lucene's Analyzer to parse/tokenize a String? Something like: String to_be_parsed = ""car window seven""; Analyzer analyzer = new StandardAnalyzer(...); List<String> tokenized_string = analyzer.analyze(to_be_parsed); That's a pretty vague question you're asking. The answer is ""Yes"". But it depends a lot on *how* you want to parse/tokenize said string. @stevevls added an example. I used List but it doesn't have to be necessarly a List. As far as I know you have to write the loop yourself. Something like this (taken straight from my source tree): public final class LuceneUtils { public static List<String> parseKeywords(Analyzer analyzer String field String keywords) { List<String> result = new ArrayList<String>(); TokenStream stream = analyzer.tokenStream(field new StringReader(keywords)); try { while(stream.incrementToken()) { result.add(stream.getAttribute(TermAttribute.class).term()); } } catch(IOException e) { // not thrown b/c we're using a string reader... } return result; } } That's exactly what I was looking for. Thank you. Just one more note: As of Lucene 3.2 TermAttribute is deprecated in favor of CharTermAttribute. Ah...good to know. I haven't upgraded Lucene in a while Thanks! Please spell out what AFAIK means. thank you.  Based off of the answer above this is slightly modified to work with Lucene 4.0. public final class LuceneUtil { private LuceneUtil() {} public static List<String> tokenizeString(Analyzer analyzer String string) { List<String> result = new ArrayList<String>(); try { TokenStream stream = analyzer.tokenStream(null new StringReader(string)); stream.reset(); while (stream.incrementToken()) { result.add(stream.getAttribute(CharTermAttribute.class).toString()); } } catch (IOException e) { // not thrown b/c we're using a string reader... throw new RuntimeException(e); } return result; } } In Lucene 4.1 you also need to add `stream.reset()` before the `while` statement @prestomanifesto: saved my day :-) You may want to add a `stream.end(); stream.close();` after the while slope."
191,A,"Using Elastic Search to Query inside arrays in CouchDB My CouchDB database is structured like this:  ""custom_details"": {""user_education"": [{""device_id"": ""358328030246627""""college_name"": ""College""""college_year"": ""2014""}]} ""custom_details_1"": {""user_education"": [{""device_id"": ""358328030246627""""college_name"": ""College""""college_year"": ""2014""}]} I have a lot of arrays within arrays. What I'm trying to do use Elastic Search to search and find terms regardless of where it's sitting in an array. Is that possible? I've been going through the examples on here and haven't quite found what I'm looking for. I've tried using Elastica the PHP Wrapper but without fully understanding how to do this with REST I'm lost. Is it even possible to search for data without knowing the field? In Lucene you could create a document instance for each device id: public void indexRecord(CouchDBRecord rec) { Document doc = new Document(); doc.add(new Field(""device_id"" rec.device_id Store.YES Index.NOT_ANALYZED)); doc.add(new Field(""college_name"" rec.college_name Store.YES Index.ANALYZED)); doc.add(new Field(""college_year"" rec.college_year.toString() Store.YES Index.NOT_ANALYZED)); this.writer.addDocument(doc); } This will allow you to search by keywords in the college name or by exact device id or year or some combination thereof.  If you are using Elastica the whole REST thing is already done for you. If you want to search in all fields you can define just the search term and it will search in all fields. If you are having some troubles with Elastica or if some features are missing you need let me know as I'm the developer of Elastica. hi Nicolas Ruflin i am working on elastic search with couchdb . i have a little bit confusion  how can i manage index for multiple users at a time . for one user at time its working perfectly but when i tried it for multiple user at same time it was having warning (org.apache.commons.httpclient.SimpleHttpConnectionManager getConnectionWithTimeout WARNING: SimpleHttpConnectionManager being used incorrectly. Be sure that HttpMethod.releaseConnection() is always called and that only one thread and/or method is using this connection manager at a time.)"
192,A,Setting lucene jar files in java classpath I'm new to lucene and is having trouble getting started. Following the beginners guide at http://lucene.apache.org/java/3_3_0/demo.html i'm trying to set the classpath copying the syntax from http://download.oracle.com/javase/1.3/docs/tooldocs/win32/classpath.html. this is what I entered in the command line: C:\Users\k>java -classpath C:\Users\k\Downloads\lucene-3.3.0\contrib\demo\lucene-demo-3.3.0.jar;C:\Users\k\Downloads\lucene-3.3.0\lucene-core-3.3.0.jar It returns a list of options usable with the java keyword. What am i doing wrong ? The command you are using is not for setting class path. It is the java command used to run java class file. You are providing it a class path arguments which determines from where to load class files. To set classpath use this command on windows: set CLASSPATH=classpath1;classpath2... So if you want to still use java command with -classpath argument then specify a class name at the end of command which is the class going to be run like C:\Users\k>java -classpath C:\Users\k\Downloads\lucene-3.3.0\contrib\demo \lucene-demo-3.3.0.jar;C:\Users\k\Downloads\lucene-3.3.0\ lucene-core-3.3.0.jar MyClassName  You need something along the lines of C:\Users\k>java -classpath C:\Users\k\Downloads\lucene-3.3.0\contrib\demo\lucene-demo-3.3.0.jar;C:\Users\k\Downloads\lucene-3.3.0\lucene-core-3.3.0.jar org.apache.lucene.demo.IndexFiles -docs {path-to-lucene}/src It looks like you set the classpath correctly all you needed to do after that was org.apache.lucene.demo.IndexFiles which tells the JVM which is the main class of the application and -docs {path-to-lucene}/src is an argument passed into the lucene demo.
193,A,"configuring nutch regex-normalize.xml I am using the Java-based Nutch web-search software. In order to prevent duplicate (url) results from being returned in my search query results I am trying to remove (a.k.a. normalize) the expressions of 'jsessionid' from the urls being indexed when running the Nutch crawler to index my intranet. However my modifications to $NUTCH_HOME/conf/regex-normalize.xml (prior to running my crawl) do not seem to be having any effect. How can I ensure that my regex-normalize.xml configuration is being engaged for my crawl? and What regular expression will successfully remove/normalize expressions of 'jsessionid' from the url during the crawl/indexing? The following is the contents of my current regex-normalize.xml: <?xml version=""1.0""?> <regex-normalize> <regex> <pattern>(.*);jsessionid=(.*)$</pattern> <substitution>$1</substitution> </regex> <regex> <pattern>(.*);jsessionid=(.*)(\&amp;|\&amp;amp;)</pattern> <substitution>$1$3</substitution> </regex> <regex> <pattern>;jsessionid=(.*)</pattern> <substitution></substitution> </regex> </regex-normalize> Here is the command that I am issuing to run my (test) 'crawl': bin/nutch crawl urls -dir /tmp/test/crawl_test -depth 3 -topN 500 What version of Nutch are you using? I'm not familiar with Nutch but the default download of Nutch 1.0 already contains a rule in regex-normalize.xml which seems to handle this problem. <!-- removes session ids from urls (such as jsessionid and PHPSESSID) --> <regex> <pattern>([;_]?((?i)l|j|bv_)?((?i)sid|phpsessid|sessionid)=.*?)(\?|&amp;|#|$)</pattern> <substitution>$4</substitution> </regex> Btw. regex-urlfilter.txt seems to contain something of relevance too # skip URLs containing certain characters as probable queries etc. -[?*!@=] Then there are some settings in nutch-default.xml which you might want to check out urlnormalizer.order urlnormalizer.regex.file plugin.includes If that all doesn't help maybe this does: How can I force fetcher to use custom nutch-config? No problem. Now just consider up-voting and accepting my answer I am using Nutch version 0.8.1. This version has the following setting in nutch-default.xml: urlnormalizer.class ...instead of urlnormalizer.order I changed the value from org.apache.nutch.net.BasicUrlNormalizer to org.apache.nutch.net.RegexUrlNormalizer. This is what causes the regex-normalize.xml file to actually be engaged when crawling. Also I added the following plugin to the 'plugin-includes' value: urlnormalizer-(pass|regex|basic) This is not included by default in version 0.8.1. Thanks soo much for pointing me in the right direction!"
194,A,Failed to load Main-Class manifest Lucene Spellchecker I'm building a spellchecker using Lucene and getting a Failed to load Main-Class manifest attribute from /lib/lucene-spellchecker-2.3.2.jar That file is in the right path a la: javac -Xlint:unchecked -cp lib/lucene-core-2.9.0.jar;lib/lucene-spellchecker-2.3.2.jar -source 1.5 -target 1.5 \ -d bin/ `find src/ -name *.java` What am I missing here? You might think about using Ant to invoke your compiler instead of `find`. It appears that you're on a Unix system so you need to use : instead of ; for your path separator. It looks like `SpellChecker` takes `Directory` in its only constructor so it's probably your code. You can use `FSDirectory.open(File)` to get an instance with your `File`. If I do so I get cannot find constructor SpellChecker(java.io.File). Does that mean there's something wrong with the code or with the path?
195,A,"Lucene not null query? How can we construct a query to search for particular field to be not null. field_name:* is not working. I tried field_name:[a* to z*] this works fine for english but does not cover all languages/ Any alternative suggestions? Try field:[* TO *] or field:["""" TO *]. But it's probably better to use a filter for this though. Tried both of them none works.  This is currently not supported by Lucene. See this for a discussion. An alternative option may be to store some pre-defined string (like nullnullnullnull) as the field value if it is null. Then you can use a negative filter to remove these records. (I don't like this much but can't think of a better option) Thank you that helps.  I found this to work in some cases field:([0 TO 9] [a TO z])  I have just started to play around with lucene (via logstash elastic search) and find that this seems to work from the kibana UI. I am not sure yet if this is some intelligence in elastic search or kibana i just know that elastic search borrows from the lucene syntax. application:unit-test && !exception will return all results from my unit tests which have not had an exception application:unit-test && exception returns those which have a non null exception indexed. so you might try just field or !field  I was having the same problem but there's a property you can set on the query parser which lets you have wildcard characters at the start of a search term. queryParser.setAllowLeadingWildcard(true); This solved the problem for me Please see Wildcard at the Beginning of a searchterm -Lucene"
196,A,"Apache Solr multiple phrase queries with OR I am using Apache Solr 1.4 with dismax and I am trying to execute a search for the following two phrases: ""call number"" ""dewey decimal"" I want to match documents that contain either of those phrases. I get matches if I search for those phrases separately but not together. I tried queries like: title:(""call number"" OR ""dewey decimal"") title:[""call number"" TO ""dewey decimal""] Any ideas? Did you also test the following query ? (title:""call number"" OR title:""dewey decimal"") How would this work for keyword searches? If the keywords come in from the user like this: q=""call number"" or ""dewey decimal"" I've tried adding phrase fields and playing with the phrase slop but can't seem to get this to work with keywords."
197,A,"Katta Execution Error Hi I tried bin/katta search IndexName ""Query"" the above one is works fine and bin/katta search IndexName ""Query"" 100 which is also works fine I write a program ILuceneClient client = new LuceneClient(); client.count(....) //works fine But the below one is throwing exception Hits hits = client.search(querynew String[] { _kattaIndexName }); ================================================================================== 11/03/22 07:43:36 WARN client.NodeInteraction:159 - Failed to interact with node hadoop5:20000. Trying with other node(s) [hadoop4:20000 hadoop1:20000] (id=6) java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at net.sf.katta.client.NodeInteraction.run(NodeInteraction.java:135) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:619) Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: Multithread shard search could not be executed: at net.sf.katta.lib.lucene.LuceneServer.search(LuceneServer.java:416) at net.sf.katta.lib.lucene.LuceneServer.search(LuceneServer.java:261) at net.sf.katta.lib.lucene.LuceneServer.search(LuceneServer.java:235) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953) I met with a similar problem when using eclipse in windows as a client. I found out that I have used a different version of lucence core as that on the katta cluster's lib. It seems that Hadoop IPC is very sensitive to versioning. So you should make sure that your eclipse has used the same version of lucene core as that your katta lib contains."
198,A,"MySql Full text or Sphinx or Lucene or anything else? I am currently using MySql and have a few tables which i need to perform boolean search on. Given the fact my tables are Innodb type I found out one of the better ways to do this is to use Sphinx or Lucene. I have a doubt in using these my queries are of the following format Select count(*) as cnt DATE_FORMAT(CONVERT_TZ(wrdTrk.createdOnGMTDate'+00:00':zone)'%Y-%m-%d') as dat from t_twitter_tracking wrdTrk where wrdTrk.word like (:word) and wrdTrk.createdOnGMTDate between :stDate and :endDate group by dat; the queries have a date field which needs to be converted to the timezone of the logged in user and then the field used to do a group by. Now if i migrate to Sphinx/lucene will I be able to get a result similar to the query above. Am a beginner in Sphinx which of these two should i use or is there anything better. Actually groupby and search ' wrdTrk.word like (:word)' is a major part of my query and I need to move to boolean search to enhance user experience. My database has approximately 23652826 rows and the db is Innodb based and MySql full text search doesnt work. Regards Roh only you need to index the data properly and will got the result can you shed more light on this. sure if you want to use lucene then you can use the zend lucene and index your search data first and then you can preform the search This is really a comment not an answer to the question. Please use ""add comment"" to leave feedback for the author.  Yes. Sphinx can do this. I don't know what LIKE (:word) does but you can do a query like @word ""exactword"" in sphinx search.  Since you only need the counts I believe it would be better for you to keep using MySQL. If you have a performance problem I suggest you use explain() and possibly better indexing to improve your queries. Only if full-text search is a major part of your use-case you should move to using Sphinx/Solr. Read Full Text Search Engine versus DBMS for a more comprehensive answer. Actually groupby and search ' wrdTrk.word like (:word)' is a major part of my query and I need to move to boolean search to enhance user experience. My database has approximately 23652826 rows and the db is Innodb based and MySql full text search doesnt work.  save your count in a meta table keep it updated. or use myisam which maintains its own count. mongodb also maintains its own count. cache the count in memcache. counting each time you need to know the count is a silly use of resources."
199,A,"How to map a component collection with compass? I need to map a collection of components with compass (using XML mapping)... Is there any way to achieve this? Thanks in advance for any suggestions. Example classes: class ClassA { private Set<ClassB> bs; // ... getBs/setBs ... } class ClassB {} Example mapping: <class name=""com.package.ClassA"" alias=""classA""> <!-- no idea how I can map Set<ClassB> in here... can I? --> </class> <class name=""com.package.ClassB"" alias=""classB""> </class> Yeah just found out how to do this the mapping is simple - you just apply the alias to the collection component/reference. Obviously all the rest is done implicitely. <class name=""com.package.ClassA"" alias=""classA""> <component name=""bs"" ref-alias=""classB"" /> </class> <class name=""com.package.ClassB"" alias=""classB""> </class>"
200,A,Nested BooleanQuery? I'm using a BooleanQuery to combine several queries. I find that if I add a BooleanQuery to the BooleanQuery then no result is returned. The added BooleanQuery is a MUST_NOT one like -city_id:100. But as lucene's spec says BooleanQuery could be nested which I think means it's okay to add such BooleanQuery. Now I have to get all clauses from the BooleanQuery to be added and then add them to the container BooleanQuery one by one. I'm a bit confused. Anybody could help? Thank you very much! Lucene does not support unary NOT operator. But you can get results for such query by ANDing it with MatchAllDocsQuery. Apologies for the broken sentences in the previous comment. Ah it seems work. Thanks. But isn't this confusing? I add BooleanQuery to BooleanQuery and still I need to check whether the query to be added has AND some query. Also does ANDing MatchAllDocsQuery in this case undermine the performance? You can read about the boolean query idiosyncracies on this thread. http://search-lucene.com/m/8x64lENo571/ Essentially NOT is just suppresses documents. You need another positive set for it to work with. I don't know the details of implementation of MatchAllDocsQuery but gettig all docs should be fast enough. The addtional ANDing operation should be fast as well as internally it operations on a bitset. Thanks for this tip it helped me to solve a huge problem!
201,A,"Inflectional forms of verbs using DBsight lucene? I know dbsight allows synonyms and stop words for searching but does this take care of inflectional forms of a verb too e.g. for 'swim' it should find swim swims swimming swam and swum Link on DBSight Wiki : http://wiki.dbsight.com/index.php?title=User%5Fdictionary The behavior you are looking for can be implemented using lemmatization. I am unaware of an existing Lucene analyzer that does this. Basis Tech's Lucene package does lemmatization but is not free and I do not know whether it works with dbsight. Thanks Yuval for pointing this out. Reading the lemmatization wiki it seems like a stemmer would work too for me. They actually have link for Lucene Snowball Stemmer (http://e-mats.org/2009/05/modifying-a-lucene-snowball-stemmer/) but I'm not sure how that'll work with DBsight My Bad :) I just found the answer to my question at http://www.dbsight.net/index.php?q=node/395 Seems like DBsight comes with analyzers as Snowball-English ( Snowball-Language) Note that stemming != lemmatization. A stemmer may convert 'swimming' into 'swim' but not 'swam' into 'swim'. Yep you're right they're not equal. In the first step I plan on using stemming and see if it takes care of most searches. If not then definitely I'll look into lemmatization solutions.  Lucene comes with a stemmer called ""Lucene SnowBall stemmer' (http://lucene.apache.org/java/2%5F4%5F0/api/contrib-snowball/index.html). Turns out that DBsight is exposing it as analyzers named SnowBall - [Language] e.g SnowBall - English SnowBall - French etc.."
202,A,Scalable Full Text Search With Per User Result Ordering What options exist for creating a scalable full text search with results that need to be sorted on a per user basis? This is for PHP/MySQL (Symfony/Doctrine as well if relevant). In our case we have a database of workouts that have been performed by users. The workouts that the user has done before should appear at the top of the results. The more frequently they've done the workout the higher it should appear in search matches. If it helps you can assume we know the number of times a user has done a workout in advance. Possible Solutions Sphinx - Use Sphinx to implement full text search do all the querying and sorting in MySQL. This seems promising (and there's a Symfony Plugin!) but I don't know much about it. Lucene - Use Lucene to perform full text search and put the users' completions into the query. As is suggested in this Stack Overflow thread. Alternatively use Lucene to retrieve the results then reorder them in PHP. However both solutions seem clunky and potentially unscalable as a user may have completed hundreds of workouts. Mysql - No native full text support (InnoDB) so we'd have use LIKE or REGEX which isn't scalable. MySQL does have a native FULLTEXT support though only in MyISAM tables. For most real-world tasks Sphinx is the fastest engine. However it is an external index so it can only be updated on a timely basis with a cron script. By using SphinxSE (a pluggable MySQL interface to Sphinx) you can join MySQL tables and Sphinx indexes in one query. Updating though will still require an external script. Since the number of workouts performed seems to change frequently keeping it in Sphinx would require too much effort on rebuilding the index. With SphinxSE you can write a query similar to that: SELECT * FROM workouts w JOIN user_workouts uw ON uw.workout = w.id WHERE w.query = 'query query query;filter=user_id$user_id' AND uw.user = $user_id ORDER BY uw.times_performed DESC  I'm not sure why you're assuming using Lucene would be unscalable. Hundreds of workouts per user is not a lot of data to deal with. Try using Solr/Lucene for the search backend. It has a JSON/XML interface which will play nicely with your PHP frontend. Store a user's completed workout # in a database table. When a query is issued take the results from Solr and you can select from the database table and resort in PHP code. Should be plenty fast and scalable. With Solr maintaining your index is dirt simple; just issue add/update/delete requests to your Solr server. The user has only completed a few hundred but in total we need to support hundreds of thousands of workouts. Are you suggesting we use Solr/Lucene to search the millions of workouts and then reorder the results? Some searches can return a large portion of the records.
203,A,"php mysql fulltext search: lucene sphinx or? This is admittedly similar to (but not a duplicate of) http://stackoverflow.com/questions/737275/pros-cons-of-full-text-search-engine-lucene-sphinx-postgresql-full-text-search however what I am looking for are specific supported recommendations from the benefit of experience with more than one of the available systems (there seems to be a lot of: ""I've used lucene but not sphinx"" and vice a versa). The setup: Standard LAMP (Mysql 5.0 PHP 5). MySQL: tables are using the InnoDB engine for foreign key constraints We are looking at indexing data not pages. data to be indexed may be in multiple languages (utf-8 charset) A number of the comparisons I've come across (like http://blog.evanweaver.com/articles/2008/03/17/rails-search-benchmarks/) are either not entirely applicable (ferret is a lucene port but not the same as Zend_Search_Lucene) or they are pushing their own systems/implementations (not exactly unbiased). Some others I've come across (such as http://whatstheplot.com/blog/tag/lucene/ and http://pagetracer.com/2008/02/15/sphinx-and-lucene-search-engines-first-impressions/) provide very different results for performance of the two systems. Also all but ignored in much of what I've read is Xapian. Might this be worth consideration as well? So... I'm hoping that some of you here on SO have some experience with this question and could help with some recommendations or point me in the right direction. I looked at Zend_Search_Lucene and Sphinx for a project that sounds similar - searching database content (in my case book information). I spent about a day looking at each. For what it's worth I found Sphinx vastly easier to set up and use. +1 thanks for the insight. I've been pleasantly surprised with Sphinx so far it has made the integration of search far easier than I expected. Once the db has more data will we be able to know more. I haven't tried delta index merges yet but hopefully they'll be as easy to implement as they look  One advantage of sphinx is that you can ""interpose"" it between your clients and the MySql server and it will only ""interfere"" on queries specifically addressing it transparently bouncing the others off MySQL -- see e.g this URL. Whether that's an advantage in your use case you're best placed to say! Sorry no real-life experience w/Xapian or Lucene -- still reading about how to deploy them makes it sound like (to me!-) as if it might be worth it only if you identified substantial advantages... otherwise sphinx's ""easy as pie"" deployment as a ""proxy"" between your clients and your MySQL server feels like a big substantial win to me!-) Sphinx appears to have a lot of advantages but given that lucene has some rather vocal advocates I was hoping to hear from some folks with experience with both In the end I went ahead and tested sphinx with the intention of testing Lucene (and perhaps Xapian) however honestly it integrated so smoothly with a PHP/MySQL setup that I'd find it hard to justify spending the time on the others."
204,A,"Match whole field in Lucene I'm currently indexing a database with lucene. I was thinking of storing the table id in the index but I can't find a way to retrieve the documents by that field. I guess some pseudo-code will further clarify the question: document.add(""_id"" 7 Field.Index.UN_TOKENIZED Field.Store.YES); // How can I query the document with _id=7 // without getting the document with _id=17 or _id=71? Which version of the Lucene API are you using? Which method are you using (Document.add() takes a Fieldable as of Lucene 2.4)? I'm actually using the php port (provided by Zend) and wasn't aware that this could make a difference as the query syntax should be the same. The query syntax is different. The principle remains. Being a port to php Zend currently supports Lucene 2.3 which is about two versions behind the current Java Lucene version. Ok many thanks I didn't know that. Just to say I've just implemented this successfully on my Zend Lucene search engine. However after some time troubleshooting I discovered that the field name and field value are the opposite way around to the way shown. To correct the example: // Fine - no change here $doc->addField(Zend_Search_Lucene_Field::Keyword('_id' '7')); // Reversed order of parameters $idTerm = new Zend_Search_Lucene_Index_Term('7' '_id'); $idQuery = new Zend_Search_Lucene_Search_Query_Term($idTerm); I hope that helps someone!  EDIT for Zend Lucene: You will need a Keyword type field in order for it to be searched. For indexing use something like: $doc->addField(Zend_Search_Lucene_Field::Keyword('_id' '7')); For search use: $idTerm = new Zend_Search_Lucene_Index_Term('_id' '7'); $idQuery = new Zend_Search_Lucene_Search_Query_Term($idTerm);"
205,A,"Lucene.NET Faceted Search I am building a faceted search with Lucene.NET not using Solr. I want to get a list of navigation items within the current query. I just want to make sure I'm pointed in the right direction. I've got an idea in mind that will work but I'm not sure if it's the right way to do this. My plan at the moment is to create hiarchry of all available filters then walk through the list using the technique described here to get a count for each excluding filters which produce zero results. Does that sound alright or am I missing something? yeah. you're missing solr. the math they used behind doing faceted searching is very impressive there is almost no good reason to not use it. the only exception i can find is if your index is small enough you can roll your own theory behind it otherwise its a good idea to stand on their shoulders. ""not using Solr"" This is a .NET application it already has Lucene.NET indexing + searching. I'm just trying to add faceted navigation. Seems like using a shotgun to swat a fly to me.  Ok so I finished my implementation. I did a lot of digging in the Lucene and Solr source code in the process and I'd recommend not using the implementation described in the linked question for several reasons. Not the least of which is that it relies on a depreciated method. It is needlessly clever; just writing your own collector will get you faster code that uses less RAM."
206,A,"How to search Lucene.NET without indicating ""top n"" hits limit? There are several overloads of IndexSearcher.Search method in Lucene. Some of them require ""top n hits"" argument some don't (these are obsolete and will be removed in Lucene.NET 3.0). Those which require ""top n"" argument actually cause memory preallocation for this entire posible range of results. So when you're in situation when you can't even approximately estimate count of results returned the only opportunity is to pass a random large number to ensure that all query results will be returned. This causes severe memory pressure and leaks due to LOH fragmentation. Is there an oficial not outdated way to search without passing ""top n"" argument? Thanks in advance guys. I'm using Lucene.NET 2.9.2 as reference point for this answer. You could build a custom collector which you pass to one of the search overloads. using System; using System.Collections.Generic; using Lucene.Net.Index; using Lucene.Net.Search; public class AwesomeCollector : Collector { private readonly List<Int32> _docIds = new List<Int32>(); private Scorer _scorer; private Int32 _docBase; public IEnumerable<Int32> DocumentIds { get { return _docIds; } } public override void SetScorer(Scorer scorer) { _scorer = scorer; } public override void Collect(Int32 doc) { var score = _scorer.Score(); if (_lowerInclusiveScore <= score) _docIds.Add(_docBase + doc); } public override void SetNextReader(IndexReader reader Int32 docBase) { _docBase = docBase; } public override bool AcceptsDocsOutOfOrder() { return true; } } That's a quite valuable piese of information on how Lucene works internally. Thank you for your time and effort Simon. PS: As to the field cache I guess we will follow your advice on building a custom FieldCache it seems like the most appropriate solution if we have to perform sorting (using native lucene mechanics) on a large index. Thank you for your suggestion. We have actually been using Collector in pretty much the same way with the only difference in using LinkedList instead of List to prevent memory reallocation on growth. This approach works great when there's no need to do Sorting. There's no Search() overload which receives both Collector and Sort objects. When using Sort we force Lucene to use default TopHitsCollector which preallocates memory in a way described. Maybe it would be a good idea to use custom collector which does it's own sorting on Coolect call. What do you think? I would change it into storing both document id and the sort value within the list and do the sorting when all results have been collected. You can use the FieldCache if you have a single keyword field as a sort field it will load (and cache) field values per segment. You _must_ use the inner reader (the one passed to you in SetNextReader) for the cache to work properly. Yes I guess this would be the best way to do it with an exception of using Field cache. This thing eats up a lot of memory in a large index and since we have to re-open it quite often I would prefer not to load all field data from index into memory just for the sake of sorting a few hundred rows. So it seems that your advice is basically an answer to my question. Thank you Simon :) Calling .Reopen on an IndexReader will reuse already opened segments and as such already opened SegmentReaders. This means that the FieldCache will already have the items in memory and no disk access is needed to retrieve the sort values. But it really consumes alot of memory yes. I can't really use Reopen since it reopens index at a specific commit point. Our production index is being frequently updated and can be randomly rebuilt. Actually it was memory pressure that pushed me to dig into Sorting and FieldCache mechanics. I'm not sure if it's true yet but most likely allocation of large memory chunks for FieldCache on every index re-opening caused LOH fragmentation. So for now I think I'll try to avoid it and see if it helps. Uhm. Reopen() will return a fresh IndexReader from the source used to create the original reader (including an IndexWriter for near-realtime-updates). It will return the same instance if no changes have occurred and reuse any segments that hasn't changed. Remember that the IndexReader you get from IndexReader.Open really is an DirectoryReader composed of several SegmentReaders. SegmentReaders are reused as long as the segment still exists (segments are only created and deleted never updated). And a quick note again DO NOT use the IndexReader retrieved by IndexReader.Open of you call the FieldCache stuff directly. It will use the reader you pass to it as a cache key and every index change will cause it to reread everything as it considers it a new cache key. You SHOULD use the SegmentReaders the DirectoryReader consists of which can be retrieved with IndexReader.GetSequentialSubReaders(). These restart their document numbering at 0 so you need to calculate their real document ids by using IndexReader.MaxDoc() from the previous readers. Use the ReaderUtil class as reference. And another note you could build your own FieldCache which doesn't store everything in giant arrays to avoid hitting the LOH. Or do you want to avoid loading the stuff into memory at all?"
207,A,"Running Long Process: Indexing 5GB docs with Lucene Situation:I have an ASP .NET application that will search through docs using Lucene. I want to run the initial indexing (the index will be incremental after the initial run so there wont be need to index the whole directory again in future). Currently I have about 5GB of docs (45000files). Problem: My application times out before completing the process. I have altered the TimeOut like this: HttpContext.Current.Server.ScriptTimeout = 200000; but it still does not complete the process. How can I run the index? What kind of timeout error does it give? I ask because usually when I'm trying to prevent a timeout there are frequently a number of ways to timeout and you need to prevent them all. Ben has a valid point however. If you don't have to do it in a web page don't. You shouldn't run these from ASP.NET. Create a service that does the work for you and kick it off via MSMQ a ""work_queue"" table in the database or whatever makes sense in your scenario. Web Requests are intended to perform work quickly and immediately return rather than do batch processing. +1 for Ben's suggestion. Also suggest looking into Solr it can take care of several issues for you. You can query Solr from ASP.NET using SolrNet. @Mikos - On one of the projects I could not use Solr because the site was hosted elsewhere. But I have now used Nutch Solr and SolrNet in my latest project and it works a treat. The documentation is not the best but the results are exactly what i was looking for."
208,A,Can someone give me a high overview of how lucene.net works? I have an MS SQL database and have a varchar field that I would like to do queries like where name like '%searchTerm%'. But right now it is too slow even with sql enterprise's full text indexing. Can someone explain how Lucene .Net might help my situation? How does the indexer work? How do queries work? What is done for me and what do I have to do? I saw this guy (Michael Neel) present on Lucene at a user group meeting - effectively you build index files (using Lucene) and they have pointers to whatever you want (database rows whatever) http://code.google.com/p/vinull/source/browse/#svn/Examples/LuceneSearch Very fast flexible and powerful. What's good with Lucene is the ability to index a variety of things (files images database rows) together in your own index using Lucene and then translating that back to your business domain whereas with SQL Server it all has to be in SQL to be indexed. It doesn't look like his slides are up there in Google code. Slides and code: http://code.google.com/p/vinull/source/browse/#svn/Presentations/Lucene  This article (strangely enough it's on the top of the Google search results :) has a fairly good description of how the Lucene search could be optimised. Properly configured Lucene should easily beat SQL (pre 2005) full-text indexing search. If you on MS SQL 2005 and your search performance is still too slow you might consider checking your DB setup. Voted down because link is dead. The link is dead. I provided what may be an alternate link: http://it-stream.blogspot.com/2007/12/full-text-search-for-database-using.html
209,A,"Lucene index with multiple fields of the same nature Each Lucene doc is a recipe and each of these recipes have ingredients. Im working towards being able to search the ingredients and give a result that says two ingredients matched out of four. (for example) So how can I add the ingredients to the doc? In solr I can just create multiple fields of and it would save them all I might be doing something wrong because its only saving the one ingredient. Also this would apply to a field like 'tags'. p.s Im using the Zend Framework for this if it matters at all. I see two possible approaches here: Denormalize your data - create a separate document for each ingredient in a recipe giving all of the documents for a recipe a common recipe id. Then during search aggregate all matches of a recipe id. A bit ugly. Concatenate all your ingredients into a common field and index it as 'Text'. Then search for ingredients using a boolean query with 'OR' (This is called 'Should' in Java Lucene terms I do not know the PHP equivalent). I suggest you try the second approach and see if it helps.  Lucene documents support the addition of multiple fields of the same name. i.e. you can repeatedly call: document.add(new Field(""name"") value) So were you to do : # (pseudo-code) document1.add(new Field(""ingredient"") ""vanilla"") document1.add(new Field(""ingredient"") ""strawberry"") index.add(document) # And then search for index.search(""ingredient"" ""vanilla"" && ""strawberry"") You will get back document1. But if you search for: index.search(""ingredient"" ""vanilla"" && ""apple"") You will not get back document1. If you searched for: index.search(""ingredient"" ""vanilla"" || ""apple"") You would also get back document1. If you want to see which ingredients match you can simply save the fields on the document as Stored fields and then for each matching document retrieve the list of fields and compare them to the user query. Also note by default the PositionIncrementGap for fields with the same name that are added to a document is 0. This means that if you added:  document1.add(new Field(""ingredient"") ""chocolate"") document1.add(new Field(""ingredient"") ""orange"") then it would be treated as if it were a single ingredient called ""chocolate orange"" which might match on : index.search(""ingredient"" ""chocolate orange"") You can avoid this set a value for PositionIncrementGap > 1 which will yield: 0 matches for: index.search(""ingredient"" ""chocolate orange"") and 1 match for: index.search(""ingredient"" ""chocolate"" && ""orange"")"
210,A,"Any tutorial or code for Tf Idf in java I am looking for a simple java class that can compute tf-idf calculation. I want to do similarity test on 2 documents. I found so many BIG API who used tf-idf class. I do not want to use a big jar file just to do my simple test. Please help ! Or atlest if some one can tell me how to find TF? and IDF? I will calculate the results :) OR If you can tell me some good java tutorial for this. Please do not tell me for looking google I already did for 3 days and couldn't find any thing :( Please also do not refer me to Lucene :( agazerboy Sujit Pal's blog post gives a thorough description of calculating TF and IDF. WRT verifying results I suggest you start with a small corpus (say 100 documents) so that you can see easily whether you are correct. For 10000 documents using Lucene begins to look like a really rational choice.  While you specifically asked not to refer Lucene please allow me to point to you the exact class. The class you are looking for is DefaultSimilarity. It has an extremely simple API to calculate TF and IDF. See the java code here. Or you could just implement yourself as specified in the DefaultSimilarity documentation.  TF = sqrt(freq) and  IDF = log(numDocs/(docFreq+1)) + 1. The log and sqrt functions are used to damp the actual values. Using the raw values can skew results dramatically. Hi Shashikant Thanks for your reply. Well in my question I said I do not want to use Lucene. Thanks for telling me the exact class. But I am 100% sure I can not use that class without Lucene :). Anyother tip? Also please read my comments above ! I have given pointer to the java code. You can simply copy-paste tf() and idf() methods in your own class. It doesn't have any other dependencies. Hi Shashikant Thanks I wrote my question in more detail I hope you can answer me. Thanks for pointing out two methods but I already know this formula type methods. so here are 2 simple questions 1 - How can i count each word? that how many time it appears in all documents? 2- When I will have TF/IDF matrix. How will I know which 2 documents are more similar than others (becoz I will have like 10000 XY metrics and manual verification is impossible? should I use clustoring? If yes then which one?  Term Frequency is the square root of the number of times a term occurs in a particular document. Inverse Document Frequency is (the log of (the total number of documents divided by the number of documents containing the term)) plus one in case the term occurs zero times -- if it does obviously don't try to divide by zero. If it isn't clear from that answer there is a TF per term per document and an IDF per term. And then TF-IDF(term document) = TF(term document) * IDF(term) Finally you use the vector space model to compare documents where each term is a new dimension and the ""length"" of the part of the vector pointing in that dimension is the TF-IDF calculation. Each document is a vector so compute the two vectors and then compute the distance between them. So to do this in Java read the file in one line at a time with a FileReader or something and split on spaces or whatever other delimiters you want to use - each word is a term. Count the number of times each term appears in each file and the number of files each term appears in. Then you have everything you need to do the above calculations. And since I have nothing else to do I looked up the vector distance formula. Here you go: D=sqrt((x2-x1)^2+(y2-y1)^2+...+(n2-n1)^2) For this purpose x1 is the TF-IDF for term x in document 1. Edit: in response to your question about how to count the words in a document: Read the file in line by line with a reader like new BufferedReader(new FileReader(filename)) - you can call BufferedReader.readLine() in a while loop checking for null each time. For each line call line.split(""\\s"") - that will split your line on whitespace and give you an array of all of the words. For each word add 1 to the word's count for the current document. This could be done using a HashMap. Now after computing D for each document you will have X values where X is the number of documents. To compare all documents against each other is to do only X^2 comparisons - this shouldn't take particularly long for 10000. Remember that two documents are MORE similar if the absolute value of the difference between their D values is lower. So then you could compute the difference between the Ds of every pair of documents and store that in a priority queue or some other sorted structure such that the most similar documents bubble up to the top. Make sense? So...let me know if you have any questions or anything. Danben Thank you so much for your answer. I know what is TF and IDF and how to calculate them :). But i was looking for the tip how to implement them in java. For example I have two folders A & b and I want to read file from A to compare them with folder B files to see how much similar they are. Lets say some how I did calculate it. I got some TF/IDF ( I will surely use your formula :) ). It would be like 10000 TF/IDF matrix. How would I know which file was more simlar to whom ? ( Sorry If i am not clear please ask me ) I told you how to implement this in Java. The only part I didn't say explicitly was that files A and B are more similar than files C and D if the distance between A and B (marked as ""D"" in my answer) is less than the distance between C and D. Did you read the last part starting with ""So to do this in Java...""? If you have specific questions about it I am more than happy to answer but I feel like you didn't really read it. thank you so much danben. You are so helpfull. Your answer is best !. Just one last question. If tf/idf can tell similarity then why mostly prefer to use clustoring too at the end to see the similarity? Hi danben Thanks for quick reply. I read your answers and understand all things. But I am unable to convey my questions properly. Let me make it more short. 1 - How can i count each word? that how many time it appears in all documents? 2- When I will have TF/IDF matrix. How will I know which 2 documents are more similar than others (becoz I will have like 10000 XY metrics and manual verification is impossible? should I use clustoring? If yes then which one? Ok I have responded to these questions in the body of the answer - see above."
211,A,"Symfony and Lucene SOLVED See my answer below. Question left unchanged for anyone else who has trouble with this. I'd like to use lucene (or anything else that could be used withs symfony for searching really) however I can't get the sfLucene plugin to work (says there are no tasks in the namespace ""lucene"" when i do ./symfony lucene:initialize). What can I do to get this to work or replace lucene? symfony info: chris@linux-9r49:~/Coding/PHP/cpn> ./sf -V symfony version 1.3.0-BETA1 (/home/chris/Coding/PHP/cpn/lib/vendor/symfony/lib) When I found the answer the question was too new to accept my own solution and I sort of forgot about it :P Hi Chris no need to add 'solved' to the top of the question you can tick your own answer to mark it as the accepted answer. Silly me I didn't enable the plugin :P. For anyone else who has the problem open projectfolder/config/ProjectConfiguration.class.php and change it so it says something like this:  $this->enablePlugins('sfDoctrinePlugin' 'sfLucenePlugin'); Then do ./symfony cc ./symfony lucene:initialize appnamehere"
212,A,Solr how to remove physically deleted documents with DataImportHandler I've read solr wiki and I know it is possible to use deletedPkQuery to remove the documents which deleted logically but how to remove physically deleted documents with DataImportHandler? possible duplicate of [Solr DIH -- How to handle deleted documents?](http://stackoverflow.com/questions/1555610/solr-dih-how-to-handle-deleted-documents) You can do it by optimizing the index. Send an optimize request to Solr as described here. But beware that optimizing the index means re-writing the index. Unless you have substantial number of deleted documents in the index the overhead of optimizing can be quite high.
213,A,"List of ""tokens"" on Lucene 3 I'm new to Lucene i started learning the version 3 branch and there's one thing i don't understand (obviously because i'm not experienced in the subject). In Lucene 2.9 if i wanted a list of tokens i would create an ArrayList of Token class ArrayList for example. That's pretty intuitive for me and the concept of token is very clear. Now that the use of Token class is disencouraged in favour of the Attribute based API do i have to create my own class to encapsulate the attributes i want? If yes isn't that almost recreating the Lucene's Token class? I'm doing a class to test analyzers and having a list of resulting tokens makes it easier to test i guess. Any help would be appreciated ;) Thank you! I think you can do something like this: TokenStream tkst = analyzer.tokenStream(""field"" ""text""); Token token = tkst.getAttribute(Token.class); while (tkst.incrementToken()) { // Do something with token. } The proper documentation is in the analysis package: http://lucene.apache.org/java/3_0_2/api/all/org/apache/lucene/analysis/package-summary.html  Use the TermAttribute class: TokenStream stream = analyzer.tokenStream(""field"" ""text""); TermAttribute termAttr = stream.getAttribute(TermAttribute.class); while (stream.incrementToken()) { String token = termAttr.term(); } Thanks anwseringbut it doesn't reply to my question.I know how to get attributes from a tokenstreamin the code you're only getting termattributeso you can save each term on a string[] and there's your list of tokens. But in case you want also a offsetattributethen you have 2 attributes and can't save them both on a string[]and my question is related to that.. the Token class encapsulates various attributes in a same structure and i need to now if in Lucene 3 since they disencourage the use of Token what is the recomended solution to encapsulate various attributes in the same structure? Apparently there isn't any at least not that I know of. I've been surprised by this decision as well. The Lucene developers apparently favor optimization over proper API design.  According to the Token Javadoc ""Even though it is not necessary to use Token anymore with the new TokenStream API it can be used as convenience class that implements all Attributes which is especially useful to easily switch from the old to the new TokenStream API."" I suggest you keep using a Token. It matches the description above. Thanks i was misunderstanding the notes about Token class ;)"
214,A,"Solr DIH -- How to handle deleted documents? I'm playing around with a Solr-powered search for my webapp and I figured it'd be best to use the DataImportHandler to handle syncing with the app via the database. I like the elegance of just checking the last_updated_date field. Good stuff. However I don't know how to handle deleting documents with this approach. The way I see it I've got 2 choices. I could either send an explicit message to Solr from the client when a document is deleted or I could add a ""deleted"" flag and leave the object in the database so that Solr will notice that the document has changed and is now ""deleted."" I could add a query filter that would disregard results with the deleted flag but it seems inefficient to include all the deleted documents in the Lucene index. What do other folks do? These are your options: Use DIH special commands $deleteDocById or $deleteDocByQuery (requires Solr 1.4+) Use the clean parameter of DIH to delete the whole index before importing. Use preImportDeleteQuery to define what's going to be cleaned up before importing. (requires Solr 1.4+) Use database triggers instead of DIH to manage updating the index. If you're using some sort of ORM use its interception capabilities instead of DIH. For example you can use hibernate events to update the index on update insert or delete. yeah Solr 1.4 just went RC final release is imminent. That's a great list! I'm still using 1.3 but that's a convincing reason to look into switching. Hi Mauricio are there any good examples how to use preImportDeleteQuery when doing delta import on deleted documents? Are there illustrations about how to use special commands? Do they have to be added to row using script?  I like to have a ""deleted"" flag so I don't actually delete my data! Depends on how paranoid you are. I like Mauricio's suggestions... ""deleted"" flag = Good call!"
215,A,"What are the limitations of boolean query in Lucene? I have a requirement to find items in a Lucene index that have two basic criterion: 1. match a specific string called a 'relation' 2. fall within a list of entitlement 'grant groups' An entitlement group defines a subset of items accessible by a member of that group and is much like an authorization role. All documents in the Lucene index have the 'relation' field and for simplicity sake one or more 'grant-group' fields. So for example a user may search for 'foobar' and that user may be a member of groups a b c. foobar let's say has grant groups apqs The query will be basically ""match 'foobar' AND (a OR b OR c). This should work according to Lucene documentation. My question is this: How far can you go with the 2nd part of the boolean query namely the part after 'AND' ? The reason for asking is this: I am about to do a small feasibility study and part of the requirements is the need to support potentially MANY groups in the 'OR' clause. Possibly up to 200 or 300 groups. Would there be noticeable performance degradation ? thanks. You should measure whatever you do. I think you probably should be ok with 200-300 groups. I think the default limit of clauses in a BooleanQuery is 1024 but that can be changed as well. If you use Solr rather than straight Lucene then I would recommend putting the grant-group part as a filterQuery so that it can be cached.  From this overview of lucene performance: To put it another way: for standard disjunctive (OR'd) queries the number of clauses doesn't really affect performance except to the extent that more documents are potential matches. As Avi mentioned you will hit a limit at 1024 clauses.  I'm not sure how many elements you can specify in OR perhaps you should do a simple proof of concept just to see how it works. Apart from that if you use Solr I would not alter original query with to implement your requirements (it would affect scoring on matched documents) but would rather use 'fq' parameter (see Filter Query):"
216,A,What happened to the Spring Modules project? I read on Spring web site that the Spring Modules project is now deprecated in favor of Spring Extensions. However unless I am missing something Spring Extensions does not support Lucene like Spring Modules did. Am I missing something? No I don't think you're missing anything. Users of Spring Modules were rather left in the lurch I feel. There is an alternative to Spring Lucene though which is Compass. Rather than being a library to make lucene easier it actually provides an abstraction layer on top of Lucene. I like it I;ve used it for several lucene-based projects now it's good quality. +1 for Compass - unfortunately it is a one man show with no new releases in the past 12 month though @sfussenegger: Yes that is starting to worry me a bit.
217,A,"Lucene.NET - Can't delete docs using IndexWriter I'm taking over a project so I'm still learning this. The project uses Lucence.NET. I also have no idea if this piece of functionality is correct or not. Anyway I am instantiating: var writer = new IndexWriter(directory analyzer false); For specific documents I'm calling: writer.DeleteDocuments(new Term(...)); In the end I'm calling the usual writer.Optimize() writer.Commit() and writer.Close(). The field in the Term object is a Guid converted to a string (.ToString(""D"")) and is stored in the document using Field.Store.YES and Field.Index.NO. However with these settings I cannot seem to delete these documents. The goal is to delete then add the updated versions so I'm getting duplicates of the same document. I can provide more code/explanation if needed. Any ideas? Thanks. I don't think there is anything wrong with how you are handling the writer. It sounds as if the term you are passing to DeleteDocuments is not returning any documents. Have you tried to do a query using the same term to see if it returns any results? Also if your goal is to simple recreate the document you can call UpdateDocument: // Updates a document by first deleting the document(s) containing term and // then adding the new document. The delete and then add are atomic as seen // by a reader on the same index (flush may happen only after the add). NOTE: // if this method hits an OutOfMemoryError you should immediately close the // writer. See above for details. You may also want to check out SimpleLucene (http://simplelucene.codeplex.com) - it makes it a bit easier to do basic Lucene tasks. [Update] Not sure how I missed it but @Shashikant Kore is correct you need to make sure the field is indexed otherwise your term query will not return anything. I can't give credit to both you guys can I?  The field must be indexed. If a field is not indexed its terms will not show up in enumeration. I realized and tested this after I did a little more research. I indexed the column rebuilt it and now documents are getting deleted correctly. Thanks. I'm seeing the same problem but I don't understand what you meant by ""the field must be indexed"". Could you please clarify?"
218,A,What is faster for radial location based search? Lucene or Sphinx? I'm part of a development team working on a job board and we're considering both Lucene and Sphinx for out search base. Does anyone have experience working with either of these open-source tools for location based search? Lucene doesn't have any specialized support for spatial searches. It's an Information Retrieval system not a GIS.  True that Lucene doesn't have it built-in but Solr (a search server built using Lucene) does. Here is an article that explains how. Thanks for the comment!
219,A,"dismax solr request handler MM  PS and Q.ALT I'm testing the dismax requesthandler  im trying to customize the mm ( Minimum Match ) Parameter ( i already looked at the documentation )  <str name=""mm""> 2<-2 3<-70% 5<-50% </str> <int name=""ps"">100</int> <str name=""q.alt"">*:*</str> I have 3 Questions : mm  i understood what it does  i want to verify if my param is ok  correct me if im wrong 2<-2 3<-70% 5<-50% stands for ? : if 1 or 2 terms match 100% of them if 3 to 4 match only 70% of them if 5 or more match only 50% ps : what is this param? Is it mandatory ? q.alt : the same i did not understand the utility of that one. Can you provide me some advise for the best configuration of DisMax ? Thank you ! 1: Your string would translate to 1-2 terms: match all terms 3 terms: total number of terms - 2 must match (i.e. 1 term must match :) 4-5 terms: 70% of the terms must match 6+ terms: 50% of the terms must match mm-string to achieve your requirement would be 2<%70 4<%50 A very good resource for mm strings can be found here. 2: The pf parameter is used to boost document relevance based on query terms occurring in close proximity of each other (as opposed to being scattered all over the document). 3: The q.alt parameter is used as a fallback query for cases when the client did not provide any search terms. In my interpretation it is meant for use if you don't do any client-side query processing/transformation but there could be other practical uses for it. Thank you Karl ! i understood the mm param  for the others param i still septic for using them ;) another question : does dismax support all the features like the standard requestHandler ? stopwords ? synonymes ? stemming ? did you hear about ""edismax"" ? Thanks again !"
220,A,invalid characters for lucene text search On my IndexController i have  public function buildAction() { $index = Zend_Search_Lucene::create(APPLICATION_PATH . '/indexes'); foreach ($this->pages as $p) { $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::unIndexed('page_id' $p['page_id'])); $doc->addField(Zend_Search_Lucene_Field::text('page_name' $p['page_name'])); $doc->addField(Zend_Search_Lucene_Field::text('page_headline' $p['page_headline'])); $doc->addField(Zend_Search_Lucene_Field::text('page_content' $p['page_content'])); $index->addDocument($doc); } $index->optimize(); $this->view->indexSize = $index->numDocs(); } and i am getting error [Tue Jan 18 16:23:32 2011] [error] [client 127.0.0.1] PHP Notice: iconv(): Detected an illegal character in input string in /usr/share/php/libzend-framework-php/Zend/Search/Lucene/Analysis/Analyzer/Common/Text.php on line 58 [Tue Jan 18 16:23:32 2011] [error] [client 127.0.0.1] PHP Notice: iconv(): Detected an illegal character in input string in /usr/share/php/libzend-framework-php/Zend/Search/Lucene/Field.php on line 222 [Tue Jan 18 16:23:32 2011] [error] [client 127.0.0.1] PHP Notice: iconv(): Detected an illegal character in input string in /usr/share/php/libzend-framework-php/Zend/Search/Lucene/Analysis/Analyzer/Common/Text.php on line 58 [Tue Jan 18 16:23:32 2011] [error] [client 127.0.0.1] PHP Notice: iconv(): Detected an illegal character in input string in /usr/share/php/libzend-framework-php/Zend/Search/Lucene/Field.php on line 222 [Tue Jan 18 16:23:32 2011] [error] [client 127.0.0.1] PHP Notice: iconv(): Detected an illegal character in input string in /usr/share/php/libzend-framework-php/Zend/Search/Lucene/Analysis/Analyzer/Common/Text.php on line 58 [Tue Jan 18 16:23:32 2011] [error] [client 127.0.0.1] PHP Notice: iconv(): Detected an illegal character in input string in /usr/share/php/libzend-framework-php/Zend/Search/Lucene/Field.php on line 222 and variable $this->pages contain array of text copied from wikipedia and i am getting error for characters — (not -) and ö for which i am getting error(i believe). i got relevent similar question at Lucene foreign chars problem which doesn't explain where to do what. Please i would be grateful if i know where to do what and also a little bit of explanation UPDATES::iconv  iconv support enabled iconv implementation glibc iconv library version 2.12.1 so it's linux that's giving me the problem? Can you check which version of iconv you are using from `phpinfo()`? @Randell please have a look at updates Just as I suspected. I'm getting the same problem right now. Zend Lucene works well on Mac OS X and on Windows XP where the iconv implementation is in libiconv instead of glibc. I also do not know the solution yet. Except in the bootsrap code to add the third parameter encoding for text-based indexes $doc->addField(Zend_Search_Lucene_Field::text('page_name' $p['page_name'] 'UTF-8'));  Try adding this to your bootstrap: Zend_Search_Lucene_Search_QueryParser::setDefaultEncoding('utf-8'); Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8_CaseInsensitive () ); thank you very very much! Yes thanks your very much. This got rid of the same error for me. Much appreciated. Great worked for me.
221,A,"How to correctly boost results in Solr Dismax query I have managed to build an index in Solr which I can search on keyword produce facets query facets etc. This is all working great. I have implemented my search using a dismax query so it searches predetermined fields. However my results are coming back sorted by score which appears to be calculated by keyword relevancy only. I would like to adjust the score where fields have pre-determined values. I think I can do this with boost query and boost functions but the documentation here: http://wiki.apache.org/solr/DisMaxRequestHandler#head-6862070cf279d9a09bdab971309135c7aea22fb3 Is not particularly helpful. I tried adding adding a bq argument to my search: &bq=media:DVD^2 (yes this is an index of films!) but I find when I start adding more and more: &bq=media:DVD^2&bq=media:BLU-RAY^1.5 I find the negative results - e.g. films that are DVD but are not BLU-RAY get negatively affected in their score. In the end it all seems to even out and my score is as it was before i started boosting. I must be doing this wrong and I wonder whether ""boost function"" comes in somewhere. Any ideas on how to correctly use boost? Apparently this is normal for films that are DVD but are not BLU-RAY get negatively affected in their score. This is because the more constraints you add the more the queryNorm value is reduced - and all scores are multiplied by this value.  It sounds like you need to apply the boost at index time instead of query time. So when you are preparing documents to be added to the index give those that are DVD a boost of 2 and those that are Blu-Ray a boost of 1.5.  This is a little late and it looks like you probably already have what you are looking for but... If you're curious about boost functions (which judging by your desired results I think you should be) you should check out the bf argument instead of the bq argument. Try setting the bf argument to media:DVD^2 media:BLU-RAY^1.5 and I think that could achieve what you want."
222,A,Does solr make faceting on empty String? Last time I made a solr index it started indexing and doing faceting on empty strings too. This never happened. It is the right behaviour? Should I filter empty strings in the DIH? Thanks. Yes I would recommend filtering them out (setting them to NULL) in your Data Import Handler to reduce the load time it takes to generate the potentially large facet count. This may present itself regularly when optional values are represented as empty strings in a dataset.
223,A,"Order by field with SQLite I'm actually working on a Symfony project at work and we are using Lucene for our search engine. I was trying to use SQLite in-memory database for unit tests (we are using MySQL) but I stumbled upon something. The search engine part of the project use Lucene indexing. Basically you query it and you get an ordered list of ids which you can use to query your database with a Where In() clause. The problem is that there is an ORDER BY Field(id ...) clause in the query which order the result in the same order as the results returned by Lucene. Is there any alternative to ORDER BY Field using SQLite ? Or is there another way to order the results the same way Lucene does ? Thanks :) Edit: Simplified query might looks like this : SELECT i.* FROM item i WHERE i.id IN(1 2 3 4 5) ORDER BY FIELD(i.id 5 1 3 2 4) can you please clarify with sample queries? http://cakebaker.42dh.com/2008/06/10/order-by-field/ explains what this MySQL-specific syntax is about. This looks like a similar question with a simpler answer: http://stackoverflow.com/questions/3303851/sqlite-and-custom-order-by This is quite nasty and clunky but it should work. Create a temporary table and insert the ordered list of IDs as returned by Lucene. Join the table containing the items to the table containing the list of ordered IDs: CREATE TABLE item ( id INTEGER PRIMARY KEY ASC thing TEXT); INSERT INTO item (thing) VALUES (""thing 1""); INSERT INTO item (thing) VALUES (""thing 2""); INSERT INTO item (thing) VALUES (""thing 3""); CREATE TEMP TABLE ordered ( id INTEGER PRIMARY KEY ASC item_id INTEGER); INSERT INTO ordered (item_id) VALUES (2); INSERT INTO ordered (item_id) VALUES (3); INSERT INTO ordered (item_id) VALUES (1); SELECT item.thing FROM item JOIN ordered ON ordered.item_id = item.id ORDER BY ordered.id; Output: thing 2 thing 3 thing 1 Yes it's the sort of SQL that will make people shudder but I don't know of a SQLite equivalent for ORDER BY FIELD. Thanks but it seems a little over-complex :) I'd rather do it directly in php as the list are quite short (30 items max). Yeah. Well I'll accept your answer anyway ;) @DuoSRX: Yeah it is a bit over complex. Worth a try though. @DuoSRX: Most kind thank you. :-)"
224,A,"Lucene Searcher return only one match result My search text goes as ""ma"" and i have two lucene document which have ma as the text in it. But in return i only get one document. Below is the code : //adding deocument document.Add(new Field(""Text""textField.Store.YES Field.Index.TOKENIZED)); //search logic : IndexReader reader = IndexReader.Open(GetFileInfo(indexName)); //create an index searcher that will perform the search IndexSearcher searcher = new IndexSearcher(reader); //List of ID List<string> searchResultID = new List<string>(); //build a query object QueryParser parser = new QueryParser(""Text"" analyzer); parser.SetAllowLeadingWildcard(true); Query query = parser.Parse(searchText); //execute the query Hits hits = searcher.Search(query); I was able to solve my issue : Index Writer must be created only once.You can check whether the index exits or not if not you create an new IndexWriter . for eg : //The last parameter bool of an IndexWriter Contructor which says that you want to create an newIndexWriter or not IndexWriter writer = new IndexWriter(GetFileInfo(indexName) analyzer true); On adding the new Document you must perform an check whether index exists or not  if it exists  then just pass bool param as false to the IndexWriter constructor: IndexWriter writer = new IndexWriter(GetFileInfo(indexName) analyzer false); writer.AddDocument(CreateDocument(Id text dateTime)); writer.Optimize(); writer.Close();  Maybe you could use luke. It's a useful diagnostic tool that can display the contents of an existing Lucene index and do other interesting stuff. I haven't used it myself so I'm not sure but I think it might help you in debugging this issue. Good luck! @Malcolm That's great! I'm happy I was able to help :) Stoyanov thanks for your reply . I used this tool and found out that i was always creating an new index writer when document was added. +1 for your answer.Finally Luke is good tool to debug.:)"
225,A,"Zend_Search_Luncene handle Querys iam trying to implement an Searchmachine into my site. Iam using Zend_Search_Lucene for this. The index is created like this : public function create($config $create = true) { $this->_config = $config; // create a new index if ($create) { Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum_CaseInsensitive() ); $this->_index = Zend_Search_Lucene::create(APPLICATION_PATH . $this->_config->index->path); } else { $this->_index = Zend_Search_Lucene::open(APPLICATION_PATH . $this->_config->index->path); } } { public function addToIndex($data) $i = 0; foreach ($data as $val) { $scriptObj = new Sl_Model_Script(); $scriptObj->title = $val['title']; $scriptObj->description = $val['description']; $scriptObj->link = $val['link']; $scriptObj->tutorials = $val['tutorials']; $scriptObj->screenshot = $val['screenshot']; $scriptObj->download = $val['download']; $scriptObj->tags = $val['tags']; $scriptObj->version = $val['version']; $this->_dao->add($scriptObj); $i++; } return $i; } /** * Add to Index * * @param Sl_Interface_Model $scriptObj */ public function add(Sl_Interface_Model $scriptObj) { // UTF-8 for INDEX $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::text('title' $scriptObj->title 'utf-8')); $doc->addField(Zend_Search_Lucene_Field::text('tags' $scriptObj->tags 'utf-8')); $doc->addField(Zend_Search_Lucene_Field::text('version' $scriptObj->version 'utf-8')); $doc->addField(Zend_Search_Lucene_Field::text('download' $scriptObj->download 'utf-8')); $doc->addField(Zend_Search_Lucene_Field::text('link' $scriptObj->link)); $doc->addField(Zend_Search_Lucene_Field::text('description' $scriptObj->description 'utf-8')); $doc->addField(Zend_Search_Lucene_Field::text('tutorials' $scriptObj->tutorials 'utf-8')); $doc->addField(Zend_Search_Lucene_Field::text('screenshot' $scriptObj->screenshot)); $this->_index->addDocument($doc); } But when i try to query the index with : $index->find('Wordpress 2.8.1' . '*'); im getting the following error : ""non-wildcard characters are required at the beginning of pattern."" any ideas how to query for a string like mine ? an query for ""wordpress"" works like excepted. Lucene cannot handle leading wildcards only trailing ones. That is it does not support queries like 'tell me everyone whose name ends with 'att'' which would be something like first_name: *att It only supports trailing wildcards. Tell me everyone whose names end that start with 'ma' first_name: ma* See this Lucene FAQ entry: http://wiki.apache.org/lucene-java/LuceneFAQ#head-4d62118417eaef0dcb87f4370583f809848ea695 There IS a workaround for Lucene 2.1 but the developers say it can be ""expensive"". No you could query for multiple words with trailing prefixes let say you wanted to find all documents that start with ""kine"" or ""dict"" you would do: name: kine* OR name: dict* Basically just OR the clauses together. Does that answer your question? thanks so its only possible to query for one ""word"" ?"
226,A,"Building a case for solr Our product consists of multiple applications All using Lucene. 2 of the applications I am involved with have Lucene indexes of about 3 Gb and 12 Gb. Another team is building an application for which they estimate the LUCENE INDEX size to be close to 1 Terabyte. New documents are added to the indexes every 15 days approx. We do not have any apparent performance issues with the current applications. So my question is: Should we be using Solr now? When should one stop using Lucene and graduate to Solr? Any disadvantages/problems for using Solr? The client applications are made in ASP.Net but I assume they will be able to use a Solr server using solrnet. Solr wraps your Lucene index with a REST-like interface. You have everything needed to add to query and administer your index with HTTP methods. So if you need to access your Lucene index on the web using Solr is the natural way to go. Maybe a disadvantage I could see is that a Lucene index is usually completely embedded within your application while a Solr instance will run on a separate process. It could add complexity to your app if you don't need what Solr has to offer.  I don't think moving from Lucene to Solr is in itself a ""graduation"". You should just use whatever works best for your particular application at the same time taking into account the expertise of the dev team. Moving to Solr does have the advantage of being easily distributed shall you need it. OTOH if you can fit the 1TB index in a single machine without performance issues then you don't need to distribute. I don't recommend distributing unless you have to. Distribution means you'll have to maintain N Solr servers instead of just one so the operational upkeep goes up. Programatically (in the .Net app) there shouldn't be much difference. Solr is kind of a batteries-included stand-alone Lucene implementing features like faceting caching spell checking... then again if you don't need these features and your team is already proficient with Lucene(.net) then stick to Lucene. I should note that it's not common to fit a 1TB index in a single machine without performance issues."
227,A,How to search a complete document in Lucene via Solr? I have a question regarding searching a complete document. 1 - I have indexed a lot of documents on lucene. 2 - Each document has a single word per line. Suppose 200 words which becomes 200 lines. 3 - I know how to search lucene via Solr but; If suppose that i indexed the document mydoc.txt on lucene containing 200 words along with other docs. Now I want to search mydoc.txt i have it on disk locally how do i post the complete document mydoc.txt via Solr to make a search for me so that I get the already indexed document mydoc.txt ranked first and get rest of the documents? Do I parse mydoc.txt and send line by line or concatinate the parsed words and then send them to lucene or is there a specific query which I can use to make a search without parsing the document and I am able to send the complete document? Queries and syntax for Solr is what I need with the steps!!!! Look up more like this http://www.google.com/webhp?sourceid=chrome-instant&ie=UTF-8&ion=1&nord=1#sclient=psy&hl=en&nord=1&site=webhp&source=hp&q=more%20like%20this%20lucene&pbx=1&oq=&aq=&aqi=&aql=&gs_sm=&gs_upl=&fp=3665754608edbaf3&ion=1&ion=1&bav=on.2or.r_gc.r_pw.&fp=3665754608edbaf3&biw=1366&bih=611&ion=1
228,A,"Howto perform a 'contains' search rather than 'starts with' using Lucene.Net We use Lucene.NET to implement a full text search on a clients website. The search itself works already but we now want to implement a modification. Currently all terms get appended a * which leads Lucene to perform what I would classify as a StartsWith search. In the future we would like to have a search that performs something like a Contains rather than a StartsWith. We use Lucene.Net 2.9.2.2 StandardAnalyzer default QueryParser Samples: (Title:Orch*) matches: Orchestra but: (Title:rch*) does not match: Orchestra We want the first and the second one to both match Orchestra. Basically I want the exact opposite of what was asked in this question I'm not sure why for this person Lucene performed a Contains and rather than a StartsWith by default: Why is this Lucene query a ""contains"" instead of a ""startsWith""? How can we make this happen? I have the feeling it has something to do with the Analyzer but I'm not sure. @Simon Svensson probably gave the better answer (i.e. you don't need this) but if you do you should use a Shingle Filter. Note that this will make your index massively larger since instead of just storing ""orchestra"" you will store ""orc"" ""rch"" ""che"" ""hes""... But just having a plain term query with leading wildcards will be massively slow. It will essentially have to look through every single term in your corpus.  First off I assume you're using StandardAnalyzer or something similar. Your linked question fail to understand that you search for terms and his case a* will match ""Fleet Africa"" because it's tokenized into ""fleet"" and ""africa"". You need to call QueryParser.SetAllowLeadingWildcard(true) to be able to write queries like field:*value*. Are you actually changing the string that's passed to QueryParser? You could parse the query as usual and then implement a QueryVisitor that rewrites all TermQuery into WildcardQuery. That way you still support phrase searches. I see no good things in rewriting queries into prefix- or wildcard-queries. There is very little shared between an orc or a chest and an Orchestra but both words will match. Instead hook up your customer with an analyzer that supports stemming synonyms and provide a spell correction feature to fix simple searching mistakes. As search engine specs are often ""do it like google"" you can say that google don't seem to allow this. Try searching for ""chestra"" ;) Yes allowing leading wildcards is a possible huge performance penalty. Queries like f:cat* are rewritten into something like f:(cat cats category ...) by using a TermsEnum. Leading wildcards means that it will need to iterate _all_ terms instead of a small range. Similar issue exists with SQL Server indexes they cant be used with LIKE '%value%'. Thx that was exactly what I was looking for regarding the task itself: Well as so often the client wants exactly this and is resistant against arguments ;) Also it's actually a non natural search for band names and event descriptions so things like stemming/synonyms etc. will most likely not be ideal in this case. Anyway works great now thx! Watchout of the serious performance penalty of using `SetAllowLeadingWildcard(true)` - there's only a clue about that in the above answer. Also depending on your intended usage you might want to look into n-grams - ShingleFilter like Xodarap suggested in another answer."
229,A,"Is There a way to use Zend Search Lucene in a way similar to Useing the WHERE LIKE sql in Databases? $select = $this->_db->select()->from($this->_namearray(""id""""fullname""""username""""email""))->where(""fullname LIKE '$query%'""); I am using this SQL statement currently to power my Ajax auto suggest if i type in ""a"" it gets me results starting with a. I want to know if this can be accomplished my using Zend Lucene indices. Your first stop should be the Zend Framework reference manual and more specifically : http://framework.zend.com/manual/en/zend.search.lucene.query-language.html So you can use * for wildcard searches just like you would use % in SQL (ie. when you provide part of a word). Awesome! Works ... Thanks a lot"
230,A,"problem with incremental update in lucene I am creating a program that can index many text files in different folder. so that's mean every folder that has text files get indexed and its index are stored in another folder. so this another folder acts like a universal index of all files in my computer. and I am using lucene to achieve this because lucene fully supported incremental update. this is the source code into which I use it for indexing. public class SimpleFileIndexer { public static void main(String[] args) throws Exception { int i=0; while(i<2) { File indexDir = new File(""C:/Users/Raden/Documents/myindex""); File dataDir = new File(""C:/Users/Raden/Documents/indexthis""); String suffix = ""txt""; SimpleFileIndexer indexer = new SimpleFileIndexer(); int numIndex = indexer.index(indexDir dataDir suffix); System.out.println(""Total files indexed "" + numIndex); i++; Thread.sleep(1000); } } private int index(File indexDir File dataDir String suffix) throws Exception { RAMDirectory ramDir = new RAMDirectory(); // 1 @SuppressWarnings(""deprecation"") IndexWriter indexWriter = new IndexWriter( ramDir // 2 new StandardAnalyzer(Version.LUCENE_CURRENT) true IndexWriter.MaxFieldLength.UNLIMITED); indexWriter.setUseCompoundFile(false); indexDirectory(indexWriter dataDir suffix); int numIndexed = indexWriter.maxDoc(); indexWriter.optimize(); indexWriter.close(); Directory.copy(ramDir FSDirectory.open(indexDir) false); // 3 return numIndexed; } private void indexDirectory(IndexWriter indexWriter File dataDir String suffix) throws IOException { File[] files = dataDir.listFiles(); for (int i = 0; i < files.length; i++) { File f = files[i]; if (f.isDirectory()) { indexDirectory(indexWriter f suffix); } else { indexFileWithIndexWriter(indexWriter f suffix); } } } private void indexFileWithIndexWriter(IndexWriter indexWriter File f String suffix) throws IOException { if (f.isHidden() || f.isDirectory() || !f.canRead() || !f.exists()) { return; } if (suffix!=null && !f.getName().endsWith(suffix)) { return; } System.out.println(""Indexing file "" + f.getCanonicalPath()); Document doc = new Document(); doc.add(new Field(""contents"" new FileReader(f))); doc.add(new Field(""filename"" f.getCanonicalPath() Field.Store.YES Field.Index.ANALYZED)); indexWriter.addDocument(doc); } } and this is the source code that I use for searching the lucene-created index public class SimpleSearcher { public static void main(String[] args) throws Exception { File indexDir = new File(""C:/Users/Raden/Documents/myindex""); String query = ""revolution""; int hits = 100; SimpleSearcher searcher = new SimpleSearcher(); searcher.searchIndex(indexDir query hits); } private void searchIndex(File indexDir String queryStr int maxHits) throws Exception { Directory directory = FSDirectory.open(indexDir); IndexSearcher searcher = new IndexSearcher(directory); @SuppressWarnings(""deprecation"") QueryParser parser = new QueryParser(Version.LUCENE_30 ""contents"" new StandardAnalyzer(Version.LUCENE_CURRENT)); Query query = parser.parse(queryStr); TopDocs topDocs = searcher.search(query maxHits); ScoreDoc[] hits = topDocs.scoreDocs; for (int i = 0; i < hits.length; i++) { int docId = hits[i].doc; Document d = searcher.doc(docId); System.out.println(d.get(""filename"")); } System.out.println(""Found "" + hits.length); } } the problem I am having now is that the indexing program I created above seem can't do any incremental update. I mean I can search for a text file but only for the file that existed in the last folder to which I already indexed and the other previous folder that I had already indexed seems to be missing in the search result and didn't get displayed. can you tell me what went wrong in my code? I just wanted to be able to have incremental update feature in my source code. so in essence my program seems to be overwriting the existing index with the new one instead of merging it. thanks though Directory.copy() overwrites the destination directory you need to use IndexWriter.addIndexes() to merge the new directory indices into the main one. You can also just re-open the main index and add documents to it directly. A RAMDirectory isn't necessarily more efficient than properly tuned buffer and merge factor settings (see IndexWriter docs). Update: instead of Directory.copy() you need to open ramDir for reading and indexDir for writing and call .addIndexes on the indexDir writer and pass it the ramDir reader. Alternatively you can use .addIndexesNoOptimize and pass it ramDir directly (without opening a reader) and optimize the index before closing. But really it's probably easier to just skip the RAMDir and open a writer on indexDir in the first place. Will make it easier to update changed files as well. Example private int index(File indexDir File dataDir String suffix) throws Exception { RAMDirectory ramDir = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDir new StandardAnalyzer(Version.LUCENE_CURRENT) true IndexWriter.MaxFieldLength.UNLIMITED); indexWriter.setUseCompoundFile(false); indexDirectory(indexWriter dataDir suffix); int numIndexed = indexWriter.maxDoc(); indexWriter.optimize(); indexWriter.close(); IndexWriter index = new IndexWriter(FSDirectory.open(indexDir) new StandardAnalyzer(Version.LUCENE_CURRENT) true IndexWriter.MaxFieldLength.UNLIMITED); index.addIndexesNoOptimize(ramDir); index.optimize(); index.close(); return numIndexed; } But just this is fine too: private int index(File indexDir File dataDir String suffix) throws Exception { IndexWriter index = new IndexWriter(FSDirectory.open(indexDir) new StandardAnalyzer(Version.LUCENE_CURRENT) true IndexWriter.MaxFieldLength.UNLIMITED); // tweak the settings for your hardware index.setUseCompoundFile(false); index.setRAMBufferSizeMB(256); index.setMergeFactor(30); indexDirectory(index dataDir suffix); index.optimize(); int numIndexed = index.maxDoc(); index.close(); // you'll need to update indexDirectory() to keep track of indexed files return numIndexed; } I am a bit loss here.can you just show me which line that need to be changed? I mean can you edit it so it would be clear for me. :-) well yes thanks you very much though. :-)"
231,A,"Bizarre error with ASP.NET + Lucene website ASP.NET newbie here I have coded up an ASP.NET website and running on win'08 (remotely hosted). The application queries 11 very large Lucene indexes (each ~100GB). I open IndexSearchers on Page_load() and keep them open and pass them by ref into the query methods based on user entered keywords. I can RDC in and run the site fine in VS-2008. I can deploy and access it via the web from my desktop.For some bizarre reason some of my team-mates have trouble running the same site - they can login fine so access is not an issue - however the application just ""hangs"" when they run some searches. Any suggestions on where I should be looking? Could this be an issue with multiple searchers querying simultaneously? Any ideas? Sounds like locking issues. You must not hold any resources in web application for longer than need (request). So I believe you should not hold the Lucene indexes. Also check the file system permissions for the account that site runs under. Additionally you should check the Event Logs you might have some warning or silent errors. I did check that and set timers to measure it doesn't appear the queries take a lot of time ( a few ms to 1 sec) ...but the binding to the gridview seems to take inordinate amount of time. Are there any best practices for working with Lucene on ASP.NET? Here is what I gleaned from searching on SO and Lucene sites: 1. Open IndexSearchers on PageLoad() and hold them for duration of user session. 2. Open searcher IndexReader = true (my writers are in a separate daemon scheduled task process) 3. Use HitsCollector instead of Hits class. 4. Move the Indexes onto multiple physical disks (I have 2) do you agree/disagree with above? anything to add? also any projects I can refer to for best practices? thanks for your help! Now I have my search methods all in a static class and invoke the method something like public static DoSearch (ref IndexSearcher string query) Instead do you recommend I open an Indexsearcher in DoSearch() and close it? First of all you need to isolate the problem. Make sure the Lucene causes that. But definitely you need to close the indexes as `Multiple index writers or readers can try to edit the lucene index files at the same time (it's important for the index writer/reader to be closed so it will release the file lock).` You say the application ""hands"". Which is usually understood as the application does not respond. But from your last comment it seems that the application is not hanging but rather performs slow when binding data to the grid. So you need to make sure you do not bind millions of records or the data is paged. Usually you should only show 10-100 entries on a web page. If there are more - you should paginate that."
232,A,"Lucene spatial accuracy I'm following the example in ""Lucene in Action"" pages 308-315 which describes Lucene Spatial. I'm using lucene 2.9.4. I've used http://geocoder.us/service/distance endpoint to calculate the distance between some locations and then written unit tests to verify that the index can find locations within a given radius. I'm wondering how accurate I can expect lucene to be. For example if I give radius 10.0 and the distance between my lat/lon points is 9.99 miles will it be able to find this location in all cases? The thing that is prompting this question is that I've found the search to be very accurate for small radius values (e.g. 10.0 or less) and inaccurate for larger values (r=25.0 for example). Is there anything I might be doing wrong? Is it possible that the searcher will select a tier that does not have all the lat/longs for a given radius? My understanding was that it selects the smallest tier that is guaranteed to have all the points within the radius i.e. the tier algorigthm is just an optimization. EDIT: Also I found this: https://issues.apache.org/jira/browse/LUCENE-2519 and the apparently fixed code here: http://code.google.com/p/spatial-search-lucene/source/browse/trunk/src/main/java/org/apache/lucene/spatial/tier/projection/SinusoidalProjector.java?r=38 but when I patched my code to use the fixed SinusoidalProjector my index returns zero ads in all cases. And this does not give me much confidence: http://www.lucidimagination.com/blog/2010/07/20/update-spatial-search-in-apache-lucene-and-solr/ http://www.lucidimagination.com/search/document/c32e81783642df47/spatial_rethinking_cartesian_tiers_implementation#c32e81783642df47 It seems to indicate that hacks exist throughout the code and simply patching SinusoidalProjector is not enough. They fixed this in Lucene 3.5.0. Large distances now work as well as small  I've spent some time looking at the source code and I think I understand what's going wrong. Firstly I made a bad assumption that the distances computed by geocoder.us would be the same as what lucene internally calcuates as distances between points. The values are close but not exact. So I switched to compute distances between lat/lon pairs by calling lucene's double distance = DistanceUtils.getInstance().getDistanceMi(lat1lon1lat2lon2); Next I dug into DistanceQueryBuilder class http://grepcode.com/file/repo1.maven.org/maven2/org.apache.lucene/lucene-spatial/2.9.4/org/apache/lucene/spatial/tier/DistanceQueryBuilder.java?av=f which I think has a bug in it. It calculates the bounding box for the purpose of fetching cartesian tiers like this: CartesianPolyFilterBuilder cpf = new CartesianPolyFilterBuilder(tierFieldPrefix); Filter cartesianFilter = cpf.getBoundingArea(lat lng miles); And it's pretty clear by looking at LLRect.createBox http://grepcode.com/file/repo1.maven.org/maven2/org.apache.lucene/lucene-spatial/2.9.4/org/apache/lucene/spatial/geometry/shape/LLRect.java#LLRect.createBox%28org.apache.lucene.spatial.geometry.LatLng%2Cdouble%2Cdouble%29 that the third parameter to getBoudningArea is going to be treated as the full width/height of a bounding box. So passing the radius value results in a bounding box that is too small. The fix was to provide an alternate version of DistanceQueryBuilder that does this: Filter cartesianFilter = cpf.getBoundingArea(latlngmiles*2); Which seems to work. I'm still convinced that DistanceApproximation http://grepcode.com/file/repo1.maven.org/maven2/org.apache.lucene/lucene-spatial/2.9.4/org/apache/lucene/spatial/geometry/shape/DistanceApproximation.java#DistanceApproximation.getMilesPerLngDeg%28double%29 is broken though because it seems that the following operations should be reversible and they're not: // similar to implementation of DistanceUtils.getBoundary(): double milesPerLng = DistanceApproximation.getMilesPerLngDeg(lat); double milesPerLat = DistanceApproximation.getMilesperLatDeg(); double lngDelta = radius / milesPerLng; double latDelta = radius / milesPerLat; // Now it seems like this should be roughly true: assertEquals(radius DistanceUtils.getInstance().getDistanceMi(latlnglatlng+lngDelta)); assertEquals(radius DistanceUtils.getInstance().getDistanceMi(latlnglat+latDeltalng)); But it's not. For example when the above code is given lat=34 lng=-118 and radius=25 (and instead of asserting I just print the results) I get: Lng delta: 0.36142327178505024 dist: 20.725929003138496 Lat delta: 0.4359569489852007 dist: 30.155567734407825 I'm guessing that the code works only because the cartesian tiers that are chosen after picking a bounding box will result in an area somewhat larger than the bounding box. But I don't think that's going to be guaranteed. I'm hoping that someone with more knowledge of this can comment because these are just observations after digging into the code for an afternoon. I did notice that what looks like the most recent code for lucene spatial is on googlecode at: http://code.google.com/p/spatial-search-lucene/ and it seems that the implementation has changed significantly but I didn't dig too deeply into the details. Thanks...I no longer work at the same company but I am using solr 4 for a project at my current gig. Lucene 4 spatial is indeed based on http://code.google.com/p/spatial-search-lucene/ and it's much different. I suggest you upgrade to Lucene 4. The spatial module in v3 was so buggy / untrustworthy and unmaintained that it was jettisoned."
233,A,"Understanding Solr MoreLikeThis Result I'm on my way to learn to use Solr. Im learning to use MoreLikeThis right now. and if I doing Query which is like this http://localhost:8080/solr/select/?q=terbang&indent=on&mlt=true&mlt.fl=entry I get a more like this result like this  <lst name=""moreLikeThis""> <result name=""67244"" numFound=""0"" start=""0""/> <result name=""67250"" numFound=""0"" start=""0""/> <result name=""146"" numFound=""0"" start=""0""/> <result name=""3993"" numFound=""0"" start=""0""/> <result name=""11758"" numFound=""0"" start=""0""/> <result name=""14828"" numFound=""0"" start=""0""/> <result name=""20820"" numFound=""0"" start=""0""/> <result name=""23336"" numFound=""0"" start=""0""/> <result name=""24267"" numFound=""0"" start=""0""/> <result name=""24779"" numFound=""0"" start=""0""/> </lst> I dont really understand whats the meaning of the ""name"" attribute. And why is there many results with different name if its not found anything anyway? The numbers in the name attribute are the unique keys of the documents in the results."
234,A,"Best Practices for implementing a Lucene Search in Java Each document in my Lucene index is kind of similar to a post in stackoverflow and I am trying to search through the index (which contains millions of documents). Each user should only be able to search through the user's company posts only. I have no control over how the data is indexed and I only need to implement a simple search (that works) on top of it. Here is my first draft: String q = ""mysql"" String companyId = ""1001"" String[] fields = { ""body"" ""subject"" ""number"" ""category"" ""tags""}; Float float10 = new Float(10); Float float5 = new Float(5); Map<String Float> boost = new HashMap<String Float>(); boost.put(""body"" float10); boost.put(""subject"" float10); boost.put(""number"" float5); boost.put(""category"" float5); boost.put(""tags"" float5);; MultiFieldQueryParser mfqp = new MultiFieldQueryParser(fields new StandardAnalyzer() boost); mfqp.setAllowLeadingWildcard(true); Query userQuery = mfqp.parse(q); TermQuery companyQuery = new TermQuery(new Term(""company_id"" companyId)); BooleanQuery booleanQuery = new BooleanQuery(); BooleanQuery.setMaxClauseCount(50000) booleanQuery.add(userQuery BooleanClause.Occur.MUST); booleanQuery.add(companyQuery BooleanClause.Occur.MUST); FSDirectory directory = FSDirectory.getDirectory(new File(""/tmp/index"")); IndexSearcher searcher = SearcherManager.getIndexSearcherInstance(directory); Hits hits = searcher.search(booleanQuery); Its mostly working functionally but I am seeing some memory issues. I get an Out of Memory error every 4 5 days and I took a heapdump and saw that Lucene Term and TermInfo objects top the list. I am using singleton instance of IndexSearcher and I can see only one instance of it in the heap. Any reviews on the way I am doing? What I am doing wrong and what I can do better in general? Typically when you open the index (with Reader or Searcher) a whole bunch of Terms are loaded in memory. While you mention that there is single instance of IndexSearcher I suspect there could be more. @adrian I used VisualVM The setup is ok but the question is hopelessly vague. Do you mean that there's a memory leak? How do you know? What's your evidence? Edited. Hope its more clear now. Did you use http://www.eclipse.org/mat/ to do the analysis? There is no obvious bug in your code (at least not as far as I can tell). It might be best to analyze your heapdump with a more powerful tool than visualvm. I recommend to use the Memory Analyzer (MAT) of eclipse (not installed by default but available from the default update site). It's awesome. If you need help using MAT please refer to this blog post ""Eclipse Memory Analyzer 10 useful tips/articles"" by Markus Kohler. He is the author of the tool.  What's your heap size? Are there certain searches that cause your memory usage to go high? My guess is that you are hitting OOME's when you perform wildcard queries. Internally Lucene expands a wildcard query to an OR query of ALL of the terms that match the wildcard. This problem is exacerbated by the fact that you are allowing leading wildcards. Running a search like ""body:*"" would load up every single term in the body field into memory. My recommendation would be to run a memory profiler while running wildcard queries and see what you get. If the wildcard queries are the culprit then at least disable leading wildcards or lower your query clause limit.  Where do you you usually experience out of memory issues? Is it around this block? MultiFieldQueryParser mfqp = new MultiFieldQueryParser(fields new StandardAnalyzer() boost); mfqp.setAllowLeadingWildcard(true); Query userQuery = mfqp.parse(q); Also are you running the code for querying in conjunction with the indexing process? The indexer and searcher run on two different systems. I think it has to do with the no of columns I have in the index and the no of documents which is causing it to create huge no of terms for each multi field search."
235,A,"Can we tell Solr/Lucene max chars to analyze for a search? I have a problem that in my lucene index files one document can have huge text. now when i search one of these huge text documents lucene/solr does not filter any results even the search term exist in the document text. the reason that i think might be the large number of characters in document text? if yes than how could we tell solr/lucene how much characters to analyze during search please explain I am using Solr 1.4.1 can any Thanks Ahsan Also have you changed the maxFieldLength setting in solrconfig.xml? I am testing out indexing the Bible at 25 MB of data and with a maxFieldLength of 10000 which is the default only the first 10000 tokens ever get analysized which leads to roughly ~2000 unique terms for my document. If you are using Lucene directly then there are a couple setting for maxFieldLength you may have ""unlimited"" and therefore getting everything. Check the JavaDocs for how to set maxFieldLength. thanx for your answer It is a index time parameter. If you already have a ginormous document in the index changing this won't change anything retroactively. i want to know that changing maxFieldLengh in solrConfig at searching time works are i need to do that at indexing time too?  Lucene can handle huge documents without trouble. It seems unlikely that the document size itself is the problem. Use a tool like Luke to inspect the index and see what terms are associated with some of these large documents. I have do the same but every thing looks ok. but problem still persists i don't know that can i post my index files here.."
236,A,All data in my database should be searchable - MySQL vs Lucene All the data in my database should be searchable from within my web app. It's lots of data more than 2 million records. So what should I do? 1) Should I index all of the data and then use Lucene esclusively to query what I need and therefore not use MySql at all? 2) Or should I use Lucene only for searching and MySQL for complex data associations? I mean I could still use Lucene for associations but maybe that's an overkill. So what's the best approach for dealing with this sort of scenario? Lucene (and most nosql databases) is not a relational database - so even if you are using trivial ORM you're going to run into difficulties trying to build even a fairly trivial application exclusively around Lucene. So I'd suggest there's no way of avoiding using mysql for the core data. (2 million records is not a HUGE amount of data) So the question then becomes whether it's worth the effort of building a method for publishing the data out of MySQL into Lucene. Bear in mind that MySQL offers full text indexing and replication out of the box and there are add-ons like sphinx specifically designed to resolve the shortcomings of the standard solution. This then covers a lot of the same ground as the 'related' questions I see on my screen currently - I suggest you have a look at through the answers - particularly this one  Take a look at Apache SOLR which is based on Lucene technology.  I don't have much experience with Lucene but it sounds to me like it doesn't support table associations as well as MySQL so I'd argue that MySQL is better in the long run. By default everything is searchable in MySQL unless you remove permissions which you may eventually want to do depending on how the growth of your database proceeds.
237,A,"How to evaluate hosted full text search solutions? What are the options when it comes to SaaS/hosted full text search? How should I evaluate the different options available? I'm looking for something that uses Lucene solr or sphinx on the backend and provides a REST API for submitting documents to index and running searches. I could build my own EC2 AMI but I'd have to configure EBS and other stuff monitor it etc. keep in mind a hosted solr environment will always be slower than a dedicated server. depending on the size of your index you may want to keep note of this. (Note: I have edited this question to make it less of a shopping list question so it can be reopened... it's just too useful to close.) @Flexo: why is this protected? I don't see how it qualifies. @MauricioScheffer - there's a lot of deleted ""answers"" from low rep (<10) users - it's quite the spam magnet. 8662224 was the one that I spotted combined with the rest made it a good candidate for protecting in my view. Another option particularly for UK people is http://www.netaphorsearch.com/ . I should point out I own Netaphor Ltd. We support the Solr REST API but also have a PHP connector so that you can get up and running very quickly.  Have a look at Artirix - UK company but also in the US http://www.artirix.com. I know they power some sites such as Globrix.com in the UK based on SOLR and have a bunch of other products for crawling and data processing  Indextank is a hosted real-time full text search solution. It's pretty simple to set up (you can get an index running in a couple of minutes) and it's very powerfull (Reddit runs over IndexTank). It provides Java Python Ruby and Php clients as well as a Rest API specification. There's an awesome support service (including live chat). You should give it a try. IndexTank was recently bought by LinkedIn and I believe their end-user search service will be going away... Very sad. Yes they don't accept any new sign-ups Hi I am founder of http://IndexDen.com - our API is 100% compatible with IndexTank's API. If you are IndexTank user and looking for replacement or looking for SaaS hosted search then please try out our service!  Another option for lower-volume websites is Midwestern Mac's hosted Solr search (I am the owner of Midwestern Mac LLC just fyi). Although it's not too hard (if you can use a command line respectably well) to provision your own server on a VPS somewhere... The hard thing is maintaining it. Every day ... No doubt about that - monitoring is another issue I have to contend with. Right now the tools for monitoring solr search are far and few between... mostly I just have a few handmade shell scripts that parse log files.  My five cents http://indexisto.com/ Offers free hosted Elastic Search if you are ready for advertisement in search results. But anyway you can start with free and switch to no ads paid account. It's also not just hosted Elastic Search but ready to ase Ajax search box (that really impress) to embed to you site (mobile and tablet adopted) and some useful features like statistics image resizing. There are several options to fill the index with documents - crawler API and DB connector  Websolr provides a cloud-based Solr with a control panel. It's in private beta as of this writing but you can get the service through Heroku. Another hosted Solr service is PowCloud also in private beta which seems to offer strong Wordpress integration. SolrHQ: another beta service providing a hosted Solr solution with Joomla and Wordpress integrations. Acquia Search offers Solr integration for Drupal sites. If you decide to build your own EC2 instance the SolrOnAmazonEC2 wiki page might be useful. Or you could just get LucidWorks Solr for EC2 which is probably the easiest and fastest way to get Solr on EC2. Engine Yard provides a cloud-based Sphinx service. Perfect. Thank you very much! Thanks for the Websolr mention Mauricio. For anyone interested we recently opened up to the public so anyone can use our hosted Solr service from any platform. The LucidWorks AMI has been removed."
238,A,How to index rows dependent on column values with Hibernate Search / Lucene? Is it possible to use hibernate search/lucene to index some entity based on values of some fields? For example let's take the following example: A product has several properties with values. e.g. property names could be color amount order-date price whatever... PRODUCT ( name description ... ) PROPERTY ( id name value fk_product ) And I only want to index PRODUCTS that have a property COLOR but I never want to search on property SIZE. Also Is it possible to index my products with hibernate search and be able to query only on specific property names (like they where fields on a project)? Some query like this: color:blue that would return me all products that have a property name=color with the value=blue. From the reference doc I don't find anything but maybe should I use Filters to restrict the querying depending on the values of some fields! It looks like http://stackoverflow.com/questions/999936/query-in-lucene But this is a two step solution and does not solve my wishes in queryable format. After rereading the reference documentation I realized I had to use a ClassBridge. (section 4.2.2.3 in the documentation) This solves exactly my problem! The example of the documentation is straight forward.
239,A,What is the use of Field.Set OmitNorms(true); in lucene I have been suggested to use Field.Set OmitNorms(true); when creating the documents for lucenesearch to sort the result according to the number of hits but I am not clear of what it does and is it safe. sort the result according to the number of hits means that the document in which search text is found maximum number of times should come on the top followed b the ones with less number of match for search text. I know its silly but I want to know before I implement this please help. By default a field is indexed with its norm a product of the document's boost the field's boost and the field's length normalisation factor (see Similarity scoring). This adds a byte to the storage and memory consumption of every field which can be ommited for selected fields or field types using omitNorms. The boosts are specified by during indexing while lengthNorm is calculated so that if two documents match a query term f times the longer document will get a lower score. So if you want your documents to be scored based on the exact number of terms matched rather than the number of terms in proportion to the document length use omitNorms (and get the memory consumption benefits free).  Check out this article for a good paragraph description of what omit norms does in term of optimisation. Basically its kind of like having a mini lucene index for the terms inside of a field so its really only useful for fields that would have a lot of text inside them. The document has been moved but can be found in the internet archive at http://web.archive.org/web/20101111232408/http://www.lucidimagination.com/Community/Hear-from-the-Experts/Articles/Scaling-Lucene-and-Solr
240,A,"Lucene search by URL I'm storing a Document which has a URL field: Document doc = new Document(); doc.add(new Field(""url"" url Field.Store.YES Field.Index.NOT_ANALYZED)); doc.add(new Field(""text"" text Field.Store.YES Field.Index.ANALYZED)); doc.add(new Field(""html"" CompressionTools.compressString(html) Field.Store.YES)); I'd like to be able to find a Document by its URL but I get 0 results: Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_30) Query query = new QueryParser(LUCENE_VERSION ""url"" analyzer).parse(url); IndexSearcher searcher = new IndexSearcher(index true); TopScoreDocCollector collector = TopScoreDocCollector.create(10 true); searcher.search(query collector); ScoreDoc[] hits = collector.topDocs().scoreDocs; // Display results for (ScoreDoc hit : hits) { System.out.println(""FOUND A MATCH""); } searcher.close(); What can I do differently so that I can store an HTML document and find it by its URL? The Lucene QueryParser is interpreting some of the url characters as part of the Query Parser Syntax. You can use a TermQuery instead like so: TermQuery query = new TermQuery(new Term(""url"" url));  You may rewrite your query to something like this Query query = new QueryParser(LUCENE_VERSION ""url"" analyzer).newTermQuery(new Term(""url"" url)).parse(url); Suggestion: I suggest you use BooleanQuery since it gives good performance and internally it is optimized. TermQuery tq= new TermQuery(new Term(""url"" url)); // BooleanClauses Enum SHOULD says Use this operator for clauses that should appear in the matching documents. BooleanQuery bq = new BooleanQuery().add(tqBooleanClause.Occur.SHOULD); IndexSearcher searcher = new IndexSearcher(index true); TopScoreDocCollector collector = TopScoreDocCollector.create(10 true); searcher.search(query collector); I see you are indexing using URL frield as Not_Analysed which is good IMO for searching As no analyzer is used the value will be stored as a single term. Now if your business case says i will give you a URL find the EXACT one from the Lucene Index then you shall look at your indexing with a different analyzer(KeywordAnalyzer etc)"
241,A,"zend search lucene I have a database that I would like to leverage with Zend_Search_Lucene. However I am having difficulty creating a ""fully searchable"" document for Lucene. Each Zend_Search_Lucene document pulls information from two relational database tables (Table_One and Table_Two). Table_One has basic information (id owner_id title description location etc.) Table_Two has a 1:N relationship to Table_One (meaning for each entry in Table_One there could be one or more entries in Table_Two). Table_Two contains: id listing_id bedrooms bathrooms price_min price_max date_available. See Figure 1. Figure 1 Table_One id (Primary Key) owner_id title description location etc... Table_Two id (Primary Key) listing_id (Foreign Key to Table_One) bedrooms (int) bathrooms (int) price_min (int) price_max (int) date_available (datetime) The problem is there are multiple Table_Two entries for each Table_One entry. [Question 1] How to create a Zend_Search_Lucene document where each field is unique? (See Figure 2) Figure 2 Lucene Document id:Keyword owner_id:Keyword title:UnStored description:UnStored location: UnStored date_registered:Keyword ... (other Table_One information) bedrooms: UnStored bathrooms: UnStored price_min: UnStored price_max: UnStored date_available: Keyword bedrooms_1: <- Would prefer not to have do this as this makes the bedrooms harder to search. Next I need to be able to do a Range Query on the bedrooms bathrooms price_min and price_max fields. (Example: finding documents that have between 1 and 3 bedrooms) Zend_Search_Lucene will only allow ranged searches on the same field. From my understanding this means each field I want to do a ranged query on can only contain one value (example: bedrooms:""1 bedroom""); What I have now within the Lucene Document is the bedrooms bathrooms price_min price_max date_available fields being space delimited. Example: Sample Table_One Entry: | 5 | 2 | ""Sample Title"" | ""Sample Description"" | ""Sample Location"" | 2008-01-12 Sample Table_Two Entries: | 10 | 5 | 3 | 1 | 900 | 1000 | 2009-10-01 | 11 | 5 | 2 | 1 | 800 | 850 | 2009-08-11 | 12 | 5 | 1 | 1 | 650 | 650 | 2009-09-15 Sample Lucene Document id:5 owner_id:2 title: ""Sample Title"" description: ""Sample Description"" location: ""Sample Location"" date_registered: [datetime stamp YYYY-MM-DD] bedrooms: ""3 bedroom 2 bedroom 1 bedroom"" bathrooms: ""1 bathroom 1 bathroom 1 bathroom"" price_min: ""900 800 650"" price_max: ""1000 850 650"" date_available: ""2009-10-01 2009-08-11 2009-09-15"" [Question 2] Can you do a Range Query search on the bedroom bathroom price_min price_max date_available fields as they are shown above or does each range query field have to contain only one value (e.g. ""1 bedroom"")? I have not been able to get the Range Query to work in its current form. I am at a lose here. Thanks in advance. I suggest you create a separate Lucene document for each entry in Table_Two. This will cause some duplication of the Table_One information common to these entries but this is not a high price to pay for much easier index structure in Lucene. Use a boolean query to combine several range queries. The number-valued fields should be something like this: bedrooms: 3 price_min: 900 and a sample query in Lucene syntax will be: date_available:[20100101 TO 20100301] AND price_min:[600 TO 1000] Thanks. That is exactly what I did and it works great. Thanks you."
242,A,"Lucene HTMLFormatter skipping last character I have this simple Lucene search code (Modified from http://www.lucenetutorial.com/lucene-in-5-minutes.html)  class Program { static void Main(string[] args) { StandardAnalyzer analyzer = new StandardAnalyzer(); Directory index = new RAMDirectory(); IndexWriter w = new IndexWriter(index analyzer true IndexWriter.MaxFieldLength.UNLIMITED); addDoc(w ""Table 1 <table> content </table>""); addDoc(w ""Table 2""); addDoc(w ""<table> content </table>""); addDoc(w ""The Art of Computer Science""); w.Close(); String querystr = ""table""; Query q = new QueryParser(""title"" analyzer).Parse(querystr); Lucene.Net.Search.IndexSearcher searcher = new Lucene.Net.Search.IndexSearcher(index); Hits hitsFound = searcher.Search(q); SimpleHTMLFormatter formatter = new SimpleHTMLFormatter(""*"" ""*""); Highlighter highlighter = null; highlighter = new Highlighter(formatter new QueryScorer(searcher.Rewrite(q))); for (int i = 0; i < hitsFound.Length(); i++) { Console.WriteLine(highlighter.GetBestFragment(analyzer ""title"" hitsFound.Doc(i).Get(""title""))); // Console.WriteLine(hitsFound.Doc(i).Get(""title"")); } Console.ReadKey(); } private static void addDoc(IndexWriter w String value) { Document doc = new Document(); doc.Add(new Field(""title"" value Field.Store.YES Field.Index.ANALYZED)); w.AddDocument(doc); } } The highlighted results always seem to skip the closing '>' of my last table tag. Any suggestions? Solved. Apparently my Highlighter.Net version was archaic. Upgrading to 2.3.2.1 Solved the problem  Lucene's highlighter out of the box is geared to handle plain text. It will work incorrectly if you try to highlight HTML or any mark-up text. I recently ran into the same problem and found a solution in Solr's HTMLStripReader which skips the content in tags. The solution is outlined on my blog at following URL. http://sigabrt.blogspot.com/2010/04/highlighting-query-in-entire-html.html I could have posted the code here but my solution is applicable for Lucene Java. For .Net you have to find out equivalent of HTMLStripReader. Thanks for the link. +1 for that. My problem was something else though. Adding an answer for that"
243,A,"Hibernate Search querying? Greetings My domain model is as follows class Species { private String name; .. .. List<Family> families; } class Family{ private String name; private String locusId; .. List<Member> members; } class Members{ private String name; private String repTranscript; } I want to use 'Hibernate Search' to execute queries like org.hibernate.lucene.search.Query luceneQuery = parser.parse( ""name:ASpeciesName or name:AGroupName or locudID:someLocusID"" ); org.hibernate.Query fullTextQuery = fullTextSession.createFullTextQuery( luceneQuery ); List result = fullTextQuery.list(); I am wonderingsince all three classes has same field 'name' does it search agains all the classes? Does the 'result' has objects of all the types? It also depends on how you index. If you index each class separately (meaning each class has a @Indexed annotation) and you don't specify a expected class type when creating the FullTextQuery you get indeed mixed classes in the result. However in your example you might consider using @IndexedEmbedded on the attribute families and members. In this case the field names in the Lucene *Document*s will families.name and families.members.name. Have a look at the Hibernate Search online documentation and the embedded indexing feature. --Hardy  Logically yes because nowhere in the query have you specified the type of objects that you want. If you want to restrict the results to specific types you need to pass those types as a vararg list: fullTextSession.createFullTextQuery( luceneQuery A.class B.class ); This is described in the docs."
244,A,"How to get Zend Lucene Range Search working properly (or help me debug) I have an implementation of the Zend Search (Lucene) framework on my website that contains an index of products with prices. I am trying to allow customers to search for something while contsraining the prices. Eg. Search for ""dog food"" between $5-$10 dollars. My search index looks like this: Keyword('name') Keyword('price') Lets say there's 2 items in the database (name and price) 'dog food' 10 'dot treats' 11 If I do the following search I receive both results. price[1 TO 15] name:dog This is exactly what I want. However If I change the range to price[5 TO 15] I get no results back. Can anyone help me understand how to debug this? This behaviour occurs both in my web implementation and in Luke. Here's the deal: These values in Lucene are represented as strings and are sorted lexicographically. Therefore you need to pad them with zeros. Say the maximal price is 999 dollars you need to insert each price as a three-digit string: 001 005 015 etc. Then your query will be: price:[005 TO 015] name:dog Which should work. That worked beautifully! Thank you so much. I was pulling my hair out trying to figure out what was going on."
245,A,what's the best way to re-index just the models that changed during solr downtime? if I have millions of User records with some text fields getting indexed to solr on create and on update how do I go back and re-index the few records that never made it to solr? i.e. what if solr goes down for a few minutes during the day and about 300 records out of millions never got indexed. I don't want to re-index millions of records just the 300. found a great gem: https://github.com/bdurand/sunspot_index_queue A good way to manage this would be to just insert the record IDs into a queue table on create and update and then have a process that runs later to index the records. That way if Solr goes down you don't have to worry about which records weren't processed they'll just continue sitting in the queue until processed. The advantage of this is that your database doesn't have to wait for the solr update to complete before completing the transaction. The downside is that Solr isn't going perfectly in sync with what's in the database. You can adjust how often the queue reading program runs to accommodate your needs for that. excellent points. why am I letting this mission critical operation occur in realtime. we have other queues this should also be a queue. thanks. +1 for this. Queueing is absolutely the way to go with Solr updates. Another approach would be to save some kind of indexed_at timestamp on your records when they have been successfully indexed and use that to find batches that need to be updated.
246,A,"Mac User - How do i set CLASSPATHS in Mac (Im working on a Lucene Demo) Im trying to get my Apache Lucene demo to work and im down to setting the classpath in this tutorial http://lucene.apache.org/java/2_3_2/demo.html Ive hunted the web and these wer the 2 solutions I found to set CLASSPATH: CLASSPATH=${CLASSPATH}:/Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/lucene-core-3.0.3.jar and setenv CLASSPATH ${CLASSPATH}:/Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/lucene-core-3.0.3.jar The second one brings up a error -bash: setenv: command not found The first one seemed to accept ok but wen i tried the next step in the tutorial i got an error. The next step was to run the following: Phil-hunters-MacBook:webapps philhunter$ java org.apache.lucene.demo.IndexFiles /Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/src which gave me the error: Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/lucene/demo/IndexFiles This leads me to believe my CLASSPATHS didnt set correctly. Would I be right in assuming this? I have tried other tutorials and demos and see to get this same error quite a bit. Im new to Lucene and relatively new to mac and Unix shell scripting. Anyone know if I am setting the CLASSPATH correctly and if thats the cause of the errors? Still no luck with this. Im getting and:Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/lucene/demo/IndexHTML Caused by: java.lang.ClassNotFoundException: org.apache.lucene.demo.IndexHTML at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) i create a .bash_profile file in my home directory and do things like export GRAILS_HOME=/usr/share/grails ... export PATH=${GRAILS_HOME}/bin:${GROOVY_HOME}/bin:/usr/local/mysql-5.1.45-osx10.6-x86_64/bin:${PATH} you can work of that to set the classpath -- these examples show how to declare an environment variable and how to use the variable in other variables. This doesnt make sense to be about creating a .bash_profile file. Im not sure what to do. @user544006 -- what does not make sense? You need to create a file '.bash_profile' in your home directory and add the relevant commands to set up your environment. +1 The default shell on Mac OS X is `bash`. See also http://www.cs.sjsu.edu/faculty/froomin/Handouts/Shell_Ref_Page.html export PHIL_HOME=/Users/philhunter CLASSPATH=${CLASSPATH}:/Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/lucene-core-3.0.3.jar CLASSPATH=${CLASSPATH}:/Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/lucene-demo-3.0.3.jar ................... This is my file that i saved in /Users/philhunter called .bash_profile Is that correct??  in the terminal type $ vim ~/.bash_profile edit the file and add one line: export CLASSPATH=${CLASSPATH}:/usr/local/lucene-3.6.2/lucene-core-3.6.2.jar:/usr/local/lucene-3.6.2/contrib/demo/lucene-demo-3.6.2.jar; make sure to change the path of yours. In your way you lose to add lucene-demo-3.0.3.jar in your classpath.  When you set an environment variable like CLASSPATH then by default it only applies to the current process (i.e. the shell process itself) - it isn't available to the java process you launch in the next line. In order to make it available to other processes you need to ""export"" the variable. In this case you can use something like: export CLASSPATH=${CLASSPATH}:/Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/lucene-core-3.0.3.jar This basically says ""set the CLASSPATH variable to its current value plus the location of the lucene jar and make the new variable available to any processes launched from this shell"". However with java the usual way of setting the classpath is to do it as part of the java command itself using the -classpath or -cp options. In your case it would look something like: Phil-hunters-MacBook:webapps philhunter$ java -cp /Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/lucene-core-3.0.3.jar org.apache.lucene.demo.IndexFiles /Users/philhunter/Desktop/COM562\ Project/lucene-3.0.3/src As an aside the error you see when using the setenv line is because setenv is the command used in the C shell to set environment variables but the default Mac shell (and the shell you're using) is bash which doesn't recognise setenv and lets you know it doesn't recognise it with the error message: -bash: setenv: command not found."
247,A,"lucene delete record deprecated? When doing research on deleting documents in lucene I have been shown to use the IndexReaders delete() method passing in the document id. Now that I actually need to do this it appears that lucene currently does not support this method and I have had very little luck in finding the current way to do this. Any ideas? Doc IDs are internal to Lucene and really should never be used. They are liable to change without warning among other issues. How are you getting the doc IDs? Presumably through a query? Then just delete based on that query. Alternatively if you have your own unique ID field you can do writer.DeleteDocuments(new Term(""MyIDField"" ""ID to delete""));  now the deletions can be done with IndexWriter http://lucene.apache.org/java/3_0_2/api/all/org/apache/lucene/index/IndexWriter.html i need to be able to delete by the docid however. i have a large index and rebuilding the index with uniqueness is not a possibility as it is over 100gb. several indexes were merged together giving duplicate entries and i need to find a way to get rid of the ones that are duplicates. I found code to do it with the index readers .delete() method but now that its gone I am having a very difficult time finding a way to do this."
248,A,Preserving dots of an acronym while indexing in Lucene If i want Lucene to preserve dots of acronyms(example: U.KU.S.A. etc) which analyzer do i need to use and how? I also want to input a set of stop words to Lucene while doing this. A WhiteSpaceAnalyzer will preserve the dots. A StopFilter removes a list of stop words. You should define exactly the analysis you need and then combine analyzers and token filters to achieve it or write your own analyzer.  StandardTokenizer preserves the dots occurring between letters. You can use StandardAnalyzer which uses StandardTokenizer. Or you could create your own analyzer with StandardTokenizer. Correction: StandardAnalyzer will not help as it uses StandardFilter which removes the dots from the acronym. You can construct your own analyzer with StandardTokenizer and additional filters (such as lower case filter) minus the StandardFilter. thanks for ur comments...FYI i'm already using StandardAnalyzer in my code: protected readonly StandardAnalyzer _analyzer = new StandardAnalyzer(stop_words); but it removes dots from acronyms...
249,A,"Relevancy of Lucene search results I've following 3 Documents in Lucene index. As MBA you will play an integral role in implementing the strategy of the business and will have the responsibilities of the statutory accounts compliance audit including banking relationships tax treasury & cash management As M.B.A. you will play an integral role in implementing the strategy of the business and will have the responsibilities of the statutory accounts compliance audit including banking relationships tax treasury & cash management As Master of Business Administration you will play an integral role in implementing the strategy of the business and will have the responsibilities of the statutory accounts compliance audit including banking relationships tax treasury & cash management My search input is :MBA and the query I search execute on Lucene is: +((description:mba^3.0) (description:m.b.a.) (description:\""master business administration\"")) I get results in following sequence after sorting results by score in descending order: Document # 3 Document # 2 Document # 1 Shouldn't Record # 1 come on top of search results since I've given it a higher boost and also that document contains the exact word MBA?? What am i missing here? Thanks. The matching query string makes up about 10% of the content of Doc#3. but only a tiny fraction of #1 and #2. You might have to adjust your boosts to reflect the different lengths of the alternative query strings. Thanks a ton Richie!This logic worked for me! thanks for ur comment...?could you please let me know how much final search query should look like? @Ed: What I'm saying is that there's another boost factor to take into account which is the ratio of the length of your alternative query to the original. So because ""master business administration"" is 10 times longer than ""MBA"" you should be reducing its boost to something like one tenth: `(description:\""master business administration\""^0.1)`. That will eliminate the bias introduced by its extra length. (It may be that `^0.1` is too much - you'll need to experiment to determine the right relationship between the length ratio and the boost factor needed to offset it.)  If you are using Lucene's StandardAnalyzer docs #1 and #2 are actually equivalent and both will match the term ""mba"". It's hard to guess why #3 has the highest score - maybe because it matches multiple terms. You may want to consider handling synonyms like this at index time. I wouldn't guess the field length would be a big factor but what you probably want to do is use IndexSearcher.Explain() to get a breakdown of the scoring - that the best way to debug problems like this one."
250,A,Lucene.Net Search result to highlight search keywords I use Lucene.Net to index some documents. I want to show the user a couple of lines as to why that document is in the result set. just like when you use google to search and it shows the link and followed by the link there are a few lines with the keywords highlighted. any ideas? When you have a result you can get the indexed text pass it along with your query through a method similar to this: public string GeneratePreviewText(Query q string text) { QueryScorer scorer = new QueryScorer(q); Formatter formatter = new SimpleHTMLFormatter(highlightStartTag highlightEndTag); Highlighter highlighter = new Highlighter(formatter scorer); highlighter.SetTextFragmenter(new SimpleFragmenter(fragmentLength)); TokenStream stream = new StandardAnalyzer().TokenStream(new StringReader(text)); return highlighter.GetBestFragments(stream text fragmentCount fragmentSeparator); } You are a gem mate. thanks. Sorted me out but had to pass an arbitrary string value as a first parameter of TokenStream.
251,A,"Lucene - Wildcards in phrases I am currently attempting to use Lucene to search data populated in an index. I can match on exact phrases by enclosing it in brackets (i.e. ""Processing Documents"") but cannot get Lucene to find that phrase by doing any sort of ""Processing Document*"". The obvious difference being the wildcard at the end. I am currently attempting to use Luke to view and search the index. (it drops the asterisk at the end of the phrase when parsing) Adding the quotes around the data seems to be the main culprit as searching for document* will work but ""document*"" does not Any assistance would be greatly appreciated Fiddling with this. Possible workaround. Is there a way to do a proximity search with wildcards? Seems like this might cause a major hit on performance though. What you're looking for is FuzzyQuery which allows one to search for results with similar words based on Levenshtein distance. Alternatively you may also want to consider using slop of PhraseQuery (also available in MultiPhraseQuery) if the order of words isn't significant.  Not only does the QueryParser not support wildcards in phrases PhraseQuery itself only supports Terms. MultiPhraseQuery comes closer but as its summary says you still need to enumerate the IndexReader.terms yourself to match the wildcard.  Another alternative is to use NGrams and specifically the EdgeNGram. http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.EdgeNGramFilterFactory This will create indexes for ngrams or parts of words. Documents with a min ngram size of 5 and max ngram size of 8 would index: Docum Docume Document Documents There is a bit of a tradeoff for index size and time. One of the Solr books quotes as a rough guide: Indexing takes 10 times longer Uses 5 times more disk space Creates 6 times more distinct terms. However the EdgeNGram will do better than that. You do need to make sure that you don't submit wildcard character in your queries. As you aren't doing a wildcard search you are matching a search term on ngrams(parts of words).  It seems that the default QueryParser cannot handle this. You can probably create a custom QueryParser for wildcards in phrases. If your example is representative stemming may solve your problem. Please read the documentation for PorterStemFilter to see whether it fits.  I was also looking for the same thing and what i found is PrefixQuery gives u a combination of some thing like this ""Processing Document*"".But the thing is your field which you are searching for should be untokenized and store it in lowercase (reason for so since it is untokenized indexer wont save your field values in lowercase) for this to work.Here is code for PrefixQuery which worked for me :- List<SearchResult> results = new List<SearchResult>(); Lucene.Net.Store.Directory searchDir = FSDirectory.GetDirectory(this._indexLocation false); IndexSearcher searcher = new IndexSearcher( searchDir ); Hits hits; BooleanQuery query = new BooleanQuery(); query.Add(new PrefixQuery(new Term(FILE_NAME_KEY keyWords.ToLower())) BooleanClause.Occur.MUST); hits = searcher.Search(query); this.FillResults(hits results);  Lucene 2.9 has ComplexPhraseQueryParser which can handle wildcards in phrases. Great advice! Thanks.  Use a SpanNearQuery with a slop of 0. Unfortunately there's no SpanWildcardQuery in Lucene.Net. Either you'll need to use SpanMultiTermQueryWrapper or with little effort you can convert the java version to C#."
252,A,"Lucene security search asp.net c# Im hoping this would be a really easy question for someone.... Basically we are indexing security information against my documents in lucene.net the information is stored in 2 document fields called viewuserids and viewroleids so when we construct a query - only documents which the user has view access to are returned. The required functionality for a query is that we want only documents to be returned if a user belongs to the roles stored in viewroleids (this bit is working fine) however if viewuserids field contains any ids (the field may not contain any values) on the index the viewroleids should be ignored and only users who are present in viewuserids should be able to see the document. As mentioned above the role part works as expected but we need a little help on constucting a term query in the API to take into account the viewuserids (effectively overriding the viewroleids query. This is what we have so far: BooleanQuery bq = new BooleanQuery(); foreach (int roleId in roleIds) { bq.Add(new TermQuery(new Term(""viewroleid"" roleId.ToString()))BooleanClause.Occur.SHOULD); } bq.Add(new TermQuery(new Term(""viewuserid"" User.Id.ToString())) BooleanClause.Occur.SHOULD); Thanks in advance for any help! NOTE: both fields are stored in the index untokenised There are multiple ways to do this but here is one: Add a field ""hasviewuserids"" which contains ""TRUE"" if a document has any viewuserids associated with it and ""FALSE"" if not. So if for example the current userid is 3 and is in roles 5 and 6 the query would look like: (+(viewroleids:5 viewroleids:6) +hasviewuserids:FALSE) OR viewuserids:3 beautiful I almost got there myself: I assumed I would need to search on a field that was blank (which i dont think you can do in lucene) so I changed the index builder to add a zzz-zzz-zzz value into viewuserid if no users were present that way I could query the index like this (using your syntax): (+(viewroleid:24 viewroleid:6) +viewuserid:ZZZ-ZZZ-ZZZ) OR viewuserid:100079 thanks bajafresh4life! I just need to write this using the API now...gulp for anyone interested i implemented like this:"
253,A,Denormalization of database tables for Lucene indexing I am just starting up with Lucene and I'm trying to index a database so I can perform searches on the content. There are 3 tables that I am interested in indexing: 1. Image table - this is a table where each entry represents an image. Each image has an unique ID and some other info (title description etc). 2. People table - this is a table where each entry represent a person. Each person has a unique ID and other info like (name address company etc) 3. Credited table - this table has 3 fields (image person and credit type). It's purpose is to associate some people to a image as the credits for that image. Each image can have multiple credited people (there's the director photographer props artist etc). Also a person is credited in multiple images. I'm trying to index these tables so I can perform some searching using Lucene but as I've read I need to flatten the structure. The first solution the came to me would be to create Lucene documents for each combination of Image/Credited Person. I'm afraid this will create a lot of duplicate content in the index (all the details of an image/person would have to be duplicated in each Document for each person that worked on the image). Is there anybody experienced with Lucene that can help me with this? I know there is no generic solution to denormalization that is why I provided a more specific example. Thank you and I will gladly provide more info on the database is anybody needs PS: Unfortunately there is no way for me to change the structure of the database (it belongs to the client). I have to work with what I have. You could create a Document for each person with all the associated images' descriptions concatenated (either appended to the person info or in a separate Field). Or you could create a minimal Document for each person create a Document for each image puts the creators' names and credit info in a separate field of the image Document and link them by putting the person ID (or person Document id) a third non-indexed field. (Lucene is geared toward flat document indexing not relational data but relations can be defined manually.) This is really a matter of what you want to search for images or persons and whether each contains enough keywords for search to function. Try several options see if they work well enough and don't exceed the available space. The credit table will probably not be a good candidate for Document construction though. Thanks for the answer. If possible I would like to search both (images and persons) and based on relations between them. I've read that manually defining relations between Documents forces me to do additional merging on the results. But I was just thinking. What if I create 2 indexes: one containing the each image and people that worked on it (concatenated) and another index containing each person and the images that he worked on. Would it be too much to always make 2 searches and then somehow choose the best results from each and merge them? @Andrei: making two indexes and doing two searches complicates matters as you'll have to devise a way to do the merging. It's ok to have two kinds of documents in a single index though; I'd go that route if you want to have both. thanks. I will try that.
254,A,"Zend Search Lucene And Persian Language ! i have the following code in my zf project : $index = Zend_Search_Lucene::open(APPLICATION_PATH . '/cache/search_index'); $doc = new Zend_Search_Lucene_Document(); $title = ""سلام سینا xxx sad""; $doc->addField(Zend_Search_Lucene_Field::Text('title' $title)); $index->addDocument($doc); $index->commit(); $index->optimize(); echo ""Index contains "" . $index->count() . "" documents.\n\n""; $results = $index->find('xxx'); foreach ($results as $res) { var_dump($res->title); } when var_dump performs output -> string(39) ""Ø³ÛŒÙ†Ø§ Ø¬Ø§Ù† xxx sad"" when i user utf_decode string(25) ""س�?ا�? س�?�?ا xxx sad"" how can i decode that correctly ! :(? i already used the solution in this SOF quesion -> lucene encoding problem in zend framework but not works and a notice error added about iconv ! plz help :) Fixed by this code: $index = Zend_Search_Lucene::open(APPLICATION_PATH . '/cache/search_index'); $doc = new Zend_Search_Lucene_Document(); $title = ""سلام سینا xxx sad""; $doc->addField(Zend_Search_Lucene_Field::Text('title' $title""UTF8"")); $index->addDocument($doc); $index->commit(); $index->optimize(); echo ""Index contains "" . $index->count() . "" documents.\n\n""; var_dump($index->getDocument(9)); echo ""Search""; $results = $index->find('سینا'); foreach ($results as $res) { var_dump($res->title); } die(1);"
255,A,"Solr - how to ""group by"" and ""limit""? Say I indexed the following from my database: ====================================== | Id | Code | Description | ====================================== | 1 | A1 | Hello world | | 2 | A1 | Hello world 123 | | 3 | A1 | World hello hi | | 4 | B1 | Quick fox jumped | | 5 | B1 | Lazy dog | ... Further say the user searches for ""hello"" which should return records 1 2 and 3. Is there a way to make Solr ""group by"" the Code field and apply a limit (say 10 records)? I'm somewhat looking for a SQL counterpart of GROUP BY and LIMIT. Also when it does this ""group by"" I want it to choose the most relevant document and use that document's Description field as part of the return. Of course I could just have Solr return everything to my application and I can manipulate the results to do the GROUP BY and LIMIT. I'd rather not do this if possible. Have a look at field collapsing available in Solr 4.0. Sorting groups on relevance: group.sort=score desc. This feature is also available in Solr v3.3+ @Karl Johansson how can that be done using django haystack? like `searchQuerySet().models(Product).group('field:""title""')`"
256,A,"Why did they create the concept of ""schema.xml"" in Solr? Lucene does searching and indexing all by taking ""coding""... Why doesn't Solr do the same ? Why do we need a schema.xml ? Whats its importance ? Is there a way to avoid placing all the fields we want into a schema.xml ? ( I guess dynamic fields are the way to go right ? ) That's just the way it was built. Lucene is a library so you link your code against it. Solr on the other hand is a server and in some cases you can just use it with very little coding (e.g. using DataImportHandler to index and Velocity plugin to browse and search). The schema allows you to declaratively define how each field is analyzed and queried. If you want a schema-less server based on Lucene take a look at ElasticSearch. Hmmm... Is there a way we can bypass schema and still use Solr like we use Lucene ? ""beast"" -> true :D @Shrinath: No. If you want Lucene use Lucene. Solr is a different beast.  If you want to avoid constantly tweaking your schema.xml then dynamic fields are indeed the way to go. For an example I like the Sunspot schema.xml — it uses dynamic fields to set up type-based naming conventions in field names. https://github.com/outoftime/sunspot/blob/master/sunspot/solr/solr/conf/schema.xml Based on this schema a field named content_text would be parsed as a text field: <dynamicField name=""*_text"" stored=""false"" type=""text"" multiValued=""true"" indexed=""true""/> Which corresponds to its earlier definition of the text fieldType. Most schema.xml files that I work with start off based on the Sunspot schema. I have found that you can save a lot of time by establishing and reusing a good convention in your schema.xml.  Solr acts as a stand-alone search server and can be configured with no coding. You can think of it as a front-end for Lucene. The purspose of the schema.xml file is to define your index. If possible I would suggest defining all your fields in the schema file. This gives you greater control over how those fields are indexed and it will allow you to take advantage of copy fields (if you need them). Is there a way we can take control by coding ? Like we do in Lucene ?"
257,A,"what's the best way to search a social network by prioritizing a users relationships first? I have a social network set up and via an api I want to search the entries. The database of the social network is mysql. I want the search to return results in the following format: Results that match the query AND are friends of the user performing the search should be prioritized over results that simply match the query. So can this be done in one query or will I have to do two separate queries and merge the results and remove duplicates? I could possibly build up a data structure using Lucene and search that index efficiently but am wondering if the penalty of updating a document everytime a new relationship is created is going to be too much? Thanks One way is to store all your social network graph separately from Lucene. Run your keyword query on Lucene and also lookup all the friends in your network graph. For all the friends that are returned boost all of those friends' search results by some factor and resort. This re-sort would be done outside of Lucene. I've done things like this before and it performs pretty well. You can also create a custom HitCollector that does the boosting as the hits are being collected in Lucene. You'd have to construct a list of internal Lucene ID's that belong to the friends of the current user. Your social network graph can be stored in Mysql in memory as a sparse adjacency matrix or you can take a look at Neo4j.  The reference to Lucene complicates the equation a little bit. Let's solve it (or at least get a baseline) without it first. Assuming the following datamodel (or something approaching.  tblUsers UserId PK UserName Age ... tblBuddies UserId FK to tblUsers.UserId FriendId tblUsers.Userid = Id of one of the friends BuddyRating float 0.0 to 1.0 (or whatever normalized scale) indicating the level of friendship/similarity/whatever tblItems ItemId PK ItemName Description Price ... tblUsersToItems UserId FK to tblUsers.UserId ItemId FK to ItemRating float 0.0 to 1.0 (or whatever normalized scale) indicating the ""value"" assigned to item by user. A naive query (but a good basis for an optimized one) could be:  SELECT [TOP 25] I.ItemId ItemName Description SUM(ItemRating * BuddyRating) FROM tblItems I LEFT JOIN tblUserToItems UI ON I.ItemId = UI.ItemId LEFT JOIN tblBuddies B ON UI.UserId = B.FriendId WHERE B.UserId = 'IdOfCurrentUser' AND SomeSearchCriteria -- Say ItemName = 'MP3 Player' GROUP BY I.ItemId ItemName Description ORDER BY SUM(ItemRating * BuddyRating) DESC The idea is that a given item is given more weight if it is recommended/used by a friend. The extra weigh is the more important if the friend is a a close friend [BuddyRating] and/or if the friend recommend this item more strongly [ItemRating] Optimizing such a query depends on the overal number of item the average/max numbers of buddies a given user has the average/max number of items a user may have in his/her list. Is this type of ideas/info you are seeking or am I missing the question?"
258,A,How to get Highlighting response for all fields with SolR hi all: I have a document with two fields name and text the content of them are same. but when I use highlighting query the response only return one field which is in parameter q. eg: htp://127.0.0.1:8983/solr/select/?q=name:sony&hl=true&hl.fl=nametext this only return name in highlighting response htp://127.0.0.1:8983/solr/select/?q=text:sony&hl=true&hl.fl=nametext this only return text in highlighting response I want to get all field with highlighting is the hl.fl useless? thanks in advance for any help. What you have should work. The one thing that stands out to me is the name of the other field you have is 'text' I would try changing the name of that field and try this again. That just looks like a word you might not want to use cause it could be reserved somewhere along the lines. Can you do a q=text:sony ? Does it give the same result? text is not reserved. its the default search field in solr. and normally a lot of fields are copied into this. @Illu: so maybe somewhere is a copyField directive (schema.xml) which copies 'name' into 'text' and messed up sth?
259,A,"Which search technology to use with ASP.NET? What's your preferred method of providing a search facility on a website? Currently I prefer to use Lucene.net over Indexing Service / SQL Server full-text search (as there's nothing to set up server-side) but what other ways are being used out there? Take a look at Solr. It uses Lucene for text indexing but it is a full blown http server so you can post documents over http and do search using urls. The best part is that it gives you faceted searching out of the box which will require a lot of work if you do it yourself. Solr also provides other features that you might end up reimplementing by going with Lucene.Net. For example: Solr can replicate indexes from one machine to another which can help with failover or performance. Solr can help deal with gigantic indexes by automatically distributing a query across mutliple ""shard"" servers. With DataImportHandler Solr can crawl a SQL database and index the data contained therein without additional code (just XML config). The main potential downside is that if you need to customize the search code you'll now have to do that customization in Java not .NET. Let it be known there are downsides to Solr. HTTP is an expensive and wasteful protocol - compared to native code running on the same machine as the web-application. If you have redundancy built into your application layer already RE-CENTRALIZING the search by aggregating all searches to a single (or few) servers is not ideal for performance either. By using Solr it becomes much more difficult to manage ever-changing indexes. It also is not a good paradigm fit if your applications have MANY different Lucene indexes or you search multiple indexes at once as part of your implementation.  If you need to index all the pages of your site (not just the ones Google indexes) or if you want to create a search for your intranet web sites the Google Mini is pretty sweet. It will cost you some money but it is really easy to have it up and running within just a couple of hours. Depending on how many pages you need to index it can be expensive though.  I'm using dtSearch and I (kind of) like it. The API isn't the greatest in the world for .NET but it can get the job done and it's pretty fast. And it's cheap so your boss will like it (~$1000 US). The results leave something to be desired as it doesn't do any kind of semantic relevance rankings or anything fancy. It does a better job than anything you can get out of MS SQL server though. It has a web spider that makes it easy to do quick search apps on a website. If you need to you can use the API to create hooks into your database and to provide item level security - but you have to do the work yourself. Their forum leaves something to be desired as well but maybe people will start posting dtSearch stuff here. :)  A lot of people are using Google's custom search these days; even a couple of banks that I know of use it for their intranet.  dtSearch is one we've often used but I'm not really that big a fan of it.  We used both Lucene.net Indexing Service and SQL Server full-text. For a project with large and heavy DB search functionality SQL search has an upper hand in terms of performance/resource hit. Otherwise Lucene is much better in all aspects.  Has anyone tried Microsoft search server express? http://www.microsoft.com/enterprisesearch/serverproducts/searchserverexpress/default.aspx I haven't tried it yet but it could potentially be powerful. From the site it looks primarily geared towards sharepoint users but given its sdk I don't see why you couldn't use it for a regular old site search  I also recommend SOLR. It's easy to set up maintain and configure. I've found it to be stable and easy to scale. There's a c# package for interfacing with solr.  you could use google it's not going to be the fastest indexer but it does provide great results when you have no budget."
260,A,"Purpose / importance of the file modification date methods in Lucene's Directory? I've built a Lucene Directory implementation for jdbm an embedded Java database. Part of the Directory API are two methods related to ""file"" modification dates: touchFile and fileModified (javadoc). My question is what is the purpose of these methods? I've searched the entire Lucene core source tree and found no usage of these methods at all. FWIW my interest is the fact that I'd like to not bother tracking mod dates given that I'm using an embedded datastore and such metadata isn't ""free"" as it is in a regular filesystem. I confirmed via the lucene-users mailing list that the methods in question are not used by any Lucene codebase but that it's possible (though very unlikely it seems to me) that applications using one's Directory implementation would expect those methods to function properly. shrug"
261,A,"Can I share a value across multiple fields in Lucene? I'm wondering if it's possible to add a value to multiple fields at once in Lucene so that I don't have to replicate the value across multiple fields and waste space unnecessarily. For example suppose I have a record representing a book and I have several sources for the book's summary perhaps Wikipedia Amazon Library of Congress. Let's suppose I have a specific field in the index to store each of these e.g. ""summary.wikipedia"" etc. At the same time I want to have a general field name called just ""summary"" that I can set to one of the specific summaries so that queries on the index can just search the ""summary"" field and not have to specify which summary they are wanting to search. What I want to be able to do is specify multiple field identifiers when adding a field so that the value can be shared across those fields without replicating the data and wasting space. E.g.: document.AddField( new string[] { ""summary.wikipedia"" ""summary"" } ""Summary of the book..."" ... ); Any chance this is possible? Or do I simply have to add the field twice with the same data but a different field name? Lucene doesn't really have a notion of ""relationships."" So the idea of having a field which is a pointer to another field is semi-anathema. I suppose if I can't do this I could always add the general ""summary"" field as indexed but not stored but it would still be difficult to get the original summary not knowing which stored field it originated from. There is no way to do this in Lucene. As you stated in your comment you can always make the general summary field an indexed but not stored field. Then you can add another field called ""summary_source"" that tells you which field the summary came from. That seems like an acceptable solution. Thanks :)"
262,A,"Should I use Lucene only for search? Our website needs to give out data to the world. This is open-source data that we have stored and we want it to make it publicly available. It's about 2 million records. We've implemented the search of these records using Lucene which is fine however we'd like to show an individual record (say the user clicks on it after the search is done) and provide more detailed information for that record. This more detailed information however isn't stored in the index directly... there are like many-to-many relationships and we use our relational database (MySQL) to provide this information. So like a single record belongs to a category we want the user to click on that category and show the rest of the records within that category (lots more associations like this). My question is should we use Lucene also to store this sort of information and retrieve it through simple search (category:apples) or should MySQL continue doing this logical job? Should I use Lucene only for the search part? EDIT I would like to point out that all of our records are pretty static.... changes are made to this data once every week or so. Lucene's strength lies in rapidly building an index of a set of documents and allowing you to search over them. If this ""detailed information"" does not need to be indexed or searched over then don't store it in Lucene. Lucene is not a database it's an index. Actually this detailed information is searched over... like a category... say a record belongs in the category 'apples'... then this is searchable and in fact it's stored in the index... however when the user clicks on 'apple' we use MySQL to retrieve that category data instead of maybe sending 'apple' in to Lucene and getting the data that way... what's the best approach?  You want to use Lucene to store data? I thing it's ok I've used Solr http://lucene.apache.org/solr/ which built on top of Lucene to work as search engine and store more data relate to the record that maybe use for front end display. It worked with 500k records for me and 2mil records I think it should be fine. I'm sorry but this is not related to my question. I'm asking if Lucene should **only** be used for search."
263,A,"Boost fresh documents with Lucene Does Lucene provide a means to boost fresh documents? For example suppose that the Lucene document includes a date field. Is it possible without having the user to alter her query anyhow to present the most recent documents with a higher score? I do not want to resort to a coarse ""sort by date"" solution as it will completely cancel the scoring algorithm. Use Document.setBoost(float value) when putting documents into the index. You can either constantly re-adjust the value on existing documents OR have a float value that increments with date so that you only need to apply it to the time that documents are inserted. For example start with a boost value of 0 for day 1 documents. Each day increment the boost by 1. It's a float value incrementing by 365 each year will last a long time. You may have to experiment with the strength of the boost to get the effect you want. Thank you for your answer. It is very smart in theory but I will have to test it thoroughly to see if it actually works. Anyway very smart indeed which raises the question why it is undocumented. One would expect to find it in a cookbook or something..  You can see Lucene in Action. In the second edition pg. 187 they give a way to do it. Basically you will want to write your own query which extends CustomScoreQuery and adds a boost. Nice answer too! Thanks for the hint!"
264,A,"Lucene: what are the hits for the query? I've just seen in 1 totalHits and scoreHits return the top and total number of hits for the query respectively. What does ""hit"" mean exactly ? Is it the position of the searched term in a document or the documents in which the term is included... or what ? thanks The totalHits is the number of documents that matched the query. A hit is essentially a match for the query you entered. However a hit may be a partial match or a full match e.g. If we use Lucene to index over a set of 3 texts: { “hello world” “hello sailor” “goodnight moon” } then searching for: hello world total hits: 2 1.078 hello world 0.181 hello sailor The float value indicates the score for the hit which is the relevance to the query string. The following post gives more details http://lingpipe-blog.com/2009/02/18/lucene-24-in-60-seconds/ ok however totalHits is still a bit confusing. It is a method of the class TopDocs which are the top relevant docs (in my case I've set the limit to 20 docs. TopDocs docs = searcher.search(query 20); So totalHits should be always 20... ?! @Patrick totalHits will only be 20 if there are 20 actual hits. What you have done is limit the totalHits to 20 in the case where there are more than 20 actual matches."
265,A,"Solr - Error when posting an ""Add"" to the server I'm Posting the following to the Solr Server: <add> <doc> <field name=""uniqueid"">5453543</field> <field name=""modifieddate"">2008-12-03T15:49:00Z</field> <field name=""title"">My Record</field> <field name=""description"">Descirption Details </field> <field name=""startdate"">2009-01-21T15:26:05.680Z</field> <field name=""enddate"">2009-01-21T15:26:05.697Z</field> <field name=""Telephone_number"">11111 111 111(text phone)</field> <field name=""Telephone_number"">11111 111 111</field> <field name=""Mobile_number"">1111111111</field> </doc> </add> I'm using SolrNet to send the documents here's an extract from the code (s is the above xml): public string Post(string relativeUrl string s) { var u = new UriBuilder(serverURL); u.Path += relativeUrl; var request = httpWebRequestFactory.Create(u.Uri); request.Method = HttpWebRequestMethod.POST; request.KeepAlive = false; if (Timeout > 0) request.Timeout = Timeout; request.ContentType = ""text/xml; charset=utf-8""; request.ContentLength = s.Length; request.ProtocolVersion = HttpVersion.Version10; try { using (var postParams = request.GetRequestStream()) { postParams.Write(xmlEncoding.GetBytes(s) 0 s.Length); using (var response = request.GetResponse()) { using (var rStream = response.GetResponseStream()) { string r = xmlEncoding.GetString(ReadFully(rStream)); //Console.WriteLine(r); return r; } } } } catch (WebException e) { throw new SolrConnectionException(e); } } When it gets to request.GetResponse it failed with this error: base {System.InvalidOperationException} = {""The remote server returned an error: (500) Internal Server Error.""} When i look on the server in the Logs for apache it gives the following reason: Unexpected end of input block in end Here's the full stack trace: Sep 17 2009 10:13:53 AM org.apache.solr.common.SolrException log SEVERE: com.ctc.wstx.exc.WstxEOFException: Unexpected end of input block in end tag at [rowcol {unknown-source}]: [261266] at com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOB(StreamScanner.java:700) at com.ctc.wstx.sr.StreamScanner.loadMoreFromCurrent(StreamScanner.java:1054) at com.ctc.wstx.sr.StreamScanner.getNextCharFromCurrent(StreamScanner.java:811) at com.ctc.wstx.sr.BasicStreamReader.readEndElem(BasicStreamReader.java:3211) at com.ctc.wstx.sr.BasicStreamReader.nextFromTree(BasicStreamReader.java:2832) at com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1019) at org.apache.solr.handler.XmlUpdateRequestHandler.processUpdate(XmlUpdateRequestHandler.java:148) at org.apache.solr.handler.XmlUpdateRequestHandler.handleRequestBody(XmlUpdateRequestHandler.java:123) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1204) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:303) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:232) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293) at org.apache.coyote.http11.Http11AprProcessor.process(Http11AprProcessor.java:859) at org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:574) at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1527) at java.lang.Thread.run(Thread.java:619) Please note the Solr server is running on the following system: Microsoft Windows Server 2003 R2 Apache Tomcat 6 Finally here's my question: The Xml i'm sending looks ok to me.. Does anyone have an ideas as to why Solr is throwing this exception? Thanks Dave Edit Answer is as follows: public string Post(string relativeUrl string s) { var u = new UriBuilder(serverURL); u.Path += relativeUrl; var request = httpWebRequestFactory.Create(u.Uri); request.Method = HttpWebRequestMethod.POST; request.KeepAlive = false; if (Timeout > 0) request.Timeout = Timeout; request.ContentType = ""text/xml; charset=utf-8""; request.ProtocolVersion = HttpVersion.Version10; try { // Set the Content length after the size of the byte array has been calculated. byte[] data = xmlEncoding.GetBytes(s); request.ContentLength = s.Length; using (var postParams = request.GetRequestStream()) { postParams.Write(data 0 data.Length); using (var response = request.GetResponse()) { using (var rStream = response.GetResponseStream()) { string r = xmlEncoding.GetString(ReadFully(rStream)); //Console.WriteLine(r); return r; } } } } catch (WebException e) { throw new SolrConnectionException(e); } } Looking at the latest version of SolrNet.. this fix is already in there :) Phew I thought that bug had come back :-) I think the version we're using is a good 11 months old.. so i must get around to updating it to the latest version. Sorry to scare you :) I'm not much familiar with .Net or Solr or .Net port of Solr. But here is my guess. postParams.Write(xmlEncoding.GetBytes(s) 0 s.Length); There are two possible errors. When you are getting bytes from String you should specify the encoding. It might be the case that the default encoding is different from UTF8 which you have set in content type header in response. The third parameter in Write() probabably refers to the length of byte array which you got from GetBytes(). The byte array could be longer than the length of string. You the man.. Once corrected the length problem.. I got another problem.. It turns out that the default Solr install doesn't accept Non ASCII characters as part of a request.. http://wiki.apache.org/solr/SolrTomcat#head-20147ee4d9dd5ca83ed264898280ab60457847c4 So i'll try and sort it out.. I'll make sure i contribute back to the Solr.Net project as well so no one else gets this problem. Hi.. thanks for the guess.. The encoding is UTF8... It's declared at the top of the class like this: private Encoding xmlEncoding = Encoding.UTF8; Hey.. Your second point was bang on I just did a ""Fiddler2"" on the POST and it misses off the last 2 digits of the XML so the becomes"
266,A,"Split string into meaningful words I am developing an application in Java which will parse a XML file and retrieve keywords from it and store it in my database. These keywords can then be searched by users and they can retrieve the related data. Now the problem is that the XML file has words like ""literacy_male""""infantmortalityrate_female"" etc. For the first one I can split the words at ""_"" before storing but for the second one I am not sure how i can split the word into meaningful words. I am using Apache Lucene to do the full text search. Just to be clear i want to split ""infantmortalityrate_female"" into ""infant mortality rate female"" define ""meaningful words"" 'who' decides which word is meaningful and which is not? there are infinite number of meaningful names because NAMES are also meaningful although they might not appear in any lexicon meaningful in the sense that if i search for infant mortality rates for female i am able to retrieve results corresponding to keyword ""infantmortalityrate_female"" which i cant till the time I split it I don't think you have any way to know which word is meaningful in indexing time [without relying on extra information] you might need to index all substrings [which is about O(n^4) strings where n is the string's size] if i cant convert it can you suggest a way through which i can atleast search for infant mortality rate in my database. Because currently when i store ""infantmortalityrate"" in my database and search it using Lucene full text i do not get any result SQL query (in at least some versions) supports a ""LIKE"" and ""GLOB"" qualifiers that can find (with appropriate wildcards) a substring in a field not separated by any specific delimiters so you could find eg ""infant"". Dunno about Lucene -- it reputedly has ""wildcard queries"" but can't tell if they're what you might need. one possibility is increasing the index size by adding all substrings of the exact same string. so for ""abc"" you will store: ""a""""b""""c""""ab""""bc""""abc"" (it's O(n^2) strings). one more possibility is using wildcards. index whatever you have and search for: <term>*a*<term>*...z*<term>* instead of for <term>. it will take a LOT more time but it will not increase the index size. note: it is necessary to search for so many terms because you CANNOT use wildcard as first letter of a term. a*<term>* means search for all terms start with a then have none or any chars then <term> and then none or any chars again. more info about terms and wild cards in lucene: http://lucene.apache.org/java/2_0_0/queryparsersyntax.html EDIT: a combination of those will provide (in my opinion) the best solution: index all suffixes of the string and then for each term (and not query!) - instead of searching for <term> search for <term>*. if the term exist as a substring it also starts at least one prefix and it will find it. for example: if you have ""lifeexpectancy"" you will index: ""lifeexpectancy""""ifeexpectancy""""feexpectancy""""eexpectancy""....""y"" for the same example when you want to search life expectancy you will search life* expectancy* thanks I will try this @user658209: making sure you saw the edit (combination of solutions) which is in my opinion better solution then the 2 I previously suggested. didnt understand your edit for example i have lifeexpectancy I will index it as lifeexpectancy and search for a*life expectancy?? @user658209: no index: ""lifeexpectancy""""ifeexpectancy""""feexpectancy""""eexpectancy""....""y"" and search for: life* expectancy*. I added this example to my answer.  computer are not intelligentthey understand what you tell 'em.So it would be easier if you maintain some standard while generating your XML file.otherwise i dont think there is any way to convert ""infantmortalityrat"" into ""infant+mortality+rate"" if i cant convert it can you suggest a way through which i can atleast search for infant mortality rate in my database. Because currently when i store ""infantmortalityrate"" in my database and search it using Lucene full text i do not get any result if you can't alter the XML can you modify DB field names according to your XML keywords?  yes it is possible to split string into words even if there are no split characters. This can be solved pretty efficient near O(n). Consider using prefix string regular expression and extract word by word from you string. You can check this tool as well http://code.google.com/p/graph-expression/wiki/RegexpOptimization. There are more robust(more effective couse it use global optimisation not local as previos) approach using spell checking automaton which is searching for most propable split of string. Check this tutorial on how its done on Chinese word strings http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html  You'll need to set some rules about how the XML-File must be formated in order to get this working. I guess you can't manipulate the XML-File (or it is already created and populated)? If you can (or it's being generated by your code) you'll need to set some rules like Keywords a separated by an  Keywords have no spaces but use _ instead With this rules you'll be able to write a parser which can make sense of your keyword-strings. If you can't do that you'll need to parse a keyword and try the different parsings (like ""split by _"") and see which one makes the best output. But this will be challenging and causes time. Please also add a sample of your XML-file to your original question. I cannot manipulate the XML file in anyway as it is generated by a standard process which is out of my control.The file will have keywords stored as follows:total_populationinfantmortalityrate_femalemedianage_female for ""total_population"" I can split it using java function at ""_"" but for ""infantmortalityrate_female"" If i do not split it and use Lucene to search for it I don't get any result Yes you can. But how do you plan to determine which one to use? There must be some kind of rule to that... Also how is Java supposed to know where to split `infantmortalityrate` into three words?  If you'd have database of strings that can be contained in that string you could do this: Split the string by separators you can identify (like _-...) and after each part could be broken to as many parts as you can identify by sum of shortest strings in DB like it you have string in 10 chars and shortest string in DB is 4 chars you can get these combos: 46 55 64 10 no 442 or sth like this and after that you can look up each part in DB and if every part is present you can say it is divided into ""meaninfull words"" but without that database or with too common dictionary you can stuck on this or it could be almost impossible database is irrelevant regarding information retrieval names technical words and shortcuts will not appear in any dictionary while they are valid search terms. sure that's why I said ""database of string that can be contained"" so we're not talking about common dictionary  There's no purely algorithmic way to accomplish your goal nor is there a way to do it with high reliability. You'd basically need to have a dictionary of ""meaningful"" words to search and ""peel"" off each word in a long combo after searching the dictionary for the longest word that was a prefix of your combo. But you can run amok if eg you have ""workmanhours"" and you parse it into ""workman"" ""hours"" when it maybe should be ""work"" ""man"" ""hours"". You could possibly finesse your search scheme by indexing selected character sequences rather than words. Eg build an index of all sequences that start with a leading vowel and then similarly strip your search terms down to a leading vowel. database is irrelevant regarding information retrieval names technical words and shortcuts will not appear in any dictionary while they are valid search terms. Well the OP didn't say they were technical terms. But you're right to a degree that where words are combined as he describes they very likely are often abbreviated or initialized to the degree that they can't be easily distinguished as words even with a dictionary."
267,A,lucene larger than Does anybody of you guys know how to search all the numbers larget than a specified one? for example: all the document number > 65 i tried like: documentNumber: [65 TO *] but i receive exception as lucene expected to parse a number there not a *. Thanks in advance! I know nothing about Lucene really but just as a random thought have you tried just using a very big number instead of *? [65 TO 99999999] Pick the largest number the datatype can handle (assuming there is such a thing) or at least a larger number than would possibly be used.  Jon is almost right but you also need to pad your numbers as numerical fields are ordered lexicographically. Thus 1243 is considered smaller than 65. Suppose you have 20000 documents. You have to pad document numbers below 10000 with leading zeros such as 00065 01243 etc. The exact syntax for your query will be documentnumber:{00065 TO 20000]  as you do not want 65 in the range. Please see this question for details and the official syntax.
268,A,"Get stemmed word in Lucene In Lucene I use the SnowballAnalyzer for indexing and searching. When I have the index built I make queries on my index. For example I make a query 'specialized' for the field 'body'. IndexSearcher returns documents containing 'specialize specialized etc.' because of the stemming done by the SnowballAnalyzer. Now - having top documents - I want to get a text snippet from the body field. This snipped should contain the stemmed version of the query word. For example one of the returned documents has the body field: ""Unfortunately in some states blind people only have access to general rehabilitation agencies which serve people with a variety of disabilities. In these cases specialized services for visually impaired people are not always available."" Then I wish to get the part 'In these cases specialized services for visually' as the snippet. Additionally I want to have terms from this snippet. Code which will do it but with one marked '?' character where I have a question is: How I want to do it is IndexReader ir = IndexReader.open(fsDir); TermPositionVector tv = (TermPositionVector)ir.getTermFreqVector(hits.scoreDocs[i].doc ""body""); ? - here: query - query has to be the term. So if the real query was 'specialized' then the query should be specialize what normally the snowball analyzer does. How can I get the term analyzed by the analyzer for a single word or a phrase since query can contain a phrase: ""specialized machines"". int idx = tv.indexOf(query); int [] idxs = tv.getTermPositions(idx); for(String t : tv.getTerms()){ int iidx = tv.indexOf(t); int [] iidxs = tv.getTermPositions(iidx); for(int ni : idxs){ tmpValue = 0.0f; for(int nni : iidxs){ if(Math.abs(nni-ni)<= Settings.termWindowSize){ edit I found the way to get the stemmed term: Query q = queryParser.parse(""some text to be parsed""); String parsedQuery = q.toString(); There is a method for the Query object toString(String fieldName); I believe you are mixing several questions. First to see the stemmed version of your query and other useful information you can use the IndexSearcher's explain() method. Please see my answer to this question. The Lucene solution for getting snippets is the Highlighter. Another option is the FastVectorHighlighter. I believe you can customize both to get the stemmed term rather than the full one. Thx for your reply. Please see my post update to see a way to get the stemmed term."
269,A,"Error while starting Hibernate Search in Web app can someone help I placing hibernate search in my web app and getting the following error: <?xml version=""1.0"" encoding=""UTF-8""?> <!DOCTYPE hibernate-configuration PUBLIC ""-//Hibernate/Hibernate Configuration DTD 3.0//EN"" ""http://hibernate.sourceforge.net/hibernate-configuration-3.0.dtd""> <hibernate-configuration> <session-factory> <property name=""hibernate.connection.driver_class"">org.postgresql.Driver</property> <property name=""hibernate.connection.url"">jdbc:postgresql://localhost/postgres</property> <property name=""hibernate.connection.username"">postgres</property> <property name=""hibernate.connection.password"">noor</property> <property name=""hibernate.connection.pool_size"">10</property> <property name=""show_sql"">true</property> <property name=""hibernate.dialect"">org.hibernate.dialect.PostgreSQLDialect</property> <property name=""hibernate.hbm2ddl.auto"">update</property> <property name=""current_session_context_class"">thread</property> <property name=""hibernate.search.default.indexBase""> /users/application/indexes </property> <property name=""hibernate.search.default.indexBase""> /users/application/indexes </property> <mapping resource=""com/BiddingSystem/Models/Users.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/ForumTopic.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/ForumMessage.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Administrator.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/PersonalUser.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/BusinessUser.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/BusinessContactNumbers.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Attribute.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/AttributeOption.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Category.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Item.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Auction.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Picture.hbm.xml""/> <mapping resource=""com/BiddingSystem/Models/Bid.hbm.xml""/> </session-factory> </hibernate-configuration> Stack Trace: org.hibernate.HibernateException: could not init listeners at org.hibernate.event.EventListeners.initializeListeners(EventListeners.java:205) at org.hibernate.cfg.Configuration.getInitializedEventListeners(Configuration.java:1980) at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:1842) at com.BiddingSystem.server.ServiceImpl.<init>(ServiceImpl.java:87) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at java.lang.Class.newInstance0(Class.java:355) at java.lang.Class.newInstance(Class.java:308) at org.mortbay.jetty.servlet.Holder.newInstance(Holder.java:153) at org.mortbay.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:339) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:463) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:362) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:729) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:324) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505) at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:843) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:647) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380) at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:395) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:488) Caused by: org.hibernate.search.SearchException: Unable to initialize directory provider: com.BiddingSystem.Models.ForumMessage at org.hibernate.search.store.DirectoryProviderFactory.createDirectoryProvider(DirectoryProviderFactory.java:169) at org.hibernate.search.store.DirectoryProviderFactory.createDirectoryProviders(DirectoryProviderFactory.java:100) at org.hibernate.search.spi.SearchFactoryBuilder.initDocumentBuilders(SearchFactoryBuilder.java:375) at org.hibernate.search.spi.SearchFactoryBuilder.buildNewSearchFactory(SearchFactoryBuilder.java:262) at org.hibernate.search.spi.SearchFactoryBuilder.buildSearchFactory(SearchFactoryBuilder.java:144) at org.hibernate.search.event.FullTextIndexEventListener.initialize(FullTextIndexEventListener.java:137) at org.hibernate.event.EventListeners$1.processListener(EventListeners.java:198) at org.hibernate.event.EventListeners.processListeners(EventListeners.java:181) at org.hibernate.event.EventListeners.initializeListeners(EventListeners.java:194) ... 28 more Caused by: org.hibernate.search.SearchException: Unable to create index directory: C:\users\application\indexes for index com.BiddingSystem.Models.ForumMessage at org.hibernate.search.store.DirectoryProviderHelper.makeSanityCheckedDirectory(DirectoryProviderHelper.java:252) at org.hibernate.search.store.DirectoryProviderHelper.getVerifiedIndexDir(DirectoryProviderHelper.java:234) at org.hibernate.search.store.FSDirectoryProvider.initialize(FSDirectoryProvider.java:62) at org.hibernate.search.store.DirectoryProviderFactory.createDirectoryProvider(DirectoryProviderFactory.java:166) ... 36 more Looking at the error message you are working on Windows: Unable to create index directory: C:\users\application\indexes for index com.BiddingSystem.Models.ForumMessage However in the configuration you specify the index base unix style: /users/application/indexes The index base directory needs to exist and it needs to be a valid directory path for your OS.  Your solution may be here"
270,A,"Lucene vs MySQL MyISAM Now we are using lucene to do search. However compared with lucene could we use Mysql MyISAM instead. What are benefits to use them? http://stackoverflow.com/questions/4638671/search-engine-lucene-vs-database-search thanks full text search and index are support for both lucene and myISM I think. see http://dev.mysql.com/doc/refman/5.5/en/fulltext-natural-language.html Probably a bad idea to use MyIsam as you can't do foreign key relations within your index. It will also be slower than Lucene for searching text fields. I won't use neither lucene nor myISM for data identity. Innodb has well support for such thing. In term of full text search I'd like to know which one is better support and which one have better performance.  With the Lucene you will be able to create (only) a Full-Text index who will perform better with full-text queries compared. Keep in mind that it is only a Full-text search and not a fully featured DBMS. Another search engine is Xapian or wrapped into an application. I don't have personal experience with it but from what I have heard it is perfect to search trough huge amounts of information.  MyISAM FullText search is not the most configurable. Some changes require recompiling MySQL. http://dev.mysql.com/doc/refman/5.5/en/fulltext-fine-tuning.html It is also somewhat limited especially if you want to FullText multiple fields (you can't say ""field A has foo or field B has bar"" within the fulltext query--you need a SQL OR). Another limitation is the fixed 50% threshold for natural language searches. I would guess that Lucene's FullText is much faster since Lucene was built to perform these searches. If your data is already in MySQL you may want to try MyISAM. If you just want to search go with Lucene. Lucene also will scale much easier than MySQL FullText."
271,A,Custom Lucene Sharding with Hibernate Search Has anyone experience with custom Lucene sharding / paritioning using Hibernate Search? The documentation of Hibernate Search says the following about Lucene Sharding : In some cases it is necessary to split (shard) the indexing data of a given entity type into several Lucene indexes. This solution is not recommended unless there is a pressing need because by default searches will be slower as all shards have to be opened for a single search. In other words don't do it until you have problems :) Has anyone implemented sharding in such a way for Hibernate Search that also queries can be target to one of the shards? In our case we have Lucene queries that should target only one shard per query. Look at http://opensource.atlassian.com/projects/hibernate/browse/HSEARCH-251 Thanks a lot for the answer
272,A,"Lucene foreign chars problem I'm having some serious issues using Zend_Lucene and foreign characters like åäö. These issues appear both when the index is created and when it's queried. I've tried both iso-8859-1 and utf-8. ISO-8859-1 The query that doesn't work looks like ""+_area:skåne"". With Zend_Lucene I'm getting no matches but if I run this query in Luke I get many matching docuements. The index contains 20 fields. The ""_area"" field is added with the following syntax: $doc->addField(Zend_Search_Lucene_Field::keyword('_area' strtolower($item['area']) 'iso-8859-1')); I am using the Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum_CaseInsensitive analyzer. While running indexing the error message below appeared sometimes (the documents indexed were randomly selected from DB with iso-8859-1 encoding) Notice: iconv(): Detected an illegal character in input string in TextNum.php. This was ""solved"" by checking if $this->_input is empty as it seemed that this caused the notices. Note: The weird query results were a pre-existing condition. When I search keyword fields using foreign characters I receive the error above but when I search text fields it behaves differently. Then it generates about a hundred of the error below. Notice: Undefined offset: 1996 in \Zend\Search\Lucene\Search\Query\MultiTerm.php on line 472 But it produces what looks like a correct result set! On a side note this second query doesn't generate any results in Luke. UTF-8 I've also tried UTF-8 because to my knowledge Zend_Lucene uses it internally. Since the data set is ISO-8859-1 I convert it using utf8_encode. But the indexing produces the following errors. Notice: Undefined offset: 266979 in \Zend\Search\Lucene\Index\SegmentInfo.php on line 632 Notice: Trying to get property of non-object in \Zend\Search\Lucene\Index\SegmentMerger.php on line 196 Notice: Trying to get property of non-object in \Zend\Search\Lucene\Index\SegmentMerger.php on line 200 Notice: Undefined index: in \Zend\Search\Lucene\Index\SegmentWriter.php on line 231 Notice: Trying to get property of non-object in \Zend\Search\Lucene\Index\SegmentWriter.php on line 231 Notice: Undefined offset: 250595 in \Zend\Search\Lucene\Index\SegmentInfo.php on line 2020 Notice: Trying to get property of non-object in \Zend\Search\Lucene\Index\SegmentInfo.php on line 2020 Notice: Undefined index: in \Zend\Search\Lucene\Index\SegmentWriter.php on line 465 ... So. Can someone please shed some light? :) I believe (after days of googling) that I'm not the only one experiencing this. please post your solution. We need it. I suggest you try using a UTF-8 compatible text analyzer. It looks like the analyzer you are using destroys the non-ASCII characters. You should make sure that the text is input properly and that it reaches Lucene in the proper format. I actually was able to solve this right after I posted but your answer would probably have lead me to the solution. I went the UTF-8 way and using all mb_* functions and such I managed to run the indexing without errors. It seems to be working now and I can query the index and it returns valid results. Thanks for the quick response! @Znarkus please post your solution. We need it. @runrunforest This was a long time ago so I'm not entirely sure what did the trick. But I think I made sure everything was encoded in UTF-8. But we've dropped Zend_Lucene in favour of Solr since then because Zend_Lucene doesn't scale at all; it's very slow. Good luck!"
273,A,Should I always use a new directory when rebuilding a Lucene index? I've been experimenting with Lucene on Google's App Engine and have run across index corruption problems when adding records to indexes. The sure fire way to avoid this is to simply rebuild a new index from scratch. Supposedly Lucene is very good at avoiding index corruption with its file formats but my experience shows differently. How are other people doing this? Has anyone else tried to switching to a new directory each time they have a rebuilt index? Lucene indexes are very hard to corrupt. Which library are you using? May I suggest you SOLR?
274,A,"Lucene case sensitive & insensitive search I have a Lucene index which is currently case sensitive. I want to add the option of having a case insensitive search as a fall-back. This means that results that match the case will get more weight and will appear first. For example if the number of results is limited to 10 and there are 10 matches which match my case this is enough. If I only found 7 results I can add 3 more results from the case-insensitive search. My case is actually more complex since I have items with different weights. Ideally having a match with ""wrong"" case will add some weight. Needless to say I do not want duplicate results. One possible approach is to have 2 indexes. One with case and one without and search both. Naturally there's some redundancy here since I need to index twice. Is there a better solution? Ideas? did you tried copyField? see http://wiki.apache.org/solr/SchemaXml Did you already tried copyField? see http://wiki.apache.org/solr/SchemaXml#Copy_Fields If not define a new field B with a different configuration and copy field A into B via copyField Well copyField is a Solr feature and I'm using bare-bones Lucene. Yet I can just add an extra field with the same indexed text in lower case. This is far better than creating a completely separate index so +1. ups ok. I had exactly the same problem but was working with solr. I added this answer a bit too fast though. I'm already up and running with the extra field so your answer gave me a nudge in the right direction. That's all I needed. Thanks again. I will keep it open to see if I can get more efficient solutions. Will mark as the right answer. Again not exactly the solution but a nudge in the right direction.  The Lucene search is case sensitive it's just that all input is usually lower-cased upon passing through Queryparser  so it feels like it's case insensitive. In other words don't lower-case your input before indexing and don't lower-case your queries (i.e. pick an Analyzer that doesn't lower-case) keyword-analyzer for example. [setLowercaseExpandedTerms][1](boolean lowercaseExpandedTerms) you can index the terms using case sensitive analyzer and when u want case-insensitive query use a class which doesnot convert your terms to lowercase look at Wildcard Prefix and Fuzzy queries Naturally using a case-sensitive analyzer with a lower-care query will not yield the correct results. can you do the same with sqlite or mysql ? @Naveen: sqlite and mysql have fullblown database engine what is your question?"
275,A,"using date range in Lucene.net I understand how Lucene.net can work for text indexing. Will I be able to efficiently search for documents based on a given date range? Or will Lucene.net just use text matching to match the dates? Lucene.Net will just use text matching so you'd need to format the dates correctly before adding to the index:  public static string Serialize(DateTime dateTime) { return dateTime.ToString(""yyyyMMddHHmmss"" CultureInfo.InvariantCulture); } public static DateTime Deserialize(string str) { return DateTime.ParseExact(str ""yyyyMMddHHmmss"" CultureInfo.InvariantCulture); } You can then for example perform a range based query to filter by date (e.g. 2006* to 2007* to include all dates in 2006 and 2007). I know little to nothing about lucene as im just trying to teach myself but this answer threw me off for a while because the milliseconds are missing from the datetime you want (well i did) ""yyyyMMddHHmmssfff"" Careful with running into problems going over maxClauseCount (default of 1024) as a RangeQuery expands under the covers to a BooleanQuery with all matching terms OR'd together. With dates (because they're indexed as text) the number of terms gets large quickly. See http://www.gossamer-threads.com/lists/lucene/java-user/34583  I went in to trouble when i converted date in to yyyymmddHHmmssff. When i tried sorting the data it gave me an exception that too big to convert..something. Hence i search and found then you need to have two columns. one in yyyymmdd and the other HHmmss and then use Sort[] and give these two columns and then use. This will solve the issue."
276,A,"Solr - LockObtainFailedException on multiple simultaneous writes My application does very frequent solr writes from multiple clients via REST. I'm using the autocommit feature by using the ""commitWithin"" attribute. LockObtainFailedException start appearing after couple of days of use. I'm having a hard time figuring out what the problem might be. Any help is appreciated. I'm using Solr 3.1 with tomcat 6 here is the error dump from solr HTTP Status 500 - Lock obtain timed out: NativeFSLock@/var/lib/solr/data/index/write.lock org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/var/lib/solr/data/index/write.lock at org.apache.lucene.store.Lock.obtain(Lock.java:84) at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:1097) at org.apache.solr.update.SolrIndexWriter.&lt;init&gt;(SolrIndexWriter.java:83) at org.apache.solr.update.UpdateHandler.createMainIndexWriter(UpdateHandler.java:102) at org.apache.solr.update.DirectUpdateHandler2.openWriter(DirectUpdateHandler2.java:174) at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:222) at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61) at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:147) at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:77) at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:55) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1360) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:356) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:252) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298) at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859) at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588) at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489) at java.lang.Thread.run(Thread.java:662) </h1><HR size=""1"" noshade=""noshade""><p><b>type</b> Status report</p><p><b>message</b> <u>Lock obtain timed out: NativeFSLock@/var/lib/solr/data/index/write.lock This usually happens because Solr terminated in a non-standard way so its lock didn't get cleaned up. Was there a JVM crash / Solr crash before this happened? Another cause is if you are trying to point multiple solr servers at the same location. See e.g. this question. thanks. I'll try to look into the JVM logs. I guess it requires some tuning. How does one clean the locks up? Solr uses file locking so rename or delete the lock file (our lock files contains `write.lock`). That file was in the index files folder -- for us `SolrHome\\data\index`. Rename or delete that file: our file `lucene-5d886598917ad7fbb03256c713a8aacb-write.lock` was renamed to `TEMP__lucene-5d886598917ad7fbb03256c713a8aacb-write_lock__TEMP`. After that re-indexing ran without locking problems. Obviously that's no replacement for (1) prevention (scheduling re-indexing) (2) maybe error handling (writer.close & writer.open?) or (3) timeout settings appropriate for you.  I increased the writeLockTimeout in solrconfig.xml to 20 seconds and it appears to be working fine now. Earlier it was set to 1 sec I was getting the same error with a single client writing a single record and increasing `writeLockTimeout` fixed it. I don't think it has anything to do with concurrency or ""multiple simultaneous writes"" which was the original title of this post. I think the index is just growing and the amount of time it takes to write has increased.  It could also be a lack of memory. A lack of memory can cause locks I'm seeing the same thing on my server. From the apache docs: If your Solr instance doesn't have enough memory allocated to it the Java virtual machine will sometimes throw a Java OutOfMemoryError. There is no danger of data corruption when this occurs and Solr will attempt to recover gracefully. Any adds/deletes/commits in progress when the error was thrown are not likely to succeed however. Other adverse effects may arise. For instance if the SimpleFSLock locking mechanism is in use (as is the case in Solr 1.2) an ill-timed OutOfMemoryError can potentially cause Solr to lose its lock on the index. If this happens further attempts to modify the index will result in SEVERE: Exception during commit/optimize:java.io.IOException: Lock obtain timed out: SimpleFSLock@/tmp/lucene-5d12dd782520964674beb001c4877b36-write.lock See http://wiki.apache.org/solr/SolrPerformanceFactors"
277,A,"DIH Scheduling in Solr I have just started playing around with Solr and I have it deployed and running on Tomcat. I have the schema and data import handler set up and it indexes the files just fine. Now I want to schedule this dataImportHandler to run every hour or so. There is a wiki page detailing the files here. But there are not instructions on where to create the files and how to deploy them A similar question has been asked on Stack Overflow before here. The answer was to ""Create classes ApplicationListener HTTPPostScheduler and SolrDataImportProperties"". I don't know where I should be creating the classes. But I took a guess and I downloaded the latest nightly build and created the classes in the org.apache.solr.handler.dataimport.scheduler package (copy pasting the classes from the wiki page). I compiled and ran the ant dist command to create the deployable jar files. I configured the dataimport.properties as per the instructions in the wiki and then added the listener in the web.xml file as instructed in the answer above. But when I started Tomcat solr was not deployed. I see this error message in the log file: INFO: Starting Servlet Engine: Apache Tomcat/7.0.14 Jun 21 2011 5:20:47 PM org.apache.catalina.startup.HostConfig deployDescriptor INFO: Deploying configuration descriptor solr.xml from /home/sabman/programs/apache-tomcat-7.0.14/conf/Catalina/localhost Jun 21 2011 5:20:47 PM org.apache.catalina.startup.HostConfig deployDescriptor WARNING: A docBase /home/sabman/programs/apache-tomcat-7.0.14/webapps/solr.war inside the host appBase has been specified and will be ignored Jun 21 2011 5:20:47 PM org.apache.catalina.startup.SetContextPropertiesRule begin WARNING: [SetContextPropertiesRule]{Context} Setting property 'debug' to '0' did not find a matching property. Jun 21 2011 5:20:48 PM org.apache.catalina.core.StandardContext startInternal SEVERE: Error listenerStart I had to remove listener code from the web.xml for it work as it was before. Any idea about what I could be doing wrong? The simplest solution. Use a cron entry to make a request to the `dataimporthandler` every hour. I got this reply from the Solr mailing list: The Wiki page describes a design for a scheduler which has not been committed to Solr yet (I checked). I did see a patch the other day (see https://issues.apache.org/jira/browse/SOLR-2305) but it didn't look well tested. I think that you're basically stuck with something like cron at this time. If your application is written in java take a look at the Quartz scheduler - http://www.quartz-scheduler.org/  If you copied the source for ApplicationListener etc and ran a build you may want to check that the files are actually being compiled into your distribution. You can do that by opening up the war file and looking to see if there is a jar containing .class files for those classes you mentioned or looking in the classes directory in the .war to see if they are there. If they're not then they won't get loaded in the web app (hence the failed deployment). You may have to compile them on your own (create your own jar file that has compiled classes) and include the jar file in the war file manually (this would be a good test at least). You could also just use the second answer from that Stackoverflow post which was to call the command line from cron or the task scheduler. Yes I did make sure that the Class files were in the jar file. I don't want to use cron jobs just yet because I want to make Solr part of an application package to give it clients. So rather than asking them to run cron jobs I would rather have the scheduler part of the solr package."
278,A,What is the best full text search open source project (.NET prefered)? Hi I've developed an index and search application with Lucene library. but this library has some limitation in custom ranking in my context aside from its performance i need scalability and access to all kinds of word frequencies and etc. is there any powerful open source full text library available ? any suggestion ? I've found that performance with Lucene.net is incredible so it's a surprise to hear someone say they've got problems with performance! (BTW Lucene has a pretty good API for custom scoring etc as well) I don't have any problem with performance of lucene but custom ranking is so difficult. http://www.sphinxsearch.com http://www.sphinxconnector.net/ Key Sphinx features are: high indexing and searching performance; advanced indexing and querying tools (flexible and feature-rich text tokenizer querying language several different ranking modes etc); advanced result set post-processing (SELECT with expressions WHERE ORDER BY GROUP BY etc over text search results); proven scalability up to billions of documents terabytes of data and thousands of queries per second; easy integration with SQL and XML data sources and SphinxAPI SphinxQL or SphinxSE search interfaces; easy scaling with distributed searches. To expand a bit Sphinx: has high indexing speed (upto 10-15 MB/sec per core on an internal benchmark); has high search speed (upto 150-250 queries/sec per core against 1000000 documents 1.2 GB of data on an internal benchmark); has high scalability (biggest known cluster indexes over 3000000000 documents and busiest one peaks over 50000000 queries/day); provides good relevance ranking through combination of phrase proximity ranking and statistical (BM25) ranking; provides distributed searching capabilities; provides document excerpts (snippets) generation; provides searching from within application with SphinxAPI or SphinxQL interfaces and from within MySQL with pluggable SphinxSE storage engine; supports boolean phrase word proximity and other types of queries; supports multiple full-text fields per document (upto 32 by default); supports multiple additional attributes per document (ie. groups timestamps etc); supports stopwords; supports morphological word forms dictionaries; supports tokenizing exceptions; supports both single-byte encodings and UTF-8; supports stemming (stemmers for English Russian and Czech are built-in; and stemmers for French Spanish Portuguese Italian Romanian German Dutch Swedish Norwegian Danish Finnish Hungarian are available by building third party libstemmer library); supports MySQL natively (all types of tables including MyISAM InnoDB NDB Archive etc are supported); supports PostgreSQL natively; supports ODBC compliant databases (MS SQL Oracle etc) natively; ...has 50+ other features not listed here refer to API and configuration manual!
279,A,"How to do a Multi field - Phrase search in Lucene? Title asks it all... I want to do a multi field - phrase search in Lucene.. How to do it ? for example : I have fields as String s[] = {""title""""author""""content""}; I want to search harry potter across all fields.. How do I do it ? Can someone please provide an example snippet ? Use MultiFieldQueryParser its a QueryParser which constructs queries to search multiple fields.. Other way is to use Create a BooleanQuery consisting of TermQurey (in your case phrase query). Third way is to include the content of other fields into your default content field. Add Generally speaking querying on multiple fields isn’t the best practice for user-entered queries. More commonly all words you want searched are indexed into a contents or keywords field by combining various fields. Update Usage: Query query = MultiFieldQueryParser.parse(Version.LUCENE_30 new String[] {""harry potter""""harry potter""""harry potter""} new String[] {""title""""author""""content""}new SimpleAnalyzer()); IndexSearcher searcher = new IndexSearcher(...); Hits hits = searcher.search(query); The MultiFieldQueryParser will resolve the query in this way: (See javadoc) Parses a query which searches on the fields specified. If x fields are specified this effectively constructs: (field1:query1) (field2:query2) (field3:query3)...(fieldx:queryx) Hope this helps. 3rd option isn't even an option to me the example I've in the question is dummy... due to the constraints of my app I require the fields not to be combined... 2nd option is what it is... a SECOND option!!! I am interested in MultiFieldQueryParser I am doing that but getting errors in doing it.. I guess that'll be the way to go... about ADD I'll keep that in mind as a good advice.. thank you :) @Favonius : do you mind instructing how MultiFieldQueryParser works with phrases ? because parsing with it returns a `Query` object and not the phraseQuery object... @Shrinath: Yes it will return the `Query` object. Under the hood it extends the default `QueryParser` and it parses a query expression using QueryParser’s static parse method for each field as the default field and combines them into a BooleanQuery. The default operator **OR** is used in the simplest parse method when adding the clauses to the `BooleanQuery`. Which again is the **Number 2** option I mentioned. @Shrinath: check my updated answer for usage. @Favonius : +1 and accepting for the code snippet. Thanks :)  intensified googling revealed this : http://lucene.472066.n3.nabble.com/Phrase-query-on-multiple-fields-td2292312.html. Since it is latest and best I'll go with his approach I guess.. Nevertheless it might help someone who is looking for something like I am... This is the second option in Favonius answer. @Simon : Yeah :)"
280,A,"Solr 1.3 ignore word ""jackie"" I am using Solr 1.3. Recently QA report a bug for the search functionality that there is no result for the word ""Jackie"". But when i looked into document using luke there is lots of document with the word ""jackie"" e.g.  Jackie Holding Kumar Rameshwaram Morris Jackson Jackie Holding Brendon Wessel Smith McShlam Jackie Redmond John Smith Martha Doum Trevor Harris Jackie Collins Martin Green If i search for any term other than jackie solr return the documents.But no documents for the word ""Jackie"". Even i try it with all analyzer solr return no document. I am not able to figure out the problem. Please help me. please post the entire fieldType declaration. @Mauricio: url of doc field type http://pastebin.com/QjZnDTQb that's not a fieldType. see http://wiki.apache.org/solr/SchemaXml @Mauricio:Sorry this is the url for schema.xml http://pastebin.com/cJr8iBeV A couple of ideas: try setting debugQuery=on when querying Jackie in the Solr admin console. See how it gets analyzed. Check that 'Jackie' is not in the stopwords list (stopwords.txt) Try adding 'Jackie' to the protected word list (protwords.txt) so it doesn't get stemmed. Thank you very much. It works by adding the word 'Jackie' in protwords.txt  Verify that in your schema the filter factories for your query and index analyzers are corresponding (they should have the same stemming configuration for instance). Of course if you have changed your schema since indexing then the query analyzer filters should match the ones that were used at index time or you will have to re-index. The problem you describe typically occurs when the query analyzer uses a different stemming configuration than the index analyzer (for instance using different languages)."
281,A,Lucene .NET result subsets I am using Lucene .NET Let's say I want to only return 50 results starting at result 100 how might I go about that? I've searched the docs but am not finding anything. Is there something I'm missing? Your code should look something like this: TopDocs topDocs = indexSearcher.Search(query null 150); for(int i=100 i<min(topDocs.totalHits150); i++) { Document doc = indexSearcher.doc(topDocs.scoreDocs[i]); // Do something with the doc } Don't use the Hits class. It is inefficient and deprecated.  I assume you are doing this for the purpose of paging. The way this is normally done in a Lucene implementation (including Solr) is by simply executing the query normally but only actually loading the stored data for the results you are interested in. In a typical paging scenario this may mean executing the same query multiple times which may seem like a waste of resources but with help from the system cache and possibly Lucene's caching it's not so bad. The benefit is statelessness which allows you to scale.
282,A,"What to do when .Net garbage collection is slower than the rate at which objects are being created/deleted? I have a live Lucene index that is updated throughout the day. When several successive batches of updates for the index come through I want those updates to be available for searching as quickly as possible. Therefore I have to recreate the IndexSearcher. The problem is that the IndexSearcher can take around 100mb of memory and when a lot of updates are coming through it can be recreated relatively often and I've noticed the .Net garbage collector seems slow to clean up the reference to the old IndexSearcher object. This results in the memory usage of the process climbing out of control as the collector seems to free up memory from old IndexSearchers more slowly than they are being recreated. I've found this problem is mitigated by crossing the line into taboo territory and calling GC.Collect() which frees up the memory immediately. The performance impact doesn't seem to be noticeable but as I'm doing something that many advice against I'd be curious if anyone else has experience of objects being created and released faster than the garbage collector is cleaning them up. I'd be particularly interested if anyone has had this problem with the Lucene IndexSearcher. I should note that the IndexSearcher is being recreated at peak times around once every 10-20 seconds. I had similar problem in the past. Storing datatables in sessions. The new objects grow more fast than the GC collected. You need the objects and his properties in the indexSearcher collection not be referenced in a long live objects. Ensure no other objects are referencing to the old indexSearcher. If any of the objects in indexSearcher are referenced in other the old indexSearcher is considered as long live duration object and will be collected in second or third GC generation not in the gen0 that is you need. are you using the latest lucene version? If yes can you post a test case that reproduces the issue? I am unable to reproduce the behavior you are seeing with the latest lucene version. I'm using a 100gb index. I add 100docs get a searcher make 50 random search. I repeated that process in a loop 100 000 times and my app is still at 275MB ram. Yeah the old searcher is not referenced anywhere after I recreate it so it's obviously being treated as long-lived. @JfBeaulac - I'm using Lucene.net which equates to Lucene 2.9.2 at this time. Also my index has 2.8million docs right now and will get up to 200million+ when it goes live. Sounds indexSearcher is some kind of cache. Please do not forget is a bad way use cache when you update frequently... No it's not being cached. I'm using a single searcher and it's being stored in its own private field. When I recreate it I simply set the field to a newly-created instance. The old reference then drops and is available for collection. Have you tried configuring garbage collection for the server? I believe it differs in that GC is on another thread: Should we use ""workstation"" garbage collection or ""server"" garbage collection? As for ""faster than cleaning them up"" if memory is available then the system will grant it to your process. The garbage collector will collect at various junctions as memory grows but it will not stop allocations in order to maintain a certain level of memory pressure - the OS manages pressure. Unfortunately I have no direct experience with Lucene so my answer is just about garbage collection in general. +1 for the link - I didn't know there were two garbage collection modes  I consider it acceptable to call GC.Collect if you just released a ton of memory and the memory can and should be freed now to reduce memory pressure. The GC does not know this memory is now available until it runs again and you don't know when that will be. In your case you said ""it can be recreated relatively often"". If so calling GC.Collect when you recreate it sounds reasonable. +1 I second the opinion that it is OK where proven to be useful. But most of the time it's not useful over normal GC behaviour hence it gets looked at suspiciously all the time. Thanks for the support - I'm always paranoid that I'm writing bad code when I'm working with areas that I don't know inside out!"
283,A,Lucene Search Error Stack I am seeing the following error when trying to search using Lucene. (version 1.4.3). Any ideas as to why I could be seeing this and how to fix it? Caused by: java.io.IOException: read past EOF at org.apache.lucene.store.InputStream.refill(InputStream.java:154) at org.apache.lucene.store.InputStream.readByte(InputStream.java:43) at org.apache.lucene.store.InputStream.readVInt(InputStream.java:83) at org.apache.lucene.index.FieldInfos.read(FieldInfos.java:195) at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:55) at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:109) at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:89) at org.apache.lucene.index.IndexReader$1.doBody(IndexReader.java:118) at org.apache.lucene.store.Lock$With.run(Lock.java:109) at org.apache.lucene.index.IndexReader.open(IndexReader.java:111) at org.apache.lucene.index.IndexReader.open(IndexReader.java:106) at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:43) In this same environment I also see the following error: Caused by: java.io.IOException: Lock obtain timed out: Lock@/tmp/lucene-3ec31395c8e06a56e2939f1fdda16c67-write.lock at org.apache.lucene.store.Lock.obtain(Lock.java:58) at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:223) at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:213) The same code works in a test environment however not in production. Cannot identify any obvious differences between the two environments. File permissions are wrong (it needs write permission) or your are not able to access a locked file that the current process needs.
284,A,"Can Lucene return several search results from a single indexed file? I am using Lucene to index and search a small number of large documents. Using the demo from the Lucene site I have indexed the documents and am able to search them. However the search result is not particularly useful as it points to the file of the document. With very large documents this isn't particularly useful. I am wondering if Lucene can index these very large documents and create an abstraction over them which provides much more fine-grained results. An example might better explain what I mean. Consider a very large book such as the Bible. One file contains the entire text of the Bible so with the demo the result of searching for say 'Damascus' would point to the file. What I would like to do is to retain the large document but searches would return results pointing to a Book Chapter or even as precise as a Verse. So a search for 'Damascus' could return (among others) Book 23 Chapter 7 Verse 8. Is this possible (and best-practice in Lucene usage) or should I instead attempt to section the large document into many small files to index? If it makes any difference I am using Java Lucene 2.9.0 and am indexing HTML files approximately 1MB - 4MB in size. Which in terms of file size is not large but it is large relative to a person reading it. I don't think I've explained this as well as I could. Here goes for another example. Say I take my large HTML file and (for arguments sake) the search term 'Damascus' appears 3 times. Once on line 100 within a <div> tag on line 2000 within a <p> tag and on line 5000 within a <h1> tag. Is it possible to index with Lucene such that there will be 3 results and they can point to the specific element the term was within? I don't think I want to provide a different document result for the term. So if the term 'Damascus' appeared twice within a specific <div> there would only be one match. It appears from a comment from Kragen that what I would want to do is parse the HTML when Lucene is going through the indexing phase. Then I can decide the chunk I want to consider as one document from what is read in by the parser. So if I see a div with a certain class I can begin a new Lucene document and it will be returned as a separate hit when a word within the div content is searched on. Does this sound like what I want to do and is it possible? Yes - Lucene records the offset of matching terms in a file so that can be used to figure out where in the indexed content you need to look for matches. There is a Lucene.Highlight add-on that does this exact task for you - try this article there are also a couple of questions on StackOverflow concerning hit highlighting (many of these are tailored to use with web apps and so also do things like surrounding matching words with <b> tags) UPDATE: Depending on how you search your index you might also find that its a good idea to split your large documents into smaller sections (for example chapters) as well - however this is more a question on how you want to organise prioritise and present your results to the end user. For example supposing a user does a search for ""foo"" and there are 2 books containing that term. The first book (book A) might contain 2 chapters each of which have many references to ""foo"" however the term is barely mentioned in the rest of the book however the second book (book B) contains many references to ""foo"" however they are scattered around the whole book. If you index by book then you will probably find that book B is the first hit however indexing by chapter you are likely to find that the 2 chapters from book A are the first 2 hits followed by the chapters from book B. Finally obviously the user will be presented with 1 hit per matching document you have in your index - if you want to present your users with a list of matching books then obviously index by book however you might find it more appropriate to present the user with a list of matching chapters in which case obviously index by chapter. You can index by chapter by giving Lucene only a subset of that file when you index - this will give you 1 hit per matching chapter. If you want to present the user with a hit per discrete match then you will need to go through and find all the occurrences for each matching document - there is no way to split a book up into enough Lucene documents so that each hit is guaranteed to correspond to exactly 1 occurrence of that word / phrase. Using your example can I keep book B as a single file in my case an HTML file and create several Lucene Documents from within that one file so that all the results from the single file can be reported to the user as discrete hits? Is it possible to index by chapter when the chapters are in the same *file*? Thanks for your answer :)  One way to do this is to create several documents out of a single book. The documents could represent books chapters or verses. As the text need not be unique this is what I would do. This way the first verse in the first chapter in the book of Genesis will be indexed four times: in the whole bible in the book of Genesis in the first chapter and as the verse. A subtlety here is the exact goal of retrieval: Do you want just to display the search keywords in context to a user? In this case consider using a Lucene highlighter. If you need the retrieval to be further used (i.e. take the retrieved pointer to a chapter or verse and do some processing on this place in the text) I would go with the finer-grained documents as I described before. Goal is to provide display the HTML in a Swing application search results will allow the user to navigate to that part of the HTML. Search may also provide a preview. Just to be clear when you say 'create several documents out of a single book' do you mean Lucene documents or new files? I mean Lucene documents."
285,A,"How do i implement tag searching? with lucene? I havent used lucene. Last time i ask (many months ago maybe a year) people suggested lucene. If i shouldnt use lucene what should i use? As am example say there are items tagged like this apples carrots apples carrots apple banana if a user search apples i dont care if there is any preference from 12 and 4. However i seen many forums do this which i HATED is when a user search apple carrots 2 and 3 have high results while 1 is hard to find even though it matches my search more closely. Also i would like the ability to do search carrots -apples which will only get me 3. I am not sure what should happen if i search carrots banana but anyways as long as more items tagged with 2 and 3 results are lower ranking then 1 when i search apples carrots i'll be happy. Can lucene do this? and where do i start? I tried looking it up and when i do i see a lot of classes and i'll see tutorials talking about documents webpages but none were clear about what to do when i like to tag something. If not lucene what should i use for tagging? Ranking of 12 and 4 on keyword apple (suppose you will use stemmer to handle plurals) differs because ranking algorithm calculates relative weight of the term in the field. In case 2 you have 1 hit and field length = 1. In cases 1 and 4 you have 1 hit in 2-term field. Roughly speaking the weight of these hits will differ by factor of 2. Lucene is very much the tool to do this. If you want apple and apples (plural) to match you just need to be careful about using the correct language stemmer when indexing and querying the index. Lucene for .net seems to be mature. No need to use Java or SOLR The Standard query language for Lucene allows equally ranked search terms and negation So if your Lucene index had a field ""tag"" your query would be tag:apple* OR tag: carrot* Which would give equal ranking to each word and more rank weighting to document with both tags To negate a tag use this tag:carrot* NOT tag:apple* Simple example to show indexing and querying with Lucene here Thanks :). I hope more ppl keep this coming (i really need help!) this tutorial looks good and the query link looks useful. I suspect i'll be messing with this before the end of the day. The important thing here (to me) would be if there is a many-to-many relationship with tags. i.e. a single item can have multiple 'tag' fields. That's really where the power of tagging comes from IMHO. I wouldn't want to store tags as string of single words i.e. '''one two three''' and have to search for '''***two***'''.  Edit: You can use Lucene. Here's an explanation how to do this in Lucene.net. Some Lucene basics are: Document - is the storage unit in Lucene. It is somewhat analogous to a database record. Field - the search unit in Lucene. Analogous to a database column. Lucene searches for text by taking a query and matching it against fields. A field should be indexed in order to enable search. Token - the search atom in Lucene. Usually a word sometimes a phrase letter or digit. Analyzer - the part of Lucene that transforms a field into tokens. Please read this blog post about creating and using a Lucene.net index. I assume you are tagging blog posts. If I am totally wrong please say so. In order to search for tags you need to represent them as Lucene entities namely as tokens inside a ""tags"" field. One way of doing so is assigning a Lucene document per blog post. The document will have at least the following fields: id: unique id of the blog post. content: the text of the blog post. tags: list of tags. Indexing: Whenever you add a tag to a post remove a tag or edit it you will need to index the post. The Analyzer will transform the fields into their token representation. Document doc = new Document(); doc.Add(new Field(""id"" i.ToString() Field.Store.YES Field.Index.NO)); doc.Add(new Field(""content"" text Field.Store.YES Field.Index.TOKENIZED)); doc.Add(new Field(""tags"" tags Field.Store.YES Field.Index.TOKENIZED)); writer.AddDocument(doc); The remaining part is retrieval. For this you need to create a QueryParser and pass it a query string like this: QueryParser qp = new QueryParser(); Query q = qp.Parse(s); Hits = Searcher.Search(q); The syntax you need for s will be: tags: apples tags: carrots To search for apples or carrots tags: carrots NOT tags: apples See the Lucene Query Parser Syntax for details on constructing s. Great answer. Too bad i overslept and didn't go to SO until the bounty was over. Adding search doesnt seem as bad as i originally thought. nice answer - the right way to implement 'tags' is an important question - because there are so many (wrong? painfully slow?) ways one could do it and the idea of tags/folksonomies is here to stay (in favor of hierarchical taxonomies that is)"
286,A,"Increasing the weight of particular terms (e.g. headings) when indexing documents in Lucene I have documents which I am indexing with Lucene. These documents basically have a title (text) and body (text). Currently I am creating an index out of Lucene Documents with (amongst other fields) a single searchable field which is basically title+"" ""+body. In this way if you search for anything which occurs in the title or in the body you will find the document. However now I have learned of the new requirement that matches in the title should cause the document to be ""more relevant"" than matches in the body. Thus if there is a document with the title ""Software design"" and the user searches for ""Software design"" then that document should be placed higher up in the search results than a document called something else which mentions software design a lot in the body. I don't really have any idea how to begin implementing this requirement. I know that Google e.g. treats certain parts of the document as ""more relevant"" (e.g. text within <h1> tags) everyone here assumes Lucene supports something similar. However The Javadoc for the Document class clearly states that fields contain text i.e. not structured text where some parts are ""more important"" than other parts. This blog post states ""With Lucene it is impossible to increase or decrease the weight of individual terms in a document."" I'm not really sure where to look. What would you suggest? Any specific information (e.g. links to Lucene documentation) stating flatly that such a thing is not possible would also be helpful then I needn't spend any further time looking for how to do it. (The software is already written with Lucene so we won't re-write it now so if Lucene doesn't support it then there's nothing anyone (my boss) can do about that.) I think you're talking about boosting fields not terms. you probably should split the combine field become title and body separately then use the run-time boost to give more relevancy for title field the run-time query will be like title:apache^20 body:apache see - http://lucene.apache.org/java/2_4_0/queryparsersyntax.html#Boosting%20a%20Term  Just use two fields title and body and while indexing boost 'title' field: title.setBoost(float) see here @Adrian Smith: Use a [MultiFieldQueryParser](http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/queryParser/MultiFieldQueryParser.html) Hi thanks for the answer. Currently when I search I use `new QueryParser(... ""title-and-body-field"" ..).parse(queryText)`. How would I query documents with two fields? Just parse the query twice for the two fields then make a `BooleanQuery` out of the two resulting queries?"
287,A,"Solr lucene schema.xml multi-valued entries please help Essentially I am trying to link a user type to a price for a product. So a product is a document in the index and each document has multiple prices one for each user type. And i am essentially trying to check the price for that usertype e.g. where user type = 2 and price > 10 But each product can have a varying number of different user types each with an individual price. I need a way to say user type = x and that user types price > 10 I am using solr version 1.4.1 and cant seem to see a way to create a field that works in this way. As i am writing a schema.xml file for this document so I dont have the option to create individual fields for each usertype I need a universal solution for however many user types I that are required. thanks in advance google and bing have given me nothing and im banging my head against a wall Hmmm... One thing you could try is using dynamic fields in solr to solve this problem. As you add fields you just name them like price_usertypex_i (see some examples in the schema.xml as well). As you add user types you create more fields but you don't have to modify your schema because it is dynamic. One caveat is that I don't think you'd be able to use dataimporthandler in this scenario since I don't think it works with dynamic fields (you'd have to roll your own import). Thankyou this solves my problem I didnt know you could create dynamic fields on the fly! this might be what im looking for will try and get back to you fingers crossed!  I'm new to SOLR but what I'd do is create a different document per each combo product/userType. Yes you'd have several documents for the same product but from what I can see so far the server handles it pretty well. ... Is there anything wrong with this approach? that was my backup approach if i could not find a way to include it in a single document. I'd rather avoid it as it would increase my index by however many user types i have. My client has 50000 products and 50 user types so its within the performance range of lucene but not ideal should that increase in the future. Seems like a valid concern. Well if you ever get a better approach please do share with the rest of us people. =) Thank you!  Not sure if you have the right data model but with the constraints you are describing you could index the prod description and append to it a separator and the U1 123 U12 154 where user 1 price is 123 dollars and user12 is 154 dollars ? This would allow you to search by user. The price interval would have to be post-processed. The more standard way would be to add a column ""price"" per user the main problem with this approach is it requires me to retreive the entire data set in order to sort or do price range queries so computationaly expensive Actually do you need full text search ? If you do you can add a product ID column and put all your users and prices in MySQL. If there are too many text search results with Solr that need to be processed then use Lucene + MySQL. Don't know if you can get what you want with facets and different columns I don't see any other solution than --- yourfulltext:word AND price_u1:[15 TO 25.5] --- yeah unfortunately I do. I already have my data in mysql but the search and mysql fulltext search is not good enough for the clients requrements. I was thinking something in facets could help me but its the linking of the two bits of data. If i was doing an exact match with price I could do it in a standard text column but as I need to do range queries im stuck."
288,A,Rewrite query in lucene When lucene want to compute weight of query it first call searcher.rewrite(Query)what does this function do for each types of query? The query rewriter turns higher-level query clauses into lower-level clauses that perform better. The end result is functionally identical. For example the javadoc for Query.rewrite says: Expert: called to re-write queries into primitive queries. For example a PrefixQuery will be rewritten into a BooleanQuery that consists of TermQuerys. If Lucene is to perform an accurate query cost analysis it needs to rewrite the query into its fastest form.
289,A,"string matching algorithms used by lucene i want to know the string matching algorithms used by Apache Lucene. i have been going through the index file format used by lucene given here. it seems that lucene stores all words occurring in the text as is with their frequency of occurrence in each document. but as far as i know that for efficient string matching it would need to preprocess the words occurring in the Documents. example: search for ""iamrohitbanga is a user of stackoverflow"" (use fuzzy matching) in some documents. it is possible that there is a document containing the string ""rohit banga"" to find that the substrings rohit and banga are present in the search string it would use some efficient substring matching. i want to know which algorithm it is. also if it does some preprocessing which function call in the java api triggers it. I don't know how Lucene works but if you are looking to do fast string searches ternary suffix trees are a good way to go. Jon Bentley and Bob Sedgewick wrote a paper about it in 1997 that you can get through searching on Google Scholar. It's titled 'Fast algorithms for sorting and searching strings.' The basic design of Lucene uses exact string matches or defines equivalent strings using an Analyzer. An analyzer breaks text into indexable tokens. During this process it may collate equivalent strings (e.g. upper and lower case stemmed strings remove diacritics etc.) The resulting tokens are stored in the index as a dictionary plus a posting list of the tokens in documents. Therefore you can build and use a Lucene index without ever using a string-matching algorithm such as KMP. However FuzzyQuery and WildCardQuery use something similar first searching for matching terms and then using them for the full match. Please see Robert Muir's Blog Post about AutomatonQuery for a new efficient approach to this problem.  As you pointed out Lucene stores only list of terms that occured in documents. How Lucene extracts these words is up to you. Default lucene analyzer simply breaks the words separated by spaces. You could write your own implementation that for example for source string 'iamrohitbanga' yields 5 tokens: 'iamrohitbanga' 'i' 'am' 'rohit' 'banga'. Please look lucene API docs for TokenFilter class. i am more concerned with the string matching algoritms like KMP suffix tree etc. neither Lucene nor it's closest competitor Xapian to the best of my knowledge support KMP or suffix trees. there is one open-source project attempting to do searching using suffix trees but it's rather not stable so stable (it's called libdoodle)  As Yuval explained in general Lucene is geared at exact matches (by normalizing terms with analyzers at both index and query time). In the Lucene trunk code (not any released version yet) there is in fact suffix tree usage for inexact matches such as Regex Wildcard and Fuzzy. The way this works is that a Lucene term dictionary itself is really a form of a suffix tree. You can see this in the file formats that you mentioned in a few places: Thus if the previous term's text was ""bone"" and the term is ""boy"" the PrefixLength is two and the suffix is ""y"". The term info index gives us ""random access"" by indexing this tree at certain intervals (every 128th term by default). So low-level it is a suffix tree but at the higher level we exploit these properties (mainly the ones specified in IndexReader.terms to treat the term dictionary as a deterministic finite state automaton (DFA): Returns an enumeration of all terms starting at a given term. If the given term does not exist the enumeration is positioned at the first term greater than the supplied term. The enumeration is ordered by Term.compareTo(). Each term is greater than all that precede it in the enumeration. Inexact queries such as Regex Wildcard and Fuzzy are themselves also defined as DFAs and the ""matching"" is simply DFA intersection."
290,A,"TermFreqVector lucene .net I can get docs by category like this: IndexSearcher searcher = new IndexSearcher(dir); Term t = new Term(""category"" ""Feline""); Query query = new TermQuery(t); Hits hits = searcher.Search(query); for (int c = 0; c < hits.Length(); c++) { Document d = hits.Doc(c); Console.WriteLine(c + "" "" + d.GetField(""category"").StringValue()); } Now I would like to obtain the TermFreqVector for the docs in hits. I would usually do this like so: for (int c = 0; c < searcher.MaxDoc(); c++) { TermFreqVector TermFreqVector = IndexReader.GetTermFreqVector(c ""content""); String[] terms = TermFreqVector.GetTerms();//get the terms int[] freqs = TermFreqVector.GetTermFrequencies();// } However I am not sure how to do it in my scenario (i.e. just get them for the docs in hits). The docs also have a db pk. Thanks. Christian The first parameter to IndexReader.GetTermFreqVector (""c"" in your example) is the document number. hits.id(c) will return the ID of the cth result. So you'd do something like: int Id = hits.id(c); TermFreqVector TermFreqVector = IndexReader.GetTermFreqVector(Id ""content""); // etc. (As a side note: the Hits class is deprecated; you probably want to use something like HitCollector or a different search overload instead.) @csetzkorn: It's officially deprecated in 2.9.3 which is what lucene.net uses. You're right that it won't be removed entirely until 3.0. (It's slow in any case regardless of whether it's officially supported or not.) Thanks. I will have a look at this. I am using Lucene .Net - probably takes a while until hits is depreceated ..."
291,A,"Lucene TermPositionVector and retrieving terms at index locations I've been looking like mad for an answer to this however I'm still in the dark: i am using int[] getTermPositions(int index) of a TermPositionVector I have for a field (which has been set to store both offsets and positions) to get the term positions of the terms I'm interested in highlighting as keyword in context. The question: What do these positions correspond to? Obviously not the String[] getTerms() that is returned by the TermFreqVector interface as that contains just raw counts of my terms. What I'm looking for is a way to get the ""tokenized"" array of my field so I can then pull out the surrounding terms around the index values returned by getTermPositions(int index) Help? Thanks a bunch. int[] getTermPositions(int index) returns an array of the term positions of term i. You can get the index i using the int indexOf(String term) method of TermFreqVector. The term positions are the positions (with term as the unit) at which the given term occurs. For example // source text: // term position 0 1 2 3 4 5 6 7 8 // the quick brown fox jumps over the lazy dog // terms: // term index 0 1 2 3 4 5 6 7 // brown dog fox jump lazy over quick the // Suppose we want to find the positions where ""the"" occurs int index = termPositionVector.indexOf(""the""); // 7 int positions = termPositionVector.getTermPositions(index); // {0 6} I got that far but now what if I want to get the words at position 5 and 7 in the source so I can output ""over the lazy"" showing 'the' in context?  Well this will accomplish what I wanted: http://lucene.apache.org/java/3_0_2/lucene-contrib/index.html#highlighter"
292,A,"Solr TermsComponent: Usage of wildcards I'm using Solr 1.4.1 and I'm willing to use TermsComponent for AutoComplete. The problem is I can't get it to match strings with spaces in them. So to say terms.fl=name&terms.lower=david&terms.prefix=david&terms.lower.incl=false&indent=true&wt=json matches all strings starting with ""david"" but if I change it to: terms.fl=name&terms.lower=david%20&terms.prefix=david%20&terms.lower.incl=false&indent=true&wt=json it doesn't match all strings starting with ""david "". Is it meant to be that way? If so are n-grams the way to go? And does anybody know if TermsComponent is implementing Tries or DAWGs or Raddix trees and if it's efficient? Cheers Parsa AFAIK TermsComponent provides raw (i.e. literal) access to the fields' terms so if there isn't any term with space (normally there isn't the whitespace tokenizer takes care of that) it won't match anything. TermsComponent doesn't implement tries or anything it just enumerates terms in the field index. IMHO ngrams are a more flexible solution for autocomplete. The next release of Solr will have a specific component to implement autosuggest (you could use it now if you use nightly builds)"
293,A,"How can I access child object property in Compas Search framework? I have following code (in grails and Searchable Plugin aka Compass): class Topic { String name; static searchable = true; } class Question extends BaseEntity { String question; static searchable = true; static hasMany = [ topics: Topic ] } How can I search Question with specific topic id? Something like Question.search(""topics#id:12"") or Question.search(""topics.id:12"") dosnt work. Chage your searchable block in Question so it looks like this: static searchable = { topics component: true } and in Topic if you dont want Topics returned as root search elements static searchable = [ root: false ] Fire up grails and add a few items then download Luke from http://www.getopt.org/luke/ and open the index for your Question domain object which will be at ~/.grails/projects/projName/searchable-index/'env'/index/question If you check the documents tab you will see the terms embedded in the index which will be something like $/Question/topics This should give you the path to put in your Question.search something like: Question.search('$/Question/topics/id:1')"
294,A,"How to get the results which has all the strings specified in the search query I am a beginner in Lucene. I am writing a search engine to search our code base for certain key words. I have a requirement for which I need your help. Say I am searching for a word ""Apple computers"" I would like Lucene to throw only the lines which have case insesitive ""apple computers"". But what I see is I see lines having Apple computers lines having only apple and lines having only computers. How do I filter it to get only the lines having apple and computer. Hi Thanks for all the help. I used the phrase query and used proper slop value. The result just was the one I was looking for. The Boolean query too worked. The Book ""Lucene in Action"" answered all my queries. I recommend this book strongly to all who wants to learn Lucene As Yuval suggested it's important to know how do you use Lucene. If you use it through lucene-java and need exact phrase results (docs that contain only ""apple computers"" together) you can use PhraseQuery. The example of how to compose it. Thanks for all the valuable inputs. I am following the same approach. But another requirement is that I need to highlight the strings in the results I get. But when i use the TermFreqVectorTermPositionVector the offsets I get still points to lines which has only apple and sole lines which have computers only. I am storing the entire content of the file as one document against the field name ""contents"". I think my approach is wrong and I need to idex line by line. Is this correct. Or which is a better approach to highlight the search results. Any help much app  How do you query Lucene? Basically what you are asking about is covered by building a query using BooleanClause.Occur.MUST. Exactly how to do this is dependent on your query construction: For the default query parser you should use something like +Apple +computers While if you are building queries programmatically you should use MUST for every term."
295,A,"In Lucene using a Standard Analyzer I want to make fields with spaces searchable In Lucene using a Standard Analyzer I want to make fields with space searchable. I set Field.Index.NOT_ANALYZED and Field.Store.YES using the StandardAnalyzer When I look at my index in LUKE the fields are as I expected a field and a value such as: location -> 'New York'. Here I found that I can use the KeywordAnalyzer to find this value using the query: location:""New York"". But I want to add another term to the query. Let's say a have a body field which contains the normalized and analyzed terms created by the StandardAnalyzer. Using the KeywordAnalyzer for this field I get different results than when I use the StandardAnalyzer. How do I combine two Analyzers in one QueryParser where one Analyzer works for some fields and another one for another fields. I though of creating my own Analyzer which could behave differently depending on the field but I have no clue how to do it. PerFieldAnalyzerWrapper lets you apply different analyzers for different fields. This was the solution we used for the exact same problem. I recommend creating a public getAnalyzer method that can be used by both the Searcher and the Writer."
296,A,"SOLR HTTP 500 Can't find resource 'solrconfig.xml' I have Apache SOLR working with ColdFusion on my local machine however when I tried to make the move to production (environments are different) I keep getting the HTTP 500 message below. Production environment is using Ubuntu Lucid Apache ColdFusion 9.0.1. Using the version of SOLR installed with ColdFusion. The path for solrconfig.xml in the error message ""/opt/jrun4/servers/prod-autofeed1/cfusion.ear/cfusion.war/WEB-INF/cfusion/collections/autofeed/conf/"" is correct. Any suggestions? Thank you. HTTP ERROR: 500 Severe errors in solr configuration. Check your log files for more detailed information on what may be wrong. If you want solr to continue after configuration errors change:  <abortOnConfigurationError>false</abortOnConfigurationError> in solr.xml ------------------------------------------------------------- java.lang.RuntimeException: Can't find resource 'solrconfig.xml' in classpath or '/opt/jrun4/servers/prod-autofeed1/cfusion.ear/cfusion.war/WEB-INF/cfusion/collections/autofeed/conf/' cwd=/opt/jrun4/servers/cfusion/cfusion-ear/cfusion-war/WEB-INF/cfusion/solr at org.apache.solr.core.SolrResourceLoader.openResource(SolrResourceLoader.java:260) at org.apache.solr.core.SolrResourceLoader.openConfig(SolrResourceLoader.java:228) at org.apache.solr.core.Config.<init>(Config.java:101) at org.apache.solr.core.SolrConfig.<init>(SolrConfig.java:130) at org.apache.solr.core.CoreContainer.create(CoreContainer.java:405) at org.apache.solr.core.CoreContainer.load(CoreContainer.java:278) at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:117) at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:83) at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:99) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:40) at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:594) at org.mortbay.jetty.servlet.Context.startContext(Context.java:139) at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1218) at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:500) at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:448) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:40) at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:147) at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:161) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:40) at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:147) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:40) at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:117) at org.mortbay.jetty.Server.doStart(Server.java:210) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:40) at org.mortbay.xml.XmlConfiguration.main(XmlConfiguration.java:929) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at org.mortbay.start.Main.invokeMain(Main.java:183) at org.mortbay.start.Main.start(Main.java:497) at org.mortbay.start.Main.main(Main.java:115) RequestURI=/solr/ Powered by Jetty:// @Frank I cannot seem to give you credit for the answer? @Frank that was the issue! I had copied over my schema.xml and solrconfig.xml files to production and completely forgot about permissions. Thank you for your help! Reposted as an answer (which maybe I should have done in the first place) if you want to accept that. Double check permissions on the directory /opt/jrun4/servers/prod-autofeed1/cfusion.ear/cfusion.war/WEB-INF/cfusion/collections/autofeed/conf and the file /opt/jrun4/servers/prod-autofeed1/cfusion.ear/cfusion.war/WEB-INF/cfusion/collections/autofeed/conf/solrconfig.xml. If the user solr is run under can't read the dir/file that'd do it. To test you might even su to the user in question and simply try to cat the config file. thank you!"
297,A,"Java kill suddenly I am getting this error when writer.optimize() called I have caugh all exceptions but hopeless .writer is an instance of apache lucene Indexwriter and tomcat collapse when optimizing the indexwriter.I am trying to index a large number of file its works for a few number of file but when number of files increase it cause to tomcat fail. logger.info(""Optimizing optimazing...""); this.writer.optimize(); logger.info(""Optimizing closing...""); this.writer.close(); logger.info(""Optimazide and closed succesfully...""); # # A fatal error has been detected by the Java Runtime Environment: # # SIGSEGV (0xb) at pc=0x00007fe6f8c38e90 pid=10316 tid=140629887768320 # # JRE version: 6.0_20-b20 # Java VM: OpenJDK 64-Bit Server VM (19.0-b09 mixed mode linux-amd64 compressed oops) # Derivative: IcedTea6 1.9.7 # Distribution: Ubuntu 10.10 package 6b20-1.9.7-0ubuntu1 # Problematic frame: # V [libjvm.so+0x54ae90] # # If you would like to submit a bug report please include # instructions how to reproduce the bug and visit: # https://bugs.launchpad.net/ubuntu/+source/openjdk-6/ # --------------- T H R E A D --------------- Current thread (0x00000000023e0000): JavaThread ""CompilerThread0"" daemon [_thread_in_native id=10333 stack(0x00007fe6f27150000x00007fe6f2816000)] siginfo:si_signo=SIGSEGV: si_errno=0 si_code=1 (SEGV_MAPERR) si_addr=0x0000000000000008 Registers: RAX=0x0000000000000000 RBX=0x00007fe6f2812930 RCX=0x00007fe6ec03e9e0 RDX=0x0000000000002000 RSP=0x00007fe6f2811150 RBP=0x00007fe6f2811190 RSI=0x00007fe6e43a20f0 RDI=0x0000000000000000 R8 =0x00007fe6e43f5a70 R9 =0x00007fe6f2812930 R10=0x00007fe6ec6f7948 R11=0x0000000000000000 R12=0x00007fe6edd326b0 R13=0x00007fe6ec6f7948 R14=0x00007fe6f2812950 R15=0x00007fe6ec068990 RIP=0x00007fe6f8c38e90 EFL=0x0000000000010206 CSGSFS=0x0000000000000033 ERR=0x0000000000000004 TRAPNO=0x000000000000000e Register to memory mapping: RAX=0x0000000000000000 0x0000000000000000 is pointing to unknown location RBX=0x00007fe6f2812930 0x00007fe6f2812930 is pointing into the stack for thread: 0x00000000023e0000 ""CompilerThread0"" daemon prio=10 tid=0x00000000023e0000 nid=0x285d runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE RCX=0x00007fe6ec03e9e0 0x00007fe6ec03e9e0 is pointing to unknown location RDX=0x0000000000002000 0x0000000000002000 is pointing to unknown location RSP=0x00007fe6f2811150 0x00007fe6f2811150 is pointing into the stack for thread: 0x00000000023e0000 ""CompilerThread0"" daemon prio=10 tid=0x00000000023e0000 nid=0x285d runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE RBP=0x00007fe6f2811190 0x00007fe6f2811190 is pointing into the stack for thread: 0x00000000023e0000 ""CompilerThread0"" daemon prio=10 tid=0x00000000023e0000 nid=0x285d runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE RSI=0x00007fe6e43a20f0 0x00007fe6e43a20f0 is pointing to unknown location RDI=0x0000000000000000 0x0000000000000000 is pointing to unknown location R8 =0x00007fe6e43f5a70 0x00007fe6e43f5a70 is pointing to unknown location R9 =0x00007fe6f2812930 0x00007fe6f2812930 is pointing into the stack for thread: 0x00000000023e0000 ""CompilerThread0"" daemon prio=10 tid=0x00000000023e0000 nid=0x285d runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE R10=0x00007fe6ec6f7948 0x00007fe6ec6f7948 is pointing to unknown location R11=0x0000000000000000 0x0000000000000000 is pointing to unknown location R12=0x00007fe6edd326b0 0x00007fe6edd326b0 is pointing to unknown location R13=0x00007fe6ec6f7948 0x00007fe6ec6f7948 is pointing to unknown location R14=0x00007fe6f2812950 0x00007fe6f2812950 is pointing into the stack for thread: 0x00000000023e0000 ""CompilerThread0"" daemon prio=10 tid=0x00000000023e0000 nid=0x285d runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE R15=0x00007fe6ec068990 0x00007fe6ec068990 is pointing to unknown location Top of Stack: (sp=0x00007fe6f2811150) 0x00007fe6f2811150: 00007fe6e4522cd0 00007fe6f2811420 0x00007fe6f2811160: 00007fe6f2811190 00007fe6f2812930 0x00007fe6f2811170: 0000000000000002 00007fe6edd326b0 0x00007fe6f2811180: 00007fe6ec5d6430 00007fe6f2811420 0x00007fe6f2811190: 00007fe6f2811200 00007fe6f8c3941b 0x00007fe6f28111a0: 0000000000000002 00007fe600000100 0x00007fe6f28111b0: 00007fe600000001 00007fe6f28132d0 w-p 00021000 08:01 17301749 /lib/ld-2.12.1.so 7fe6fa020000-7fe6fa021000 rw-p 00000000 00:00 0 7fffd5558000-7fffd5579000 rw-p 00000000 00:00 0 [stack] 7fffd55ff000-7fffd5600000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] VM Arguments: jvm_args: -Djava.util.logging.config.file=/home/murat/Desktop/servers/apache-tomcat-6.0.24/conf/logging.properties -Dhttp.nonProxyHosts=localhost|127.0.0.1|expertPC -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -agentlib:jdwp=transport=dt_socketaddress=11550server=ysuspend=n -Djava.endorsed.dirs=/home/murat/Desktop/servers/apache-tomcat-6.0.24/endorsed -Dcatalina.base=/home/murat/Desktop/servers/apache-tomcat-6.0.24 -Dcatalina.home=/home/murat/Desktop/servers/apache-tomcat-6.0.24 -Djava.io.tmpdir=/home/murat/Desktop/servers/apache-tomcat-6.0.24/temp java_command: org.apache.catalina.startup.Bootstrap start Launcher Type: SUN_STANDARD Environment Variables: JAVA_HOME=/usr/lib/jvm/java-6-openjdk JRE_HOME=/usr/lib/jvm/java-6-openjdk PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games USERNAME=murat LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk/jre/../lib/amd64 SHELL=/bin/bash DISPLAY=:0.0 Signal Handlers: SIGSEGV: [libjvm.so+0x723630] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 SIGBUS: [libjvm.so+0x723630] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 SIGFPE: [libjvm.so+0x5e0000] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 SIGPIPE: SIG_IGN sa_mask[0]=0x00000000 sa_flags=0x00000000 SIGXFSZ: [libjvm.so+0x5e0000] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 SIGILL: [libjvm.so+0x5e0000] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 SIGUSR1: SIG_DFL sa_mask[0]=0x00000000 sa_flags=0x00000000 SIGUSR2: [libjvm.so+0x5df710] sa_mask[0]=0x00000004 sa_flags=0x10000004 SIGHUP: [libjvm.so+0x5e2180] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 SIGINT: SIG_IGN sa_mask[0]=0x00000000 sa_flags=0x00000000 SIGTERM: [libjvm.so+0x5e2180] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 SIGQUIT: [libjvm.so+0x5e2180] sa_mask[0]=0x7ffbfeff sa_flags=0x10000004 --------------- S Y S T E M --------------- OS:Ubuntu 10.10 (maverick) uname:Linux 2.6.35-28-generic #50-Ubuntu SMP Fri Mar 18 18:42:20 UTC 2011 x86_64 libc:glibc 2.12.1 NPTL 2.12.1 rlimit: STACK 8192k CORE 0k NPROC infinity NOFILE 1024 AS infinity load average:0.77 0.30 0.14 /proc/meminfo: MemTotal: 4054828 kB MemFree: 176928 kB Buffers: 207640 kB Cached: 1332820 kB SwapCached: 17608 kB Active: 2419624 kB Inactive: 1004992 kB Active(anon): 1834536 kB Inactive(anon): 67792 kB Active(file): 585088 kB Inactive(file): 937200 kB Unevictable: 16 kB Mlocked: 16 kB SwapTotal: 11876348 kB SwapFree: 11687616 kB Dirty: 3508 kB Writeback: 32 kB AnonPages: 1873148 kB Mapped: 197036 kB Shmem: 18240 kB Slab: 157916 kB SReclaimable: 131452 kB SUnreclaim: 26464 kB KernelStack: 3928 kB PageTables: 32140 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 13903760 kB Committed_AS: 3427468 kB VmallocTotal: 34359738367 kB VmallocUsed: 323536 kB VmallocChunk: 34359412360 kB HardwareCorrupted: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB DirectMap4k: 272384 kB DirectMap2M: 3919872 kB CPU:total 2 (2 cores per cpu 1 threads per core) family 6 model 23 stepping 10 cmov cx8 fxsr mmx sse sse2 sse3 ssse3 sse4.1 Memory: 4k page physical 4054828k(176928k free) swap 11876348k(11687616k free) vm_info: OpenJDK 64-Bit Server VM (19.0-b09) for linux-amd64 JRE (1.6.0_20-b20) built on Feb 23 2011 09:05:53 by ""buildd"" with gcc 4.4.5 time: Mon Jun 6 22:24:11 2011 elapsed time: 2076 seconds looks like internal bug in the JVM could be an issue w/ attaching a debugger and deoptimizing/optimizing? also tomcat 6.24 is quite buggy on its own (not related to your case though) Since it is most likely a bug in the JVM the first thing I would try is updating your JVM to Java 6 update 25. You could try the Sun/Oracle JVM but it likely to be the same. I am assuming you are not using an JNI libraries? it's the compiler thread doesn't look like JNI issue.  Use sun/oracle java jdk not the open jdk... Regards Stéphane  As Stéphane says try different JREs to see if you can get a different error message. There's a chance (but hard to quantify) that this is related to reaching a memory limit but it'll be hard to be sure unless you do get an error saying which one! memory issue seems a good target since the java args miss any memory specific options"
298,A,"Solr multiple search and group by date range if have the problem to execute multiple Solr queries at ones with the same phrase but for different timeranges. Example: search for ""atom"" at: 2011-04-01T10:20:22.0Z TO 2011-04-01T12:20:22.0Z 2011-03-08T10:20:22.0Z TO 2011-03-08T12:20:22.0Z 2011-02-05T10:20:22.0Z TO 2011-02-05T12:20:22.0Z So i need a few messages from each 2 hour interval. First of all i thought about facet search but i don't think thats a way is'n it? 2nd idea was to fire one solr request for every time range. But probably there is to much (network)overhead for that because this example is only an simplified version. Maybe anybody has an idea how could i handle this? What solr functionality is the best way for this? Thank you. Use FieldCollapsing with the group by query option. oh yeah very nice new feature. thank you - that is what i was looking for!"
299,A,"How far behind the original is Lucene.Net? I've noticed that Lucene recently released v2.9 (on 25th September this year - 2009) whereas Lucene.Net appears to be v2.0 (released back in 2007): Does the v2.0 of Lucene.net correspond to the features found in v2.0 of the original Apache Lucene Are the improvements made in Apache Lucene since 2007 significant enough to warrant considering using the Java version (with some interop) instead? The version numbers match between the Java and .Net versions - the .Net version is a direct port of the Java version (so yes the features in v2.0 of Lucene.net correspond to the features found in v2.0 of the original Apache Lucene). There are later versions than v2.0 of Lucene.Net available. On the Lucene.Net user list it was announced recently that the port of v2.9 is under way and will be ready for testing soon. I am using Lucene.Net v2.3.2 (which was released on 24 July 2009) in a production application without any problems. There is also a later v2.4 which was released on 19th August 2009. The news page of Lucene.Net's project pages has a full list of releases. You can find the later versions of the Lucene.Net library that @adrianbanks mentions under the ""tags"" folder in the SVN source. We're currently using the v2.4 tag and so far it works fine.  It depends on your project requirements and overall architecture. Lucene as you know is a class library not a stand alone service so likely you will be writing code which uses the lucene library in either .NET (c#) or Java. If the rest of your project is .NET then it may make more sense to use the Lucene.NET port even though it is behind the Java version. On the other hand if you need very generic Lucene functionality you may be able to use SOLR as a stand-alone service and then use web services to interface to Lucene from your other components such as .NET services web-site front-ends etc. There is not huge amount of difference between Lucene.NET and Java versions mostly some performance improvements and some fixes/changes to range queries and also more payload support but those are mostly advanced features you may not need. We use Lucene.NET 1.9 and it is very stable and very fast for us. We have over 200 million documents distributed over about 8 indexes.  They have much more recent builds in their SVN repos. For all the basic stuff I think it works really well. I'm sure that in corner cases the latest JAVA ones edge it out but for basic stuff you should be ok. See here"
300,A,"How to get frequency of multi-word terms in Lucene? I'm using Lucene to get frequency of terms in documents i.e. number of occurrences of some term in each document. I use IndexReader.termDocs() for this purpose and it works fine for single-word terms but since all words are stored in index separately it doesn't work for multi-word terms. Example (taken from this question): I'm interested in frequency of term ""basket-ball"" (or even ""basket ball"") but after tokenizing there will be two words and I'll be able to get frequency of term ""basket"" and term ""ball"" but not of term ""basket-ball"". I know all multi-word terms I want to get frequency for also I'm not interested in storing original text - only in getting statistics. So my first approach was to just concatenate words in a term. E.g. ""I played basket ball yesterday"" becomes ""I played basketball yesterday"" and ""My favorite writer is Kurt Vonnegut"" becomes ""My favorite writer is KurtVonnegut"". This one works: concatenated terms are treated as any other single word so I can easily get frequency. But this method is ugly and more importantly very slow. So I came to another one. My second approach is to write special token filter which will capture tokens and check if they are part of terms to be replaced (something like SynonymFilter from Lucene in Action). In our case when filter will see word ""basket"" it will read one more token and if it is ""ball"" filter will place one term (""basketball"") instead of two (""basket"" and ""ball"") in an output token stream. Advantage of this method compared to the previous is that it searches matches between complete words and doesn't scan full text for substrings. In fact most tokens will have different lengths and so will be discarded without even checking for correspondence of any letter in them. But such a filter isn't easy to write moreover I'm not sure it will be fast enough to fit my needs. Third approach I can think about is to play around with positions of two words in same documents. But most probably it will involve iterating through TermDocs during getting frequency time which costs much more then indexing time. So finally my question is: is there a way to efficiently index and get frequency of multi-word terms in Lucene? Look up shingling.. This indexes groups of terms. It's in the solr 1.4 book. and here So if you have the string : ""Basket ball started in the early 1900's . You would get back all the individual terms indexed but then also "" ""basket ball"" ""ball started"" ""started in"" early 1900's"" etc... and through configuration also ""basket ball started"" ""ball started in"" ""the early 1900's"" etc... I've found Lucene's [ShingleFilter](http://lucene.apache.org/java/3_0_3/api/contrib-analyzers/org/apache/lucene/analysis/shingle/ShingleFilter.html) and [ShingleAnalyzerWrapper](http://lucene.apache.org/java/3_0_3/api/contrib-analyzers/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapper.html) which are exactly what I need thanks!"
301,A,"Stop words in ""All of the words"" feature I'm working on ""all of these words"" feature using Lucene. I'm using StandardAnalyzer without any stop words. When user types in words which contain ""the"" ""and"" etc lucene does not return any result. If i remove the stop words from the input then lucene gives search results. Am using booleanquery with BooleanClause.Occur.MUST clause. Am i missing out on something here? Thanks. Did you re-index with the same analyzer set up? You have to make the change to the query parser and the indexer."
302,A,"Lucene - Which analyzer to use to avoid prepositions I am using the Lucene standard analyzer to parse text. however it is returning prepositions as well as words like ""i"" ""the"" ""and"" etc... Is there an Analyzer I can use that will not return these words? Thanks StandardAnalyzer uses StopFilter. By default the words in the STOP_WORDS_SET are excluded. If this is not sufficient there are constructors which allow you to pass in a list of stop words which should be removed from the token stream. You can provide the list using a File a Set or a Reader. ok thanks for that. If I want to pass in my own words do you know of a list of words that say Google will not index? or any other search engine for that matter. I had a look around but couldn't find anything. Thanks again @Joeblackdev Hmm...sorry I don't know of such a list. no problem. thanks for the help ;) Actually Google doesn't omit these types of words. An excellent example of this is go Google the band ""The The""."
303,A,Lucene.NET Stemmer for romanian language I'm looking for a stemmer for the .NET version of Lucene that supports the romanian language. Do you by any chance know where I can find one ? Google doesn't seem to be very helpful with this topic. Thank you ! lucene-hunspell supports the Romanian language. Simon Svensson has ported this to Lucene.net. Seems like it is worth a try. Yes thank you ! This is exactly what I was looking for.  There is not currently any stemmer that specifically supports the Romanian language.
304,A,"Example using WikipediaTokenizer in Lucene I want to use WikipediaTokenizer in lucene project - http://lucene.apache.org/java/3_0_2/api/contrib-wikipedia/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.html But I never used lucene. I just want to convert a wikipedia string into a list of tokens. But I see that there are only four methods available in this class end incrementToken reset reset(reader). Can someone point me to an example to use it. Thank you. See also http://stackoverflow.com/questions/3916806/3916947#3916947 In Lucene 3.0 next() method is removed. Now you should use incrementToken to iterate through the tokens and it returns false when you reach the end of the input stream. To obtain the each token you should use the methods of the AttributeSource class. Depending on the attributes that you want to obtain (term type payload etc) you need to add the class type of the corresponding attribute to your tokenizer using addAttribute method. Following partial code sample is from the test class of the WikipediaTokenizer which you can find if you download the source code of the Lucene. ... WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test)); int count = 0; int numItalics = 0; int numBoldItalics = 0; int numCategory = 0; int numCitation = 0; TermAttribute termAtt = tf.addAttribute(TermAttribute.class); TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class); while (tf.incrementToken()) { String tokText = termAtt.term(); //System.out.println(""Text: "" + tokText + "" Type: "" + token.type()); String expectedType = (String) tcm.get(tokText); assertTrue(""expectedType is null and it shouldn't be for: "" + tf.toString() expectedType != null); assertTrue(typeAtt.type() + "" is not equal to "" + expectedType + "" for "" + tf.toString() typeAtt.type().equals(expectedType) == true); count++; if (typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true){ numItalics++; } else if (typeAtt.type().equals(WikipediaTokenizer.BOLD_ITALICS) == true){ numBoldItalics++; } else if (typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true){ numCategory++; } else if (typeAtt.type().equals(WikipediaTokenizer.CITATION) == true){ numCitation++; } } ...  public class WikipediaTokenizerTest { static Logger logger = Logger.getLogger(WikipediaTokenizerTest.class); protected static final String LINK_PHRASES = ""click [[link here again]] click [http://lucene.apache.org here again] [[Category:a b c d]]""; public WikipediaTokenizer testSimple() throws Exception { String text = ""This is a [[Category:foo]]""; return new WikipediaTokenizer(new StringReader(text)); } public static void main(String[] args){ WikipediaTokenizerTest wtt = new WikipediaTokenizerTest(); try { WikipediaTokenizer x = wtt.testSimple(); logger.info(x.hasAttributes()); Token token = new Token(); int count = 0; int numItalics = 0; int numBoldItalics = 0; int numCategory = 0; int numCitation = 0; while (x.incrementToken() == true) { logger.info(""seen something""); } } catch(Exception e){ logger.error(""Exception while tokenizing Wiki Text: "" + e.getMessage()); } } TermAttribute is not available in Lucene 4.2.1. So how do you access the attributes when having moved to a higher version of Lucene?  WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test)); Token token = new Token(); token = tf.next(token); http://www.javadocexamples.com/java_source/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java.html Regards"
305,A,Katta usage examples Does anybody use Katta with Java? Are any samples avalible? We are using. You can find samples in unit tests.  Take a look at the tests here and here for reference. hm... I know about some tests and other useful code. But may be there are some articles/tutorials available?
306,A,"Make Zend Search Lucene more ""sensitive"" I was wondering if someone knows what I need to do to make Zend Search more sensitive. Currently say I am searching for the word: Penelope from my index I need to type in the entire word in order to get a match. What I'm after is that when I type 'P' it immediately returns relevant results for content having words beginning with 'P'. I am using the standard $index->find('Penelope'); Do I need to use the query builder to achieve this or am I missing something obvious? Thanks According to the lucene documentation you can use wildcards. ? for single character wildcards and * for multicharacter wildcards. try something like this. $index->find('Pe*'); More info here: http://framework.zend.com/manual/en/zend.search.lucene.query-language.html If this is what you are looking for please select the check mark next to this answer. Thanks! Yes this is exactly what I was looking for thanks."
307,A,"Range Query with Lucene 2.9.x - Dates in Index are not working I use the following statement to index a date: luceneDoc.add(new NumericField(key).setLongValue(date.getTime())); I also use statements as follows to add text properties: luceneDoc.add(new Field(key value Field.Store.YES Field.Index.ANALYZED)); Then I perform a text property query: author:hans This works perfect. But when I perform a range query nothing gets returned: my-date-property:[20100101 TO 20110101] What am I missing here? I had a look at the index with Luke I see all my text property for a document but the date properties only appear in the overview page... maybe that is normal. I actually DO SEE the date properties if I add it like this: NumericField field = new NumericField(key Field.Store.YES true); field.setLongValue(date.getTime()); luceneDoc.add(field); But: the query still does not work! Maybe it only works from Java with the Query Builder? I have not tried out that. But it would be great if the text query would work too. ANY IDEA??? Try to declare my-date-property as DateField. Since you are using a NumbericField I suppose that the range you specified is interpreted as a numeric range instead of a date range. In this case the numbers 20100101 and 20110101 are far too low to get any reasonable results. I tried that but the class does not exist anymore it is fully deprecated. Also in the ""Lucene In Action"" book they say to use NumericField. See the javadoc: [DateField Javadoc](http://lucene.apache.org/java/2_9_0/api/all/org/apache/lucene/document/DateField.html) I EDITED my question please have a look. It is still not working!  Numeric fields and numeric range queries are absolutely brilliant but they really are tricky to use for the first time! Currently the standard query parser doesn't support numeric range queries. In order to make use of numeric fields you'll need to derive your own query parser variants and construct numeric range queries where appropriate. Clarifying my original answer a little (I've just seen it referred to in a comment...) I should note that range queries where numbers are converted to (usually) zero prefixed text work fine (albeit relatively slowly) in the standard query parser. From the original information posted the question is how to use numeric (trie encoded) fields in a query. For that you need to parse a query in such a way as to produce numeric range queries (which understand trie encodings). These work much faster than text encoded numeric fields. Good luck  If you want a range query to work with dates in the form of YYYYMMDD then index your date like this: String dateString = DateTools.dateToString(date Resolution.DAY); luceneDoc.add(new Field(key dateString Store.YES Index.NOT_ANALYZED)); Yes you can do range queries over strings see http://lucene.apache.org/java/2_4_0/queryparsersyntax.html#Range%20Searches. Thanks! range queries also work with Text. I'm testing now. THANKS! This works like a charm! Thanks I'll try this out... So you are also saying that the standard query parser DOES support range queries right? Because ['Moleski'](http://stackoverflow.com/questions/5835232/range-query-with-lucene-2-9-x-dates-in-index-are-not-working/5844872#5844872) says something different... Just a question: by adding a `Field` instead of `NumericField` I assume that it is stored as TEXT property in the index. Currently I see my date values as DATE in index (via Luke). If it is stored as TEXT I guess you can't do range queries via the API right? If so it is also not what I wanted... If i want to do range search or sort by the date or numberthe field must not be Not_analyzed?thanks"
308,A,"how do i filter my lucene search results? Say my requirement is ""search for all users by name who are over 18"" If i were using SQL i might write something like: Select * from [Users] Where ([firstname] like '%' + @searchTerm + '%' OR [lastname] like '%' + @searchTerm + '%') AND [age] >= 18 However im having difficulty translating this into lucene.net. This is what i have so far: var parser = new MultiFieldQueryParser({ ""firstname"" ""lastname""} new StandardAnalyser()); var luceneQuery = parser.Parse(searchterm) var query = FullTextSession.CreateFullTextQuery(luceneQuery typeof(User)); var results = query.List<User>(); How do i add in the ""where age >= 18"" bit? I've heard about .SetFilter() but this only accepts LuceneQueries and not IQueries. If SetFilter is the right thing to use how do I make the appropriate filter? If not what do I use and how do i do it? Thanks! P.S. This is a vastly simplified version of what I'm trying to do for clarity my WHERE clause is actually a lot more complicated than shown here. In reality i need to check if ids exist in subqueries and check a number of unindexed properties. Any solutions given need to support this. Thanks For the age field you need a range search written in the syntax of Lucene something like: age:[18 TO 100] As Gandalf said you can use a QueryWrapperFilter. I am not sure this exists in Nhibernate Search. Similarily you can use ""AND"" to further constrain your query. I am not sure what you can do about unindexed properties.  Use a QueryWrapperFilter. can you give an example? In Lucene.Net QueryFilter only accepts LunceneQueries not nhib queries. How do i create the query (in a generic way if possible not just for `age >= 18`)? thanks  In the end i did away with NHibernate.Search and simply talked directly to lucene to get the IDs then passed these into an HQL where clause much simple and more efficient. Edit: There is a restriction in NH.Search that prevents this from working. It can be simply patched but once you've read through the NH.S code you realise how awfully inefficient it is. Going straight to Lucene is the best option."
309,A,"Lucene MoreLikeThis cannot be resolved to a type I'm trying to use MoreLikeThis class from lucene as described in ""Lucene in action"" book but that class does not seem to exist :/ I'm using lucene-core-2.9.4.jar normal indexing and searching works fine. I have looked iniside the jar (and the 3.x.x version) but there is no such class inside what am I missing? Go to this page: http://www.manning.com/hatcher2/ and download the source code. The class MoreLikeThis is not part of lucene itself rather an example class related to the book Hmm although I can't see if in the source tree. Is the class file definitely called MoreLikeThis?  Are you referring to the MoreLikeThis class? It should be inside lucene-queries-2.9.4.jar. Look under the contrib/queries folder of your Lucene binary download."
310,A,"Search filters with Lucene.NET I'm using Lucene.Net to create a website to search books articles etc stored as PDFs. I need to be able to filter my search results based on author name for example. Can this be done with just Lucene? Or do I need a DB to store the filter fields for each document? Also what's the best way to index my documents? I'll have about 50 documents to start with and periodically I'll have to add a bunch of documents to the index--may be through a web form. Should I use a DB to store the document paths? Thanks. Lucene has a couple of different Analyzers that can scrub out the noise and do ""stemming"" which is helpful when you want to do fulltext searching but you're still going to need to store the PDF itself somewhere. Lucene.Net is happy to build an index on the file system and you could add a field to the Document it builds called something like ""PATH"" with the path to the document.  Here is a list of what you need to do IMO: Extract raw text from PDF - please see this question which recommends iTextSharp for this purpose. For each PDF document create a Lucene.net document that has several fields: author title document text and whatever you want to search. It is recommended to also have a unique id field per document. I suggest you also store a field with the path to the original PDF document. After indexing all the documents you will have a Lucene index you can search by fields. You can add new documents by repeating step 2. It is easier to do this offline - incremental updates are tough. You will need a DB if you need DB functionality such as joins or sophisticated selects. This paper: http://www.lucidimagination.com/Community/Hear-from-the-Experts/Articles/Search-Engine-versus-DBMS addresses the issue of what to put in a database vs. what to put in a search engine. A DB may be the right place for additional information you only need to display not search. Excellent answer thanks for simplifying it. So there's no need for a DB at all? If I'm going to do step 2 offline and say I let my users add documents would it help to send all requests to a DB and then I can have a separate process that indexes the ones that haven't already been indexed and use the primary key id as the unique id in the index? Do you think it makes sense to have a DB? In case in the future I decide to have some ""related information"" or something like that for each document a DB would help right?"
311,A,"Lucene Hebrew analyzer Does anybody know whether one exists? I've been googling this for monthes... Thanks It's time the open source community makes one. It seems to me that true stemming is _very_ difficult to the point of requiring vast amounts of manpower but that some basic stemming is possible and perhaps a minimal stemmer is better than zero stemming. I'll probably start working on this on my own. If anyone is interested please contact me. ...And as a first step I'll try to use hspell(3)'s enumeration. It's effectively a ready stemmer! Update HebMorph Out of curiosity sparked by your question I contacted Itamar Syn-Hershko who was active on Lucene mailing lists about a year ago when he was working on a Hebrew analyzer for Lucene. I asked him if he completed his analyzer. Here are some relevant bits from his response: To make a long story short no I didn't. There is no decent free / open-source Hebrew analyzer for Lucene that I can say for sure. I'm not sure what is your background on the subject but believe me when I say there is no easy way of doing this; it might be also the Lucene isn't built for Hebrew searches but I do agree a solution has to be given. Granted the safest way to index and search Hebrew texts is to use a specialized stemmer and integration with Lucene is not the easiest even after you've done this. There are a few very good solutions for Hebrew search in the market only one that I know of is using Lucene in it's core; I've recently tried contacting them no response yet... The commercial product based on Lucene that is mentioned is called ATTIVIO and the ATTIVIO website does claim to have Hebrew support. At SIGTRS (Hebrew Text Retrieval interest group) there has been some discussion regarding ATTIVIO that claims it is Lucene based. So apparently it is possible to create a decent Hebrew analyzer for Lucene but there is no free analyzer available at this time. Wow thanks a whole bunch! No problem; glad it was helpful. ;) Hi I finally got the time to start working on one and so far it looks promising. See: http://www.code972.com/blog/hebmorph/. Itamar. Itamar thanks for the link! Glad to see you on SO.  dtsearch has a hebrew stemming plugin call ""pensim"". It appears to be developed by ""wizcomtech.com""."
312,A,"Which field had my search text in Lucene when using a MultiFieldQueryParser? I'm using Lucene.Net's MultiFieldQueryParser to search multiple fields in my documents. I want to find out which field the text was found. For example my search might look like this: var parser = new MultiFieldQueryParser(new string[] {""question""""answer""} analyzer); var query = parser.Parse(searchphrase); for(int idx=0; idx<hits.Length() ++idx) { var doc = hits.Doc(i); // was this hit found in ""answer"" or ""question""?? } I want to determine whether searchphrase was found in the answer or question field The only way to tell would be to store the fields retrieve them for each hit and examine them yourself for a match. A hit can result from some terms of the search phrase being found in the question with the rest in the answer. If you search questions and answers together you can't easily determine which was which.  For debugging purposes you can use Lucene's explain() method which walks you through the matching. It is as costly as the search itself so it is not so good for production. See also Debugging Relevance Issues in Search by Grant Ingersoll about other ways to get this information."
313,A,"use compass-lucene as caching technique Any example of scenarios other than doing search for which I could use ""compass""? Lets say we have a page that list top 10 most view article. How to use compass to show this kind of results. Any demo/sample project on this to refer to? definitely Jira would be a good example but its source code is not available. I want to know how to maximize the benefits of using compass-lucene in an application. May i know where can i download spring-compass jpa @annotated example? The nightly built i downloaded is xml-based. Any example of scenarios other than doing search for which I could use ""compass""? Well AFAIK this is what is has been designed for. Lets say we have a page that list top 10 most view article. How to use compass to show this kind of results. I'm not sure this is a good use-case Compass is in my opinion useful when you want to search results across the whole application business domain model i.e. not only lets say articles (in that case you can just query the database). Now let's imagine that your domains objects are searchable classes and have a searchable property numberOfViews I guess it would be possible to search on this property (refer to the whole Searching section). Any demo/sample project on this to refer to? The compass distribution includes samples: a Library basic example that highlights the main features of Compass::Core and a Petclinic sample that shows how to add compass to an existing application (the Spring petclinic sample). I want to know how to maximize the benefits of using compass-lucene in an application. Read the author's blog Compass wiki and the reference documentation :) May i know where I can download spring-compass jpa @annotated example? As mentioned the spring-compass sample included in compass distribution is based on Spring's petclinic which doesn't use annotations (see SPR-2960). Just in case petclinic annotated entities are attached to SPR-2960 so feel free to use them."
314,A,"Inverted search: Phrases per document I have a database full of phrases (80-100 characters) and some longish documents (50-100Kb) and I would like a ranked list of phrases for a given document; rather than the usual output of a search engine list of documents for a given phrase. I've used MYSQL fulltext indexing before and looked into lucene but never used it. They both seem geared to compare the short (search term) with the long (document). How would you get the inverse of this? Would it be too slow to turn each phrase into a regex and run each one on the document counting the number of occurrences? If that doesn't work maybe you can combine all the phrases into one huge regex (using |) and compile it. Then run that huge regex starting from every character in the document. Count the number of matches as you go through the characters. I can trade time to build an index so that looking up a list of phrases (for a given document) is as fast as possible.  How large is the database of phrases? I am assuming that it is very large. I would do the following: Index the phrases by one of the words in it. You might choose the least common word in each phrase.You might make the search better by assuming that the word is at least e.g. 5 characters long and padding the word to 5 chars if it is shorter. The padding can be the space after the word followed by the subsequent word to reduce matches or some default character (e.g. ""XX"") if the word occurs at the end of the phrase. Go through your document converting each word (common ones can be discarded) to a key by padding if necessary retrieving phrases. Retrieve the relevant phrases by these keywords. Use an in-memory text search to find the number of occurrences of each of the retrieved phrases. I am assuming that phrases cannot cross a sentence boundary. In this case you can read each sentence of the document into a substring in an array and use the substring function to search through each sentence for each of the phrases and count occurrences keeping a running sum for each phrase.  I did something similar with a database of Wikipedia titles and managed to get down to a few hundred milliseconds for each ~50KB document. That was still not fast enough for my needs but maybe it can work for you. Basically the idea was to work with hashes as much as possible and only do string comparisons on possible matches which are pretty rare. First you take your database and convert it into an array of hashes. If you have billions of phrases this may not be for you. When you calculate the hash be sure to pass the phrases through a tokenizer that will remove punctuation and whitespace. This part needs to be done only once. Then you go though the document with the same tokenizer keeping a running list of the last 12..n tokens hashed. At every iteration you do a binary search of the hashes you have against the hashes database. When you find a match you do the actual string comparison to see if you found a match. Here's some code to give you a taste of whet I mean tough this example doesn't actually do the string comparison:  HashSet<Long> foundHashes = new HashSet<Long>(); LinkedList<String> words = new LinkedList<String>(); for(int i=0; i<params.maxPhrase; i++) words.addLast(""""); StandardTokenizer st = new StandardTokenizer(new StringReader(docText)); Token t = new Token(); while(st.next(t) != null) { String token = new String(t.termBuffer() 0 t.termLength()); words.addLast(token); words.removeFirst(); for(int len=params.minPhrase; len<params.maxPhrase; len++) { String term = Utils.join(new ArrayList<String>(words.subList(params.maxPhrase-lenparams.maxPhrase)) "" ""); long hash = Utils.longHash(term); if(params.lexicon.isTermHash(hash)) { foundHashes.add(hash); } } } for(long hash : foundHashes) { if(count.containsKey(hash)) { count.put(hash count.get(hash) + 1); } else { count.put(hash 1); } } A few hundred milliseconds is acceptable. I'll give this approach a go  Maybe reading Peter Turney on keyphrase extraction will give you some ideas. Overall his approach has some similarity to what itsadok has suggested."
315,A,Lucene.NET (strings fuzzy matching) Could anyone give me an example about how to do fuzzy matching of two strings using Lucene.NET (or using Java version of Lucene or in any other language that has port of Lucene). Could you be a bit more specific on what you mean by fuzzy matching? Lucene offers fuzzy queries using the tilde(~) operator and the wildcards (* & ?) See here If you want to compare string distance of 2 strings using methods such as Levenshtein Jaro-Winkler etc. you are better off using a separate library such as SimMetrics. I use Simmetrics in my production site and it works fab. SimMetricsMetricUtilities.Levenstein ls = new SimMetricsMetricUtilities.Levenstein(); //compare string 1 string 2 double sim = ls.GetSimilarity(string_1 string_2); if(sim > [some value]) { //do something } Thank you for answer sir. You are right I want to compare two strings using Levenshtein algorithm could you give me an example how you doing fuzzy match two strings using SimMetrics? Is actually very straight-forward: 1. Add a reference to the Simmetrics dll in your project 2. In the method where you want to run the similarity SimMetricsMetricUtilities.Levenstein ls = new SimMetricsMetricUtilities.Levenstein(); //compare string 1 string 2 double sim = js.GetSimilarity(string_1 string_2); if(sim > [some value]) { //do something } the only problem i see with that is that you do the matching AFTER getting results from lucene.
316,A,"Handling different non-accented versions of Umlaut characters The German accented Umlaut characters “ö” “ä” and “ü” are often replaced with non-accented versions when users type often for convenience when they do not have the correct keyboard. With most accented characters there is a particular non-accented version that most people use. The accented “è” for instance is always replaced with a standard “e”. With the Umlaut characters there appears to be a difference between the convention adopted by our British and our American users. British users will replace them with “o” “a” and “u” respectively where as... American users will replace them with “oe” “ae” and “ue” respectively. Our search is built on Lucene.Net and like with any search framework the technique used to match all combinations of accented characters is to replace them both when the index is created and when the search criteria is supplied therefore allowing the matching to be done with purely non-accented characters. How would I parse the accented characters in order to support the following... A German customer types – “Götz” A British customer types – “Gotz” An American customer types “Goetz” Given that the name is in our database in its correct form of “Götz” then how would I parse “Götz” so that all three of the users can find it in the index? EDIT I found this article on CodeProject that was exactly what I was looking for. The example shows how Synonyms for words can also be added to the Lucene index so that they are matched as well as the original word. With a small adaptation I was able to do exactly what I wanted. I found this article on CodeProject that was exactly what I was looking for. The example shows how Synonyms for words can also be added to the Lucene index so that they are matched as well as the original word. With a small adaptation I was able to do exactly what I wanted. Added this from my question Edit so that there is an accepted answer to this question.  Convert ""Götz"" to both ""Gotz"" and ""Goetz"" at index time. You can use setPositionIncrement(0) on the second term to make phrase searches work correctly. Thanks a lot for the suggestion. It put me on the right track which enabled me to find a nice article on CodeProject that was exactly what I needed. See above"
317,A,"Lucene.Net fails at my host because it calls GetTempPath(). What's the work around? I'm using Lucene.Net in an ASP.NET application on a shared host. Got this stack trace shown below. What's the work around? [SecurityException: Request for the permission of type 'System.Security.Permissions.EnvironmentPermission mscorlib Version=2.0.0.0 Culture=neutral PublicKeyToken=b77a5c561934e089' failed.] System.Security.CodeAccessSecurityEngine.Check(Object demand StackCrawlMark& stackMark Boolean isPermSet) +0 System.Security.CodeAccessPermission.Demand() +59 System.IO.Path.GetTempPath() +54 Lucene.Net.Store.FSDirectory..cctor() +73 Here's the answer to my own question. The solution was to modify Lucene.Net.Store.FSDirectory by commenting out this unused line: // Comments out by Corey Trager Oct 2008 to workaround permission restrictions at shared host. This is not used. // public static readonly System.String LOCK_DIR = SupportClass.AppSettings.Get(""Lucene.Net.lockDir"" System.IO.Path.GetTempPath()); There was one more security permission hurdle after that and here's that workaround too. I don't understand why one way of getting the names of files in a directory would be blocked and another way not blocked.  public override System.String[] List() { /* Changes by Corey Trager Oct 2008 to workaround permission restrictions at shared host */ System.IO.DirectoryInfo dir = new System.IO.DirectoryInfo(directory.FullName); System.IO.FileInfo[] files = dir.GetFiles(); string[] list = new string[files.Length]; for (int i = 0; i < files.Length; i++) { list[i] = files[i].Name; } return list; /* end of changes */ // System.String[] files = SupportClass.FileSupport.GetLuceneIndexFiles(directory.FullName IndexFileNameFilter.GetFilter()); // for (int i = 0; i < files.Length; i++) // { // System.IO.FileInfo fi = new System.IO.FileInfo(files[i]); // files[i] = fi.Name; // } // return files; }"
318,A,"Combining hits from multiple documents into a single hit in Lucene I am trying to get a particular search to work and it is proving problematic. The actual source data is quite complex but can be summarised by the following example: I have articles that are indexed so that they can be searched. Each article also has multiple properties associated with it which are also indexed and searchable. When users search they can get hits in either the main article or the associated properties. Regardless of where a hit is achieved the article is returned as a search hit (ie. the properties are never a hit in their own right). Now for the complexity: Each property has security on it which means that for any given user they may or may not be able to see the property. If a user cannot see a property they obviously do not get a search hit in it. This security check is proprietary and cannot be done using the typical mechanism of storing a role in the index alongside the other fields in the document. I currently have an index that contains the articles and properties indexed separately (ie. an article is indexed as a document and each property has its own document). When a search happens a hit in article A or a hit in any of the properties of article A should be classed as hit for article A alone with the scores combined. To achieve this originally Lucene v1.3 was modified to allow this to happen by changing BooleanQuery to have a custom Scorer that could apply the logic of the security check and the combination of two hits in different documents being classed as a hit in a single document. I am trying to upgrade this version to the latest (v2.3.2 - I am using Lucene.Net) but ideally without having to modify Lucene in any way. An additional problem occurs if I do an AND search. If an article contains the word foo and one of its properties contains the word bar then searching for ""foo AND bar"" will return the article as a hit. My current code deals with this inside the custom Scorer. Any ideas how/if this can be done? I am thinking along the lines of using a custom HitCollector and passing that into the search but when doing the boolean search ""foo AND bar"" execution never reaches my HitCollector as the ConjunctionScorer filters out all of the results from the sub-queries before getting there. EDIT: Whether or not a user can see a property is not based on the property itself but on the value of the property. I cannot therefore put the extra security conditions into the query upfront as I don't know the value to filter by. As an example: +---------+------------+------------+ | Article | Property 1 | Property 2 | +---------+------------+------------+ | A | X | J | | B | Y | K | | C | Z | L | +---------+------------+------------+ If a user can see everything then searching for ""B and Y"" will return a single search result for article B. If another user cannot see a property if its value contains Y then searching for ""B and Y"" will return no hits. I have no way of knowing what values a user can and cannot see upfront. They only way to tell is to perform the security check (currently done at the time of filtering a hit from a field in the document) which I obviously cannot do for every possible data value for each user. Review just another way This security check is proprietary and cannot be done using the typical mechanism of storing a role in the index alongside the other fields in the document. What about checking the permission of property on query building stage? So if property explicitly hidden from user avoid including it to result tree The security check is not based on the property but on the value of the property. A user might be able to see (the value of) property A for article A but not see (the value of) property A for article B. I see. Ok what about methods: Searcher.Search(Query query HitCollector results) Or Search(Query query Filter filter HitCollector results) You pass hit collector explicitly so you solve problem with AND query. (I'm looking on Lucene.Net.2.3.1) I have already tried that but the HitCollector never gets called as it is deemed that there are no results that contain both terms from the query meaning there are no hits to collect. A-ha! then you need Lucene.Net.Search.MultiSearcher - accepts multiple searchers (your case: articles & properties). After it hitcollector will accept all calls How does that deal with the AND search? I've tried a quick demo with two separate indices (one for articles and one for properties) and searching for ""b AND y"" still returns no results as each index does indeed contain no results for the AND query. Hmmm you have talking about absence (at lucene level) relation between article and property. May be you need not AND but another set operation (multiplication  'OR')  Having now implemented this (after a lot of head-scratching and stepping through Lucene searches) I thought I'd post back on how I achieved it. Because I am interested in all of the results (ie. not a page at a time) I can avoid using the Hits object (which has been deprecated in later versions of Lucene anyway). This means I can do my own hit collection using the Search(Weight Filter HitCollector) method of IndexSearcher iterating over all possible results and combining document hits as appropriate. To do this I had to hook into Lucene's querying mechanism but only when AND and NOT clauses are present. This is achieved by: Creating a custom QueryParser and overriding GetBooleanQuery(ArrayList bool) to return my own implementation. Creating a custom BooleanQuery (returned from the custom QueryParser) and overriding CreateWeight(Searcher) to return my own implementation. Creating a custom Weight (returned from the custom BooleanQuery) and overriding Scorer(IndexReader) to return my own implementation. Creating a custom BooleanScorer2 (returned from the custom Weight) and overriding the Score(HitCollector) method. This is what deals with the custom logic. This might seem like a lot of classes but most of them derive from a Lucene class and just override a single method. The implementation of the Score(HitCollector) method in the custom BooleanScorer2 class now has the responsibility of doing the custom logic. If there are no required sub-scorers the scoring can be passed to the base Score method and run as normal. If there are required sub-scorers it means there was a NOT or an AND clause in the query. In this case the special combination logic mentioned in the question comes into play. I have a class called ConjunctionScorer that does this (this is not related to the ConjunctionScorer in Lucene). The ConjunctionScorer takes a list of scorers and iterates over them. For each one I extract the hits and their scores (using the Doc() and Score() methods) and create my own search hits collection containing only those hits that the current user can see after performing the relevant security checks. If a hit has already been found by another scorer I combine them together (using the mean of their scores for their new score). If a hit is from a prohibited scorer I remove the hit if it was already found. At the end of all of this I set the hits onto the HitCollector passed into the BooleanScorer2.Score(HitCollector) method. This is a custom HitCollector that I passed into the IndexSearcher.Search(Query HitCollector) method to originally perform the search. When this method returns my custom HitCollector now contains my search results combined together as I wanted. Hopefully this information will be useful to someone else faced with the same problem. It sounds like a lot of effort but it is actually pretty trivial. Most of the work is done in combining the hits together in the ConjunctionScorer. Note that this is for Lucene v2.3.2 and may be different in later versions."
319,A,"Can documents indexed with Solr on JDK6 be retrieved using only lucene api on JDK1.4? My runtime environment is still on JDK1.4 but I like the Solr features related to how documents are ingested and indexed. Would I be able to index my documents using Solr offline on a recent version of the JDK copy the index over and use it in my runtime environment with an older version of the JDK? Version wise Solr 1.4.0 uses Apache Lucene 2.9.1 which is JDK1.4 compatible. (but Solr itself requires JDK5). Assuming what I'm trying to do is even possible what features would I lose if I search Solr indices only with the Lucene API? Yes the Solr index is a standard Lucene index which you can open with ""raw"" Lucene however you would lose all the features that Solr does for you like faceting caching highlighting etc."
320,A,"Get field names from a lucene query string If I have a Lucene query string ""field1:value1 myField:aValue"" Is there a way to let Lucene parse this so I can get term queries? I ultimately want to be able to get the field names and their values back to my viewdata so I can fill them in my textboxes on post back. lucene's QueryParser will convert that string into a BooleanQuery containing two TermQuery clauses. You'll need to use the getClauses method of BooleanQuery to get the term queries."
321,A,"Lucene.NET MatchAllDocsQuery doesn't honor document boost? I have a Lucene index of document all nearly identical (test 1 test 2 etc.) except that some have a higher boost than others. When using a default query (MatchAllDocsQuery OR .Parse("":"") on the query parser) the documents come back in the order they went in every time. By adding a search term (""test"" in this case) the document boost is apparent and the documents are sorted according to the boost. I can change the boost levels around and the new order is reflected in the results. All my code is pretty standard fair I'm using a default Sort() is both cases. I found that this same bug was reported and fixed in Lucene back in 2005-2006 and I checked my MatchAllDocsQuery.cs file (Lucene .NET 2.9.2) and it seems to have this change present but the behavior is as described in the ticket above. Any ideas what I might be doing wrong? Perhaps someone running the Java version has experienced this (or not)? Thanks. Uh don't I feel silly now. This is as-designed behavior. I guess. According to Lucene in Action MatchAllDocsQuery uses a constant for the boost."
322,A,"Hibernate Search Filter not having expected results with Enum I'm using hibernate search 3.4 and I'm running in to a small problem. I have a filter I'm attempting to use (CourseStatusFilterFactory) but every time I enable it no results are returned. I have another filter that works without issues (DeletedFilterFactory) so I'm not sure what the problem is. Here is the object I am trying to search: @Entity @Cache(usage = CacheConcurrencyStrategy.READ_WRITE) @Indexed @FullTextFilterDefs({ @FullTextFilterDef(name = ""statusFilter"" impl = CourseStatusFilterFactory.class cache = FilterCacheModeType.NONE) @FullTextFilterDef(name = ""deletedCourse"" impl = DeletedFilterFactory.class cache = FilterCacheModeType.NONE)}) public class Course extends LightEntity implements Serializable { private static final long serialVersionUID = 21L; @Id @DocumentId @GeneratedValue(strategy = GenerationType.AUTO) private Long id; @Field(name = ""title"" index = Index.TOKENIZED store = Store.YES) private String title; @Field(name = ""coursestatus"" index = Index.TOKENIZED store = Store.YES) @Enumerated(EnumType.STRING) private CourseStatus status;} Any my FilterFactory: public class CourseStatusFilterFactory { private CourseStatus status; public void setStatus(CourseStatus status) { this.status = status; } @Key public FilterKey getKey() { StandardFilterKey key = new StandardFilterKey(); key.addParameter(status); return key; } @Factory public Filter getFilter() { String statusString = new EnumBridge().objectToString(this.status); Query query = new TermQuery(new Term(""coursestatus"" statusString)); CachingWrapperFilter cachingWrapperFilter = new CachingWrapperFilter(new QueryWrapperFilter(query)); return cachingWrapperFilter; }} and to enable my filter: persistenceQuery.enableFullTextFilter(""statusFilter"").setParameter(""status"" CourseStatus.PUBLISHED); When debugging the code I can see that my query in the filter does get set to ""coursestatus:PUBLISHED"" but I still have 0 results even though there should be dozens. Any ideas of where to start? Thanks to the help of some people in the hibernate forum I was able to fix the problem. I needed to change @Field(name = ""coursestatus"" index = Index.TOKENIZED store = Store.YES) to @Field(name = ""coursestatus"" index = Index.UN_TOKENIZED store = Store.YES)"
323,A,"Is there a set of best practices for building a Lucene index from a relational DB? I'm looking into using Lucene and/or Solr to provide search in an RDBMS-powered web application. Unfortunately for me all the documentation I've skimmed deals with how to get the data out of the index; I'm more concerned with how to build a useful index. Are there any ""best practices"" for doing this? Will multiple applications be writing to the database? If so it's a bit tricky; you have to have some mechanism to identify new records to feed to the Lucene indexer. Another point to consider is do you want one index that covers all of your tables or one index per table. In general I recommend one index with a field in that index to indicate which table the record came from. Hibernate has support for full text search if you want to search persistent objects rather than unstructured documents. There's an OpenSymphony project called Compass of which you should be aware. I have stayed away from it myself primarily because it seems to be way more complicated than search needs to be. Also as I can tell from the documentation (I confess I haven't found the time necessary to read it all) it stores Lucene segments as blobs in the database. If you're familiar with the Lucene architecture Compass implements a Lucene Directory on top of the database. I think this is the wrong approach. I would leverage the database's built-in support for indexing and implement a Lucene IndexReader instead. The same criticism applies to distributed cache implementations etc.  As introduction: Brian McCallister wrote a nice blog post: Using Lucene with OJB.  We are rolling out our first application that uses Solr tonight. With Solr 1.3 they've included the DataImportHandler that allows you to specify your database tables (they call them entities) along with their relationships. Once defined a simple HTTP request will tirgger an import of your data. Take a look at the Solr wiki page for DataImportHandler for details.  I haven't explored this at all but take a look at LuSql. Using Solr would be straightforward as well but there'll be some DRY-violations with the Solr schema.xml and your actual database schema. (FYI Solr does support wildcards though.)"
324,A,What is the actual meaning of 'Rank' in Lucene when view by Luke? I am using Luke to view a Lucene index. There is a column named 'Rank'. What is the actual meaning of it? My guess is that the Rank means number of occurrence and the larger Rank number meaning the term is more significant. But I don't understand is that it is a full text search. If I search for 'apple' all the 'apple' index will be returned that doesn't matter with what Rank 'apple' has. Am I having a wrong understanding? If not what is the actual use for the Rank column? When I inspect the index it seems there are quite some 'noise' there e.g. the character 'o' has a very high Rank number. Does it mean this index is bad? How should I fix it? Thanks in advance. 'Rank' is the frequency of a term within a field. It does not mean it is more significant. In fact the least frequent terms are often the most significant of an index. But knowing the most frequent terms of your index is sometimes important for analysis or debug purpose (see this question for example). The fact that you have a lot of terms like 'o' does not mean your index is bad. Check the tokenizer and analyzer used for indexing. Some tokenizer strips words on punctuation mark. Some analyzers will stem words and often it will yield single letter terms. There are a lot of reasons that can explain the presence of single letter terms. If you see a lot of undesirable terms in your index you might consider using a stop words filter at index time. Lucene provides functionalities for this.
325,A,"Good opensource Java Shopping cart frameworks that can be extended to use Lucene and PDFbox Hi I'm looking for a Java open source shopping cart framework. I have had a look at KonaKart and OfBiz but I'm looking for other examples for comparison. I will need to have the search on the cart use Lucene to search through the PDFs so that keywords from the documents can return results. So I will need to be able to 'hook"" into the search of the framework to modify it. Thanks Please check out SoftSlate Commerce. It uses Lucene and is pretty easy to extend to suit whatever needs you have. Full source code comes with the $495 Standard Edition license.  Nice discussion. An example of custom made shopping cart can be found at http://www.tech-freaks.in/Java-Programming/JSP-Servlets/shopping-cart.html  you can also use Jadasite. 100% java based. It has real time currency conversion and all. Also it lets you choose gateways depending on currency.  Shopizer www.shopizer.com it also does online invoicing While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.  Plenty of good suggestions in this SO post. E-Commerce solutions (perhaps too heavy for your purposes) include JadaSite and OpenEdit. That is where I had found out about KonaKart and OfBiz but I am interested in other options too and prefer to be in Java There are some FOSS e-commerce suites in Java; see updated answer above.  There is one CMS I know please if it can help you.  I decided on Konakart as it contained most of the functionality required and also the company were open to developing the search functionality required!"
326,A,"Not able to search by certain columns when configuring mysql with lucene My database has three columns which I configured lucene to index. However I can search by only one of them. Complete description is the following: I am following these instructions to configure solr to use mysql data: http://digitalpbk.com/apachesolr/apache-solr-mysql-sample-data-config I downloaded the jdbc driver put it in /example/lib and created a new requestHandler in /example/conf/solrconfig.xml. My database table items has three columns: id: int primary autoincrement key name: varchar(256) description: varchar(511) So I create following data-config.xml: <dataConfig> <dataSource type=""JdbcDataSource"" driver=""com.mysql.jdbc.Driver"" url=""jdbc:mysql://SERVER/DATABASE"" user=""USERNAME"" password=""PASSWORD""/> <document name=""content""> <entity name=""node"" query=""select id name description from items""> <field column=""id"" name=""id"" /> <field column=""name"" name=""name"" /> <field column=""description"" name=""description"" /> </entity> </document> </dataConfig> Next I edit schema.xml in /example/solr/conf to let it know about new names: <field name=""id"" type=""string"" indexed=""true"" stored=""true""> <field name=""name"" type=""string"" indexed=""true"" stored=""true""> <field name=""description"" type=""string"" indexed=""true"" stored=""true""> I had to uncomment the descriptions of id and name which were present in this file earlier since they clashed with my descriptions. Next I imported the database (around 100K rows) successfully. At the end of all this I can successfully search by name but I am unable to search by description or by id. I do not understand why this should be the case. Any help or pointers would be appreciated. You are spot on hkn! I put `description:query` and solr searched in description. Now I need to figure out how to ask solr to search in all the fields. Name field may be the default field name to search. So make sure you are referring the field names in your query like `description:queryString`. Name field may be the default field name to search. So make sure you are referring the field names in your query. To search in description field use query: description:queryString To search in all fields use query: id:queryString OR name:queryString OR description:queryString For more information please check http://wiki.apache.org/solr/SolrRelevancyFAQ"
327,A,"Lucene QueryParser in multiple threads: synchronize or construct new each time? I have a web application where users submit queries to a Lucene index. The queries are parsed by a Lucene QueryParser. I learned the hard way that QueryParser is not thread-safe. Is it better to use a single QueryParser instance and synchronize on calls to its parse() method? Or is it better to construct a new instance for each query? (Or would I be better served by a pool of QueryParsers?) I know that in general questions like this depend on the particulars and require profiling but maybe someone out there can say definitively ""QueryParsers are extremely inexpensive/expensive to construct""? Create a new one each time. These are lightweight objects and the JVM handles object creation and garbage collection very well. Definitely do not use an object pool. +1 yes definitely no reason to pool (or share a single) `QueryParser` object. Further reading: [Java theory and practice: Urban performance legends revisited](http://www.ibm.com/developerworks/java/library/j-jtp09275/index.html) Thanks! @WhiteFang34 that link is about the low cost of allocation in general but it's surely worth pointing that there **are** some types of objects where construction is not cheap because a lot of work is done beyond the allocation itself. In the Lucene world one is often reminded to share a single `IndexSearcher` for performance reasons."
328,A,"Best way to keep index real time? I have a Solr/Lucene index file of approximately 700 Gb. The documents that I need to index are being read in real-time roughly 1000 docs every 30 minutes are submitted and need to be indexed. In my scenario a script is run every 30 mins that indexes the documents that are not yet indexed since it is a requirement that new documents should be searchable as soon as possible but this process slow down the searching. Is this the best way i can index latest documents or there is some other better way! Check http://code.google.com/p/zoie/ wrapper around Lucene to make it real time - code donated from Linkedin.  First remember that Solr is not a real-time search engine (yet). There is still work to be done. You can use a master/slave setup where the indexation are done on the master and the search on the slave. With this indexation does not affect search performance. After the commit is done on the master force the slave to fetch the latest index from the master. While the new index is being replicated on the slave it is still processing queries with the previous index. Also check you cache warming settings. Remember that this might slow down the searches if those settings are too aggressive. Also check the queries launched on the new searcher event. Update: Solr now has (near) real-time search capabilities. @mt3 link for more info? @Simon Sorry for delayed reply. It's in the trunk of the Solr/Lucene branch. http://wiki.apache.org/solr/NearRealtimeSearch  You can do this with Lucene easily. Split the indexes in multiple parts (or to be precise while building indexes create ""smaller"" parts.) Create searcher for each of the part and store a reference to them. You can create a MultiSearcher on top of these individual parts. Now there will be only one index that will get the new documents. At regular intervals add documents to this index commit and re-open this searcher. After the last index is updated you can create a new multi-searcher again using the previously opened searchers. Thus at any point you will be re-opening only one searcher and that will be quite fast.  ^^i do this with normal lucene non solr and it works really nice. however not sure if there is a solr way to do that at the moment. twitter recently went with lucene for searching and has effectively real time searching by just writing to their index at any update. their index resides completely in memory so updating/reading the index is of no consequence and happens instantly a lucene index can always be read while being written to as long as there is only one writer at a time. ""happens instantly"" -> latency is still around 10seconds  Check out this wiki page Please include more than just a link in your answer. Pull out the relevant info so not everyone has to click through and it still has some value if the link goes dead."
329,A,make lucene into full fledged search engine like google i wanted to build a search engine like google where if i enter a search term it retrieves the urls to websites. i used lucene with tomcat but it searches the files residing in my system. i want to search throughout the web.Please tell me how to do this using lucene? if we can't do this using luceneplease suggest alternatives. Use Nutch.
330,A,"Retrieving per keyword/field match position in Lucene Solr -- possible? Is there any way to retrieve the match field/position for each keyword for each matching document from solr? For example if the document has title ""Retrieving per keyword/field match position in Lucene Solr -- possible?"" and the query is ""solr keyword"" I'd like to get in addition to the doc-id (I normally only want the doc-id not the full document) something that can tell me the matches are at: solr: title: 9 keyword: title: 3 I'm pretty sure such info is computing during query execution (for phrase queries) but is it possible to return these to the application? Thanks! Debugging Relevance Issues in Search suggest using Solr analysis which you can get to from the admin URL using something like http://localhost:8983/solr/admin/analysis.jsp?highlight=on . This highlights matching terms and gives their position. Thanks! I'll try that out.  AFAIK there is no way to do that directly but you can use hit highlighting to implement it."
331,A,hibernate search multiple fields based on language I'm interested in changing db full text search to lucene. I'm using hibernate so I guess it would be smart to use hibernate search. I have a problem though. Our record has a list of informations and titles from different languages and I need to be able to search based on a single language and over all languages. I could probably do it in plain lucene but I don't know how well it would work with current transactions. So using hibernate search and hibernate to deal with the index would be much better. Is it possible to create such fields in the index to search the way I described? class Record{ List<Info> infos; } class Info{ String title; String infoText; String langCode; } Can I do it like this. Create getters in Record like this: public String getEnghlishTitle(){...} public String getFullInfos(){...} And then put index annotations on these getters and then have necessary fields in index? I would write a custom FieldBridge for the infos property. Then you have full control which fields you add to the index eg you could could use text. as field names. This should allow to dynamically decide which language to search for. Remember you have to think about the analyzers too. A custom per field analyzer would work.
332,A,What are the advantages and disadvantages of using a search engine as a key value store? Given a search engine like Lucene and a set of XML documents which need to be fully preserved what are the advantages and disadvantages of using the search engine as key value store for returning XML doucments given a unique primary key which each document contains? You need ACID (or near ACID) sematics which Lucene doesn't guarantee. To put it simply DB can recover from failure to last consistent state. A crash while writing to Lucene index _might_ render it useless. I can't make out what you mean. Is the idea that you do conventional full-text indexing or that you use some sort of schema mapping to turn data items from XML into many fields in Lucene? I want to be able to fetch (or reconstruct I suppose but preferably fetch) the original document I put in by a simple query for an unambiguous unique key for that given document. Basically I want to treat the search engine as a SQL database with a primary key field and a clob field for each row. Whoever voted to close as not a real question: like hell. My boss specifically wants me to come up with the pros and cons of doing this as an implementation of a KV store for documents (as opposed to using the filesystem or something like couch db) Perhaps you might edit the question to include the material in the your comment? If all you are going to do is test for key equality and retrieve a blob Lucene has no visible advantage over say bdb. And you have no transactions until you layer something else on top. And concurrency has certain complexities to it. And the API is well a bit baroque for the simple thing you are doing. I've implemented something like what you describe but actual full text search on the data was a critical requirement that justified the rest.  If you use something like Compass and it's XML-to-Lucene mapping engine it's a great solution for storing and querying XML documents without going all the way to a XML database. One downside is that the XML documents can only be retrieved via the Lucene API (the underlying data store is pretty impenetrable) but I can live with that.  Read Search Engine versus DBMS. IMO your application falls in the DBMS realm and will probably be best served by a key-value database such as couchDB. This is because you take no advantage of textual operations such as tokenization stemming etc.
333,A,"Comparison of full text search engine - Lucene Sphinx Postgresql MySQL? I'm building a Django site and I am looking for a search engine. A few candidates: Lucene/Lucene with Compass/Solr Sphinx Postgresql built-in full text search MySQl built-in full text search Selection criteria: result relevance and ranking searching and indexing speed ease of use and ease of integration with Django resource requirements - site will be hosted on a VPS so ideally the search engine wouldn't require a lot of RAM and CPU scalability extra features such as ""did you mean?"" related searches etc Anyone who has had experience with the search engines above or other engines not in the list -- I would love to hear your opinions. EDIT: As for indexing needs as users keep entering data into the site those data would need to be indexed continuously. It doesn't have to be real time but ideally new data would show up in index with no more than 15 - 30 minutes delay 2¢: MySQL fulltext search and transactions are (presently) mutually exclusive. MySQL fulltext indexes require the MyISAM table type which doesn't support transactions. (As opposed to the InnoDB table type which supports transactions but not fulltext indexes.) PostgreSQL full-text search `Tsearch` *does not* support phrase search. However it's on the TODO list http://www.sai.msu.su/~megera/wiki/FTS_Todo. Anyone looking at this for Django should checkout the haystack app. http://haystacksearch.org/ http://www.slideshare.net/billkarwin/practical-full-text-search-with-my-sql @CarlG  Just for everybody's reference. MySQL 5.6+ has Full text search support with innodb engine SearchTools-Avi said ""MySQL text search which doesn't even index words of three letters or fewer."" FYIs The MySQL fulltext min word length is adjustable since at least MySQL 5.0. Google 'mysql fulltext min length' for simple instructions. That said MySQL fulltext has limitations: for one it gets slow to update once you reach a million records or so ...  I don't know Sphinx but as for Lucene vs a database full-text search I think that Lucene performance is unmatched. You should be able to do almost any search in less than 10 ms no matter how many records you have to search provided that you have set up your Lucene index correctly. Here comes the biggest hurdle though: personally I think integrating Lucene in your project is not easy. Sure it is not too hard to set it up so you can do some basic search but if you want to get the most out of it with optimal performance then you definitely need a good book about Lucene. As for CPU & RAM requirements performing a search in Lucene doesn't task your CPU too much though indexing your data is although you don't do that too often (maybe once or twice a day) so that isn't much of a hurdle. It doesn't answer all of your questions but in short if you have a lot of data to search and you want great performance then I think Lucene is definitely the way to go. If you're not going to have that much data to search then you might as well go for a database full-text search. Setting up a MySQL full-text search is definitely easier in my book. Compare to sphinx  lucence is tooo slow and bulky. I had used both in my project and i finally sticked to sphinx. Lucence is in java  and it takes a lot more CPU and RAM than Sphinx. I have to disagree here. Lucene is lightning fast IF you build a correct index. You can basically do an advanced query over millions of records in just a couple of milliseconds. You just need to know what you are doing. And Lucene is in java... your point being? There's also .NET port Lucene.NET btw. Valid questions. I never said that Lucene is faster than Sphinx I mentioned that Lucene vs a database full-text search is unmatched. And it is. No question about that. Lucene is based upon an inverted index. Now I don't know Sphinx as mentioned before but if it also uses an inverted index or a similar indexing method then it is possible that they are equally performing. Stating that Lucene compared to Sphinx would be 'tooo slow and bulky' is not based upon facts. Especially not when it is only said that Lucene is in 'Java' which is just a ridiculous non-issue in terms of performance. but you clearly stated that you don't use sphinx and v3sson has used both. how can you state that lucene's performance is unmatched in the same sentence that you state you haven't used sphinx? Making the assumption that Lucene is slow because its in Java is ridiculous. Java is not slow the hotspot vm is a work of art and in most cases your correctly written app will run faster that a c application. I my experience Lucene is light weight fast albeit a bit fiddly. V3ss0n did not say that in his opinion Lucene is slow because it is Java. He said that it is Java and it ""takes a lot more CPU and RAM than Sphinx"".  I'm looking at PostgreSQL full-text search right now and it has all the right features of a modern search engine really good extended character and multilingual support nice tight integration with text fields in the database. But it doesn't have user-friendly search operators like + or AND (uses & | !) and I'm not thrilled with how it works on their documentation site. While it has bolding of match terms in the results snippets the default algorithm for which match terms is not great. Also if you want to index rtf PDF MS Office you have to find and integrate a file format converter. OTOH it's way better than the MySQL text search which doesn't even index words of three letters or fewer. It's the default for the MediaWiki search and I really think it's no good for end-users: http://www.searchtools.com/analysis/mediawiki-search/ In all cases I've seen Lucene/Solr and Sphinx are really great. They're solid code and have evolved with significant improvements in usability so the tools are all there to make search that satisfies almost everyone. for SHAILI - SOLR includes the Lucene search code library and has the components to be a nice stand-alone search engine. I believe that by PostgreSQL full-text search you're referring to `Tsearch`. But Tsearch *does not* support phrase search. It's still on their TODO list http://www.sai.msu.su/~megera/wiki/FTS_Todo. Just done a bunch of testing on Postgres 9.0 full text search; was disappointed to find that French text isn't matched if the user forgets to get all the accents right. Matching of word forms is patchy - for example in English ""say"" doesn't match text containing ""said"". Overall fairly impressive though for an integrated feature across the languages tested (en fr ru). @romkyns: you need to install an unaccent dictionary to strip them out. ""OTOH it's way better than the MySQL text search which doesn't even index words of three letters or fewer."" That's not a built-in restriction of MySQL -- it's whatever you set in the config file. If you want to index one-letter words just change one value in the config. For phrase searches see the ""synonym"" dictionary feature. That's an odd name for a feature which allows phrase searches to work well but I had not trouble setting up a dictionary of legal terms that way.  Just my two cents to this very old question. I would highly recommend taking a look at ElasticSearch. Elasticsearch is a search server based on Lucene. It provides a distributed multitenant-capable full-text search engine with a RESTful web interface and schema-free JSON documents. Elasticsearch is developed in Java and is released as open source under the terms of the Apache License. The advantages over other FTS (full text search) Engines are: RESTful interface Better scalability Large community Built by Lucene developers Extensive documentation There are many open source libraries available (including Django) We are using this search engine at our project and very happy with it.  I am surprised that there isn't more information posted about Solr. Solr is quite similar to Sphinx but has more advanced features (AFAIK as I haven't used Sphinx -- only read about it). The answer at the link below details a few things about Sphinx which also applies to Solr. Comparison of full text search engine - Lucene Sphinx Postgresql MySQL? Solr also provides the following additional features: Supports replication Multiple cores (think of these as separate databases with their own configuration and own indexes) Boolean searches Highlighting of keywords (fairly easy to do in application code if you have regex-fu; however why not let a specialized tool do a better job for you) Update index via XML or delimited file Communicate with the search server via HTTP (it can even return Json Native PHP/Ruby/Python) PDF Word document indexing Dynamic fields Facets Aggregate fields Stop words synonyms etc. More Like this... Index directly from the database with custom queries Auto-suggest Cache Autowarming Fast indexing (compare to MySQL full-text search indexing times) -- Lucene uses a binary inverted index format. Boosting (custom rules for increasing relevance of a particular keyword or phrase etc.) Fielded searches (if a search user knows the field he/she wants to search they narrow down their search by typing the field then the value and ONLY that field is searched rather than everything -- much better user experience) BTW there are tons more features; however I've listed just the features that I have actually used in production. BTW out of the box MySQL supports #1 #3 and #11 (limited) on the list above. For the features you are looking for a relational database isn't going to cut it. I'd eliminate those straight away. Also another benefit is that Solr (well Lucene actually) is a document database (e.g. NoSQL) so many of the benefits of any other document database can be realized with Solr. In other words you can use it for more than just search (i.e. Performance). Get creative with it :) Sphinx too about Supports replication Multiple cores Boolean searches Highlighting of keywords Update index via XML -or delimited file- PDF Word document indexing (via xml) Facets Stop words synonyms etc. Index directly from the database with custom queries Auto-suggest Fast indexing Boosting Fielded searches About Dynamic fields Aggregate fields Cache Autowarming I just don't know  I would add mnoGoSearch to the list. Extremely performant and flexible solution which works as Google : indexer fetches data from multiple sites You could use basic criterias or invent Your own hooks to have maximal search quality. Also it could fetch the data directly from the database. The solution is not so known today but it feets maximum needs. You could compile and install it or on standalone server or even on Your principal server it doesn't need so much ressources as Solr as it's written in C and runs perfectly even on small servers. In the beginning You need to compile it Yourself so it requires some knowledge. I made a tiny script for Debian which could help. Any adjustments are welcome. As You are using Django framework You could use or PHP client in the middle or find a solution in Python I saw some articles. And of course mnoGoSearch is open source GNU GPL.  Good to see someone's chimed in about Lucene - because I've no idea about that. Sphinx on the other hand I know quite well so let's see if I can be of some help. Result relevance ranking is the default. You can set up your own sorting should you wish and give specific fields higher weightings. Indexing speed is super-fast because it talks directly to the database. Any slowness will come from complex SQL queries and un-indexed foreign keys and other such problems. I've never noticed any slowness in searching either. I'm a Rails guy so I've no idea how easy it is to implement with Django. There is a Python API that comes with the Sphinx source though. The search service daemon (searchd) is pretty low on memory usage - and you can set limits on how much memory the indexer process uses too. Scalability is where my knowledge is more sketchy - but it's easy enough to copy index files to multiple machines and run several searchd daemons. The general impression I get from others though is that it's pretty damn good under high load so scaling it out across multiple machines isn't something that needs to be dealt with. There's no support for 'did-you-mean' etc - although these can be done with other tools easily enough. Sphinx does stem words though using dictionaries so 'driving' and 'drive' (for example) would be considered the same in searches. Sphinx doesn't allow partial index updates for field data though. The common approach to this is to maintain a delta index with all the recent changes and re-index this after every change (and those new results appear within a second or two). Because of the small amount of data this can take a matter of seconds. You will still need to re-index the main dataset regularly though (although how regularly depends on the volatility of your data - every day? every hour?). The fast indexing speeds keep this all pretty painless though. I've no idea how applicable to your situation this is but Evan Weaver compared a few of the common Rails search options (Sphinx Ferret (a port of Lucene for Ruby) and Solr) running some benchmarks. Could be useful I guess. I've not plumbed the depths of MySQL's full-text search but I know it doesn't compete speed-wise nor feature-wise with Sphinx Lucene or Solr. Sphinx does allow you to update individual attributes of items in current indexes but not remove/update full records. sphinx RT allows you to do partial updates/removals. it is in early stage but already [almost] works. http://sphinxsearch.com/wiki/doku.php?id=rt_tutorial Sphinx 2.0-beta1 have already stable RT Indexes :) [Here is an answer on Solr](http://stackoverflow.com/questions/1284083/choosing-a-stand-alone-full-text-search-server-sphinx-or-solr) that is a good pair to this answer on Sphinx"
334,A,"Hibernate Search Fails with Double Type Greater than 5 Digits I have the following fields on an entity: @Field(index = Index.TOKENIZED store = Store.YES) @Column(name = ""total_credit_amount"" nullable = false) @FieldBridge(impl = RoundedDoubleBridge.class) private Double totalCreditAmount; @Field(index = Index.TOKENIZED store = Store.YES) @Column(name = ""total_debit_amount"" nullable = false) @FieldBridge(impl = RoundedDoubleBridge.class) private Double totalDebitAmount; The Double to String bridge implementation is the following: public class RoundedDoubleBridge implements StringBridge { @Override public String objectToString(Object value) { // Do not index null strings if (value == null) { return null; } if (value instanceof Double) { long price = round((Double) value); return Long.toString(price); } else { throw new IllegalArgumentException( RoundedDoubleBridge.class + "" used on a non double type: "" + value.getClass()); } } private long round(double price) { double rounded = Math.floor(price / 3) * 3; if (rounded != price) { rounded += 3; //we round up } return (long) rounded; } } So the issue here is that the search I am performing retreives results whenever the values on totalDebitAmount or totalCreditAmount are less than 100000 whenever they are greater or equal to 100000 search fails.. Any help appreciated.. The following is the way I am making the search: public List<AbstractRecord> doSearch(String stringToFind) { List<AbstractRecord> result = null; // Test Search for specific values of an Abstract Record // Aim to return the number of retreived results em = emf.createEntityManager(); FullTextEntityManager fullTextEntityManager = org.hibernate.search.jpa.Search.getFullTextEntityManager(em); //em.getTransaction().begin(); String[] fields = // Fields to be reviewed new String[] { .... ""totalCreditAmount"" ""totalDebitAmount"" .... }; //Create a multi-field Lucene query StandardAnalyzer stdAnalyzer = new StandardAnalyzer(Version.LUCENE_30); MultiFieldQueryParser parser = new MultiFieldQueryParser(Version.LUCENE_30 fields stdAnalyzer); org.apache.lucene.search.Query query = null; try { query = parser.parse(stringToFind); } catch (ParseException ex) { Logger.getLogger(SearchFacade.class.getName()).log(Level.SEVERE null ex); } long time1 = System.currentTimeMillis(); // Wrap Lucene query in a javax.persistence.Query javax.persistence.Query persistenceQuery = fullTextEntityManager.createFullTextQuery(query); // Execute search result = (List<AbstractRecord>) persistenceQuery.getResultList(); em.close(); return result; } Edit: Testing with an Integer value of 6 or more digits the search succeds... So I guess my bridge Implementation is the cause of the search failure?? Thanks Edit: Hypotheses from my colleagues suggest that scientific notation applied to the double type is the one causing this problem... Any ideas? fails as in returns zero results? throws an error? Hireturns zero results. I think it is related with the Analyzer not sure though I am a Hibernate Search & Lucene Query Newb. If anyone can light me up I will greatly appreciate it. Well I feel kind of dumb. I found my problem.. It is specifically at the rounding function of the implementation of my FieldBridge The relevant snippet of the bridge is the following: private long round(double price) { double rounded = Math.floor(price / 3) * 3; if (rounded != price) { rounded += 3; //we round up } return (long) rounded; } Notice for price = 100000 the variable rounded = 99999 After checking the if condition since its different from price it will add 3 hence indexing 100002 instead of 100000 therefore If I look up for 100002 I will find the appropriate record.. Lessons learned here: If you are implementing a customized DoubleBridge that pads and/or rounds make sure the rounding function meets all your data value ranges to avoid problems like the one I had.. If you need to show in screen rounded up Doubles in plain notation and not scientific you will need to implement a TwoWayStringBridge that performs the ObjectToString conversion padding and rounding and later on the StringToObject conversion returning a plain notation Double and not on its scientific notation. Hope this post can help anyone with similar issues. Greets"
335,A,"Lucene - querying with long strings I have an index with a field ""Affiliation"" some example values are: ""Stanford University School of Medicine Palo Alto CA USA"" ""Institute of Neurobiology School of Medicine Stanford University Palo Alto CA"" ""School of Medicine Harvard University Boston MA"" ""Brigham & Women's Harvard University School of Medicine Boston MA"" ""Harvard University Cambridge MA"" and so on... (the bottom-line being the affiliations are written in multiple ways with no apparent consistency) I query the index on the affiliation field using say ""School of Medicine Stanford University Palo Alto CA"" (with QueryParser) to find all Stanford related documents I get a lot of false +ves presumably because of the presence of School of Medicine etc. etc. (note: I cannot use Phrase query because of variability in the way affiliation is constructed) I have tried the following: Use a SpanNearQuery by splitting the search phrase with a whitespace (here I get no results!) Tried boosting (using ^) by splitting with the comma and boosting the last parts such as ""Palo Alto CA"" with a much higher boost than the initial phrases. Here I still get lots of false +ves. Any suggestions on how to approach this? If SpanNearQuery the way to go Any ideas on why I get 0 results? Here is how I did it: Added the common terms such as ""University"" ""School"" ""Medicine"" ""Institute"" etc. to stopwords list. Used a booleanquery for each of the terms and setMinimumNumberShouldMatch() to 75% of the query string length. Finally loop through the hits collector and use a string comparison algorithm like Jaro-Winkler Levenstein etc. for a second-level filter. (this is slow but ensures precision). Hope this helps.  Are you using OR search instead of AND? You can set default operator to AND with QueryParser.setDefaultOperator(). Setting default operator to AND should eliminate all the false positives. But you might risk false negatives in case your indexed values is ""Stanford University School of Medicine Palo Alto CA "" and you are searching for ""Stanford University School of Medicine Palo Alto CA USA"" (note the extra term USA in query.) If your queries are not going to have more terms than the indexed value this should resolve your problem. +1 for your response but I approached it differently. Thanks I did try that approach however I cannot be assure that query terms will be lesser than the indexed value. To give you an idea the queries are made from clicking on a link of text. Any idea why spanquery might not be working?"
336,A,"Reverse search in Hibernate Search I'm using Hibernate Search (which uses Lucene) for searching some Data I have indexed in a directory. It works fine but I need to do a reverse search. By reverse search I mean that I have a list of queries stored in my database I need to check which one of these queries match with a Data object each time Data Object is created. I need it to alert the user when a Data Object matches with a Query he has created. So I need to index this single Data Object which has just been created and see which queries of my list has this object as a result. I've seen Lucene MemoryIndex Class to create an index in memory so I can do something like this example for every query in a list (though iterating in a Java list of queries would not be very efficient): //Iterating over my list<Query> MemoryIndex index = new MemoryIndex(); //Add all fields index.addField(""myField"" ""myFieldData"" analyzer); ... QueryParser parser = new QueryParser(""myField"" analyzer); float score = index.search(query); if (score > 0.0f) { System.out.println(""it's a match""); } else { System.out.println(""no match found""); } The problem here is that this Data Class has several Hibernate Search Annotations @Field@IndexedEmbedded... which indicated how fields should be indexed so when I invoke index() method on the FullTextEntityManager instance it uses this information to index the object in the directory. Is there a similar way to index it in memory using this information? Is there a more efficient way of doing this reverse search? Just index the new object (if you use automatic indexing you don't have to do anything besides committing the current transaction) then retrieve the queries you want to run and run all of them in a boolean query combining the stored query with the id of the new object. Something like this: ... BooleanQuery query = new BooleanQuery(); query.add(storedQuery BooleanClause.Occur.MUST); query.add(new TermQuery(ProjectionConstants.ID id) BooleanClause.Occur.MUST); ... If you get a result you know the query matched.  Since MemoryIndex is a completely separate component that doesn't extend or implement Lucene's Directory or IndexReader I don't think there's a way you can plug this into Hibernate Search Annotations. I'm guessing that if you choose to use MemoryIndex you'll need to write your addField() calls which basically mirrors what you're doing in the annotations. How many queries are we talking about here? Depending on how many there are you might be able to get away with just running the queries on the main index that Hibernate maintains ensuring to constrain the search to the document ID you just added. Or for every document that's added create a one-document in-memory index using RAMDirectory and run the queries through that."
337,A,"Lucene with PHP Can I use Lucene with PHP ? I don't want to use Zend. Can I use in native PHP (not framework) ? ""Can I"" is a very wide question. Of course you ""can"". However is it reasonable? Please expand your question to illustrate how you intend to use it. I want to use full text search with Lucene. MySQL too slow for searching over 8 millions record. So I want to try with Lucene I'm using Lucene with PHP doing system calls on Java for example: java ... .SearchFiles -index C:\shop\system\index -high -queries Computer* I have adapted SearchFiles to produce HTML code but the output may as well be dynamic PHP code (for example an array holding the search results). The system is very fast and you don't need more than Java on the server.  I recommend apache SOLR and then use php extension for solr. http://php.net/manual/en/book.solr.php No need for zend framework just native php  When you say you don't want to use Zend I'm assuming you mean you don't want to use the whole Zend Framework. Well you don't have to - the individual Zend components can be used on their own without needing to be part of a Zend framework project.  Yeah you can simply code a java module for indexing and searching purpose using apache lucene library. Then you can merge it with php module with php/java bridge or SOAP. It will be quite good learning experience for you.  I would recommend using Apache SOLR as your Lucene backend and connecting via web service calls from your PHP code. I'd also note that it's easy to pick and choose components of Zend Framework for use in your application without loading the entire framework. You could use Zend_Search_Lucene in your site and forego Zend's MVC database and related components. Thank. I will try with Apache SOLR I'd like to tag on to that a recommendation (personal preference) that you use JSON rather than XML for retrieving your results.  Also worth noting SOLR (http://lucene.apache.org/solr/) has a simple HTTP API and is built on top of Lucene so if it provides what you need then that's an easy answer! I don't have direct experience with Lucene with PHP so I'll defer to the experts on that."
338,A,"Lucene.NET - What is the Version parameter in MultiFieldQueryParser constructor? We're running into a serious bug with the Lucene.NET 2.3 codebase. We're upgrading to Lucene 2.9 in hopes the bug is fixed. Upgrading to the latest version we see that the MultiFieldQueryParser contructor is [Obsolete]: [Obsolete(""Use the ctor with Version param instead."")] public MultiFieldQueryParser(string[] fields Analyzer analyzer) Instead we're to use the constructor that takes a Version parameter: public MultiFieldQueryParser(Version version string[] fields Analyzer analyzer) Problem is I can't find any documentation regarding what the version parameter is what it's supposed to be what I'm supposed to pass in here. Can anyone shine some light on this? The version parameter was added to provide backwards compatibility in a way that can be extended to accommodate future changes. If you don't care about backwards compatibility just use Version.LUCENE_CURRENT. If you really need to know exactly what changed you usually have to go diving into the source code. General Lucene tip: you usually get better documentation looking at the java version. Are you kidding me? Jeez. Actual version number...ok...Version takes 2 params. A string and an int. Where's the documentation for this? Ah there it is: Lucene.Net.Util.Version.LUCENE_29 as documented here: http://lucene.apache.org/java/2_9_1/api/all/org/apache/lucene/util/Version.html Thanks a bunch! @ShashikantKore Good catch. In 3.0.1 Version.LUCENE_CURRENT is deprecated http://lucene.apache.org/java/3_0_1/changes/Changes.html#3.0.1.api_changes So use the actual version number."
339,A,Solr - OverlappingFileLockException when concurrent commits I'm using solr version 1.4.0 with tomcat 6. I've 2 solr instances running as 2 different web apps with separate data folders. My application requires frequent commits from multiple clients. I've noticed that when more than one client try to commit at the same time these OverlappingFileLockException start to appear. Can anything be done to rectify this problem? Please find the error log below. Thanks HTTP Status 500 - null java.nio.channels.OverlappingFileLockException at sun.nio.ch.FileChannelImpl$SharedFileLockTable.checkList(FileChannelImpl.java:1215) at sun.nio.ch.FileChannelImpl$SharedFileLockTable.add(FileChannelImpl.java:1117) at sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:923) at java.nio.channels.FileChannel.tryLock(FileChannel.java:978) at org.apache.lucene.store.NativeFSLock.obtain(NativeFSLockFactory.java:233) at org.apache.lucene.store.Lock.obtain(Lock.java:73) at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1550) at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1407) at org.apache.solr.update.SolrIndexWriter.<init>(SolrIndexWriter.java:190) at org.apache.solr.update.UpdateHandler.createMainIndexWriter(UpdateHandler.java:98) at org.apache.solr.update.DirectUpdateHandler2.openWriter(DirectUpdateHandler2.java:173) at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:220) at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61) at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:139) at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69) at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1317) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298) at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859) at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588) at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489) at java.lang.Thread.run(Thread.java:636) type Status report message null java.nio.channels.OverlappingFileLockException at sun.nio.ch.FileChannelImpl$SharedFileLockTable.checkList(FileChannelImpl.java:1215) at sun.nio.ch.FileChannelImpl$SharedFileLockTable.add(FileChannelImpl.java:1117) at sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:923) at java.nio.channels.FileChannel.tryLock(FileChannel.java:978) at org.apache.lucene.store.NativeFSLock.obtain(NativeFSLockFactory.java:233) at org.apache.lucene.store.Lock.obtain(Lock.java:73) at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1550) at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1407) at org.apache.solr.update.SolrIndexWriter.<init>(SolrIndexWriter.java:190) at org.apache.solr.update.UpdateHandler.createMainIndexWriter(UpdateHandler.java:98) at org.apache.solr.update.DirectUpdateHandler2.openWriter(DirectUpdateHandler2.java:173) at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:220) at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61) at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:139) at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69) at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1317) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298) at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859) at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588) at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489) at java.lang.Thread.run(Thread.java:636) description The server encountered an internal error (null java.nio.channels.OverlappingFileLockException at sun.nio.ch.FileChannelImpl$SharedFileLockTable.checkList(FileChannelImpl.java:1215) at sun.nio.ch.FileChannelImpl$SharedFileLockTable.add(FileChannelImpl.java:1117) at sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:923) at java.nio.channels.FileChannel.tryLock(FileChannel.java:978) at org.apache.lucene.store.NativeFSLock.obtain(NativeFSLockFactory.java:233) at org.apache.lucene.store.Lock.obtain(Lock.java:73) at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1550) at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1407) at org.apache.solr.update.SolrIndexWriter.<init>(SolrIndexWriter.java:190) at org.apache.solr.update.UpdateHandler.createMainIndexWriter(UpdateHandler.java:98) at org.apache.solr.update.DirectUpdateHandler2.openWriter(DirectUpdateHandler2.java:173) at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:220) at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61) at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:139) at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69) at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1317) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298) at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859) at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588) at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489) at java.lang.Thread.run(Thread.java:636) ) that prevented it from fulfilling this request. The problem is that you're doing multiple concurrent commits. Don't do that. Instead switch to autoCommit (either by configuration or by using add with commitWithin parameter). That way Solr will figure out the actual commits for you. thanks ! it works well now
340,A,"Lucene crashes with Highlighting I have a Ubuntu setup with Luciene Solr and a Rails application. I use Solr for indexing and searching data from the Rails application. This works fine except sometimes. I get this execption:  Net::HTTPFatalError (500 ""org/apache/lucene/index/memory/MemoryIndex java.lang.NoClassDefFoundError: org/apache/lucene/index/memory/MemoryIndex org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getReaderForField(WeightedSpanTermExtractor.java:361) org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extractWeightedSpanTerms(WeightedSpanTermExtractor.java:282) org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:149) org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:414) org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:216) org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:184) org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:226) org.apache.solr.highlight.DefaultSolrHighlighter.doHighlighting(DefaultSolrHighlighter.java:335) org.apache.solr.handler.component.HighlightComponent.process(HighlightComponent.java:89) org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195) org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) org.apache.solr.core.SolrCore.execute(SolrCore.java:1317) org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338) org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241) org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157) org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388) org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765) org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418) org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230) org.mor""): /usr/local/ruby/1.9.2-rc2/lib/ruby/1.9.1/net/http.rb:2295:in `error!' I search for this and found that I need the lucene-memory.jar but this is set up on my server:  $ locate lucene-memory /usr/share/java/lucene-memory-2.9.2.jar /usr/share/java/lucene-memory.jar I don't know why?!? Try to put that jar to: solr\VERSION\lib\ where VERSION might be 1.4.1 or whatever you have. The reason for that is to load that jar in a explicit way from solr. Great! A ""cd /usr/share/solr/WEB-INF/lib && ln -s ../../../java/lucene-memory.jar ."" fixed that problem! Thanks! Terrific! I am happy for you."
341,A,"Lucene.NET: Camel case tokenizer? I've started playing with Lucene.NET today and I wrote a simple test method to do indexing and searching on source code files. The problem is that the standard analyzers/tokenizers treat the whole camel case source code identifier name as a single token. I'm looking for a way to treat camel case identifiers like MaxWidth into three tokens: maxwidth max and width. I've looked for such a tokenizer but I couldn't find it. Before writing my own: is there something in this direction? Or is there a better approach than writing a tokenizer from scratch? UPDATE: in the end I decided to get my hands dirty and I wrote a CamelCaseTokenFilter myself. I'll write a post about it on my blog and I'll update the question. Here is my implementation : package corp.sap.research.indexing; import java.io.IOException; import org.apache.lucene.analysis.TokenFilter; import org.apache.lucene.analysis.TokenStream; import org.apache.lucene.analysis.tokenattributes.CharTermAttribute; public class CamelCaseFilter extends TokenFilter { private final CharTermAttribute _termAtt; protected CamelCaseScoreFilter(TokenStream input) { super(input); this._termAtt = addAttribute(CharTermAttribute.class); } @Override public boolean incrementToken() throws IOException { if (!input.incrementToken()) return false; CharTermAttribute a = this.getAttribute(CharTermAttribute.class); String spliettedString = splitCamelCase(a.toString()); _termAtt.setEmpty(); _termAtt.append(spliettedString); return true; } static String splitCamelCase(String s) { return s.replaceAll( String.format(""%s|%s|%s"" ""(?<=[A-Z])(?=[A-Z][a-z])"" ""(?<=[^A-Z])(?=[A-Z])"" ""(?<=[A-Za-z])(?=[^A-Za-z])"" ) "" "" ); } } Adir this seems to work great. Here's my implementation of the core of it in python: `re.sub('((?<=[A-Z])(?=[A-Z][a-z])|(?<=[^A-Z])(?=[A-Z])|(?<=[A-Za-z])(?=[^A-Za-z]))' ' ' ""CamelCaseAWordFORMe"")`  Solr has a WordDelimiterFactory which generates a tokenizer similar to what you need. Maybe you can translate the source code into C#. Yes I've noticed it although it doesn't really do what I'm looking for. In the end I wrote CamelCaseTokenFilter myself. But I'll accept your answer.  Below link might be helpful to write custom tokenizer... http://karticles.com/NoSql/lucene_custom_tokenizer.html"
342,A,"How to calculate the frequency for a special term in a document field? I just wonder how Lucene can make itand from the source code I know that it opens and loads the segment files when intializing a searcher with a IndexReaderbut Is there any kind person tell me how Lucene calculates the term frequency in a document with special field. Is there any special algorithm? I can not figure it out when reading the explan code on tf like: Explanation tfExplanation = new Explanation(); int d = scorer.advance(doc); float phraseFreq = (d == doc) ? scorer.currentFreq() : 0.0f; tfExplanation.setValue(similarity.tf(phraseFreq)); tfExplanation.setDescription(""tf(phraseFreq="" + phraseFreq + "")""); the Idf>0but why phraseFreq in the code is 0.0and I know it is because (d == doc) is falsebecause the d=Integer.MAX_VALUEI don't know why and what is the problem. ps:I have only one document with one fieldwhich is indexed and storedand the doc which is used in the debug code is 1like searcher.explan(booleanQuery1); Sincerely hope someone could give me some advice! Best regards! I finally found that it is all because of the useage of method explain in lucene.explain only works fine with the search resultbut I used it in the way with wrong input variable (queryint)and the int isn't a doc number. You can vote yourself as the right answer and pick up some points. Good Luck! thanks for your advice shellter"
343,A,"Lucene query praser only read a certain fields query behavior changed in 2.9.3 I need my query parser to only read fields that are ""text"". for example lets say my query is: text:""this fox"" OR title:""brown dog"" for highlighting purposes i need the parser/searcher to only search using the text:""this fox"" part. in 2.4 this worked fine but since upgrading to 2.9.3 something has changed. example code: IndexSearcher is = new IndexSearcher(fsDir); QueryParser qp = new QueryParser(""text"" new StandardAnalyzer(nostop)); qp.setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE); Query queryDiv; try { query = is.rewrite(queryParser.parse(query_str)); } catch (ParseException e) { pw.print(""error: Incorrect query format""); pw.close(); return; } Hits hits = is.search(queryDiv sort); QueryScorer scorer = new QueryScorer(query ""text""); for some reason unknown to me lucene 2.9.3 is now showing no results when in 2.4 it did as it ignored the fields the document didnt have not to mention that there is an OR condition in there. any ideas? please disregard this. there was a flaw in my logic that i was applying to the back end. thanks for looking"
344,A,"Lucene search with complex query Here's what I want to do using pseudo-code: lucene.Find((someField == ""bar"" || someField == ""baz"") && anotherField == ""foo""); Or in English ""find all documents where someField is 'bar' or 'baz' and where anotherField is 'foo'"". How can I do a query like this with Lucene? In Lucene query syntax: +(someField:bar someField:baz) +anotherField:foo The ""+"" means that the term is required just like Google search syntax. The parentheses group terms to act like a single term. Without a ""+"" (or ""-"") a term is optional; at least one of the terms has to match and the more terms that match the higher the score. Pass this string to the QueryParser to create a Query object. The query can then be passed to one of several search methods depending on your needs. Thanks! I'll give that a try. In the meantime I'll mark this as the correct answer. I'll let you know if it works as expected. Seems to work great. Thanks again."
345,A,Restrict the fields that Lucene returns Can I restrict the fields that Lucene.net (v2) returns when running a search? I am looking for the most efficient way to return a json-formatted response to the client where I only want a subset of the stored fields included in the response. Thanks very much. In the Java version you can specify an optional FieldSelector when calling IndexReader.document(...). See the JavaDoc.
346,A,implement search filters in java We need to implement a search filter (Net-log like) for my social networking site against user profile filters on profile include age range gender and interests we have approx 1M profiles running on MySQL MySQL doesn't seems the right option to implement such filters so we are looking on Cassandra as well So what is the best way to implement such filter The result need to be very quick e.g. age = 18 - 24 and gender = male and interest = Football Age in Date Gender and interests are varchar EDITED: Let me rephrase the problem How can I get fastest result of any type of search. It could be on the bases of profile name or any other profile thing on 1M profile records. Thanks Why doesn't MySQL seem like the right option? Have you found that it's too slow? like statement in mysql is over killing Who said anything about `like`? I tried a simple select on interest with like %Football% and results of the query took a long time any suggestion Matt? It would serve your project well to make an underlying SQL change. You might want to consider changing the Interest column from a free-input field (varchar) to a tag (Many-to-many on an additional table for example). You used the example of Football and having a like operator on it. If you changed it to a tag then you will have an initial structural problem of deciding where to place: football Football American Football Australian-rules football But once you have done so the tags will help your select statement go much faster. Without this change you will be pushing your data management problem from a database (which is equipped to handle it) to Java (which might not be). you are right regarding tag but the problem will still be there even for age and gender search. The normal select query even a count on large table take a long time. Have you tried birthday between '1987-06-17' and '1993-06-17'? The SQL engine is equipped to optimize it. What about making gender a nullable boolean Male (so that true means Male false means female and null means didn't respond)? Bear in mind that the SQL engine will weed out unwanted records very quickly and obtain the desired subset while this is harder to do in Java (where you don't know the records until you select the records). Basic SQL optimization will buy you a lot with no programming changes so think first about indexes no wildcards and using the built-in EXPLAIN instruction.  It may make some sense to try to optimize your query (there may at least be some things that you can do). It sounds like you have a large database and if you are returning a large result set and filtering the results with java you may get performance issues because of all of the data kept in cache. If this is the case one thing that you could try is looking into caching the results outside of the database and reading from that. This is something that Hibernate does very well but you could implement your own version if needed. If this is something that you are interested in Memcached is a good starting place. I just noticed this for MySQL. I do not know how efficient it is but they have some build in full text searching functions that may help speed things up. We do not want to filter results in java like by running loop on 1M records and we have hibernate and memcached on our backend servers. Good I didnt know your setup and I figured that is a good place to start. Do you know ahead of time what keywords people will search for or have an idea of what is commonly used? You could look into adding tags or some form of indexing some of the text that will be searched on so all you have to do is search on the tags if there is a tag for what the user is looking for. The tagging could be done when the text is saved so there wouldn't be a real performance loss.
347,A,Advice on reading indexes I'm trying to figure out the right way to read lucene index only once whilst running the application multiple times how can I do that in java? Because indexed data will not change so reading them each time would not be necessary. Can someone explain me the logic of it reading them only once? thank you UPDATE : public List initTableObject() throws IOException { Directory fSDirectory = FSDirectory.open(new File(INDEX_NAME)); List<String>termList = new ArrayList<String>(); RAMDirectory directory = new RAMDirectory(fSDirectory); IndexReader iReader = IndexReader.open(fSDirectory); FilterIndexReader fReader = new FilterIndexReader(iReader); // int numOfDocs = fReader.numDocs(); TermEnum terms = fReader.terms(); while (terms.next()){ Term term = terms.term(); String termText = term.text(); termList.add(termText); } iReader.close(); return termList; } I'm really new with lucene and this so here is what I've got so far I'm just not there yet with RAMDirectory. This method retrieves list because I need this index list to compare with some files that I have. How can I store this list to the RAM so I can use it in my other part of application for comparison ? I think the answer on this question might be of use. gr8 I'll get right into it +1 tnx I updated my question any help is appreciated
348,A,Remove Expired Records in solr query syntax I'm having a solr query syntax issue (I think) with solr 1.4. I'm trying exclude expired records from a solr query. However if the record doesn't have an expiry record i would like to bring that record back also. E.g. To only get a list of record that haven't expired i am using this query: expirydate:[NOW/DAY TO *] Then I thought to get a list of records which don't have an expiry date i can use -expirydate:[* TO *] Both queries work on their own. I.e. the first query brings back 3 records. The 2nd query brings back 921 records. However when I combine the 2 queries together with an OR I get 0 records: expirydate:[NOW/DAY TO *] OR -expirydate:[* TO *] Any ideas what I'm doing wrong? Thanks Dave duplicate: http://stackoverflow.com/questions/634765/using-or-and-not-in-solr-query Not 100% sure here but I don't think you can combine OR with the negation operator in this manner. Try adding a dummy field with the same value in every document and try enddate:[NOW/DAY TO *] OR (dummy:yes -enddate:[* TO *]) Wicked.. that totally works. Thank you. I already have a field i can use which doesn't change. +1 Do you think this is a bug in Solr? Or am i just not using the syntax correctly?  I just had to tackle this problem I went about implementing the approved answer but then realized that I could just do this: -expired-date:[* TO NOW] That worked regardless if the document had expired-date set.
349,A,"Fulltext Solr statistical search Consider I'm having a couple of documents indexed with Solr 4.0. Each has 2 fields - unique ID and text DATA field. DATA field contains few paragraphs of text. Who could advise me what kind of analyzers/parsers I should use and how to build statistical query to find out sorted list of most frequently used words in all DATA fields of all documents. besides the answers mentioned here you can use the ""HighFreqTerms"" class: its in the lucene-misc-4.0 jar (which is bundled with Solr). This is a command line application which lets you see the top terms for any field either by document frequency or by total term frequency (the -t option) Here is the usage:  java org.apache.lucene.misc.HighFreqTerms [-t] [number_terms] [field] -t: include totalTermFreq Here's the original patch which is committed and in the 4.0 (trunk) and branch_3x codebases: https://issues.apache.org/jira/browse/LUCENE-2393  For ID field use analyzer based on keyword tokenizer - it will take all the content of the field as a single token. For DATA field use language specific analyzer. Notice that there's possibility to auto-detect the language of the text (patch). I'm not sure if it's possible to find the most frequent words with Solr but if you can use Lucene itself pay attention to this question. My own suggestion for it is to use HighFreqTerms class from Luke project.  for the most frequent terms look into the terms- and statistical component"
350,A,"Lucene indexing with for structured document where each text line has meta-data Hi I have a document structure where each text line in the document has some meta-data associated with it. The search result must show the line and the meta-data for the line. Currently I am storing each such line as a Lucene documents and storing the metata-data as one of the non-indexed fields. That is I create and add a Lucene Document structure for each line. My concerns is that I may end up with too many Documents in the index. Is there a more elegant approach ? Thanks Have you looked into Payloads in Lucene? They let you store additional information along with each term. Personally I'd index the documents as normal and figure out the metadata / line number later. There is no question about whether or not Lucene can cope with that many documents however it might degrade the search results somewhat. For you can perform searches where you look for multiple terms in close proximity to each other however this obviously won't work when the terms are split over multiple documents (lines). You are correct. I tried creating multiple documents one per line with meteda data stored as a part of index. That did not work well as the queries started to produce unacccepetable results. For example if I queried for a ""This"" and ""That"" tt would fail as 'this' and 'that' might exist in the file but would be in two different Lucen docs. And span queries were simple out of the question. So you are right: e documents as normal and figure out the metadata / line number later is the right approach.  How many is ""too many""? Lucene has been known to handle hundreds of millions of records in a single index so I doubt that you should have a problem. That being said there's no substitute for testing and benchmarking yourself to see if this approach is good for your needs."
351,A,Culture-independent stemmer/analyzer for Lucene.NET We're currently developing a full-text-search-enabled app and we Lucene.NET is our weapon of choice. What's expected is that an app will be used by people from different countries so Lucene.NET has to be able to search across Russian English and other texts equally well. Are there any universal and culture-independent stemmers and analyzers to suit our needs? I understand that eventually we'd have to use culture-specific ones but we want to get up and running with this potentially quick and dirty approach. Given that the spelling grammar and character sets of English and Russian are significantly different any stemmer which tried to do both would either be massively large or poorly performant (most likely both). It would probably be much better to use a stemmer for each language and pick which one to use based on either UI clues (what language is being used to query) or by explicit selection. Having said that it's unlikely that any Russian text will match an English search term correctly or vice-versa. This sounds like a case where a little more business analysis would help more than code.  There is no such a thing as a language-independent stemmer. In fact whether stemming improves retrieval performance varies per language. The best you can do is language guessing on the documents and queries then dispatch to the appropriate analyzer/stemmer. Language guessing on short queries is hard though (as in state-of-the-art not quick 'n' dirty). If your queries are short you might want use a simple whitespace analyzer on the queries and not stem anything.
352,A,"Zend_Search_Lucene crashes during indexing I wanted to create search engine for my webpage but during indexing on server it crashes with errors : Warning: opendir(/admin/lucene/) [function.opendir]: failed to open dir: Too many open files in /admin/includes/Zend/Search/Lucene/Storage/Directory/Filesystem.php on line 159 Warning: readdir(): supplied argument is not a valid Directory resource in /admin/includes/Zend/Search/Lucene/Storage/Directory/Filesystem.php on line 160 Warning: closedir(): supplied argument is not a valid Directory resource in /admin/includes/Zend/Search/Lucene/Storage/Directory/Filesystem.php on line 167 Fatal error: Ignoring exception from Zend_Search_Lucene_Proxy::__destruct() while an exception is already active (Uncaught Zend_Search_Lucene_Exception in /admin/includes/Zend/Search/Lucene/Storage/File/Filesystem.php on line 66) in /admin/test.php on line 549 I am using newest version of ZF. Is there code solution for such error - I run script on localhost and it works great. Thanks for any help. For the record ""newest version of ZF"" on Apr 28 2009 when you asked this question was ZF 1.8.0 Beta 1. PHP has hit the limit on the number of files it can have open at once it seems might be an option to change in php.ini could be an OS (quota) limit or you might be able to tell the indexer to slow down and not have so many files open simultaneously.  This is most definitely a Linux/kernel imposed limitation. Use the following command as root on your machine: cat /proc/sys/fs/file-nr Return values are defined as: Total allocated file descriptors Total free allocated file descriptors Maximum open file descriptors I'm also going to take a guess and say you are on a shared hosting machine. If this is the case I imagine that this sort of issue may come up frequently. Finally the following article provides a good amount of information on Linux and open file descriptors even if it is a little dated. http://www.netadmintools.com/art295.html  It seems the problem is in the large number of segments in the index. Could you check how much files does index folder contain? There are two ways to solve this problem: a) Optimize index more often. b) Use another MaxBufferedDocs/MergeFactor parameters. See Zend_Search_Lucene documentation for details. If it doesn't help please register JIRA issue for the problem."
353,A,"Indexing different type of Entities/Objects with Solr Lucene Let's say I want to index my shop using Solr Lucene. I have many types of entities : Products Product Reviews Articles How do I get my Lucene to index those types but each type with different Schema ? Multi-core is an approach to use with care. With a simple schema like yours it's a better way to do as buru recommands. That means to find common fields between your different entities and then fields that will be used only by on or several of them. You can then add a field ""type"" or ""type_id"" which will say if your entity is product a product review... Doing so will enable you to have an unique index and to process queries fastly.  With Lucene/Solr each document does not need to set a value for each field. Within the same schema you can have a set of fields for entity A and another set of fields for entity B and just populate the appropriate field depending on the entity. With Solr you also have the option to go multi-core. Each core have its own schema. You could define a core for each entity.  I recommend creating your index in a way that all of you entities have more or less the same basic fields: title content url uuid entity_type entity_sourcename etc. If each of your entities has a unique set of corresponding index field you'll have hard time constructing query to search all entities simultaneously and your results view may become a huge mess. If you need some specific fields for a specific entity then add it and perform special logic for this entity based on its entity_type. I'm speaking from experience: we're managing an index with over 10 different entities and this approach works like charm. P.S. A few other simple advices. Make sure your Lucene document contains all of the necessary data to construct the result and show it to user (so that you don't need to go to the database to construct the result). Lucene queries are generally much faster than database queries. If you absolutely need to use database to construct your result set (e.g. to apply permissions) use Lucene query first to narrow results database query second to filter them. Don't be afraid to add custom fields to some of your documents if you need it: think of Lucene document as of key-value datastore.  You might want to have 3 indexes called Products ProductReviews and Articles. Each index can have its own schema. The difference between Lucene and a relational db approach is that a row in a db roughly translates to a document in Lucene. Note: each document can have its own schema (which is another difference from a relational db)."
354,A,Finding the number of documents in a lucene index Using Java how would you find out the number of documents in an lucene index? The official documentation: http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/index/IndexReader.html#numDocs() Here's an updated link. http://lucene.apache.org/core/3_6_1/api/core/index.html  Using java you can find the number of documents like this : IndexReader reader = IndexReader.open(FSDirectory.open(indexDirectory)); System.out.println(reader.maxDoc()); //this will give ya what you need. Technically this will include the documents that have been deleted. `reader.numDocs()` takes this into account.  IndexReader contains the methods you need in particular numDocs http://lucene.apache.org/core/3_6_0/api/all/org/apache/lucene/index/IndexReader.html#numDocs() The link no longer works but a more recent document exists here: http://lucene.apache.org/core/3_6_0/api/all/org/apache/lucene/index/IndexReader.html#numDocs()
355,A,"Using PyLucene as a K-NN Classifier I have a dataset composed of millions of examples where each example contains 128 continuous-value features classified with a name. I'm trying to find a large robust database/index to use to use as a KNN classifier for high-dimensional data. I tried Weka's IBk classifier but it chokes on this much data and even then it has to be loaded into memory. Would Lucene specifically through the PyLucene interface be a possible alternative? I've found Lire which seems to use Lucene in a similar way but after reviewing the code I'm not sure how they're pulling it off or if it's the same thing I'm trying to do. I realize Lucene is designed as a text indexing tool and not as a general purpose classifier but is it possible to use it in this way? I can't find any documentation for Mahout's KNN other than a brief reference to it in the Taste component which explicitly states it only supports boolean features. Mahout doesn't appear usable as a general purpose KNN. To process ""millions of examples"" you should take a look on apache mahout - a distributed machine learning framework - it seems to have kNN: https://issues.apache.org/jira/browse/MAHOUT-115. Lucene doesn't seem like the right choice given what you've told us. Lucene would give you a way to store the data but in terms of retrieval it's not designed to do anything but search over textual strings. Since K-NN is so simple you might be better off creating your own data store in a typical RDBMS or something like Berkeley DB. You could create keys/indicies based on sub-hypercubes of the various dimensions to speed things up - start at the bucket of the item to be classified and move outward... I haven't seen any RDBM support for KNN classification outside of maybe the GIS standard which is mostly only supported by expensive proprietary systems. I'm not sure what you mean by creating keys/indicies with ""hypercubes"". Could you please cite some sources? You'll have to roll your own if you use a RDBMS. If you have a large dataset you could store in a BDB or RDMNS all the pairs and then index them along each dimension. For two dimensions this would be like drawing a grid over space of the parameters. You would then look up the cell and adjacent cell for the nearest items. No sources just an idea.  This is done in Lucene already with geospatial searches. Of course the built-in geospatial searches only use two dimensions so you'll have to modify it a bit. But the basic idea of using numeric range queries will work. (Note: I'm not aware of anyone doing high-dimensional kNN with Lucene. So I can't comment on how fast it will be.)"
356,A,"How can i sort the results of a lucene search between multiple indexes? I have two lucene indexes and i need to search on the two indexes. How can i execute a search in multiple lucene indexes? How can i sort these results? Thanks Luiz Costa basic code.. just typed it up check out the doc for more details IndexSearcher[] searchers = new IndexSearcher[2]; searchers[0] = new IndexSearcher(searchDirOne); searchers[1] = new IndexSearcher(searchDirTwo); MultiSearcher searcher = new MultiSearcher(searchers); Query query = QueryParser.Parse(""foo""""bar""  new StandardAnalyzer()); Hits hits = searcher.Search(query); MultiSearcher Documentation thanks Aeron it's works. But now i am trying sorting these results."
357,A,"Searching for exact phrase How do i achieve ""Exact Phrase"" functionality on this field using BooleanQuery/any other class? For example if a user types in ""top selling book"" then it should return books which has this phrase in its description. Thanks again! Having the query within double quotes should work."
358,A,"Solr - one word phrase search to avoid stemming I have stemming enabled in my Solr instance I had assumed that in order to perform an exact word search without disabling stemming it would be as simple as putting the word into quotes. This however does not appear to be the case? Is there a simple way to acheive this? Thanks in advance Ruth There is a simple way if what you're referring to is the ""slop"" (required similarity) as part of a fuzzy search (see the Lucene Query Syntax here). For example if I perform this search:  q=field_name:determine I see results that contain ""determine"" ""determining"" ""determined"" etc.. If I then modify the query like so:  q=field_name:determine~1 I only see results that contain the word ""determine"". This is because I'm specifying a required similarity of 1 which means ""exact match"". I can specify this value anywhere from 0 to 1. thanks great help  Another thing you can do is index the same text without stemming in one field and with stemming in another. Boost the non-stemmed field & that should prefer exact versions of words to stemmed versions. Of course you could also write your own query parser that directs quoted phrases to the non-stemmed field only."
359,A,"Wordpress search large site shared server-Should I use MySQL FullText Index Lucene.net...? I'm creating a site that is running on a shared server and I need to find a good search engine. What search engine framework should I use to fit my requirements? Here are some requirements ~100000 documents need to be indexed Shared Server (but can run ASP.Net and php apps) Need to be able to restrict search results to specific tags categories Need to be able to sort by relevance + popularity or relevance + date A search is preformed on every page load (although i might implement caching). The way it works is kind of like stackoverflow. I have a main document and then suggestions for related documents are loaded on the right. This occurs on every page Software is free and has very little budget for any type of hosted search solution (at this time anyway) Here are my thoughts zend lucene search - performance is not good enough for such a large site Google custom search - number of sites/queries is limited Solr Sphinx java lucene - on a shared server so I cannot install these Lucene.net - I'm not sure if this is is possible. My hosting company allows me to run php and asp.net websites...but perhaps Lucene.net has to run as a separate process? MySql FullText search - I am not aware of performance for large sites like I have described This seems like a tough bill to satisfy but I'm hoping I don't need to come up with an alternative design. What is your question? Looks like lucene.net is possible with SharedHosting according to this post http://stackoverflow.com/questions/1708032/lucene-net-on-shared-hosting whoops. Thanks Mat If I am not wrong you are using WOrdpress. Will you be able to install MongoDB and php-mongo extension to your server if yes then MongoDB FUlltext Search with MongoLantern can be efficient plugin for you.It can also be installed with wordpress and override the wordpress search with mongodb fulltext search. I have used it in few of my projects and they seems worked quite well. You can have MongoLantern WP plugin from here: http://wordpress.org/extend/plugins/mongolantern/  For this kind of features and such a big number of documents I would absolutly not go with MySQL's fulltext : I would definitly use some external indexing/searching solution (like Solr Lucene ...) Like you said : You have too many documents for Zend Lucene (pure PHP implementation). MySQL fulltext -- ergh not that powerful slow ... Solr/Sphinx require you to install them Not sure about Lucene.NET but with that kind of volume of data can you really not get your own server so you can install what you need to work properly ? And that's especially true if search is an important part of your application (it seems it is). hmm...I should add the requirement that the software I'm making is free and has no income. I would only upgrade to a separate server if search volume warranted it. So I understand that performance will not be as good as if I had a separate search server but it is not something I can afford in the initial requirements (but definitely something to consider upgrading to in the future if needed). If I can't meet my current requirements reasonably well with a shared host solution then I will simply have to change the overall design of the app. Oh OK ; I thought/hoped that was some kind of ""work"" which meant having money you could invest in a new server. Thanks. Your answer helps me pretty much rules out everything other than Lucene.net so I guess I just need to see if it can work. You're welcome :-) Have fun !"
360,A,"Lucene query - ""Match exactly one of x y z"" I have a Lucene index that contains documents that have a ""type"" field this field can be one of three values ""article"" ""forum"" or ""blog"". I want the user to be able to search within these types (there is a checkbox for each document type) How do I create a Lucene query dependent on which types the user has selected? A couple of prerequisites are: If the user doesn't select one of the types I want no results from that type. The ordering of the results should not be affected by restricting the type field. For reference if I were to write this in SQL (for a ""blog or forum search"") I'd write: SELECT * FROM Docs WHERE [type] in ('blog' 'forum') For reference should anyone else come across this problem here is my solution: IList<string> ALL_TYPES = new[] { ""article"" ""blog"" ""forum"" }; string q = ...; // The user's search string IList<string> includeTypes = ...; // List of types to include Query searchQuery = parser.Parse(q); Query parentQuery = new BooleanQuery(); parentQuery.Add(searchQuery BooleanClause.Occur.SHOULD); // Invert the logic exclude the other types foreach (var type in ALL_TYPES.Except(includeTypes)) { query.Add( new TermQuery(new Term(""type"" type)) BooleanClause.Occur.MUST_NOT ); } searchQuery = parentQuery; I inverted the logic (i.e. excluded the types the user had not selected) because if you don't the ordering of the results is lost. I'm not sure why though...! It is a shame as it makes the code less clear / maintainable but at least it works!  Add a constraints to reject documents that weren't selected. For example if only ""article"" was checked the constraint would be -(type:forum type:blog) This is what I did in the end although I used the API rather than creating it as a string see my answer if you're interested.  While erickson's suggestion seems fine you could use a positive constraint ANDed with your search term such as text:foo AND type:article for the case only ""article"" was checked or text:foo AND (type:article OR type:forum) for the case both ""article"" and ""forum"" were checked. Lucene doesn't have an ""AND"" operator. It has + (require) and - (prohibit) operators. @erickson: I beg to differ: e.g. http://incubator.apache.org/lucene.net/docs/2.1/Lucene.Net.QueryParsers.QueryParser.AND_OPERATOR.html Intriguingly the two queries ""text:foo AND (type:article OR type:forum)"" and ""text:foo AND -type:blog"" do not give the same results the first query returns the blogs first where as the second query maintains the ordering (i.e. blogs and articles are mixed). Any idea why? Hey where'd that come from?"
361,A,"Migrating from Hit/Hits to TopDocs/TopDocCollector I have existing code that's like: final Term t = /* ... */; final Iterator i = searcher.search( new TermQuery( t ) ).iterator(); while ( i.hasNext() ) { Hit hit = (Hit)i.next(); // ""FILE"" is the field that recorded the original file indexed File f = new File( hit.get( ""FILE"" ) ); // ... } It's not clear to me how to rewrite the code using TopDocs/TopDocCollector and how to iterate over all results. Basically you have to decide on a limit to the number of results you expect. Then you iterate over all the ScoreDocs in the resulting TopDocs. final MAX_RESULTS = 10000; final Term t = /* ... */; final TopDocs topDocs = searcher.search( new TermQuery( t ) MAX_RESULTS ); for ( ScoreDoc scoreDoc : topDocs.scoreDocs ) { Document doc = searcher.doc( scoreDoc.doc ) // ""FILE"" is the field that recorded the original file indexed File f = new File( doc.get( ""FILE"" ) ); // ... } This is basically what the Hits class does only it sets the limit at 50 results and if you iterate past that then the search is repeated which is usually wasteful. That is why it was deprecated. ADDED: If there isn't a limit you can put on the number of the results you should use a HitCollector: final Term t = /* ... */; final ArrayList<Integer> docs = new ArrayList<Integer>(); searcher.search( new TermQuery( t ) new HitCollector() { public void collect(int doc float score) { docs.add(doc); } }); for(Integer docid : docs) { Document doc = searcher.doc(docid); // ""FILE"" is the field that recorded the original file indexed File f = new File( doc.get( ""FILE"" ) ); // ... } Except I want all results. (I want it to function more like grep.)"
362,A,"Implement Lucene on Existing .NET / SQL Server stack with multiple webservers I want to look at using Lucene for a fulltext search solution for a site that I currently manage. The site is built entirely on SQL Server 2008 / C# .NET 4 technologies. The data I'm looking to index is actually quite simple with only a couple of fields per record and only one of those fields actually searchable. It's not clear to me what the best toolset I need to be using is or what the architecture I should be using is. Specifically: Where should I put the index? I've seen people recommend putting it on the webserver but that would seem wasteful for a large number of webservers. Surely centralising would be better here? If the index is centralised how would I query it given that it just lives on the filesystem? Will I have to effectively put it on a network share that all the webservers can see? Are there any pre-existing tools that will incrementally populate a Lucene index on a schedule pulling the data from an SQL Server database? Would I be better off rolling my own service here? When I query the index should I be looking to just pull back a bunch of record id's which I then go back to the DB for the actual record or should I be aiming to pull everything I need for the search straight out of the index? Is there value in trying to implement something like Solr in this flavour environment? If so I'd probably give it it's own *nix VM and run it within Tomcat on that. But I'm not sure what Solr would buy me in this case. I'll answer a bit based on how we chose to implement Lucene.Net here on Stack Overflow and some lessons I learned along the way: Where should I put the index? I've seen people recommend putting it on the webserver but that would seem wasteful for a large number of webservers. Surely centralising would be better here? It depends on your goals here we had a severely under-utilized web tier (~10% CPU) and an overloaded database doing FullText searching (around 60% CPU we wanted it lower). Loading up the same index on each web tier let us utilize those machines and have a ton of redundancy we can still lose 9 out of 10 web servers and keep the Stack Exchange network up if need be. There is a downside to this it's very IO (read) intensive for us and the web tier was not bought with this in mind (this is often the case at most companies). While it works fine we'll still be upgrading our web tier to SSDs and implementing some other bits left out of the .Net port to compensate for this hardware deficiency (NIOFSDirectory for example). The other downside if we index all our databases n times for the web tier but luckily we're not starved for network bandwidth and SQL server caching the results makes this a very fast delta indexing operation each time. With a large number of web servers that alone may eliminate this option. If the index is centralised how would I query it given that it just lives on the filesystem? Will I have to effectively put it on a network share that all the webservers can see? You can query it on a file share either way just make sure only one is indexing at a time (write.lock the directory locking mechanism will ensure this and error when you try multiple IndexWriters at once). Keep in mind my notes above this is is IO intensive when a lot of readers are flying around so you need ample bandwidth to your store short of at least iSCSI or a fiber SAN I'd be cautious of this approach on a high traffic (hundreds of thousands of searches a day) use. Another consideration is how you update/alert your web servers (or whatever tier is querying it). When you finishing an indexing pass you'll need to re-open your IndexReaders to get the updated index with new documents. We use a redis messaging channel to alert whoever cares that the index has updated...any messaging mechanism would work here. Are there any pre-existing tools that will incrementally populate a Lucene index on a schedule pulling the data from an SQL Server database? Would I be better off rolling my own service here? Unfortunately there are none that I know of but I can share with you how I approached this. When indexing a specific table (akin to a document in Lucene) we added a rowversion to that table. When we index we select based off the last rowversion (a timestamp datatype pulled back as a bigint). I chose to store the last index date and last indexed rowversion on the file system via a simple .txt file for one reason: everything else in Lucene is stored there. This means if there's ever a large problem you can just delete the folder containing the index and the next indexing pass will recover and have a fully up-to-date index just add some code to handle nothing being there meaning ""index everything"". When I query the index should I be looking to just pull back a bunch of record id's which I then go back to the DB for the actual record or should I be aiming to pull everything I need for the search straight out of the index? This really depends on your data for us it's not really feasible to store everything in the index (nor is this recommended). What I suggest is you store the fields for your search results in the index and by that I mean what you need to present your search results in a list before the user clicks to go to the full [insert type here]. Another consideration is how often your data is changing. If a lot of fields you're not searching on are changing rapidly you'll need to re-index those rows (documents) to update your index not only when the field you're searching on changes. Is there value in trying to implement something like Solr in this flavour environment? If so I'd probably give it it's own *nix VM and run it within Tomcat on that. But I'm not sure what Solr would buy me in this case. Sure there is it's the centralized search you're talking about (with a high number of searches you may again hit a limit with a VM setup keep an eye on this). We didn't do this because it introduced a lot of (we feel) unwarranted complexity in our technology stack and build process but for a larger number of web servers it makes much more sense. What does it buy you? performance mainly and a dedicated indexing server(s). Instead of n servers crawling a network share (competing for IO as well) they can hit a single server that only deals with requests and results over the network not crawling the index which is a lot more data going back and forth...this would be local on the Solr server(s). Also you're not hitting your SQL server as much since fewer servers are indexing. What it doesn't buy you is as much redundancy but it's up to you how important this is. If you can operate fine on degraded search or without it simply have your app handle that. If you can't then a backup Solr server or more may also be a valid solution...and it is possible another software stack to maintain. Wow very useful answer thanks Nick Brilliant. Just the sort of detail I was after. I can 100% understand the approach of wanting to use idle time in the web tier to do the fulltext indexing and especially not wanting to add another component into the infrastructure. That said I can also forsee a number of issues with taking that approach (scaling knowing what to incrementally index troubleshooting searching problems across the farm) but none that are insurmountable. Given that Solr can (in theory) cluster I'm going to initially head off down the Solr route but might ditch it in favour of the web-tier route later on. With regard to updating/alerting your web servers when you finishing an indexing pass: Lucene provides the reopen() method on a reader which returns either the current reader or an updated reader if the underlying data has changed. You can then compare references and use the new one if needed. It covers changes to the reader while sharing resources from the old one for efficiency. For many situations this is sufficient and removes the need for any notifications between indexing and reading applications. Thanks for sharing your experience with us. It is very useful to read your answer indeed."
363,A,"Is it possible to iterate through documents stored in Lucene Index? I have some documents stored in a Lucene index with a docId field. I want to get all docIds stored in the index. There is also a problem. Number of documents is about 300 000 so I would prefer to get this docIds in chunks of size 500. Is it possible to do so? Document numbers (or ids) will be subsequent numbers from 0 to IndexReader.maxDoc()-1. These numbers are not persistent and are valid only for opened IndexReader. You could check if the document is deleted with IndexReader.isDeleted(int documentNumber) method  IndexReader reader = // create IndexReader for (int i=0; i<reader.maxDoc(); i++) { if (reader.isDeleted(i)) continue; Document doc = reader.document(i); String docId = doc.get(""docId""); // do something with docId here... } What does happen if (reader.isDeleted(i)) is missing? Without the isDeleted() check you would output id's for documents that had been previously deleted To complete comment from above. Index changes are commited when index is reopen so reader.isDeleted(i) is necessary to guarantee that documents are valid.  Lucene 4 Bits liveDocs = MultiFields.getLiveDocs(reader); for (int i=0; i<reader.maxDoc(); i++) { if (liveDocs != null && !liveDocs.get(i)) continue; Document doc = reader.document(i); } See LUCENE-2600 on this page for details: https://lucene.apache.org/core/4_0_0/MIGRATE.html This was rolled back by another user but the original editor was correct liveDocs can be null"
364,A,"How to configure the indexer so that ""word1.word2"" is considered as two words supose a file 'test.txt' being indexed the content of file is: word1.word2 what should I do to make lucene consider ""word1.word2"" as two words ""word1"" and ""word2"" not ""word1.word2"" Lucene indexing with an analyzer will convert your words into Tokens of terms(technically it converts the words into fields forming a document) basically you can 1) create a StopAnalyzer and pass a HashSet with stop word as "".""(period) this can have adverse effect on indexing(since you must use same analyzer while searching and indexing) 2) split the . with space and index them  That depends on which Analyzer you are using. The short generic answer would be to use a SimpleAnalyzer that uses a LetterTokenizer. The LetterTokenizer splits at any non-letter thus including the dot character. If you have more specific tokenization requirements you must code a custom Analyzer class whose tokenStream method returns a custom TokenStream or Tokenizer object."
365,A,Python file indexing and searching I have a large set off files (hdf) that I need to enable search for. For Java I would use Lucene for this as it's a file and document indexing engine. I don't know what the python equivalent would be though. Can anyone recommend which library I should use for indexing a large collection of files for fast search? Or is the prefered way to roll your own? I have looked at pylucene and lupy but both projects seem rather inactive and unsupported so I am not sure if should rely on them. Final notes: Woosh and pylucene seems promising but woosh is still alpha so I am not sure I want to rely on it and I have problems compiling pylucene and there are no actual releases off it. After I have looked a bit more at the data it's mostly numbers and default text strings so as off now an indexing engine won't help me. Hopefully these libraries will stabilize and later visitors will find some use for them. I haven't done indexing before however the following may be helpful :- pyIndex - http://rgaucher.info/beta/pyIndex/ -- File indexing library for Python http://www.xml.com/pub/a/ws/2003/05/13/email.html -- Thats a script for searching Outlook email using Python and Lucene http://gadfly.sourceforge.net/ - Aaron water's gadfly database (I think you can use this one for indexing. Haven't used it myself.) As far as using HDF files goes I have heard of a module called h5py. I hope this helps. I can read the hdf5 files fine using pytables I just need to find the right tool to index the information I extract. I have little experience in the area. Since you can already read hd5 files I think that pyIndexer might work for you. I have little experience in the area and I hope your project works out well.  Lupy has been retired and the developers recommend PyLucene instead. As for PyLucene its mailing list activity may be low but it is definitely supported. In fact it just recently became an official apache subproject. You may also want to look at a new contender: Whoosh. It's similar to lucene but implemented in pure python.  I'd suggest Sphinx. It's very active has much more features and seems faster than Lucene. Sphinx is great and IMHO easier to install configure etc than pylucene.  A popular C++ based information retrieval library that is often used with Python is Xapian http://xapian.org/ It's incredibly quick and can happily manage large amounts of data however it's not quite as easily extensible as Lucene.
366,A,"""phrase search"" in solr/lucene I'm using solr 1.4 and solr 4 for fulltext-search inside documents. At the moment I'm unable to search whole phrases like ""The dog runs"" at the textblock: ""The dog runs through the house."" For this testcase I use an simple solr URL: http://plocalhost:8088/solr/select/?start=0&q=""the dog runs"" I'm using an tokenized stemmed textfiled with the following options: <fieldType name=""text_de"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords-de.txt"" enablePositionIncrements=""true"" /> <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""1"" catenateNumbers=""1"" catenateAll=""0"" splitOnCaseChange=""1""/> <filter class=""solr.KeywordMarkerFilterFactory"" protected=""protwords.txt""/> <filter class=""solr.SnowballPorterFilterFactory"" language=""German"" /> </analyzer> <analyzer type=""query""> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms-de.txt"" ignoreCase=""true"" expand=""true""/> <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""1"" catenateNumbers=""1"" catenateAll=""0"" splitOnCaseChange=""1""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.KeywordMarkerFilterFactory"" protected=""protwords.txt""/> <filter class=""solr.SnowballPorterFilterFactory"" language=""German"" /> </analyzer> </fieldType> I have no idea why it's not working. :-( ...thank you for any hint. how: I'm using different tomcat installations at the same server (development system). why: for running different versions of an application - old (current) version using 1.4 for bug-fixing and new version using 4.x (which is in development) OK I see. Original description wasn't clear. It sounded like you were indexing with 1.4 and then querying with 4.X. How are you using Solr 1.4 and Solr 4.X at the same time? I don't understand why or how. To answer my own question: The analyzer on index time is using a stopwords list while the analyzer on query time does NOT use a stopword list. So the phrase in the index was not the same as the phrase on query time. I only had to add the StopFilterFactory at the ""query""-analyzer."
367,A,"Lucene numeric range search with LUKE I have a number of numeric Lucene indexed fields: 60000 78500 105000 If I use LUKE to query for 78500 as follows: price:78500 It returns the correct record however if I try to return all three record as a range I get no results. price:[60000 TO 105000] I realise this is due to padding as numbers are treated strings by Lucene however I just wish to know what I should be putting into LUKE to return the three records. Many thanks for any help. Have you tried: price:[""60000"" TO ""105000""] Thanks Joel yeah I tried this and still no results returned? I assume these fields are indexed as NumericFields. The problem with them is that Lucene/Luke does not know how to parse numeric queries automatically. You need to override Lucene's QueryParser and provide your own logic how these numbers should be interpreted. As far as I know Luke allows sticking in your custom parser it just need to be present in the CLASSPATH. Have a look at this thread on Lucene mailing list: http://mail-archives.apache.org/mod_mbox/lucene-java-user/201102.mbox/%3CAANLkTi=XUpyw09tcbjuTzNRpMJa730Cq-6_1agMAjYz6@mail.gmail.com%3E Thanks very much for this information I have managed to resolve this now. This is the correct answer to the OP's question as it states why what the OP wanted to do **does not work** (automatically interpreting string input as a numeric range query).  Zero padding won't come into this particular query since all the numbers you've shown have the same number of digits The range query you've shown has too many zeros on the second part of the range So the query for the data you've shown would be price:[10500 TO 78500] Hope this helps Apologies the last figure was meant to read 105000 I will edit now. I have managed to resolve this so will post an answer now. Many thanks for your tips with this though.  The solution I used for this was that the values inputted for price needed to be added to the index in padded form. Then I would just query the new padded value which works great. Therefore the new values in the index were: 060000 078500 105000 This solution was tied into an Examine search issue for Umbraco so there is a thread on the Forum of how to implement a numeric based range search if anyone requires this it is located here with a walk through end to end. Umbraco Forum Thread This is probably the best solution if you want users to be able to enter the numeric range query as text. If you have a different way to gather input that you should use the `NumericRangeQuery` class and use it to construct your query by hand rather that using the `QueryParser`.  If the fields are indexed as NumericField you must use ""Use XML Query Parser"" option in query parser tab and the 3.5 version of Luke: https://code.google.com/p/luke/downloads/detail?name=lukeall-3.5.0.jar&can=2&q= An example of query with a string and numeric field is: <BooleanQuery> <Clause fieldName=""colour"" occurs=""must""> <TermQuery>rojo</TermQuery> </Clause> <Clause fieldName=""price"" occurs=""must""> <NumericRangeQuery type=""int"" lowerTerm=""4000"" upperTerm=""5000"" /> </Clause> </BooleanQuery>"
368,A,"Exact phrase search using Lucene.net I am having trouble searching for an exact phrase using Lucene.NET 2.0.0.4 For example I am searching for ""scope attribute sets the variable"" (including quotes) but receive no matches I have confirmed 100% that the phrase exists. Can anyone suggest where I am going wrong? Is this even supported with Lucene.NET? As usual the API documentation is not too helpful and a few CodeProject articles I've read don't specifically touch on this. Using the following code to create the index: Directory dir = Lucene.Net.Store.FSDirectory.GetDirectory(""Index"" true); Analyzer analyzer = new Lucene.Net.Analysis.SimpleAnalyzer(); IndexWriter indexWriter = new Lucene.Net.Index.IndexWriter(dir analyzertrue); //create a document add in a single field Lucene.Net.Documents.Document doc = new Lucene.Net.Documents.Document(); Lucene.Net.Documents.Field fldContent = new Lucene.Net.Documents.Field( ""content"" File.ReadAllText(@""Documents\100.txt"") Lucene.Net.Documents.Field.Store.YES Lucene.Net.Documents.Field.Index.TOKENIZED); doc.Add(fldContent); //write the document to the index indexWriter.AddDocument(doc); I then search for a phrase using: //state the file location of the index Directory dir = Lucene.Net.Store.FSDirectory.GetDirectory(""Index"" false); //create an index searcher that will perform the search IndexSearcher searcher = new Lucene.Net.Search.IndexSearcher(dir); QueryParser qp = new QueryParser(""content"" new SimpleAnalyzer()); // txtSearch.Text Contains a phrase such as ""this is a phrase"" Query q=qp.Parse(txtSearch.Text); //execute the query Lucene.Net.Search.Hits hits = searcher.Search(q); The target document is about 7 MB plain text. I have seen this previous question however I don't want a proximity search just an exact phrase search. You have not enabled the term positions. Creating field as follows should solve your problem. Lucene.Net.Documents.Field fldContent = new Lucene.Net.Documents.Field(""content"" File.ReadAllText(@""Documents\100.txt"") Lucene.Net.Documents.Field.Store.YES Lucene.Net.Documents.Field.Index.TOKENIZED Lucene.Net.Documents.Field.TermVector.WITH_POSITIONS_OFFSETS);  Shashikant Kore is correct with his answer you need to enable term positions... However I would recommend not storing the text of the document in the field unless you absolutely need it to return back to you in the search results... Setting the store to 'NO' might help reduce the size of your index a bit. Lucene.Net.Documents.Field fldContent = new Lucene.Net.Documents.Field(""content"" File.ReadAllText(@""Documents\100.txt"") Lucene.Net.Documents.Field.Store.NO Lucene.Net.Documents.Field.Index.TOKENIZED Lucene.Net.Documents.Field.TermVector.WITH_POSITIONS_OFFSETS);"
369,A,"Lucene Interview Questions I am interviewing candidates for a position developing an application which relies heavily on Lucene. In addition to the usual questions I ask I'd like to be able to ask one or two Lucene-specific questions that will give me a rough idea of how familiar they are with the library. The problem is that I have no experience with Lucene myself. Any suggestions? Suggest checking the Lucene/Solr mailing lists to see if the candidate is a regular contributor. Also to get a taste of the kinds of ongoing programming challenges which you can pose. great idea i post patches to every open-source project when I look for work just like everybody else. the trick is to do that instead of working :) Is Lucene that buggy that you have to contribute just to use it? This is a tricky task. You're looking for the guy who knows more about Lucene than you do; therefore you can't be a reliable judge of the candidates' knowledge (although you should be able to at least eliminate the ones who obviously know less than you). My advice is to ask the candidates to explain to you some aspect of Lucene that you are confused about. When the interview's over you can look it up to see if the answer made sense. This has the added benefit of testing their ability to communicate complex ideas. (And if the answer is ""I don't know"" then you should take that as a good sign: people who are willing to admit their ignorance are worth a lot more than those who aren't.)  A couple of questions I would ask: What is the Lucene data structure? (inverted index) How does Lucene computes the relevancy of a document? (vector space model boolean model) What is a segment? (a portion of the index) How text is being indexed? (analyzers tokenizers) What is a document? (collection of fields) What is the Lucene query syntax looks like? (boolean query boost fuzzy searches) How it differs from a relational database and when would you use one over the other? Thanks for the list. I will definitely ask some questions pertaining to inverted indexes since that is a concept I already understand.  If the candidate has a long history of Java development familiarity with the Lucene API shouldn't be that important. Someone unfamiliar with Lucene might take a little longer to get started but in the long run I would feel much more comfortable with a Very experienced Java developer than a somewhat experienced java Developer with Lucene experience. In fact I might prefer an very experienced non-java programmer if there portfolio was impressive. Also while I agree in the ""long run"" you are right this is for a short term contract. There won't be a lot of time to get them up to speed. Totally agree. The Lucene portion of the interview won't be the most important factor. However these candidates were chosen specifically for their experience with Lucene (not by me). It would be negligent of me not to cover the topic at all."
370,A,"Lucene .NET not returning search results For some reason lucene is not returning any results when it should be. Here is the 'search' code Dim util As New IndexerUtil() Dim dir As Lucene.Net.Store.Directory = FSDirectory.Open(New DirectoryInfo(util.getIndexDir())) Dim indexSearcher As New IndexSearcher(dir False) Dim indexWriter As New IndexWriter(dir New SimpleAnalyzer() False indexWriter.MaxFieldLength.UNLIMITED) Dim term As New Term(""id"" ""346"") Dim query As New TermQuery(term) Dim topDocs As TopDocs = indexSearcher.Search(query 100) There are no scoreDocs (results) in topDocs. I know for a fact that there is a document in the index where the id field is equal to 346 however for some reason the search is not finding it. Here is how the ""id"" field is being created doc.Add(New Field(""id"" ID Field.Store.YES Field.Index.ANALYZED)) //ID is an integer I have other fields to search on and those work fine (e.g. if I search on the subject field I get the results I should) SimpleAnalyzer uses LetterTokenizer which only returns letters. Consider using the KeywordAnalyzer instead for the id field. I can't upvote until I have 15 rep. Otherwise I would. I've upvoted your questions. I think you get 2 rep points per accepted answer so if you click the check beside 2 answers you'll get the additional 4 rep points you need to upvote. ;-) Just a little stack overflow tip! That fixed it ... thanks again! I'm glad that worked for you. By the way around StackOverflow a good way to show gratitude is to upvote and accept answers. Best of luck with Lucene.net!"
371,A,"Including boolean property check in Search Query builder with Grails Searchable Plugin I am trying to limit my search criteria to return entities with a boolean property set to true. For example class Product { def name Boolean enabled } How can I do it using a search query builder my simple search so far is: Product.search(query analyzer: 'whitespace') I tried using query builder with term but it does not work: Product.search { must(queryString(query)) must(term('enabled'true)) } Any ideas? Thank you. Adding +enabled:true works properly and does not return the results with enabled set to false Can you try putting it in the query? e.g. `+enabled:true +name:foo`. This will help determine if there is a problem with how it's being indexed etc. nope not resolved .. If this issue has been solved somebody should post the solution as an answer. It'll be the spelling error (""available"" vs. ""enabled""). According to this thread anyway your syntax is correct. I made an error here but the actual code is correct updated my question as well. Thanks.  I was using 0.5.5.1 version of searchable plugin. After updating to the latest 0.6 SNAPSHOT enabled field is being picked up like it supposed to and results are accurate. Thanks."
372,A,.NET version of elasticsearch? Is anyone aware of a .NET port of elasticsearch that has been implemented? We have found a .NET client (easy enough to create on our own) but not the server. Why does it matter? It's a self-contained it shouldn't matter how it's implemented. No but why would you want a .net version of the server? It is a standalone server so it doesn't matter what language it is written in. ElasticSearch uses the Lucene libraries which are in Java so not only would you have to rewrite ElasticSearch you'd also have to rewrite Lucene. @John: You don't need a LAMP stack elasticsearch runs on Windows just fine. @DrTech but there is Lucene.net so the OP needs only to implement ES.net ;)
373,A,Lucene index deleted when opening with Luke/Indexreader I was creating a lucene index when my indexing program crashed. The indexer had processed about 3M documents before crashing producing a 14GB file. When I opened the index in Luke (with force unlock) the whole index was gone!. poof. The opened index had 0 documents and its size was reduced to 1kb. Did anyone experience this or can offer an explanation (Using Lucene.Net 2.9) Most probably your indexing code never called commit() before crashing. If you don't want to lose all your changes you should call commit() every X added documents.
374,A,"Why is my Lucene index getting locked? I had an issue with my search not return the results I expect. I tried to run Luke on my index but it said it was locked and I needed to Force Unlock it (I'm not a Jedi/Sith though) I tried to delete the index folder and run my recreate-indicies application but the folder was locked. Using unlocker I've found that there are about 100 entries of w3wp.exe (same PID different Handle) with a lock on the index. Whats going on? I'm doing this in my NHibernate configuration: c.SetListener(ListenerType.PostUpdate new FullTextIndexEventListener()); c.SetListener(ListenerType.PostInsert new FullTextIndexEventListener()); c.SetListener(ListenerType.PostDelete new FullTextIndexEventListener()); And here is the only place i query the index: var fullTextSession = NHibernate.Search.Search.CreateFullTextSession(this.unitOfWork.Session); var fullTextQuery = fullTextSession.CreateFullTextQuery(query typeof (Person)); fullTextQuery.SetMaxResults(100); return fullTextQuery.List<Person>(); Whats going on? What am i doing wrong? Thanks you may not be a Jedi but Luke is Delete ""write.lock"" file in the index folder. the question is ""why is it getting locked?"" not ""tell me an arbitrary manual solution"" Check for any write operations. As @Kragen mentioned it is more likely your index write operation terminated and left the write.lock file. I would start with deleting write.lock and see if it gets recreated; if so then I would look for processes that access the index.  Looks like this was the issue http://www.interworks.com/blogs/banderton/2009/07/09/nhibernatesearch-threading-issues-or-maybe-not  The Lucene.Net index only blocks concurrent write operations on the index. You can have as many threads as you like searching / reading from the index and they wont block - either on each other or on anyone doing a write however if you have two threads doing a write on the index at the same time then there is a chance that one of them will block on the other. If lucene tells you your index is locked then this means that either someone is currently writing to the index or (this sounds more likely) something that was writing to the index was killed during writing and so couldn't remove the lock. You should make sure that you properly dispose of any Lucene objects that write to the index as soon as they are done. To remove the lock manually there is a .lock file that you need to delete inside the Lucene directory (my big book of Lucene is not near me at the moment so I don't know exactly where it is but doing a search for ""lock"" or "".lock"" in the Lucene directory should find it) The handles that w3wp.exe had on this directory were probably the handles owned by the threads reading from the lucene index - although these will prevent you from deleting the directory they shouldn't prevent you from searching or writing to the index. Great thanks for the explanation :) The part thats doing the writing is the NH.Search FullTextIndexEventListener are you saying its likely that this is not threadsafe and two writes are occuring at the same time and one is getting blocked and then not properly disposed? hmmm looking at the source it does appear to take threading into account (doesn't mean its bug free). Any suggestions on how I can debug this? I don't see any reason (in my code) for an index write to fail"
375,A,"Indexing lucene document with different analysers Is it okay to index the lucene documents with two different analysers? Like i need to support both case-sensitive and case-insensitive search. So wondering if I can use two analysers for same document. writer.addDocument(docnew StandardAnalyzer(Version.LUCENE_30)); writer.addDocument(docnew custom_analyser); I am planning to have a custom analyser that supports all filter the standard analyser does except for lowercase filter. While I try to search results from indices I think we might end up getting duplicates.. Any comment/ideas? EDIT: @Simon Analyzer defaultAnalyzer = new StandardAnalyzer(Version.LUCENE_30); PerFieldAnalyzerWrapper wrapper = new PerFieldAnalyzerWrapper(defaultAnalyzer); wrapper.addAnalyzer(""CaseSensitiveContents"" new WhitespaceAnalyzer()); writer = new IndexWriter(FSDirectory.open(index) wrapper true new IndexWriter.MaxFieldLength(100)) doc.add(new Field(""contents"" parser.getReader() TermVector.YES)); doc.add(new Field(""CaseSensitiveContents"" parser.getReader() TermVector.YES)); writer.add(doc) Your example code would add two almost identical documents (except their casing) to your index. How about adding two fields to one document one being case sensitive one not? You can use the PerFieldAnalyzer for this. Potential problems may be that your html parser executes twice or you have huge amounts of data. Have you tried a profiler? @Simon - This work around for using case-sensitive/insensitive searches has slowed down the indexing and searching time its noticeable. Is there a way to make it faster? @Simon I am getting a stream closed exception when I try to add same contents to two different feilds...how do I fix that? It sounds like you're adding the same token stream twice? I've never seen this error when working with pure strings. I would need to see some code to debug this preferably where you add your Fieldable (usually Field). I have added some part of my code here No I haven't tried profiler between I am converting all data from HTML parser to text and passing the text to field. I can't create instances of HTML parser that way. I think thats causing the delay. I think converting the reader to text is consuming a lot of time Well..the parser method is a HTML parser that parses the HTML files. It rips off the HTML tags and gives the reader stream. do you think it would be possible to read the stream into a different reader obj and pass that stream to the field? Try first creating two instances of the html parser one for each field. That should show if this is the problem or something else is acting up. I would look into your calls to parser.getReader(). I do not know the inner workings of that method but I would guess that it returns the same reader/stream twice. It is read from when inserting 'contents' and disposed of when it's time to read data for the 'CaseSensitiveContents'. Is it possible to index the strings directly instead? Like `new Field(""..."" parser.getText() TermVector.YES)` or something?"
376,A,"can't find any results from valid index with either PhraseQuery or WildcardQuery? For some reason I can't find any results from my valid index of 3552 items. Please see the code below followed by the console output of the program when I run it. 3552 is the number of indexed documents. /c:/test/stuff.txt is the correct indexed path that is retrieved from document 5 as a test. And all the text at the bottom is the full text (in XML type output) of the test file. What am I missing that my simple query does not produce results? Maybe my WildcardQuery syntax is bad? I thought this would be inefficient (due to the wildcard at the beginning and end) but that it would at least return this document from the index... import java.io.File; import java.io.IOException; import org.apache.lucene.document.Document; import org.apache.lucene.document.Fieldable; import org.apache.lucene.index.CorruptIndexException; import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.Term; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.ScoreDoc; import org.apache.lucene.search.TopDocs; import org.apache.lucene.search.WildcardQuery; import org.apache.lucene.store.FSDirectory; public class Searcher { /** * @param args * @throws IOException * @throws CorruptIndexException */ public static void main(String[] args) throws CorruptIndexException IOException { System.out.println(""Begin searching test...""); IndexSearcher searcher = new IndexSearcher(FSDirectory.open(new File(args[0]))); // termContainsWildcard is shown to be true here when debugging // numberOfTerms is 0 WildcardQuery query = new WildcardQuery(new Term(""contents"" ""*stuff*"")); System.out.println(""Query field is: "" + query.getTerm().field()); System.out.println(""Query field contents is: "" + query.getTerm().text()); TopDocs results = searcher.search(query 5000); // no results returned :( System.out.println(""Total results from index "" + args[0] + "": "" + results.totalHits); for (ScoreDoc sd : results.scoreDocs) { System.out.println(""Document matched. Number: "" + sd.doc); } System.out.println(); System.out.println(""Begin reading test...""); // now read from the index to see if I am crazy IndexReader reader = IndexReader.open(FSDirectory.open(new File(args[0]))); // correctly shows the number of documents in the local index System.out.println(""Number of indexed documents: "" + reader.numDocs()); // pick out a random small document and check its fields Document d = reader.document(5); for (Fieldable f : d.getFields()) { System.out.println(""Field name is: "" + f.name()); System.out.println(new String(f.getBinaryValue())); } } } CONSOLE OUTPUT WHEN RUN Begin searching test... Query field is: contents Query field contents is: *stuff* Total results from index C:\INDEX2: 0 Begin reading test... Number of indexed documents: 3552 Field name is: path /c:/test/stuff.txt Field name is: contents <?xml version=""1.0"" encoding=""UTF-8""?> <html xmlns=""http://www.w3.org/1999/xhtml""> <head> <meta name=""Content-Length"" content=""8""/> <meta name=""Content-Encoding"" content=""UTF-8""/> <meta name=""Content-Type"" content=""text/plain""/> <meta name=""resourceName"" content=""stuff.txt""/> <title/> </head> <body> <p>stuff &#13; </p> </body> </html> You might try using Luke to run your queries & test some different queries. You can also use Luke to browse the indexed terms which might give you a clue as to what's going on. The code you used to index documents might also give some hints: for example are your fields indexed? You are getting a binary value out of contents which may mean that it was never tokenized and thus indexed. I was able to figure this out thanks to your Luke suggestion! You were correct-- binary fields that were not actually tokenized. Quite confusing for a newbie.  By default prefix wildcard queries (wildcard queries with a leading *) are disabled in Lucene. See the Lucene FAQ for more info. If you want to enable prefix wildcard queries try: QueryParser.setAllowLeadingWildcard(true) Thanks for the answer...is this for version 2 lucene? I am running 3.1.0 and do not see this as a static method =( for what it is worth-- removing the second wildcard (so that WildcardQuery query = new WildcardQuery(new Term(""contents"" ""*stuff"")) is what is shown still shows termContainsWildcard equal to true for query while debugging. That suggests to me that it is recognizing the wildcard at least."
377,A,"Prevent KeywordTokenizer from creating multiple key-value pairs I use the Lucene java QueryParser with KeywordAnalyzer. A query like topic:(hello world) is broken up in to multiple parts by the KeywordTokenizer so the resulting Query object looks like this topic:(hello) topic:(world) i.e. Instead of one I now have two key-value pairs. I would like the QueryParser to interpret hello world as one value without using double quotes. What is the best way to do so? Parsing topic:(""hello world"") results in a single key value combination but using double quotes is not an option. I am not using the Lucene search engine. I am using Lucene's QueryParser just for parsing the query not for searching. The text Hello World is entered at runtime by the user so that can change. I would like KeywordTokenizer to treat Hello World as one Token instead of parsing splitting it in to two Tokens. QueryParser qp = new QueryParser(""unknown"" new KeywordAnalyzer()); qp.setDefaultOperator(QueryParser.AND_OPERATOR); Query query = qp.parse(aString); aString comes from the user of the web application. If aString is topic:(hello world) then I get two Terms like topic:hello topic:world The user of my web application can enclose aString in double quotes like this topic:(""hello world""). That would give me one Term object to work with instead of two. But that Term is not perfect. It automatically chops off the double quotes and the resulting key:value will look like topic:hello world without the double quotes. I do not want the Query Parser to remove double quotes entered by the user because my non-Lucene search engine relies on these double quotes to perform an exact phrase search Could you please add a code snippet specifying how you call the query parser? I believe you can write a custom query parser yourself or add the quotes as a preprocessing stage. If not please explain why this is not so. You can construct the query directly as follows. This preserves the space. Query query = new TermQuery(new Term(""field"" ""value has space"")); If you print query as System.out.println(query); you will see the following result. field:value has space Thanks. This is not quite what I need.  You'll need to use a BooleanQuery. Here's a code snippet using the .NET port of Lucene. This should work with both the KeywordAnalyzer and the StandardAnalyzer. var luceneAnalyzer = new KeywordAnalyzer(); var query1 = new QueryParser(""Topic"" luceneAnalyzer).Parse(""hello""); var query2 = new QueryParser(""Topic"" luceneAnalyzer).Parse(""world""); BooleanQuery filterQuery = new BooleanQuery(); filterQuery.Add(query1 BooleanClause.Occur.MUST); filterQuery.Add(query1 BooleanClause.Occur.MUST); TopDocs results = searcher.Search(filterQuery); I am not using the Lucene search engine. I am using Lucene's QueryParser just for parsing the query not for searching. The text hello world is entered at runtime by the user so that can change. I just want KeywordTokenizer to treat Hello World as one Token instead of parsing splitting it in to two Tokens. Does not exactly answer the question but I am selecting it because of the beautiful description and working code that you have provided. It should help others getting started with Lucene Query Parser. Would be better if you accepted an answer to the question - I just got confused!"
378,A,"Lucene Boolean value search with boolean query There is a field say XYZ with value as either TRUE or FALSE. i am searching as following +Contents:risk +XYZ:TRUE is it legal to search like that? i tried but it showed me results with FALSE value too. What was more amazing is that i searched by +XYZ:[TRUE TO TRUE] and it worked. can some one tell me what exactly is my mistake? Your query is correct. Since you have obfuscated the field name I am not sure if the actual field name for XYZ is correct. Field names are case sensitive in Lucene. You may wish to verify that. thank for hint; let me double check if its because of case sensitivity. it's perfectly fine to search like that. Did you ensure that your analyzers are correct? I am using StandardAnalyzer. I had indexed with Lucene.NET v2.9 and searching with SOLR.NET. But before calling SOLR to search i made a booleanQuery myself. Following is the configuration in SOLR Let me know if i am missing something in this; To add to above i used StandardAnalyzer as : analyzer = new Lucene.Net.Analysis.Standard.StandardAnalyzer(new string[] { """" });"
379,A,"Find total term count in Solr index I need to figure out how many unique terms are in our Solr (Lucene) index. And by unique terms I dont mean by document. I mean total unique terms from all documents in the index. The Solr admin console shows how many documents but not total terms. I found a tool called Luke (Lucene Index Toolbox) that will give you that info but its a UI tool and my Sorl index is on a Ubuntu server so I cant run it. Any Ideas? do you need to do this programmatically? what do you intend to do with that information? You can use the LukeRequestHandler. You should see the tag ""numTerms"" in the xml returned. Thanks. I ended up running Luke over X11 (like a super slow terminal service) and got the info I needed before I saw your post. Glad Solr put luke into its admin features."
380,A,Filtering by date range in Lucene I know the title might suggest it is a duplicate but I haven't been able to find the answer to this specific issue: I have to filter search results based on a date range. Date of each document is stored (but not indexed) on each one. When using a Filter I noticed the filter is called with all the documents in the index. This means the filter will get slower as the index grows (currently only ~300000 documents in it) as it has to iterate through every single document. I can't using RangeQuery since the date is not indexed. How can I apply the filter AFTER only on the documents that are the results of the query to make it more efficient? I prefer to do it before I am handed the results not to mess up the scores and collectors I have. Not quite sure if this will help but I had a similar problem to yours and came up with the following (+ notes): I think you're really going to have to index the date field. Nothing else makes any sense in terms of querying/filtering etc. In Lucene.net v2.9 range querying where there are lots of terms seems to have got terribly slow compared to v2.9 I fixed my speed issues when using date fields by switching to using a numeric field and numeric field queries. This actually gave me quite a speed boost over my Lucene.net v2.4 baseline. Wrapping your query in a caching wrapper filter means you can hang onto the document bit set for the filter. This will also dramatically speed up subsequent queries using the same filter. A filter doesn't play a part in the scoring for a set of query results Joining your cached filter to the rest of your query (where I guess you've got your custom scores and collectors) means it should meet the final part of your criteria So to summarise: index your date fields as numeric fields; build your queries as numeric range queries; transform these into cached filter wrappers and hang onto them. I think you'll see some spectacular speedups over your current index usage. Good luck! p.s. I would never second guess what'll be fast or slow when using Lucene. I've always been surprised in both directions!  First to filter on a field it has to be indexed. Second using a Filter is considered to be the best way to restrict the set of document to search on. One reason for this is that you can cache the filter results to be used for other queries. And the filter data structure is pretty efficient: it is a bit set of documents matching the filter. But if you insist on not using filters I think the only way is to use a boolean query to do the filtering. Is it a bit set for documents matching the filter (which means you have to yield all documents) or a bit set for the Terms matching the filter? I guess caching would be possible if it was on Terms. It is a bit set of documents matching the filter. It allows to search on the same subset of documents for another query when the same filter is used. Pascal are there any strategies for keeping this filter BitSet up to date assuming documents change and new documents are added and some deleted?
381,A,"Combining Lucene's WildcardQuery with FuzzyQuery Using Lucene.Net 2.4.0 is there some kind of built-in support for joining the results of two different queries that target the same index similar to the support for targeting two or more indexes with a single query? I'm looking for ways to support both trailing wildcard and fuzzy searches without forcing users to choose one or the other. I could achieve this by executing a wildcard query and a fuzzy search sequentially and then manually merge the two results and sort by the score of the individual documents in hopes that their relative scores will make sense. Is there another way? To clarify: queries such as ""apoca"" and ""appockalypze"" should both produce a hit on ""Apocalypse Now"" given such a document exists in the index and increasing the fuzzyness from 0.5 to 1 is not really an option. you could join two or more queries with BooleanQuery Yeah that'd probably work. I mean why turn a perfectly straight-forward query into something complicated. I feel silly :P"
382,A,"Does Lucene Support Unicode? I am building a full text search facility for my website coded in asp.net mvc with mysql database. This website is for a non-english language. I have started work on it using Lucense as the engine for searching the text but I can't find any info on whether it supports unicode? Does anyone have any information on whether Lucene supports Unicode? I don't want a nasty surprise.. Also links to beginner articles on implementing lucene.net will be appreciated. I'm highly recommend Lucene in Action http://www.amazon.com/Lucene-Action-Otis-Gospodnetic/dp/1932394281 Lucene does support unicode but there are limitations. For example some document readers don't support unicode. Also lucene does things like pluralize or un-pluralize words. When you are using a foreign language some of that goes away.  Yes. It fully support unicode. But for analyzing you should explicitly assign appropriate stemmers and correct stopwords. As for sample. Here is copy from our last project directory = new RAMDirectory(); analyzer = new StandardAnalyzer(version new Hashtable()); var indexWriter = new IndexWriter(directory analyzer true IndexWriter.MaxFieldLength.UNLIMITED); using (var session = sessionFactory.OpenStatelessSession()) { organizations = session.CreateCriteria(typeof(Organization)).List<Organization>(); foreach (var organization in organizations) { var document = new Document(); document.Add(new Field(""Id"" organization.ID.ToString() Field.Store.YES Field.Index.NOT_ANALYZED_NO_NORMS)); document.Add(new Field(""FullName"" organization.FullName Field.Store.NO Field.Index.ANALYZED_NO_NORMS)); document.Add(new Field(""ObjectTypeInvariantName"" typeof(Organization).FullName Field.Store.YES Field.Index.NOT_ANALYZED_NO_NORMS)); indexWriter.AddDocument(document); } var persistentType = typeof(Order); var classMetadata = DbContext.SessionFactory.GetClassMetadata(persistentType); var properties = new List<PropertyInfo>(); for (int i = 0; i < classMetadata.PropertyTypes.Length; i++) { var propertyType = classMetadata.PropertyTypes[i]; if (propertyType.IsCollectionType || propertyType.IsEntityType) continue; properties.Add(typeof(Order).GetProperty(classMetadata.PropertyNames[i])); } orders = session.CreateCriteria(typeof(Order)).List<Order>(); var idProperty = typeof(Order).GetProperty(classMetadata.IdentifierPropertyName); foreach (var order in orders) { var document = new Document(); document.Add(new Field(""Id"" idProperty.GetValue(order null).ToString() Field.Store.YES Field.Index.NOT_ANALYZED_NO_NORMS)); document.Add(new Field(""ObjectTypeInvariantName"" typeof(Order).FullName Field.Store.YES Field.Index.NOT_ANALYZED_NO_NORMS)); foreach (var property in properties) { var value = property.GetValue(order null); if (value != null) { document.Add(new Field(property.Name value.ToString() Field.Store.NO Field.Index.ANALYZED_NO_NORMS)); } } indexWriter.AddDocument(document); } indexWriter.Optimize(true); indexWriter.Commit(); return indexWriter.GetReader(); } I'm querying Organization objects from NHibernate and put them into Lucene.NET Here is simple search var searchValue = textEdit1.Text; var parser = new QueryParser(version ""FullName"" analyzer); parser.SetLocale(new CultureInfo(""ru-RU"")); Query query = parser.Parse(searchValue); var indexSearcher = new IndexSearcher(directory true); var docs = indexSearcher.Search(query 10); lblSearchTotal.Text = string.Format(totalPattern docs.totalHits organizations.Count() + orders.Count); resultPanel.Controls.Clear(); foreach (var found in docs.scoreDocs) { var document = indexSearcher.Doc(found.doc); var objectId = document.Get(""Id""); var objectType = document.Get(""ObjectTypeInvariantName""); if (resultPanel.Controls.Count > 0) { var labelSeparator = CreateSeparatorLabelControl(); resultPanel.Controls.Add(labelSeparator); } var labelCard = CreateFoundLabelControl(); resultPanel.Controls.Add(labelCard); var organization = organizations.Where(o => o.ID.ToString() == objectId).FirstOrDefault(); if (organization != null) { labelCard.Text = string.Format(""<b>{0}</b></br>{1}"" organization.AccountNumber organization.FullName); labelCard.Tag = organization; //labels[count].Text = string.Format(""<b>{0}</b></br>{1}"" organization.AccountNumber organization.FullName); //labels[count].Visible = true; } else { labelCard.Text = string.Format(""Найден объект типа '{0}' с идентификатором '{1}'"" objectType objectId); labelCard.Tag = mainForm.GetObject(objectType objectId); } labelCard.Visible = true; //count++; } also NHibernate Search can be used.  Yes Lucene supports unicode because it stores strings in UTF-8 format. http://lucene.apache.org/java/3_0_3/fileformats.html Chars Lucene writes unicode character sequences as UTF-8 encoded bytes. String Lucene writes strings as UTF-8 encoded bytes. First the length in bytes is written as a VInt followed by the bytes. String --> VInt Chars"
383,A,"Wildcards in Lucene Why does the wildcard query ""dog#V*"" fail to retrieve a document that contains ""dog#VVP""? The following code written in Jython for Lucene 3.0.0 fails to retrieve the indexed document. Am I missing something? analyzer = WhitespaceAnalyzer() directory = FSDirectory.open(java.io.File(""testindex"")) iwriter = IndexWriter(directory analyzer True IndexWriter.MaxFieldLength(25000)) doc = Document() doc.add(Field(""sentence"" ""dog#VVP"" Field.Store.YES Field.Index.ANALYZED)) iwriter.addDocument(doc) iwriter.close() directory.close() parser = QueryParser(Version.LUCENE_CURRENT ""sentence"" analyzer) directory = FSDirectory.open(java.io.File(""testindex"")) isearcher = IndexSearcher(directory True) # read-only=true query = parser.parse(""dog#V*"") hits = isearcher.search(query None 10).scoreDocs print query_text + "":"" + "" "".join([str(x) for x in list(hits)]) Output is: dog#V*: It doesn't return anything. I see the same behaviour for dog#VV* or with separators characters other than ""#"" (I tried ""__"" and ""aaa""). Interestingly the following queries work: dog#??? dog#*. If you'd looked carefully at the result of parser.parse(""dog#V*"") you'd have seen sentence:dog#v* Note the lowercase v! To avoid the automatic lowercasing of terms in a wildcard query you'll have to do parser.setLowercaseExpandedTerms(False) before parsing query strings. I have no idea why the default is to lowercase. Thanks! That solves my current problem. As far as I understand LowerCase is just another filter in WhitespaceAnalysis. I will try and see what happens if I use my own custom Analysis class (which will employ a TurkishLowerCase) but is there anyone who could explain the mechanism and rationale behind this default? @Amaç - Note that lowercasing is NOT part of WhiteSpaceAnalyzer but rather a default behavior of the query parser. Therefore if you want to change lowercasing you should either set the flag as Jonathan suggested or write your own query parser class. And Yuval's comment corrects my misunderstanding about lowercasing behavior. Everything is clear now thank you again."
384,A,"Query Builders - Must Not / Should Not I am new to the search-engine scene and I was wondering if anyone might be able to help me clarify the Must/MustNot and Should/ShouldNot search queries. My understanding is as follows: The Must/MustNot queries are absolute definitions (ie the result must/must not contain a specified field) The Should/ShouldNot queries are relative definitions (ie in some scenarios it should/should not contain a specified fields) If this is correct could someone please provide me with an example of when you would use Should/ShouldNot...and if my understanding is completely wrong - would someone be kind enough to explain it for me (or point me to a good site)? Thanks! Ps. I am using elastic-search (based on Lucene) - but any examples / explanations are welcome Your understanding is right. SHOULD NOT results get lower weight when found while MUST NOT never included. SHOULD NOT is used when the term is likely belongs to a different domain but could in some cases still be on the same document with the one I'm looking for. For instance Google knows I'm a Java programmer. When I ask ""ant"" the first result would be ""Apache Ant"". I may use MUST NOT as ""-Apache"" or I may use SHOULD NOT ""Apache"" just in case some page describes a special place of ants in Apache natives worldview. Or for instance I'm looking for a free software to do a task; still I would consider a paid one if it is not expensive or they have a personal license. Therefore I'd use SHOULD NOT ""buy now"" and the results would include paid software but lower in the list. Excellent great explanation. Thank you very much!"
385,A,"Can you read from a lucene index while updating the index I can't find a straightforward yes or no answer to this! I know I can send multiple reads in parallel but can I query an index while a seperate process/thread is updating it? It's been a while since I used Lucene. However assuming you are talking about the Java version the FAQ has this to say: Does Lucene allow searching and indexing simultaneously? Yes. However an IndexReader only searches the index as of the ""point in time"" that it was opened. Any updates to the index either added or deleted documents will not be visible until the IndexReader is re-opened. So your application must periodically re-open its IndexReaders to see the latest updates. The IndexReader.isCurrent() method allows you to test whether any updates have occurred to the index since your IndexReader was opened. I don't know how I missed this :-)  See also Lucene's near-real-time feature which gives fast turnaround upon changes (adds deletes updates) to the index to being able to search those changes. For example using near-real-time you could make changes to the index and then reopen the reader every few seconds."
386,A,"Lucene.NET query + highlighting I am using Umbraco and came across Lucene. I found a lot of code and articles on Lucene but I still can't build an acceptable search. I have a number of fields to search from eg. ""nodeName"" and ""bodyText"" What I need: When I search for ""men shoes"" it should only return results that have both ""men"" and ""shoes"" but also return a page where the nodeName only has ""shoes"" and the bodyText only has ""men"". When I search for ""shoes"" I want results containing ""shoe"" or ""shoes."" but not ""hoes"" if possible Boost the nodeName field Get a snippet of bodyText that contains the matched word(s) Highlight the matched words on both the page name and the snippet of the bodyText Has anyone ever done this? Thanks. How do I use that with Umbraco...? Yes Lucene does all you need and *much* more. However since you appear to be a newbie I'd recommend using Apache Solr (which is a search server built around Lucene and offers many features out of the box without having to tinker with the innards. This might get you started. var manager = ExamineManager.Instance; var searcher = manager.SearchProviderCollection[""YOURSearcher""]; var query = manager.SearchProviderCollection[""YOURSearcher""].CreateSearchCriteria(BooleanOperation.Or) .Field(""nodeName"" keywords.Boost(10)) .Or().Field(""nodeName"" keywords.Fuzzy()) .Or().Field(""bodyContent"" keywords.Boost(5)) .Or().Field(""otherField"" keywords.Boost(3)); var results = searcher.Search(query.Compile()); Thanks Kieran! What is keywords? It's not string is it? Because string doesn't have Boost method. It is an extension to string you must add using Examine.LuceneEngine.SearchCriteria; When I search for multiple words it never returns any results  The code by Jonathan Lathigee works it's the most google-like I could find so far http://our.umbraco.org/forum/developers/extending-umbraco/19329-Search-multiple-fields-for-multiple-terms-with-examine?p=0"
387,A,integrate wordnet with solr I am trying to integrate wordnet api in to Apache solr. But it is not seems to be working and there is no good documentation as well. Could you please post me the steps if any body has experience on it? There are more than one way to do this: 1) https://issues.apache.org/jira/browse/LUCENE-2347 2) https://gist.github.com/562776 These are simple Java classes which extract the synonyms from WordNet's prolog file - more or less the same way. Hope this helps. Péter
388,A,"Unit test for Lucene indices I'm working on legacy code that builds an index of popular terms in another index. There are no unit tests in place and the indexing process is a pain to wait for because the first index takes so long to build. I want to structure the second (popular term) index differently. Is there a best practice for testing to see if a Lucene index is being created properly? EDIT>> Per @Pascal's advice I'm using a RAMDirectory then to test the index I just wrote I set up an indexReader and iterate through the term results printing out each term to make sure the data looks alright. Code: IndexReader reader = IndexReader.open(dir2); TermEnum terms = reader.terms(); System.out.println(""Here come the terms!""); while (terms.next()){ if (terms.term().field().equals(""FULLTEXT"")){ System.out.println(terms.term()); } } int numDocs = reader.maxDoc(); System.out.println(""Number of Docs: "" + numDocs); If the index is really large I let it run for a bit then just stop it midway through. Also Luke is a great tool for inspecting the index if you want to be more thorough... I'm just looking for something fast. Any other ideas are welcome! When unit-testing Lucene index I often use the RAMDirectory as it is quick to build. This works really well thanks Pascal! Good idea that way it also doesn't persist right? Yes it won't persist to disk but it will stay in memory for the time of the test."
389,A,"Lucene : Use SpanTermQuery to get results for words with special characters Is it possible to search for results in Lucene for non-character words for example if I am trying to find results for ""word-processing"" or ""foo-bar"". It doesn't look like they are considered as single term while using SpanTermQuery. I get results for it using QueryParser but not SpanTermQuery. I am just wondering how it works Any comments/ Ideas on how to have SpanTermQuery work for it? I would recommend taking a look at how your field's Tokenizers and Analyzers are configured. Read the javadocs for the existing out of the box Tokenizers/Analyzers to see if one of them meet your needs. If one doesn't meet your needs it's pretty easy to extend and create your own Tokenizer and/or Analyzer. http://wiki.apache.org/lucene-java/LuceneFAQ#How_do_I_write_my_own_Analyzer.3F http://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/analysis/Analyzer.html http://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/analysis/Tokenizer.html"
390,A,Is there a way to index CHM files in Lucene? Can anyone please suggest me a method by which a chm file can be indexed in such as pdfbox for pdf. @dethly and @ffriend  Both the Answers are appropriate for me. So Both are two different approaches so I'm in a dilemma to choose the Accepting Answer. Apache Tika is more common to use with Lucene I just didn't know about their support for CHM. So accept deathy's answer please. If you have also other document formats which you need to index you might find a better and more general solution in Apache Tika They just added a CHM Parser recently (for reference: Support of CHM Format) and it will be in the next version. thank u . I will have a look :)  If you're talking about Microsoft Compiled HTML Help files you can just extract text from them with JChm and then index it in a normal way. thnx. I will have a look. :) Be careful. There might be a binary and a textual (.xml stored as .hhk) index and they might not contain the same things. I used ChmParser amd used its retrieve file and have put some workaround. It seems to work well  and the .hhc issue is resolved . Thnx again
391,A,"What is wrong with this solr query I want to find all the documents that is similar to a specific document within solr. I have installed solr and made some queries. The query I am trying to make gives an error which I cannot make out or research on the internet. Can you give me some light on this? I am using solrnet client but if solrnet is not appropriate for this type of query I will gladly use pure solr and read the XML. Here is the query I am using: http://192.168.1.10:8080/solr/mlt?q=id:12&mlt.fl=content&mlt.mindf=1&mlt.mintf=1 here is my schema xml here  <fields> <field name=""id"" type=""string"" indexed=""true"" stored=""true"" /> <field name=""title"" type=""string"" indexed=""true"" stored=""false""/> <field name=""content"" type=""text_general"" indexed=""true"" stored=""false""/> </fields> Here is the error I am receiving: Yes I can send queries just fine. But when I try to use the MoreLikeThis handler it gives the error. Can you send _any_ queries to your SOLR instance ? What URL (example) works ? It seems you haven't registered the MLT handler in solrconfig.xml. A simple registration looks like this: <requestHandler name=""/mlt"" class=""org.apache.solr.handler.MoreLikeThisHandler""/> whoa..this really worked. I can't believe it. I have the the solr book and follwed the directions but i think it needs to be updated. Thanks alot man - I really appreciate it."
392,A,"What are my options for a search engine database on windows I have a project to create a high traffic search engine similar to altavista.com. The windows .NET C# will be used for the project. I am looking for a good search engine database that can handle a very high load. I have taken a look at lucene and sql server 2008. I have read that lucene tends to get corrupt when the load is very high. So I am considering sql server 2008 but I am uncertain sql server can handle a very high load over gigabytes of data. The database will be given a chunk of data and needs to fetch similar data elsewhere in the table. it will act just the similar questions area on stackoverflow. It will need to search over millions of rows. Is sql server 2008 and lucene my only options ""I have read that lucene tends to get corrupt when the load is very high"" - care to qualify that statement with a link or two? BTW millions of rows is not large... I have used Lucene sucessfully with high volume datas and i dint face data corruption problems. But if you are worried about the standalone lucene you can try apache solr instead its a open-source search server based on the Lucene Solr is the popular blazing fast open source enterprise search platform from the Apache Lucene project. Its major features include powerful full-text search hit highlighting faceted search dynamic clustering database integration and rich document (e.g. Word PDF) handling. Solr is highly scalable providing distributed search and index replication and it powers the search and navigation features of many of the world's largest internet sites. There is a .net client for solr in google code solr.net.. You can try this"
393,A,"Lucene adding additional filter returns no results I am attempting query some results using a Boolean Query. However the query does not return any results. Here is the FilterQuery I am running. This returns no results even though the field foo contains bar and the field foo3 contains bar3. And I have triple checked my fields to make sure that the fields do exist in the index. +(foo:bar foo2:bar2) +foo3:bar3 Now If I remove the +foo3:bar3 from the query I get results back correctly. Also foo3:bar3 is being added programatically so I am not parsing it. Here is some relevant code //This code creates the first part of the query. MultiFieldQueryParser mfqp = new MultiFieldQueryParser(Lucene.Net.Util.Version.LUCENE_29SearchFields analyzer); Query q = mfqp.Parse(query); BooleanQuery filterquery = new BooleanQuery(); filterquery.Add(qBooleanClause.Occur.MUST); //This code creates the second part of the query Query fq = new TermQuery(new Term(""foo3""""bar3"")); filterquery.Add(fq BooleanClause.Occur.MUST); //Perform the search ScoreDoc[] hits = isearch.Search(filterquery null ResultsToReturn).scoreDocs; Just for reference I am current setting the fields to be analyzed and the vector is set to With_positions_offsets does +foo3:bar3 by itself return any documents? It does return documents if I modify the way the search works. If the MFQP passes in the term to the booleanquery then it works. but if I use the termquery it fails I changed from using a TermQuery to using a QueryParser which seems to have fixed the issue."
394,A,hibernate search without database Is it possible to use hibernate-search only for it's annotations (bean => document/document => bean mapping) without using a database at all? If so are there any online samples that show basically how to set this up? I found the following: http://mojodna.net/2006/10/02/searchable-annotation-driven-indexing-and-searching-with-lucene.html but I'd prefer hibernate-search if it supports my use case. I don't think that's possible because when you enable Hibernate search you are enabling that on a Entity and that Entity has references to the table and the search index. that's what I thought...thanks.  Starman is correct Hibernate Search in version 3.4 is abstracting the search engine from Hibernate Core and the Infinispan Query is an integration example which works fine without a database. There would be no problems with Spring either but you'd need to make sure to send update event to the query engine so that the index doesn't get out of synch. When using Hibernate the advantage is that it transparently listens for changes to the database and applies them to the index at transaction commit so the index is always in synch (or close if configuring Search to use async backends). I'd suggest to look into the code of Infinispan Query as it's very small and just delegating calls to expose an Infinispan flavoured API. Most of the code is tests or integration to properly manage the lifecycle of the engine: start and stop it together with Infinispan.  Hibernate search 3.4 has decoupled the query engine from Hibernate Core. For instance Hibernate Search is reused to implement queries with Infinispan. I don't know if the code is packaged so that you could use HS with let's say Spring and JDBCTemplate (something I would like to do). That's a lead I will investigate later but maybe you can check it out...
395,A,custom synonyms support for lucene can any one direct me on creating the custom synonyms using lucene in java? If you use Solr you can use a SynonymFilterFactory. Otherwise Lucene In Action has an example of how to write a synonym filter if you want to do it in pure Lucene. Xodarap - Will I be able to find the number of hits in a document using lucene? @sharma: yes see [this question](http://stackoverflow.com/questions/1920726/get-search-word-hits-number-of-occurences-per-document-in-lucene). If you have additional questions please ask them separately rather than as comments. @sharma: the format of the synonyms text file is given in the page I linked. How do I convert the synonyms.dat file into lucene index for synonyms? The synonym.dat is simple text file having customs synonyms in it. All synonyms are comma separated and next set is followed in next line.
396,A,What lucene analyzer can be used to handle Japanese text? Which lucene analyzer can be used to handle Japanese text properly? It should be able to handle Kanji Hiragana Katakana Romaji and any of their combination. You should probably look at the CJK package that is in the contrib area of Lucene. There is an analyzer and a tokenizer specifically for dealing with Chinese Japanese and Korean. I've never used the CJK analyzer myself so cannot say. You could try asking on the Lucene mailing list (http://lucene.apache.org/java/docs/mailinglists.html#Java User List) for more specific help - there are people who are very experienced with Lucene on that list. The CJK Analyzer seems to be a naive way of searching things and from previous experience does not seem to provide very relevant search results. Is there anything I need to do specifically to make CJK Analyzer work like modify some weights or something ? Thanks  I found lucene-gosen while doing a search for my own purposes: Their example looks fairly decent but I guess it's the kind of thing that needs extensive testing. I'm also worried about their backwards-compatibility policy (or rather the complete lack of one.) We didn't use lucene-gosen but we did use gosen. So I'm accepting this answer (since it's close enough and the project does look interesting). CJK does a very naive searching wherein it just matches characters and not words unlike gosen (which uses a dictionary for proper parsing).
397,A,"Concurrency in Lucene.NET. I want to use Lucene.NET for fulltext search shared between two apps: one is an ASP.NET MVC application and the other one is a console application. Both applications are supposed to search and update index. How the concurrency should be handled? I found a tutorial on ifdefined.com where the similar use case is discussed. My concern is that locking will be a big bottleneck. PS: Also I noticed that IndexSearcher uses a snapshot of index and in the tutorial mentioned above searcher is created only when index is updated. Is this a good approach? Can I just create a regular searcher object at each search and if yes what is the overhead? I found a related question Does Lucene.Net manage multiple threads accessing the same index one indexing while the other is searching? what claims that interprocess concurrency is safe. Does it mean that it is are no race conditions for index? Also one very important aspect. What is the performance hit involved if let's say 10-15 threads are trying to update Lucene index via acquiring shared lock presented in this solution? After using it couple of months I have to add that opening index for search often can create OutOfMemory exception under high CPU and memory loads if query uses sorting. Cost of index opening operation is small (in my experience) but cost of GC can be quite high. I also have a lucene search index that's used by multiple clients I solve this issue by making the 'Lucene Search Service' a separate web service running in its own App Domain. As both clients hit the same web service to search or update the index I can make it thread-safe with locks on Lucene's Indexers. Other than that if you want to keep it in process I suggest using file locks to make sure only one client can write to the index. To get it to use a a new index I create one on the side and then tell the Search Index service to swap over to use the new index by safe disposing of any Indexers on the current index and renaming directories e.g. Index.Current > Index.Old Index.New > Index.Current Yeah I just meant to create an empty file called something like 'write.lock' on the file-system to indicate you are writing to the index. When you are finished writing to the index you just remove it. Then you just have to make sure that only the process that created the lock can read/write to the index. Could you be more clear about file locks? So you are rebuilding a new index and then making the switch to the new one and after that delete the old one? Thanks.  If you will have multiple writers in different processes and they will spend more than 10 seconds writing their changes to the index (which will cause waiting writers to timeout) then you can synchronize access across processes by using named Mutexes. Simply open/create a Mutex of the same global name in each application and use Mutex.WaitOne before writing and Mutex.ReleaseMutex after writing. var mut = Mutex.OpenExisting(""myUniqueMutexName""); // wrap in try..catch to create if non-existent mut.WaitOne(); try { // write logic } finally { // recover from write failure mut.ReleaseMutex(); } Probably better to make the Mutex a singleton since they're a little expensive to construct. Update (per comment): If the processes are on separate machines I think your only alternative is to layer your own filesystem locking (using old-fashioned lock files) to synchronize access. Since the built-in locking uses filesystem locks anyway I would actually recommend you just increase the IndexWriter timeout everytime you construct one. var iw = new IndexWriter(); iw.WRITE_LOCK_TIMEOUT = 60000; You can also just keep trying a specified number of times. var committed = false; var attempts = 0; while(!committed && attempts < 10) { try { // write logic committed = true; } catch (LockObtainFailedException) { attempts++; } } Thank you for your solution. I would be a good one but because of the infrastructure it cannot be applied because processes are running on different machines and are accessing Lucene index in a shared network folder. So mutex won't be able to block those processes. My bad though I didn't specify this in question. I'm sorry. I've updated my answer in response to your comments. Has anyone used this approach successfully?  First of all we have to define a ""write"" operation. A write operation will object a lock once you start a write operation and will continue until you close the object that is performing the work. Such as creating an IndexWriter and indexing a document will cause the write to object a lock and it will keep this lock until you close the IndexWriter. Now we can talk about the lock a little bit. This lock that is object is a file based lock. Like mythz mentioned earlier there is a file called 'write.lock' that is created. Once a write lock is objected it is exclusive! This lock causes all index modifying operations (IndexWriter and some methods from IndexReader) to wait until the lock is removed. Overall you and have multiple reads on an index. You can even read and write at the same time no problem. But there is a problem when having multiple writers. If one thread is waiting for the lock too long it will time out. 1) Possible Solution #1 Direct Operations If you are sure that your indexing operations are short and quick you may be able to just use the same index at the same time. Otherwise you will have to think about how you want to organize the indexing operations of the applications. 2) Possible Solution #2 Web Service Since you are working with a web solution it might be possible to create a web service. When implementing this web service I would dedicate a worker thread for indexing. I would create a work queue to contain the work and if the queue contained multiple jobs to do it should grab them all and do them into batch. This will solve all of the problems. 3) create another index then merge If the console application does heavy work on the index you may be able to look into having the console application you could create a seperate index in the console application and then merge the indexes at some safe scheduled time using IndexWriter.AddIndexes. from here you can do this in two ways you can merge with the direct index. Or you can merge to create a 3rd index and then when this index is ready replace the original index. You have to be careful in what your doing here as well to make sure that your not going to lock something in heavy use and cause a timeout for other write operations. 4) Index & Search multiple indexes Personally I think people need to separate their indexes out. This helps separates responsibilities of the programs and minimizes down time and maintained of having a single point for all indexes. For example if your console application is responsible for only adding in certain fields or your are kind of extending an index you could look separate the indexes out but maintain identity by using an ID field in each document. Now with this you can take advantage of the built in support for searching multiple indexes using the MultiSercher class. Or if your wanting there is also a nice ParallelMultiSearch class that can search both indexes at once. 5) Look into SOLR Something else that can help your issue of maintaining a single place for you index you could change your program to work with a SOLR server. http://lucene.apache.org/solr/ there is also a nice SOLRNET http://code.google.com/p/solrnet/ library that can be helpful in this situation. Although I'm not experienced with solr but i am under the impression that it will help you manage situation such as this. Also it has other benefits such as hit highlighting and searching for related items by finding items ""MoreLikeThis"" or provide spell checking. I'm sure there are other methods but these are all the ones that I can think of. Overall it your solution depends upon how many people are writing and how up to date the search index you need it to be. Overall if you can defer some operations for a latter time and do some batch operations in any situation will give you the most performance. My suggestion is to understand what your able to work with and go from there. good luck Wow. Thanks. I was thinking of a solution that is somehow related to 2_. In meantime I have other question: ""How many indexes can ParallelMultiSearch or MultiSercher support""?"
398,A,"Lucene field from TokenStream with stored values I have a field which needs to come from a token stream; it cannot be instantiated with a string and then analyzed into tokens. For example I might want to combine the data from multiple columns (in my RDBMS) into a single Lucene field but I want to analyze each column in its own way. So I cannot simply concat them all as a single string then analyze the resulting string. The problem I am running into now is that fields created from token streams cannot be stored which makes sense in the general case since the stream may not have an obvious string representation. However I know the string representation and I would like to store that. I tried adding the same field twice once with it being stored and having string data and once with it coming from a token stream but it seems that this can't be done. Apart from some hack like adding a field with a name of ""myfield__stored"" is there a way to do this? I am using 2.9.2. I found a way. You can sneak it in by instantiating it as a normal field but calling SetTokenStream later: Field f = new Field(Name StringValue Store Analyzed TV); f.SetTokenStream(TokenStreamValue); Because the reader/string value is only indexed if the token stream value is null the token stream value will be indexed. The store methods look at string/reader regardless of token stream so it will be this value which is stored."
399,A,"Lucene: delete from index based on multiple fields I need to perform deletion of the document from lucene search index. Standard approach : indexReader.deleteDocuments(new Term(""field_name"" ""field value"")); Won't do the trick: I need to perform the deletion based on multiple fields. I need something like this: (pseudo code) TermAggregator terms = new TermAggregator(); terms.add(new Term(""field_name1"" ""field value 1"")); terms.add(new Term(""field_name2"" ""field value 2"")); indexReader.deleteDocuments(terms.toTerm()); Is there any constructs for that? IndexWriter has methods that allow more powerful deleting such as IndexWriter.deleteDocuments(Query). You can build a BooleanQuery with the conjunction of terms you wish to delete and use that. Great thank you. @Avi do you know of an approach for updating a document based on multiple fields? the updateDocument() method only takes a Term for its first parameter."
400,A,"Lucene Javadoc package Does Lucene has separated jar with javadoc inside ? Everything in contrib section has separated javadoc jars but it look to me that core has none. Am I right? No it comes without jar for core javadocs. However you can easily make it by running ""jar"" utility from JDK on ""/docs/api/core"" folder from lucene-core-*.jar ."
401,A,"I'm trying to do a solr search such that results are shown if a certain field has ANY value I'm running a search with a type field. I'd like to show results of a certain type ONLY if two other field have values for them. So in my filter query I thought it would be(type:sometype AND field1:* AND field2:*) but wildcard queries can't start with the *. Use a range query to express ""field must have any value"" e.g.: type:sometype AND field1:[* TO *] AND field2:[* TO *]"
402,A,no segments* file found I need to access a lucene index ( created by crawling several webpages using Nutch) but it is giving the error shown above : java.io.FileNotFoundException: no segments* file found in org.apache.lucene.store.FSDirectory@/home/<path>: files: at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:516) at org.apache.lucene.index.IndexReader.open(IndexReader.java:185) at org.apache.lucene.index.IndexReader.open(IndexReader.java:148) at DictionaryGenerator.generateDict(DictionaryGenerator.java:24) at DictionaryGenerator.main(DictionaryGenerator.java:56) I googled but the reasons given were not matching the requirements. The fact that files are being shown ( the path) probably means that the directory is not empty. Thanks Basically the error message says that Lucene did not find the proper files in the index directory. I suggest checking the following: Verify the path of the index directory fits what you think it should be. Do the Nutch and Lucene versions used match? This may stem from a version difference. Is there a permissions issue? Can you read the files in the directory? Try looking at the index using Luke. If you cannot there is probably some corruption in the index. If all these do not help Please post the indexing part of the code. Like Yuval said make sure that the Java program that you use to read the index uses the same version of Lucene that Nutch used to create the index. I did all of them except the Nutch and Lucene versions.I was not aware that there has to be a compatibility between Lucene and Nutch . If it helps the lucene version is 2.2 . I can access the files. Infacti am running the java program in the same directory as the index . Also i checked the index using Luke and its definitely fine . Also the thing is that i just became a part of the project. The index is the result of an extensive crawl by Nutch . So  i do not have any indexing code. It was just a crawl .But i will still try to find out the exact picture. One thing i have observed is that the newer version of Nutch (1.1) generates 5 folders after a crawl while the data which i have has only 4( out of which segments is one) folders . Can that be an issue ?  Another hint as I was having the same error and found that after creating indexes I did not close IndexWriter and it proved very unforgiven. In my indexdirectory I have some .lock files and no segments or segments.gen files which is what Reader is looking for. See here #3 for details
403,A,"Fastest full text search today? spoiler : This is just another Lucene vs Sphinx vs whatever I saw that all other threads were almost two years old so decided to start again.. Here is the requirement : data size : max 10 GB. rows : nearly billions indexing should be fast searching should be under 0 ms [ ok joke... laugh... but keep this as low as possible ] In today's world which/what/how do I go about it ? edit : I did some timing on lucene and for indexing 1.8gb data it took 5 minutes. searching is pretty fast unless I do a a*. a* takes 400 ~ 500 ms. My biggest worry is indexing which is taking loooonnnnggg time and lot of resources!! you only have to do index on new data updated data deleted data not always the whole collection My biggest worry is indexing which is taking loooonnnnggg time and lot of resources!! Take a look at Lusql we used it once FWIW 100 GBdata from mysql on a decent machine took little more than an hour to index on filesystem(NTFS) Now if u add SSD or whatever ultra fast disk tecnnology you can bring it down considerably  Please check Lucene wiki for tips on improving Lucene indexing speed. This is quite succinct. In general Lucene is quite fast (it is used for real-time search.) The tips will be handy to figure out if you are missing out on something ""obvious."" I've done everything ""obvious"" by now :) just wanted to know if ""this"" IS the way to go :) And btw is the indexing time allright ? its 5 minutes to 1.8GB ? Size is somewhat inaccurate metric. Indexing 1.8G of plain text will be different from indexing 1.8G HTML (which you will parse and index extracted text.) You need to see if that is ""fast enough"" for your needs. If existing indexing speed falls short of your expectations you may wish to explore how to use Lucene in real-time environment. That is non-trivial. @Shrinath - your indexing speed is limited by how fast you can read off disk and how much that data needs to be processed before index insertion. @Richard : Agreed.. There is just a few String manipulations done before inserting that is adding to the time too... I will try to reduce manipulations but just wanted to be sure if there is a way to speed up lucene more..  I have no experience other than with Lucene - it's pretty much the default indexing solution so don't think you can go too wrong. 10GB is not a lot of data. You'll be able to re-index it pretty rapidly - or keep it on SSDs for extra speed. And of course keep your whole index in RAM (which Lucene supports) for super-fast lookups. I'm going to keep everything on clouds so I don't see anyone giving SSD like speeds there :( And btw whole data on RAM I can't take it for the app I am working on... It'd be like 1000 GB of unique data per computer so everything can't be brought into memory... OK - well the SSDs will only make diff wrt to building the index. BUt confused - you said max data size 10GB not 1000? Lol :D true not 1000 GB :) its only 10 GB... Check the edits now :) ok well 10GB you can put in RAM right? well its not that simple which for certain reasons I didn't specify in the post... There are going to be multiple indexes of 10 gb each... and there'll be multiple searchers going for every different index.. then how does this work ? that was my point... sorry for the confusion if it was only 10 GB you are 100% right..."
404,A,Can I use the same instance of IndexSearcher in Lucene.Net by multiple threads concurrently? I want to be able to search by multiple clients and index at the same time in Lucene.Net Is it possible and thread safe? Can the same instance of IndexSearcher be shared across threads? Yes.Very much. Even indexing is!  The index search CAN and SHOULD be shared across threads. The trick is deciding when to refresh your searcher with a new snapshot of the index. I wrote an article showing how I coded sharing a searcher across threads while at the same time making sure that the searcher was always using an up-to-date index. I'm not saying my solution is the best for everybody - I don't think it would be good for a website with a huge number of searches going on - but it's working fine for my low volume application. Here's the article: http://ifdefined.com/blog/post/Full-Text-Search-in-ASPNET-using-LuceneNET.aspx Thanks and by the way we use Bugtracker.Net :)  You can index and search concurrently but the changes you make to the index will not be visible to the searcher till you re-create the searcher. Searcher will have the snapshot of the index when you created the searcher object.
405,A,Are there any technologies that help develop website search? PROBLEM: I need to write an advanced search functionality for a website. All the data is stored in MySQL and I'm using Zend Framework on top. I know that I can write a script that takes the search page and builds an SQL query out of it but this becomes extremely slow if there's a lot of hits. Then I would have to get down to the gritty details of optimizing the database tables/fields/etc. which I'm trying to avoid if possible. Lucene: I gave Lucene a try but since it's a full-text search engine it does not allow any mathematical operators!! So if I wanted to get all the records where field_x > 5 there is no way to do it (correct?) General Practice? I would like to know how large sites deal with this dilemma. Is there a standard way of doing this that I don't know about or does everyone have to deal with the nasty details of optimizing the database at some point? I was hoping that some fast indexing/searching technology existed (e.g. Lucene) that would address this problem. ANY OTHER COMMENTS OR SUGGESTION ARE MOST WELCOME!! Thanks a lot guys! Ali Use Lucene for your text-based searches and use SQL for field_x > 5 searches. I say this because text-based search is hard to get right and you're probably better off leaving that to an expert. If you need your users to have the capability of building mathematical expression searches consider writing an expression builder dialog like this example to collect the search phrase. Then use a parameterized SQL query to execute the search. SqlWhereBuilder ASP.NET Server Control http://www.codeproject.com/KB/custom-controls/SqlWhereBuilder.aspx Thanks for your comment Robert. I don't need anything fancy like the expression-builder you mentioned. Just need to query the database using mathematical expressions internally. OK. You should consider moving your memo fields from your mySQL database into the Lucene database so that you can do text search on them. Presumably you can also put a key into the Lucene database so you can pull the text from the mySQL side. Now you have the best of both worlds; you can do full text searches and can still do math searches on the SQL database. Yes I understand how that would work. Thanks a lot for your help Rob :)  You can use filters in Lucene to carry out a text search of a reduced set of records. So if you query the database first to get all records where field_x > 5 build a filter (a list of lucene document IDs) and pass this into the lucene search method along with the text query. I'm just learning about this here's a link to a question I asked (it uses Lucene.Net and C# but it may help) - ignore my question just check out the accepted answer: http://stackoverflow.com/questions/1079934/how-do-you-implement-a-custom-filter-with-lucene-net To be honest I don't even need a full-text search much. I just want to reduce the load on the database and let a fast specialized index (such as Lucene) handle my large searches. So is there a faster way than querying the DB if I just have mathematical conditions?  You can use Zend Lucene for textual search and combine it with MySQL for joins. Please see Mark Krellenstein's Search Engine vs DBMS paper about the choice; Basically search engines are better for ranked text search; Databases are better for more complex data manipulations such as joins using different record structures. For a simple x>5 type query you can use a range query inside Lucene. Thanks Yuval. I was already using Zend_Lucene. I read Mark's article and it's very interesting. I suppose I will go with the database option for now until my needs for full-text search grow to such an extent that it will be worth duplicating the effort for keeping the index up-to-date. What I don't understand is why Lucene won't allow you to perform these simple mathematical operations in its queries?! I'm no expert at writing search engines(obviously :) but compared to what they've already accomplished it seems like it would be trivial. Thanks again.
406,A,"Zend Lucene - tokenizing swedish chars I use Zend Lucene to index swedish texts. The problem is that lucene tokenizes words at swedish chars åäö. For example the word ""världens"" becomes two words ""v"" and ""ldens"" in the index. Is there a way to add characters that zend lucene should accept and not tokenize at? use an UTF-8 compatible text analyzer instead of the default text analyzer for tokenization. note that this requires PHP's PCRE (Perl-compatible regular expressions) library to be compiled with UTF-8 support (the default if you use the PCRE library bundled with PHP but possibly not enabled if you use a shared library). for case insensitive versions of the UTF-8 compatible analyzers you also need the mbstring extension to be enabled.  Using Analysers. See the docs about text analysis using utf8 and docs about writing your own analyser. I recommend you just use a UTF-8 analyser."
407,A,Intersecting boundaries with lucene I'm using Lucene and I'm trying to find a way to index and retrieve documents that have a ranged property. For example I have: Document 1: Price:[30 TO 50] Document 2: Price:[45 TO 60] Document 3: Price:[60 TO 70] And I would like to search for all the documents whose ranges intersect a specific interval in the above example if I search for Price in [55 TO 65] I should get Document 2 and Document 3 as results. I don't think NumericRangeQueries alone would do the trick I need to work on the index with something similar to R-trees but are they implemented in Lucene? Also I suppose that what I need should be a subclass of MultiTermQuery because the query Price in [55 TO 65] has two boundaries but I don't see anything suitable among MultiTermQuery's subclasses. Any help is appreciated thanks Silvio P.S. I'm using Lucene 2.9.0 but I can update to the latest release if needed. One simple option to try is during index time simply expand your ranges to each discrete value in the range. So [30 TO 50] would be indexed as 30 31 32 33 34 etc. Then use the normal range query to query the range. Just as long as there aren't tons of discrete values (millions) this might perform well enough. You don't need to create separate fields. All values would go in the same field no change to document schema needed. That way I would be tied to the number of values I use to discretize the intervals also it would generate a lot of fields (one for each discrete value) which would bloat my document schema. It's a possible solution of course but I'd keep it as a last resort
408,A,"Lucene Index Size I have data like 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20 22 23 24 25 26 28 30 36 37 39 40 41 46 48 49 51 52 53 54 55 56 58 60 66 67 68 71 72 74 77 78 85 89 90 91 108 109 110 116 117 118 120 121 123 137 138 145 146 147 148 154 157 159 162 165 166 168 175 179 181 198 201 203 212 215 216 223 231 233 254 266 270 274 323 327 329 331 347 352 355 360 363 370 411 415 434 438 442 444 445 462 470 471 477 486 495 499 503 524 525 536 542 595 603 608 636 644 646 647 670 692 694 698 762 763 798 809 822 970 981 987 992 1040 1057 1066 1079 1089 1111 1233 1244 1302 1315 1327 1333 1336 1387 1411 1412 1432 1458 1486 1498 1509 1572 1573 1574 1607 1625 1784 1808 1824 1909 1933 1938 1940 2011 2077 2081 2093 2286 2289 2395 2427 2467 2911 2944 2962 2975 3121 3170 3172 3197 3236 3267 3334 3699 3731 3905 3945 3982 3999 4008 4161 4234 4235 4296 4374 4457 4494 4526 4717 4720 4723 4820 4875 5352 5423 5472 5728 5799 5813 5821 6032 6230 6244 6278 6859 6868 7186 7280 7401 8734 8832 8885 8886 8925 9363 9510 9517 9592 9707 9802 10002 11097 11192 11715 11716 11836 11945 11996 12025 12482 12703 12706 12887 13122 13372 13482 13577 14150 14161 14169 14461 14626 16057 16268 16415 17183 17398 17440 17464 18097 18690 18731 18834 20576 20603 21558 21839 22202 26201 26497 26654 26658 26776 28088 28531 28551 28775 29122 29407. This is one line of data many are there like that stored in ""training.txt"". I am index it using following lucene indexing code public class training { public static void main(String args[]) throws IOException ParseException { StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_30); // IndexWriter w = new IndexWriter(FSDirectory.open(new File(""../search/index"")) analyzer true new IndexWriter.MaxFieldLength(1000000)); IndexWriter w = new IndexWriter(FSDirectory.open(new File(""index"")) analyzer true new IndexWriter.MaxFieldLength(2139999999)); File file = new File(""training.txt""); FileInputStream fis = null; BufferedInputStream bis = null; DataInputStream dis = null; File file1 = new File(""fileName.txt""); FileInputStream fis1 = null; BufferedInputStream bis1 = null; DataInputStream dis1 = null; try { fis = new FileInputStream(file); // Here BufferedInputStream is added for fast reading. bis = new BufferedInputStream(fis); dis = new DataInputStream(bis); fis1 = new FileInputStream(file1); // Here BufferedInputStream is added for fast reading. bis1 = new BufferedInputStream(fis1); dis1 = new DataInputStream(bis1); // dis.available() returns 0 if the file does not have more lines. while (dis.available() != 0 && dis1.available() != 0 ) { String tempImg=dis1.readLine(); String temp=dis.readLine(); addDoc(wtempImgtemp); // System.out.println(temp); } // dispose all the resources after using them. fis.close(); bis.close(); dis.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } w.optimize(); w.close(); } private static void addDoc(IndexWriter w String value1String value2) throws IOException { Document doc = new Document(); doc.add(new Field(""fileId"" value1 Field.Store.YES Field.Index.ANALYZED)); doc.add(new Field(""visualId"" value2 Field.Store.YES Field.Index.ANALYZED)); w.addDocument(doc); } } There is another file ""fileName.txt"" for file name. My ""training.txt"" is of size 127.1 MB & index folder is getting created of size 217.2 MB. I believe it should get reduced. My Search Code : public class search { public static void main(String args[]) throws IOException ParseException { StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_30); String fname = ""test.txt""; File file = new File(fname); FileInputStream fis = null; BufferedInputStream bis = null; DataInputStream dis = null; try { fis = new FileInputStream(file); Writer fos = null; File outputFile = new File(""outList.txt""); fos = new BufferedWriter(new FileWriter(outputFile)); // Here BufferedInputStream is added for fast reading. bis = new BufferedInputStream(fis); dis = new DataInputStream(bis); while (dis.available() != 0) { Query q = new QueryParser(Version.LUCENE_CURRENT ""visualId"" analyzer).parse(dis.readLine()); //3.search int hitsPerPage = 200; IndexSearcher searcher = new IndexSearcher(IndexReader.open(FSDirectory.open(new File(""index"")) true)); TopScoreDocCollector collector = TopScoreDocCollector.create(hitsPerPage true); long startTime = System.currentTimeMillis(); searcher.search(q collector); long endTime = System.currentTimeMillis(); ScoreDoc[] hits = collector.topDocs().scoreDocs; for(int i=0;i<hits.length;++i) { int docId = hits[i].doc; Document d = searcher.doc(docId); String text = d.get(""fileId""); fos.write(text); fos.write(""\n""); } searcher.close(); } // dispose all the resources after using them. fis.close(); fos.close(); bis.close(); dis.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } //out.close(); } } My ""test.txt"" is having content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 55 56 57 58 59 60 61 63 64 65 66 67 69 70 72 73 76 77 78 80 82 83 85 86 88 89 90 91 92 93 94 95 97 99 100 102 105 106 107 108 109 110 111 112 114 115 116 117 118 119 120 121 122 124 126 127 128 129 130 132 133 135 136 137 138 141 142 143 144 145 147 148 151 153 154 155 156 157 160 164 165 167 168 169 170 172 173 174 175 176 178 179 180 181 182 183 184 190 191 194 195 199 200 202 206 208 211 215 216 217 220 228 231 234 239 246 248 250 254 259 264 266 267 268 270 271 272 275 276 278 281 284 285 292 296 297 300 306 307 314 316 317 320 321 322 323 325 326 327 330 331 333 336 343 345 348 349 350 351 353 354 355 357 358 360 361 362 364 365 367 371 372 379 381 384 385 386 388 391 396 398 399 404 405 406 407 409 412 415 423 424 427 428 429 431 432 434 435 436 442 443 444 453 458 461 462 466 468 472 479 493 494 495 496 500 501 502 503 504 506 507 508 509 510 515 518 519 521 526 528 533 535 537 538 540 544 545 547 549 551 569 570 574 582 583 586 597 599 601 605 607 618 623 624 632 644 645 649 651 661 683 694 701 702 718 737 738 739 743 751 762 776 777 778 792 797 800 803 809 811 812 813 817 825 828 833 843 853 854 875 889 892 900 918 919 922 941 949 951 961 963 964 965 966 967 969 975 976 977 979 980 990 992 993 1000 1007 1008 1009 1029 1036 1045 1047 1051 1052 1053 1058 1059 1061 1062 1064 1065 1066 1070 1072 1075 1081 1082 1083 1086 1093 1094 1101 1114 1116 1117 1136 1143 1152 1154 1158 1159 1165 1172 1188 1194 1198 1212 1216 1218 1220 1227 1236 1245 1269 1272 1280 1283 1284 1285 1287 1293 1295 1296 1303 1305 1307 1327 1329 1332 1358 1373 1374 1375 1384 1385 1386 1397 1404 1415 1416 1436 1437 1478 1481 1482 1485 1487 1489 1501 1503 1505 1506 1508 1511 1517 1518 1520 1521 1522 1524 1525 1527 1529 1545 1555 1556 1564 1577 1579 1583 1599 1606 1610 1611 1612 1615 1620 1632 1636 1640 1648 1654 1706 1711 1721 1746 1750 1758 1792 1796 1802 1814 1820 1853 1869 1872 1897 1931 1932 1935 1946 1953 1982 2049 2082 2104 2107 2155 2211 2213 2216 2228 2253 2286 2329 2330 2332 2334 2377 2390 2399 2408 2427 2428 2433 2435 2440 2452 2475 2484 2498 2529 2559 2563 2626 2666 2675 2699 2754 2758 2765 2822 2847 2852 2882 2889 2893 2895 2898 2902 2906 2908 2925 2929 2932 2936 2939 2940 2971 2977 2980 2999 3022 3023 3024 3028 3086 3107 3134 3136 3140 3152 3156 3160 3174 3176 3182 3186 3192 3195 3197 3209 3216 3225 3242 3247 3249 3259 3279 3283 3303 3341 3349 3350 3352 3407 3429 3455 3462 3475 3476 3495 3515 3564 3581 3595 3637 3648 3653 3660 3681 3707 3735 3807 3817 3839 3850 3852 3856 3860 3878 3884 3889 3909 3916 3920 3980 3988 3997 4075 4120 4122 4123 4125 4152 4156 4157 4159 4191 4211 4244 4248 4307 4310 4434 4444 4446 4455 4462 4466 4503 4509 4516 4517 4525 4532 4551 4554 4559 4563 4564 4565 4573 4576 4581 4586 4634 4666 4669 4691 4730 4738 4748 4796 4817 4829 4832 4837 4846 4859 4896 4909 4919 4943 4962 5119 5132 5162 5237 5251 5275 5376 5387 5407 5441 5461 5559 5606 5608 5616 5692 5792 5797 5806 5837 5858 5947 6146 6245 6313 6320 6466 6632 6640 6648 6683 6759 6859 6987 6988 6989 6995 7003 7131 7171 7197 7223 7225 7280 7283 7299 7304 7320 7355 7357 7424 7451 7493 7586 7678 7690 7878 7997 8024 8096 8261 8275 8294 8465 8542 8556 8646 8667 8679 8685 8695 8707 8718 8724 8774 8786 8795 8808 8817 8819 8913 8932 8941 8996 9065 9069 9071 9085 9258 9321 9403 9408 9420 9456 9468 9481 9523 9528 9546 9559 9575 9584 9590 9592 9626 9648 9675 9727 9740 9742 9747 9776 9778 9836 9850 9909 10022 10046 10049 10056 10222 10288 10366 10385 10425 10429 10485 10546 10691 10744 10786 10912 10945 10958 10980 11043 11120 11205 11420 11451 11518 11551 11557 11568 11580 11633 11635 11652 11667 11728 11749 11760 11940 11963 11990 12225 12360 12367 12370 12375 12455 12468 12472 12476 12573 12632 12633 12731 12732 12745 12921 12922 12931 13303 13331 13332 13338 13364 13366 13386 13397 13510 13528 13548 13551 13575 13597 13654 13662 13676 13688 13689 13690 13693 13694 13720 13728 13743 13757 13901 13999 14007 14074 14190 14214 14245 14389 14452 14487 14496 14511 14538 14578 14689 14726 14756 14829 14887 15357 15395 15485 15710 15754 15824 16128 16161 16220 16323 16384 16678 16819 16825 16848 17075 17375 17391 17417 17511 17575 17841 18439 18734 18940 18961 19399 19896 19920 19945 20050 20276 20578 20960 20964 20967 20986 21009 21393 21513 21591 21670 21676 21839 21849 21898 21911 21960 22066 22072 22271 22354 22480 22759 23033 23070 23635 23990 24073 24287 24784 24824 24882 25395 25625 25668 25938 26002 26036 26054 26056 26085 26122 26153 26173 26321 26358 26385 26423 26450 26456 26739 26796 26823 26987 27196 27206 27214 27255 27773 27962 28209 28225 28260 28369 28405 28443 28568 28585 28637 28676 28724 28753 28770 28775 28877 28944 29026 29180 29221 29225 29240 29327 29333 29507 Thanks Ravi. NB dis.available() does *not* return 0 if the file does not have more lines. Check the Javadoc. available() should not be used for testing for EOF. Why do you believe this? Who's to say the index has to be the same size as your original data file? When you are adding Field.Store.YES to a Lucene field it is stored as well as indexed. The result would be that your index becomes larger than expected. Thanks done by mistake in training I put both ""NO"". Now Size is getting reduced. Currently it is 89.8 MB. disabling norms and term vectors I will try to find out how to do that. @ravi yes you will get a NullPointerException if you try to write the content of a field that is not stored. @ravi you might also want to use a `WhitespaceAnalyzer` rather than a `StandardAnalyzer` if your files contain only space-delimited tokens. It'll make your code run somewhat faster. Ok if I don't store the index then size is getting reduces but then How searching will be done ? +1. Also disabling norms and term vectors can reduce the index size though the gains are incremental. @ravi: The field will still be indexed and available for searching. The value is just no longer stored in the index. I have updated my search code. While searching I am getting error : Exception in thread ""main"" java.lang.NullPointerException at java.io.Writer.write(Writer.java:140) at search.main(search.java:75). Previously it was working fine."
409,A,"how do I normalise a solr/lucene score? I am trying to work out how to improve the scoring of solr search results. My application needs to take the score from the solr results and display a number of “stars” depending on how good the result(s) are to the query. 5 Stars = almost/exact down to 0 stars meaning not matching the search very well e.g. only one element hits. However I am getting scores from 1.4 to 0.8660254 both are returning results that I would give 5 stars to. What I need to do is somehow turn these results in to a percentage so that I can mark these results with the correct number of stars. The query that I run that gives me the 1.4 score is: euallowed:true AND(grade:""2:1"") The query that gives me the 0.8660254 score is: euallowed:true AND(grade:""2:1"" OR grade:""1st"") I've already updated the Similarity so that the tf and idf return 1.0 as I am only interested if a document has a term not the number of that term in the document. This is what my similarity code looks like: import org.apache.lucene.search.Similarity; public class StudentSearchSimilarity extends Similarity { @Override public float lengthNorm(String fieldName int numTerms) { return (float) (1.0 / Math.sqrt(numTerms)); } @Override public float queryNorm(float sumOfSquaredWeights) { return (float) (1.0 / Math.sqrt(sumOfSquaredWeights)); } @Override public float sloppyFreq(int distance) { return 1.0f / (distance + 1); } @Override public float tf(float freq) { return (float) 1.0; } @Override public float idf(int docFreq int numDocs) { //return (float) (Math.log(numDocs / (double) (docFreq + 1)) + 1.0); return (float)1.0; } @Override public float coord(int overlap int maxOverlap) { return overlap / (float) maxOverlap; } } So I suppose my questions are: How is the best way of normalising the score so that I can work out how many “stars” to give? Is there another way of scoring the results? Thanks Grant I've never had to do anything this complicated in Solr so there may be a way to hook this in as a plugin - but you could handle it in the client when a result set is returned. If you've sorted by relevance this should be staightforward - get the relevence of the first result (max) and the last (min). Then for each result with relevance x you can calculate normalisedValue = (x - min) / (max - min) which will give you a value between 0 and 1. Multiply by 5 and round to get the number of stars.  It's called normalized score (Scores As Percentages). You can use the following the following parameters to achieve that: ns = {!func}product(scale(product(query({!type=edismax v=$q})1)01)100) fq = {!frange l=20}$ns Where 20 is your 20% threshold. See also: Remove results below a certain score threshold in Solr/Lucene? http://article.gmane.org/gmane.comp.jakarta.lucene.user/12076 http://article.gmane.org/gmane.comp.jakarta.lucene.user/10810  To quote http://wiki.apache.org/lucene-java/ScoresAsPercentages: People frequently want to compute a ""Percentage"" from Lucene scores to determine what is a ""100% perfect"" match vs a ""50%"" match. This is also somethings called a ""normalized score"" Don't do this. Seriously. Stop trying to think about your problem this way it's not going to end well. That page does give an example of how you could in theory do this but it's very hard. Humm... thanks for this. It makes a very good argument but not sure what happens when I override tf and idf. I think that I might have to look at this in a different way. Even if it means not ""scoring"" by stars."
410,A,"Lucence SweetSpotSimilarity lengthNorm http://lucene.apache.org/java/2_3_0/api/org/apache/lucene/misc/SweetSpotSimilarity.html Implemented as: 1/sqrt( steepness * (abs(x-min) + abs(x-max) - (max-min)) + 1 ) . This degrades to 1/sqrt(x) when min and max are both 1 and steepness is 0.5 Can anyone explain this formula for me? How steepness is decided and what is exactly referring to? Any help is appreciated. With the DefaultSimilarity the shorter the field in terms of number of tokens the higher the score. e.g. if you have two docs with indexed field values of ""the quick brown fox"" and ""brown fox"" respectively the latter would score higher in a query for ""fox"". SweetSpotSimilarity lets you define a ""sweet spot"" for the length of a field in terms of a range defined by min and max. Field lengths within the range will score equally and field lengths outside the range will score lower depending on the distance the length is form the range boundary. ""steepness"" determines how quickly the score degrades as a function of distance."
411,A,FuzzyQuery and BooleanQuery I work with Hibernate Search 3.1.1.GA. I am trying my luck at fuzzy queries. This query works (it retrieves records containing Shakespeare): lastName:shakespere~0.1 But this one does not: firstName:shakespere~0.1 lastName:shakespere~0.1 I create a BooleanQuery and stuff it with FuzzyQuery instances with Occur.SHOULD. Wrapping the FuzzyQuery instances in BooleanClause does not seem to make a difference. Any hint? Thanks Francois I'm not sure this is what you need try take a look at [MultiFieldQueryParser][1] [1]: http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/queryParser/MultiFieldQueryParser.html#MultiFieldQueryParser(java.lang.String[] org.apache.lucene.analysis.Analyzer)
412,A,"Zend lucene : Multiple criteria on search = bad results I new to lucene and i noticed something annoying : In my search bar if I type ""USA"" : return all the matches -> OK. If I type ""Developper"" : return all the matches -> OK BUT -if i type ""USA Developper"" it'll not return me all the developper in the USA. It'll return me some developper in UK DE FR + Developpers Stars Engineers in USA How to fix that please ? You need to set default search operator as AND. (Not sure how you do it in php.) Or in your current framework if you search for +developer +usa (ie prefix a + before each term) it will return you developers in USA."
413,A,"Highlighting in Solr 1.4 - requireFieldMatch I have an object Title : foo Summary : foo bar Body : this is a published story about a foo and a bar All three are set up as fields with stored=true. The user searches across my system for the word ""foo"" I would like to highlight foo in all three places. The user searches for the word foo in the title ""title:foo"" I only want to highlight foo within the title. When I added hl.requireFieldMatch=true and hl.usePhraseHighlighter=true as part of my query over to SOLR I am unable to get the highlighting in all three places when doing a generic non fielded search. Is there a way to get both scenarios to work? I had these two items turned off but I am adding in some fielded portions of the query that the user does not see which only display Published items for instance. the problem is (foo AND status:published) is causing the word published in the body to highlight when the user only searched for the word ""foo"". Turns out that this may be more of a quirk with act_as_solr than it is with solr / lucene. I took advantage of the fq option within solr http://wiki.apache.org/solr/CommonQueryParameters#fq the query no longer looks like (foo AND status:published) Looks more like q=foo&fq=status:published when it hits solr The fq does not impact highlighting and it seems like a better way to do this since you get some solr caching benefits. There is still a bug / feature not implemented with acts_as_solr which needs to take advantage of this as well. For instance if my model is named article and the user searches for the word article AAS is building a query behind the scenes that looks like (article AND type_t:article) This causes the same problem that the word article highlights all over the place when you don't want it to as well as when the user searches for the term ""article"" they get back every single object in the system. Took a quick look at GitHub forks of acts_as_solr and didn't see anyone that has implemented this yet. There is nice #TODO sitting there within parser_methods.rb"
414,A,"How reliable is Lucene when counting hits/documents? If a run a search: ""+house +car"" and returns 5343562 hits Is that the exact number of documents I have or it's an approximation. If it's an approximation is there a way to make it to return the extract number of documents that qualifies for a search query? It's exact. What makes you think it's an approximation? ........my boss Does your boss has more cars and houses then 5343562. He should rather be right"
415,A,do I need to rebuild my lucene index for this change? Do I need to rebuild a Lucene index when I only add a random field to a schema? Or could I run some code to update that field without rebuilding the index? This is the field I need to add: http://lucene.apache.org/solr/api/org/apache/solr/schema/RandomSortField.html In this case Lucene is running on Solr. IIRC you don't need to rebuild the index or run any code to update a random field since that field type doesn't really have a value i.e. its value is generated on demand depending on the full field name. Thanks that turned out to be the case. My Lucene book is still in the mail. :)
416,A,Lucene Indexing - using a web service I have an incremental index on a shared hosting server. I would have used a console application to perform the indexing by the hosting provider does not allow console apps on the server. I am thinking of using a web service to do the indexing (with some queuing mechanism). Is this a good idea? What are the pros/cons or alternatives? Apache Solr does it all for you.
417,A,Comparison: DB Full Text search to Search engine (Lucene) With stackoveflow.com in perspective (team of 2-3 engineers building a website project intended to scale) does it make sense to spend effort early in the process of development to build a search based on Lucene/Autonomy… as opposed to a database based full text search. Pros/Cons: With a mature Lucene implementation like nutch or autonomy the cost of moving to Lucene (which is inevitable) at a later stage is negligible. In large volumes adding additional index servers (say with nutch) to maintain the growing search index is relatively easy. With a Lucene implementation I’ll mostly likely need an additional server to main the in-memory index (much early in the process of scaling). You should keep it isolated though - don't start throwing SELECTS all over your code if you know you will replace them with a search engine query. Wrap your DB's full text search with a thin abstraction layer that makes sure you don't use database capabilities where you shouldn't. I second the accepted answer though - premature optimization here is definitely evil.  Database fulltext search performance varies from database to database but it's by far the easiest option to setup. So start with that and move to lucene or sphinx if it proves to be too slow. if your db's full text search is good enough use it (unless you have an exotic requirement such as db-independence).
418,A,"How to use lucene across multiple websites I've got four websites that are edited via one CMS (hanging off one of the sites) like this: www.domain1.com www.domain2.com www.domain3.com www.domain4.com www.domain4.com/cms I'll be using Lucene to index the textual content (from database and uploaded documents) of all four sites. The index will have to be available to both the CMS system and the search page on each domain / website (which may be on different physical servers). So should I use one lucene index on the domain that hosts the CMS system or put an index on each domain? (I'm guessing I'll have to write some sort of web service to tie everything together for either solution). Is there a recommended method for achieving this? Thanks. Possible reasons for using separate indexes are having distinct content on each domain where you do not wish to find search matches from the other domains or a scalability issue. Otherwise it seems better to use a single search server having clients on four domains. We have had some success with simple placing the lucene index on a file share that is accessible by all servers. This isn't really supported but works perfectly for our needs. This is a relatively low traffic scenario though. If your index is only updated from one of the sites then you can simply copy it across to the other servers after each update. Or on a hourly/daily basis. +1 Didn't think of just copying it over..  I'm planning something very similar to this.... What i'll probably do is have a single lucene index with a ""service"" (web service or wcf) above it to query the index and update the index. think of it as a seperate domain that handles searching This was the direction I was thinking of too seems less hassle than telling the CMS which lucene index it should be updating every time As an update to this I used Solr for what I needed to do  How about running Lucene as a stand-alone server shared by these clients? Check out SOLR."
419,A,"Lucene Search Problem I have built an index on my database rows (Each row as a document) which are of unicode type in MySQL(i.e. Charset: utf8 and Collation: utf8-bin). But When I search any word English or non-English it gives me no answers. It says: 0 total matching documents My code is the demo code of lucene for search except that I have changed field names to my inserted column names. Anyway it prints this message before reaching that part of code. And also I have changed the read query encoding to UTF-8. I have checked the reading of database part. It's OK. What's the problem? If it helps here is my insertion code: static void indexDocs(IndexWriter writer Connection conn) throws SQLException CorruptIndexException IOException { String sql = ""select id name description text from users""; Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(sql); while (rs.next()) { Document d = new Document(); d.add(new Field(""id"" rs.getString(""id"") Field.Store.YES Field.Index.NOT_ANALYZED)); d.add(new Field(""name"" rs.getString(""name"") Field.Store.NO Field.Index.NOT_ANALYZED)); String tmp = rs.getString(""description""); if (tmp == null) { tmp = """"; } d.add(new Field(""description"" tmp Field.Store.NO Field.Index.ANALYZED)); tmp = rs.getString(""text""); if (tmp == null) { tmp = """"; } d.add(new Field(""text"" tmp Field.Store.NO Field.Index.ANALYZED)); writer.addDocument(d); } } Also this is my search code: import java.io.BufferedReader; import java.io.File; import java.io.FileReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.Date; import org.apache.lucene.analysis.Analyzer; import org.apache.lucene.analysis.standard.StandardAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.index.FilterIndexReader; import org.apache.lucene.index.IndexReader; import org.apache.lucene.queryParser.QueryParser; import org.apache.lucene.search.Collector; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.Query; import org.apache.lucene.search.ScoreDoc; import org.apache.lucene.search.Scorer; import org.apache.lucene.search.Searcher; import org.apache.lucene.search.TopScoreDocCollector; import org.apache.lucene.store.FSDirectory; import org.apache.lucene.util.Version; /** Simple command-line based search demo. */ public class Search { /** Use the norms from one field for all fields. Norms are read into memory * using a byte of memory per document per searched field. This can cause * search of large collections with a large number of fields to run out of * memory. If all of the fields contain only a single token then the norms * are all identical then single norm vector may be shared. */ private static class OneNormsReader extends FilterIndexReader { private String field; public OneNormsReader(IndexReader in String field) { super(in); this.field = field; } @Override public byte[] norms(String field) throws IOException { return in.norms(this.field); } } private Search() { } /** Simple command-line based search demo. */ public static void main(String[] args) throws Exception { String usage = ""Usage:\tjava org.apache.lucene.demo.SearchFiles [-index dir] [-field f] [-repeat n] [-queries file] [-raw] [-norms field] [-paging hitsPerPage]""; usage += ""\n\tSpecify 'false' for hitsPerPage to use streaming instead of paging search.""; if (args.length > 0 && (""-h"".equals(args[0]) || ""-help"".equals(args[0]))) { System.out.println(usage); System.exit(0); } String index = ""index""; String field = ""contents""; String queries = null; int repeat = 0; boolean raw = false; String normsField = null; boolean paging = true; int hitsPerPage = 10; for (int i = 0; i < args.length; i++) { if (""-index"".equals(args[i])) { index = args[i + 1]; i++; } else if (""-field"".equals(args[i])) { field = args[i + 1]; i++; } else if (""-queries"".equals(args[i])) { queries = args[i + 1]; i++; } else if (""-repeat"".equals(args[i])) { repeat = Integer.parseInt(args[i + 1]); i++; } else if (""-raw"".equals(args[i])) { raw = true; } else if (""-norms"".equals(args[i])) { normsField = args[i + 1]; i++; } else if (""-paging"".equals(args[i])) { if (args[i + 1].equals(""false"")) { paging = false; } else { hitsPerPage = Integer.parseInt(args[i + 1]); if (hitsPerPage == 0) { paging = false; } } i++; } } IndexReader reader = IndexReader.open(FSDirectory.open(new File(index)) true); // only searching so read-only=true if (normsField != null) { reader = new OneNormsReader(reader normsField); } Searcher searcher = new IndexSearcher(reader); Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT); BufferedReader in = null; if (queries != null) { in = new BufferedReader(new FileReader(queries)); } else { in = new BufferedReader(new InputStreamReader(System.in ""UTF-8"")); } QueryParser parser = new QueryParser(Version.LUCENE_CURRENT field analyzer); while (true) { if (queries == null) // prompt the user { System.out.println(""Enter query: ""); } String line = in.readLine(); line = new String(line.getBytes(""8859_1"") ""UTF8""); if (line == null || line.length() == -1) { break; } line = line.trim(); if (line.length() == 0) { break; } Query query = parser.parse(line); System.out.println(""Searching for: "" + query.toString(field)); if (repeat > 0) { // repeat & time as benchmark Date start = new Date(); for (int i = 0; i < repeat; i++) { searcher.search(query null 100); } Date end = new Date(); System.out.println(""Time: "" + (end.getTime() - start.getTime()) + ""ms""); } if (paging) { doPagingSearch(in searcher query hitsPerPage raw queries == null); } else { doStreamingSearch(searcher query); } } reader.close(); } /** * This method uses a custom HitCollector implementation which simply prints out * the docId and score of every matching document. * * This simulates the streaming search use case where all hits are supposed to * be processed regardless of their relevance. */ public static void doStreamingSearch(final Searcher searcher Query query) throws IOException { Collector streamingHitCollector = new Collector() { private Scorer scorer; private int docBase; // simply print docId and score of every matching document @Override public void collect(int doc) throws IOException { System.out.println(""doc="" + doc + docBase + "" score="" + scorer.score()); } @Override public boolean acceptsDocsOutOfOrder() { return true; } @Override public void setNextReader(IndexReader reader int docBase) throws IOException { this.docBase = docBase; } @Override public void setScorer(Scorer scorer) throws IOException { this.scorer = scorer; } }; searcher.search(query streamingHitCollector); } /** * This demonstrates a typical paging search scenario where the search engine presents * pages of size n to the user. The user can then go to the next page if interested in * the next hits. * * When the query is executed for the first time then only enough results are collected * to fill 5 result pages. If the user wants to page beyond this limit then the query * is executed another time and all hits are collected. * */ public static void doPagingSearch(BufferedReader in Searcher searcher Query query int hitsPerPage boolean raw boolean interactive) throws IOException { // Collect enough docs to show 5 pages TopScoreDocCollector collector = TopScoreDocCollector.create( 5 * hitsPerPage false); searcher.search(query collector); ScoreDoc[] hits = collector.topDocs().scoreDocs; int numTotalHits = collector.getTotalHits(); System.out.println(numTotalHits + "" total matching documents""); int start = 0; int end = Math.min(numTotalHits hitsPerPage); while (true) { if (end > hits.length) { System.out.println(""Only results 1 - "" + hits.length + "" of "" + numTotalHits + "" total matching documents collected.""); System.out.println(""Collect more (y/n) ?""); String line = in.readLine(); if (line.length() == 0 || line.charAt(0) == 'n') { break; } collector = TopScoreDocCollector.create(numTotalHits false); searcher.search(query collector); hits = collector.topDocs().scoreDocs; } end = Math.min(hits.length start + hitsPerPage); for (int i = start; i < end; i++) { if (raw) { // output raw format System.out.println(""doc="" + hits[i].doc + "" score="" + hits[i].score); continue; } Document doc = searcher.doc(hits[i].doc); String id = doc.get(""id""); if (id != null) { System.out.println((i + 1) + "". "" + id); String name = doc.get(""name""); if (name != null) { System.out.println("" name: "" + doc.get(""name"")); } String description = doc.get(""description""); if (description != null) { System.out.println("" description: "" + doc.get(""description"")); } String text= doc.get(""text""); if (text != null) { System.out.println("" text: "" + doc.get(""text"")); } } else { System.out.println((i + 1) + "". "" + ""No path for this document""); } } if (!interactive) { break; } if (numTotalHits >= end) { boolean quit = false; while (true) { System.out.print(""Press ""); if (start - hitsPerPage >= 0) { System.out.print(""(p)revious page ""); } if (start + hitsPerPage < numTotalHits) { System.out.print(""(n)ext page ""); } System.out.println(""(q)uit or enter number to jump to a page.""); String line = in.readLine(); if (line.length() == 0 || line.charAt(0) == 'q') { quit = true; break; } if (line.charAt(0) == 'p') { start = Math.max(0 start - hitsPerPage); break; } else if (line.charAt(0) == 'n') { if (start + hitsPerPage < numTotalHits) { start += hitsPerPage; } break; } else { int page = Integer.parseInt(line); if ((page - 1) * hitsPerPage < numTotalHits) { start = (page - 1) * hitsPerPage; break; } else { System.out.println(""No such page""); } } } if (quit) { break; } end = Math.min(numTotalHits start + hitsPerPage); } } } } Thank you. I found out. I should specify the column which I want to search. e.g. For searching in text field I should say: ""text:MyWord"""
420,A,NLP programming tools using PHP? Since big web applications came into existence searching for data (and doing it lightning fast and accurate) has been one of the most important problems in web applications. For a while I've worked using Lucene.NET which is a C# port of the Lucene project. I also work using PHP using Zend Framework's Lucene API which brings me to my question. Most times for providing good indexing we need to perform some NLP tools like tokenizing lemmatizing and many more the question is: Do you know of any good NLP programming framework/toolset using PHP? PS: I'm very aware of the Zend API for Lucene but indexing data properly is not just storing and relying in Lucene you need to perform some extra tasks like those above. I would suggest that you look at Solr which is a best practice implementation of Lucene. Solr uses a REST based API that also has a very good PHP client. This will allow you to leverage the power of Lucene without needing to perform any of the low level programming to get the NLP power that you want. Also you would probably want to grab the trunk version of Solr as the NLP development is very active right now and new capabilities are being added every day.  Seems like you are looking for the same stuff i googled a few months back :D... I'm running a php/zend based project with Solr (via php-solr-client lib) and so far I havent found anything in php for advanced NLP. For basic stuff as everyone mentions you can get away with Solr (stemming tag clouds / phrase tag clouds tokenizing etc) and there are a few basic but useful text processing php libraries out there (nothing fancy really better rely on Solr itself)... but if you are looking for more algorithmic/semantic/sentiment NLP analysis I suggest you move a bit from PHP and get into Java as there are more libraries that can help you in this area(such as OpenNLP). In case te adavanced stuff is what you are looking for you probably might want to take a look at Mahout: http://www.lucidimagination.com/blog/2010/03/16/integrating-apache-mahout-with-apache-lucene-and-solr-part-i-of-3/  Zend has a full port of lucene to PHP. See docs here. Lucene has tokenizers Lucene has a porter stemmer Lucene has snowball Lucene can tie in with wordnet Yes Im aware of it and I use it but my NLP tools where about finding any tokenizers name parsers or something like it. I'll edit the question anyways because perhaps is not clear enough. @David: I added more to my answer; Lucene can indeed tokenize and lemmatize. Im also aware of the abilities of Lucene but you are signaling to the Java original project and I think that the Zend port does not contain them so I still the same. Thks anyways @David: Zend has ported a lot. It obviously can [tokenize](http://framework.zend.com/svn/framework/standard/trunk/library/Zend/Search/Lucene/Analysis/) (or else it would be useless) and there are also [stemmers](http://codefury.net/2008/06/a-stemming-analyzer-for-zends-php-lucene/).
421,A,"Using RAMDirectory When do i use Lucene RAMDirectory?what's the advantage of using it? Can i have some code sample please? Thanks. Internally it uses a concurrenthasmap to arrange the files. When you don’t want to permanently store your index data. I use this for testing purposes. Add data to your RAMDirectory Do your unit tests in RAMDir. e.g.  public static void main(String[] args) { try { Directory directory = new RAMDirectory(); Analyzer analyzer = new SimpleAnalyzer(); IndexWriter writer = new IndexWriter(directory analyzer true); OR  public void testRAMDirectory () throws IOException { Directory dir = FSDirectory.getDirectory(indexDir); MockRAMDirectory ramDir = new MockRAMDirectory(dir); // close the underlaying directory dir.close(); // Check size assertEquals(ramDir.sizeInBytes() ramDir.getRecomputedSizeInBytes()); // open reader to test document count IndexReader reader = IndexReader.open(ramDir); assertEquals(docsToAdd reader.numDocs()); // open search zo check if all doc's are there IndexSearcher searcher = new IndexSearcher(reader); // search for all documents for (int i = 0; i < docsToAdd; i++) { Document doc = searcher.doc(i); assertTrue(doc.getField(""content"") != null); } // cleanup reader.close(); searcher.close(); } Usually if things work out with RAMDirectory it will pretty much work fine with others. i.e. to permanently store your index. Alternate to this is FSDirectory. You will have to take care of filesystem permissions in this case(which is not valid with RAMDirectory) Functionallythere is not distinct advantage of RAMDirectory over FSDirectory(other than the fact that RAMDirectory will be visibly faster than FSDirectory). They both server two different needs. RAMDirectory -> Primary memory FSDirectory -> Secondary memory Pretty similar to RAM & Hard disk . I am not sure what will happen to RAMDirectory if it exceeds memory limit. I’d except a OutOfMemoryException : System.SystemException thrown. hi thanks for your response...what is the advantage of using RAMDirectory over FSDirectory? And also what if the size of index exceeds the RAM memory? There is a 2GM limit to an object's size. If your RAMDirecotry exceeds that you will get an OutOfMemoryException even if you have plenty of RAM."
422,A,Choosing a solr/lucene commit strategy I have 120k db records to commit into a Solr index. My question is: should I commit after submitting every 10k records or only commit once after submitting all the 120k records? Is there any difference between these two options? The recommended way is to use commitWithin instead of <autoCommit>. If you are using SolrJ almost all methods have a commitWithin parameter to use this feature.  Use Solr's default auto-commit values which I believe are quite reasonable. If not you can adjust them to suit your needs: <!-- autocommit pending docs if certain criteria are met. Future versions may expand the available criteria --> <autoCommit> <maxDocs>10000</maxDocs> <!-- maximum uncommited docs before autocommit triggered --> <maxTime>50000</maxTime> <!-- maximum time (in MS) after adding a doc before an autocommit is triggered --> </autoCommit> This means that it will commit when there are more than 10000 docs waiting to be committed or 50s have passed since a document was added.  According to the Lucene 2.9.3 documentation commit() allows readers to see the added documents and puts all added/deleted documents on the index in the disk. It is a costly operation. So if you want to see part of the documents while adding others or want an assurance that you will not lose an added set of documents larger than 10000 documents you need to commit every 10000 records. OTOH If you prefer to save the extra commits time and are not afraid to lose documents if the machine fails commit only after all of the documents were added. @mizboy I am not sure that it will cost any memory. I believe you already pay the memory price when adding the documents because they are added to the index inside memory. You probably need to benchmark this and decide. if i commit all the records at lastis this will cost many memoryi didn't know the detail of lucene commit
423,A,Zend_Lucene and wilcard operator weirdness A quick summary of my problem the wildcard operator doesn't seem to return the result I am expecting. I am testing this against some Keyword field. Here come a sample showing the issue include 'Zend/Loader/Autoloader.php'; $autoloader = Zend_Loader_Autoloader::getInstance(); $autoloader->setFallbackAutoloader(true); Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8_CaseInsensitive()); @mkdir('/tmp/test-lucene'); $index = Zend_Search_Lucene::create('/tmp/test-lucene'); $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Keyword('path' 'root/1/2/3')); $doc->addField(Zend_Search_Lucene_Field::UnStored('contents' 'The lazy fox jump over the dog bla bla bla')); $index->addDocument($doc); $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Keyword('path' 'root/1')); $doc->addField(Zend_Search_Lucene_Field::UnStored('contents' 'The lazy fox jump over the dog bla bla bla')); $index->addDocument($doc); $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Keyword('path' 'root/3/2/1')); $doc->addField(Zend_Search_Lucene_Field::UnStored('contents' 'The lazy fox jump over the dog bla bla bla')); $index->addDocument($doc); $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Keyword('path' 'root/3/2/2')); $doc->addField(Zend_Search_Lucene_Field::UnStored('contents' 'The lazy fox jump over the dog bla bla bla')); $index->addDocument($doc); $hits = $index->find('path:root/3/2*'); foreach($hits as $hit){ $doc = $hit->getDocument(); echo $doc->getFieldValue('path') . PHP_EOL; } This will return the whole set of documents instead of the last two like I would expected output: root/1/2/3 root/1 root/3/2/1 root/3/2/2 So here my question why lucene (Zend_Lucene in that case) matches the first documents I thought Keyword fields are not tokenized. PS: for those who might wants to know why I am running this test. I have an ecommerce website with some database the category table have some path field. For example a category might have this path '/1/2/3' which means it's category with id 3 and the parent category is index 2 etc ... The problem is when an user do a full text search and specify a category ideally I want to return the results from that category but also children categories so I need a lucene way of doing path LIKE '/1/2%'. One other possibility would be to merge the results from a SQL query and lucene hits if possible I would like to avoid this case because it could performs poorly. If you have any ideas you are welcomed. Use Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8Num_CaseInsensitive and replace the slashes with a character that does not occur in your paths but is a word character to Zend_Search_Lucene. I used german ß. include 'Zend/Loader/Autoloader.php'; $autoloader = Zend_Loader_Autoloader::getInstance(); $autoloader->setFallbackAutoloader(true); Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8Num_CaseInsensitive()); @mkdir('/tmp/test-lucene'); $index = Zend_Search_Lucene::create('/tmp/test-lucene'); foreach (array('root/1/2/3' 'root/1' 'root/3/2/1' 'root/3/2/2') as $path) { $path = str_replace('/' 'ß' $path); $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Keyword('path' $path)); $index->addDocument($doc); } $hits = $index->find(str_replace('/' 'ß' 'path:root/3/2*')); foreach($hits as $hit){ echo str_replace('ß' '/' $hit->getDocument()->getFieldValue('path')) . PHP_EOL; } Thanks for the answers it works. I see so lucene was removing the numbers because of the analyzer. After I cannot really explain why replace the / by a word letter change anything Keyword field shouldn't be tokenized right? Keyword fields aren't tokenized that's true. But the search query is. good point that explains it
424,A,"lucence search all? with QueryParser Here's part of my code. Instead of searching the text in desc i'd like to search in everything (desc title example etc). How do i do this? do i make another field call all and copy each field into it? can i do something like """" null or ""*"" to denote search them all? (I tried each and got no results). How do i search all fields with my text? public static List<Pair<long float>> Search(string text) { var searcher = new IndexSearcher(directory true); var parser = new QueryParser(Lucene.Net.Util.Version.LUCENE_29 ""desc"" analyzer); var query = parser.Parse(text); var hits = searcher.Search(query); // etc } It would be good if lucene implicitly supported the notion of ""all"". You are left with indexing 1 additional filed (name it as ""all"") whose contents will be a concatenation of desc title example etc but do not store it just index. I tried it out and the search works. So storing it helps with highlighting? i never heard of highlighting. So i really dont need to store any of these fields? i'll change it and see what happens. BTW almost all this data is also in my mysql db. I use lucence strictly for searching If you have all the data in DB then just index the data without storing anything in lucene except the primary/natural key. This way you perform search against the lucene index and while displaying you can retrieve the details from mysql db. The link between the lucene index and the mysql db is the primary/natural key hmm what is the difference between yes no (and i can guess compress). I dont understand why that option exist Yes means the data is also stored by lucene and it increases the size of the index. Increase in index size means more time while merging/optimizing operations. But if you want the highlighting functionality then I believe you have to store it. Accept the answer if I've answered your question"
425,A,"Query in Lucene The structure of the table ""testtable"" is id int primary key productid int attributeid int value varchar(250) where productid is the unique id of a product attributeid is the unique id of attribute of a product e.g. size qualityheight color and 'value' is the value for the attribute i have to filter a result. I achieve the requirement by this query. But i am not able to make it in a query. select a.* from dbo.testtable a where a.attributeId=10 and a.[Value]='Romance' and productId in ( select productId from dbo.testtable where attributeId =7 and [Value]='Hindi' ) Need help to build this query.. You can't query tables with lucene. Do you have an existing lucene index for this data? Yes.. The data is already indexed Look at using Hibernate Search which provides you with semantics of lucene based searching on a database. Alternatively look at luke and figure out how lucene has indexed your data. Play around with it and it will help you frame lucene queries as it gives you a deeper look into lucene indexing and searching. Yes you are right. I am using Hibernate Search. [http://en.wikipedia.org/wiki/Entity-Attribute-Value_model]. The requirement is to filter the data using multiple attribute.  I think you have to do this in two steps: Step 1: extract product ids BooleanQuery query = new BooleanQuery(); query.add(new TermQuery(""attributeId"" 7) BooleanClause.Occur.MUST); query.add(new TermQuery(""value"" ""hindi"") BooleanClause.Occur.MUST); TopDocs docs = searcher.search(query null searchLimit); You then need to extract the productId from the docs Step 2: run query BooleanQuery query = new BooleanQuery(); query.add(new TermQuery(""attributeId"" 10) BooleanClause.Occur.MUST); query.add(new TermQuery(""value"" ""Romance"") BooleanClause.Occur.MUST); // build ""IN"" clause BooleanQuery pidQuery = new BooleanQuery(); for( long productId : productIds ){ pidQuery.add(new TermQuery(""productId"" productId) BooleanClause.Occur.SHOULD); } query.add(pidQuery BooleanClause.Occur.MUST); TopDocs docs = searcher.search(query null searchLimit);"
426,A,Solr can't find JDBC driver I haven't done anything with Java in years so I'm trying to get this done as simply as possible. I'm running Ubuntu 10.04. So far I've just done: apt-get install solr-jetty libmysql-java and set up all my config files to pull in documents from my MySQL database. Now however I'm getting this in the logs when I try to do a full import: SEVERE: Full Import failed org.apache.solr.handler.dataimport.DataImportHandlerException: Could not load driver: com.mysql.jdbc.Driver Processing Document # 1 Now I'm a bit stuck because if apt installing libmysql-java didn't get me the JDBC driver I have no idea what will. put the file mysql-connector-java-5.1.22-bin.jar (or its symbol link) to this directory: /var/lib/tomcat6/webapps/solr/WEB-INF/lib/ I know this is for tomcat case but you can do the similar.  I ended up just symlinking to .jars from /usr/share/java; maybe there's a better way though. ln -s /usr/share/java/mysql-connector-java.jar \ /usr/share/solr/WEB-INF/lib/mysql-connector-java.jar sudo ln -s /usr/share/java/mysql.jar /usr/share/solr/WEB-INF/lib/mysql.jar  These error means that your mysql dirver cannot be found on java classpath and you should just add it to it. In case of Jetty you should put your database driver in $JETTY_HOME/lib/ext directory. Just put there symbolic link or copy jar file. If you don't know where is MySql JDBC jar file you can check it using these command: dpkg-query -L libmysql-java Or you can just simply download these driver from MySql download page. PS. I have never used Jetty and I found these information here: http://content.liferay.com/4.2/doc/installation/liferay_4_installation_guide/multipage/ch03.html#d0e893 point 9 Jetty 5.1 installation. That's the ticket! Thank you so much!
427,A,What does Field.Index.NOT_ANALYZED_NO_NORMS mean I know what does not_analyzed mean. In short the field will not be tokenized by specified Analyzer. However what does a NO_NORMS means? I see the documentation but please explain me in plain English. what is index-time field and document boosting and field length normalization ? good question and good answer thanks a lot! Hijacking this question for a little one of my own. What is difference between Field.Index.NOT_ANALYZED_NO_NORMS and Fieldable.SetOmitNorms() method? It disables the following features: index-time field and document boosting: this means that the index will ignore any boosts you did to fields (AbstractField.setBoost) or documents (Document.setBoost). A matching token will always be worth the same. field length normalization: this means that the index will ignore whether a matching token was in a short field (which should be more relevant) vs. a long field (less relevant). Again a matching token will always be worth the same no matter the length of the field. I didn't mean that a matching term doesn't affect ranking I meant that it does in the same magnitude no matter the field's length. so in short a field marked as NO_NORMS does not affect ranking of result at all is that what this mean? This is inexact. If a field is marked as NO_NORM term matches on it DO affect ranking. Just its boost and length do not. Great answer. Do you the difference between Field.Index.NOT_ANALYZED_NO_NORMS and the Fieldable.SetOmitNorms() method?
428,A,"How to write a Lucene query that returns all words containing the letter ""t""? I tried this Lucene code example which worked: http://snippets.dzone.com/posts/show/8965 However changing: Query query = parser.parse(""st.""); to Query query = parser.parse(""t""); returned zero hits. How to write a Lucene query that returns all words containing the letter ""t"" ? (max nbr of hits to return = 20) Edit: here's what worked: RegexQuery regexquery = new RegexQuery(new Term(""fieldname"" "".t."")); isearcher.search(regexquery collector); System.out.println(""collector.getTotalHits()="" + collector.getTotalHits()); I have good news and bad news. The good news is that you can use wildcards to match any text: parser.parse(""st*""); // Will math ""st."" ""station"" ""steal"" etc... Unfortunately the documentation indicates: Note: You cannot use a * or ? symbol as the first character of a search. Meaning you cannot use this syntax: parser.parse(""*t*""); Therefore you cannot ask Lucene to return terms that contain the letter 't' at an arbitrary location. You can ask Lucene to return terms that begin with a certain letter. You're only option at this point appears to be iterating through all terms doing you're own matching. Actually you can use leading wildcards so the query ""*t*"" is possible. You just need to enable them. From the Lucene FAQ (http://wiki.apache.org/lucene-java/LuceneFAQ): ""Leading wildcards (e.g. *ook) are not supported by the QueryParser by default. As of Lucene 2.1 they can be enabled by calling QueryParser.setAllowLeadingWildcard( true ). Note that this can be an expensive operation: it requires scanning the list of tokens in the index in its entirety to look for those that match the pattern."" @Kai: Neat! Wow I really have been away from the Lucene library for a while!  You need a different Analyzer. The example uses StandardAnalyzer which removes punctuation and breaks words according to white space and some other more elaborate rules. It does not however break words into characters. You will probably need to build your own custom analyzer to do this and it seems it will be costly in both run time and memory consumption. Another (probably better) option is to use a RegexQuery. Wow I had never heard of the `RegexQuery`. When did it get added to the library? I admit I haven't worked with Lucene for a few years now. I also heard about it incidentally by reading a colleague's code. From looking at the subversion logs for RegexQuery (one of the joys of open source) it has been in Lucene at least since December 28th 2005. However this is part of contrib and not (yet?) one of Lucene's core queries. This worked:  RegexQuery regexquery = new RegexQuery(new Term(""fieldname"" "".*t.*"")); isearcher.search(regexquery collector); System.out.println(""collector.getTotalHits()="" + collector.getTotalHits()); Ahhh that makes sense. @hjo1620: I see that you were trying to format some code in your comment. All you have to do is surround your code with back ticks (it's just to the left of the '1' key on your keyboard)."
429,A,Extend JackRabbit or build up from Lucene? I've been working on a site idea the general concept is a full text search of documents that also allows user ratings based on these rating I wanted to boost the item's value in the Lucene index. But I'm trying to find if I should extend JackRabbit or just build from the Lucene base. Is there any good way to extend JackRabbit in this way and effect the index or would it be best to work directly off Lucene? Either way I go I am strongly leaning to using groovy on grails with either the searchable plugin or work directly with JackRabbit is there any major reasons I should just stick to Java? Clarification: I would like to boost an item based on the average user rating of an item is JackRabbit open enough or expandable enough where I can capture user ratings then have those effect the index within JackRabbit or is it so far out of the core of JackRabbit I should just build up from Lucene? I would recommend you to use Apache Sling it comes with Jackrabbit/Lucene built-in. Most of the committers are also involved with Jackrabbit so it's designed to work well with it -- even better it's designed to run on top of it. One of the nice features of Sling is that it mounts the entire JCR repository in the URL space and exposes it via REST endpoints. So you can access your documents/metadata very easily by doing a simple HTTP request to it. It also allows you to write your own servlets and expose them as REST endpoints. (This is extremely easy -- no fiddling about with applicationContext.xml files just 1 annotation) It also allows you to write jsp esp groovy ... Sounds interesting how does it let you effect the search results? I don't think you need to extend Jackrabbit/Lucene for this. I would probably add a property on the item called 'my:score' and each time some positive feedback has been left I'd increase the value. Then I would do a standard query and order the items descending on 'my:score'. To keep things fast you would probably have to create an index for the 'my:score' property.  I recommend using JCR with the implementation of Jackrabbit behind it. JCR allows you to separate between what you store and how you store it. By staying within a JCR framework you should be able to easily switch among JCR implementations. (There are several not just Apache's.) Even within Jackrabbit are many persistence managers not just Lucene. This flexibility is useful when you want to trade off between storage space and performance. JCR already includes full text searches and the ability to maintain user ratings. It should be a good fit for your project.  I would recommend using JCR/Jackrabbit on top of Lucene for a couple of reasons: 1) Your repository structure could readily support document nodes with child nodes that store all of your meta-data including owner ratings flagging comments etc. 2) JCR is ideal for document/node based app development providing a lot of the heavy lifting at the framework level while not getting in your way at the app level. So there is a way to make a child node that is meta data effectively boost the parent node in search?  is there any major reasons I should just stick to Java? Not really. As you probably already know you can use any Java library with Groovy/Grails so there's nothing you can do in Java that you can't do in Groovy. Although the contrary is also true in my experience it takes a lot more (boilerplate) code to get things done in Java. Although Java is considerable faster than Groovy this doesn't necessarily mean your app will be faster if written in Java as the bottleneck could likely be the database rather than code execution. As for whether you should use Lucene/Searchable or JackRabbit it's very difficult to say without knowing much about what you can achieve. All you've told us so far is that you want to index documents and boost certain items in the index. You can certainly do both of those with Lucene. I have tried to clarify my question the main question is around JackRabbit vs Lucene. With the groovy question just double checking there is not gotcha with either JackRabbit or Lucene.
430,A,Querying Raven with Where() only filters against the first 128 documents? We're using Raven to validate logins so people can get into our site. What we've found is that if you do this: // Context is an IDocumentSession Context.Query<UserModels>() .SingleOrDefault(u => u.Email.ToLower() == email.ToLower()); The query only filters on the first 128 docs of the documents in Raven. There are several thousand in our database so unless your email happens to be in that first 128 returned you're out of luck. None of the Raven samples code or any sample code I've come across on the net performs any looping using Skip() and Take() to iterate through the set. Is this the desired behavior of Raven? Is it the same behavior even if you use an advanced Lucene Query? ie; Do advanced queries behave any differently? Is the solution below appropriate? Looks a little ugly. :P My solution is to loop through the set of all documents until I encounter a non null result then I break and return . public T SingleWithIndex(string indexName Func<T bool> where) { var pageIndex = 1; const int pageSize = 1024; RavenQueryStatistics stats; var queryResults = Context.Query<T>(indexName) .Statistics(out stats) .Customize(x => x.WaitForNonStaleResults()) .Take(pageSize) .Where(where).SingleOrDefault(); if (queryResults == null && stats.TotalResults > pageSize) { for (var i = 0; i < (stats.TotalResults / (pageIndex * pageSize)); i++) { queryResults = Context.Query<T>(indexName) .Statistics(out stats) .Customize(x => x.WaitForNonStaleResults()) .Skip(pageIndex * pageSize) .Take(pageSize) .Where(where).SingleOrDefault(); if (queryResults != null) break; pageIndex++; } } return queryResults; } EDIT: Using the fix below is not passing query params to my RavenDB instance. Not sure why yet. Context.Query<UserModels>() .Where(u => u.Email == email) .SingleOrDefault(); In the end I am using Advanced Lucene Syntax instead of linq queries and things are working as expected. RavenDB does not understand SingleOrDefault so it performs a query without the filter. Your condition is then executed on the result set but per default Raven only returns the first 128 documents. Instead you have to call Context.Query<UserModels>() .Where(u => u.Email == email) .SingleOrDefault(); so the filtering is done by RavenDB/Lucene. I cross posted this to the google group and Ayende mentioned they don't yet support a predicate in SingleOrDefault(). My problem turns out that linq querying does not pass a query parameter to my server.
431,A,"What are the pros and cons of Solr & ElasticSearch? Both Solr and ElasticSearch are built upon Lucene. How do they compare to each other in terms of: Features (facet & multi-language support in particular) Performance Scalability Stability Manageability Any experiences you have with either software that you can share? Thanks. Well to make is short and simple: Use SOLR if you want to be able to fine tune your performance(by fiddling with the internals) want more control and also a huge community. Use elastic search if you want faster deployment ready to live with a lesser grain of control(there are advanced options though) and get the actual output(during development) that you want to get during deployment. Both are known to be scalable and stable and offer great performance. PS: I have read about a person 'getting stuck' with some minor problems/bugs in elastic search. However there are plenty that are satisfied. :D  I can only speak about Solr since that is what we are using. I deployed a Solr stack on EC2 and we are handling several million records on each database. I currently have a master/slave setup and a very nice schema defined. Besides the obvious performance benefits of Solr and the amazing queries you can perform one thing that's often overlooked is just how easy it is to setup and learn! It took me a day to read Solr 1.4 Enterprise Search Server and within a week I had built pretty solid Master & Slave AMI's on Amazon EC2 performed load testing and configured my Map Reduce jobs to continuously pump data into the Solr Master. My two cents. A week of ""building"" and reading ""Enterprise Search Server"" documents may be easy by some definition of easy... but for the sake of completeness these tasks take hours in ElasticSearch."
432,A,"Lucene's nested query evaluation regarding negation I am adding Apache Lucene support to Querydsl (which offers type-safe queries for Java) and I am having problems understanding how Lucene evaluates queries especially regarding negation in nested queries. For instance the following two queries in my opinion are semantically the same but only the first one returns results. +year:1990 -title:""Jurassic Park"" +year:1990 +(-title:""Jurassic Park"") The simplified object tree in the second example is shown below. query : Query clauses : ArrayList [0] : BooleanClause ""MUST"" occur : BooleanClause.Occur ""year:1990"" query : TermQuery [1] : BooleanClause ""MUST"" occur : BooleanClause.Occur query : BooleanQuery clauses : ArrayList [0] : BooleanClause ""MUST_NOT"" occur : BooleanClause.Occur ""title:""Jurassic Park"""" query : TermQuery Lucene's own QueryParser seems to evaluate ""AND (NOT"" into the same kind of object trees. Is this a bug in Lucene or have I misunderstood Lucene's query evaluation? I am happy to give more information if necessary. They are not semantically the same. In +year:1990 +(-title:""Jurassic Park"") You have a subquery that only has one NOT clause. What's happening is that Lucene is evaluating the -title:""Jurassic Park"" clause and it's returning 0 documents. Then you're indicating that the subquery MUST occur and since it's return zero documents it negates the rest of the query. Thanks a bunch it makes perfect sense now. How does do a NOT only search when one really needs too.. Ok..This seems to make sense. But what if (-title:""Jurassic Park"") returned more than 1 documents..Does that signify if +year:1990 is present in that document only that document will be provided as hit ??..Seems like that...Just want to understand a bit more."
433,A,Solr Incremental backup on real-time system with heavy index I implement search engine with solr that import minimal 2 million doc per day. User must can search on imported doc ASAP (near real-time). I using 2 dedicated Windows x64 with tomcat 6 (Solr shard mode). every server index about 120 million doc and about 220 GB (total 500 GB). I want to get backup incremental from solr index file during update or search. after search it find rsync tools for UNIX and DeltaCopy for windows (GUI rsync for windows). but get error (vanished) during update. how to solve this problem. Note1:File copy really slow when file size very large. therefore i can't use this way. Note2: Can i prevent corrupt index files during update if windows crash or hardware reset or any other problem ? Don't run a backup while updating the index. You will probably get a corrupt (therefore useless) backup. Some ideas to work around it: Batch up your updates i.e. instead of adding/updating documents all the time add/update every n minutes. This will let you run the backup in between those n minutes. Cons: document freshness is affected. Use a second passive Solr core: Set up two cores per shard one active and one passive. All queries are issued against the active core. Use replication to keep the passive core up to date. Run the backup against the passive core. You'd have to disable replication while running the backup. Cons: complex more moving parts requires double the disk space to maintain the passive core. Isn't the passive index already the perfect backup? I am not very familiar with backup stuff. But what is off-site storage? (You could place the passive index on another server) @Karusell: off-site storage: placing copies of the backup in other buildings/cities/states/countries. The passive index should be as closest as possible to the main index to make replication fast. Backup should also be done close to the passive index to keep replication disabled as little as possible. Only when you have that backup you can choose to store it off-site. @Karussell: it's just a copy and not a proper backup by itself since you can't apply backup policies like off-site storage incremental/differential/full backup etc. There's a lot more to backup than just copying stuff. thanks a lot Mauricio  You can take a hot backup (i.e. while writing to the index) using the ReplicationHandler to copy Solr's data directory elsewhere on the local system. Then do whatever you like with that directory. You can launch the backup whenever you want by going to a URL like this: http://host:8080/solr/replication?command=backup&location=/home/jboss/backup Obviously you could script that with wget+cron. More details can be found here: http://wiki.apache.org/solr/SolrReplication The Lucene in Action book has a section on hot backups with Lucene and it appears to me that the code in Solr's ReplicationHandler uses the same strategy as outlined there. One of that book's authors even elaborated on how it works in another StackOverflow answer.
434,A,Zend Lucene is under processing now... crashed my website {Zend_Search_Lucene_Exception} Index is under processing now This is what i get in my error log which makes crash the 50% of my website. What should i do to fix that please ? Thanks If you mean a 500 error there is an `error.log` that should contain a description of what went wrong. {Zend_Search_Lucene_Exception} Index is under processing now is only what i get in my error.log ok i deleted the whole index and put the chmod777 on the entire folders+subfolders index and it fixed the problem.
435,A,Lucene TermFrequenciesVector what do I obtain if I call IndexReader.getTermFrequenciesVector(...) on an index created with TermVector.YES option? What do you mean? Do you have a specific question that the [docs](http://lucene.apache.org/java/2_1_0/api/org/apache/lucene/index/IndexReader.html#getTermFreqVector%28int%20java.lang.String%29) don't answer? Or you can implement proximity or first occurrence type score contributions. Which highlighting won't help you with at all.  The documentation already answers this as Xodorap notes in a comment. The TermFreqVector object returned can retrieve which terms (words produced by your analyzer) a field contains and how many times each of those terms exists within that field. You can cast the returned TermFreqVector to the interface TermPositionVector if you index the field using TermVector.WITH_OFFSETS TermVector.WITH_POSITIONS or TermVector.WITH_POSITIONS_OFFSETS. This gives you access to GetTermPositions with allow you to check where in the field the term exists and GetOffsets which allows you to check where in the original content the term originated from. The later allows combined with Store.YES highlighting of matching terms in a search query. There are different contributed highlighters available under Contrib area found at the Lucene homepage.
436,A,mg4j vs. apache lucene Can anyone provide a simple comparative analysis of these search engines? What advantages does either framework have? BTW I've seen the following basic explanations of choosing mg4j from several academic papers: combining indices over the same collection multi-index queries Update: These slides (from mir2ed.org) contain a more fresh overview of open source search engines including Lucene and mg4j on benchmarking various aspects: memory & CPU index size search performance search quality etc. Jeff Dalton reviewed many open source search engines including Lucene and mg4j in 2007 and updated the comparison in 2009. I have not used mg4j. I have used Lucene though. The number one feature of Lucene IMO is its wide adoption and wonderful community of users/developers/committers. This means that there is a fair chance that somebody worked on a use case similar to yours using Lucene. Current weak points of Lucene are its scoring model and its ability to scale to large collections of text. The Lucene developers are working on these issues. I believe that the choice of a search library is very dependent on your (academic or industrial) setting the other parts of your application and your use case. Thanks. What about [SOLR](http://lucene.apache.org/solr/features.html)? Does it solve these issues of Lucene? Solr is a search engine that adds functionality to Lucene. It adds some scaling abilities to Lucene and is much more easy to start working with. Solr Cloud - http://wiki.apache.org/solr/SolrCloud is an effort to make Solr much more robust and scalable. The scoring in Solr is identical to Lucene.
437,A,"How do I perform an AND search in Lucene.net when multiple words are used in a search? I am playing around with Lucene.net to try and get a handle of how to implement it in my application. I have the following code  ..... // Add 2 documents var doc1 = new Document(); var doc2 = new Document(); doc1.Add(new Field(""id"" ""doc1"" Field.Store.YES Field.Index.ANALYZED)); doc1.Add(new Field(""content"" ""This is my first document"" Field.Store.YES Field.Index.ANALYZED)); doc2.Add(new Field(""id"" ""doc2"" Field.Store.YES Field.Index.ANALYZED)); doc2.Add(new Field(""content"" ""The big red fox jumped"" Field.Store.YES Field.Index.ANALYZED)); writer.AddDocument(doc1); writer.AddDocument(doc2); writer.Optimize(); writer.Close(); // Search for doc2 var parser = new QueryParser(Lucene.Net.Util.Version.LUCENE_29 ""content"" new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_29)); var query = parser.Parse(""big abcdefg test1234""); var searcher = new IndexSearcher(indexDirectory true); var hits = searcher.Search(query); Assert.AreEqual(1 hits.Length()); var document = hits.Doc(0); Assert.AreEqual(""doc2"" document.Get(""id"")); Assert.AreEqual(""The big red fox jumped"" document.Get(""content"")); This test passes which dismays me a bit. I assume this means that Lucene.Net uses OR for searches between terms and not an AND but I can't find any information on how to actually perform an AND search. The end result I am going for is if someone searches for ""Matthew Anderson"" I don't want it to bring up documents that refer to ""Matthew Doe""  as that isn't relevant in any way shape or form. What do you get when your query is var query = parser.Parse(""+big +abcdefg +test1234""); That should cause the parser to require all terms to be present in matching documents. Another possibility is to construct the query programmatically. BooleanQuery query = new BooleanQuery(); query.add(new BooleanClause(new TermQuery(new Term(""field"" ""big""))) Occur.MUST); query.add(new BooleanClause(new TermQuery(new Term(""field"" ""abcdefg""))) Occur.MUST); query.add(new BooleanClause(new TermQuery(new Term(""field"" ""test1234""))) Occur.MUST);  A. If you require all words to be in a document but don't require the words to be consecutive and in the order you specify: The query +big +red matches * the big red fox jumped * the red big fox jumped * the big fast red fox jumped but does not match * the small red fox jumped B. If you want to match a phrase (i.e. all words required; the words have to be consecutive and in the order specified) instead: The query +""big red"" matches * the big red fox jumped but does not match * the red big fox jumped * the big fast red fox jumped * the small red fox jumped Ok that makes sense. Marking this as the answer since you gave me examples for consecutive matching as well. I guess I need to write a string parser to convert user input into a correct lucene query string that should be interesting. sorry to ask after long time. if i want to return data if there the word foudn called Red but big is not there or big is there but red is not there then how the query should look like. @Thomas The query syntax supports grouping clauses by parentheses. So this should work: `(+red -big) (+big -red)`"
438,A,"Lucene: Setting minimum required similarity on searches I'm having a lot of trouble dealing with Lucene's similarity factor. I want it to apply a similarity factor different than its default (which is 0.5 according to documentation) but it doesn't seem to be working. When I type a query that explicitly sets the required similarity factor like [tinberland~0.5] (notice that I wrote tiNberland with an ""N"" while the correct would be with an ""M"") it brings many products by the Timberland manufacturer. But when I just type [tinberland] (no similarity factor explicitly defined) and try to set the similarity via code it doesn't work (returns no results). The code I wrote to set the similarity is like: multiFieldQueryParser.SetFuzzyMinSim(0.5F); And I didn't change the Similarity algorithm so it is using the DefaultSimilarity class. Isn't that the correct or recommended way of applying similarity via code? Is there a specific QueryParser for fuzzy queries? Any help is highly appreciated. Thanks in advance! What you are setting is the minimal similarity so e.g. if someone searched for foo~.1 the parser would change it to foo~.5. It's not saying ""turn every query into a fuzzy query."" You can use MultiFieldQueryParser.getFuzzyQuery like so: Query q = parser.getFuzzyQuery(field term minSimilarity); but that will of course require you calling getFuzzyQuery for each field. I'm not aware of a ""MultiFieldFuzzyQueryParser"" class but all it would do is just combine a bunch of those getFuzzyQuery calls. Oh I see... that makes sense now. Thanks!"
439,A,solr 3.1 - where to download I wanted to download solr 3.1 but the links from solr's official site are not working: http://wiki.apache.org/solr/FrontPage#solr_development any ideas ? cheers /Marcin solr 3.1??? 1.4 the latest from http://mirror.nus.edu.sg/apache//lucene/solr/ http://wiki.apache.org/solr/Solr3.1 - Solr 3.1 has not been released Technically not a programming question and likely too localised. I've found the correct link in here: http://archive.apache.org/dist/lucene/solr/3.1.0/  You should check out the 3x branch from SVN: http://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x/ This link is also not working anymore. Solr 3.1 is now outdated. The latest stable release (3.6.1 as of this writing) can be found on http://lucene.apache.org/solr/downloads.html This link doesn't work  3.1 Release: http://lucene.apache.org/#31+March+2011+-+Lucene+Core+3.1+and+Solr+3.1+Available Download: http://www.apache.org/dyn/closer.cgi/lucene/solr/ That link no longer works.... Updated linked 3.1 release Looks like the links need an update again. Or is this question to localized?
440,A,What does Lucene's ScoreDoc.score mean? I am performing a boolean query with multiple terms. I only want to process results with a score above a particular threshold. My problem is I don't understand how this value is calculated. I understand that high numbers mean its a good match and low numbers mean its a bad match but there doesn't seem to be any upper bounds? Is it possible to normalize the scores over the range [01]? Here is a page describing how scores are calculated in Lucene: http://lucene.apache.org/java/3_0_0/scoring.html The short answer is that the absolute values of each document's score doesn't really mean anything outside the context of a given search result set. In other words there isn't really a good way of translating the scores to a human definition of relevance even if you do normalize the scores. That being said you can easily normalize the scores by dividing each hit's score by the maximum score. So if the first hit's score is 2.5 then divide every hit's score by 2.5 and you'll get a number in between 0 and 1. Thanks this link is exactly what I am looking for.
441,A,Can zend's lucene implementation be configured to use a mysql database instead of the file system? Is there an option for Zend's lucene implementation (or a third-party plugin) that would allow me to put the lucene dictionary into a [MySQL] database? The reason I need to ask is that the database is the only common resource for our two otherwise independent web servers. Although there are interfaces that could be implemented I didn't find any readily available plugin. We will switch to another implementation I'm currently evaluating solr.
442,A,"Cross Referencing Databases on Fuzzy Data I am currently working on project where I have to match up a large quantity of user-generated names with a separate list of the same names in a canonical format. The problem is that the user-generated names contains numerous misspellings abbreviations as well as simply invalid data making it hard to do a cross-reference with the canonical data. Any suggestions on methods to do this? This does not have to be done in real-time and in this case accuracy is more important than speed. Current ideas for this are: Do a fuzzy search for the user entered name in the canonical database using an existing search implementation like Lucene or Sphinx which I presume use something like the Levenshtein distance for this. Cross-reference on the SOUNDEX hash (which is supposedly computed on the sound of the name rather than spelling) instead of using the actual name. Some combination of the above Anyone have any feedback on any of these or ideas of their own? One of my concerns is that none of the above methods will handle abbreviations very well. Can anyone point me in a direction for some machine learning methods to actually search on expanded abbreviations (or tell me I'm crazy)? Thanks in advance. First I'd add to your list the techniques discussed at Peter Norvig's post on spelling correction. Second I'd ask what kind of ""user-generated names"" you're talking about. Having dealt with both I believe that the heuristics you'd use for street names are somewhat different from the heuristics for person names. (As a simple example does ""Dr"" expand to ""Drive"" or ""Doctor""?) Third I'd look at a combination using testing to establish the set of coefficients for combining the results of the various techniques. Thanks I feel like there really is no perfect answer to this. I've decided to go with using Lucene as the main way of cross referencing and to use different/custom Analyzer's to expand abbreviations and to do the fuzzy searching."
443,A,"Full Text Searching with Rails I've been looking into searching plugins/gems for Rails. Most of the articles compare Ferret (Lucene) to Ultrasphinx or possibly Thinking Sphinx but none that talk about SearchLogic. Does anyone have any clues as to how that one compares? What do you use and how does it perform? SearchLogic is a good plugin but is really meant to make your search code more readable it doesn't provide the automatic indexing that Sphinx does. I haven't used Ferret but Sphinx is incredibly powerful. http://railscasts.com/episodes/120-thinking-sphinx Great introduction to see how flexible it is.  Given this question is still highly ranked at google for full text search I'd really like to say that Sunspot is even stronger today if you're interested in adding full text search capabilities to your Rails application (and would like to have Solr behind you for that). You can check a full tutorial on this here. And while we're at it another contender that has arrived in the field is ElasticSearch that aims to be a real time full text search engine built on top of Lucene (but doing things differently when compared to Solr). ElasticSearch includes out-of-the-box sharding and replication to multiple nodes faster real time search ""percolators"" to allow you to receive notifications when something that matches your criteria becomes available and it's moving really fast with many more other features. It's easy to build something on top of it since the API is dead simple and completely based on REST using JSON as a format. One could say you don't even need a plugin to use it.  First off my obvious bias: I created and maintain Thinking Sphinx. As it so happens I actually saw Ben Johnson (creator of SearchLogic) present at the NYC ruby meet about it last night. SearchLogic is SQL-only - so if you're not dealing with massive tables and relevance rankings aren't needed then it could be exactly what you're looking for. The syntax is pretty clean too. However if you want all the query intelligence handled by code that is not your own then Sphinx or Solr (which is Lucene under the hood I think) is probably going to work out better.  Personally I don't bother with database agnostics for web applications and am quite happy using the full text search in pg83. The benefit is if and when you change your framework/language that you will still have full text search.  For anyone looking for a simple search gem without any dependencies check out acts_as_indexed  I have not used SearchLogic but I can tell you that Lucene is a very mature project that has implementation in many languages. It is fast and flexible and the API is fun to work with. It's a good bet.  thinking_sphinx and sphinx work beautifully no indexing query install problems ever (5 or 6 install including production slicehost ) why doesn't everybody use sphinx like say craigslist? read here about its limitations (year and a half old articles. The sphinx developer Aksyonoff is working on these and he's putting in features and reliability and stamping out bugs at an amazing pace) http://codemonkey.ravelry.com/2008/01/09/sphinx-for-search/ http://www.ibm.com/developerworks/opensource/library/os-php-apachesolr/ http://stackoverflow.com/questions/737275/pros-cons-of-full-text-search-engine-lucene-sphinx-postgresql-full-text-searc ferret: easy install doesn't stem properly very slow indexing (one mysql db: sphinx: 3 seconds ferret: 50 minutes). Well documented problems (index corruption) in drb servers in production under load. Having said that i have use it in develometn since acts-as_ferret came out 3 years ago and it has served me well. Not adhering to porter stemming is an advantage in some contexts. Lucene and Solr is the gorilla/mack truck / heavyweight champ of open source search. The teams have been doing an impressive number of new features in solr 14 release: acts-as-solr: works well once the tomcat or jetty is in place but those sometimes are a pain. The A-A-S fork by mattmatt is the main fork but the project is relatively unmaintained. re the tomcat install: SOLR/lucene has unquestionably the best knowledge base/ support search engine of any software package i've seen ( i guess i'm not that surprised) the search box here: http://www.lucidimagination.com/ Sunspot the new ruby wrapper build on solr-ruby. Looks promising but I couldn't get it to install on OSX. Indexes all ruby objects not just databases through AR one thing that's really instructive is to install 2 search plugins e.g. sphinx and SOLR sphinx and ferret and see what different results they return. It's as easy as @sphinx_results - @ferret_results just saw this post and responses http://zooie.wordpress.com/2009/07/06/a-comparison-of-open-source-search-engines-and-indexing-twitter/ http://www.jroller.com/otis/entry/open_source_search_engine_benchmark http://www.flax.co.uk/blog/2009/07/07/xapian-compared/"
444,A,"using OR and NOT in solr query I'm working on a solr query similar to the following: ((myField:superneat AND myOtherField:somethingElse) OR NOT myField:superneat) When running this no results are returned. Using criteria on either side of the OR NOT returns results that I'd expect - they are just not working well together. In the case that myField matches superneat I'm intending to also ensure that myOtherField is set to somethingElse but if myField is not superneat include it in the results. Can someone explain why solr is not returning results for this kind of query? Should the query be restructured somehow - or is there a different way in which solr can be used to achieve the desired result? I don't know why that doesn't work but this one is logically equivalent and it does work: -(myField:superneat AND -myOtherField:somethingElse) Maybe it has something to do with defining the same field twice in the query... Try asking in the solr-user group then post back here the final answer! Thank you for your help! This does indeed work - and I have posed this to the solr-user group. I'll post any useful things I hear from them here. Note that `-myField:superneat OR myOtherField:somethingElse` would also be the same and is slightly simpler. @YorickSijsling the point is that even though logically equivalent Solr sometimes doesn't cope very well with purely negative queries like the one the OP posted or the one you posted. @Mauricio Scheffer - I would question that entirely. Could you explain more on how it doesn't cope very well? We run rather complex conditionals here and found it copes very well with billions of documents. @terrance.a.snyder ""complex conditionals"" != ""purely negative queries"". Also things might have improved in recent versions of Solr I haven't checked. In Solr 4.1 I get this problem on date ranges like you answered here: http://stackoverflow.com/questions/1343794/searching-for-date-range-or-null-no-field-in-solr I found that as of Solr 3.6 if I wanted to do a negative query OR'd with another condition I needed to do ""( -myfield:myValue ) OR myField:otherValue"". If I did not include spaces within the parenthesis it did not parse. I think that is corrected in 4+.  Instead of ""NOT [condition]"" use ""(*:* NOT [condition])"" Thanx alot! This one worked for me even for complex queries while -(myField:superneat AND -myOtherField:somethingElse) approach - didn't!  Solr currently checks for a ""pure negative"" query and inserts *:* (which matches all documents) so that it works correctly. -foo is transformed by solr into (*:* -foo) The big caveat is that Solr only checks to see if the top level query is a pure negative query! So this means that a query like bar OR (-foo) is not changed since the pure negative query is in a sub-clause of the top level query. You need to transform this query yourself into bar OR (*:* -foo)  You can find the follow up to the solr-user group on: solr user mailling list The prevailing thought is that the NOT operator may only be used to remove results from a query - not just exclude things out of the entire dataset. I happen to like the syntax you suggested mausch - thanks!  Putting together comments from a couple different answers here in the Solr docs and on the other SO question I found that the following syntax produces the correct result for my use case (my_field=my_value or my_field is null): (my_field:""my_value"" OR (*:* NOT my_field:*)) This works for solr 4.1.0. This is slightly different than the use case in the OP; but I thought that others would find it useful."
445,A,"Lucene keyword notification What's the best way to get a notification (say an event) when a keyword is found in a document in Lucene? The brute force way is to keep searching for the keyword in short intervals but that seems very inefficient as well as not as ""real-time"" Unless I am missing something how about looking through the tokens of the n+1th document when you are adding it? Why do you need to hit the entire index? Take a look at MemoryIndex: http://lucene.apache.org/java/2_2_0/api/org/apache/lucene/index/memory/MemoryIndex.html It's part of Lucene contrib and it is designed for publish/subscribe systems that you're building. However it's been a while since I've looked at this and I'm not sure if it's being actively maintained. I think this is closest I can get to what I want. Although I'm not quite comfortable with the fact that it holds the index in memory. I'd rather have a scalable but slower solution."
446,A,"Solr Index appears to be valid - but returns no results Solr newbie here. I have created a Solr index and write a whole bunch of docs into it. I can see from the Solr admin page that the docs exist and the schema is fine as well. But when I perform a search using a test keyword I do not get any results back. On entering * : * into the query (in Solr admin page) I get all the results. However when I enter any other query (e.g. a term or phrase) I get no results. I have verified that the field being queried is Indexed and contains the values I am searching for. So I am confused what I am doing wrong. what field type is Title? Title field is Indexed String and Stored same as Description (which is my default search field) BTW I am using the Lucid Imagination Solr/Tomcat which runs through the setup application. Switch to `text` field type. See my updated answer. aha! thanks so much! let me try that... Probably you don't have a <defaultSearchField> correctly set up. See this question. Another possibility: your field is of type string instead of text. String fields in contrast to text fields are not analyzed but stored and indexed verbatim. That was it! Converting to text field typed did the trick. Silly me well you learn something new every day. Thanks Mauricio. Thanks Mauricio - I do have a default field setup. I also have tried field-specific queries e.g. Title:Patterns. But 0 results is perplexing. I had the same issue field type of text instead of string fixed it. Thanks!  With solr 4 I had to solve this as per Mauricio's answer by defining type=""text_en"" to the field."
447,A,"Why Zend Lucene doesn't find results but Luke does for the same fuzzy query Hello I am coding search engine using Zend Framework Lucene. I'm trying to make fuzzy query: ""name:sxample~"" When I put it into Luke - it founds 14 results (all with word ""sample""). When I use my php code - $query = 'name:sxample~'; $query = Zend_Search_Lucene_Search_QueryParser::parse($query'utf-8'); try { $hits = $index->find($query); } catch (Zend_Search_Lucene_Exception $ex) { $hits = array(); } - the hits array is empty. I guess indexing is ok while Luke and ZF uses the same files. I'm using Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8 as my analyzer. Can you tell me what is wrong with my php query or is it maybe ZF bug? Greetings Could it be that you misspelled sample as sxample in your PHP code? Misspel was purposeful - I wanted that user can find answer for misspele query f.e. - saample. After research I found what to change - if misspelled letter is in the first 3 letters - then it isn't found. I had to set : Zend_Search_Lucene_Search_Query_Fuzzy::setDefaultPrefixLength(1);  Have you tried putting a var_dump of $ex inside the catch-statement to ensure you don't get an exception thus setting the $hits-variable to an empty array? I've tried it - the query is correct no exception."
448,A,"How to perform a wildcard search in Lucene I know that Lucene has extensive support for wildcard searches and I know you can search for things like: Stackover* (which will return Stackoverflow) That said my users's aren't interested in learning a query syntax. Can Lucene preform this type of wild card search using an out-of-box Analyzer? Or should I append ""*"" to every search query? Doing this with string manipulations is tricky to get right especially since the QueryParser supports boosting phrases etc. You could use a QueryVisitor that rewrites TermQuery into PrefixQuery. public class PrefixRewriter : QueryVisitor { protected override Query VisitTermQuery(TermQuery query) { var term = query.GetTerm(); var newQuery = new PrefixQuery(term); return CopyBoost(query newQuery); } } The QueryVisitor class can be found at A QueryVisitor for Lucene.  If I want to do something like that I normally format the term before searching e.g. searchTerm = QueryParser.EscapesearchTerm); if(!searchTerm.EndsWith("" "")) { searchTerm = string.Format(""{0}*"" searchTerm); } which will escape any special characters people have put in. and if the term doesnt ends with a space appends a * on the end. Since * on its own would cause a parsing exception.  If you are considering turning every query into a wildcard I would ask myself these questions: Is Lucene the best tool for the job? by default wildcards rewrite to constant-score queries which means you are throwing away relevance ranking completely and no longer ""searching"" but instead ""matching"". Perhaps for your application a search engine library is not the best solution and another tool (e.g. database) would be better. If the answer to #1 is still 'yes' then I would recommend taking a look at what the exact relevance problem is that you are trying to solve. For example if its that you want queries to match compound or stemmed words maybe instead add a decompounder or stemmer to your analysis chain instead. You can also consider using an n-gram indexing technique as another alternative. LUCENE Wild Card search is still faster than SQL Server for large amounts of records."
449,A,Rails - Sunspot conditional model indexing Is there any way to determine on runtime if a model should be indexed or not? Something like: class Article < ActiveRecord::Base searchable :if => :indexable? do ... end private def indexable? ... end end Answered here Exclude draft articles from Solr index with Sunspot  Here's a good article on conditional indexing: http://mikepackdev.com/blog_posts/19-conditional-indexing-with-sunspot That is exactly the API you propose in your question.
450,A,"Using Lucene to query an RDBMS database I've skimmed the docs for the Java version of Lucene but I can't really see the top-level ""this is how it works"" info so far (I'm aware I need to RTFM I just can't see the wood for the trees). I understand Lucene uses search indexes to return results. As far as I know it only returns ""hits"" from those indexes. If I haven't added an item of data when building the index then it won't be returned. That's fine so now I want to check the following assumption: Q: Does that mean that any data I want displayed on a search page needs to be added to the Lucene index? I.e. If I want to search for Products by things like sku description category name etc but I also want to display the Customer they belong to in search results do I: Make sure the Lucene index has the denormalised Customer's name in the index. Use the hits returned by Lucene to somehow query the database for the actual product records and use a JOIN to get the Customer's name. I assume it's option 1 since I'm assuming there's no way to ""join"" the results of a Lucene query to an RDBMS but wanted to ask it my assumptions about the general usage are correct. Based on BrokenGlass's answer I've thought some more and am proposing the following to see if I'm on the right lines: Basically taking option 2 further one could do the following: Put only the data you want to search on into the Lucene index plus some sort of key value (e.g. the PK of a table in your database). Query Lucene to get a list of hits. Using your data access layer of choice build a query for your database that includes an IN (value [ value]) predicate. Get the results for that query from your database (which may well include JOINs to other tables). Put those results in a dictionary using the PK of the resultset as the key. Iterate the Lucene hits again in order pulling the items from the dictionary using the PK so you can build a list of results in the order that Lucene returned the hits (i.e. sorted by relevance). Display that ""sorted"" list of results to the user. Of course steps 5 and 6 could be better but for the sake of explanation I put that verbose method in my description. If the Lucene hits include some sort of ""relevance"" value then you could attribute that to the resultset and perform a standard sort but that's an exercise for the reader. :) Could this be it? this is what I would recommend for searching over a ""large"" data set with Lucene - if you don't have that much data you might as well put everything into your index though and not worry about it - if you do want to scale you must minimize the index though.  I have been trying to figure out the same problem but I think that its too much work. I'm thinking of this as an alternative. Plse correct me if I'm wrong in my thinking! Your situation is like this: RDBMS product (many) <------> (many) Customer Instead of putting only customer in lucene index to get product keys and then query RDBMS with IN Query I'd suggest create the lucene index with the cartesian product of Product as well as Customer. Like customer_1 product_1 customer_1 product_2 customer_2 product_2.. This way when you are searching for a product in lucene it will give both the customer as well as the products id.. and instead of joining them in RDBMS you can simply look up those customers as well as products for more information from RDBMS if there is a need. If you are using caching then the additional details lookup cost will also go down. I like this idea. I see your point - to put more information in the index in the first place. It does require a bit more forward-thinking but if there's a good way to rebuild or update indexes in this way as the app is upgraded then it should work out fine.  Usually the index would only contain the fields you want to search on not necessarily the ones you want to display. Indexes should be optimized to be as small as possible to keep search performance good. To be able to display more data add a field to your index that allows you to retrieve your full document/data i.e. a unique key for your Product (product id?). As always it depends - this approach is for optimizing search performance but not display performance. In general though especially with a paged results view the hit for retrieving a few product details at a time is negligible compared to blowing up your lucene index Ahh so you propose option 2 then? I'm fine with that but if I get a list back I'd have to hit the database once for each item in the results to get the full details back or have a potentially massive `IN ([PK])` statement that I build from the hits. I might answer my own question with that and see if it gets any votes to suggest I'm on the right lines - thanks for getting my brain working! :)"
451,A,BooleanQuery Too Many Clauses in Lucene For a bit of background to know what i am doing: http://stackoverflow.com/questions/2409870/using-hit-highlighter-in-lucene Now to solve this problem i am setting maxclause count to 50000. it works. can there be any problems by increasing the number of clauses If you have too many clauses performance will suffer especially if you are running disjunctive (OR) queries. In some rare cases you might run out of memory.
452,A,"Lucene number extracting I have this number extracting problem. I want to get all matches that don't have a certain number in it ex : 125501874 125001873 Every number that as 55 at the position 2 are not to be considered. The first numbers range is 0 to 9 and the second is 1-9 so the real range is [01-99] (we cannot have 00 as the first two number) With Lucene I wanted to add NOT field:[01-99]55* But it doesn't seem to work. Is there an easy way to find ??55* and disregard it in a Search(""NOT field:[01-99]55*"")? Thank you Lucene guru Thank you erickson Your solution is probably the best using ParallelReader if only I could use temporary indexes cause we cache the search query we will need those later. But like you said before better start with an index on the relevant digits straighaway. I have another solution. NOT field:0?55* NOT field:1?55* ... NOT field:9?55* It is efficient enough for the search I'm doing and it bypass the first character wildcard limitation. I wouldn't use that if their where more digits to check or if they where farther from the start. Now I'm testing this on a million of row and it's pretty efficient for our needs. Yes that's a good workaround too. If you are sure that the first two characters in field are always 01-99 can you just use ""NOT field:??55*"" I tried that the limitations is this one : http://lucene.apache.org/java/2_3_2/queryparsersyntax.html#Wildcard%20Searches ""Note: You cannot use a * or ? symbol as the first character of a search""  Lucene can do this very efficiently if one creates an ""index-only"" field with only the third and fourth digits in it. The complete value can be ""stored"" (or stored and indexed if other queries use the whole number) in the original field. Update: A followup comment asked ""Is [there] a way to create a temporary index on only the second digit?"" Using a ParallelReader ""vertically partitions"" the fields of an index. One partition could hold the current index with its fields while the other is a temporary index with the new field possibly stored in a RAMDirectory. Assuming the number is ""stored"" in the original index iterate over each document in the original index retrieve the stored field parse out the key digits and add a Document to the temporary index with the new field. As the ParallelReader documentation states it is imperative that the document numbers match in both indexes. And if I don't have the possibility to add another index? We have already an index on those number. Is their a way to create a temporary index on only the second digit?"
453,A,performance comparision between Zend Lucene and Java Lucene Zend Lucene and Java Lucene are built in PHP and java repectively and PHP language has a higher level than java. Just wondering How big the performance difference among these two regarding to index building and data searching? Is it much more effective to let java create and rebuild index and let php use the index? This is a quote from a Zend Certified Engineer. Against my better judgment the company I work for migrated our previous search solution to Zend_Search_Lucene. On pretty heavy-duty hardware indexing a million documents took several hours and searches were relatively slow. The indexing process consumed vast amounts of memory and the indexes frequently became corrupted (using 1.5.2). A single wild card search literally brought the web server to its knees so we disabled that feature. Memory usage was very high for searches and as a result requests per second necessarily declined heavily as we had to reduce the number of Apache child processes. We have since moved to Solr (a Lucene-based Java search server) and the difference is dramatic. Indexing now takes around 10 minutes and searches are lightning fast. What a difference a language makes. Original Article In this case Java. Ditto Inkspeak as well. Thanks. I thnk I would still stick to Zend_Search_Lucene at the beginning for handy development with Zend framework. I then move on to advanced solutions once the performance is unacceptable. Also Zend Lucene is compatible with index built by java Lucene. So I can easily make a shift to JAVA Lucene indexing in the future Here's a tip - when developing abstract your search calls so that you can easily plugin whatever search engine you choose to use. This will save you time should you ever need to re-engineer your code for a different data source. nice idea. I will keep it in mind :)  When I asked this question of a Zend Evangelist and one of their hired guns I was told the Java Lucene would provide substantially better performance. Their main points dealt with the handling of UTF8 characters and the speed of indexing (if I remember properly). Apparently Java Lucene is far better at this. http://stackoverflow.com/questions/1254759/does-zend-lucene-need-java-lucene/1316756#1316756
454,A,"Multiple Field Search using Lucene Parser with Solr Using Sunspot I'm using Solr with Sunspot (ruby) and due to other constraints i have to use the Lucene parser instead of the DisMax parser. I need to be able to search using username as well as first_name fields at the same time. If i were using DisMax i can specify qf=""username+first_name"" but using only the lucene parser I am only able to set df (default field) and it will not allow me to specify more than one field. How can I search multiple fields using the lucene parser? Update: Answer: just use the q parameter adjust_solr_params do |params| params[:defType] = ""lucene"" params[:q] = ""username:\""#{params[:q]}\"" OR first_name:\""#{params[:q]}\"""" end do you mean the standard query handler? did you try q=username:xy OR first_name:bob ? you could even boost them differently with ^2 or similar You can use copy fields instructions in your schema to create a ""catch all"" field from all the fields you want to search on. You then set df to that field.  To expand on Karussell's comment the default field is just that the default. You can explicitly specify however many fields you want it's only if you don't specify one that the default comes into play. So a query like username:foo first_name:bar will find documents with a username of ""foo"" and a first_name of ""bar."""
455,A,"why ""multi foo"" parsed value is ""(multi multi2) foo"" so sorry my poor English hope you can see what I say. In Lucene3 Junit test code : org.apache.lucene.queryParser.TestMultiAnalyzer.testMultiAnalyzer() QueryParser qp = new QueryParser(Version.LUCENE_CURRENT """" new MultiAnalyzer()); // two tokens at the same position: assertEquals(""(multi multi2) foo"" qp.parse(""multi foo"").toString()); assertEquals(""foo (multi multi2)"" qp.parse(""foo multi"").toString()); I don't understand why ""multi foo"" parsed value is ""(multi multi2) foo"". I'm searched in google.com and baidu.com no result. We'll need to know what the method `qp.parse` does. Which class does that belong to? Do you have the Javadoc for it? You are writing the test case for that and you don't know why it parses like that then how can we know without looking at the code. It's not my test code it's luence's test code.HOHO The MultiAnalyzer class (defined in the same Java file) has the comment: /** * Expands ""multi"" to ""multi"" and ""multi2"" both at the same position * and expands ""triplemulti"" to ""triplemulti"" ""multi3"" and ""multi2"". */ Which probably explains it...  It looks like you've taken the test code from TestMultiAnalyzer.java. If you look at the code you can see the Javadoc and definition of the class MultiAnalyzer: /** * Expands ""multi"" to ""multi"" and ""multi2"" both at the same position * and expands ""triplemulti"" to ""triplemulti"" ""multi3"" and ""multi2"". */ private class MultiAnalyzer extends Analyzer { // <snipped> } So the Javadoc explains what's going on: ""multi"" will become ""multi multi2"". If you want to know exactly why this happens debug through the code and read all relevant Javadoc. Ask questions here if you get stuck again."
456,A,Java create a JAR file for contrib packages in Lucene I have downloaded the source code of Apache Lucene using Subversion. Now I want to create a JAR file for a particular Java file in the contrib portion of the code. The problem is that when I do javac x.java to get a class file and package it into a JAR file using jar cf jarfile.jar x.class the package hierarchy is not preserved in the JAR file. What is the correct way of packaging class files into the JAR file maintaining the package hierarchy? Particularly with Subversion checked out code is there a better way of packaging selected Java files. mindas:/tmp/test$ pwd /tmp/test mindas:/tmp/test$ ls mindas:/tmp/test$ mkdir some mindas:/tmp/test$ mkdir some/package mindas:/tmp/test$ touch some/package/SomeFile.class mindas:/tmp/test$ jar cf jarfile.jar some/package/SomeFile.class That does preserve the structure.  Lucene have ant's build.xml you can make jar by using ant package at lucene source directory. For contribs there are also contrib/contrib-build.xml
457,A,"Querying against a comma separated list of IDs with Examine and Lucene.Net? I am using Examine for Umbraco (which is built on top of Lucene.net) to do my search. I am quite sure my problem is Lucene related. One of my fields contains a list of comma separated IDs. How do I query this field in the right way? Eg. I have a field with the values ""6465"". I have tried using MultipleCharacterWildcard which only returns a result if I query for the ID 64 but not for ID 65. SingleCharacterWildcard does not return anything and Fuzzy only returns something if there is only one ID in the field. Any ideas of how to do a proper search? I guess what I am looking for is a ""Contains""-query. Also is this the right way to handle fields with comma separated lists or would it be better to instead split the comma separated list up into individual fields? I would certainly split the list up into separate fields. You can have multiple values for the same field name in a document which is a fairly natural way to represent a set of values: venue_id: 12345 treatment_id_set: 1234 treatment_id_set: 2345 With documents like this I can simply query for ""treatment_id_set:1234"" to find all the venues supporting that treatment. Of course the ordering of the treatments is lost. If you need to recover it store the comma-separated value while indexing the individual members: # stored indexed venue_id: 12345 # stored not indexed treatment_id_list: 12342345 # not stored indexed treatment_id_set: 1234 treatment_id_set: 2345"
458,A,"Indexing PDF documents in Solr with no UniqueKey I want to index PDF (and other rich) documents. I am using the DataImportHandler. Here is how my schema.xml looks: ......... ......... <field name=""title"" type=""text"" indexed=""true"" stored=""true"" multiValued=""false""/> <field name=""description"" type=""text"" indexed=""true"" stored=""true"" multiValued=""false""/> <field name=""date_published"" type=""string"" indexed=""false"" stored=""true"" multiValued=""false""/> <field name=""link"" type=""string"" indexed=""true"" stored=""true"" multiValued=""false"" required=""false""/> <dynamicField name=""attr_*"" type=""textgen"" indexed=""true"" stored=""true"" multiValued=""false""/> ........ ........ <uniqueKey>link</uniqueKey> As you can see I have set link as the unique key so that when the indexing happens documents are not duplicated again. Now I have the file paths stored in a database and I have set the DataImportHandler to get a list of all the file paths and index each document. To test it I used the tutorial.pdf file that comes with example docs in Solr. The problem is of course this pdf document won't have a field 'link'. I am thinking of way how I can manually set the file path as link when indexing these documents. I tried the data-config settings as below  <entity name=""fileItems"" rootEntity=""false"" dataSource=""dbSource"" query=""select path from file_paths""> <entity name=""tika-test"" processor=""TikaEntityProcessor"" url=""${fileItems.path}"" dataSource=""fileSource""> <field column=""title"" name=""title"" meta=""true""/> <field column=""Creation-Date"" name=""date_published"" meta=""true""/> <entity name=""filePath"" dataSource=""dbSource"" query=""SELECT path FROM file_paths as link where path = '${fileItems.path}'""> <field column=""link"" name=""link""/> </entity> </entity> </entity> where I create a sub-entity which queries for the path name and makes it return the results in a column titled 'link'. But I still see this error: WARNING: Error creating document : SolrInputDocument[{date_published=date_published(1.0)={2011-06-23T12:47:45Z} title=title(1.0)={Solr tutorial}}] org.apache.solr.common.SolrException: Document is missing mandatory uniqueKey field: link Is there anyway for me to create a field called link for the pdf documents? This was already asked here before but the solution provided uses ExtractRequestHandler but I want to use it through the DataImportHandler. Try this: <entity name=""fileItems"" rootEntity=""false"" dataSource=""dbSource"" query=""select path from file_paths""> <field column=""path"" name=""link""/> <entity name=""tika-test"" processor=""TikaEntityProcessor"" url=""${fileItems.path}"" dataSource=""fileSource""> <field column=""title"" name=""title"" meta=""true""/> <field column=""Creation-Date"" name=""date_published"" meta=""true""/> </entity> </entity> It worked. Thanks!"
459,A,"Lucene vs ""select .. like .."" I am writing web application where users create content (posts on forum). Is it better to use select .. like .. or Lucene? What are advantages of Lucene in not advanced searching. You are kind of comparing apples to oranges here. Lucene is a great tool for indexing *static* content (plain old html pages). And a database is very good for dealing with *dynamic* content (pages generated by the server-side code). Leucene will likely be faster for large datasets because it can use a full text index. A select ... like query on a traditional relational database can typically only use an index if the the argument to like does not begin with a wildcard. For example: select * from mytable where mycolumn like 'fred%'; -- may use an index on mycolumn select * from mytable where mycolumn like '%fred%'; -- cannot use an index on mycolumn If you need to do a lot of the second kind of query it's unlikely to scale well. If you're using MySQL with the MyISAM table engine (default but doesn't support foreign keys) you can use MySQL's full text indexing capabilities but the syntax is different and MySQL-specific. It doesn't use the like keyword. thanks for that example. cannot use an index in the second select is huge disadvantage. @avv Do you have any idea why the index can't be used when querying "" like '%fred%' ""? The answer is that an index is just like in a book a sorted list of values from the indexed column(s) with a reference to the actual row in a table. It's easy to search that list for values starting with 'f'.  You shouldn't consider messing around with Lucene yourself. Instead you should rather try stand-alone products like Solr or Elastic Search or libraries like Hibernate Search. Unfortunately my personal favorite Compass is abandoned now. I've tried Hibernate Search a while ago and dropped it in favor of Compass. Now it looks as if Elastic Search is probably the easiest way to provide advanced search capabilities. Additionally some RDBMSs support more or less advanced fulltext search capabilities e.g. MySQL MS SQL and Oracle.  Lucene is hugely more powerful/flexible but a SELECT ... LIKE might be a good starting point. Finish your app with SELECT ... LIKE then you can add in Lucene if you need to as it's a lot more work. You might want to get your app to the point where it's sufficiently heavily used to even justify the Lucene time investment. Short answer: SELECT ... LIKE is probably good enough to start with. In spite of having to regenerate the Lucene's indices every time you add some new static content I really think there is not that much work to deal with it. And again Lucene is good for static content; db is great for dynamic - and sometimes you need both. I suspect Lucene is perhaps easy to use once you know what you're doing but I've always felt there's a steep learning curve to fully ""getting it"". I might be wrong documentation etc these days might make getting up and running a much smoother process. @rsenna you dont need to regenerate the complete index it's enough by just adding another document (key-value) representing the new data and perhaps remove the old document. @Simon Svensson: fair enough thanks for the info."
460,A,"Why am i getting this error java.lang.ClassNotFoundException? Why am i getting this error message? Testcase: createIndexBatch_usingTheTestArchive(src.LireHandlerTest): Caused an ERROR at/lux/imageanalysis/ColorLayoutImpl java.lang.NoClassDefFoundError: at/lux/imageanalysis/ColorLayoutImpl at net.semanticmetadata.lire.impl.SimpleDocumentBuilder.createDocument(Unknown Source) at net.semanticmetadata.lire.AbstractDocumentBuilder.createDocument(Unknown Source) at backend.storage.LireHandler.createIndexBatch(LireHandler.java:49) at backend.storage.LireHandler.createIndexBatch(LireHandler.java:57) at src.LireHandlerTest.createIndexBatch_usingTheTestArchive(LireHandlerTest.java:56) Caused by: java.lang.ClassNotFoundException: at.lux.imageanalysis.ColorLayoutImpl at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:307) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:248) I am trying to create a Lucene index using documents that are created using Lire. When i am getting to the point where it tries to create a document using the document builder it gives me this error message. input paramaters first run: filepath: ""test_archive"" (this is a image archive) whereToStoreIndex: ""test_index"" (Where i am going to store the index) createNewIndex: true Since the method is recursive (see the if statement where it checks if is a directory) it will call itself multiple times but all the recursive calls uses createNewIndex = false. Here is the code: public static boolean createIndexBatch(String filepath String whereToStoreIndex boolean createNewIndex){ DocumentBuilder docBldr = DocumentBuilderFactory.getDefaultDocumentBuilder(); try{ IndexWriter indexWriter = new IndexWriter( FSDirectory.open(new File(whereToStoreIndex)) new SimpleAnalyzer() createNewIndex IndexWriter.MaxFieldLength.UNLIMITED ); File[] files = FileHandler.getFilesFromDirectory(filepath); for(File f:files){ if(f.isFile()){ //Hopper over Thumbs.db filene... if(!f.getName().equals(""Thumbs.db"")){ //Creating the document that is going to be stored in the index String name = f.getName(); String path = f.getPath(); FileInputStream stream = new FileInputStream(f); Document doc = docBldr.createDocument(stream f.getName()); //Add document to the index indexWriter.addDocument(doc); } }else if(f.isDirectory()){ indexWriter.optimize(); indexWriter.close(); LireHandler.createIndexBatch(f.getPath() whereToStoreIndex false); } } indexWriter.close(); }catch(IOException e){ System.out.print(""IOException in createIndexBatch:\n""+e.getMessage()+""\n""); return false; } return true; } It sounds like you are missing a java library. I think you need to download Caliph & Emir that may be used indirectly by Lire. http://www.semanticmetadata.net/download/ Thank you! worked! I was missing a jar library called caliph-emir-cbir.jar. I only had the lucene and lire library."
461,A,Solr remove ranking or modify ranking feature I want to optimise my Solr engine. I don't want ranked results. I just want all docs which match my query is there any way I can remove it . So that retrieving data improves ? There is no need to remove relevancy ranking to achieve what you want. By setting the rows parameter to the same number as the numfound you will retrieve all documents matching your query. http://url-to-some-server/solr/select?q=somequery&rows=200 if you want the documents sorted in another way than by relevance just add a &sort=somefield desc where somefield is a sortable field.
462,A,lucene indexing objects in memory I’m just stuck with one problem and don’t know how to figure it out. I’m working on the indexation of the objects that are in computer memory (they exist only in my java code). Don’t have any problems with indexing it however I have no idea how to re-index it if they change during the execution of this code; one of my ideas is adding some events to this objects (if you change any parameters -> reindex it ). However I’m not sure about its efficiency? Thank you in advance Daniel Lucene is already very efficient when writing to a disk index. If you have objects already in memory you have less work to do than usual (read the objects from disk is the typical scenario) so reindexing will not usually be a problem. You just have to delete the object and index it again (in IndexWriter). You did not specify this but if your index needs not to be persistent then you can do all in memory so it will be much faster see RAMDirectory and MemoryIndex
463,A,"Index replication and Load balancing Am using Lucene API in my web portal which is going to have 1000s of concurrent users. Our web server will call Lucene API which will be sitting on an app server.We plan to use 2 app servers for load balancing. Given this what should be our strategy for replicating lucene indexes on the 2nd app server?any tips please? You could use solr which contains built in replication. This is possibly the best and easiest solution since it probably would take quite a lot of work to implement your own replication scheme. That said I'm about to do exactly that myself for a project I'm working on. The difference is that since we're using PHP for the frontend we've implemented lucene in a socket server that accepts queries and returns a list of db primary keys. My plan is to push changes to the server and store them in a queue where I'll first store them into the the memory index and then flush the memory index to disk when the load is low enough. Still it's a complex thing to do and I'm set on doing quite a lot of work before we have a stable final solution that's reliable enough.  I'm creating the Indices on the publishing Backend machines into the filesystem and replicate those over to the marketing. That way every single load & fail balanced node has it's own index without network latency. Only drawback is you shouldn't try to recreate the index within the replicated folder as you'll have the lockfile lying around at every node blocking the indexreader until your reindex finished.  From experience Lucene should have no problem scaling to thousands of users. That said if you're only using your second App server for load balancing and not for fail over situations you should be fine hosting Lucene on only one of those servers and accessing it via NDS (if you have a unix environment) or shared directory (in windows environment) from the second server. Again this is dependent on your specific situation. If you're talking about having millions (5 or more) of documents in your index and needing your lucene index to be failoverable you may want to look into Solr or Katta.  We are working on a similar implementation to what you are describing as a proof of concept. What we see as an end-product for us consists of three separate servers to accomplish this. There is a ""publication"" server that is responsible for generating the indices that will be used. There is a service implementation that handles the workflows used to build these indices as well as being able to signal completion (a custom management API exposed via WCF web services). There are two ""site-facing"" Lucene.NET servers. Access to the API is provided via WCF Services to the site. They sit behind a physical load balancer and will periodically ""ping"" the publication server to see if there is a more current set of indicies than what is currently running. If it is it requests a lock from the publication server and updates the local indices by initiating a transfer to a local ""incoming"" folder. Once there it is just a matter of suspending the searcher while the index is attached. It then releases its lock and the other server is available to do the same. Like I said we are only approaching the proof of concept stage with this as a replacement for our current solution which is a load balanced Endeca cluster. The size of the indices and the amount of time it will take to actually complete the tasks required are the larger questions that have yet to be proved out. Just some random things that we are considering: The downtime of a given server could be reduced if two local folders are used on each machine receiving data to achieve a ""round-robin"" approach. We are looking to see if the load balancer allows programmatic access to have a node remove and add itself from the cluster. This would lessen the chance that a user experiences a hang if he/she accesses during an update. We are looking at ""request forwarding"" in the event that cluster manipulation is not possible. We looked at solr too. While a lot of it just works out of the box we have some bench time to explore this path as a learning exercise - learning things like Lucene.NET improving our WF and WCF skills and implementing ASP.NET MVC for a management front-end. Worst case scenario we go with something like solr but have gained experience in some skills we are looking to improve on."
464,A,"Best practices for implementing a Lucene search in asp.net eCommerce site I've been tasked with seeting up a search service on an eCommerce site. Currently it uses full text indexing on sql server which isn't ideal as it's slow and not all that flexible. How would you suggest i approach changing this over to lucene? By that i mean how would i initially load all the data into the indexes and how would it be maintained? on my ""insert product"" methods would i also have it insert it into the index? any information is of great help! If you do decide to use Lucene.NET for your search you need to do some of the following: create your initial index by iterating through all your records and writing the data that you want searched into your index if the amount of records and data that you are writing to your indexes makes for large indexes then consider stuffing them into multiple indexes (this means you will have to make a more complex search program as you need to search each index and then merge the results!!) when a product is updated or created you need to update your index (there is a process here to create additional index parts and then merge the indexes) if you have a high traffic site and there is the possibility of multiple searches occurring at the exact same moment then you need to create a wrapper that can do the search for you across multiple duplicate indexs (or sets of indexes) (think singleton pattern here) as the index can only be accessed (opened) for one search at a time This is a great platform. We initially tried to use the freetext search and found it to be a pain to create the indexes update and manage. The searches were not that much faster than a standard sql search. They did provide some flexibility in the search query...but even this pales in comparison to the power of Lucene!  I'm currently using Solr which is build on top of Lucene as the search engine for one of my e-commerce projects. It works great. http://lucene.apache.org/solr/ Also as far as keeping the products in sync between the DB and Solr you can either build your own ""sweeper"" or implement the DataImportHandler in Solr. http://wiki.apache.org/solr/DataImportHandler We build our own sweeper that reads a DB view at some interval and checks to see if there are new products or any product data has been updated. It's a brute force method and I wish I knew about the DataImportHandler before. Facets are also a really powerful part of Solr. I highly recommend using them."
465,A,"Problem with Solr dynamic/copy Field I have a problem that i have a dynamic field in schema.xml as <dynamicField name=""sec_*"" type=""text"" indexed=""true"" stored=""false""/> and <field name=""Contents"" type=""text"" indexed=""true"" stored=""false"" multiValued=""true""/> dynamic field is copied to Contents field as <copyField source=""sec_*"" dest=""Contents""/> now when i perform search using some dynamic fields like ""sec_1069:risk"" it filters documents that does not contains that dynamic field called sec_1069 can any body help how i can force this thing that solr should not filter documents that don't have that dynamic field. Try sec_1069:risk OR -sec_1069:[* TO *]"
466,A,"problem with precision and recall measuring in lucene I need to calculate precision and recall value in lucene and I use this source code to do that public class PrecisionRecall { public static void main(String[] args) throws Throwable { File topicsFile = new File(""C:/Users/Raden/Documents/lucene/LuceneHibernate/LIA/lia2e/src/lia/benchmark/topics.txt""); File qrelsFile = new File(""C:/Users/Raden/Documents/lucene/LuceneHibernate/LIA/lia2e/src/lia/benchmark/qrels.txt""); Directory dir = FSDirectory.open(new File(""C:/Users/Raden/Documents/myindex"")); Searcher searcher = new IndexSearcher(dir true); String docNameField = ""filename""; PrintWriter logger = new PrintWriter(System.out true); TrecTopicsReader qReader = new TrecTopicsReader(); //#1 QualityQuery qqs[] = qReader.readQueries( //#1 new BufferedReader(new FileReader(topicsFile))); //#1 Judge judge = new TrecJudge(new BufferedReader( //#2 new FileReader(qrelsFile))); //#2 judge.validateData(qqs logger); //#3 QualityQueryParser qqParser = new SimpleQQParser(""title"" ""contents""); //#4 QualityBenchmark qrun = new QualityBenchmark(qqs qqParser searcher docNameField); SubmissionReport submitLog = null; QualityStats stats[] = qrun.execute(judge //#5 submitLog logger); QualityStats avg = QualityStats.average(stats); //#6 avg.log(""SUMMARY""2logger "" ""); dir.close(); } } and here is the contents of topicsfile  <top> <num> Number: 0 <title> apache source <desc> Description: <narr> Narrative: </top> and this is the contents of qrelsfile # Format: # # qnum 0 doc-name is-relevant # # 0 0 apache1.0.txt 1 0 0 apache1.1.txt 1 0 0 apache2.0.txt 1 now the problem occur when I ran that source code which displayed the value of precision and recall to be zero. here is the result when I ran the source code. 0 - contents:apache contents:source 0 Stats: Search Seconds: 0.047 DocName Seconds: 0.039 Num Points: 56.000 Num Good Points: 0.000 Max Good Points: 3.000 Average Precision: 0.000 MRR: 0.000 Recall: 0.000 Precision At 1: 0.000 Precision At 2: 0.000 Precision At 3: 0.000 Precision At 4: 0.000 Precision At 5: 0.000 Precision At 6: 0.000 Precision At 7: 0.000 Precision At 8: 0.000 Precision At 9: 0.000 Precision At 10: 0.000 Precision At 11: 0.000 Precision At 12: 0.000 Precision At 13: 0.000 Precision At 14: 0.000 Precision At 15: 0.000 Precision At 16: 0.000 Precision At 17: 0.000 Precision At 18: 0.000 Precision At 19: 0.000 Precision At 20: 0.000 SUMMARY Search Seconds: 0.047 DocName Seconds: 0.039 Num Points: 56.000 Num Good Points: 0.000 Max Good Points: 3.000 Average Precision: 0.000 MRR: 0.000 Recall: 0.000 Precision At 1: 0.000 Precision At 2: 0.000 Precision At 3: 0.000 Precision At 4: 0.000 Precision At 5: 0.000 Precision At 6: 0.000 Precision At 7: 0.000 Precision At 8: 0.000 Precision At 9: 0.000 Precision At 10: 0.000 Precision At 11: 0.000 Precision At 12: 0.000 Precision At 13: 0.000 Precision At 14: 0.000 Precision At 15: 0.000 Precision At 16: 0.000 Precision At 17: 0.000 Precision At 18: 0.000 Precision At 19: 0.000 Precision At 20: 0.000 now can you tell me what had I done wrong which make the precision and recall values to be zeros? and also what does it mean when the precision and recall value is zero? the reason I am doing this is because I need to measure the performance of my search engine and precision and recall is one of the way for me to achieve it. thanks though Precision = 0 means none of your results were correct. See the wikipedia article for example. I would suggest trying an individual query and see what your results are. You probably have an issue with your tokenizer; maybe you are not casing things right etc. yes you're right.I am gonna look into it then. :-)"
467,A,"Indexing Lucene with Parallel Extensions I'd like to speed-up the indexing of 10GB of data into a Lucene index. Would TPL be a good way to do this? Would I need to divided the data up into chunks and then have each thread start indexing chunks? To keep the UI responsive would BackgroundWorker be the best approach or Task or something else? Does SOLR already do something like this? Or would it still be worthwhile to code this myself. If you want multiple threads to write to a single IndexWriter then I would just spawn one thread which does something like Parallel.ForEach(docs d => { writer.Add(danalyzer) }); So that .NET deals with splitting up the data. At large index sizes some people find performance improvements in having multiple indexes that they write to and then merge all the indexes together. My understanding is that this is really useful only for truly massive indexes but if you want to do this then you will probably need to deal with splitting up the data yourself. In that case using a more full-featured library like tpl might be useful. Solr is inherently multi-threaded so you would do the exact same snippet as I gave you before except instead of calling the writer directly you would call your REST/SolrNet method. As a general rule if you ask ""Should I use Solr or make it myself?"" the answer is almost always ""use Solr"". I can't think of any reason that you would want to make it yourself here unless your jvm is really bad or you really hate java.  Assuming you are using Java - I've had good experiences indexing using multiple threads. Lucene indexing is basically CPU-bound in my experience meaning if you spawn N threads you can use all your N cores. The Lucene IndexWriter handles the concurrency so you don't need to worry about that. Your threads can just call indexWriter.addDocument whenever they are ready to do so. In one project the documents came from a SELECT statement from a database. I created N threads and each one took the next document from the ResultSet and added it to the index. The thread exited when there were no more rows and the main thread waited on a CountDownLatch. The second project was a bit more complex. The system was ""crawling"" a set of documents i.e. it was not clear from the outset how many documents there were going to be. So it was necessary to maintain a ""queue"" of documents which had already been discovered. And in the process of analyzing and indexing those documents it was possible to discover more documents which were then also added to the queue. The queue was populated at the start with the initial / seed document. I created a class AutoStopThreadPool to manage the threads you are welcome to download it if you like. (The JVM thread pools you need to ""add"" all the tasks then ""wait for completion"" which wasn't suitable as the processing of a task could result in the discovery of new tasks)"
468,A,"Sort optional fields in Lucene.net I have a Lucene index which: always contains field Title may contain field Tag (depending on data that's being indexed) I have a requirement to sort search results by Title and Tag fields. At the moment if none of the documents in the index contain Tag field the search throws an SystemException: ""field ""Tag"" does not appear to be indexed"". I am aware this behaviour is by design. Is there a way to tell Lucene to optionally sort by Tag field if it exists? No. You have to implement a custom sort on the search results  What I'd do here is add another field that is concatenated title and tags and then sort by that -- should get you what you want and you'll still be able to ride the lucene rails."
469,A,In Lucene what is the difference between ANALYZED and ANALYZED_NO_NORMS? I could not understand the difference between two ways of indexing: ANALYZED and ANALYZED_NO_NORMS. I read the Lucene Javadoc but did not understand the difference. Can someone tell me more about NORMS? What are the benefits or limitations that they bring to indexing? ANALYZED Index the tokens produced by running the field's value through an Analyzer. This is useful for common text. An analyzer might be something like a Snowball Stemmer Analyzer: http://e-mats.org/2009/05/modifying-a-lucene-snowball-stemmer/ ANALYZED_NO_NORMS Uses an analyzer however it doesn't create norms for fields. http://lucene.apache.org/java/2_4_0/scoring.html Norms are created for quick scoring of documents at query time. These norms are usually all loaded into memory so that when you run a query analyzer over an index it can quickly score the search results. No norms means that index-time field and document boosting and field length normalization are disabled. The benefit is less memory usage as norms take up one byte of RAM per indexed field for every document in the index during searching.
470,A,"Indexing previous records with Doctrine (and Symfony!) with Zend Lucene I have a Symfony application that uses Doctrine as its ORM. Based on Symfony's ""Practical symfony"" book I have Zend Lucene added to my web app. However the problem is that there are around 1.1 million rows existing in the database that I want to index for Lucene as well. The only things being indexed are edited rows and the rows have been added since I starting using Lucene (about 50-75k). I'm not sure the best method to go about this so I figured I would ask some opinions. I recommend not using Zend Lucene especially for this amount of data. Solr might be a better fit (it's a lucene based as well). Why exactly not? I created a task that clears the current index and rebuilds it with all the records. It's on github you can find it here. Although it uses propel you should be able to adapt it for your needs. Used a Symfony task costumed to my own liking thanks for the idea!"
471,A,"how to import mysql tables to SOLR i can never understand how solr works. it just talks about schema files all the way but how do i import content from the database to it with a painless method? i have tried to figure it out by reading their tutorials but it just mess up my head. its written for the Einsteins out there cause apparently there are a lot of people who also have difficulty of understanding it. and they keep talking about the example folder. ""just type java -jar ./start.jar"". i mean..is this an example or can you use it as final for your website? where is data-config.xml located??? there are just no good tutorials out there explaining to that first time beginners can understand. For folks who don't know what Solr is it's part of the Apache Lucene project. It's a server that runs within a container such as Tomcat. Solr hosts a Lucene index and provides a ""REST-like"" interface to update and query an index via HTTP. The ""Getting Started"" tutorial talks about starting the Solr server with java -jar start.jar but that's only to get the Solr server running. It's like starting an instance of MySQL Server -- necessary before you can query it but this step alone doesn't populate it with data or make it serve up any results. The tutorial goes on to show an example of posting documents to the Solr server: user:~/solr/example/exampledocs$ java -jar post.jar solr.xml monitor.xml That example posts two documents solr.xml and monitor.xml to be indexed. You don't have to use their post.jar example -- since Solr supports HTTP you should be able to use any HTTP client such as curl. To index the entire result of an SQL query this way you would have to write a script to loop over the result and post data to Solr row-by-row but this would probably be excessively time-consuming making a separate HTTP POST request per row of data. I'm guessing the faster way is to use Solr's support for batch data in CSV format. See http://wiki.apache.org/solr/UpdateCSV for examples. i downloaded an ebook explaining how Solr works in 300 pages. now i know more of how it works.  The easiest way to import data from a RDBMS is the DataImportHandler. Check out this step-by-step quick start. Also here's a pretty thorough walk-through of its usage."
472,A,"Java cannot find symbol : method methodName(org.bla.blabla.myClass) I'm using Lucene APIs and I get the following error on this line of my code: import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.document.Fieldable; ... Document _document = new Document(); _document.add(new Field(""type"" document.getType())); Error: CollectionIndexer.java:34: cannot find symbol symbol : method add(org.apache.lucene.document.Field) location: class CollectionIndexer.Document _document.add(new Field(""type"" document.getType())); This is the documentation about the method: http://lucene.apache.org/java/3_0_3/api/all/org/apache/lucene/document/Document.html#add(org.apache.lucene.document.Fieldable) thanks Update: javac -cp commons-digester-2.1/commons-digester-2.1.jar:lucene-core-3.0.3.jar myApp.java On what object do you call the myMethod method? There's hardly enough information here to suggest anything sensible! Can you add a minimable (non)-compilable code snippet? @Patrick please provide an [SSCCE](http://sscce.org/) for this problem. `myMethod` != `methodName`. @reef @Oli Charlesworth @aioobe and @BalusC I've updated my question with the real implementation Maybe you can clean you're dev environment I know sometimes my eclipse is a little bit lost... @reef I'm running it from terminal. I've updated my question with the dependencies I'm using. Do you call getType() on document (a variable not showed in your example) or on _document (the lucene API Document instance in your example)? Because I cannot find any getType() method on the lucene Document doc (see http://lucene.apache.org/java/3_0_3/api/all/org/apache/lucene/document/Document.html). @reef Sorry you right getType is returning a String. It is in a method defined by me. Updated based on updates to question: Make sure your curly braces around this line up and that there is not something else causing an issue. Reduce the code down just to as few lines as possible to eliminate any other items that could be throughing the compiler off. Compile without the commons-digester-2.1 if you can to eliminate possible conflicts. Break the line up so the a Field object is created on a separate line than adding the field to the document so that you can confirm there is no problem with your constructor call. I've updated my question with real implementation  The problem comes from the fact that your document.getType() method returns a String and there is no constructor in the Field class that matches your call. See http://lucene.apache.org/java/3_0_3/api/all/org/apache/lucene/document/Field.html. If I test your code in my environment Eclipse says: The constructor Field(String String) is undefined Maybe you could do as the following: Document _document = new Document(); _document.add(new Field(""type"" document.getType().getBytes() Store.YES); // Or document.add(new Field(""type"" document.getType().getBytes() Store.NO); UPDATE after source code submission -------------------- The problem comes from the fact that in your class you have an inner-class called Document. There is a name conflict between your Document class and the Lucene's one. When you instanciate your document with the line Document _document = new Document(); you're actually instanciating YOUR Document class. That's why the compiler cannot find the add method. Multiple solution: a. Instanciate the Document prefixing it with the Lucene package name org.apache.lucene.document.Document _document = new org.apache.lucene.document.Document(); b. Rename your inner class so that you don't have any name conflict. @reef I get this error: cannot find symbol: method getBytes() (and I imported java.io.Reader) Your getType() method returns a String? In my example I call the constructor Field(String name byte[] value Field.Store store) so you don't need to import java.io.Reader. The getBytes() method called in my example comes from the String class (see http://download.oracle.com/javase/1.5.0/docs/api/java/lang/String.html). @reef Please ignore my last message. I'm still getting the same error and my code is now: document.add(new Field(""type"" new byte[3] Field.Store.YES)); I don't understand however why you want me too add Store variable since I can omit it according to documentation: Field(String name Reader reader) That was just an example of own to properly create a Field. You can use the constructor with the Reader instead of course ;) So you still have the issue? The same message (cannot find symbol)? @reef yes same error. I still have it. I can attach all code if you want. This is the file: http://dl.dropbox.com/u/72686/CollectionIndexer.java @Patrick Yep attach it please. This sounds strange to me however... @reef I've added the link: http://dl.dropbox.com/u/72686/CollectionIndexer.java Ok got it. The problem comes from the fact that you have an internal class named Document and when you make new Document() you're instanciating your inner-class instead of the Lucene one. First solution: you can instanciate your document using the package name of the lucene class: org.apache.lucene.document.Document _document = new org.apache.lucene.document.Document(); Second solution: rename your inner-class to be sure to avoid name conflict between your Document and the Lucene's one. @reef I see... thanks.. Just edited my answer.  When I'm stumped over this type of error it is usually due to the fact that I've two definitions of InterfaceName and accidentally imported the wrong one in one or more places. (Happens for instance when I accidentally choose java.awt.List instead of java.util.List when auto-importing missing classes.) Make sure that ... symbol : method methodName(org.bla.blabla.myClass) \____________________/ ... this part ... ... matches the expected package / class."
473,A,"Lucene: build a query by tokenizing a string and pass I need to extract single terms from a string to build a query using BooleanQuery. I'm using QueryParser.parse() method for it this is my code snippet: booleanQuery.add(new QueryParser(org.apache.lucene.util.Version.LUCENE_40 ""tags"" new WhitespaceAnalyzer(org.apache.lucene.util.Version.LUCENE_40)).parse(""tag1 tag2 tag3"") BooleanClause.Occur.SHOULD); I'm however wondering if this is correct way to pass single terms to booleanQuery. QueryParser.parse method returns a SrndQuery object which I directly pass to booleanQuery.add method Not sure if this is correct. Should I extract single terms instead from SrndQuery... or something like that and invoke booleanQuery.add several times ? thanks Update: printed query .:*.* title:Flickrmeetup_01 description:Michael description:R. description:Ross tags:rochester tags:ny tags:usa tags:flickrmeetup tags:king76 tags:eos350d tags:canon50mmf14 tags:mikros tags:canon tags:ef tags:50mm tags:f14 tags:usm tags:canonef50mmf14 tags:canonef50mmf14usm I believe you should extract the tokens wrap each one in a Term then create a TermQuery for it then add the TermQuery to the BooleanQuery. SrndQuery is abstract anyway so I guess your current code would create an instance of a subclass which is probably not what you mean to do. You may want to create your own custom QueryParser for this. well I've been suggested to print the query and this is the result.. apparently it works: the same field is repeated for each term. But please could you give a look at the printed query (added to the question) and let me know if I'm correct ? @Patrick - It looks fine. I do not like the global part (.:*.*) - it may hurt performance. I suggest you check the result set to see if you get what you expect and check performance as well. OK well I guess that .:*.* is because of MatchAllDocsQuery"
474,A,In Lucene can I search one index but use IDF from another one? I'm building a system where I want to show only results indexed in the past few days. Furthermore I don't want to maintain a giant index with a million documents if I only want to return results from a couple of days (thousands of documents). On the other hand my system heavily relies that the occurrences of terms in documents stored in the index have a realistic distribution (consequently: realistic IDF). That said I would like to use a small index to return results but I want to compute documents score using a IDF from a much greater Index (or even an external source). The Similarity API doesn't seem to allow me to do this. The idf method does not receive as parameter the term being used. Another possibility is to use TrieRangeQuery to make sure the documents shown are within the last couple of days. Again I rather not mantain a larger index. Also this kind of query is not cheap. You should be able to extend IndexReader and override the docFreq() methods to provide whatever values you'd like. One thing this implementation can do is open two IndexReader instances -- one for the small index and one for the large index. All the methods are delegated to the small IndexReader except for docFreq() which is delegated to the large index. You'll need to scale the value returned i.e. int myNewDocFreq = bigIndexReader.docFreq(t) / bigIndexReader.maxDoc() * smallIndexReader.maxDoc() I didn't quite understand the need of scaling. Can you clarify?
475,A,"Is excessive use of lucene good? In my project entire searching and listing of content is depend on Lucene. I am not facing any performance issues. Still the project is in development phase and long way to go in production. I have to find out the performance issues before the project completed in large structure. Whether the excessive use of lucene is feasible or not? I'd say excessive by definition means not good. http://www.merriam-webster.com/dictionary/excessive ;) You should describe some examples. Perhaps you only think your usage is excessive but it isn't at all. Sorry for the confusion. Entire application is built on Lucene search. I thought i am using lucene very much excessive. Lucene is very mature and has very good performance for what it was designed to do. However it is not an RDBMS. The amount of fine-tuning you can do to improve performance is more limited than a database engine. You shouldn't rely only on lucene if: You need frequent updates You need to do joined queries You need sophisticated backup solutions I would say that if your project is large enough to hire a DBA you should use one... Performance wise I am seeing acceptable performance on a 400GB index across 10 servers (a single (4GB 2CPU) server can handle 40GB of lucene index but no more. YMMV). What do you consider ""acceptable performance""?  As an example I have about 3 GB of text in a Lucene index and it functions very quickly (milliseconds response times on searches filters and sorts). This index contains about 300000 documents. Hope that gave some context to your concerns. This is in a production environment.  We use lucence to enable type-ahead searching. This means for every letter typed it hits the lucence index to get the results. Multiple that to tens of textboxes on multiple interfaces and again tens of employees typing with no complaints and extremely fast response times. (Actually it works faster than any other type-ahead solution we tried).  By excessive do you mean extensive/exclusive? Lucene's performance is generally very good. I recently ran some performance tests for Lucene on my Desktop with QuadCore @ 2.4 GHz 2.39 GHz I ran various search queries against a disk index composed of 10MM documents and the slowest query (MatchAllDocs) returned results within 1500 ms. Search queries with two or more search terms would return around 100 ms. There are tons of performance tweaks you can do for Lucene and they can significantly increase your search speed.  What would you define as excessive? If your application has a solid design and the performance is good I wouldn't worry too much about it. Perhaps you could get a data dump to test the performance in a live scenario."
476,A,"Do you recomend Sql Server for storing and indexing files (pdf office etc)? Possible Duplicate: Lucene.Net and SQL Server I need to storage and index files like PDF and office files. Currently I'm using Sql Server 2k8 to perform this task using the Full text search with IFilters. My question is: Is this the ""best"" way? Should I switch for instance to Lucene for indexing? Thanks Dupe check on zihotki's links. This question is not new. Read this and this threads. And I recommend to use Lucene from my experience. It's a bit harder to set up it but it works very good. Also you may have a look at Sphinx. It looks very good and has positive replies."
477,A,"Indexing texts with many numbers in Lucene Is it OK to create a term for each number in a text? Example text: I got 2295910 unique terms. The numbers can be timestamps port numbers anything. The unique numbers lead to a very large number of unique terms. It does not feel right to have the same number of unique terms as documents. Lucene memory usage grows with the number of unique terms. Is there a special analyzer or a trick for texts with numbers? The StandardAnalyzer creates a term for each unique number. The needs: The numbers should remain searchable. There could be multiple numbers in a document. The memory usage is the issue. I have 800M documents in multiple index directories. The memory usage forces me to close the least recently used IndexSearchers. Untested ideas: Use a special analyzer. It would split the numbers into chunks. 123456 would become ""123 456"". The query parser would use a phrase search to find a number. Change Lucene code to use a bigger termInfosIndexDivisor when seeing numeric terms. Maybe I'm reinventing the wheel. Was it solved by somebody already? Are you currently having a memory problem? It is true that Lucene memory usage grows with the number of unique terms but it's still a relatively minuscule amount of memory even for indices that have a lot a terms. If memory is an issue and you've profiled your code to ensure that it is indeed Lucene that is the problem you can create another Analyzer that throws away numeric terms. If you do that obviously you won't be able to search for documents using numbers. OK. I guess that it is OK to have many unique terms in the index. A bigger termInfosIndexDivisor helps to decrease the memory usage.  The answer depends on your needs. Do you need to search on these terms? If you need to search on these terms then this is just the nature of your search index. There are some tricks you can do if you don't need to search exact values (like range searches) but if you need exact matches then you are stuck with this. If you don't need to search these terms why index them?  As Bajafresh says: premature optimization is the root of all evil. But supposing this really is a problem: One option is to duplicate the field and analyze once throwing out numbers and the other time throwing out everything but numbers then indexing the latter as a numeric field. Numeric fields have a special storage mechanism which means that only a very few unique terms will be stored (usually less than 256 at the cost of some precision). Of course this will mean that phrase queries will not work but other kinds should still be fine (assuming you mess with the query parser enough to get this to work)."
478,A,"How to write a Lucene.Net RAMDirectory back to disk? I have been working with Lucene.Net and FSDirectory for some time now so I am familiar with the basics of working with it. However I am now attempting to rewrite some key code by using RAMDirectory's when possible to speed up index use. Loading an existing FSDirectory into a RAMDirectory is easy enough just by using the appropriate constructor. However I can't seem to figure out how I write it back to disk again. I've seen some mention of a static Directory.copy() method in the Java version but this doesn't seem to exist in Lucene.Net. Is this possible? Update: Turns out I was using an old version of Lucene.Net that didn't support this method. The ""official binaries"" on the official Lucene.Net site are apparently quite out of date. Thanks to CVertex for suggesting using NuGet to download and install the latest version of Lucene right within Visual Studio. Directory.Copy static exists as per the Java version public static void Copy(Lucene.Net.Store.Directory src Lucene.Net.Store.Directory dest bool closeDirSrc) Member of Lucene.Net.Store.Directory Perhaps you're not looking in right namespace. Thanks CVertex that was extremely simple! Actually I think I might be using an old version but I can't seem to find a zipped copy of anything newer on http://lucene.apache.org/lucene.net/ and I can't for the life of me figure out how to download from svn.apache.org--it just gives me directory listings (do I need to install an SVN client just to download from there?) yes get a svn client tortoise will do the job http://tortoisesvn.net/downloads.html ok will do--thanks! I recommend using NuGet and download it from there (which is what I did) which will be kept pretty up to date with the latest official releases. Using the trunk is a bit dangerous as it's always in flux as they sync with the latest Java version. ah yes that's an even better alternative!"
479,A,"What's wrong with my Lucene query? I'm running Zend_Search_Lucene which is almost exactly the same as normal Lucene. I'm keeping stores in my Lucene index and they're holding this: id of store for sql - 'store_id' of field type keyword name of store - 'name' of field type text latitude of store - 'lat' of field type keyword longitude of store - 'lng' of field type keyword I only have one store to test in the database. It has name ""the super awesome store"" has an index of 11 lat of 73.9 and lng of 40.6. However the results aren't working like they should. Here is an example of one of my queries that doesn't work: (name:'awesom*') AND lat:[-74.486951 TO -73.486951] AND lng:[40.256054 TO 41.256054] The * is supposed to represent ""and then anything you want"" and it won't return the store. If I make name ""awesome*"" it will return correctly. I don't know how to make it search for awesomatic/awesome/etc. My other problem is that the lat and lng search aren't working either. They don't seem to matter no matter what the span of lat or lng is. Even if I put in the lat/lng of china as long as the name matched it returns the result. I need it to only return the result if that lat and lng are within their correct ranges. What am I doing wrong?! Please help! At a guess because lat and long are type ""keyword"" instead of a numeric type maybe it can't do the ""TO"" query on them correctly? What field type should they be? That was the only one that seemingly made sense to me. The reason I made the previous a comment rather than an answer is I have no idea. I use Solr rather than bare Lucene and in Solr I'd use solr.FloatField and I don't know what that translates to in Lucene. okay well when i take out the lat/lng part that first part works. So it's gotta be something with the floats not being able to work right. Please see this question about geo-search in Lucene. I believe indexing and searching float values will not work. Try Making these large zero padded integers. e.g. lat:[-74486951 TO -73486951] This could have a detrimental effect in terms of performance so consider using a lower resolution or use some techniques from This question which discusses indexing longitude and latitude data in Java Lucene (Zend Lucene is a bit behind in versions so try to use the older stuff)."
480,A,What is the best search approach? I'm using lucene in my project. Here is my question: should I use lucene to replace the whole search module which has been implemented with sql using a large number of like statement and accurate search by id or sth or should I just use lucene in fuzzy search(i mean full text search)? I'm actually very impressed by Solr at work we were looking for a replacement for our Google Mini (it's woefully inadequate for any serious site search) and were expecting something that would take a while to implement. Within 30 minutes of installing Solr we had done what we had expected to take at least a few days and provided us with a far more powerful search interface than we had before. You could probably use Solr to do quite a lot of clever things beyond a simple site search.  I don't think using like statement abusively is a good idea. And I believe the performance of lucene will be better than database.  Probably you should use lucene unless the SQL search is very performant. We are right now moving to Solr (based on Lucene) because our search queries are inherently slow and cannot be sped up with our database.... If you have reasonably large tables your search queries will start to get really slow unless the DB has some kind of highly optimized free text search mechanisms. Thus let Lucene do what it does best.... You helped! Thank you!
481,A,"Using FieldSelector when searching with Lucene I'm searching articles in PubMed via Lucene. Each of the 20000000 articles has an abstract with ~250 words and an ID. At the moment I store my searches with each take multiple seconds in a TopDocs object. Searchs can find thousands of articles. I'm just interested in the ID of the article. Does Lucene load the abstracts internally into the TopDocs? If so can I prevent that behavior through FieldSelectors or do FieldSelectors only work with IndexReader and don't work with IndexSearcher? You're on the right lines. Try using a SetBasedFieldSelector when you retrieve the document from the index. As another poster noted iterating through the hits will return a ScoreDoc object. This will give you the document Id that can be used to retrieve the document using the IndexReader associated with the IndexSearcher. If IO is a problem because of loading fields you aren't interested in you should be in for a pleasant surprise. Hope this helps  No Lucene does not load the values of fields into TopDocs. TopDocs only contains the doc number and score for each one of the matching documents. If you're having performance issues here's another SO question that can help you: http://stackoverflow.com/questions/668441/optimizing-lucene-performance  Lucene by default does not load any stored fields. If you want to retrieve only the ID field and if you can afford to load up all the IDs in memory then you can load all values as follows and reuse them. String[] allIDs = FieldCache.DEFAULT.getStrings(indexReader ""IDFieldName"") Please check the answer for FieldCache. http://stackoverflow.com/questions/2511879/best-way-to-retrieve-certain-field-of-all-documents-returned-by-a-lucen-search/2513252#2513252"
482,A,"Get all lucene values that have a certain fieldName To solve this problem I created a new Lucene index where all possible distincted values of each field are indexed seperatly. So it's an index with a few thousand docs that have a single Term. I want to extract all the values for a certain term. For example I would like all values that have the fieldName ""companyName"". Defining a WildcardQuery is off course not a solution. Neither is enumerating ALL fields and only saving the ones with the correct fieldName. This should work (I take it it still is in C#) IndexReader.Open(/* path to index */).Terms(new Term(""companyName"" String.Empty)); ah empty string :) (added C# tag) I always missed that in Java. But I realize `String.MissingString` would have been appropriate. (Huh good that java did not have this!)"
483,A,"lucene BooleanQuery problem I am searching in lucene with a ""equals"" operator implemented like: return new TermQuery(new Term(getName() getValue())); for a vale like: customerID:YADA-UT-08ec5de9-8813-4361-be88-55695ddfaa00 This is working. BUT if i use an ""in"" operator implemented with a BooleanQuery like; final BooleanQuery booleanQuery = new BooleanQuery(); for (final String aValue : value) { booleanQuery.add(new TermQuery(new Term(getName() aValue)) BooleanClause.Occur.SHOULD); } it will not find any customer with YADA-UT-08ec5de9-8813-4361-be88-55695ddfaa00 After a lot of tests i am assuming that the length of 'YADA-UT-08ec5de9-8813-4361-be88-55695ddfaa00' or dashes can be the problem. (i use token to keep it in db) when using with BooleanQuery. Any clue ? EDIT: What is strange is that: - this is working with ""in"" (Boolean query): 25c20c21-bd88-4a6d-aa02-209b5fb6fb11 - this is not working with it: YADA-UT-08ec5de9-8813-4361-be88-55695ddfaa00 Solution Found: the lucene words were indexed as lower case :) Try calling `toString()` on the `Query` objects this gives you a textual representation of what will be executed and can be very useful for debugging Generally you should use the same analyzer when indexing and parsing query string. Lowercasing filter is a part of StandardAnalyzer. fyi - if you find a solution its best to post it as the answer to your own question :-) Please post your solution as an answer and accept it on a daily basis I see this question I click to answer it only to realize it has been already solved :) Strange solution for me but...that's it:) the lucene words were indexed as lower case :)"
484,A,"How do I get solr term frequency? I have a question that how could somebody get term frequency as we do get in lucene by the following method DocFreq(new Term(""Field"" ""value"")); using solr/solrnet. Try debugQuery=on or TermsComponent. None of them are currently supported through SolrNet so you can either work around it or implement them and contribute them to the project. +1 for the contributor appeal. Actually i will have a look inside your SolrNet (http://code.google.com/p/solrnet/) project."
485,A,"Filters in Lucene Friends I am new to lucene full text search. i have developed page with full text seach. it works fine till. but now i want to add extra condition like where clause. how to do it. The requirement given for me is i have to list proposal which is created by logged in user. I have to add this condition in back end without user knowledge. I heard about filter. Which filter is correct?how to apply that.Give me an sample. this evening i have demo. help me. First you need to ensure that the user id is being added to the document in the index in a field when you index let's call it user_id. In a pinch you can add a field to the query string entered by the user behind the scenes before you send it to the query parser. So take whatever query was entered and add "" AND user_id:4"" (where 4 is the value of the variable containing the current user id) onto the end of it. Thanks. Is it correct if i add Query + ( user_id: 4 ) + (status_id : 3). I tried like this and it is working what i thought."
486,A,How to maintain lucene indexes in azure cloud-app I just started playing with the Azure Library for Lucene.NET (http://code.msdn.microsoft.com/AzureDirectory). Until now I was using my own custom code for writing lucene indexes on the azure blob. So I was copying the blob to localstorage of the azure web/worker role and reading/writing docs to the index. I was using my custom locking mechanism to make sure we dont have clashes between reads and writes to the blob. I am hoping Azure Library would take care of these issues for me. However while trying out the test app I tweaked the code to use compound-file option and that created a new file everytime I wrote to the index. Now my question is if I have to maintain the index - i.e keep a snapshot of the index file and use it if the main index gets corrupt then how do I go about doing this. Should I keep a backup of all the .cfs files that are created or handling only the latest one is fine. Are there api calls to clean up the blob to keep the latest file after each write to the index? Thanks Kapil whouldn't it be better (like they wrote in _Azure Library for Lucene.Net_) to create another role that periodically downloads the index from the BlobStorage and allows searching through a Web Service? i am using AzureDirectory for Full Text indexing on Azure and i am getting some odd results also... but hopefully this answer will be of some use to you... firstly the compound-file option: from what i am reading and figuring out the compound file is a single large file with all the index data inside. the alliterative to this is having lots of smaller files (configured using the SetMaxMergeDocs(int) function of IndexWriter) written to storage. the problem with this is once you get to lots of files (i foolishly set this to about 5000) it takes an age to download the indexes (On the Azure server it takes about a minute of my dev box... well its been running for 20 min now and still not finished...). as for backing up indexes i have not come up against this yet but given we have about 5 million records currently and that will grow i am wondering about this also. if you are using a single compounded file maybe downloading the files to a worker role zipping them and uploading them with todays date would work... if you have a smaller set of documents you might get away with re-indexing the data if something goes wrong... but again depends on the number....  After i answered this we ended up changing our search infrastructure and used Windows Azure Drive. We had a Worker Role which would mount a VHD using the Block Storage and host the Lucene.NET Index on it. The code checked to make sure the VHD was mounted first and that the index directory existed. If the worker role fell over the VHD would automatically dismount after 60 seconds and a second worker role could pick it up. We have since changed our infrastructure again and moved to Amazon with a Solr instance for search but the VHD option worked well during development. it could have worked well in Test and Production but Requirements meant we needed to move to EC2.
487,A,Which is better enabling indexing on RDBMS or Lucene Indexing I have an application which uses traditional Database for all of its data  i need to develop a search functionality i did small prototype with lucene and results are gr8  now the bigger question arises  for each of users add/delete/update operations i need to update db and the Lucene index too  will I get similar search performance if i just enable indexing on few fields in traditional db instead of moving to Lucene ? is it worth the effort ?. Please check this question - http://stackoverflow.com/questions/553055/best-full-text-search-for-mysql and this one http://stackoverflow.com/questions/119994/should-an-index-be-optimised-after-incremental-indexes-in-lucene It depends entirely on the size of the corpus and on the type and frequency of updates. A separated full-text search solution like lucene gives you much more flexibility when tweaking relevance and by decoupling the updates of the rdbm and the full-text index gives you more options when trying to optimize performance. If your never played with Lucene I would greatly recommend you to use some more high-level solution like Solr (or websolr) Sphinx ElasticSearch or IndexTank. Lucene is very low level.
488,A,"Calling search gurus: Numeric range search performance with Lucene? I'm working on a system that performs matching on large sets of records based on strings and numeric ranges and date ranges. The String matches are mostly exact matches as far as I can tell as opposed to less exact full text search type results that I understand lucene is generally designed for. Numeric precision is important as the data concerns prices. I noticed that Lucene recently added some support for numeric range searching but it's not something it's originally designed for. Currently the system uses procedural SQL to do the matching and the limits are being reached as to the scalability of the system. I'm researching ways to scale the system horizontally and using search engine technology seems like a possibility given that there are technologies that can scale to very large data sets while performing very fast search results. I'd like to investigate if it's possible to take a lot of load off the database by doing the matching with the lucene generated metadata without hitting the database for the full records until the matching rules have determined what should be retrieved. I would like to aim eventually for near real time results although we are a long way from that at this point. My question is as follows: Is it likely that Lucene would perform many times faster and scale to greater data sets more cheaply than an RDBMS for this type of indexing and searching? http://wiki.apache.org/lucene-java/SearchNumericalFields Lucene stores its numeric stuff as a trie; a SQL implementation will probably store it as a b-tree or an r-tree. The way Lucene stores its trie and SQL uses an R-tree are pretty similar and I would be surprised if you saw a huge difference (unless you leveraged some of the scalability that comes from Solr). As a general question of the performance of Lucene vs. SQL fulltext a good study I've found is: Jing Y. C. Zhang and X. Wang. “An Empirical Study on Performance Comparison of Lucene and Relational Database.” In Communication Software and Networks 2009. ICCSN'09. International Conference on 336-340. IEEE 2009. First when executing exact query the performance of Lucene is much better than that of unindexed-RDB while is almost same as that of indexed-RDB. Second when the wildcard query is a prefix query then the indexed-RDB and Lucene both perform very well still by leveraging the index... Third for combinational query Lucene performs smoothly and usually costs little time while the query time of RDB is related to the combinational search conditions and the number of indexed fields. If some fields in the combinational condition haven’t been indexed search will cost much more time. Fourth the query time of Lucene and unindexed-RDB has relations with the record complexity but the indexed-RDB is nearly independent of it. In short if you are doing a search like ""select * where x = y"" it doesn't matter which you use. The more clauses you add in (x = y OR (x = z AND y = x)...) the better Lucene becomes. They don't really mention this but a huge advantage of Lucene is all the built-in functionality: stemming query parsing etc. @barrymac: You may also be interested in http://philosophyforprogrammers.blogspot.com/2010/09/lucene-performance.html and the papers cited therein. You can plug in some values of sample searches and see what your expected performance gain might be (assuming you know the metrics for your current implementation of course). Great answer! That really shines the light. This problem will have some cases where tens of rules may be running sequentially on the dataset and it may be able to improve things a lot here. I'm an aspiring empiricist so I really appreciate the reference. I'm thinking also that as well as some of the nice functionality there's also an architectural advantage to separating out the indexing load.  At its heart and in its simplest form Lucene is a word density search engine. Lucene can scale to handle extremely large data sets and when indexed correctly return results in a blistering speed. For text based searching it is possible and very probable that search results will return quicker in Lucene as opposed to SQL Server/Oracle/My SQL. That being said it is unfair to compare Lucene to traditional RDBMS as they both have completely different usages. I am considering offloading only the text searches to lucene but first I will have to find out what proportion of the load is attributed to text searches to justify the investment. I'm sure it's a lot faster at this. Well technology comparison aside I'm considering adding lucene as a whole system optimisation as opposed to an either or option. I would definately recommend using a combination of Lucene and RDBMS you'll be really surprised by the performance. Well I implemented a trivial hibernate search setup once. Although that particular setup is actually quite limited the speed and power of lucene on large data sets would be enough to make you giddy :-)  I suggest you read Marc Krellenstein's ""Full Text Search Engines vs DBMS"". A relatively easy way to start using Lucene is by trying Solr. You can scale Lucene and Solr using replication and sharding. Thank you very much for those helpful links!"
489,A,Lucene and Django with Limited Memory I'm running a Django application on a shared web server with limited application memory. I want to incorporate Lucene for a search functionality. What are my options given the limited amount memory? I thought about using Solr via solrpy but it looks like Solr is very memory hungry. Do you think it would be possible to reduce its memory footprint to a mere 15-20MB for a small dataset (only thousands of sentences). I would prefer solutions that require only Python but I am also open to other suggestions. Thanks in advance. You're not going to get the JVM to even fit in 15MB. I wouldn't try to run Solr unless there's at least 200MB memory. You could try Woosh a pure Python search library or Xapian which is in C++. Haystack makes it easy to integrate either into Django. I've used Xapian + Django in a limited memory environment successfully.
490,A,"Highlighting whole sentence in Lucene.net 2.9.2 Currently I'm working with the Lucene.net 2.9.2 framework. As a result of my search I would like to achieve result page (asp.net) with highlighted text fragment. I would like that the selected fragment is a whole sentence and not only few words. For example if I have text: Lorem ipsum dolor sit amet consectetur adipisicing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. and I'm searching for cupidatat I would like to get fragment: Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. The code that I have right now is: var scorer = new QueryScorer(q); var formatter = new SimpleHTMLFormatter(""<div>"" ""</div>""); var highlighter = new Highlighter(formatter scorer); highlighter.SetTextFragmenter(new SimpleFragmenter(100)); var fragments = highlighter.GetBestFragments(stream text 1); but it returns only text range of size 100. I will be thankful for any suggestion. You want to create a new Fragmenter (Similar to SimpleFragmenter). The function you need to adjust is: public virtual bool IsNewFragment(Token token) { bool isNewFrag = token.EndOffset() >= (fragmentSize * currentNumFrags); if (isNewFrag) { currentNumFrags++; } return isNewFrag; } This will likely need some adjustment until you get the correct logic but that should give you a pretty good head start Thank you - I will try to work with it today."
491,A,Luke-Lucene Index Tool for searching I have a scenario to search. for that i am using Lucene Index Tool. so how can i make a query with less than or greater than conditions. presently it is taking only OR  AND. From the website http://lucene.apache.org/java/3_2_0/queryparsersyntax.html luncene is not support less than or greater than conditions. Maybe Range Searches will do it. thanks. That what you said is right and i am using TO to get the effect of between
492,A,"Lucene: find all words that start with specific prefix I want to get a list of all words in a Lucene index that start with a specific prefix. I've been looking for a way to query the terms in the index (I need the terms I don't care about the documents they are from) but without success. Any ideas? Got it! FilteredTermEnum subclasses (FuzzyTermEnum RegexTermEnum WildcardTermEnum) do exactly what I need. Here's a quick example: FSDirectory dir = FSDirectory.open(new File(""index"")); IndexWriter writer = new IndexWriter(dir new WhitespaceAnalyzer() true new IndexWriter.MaxFieldLength(20)); IndexReader reader = IndexReader.open(dir); Document doc = new Document(); doc.add(new Field( ""text"" ""Life #consists not in #holding good cards but in playing those you hold well."" Field.Store.NO Field.Index.ANALYZED)); writer.addDocument(doc); writer.close(); WildcardTermEnum tagsEnum = new WildcardTermEnum(reader new Term(""text"" ""#*"")); do { System.out.println(tagsEnum.term()); } while (tagsEnum.next());"
493,A,"SpatialQuery for location based search using Lucene My lucene index has got latitude and longitudes fields indexed as follows: doc.Add(new Field(""latitude"" latitude.ToString()  Field.Store.YES Field.Index.UN_TOKENIZED)); doc.Add(new Field(""longitude"" longitude.ToString() Field.Store.YES Field.Index.UN_TOKENIZED)); I want to retrieve a set of documents from this index whose lat and long values are in a given range. As you already know Lat and long could be negative values. How do i correctly store signed decimal numbers in Lucene? Would the approach mentioned below give correct results or is there any other way to do this?  Term lowerLatitude = new Term(""latitude"" bounds.South.ToString() ); Term upperLatitude = new Term(""latitude"" bounds.North.ToString()); RangeQuery latitudeRangeQuery = new RangeQuery(lowerLatitude upperLatitude true); findLocationQuery.Add(latitudeRangeQuery BooleanClause.Occur.SHOULD); Term lowerLongitude = new Term(""longitude"" bounds.West.ToString()); Term upperLongitude = new Term(""longitude"" bounds.East.ToString()); RangeQuery longitudeRangeQuery = new RangeQuery(lowerLongitude upperLongitude true); findLocationQuery.Add(longitudeRangeQuery BooleanClause.Occur.SHOULD); AlsoI wanted to know how Lucene's ConstantScoreRangeQuery is better than RangeQuery class. Am facing another problem in this context: I've one of the documents in the index with the following 3 cities: Lyons IL Oak Brook IL San Francisco CA If i give input as ""Lyons IL"" then this record comes up. But if i give San Francisco CA as input then it does not. However if i store the cities for this document as follows: San Francisco CA Lyons IL Oak Brook IL and when i give San Francisco CA as input then this record shows in the search results. What i want here is that if i type any of the 3 cities in inputI should get this document in the search results. Please help me achieve this. Thanks. This is really 3 separate questions. Why don't you split it? Here. I did the first step for you: http://stackoverflow.com/questions/1054719 thanks itsadok... Following up on skaffman's suggestion you can use the same tile coordinate system used by all the popular map apps. Choose whatever zoom level is granular enough for your needs and don't forget to pad with leading zeros. Regarding RangeQuery it's slower than ConstantScoreRangeQuery and limits the range of values. Regarding the city-state problem we can only speculate. But the first things to check are that the indexed terms and the parsed query are what you expect them to be.  One option here is to convert the coordinates into a system that doesn't have negative numbers. For example I've had a similar problem for a google maps webapp for the UK and I stored UK Easting/Northings (which range from 0 to 7 digits) fields in Lucene alongside the lat/long values. By formatting these eastings/northings with left-padded zeroes I could do lucene range queries. Is there a similar coordinate system for the US? thanks..i shall explore that option...could you please anwer second part of my question regarding multiple cities? Not without a good deal more information no  I think the best way is to convert/normalize the coordinates as suggested in the previous post. This article does exactly this. It's actually quite nice object orientated code. Regarding your second problem. I would assume you have some sort of Analyzer problem. Are you using the same Analyzer for indexing and querying? Which tokenizers do you use? I recommend to use Luke to inspect your generated index to see what tokens are actually searchable. --Hardy"
494,A,Why is the analyzer defined globally in Zend.Search.Lucene? I just noticed that the Zend lucene implementation has a default analyzer that can be modified using Zend_Search_Lucene_Analysis_Analyzer::setDefault() but I couldn't find a way to override that default when performing a query. Do I really need to reset the default analyzer if I'm working on multiple indexes or am I missing a function? In the original Java API for Lucene QueryParser takes an analyzer argument. I'm not sure why they decided to use a global variable in Zend Framework but apparently setting the analyzer globally is the only way to do it. Thanks I guess the API is just stupid.  I use the TextNum analyzer because the default (Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum_CaseInsensitive) doesn't allow searching by integers. To override the default I run: Zend_Search_Lucene_Analysis_Analyzer::setDefault(new Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum());
495,A,Building search to the website I have a website which has about 200 to 300 static public pages. I am required to bring about some kind of search functionality on the website which will search all of its public pages. I don't want to use external tools like Google site search etc. Is there a tool or open source code that will index the content and then display the search results? I am looking for a tool that will maintain its own index and run on the server along with the website. If I can add items to the index and manage it that would be a plus. I have looked at Zend Search Lucene if there is a tool out there I am hoping to use it before I make one myself. Lucene and Solr are pretty much the right tools for the job You can try - Sphider is a lightweight web spider and search engine written in PHP using MySQL as its back end database. It is a great tool for adding search functionality to your web site or building your custom search engine. Sphider is small easy to set up and modify and is used in thousands of websites across the world. http://www.sphider.eu/  Zend_Search_Lucene is a very good choice. It is compatible with the Java version of Lucene (I mean the index files). It can index html documents it's quite simple to use and configure and has a good documentation. Using Solr as a service is an option as well. Sphinx is another tool you might want to look at. All three tools Emil mentioned can probably do the job. I believe you should choose among them according to their learning curve for you (Zend_Search_Lucene complements an existing PHP code base Solr is a stand-alone search server easy to set up; Sphinx I believe to be similar to Solr in the set up respect).
496,A,"Update specific field on SOLR index I want to using solr for search on articles I have 3 table: Group (id  group name) ArticleBase (id groupId some other field) Article(id articleBaseId title date ...) in solr schema.xml file i just define all article field that mixed with ArticleBase table (for use one index on solr) like this: (id articleBaseId groupId ...) problem: Admin want to change group (ArticleBase) therefore i must update (or replace) all indexed article in solr. right ? can i update groupId only in solr index ? have any solution ? Note:Article table contains more than 200 million article and i using solr for index only (not store any field data except article id) Please refer to this document about the ""Partial Documents Update"" Feature in Solr 4.0 Solr 4.0 is now final and production-ready. This feature makes it possible to update fields and even adding values to multiValued fields. Mauricio was right with his answer back in 2010 but this is the way things are today.  SolrPHP doesn't provide any method to update a specific field in Solr. However you can make a Curl call in PHP to update a specific field: <?php // Update array $update = array( 'id' => $docId $solrFieldName => array( 'set' => $solrFieldValue ) ); $update = json_encode(array($update)); // Create curl resource and URL $ch = curl_init('http://'.SOLR_HOSTNAME.':'.SOLR_PORT.'/'.SOLR_COLLECTION.'/update?commit=true'); // Set Login/Password auth (if required) curl_setopt($ch CURLOPT_USERPWD SOLR_LOGIN.':'.SOLR_PASSWORD); // Set POST fields curl_setopt($ch CURLOPT_POSTtrue); curl_setopt($ch CURLOPT_POSTFIELDS $update); // Return transfert curl_setopt($ch CURLOPT_RETURNTRANSFER 1); // Set type of data sent curl_setopt($ch CURLOPT_HTTPHEADER array('Content-Type:application/json')); // Get response result $output = json_decode(curl_exec($ch)); // Get response code $responseCode = curl_getinfo($ch CURLINFO_HTTP_CODE); // Close Curl resource curl_close($ch); if ($responseCode == 200) { echo 'SOLR: Updated successfully field '.$solrFieldName.' for id:'.$docId.' (query time: '.$output->responseHeader->QTime.'ms).'; } else { echo ('SOLR: Can\'t update field '.$solrFieldName.' for id:'.$docId.' response ('.$responseCode.') is: '.print_r($outputtrue)); } I use this code to update in JSON you can also provide data in XML.  Solr does not support updating individual fields yet but there is a JIRA issue about this (almost 3 years old as of this writing). Until this is implemented you have to update the whole document. UPDATE: as of Solr 4+ this is implemented here's the documentation. Let me just comment that Mauricio means just that one single document needs to be updated not your entire 200 million article table."
497,A,"Can we customize Lucene which is embedded in Solr? Can we customize Lucene which is embedded in Solr just as we can in raw Lucene ? So that we can have ""everything"" that we have in Lucene in Solr ? I am asking this because we are stuck at a point of deciding Solr vs Lucene thinking like so : Argument 1 : ""We might hit a dead zone in future if we choose Solr and Lucene is a better choice hence... So we might as well start writing HTTP wrappers and almost half of Solr ourselves on top of Lucene to be on safer side. "" Argument 2 : ""Solr already has all the features we want to use so why not just use it ? Since people who commit to Lucene are also responsible for committing to Solr all features of Lucene are available to Solr too..."" I went through many blogs and posts that say something like : For situations where you have very customized requirements requiring low-level access to the Lucene API classes Solr would be more a hindrance than a help since it is an extra layer of indirection. -http://www.lucenetutorial.com/lucene-vs-solr.html One way of defending Argument 2 is by confirming that we can customize the underlying Lucene in Solr just like we would do if we had only Lucene. Can someone provide a better way of closing this argument ? :) ps : We need a fast search with indexing and sharding terabytes of data... Can we customize Lucene which is embedded in Solr ? Yes you can. But keep this in mind: Lucene and Solr committers are some of the foremost experts in the field of full-text search. They have several years of experience in this field. If you think you can do better than them then go ahead and change Solr to your needs (it's Apache-licensed so there aren't any commercial restrictions) and if you do so try to do it so that you can later contribute it back to the project so everyone can benefit and the project moves forward. For the vast majority of Solr users though the stock product is more than enough and satisfies all needs. In other words before jumping in to change the code ask on a mailing list (stackoverflow or solr-user) there's a good chance that you don't really need to change any code. ""Fast search with indexing and sharding terabytes of data"" is precisely what Solr was built for. It would be a bad case of Not-Invented-Here not to use it or any of the other similar solutions such as ElasticSearch Sphinx Xapian etc. If you think you'll need to customize or extend the search server in any way consider the license and underlying code of each one. Solr and ElasticSearch are both Apache-licensed so they don't have commercial restrictions and are built on top of Lucene a well-known library. Thanks for the answer I appreciate you making time to read a huge rant and all but you didn't answer my question... Just consider a case where I ""might"" run into a situation where I don't find what I am looking for in Solr and want to alter something... Is it _Possible_ ? @Shrinath: sorry if I wasn't clear I thought I did answer that the direct answer is **yes you can it's possible** Awesome :) Thank you :)"
498,A,java AbstractMethodError How to handle this error in lucene: java.lang.AbstractMethodError: org.apache.lucene.store.Directory.listAll()[Ljava/lang/String; at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:568) at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:69) at org.apache.lucene.index.IndexReader.open(IndexReader.java:316) at org.apache.lucene.index.IndexReader.open(IndexReader.java:188) I am making a lucene function call but unfortunately it itself calls an abstract method of some class as is evident from the error above. What is the work around for this? Thanks Akhil Another option is that something bad happened to your index - either it was built using a different version of Lucene or a file is missing. Try opening the index using luke. Nah that couldn't cause an AbstractMethodError.  Ok! I found the answer. It was not the problem of version mismatch. Rather the hadoop contrib's FileSystemDirectory which extends abstract class Directory did not implement the abstract function listAll(). listAll() function was being called by lucene indexReader.open() function. I added this function and it is up and running now. Thanks  An AbstractMethodError can only occur when a class definition has changed incompatibly so it looks like you're using an incompatible combination of JARs of different parts of Lucene. Try updating all your Lucene JARs to the latest version. You are right! I am using lucene-core-3.0.0 and another hadoop-contrib-index.jar that I got with hadoop-0.19.0 a year ago. Probably at that time lucene had some otehr version. I will try to get new jar for hadoop-contrib-index. let's see if that works. Thanks!
499,A,IKVM.NET and Lucene I am using Lucene.Net but there are some interesting Java components for Lucene (especially analyzers) that haven't been ported to Lucene.NET yet so maybe IKVM is a better choice. Some research has shown that IKVM seems to work pretty well but I haven't seen anything regarding Lucene. Does anybody have experience running Lucene with IKVM.NET and can share his impressions? Thanks Do not quite have an answer for you since I am in the same boat. In fact it might be worthwhile to try it out and open-source it. We are using Lucene through IKVM in a big C#-project. It works very well also the performance is quite good. We also using the highlighter component - as described under the link in the post above. But also you should keep an eye on Lucene.Net in the future. It is now a official apache top-level-project (http://www.infoq.com/news/2012/08/Lucene-net). So i guess it will catch up to Lucene (Java) in some time..  Not my experiences but somebody else's from a while ago: http://stackoverflow.com/questions/252249/how-do-you-run-lucene-on-net On a related note I've used Tika through IKVM and it worked pretty well. Excellent! Thanks for the link and sharing your experience with Tika. Tika is also on the list of libraries I would like to use from .NET. Right now I am starting the java version as separate process but a direct integration would be nicer. It seems IKVM is definitely worth a try.
500,A,Searching in multiple classes using playframework's search-module As described at playframework - search module I've installed the search-module. Of course my model consists of multiple classes. I'd now like to search for entries whose attributes consist of several tables i.e. i'd like to search for users who have bought an article that costs more than say 500$ whereas there would be a table for customers one for orders and one for articles. Does anyone know how to realize this using the playframework's lucene-query-language? thanks a lot! by the way i'm using search-module version 2.0. The query language isn't specific to Play! You can learn about the syntax from the official Lucene documentation: http://lucene.apache.org/java/2_0_0/queryparsersyntax.html
501,A,"Solr/Solrj: How can I determine the total number of documents in an index? How can I determine the total number of documents in a Solr index using Solrj? After hours of searching on my own I actually have an answer (given below); I'm only posting this question so others can find the solution more easily. Here's what I'm using. Is this canonical? Is there a better way?  SolrQuery q = new SolrQuery(""*:*""); q.setRows(0); // don't actually request any data return server.query(q).getResults().getNumFound(); yes this is correct."
502,A,Sitecore Lucene indexing - save child field values in parent Lucene doc I have a Sitecore content structure where any single item can have a number of child items that are used to store enumerable content for lists (obviously a fairly standard approach). I am hoping to index these items but store their index data against the parent doc in Lucene. This should hopefully speed up the search bit by saving time sorting through multiple results which all effectively point to the same URL. Below is some basic code for the custom indexer I will implement. Can anyone let me know if this is (a) possible and (b) a good idea? The main issues I see are that the Lucene doc already looks like it has been created - do I need to delete it? Also if the Lucene doc for the parent item does not exist do I need to create it? And will it be overwritten/lost when the parent item is indexed. Looks like a bit of room of conflict there. Another option is that I don't index child items but get their values when I am indexing the parent. Now that I think about it this seems like the better way to go.. opinions? public class CustomIndex : Sitecore.Data.Indexing.Index { public CustomIndex(string indexName): base(indexName) {} protected override void AddFields(Item item Document document) { //is item a sub-item (promo item) if (...) { //delete the sub-item lucene doc DeleteDoc(document); //is this possible or needed? //get parent item Item parentItem = item.Parent; //get lucene document for parent item Document parentDoc = GetParentDoc(); //add fields to parent item lucene document parentDoc.Add(...); parentDoc.Add(...); } else { base.AddFields(item document); } } } yes i agree option #2 is better - index the children when you're at the parent. mainly because you aren't guaranteed what order the traversal will happen in so the document may get re-created as you said. I agree with you. You may also decide to override the Sitecore.Search.Crawlers.DatabaseCrawler so that you won't have to much data to process.
503,A,"Better search results using Lucene I've got a database with a lot of books in it. I've got fields like title descriptions authors etc. I'm indexing title with a boost of 100f and description with a boost of 0.1f both fields tokenized and stemmed. I'm searching with a single input field that searches in all available fields using a booleanquery joined with BooleanClause.Occur.SHOULD and containing a wildcardquery for each field. I also remove all ""stopwords"" from the query to start with. The problem i'm having is when i search for the string without the quotes ""de wetenschap van het leven"" after removing the stop words i get ""wetenschap leven"" The Title query becomes ""*wetenschap* *leven*"" the description query the same with a wrapping booleanquery joined with BooleanClause.Occur.SHOULD. The following books are in the db Wetenschappelijk denken. Een inleiding voor de medische en biomedische wetenschappen en voor de andere levenswetenschap. De wetenschap van de aarde. Over een levende planeet Atlas van de menselijke levensloop De wetenschap van het leven. Over eenheid in biologische diversiteit The book return in the first 4 books that's good but in this implementation we cut off at 3 and the rest is below a read more link. Just upping the cutoff is not an option For me the ""De wetenschap van het leven. Over eenheid in biologische diversiteit"" book matches the query ""more"" then the others (or so i feel) but i'm unable to find the correct index/search combination to make this work. Does anyone have an idea? I improved the relevance by adding a phrase search for the entire string as well. This way we still get the ""search in everything"" behavior and the titles are a lot more relevant then the rest. It's been awhile but how did you do an entire-string phrase search as well did you just do two searches then combine the results somehow?  I think a SpanQuery (specifically a SpanNearQuery) might be what you need. Given a document ""a quick brown fox jumps over a lazy dog"" it can find a match for ""brown fox "" and ""lazy dog"". You can adjust the slop setting to adjust the distance between the two search query phrases/terms....in short it gives you a lot of tools to tweak your search. Also unfamiliar with dutch(?) language you might want to stem your queries if possible and avoid leading wildcards - they are quite expensive and lead to lower precision and recall.  A few suggestions: Do not remove stop words - they seem to be an important part of your search query. Do not use wildcards - search just for the words you need. I believe the best will be to use a PhraseQuery - e.g. ""de wetenschap van het leven"". Do not search past sentence end. This is tougher - you may need to index each sentence separately. Read Debugging Relevance Issues in Search - you will probably get other ideas there."
504,A,Updating Lucene index from two different threads in a web application I've a .net web application which uses Lucene.net for company search functionality. When registered users add a new companyit is saved to database and also gets indexed in Lucene based company search index in real time. When adding company in Lucene index how do I handle use case of two or more logged-in users posting a new company at the same time?Also will both these companies get indexed without any file lock lock time out etc. related issues? Would appreciate if i could help with code as well. Thanks. By default Lucene.Net has inbuilt index locking using a text file. However if the default locking mode isn't good enough then there are others that you can use instead (which are included in the Lucene.Net source code). What if there many new contents? Won't it be resource intensive for all the commits?
505,A,"How to parse Maven repository indexes generated by Nexus I have downloaded the indexes generated for Maven Central from http://mirrors.ibiblio.org/pub/mirrors/maven2/dot-index/nexus-maven-repository-index.gz I would like to list the artifacts information from these indexes (groupId artifactId version for example). I have read that there is a high level API for that. It seems that I have to use the following maven dependency. However I don't know what is the entry point to use (which class?) and how to use it? <dependency> <groupId>org.sonatype.nexus</groupId> <artifactId>nexus-indexer</artifactId> <version>3.0.4</version> </dependency> Take a peek at https://github.com/cstamas/maven-indexer-examples project. In short: you dont need to download the GZ/ZIP (new/legacy format) manually it will indexer take care of doing it for you (moreover it will handle incremental updates for you too if possible). GZ is the ""new"" format independent of Lucene index-format (hence independent of Lucene version) containing data only while the ZIP is ""old"" format which is actually plain Lucene 2.4.x index zipped up. No data content change happens currently but is planned in future. As I said there is no data content change between two but some fileds (like you noticed) are Indexed but not stored on index hence if you consume the ZIP format you will have them searchable but not retrievable. Thanks ~t~ Thanks for the clarifications. It's exactly what I was looking for.  The legacy zip index is a simple lucene index. I was able to open it with Luke and write some simple lucene code to dump out the headers of interest (""u"" in this case) import org.apache.lucene.document.Document; import org.apache.lucene.search.IndexSearcher; public class Dumper { public static void main(String[] args) throws Exception { IndexSearcher searcher = new IndexSearcher(""c:/PROJECTS/Test/index""); for (int i = 0; i < searcher.maxDoc(); i++) { Document doc = searcher.doc(i); String metadata = doc.get(""u""); if (metadata != null) { System.out.println(metadata); } } } } Sample output ... org.ioke|ioke-lang-lib|P-0.4.0-p11|NA org.jboss.weld.archetypes|jboss-javaee6-webapp|1.0.1.CR2|sources|jar org.jboss.weld.archetypes|jboss-javaee6-webapp|1.0.1.CR2|NA org.nutz|nutz|1.b.37|javadoc|jar org.nutz|nutz|1.b.37|sources|jar org.nutz|nutz|1.b.37|NA org.openengsb.wrapped|com.google.gdata|1.41.5.w1|NA org.openengsb.wrapped|openengsb-wrapped-parent|6|NA There may be better ways to achieve this though... I found that the gz contains much more information like the JAR file size the description the classes contained by the JAR and so on. I have tried to parse the gz version as in your example but it is not possible because the gz archive does not contain lucene segments. Also before to post my question here I have already seen the github link you point me out. However from this github example I cannot understand what is the main class to use as an entry point supposing that I have the unzipped gz file. hmm i believed this alternate method would solve your problem. You can retrieve the jar from the index using the lucene api and get its size and the classes it contains. Best of luck finding a nexus-indexer implementation Thanks for your help. Do you know what is the difference between the zip and the gz one? I know that the gz use a proprietary binary format but does it contain more information? Do you know how to iterate on it? Not really sure of the internal differences. Googling says that the gz is optimized for speed. If you'd like to use it as input there seems to be a sample available @ https://github.com/sonatype/nexus/tree/master/sandbox/nexus-indexer-sample"
506,A,"Searching for multiple words in on field in Lucene index I'm having problem with Zend_Search_Lucene. I have few documents with field ""tags"" in index. Documents ""tags"" have following values: tag1 tag2 tag3 tag1 tag4 I would like to find document only with tag1 AND tag4 so I use query ""+tags:tag1 +tags:tag2"". I can't figure out why I get 0 hits from index. Hard to tell from just that description have you tried using Luke to run the query? Also Check which analyzer you are using to query. I resolved this problem. Default Zend_Search_Lucene analyzer skips digits. There is a special analyzer for this and it should be set as default before indexing and searching. Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum_CaseInsensitive() );"
507,A,Document boosting in Hibernate Search / Lucene Everything else remaining the same some of my objects are more valuable than others. Is it possible to boost objects at index time let's say according to the USD price? Or if you have a limited number of values I was hoping that this would work: f:aaa^4 f:bbb^3 f:ccc^2 animal:elephant but I must be missing something. What you are saying will work. However you will probably want to reformat it as (boost query) +(original query) e.g. (f:a^4 f:b^3 b:c^2) +animal:elephant. The way you have it now will find things which only have f:aaa regardless of animal:elephant. You can see Lucene in Action for an example of how to write a custom scorer which allows you to do somethign more like boosting according to price. I'm not aware of any way you can do this without writing some code though.  If you are using hibernate-search you can add boosting directly to fields using annotation. There are two type of boost: dynamic boost which apply to the whole object or field level boost. You probably want dynamic boost and can be specified using: @DynamicBoost(impl = VIPBoostStrategy.class) See http://docs.jboss.org/hibernate/search/3.2/reference/en-US/html/search-mapping.html#section-boost-annotation
508,A,"Inserting values into Solr boolean fields I'm trying to insert a value into a boolean field in solr by passing it as a field in a doc thus: <add> <doc> <field name=""WouldBuySameModelAgain"">value-here</field> </doc> </add> The field definition in schema.xml is: <field name=""WouldBuySameModelAgain"" type=""boolean"" index=""false"" stored=""true"" required=""false"" /> I haven't been able to find any documentation on what value should be used where it says ""value-here"" in my example. I have tried true & false True & False TRUE & FALSE 1 & 0 all to no avail - there are still no documents in my index with a value in the boolean field. All of my non-boolean fields with stored=""true"" are getting values. All suggestions welcomed. The answer is ""true"" or ""false"" doesn't appear to be case sensitive. For example: <field name=""WouldBuySameModelAgain"">true</field> An error elsewhere in my app was putting an empty string in where I was expecting a value. You can accept your own answer and get 15 points. Thanks for sharing your find! Currently the reference guide says that ""Values of ""1"" ""t"" or ""T"" in the first character are interpreted as true. Any other values in the first character are interpreted as false."""
509,A,"Could not reserve enough space for object heap I'm writing a wrapper to Lucene. When a search request is made frequently it's possible ""Could not reserve enough space for object heap"" will be thrown. How can I get the size of the object heap? And how can I solve that? possible duplicate of [Could not reserve enough space for object heap](http://stackoverflow.com/questions/4401396/could-not-reserve-enough-space-for-object-heap) (Duplicate even has the same title ;) ) well the title is same but the situation is differen. User in the referenced duplicate get's the error each time. While here we have trouble if only we're launchind indexer frequently but the solutions may fit: fragmented memory prevents from creating a sufficient object heap. What's your OS? 32 bit windows systems have some unsuspected limitations regarding heap size... System is: uname -a Linux 2.6.18-14-fza-amd64 #1 SMP Mon Jan 5 17:36:46 UTC 2009 i686 i686 i386 GNU/Linux I tried to specify max size of heap(even minimum size) that didn't help. I believe that the underlying problem is the same as is described in the good answers to the SO question Could not reserve enough space for object heap. That is the JVM is attempting to ask the OS for memory for the heap and the OS is refusing because it has already allocated all virtual memory to other processes. I expect that this happens when you launch the indexer frequently because: it is increasing the system average load (number of processes running / waiting to run) and therefore it is increasing the system's average committed virtual memory resources and therefore it is making it more likely that the OS has to say ""No"" when a new JVM starts to run the indexer. Of course this is largely conjecture ... What can you do about it? Increase the size of the disc file or partition used for paging. Add more physical memory to the machine. Run the indexer less often and / or when the system is not busy. Trim the size of the indexer's heap. Move some of the other load off onto another machine. Some of these need to be done with care because they could impact on overall system performance in various ways. (Incidentally I don't think that switching to a 64 bit OS will necessarily help and running a 64 bit JVM certainly won't help ...) Well I executed ""free -m"". the metter of the fact is that there only 256Mb of memory on test server."
510,A,"solr multiple tokenizers for query I am rather new to SolR. I would like to use multiple tokenizers. I am using the standard tokenizer so that words get split via \t space comma etc. Now I would like to use an additional tokenizer. If there is the word ""cowshed"" I would like it to become ""cow"" and ""shed"". There are only I few words which are common to the search index which I would like to split. Therefore I planned using the regex tokenizer. However I get an error message when I try to (""multiple tokenizers at xml root""). Is it not possible? Do I need to change the code? Am I doing it wrong? Thanks for your hints :) You can only have one tokenizer per analyzer. If you need to modify the tokens generated by the tokenizer you can use token filters. Perfect thank you! I now simply send ""cowshed"" through the synonyme filter definining ""cowshed => cow shed"". Thanks!"
511,A,"Searching problem with Lucene I have a Lucene index of around 22000 lucene documents but I have been facing a unique problem with it while creating a search program. Each document has a Title description and long_description fields these fields have data related to different diseases and their symptoms. Now when I search for a phrase like following ""infection of the small intestine"" I am expecting ""Cholera"" to be the first result(By the way I am using MultiFieldQueryParser with StandardAnalyzer.) The reason I expect Cholera to be the first one is because it has exact phrase ""infection of the small intestine"" in the long description fields. But instead of this result coming on top it comes way at the bottom because there are plenty of other documents which mentions the term ""infection"" in the title field(which is substantially smaller in length than description field). This can be easily seen in the screenshot bellow. So just because ""cholera"" does not have the most pertinent information in the ""title"" field it comes way at the bottom. I saw following thread where the use of ""~3"" is suggested but is that what I should do for all my queries from behind the scene? Isn't there a better way of doing it? Searching phrases in Lucene Kind of solved it by setting the boost of title field to less then 0.5 You can change computeNorm in DefaultSimilarity. Please check http://www.supermind.org/blog/378/lucene-scoring-for-dummies and http://blog.architexa.com/2010/12/custom-lucene-scoring/  Make your query boost the hits in title high description medium and long_desc low like this: title:intestine^100 description:intestine^10 long_description:intestine^1 This example gives title matches score ""+100"" description matches score ""+10"" and long_description matches score ""+1"". Higher total boost scores are sorted first. You can pick any numbers you like for the boost values."
512,A,"Solr/Lucene: Possible to have Strings tokenized? I am indexing film titles. Currently I have two fields. One is a the pre-configured textgen and one is a string which I altered to be case insensitive. I use the copyfield directives to index the same data in both fields. I am using dismax request handler. I do this to be able to find ""lord rings"" in the title ""the lord of the rings"" but as well boost exact matches. So for example ""lord or the rings"" scores higher than ""rings of the lord"" but both is found. Now I played with string fields which seems to be necessary to have exact matching. But I just dont get any results. I only get results if I search for the exact string. Not even spaces are working... Is it even possible to have a String customized What field configuraition / analyzation would you recommend for this use case? Have a look at the example solr/conf/schema.xml there are a lot of different types of fields well documented. For your type of search you probably need a simple tokenised lower-cased field with positions such as: <fieldType name=""text"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.StandardFilterFactory""/> <filter class=""solr.ASCIIFoldingFilterFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> </analyzer> </fieldType> The ranking should be pretty good already with this. For standard text search one uses ""stopwords"" and ""stemming"" to improve the ranking (as in field below) but for searching titles i would probably not do it:  <fieldType name=""text"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.StandardFilterFactory""/> <filter class=""solr.ASCIIFoldingFilterFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords_spanish.txt"" enablePositionIncrements=""true"" /> <filter class=""solr.SnowballPorterFilterFactory"" language=""English""> </analyzer> <analyzer type=""query""> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.StandardFilterFactory""/> <filter class=""solr.ASCIIFoldingFilterFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords.txt"" enablePositionIncrements=""true"" /> <filter class=""solr.SnowballPorterFilterFactory"" language=""English""/\ > </analyzer> </fieldType>  ""String"" fields are not tokenized (so only exact matches will work); you can try switching its data type to ""text"" or else add a WhitespaceTokenizer to your chain. Also you shouldn't need to boost exact matches manually the scoring algorithm will do it for you."
513,A,lucene usage on trec data Does anyone know of any publicly available Question answering applications which have built using lucene on TREC data? Thanks Any one of the data sets from the Question Answering track... There are tons of different TREC data sets. Which one are you talking about? I assume this answers your question? http://search-lucene.com/m/A6gcf1wWZdv
514,A,Which one is the best java indexing program? I am looking for a keyword indexing library for java. I found Lucene in google search. I think it is a very popular one but just wondering if it is the best (in terms of speed performance) indexing library (of course it can be subjective but your opinion should be good enough for a beginner like me)? Is the example in this site http://snippets.dzone.com/posts/show/4020 good enough or you have a better recommendation? Thanks in advance. We have tested Lucene (but .Net version) against MSSQL's Full Text Search. It is rather difficult comparison since both system provides indexing in incomparable way but we do it for well defined task - index some product with multiple text fields (so fileds have different weight in search results) and provide user searching on these products. Lucene wins because we have full control over compounding query solve which indexes are in memory and which are stored on filesystem we have not been restrict by language pack (MSSQL FTS have limited list of supported languages). Lucene allows us use non-static noise word dictionary (for multiple product category we have used different set of noises). So it is hard to talk about pure performance but rich functional of Lucenr opens many ways for optimization.  Lucene is an awesome search tool but I would also urge you to take a look at Apache Solr a full-fledged search server built using Lucene over a RESTful/HTTP interface.  The content management software Alfresco has to ingest tons of documents as fast as possible so I guess the indexer they use is amongst the fastest they could find. Yes they use Lucene.  Databases like MySQL have an integrated Full Text Index (see: MySQL Index creation) you can use. This is quite fast but not as easy to configure as Lucene. I tried it one day and didn't get the results I intended (Especially since the included tokenizer can not be exchanged as easily as with Lucene). Another alternative would be to use a simple database table where you have one column with the index terms and another pointing to the postings (all documents containing the term) list. A collegue of me does it that way and says he evaluated performance against Lucene and the result was that the dabase is much faster. However as a conclusion I must say whenever I tried some different technology I was back at Lucene quite fast. The documentation is one of the best I ever read and the configuration as easy as it is extensive.
515,A,"ElasticSearch Sphinx Lucene Solr Xapian. Which fits for which usage? I'm currently looking at other search methods rather than having a huge SQL query. I saw elasticsearch recently and played with whoosh (a Python implementation of a search engine). Can you give reasons for your choice(s)? Sphinx vs Solr comparison: http://stackoverflow.com/questions/1284083/choosing-a-stand-alone-full-text-search-server-sphinx-or-solr Lucene vs Solr: http://stackoverflow.com/questions/1400892/search-engine-lucene-or-solr Whoosh v. Solr: http://stackoverflow.com/questions/3226596/full-text-search-whoosh-vs-solr I realy do not understand people that close such a CONSTRUCTIVE question. Such questions are realy important... Try indextank. As the case of elastic search it was conceived to be much easier to use than lucene/solr. It also includes very flexible scoring system that can be tweaked without reindexing. scoring can be tweek at runtime with solr too now there is no indextank anymore LinkdenIn open sources IndexTank https://github.com/linkedin/indextank-engine  We use Sphinx in a Vertical Search project with 10.000.000 + of MySql records and 10+ different database . It has got very excellent support for MySQL and high performance on indexing  research is fast but maybe a little less than Lucene. However it's the right choice if you need quickly indexing every day and use a MySQL db.  Lucene is nice and all but their stop word set is awful. I had to manually add a ton of stop words to StopAnalyzer.ENGLISH_STOP_WORDS_SET just to get it anywhere near usable. I haven't used Sphinx but I know people swear by its speed and near-magical ""ease of setup to awesomeness"" ratio.  We use Lucene regularly to index and search tens of millions of documents. Searches are quick enough and we use incremental updates that do not take a long time. It did take us some time to get here. The strong points of Lucene are its scalability a large range of features and an active community of developers. Using bare Lucene requires programming in Java. If you are starting afresh the tool for you in the Lucene family is Solr which is much easier to set up than bare Lucene and has almost all of Lucene's power. It can import database documents easily. Solr are written in Java so any modification of Solr requires Java knowledge but you can do a lot just by tweaking configuration files. I have also heard good things about Sphinx especially in conjunction with a MySQL database. Have not used it though. IMO you should choose according to: The required functionality - e.g. do you need a French stemmer? Lucene and Solr have one I do not know about the others. Proficiency in the implementation language - Do not touch Java Lucene if you do not know Java. You may need C++ to do stuff with Sphinx. Lucene has also been ported into other languages. This is mostly important if you want to extend the search engine. Ease of experimentation - I believe Solr is best in this aspect. Interfacing with other software - Sphinx has a good interface with MySQL. Solr supports ruby XML and JSON interfaces as a RESTful server. Lucene only gives you programmatic access through Java. Compass and Hibernate Search are wrappers of Lucene that integrate it into larger frameworks. you raised an important notion that a search-engine must be adaptable. What about Xapian? I have never used Xapian. It looks like a fine search library whose features are on a par with Lucene's. Again things that matter most are your application needs the environment in which you want the search engine to run your proficiency in the implementation language (C++ in Xapian search with bindings to many other languages) and how customizable is the engine.  I would recommend DBSight. You just use the free version during your development cycle. It's built-in SQL crawler would save you lots of time to configure crawling. And many other features like generating search results via scaffolding etc. http://www.dbsight.net  I have used both Sphinx Solr and Elasticsearch. Solr/elasticsearch are built on top of Lucene. It adds many common functionality: web server api faceting caching etc. If you want to just have a simple full text search setup sphinx is a better choice. If you want to customize your search at all elasticsearch and solr are the better choices. They very extensible: you can write your own plugins to adjust result scoring. Some example usages: Sphinx: craigslist.org Solr: Cnet Netflix digg.com Elasticsearch: Foursquare Github  An experiment to compare ElasticSearch and Solr  I've found this interesting comparison: http://blog.socialcast.com/realtime-search-solr-vs-elasticsearch/  The only elasticsearch vs solr performance comparison I've been able to find so far is here: Solr vs elasticsearch Deathmatch! that's a bad one. he does not present comments! see this discussion: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/19e634e3d7e490c6/16598f6c4b2d116b -1 because ""My Your comment is awaiting moderation."" and others too see the google groups link above -1  more than a year later no comments are allowed at that thread I would seriously consider ignoring it completely.  As the creator of ElasticSearch maybe I can give you some reasoning on why I went ahead and created it in the first place :). Using pure Lucene is challenging. There are many things that you need to take care for if you want it to really perform well and also its a library so no distributed support its just an embedded Java library that you need to maintain. In terms of Lucene usability way back when (almost 6 years now) I created Compass. Its aim was to simplify using Lucene and make everyday Lucene simpler. What I came across time and time again is the requirement to be able to have Compass distributed. I started to work on it from within Compass by integrating with data grid solutions like GigaSpaces Coherence and Terracotta but its not enough. At its core a distributed Lucene solution needs to be sharded. Also with the advancement of HTTP and JSON as ubiquitous APIs it means that a solution that many different systems with different languages can easily be used. This is why I went ahead and created ElasticSearch. It has a very advanced distributed model speaks JSON natively and exposes many advanced search features all seamlessly expressed through JSON DSL. Solr is also a solution for exposing an indexing/search server over HTTP but I would argue that ElasticSearch provides a much superior distributed model and ease of use (though currently lacking on some of the search features but not for long and in any case the plan is to get all Compass features into ElasticSearch). Of course I am biased since I created ElasticSearch so you might need to check for yourself. As for Sphinx I have not used it so I can't comment. What I can refer you is to this thread at Sphinx forum which I think proves the superior distributed model of ElasticSearch. Of course ElasticSearch has many more features then just being distributed. It is actually built with cloud in mind. You can check the feature list on the site. ""You know for search"". +1 for Hudsucker Proxy. Also I'm intrigued by the software ;) Also the video was really well done. You should add some more of those! ""As the creator of ElasticSearch ..."" enough for +1. Nice I found that I can use elasticsearch free with heroku opposed to using something like solr which costs money...  My sphinx.conf source post_source_1 { ### Why '_1''_2''_3' stuff ? Because sphinx has limit [4GB] per index file \ ### so you must split index sources... type = mysql sql_host = localhost sql_user = xxx sql_pass = xxx sql_db = xxx sql_port = 3306 sql_query_pre = SET NAMES utf8 ### Query before fething for indexing... sql_query = SELECT * id AS pid CRC32(slug) as slug_crc32 FROM hb_posts ### Custom fields can be set (slug_crc32) sql_attr_uint = pid # my post ID # Why sphinx ? sql_field_string = title sql_field_string = slug sql_field_string = content sql_field_string = tags # I can store string fields into RAM what i want sql_attr_uint = category # my post category ID sql_attr_timestamp = date # my post int date sql_attr_uint = views # my post int date sql_query_info_pre = SET NAMES utf8 ### 'sql_query_info_pre' You must use .patch(s) (requires source [re]build) \ ### OR source edit(C++) for UTF support for string fields 'sql_field_string' sql_query_info = SELECT * FROM my_posts WHERE id=$id # lets index... } index post_1 { source = post_source_1 # My valid and declared source name path = /var/data/post # Where to locate index file(s) (Are you have SSD? :) charset_type = utf-8 # Charset must be same for all... } Test script: <?php $slug = $_GET[""my_post_slug""]; # Or explode REQUEST URI etc... // $slug = preg_replace(""/[a-z0-9\-_]/i""""""$slug); # There is no SPX INJECTION yet :D $conf = getMyConf(); ### new sphinx instance require ""sphinxapi.php""; $cl = New SphinxClient (); $cl->SetServer($conf[""server""] $conf[""port""]); $cl->SetConnectTimeout($conf[""timeout""]); $cl->setMaxQueryTime($conf[""max""]); ### new researching setup $cl->SetMatchMode(SPH_MATCH_FULLSCAN); # I am not using sphinx as a only 'searching daemon' because i can use it as a 'database platform' directly... $cl->SetArrayResult(TRUE); $cl->setLimits(011); # I am looking for only post not searching a keyword... $cl->SetFilter(""slug_crc32"" array(crc32($slug))); # int > faster and safer than strings... $post = $cl->Query(null ""post_1""); # Quering NULL: Give me result(s) by filtering all posts(SPH_MATCH_FULLSCAN) by my filters(slug_crc32limits etc...) echo ""<pre>""; var_dump($post); echo ""</pre>""; exit(""1""); ?> Result: [array] => ""id"" => 123 ""title"" => xxx ""content"" => ""yyy ÇÇ şşş İİÇ ĞŞÇŞİĞ <br> <p> asdasdasd </p> 123 .!?* "" ... and all of the defined attr. listing here... (including query time and total match count[limits not necessary] ) Query time: 0.001 sec. MySQL query and time (one query at the same time): ""SELECT * FROM my_posts WHERE id = 123123;"" => 0.032 sec. Mysql query and time (Under stress and 10000 query at the same time via 10 worker): ""SELECT * FROM my_posts WHERE id = 123123;"" => 2.117 sec. (average) => 3.021 sec. (average of last 10 query) Sphinx query(above) and time (One query at the same time): 0.001 sec. Sphinx query(above) and time (Under stress and 10000 query at the same time via 10 worker): => 0.346 sec. (average) => 0.260 sec. (average of last 10 query) So now I have 8gb ram and intel x e3x4 processor +3m posts +12m tags. MySQL clustering? No I am happy with Sphinx (stability first speed second simplicity third)... No more mysql_connect :D Did you tried sphinx or elasticsearch ? @dzen this IS sphinx; he's using mysql query as a comparison of query execution speeds."
516,A,Indexing PDF files with Symfony using Lucene I am a Symfony developer and my web server is Linux. I already use the sfLucene plugin. What is the simplest way of indexing PDF files for search on a Linux PHP server? XPDF installed like this Apache Tika via the SOLR sfLucene plugin branch A 3rd option? Thanks! Coming from a Zend background i generally recommend using Zend_Search_Lucene. The XPDF example is really straight forward and looks simple. XPDF is licenced as GPL - if that fits your need go for #1! ZF can easily be integrated within your Symfony projects e.g. for a Twitter Call.  There are many libraries for extracting text content from PDF. With any of these you then need to create a lucene document with the content. The most useful ones will be those that already have lucene integration. Apache PDFBox can create a lucene document directly from PDF file. It will include PDF metadata fields as well as text content.
517,A,"Solr queryparser for lucene indices? I've created an index (using Lucene 2.9) which stores text messages. (the documents also contain some other meta data which are not indexed just stored) I use the StandardAnalyzer for parsing these messages. I'm trying to run some tests on this index using Solr (I replaced the example app index with my index) to see what kind of results I get from various queries. When I tried the following query  I got 0 results ""text:happiness"" However changing that to ""text:happiness*"" gives me some results. All of them contain terms like ""happiness"" ""happiness."" etc. So I thought that it was a tokenization issue during index creation however when I used Luke (a lucene index debugging tool) to run the same query (text:happiness) I got the exact same results that I get for happiness* from Solr which led me to believe that the problem is not while indexing but in the way that I'm specifying my Solr query. I looked at the solrconfig.xml and noticed that it has the following line (commented) I tried uncommenting it and then modified my query to use ""defType=lucene"" in addition to the original query but got the same results.  <queryParser name=""lucene"" class=""org.apache.solr.search.LuceneQParserPlugin""/> I have very little experience with Solr so any help is greatly appreciated :) I was able to solve this by changing my solrconfig.xml you can post your solution as an answer and accept it. I posted my solution I'll have to wait 2 days to accept it. The field that I was querying on was defined as type ""text"" in the solr schema.xml (not solrconfig.xml as I incorrectly mentioned in my earlier comment). Here's a relevant snippet from the schema.xml <fieldType name=""text"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <!-- in this example we will only use synonyms at query time <filter class=""solr.SynonymFilterFactory"" synonyms=""index_synonyms.txt"" ignoreCase=""true"" expand=""false""/> --> <!-- Case insensitive stop word removal. add enablePositionIncrements=true in both the index and query analyzers to leave a 'gap' for more accurate phrase queries. --> I replaced it with the following <fieldType name = ""text"" class=""solr.TextField""> <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> </fieldType> Which gives me the required behavior."
518,A,"where do i have to look in order to understand location based search concepts? Inorder to understand - cartessian tiershow are they contributing in location based search - What is happening internally when we give query to solr like http://localhost:8983/solr/select/?q=name:Minneapolis AND _val_:""recip(hsin(0.78 -1.6 lat_rad lon_rad 3963.205) 1 1 0)""^100 and other functions like ghhsin()sqedist()dist() .how is it working to retrieve relevent records? Can any one suggest me any link that will help me understand all these concepts better. Thank you. Please accept an answer for your previous questions. For the big picture here's Grant Ingersoll paper on location-aware search in Lucene and Solr. For the details try the Solr Wiki FunctionQuery page or Ingersoll's Fun with Solr Functions."
519,A,"Lucene strange behaviour I'm trying to start using lucene. The code I'm using to index documents is: public void index(String type String words) { IndexWriter indexWriter = null; try { if (dir == null) dir = createAndPropagate(); indexWriter = new IndexWriter(dir new StandardAnalyzer() true new KeepOnlyLastCommitDeletionPolicy() IndexWriter.MaxFieldLength.UNLIMITED); Field wordsField = new Field(FIELD_WORDS words Field.Store.YES Field.Index.ANALYZED); Field typeField = new Field(FIELD_TYPE type Field.Store.YES Field.Index.ANALYZED); Document doc = new Document(); doc.add(wordsField); doc.add(typeField); indexWriter.addDocument(doc); indexWriter.commit(); } catch (IOException e) { logger.error(""Problems while adding entry to index."" e); } finally { try { if (indexWriter != null) indexWriter.close(); } catch (IOException e) { logger.error(""Unable to close index writer."" e); } } } The search looks like this: public List<TagSearchEntity> searchFor(final String type String words int amount) { List<TagSearchEntity> result = new ArrayList<TagSearchEntity>(); try { if (dir == null) dir = createAndPropagate(); for (final Document doc : searchFor(dir type words amount)) { @SuppressWarnings(""serial"") TagSearchEntity searchResult = new TagSearchEntity() {{ setType(type); setWords(doc.getField(FIELD_WORDS).stringValue()); }}; result.add(searchResult); } } catch (IOException e) { logger.error(""Problems while searching"" e); } return result; } private List<Document> searchFor(Directory indexDirectory String type String words int amount) throws IOException { Searcher indexSearcher = new IndexSearcher(indexDirectory); final Query tagQuery = new TermQuery(new Term(FIELD_WORDS words)); final Query typeQuery = new TermQuery(new Term(FIELD_TYPE type)); @SuppressWarnings(""serial"") BooleanQuery query = new BooleanQuery() {{ add(tagQuery BooleanClause.Occur.SHOULD); add(typeQuery BooleanClause.Occur.MUST); }}; List<Document> result = new ArrayList<Document>(); for (ScoreDoc scoreDoc : indexSearcher.search(query amount).scoreDocs) { result.add(indexSearcher.doc(scoreDoc.doc)); } indexSearcher.close(); return result; } I've got two use cases. The first one adds document of some type then searches for it then adds document of another type then searches for it etc. The other one adds all documents then searches for them. The first one works fine: @Test public void testSearch() { search.index(""type1"" ""test type1 for test purposes test test""); List<TagSearchEntity> result = search.searchFor(""type1"" ""test"" 10); assertNotNull(""Retrieved list should not be null."" result); assertTrue(""Retrieved list should not be empty."" !result.isEmpty()); search.index(""type2"" ""test type2 for test purposes test test""); result.clear(); result = search.searchFor(""type2"" ""test"" 10); assertTrue(""Retrieved list should not be empty."" !result.isEmpty()); search.index(""type3"" ""test type3 for test purposes test test""); result.clear(); result = search.searchFor(""type3"" ""test"" 10); assertTrue(""Retrieved list should not be empty."" !result.isEmpty()); } But the other one seems to be only indexing the last document: @Test public void testBuggy() { search.index(""type1"" ""test type1 for test purposes test test""); search.index(""type2"" ""test type2 for test purposes test test""); search.index(""type3"" ""test type3 for test purposes test test""); List<TagSearchEntity> result = search.searchFor(""type3"" ""test"" 10); assertNotNull(""Retrieved list should not be null."" result); assertTrue(""Retrieved list should not be empty."" !result.isEmpty()); result.clear(); result = search.searchFor(""type2"" ""test"" 10); assertTrue(""Retrieved list should not be empty."" !result.isEmpty()); result.clear(); result = search.searchFor(""type1"" ""test"" 10); assertTrue(""Retrieved list should not be empty."" !result.isEmpty()); } It successfully finds type3 but fails to find all the others. If I shullfle those calls around it will still successfully find only the last indexed document. Lucene version I'm using is:  <dependency> <groupId>org.apache.lucene</groupId> <artifactId>lucene-core</artifactId> <version>2.4.1</version> </dependency> <dependency> <groupId>lucene</groupId> <artifactId>lucene</artifactId> <version>1.4.3</version> </dependency> What am I doing wrong? How to make it index all documents? A new index is getting created after every index operation. The third argument is the create flag and it is being set to true. As per the documentation of IndexWriter if this flag is set it will either create a new index or overwrite the existing one. Set it to false to append to the existing index. Thanks a lot this completely solved my problem."
520,A,Question Answering with Lucene For a toy project I want to implement an automated question answering system with Lucene and I'm trying to figure out a reasonable way to implement it. The basic operation is as follows: 1) The user will enter a question. 2) The system will identify the keywords in the question. 3) The keywords will be searched in a large knowledgebase and matching sentences will be shown as answers. My knowledgebase (i.e. corpus) is not structured. It is just a large continuous text (say a user manual without any chapters). I mean that the only structure is that sentences and paragraphs are identified. I plan to treat each sentence or paragraph as a separate document. To present the answer in a context I may consider keeping one sentence/paragraph before/after the indexed one as payload. I would like to know if that makes sense. Also I'm wondering if there are other tried and well-known approaches for that kind of systems. As an example another approach that comes to mind is to index large chunks of the corpus as documents with the token positions then process the vicinity of found keywords to construct my answers. I would appreciate direct recommendations based on experience or intuition but also tutorials or introductory materials to question-answering systems with Lucene in mind. Thanks. Maybe I should also add that memory is a concern. I wouldn't like to keep all my knowledgebase in memory. Probably that rules out approaches using the highlighter. Lucene is very fast and efficient when it comes to handling large document sets. By default the index is on disk although you can map to memory. Indexing each sentence as a document will give you some problems. You've pointed out one: you would need to store the surrounding texts a payloads. That means you'll need to store each sentence three times (before during and after) and you'll have to manually get into the payload. If you want to go the route of each sentence being a document I would recommend coming up with an ID for each sentence and storing that as a separate field. Then you can display [ID-1 ID ID+1] in each result. The bigger question though is: how should you break up the text into documents? Identifying semantically related areas seems difficult so doing it by sentence/paragraph might be the only way to go. A better way would be if you could find which text is the header of a section and then put everything in that section as a document. You might also want to use the index (if your corpus has one). The terms there could be boosted as they are presumably more important.  It's not an unreasonable approach to take. One enhancement you might consider is incorporating learning feedback so that you can continually improve the scoring of content vs search terms. To do this you would ask users to rate the answers that come back ('helpful vs unhelpful') that way you can start to rank documents against keywords based on the historical data. You could classify potential documents as helpful/unhelpful for given keywords by using a simple Bayesian classifier.  Instead of luncene which does text indexing search and retrieval I think using something like Apache Mahout would help with this. Mahout considers text as knowledge and doing that makes the answering the question better than just text matching. Mahout is a machine learning and data mining f/w which fits this domain better. Just a very high level thought. --Sai which Mahout learning algorithm would you recommend for this problem? For me this more or less looks like a classification problem. What's being classified? Thanks. That might be a promising approach; but my current problem is not to improve the performance of question answerer. I just want to see the best way to deal with such a problem within the Lucene context as a challenge. I mean I'm more interested in the technical problem of storing lots of unstructured data on disk/ in memory and randomly access it according to several keywords. @Amaç - Lucene will happily handle very large document sets. Unless your considering 100s of millions of docs or have very limited hardware you'll probably not have to worry much about scaling.
521,A,Zend Lucene MoreLikeThis I'm using Zend_Search_Lucene for my search engine. Sadly it is missing an implementation of the MorelikeThis methods which can find similar documents in the index. Has anybody come across a decent Zend port of this function? I found a drupal module but have no idea if this can be used with Zend without some serious hacking. The ZF guys did a great job with Zend_Search_Lucene but it just isn't as robust as Sphinx or Solr. I would highly recommend using either one of those as they would give you the MoreLikeThis functionality that you are looking for in addition to so many more features. If you Google 'solr vs sphinx' you'll find several articles comparing the two. Both search engines have PHP interfaces so that makes integration easy. http://us2.php.net/manual/en/refs.search.php +1 for an actual answer at long last. :) I did look at solr and came to the conclusion it would be the best way forward. I left this project in January so maybe the current team will stumble upon this question and come to the same conclusion :) Unfortunately not. Both require more than just PHP. Given one uses a shared hosting env where pure php is the only options to go - would Solr or Sphinx be usable there?  Solr is accesible from a rest API which allows you to use it with any language. It's also owned by the people who own Lucene so you are more likely to get updates and/or patches such as this in a timely manner. For example we have a patch for field collapsing that works in 1.4 that functionality is not slated until version 4.0 3.0 was just released in March if that gives you any idea about how easy it is to get access to new features in Lucene.
522,A,"Getting the number of Hits in a document(doc) in lucene How can I get the number of Hits per document in Lucene in Java. I have  IndexReader reader; reader = IndexReader.open(FSDirectory.open(new File(index)) true); Searcher searcher = new IndexSearcher(reader); String feild = ""contents"" QueryParser parser = new QueryParser(Version.LUCENE_CURRENT fieldanalyzer); Query query = parser.parse(""test""); TopScoreDocCollector collector = TopScoreDocCollector.create( 5 * hitsPerPage false); searcher.search(query collector); ScoreDoc[] hits = collector.topDocs().scoreDocs; Searcher searcher = new IndexSearcher(reader); int numTotalHits = collector.getTotalHits(); System.out.println(numTotalHits + "" total matching documents"");  for (int i = start; i < end; i++) { int id = hits[i].doc; TermFreqVector[] Tfv = reader.getTermFreqVectors(id); The tfv is getting to be null :( Can some one direct on how to get the hits in each document from there. EDIT: If we set the TermVector.YES while indexing it works. Thanks Sharma This is a duplicate of Get search word Hits ( number of occurences) per document in Lucene As that answer says you can use the term freq vector. jarekrozanski's answer is faster but you will need to make a custom similarity class which you might dislike doing. Well the link suggests us to use term freq vector for the feild which no more exists in 3.0 release for lucene. We can get it from the reader Object though while it needs docNumber. Can you let me know what the document number is? @sharma: ""docNumber"" is just the ID of the doc i.e. `reader.doc()` and `searcher.doc()` do the same thing. So using your code the doc id can be found as `hits[i].doc`. @Xodarap: When I use IndexReader Object to get TermFreqVector it returns null for some reason. In 3.0 release is there any other object apart from IndexReader to get the TermFreqVector  that you know of? @sharma: Everything will be based off the reader. Are you sure you're passing in the correct field name? I have updated the code here of what I have can you make if suggestions please let me know. I am sure that the feild name is right @Xodarap I got the number of hits for a single word like ""Hello"" using the TermDocs can you suggest me a way if I can get two word seach like ""Hello there"". @sharma: could you try explicitly passing the field name in? e.g. `reader.getTermFreqVector(id field)` @Xodarap I have tried that. It still shows null. But works with reader.termDocs(). The problem with it maches only single word like 'Hello' and not 'Hello world'. @sharma: yes a term is a term. If you want to find the frequency of multiple terms it is much harder. You can check out what the [highlighter does](http://www.docjar.org/html/api/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java.html).  You can write custom Similarity implementation. You will gain access to term frequency which will give you number of times given terms occurs in given document. can you direct me to an example? Just extends Similarity class. Implement tf(float frequency) method that stores frequency. Do not forget to attach you similarity to index searcher http://lucene.apache.org/java/3_0_3/api/all/org/apache/lucene/search/Searcher.html#setSimilarity%28org.apache.lucene.search.Similarity%29"
523,A,"Efficient Filtering / Searching We have a hosted application that manages pages of content. Each page can have a number of customized fields and some standard fields (timestamp user name user email etc). With potentially hundreds of different sites using the system -- what is an efficient way to handle filtering/searching? Picture a grid view that you want to narrow down. You can filter on specific fields (userid date) or you can enter a full-text search. For example ""all pages started by userid 10"" would be a pretty quick query against a MySQL database. But things like ""all pages started by a user whose userid is 10 and matches [some search query]"" would suck against the database so it's suited for a search engine like Lucene. Basically I'm wondering how other large sites do this sort of thing. Do they utilize a search engine 100% for all types of filtering? Do they mix database queries with a search engine? If we use only a search engine there's a problem with the delay time it takes for a new/updated object to appear in the search index. That is I've read that it's not smart to update the index immediately and to do it in batches instead. Even if this means every 5 minutes users will get confused when their recently added page isn't immediately listed when they view a simple page listing (say a search query of ""category:5""). We are using MySQL and have been looking closely at Lucene for searching. Is there some other technology I don't know about? My thought is to offer a simple filtering page which uses MySQL to filter on basic fields. Then offer a separate fulltext search page that would present results similar to Google. Is this the only way? Don't write-off MySQL so readily! Implement it using the database e.g. a select with a 'like' in the where-clause or whatever. Profile it add indexes if necessary. Roll out a beta so you get real numbers from user's actual data patterns - not all columns might be equally asked after etc. If the performance does suck then thats when you consider other options. You can consider tuning your SQL your database the machine the database is running on and finally using another technology stack...  Solr or grassyknoll both provide slightly more abstract interfaces to Lucene. That said: Yes. If you are a primarily content driven site providing fulltext searching over your data there is something in play beyond LIKE. While MySql's FULLTEXT indexies aren't perfect it might be an acceptable placeholder in the interim. Assuming you do create a Lucene index linking Lucene Documents to your relational objects is pretty straightforward simply add a stored property to the document at index time (this property can be a url ID GUID etc.) Then searching becomes a 2 phase system: 1) Issue query to Lucene indexies (Display simple results like title) 2) Get more detailed information about the object from your relational stores by its key Since instantiation of Documents is relatively expensive in Lucene you only want to store fields searched in the Lucene index as opposed to complete clones of your relational objects.  In case you want to use MySQL or PostgreSQL a open source solution that works great with it is Sphinx: http://www.sphinxsearch.com/ We are having the same problem and considering Sphinx and Lucene as possible solutions."
524,A,"Retrieving specific fields in a Solr query? I am running a Solr instance on Jetty and when I search using the Solr admin panel it returns the entire document. What should I do to get only specified fields from each Solr document returned by the search? From the Solr Admin home page click on ""Full Interface"". On that page there is a box called ""Fields to Return"". You can list the you want here (comma-separated). ""*"" means all fields.  /?q=query&fl=field1field2field3 http://wiki.apache.org/solr/CommonQueryParameters Andrew Any idea how to retrieve fields using solr autosuggest like this /suggest?spellcheck.q=india&fl=count:totaltermfreq(title 'untitled')title."
525,A,Reusing Lucene Query objects Are Lucene's Query objects reusable/stateless ? If not will clone()ing a TermQuery be faster than rebuilding it ? yes: The purpose of Weight is to ensure searching does not modify a Query so that a Query instance can be reused.
526,A,"What is the Use of Lucene? Hey Friend i have heard lot of time the name Lucene  while i try to fetch details of web crawler it show up most of time.whats the use of Lucene? Lucene is a search engine library designed to address the problem of performing keyword search over a large number of documents. The system works by processing the documents to extract all of the words and then creating a reverse index. This index allows the search engine to quickly identify the documents containing the user's search term or terms rank them and then return them to the user. Lucene supports a variety of advanced features such as phrase queries wildcard queries and proximity queries (i.e. ""cat"" near ""dog"") search for keywords within particular ""fields"" (e.g. subject author) and so on. Basically it is one of the ways to add text search capability to document management applications of various kinds. This is a nice description. However Lucene is not a search engine. It is a search library. You always have to embed Lucene inside a program (in Java or one of the languages Lucene was ported to) in order to use it. Solr is a search engine based on Lucene. The Lucene site describes Lucene as a ""search engine library"" ... so I think you may be splitting hairs. I don't think this is splitting hairs. IMO Saying Lucene is a search engine implies that you can use it out-of-the-box for searching. I do not think this is possible. It doesn't imply that to me at all. I mean you cannot roll an auto engine out of the shed and drive it down the road. You put into a car and connect things up ... and eventually you can take the car for a drive.  Lucene is a search engine. You would use lucene in a project if you wanted a fast indexed search. More details can be found on http://lucene.apache.org/java/docs/index.html"
527,A,"Lucene (Java) - How to specify default search field programatically? I have the following code and would appreciate your advice.  QueryParser queryParser = new QueryParser(searchTerm analyzer); Query query = queryParser.parse(searchTerm); My first question is this ""doubled""? As I have the ""String to search for (=searchTerm)"" in the constructor as well as in the parse() method. Is this really required? (For further usage i need a Query object). If i do it this way does this maybe even introduce some negative side effects? And I am not able to specify programatically the ""default field"" to search for. In my quries I write ""content:House"" and it searches in the field ""content"". But how can I specify this programatically that ""content:"" is my default field and a user only has to enter ""House"" (and lucene then automatically searches in the ""content"" field). thank you so much jan as long as i know there is no such option The first argument to the QueryParser constructor is the default search field even if the javadoc doesn't make that obvious. So you want this: QueryParser queryParser = new QueryParser(""content"" analyzer); Query query = queryParser.parse(searchTerm); Hello skaffman. Thanks so much for your help!!! jens"
528,A,Novice Needs Help - Lucene Im doing a uni project that involves designing a website. I am in the early stages of development and just want to clarify my understanding of what I have learnt so far. My website will contain a database (SQL or Oracle possibly) with powerpoint word and maybe some other files but mainly these. I can use Lucene to: 1) Search my site using keywords to find and retrieve the relevant pages 2) Search and extract the relevant powerpoint and word files from the database From what I gathered core Lucene will index my site and database and find the relevant pages and files. I will need to add other software though such as crawlers and a user interface. As I say I am new to this and it is all quite complicated at the start but is what I have gathered so far correct? Many Thanks Phil 1) You will need to create a lucene Index. 2) One way of doing is to use nutch to crawl your site and it will return you a lucene index which will have all the links/data from your website indexed. 3) I believe you can create a lucene index for a database too.
529,A,SolrNet/Solr - Update vs. Overwrite Document I am using SolrNet to intreract with a Solr index. I have a daemon application writing to the Solr index with adds/updates/deletes. However with SolrNet an Add with the same unique-key over-writes (replaces) the existing document instead of appending (combining) them. In Lucene I could do something like this where term is the Lucene term for the document key. How can I do this in SolrNet? I know of the (painful) way of appending field-by-field in a method but surely there has to be a simpler way... //where term is a Lucene term for the document key if (objFacetsSearcher.DocFreq(term) > 0) { objWriter.UpdateDocument(term doc); updated++; } else { objWriter.AddDocument(doc); added++; } possible duplicate of [Update specific field on SOLR index](http://stackoverflow.com/questions/2032813/update-specific-field-on-solr-index) @Mauricio - don't think that is related to my question. it's exactly the same. In Solr you can't update individual fields. As far as I know this isn't supported in Solr yet. See SOLR-139. thanks much. What do you think of writing directly to the index using Lucene? Any potential issues to be aware of? Essentially the only way is to read the document back append stuff and write back? That seems a of I/O - injurious to health. What approaches do you take to get around this? I was thinking of using Lucene to write to the Solr index directly. Any drawbacks to be aware of? @mikOS depending on environmental constraints the most obvious way would be to read the entire document from the source system again. If that is not a viable option then consider either implementing caching functionality in your connector (or as part of your document processing) or make input fields stored in your index and read the document from Solr to populate any fields outside the delta.
530,A,"Nutch Newbie - JSP with html problem System: Mac OSX I have set up nutch so that it crawls and indexes my site. It also returns search results. My problem is that I want to customise the Nutch index.jsp and search.jsp pages to fit with my site. Ive read up and on jsp and it says its just a matter of putting in the html tags and then using <% %> to enclose the Java scriplets you want. For some reason nothing changes when i edit the files (index and search) Here is what the original file displays: <%@ page session=""false"" import=""java.io.*"" import=""java.util.*"" %><% String language = ResourceBundle.getBundle(""org.nutch.jsp.search"" request.getLocale()) .getLocale().getLanguage(); String requestURI = HttpUtils.getRequestURL(request).toString(); String base = requestURI.substring(0 requestURI.lastIndexOf('/')); response.sendRedirect(language + ""/""); %> Here is my edited version with sum gibberish test added to test it: <html> <head> </head> <body> gigigyigig <%@ page session=""false"" import=""java.io.*"" import=""java.util.*"" %><% String language = ResourceBundle.getBundle(""org.nutch.jsp.search"" request.getLocale()) .getLocale().getLanguage(); String requestURI = HttpUtils.getRequestURL(request).toString(); String base = requestURI.substring(0 requestURI.lastIndexOf('/')); response.sendRedirect(language + ""/""); %> ghjgjkbkhb hjgjvjhvj </body> </html> Nothing has changed tho and the nutch homepage/index.jsp still displays the same as original. This is my first encounter with JSP so its just what ive picked up so far. Can anyone tell me why the page isnt displaying the html with gibberish typed?? I have my search totaly modified. However I have my <html>... tags after the second scriptlet ie <% %> not <%@ page. As for your index.jsp modified it has a redirection response.sendRedirect and therefore it looks normal to me that you see nothing. Also I presume you took care of loading the jsp pages at the right place under the tomcat/webapps tree because the standard ant make file doesn't. So I ended up adding some Ant task to patch my test website. Beware if you are going to change the .jar files you also need to restart Tomcat. response.sendRedirect does pretty much the same thing as location.replace in javaScript however on the server side. It should put you on page `localhost:8080/nutch/en/` if u use english which actually is nutch/en/search.html. Search.jsp gets only called after submitting a first query. I put all the tags after the 2nd scriptlet as instructed. The files are in the tomcat/webapps/nutch directory but the page is loaded from localhost:8080/nutch so i think everything is okay there. There is still no change in the page tho gibberish not showing yet. I am not sure what the response.sendRedirect does which u spoke of so i might have to look into that some more. But still no change in the page."
531,A,"CF9's Apache Lucene vs SQL Server's full text search? ColdFusion 9's full text search is now based on Apache Lucene Solr (or Verity but it has too much limitations). We also use SQL Server. Which one's better? Which one's easier? UPDATE: going to use for... searching against the name & description fields of the Products table. Thanks! The important question: What are you going to use it for? Can't pick the right tool for the job when you don't know what the job is ;) going to use for... searching against the name & description fields of the Products table.  Here's my 2 cents tested with ~ 3 000 000 of images with captions (primary key + image caption text from 100 to 500 chars): CF9's Solr implementation is fast in returning results really easy to setup fairly fast during building index. SQL Server 2005 FTS wasn't good enough tried it some time ago and didn't put it in production. SQL Server 2008 FTS is much better though currently using it on our application. But basic setup had to be adjusted in order to get high level results. Based on experiences of other colleagues working with huge data sets and applications mostly based on search and finding things I made my top list: Lucene Tuned SQL Server 2008 FTS Solr SQL Server 2005 Of course CF9's Solr is winner here if you are chasing fast setup since you need 3 tags to finish the job and get awesome results. btw what's the diff between Lucene and Solr? thx. Performance wise Lucene should be faster considering that you don't need all features of Solr and you build search indexing text filtering and parsing exactly how you need it. Another plus IMHO for Lucene is that you have one thing less running separate form your main application in servlet container. As I said we use SQL server FTS but for preparing texts parsing filtering etc. we are using Lucene library with javaLoader.cfc. @Henry - Lucene is a search library originally written in Java but ported to several other languages and environments. It is low-level takes a while to learn and has lots of search functionality. Solr is a search server built on top of Lucene and runs as a webapp in a servlet container. It is much easier to set up and use. I am not certain that Solr's performance is slower than bare Lucene as the Lucene community describes Solr as ""Lucene Best Practices"" which means that many parameter settings for Lucene are chosen in Solr to be optimal. Both are useful FTS products."
532,A,locking a lucene folder I am writing a wrapper around Zend's lucene implementation and wanted to add a function rebuildIndex() which reads all relevant fields from the database and re-creates the index file in a temporary folder. When the operation is finished I want to replace the original folder with the new one. How can I lock the original lucene folder while replacing its contents? I haven't found anything in Zend's API docs but I had read somewhere that locking works with files in lucene. Which folders/files do I need? Lucene use locking internally to maintain index consistency so you can not use it in your code. I'd suggest using the following strategy: Create directory 'indexes' which contains directories for 2 different versions of index e.g. 'index1' and 'index2' and a symlink 'current' to the index that should be used for searches. When updating index you drop files in inactive index directory re-create the index and when it's done set 'current' to the newly indexed directory Wait 1 minute for search queries to the old index files to complete and drop files from the old directory. What is the reason I cannot use it in code? Does the locking mechanism change every other version? Or is there another reason? The locking mechanism is used internally by lucene. You can use it but it would require very good understanding of lucene internals. It's not so trivial and needs very thorough testing. In your case more reliable solution is to just change the folder you pass to Zend_Search_Lucene::open function. Anyway index updates require reopening index reader.
533,A,"Exact search with Lucene.Net I already have seen few similar questions but I still don't have an answer. I think I have a simple problem. In sentence In this text only Meta Files are important and Test Generation. Anything else is irrelevant I want to index only Meta Files and Test Generation. That means that I need exact match. Could someone please explain me how to achieve this? And here is the code: Analyzer analyzer = new StandardAnalyzer(); Lucene.Net.Store.Directory directory = new RAMDirectory(); indexWriter iwriter = new IndexWriter(directory analyzer true); iwriter.SetMaxFieldLength(10000); Document doc = new Document(); doc.Add(new Field(""textFragment"" text Field.Store.YES Field.Index.TOKENIZED Field.TermVector.YES)); iwriter.AddDocument(doc); iwriter.Close(); IndexSearcher isearcher = new IndexSearcher(directory); QueryParser parser = new QueryParser(""textFragment"" analyzer); foreach (DictionaryEntry de in OntologyLayer.OntologyLayer.HashTable) { List<string> buffer = new List<string>(); double weight = 0; List<OntologyLayer.Term> list = (List<OntologyLayer.Term>)de.Value; foreach (OntologyLayer.Term t in list) { Hits hits = null; string label = t.Label; string[] words = label.Split(' '); int numOfWords = words.Length; double wordWeight = 1 / (double)numOfWords; double localWeight = 0; foreach (string a in words) { try { if (!buffer.Contains(a)) { Lucene.Net.Search.Query query = parser.Parse(a); hits = isearcher.Search(query); if (hits != null && hits.Length() > 0) { localWeight = localWeight + t.Weight * wordWeight * hits.Length(); } buffer.Add(a); } } catch (Exception ex) {} } weight = weight + localWeight; } sbWeight.AppendLine(weight.ToString()); if (weight > 0) { string objectURI = (string)de.Key; conceptList.Add(objectURI); } } ok...I have an ontology with concepts. I read every cocnept and try to find it in text. In this case it should return only Meta Files and Test Generation but I get:Random Number Generation Test Generation Text Processing Language Generation Document and Text Editing Line and Curve Generation Machine-Independent Microcode Generation Plan Execution Formation and Generation Index Generation Meta Files Files Text Analysis Picture Image Generation Large Text Archives Document and Text Processing you should provide examples of input and matches to given queries and results. Take a look at Stupid Lucene Tricks: Exact Match Starts With Ends With. it was long time ago but since I didn't provide an answer and looks like this really helps I'll accept it."
534,A,"How does Lucene work I would like to find out how lucene search works so fast. I can't find any useful docs on the web. If you have anything (short of lucene source code) to read let me know. A text search query using mysql5 text search with index takes about 18 minutes in my case. A lucene search for the same query takes less than a second. Can I request this question to be converted as a community wiki ? Lucene sounds like a platform now. Lucene is an inverted full-text index. This means that it takes all the documents splits them into words and then builds an index for each word. Since the index is an exact string-match unordered it can be extremely fast. Hypothetically an SQL unordered index on a varchar field could be just as fast and in fact I think you'll find the big databases can do a simple string-equality query very quickly in that case. Lucene does not have to optimize for transaction processing. When you add a document it need not ensure that queries see it instantly. And it need not optimize for updates to existing documents. However at the end of the day if you really want to know you need to read the source. Both things you reference are open source after all. If I understand correctly the thing that sets text search engines apart is how they handle multi-word searches and joining the results of searches to multiple indexes in real time. I would not suggest consulting Lucene source for this. It would probably be better to read a little about text search theory @alienCoder's answer helped me.  Lucene creates an big index. The index contains word id no of docs where the word is present the position of the word in those documents. So when you give a single word query it just searches the index (O(1) time complexity). Then the result is ranked using different algorithms. For multiword query just take intersection of the set of files where the words are present. Thus lucene is very very fast. For more info read this article by Google deelopers- http://infolab.stanford.edu/~backrub/google.html One popular algorithm is pigeon rank algorithm. Although I don't know much about it. Skimmed over that paper it was pretty helpful. Specifically ""4.5 Searching"" had the answer I was looking for. Specifically it sounds like an O(1) hash search is used for individual words but then an O(n) scan is used to join the results with a 40000 document limit. I assume a map-reduce algorithm is used to split this work up so that the user gets instantaneous results. That paper is amusing: ""In this paper we present Google a prototype..."". I guess Google wasn't always a mega-corporation.  In a word: indexing. Lucene creates an index of your document that allows it to search much more quickly. It's the same difference between a list O(N) data structure and a hash table O(1) data structure. The list has to walk through the entire collection to find what you want. The hash table has an index that lets it figure out exactly where the desired item is and simply fetch it. Yes I understand the indexing part but again lucene index searches are a lot faster than mysql index searches. How does that happen"
535,A,Indexing a document If I'm looking to index a document (HTML) in java and the count the number of indexes is Lucene the way to go? I have the feeling that Lucene is just a search engine. Many thanks indeed Yes! I have done this exact thing. I used JSOUP and Lucene to fetch an HTML page to index the content. JSOUP is a similar library to jqeury except for java. So I was able to get the div I wanted to index and get all the text for that. I can share examples if you want. What is your doubt and maybe I can help you with that? Edit: Here is an example of a project I did a while back https://github.com/amir20/iAuthor/blob/master/wikitool/src/main/java/edu/gwu/raminfar/iauthor/wikitool/WikiTool.java#L180 It works really well if you want a true indexing service. If you want to just store the HTML in a database then Lucene might not be your choice. Hi there. Thanks very much for the reply. I just didn't know where to start. I'm looking to take an HTML file strip out (if needed) and index the terms in the document minus any html markup tags. I wasn't sure if Lucene could do this. What exactly did you use JSOUP for and how did Lucene help? Thanks indeed. cool that's exactly what i used it for. Look at my edit. I posted a link to my project. I used jsoup to get all the text for a webpage. I was actually parsing wiki pages for content and then indexing the content to search later. You can ignore the NlpService class. That was just parsing the nouns and verbs so I can have better accuracy. Does this help? Hi I don't need to store the HTML in a database. Just one other quick question. I just checked out JSOUP but what exactly did you use Lucene for in this context? Thank you very much for your help. Best regards. I pulled all the text using Jsoup and then used lucene to index the text. This allowed me to search for all documents that contained some keywords. If you don't need an indexing service then don't use lucene. You can use a regular database and you should be fine. I am still confused on what you want to do with the page. Do you need to search it later? If you don't need to store it then what are you trying to do? Essentially I just want to take the HTML page and index all of the content (except for the HTML markup). So the likes of content in between tags tags etc... Then I want to search each unique index in a buffer that I will write to a file at a later stage. I won't need to search it later. The indexing is only to happen once and that's it. Does that make sense? Thanks I think so then lucene would be right for you. You are really doing the same thing I did. Get all the HTML parse it using jsoup index it using lucene and store it somewhere. Then read it using lucene and search for what you want to search. You'll have to also store the url in lucene so you know later where it came from. You don't have to store the content just set index=true and store=false.  Well yeah - Lucene is a search engine. (To be more precise: it's a library that allows you to build a search engine). To get a search engine you need a text index and Lucene provides that too. And it's a pretty powerful tool - it includes stemmers for English and a few other languages and in my experience it runs fast even with large amounts of data. Lucene won't parse the HTML for you so you'll need to do that before putting the text into the index. Excellent thanks for the information.
536,A,How can I configure Sitecore search to retrieve custom values from the search index I am using the AdvancedDatabaseCrawler as a base for my search page. I have configured it so that I can search for what I want and it is very fast. The problem is that as soon as you want to do anything with the search results that requires accessing field values the performance goes through the roof. The main search results part is fine as even if there are 1000 results returned from the search I am only showing 10 or 20 results per page which means I only have to retrieve 10 or 20 items. However in the sidebar I am listing out various filtering options with the number or results associated with each filtering option (eBay style). In order to retrieve these filter options I perform a relationship search based on the search results. Since the search results only contain SkinnyItems it has to call GetItem() on every single result to get the actual item in order to get the value that I'm filtering by. In other words it will call Database.GetItem(id) 1000 times! Obviously that is not terribly efficient. Am I missing something here? Is there any way to configure Sitecore search to retrieve custom values from the search index? If I can search for the values in the index why can't I also retrieve them? If I can't how else can I process the results without getting each individual item from the database? Here is an idea of the functionality that I’m after: http://cameras.shop.ebay.com.au/Digital-Cameras-/31388/i.html Klaus answered on SDN: use facetting with Apache Solr or similar. http://sdn.sitecore.net/SDN5/Forum/ShowPost.aspx?PostID=35618  I've currently resolved this by defining dynamic fields for every field that I will need to filter by or return in the search result collection. That way I can achieve the facetted searching that is required without needing to grab field values from the database. I'm assuming that by adding the dynamic fields we are taking a performance hit when rebuilding the index. But I can live with that. In the future we'll probably look at utilizing a product like Apache Solr.
537,A,"Updating Solr Schema I am new to Solr and I'm curious what the procedure is for changing/updating the schema? I noticed that I can ADD new fields easily without causing any issues but any time that I've had to UPDATE a field it's caused issues. Due to the amount of data ingested into the system I will not be able to retain the original data that was used to generate the add/doc queries to solr so I'll be unable to simply re-index everything when a change occurs. For instance I am looking to change an existing field from the type ""string"" to ""text"" and the text field type has many tokenizers filters etc that I would like to put to use immediately on the existing data. I am ideally looking for a way to update the schema re-index/optimize the existing data set and be able to track how long it will take until the operation is complete. If someone can help me understand this I would much appreciate it! You have to reindex. There is no other way around it. Indexing is a destructive process with relation to its input: text is sliced and diced to make it faster for search so you can't recover the original text unless you had it in a stored field. (stored=true in your Solr field definition in schema.xml). If you did have it in a stored field all you have to do is a little process to iterate through the documents and just re-send them so they're reindexed. Thanks for the answer. Not the one I was hoping for but honest and to the point."
538,A,"Choosing a stand-alone full-text search server: Sphinx or SOLR? I'm looking for a stand-alone full-text search server with the following properties: Must operate as a stand-alone server that can serve search requests from multiple clients Must be able to do ""bulk indexing"" by indexing the result of an SQL query: say ""SELECT id text_to_index FROM documents;"" Must be free software and must run on Linux with MySQL as the database Must be fast (rules out MySQL's internal full-text search) The alternatives I've found that have these properties are: Solr (based on Lucene) ElasticSearch (also based on Lucene) Sphinx My questions: How do they compare? Have I missed any alternatives? I know that each use case is different but are there certain cases where I would definitely not want to use a certain package? Have you ruled out using straight Lucene? Solr is a service on top of lucene so straight Lucene could stile be a possibility. Personally I like Sphinx. However during a ""large"" project recently the latest release candidate (0.9.9-rc2) had show stopper bugs when using multi-value arrays (MVA). It would random results! So we moved to SOLR as to get around this. Once SOLR was up and running the performance was fine and without the show stopper bug. Does Lucene have a stand-alone server mode? I thought that was one of the things SOLR added? I haven't ruled out anything - so feel free to advocate Lucene if that is the best choice given the requirements :-) What's your client platform? mausch: Mainly Java but also other languages. Have you looked at elasticsearch.com ? that said we're piping data thru xml to sphinxsearch. rough and ugly but once done it is so freakin fast. I have been using Sphinx for almost a year now and it has been amazing. I can index 1.5 million documents in about a minute on my MacBook and even quicker on the server. I am also using Sphinx to limit searches to places within specific latitudes & longitudes and it is very fast. Also how results are ranked is very tweakable. Easy to install & setup if you read a tutorial or two. Almost 1.0 status but their Release Candidates have been rock solid. Geographical searching can be done in Solr with the LocalSolr plugin: http://www.gissearch.com/localsolr  Lucene / Solr appears to be more featured and with longer years in business and a much stronger user community. imho if you can get past the initial setup issues as some seems to have faced (not we) then I would say Lucene / Solr is your best bet. User community is an important point. There are a couple of VERY VERY helpful people in the Sphinx forums but there isn't a strong community otherwise.  I've been using Solr successfully for almost 2 years now and have never used Sphinx so I'm obviously biased. However I'll try to keep it objective by quoting the docs or other people. I'll also take patches to my answer :-) Similarities: Both Solr and Sphinx satisfy all of your requirements. They're fast and designed to index and search large bodies of data efficiently. Both have a long list of high-traffic sites using them (Solr Sphinx) Both offer commercial support. (Solr Sphinx) Both offer client API bindings for several platforms/languages (Sphinx Solr) Both can be distributed to increase speed and capacity (Sphinx Solr) Here are some differences: Solr being an Apache project is obviously Apache2-licensed. Sphinx is GPLv2. This means that if you ever need to embed or extend (not just ""use"") Sphinx in a commercial application you'll have to buy a commercial license (rationale) Solr is easily embeddable in Java applications. Solr is built on top of Lucene which is a proven technology over 8 years old with a huge user base (this is only a small part). Whenever Lucene gets a new feature or speedup Solr gets it too. Many of the devs committing to Solr are also Lucene committers. Sphinx integrates more tightly with RDBMSs especially MySQL. Solr can be integrated with Hadoop to build distributed applications Solr can be integrated with Nutch to quickly build a fully-fledged web search engine with crawler. Solr can index proprietary formats like Microsoft Word PDF etc. Sphinx can't. Solr comes with a spell-checker out of the box. Solr comes with facet support out of the box. Faceting in Sphinx takes more work. Sphinx doesn't allow partial index updates for field data. In Sphinx all document ids must be unique unsigned non-zero integer numbers. Solr doesn't even require an unique key for many operations and unique keys can be either integers or strings. Solr supports field collapsing (currently as an additional patch only) to avoid duplicating similar results. Sphinx doesn't seem to provide any feature like this. While Sphinx is designed to only retrieve document ids in Solr you can directly get whole documents with pretty much any kind of data making it more independent of any external data store and it saves the extra roundtrip. Solr except when used embedded runs in a Java web container such as Tomcat or Jetty which require additional specific configuration and tuning (or you can use the included Jetty and just launch it with java -jar start.jar). Sphinx has no additional configuration. Related questions: Full Text Searching with Rails Comparison of full text search engine - Lucene Sphinx Postgresql MySQL? Talking about devs committing to both Solr and Lucene it seems they have merged the two products making further development easier and faster - http://www.lucidimagination.com/blog/2010/03/26/lucene-and-solr-development-have-merged/. @Stann : how so? I've used Solr for nearly 5 years ago and never needed to write a single line of Java. @MauricioScheffer Do u really think that java code will be faster than C++. Here's the comparison made by Bill Karwin and Sphinx there queries things 10 times faster than lucene (and solr have gotta be even slower than.) http://www.slideshare.net/billkarwin/practical-full-text-search-with-my-sql @Stann : do you really think you need more performance than whitehouse.gov Netflix The Guardian digg just to name a few websites using Solr? http://wiki.apache.org/solr/PublicServers @Stann : I also recommend checking out https://www.google.com/search?q=java+slow+myth @Stann : also Solr can actually be *faster* than Lucene due to caching in real-world scenarios (not contrived benchmarks like Karwin's...) [Here is an answer on Sphinx](http://stackoverflow.com/questions/737275/comparison-of-full-text-search-engine-lucene-sphinx-postgresql-mysql) that is a good pair to this answer on Solr  Note: There are many users with the same question in mind. So to answer to the point: Which and why? Use Solr if you intend to use it in your web-app(example-site search engine). It will definitely turn out to be great thanks to its API. You will definitely need that power for a web-app. Use Sphinx if you want to search through tons of documents/files real quick. It indexes real fast too. I would recommend not to use it in an app that involves JSON or parsing XML to get the search results. Use it for direct dB searches. It works great on MySQL. Alternatives Although these are the giants there are plenty more. Also there are those that use these to power their custom frameworks. So i would say that you really haven't missed any. Although there is one elasticsearch that has a good user base. that awkward moment when I read this answer after a year and a half and click on upvote and see that I wrote this answer myself. lol. :D A small addition to this though: After 18 months elasticsearch has turned out to be a great alternative and has a decent community too. Cool bonsai cool! It doesn't matter what language the web app is written in. Choose based on your use case! Augustus! That awkward moment :D. So for a python web-app what do you think is best now ? Solr or elastic search based on performance memory usage and easiness to setup any idea ?  Unless you need to extend the search functionality in any proprietary way Sphinx is your best bet. Sphinx advantages: Development and setup is faster Much better (and faster) aggregation. This was the killer feature for us. Not XML. This is what ultimately ruled out Solr for us. We had to return rather large result sets (think hundreds of results) and then aggregate them ourselves since Solr aggregation was lacking. The amount of time to serialize to and from XML just absolutely killed performance. For small results sets though it was perfectly fine. Best documentation I've seen in an open source app Solr advantages: Can be extended. Can hit it directly from a web app i.e. you can have autocomplete-like searches hit the Solr server directly via AJAX. I should have linked to the wiki: http://wiki.apache.org/solr/QueryResponseWriter#head-e82d899e83a861380fb6d0c34c1228a2f79f6c98 Solr has many response writers other than xml including JSON PHP Ruby Python and a java binary format: http://lucene.apache.org/solr/api/org/apache/solr/request/QueryResponseWriter.html Did I mention how terrible the Solr/Lucene documentation is? Having to root through Javadocs to figure out functionality is not my idea of documentation. I spend the whole day fixing some installation bug of sphinx 0.9.9 on my mac. So far it is still not working. It is so buggy. I used very ways suggested. I am givin up Really frustrating... solr's documentation is not so good as sphinx. but the community is large. And I can always figure out everything by reading the source code of solr."
539,A,"How do I generate a unique id using Lucene? I am using Lucene to store (as well as index) various documents. Each document needs a persistent unique identifier (to be used as part of a URL). If I was using a SQL database I could use an integer primary key auto_increment (or similar) field to automatically generate a unique id for every record that was added. Is there any way of doing this with Lucene? I am aware that documents in Lucene are numbered but have noted that these numbers are reallocated over time. (I'm using the Java version of Lucene 3.0.3.) Cant you just index an UUID.randomUUID() and use it for permanent keys? Ideally I want my ids to be shorter. As larsmans said you need to store this in a separate field. I suggest that you make the field indexed as well as stored and index it using a KeywordAnalyzer. You can keep a counter in memory and update it for each new document. What remains is the problem of persistence - how to store the maximal id when the Lucene process stops. One possibility is to use a text file which saves the maximal id. I believe Flexible Indexing will allow you to add the maximal id to the index as a ""global"" field. If you are willing to work with Lucene's trunk you can try flexible indexing to see whether it fits the bill.  For similar situations I use following algorithm (has nothing to do with Lucene but you can use it anyway). Create new AtomicLong. Start with initial value obtained from System.currentTimeMillis() or System.nanoTime() Each next ID is generated by calling .incrementAndGet or .getAndIncrement on that AtomicLong. if the system is restarted AtomicLong is again initialized to current timestamp during the startup. Pros: simple effective thread-safe non-blocking. If you need clustered id support just add space for hi/lo algorithm on top of existing long or sacrifice some high bytes. Cons: does not work if the frequency of adding new entities if more than 1/ms (for System.currentTimeMillis()) or 1/ns (for System.nanoTime()). Does not tolerate clock abnormalities. Can consider using UUID as yet another alternative. Probability of a duplicate in UUID is virtually non-existant.  EDIT: Several commenters have raised possible issues with this approach and I don't have time to test it thoroughly. I'm leaving it here because Yuval F. refers to it. Please don't downvote unnecessarily. Given an IndexWriter w you can use w.maxDoc() + 1 as an id and store that (as a string) in a separate Field. Make sure the Field is stored. I don't think this will work after an optimize of the index. I don't think this will work. Suppose there are n docs. Add n+1. Delete one. Add another. Now you have two docs with ID n+1. (You'd also get really boned if you merged indexes etc.) Excuse me @Dave misread your question. Of course you can index it if you want. @Simon Svensson: the API docs state ""Returns total number of docs in this index (...) not counting deletions"". Why store the id field without indexing it? Doesn't that mean I cannot search by id? @Xodarap: If I read the API doc correctly (""not counting deletions"") then this approach does guard against that. In fact that seems to be why `IndexWriter` has both `maxDoc` and `numDocs` methods. @larsmans: As Pascal mentions once you optimize the seg info no longer contains the count of deleted docs. You can try it with luke: delete optimize and then see that the count doesn't include your deletes. Also wouldn't this be affected by merging and reuse ids when deleted documents are pruned?"
540,A,"sfLucene in Symfony 1.3 or 1.4 Has anyone taken the sfLucenePlugin and made it work in Symfony 1.3 or 1.4? The plugin requires pake functions and the new config class to be adjusted but looks like it could be done if persistent. I have seen the Jobeet tutorial for implementing Zend Lucene but am not keen to lose all of the .yml functionality that sfLucene provides. I'm using sfLucenePlugin in project based on symfony 1.4. I'm using Doctrine branch of sfLucenePlugin from svn. Also I've fixed erroneous calls to sfLoader::loadHelper() (here's patch). After that it works like a charm.  Combining sfLucenePlugin and Propel ORM only works under sf1.0 and sf1.1 - the newer versions for 1.2+ have abandoned Propel for Doctrine. This is in line with most symfony developers but with Propel 1.5 making serious improvements there is now a need for a good plugin for Propel and Lucene under sf1.3+. Sadly the lead developer on plugins that did this has not kept up development on these projects I suspect due to being too busy to contribute. However there is a plugin that might help you out: rsLucenePlugin: http://www.symfony-project.org/plugins/rsLucenePlugin. This plugin mimics the older sfLucenePlugin for Propel although it is claimed ""It's working with symfony 1.4 and PHP 5.3"". I have not tried it myself (I prefer the control of integrating ZSL a la the Jobeet tutorial) but I imagine it should work with sf1.3 and PHP 5.2. It is fairly basic but solves the Propel problem. Crucially for you the YAML configurations you mention are very similar to the format you will be familiar with so it should be a good starting point. As with all symfony plugins if you find it particularly useful I urge you to let the developer know directly as it could make the difference between its maintenance and its abandonment - sfLucenePlugin is easily one of the most useful plugins for symfony so it's sad to see its demise... Sounds like the best plan will be to grab the rsLucenePlugin then try to help the owner sort out any bugs that I find."
541,A,"Showing search documents count under each category I need to show total documents count for each category in my search results...for example: Rock(1010) Blues(5030) Pop(2209) : : I was reading somewhere that using TopFieldDocCollector is more efficient than HitCollector class. Given my requirement how do I use TopFieldDocCollector class?or is there any other approach in Lucene? HitCollector is an abstract class. TopDocCollector and TopFieldDocCollector are implementations of that class. They can't be ""more efficient"" because there's no actual code in HitCollector. As for the difference between TopDocCollector and TopFieldDocCollector - the former is used when you want to sort the results by relevance and the latter is for when you want a custom sort. Now regarding your question check out my answer to a similar question."
542,A,"Getting terms matched in a document when searching using a wildcard search I am looking for a way to find the terms that matched in the document using waldcard search in Lucene. I used the explainer to try and find the terms but this failed. A portion of the relevant code is below. ScoreDoc[] myHits = myTopDocs.scoreDocs; int hitsCount = myHits.Length; for (int myCounter = 0; myCounter < hitsCount; myCounter++) { Document doc = searcher.Doc(myHits[myCounter].doc); Explanation explanation = searcher.Explain(myQuery myCounter); string myExplanation = explanation.ToString(); ... When I do a search on say micro* documents are found and it enter the loop but myExplanation contains NON-MATCH and no other information. How do I get the term that was found in this document ? Any help would be most appreciated. Regards One way is to use the Highlighter; another way would be to mimic what the Highlighter does by rewriting your query by calling myQuery.rewrite() with an appropriate rewriter; this is probably closer in spirit to what you were trying. This will rewrite the query to a BooleanQuery containing all the matching Terms; you can get the words out of those pretty easily. Is that enough to get you going? Here's the idea I had in mind; sorry about the confusion re: rewriting queries; it's not really relevant here.  TokenStream tokens = TokenSources.getAnyTokenStream(IndexReader reader int docId String field Analyzer analyzer); CharTermAttribute termAtt = tokens.addAttribute(CharTermAttribute.class); while (tokens.incrementToken()) { // do something with termAtt which holds the matched term } Actually what I am looking for is to be able to get only those terms that are found in the document. So that if one document contains microscope and another contains microsoft then when i am on the first document I should get only microscope and when I am on the second document I should get only microsoft as the matched term. Your suggestion would give me all the terms that would match micro* in the index field. I hope I am able to explain what I am looking for. I edited the answer - hopefully closer to what you need   class TVM : TermVectorMapper { public List<string> FoundTerms = new List<string>(); HashSet<string> _termTexts = new HashSet<string>(); public TVM(Query q IndexReader r) : base() { List<Term> allTerms = new List<Term>(); q.Rewrite(r).ExtractTerms(allTerms); foreach (Term t in allTerms) _termTexts.Add(t.Text()); } public override void SetExpectations(string field int numTerms bool storeOffsets bool storePositions) { } public override void Map(string term int frequency TermVectorOffsetInfo[] offsets int[] positions) { if (_termTexts.Contains(term)) FoundTerms.Add(term); } } void TermVectorMapperTest() { RAMDirectory dir = new RAMDirectory(); IndexWriter writer = new IndexWriter(dir new Lucene.Net.Analysis.Standard.StandardAnalyzer() true); Document d = null; d = new Document(); d.Add(new Field(""text"" ""microscope aaa"" Field.Store.YES Field.Index.ANALYZEDField.TermVector.WITH_POSITIONS_OFFSETS)); writer.AddDocument(d); d = new Document(); d.Add(new Field(""text"" ""microsoft bbb"" Field.Store.YES Field.Index.ANALYZED Field.TermVector.WITH_POSITIONS_OFFSETS)); writer.AddDocument(d); writer.Close(); IndexReader reader = IndexReader.Open(dir); IndexSearcher searcher = new IndexSearcher(reader); QueryParser queryParser = new QueryParser(""text"" new Lucene.Net.Analysis.Standard.StandardAnalyzer()); queryParser.SetMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE); Query query = queryParser.Parse(""micro*""); TopDocs results = searcher.Search(query 5); System.Diagnostics.Debug.Assert(results.TotalHits == 2); TVM tvm = new TVM(query reader); for (int i = 0; i < results.ScoreDocs.Length; i++) { Console.Write(""DOCID:"" + results.ScoreDocs[i].Doc + "" > ""); reader.GetTermFreqVector(results.ScoreDocs[i].Doc ""text"" tvm); foreach (string term in tvm.FoundTerms) Console.Write(term + "" ""); tvm.FoundTerms.Clear(); Console.WriteLine(); } } You don't have to modify it with Lucene.Net 2.9.4g at https://svn.apache.org/repos/asf/incubator/lucene.net/branches/Lucene.Net_2_9_4g/src Had to modify the TVM class to use HashTable for C#. Thanks worked as I wanted it to."
543,A,"Does Zend Lucene support MultiValued Fields? I wanted to know if Zend Lucene supports multivalued fields. I tried passing a an array to a field and it doesnt give any errors during indexing. But its not returning any results when i search. Any help is appreciated. It doesn't: Fields are always stored and returned from the index in UTF-8 encoding. Any required conversion to UTF-8 happens automatically. I would think UTF-8 Encoding an array would not work unless there's some recursion happening there. You could join the array with a """" or something or serialize or json_encode the array. If you are using it as a search index that might not work though. You could also use the Binary field type if you need to store something more complex like an image or something.  Actually if you add space-separated Text fields they are stored as multiple fields this is confirmed in Luke. $doc->addField(Zend_Search_Lucene_Field :: Text($fieldName implode(' ' $fieldValue))); The only problem then is that the values are lower cased because they are tokenized."
544,A,"How to use Version method 'valueOf(String)' Wondering if anyone can tell me how to use the Apache Lucene method 'valueOf(String)' for Version? Is this used to return the current version that should be used from the enum? http://lucene.apache.org/java/3_1_0/api/core/org/apache/lucene/util/Version.html#valueOf(java.lang.String) For example does: Version.valueOf(""StandardAnalyzer""); return the emum version that should be used with the StandardAnalyzer? Thanks! This valueOf method is defined for every method and simply converts a string like ""LUCENE_24"" to the corresponding enum constant LUCENE_24.  I think the enum is there to let you safely pick the version you want to use. I guess you're supposed to go with the latest one if you're starting from scratch but otherwise you are supposed to make a willing choice to upgrade from one version to another. They deprecated the LUCENE_CURRENT constant so that you don't get silently ""updated"" when picking a new version of the library. So basically I would pass Version.LUCENE_31 as a configuration for a new project. The enum can also be used to compare two versions with the onOrAfter method. Still to answer you question I think (but cannot test right now) that if for some reason you want to use the valueOf method you'd have to pass a string with the version name. For example Version.valueOf(""LUCENE_23""). Of course this would be useful if your version name comes from a config file. Hope that helps Thanks for your answer. I am using version 3.1.0 of Lucene and the reason I posted is because whenever I used the Version.LUCENE_31 for StandardAnalyzer I get the exception: org.springframework.web.util.NestedServletException: Handler processing failed; nested exception is java.lang.NoClassDefFoundError: org/apache/lucene/util/Version Oh. This is another issue. Is it the only lucene class giving you this error ?  i think that this way you need: probe it for(Version v : Version.valueOf(""StandardAnalyzer"")) System.out.println(v); This won't compile will it?"
545,A,"Hibernate Search filter out object with the highest value Hi i have a EntityObject like this public class Adm{ private String id; private String version; private String name; private String mimetype; ... ... ... } I would like to add an filter out all objects with the highest version of those objects with the same name. Anyone got any idea how todo this with a filter or when creating the query? I use Hibernate Search Version 3.3.0. //Trind One possible way is to use HQL (Hibernate Query Language)  SessionFactory sessionFactory= new Configuration().configure().buildSessionFactory(); sess = sessionFactory.openSession(); String SQL_QUERY = ""select max(version)from Adm adm""; Query query = sess.createQuery(SQL_QUERY); List list = query.list(); sess.close(); For more details here. Sorry i skip the sentence with Hibernate Search. Well i don't see how i should combined this with hibernate search? Right HQL and Hibernat Search really don't go together like this  The way to do filtering in Search is via FullTextFilters. See http://docs.jboss.org/hibernate/stable/search/reference/en-US/html_single/#query-filter You can pass parameters to the filter when you enable them eg fullTextQuery.enableFullTextFilter(""version"").setParameter( ""max"" 1001 ); You can pass as many parameters you like and also pass any parameter type you want (you will just have to cast appropriately in the filter implementation). You would probably need another query to determine the max values. Maybe a HQL or Criteria query after all. Within the Filter you could use a NumericRangeQuery. Of course this all depends on your domain model. You haven't included the Hibernate Search annotations and the Hibernate Search query you are trying to run. Also is the max version something you can determine beforehand and cache? Hope this gives you some pointers. How do i solve this in a good way when i want the highest version number of those with the same name? The way i have done it now is to index if a object is the highest value however the problem i have with that is that i have to reinxex all the others with same name if there is a new entry with higher version. I haven't figure out if i can just reindex one field on an entity."
546,A,"Lucene indexing: shared or isolated by account? I'm evaluating Lucene to implement a global search feature in a SaaS application. We do not want users to see the content of the other accounts so searches will always be limited by account. Is it better to have one single index with an account id field or one index per account? What are the advantages and disadvantages of each approach? My concern is that a global index might affect performance due to the frequent updates. Thank you. EDIT Estimated number of total documents: 5000000 Number of accounts: 4000 Indexable data is never shared between accounts Account users might update their indexable data several times a day (not more than 100 in most cases) The amount of indexed data tends to be stable after the initial setup process We need to store 10-20 fields per document Your question is too broad/complex; the answer depends very much on other aspects of your application and its architecture. What is the running environment in which indexes will be queried? Is indexable data often shared between many accounts? Is data updated often? How often? What is the growth rate of indexed data for a typical account? And so on and so forth. here are some things I would think about in addition to the usual problems (e.g. index updates and such): The way lucene returns ranked results depends upon some ""corpus-wide"" statistics for example the total number of documents that a term appears in for that field. So if the index statistics for customer a are inappropriate for customer b its going to hurt relevance for both customers besides being a security risk... if oscar is smart enough he truly can start reversing bob's documents because of the nature of the inverted index: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.159.9682 You could probably work around this with something like this ranking algorithm: https://issues.apache.org/jira/browse/LUCENE-2864 Some other things in lucene apply to a ""field as a whole"" or ""index as a whole"" and you should know that they can't be really changed on a per-customer basis if you group indexes together: things like omitTF (if you set it on a single document for a field its omitted across the board for that field) similarity (in any released version of lucene you can only set similarity across the board so customers wouldn't be able to tune the ranking model) spellchecking (you would have to hack something up where each customer has their own ""filtered"" spellcheck index) ... On the other hand if you have many terms quite a bit of RAM is required and by giving each customer their own index you will need more memory to hold the terms index in RAM for all the indexes. You can however lower this somewhat by adjusting things like termIndexInterval/Divisor. Regarding the paper mentioned in 1. is it correct to say that one can implement their 2nd approach (Query Integration) using a custom search filter? Not unless you do something about the IDF... the simplest solution to this for lucene being to use filters + a similarity that does not use any global statistics at all.  If it were me if there is no regulatory reason why you can not I'd dump them all in to a single index. This is simply my ""don't optimize what you don't have to"" hat speaking. The first concern is simply legal: are you even ALLOWED to co-host and intermix data even if it is separated by logical means. That's up to your lawyers customers and service agreements. This is not a technical concern. Assuming you can then the next question is what impact will other users have upon each other. If User A is using the system and User B is in the process of importing their 100K documents is that going to impact User A? Is it impacting User A because of how Lucene works or simply because of the overall system load that occurs when importing and indexing documents. Try it and see. The key thing is to make sure that your client systems do not access Lucene directly but rather through a facade of some kind. This facade is a perfect place to enforce the client segregation and it's also a good place to redirect traffic if at some later time you decide you need to shard your indexes. Perhaps you need to tear out a single heavy user. Or you sell a higher level of response time to someone that is guaranteed more resources in their SLA etc. But deciding right now what the better path is? Eh seems early. 500K documents is not a lot of data to Lucene. Just make sure you have flexibility in your implementation to add capability later if you find out that hosting it all in a single instance isn't viable. And by ""add capability"" I mean exactly that add it. Don't actually IMPLEMENT say sharding based on client. But rather have a good point where it COULD be implemented without redoing a bunch of plumbing later.  I've done a few ""security trimmed"" indexes here and there -- definitely possible if it is allowed. That said my general inclination with SAAS-type stuff with multiple clients would be to separate the clients as much as possible for a few reasons: a) Ensures coding errors don't result in data leaks angry clients lawsuits and other hoo ha. b) Makes per-client customization much easier -- your entire codebase need not deal with client-specific fubar requests c) Forces you into a horizontally scalable architecture from day one -- scaling is easy if adding instances is easy right? Oh and definitely take Will Hartung's advice -- facade search that stuff really should not creep out of it's layer."
547,A,"Debugging Solr search queries on Sunspot How can I debug Solr search queries when using the Sunspot gem on Rails? I have some queries that are returning bizarrely high scores and I'm trying to get to the bottom of why this is happening. It doesn't seem like any debugging information is exposed to Sunspot so I think that I need to debug through Solr directly. Fortunately Solr has a handy web interface to search from but for some reason any queries I enter there return with 0 results. For example when I search for the word ""test"" on my web app it returns plenty of hits. When I search for the same thing on the Solr admin interface this is what I get: <response> <lst name=""responseHeader""> <int name=""status"">0</int> <int name=""QTime"">172</int> <lst name=""params""> <str name=""explainOther""/> <str name=""fl"">*score</str> <str name=""indent"">on</str> <str name=""start"">0</str> <str name=""q"">test</str> <str name=""hl.fl""/> <str name=""qt"">standard</str> <str name=""wt"">standard</str> <str name=""fq""/> <str name=""version"">2.2</str> <str name=""rows"">10</str> </lst> </lst> <result name=""response"" numFound=""0"" start=""0"" maxScore=""0.0""/> </response> What's your query URL? There seems to be an empty fq there not good... possible duplicate of [Solr Search Using Susnpot Gem](http://stackoverflow.com/questions/8078632/solr-search-using-susnpot-gem) To debug requests sent to Solr I often use an http request analyzer like tcpmon or fiddler.  Try reading Debugging Search Application Relevance Issues which discusses explanations and the Solr analysis tool.  If setting log_level to FINEST doesn't work you should add this line to an initializer: require ""sunspot/rails/solr_logging"" source: http://outoftime.github.com/2010/03/03/sunspot-1-0.html  Set the log level to FINEST and you will see the exact query in the log file.  When you search from your web app do you search specific fields or just the default field? When you type something into the admin console to debug searches its easy to forget to tell it which field(s) you want to search on and if you don't tell it then only the default field is searched. http://stackoverflow.com/a/8082936/474597 has a better explaination in which you need to include the search column name such as body_text:your_key_words"
548,A,"Is {Filter}ing faster than {Query}ing in Lucene? While reading ""Lucene in Action 2nd edition"" I came across the description of Filter classes which are could be used for result filtering in Lucene. Lucene has a lot of filters repeating Query classes. For example NumericRangeQuery and NumericRangeFilter. The book says that NRF does exactly the same as NRQ but without document scoring. Does this means that if I do not need scoring or sort documents by document field value I should prefer Filtering over Querying from performance point of view? Is the database local or on a different server? The database is stored locally. On several servers we have SSD drives as well. I found this in http://wiki.apache.org/lucene-java/ImproveSearchingSpeed which seems to suggest to use filters rather than queries. Intuitively it makes more sense to me as they pretty much should do the same thing the only difference being that filters are not used in the score. Consider using filters. It can be much more efficient to restrict results to a part of the index using a cached bit set filter rather than using a query clause. This is especially true for restrictions that match a great number of documents of a large index. Filters are typically used to restrict the results to a category but could in many cases be used to replace any query clause. One difference between using a Query and a Filter is that the Query has an impact on the score while a Filter does not.  I receive a great answer from Uwe Schindler let me repost it here. If you dont cache filters queries will be faster as the ConjunctionScorer in Lucene has optimizations which are currently not used for Filters. Filters are fine if you cache them (e.g. if you always have the same access restrictions for a specific user that are applied to all his queries). In that case the Filter is only executed once and cached for all further requests and then intersected with the query result set. If you only want to e.g. randomly ""filter"" e.g. by a variable numeric range like a bounding box in a geographic search use queries queries are in most cases faster (e.g. Range Queries and similar stuff - called MultiTermQueries - are internally also implemented by the same BitSet algorithm like the Filter - in fact they are only Filters wrapped by a Scorer-impl). But the Scorer that ANDs the query and your ""filter"" query together (ConjunctionScorer) is generally faster than the code that applies the filter after searching. This may some improvement possible but in general filters are something in Lucene that is not really needed anymore so there were already some approaches to make Filters and Queries the same and instead then be able to also cache non-scoring queries. This would make lots of code easier. Filters can bring a huge speed improvement with Lucene 4.0 if they are plugged ontop of the IndexReader to filter the documents before scoring but that's not yet implemented (see https://issues.apache.org/jira/browse/LUCENE-3212) - I am working on it. We may also make Filters random access (it's easy as they are bitsets) which could improve also the after-query filtering. But I would then also make Queries partially random access if they could support it (like queries that are only based on FieldCache). Uwe  In contrast to Dennis' answer: no you probably don't want to use a filter unless you're going to reuse the same query multiple times. A NumericRangeFilter is just a subclass of MultiTermQueryWrapperFilter which means that essentially it does something like this: for each document in index: if document matches query: match[i] = 1 else match[i] = 0 So it will run in linear time over your index instead of logarithmic time like a normal query. Additionally the filter will take up more memory (one bit for every doc in your index). If you're going to be using the same query over and over again then it's probably worth it to you to pay the performance/memory hit once and have later usages be faster. But if it's a one-off query it's almost certainly not worth it. (Also if you're going to reuse it use a CachingWrapperFilter so that the filter is cached.)  If the filter will be reused it is wise to use this instead of queries because of caching purposes. If you are not going to be using the scoring or field values it also makes sense to use filter over query. Hope this helps."
549,A,"Problem using same instance of indexSearcher for multiple requests Am using Lucene API in a .net web application. I want to use the same instance of Indexsearcher for all the requests.Hence am storing indexsearcher instance in http cache. here is my code for the same: if (HttpRuntime.Cache[""IndexSearcher""] == null) { searcher = new IndexSearcher(jobIndexFolderPath); HttpRuntime.Cache[""IndexSearcher""] = searcher; } else { searcher = (IndexSearcher)HttpRuntime.Cache[""IndexSearcher""]; } When I execute the statement below I get a runtime error :""Object reference not set to an instance of an object."" Hits hits = searcher.Search(myQuery); What am i missing here? Thanks for reading! I am not conversant with .net or HttpRuntime. One problem with your code is lack of synchronization. But that should just result into poor performance and not correctness issue. You can still give a try to synchronized initialization. Second problem I suspect is searcher not getting initialized at all due to some problem. Check if Searcher is not null after you create new IndexSearcher. Two comments : 1.) are you having the same problems with other objects stored in HttpRuntime? 2.) just in case you missed it an already opened IndexSearcher cannot see new entries in the index. so if you're application is a long running one with updates to the search index you should probably reconsider using the same instance of IndexSearcher for all requests Ubbenit never works....I always get a null reference... Shashikant am aware of the synchronization issues which this code does not address...i thought of countering those once i get past the basic problem...ie. using only instance of indexsearcher... So it never works or does it start to get the null pointer error after a period of time? fyiam able to store and retrieve other objects in HttpRuntime Instead of caching indexSearcher am now caching IndexReader. If IndexReader is already in cache am making a check if it is up to date.Otherwise am opening it and passing that instance to indexSearcher constructor. Does this logic/code make sense wrt optimized search query response incase multiple requests are hitting the web server for search? Thanks for reading. string key = MyConstants.CacheKey.IndexReader; indexReader = MyCacheManager.Get<IndexReader>(key); if (indexReader == null)//cache is empty.open indexreader { indexReader = IndexReader.Open(myIndexFolderPath); MyCacheManager.Add(key indexReader); indexSearcher = new IndexSearcher(indexReader); } else//cache contains indexreader...check if it is up to date { indexSearcher = base.GetIndexSearcher(myIndexFolderPath indexReader); } protected IndexSearcher GetIndexSearcher(string indexFolderPath IndexReader indexReader) { IndexSearcher indexSearcher = null; if (!indexReader.IsCurrent())//index is not up to date { indexReader = IndexReader.Open(indexFolderPath); indexSearcher = new IndexSearcher(indexReader); } else { indexSearcher = new IndexSearcher(indexReader); } return indexSearcher; }  Two things: If you're using .Net 2.0 and have not applied SP1 this may help. Look at the problem this person was having. Both entries refer to objects in the cache being expired too soon - almost immediately in both cases. Things might also be complicated by the fact that objects in the cache are not thread safe. If you have to have a single IndexSearcher why not provide it to the web app as a service?  First of all it's not safe at all it should be: var searcher = (IndexSearcher)HttpRuntime.Cache[""IndexSearcher""]; if(searcher == null) { searcher = new IndexSearcher(jobIndexFolderPath); HttpRuntime.Cache[""IndexSearcher""] = searcher; } In your code cache can expire between check and assignment very good point buddy...!but still my problme is that the indexSearcher retrieved from http cache is not working  My quick answer is... You don't really need to use the same index searcher object for all request in fact i would recommend against it. You only need to make sure there is only one thread updating the index. If you really want one how about a static member variable in the application that is initialized once and used by all? The long answer is... I will try and find my code and see exactly how I handled the problem It is best practice to reuse the index searcher. Searches on a large index are much faster on a warmed up index searcher as many things it needs are cached.  Try something like the following: protected static IndexSearcher searcher = null; ... if (searcher == null) { searcher = new IndexSearcher(jobIndexFolderPath); } I don't understand why the complexity? The code I posted above should work for multiple requests even though it's just a simple example. The key is declaring your searcher as a static variable because it's shared across the application context. please see my answer below and kindly comment.  I also have a web application that uses the Lucene API to query (my web app does not writes on the index) and I create a new instance of the searcher for every request. It might not be very ""performant"" but I never had that kind of problem. If you'd like my web app is on Google Code so you can download the source code and take a look at what I did. Here's the url to the project http://code.google.com/p/goomez/  Steve see best ways of using IndexSearcher. This is a bit dated but the principle remains: Use a single instance of IndexSearcher guard it using the proper thread safe code (which I do not know how to do in .Net) and invalidate it once the index is updated. I believe this is what Jesse has suggested and I second this idea."
550,A,inverse document frequency The inverse document freqency is defined as follows: IDF(termdocument) = tf(term) * log(1 + n/df(term)) where tf(term) = 'frequency of term in document' n = 'number of documents' df(term) = 'number of docs containing term'. Just curious about df(term) - do I only count a document ones even if it contains the term more than once? Also is it easy to determine this stat with lucene(.net)? I am only starting to use the latter and use a relational db at the moment. Thanks. Christian For using idf with Lucene check the API for example here. You are right about the docs being counted only once. The idea is to get a function with a lower bound in the log part. Like this: If you are interested in the idf theory behind the scenes you may peep at this paper. HTH! Thanks excellent - I will have to digest the paper. The API link points to the paper - could this be a mistake? Thanks! @csetzkom Thanks. Link corrected. Good luck! Thanks. I am still quite ignorant about the lucene api but how do I get the IDF given a document and a term (as in single word)? idf(Term term Searcher searcher) comes close but not sure about it ... @csetzkom It's this one http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/Similarity.html#idf%28org.apache.lucene.index.Term%20org.apache.lucene.search.Searcher%29  Of course you have to count the DF(term) once. therefore you should group the words to get distinct words. See my class IDF here
551,A,"Lucene document Boosting I am having problem with lucene boosting Iam trying to boost a particular document which matches with the (firstname)field specified I have posted the part of the code private static Document createDoc(String lucDescriptionString primarykString specialString){ Document doc = new Document(); doc.add(new Field(""lucDescription""lucDescription Field.Store.NO Field.Index.TOKENIZED)); doc.add(new Field(""primarykey""primarykField.Store.YESField.Index.NO)); doc.add(new Field(""specialDescription""specialString Field.Store.NO Field.Index.UN_TOKENIZED)); doc.setBoost ((float)(0.00001)); if (specialString.equals(""chris"")) doc.setBoost ((float)(100000.1)); return doc; } why is this not working? public static String dbSearch(String searchString){ List<String> pkList = new ArrayList<String>(); String conCat=""(""; try{ String querystr = searchString; Query query = new QueryParser(""lucDescription"" new StandardAnalyzer()).parse(querystr); IndexSearcher searchIndex = new IndexSearcher(""/home/athreya/docsIndexFile""); // Index of the User table--> /home/araghu/aditya/indexFile. Hits hits = searchIndex.search(query); System.out.println(""Found "" + hits.length() + "" hits.""); for(int iterator=0;iterator<hits.length();iterator++) { String primKey=hits.doc(iterator).get(""primarykey""); System.out.println(primKey); pkList.add(primKey); } searchIndex.close(); Thank you in advance Athreya Hard to say what could be wrong just looking at the code couple of things to try: open the index with Luke and see the score for the document (containing ""chris"") Unsure if you are bypassing one or the other setboost calls. if (specialString.equals(""chris"")) doc.setBoost ((float)(100000.1)); else doc.setBoost ((float)(0.00001)); Ok thanks for the reply if (specialString.equals(""chris"")) doc.setBoost ((float)(100000.1)); else doc.setBoost ((float)(0.00001));here I am trying here to nullify the boost if the doc is not found and maximise it if found so that the score for the doc if found will be high ok but your code above seems counterintuitive perhaps put the un-boost in an else statement. Also look at the index using Luke that would tell you a lot more about scoring."
552,A,"Teracotta and Hibernate Search Does anyone have experience with using Terracotta with Hibernate Search to satisfy application Queries? If so: What magnitude of ""object updates"" can it handle? (How's the performance) What kind of performance do the Queries have? Is it possible to use Terracotta Hibernate Search without even having a backing Database to satisfy all ""queries"" in Memory? Since people on the Hibernate forums keep referring to this post I feel in need to point out that while Ari's comments where correct at the beginning of 2009 we have been developing and improving a lot. Hibernate Search provides a set of backend channels out of the box like the already mentioned JMS based and a more recent addition using JGroups but we made it also pretty easy to plug in alternative implementations or override some. In addition to using a custom backend it's now possible since version 4 to replace the whole strategy and instead of changing the backend implementation only you can use an IndexManager which follows a different design and doesn't use a backend at all; at this time we have two IndexManagers only but we're working on more alternatives; again the idea is to provide nice implementations for the most common It does have an Infinispan based backend for very quick distribution of the index across different nodes and it should be straight forward to contribute one based on Terracotta or any other clustering technology. More solutions are coming.  I am Terracotta's CTO. I spent some time last month looking at Hibernate Search. It is not built in a way to be clustered transparently by Terracotta. Here's why in a nutshell: Hibernate has a custom-built JMS replication of Lucene indexes across JVMs. The basic idea in Search is that talking to local disk under lucene works really well whereas fragmenting or partitioning up Lucene indexes across the network introduces sooo much latency as to make Lucene seem bad when it is not Lucene's fault at all. To that end HIbernate Search doesn't rely on JBossCache or any in-memory partitioning / caching schemes and instead relies on JMS and each JVM's local disk in order to provide up-to-date indexing across a cluster with simultaneous low latency. Then the beauty of Hibernate Search is that standard Hibernate queries and more can be launch through Hibernate at these natural language indexes in each machine. At Terracotta it turns out we had a similar idea to Emmanuel and built a SearchableMap product on top of Compass. Each machine gets its own Compass store and the store is configured to spill to disk locally. Terracotta is used to create a multi-master writing capability where any JVM can add to the index and the delta is sent through Terracotta to be replayed / reapplied locally to each disk. It works just like Hibernate Search but with DSO as the networking protocol in place of JMS and w/o the nice Hibernate interfaces but instead with Compass interfaces. I think we will support Hibernate Search w/ help from JBoss (they would need to factor out the JMS impl as pluggable) by end of the year. Now to your questions directly: 1.Object updates/sec in Hibernate or SearchableMap should be quite high because both are sending only deltas. In Hibernate's case it is a function of our JMS provider. In Terracotta it is scalable just by adding more Terracotta Servers to the array. Query performance in both is very fast. Local memory performance in most cases. And if you need to page in from disk it turns out most OSes do a good job and can respond way faster than any network-based clustering can to queries. It will be I think once we get JBoss to factor out their JMS assumptions etc. Cheers --Ari"
553,A,"How to do a constant score query in Solr I'm using SolrNet to access a Solr index where I have a multivalue field called ""tags"". I want to perform the following pseudo-code query: (tags:stack)^10 OR (tags:over)^5 OR (tags:flow)^2 where the term ""stack"" is being boosted by 10 ""over"" is being boosted by 5 and ""flow"" is being boosted by 2. The result I'm after is that results with ""stack"" will appear higher than those with ""flow"" etc. The problem I'm having is that say ""flow"" only appears in a couple of documents but ""stack"" appears in loads then due to a high idf value documents with ""flow"" appear above those with ""stack"". When this was project was implemented straight in Lucene I used ConstantScoreQuery and these eliminated the idf based the score solely on the boost value. How can this be achieved with Solr and SolrNet where I'm effectivly just passing Solr a query string? If it can't is there an alternative way I can approach this problem? Thanks in advance! what if you take the score boost off your last clause? `(tags:flow)` I believe that is a constant score query. Effectively it's a score boost of 1 which is the default. I don't think there any way to directly express a ConstantScoreQuery in Solr but it seems that range and prefix queries use ConstantScoreQuery under the hood so you could try faking a range query e.g. tags:[flow TO flow] Alternatively you could implement your own Solr QueryParser. Faking a range query works a treat (for now at least). I may look at implementing a custom query parser if time allows. Thanks.  The taxonomy of your field is useless: in this example. I suggest you consult a library scientist. The example is a simplified case. In reality I have tags_* which is a dynamic multi value field that maps against a dictionary (key: tag type value list of tag values). E.g. tags_Interest and tags_Subject. So I effectively have a series of lists of tags which I can use for faceting and filtering. It's only a problem when in one particular use case I need to rank certain tags above others based on user preference hence the boosts. If you can see that this index structure is useless or flawed I would be grateful of any alternative suggestions.  Heliosearch (a Solr fork intended to be the successor to Solr) has this built into the query parser syntax via the ^= operator. So just take your original query: (tags:stack)^10 OR (tags:over)^5 OR (tags:flow)^2 And replace the ^ with ^= to change from boosted to constant: (tags:stack)^=10 OR (tags:over)^=5 OR (tags:flow)^=2 http://heliosearch.org/solr/query-syntax/"
554,A,"get all results with Dismax like q=*:*? Is not possible to do this like q=*:* with DisMax ? first thanks for you answers. Actually i found the solution that fit my need and it's :  < str name=""q.alt"">*:*< /str> now i can list all the results . ;)  It is not possible to get all results with dismax using the *:* query. To get the count of documents with this query you have to use the standard query handler. You can switch to it by adding qt=standard in your query. Note that 'standard' is the default name so check in your solrconfig.xml if it is the name you are really using. Thanks it is a good idea. I tried it out and you are right *:* doesn't seem to play nice with Dismax. HOwever instead of doing qt=standard and therefore pointing to a completely different requestHandler instead do defType=lucene and reuse the same requestHandler just overtide the use of dismax."
555,A,"How can I order the list in LuceneSearch according to number of hits I am using Lucene Search to get the articles that are matching the search text. Is there any way to get them in ascending order of number of hits in the Article. Example: If my search text is stack and in first Article there are two occurrences of the word stack and in the second Article there are three occurrences of stack then the second one should come first and the first one should come second. Any idea how can I get it done? Below is the code that I am using List<LuceneSearchResult> searchResult = new List<LuceneSearchResult>(); LuceneSearchResult result; IndexReader reader = IndexReader.Open(INDEX_DIR); Searcher searcher = new IndexSearcher(reader); Analyzer analyzer = new StandardAnalyzer(); QueryParser parser = new QueryParser(""Text"" analyzer); //Text and Type are column name Query q = parser.Parse(string.Format(""Text:{0} AND Type:{1}"" finalText type)); Hits hs = searcher.Search(q); ArrayList idList = new ArrayList(); for (int i = 0; i < hs.Length(); i++) { Document doc = hs.Doc(i); result = new LuceneSearchResult(); result.ID = doc.Get(""ID""); result.Type = doc.Get(""Type""); if (!idList.Contains(result.ID)) { searchResult.Add(result); idList.Add(result.ID); } } return searchResult.ToArray(); Lucene ranks documents by score. There are several components to the score for a document for a given query. One of them is the frequency of the term in the field queried. However for a search on a single term the calculation is pretty simple. It's proportional to the square root of the number of occurrences of the term in the field normalized by field length. This could be where you are running into trouble. If you search for the word ""stack"" and doc A has 1 occurrences and doc B has 2 occurrences doc A could still rank higher in the results if the field length is significantly greater than that of doc B. The good news is you can disable field normalization. The bad news is that you need to do it before you index unless you over the Similarity class to always factor it out but I wouldn't recommend doing it this way. To disable norms at index time in your indexing code call Field.setOmitNorms(true) on the Field object you add to the IndexWriter. In your case this would be for the ""text"" field. Hi KenE this sounds great but where do I implement Field.setOmitNorms(true) ?? You would call it in your indexing code.  Lucene should do this automatically but it depends in some part on how you formulate your query. By default if you do a query with more than one word then those are ORd together. For example say your query was something like this (searching the contents field): contents:apples oranges This would return any pages with the term apples OR oranges in it. If a page contains the word ""apples"" 50 times but no reference to orange that page would still rank higher than a page that just contained the word ""apples"" once and ""oranges"" once. What you probably want to do is AND your query like this: contents:apples AND oranges Note: uppercase AND This will only return pages that have both the word ""apples"" AND ""oranges"" in it which is probably nearer to what you want. Have a read of Lucene - Query Parser Syntax for more info on how to forumulate queries  I have googled around and found that Lucene lists the search result in the order of score of the hitswhich is not the phenomenon of number of occurence of the phrase but is calculated depending on various factors and therefore I think it will not be possible to get it from Lucene straight but if you find some way please let me know.  On first sight your code looks like it should function as expected. Could you show us an example of a finalText type and the results? When I get unexpected results I usually check what query was actually used (in debug mode check the value of q) and use that query in Luke to see what results it gives. In my code I usually use hits.Max instead of hits.Length. Don't know what the difference is but it's something I noted. Also as a side note unless the rest of your program dictates you otherwise you might want to check out the HashTable instead of a ArrayList for your IdList it's usually faster.  I agree with Dan that this should be Lucene's default behavior. If your implementation does not behave this way please add details so we can help you diagnose why. Lucene's Similarity class documentation explains the details of Lucene scoring which is responsible for the order of the hits. I am using AND in the query please see above I have included the code that I am using"
556,A,What are the current java search options like Hibernate Search or Compass? With Compass going the way of the dodo (or at least no longer being actively developed) I wonder what other technologies there are that fill a similar role. I'm aware of Hibernate Search but nothing else really. It seems the direction things are going is towards full indexing agnostic of entities and relationships. Are there other technologies that are worth looking into? Are there benefits to using something more agnostic like Solr? Solr is awesome. I don't know your exact use case but solr will probably handle it. I think Solr's DataImportHandler may fit the bill if you want incremental indexing of database entities.  I never ended up finding any better in-app solutions other than Compass and Hibernate Search. We implemented search with Compass. In retrospect I find it hard to get answers to my questions and while it works respectably well I can't help but think that in-app searching is not the way to go. While it's not necessarily a great idea to muck up your environment with several interconnecting applications from the in-app search land the grass is certainly greener looking over in the Solr pasture (and the ElasticSearch one for that matter).  There is always solrj http://wiki.apache.org/solr/Solrj
557,A,Zend Search Lucene floating point numbers range search I have problem with Zend Search Lucene when searching float numbers. The problem is that when I execute query like avg:[0.15 TO 0.30] I get error message Range query boundary terms must be non-multiple word terms. For parsing the query I use the default parser. I a little desperate now because searching float numbers is main aim of my application. Tested with Luke and the index looks ok and everything works fine. Does anybody have an idea? Don't use floating numbers for searching within a range of values. Transform the floating number in strings without the decimal point. Your question is very similar to another I've answered few weeks ago. this is really old one now :-) I have found the solution and forgot to post it here.. I will do that ASAP. See my above answer.  For anyone trying to solve similar problem. Transform your numbers to string WITHOUT decimal point. You can use regexp that I use and is provided here. Php format numbers in a string with regexp
558,A,"Can I search Solr documents by member of a multi-value field? I have a set of Solr documents containing (among other fields) multi-value fields with percentage data or -1 if the value is null e.g. <doc> ... <arr name=""alpha""> <float>0.23</float> <float>0.23</float> <float>0.43</float> </arr> <arr name=""beta""> <float>0.52</float> <float>-1.0</float> <float>0.34</float> </arr> <arr name=""gamma""> <float>-1.0</float> <float>-1.0</float> <float>-1.0</float> </arr> ... </doc> I need to find documents where a multi-value field contains or doesn't contain a certain member for a complete set of test cases. If I can get either of the queries below to work it would be a tremendous help to locate a particular document out of several hundred thousand: 1) Can I find a document where none of the members of a specific multi-value field meet a certain criterion? (The above doc would be returned if I queried for ""alpha has no members matching -1"".) 2) Can I find a document where at least one of the members of a specific multi-value field meets a certain criterion? (The above doc would be returned if I queried for ""alpha has least one member > 0"" or ""beta has at least one member > 0"".) I'm assuming that a query like alpha:[0 TO 1] doesn't work because the field is an array instead of a scalar. A definitive answer of ""This is impossible"" is just as useful as an answer of ""Here's how you do it"" -- thanks in advance. EDIT: As with so many problems the answer is ""recheck your assumptions"" -- specifically the developer who generated our documents turned off indexing on the percentage fields. It is certainly possible. I usually use the FQ (filter query) parameter to get what you want: http://wiki.apache.org/solr/CommonQueryParameters#fq But you can just throw it on the query as well. Solution for #1: fq=-alpha:-1.0 Filters out anything that has alpha equal to -1.0 I am not sure about solution #2. Have you tried the code you mentioned? fq=beta:[0.0 TO 1.0] I don't have a good sample dataset to test on. Thanks for your input.  Yes. -alpha:""-1.0"" achieves this. Your own example alpha:[0 TO 1] is the solution. To put simply why this works: Each field is not a value or an array but rather a vector of terms. Querying a field for a certain term is a request for inclusion (or exclusion) not an equality operation. The array you are referring to is a part of the result set which is plain stored data that is returned by Solr as part of the search results. Thanks for the explanation. Unfortunately even the correct query is useless to me in my current situation but at least now I've figured out why -- see the edit. Thank you this helped me a lot! :)"
559,A,Solr or Nhibernate Search Bit confused here How’s Solr or Solrnet any different from Nhibernate Search? Does Solr offer anything more to Lucene.net that Nhibernate Search? I explained the differences and relationships between these projects in this blog post. In a nutshell: while Lucene(.net) is a library Solr is a stand-alone Java application that uses Lucene to provide full-text indexing and searching through a XML/HTTP interface. This means that it can be used from any platform/language. While very flexible it's easier to use than raw Lucene and provides features commonly used in search applications like faceted search and hit highlighting. It also handles caching replication sharding and has a nice web admin interface. None of those features are directly provided by Lucene.net / NHibernate.Search. SolrNet is a client to communicate with Solr from a .net application.
560,A,"Recommended way to perform Lucene search without limit The Lucene documents tell me that ""Hits"" will be removed from the API in Lucene 3.0. Deprecated. Hits will be removed in Lucene 3.0. Use search(Query Filter int) instead. The proposed overload limits the number of documents returned to the value of the int. So my question is: what is the recommended way to perform a search in Lucene with no limit on the number of documents to be returned? The highest integer in Java is pretty darned high you could use Integer.MAX_VALUE for the limit. I bet something else breaks before you actually hit the limit of 2^31-1 (2147483647) documents. :-) Alternately you can use a HitCollector: search(Query query HitCollector results) or search(Query query Filter filter HitCollector results); the docs say: Applications should only use this if they need all of the matching documents Setting the topcount to a high enough value indeed is the easiest answer. If I really would have needed everything then the custom HitCollector would have been the ideal solution. But i'd rather just use the high-level search api. @Thomas: Yeah that makes the most sense to me as well. :-)"
561,A,"Luke Lucene BooleanQuery In Luke the following search expression returns 23 results: docurl:www.siteurl.com docfile:Tomatoes* If I pass this same expression into my C# Lucene.NET app with the following implementation:  IndexReader reader = IndexReader.Open(indexName); Searcher searcher = new IndexSearcher(reader); try { QueryParser parser = new QueryParser(""docurl"" new StandardAnalyzer()); BooleanQuery bquery = new BooleanQuery(); Query parsedQuery = parser.Parse(query); bquery.Add(parsedQuery Lucene.Net.Search.BooleanClause.Occur.MUST); int _max = searcher.MaxDoc(); BooleanQuery.SetMaxClauseCount(Int32.MaxValue); TopDocs hits = searcher.Search(parsedQuery _max) ... } I get 0 results Luke is using StandardAnalyzer and this is what the Explain Structure window looks like: Must I manually create BooleanClause objects for each field I search on specifying Should for each one then add them to the BooleanQuery object with .Add()? I thought the QueryParser would do this for me. What am I missing? Edit: Simplifying a tad docfile:Tomatoes* returns 23 docs in Luke yet 0 in my app. Per Gene's suggestion I've changed from MUST to SHOULD:  QueryParser parser = new QueryParser(""docurl"" new StandardAnalyzer()); BooleanQuery bquery = new BooleanQuery(); Query parsedQuery = parser.Parse(query); bquery.Add(parsedQuery Lucene.Net.Search.BooleanClause.Occur.SHOULD); int _max = searcher.MaxDoc(); BooleanQuery.SetMaxClauseCount(Int32.MaxValue); TopDocs hits = searcher.Search(parsedQuery _max); parsedQuery is simply docfile:tomatoes* Edit2: I think I've finally gotten to the root problem:  QueryParser parser = new QueryParser(""docurl"" new StandardAnalyzer()); Query parsedQuery = parser.Parse(query); In the second line query is ""docfile:Tomatoes*"" but parsedQuery is {docfile:tomatoes*}. Notice the difference? Lower case 't' in the parsed query. I never noticed this before. If I change the value in the IDE to 'T' 23 results return. I've verified that StandardAnalyzer is being used when indexing and reading the index. How do I force queryParser to keep the case of the value of query? Edit3: Wow how frustrating. According to the documentation I can accomplish this with: parser.setLowercaseExpandedTerms(false); Whether terms of wildcard prefix fuzzy and range queries are to be automatically lower-cased or not. Default is true. I won't argue whether that's a sensible default or not. I suppose SimpleAnalyzer should have been used to lowercase everything in and out of the index. The frustrating part is at least with the version I'm using Luke defaults the other way! At least I learned a bit more about Lucene. I would assume query is the string he is searching on: docurl:www.siteurl.com docfile:Tomatoes* What's an example value of the variable ""query""? Also what is the purpose of ""bquery""? QueryParser will indeed take a query like ""docurl:www.siteurl.com docfile:Tomatoes*"" and build a proper query out of it (boolean query range query etc.) depending on the query given (see query syntax). Your first step should be to attach a debugger and inspect the value and type of parsedQuery. What is the type of parsedQuery? It should be a boolean query and be composed of two other queries (matching the query structure you see in Luke). Two questions: 1 - what it is the purpose of bquery (it doesn't seem to be used at all)? 2 - Do you get the same results if you use another different search method (one of the other overloads)? The reason for using `bquery` is to specify more than one search term which is when the MUST/SHOULD distinction will come into play. You're right about bquery I never noticed it isn't used. I didn't write this code. Looking into this now. The default `BooleanClause` for `QueryParser` is SHOULD so that's not likely the problem. Without more information I can't really say why it's not working. All I can suggest is to try different queries try an index with only a few documents in it. Just try to control the environment to so you can do some experiments. `bquery` is not used as an argument to the `IndexSearcher` though. You both helped but Ryan gave more input. Points to him. parsedQuery = docurl:www.toledoblade.com docfile:tomatoes*. So it looks like the QueryParser is doing it's thing. Changing the last line to TopDocs hits = searcher.Search(bquery _max); doesn't seem to have helped either. Still 0 hits. Right I don't think `bquery` is necessary here at all. Sorry I can see how my last comment might have suggested that. The `QueryParser` will take your query expression (`docurl:www.toledoblade.com docfile:tomatoes*`) and turn it into the proper query structure (a `BooleanQuery` composed of a `TermQuery` and `PrefixQuery`). I would suggest digging into `parsedQuery` in the debugger and looking to see what it is composed of. There should be a queries collection/enumerable/array inside of `parsedQuery`. Got it I see both the TermQuery and PrefixQuery in the clauses collection. Both have an occur member that seems to be blank {} of type Lucene.Net.Search.BooleanClause.Occur. If I prefix both field names with '+' then the value for the occur variable for both is {+} which based on Gene's comment seems to force MUST. Is there any way to add something in the query expression to force a SHOULD? If not do you have any suggestions for how else I can default to SHOULD in my parsedQuery BooleanQuery object?  Using Occur.MUST is equivalent to using the + operator with the standard query parser. Thus you code is evaluating +docurl:www.siteurl.com +docfile:Tomatoes* rather than the expression you typed into Luke. To get that behavior try Occur.SHOULD when adding your clauses. try searching on `bquery' rather than on `parsedQuery` and as @ryan mentioned below see what the class of `parsedQuery` is. Stepping through the code in the debugger is often helpful. Changing my search expression in Luke to +docurl:www.toledoblade.com +docfile:Tomatoes* does indeed return 0 docs. However changing my clause to SHOULD doesn't seem to have the reverse effect. bquery.Add(parsedQuery Lucene.Net.Search.BooleanClause.Occur.SHOULD);. hits.totalHits is still 0."
562,A,"Lucene IndexReader.reopen doesn't seem to work correctly I have a problem with Lucene 2.4 the situation being as follows: I have to deal with the possibility that there are 2 seperate processes operating on the same Index directory and they need to have the same data. This means that when one Instance adds a Document to the Index the other application instances shall find the added Documents on their next search. According to the Lucene Documentation IndexReader.reopen is what I need. So I invented the following testcase: package de.samedi.searcher; import static org.junit.Assert.assertEquals; import static org.junit.Assert.fail; import java.io.IOException; import org.apache.lucene.analysis.standard.StandardAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.index.CorruptIndexException; import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.queryParser.QueryParser; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.Query; import org.apache.lucene.search.TopDocs; import org.apache.lucene.store.FSDirectory; import org.junit.Test; public class LuceneReload { private IndexSearcher searcher1; private IndexSearcher searcher2; private FSDirectory directory1 directory2; private IndexWriter writer1 writer2; @Test public void testReload() throws Exception { String home = System.getProperty(""user.home""); this.directory1 = FSDirectory.getDirectory(home + ""/testIndex""); this.directory2 = FSDirectory.getDirectory(home + ""/testIndex""); this.writer1 = new IndexWriter(this.directory1 new StandardAnalyzer() true IndexWriter.MaxFieldLength.LIMITED); this.writer2 = new IndexWriter(this.directory2 new StandardAnalyzer() true IndexWriter.MaxFieldLength.LIMITED); // assert that we're empty assertFound(getSearcher1() ""test"" 0); assertFound(getSearcher2() ""test"" 0); add(this.writer1 ""test""); assertFound(getSearcher1() ""test"" 1); assertFound(getSearcher2() ""test"" 1); add(this.writer2 ""foobar""); assertFound(getSearcher1() ""foobar"" 1); assertFound(getSearcher2() ""foobar"" 1); } public void assertFound(IndexSearcher searcher String q int expected_number) { try { QueryParser parser = new QueryParser(""name"" new StandardAnalyzer()); Query query = parser.parse(q); TopDocs t = searcher.search(query null 50); assertEquals(expected_number t.totalHits); } catch (Exception e) { e.printStackTrace(); fail(); } } public IndexSearcher getSearcher1() throws CorruptIndexException IOException { if (this.searcher1 == null) { this.searcher1 = new IndexSearcher(IndexReader.open(this.directory1)); } else { IndexReader new_reader old_reader; old_reader = this.searcher1.getIndexReader(); new_reader = old_reader.reopen(); if (new_reader != old_reader) { System.err.println(""index1 changed""); this.searcher1.close(); old_reader.close(); this.searcher1 = new IndexSearcher(new_reader); } } return this.searcher1; } public IndexSearcher getSearcher2() throws CorruptIndexException IOException { if (this.searcher2 == null) { this.searcher2 = new IndexSearcher(this.directory2); } else { IndexReader new_reader old_reader; old_reader = this.searcher2.getIndexReader(); new_reader = old_reader.reopen(); if (new_reader != old_reader) { System.err.println(""index2 changed""); this.searcher2.close(); old_reader.close(); this.searcher2 = new IndexSearcher(new_reader); } } return this.searcher2; } public void add(IndexWriter writer String name) throws CorruptIndexException IOException { Document d = new Document(); d.add(new Field(""name"" name Field.Store.YES Field.Index.ANALYZED)); writer.addDocument(d); writer.commit(); IndexWriter.unlock(writer.getDirectory()); } } When I instead of the reopen() calls use new_reader = IndexReader.open(this.directory1); the tests go green. Did I miss any important points see the IndexWriter#unlock javadoc:  /** * Forcibly unlocks the index in the named directory. * <P> * Caution: this should only be used by failure recovery code * when it is known that no other process nor thread is in fact * currently accessing this index. */ I wouldn't use that for normal operations. Instead open a new writer and close it. That will work correctly - though its best to use only a single IndexWriter. ahhhhhh yesss :) that works now. heres the updated and working code for anyone who cares: http://gist.github.com/173978"
563,A,"Lucene QueryParser ignores search term ""BE"" I'm trying to search a couple of fields and it works fine unless I use the term ""BE"". In this case lucene simply ignores the field. If I do something like what is shown below I get the correct results and the ""query"" object is shown as ""+flag:bf +type:cgo"". If I set either of the flag or the type terms to be ""BE"" that part of the search will be ignored. For example if I set the queryString to ""flag:\""BE\"" AND type:\""CGO\"""" the query object will be shown as: ""+type:cgo"" and I'll get a lot more hits. Same happens for ""type"" - If I change ""CGO"" in the last example to ""BE"" it will be ignored. I have not tried every possible 2 character combinations (but I've tried many) but all work as expected except this one. I'm not using any stop terms. Thanks Gene String queryString = ""flag:\""BF\"" AND type:\""CGO\""""; QueryParser qp = new QueryParser(Version.LUCENE_30 ""type"" new StandardAnalyzer(Version.LUCENE_30)); Query query = qp.parse(queryString); IndexSearcher searcher = new IndexSearcher(reader.reopen()); TopDocs td = searcher.search(q 5000); logger.info(""Found "" + td.totalHits + "" hits using "" + query.toString() ); By default the StandardAnalyzer uses a set of stop words to exclude ""noise"" from the indexed terms in text. I think that ""BE"" would normally be considered a stop word in the context of the StandardAnalyzer. Luckily you've got a few choices available to you The obvious one is to pass an empty set of stop words to the constructor of the StandardAnalyzer used. However looking at the names of your fields (""flag"" and ""type"") they don't exactly look like they're intended to contain straightforward text but more likely to contain coded words. With that in mind you might find the keyword analyzer is a better fit. Good luck Thanks. Using the keyword analyzer rather than the standard analyzer resolved this problem.  You are indeed using stopwords although you might not be trying to: QueryParser qp = new QueryParser(Version.LUCENE_30 ""type"" new StandardAnalyzer(Version.LUCENE_30)); StandardAnalyzer uses the standard English stopwords by default which includes ""be"". Yes you are correct. I did not realize that the Standard Analyzer as I was creating it was using stop words. Fixed it by using a keyword analyzer instead. Thanks."
564,A,What is the real difference between INDEX.TOKENIZER vs INDEX.ANALYZER in Lucene 2.9? With lucene 2.9.1 INDEX.TOKENIZED is deprecated. The documentation says it is just renamed to ANALYZER but I don't think the meaning has stayed the same. I have an existing app based on 2.3 and I'm upgrading to 2.9 but the expected behavior seems to have changed. Anyone know any more details about INDEX.TOKENIZER vs INDEX.ANALYZER? I assume you refer to the Field.Index fields ANALYZED and TOKENIZED? It is true that the TOKENIZED Field has been deprecated. This was the case already with the 2.4. The Field.Index.ANALYZED is equal to the old Field.Index.TOKENIZED. Could you show how your results deviate from the behaviour you expect? thanks for the answer was so long ago now I don't have access to the code any more sorry for such a long delay in responding
565,A,"Full-text search for static HTML files on CD-Rom via javascript I will be delivering a set of static HTML pages on CD-Rom; these pages need to be fully viewable with no Internet access whatsoever. I'd like to provide a full-text search (Lucene-like) for the content of those pages which should ""just work"" from the CD-Rom with no software installation on the client machine. A search engine implementation in javascript would be the perfect solution but I have trouble finding any that looks solid / current / popular...? I did find these: + jsFind + js-search but both projects seem rather inactive? Another solution besides a specific search engine in javascript would be the ability to access local Lucene indices from javascript: the indices themselves would be built with Lucene and copied to the CD-Rom along with the HTML files. Edit: built it myself (see below). I know a lot of people use Java to write CD search applets. I have a slightly elderly list of various free and commercial programs at Search Tools for CD-ROMs and DVDs.  Fullproof is a nifty little javascript library that can act as a text search for you. It would be useful in this context but it's also useful in the ""thick-javascript-webpage"" model.  Well in fact I built it myself. The existing solutions (that I could find) were unconvincing. I wanted to be able to search a very long tree (ul/li/ul...) that is displayed as one page; it contains 5000+ items. It sounds a little weird to display such a long tree on one page but in fact with collapse / expand it's much more intuitive than separate pages and since we're offline download times are not a problem (parsing times are though but Chrome is amazing ;-) The ""search"" function provided with modern browsers (FF and Chrome anyway) have two big problems: they only search visible items on the page and they can't search non-consecutive words. I want to be able to search collapsed items (not visible on the screen); I want to find ""one two three"" when searching ""one three"" (just like with Google / Lucene); and I want to open just the branches of the tree containing found items. So what I did was: create an inverted index of words <-> ids of items from the list (via xslt) (approx. 4500 unique words in the document) convert this index to bunch of javascript arrays (one word = one array containing ids) when searching intersect the arrays represented by the search words step 3 returns an array of ids that I can then open / highlight It does exactly what I needed and it's really fast. Better yet since it searches from an independant ""index"" (arrays of ids) it can search when the list is not even loaded in the browser! Thanks for reporting back! Are there any examples of this we could look at?  Have a look at CLucene - http://sourceforge.net/projects/clucene http://clucene.git.sourceforge.net/git/gitweb.cgi?p=clucene/clucene;a=summary Compiling the C++ sources into a console or a Win32 executable would make the above possible also using the Lucene technology (which I assume you'd rather want to stick with).  Zoom Search Engine can do this. I haven't used the CD version but I use the PHP version for my website and it works very well. I did look at that thanks but it seemed quite complex to adapt to my specific needs.  Initial question was asked in '09 As of '14 there is http://lunrjs.com/ // https://github.com/olivernn/lunr.js (1400+ stars) Simple full-text search in your browser"
566,A,Recommendations for a spidering tool to use with Lucene or Solr? What is a good crawler (spider) to use against HTML and XML documents (local or web-based) and that works well in the Lucene / Solr solution space? Could be Java-based but does not have to be. I suggest you to check out Nutch to get some inspiration: Nutch is open source web-search software. It builds on Lucene Java adding web-specifics such as a crawler a link-graph database parsers for HTML and other document formats etc.  In my opinion this is a pretty significant hole which is keeping down the widespread adoption of Solr. The new DataImportHandler is a good first step to import structured data but there is not a good document ingestion pipeline for Solr. Nutch does work but the integration between Nutch crawler and Solr is somewhat clumsy. I've tried every open-source crawler that I can find and none of them integrates out-of-the-box with Solr. Keep an eye on OpenPipeline and Apache Tika.  Nutch might be your closest match but it's not too flexible. If you need something more you will have to pretty much hack your own crawler. It's not as bad as it sounds every language has web libraries so you just need to connect some task queue manager with HTTP downloader and HTML parser it's not really that much work. You can most likely get away with a single box as crawling is mostly bandwidth-intentive not CPU-intensive.  Also check Apache Droids [http://incubator.apache.org/droids/] -- this hopes not be a simple spider/crawler/worker framework. It is new and is not yet easy to use off the shelf (it will take some tweeking to get running) but is a good thing to keep your eye on.  I've tried nutch but it was very difficult to integrate with Solr. I would take a look at Heritrix. It has an extensive plugin system to make it easy to integrate with Solr and it is much much faster at crawling. It makes extensive use of threads to speed up the process.  Did anyone tried Xapian? It seams much quicker than solr and written in c++.  http://arachnode.net C# but produces Lucene (Java and C#) consumable index files.
567,A,"Solr Query Syntax I just got started looking at using Solr as my search web service. I don't know whether Solr supports these query types: Startswith Exact Match Contain Doesn't Contain In the range Could anyone guide me how to implement those features in Solr? Cheers Samnang Samnang please try the SolrQuerySyntax page in the Solr Wiki.  Solr is capable of all those things but to adequately explain how to do each of time an answer would become a mini-manual for Solr. I'd suggest you read the actual manual and tutorials linked from the Solr homepage. In short though: Startswith can be implemented using Lucene wildcards. Exact matches will only be found if a field is not tokanized. I.e. the entire field is viewed as a single token. Contain is the default search format. I.e. a search for ""John"" will find any document's whose search field contains the value ""John"". Prefixing with - (e.g. ""-John"" will only find documents that do not contain John). Ranges (be they date or integer) are possible and quite powerful example date:[* TO NOW] would find any document whose date is not in the future.  You can also find interesting infos on Solr query syntax here: http://www.solrtutorial.com/solr-query-syntax.html"
568,A,Lucene gotchas with punctuation Whilst building some unit tests for my Lucene queries I noticed some strange behavior related to punctuation in particular around parentheses. What are some of the best ways to deal with search fields that contain significant amounts of punctuation? If you haven't customized the query parser Lucene should behave according to the default query parser syntax. Are you getting something different than that? Do you want punctuation to have a special meaning or just to remove the punctuation from searches? The other usual suspect here is the Analyzer which determines how your field is indexed and how the query is broken into pieces for searching. Can you post specific examples of bad behavior? Thanks for your response. I have moved forward with this by having a 'clean' field on my document that is purely for the purposes of searching. This forces me to also 'clean' all search query strings. Seems to work well and I return the full field as the result from the query.  It is not not just parentheses other punctuations such as the colon hyphen etc. will cause issues. Here is a way to deal with them.
569,A,"Hive with Lucene Is it possible to use Hive for querying Lucene index which is distributed over Hadoop??? You could write a custom input format for Hive to access lucene index in Hadoop.  As far as I know you can essentially write custom ""row-extraction"" code in Hive so I would guess that you could. I've never used Lucene and barely used Hive so I can't be sure. If you find a more conclusive answer to your question please post it!  I know this is a fairly old post but thought I could offer a better alternative. In your case instead of going through the hassle of mapping your HDFS Lucene index to hive schema it's better to push them into pig because pig can read flat files. Unless you want a Relational way of storing your data you could probably process them through Pig and use Hbase as your DB.  Hadapt is a startup whose software bridges Hadoop with a SQL front-end (like Hive) and hybrid storage engines. They offer a archival text search capability that may meet your needs. Disclaimer: I work for Hadapt."
570,A,"Finding exact match using Lucene search API I'm working on a company search API using Lucene. My Lucene company index has got 2 companies: 1.Abigail Adams National Bancorp Inc. 2.National Bancorp If the user types in National Bancorp then only company # 2(ie. National Bancorp) should be returned and not #1.....ie. only exact matches should be returned. How do I achieve this functionality? Thanks for reading. Future searchers: if you're just searching a lucene-indexed service the answer by Somonath Sabat with no upvotes appears to be correct in at least one instance - put the phrase in double quotes. Verified against musicbrainz. You can use KeywordAnalyzer to index and search on this field. Keyword Analyzer will generate only one token for the entire string. Can you please answer this one? http://stackoverflow.com/questions/899542/problem-using-same-instance-of-indexsearcher-for-multiple-requests  I googled a lot with no help for the same problem. After scratching my head for a while I found the solution. Search the string within double quotes that will solve your problem. National Bancorp will return both #1 and #2 but ""National Bancorp"" will return only #2.  This is something that may warrant the use of the shingle filter. This filter groups multiple words together. For example Abigail Adams National Bancorp with a ShingleFilter of 3 tokens would produce (assuming a simple WhitespaceAnalyzer) [Abigail] [Abigail Adams] [Abigail Adams National] [Adams National Bancorp] [Adams National] [Adams] [National] [National Bancorp] and [Bancorp]. If a user the queries for National Bancorp you will get an exact match on National Bancorp itself and a lower scored exact match on Abigail Adams National Bancorp (lower scored because this one has much more tokens in the field thus lowering the idf). I think it makes sense to return both documents on such a query. You may want to apply the shingle filter at query time as well depending on the use case.  You may want to reconsider your requirements depending on whether or not I correctly understood your question. Please bare with me if I did misunderstand you. Just a little food for thought: If you only want exact matches returned then why are you searching in the first place? Are you sure that the user expects exact matches? I typically search assuming that the search engine will accommodate missing words. Suppose the user searched for National Bank but National Bank was no longer in your index. Would you still want Abigail Adams National Bancorp Inc to be excluded from the results simply because it was not an exact match? In light of this I would suggest you continue to present all possible matches (exact or not) to the user and let them decide for themselves which is most appropriate for them. I say this simply because you may not be thinking the same way as all of your users. Lucene will take care of making sure the closest matches rank highest in the results helping them make quicker choices."
571,A,"Lucene fulltext query spellchecking We have homegrown search engine based on Lucene and I'm intrested in the algorithms and tools for spellchecking (""Did you mean"" feature). Does Lucene contains some examples of such a functionality or maybe other open source community does? Yes there is SpellChecker that you can use. It requires building a dictionary of your words based on which suggestions are matched. Here is a related post ""Did you mean?"" feature in Lucene.net"
572,A,"Lucene: Filtering for documents NOT containing a Term I have an index whose documents have two fields (actually more like 800 fields but the other fields won't concern us here): The contents field contains the analyzed/tokenized text of the document. The query string is searched for in this field. The category field contains the single category identifier of the document. There are about 2500 different categories and a document may occur in several of them (i.e. a document may have multiple category entries. The results are filtered by this field. The index contains about 20 mio. documents and is 5 GB in size. The index is queried with a user-provided query string plus an optional set of a few categories the user is not interested in. The question is: how can I remove those documents matching not only the query string but also the unwanted categories. I could use a BooleanQuery with a MUST_NOT clause i.e. something like this: BooleanQuery q = new BooleanQuery(); q.add(contentQuery BooleanClause.MUST); for (String unwanted: unwantedCategories) { q.add(new TermsQuery(new Term(""category"" unwanted) BooleanClause.MUST_NOT); } Is there a way to do this with Lucene filters? Performance is an issue here and there will only be a few recurring variants of unwantedCategories so a CachingWrapperFilter would probably help a lot. Also due to the way the Lucene queries are generated in the existing code base it is difficult to fit this in whereas an extra Filter could be introduced easily. In other words How do I create a Filter based on what terms must _not_ occur in a document? @ajreal: I'm not using the Lucene analyzer; which AFAIK generates queries anyway not filters. You can use a QueryWrapperFilter to turn an arbitrary query into a filter. And you can use a CachingWrapperFilter to cache any filter. So something like: BooleanQuery bq = new BooleanQuery(); // set up bq Filter myFilter = new CachingWrapperFilter ( new QueryWrapperFilter (bq) );  One word answer: BooleanFilter found it minutes after formulating the question: BooleanFilter f = new BooleanFilter(); for (String unwanted: unwantedCategories) { TermsFilter tf = new TermsFilter(new Term(""category"" unwanted)); f.add(new FilterClause(tf BooleanClause.MUST_NOT)); } I am having similar issue. My filter is not working at all for TextValues. Filter LEA = new QueryWrapperFilter(new TermQuery(new Term(field ""0401000""))); I have created a question on Stack Overflow it will be great if yu can adderss it http://stackoverflow.com/questions/16906689/filter-not-working-with-text-values-lucene-3-0-3"
573,A,Files count aftet MassIndexer is HUGE for hibernate-search 3.4.0 I use MassIndexer to index. After migrate to Hibernate-search 3.4 from 3.2.1.Final count of files is really really huge (with .cfs extension). Before it was OK. And the same time migrate onto lucene-core 3.1.0 Please could somebody explain why it happened?  MassIndexer massIndexe = fullTextSession.createIndexer(SearchLuceneDocument.class); massIndexe.purgeAllOnStart(true) // true by default highly recommended .optimizeAfterPurge(true) // true is default saves some disk space .optimizeOnFinish(true) // true by default .batchSizeToLoadObjects(100) .threadsForSubsequentFetching(15) .threadsToLoadObjects(10) .threadsForIndexWriter(4) .cacheMode(CacheMode.IGNORE) // defaults to CacheMode.IGNORE .startAndWait(); Tanks to advance! Artem what filesystem are you on? It's known that some NFS leak file descriptors; in fact that's why we propose different alternatives for clustering - none of them involves NFS. I'm not aware of bugs of us not closing the files during massindexer but if you could contribute a test highlighting the error I'd be glad to look into it; post it on JIRA or on the hibernate forums in the Search sections. thanks Ok. I will do post. thanks
574,A,"Combining Numeric Range Query with Term Query in Lucene I would like to combine a numeric range query with a term query in Lucene. For example I want to search for documents that I have indexed that contain between 10 and 20 pages and have the title ""Hello World"". It does not seem possibly to use the QueryParser to generate this query for me; the range query that the QueryParser generates appears to be a text one. I definitely would appreciate an example of how to combine a numeric range query with a term query. I would also be open taking an alternative to searching my index. Thanks Well it looks like I figured this one out on my own. You can use Query.combine() to OR queries together. I have included an example below. String termQueryString = ""title:\""hello world\""""; Query termQuery = parser.parse(termQueryString); Query pageQueryRange = NumericRangeQuery.newIntRange(""page_count"" 10 20 true true); Query query = termQuery.combine(new Query[]{termQuery pageQueryRange});  RangeQuery amountQuery = new RangeQuery(lowerTerm upperTerm true); Lucene treats numbers as words so the numbers are ordered alphabetically. 1 12 123 1234 etc. That being said you can still use the range query you just need to be more clever about it. In order to query numeric values correctly you need to pad your integers so the same lengths (whatever your maximum supported value is) 0001 0012 0123 1234 Obviously this doesn't work for negative numbers (since -2 < -1) and hopefully you won't have to deal with them. Here's a useful article for negatives if you do encounter them: http://wiki.apache.org/lucene-java/SearchNumericalFields  You may use BooleanQuery: var combinedQuery = new BooleanQuery(); combinedQuery.Add(new TermQuery(new Term(""title""""hello world""))Occur.MUST); combinedQuery.Add(NumericRangeQuery.newIntRange(""page_count"" 10 20 true true)Occur.MUST);  You can also create a custom QueryParser overriding protected Query getRangeQuery(...) method which should return NumericRangeQuery instance when ""page_count"" field is encountered. Like so... public class CustomQueryParser extends QueryParser { public CustomQueryParser(Version matchVersion String f Analyzer a) { super(matchVersion f a); } @Override protected Query getRangeQuery(final String field final String part1 final String part2 final boolean inclusive) throws ParseException { if (""page_count"".equals(field)) { return NumericRangeQuery.newIntRange(field Integer.parseInt(part1) Integer.parseInt(part2) inclusive inclusive); } // return default return super.getRangeQuery(field part1 part2 inclusive); } } Then use CustomQueryParser when parsing textual queries.. Like so... ... final QueryParser parser = new CustomQueryParser(Version.LUCENE_35 ""some_default_field"" new StandardAnalyzer(Version.LUCENE_35)); final Query q = parser.parse(""title:\""hello world\"" AND page_count:[10 TO 20]""); ... This all of course assumes that NumericField(...).setIntValue(...) was used when page_count values were added to documents"
575,A,Is Lucene a good choice for Key/Value HashMap? I am facing a problem. I am doing a mini web crawler. Right now is important to have an efficient HashMap. I just want key/value data structure with only inserts and lookups. I know Lucene can do the job just by having two fields: key and value; but is it efficient? Is there any other solutions more simple? Ps: It can be in PHP or Java but I would prefer PHP. Note: I need it to be persisted. And it will be open and closed several times. I need it to be persisted. SO I write it save it. I loaded it again append some more and save it. Open it and lookup for something. What is the key type? The value type? Do you need to search for words inside a larger text? Why not use a database? In java what is preventing you from using a plain `HashMap`? Please update your question so that it communicates that you need persistence. It's currently unclear. I've (ab)used solr as a key value store on a couple occasions with tens of millions of records. Also we have a index in production that includes a full copy of the indexed data in json format and we run queries that return this value so that we can avoid a redundant and much slower database lookup. So depending on your needs it is a quite OK solution but you need to be aware of the limitations. Pros. 1) If you are already using solr or lucene it is convenient to not have to use another technology. 2) Lucene is pretty good at lookups of single rows and should scale well for that purpose. 3) With a few extra columns you gain querying capability as well. Cons 1) Lucene is not designed as a transactional store. Typically you add multiple rows and then commit them. So writes are not atomic in the ACID sense. Usually that's a bad thing if you are storing important data. (near) real-time indexing is possible these days but it still requires a lot of fiddling to get right. 2) Because there is a delay between when you add and when you commit that means reading your own writes may be problematic. 3) If you need a lot of write throughput it is best to index in bulk. If you need to write individual keys one by one your throughput is going to suffer. 4) While lucene excels at querying large result sets are problematic. For example a query that produces all the keys of your values can get very expensive on a solr index with tens of millions of rows.  If all you want is a fast persistent key-value store for a non-enormous dataset Lucene probably isn't the best solution- Berkeley DB would be the obvious choice. That said Grant Ingersoll gave a talk at this year's Lucene Revolution conference about exactly this. He intentionally came at the question with a pro-Lucene bias and got into a back-and-forth with several audience members about what contemporary document databases (like CouchDB) provide that Lucene doesn't. For any non-huge dataset that might eventually need secondary indexes I think this is a great solution. Lucene's performance for key/value lookups won't be quite as fast Berkeley DB CouchDB Tokyo Tyrant or the like but it's still quite speedy more than adequate for many apps. I think he measured roughly 50ms for a key/value lookup on a recent laptop. And if later on you need to add secondary indexes (as it seems like you might on the results of a web crawl) you'll have a much easier time with Lucene than with those products. Other tools like BDB will be simpler to code for than Lucene. But if that's a concern just use Solr which makes it easy to add docs and search via simple HTTP calls (you'll want to modify the fields in the schema.xml config file but otherwise Solr should be ready-to-use out of the box). Now if your dataset is too big to reasonably fit on one machine distributed key-value stores like Project Voldemort or Riak might be easier to setup and administer. But Lucene will get you pretty far on one machine especially if you aren't indexing many fields- at least a TB I'd guess. If you do use Lucene I'd think hard about whether there truly aren't any properties other than the key you'd like to search by- might as well get them stored the first time since Lucene makes it easy.  You might want to look into Solr it is a best practice implementation of Lucene. It is a REST based interface and is pretty straight forward to setup and there is a PHP client that you can use. I looked at Solr. But i want a simple thing and I dont need REST calls. I just need in memory (still persistent) key/value structure.  You could have a look at document-oriented database such as Couchdb or MongoDB.  Lucene is the wrong tool for the job you describe. The simplest solution is a HashMap and it's fairly efficient. Is there any particular reason you think a HashMap would be a bad solution? If you need to scale out to a cluster I'd switch over to Memcached.
576,A,Solrnet /ASP.NET sample without MVC I am trying to get a handle on Solrnet and interacting an ASP.NET site with a Solr server. However the sample app (on the code repository) is MVC based does anyone know of a version in plain vanilla ASP.NET? Thanks Interaction between asp.net and Solr would be the same regardless of MVC or WebForms wouldn't it? Not necessarily AFAIK the webforms querying etc. model is different from MVC. @Mikos: Yes getting parameters from the page and passing them on will be different but I assume that's not the part of the interaction you're trying to figure out. There aren't any major differences really: Initialize the library in your Application_Start() just like in the MVC sample app. The simplest way to use it in a code-behind is to use the service locator to get the main SolrNet interface (e.g. var solr = ServiceLocator.Current.GetInstance<ISolrOperations<MyDocumentClass>>()) (in MVC it's easy to instead inject the interface using an IoC container) Then you can use that instance to run any query you want update documents etc. In the MVC sample app a ModelBinder is used to get the search parameters from the querystring but that's a MVC feature so getting the search parameters is up to you. Then bind the query results to the page (I mostly use a simple foreach you could also try ObjectDataSource) ok thanks will give that shot. Appreciate your help. @Mauricio Scheffer: It's not terribly difficult to do dependency injection in webforms as well you just can't (as far as I know) do constructor dependency injection. http://code.google.com/p/autofac/wiki/AspNetIntegration @r0manarmy: exactly; if you can't take control of instantiation you can only *hack around it*. Not the real thing.  This is little late. But for people still looking for Solrnet /ASP.NET sample without MVC can look at the following: http://crazorsharp.blogspot.com/2010/01/full-text-search-using-solr-lucene-and.html http://blog.dileno.com/archive/201009/get-started-using-solr-for-search-with-aspnet/
577,A,"Get term frequencies in Lucene Is there a fast and easy way of getting term frequencies from a Lucene index without doing it through the TermVectorFrequencies class since that takes an awful lot of time for large collections? What I mean is is there something like TermEnum which has not just the document frequency but term frequency as well? UPDATE: Using TermDocs is way too slow. Use TermDocs to get the term frequency for a given document. Like the document frequency you get the term documents from an IndexReader using the term of interest. You won't find a faster method than TermDocs without losing some generality. TermDocs reads directly from the "".frq"" file in an index segment where each term frequency is listed in document order. If that's ""too slow"" make sure that you've optimized your index to merge multiple segments into a single segment. Iterate over the documents in order (skips are alright but you can't jump back and forth in the document list efficiently). Your next step might be additional processing to create an even more specialized file structure that leaves out the SkipData. Personally I would look for a better algorithm to achieve my objective or provide better hardware—lots of memory either to hold a RAMDirectory or to give to the OS for use on its own file-caching system.  TermDocs gives the TF of a given term in each document that contains the term. You can get the DF by iterating through each <document frequency> pair and counting the number of pairs although TermEnums should be faster. IndexReader has a termDocs(Term) method that returns a TermDocs for the given Term and index. can this approach be used to determine term frequencies is a result set of a Lucene query? is it possible to use termDocs to get the PhraseFrequency ?  The trunk version of Lucene (to be 4.0 eventually) now exposes the totalTermFreq() for each term from the TermsEnum. This is the total number of times this term appeared in all content (but like docFreq does not take into account deletions). Using lucene 4.0 what is the equivalent of td.read(doc freq) where td is a TermDoc and doc and freq are int[] ?"
578,A,"Newbie to Lucene.net best aproach to complex queries? I'm building a website for learning pruposes and i'm looking at lucene.net as a full text indexer for my content but I have some questions. Lets say I have a hierarchy (n levels) of categories and articles that are assigned to one category (1 cat -> n articles). Using a simple RDB would be very easy to search for an article under a category or any of it's subcategories. But i'm struggling to imagine how i'd build this kind of query using lucene. Options I think that might work: Suposing that i'm idexing ""title text category"" for every article one option would be to first get a list with the id's of every subcategory from the DB and then search in lucene with that list. Other option would be to index the entire category ""path"" of the article inside a field in lucene. Something like ""title"" ""text"" ""catparent1 catparent2 catparent3 category"" ? What's the best aproach when doing this kind of query with complex relational filters? (not just text search) Add the category path as an indexed field and use a phrase search to search it: ID Title Categories ""MyDoc1"" ""Hello world!"" ""/programming/beginner/samples"" ""MyDoc2"" ""Prove that P=NP"" ""/programming/advanced/samples"" Now you can query the categories either hierarchically using a phrase search: ""/programming/beginner"" or not-hierarchically using a word search: ""samples"" I use this method for indexing files with their pathnames - you can query for ""dirname"" or ""parent/child"" or ""/root/parent/child"" and it all works nicely. You can control whether your search starts at the root by including or excluding the leading slash. In terms of ""complex relational filters"" you can then combine these category searches with other searches and filters using boolean queries."
579,A,"Search in huge table I got table with over 1 millions rows. This table represents user information e.g userName email gender marrial status etc. I'm going to write search over all rows in this table when some conditions are applied. In simples case when search is perfomed only on userName it takes over 4-7 seconds to find result. select from u where u.name ilike "" ... "" Yes i got indexes over some fileds. I checked that they are applied using explain analyse command. How search can be boost ? I heart something about Lucene can it help ? I'm wondering how does Facebook search working they got billions users and their search works much faster. actually Facebook has just over 500 million active users http://www.facebook.com/press/info.php?statistics What flavour of database are you using at the moment? I'd guess PostgreSQL from the use of ilike. yes PostgreSQL you need to post more info about your DB setup 4-7 secs for even 10 mill rows seems really slow. I think you're still missing something in your DB setup unless you are running on a 486 or have 10MB ethernet or someother bottle neck in your system. Good Luck! I heart something about Lucene can it help ? Yes it can. I'm sure you will love it! I had the same problem: An table with round about 1.2 Million Messages. By searching trough these Messages it needs some seconds. An full text search on the ""message"" collum needs about 10 seconds. At the same server hardware lucene returns the result in about 200-400ms. That's very fast. Cached results returns in round about 5-10 ms. Lucene is able to connect to your sql database (for example mysql) - scans your database an builds an searchable index. For searching this index it depends on the kind of application. I my case my PHP Webaplication uses solr for searching inside lucene. http://lucene.apache.org/solr/  Try to use table partitioning. In large table scenarios can be helpful to partiton a table. For PostgreSQL try here PostgreSQL Partitioning. For high scalable fast performance searches sometimes may be useful to adopt NoSQL database (like Facebook does).  Take a look at Hibernate Search this is using Lucene but a lot more easier to implement. Google or Facebook are using different approaches. They have distributed systems. Googles BigTable is a good keyword or the ""Map and Reduce"" concept (Apache Hadoop) is a good starting point for more research. As far as i know map reduce is not for online searching. Hadopp is used for large dataset analize map reduce jobs take too many time and ussually works as background tasks you have to index too not only to search. And for distributed systems you need a good base for that and thats what hadoop is doing. But fore sure thats out of scope for the original question I just mentioned it because Google and other big ones have ""other"" approaches which are not a easy solution to implement in one day.  There is great difference between these three queries: a) SELECT * FROM u WHERE u.name LIKE ""George%"" b) SELECT * FROM u WHERE u.name LIKE ""%George"" c) SELECT * FROM u WHERE u.name LIKE ""%George%"" a) The first will use the index on u.name (if there is one) and will be very fast. b) The second will not be able to use any index on u.name but there are ways to circumvent that rather easily. For example you could add another field nameReversed in the table where REVERSE(name) is stored. With an index on that field the query will be rewritten as (and will be as fast as the first one): b2) SELECT * FROM u WHERE u.nameReversed LIKE REVERSE(""%George"") c) The third query poses the greatest difficulty as neither of the two previous indexes will be of any help and the query will scan the whole table. Alternatives are: Using a dedicated for such problems solution (search for ""full text search"") like Sphinx. See this question on SO with more details: which-is-best-search-technique-to-search-records If your field has names only (or another limited set of words say a few hundred different words) you could create another auxilary table with those names (words) and store only a foreign key in table u. If off course that is not the case and you have tens of thousands or millions different words or the field contains whole phrases then to solve the problem with many auxilary tables it's like creating a full text search tool for yourself. It's a nice exercise and you won't have to use Sphinx (or other) besides the RDBMS but it's not trivial. It's not actually true select with preceding % in ILIKE can use indexes i read in documentation and tested myself. At least in Postgres. Anyway i accept your answer since it is the most voluminous. Probably best solution in this case to use Lucene or Sphinx. I didn't know that (*preceding % in ILIKE can use indexes*). Can you provide reference to documentation for this behaviour of Postgres? I'm sorry you are completely right. Just checked documentation don't even know why i was thinking the opposite."
580,A,"How-to index arrays (tags) in CouchDB using couchdb-lucene The setup: I have a project that is using CouchDB. The documents will have a field called ""tags"". This ""tags"" field is an array of strings (e.g. ""tags"":[""tag1""""tag2""""etc""]). I am using couchdb-lucene as my search provider. The question: What function can be used to get couchdb-lucene to index the elements of ""tags""? If you have an idea but no test environment type it out I'll try it and give the result here. Well it was quite easy after I figured it out. Please realize that the $ character has no significance to the code my fields in this case just begin with $. Posted the answer for anyone with this question in the future. function(doc) { var result = new Document(); for(var i in doc.$tags) { result.add(doc.$tags[i]); } return result; }  Perhaps the syntax has changed but you can construct a view to search by any item in a document's array: function(doc) { for (var i=0; i<doc.page.length; i++) { emit(doc.page[i].url doc._id); } }"
581,A,Lucene 3.0.2 features What are new features in lucene 3.0.2? [Lucene Change Log](http://lucene.apache.org/java/3_0_2/changes/Changes.html#3.0.2)? Here they are in the changelog
582,A,Lucene: Building a query of single terms I'm new to Lucene and I would like to know what's the difference (if there is any) between PhraseQuery.add(Term1) PhraseQuery.add(Term2) PhraseQuery.add(Term3) and term1 = new TermQuery(new Term(...)); booleanQuery.add(term1 BooleanClause.Occur.SHOULD); term2 = new TermQuery(new Term(...)); booleanQuery.add(term2 BooleanClause.Occur.SHOULD); term3 = new TermQuery(new Term(...)); booleanQuery.add(term3 BooleanClause.Occur.SHOULD); PhraseQuery requires that all the terms exist in the field being searched. Your BooleanQuery does not require that all the terms exist. This leads to the question of what is the difference between your PhraseQuery and: term1 = new TermQuery(new Term(...)); booleanQuery.add(term1 BooleanClause.Occur.MUST); term2 = new TermQuery(new Term(...)); booleanQuery.add(term2 BooleanClause.Occur.MUST); term3 = new TermQuery(new Term(...)); booleanQuery.add(term3 BooleanClause.Occur.MUST); The difference here is that the PhraseQuery would require the terms be in the correct order as opposed to the BooleanQuery which would not have any particular order requirement. So roughly speaking each term in PhraseQuery always has the MUST property and he order is fixed. Then I will definitely go with BooleanQuery. What about the strings parsing (in order to add the terms to booleanQuery) ? Should I do it manually or can I use Lucene ? thanks @Patrick You can parse the input text using Lucene by taking advantage of one of the existing analyzers or writing your own. Generally speaking you should use the same analyzer to both build your index and construct your query. http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/analysis/Analyzer.html
583,A,"storing state of enterprise application in Java EE server (e.g for lucene index) I need to create a Lucene index on a Java EE application startup but I do not want to decide on the location of the index in filesystem myself. How do applications in general store any files created while running is there any kind of store provided by the engine per application basis etc. that I can use. I would say that the best way is to store the data on a default path in the filesystem for example /var/lib/your_application/ and then make that location be configurable. You can read more here: http://www.pathname.com/fhs/pub/fhs-2.3.html hi what is the default path? Hi The linux standard base says that that kind of information should go into /var if I remember correctly. That's why I suggested /var/lib/your_application/ as an example.  I do not want to decide on the location of the index in filesystem myself. How do applications in general store any files created while running is there any kind of store provided by the engine per application basis etc By default the classes in the java.io package resolve relative pathnames against the current working directory - i.e. the location in the file system from where the java command was invoked - that you can get using the user.dir system property: String curDir = System.getProperty(""user.dir""); But doing this is far from ideal (actually writing to files is not ideal for portable applications) and I don't recommend this approach but suggest using absolutes filesystem paths and e.g. system properties: new File(System.getProperty(""my.directory"" + File.separator + ""xxx""); Where the property my.directory would be set in the app server startup script e.g. assuming JBoss is installed under /opt using -Dmy.directory=/var/opt/jboss/<somedir> to be FHS compliant. Just keep in mind that: Writing to the FS is not ideal for application portability. Prefer writing to a database if possible Using java.io from EJBs is in theory forbidden.  Normally you create a Lucene index in application servers like JBoss and Tomcat in the current directory (.) index folder is created in bin folder which is not a good idea at all. You can either store your index files in database (or virtually any other storing device) using Compass or store it in a path on the file system. There is no special per-application storage as far as I know in Java EE containers."
584,A,Use Lucene in PHP I'm wondering how can I use Lucene for searching and indexing with PHP Google said that I can do so just if It will be through Zend framework this latter unlikely does not present in my dictionary I use standard PHP can I use Zend just for get Lucene work on my site for indexing my e-books? I'm using Netbeans 6.8 for information and I found this word 'Zend' (all what I know about is his name ^^) in somewhere on Netbeans options. Really can't understand what some sites suggest please help! and give me the simplest way to get started and then I'll take the wheel! Thanks in advance Regards! Very interesting that I can use Zend_Lucene without using the rest of the Zend Framework. just a quest: which one is easier in term of his configuration in my project? Zend_Lucene or Solr? You can use Zend_Lucene without using the rest of the Zend Framework. However from personal experience I don't recommend using Zend_Lucene on large indexes -- it performs at a fraction of the speed of solr. FOr a small project with an index no larger than a few hundred documents it can be convenient. Solr has a REST API which could be used from PHP or any language for that matter. Really? I'm going to check it out thanks!  Either use Sphinx or Lucene/Solr directly. Zend_Lucence works OK for small projects but is not as fast as Sphinx or Solr.
585,A,"JPA HibernateSearch Projections I'm trying to use JPA with HibernateSearch. I used Example 5.3 in http://docs.jboss.org/hibernate/stable/search/reference/en/html/search-query.html. The results come out as expected. However the data coming back is a huge graph. I only need the primary key of the data. So I tried Example 5.9 but it only shows the Hibernate API. There was not a javax.persistence.Query.setProjection() method. What can I use to get just the primary key of a search result? Should I try to get the hibernate session from the EntityManager in JPA? Thanks for any help. Example 5.3 was a bit misleading. javax.persistence.Query doesn't have to be used. Instead org.hibernate.search.jpa.FullTextQuery has the setProject() method that I needed. Here is the resulting code (with fully qualified class names):  //Open JPA session javax.persistence.EntityManagerFactory emf=javax.persistence.Persistence.createEntityManagerFactory(""manager1""); javax.persistence.EntityManager em=emf.createEntityManager(); em.getTransaction().begin(); //Make a FullText EM from the JPA session. org.hibernate.search.jpa.FullTextEntityManager fullTextSession=org.hibernate.search.jpa.Search.getFullTextEntityManager(em); //Build the lucene query. org.apache.lucene.queryParser.QueryParser parser=new org.apache.lucene.queryParser.QueryParser(""data1""new org.apache.lucene.analysis.standard.StandardAnalyzer()); org.apache.lucene.search.Query query=parser.parse(""FindMe""); //Convert to a hibernate query. org.hibernate.search.jpa.FullTextQuery query2=fullTextSession.createFullTextQuery(query SampleBean.class); //Set the projections query2.setProjection(""id""); //Run the query. for (Object[] row:(List)query2.getResultList()){ //Show the list of id's System.out.println(row[0]); } //Close em.getTransaction().commit(); em.close(); emf.close();  query2 does the projection and all is well!"
586,A,"Only getting query terms as highlighted results with lucene highlighter (3.0) I'm using the current code to get fragments to highlight on lucene output but the results are always just the searched for string. var parser = new MultiFieldQueryParser(new[] { ""contents"" ""PageName"" } new StandardAnalyzer()); Query query = parser.Parse(Query); QueryScorer scorer = new QueryScorer(query); Formatter formatter = new SimpleHTMLFormatter(config.HighlightFormatterPrefix config.HighlightFormatterSuffix); Highlighter highlighter = new Highlighter(formatter scorer); highlighter.SetTextFragmenter(new SimpleFragmenter(100)); TokenStream stream = new StandardAnalyzer().TokenStream(""contents"" new StringReader(Query)); return highlighter.GetBestFragments(stream Query 2 "".""); In case it is helpful here is the code used for the query: var parser = new MultiFieldQueryParser(new[]{""contents""""PageName""} new StandardAnalyzer()); Query query = parser.Parse(searchString); Hits results = searcher.Search(query); var hits = new List<LuceneSearchResult>(); for (int index = 0; index < results.Length(); index++) { Document document = results.Doc(index); var searchResult = new LuceneSearchResult(); searchResult.Document = document; searchResult.Query = searchString; searchResult.Id = document.GetField(""ID"").StringValue(); searchResult.Score = results.Score(index); hits.Add(searchResult); } Whatever I search for is the exact same as the string that is returned for the highlighted fragments. I found the issue. The resulting working code was a change in the results: var parser = new QueryParser(""contents"" new StandardAnalyzer()); Query query = parser.Parse(Query); SimpleHTMLFormatter formatter = new SimpleHTMLFormatter(config.HighlightFormatterPrefix config.HighlightFormatterSuffix); QueryScorer fragmentScorer = new QueryScorer(query""contents""); Highlighter highlighter = new Highlighter(formatter fragmentScorer); highlighter.SetTextFragmenter(new SimpleFragmenter(100)); TokenStream tokenStream = new SimpleAnalyzer().TokenStream(config.MainContentFieldName new StringReader(field.StringValue())); return highlighter.GetBestFragments(tokenStream field.StringValue() 2 "".""); I changed from multiple fields to a single field on the query since the pagename would never be useful in the summary this is being used for and changed from a Formatter to a SimpleFormatter"
587,A,"Refining Solr searches getting exact matches? Afternoon chaps Right I'm constructing a fairly complex (to me anyway) search system for a website using Solr although this question is quite simple I think... I have two search criteria location and type. I want to return results that are exact matches to type (letter to letter no exceptions) and like location. My current search query is as follows ../select/?q=location:N1 type:blue&rows=100&fl=*score&debugQuery=true This firstly returns all the type blue's that match N1 but then returns any type that matches N1 which is opposite to what I'm after. Both fields are set as textgen in the Solr schema. Any pointers? Cheers gang If you only want to have the ""type"" clause to be mandatory you can keep the OR as the default operator and use the encoded '+' sign in front of the ""type"" clause: ./select/?q=location:N1 %2Btype:blue&rows=100&fl=*score&debugQuery=true Worked wonderfully thanks Pascal!  By default Solr uses the OR operator to combine the query terms. If you only want results with location:N1 AND type:blue instead of location:N1 OR type:blue you'll need to change the operator. The simplest way to change this is by adding an additional parameter q.op=AND to the URL when querying: ../select/?q=location:N1 type:blue&rows=100&fl=*score&debugQuery=true&q.op=AND You can also change this for all queries by editing your schema.xml file; look for <solrQueryParser defaultOperator=""OR""/> and change it to <solrQueryParser defaultOperator=""AND""/> You might also want to change the field-type of your type field to something that is not tokenized string for example. This will ensure that the match on that field is exact. <field name=""type"" type=""string"" indexed=""true"" stored=""true""/> Cheers buddy some very useful tips much appreciated :)"
588,A,"What are the downsides of using Lucene? I'm thinking about using Lucene in my project to do very fast searches. I know that Lucene creates its own files where it keeps all the data/indexes. I wonder what are the downsides of using Lucene? Are there any? Do you have to do anything with the file database or does it work great without any outside help? P.S. I know there is also Lucene .NET and I bet the same rules apply there. Lucene does great work for many people and companies. Your mileage may vary though. A possible problem is Lucene's scoring model - It uses a combination of TF/IDF and boolean scoring while other IR tools use the probabilistic BM25 which is stronger. However You may work with Lucene for years and the search results would be good enough. Also scaling to many millions of documents is not easy. It boils down to your specific use-case. It is best to start a test using Solr and see whether is seems to fit your needs.  Lucene do have scalability issue . Its performance degrades when the index is getting larger and larger. That is not a lucene specific issue the same is true of any indexing system.  I have limited experience with Lucene so far it has been great though. The downsides I can see are mainly from a business perspective: I have to actively make the case for using Lucene to my boss by default we would use SQL Server. To make the switch I will have to prove without a doubt that Lucene performs better (and not just similar) for the use case we have. I guess this one goes to the ""Nobody ever got fired for buying IBM equipment"" syndrome. Ongoing development/bug fixes for Lucene.Net in particular are questionable at this point again a tougher sell w/o this. I hope the community can rally.  Lucene is great. Very flexible surprisingly fast and a solid API. The mailing list is extremely helpful. The files do need a bit of maintenance but it can be done with provided tools. Of primary importance is optimizing the index on occasion but this is only needed if you update the index regularly. I would suggest looking into Solr as well. It's essentially a webapp and tools that sit on top of Lucene. It makes it a tad easier to create new indexes keep them optimized as well as providing master/slave synchronization for a scalable search cluster. This of course depends on your actual needs. For a personal example I used to maintain a search index for a large well-known gaming company. The index had hundreds of thousands of entries in multiple languages (world-wide) and locales. It performed a million searches each day on the cluster without using hardly any CPU and a reasonable amount of memory. It had load tested out to around 300 million searches per day on the hardware we had and would scale linearly by simply adding more boxes to the cluser. Solr and Lucene were the primary tools for this. If I had to give a downside it would be learning curve. There is quite a bit to understand and if you want a truly optimized solution you need to know it well. However this will happen with any search tool you use if you do it yourself. The documentation wikis and mailing list provide plenty of support for this ramp up."
589,A,"using hit highlighter in lucene I have two questions regarding hit highlighter provided with apache lucene: see this function could you explain the use of token stream parameter. I have several large lucene document containing many fields and each field has some strings in it. Now I have found the most relevant document for a particular query. Now this document was found because several words in the query might have matched with the words in the document. I want to find out what words in the query caused this. So for this I plan to use Lucene Hit Highlighter. Example: if the query is ""skin doctor delhi"" and the document titled ""dermatologist"" contains the words ""skin"" and ""doctor"" then after hit highlighting i should be able to separate out ""skin"" and ""doctor"" from the query. I have been trying to write the code for this for several weeks now. Not able to get what i want. Could you help me please? Thanks in advance. Update: Current Approach: I create a query containing all the words in the document. Field[] field = doc.getFields(""description""); String desc = """"; for (int j = 0; j < field.length; ++j) { desc += field[j].stringValue() + "" ""; } Query q = qp.parse(desc); QueryScorer scorer = new QueryScorer(q reader ""description""); Highlighter highlighter = new Highlighter(scorer); String fragment = highlighter.getBestFragment(analyzer ""description"" text); It works for small documents but does not work for large documents. The following stacktrace is obtained.  org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024 at org.apache.lucene.search.BooleanQuery.add(BooleanQuery.java:152) at org.apache.lucene.queryParser.QueryParser.getBooleanQuery(QueryParser.java:891) at org.apache.lucene.queryParser.QueryParser.getBooleanQuery(QueryParser.java:866) at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:1213) at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:1167) at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:182) It is obvious that the approach is unreasonable for large documents. What should be done to correct this? BTW I am using FuzzyQuery matching. EDIT: added some details about explain(). Some general introduction: The Lucene Highlighter is meant to find text snippets from a hit document and to highlight tokens matching the query. Therefore The TokenStream parameter is used to break the hit text into tokens. The highlighter's scorer then scores each token in order to score fragments and choose snippets and tokens to be highlighted. I believe you are doing it wrong. If all you want to do is understand which query terms were matched in the document you should use the explain() method. Basically after you have instantiated a searcher use: Explanation expl = searcher.explain(query docId); String asText = expl.toString(); String asHtml = expl.toHtml(); docId is the raw document id from the search results. Only if you do need the snippets and/or highlights you should use the Highlighter. If you still want to use the highlighter follow Nicholas Hrychan's advice. Beware though as he describes the Lucene 2.4.1 API - If you use a more advanced version you should use ""QueryScorer"" where he says ""SpanScorer"" . this is what i get when i search for skin. the 0th document hit produces this with explain(). `0.0 = (NON-MATCH) sum of:` how do i interpret it? System.out.println(searcher.explain(query i).toString()); i is zero in the previous function call I have not understood the explain method. It returns an Explanation object what function is required hereon to get the matched query terms. I am not satisfied with the documentation of Lucene. Please see my edit about explain(). ok cool. what about getDetail() method. Here's a fuller version: http://stackoverflow.com/questions/1742124/different-lucene-search-results-using-different-search-space-size OK thank you finally got it working. Now i am using fuzzy query matching. so if the document is a hit explain would tell me the word as it occurs in the document and not in the query. how to achieve this? The Lucene Explanation has a recursive structure. toString() and toHtml() give you the full explanation tree. getDetails() gives you a subtree of the tree at a time. I would try looking at the full tree first and only if this is too complicated go to the subtrees."
590,A,"Can I run iPad with 200 thousand record with sqlite? Current database have 200 thousand record. I want to make iPad application  can I run 200 thousand record with sqlite ? I don't want to use sqlite because searching is too slow over 32000. Any search engine like lucene for iPhone SDK ? if we can run lucene in iPad that will be awesome because current project is base on lucene. Can you give me a suggestion ? Thank You Dig those `indexes` up! 32k lines of data can't be slow on any database! 32k lines of data can't be slow on any data support! :) I suggest to use ""How to do fulltext search on iPad"" as the title of the question. SQLite is not really the problem here. thank. I will try with index. Yes I didn't index in sqlite and I'm using auto suggest. It's not too slow. When I tired with index 32k lines are more faster than before. Thank all. I recently built an iPhone app with 86000 rows and SQLite. At first I didn't index my serach row and searches were taking about 1 second to execut on an iPod touch 2nd generation. Once I added my indexes searching was instant. To be honest you might be able to get away with ""like"" queries on the field you are looking for? It might actually be good enough. I think you will find that once you add indexes to your data the database is really going to grow in size and could make you app pretty big and putting a full blown search engine in there could be a real pain. Here is some code to quickly do some tests for a SQLite database. http://www.rvaidya.com/blog/iphone/2009/02/14/wrapper-classes-for-using-sqlite-on-iphonecocoa/ SQLiteResult *result = [SQLite query:@""SELECT * from test;""]; NSArray *row = [result.rows objectAtIndex:0]; NSString *firstValue = [row objectAtIndex:0]; If you use LIKE 'text%' then it's OK (SQLite can use an index in that case) but I would avoid using LIKE '%text%'. This will result in a table scan and a table scan on 200'000 rows is slow - it doesn't matter that much what platform you use. You might get away with it with test data but it's probably a bigger pain to change the application later on than to start with the right algorithm.  I understand Lucene is written in Java - that'd mean you can't use it on the iPad. Have you actually created indexes in your Sqlite database? In my experience Sqlite is really quite fast. If Core Data is too slow you could also try using the native C API. yes. I know. Lucene can't run on iPad. I will try with sqlite or if I'm satisfy for speed we should write native C API.  This blog post describes a port of Lucene to Objective C. You may try to use the code though it seems a bit dated.  I suggest to build you own fulltext index. I don't think SQLite on the iPhone supports triggers but you can still use a similar algorithm than the H2 fulltext search implementation. The idea is to split the text data into words and then add those to an table that is indexed on the word. The algorithm for building the index is relatively simple. The native H2 fulltext search doesn't have all the features the Lucene fulltext search has but it's much simpler to implement. I like this idea. Its actually pretty simple and I did this not so long ago."
591,A,SOLR/Lucene: How would one go about extending the Scorer class s.t. it could then be wrapped in a Solr plugin The reason I am asking this is because extending the Similarity class or using query function is not enough for me. I plan to personalize user queries in terms of their preferences with respect to document fields. I need to update the score of the documents after the text based scoring has been computed using these preferences (which would have been cached by the Solr plugin). Any thoughts? I'd write a custom function query it fits your definition of modifying the calculated score with a custom algorithm. That's what I was thinking as well however I not sure about one thing: since I need to score each matching document differently by this function I am guessing I need to use the \_val\_ hook as the boost value... have you any idea on how does this actually work? is this supported by the LuceneQueryParser?
592,A,"how to use BaseTokenStreamTestCase classe with maven I want to unit test my lucene filter (which extends TokenFilter). I use maven. The ""BaseTokenStreamTestCase"" class looks perfect but I have no idea in which maven artifactId I can find it ? Any idea ? The class o.a.l.a.BaseTokenStreamTestCase.java is part of Lucene's test tree and it doesn't seem to be published as a Maven artifact (I may be wrong but I couldn't find it in any repository search engine). Maybe the easiest way would be to duplicate it in your own sources. It's of course a solution. You need to duplicate LuceneTestCase and LuceneTestCaseJ4 classes too. Thanks Pascal ! @guillaume06: it's ugly but I don't have a better solution."
593,A,Has anyone used lucene.net with Linq-to-Entities? If anyone has done this please let me know. I don't know anything about lucene.net. I have never used it but I heard about it. I was wondering how something like that would integrate with the Linq entity framework? so.. any update on this? I'm about to dive in head first so if you have any remarks please make them public :) Sorry I never ended up using it for the project I was working on. Check this article in linq to lucene discussion Linq to Lucene for Entity Framework working with entity framework only one class add  Check out Linq to Lucene project.
594,A,"Searching TokenStream fields in Lucene I am just starting out with Lucene and I feel like I must have a fundamental misunderstanding of it but from the samples and documentation I could not figure out this issue. I cannot seem to get Lucene to return results for fields which are initialized with a TokenStream whereas fields initialized with a string work fine. I am using Lucene.NET 2.9.2 RC2. [Edit] I've also tried this with the latest Java version (3.0.3) and see the same behavior so it is not some quirk of the port. Here is a basic example: Directory index = new RAMDirectory(); Document doc = new Document(); doc.Add(new Field(""fieldName"" new StandardTokenizer(new StringReader(""Field Value Goes Here"")))); IndexWriter iw = new IndexWriter(index new StandardAnalyzer()); iw.AddDocument(doc); iw.Commit(); iw.Close(); Query q = new QueryParser(""fieldName"" new StandardAnalyzer()).Parse(""value""); IndexSearcher searcher = new IndexSearcher(index true); Console.WriteLine(searcher.Search(q).Length()); (I realize this uses APIs deprecated with 2.9 but that's just for brevity... pretend the arguments that specify the version are there and I use one of the new Searchs). This returns no results. However if I replace the line that adds the field with doc.Add(new Field(""fieldName"" ""Field Value Goes Here"" Field.Store.NO Field.Index.ANALYZED)); then the query returns a hit as I would expect. It also works if I use the TextReader version. Both fields are indexed and tokenized with (I think) the same tokenizer/analyzer (I've also tried others) and neither are stored so my intuition is that they should behave the same. What am I missing? I have found the answer to be casing. The token stream created by StandardAnalyzer has a LowerCaseFilter while creating the StandardTokenizer directly does not apply such a filter. not only that but StandardAnalyzer also filters out common english StopWords via a StopFilter. The rule of thumb with Lucene is to search and index with the exact same setup of TokenStreams on both side. In your code you should not instanciate StandardTokenizer directly but use StandardAnalyzer.TokenStream() to create it"
595,A,How to install Lucene 3.0.0 in Ubuntu 8.10 I have downloaded Lucene 3.0.0 and when I used the command java -jar lucene-core-3.0.0.jar in the directory where Lucene is present I got this message Failed to load Main-Class manifest attribute from lucene-core-3.0.0.jar How do I proceed? Lucene is a Java library to be used internally within applications not an executable JAR. What were you expecting it to do? Skaffman is correct. Here's a minimal Lucene application. If you want a self-contained search server try Solr. thank you; I will go with Solr  That error means that is not an executable jar. Perhaps you should put all the jars in your classpath then start with the demo at http://lucene.apache.org/java/3_0_0/demo.html .  I am using lucene for information retrieval. you need to set the classpath after downloading lucene: Setting up the environment in Linux setenv LUCENEDIR /opt/lucene-2.4.1 setenv CLASSPATH .:./lucene-core-2.4.1.jar: ./lucene-demos-2.4.1.jar:./lukemin-0.9.2.jar: $LUCENEDIR/lucene-core-2.4.1.jar:$LUCENEDIR/lucene-demos-2.4.1.jar: $LUCENEDIR/lukemin-0.9.2.jar:$CLASSPATH
596,A,"Directory lock error with Lucene.Net usage in an ASP.NET MVC site I'm building an ASP.NET MVC site where I want to use Lucene.Net for search. I've already built a SearchController and all of its methods but I'm getting an error at runtime that occurs when the SearchController is first initialized. In SearchController here's how I'm creating an IndexWriter: public static string IndexLocation = HostingEnvironment.MapPath(""~/lucene""); public static Lucene.Net.Analysis.Standard.StandardAnalyzer analyzer = new Lucene.Net.Analysis.Standard.StandardAnalyzer(); public static IndexWriter writer = new IndexWriter(IndexLocationanalyzer); The error occurs on the last line. Here's the message that I'm getting: Lucene.Net.Store.LockObtainFailedException: Lock obtain timed out: SimpleFSLock@C:\Users\Username\Desktop\SiteSolution\Site\lucene\write.lock Furthermore here's the stack trace: [LockObtainFailedException: Lock obtain timed out: SimpleFSLock@C:\Users\Username\Desktop\SiteSolution\Site\lucene\write.lock] Lucene.Net.Store.Lock.Obtain(Int64 lockWaitTimeout) in C:\Users\Username\Desktop\Lucene.Net_2_9_2\src\Lucene.Net\Store\Lock.cs:107 Lucene.Net.Index.IndexWriter.Init(Directory d Analyzer a Boolean create Boolean closeDir IndexDeletionPolicy deletionPolicy Boolean autoCommit Int32 maxFieldLength IndexingChain indexingChain IndexCommit commit) in C:\Users\Username\Desktop\Lucene.Net_2_9_2\src\Lucene.Net\Index\IndexWriter.cs:1827 Lucene.Net.Index.IndexWriter.Init(Directory d Analyzer a Boolean closeDir IndexDeletionPolicy deletionPolicy Boolean autoCommit Int32 maxFieldLength IndexingChain indexingChain IndexCommit commit) in C:\Users\Username\Desktop\Lucene.Net_2_9_2\src\Lucene.Net\Index\IndexWriter.cs:1801 Lucene.Net.Index.IndexWriter..ctor(String path Analyzer a) in C:\Users\Username\Desktop\Lucene.Net_2_9_2\src\Lucene.Net\Index\IndexWriter.cs:1350 Site.Controllers.SearchController..cctor() in C:\Users\Username\Desktop\SiteSolution\Site\Controllers\SearchController.cs:95 [TypeInitializationException: The type initializer for 'Site.Controllers.SearchController' threw an exception.] [TargetInvocationException: Exception has been thrown by the target of an invocation.] System.RuntimeTypeHandle.CreateInstance(RuntimeType type Boolean publicOnly Boolean noCheck Boolean& canBeCached RuntimeMethodHandle& ctor Boolean& bNeedSecurityCheck) +0 System.RuntimeType.CreateInstanceSlow(Boolean publicOnly Boolean fillCache) +86 System.RuntimeType.CreateInstanceImpl(Boolean publicOnly Boolean skipVisibilityChecks Boolean fillCache) +230 System.Activator.CreateInstance(Type type Boolean nonPublic) +67 System.Web.Mvc.DefaultControllerFactory.GetControllerInstance(RequestContext requestContext Type controllerType) +80 [InvalidOperationException: An error occurred when trying to create a controller of type 'Site.Controllers.SearchController'. Make sure that the controller has a parameterless public constructor.] System.Web.Mvc.DefaultControllerFactory.GetControllerInstance(RequestContext requestContext Type controllerType) +190 System.Web.Mvc.DefaultControllerFactory.CreateController(RequestContext requestContext String controllerName) +68 System.Web.Mvc.MvcHandler.ProcessRequestInit(HttpContextBase httpContext IController& controller IControllerFactory& factory) +118 System.Web.Mvc.MvcHandler.BeginProcessRequest(HttpContextBase httpContext AsyncCallback callback Object state) +46 System.Web.Mvc.MvcHandler.BeginProcessRequest(HttpContext httpContext AsyncCallback callback Object state) +63 System.Web.Mvc.MvcHandler.System.Web.IHttpAsyncHandler.BeginProcessRequest(HttpContext context AsyncCallback cb Object extraData) +13 System.Web.CallHandlerExecutionStep.System.Web.HttpApplication.IExecutionStep.Execute() +8682818 System.Web.HttpApplication.ExecuteStep(IExecutionStep step Boolean& completedSynchronously) +155 How can I resolve this issue? UPDATE: I've started working on this particular project again and it seems that I haven't fully resolved this issue yet. The real issue is that the write.lock file isn't being removed after index usage ends. Based on the answer I have accepted I understand the basic implementation logic but I'm not sure if I have implemented it correctly. Here are some other methods in my class that are probably invalid:  public ActionResult Search(string query) { var reader = writer.GetReader(); // Get reader from writer var searcher = new IndexSearcher(reader); // Build IndexSearch //Execute search... // Dispose of objects searcher = null; reader = null; return View(); } public void AddToIndex(Document doc) { writer.AddDocument(doc); writer.Flush(); writer.Optimize(); writer.Flush(); } private bool disposed = false; protected override void Dispose(bool disposing) { if (!disposed) { if (disposing) { // Release managed resources. } try { writer.Close(); writer = null; } catch { } // Release unmanaged resources. // Set large fields to null. // Call Dispose on your base class. disposed = true; } base.Dispose(disposing); } Any thoughts? Researching this myself it seems that the indended approach would be to use multiple physical indexes and then merge them using the IndexWriter's .addIndexesNoOptimize or .addIndexes to merge all concurrent index changes. Lucene documentation  Haven't checked it myself but wouldn't this work (write after creating azureDirectory object)? azureDirectory.ClearLock(""write.lock"")  try { writer = new IndexWriter(directory new StandardAnalyzer() IndexWriter.MaxFieldLength.UNLIMITED); } catch (LockObtainFailedException ex) { DirectoryInfo indexDirInfo = new DirectoryInfo(directory); FSDirectory indexFSDir = FSDirectory.Open(indexDirInfo new Lucene.Net.Store.SimpleFSLockFactory(indexDirInfo)); IndexWriter.Unlock(indexFSDir); writer = new IndexWriter(directory new StandardAnalyzer() IndexWriter.MaxFieldLength.UNLIMITED); } This code doesn't compile. DirectoryInfo constructor takes a string not AzureDirectory object. Perhaps I am missing something.  Seems to be a dead-lock on lucene. If supposedly NO index update into the collection simply remove this lock file C:\Users\Username\Desktop\SiteSolution\Site\lucene\write.lock. After that re-run the index writing.  The reason why this happens is that the writer creates an empty file called write.lock as a cross-process lock. When that file exists Lucene assumes that someone has a write lock on that directory. When you terminate your process incorrectly the file will not get deleted. So Lucene thinks that someone is still holding on to the lock. That's why you should always have a finally statement which closes the index. If you are sure that the file is there in error (i.e. no Lucene processes are running) it is fine to just delete the file. Your index may however be in a corrupted state since the writing was obviously terminated midstream. Thanks for your answer! My static IndexWriter never really gets closed... is that a problem? @Maxim: The only reason that would be a problem is if multiple processes need to write. Otherwise it's usually best to keep the writer open and let lucene decide when to flush to disk. @Xodarap: Nice notice! Let me go check what's going on there. @Xodarap: huh it seems like it's not being called at all. The way I'm modifying the index from another controller is as such: `var s = new SearchController(); s.ChangeIndex(post); s = null;`. When I do that the **`Dispose` method isn't being called at all**. What did I implement incorrectly? Thanks! @Xodarap: Never mind fixed! I read the Lucene documentation which says that closing the index should be done rarely. I changed all my closes to commits and only close when the application exits. It works now thanks for your advice! :) @Maxim: and you're sure that the close() is being called without errors? Your sample code just had an empty catch which worries me if there was some error you are getting. @Xodarap: the only issue is that the write.lock file that is being created isn't removed after the index usage ends. @Xodarap: I've started working again on this project and I still haven't fully fixed the issue. I'm about to update the question with some of the code I'm using. I understand the basic implementation logic but I'm not sure if I've implemented that correctly. @Maxim: Are you getting errors on closing the index? On updating?"
597,A,Lucene - is it the right answer for huge index? Is Lucene capable of indexing 500M text documents of 50K each? What performance can be expected such index for single term search and for 10 terms search? Should I be worried and directly move to distributed index environment? Saar Yes Lucene should be able to handle this according to the following article: http://www.lucidimagination.com/content/scaling-lucene-and-solr Here's a quote: Depending on a multitude of factors a single machine can easily host a Lucene/Solr index of 5 – 80+ million documents while a distributed solution can provide subsecond search response times across billions of documents. The article goes into great depth about scaling to multiple servers. So you can start small and scale if needed. A great resource about Lucene's performance is the blog of Mike McCandless who is actively involved in the development of Lucene: http://blog.mikemccandless.com/ He often uses Wikipedia's content (25 GB) as test input for Lucene. Also it might be interesting that Twitter's real-time search is now implemented with Lucene (see http://engineering.twitter.com/2010/10/twitters-new-search-architecture.html). However I am wondering if the numbers you provided are correct: 500 million documents x 50 KB = ~23 TB -- Do you really have that much data?
598,A,Grails: Lucene Compass Query Builder and date ranges I have the searchable plugin working with my grails project. I have it indexing 4 different tables at work. Unfortunately each table has a date field that is named differently. Some are named createdAt some named publishedOn etc... Within my search I need to get items that are within a specific date range out of those fields. Is there a way to do this? I've seen one specific instance in the documentation for the plugin but it doesn't take into account different field names like I have to deal with. you can configure your domain classes to override or provide additional Lucene index entries for a property under different names. So suppose that you have a class with a 'publishedOn' property but you want that property to be searchable as both 'publishedOn' and 'createdAt'. You would do something like the following: class ADomainClass { Date publishedOn static searchable = { 'publishedOn' format:'yyyyMMdd' 'publishedOn' name: 'createdAt' format 'yyyyMMdd' } } If you only want it to be searchable as 'createdAt' then just leave out the first 'searchable' entry. Super awesome. Thank you!
599,A,"Deploying lucene-surround QueryParser to Solr I am trying to deploy the Lucene-surround QueryParser to Solr (1.4.0) I was told its as simple as downloading the surround QueryParser jar and adding it to the lib directory in solr.war then referencing it in solrconfig.xml by adding the line <queryParser name=""SurroundQParser"" class=""org.apache.lucene.queryParser.surround.parser.QueryParser""/> When I do that I receive this error SEVERE: org.apache.solr.common.SolrException: Error Instantiating QParserPlugin org.apache.lucene.queryParser.surround.parser.QueryParser is not a org.apache.s olr.search.QParserPlugin at org.apache.solr.core.SolrCore.createInstance(SolrCore.java:415) at org.apache.solr.core.SolrCore.createInitInstance(SolrCore.java:435) at org.apache.solr.core.SolrCore.initPlugins(SolrCore.java:1498) at org.apache.solr.core.SolrCore.initPlugins(SolrCore.java:1492) at org.apache.solr.core.SolrCore.initPlugins(SolrCore.java:1525) at org.apache.solr.core.SolrCore.initQParsers(SolrCore.java:1442) at org.apache.solr.core.SolrCore.(SolrCore.java:548) at org.apache.solr.core.CoreContainer.create(CoreContainer.java:428) at org.apache.solr.core.CoreContainer.load(CoreContainer.java:278) at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContain er.java:117) at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.ja va:83) at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:99) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java: 40) at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.ja va:594) at org.mortbay.jetty.servlet.Context.startContext(Context.java:139) at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.jav a:1218) at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java: 500) at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:448 ) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java: 40) at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection .java:147) at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHan dlerCollection.java:161) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java: 40) at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection .java:147) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java: 40) at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java: 117) at org.mortbay.jetty.Server.doStart(Server.java:210) at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java: 40) at org.mortbay.xml.XmlConfiguration.main(XmlConfiguration.java:929) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at org.mortbay.start.Main.invokeMain(Main.java:183) at org.mortbay.start.Main.start(Main.java:497) at org.mortbay.start.Main.main(Main.java:115) If someone could give me some advice here I would be very gratefull Thanks in advance Ruth I think you need to extend QParserPlugin. The Solr Wiki explains how - basically your createParser method should return the surround query parser instead of QParser. Mark Miller's post suggests some alternatives. Thank you very much"
600,A,"Solr proximity ordered vs unordered In Solr you can perform an ordered proximity search using syntax ""word1 word2""~10 By ordered I mean word1 will always come before word2 in the document. I would like to know if there is an easy way to perform an unordered proximity search ie. word1 and word2 occur within 10 words of each other and it doesn't matter which comes first. One way to do this would be: ""word1 word2""~10 OR ""word2 word1""~10 The above will work but i'm looking for something simpler if possible Thanks in Advance Ruth Are you sure it's already doesn't work like that? There is nothing in documentation saying that it's 'ordered': A proximity search can be done with a sloppy phrase query. The closer together the two terms appear in the document the higher the score will be. A sloppy phrase query specifies a maximum ""slop"" or the number of positions tokens need to be moved to get a match. This example for the standard request handler will find all documents where ""batman"" occurs within 100 words of ""movie"": http://wiki.apache.org/solr/SolrRelevancyFAQ#How_can_I_search_for_one_term_near_another_term_.28say.2C_.22batman.22_and_.22movie.22.29 Yea the documentation isn't clear about it but in the instance I have running this is how it behaves ie. ""proximity Solr""~2 would give different results to ""Solr proximity""~2  Slop means how many word transpositions can occur. So ""a b"" is going to be different than ""b a"" because a different number of transpositions are allowed. a foo b has positions (a1) (foo 2) (b 3). To match (a1) (b2) will require one change: (b2) => (b3) However to match (b1) (a2) you will need (a2) => (a1) and (b1) => (b3) for a total of three position movements In general if ""a b""~n matches something then ""b a""~(n+2) will match it too. EDIT: I guess I never gave an answer. I see two options: If you want a slop of n increase it to n+2 Manually disjunctivize your search like you suggested I think #2 is probably better unless your slop is very large to begin with. Thank You I found this to be very helpfull  Since Solr 4 it is possible with SurroundQueryParser. E.g. to do ordered search (query where ""phrase two"" follows ""phrase one"" not further than 3 words after): 3W(phrase W one phrase W two) To do unordered search (query ""phrase two"" in proximity of 5 words of ""phrase one""): 5N(phrase W one phrase W two)"
601,A,"how can I build a lucene query with two or more ""in"" clauses My document in Lucene (solr really) has several fields lets call them textField1 numField1 numField2. My application will have a list of search terms words1 textField needs to query against a list of numbers list1 for numField1 and another list of numbers list2 for numField2. I will like to create a Lucene query that does this: textField matches one or more words in words1 AND either ( numfield1 is one of the values in list1 OR numfield2 is one of the values in list2) I'm trying to use BooleanQuery to build the query but I'm not exactly sure how to do it. It seems the AND and OR boolean operations map to the different enums in BooleanClause.Occurs but I can't glean from the documentation how they map to each other. you should use textfieldl1:words1 AND (list1:numfield1 OR list1:numfield2)  I recommend you to get Luke and play with it until you are sure your query is correct and brings back what you want. When you're done you can have a look at parsed query (hint: click on Explain structure) and it will show you how BooleanQueries are stacked. Just make sure you are using the same analyzer you used to index the documents."
602,A,"Searching is not responsive during indexing with Lucene When I re-index the DB data of my application and there is a search executed on the same time the thread that runs the search is going to sleep until the re-indexing is done. I assume that the indexing methods are thread-safe in order to prevent change of the data while indexing. Is there any built in way in Lucene to make it responsive only for search (where the data is not being changed)? Or should I start thinking about something on my own? I'm running my application on a Tomcat server. Thanks Tomer There should be no problem in writing and reading to the index simultaneously. The readers will only see committed data (unless you're using near-real-time searches). What you describe should work. Could you give us example code that experience this problem? I assume that you are actually rebuilding the index (or reindexing everything from scratch as opposed to reindexing individual documents). While the index is being rebuilt you cannot perform the queries against it because it's not in consistent state. The simplest solution that is often used is to rebuild the index in the background (while still performing the queries against the old one) and then replace it with the fresh one. If the problem you are facing is connected with frequent server crashes it might be worthwhile to look at some more systematical approach like the one that is implemented for example in Zoie -- it records subsequent indexing requests so it can recover from the last correct snapshot of the index. Is there any built in ""Copy on write"" mechanism in Lucene for that? I don't know of any such mechanism in Lucene itself. Managing of the indexes is passed to the application or search server (Solr/ES). I think that for example [Solr's multicores support](http://wiki.apache.org/solr/CoreAdmin) can be used for that."
603,A,"Set Solr NOT to match all keywords of query I have got a solr search implemented and everything is working fine. Just a quick question. When user searches in our database for e.g. New Honda Civic. Solr would only return the results which have all three keywords ""New"" ""Honda"" and ""Civic"". How can i make solr to return the results which have all three keywords NEW HONDA CIVIC as well as fewer keywords i.e. HONDA CIVIC. what's the field type like? You'll want to make sure your schema defines the field in a way that an analyzer can break it up into smaller terms. For example: <fieldType name=""text"" class=""solr.TextField"" positionIncrementGap=""100"" omitNorms=""false""> <analyzer> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.StandardFilterFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.StopFilterFactory""/> <filter class=""solr.PorterStemFilterFactory""/> </analyzer> </fieldType> StandardTokenizerFactory will break your text into words. StandardFilterFactory removes dots from acronyms and 's from the end of tokens. LowerCaseFilterFactory gets rid of capitalization woes. StopFilterFactory removes common english words. PorterStemFilterFactory normalizes words that have endings like -ing -es and such suffixes. If you just use: <fieldType name=""string"" class=""solr.StrField"" sortMissingLast=""true"" omitNorms=""false""/> You'll only be able to match on the entire string. You will need to reindex for these changes to take effect. For more information about how Solr processes data for indexing and the data that is queried check out: http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters This is a good point Peter will want to make sure he uses OR as the defaultOperator (specified in schema.xml). However since OR is the default value for defaultOperator I kind of assumed this is how his schema was set up. Even with OR as the defaultOperator text fields should still be tokenized in order to match on individual words. Thank you so much Mike!! Can't we achieve this with OR as default operator?"
604,A,Lucene: queries and docs with multiple fields I have a collection of documents consisting of several fields and I need to perform queries with several terms coming from multiple fields. What do you suggest me to use ? MultiFieldQueryParser or MultiPhraseQuery ? thanks How about BooleanQuery? http://lucene.apache.org/java/3_0_2/api/core/org/apache/lucene/search/BooleanQuery.html I'm actually studying it right now. It seems the best option using BooleanClause.Occur.SHOULD parameter for each field. Also I was thinking if I should use multiple TermQueries per field (one for each term in each field) or should I use PhraseQuery. One of the fields are a list of tags so I guess I need to use a list of TermQueries having the same field.
605,A,"Stemming English words with Lucene I'm processing some English texts in a Java application and I need to stem them. For example from the text ""amenities/amenity"" I need to get ""amenit"". The function looks like: String stemTerm(String term){ ... } I've found the Lucene Analyzer but it looks way too complicated for what I need. http://lucene.apache.org/java/2_2_0/api/org/apache/lucene/analysis/PorterStemFilter.html Is there a way to use it to stem words without building an Analyzer? I don't understand all the Analyzer business... EDIT: I actually need a stemming + lemmatization. Can Lucene do this? Why do you need to stem the words yourself? Lucene has an analyzer called SnowballAnalyzer which you just instantiate with the stemmer name e.g. `new SnowballAnalyzer(""English"");`. Knuth-Pratt Algorithm Implementation http://www.fmi.uni-sofia.bg/fmi/logic/vboutchkova/sources/KMPMatch_java.html Why aren't you using the ""EnglishAnalyzer""? It's simple to use it and I think it'd solve your problem: EnglishAnalyzer en_an = new EnglishAnalyzer(Version.LUCENE_34); QueryParser parser = new QueryParser(Version.LUCENE_34 ""your_field"" en_an); String str = ""amenities""; System.out.println(""result: "" + parser.parse(str)); //amenit Hope it helps you! What is this ""your_field"" doing? Documentation says a cryptic ""the default field for query terms."" That chops it down to words but doesn't stem. Not for me at least.  SnowballAnalyzer is deprecated you can use Lucene Porter Stemmer instead:  PorterStemmer stem = new PorterStemmer(); stem.setCurrent(word); stem.stem(); String result = stem.getCurrent(); Hope this help! PorterStemmer no longer public (stupidly) - see also http://stackoverflow.com/questions/15422485/lucene-porter-stemmer-not-public  You can use snowball Stemmer for stemming. This article has a brief note on the snowball stemmer. http://preciselyconcise.com/apis_and_installations/snowball_stemmer.php  Ling pipe provides a number of tokenizers . They can be used for stemming and stop word removal . Its a simple and a effective means of stemming.  import org.apache.lucene.analysis.PorterStemmer; ... String stemTerm (String term) { PorterStemmer stemmer = new PorterStemmer(); return stemmer.stem(term); } See here for more details. If stemming is all you want to do then you should use this instead of Lucene. Edit: You should lowercase term before passing it to stem(). Is it possible to combine the filter for stop words with the stemmer? Do you want to filter stop words from a string with multiple words or have you already tokenised (separated) the words and want to check just a single word? If its just a single term like above then just create a `Set` of all stop words and do a `.contains()`. As of the current version of Lucene (3.5) PorterStemmer although it exists is not public. I'm not sure who/what uses it but we can't. PorterStemmer no longer public (stupidly) - see also http://stackoverflow.com/questions/15422485/lucene-porter-stemmer-not-public  The previous example applies stemming to a search query so if you are interesting to stem a full text you can try the following: import java.io.*; import org.apache.lucene.analysis.*; import org.apache.lucene.analysis.tokenattributes.*; import org.apache.lucene.analysis.snowball.*; import org.apache.lucene.util.*; ... public class Stemmer{ public static String Stem(String text String language){ StringBuffer result = new StringBuffer(); if (text!=null && text.trim().length()>0){ StringReader tReader = new StringReader(text); Analyzer analyzer = new SnowballAnalyzer(Version.LUCENE_35language); TokenStream tStream = analyzer.tokenStream(""contents"" tReader); TermAttribute term = tStream.addAttribute(TermAttribute.class); try { while (tStream.incrementToken()){ result.append(term.term()); result.append("" ""); } } catch (IOException ioe){ System.out.println(""Error: ""+ioe.getMessage()); } } // If for some reason the stemming did not happen return the original text if (result.length()==0) result.append(text); return result.toString().trim(); } public static void main (String[] args){ Stemmer.Stem(""Michele Bachmann amenities pressed her allegations that the former head of her Iowa presidential bid was bribed by the campaign of rival Ron Paul to endorse him even as one of her own aides denied the charge."" ""English""); } } The TermAttribute class has been deprecated and will not longer be supported in Lucene 4 but the documentation is not clear on what to use at its place. Also in the first example the PorterStemmer is not available as a class (hidden) so you cannot use it directly. Hope this helps. Giancarlo's Answer is correct with a minor change of TermAttribute to CharTermAttribute as TermAttribute is deprecated.  Here is how you can use Snowball Stemmer in JAVA: import org.tartarus.snowball.ext.EnglishStemmer; EnglishStemmer english = new EnglishStemmer(); String[] words = tokenizer(""bank banker banking""); for(int i = 0; i < words.length; i++){ english.setCurrent(words[i]); english.stem(); System.out.println(english.getCurrent()); }"
606,A,"Lucene - Searching several terms in different fields I have a Lucene index which populates from a database. I store/index some fields and then add a FullText field in which I index the contents of all the other fields so I can do a general search. Now let's say I have a document with the following two fields: fld1 - ""Samsung releases a new 22'' LCD screen"" fld2 - ""Sony Ericsson phone's batteries explode"" If an user does a ""Samsung phone"" he probably just wants news about samsung phones not a document with info about a samsung screen and a sony phone but searching by the FullText field I will get this as a valid result. Is there a nice way to handle this? I've thought of indexing with some separator and the doing a SpanNotQuery so the FullText field would have this contents: ""Samsung releases a new 22'' LCD screen MYLUCENESEPARATOR Sony Ericsson phone's batteries explode"" and then doing a SpanNotQuery with MYLUCENESEPARATOR as the non-spanning term. Is this a good solution? Does it scale well with more than two terms? I fear it would be a performance killer. Is there a better way to achieve this? If the number of fields is limited you can put the two description strings in two different fields. Then you can use MultiFieldQueryParser to search on these fields. Since these are two separate fields the document will match only if both the terms appear in a single field with AND operator. Let's take your example. fld1 - ""Samsung releases a new 22'' LCD screen"" fld2 - ""Sony Ericsson phone's batteries explode"" If these are indexed in separate fields fld1 & fld2 your query becomes (+fld1:samsung +fld1:phone) (+fld2:samsung +fld2:phone) Multifield query helps you to construct such queries easily so that you don't need to repeat a query for multiple fields. Thank you Jaime. I accepted your answer but forgot to vote up. Perhaps a bit late but just did it now :-)"
607,A,Could you use Lucene as an OODB? Given that Lucene is a robust document based search engine could it be used as an Object Database for simple applications (E.G. CMS style applications) and if so what do you see the benefits and limitations? I understand the role of the RDBMS (and use them on a daily basis) but watned to explore other technologies/ideas. For example say my domain entities are like: [Serializable] public class Employee { public string FirstName {get;set;} public string Surname {get;set;} } Could I use reflection and store the property values of the Employee object as fields in a Lucene document plus store a binary serialized version of the Employee object into another field in the same Lucene document? No. Trying to use Lucene as an effective OODB (Object Oriented Database) is going to be like trying to fit a square peg into a round hole. They're really two completely different beasts. Lucene is good at building a text index of a set of documents...not storing objects (in a programming sense). Maybe you mis-understand what an Object Oriented Database is. You can check out the definition at Wikipedia: Object Databases Object Oriented Databases have their place. If you truly have an application that would benefit from an OODB I would suggest checking out something like InterSystems Caché Even with the added context I think I'd still stick with my original answer. Since Lucene is written to hold and query indexed documents...trying to store objects in documents for Lucene to *reliably* search is going to be difficult and inefficient. Thanks Justin. Cheers Hi Justin sorry by my original question lacked some context and hopefully the question is better explained.
608,A,"Optimal lucene query options for doing auto completion I have lucene acting as my data provider for querying a list of countries to do auto completion from a text box which works fine. My question is in regards what type of query string should I be sending over to get the most expected return results? Currently I have something along the lines of var query = string.Format(""*{0}*~0.5"" txtCountry.Text) Would there be any recommended tweaks to that for this usage? Use the spellcheck contrib instead. The query you're doing is very inefficient since it uses leading wildcards. If you really don't want to make an n-gram index then I guess I don't see any real improvements (except obviously increasing the allowable distance will increase the number of results). I was considering removing the leading wildcard anyway since it operates in somewhat non-obvious way to the user. Using the spell check stuff probably isn't feasible for me currently so I guess I'll have to live with this for the time being."
609,A,"Lucene Search index breaks regularly on shared hosting when site has high volume of write access I have implemented Lucene on my website. About once every 4 days my search index breaks. I get an error saying that the index is unreadable and the site shows a 500 error to users. I SSH in rebuild my index and eveything goes back to normal. The only part of this project which is slightly different to normal is the high number of writes I am doing to the DB. I am incrementing a ViewCount field on every page view. I presume Lucene updates the document every time. Presuming that this is the issue: Is there a way to tell Lucene to NOT update the index when we are simply incrementing the count field? NB: My project uses sfLucenePlugin within Symfony NB2: The error message is similar to: Sep 03 18:52:21 symfony [err] {sfException} File '/home/username/symfony_project/data/index/MyIndex/en/_1nws_s.del' is not readable. in /home/username/symfony_project/plugins/sfLucenePlugin/lib/vendor/Zend/Search/Lucene/Storage/File/Filesystem.php line 59 @ajreal - Thanks I'll try that next time! a wild guess many writes resulted lots of files open and the collection in the state of very fragmented have you try to optimize it instead just rebuild the index ? Are you using NRT? If so you should never need to explicitly flush to disk. That configuration is very good for high-volume writes. In any case it doesn't sound correct that writing a lot breaks the index. Are you sure your code is entirely thread-safe? Every time I've thought that I've found an issue with Lucene's integrity it has been because my code didn't handle locking properly. (As ajreal suggested your operating system might be throwing a ""too many open files"" error or something similar; a rare error like this might not always be handled correctly.)  Are you seeing messages like this in your log files? Sep 03 18:52:21 symfony [err] {sfException} File '/home/username/symfony_project/data/index/MyIndex/en/_1nws_s.del' is not readable. in /home/username/symfony_project/plugins/sfLucenePlugin/lib/vendor/Zend/Search/Lucene/Storage/File/Filesystem.php line 59 If you are the key point is probably that your index is being corrupted by the high number of files being open concurrently on your server. This is a limitation that is often encountered on shared hosting as other users even if on different virtual servers add up to a lot of file reads/writes especially for webserving. Lucene creates new fragments of the index for each update and over time this means the index is spread over a number of files rather than a well-optimised index of just one file. This means the likelihood of a concurrency error increases over time for a badly-optimised index. Optimising often can help but this can be time-consuming for large indexes and you are still at risk of a concurrency error even if it's a lower probability. The trick to solving this is to balance the optimisation schedule using a cronjob and also as you note to not update the index for trivial data changes (e.g. modified dates view counts). For the latter point you could create a softUpdate() method in each of your model classes that form part of the index. Create some logic here which discounts the trivial column updates and does not hook the search-index updates of sfLucenePlugin. Now this is not as easy as it sounds as sfLucenePlugin uses Propel behaviours which are run 'globally' for your objects... The solution is to edit the behaviour directly or drop the behaviour and write your own methods to update the index. Luckily there is a good example of the functions required to do this in the symfony Jobeet tutorial day 17: http://www.symfony-project.org/jobeet/1_4/Propel/en/17#chapter_17_sub_the_save_method The downside here is you may end up needing to 'rebuild' the indexing strategy that you'd neatly formed in sfLucenePlugin's YAML syntax in PHP... The syntax is not hard but the complexity may be. I hope this makes sense and helps in some way. Your index will still be corrupted - this isn't a catchable ""error"" or exception in PHP or symfony this is your filesystem failing to allow any more files to be opened for writing. Detecting that is an order of magnitude more complex. You need to dispense with sfLucenePlugin and use ZSL directly or do some pretty hefty work to make sfLucenePlugin not update indexes so aggressively... check my answer for ideas ;) Yes this is the error I see. I suppose I could just catch the error and dismiss it - that way on the occasion the file is locked the search index doesn't get updated but I bet 99% of requests are fine."
610,A,"Two Applications using the same index file with Hibernate Search I want to know if it is possible to use the same index file for an entity in two applications. Let me be more specific: We have an online Application with a frondend for the users and an application for the backend tasks (= administrator interface). Both are running on the same JBOSS AS. Both Applications are using the same database so they are using the same entities. Of course the package names are not the same in both applications for the entities. So this is our usecase: A user should be able to search via the frondend. The user is only allowed to see results which are tagged with ""visible"". This tagging happens in our admin interface so the index for the frontend should be updated every time an entity is tagged as ""visible"" in the backend. Of course both applications do have the same index root folder. In my index folder there are 2 index files: de.x.x.admin.model.Product de.x.x.frondend.model.Product How to ""merge"" this via Hibernate Search Configuration? I just did not get it via the documentation... Thanks for any help! Ok it seems that this is not possible... I'm having a similar problem. I have a WebService that consults a table from a database I only have reading permission and it's too slow. Now can Hibernate Search help me with that?"
611,A,Lucene: unstored fields I just wondering whenever exist a way to read the unstored but indexed field in Lucene index? I need because I have an index and I'm going to iterate over all documents in the index in order to apply some analysis and I need to update those documents later in order to update I need first delete and when to re-insert the document. The problem is that I don't know whenever it's possible at all to read the unstored fields in order to copy them as is into updated document. Unstored fields are just that - not stored. Their contents cannot be retrieved from the index. In order to do what you have said you have a few options: make each field stored so that you can make a new document from an existing one if your unstored field is large (ie. the contents of a text file) store a pointer to the original contents in the index (ie. its file path). When creating a new document read this pointer from the existing document fetch the field contents from the original source (ie. from the text file) and then add it unstored to your new document if you are not altering the unstored field you can retrieve the existing document update its other fields and then put it back into the index. This might only be possible in later versions of Lucene though (v2.2 upwards). EDIT: having tried this option it does not work - see my comment below. Ultimately if you need to get the value of the unstored field you will have to make it stored. I'm curious about last options you have mentioned indeed I'm not going to update the content the only thing I need to do some analysis based on other field update them and that's all. So I'm not sure I understood how do I need to do it. From what you saying it's enough to find document in the index and update only the stored fields I'm interested in but what do I need to do next? Just add the updated document as is into the index? So how will it connect with previously unstored fields? Apologies. Having just tried the third option it appears that unstored fields are removed from the document when you retrieve it from the `IndexReader`. Doing as I said will re-add the document with the unstored field missing so this approach is not really a valid option. It looks like you will have to store the value of the field to do what you want to do. [Updated answer to reflect this]  You can use Luke for an easy way to view the index. EDIT: I think I understand the problem now. Here is Andrzej Bialecki's proposed solution which says: Create an index containing documents with just the new/modified fields. Each document in the original index will have a conjugate document with the calculated fields. Use a ParallelReader to search pairs of documents having the original and calculated fields. Well in my case I just cannot do it since I want to update the index after I've finished the indexation of data set. Actually I need to do some link analysis so I need to index once and after analysis update the documents within it. +1 for Luke. it's an eye opener @Artem: Please explain why you cannot do this. Why not do the following: 1. Index your data set putting the result in index A. 2. Go over A doing your link analysis and storing the results in index B. B will contain either fields you copied verbatim from A or analysis results which I consider to be new fields. For every document in A you will have a mirror document in B. 3. Close indexes A and B. 4. Copy A to a backup. 5. Use index B for all your retrieval needs. If I am missing something please tell me. @Yuval F: That won't work because any fields that are unstored in index A cannot be retrieved to be copied into index B so they will end up missing from index B. Luke (v0.9.2) does have the ability to reconstruct an unstored field thereby getting its value but this is a brute force approach using the index statistics and may actually get a different value from the original indexed value. The only way to copy a field from one index to another is to make it a stored field in the original index. @adrianbanks: Thanks. See my edit that suggest another option.
612,A,"Searching Techniques Recommendations This is more of a theory question rather than practice. I'm working on a project which is quite a simple catalog of links. The whole model is similar to the Dmoz or Yahoo catalog except that each entry has certain additional attributes. I have hierarchical taxonomy working on all entries with many-to-many relationship all entries are now sorted into these categories and everything seems to work fine. Now what use is a catalog if there's no search option? Here's a little bit more detail about my models: Each entry has a title description URL and several social profiles: YouTube Twitter Flickr and a couple of others. Each entry could have a logo attached to it and a hidden field for tags. Also the title and description are stored in three different languages. So basically I'd like the search results to be: Relevant (including taxonomy) Possibly ones with logos Possibly ones with 100% filled out profiles I've tried Sphinx and currently working with Lucene but it seems that I'm not getting the search right in theory. I hope it does make sense that filled entries should appear higher than the others but I can't really figure out the scores. I wouldn't like irrelevant entries appear on top if there's simply one word match in the entire description since titles are more relevant. So my question is - are there any books techniques or even other search engines (if Sphinx and Lucene are not good enough) that you would recommend for this matter? Not only I would like to get full control over search results and their ranking but also give my visitors correct and relevant information. Links on cool articles are appreciated too! And No I'm not trying to rebuild Google :) Thanks :) P.S. I'm willing to give a bounty for the best answer. Lucene or Solr would do the job. Solr is built on top of lucene see here for more info I would go with solr. download + setting it up is easy and fast. Get started with the tutorial and my link collection. Relevancy should be fine with solr and is easy tunable. Look into Dewfy and Matthijs Bierman answer for some good points. Then choose the dismax query handler and you can prefer docs with certain properties. E.g. for the percentage of a full profile you define a separate field 'profile_completness' then you can add profile_completeness to bf (boostfunction) of dismax handler: the more complete the profile is the more those docs will be boosted. I mentioned before that you can easily tune the relevancy: e.g. you can set up bf to sth. like: bf=title^10 tags^5 profile_completeness^1 ""Possibly ones with logos"" can be solved via boost queries: bq=logo:[* TO *]^1. Where logo:[* TO *] means ""only docs which contains the field logo"" To display a deeply nested category tree you will need to create that tree in memory and feed solr with a special import. We have a working app for that. You can use our approach If you need further assistance don't hesitate to comment.  Excellent book: Lucene in Action (2nd edition) When we started with Lucene we had the first edition it really takes you through everything you need step by step. Highly recommended. The 2nd edition is updated for the latest and greatest version (3.x.x). The Tf-Idf algorithm works very well on (larger) texts but if you have a record-like structure it may backfire: the documents with a few terms are considered more ""relevant"" than the ones with many terms. With Lucene you will get it to work but you'll have to get your hands dirty. What you'll basically have to do is boost your title field so it becomes more relevant. You may also change the scoring mechanism to assign higher scores for documents that have more information. Have fun. If you can't figure it out there is excellent support on the Lucene mailinglist.  I will try to add to the fine answers by Matthijs Dewfy and Karussell. Basically you are trying to improve your search relevance. I suggest you read Grant Ingersoll's Debugging Search Application Relevance Issues and his Optimizing Findability in Lucene and Solr as well as his Practical Relevance slides. For different languages and for faceting I suggest you use Solr. It is a search engine built using Lucene which is easy to use. It can support multiple languages by using a different Solr Core per each language.  I'm pretty sure that Lucene is enough. We have solved similar task and did it well. Here are some hints that I can propose you looking back at my project at Lucene.Net . Taxonomy: Category has represented as integer key in db so each document has multiple instances of field 'CATEGORY' of type Number. For example document:[12510 'Wheel'] - means that wheel belongs to each of category. Non-searchable fields (logos social profile): Of course you can store non-searchable values in lucene's non-indexed fields. But we have stored all product related information in DB to avoid rebuilding Lucene's index. So Lucene owns only by ID of product and indexed but stored values for key fields. Three languages and multiple fields: We have only 2 languages. So different titles of product can be stored in the same Lucene's document and relate to single ID of product (as I write before ID refers to DB). This allows you search product even if user request uses mix of languages. Obviously title tags and description have different weight for search result. Lucene handles it by assigning to field weight."
613,A,"Python Website Full-Site Search I'm wondering if anyone has any recommendations for a Python full-text search engine similar to mnogosearch. I'm trying to get it to function like Mnogosearch but not sure how that compares to other options (if there are better options out there). Haystack for example seems like it does a great job at indexing given Django model fields that you specify but I'm not sure if it can search the entire contents of a website. Solr/Lucine seem promising but I'm not too familiar with it. Whoosh seems like it might also be an interesting option but I'm wondering if the Python implementation will make it slower? I'm pretty new to search so I'm trying to wrap my head around the different options. Does anyone have any good opinions on which search technologies work well for indexing an entire site? Thanks for reading. Any comments are much appreciated. Joe How many good alternatives do you really need? Solr is perfectly fine together with the solrpy Python bindings. Thanks RestRisiko - that sorlpy option is good to know about. [PyLucene] http://lucene.apache.org/pylucene/ IMHO Solr is probably the best option performance-wise and feature-wise and it's based on heavily tested technology. And if you're looking for a pure Python solution Whoosh looks good - I haven't used it in production just checked it out and looked at the code. It may be slower but I doubt that it's noticeable for sites that get lighter traffic on the search feature. A nice feature whoosh has is an implementation of BM25F which can take into consideration site structure such as title/header/body/footer etc. and is considered state of the art in generic web search models. If you're using Django as far as I know Haystack would let you change the search engine backend much like you are able to change the RDBMS backend. I'm not sure what exactly you mean by ""whole site content"". If most of your site's page content isn't generated from model content then using an integrated search index might not be a best option maybe something like IndexTank or Google site search. But in the opposite case I'd recommend haystack if you are new to either search technology as it will make life easier for you with magic. Thank you Vasil - your post has helped give me some direction on this. It's good to know that Solr seems pretty stable and standard. Sorry to be vague about 'whole site content'. I am looking for something that does scrape pages and integrate them into a site like Google Site Search or IndexTank. Mnogosearch is a PHP-based solution for doing this locally and I was wondering if there existed similar functionality to that in one of the more common tools like Solr/Lucine Whoosh or Xapian as I'm not sure how much momentum Mnogosearch has. @Joe J apache nutch integrates with solr and can crawl a site. Although it might be overkill but a quality solution nevertheless. http://wiki.apache.org/nutch/NutchTutorial"
614,A,"Lucene Entity Extraction Given a finite dictionary of entity terms I'm looking for a way to do Entity Extraction with intelligent tagging using Lucene. Currently I've been able to use Lucene for: - Searching for complex phrases with some fuzzyness - Highlighting results However I 'm not aware how to: -Get accurate offsets of the matched phrases -Do entity-specific annotaions per match(not just tags for every single hit) I have tried using the explain() method - but this only gives the terms in the query which got the hit - not the offsets of the hit within the original text. Has anybody faced a similar problem and is willing to share a potential solution? Thank you in advance for you help! For the offset see this question: How get the offset of term in Lucene ? I don't quite understand your second question. It sounds to me like you want to get the data from a stored field though. To get the data from a stored field: TopDocs results = searcher.Search(query filter num); foreach (ScoreDoc result in results.scoreDocs) { Document resultDoc = searcher.Doc(result.doc); string valOfField = resultDoc.Get(""My Field""); } @Dima_F: I added code to show how to use stored fields. wrt phrase offsets: I don't think you can. You can take a look at what the [highlighter does](http://www.docjar.org/html/api/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java.html) but your best bet might be to modifier the highlighter code to return the offset. Thank you very much for your help on this! I will let you know where I can get with the Highlighter modification. The above is to get the offset for a single Term however I need the offset of the full Phrase that has matched my search. In terms of the stored field how would I get the data directly from it for each on of the dictionary phrases?"
615,A,The process which Lucene tokenizes text This can be considered as a general Java question but for better understanding I'm using Lucene as example. You can use different Tokenizers in Lucene to tokenize text. There's the main abstract Tokenizer class and then many different classes that extend it. Same thing for TokenFilter. Now it seems that each time you want to index a document a new Tokenizer is created. The question is since Tokeinzer is just a utility class why not make it static? for example a Tokenizer that converts all letters to lower case can have a static method that does just that for every input it gets. What's the point of creating a new object for every piece of text we want to index? One thing to mention - Tokeinzer has a private field that contains the input it receives to tokenize. I just don't see why we need to store it this way because the object is destroyed right after the tokenization process is over and the new tokenized text is returned. The only thing I can think of is multi-threaded access maybe? Thank you! Now it seems that each time you want to index a document a new Tokenizer is created This is not true the Analyzer.reusableTokenStream method is called which re-uses not just a Tokenizer but also the entire chain (TokenFilters etc). See http://lucene.apache.org/java/3_0_0/api/core/org/apache/lucene/analysis/Analyzer.html#reusableTokenStream(java.lang.String java.io.Reader) One thing to mention - Tokeinzer has a private field that contains the input it receives to tokenize. I just don't see why we need to store it this way because the object is destroyed right after the tokenization process is over and the new tokenized text is returned. The only thing I can think of is multi-threaded access maybe? As mentioned earlier the entire Chain of tokenizers and tokenfilters is reused across documents. So all of their Attributes are reused but also its important to note that attributes are shared across the chain (e.g. all Tokenizers and TokenFilters' Attribute references point to the same instances). This is why it is crucial to call clearAttributes() in your tokenizer to reset all attributes. As an example a Whitespace tokenizer adds a reference to a TermAttribute in its ctor and its wrapped by a LowerCaseFilter which adds a reference to a TermAttribute in its ctor too. Both these TermAttributes point to the same underlying char[]. When a new document is processed Analyzer.reusableTokenStream is invoked which returns the same TokenStream chain (in this case Whitespace wrapped with LowerCaseFilter) used in the previous document. The reset(Reader) method is called resetting the tokenizer's input to the new document contents. Finally reset() is called on the entire stream which resets any internal state from the previous document and the contents are processed until incrementToken() returns false. Thanks a lot. This makes a lot of sense now.  Dont worry about creating an instance here and there of a class when doing something complex like indexing a document w/ Lucene. There is probably going to be lots and lots of objects created inside the tokenizing and indexing process. One more tokeniser instance is literally nothing when one compares the left over garbage from thrown away objects when the process completes. If you dont believe me get a profile and watch object creation counts.
616,A,"Mocking and Unit Testing Solr and Lucene Index We need control of the data in the production solr index and we need it to be compatible with new development. Ideally we'd like to mock the index on local machines query with it solr and write unit tests to query it for quicker iterations. RamDirectory is used in another question to do something similar but the question is from 2 years back. This example appears to do just that (using FSDirectory instead of RamDirectory). Are these the right approaches to this problem? Are there better ways to do this? We'd like to write tests like: setup mock index; query mock index; assert(stuff that should be true); teardown mock index; EDIT: Additional details: Our thought was we would build an index have a simple way of adding documents without needing the indexer and the rest of the system except perhaps a local database that we could keep in version control. In the past we generated an index and when incompatibilities arose we regenerated it. If we re-index we're adding in a lot of overhead and mocking the indexer doesn't seem like a good option given that our indexer contains a lot of data processing logic (like adding data to searchable fields from a db). Our indexer connects to an external db so we'd need to support that too. We could have a local test database as stated above which has little no overhead. Once we have a test db we need to build an index and then we could go off the second link above. The question becomes how do we build an index really quickly for testing say of the size 1000 documents. The problem with this is we then need to keep our local db schema in sync with the production schema. The production schema changes often enough that this is a problem. We'd like to have a test infrastructure that's flexible enough to handle this- the approach as of now is just rebuild the database each time which is slow and pisses off other people! Oracle and it's pretty optimized Is it it me or do oracle restores also take forever? We were talking about this a bit today and one possibility seems to just do SELECT * on the db and load it into a hash so that there's never a schema problem locally. Columns almost never get removed and the unit tests should work fine if columns are missing/underspecified (for creating docs). haha fortunately I'm not involved in any way shape or form with the db end and for better or for worse it's staying oracle (and for reasons beyond my control) What database are you using... my guess is its MySQL which is notorious for slow backups and restores. We switched to Postgresql because of that. SQLServer also has fast backup/restore. If you are using Solr I wouldn't even bother with mocking or emulating (ie don't change its config). Instead write an integration test that sets up your solr index. The setting up would be to just to index the data like you normally would. You will probably want your developers to run their own solr. I wouldn't worry that much about speed because solr indexes incredible fast (100000 documents in less than 30 seconds for our environment... infact the bottle neck is pulling the data from the database). So really your mock index should just be a small subset of production data that you will index into solr (you can do this once for each TestCase class with @BeforeClass). EDIT (based on your Edits): I'll tell you how we do it (and how I have seen others do it): We have a development schema/db and production schema/db. When developers are working on stuff they just make a copy of the ""build machines"" development database and restore it locally. This database is much smaller than the production db and is ideal for testing. Your production db should no be that much different than your development db schema wise (make smaller changes and release more often if it is the case.) Indexing the data as we normally do takes 2+ hours because we have millions of records! Our indexer has lots of processing logic so we'd prefer not to run it. We don't need production data; just data to test various functionality and performance. Furthermore we want to control this dataset similar to the 'example' link in the original question. This example used the 'LiaTestCase' which loads a local index that's already pre-populated. Is it workable to build the index off a local db?"
617,A,using kohana zend lucene search i have a kohana-based website and i want to use zend-lucene search. as i have seen in the documentation here https://github.com/evopix/kohana-search/blob/master/README.markdown but i do not understand: must i re-create the models i already have in order to be able to use it? the model should extend the ORM_Searchable (it is a must)? and where can i find the ORM_Searchable class in kohana? thanks a lot! You don't have to recreate the models. You extend from a new class implement its methods which specify what data you want to be indexed by Zend Lucene. The documentation is really straight forward. the model should extend the ORM_Searchable (it is a must)? and where can i find the ORM_Searchable class in kohana? This is in the code repository you linked to. classes/kohana/orm/searchable.php
618,A,"Search multiple indices at once using Lucene Search I am using Zend_Search_Lucene to implement site search. I created separate indices for different data types (e.g. one for users one for posts etc). The results are similarly divided by data type however there is an 'all' option which should show a combination of the different result types. Is it possible to search across the different indices at once? or do i have to index everything in an all index? Update: The readme for ZF 1.8 suggests that it's now possible to do in ZF 1.8 but I've been unable to track down where this is at in the documentation. So after some research you have to use Zend_Search_Lucene_Interface_MultiSearcher. I don't see any mention of it in the documentation as of this writing but if you look at the actual class in ZF 1.8 it's straightforward t use $index = new Zend_Search_Lucene_Interface_MultiSearcher(); $index->addIndex(Zend_Search_Lucene::open('search/index1')); $index->addIndex(Zend_Search_Lucene::open('search/index2')); $index->find('someSearchQuery'); NB it doesn't follow PEAR syntax so won'w work with Zend_Loader::loadClass How might one load this class then?  I don't how it integrates with Zend but in Lucene one would use a MultiSearcher instead of the usual IndexSearcher. I found this link in my search before but I am looking for a Zend solution. Thanks though because this does confirm that it can be done.  That's exactly how I handled search for huddler.com. I used multiple Zend_Search_Lucene indexes one per datatype. For the ""all"" option I simply had another index which included everything from all indexes -- so when I added docs to the index I added them twice once to the appropriate ""type"" index and once to the ""all"" index. Zend Lucene is severely underfeatured compared to other Lucene implementations so this was the best solution I found. You'll find that Zend's port supports only a subset of the lucene query syntax and poorly -- even on moderate indexes (10-100 MB) queries as simple as ""a*"" or quoted phrases fail to perform adequately (if at all). When we brought a large site onto our platform we discovered that Zend Lucene doesn't scale. Our index reached roughly 1.0 GB and simple queries took up to 15 seconds. Some queries took a minute or longer. And building the index from scratch took about 20 hours. I switched to Solr; Solr not only performs 50x faster during indexing and 1000x faster for many queries (most queries finish in < 5ms all finish in < 100ms) it's far more powerful. Also we were able to rebuild our 100000+ document index from scratch in 30 minutes (down from 20 hours). Now everything's in one Solr index with a ""type"" field; I run multiple queries against the index for each search each one with a different ""type:"" filter query and one without a ""type:"" for the ""all"" option. If you plan on growing your index to 100+ MB you receive at least a few search requests per minute or you want to offer any sort of advanced search functionality I strongly recommend abandoning Zend_Search_Lucene. Glad to see that you also did what I ended up doing (using a type field to field and used one index). Thanks for the tip on Solr I'll check it out because this is my third time implementing Zend_Lucene and all times I wasn't satisfied. Marked this as the answer before but the new version of ZF 1.8 claims to allow searching across multiple indices though I've been unable to find it in the documentation. (Solr seemed to be a big departure from a lot of stuff that we've written I was interested but have to think about my project first)."
619,A,"With Lucene: Why do I get a Too Many Clauses error if I do a prefix search? I've had an app doing prefix searches for a while. Recently the index size was increased and it turned out that some prefixes were too darned numerous for lucene to handle. It kept throwing me a Too Many Clauses error which was very frustrating as I kept looking at my JARs and confirming that none of the included code actually used a boolean query. Why doesn't it throw something like a Too Many Hits exception? And why does increasing the boolean query's static max clauses integer actually make this error go away when I'm definitely only using a prefix query? Is there something fundamental to how queries are run that I'm not understanding; is it that they secretly become Boolean queries? When running a prefix query Lucene searches for all terms in its ""dictionary"" that match the query. If more than 1024 (by default) match the TooManyClauses-Exception is thrown. You can call BooleanQuery.setMaxClauseCount to increase the maximum number of clauses permitted per BooleanQuery. That makes sense but the issue for me was that I had no way of knowing a PrefixQuery actually became a BooleanQuery.  I've hit this before. It has to do with the fact that lucene under the covers turns many (all?) things into boolean queries when you call Query.rewrite() From: http://lucene.apache.org/java/2_2_0/api/org/apache/lucene/search/Query.html#rewrite(org.apache.lucene.index.IndexReader) public Query rewrite(IndexReader reader) throws IOException Expert: called to re-write queries into primitive queries. For example a PrefixQuery will be rewritten into a BooleanQuery that consists of TermQuerys. Throws: IOException And this `Query.rewrite()` transformation *always* happens before the query is actually executed? (That'd seem reasonable that the query needs to be broken down to primitive queries before being executed.)  The API reference page of TooManyClauses shows that PrefixQuery FuzzyQuery WildcardQuery and RangeQuery are expanded this way (into BooleanQuery). Since it is in the API reference it should be a behavior that users can rely on. Lucene does not place arbitrary limits on the number of hits (other than a document ID being an int) so a ""too many hits"" exception might not make sense. Perhaps PrefixQuery.rewrite(IndexReader) should catch the TooManyClauses and throw a ""too many prefixes"" exception but right now it does not behave that way. By the way another way to search by prefix is to use PrefixFilter. Either filter your query with it or wrap the filter with a ConstantScoreQuery. Is `PrefixFilter` also expanded into boolean clauses? (Or is it implemented differently somehow?)"
620,A,"Searching phrases in Lucene Could somebody point me to an example how to search for phrases with Lucene.net? Let's say I have in my index a document with field ""name"" value ""Jon Skeet"". Now I want to be able to find that document when searching for ""jon skeet"". What would be the code for the Proximity Search with Lucene.Net ? You can use a proximity search to find terms within a certain distance of each other. The Lucene query syntax looks like this ""jon skeet""~3 meaning find ""jon"" and ""skeet"" within three words of each other. With this syntax relative order doesn't matter; ""jon q. skeet"" ""skeet q. jon"" and ""jon skeet"" would all match. If you have a list of phrases that you want to treat as a single token you need to take care of that in your analyzer. For instance you want to treat ""near east"" ""middle east"" and ""far east"" as individual tokens. You need to write an analyzer with some lookahead so that it can treat these phrases as if they were one word. This analyzer is used both in the indexer and against user input in the search application."
621,A,Is lucene index created with 2.3.1 compatible with Lucene 3.0.3 We have indexed our documents with Lucene 2.3.1 and now want to move to Lucene 3.0.3 for better features. I want to know whether the index will work as is and will I be able to add more documents with 3.0.3 to the existing index without any hassles or do I have to re-index the whole thing. Thanks a lot in advance. I am quite sure that the indexes will be incompatible with Lucene 3 if they were built under Lucene 2 (in fact I'm 99% positive of this). However you may be able to convert them rather than rebuild them. Have a look here for some high-level guidance in this area. Thanks Brent. That article was very informative. As it mentions the change seems to be only in the string compression. If I had not used any compression at all (Field.Store.COMPRESS) then it should not be an issue I think. Am I right ? Based on the article it would seem that you are correct. Best to give the upgrade a try (maybe on a branch of your code base) and see what happens. I think it will work out to be quite simple in your case. The key note is that he's using a really old version in the 2.x branch. I believe that the newest version of the 2.x writers (2.9.x) would write indexes that 3.0 readers can work with but not the other way around. Thanks Brent and Simon. I will give direct switch a try or else go 2.3.1 -> 2.9.4 -> 3.0.3 readers first and then writer.
622,A,"Display ellipsis before and after fragment in SOLR I have SOLR configured to return fragments with a fragsize of 500. Sometimes the whole field is 500 characters or less so the fragment is identical to the field. For fields that are longer than that SOLR just returns the fragment without any indication (or so it seems) that the fragment only represents part of the content of a field. That means the fragment could start mid-sentence. I want to make it clear to users that they're looking at a fragment and simply display ellipsis at the end and/or start of such a fragment. Is that functionality built into SOLR? If not how would you go about inserting ellipsis? I just dealt with the same issue. The way I went about doing this is the following: Get the original string Get both the first and last 10 characters of the original string Get both the first and last 10 characters of the fragment returned by the Solr search Compare both and fill in a variable when needed! $f_ellip = NULL; $l_ellip = NULL; if ($orig_body_beggining != substr((string)$hl_content->str 0 10)) { $f_ellip = ""&#8230; ""; } if ($orig_body_end != substr((string)$hl_content->str 0 -10)) { $l_ellip = "" &#8230;""; } $entry_body = $f_ellip.(string)$hl_content->str.$l_ellip;  Here's another better approach. While creating the index add a ""teaser"" attribute that has this logic already applied. Push that effort out of your requests altogether. That's exactly how I would do this. At index-time you don't know which part of the field matches and thus what the fragment is going to be.  What I ended up doing was returning both the fragment and the unaltered field from which the fragment was created. I then wrote some logic that compared the two in order to determine whether ellipsis should be added to the fragment and if so whether to add it before after or both before and after the fragment. That's what I did too :) +1  Solr won't return an indicator you're right. Set your fragsize to 501. Then its a quick bit of logic in your UI to determine whether ellipsis should be displayed or not. Truncate anything 501 to 500 and add ellipsis. Yeah I was just hoping that there is a built-in way to do that. Also that still leaves me with the problem of not knowing whether to display the ellipsis at the beginning or the end of the fragment (or both)."
623,A,lucene and plurals If you look at the comment here you'll see Lucene is very much the tool to do this. If you want apple and apples (plural) to match you just need to be careful about using the correct language stemmer when indexing and querying the index. I'm new to lucene and barely understand how adding and saving document work. How do I search my tag field so apples and apple are the same? I am using lucene.net 2.9.1 I suppose you're looking for the stemming algorithm here is an example reduced for plurals You may find how-to-enable-stemming-when-searching-using-lucene-net helpful for .net
624,A,"Hibernate Search - searching in given scope Let's say I have following classes. (only most important things included) public class Client { /* Some Properties */ } public class ClientDocumentAssociation { @ManyToOne private Client client; /* Some Properties */ } @Indexed public class Document { @OneToOne private ClientDocumentAssociation clientAssociation; @Field(name = ""text"") private String text; /* Some Properties */ } My basic document search is like this: public List<AbstractDocument> searchDocuments(String text) { if (text == null) { return newArrayList(); } FullTextEntityManager ftem = Search.getFullTextEntityManager(entityManagerProvider.get()); MultiFieldQueryParser parser = new MultiFieldQueryParser(DOCUMENT_FIELDS new StandardAnalyzer()); parser.setDefaultOperator(Operator.AND); FullTextQuery ftq; try { Query q = parser.parse(text + ""*""); ftq = ftem.createFullTextQuery(q Document.class); ftq.setMaxResults(20); List<AbstractDocument> results = ftq.getResultList(); return results; } catch (ParseException e) { e.printStackTrace(); } return newArrayList(); } Now I want to be able to search for documents but not in the scope of the whole index but just find documents that belong to given Client. The only thing that comes to my mind is adding the association to the index and add client id to the appropriate field in search. But that does not seem right. There must be another option and that's what I am asking for. The other way you can do this is to use Filters. A filter can be applied to a Lucene search. Hibernate supports adding filters as annotations and enabling them at run time  Why does your initial idea seem to be wrong? In fact indexing all the data needed for your search is the recommended way of doing this. That's what @IndexedEmbedded is there for. Indexing the data will also give you more flexibility for changes in the query and/or new queries. Well I thought that using indexing for some values would not be appropriate (eg filtering on some boolean data doesn't seem right) + given example is simplified and real associations are far more complex -therefore I wanted to filter results some other way. Hmmm ... I did some research and I think you are right that Criteria are not the best way to do this. I will go with indexing the other values. Thanks.  Ok I actually found a solution. The thing I (and anyone who was searching for solution of the same problem) needed is setting up Criteria for the FullTextQuery.  Session session = (Session) ftem.getDelegate(); Criteria criteria = session.createCriteria(Document.class).createCriteria(""clientAssociation"").add( Restrictions.eq(""client"" owner)); /* .... */ ftq.setCriteriaQuery(criteria); Seems to work ok :) This is not recommended. For example getResultSize() will return an incorrect value and pagination will be screwed. I'd recommend Hardy's or Kango_V's solutions"
625,A,Proper structuring of Lucene.Net usage in an ASP.NET MVC site I'm building an ASP.NET MVC site where I plan to use Lucene.Net. I've envisioned a way to structure the usage of Lucene but not sure whether my planned architecture is OK and efficient. My Plan: On Application_Start event in Global.asax: I check for the existence of the index on the file system - if it doesn't exist I create it and fill it with documents extracted it from the database. When new content is submitted: I create an IndexWriter fill up a document write to the index and finally dispose of the IndexWriter. IndexWriters are not reused as I can't imagine a good way to do that in an ASP.NET MVC application. When content is edited: I repeat the same process as when new content is submitted except that I first delete the old content and then add the edits. When a user searches for content: I check HttpRuntime.Cache to see if a user has already searched for this term in the last 5 minutes - if they have I return those results; otherwise I create an IndexReader build and run a query put the results in HttpRuntime.Cache return them to the user and finally dispose of the IndexReader. Once again IndexReaders aren't reused. My Questions: Is that a good structure - how can I improve it? Are there any performance/efficiency problems I should be aware of? Also is not reusing the IndexReaders and IndexWriters a huge code smell? It would be awesome if you wrote a short step-by-step tutorial on how you integrated Lucene.NET with your ASP.NET MVC site preferably as a wiki-style answer on SO. @FreshCode Good call. My implementation is not perfect but it works and I think I'll write it up as soon as my finals end next week. I've been meaning to publish a bunch of ASP.NET MVC helpers anyway so I'll keep you posted. @MaximZaslavsky Did you ever write that tutorial? I would be interested in reading it. I would probably skip the caching -- Lucene is very very efficent. Perhaps so efficent that it is faster to search again than cache. The OnApplication_Start full index feels a bit off to me -- should probably be run in it's own thread so as not to block other expensive startup activities. thanks for the tips!  The answer to all three of your questions is the same: reuse your readers (and possibly your writers). You can use a singleton pattern to do this (i.e. declare your reader/writer as public static). Lucene's FAQ tells you the same thing: share your readers because the first query is reaaalllyyyy slow. Lucene handles all the locking for you so there is really no reason why you shouldn't have a shared reader. It's probably easiest to just keep your writer around and (using the NRT model) get the readers from that. If it's rare that you are writing to the index or if you don't have a huge need for speed then it's probably OK to open your writer each time instead. That is what I do. Edit: added a code sample: public static IndexWriter writer = new IndexWriter(myDir); public JsonResult SearchForStuff(string query) { IndexReader reader = writer.GetReader(); IndexSearcher search = new IndexSearcher(reader); // do the search } Thanks for your answer. This means that I should just put the IndexReader as a controller `public static` field? Also how do I renew the IndexReader (when the index is updated)? :) Or are you saying that it's better to keep the writer around rather than the reader? Yes make it a public static field. Unless you will have multiple processes writing to the same location I think it is better to persist the writer and use the NRT model to get your readers. If you decide to persist readers though reader.IsCurrent() will tell you if the reader is current and reader.Reopen() will reopen it. I added a code sample for the NRT style. @jorgebg: you should only need to close the writer when your app shuts down (in the general case) Isn't it necessary to close the IndexWriter afterwards or frequently commiting is enough? @Xodarap - This post was really useful thanks! In our MVC app we're not writing directly to the index because we didn't want to block so we're using message queuing to handle the writes. Do you think this is a good strategy? We're making our Reader static and reopening it when it is not current. @Pandincus: Yes I think queuing is a good way to make it non-blocking (as long as you are willing to lose queued data in the event of a crash).
626,A,"Solr commit taking too long My commit seems to be taking too much time if you notice from the Dataimport status given below to commit 1000 docs its taking longer than 24 minutes <str name=""status"">busy</str> <str name=""importResponse"">A command is still running...</str> <lst name=""statusMessages""> <str name=""Time Elapsed"">0:24:43.156</str> <str name=""Total Requests made to DataSource"">1001</str> <str name=""Total Rows Fetched"">1658</str> <str name=""Total Documents Skipped"">0</str> <str name=""Full Dump Started"">2011-06-07 09:15:17</str> <str name=""""> Indexing completed. Added/Updated: 1000 documents. Deleted 0 documents. </str> </lst> What can be causing this I have tried looking for a reason or a way to improve this but am just not able to find. At this rate my documents would never get indexed given that I have more than 100000 records coming into the database every hour. Regards Rohit Would need to see your data import handler configuration to get a better idea. Have you looked at your database latency? Run the same query on your database and see how long it takes... Have added optimize=false in the data import handler. This has made things faster to some extent now I am optimizing only during off peak hours.  I don't know if you use solrj public abstract class SolrServer but if you do you really need to index by chuncks/collections:  public UpdateResponse add(Collection<SolrInputDocument> docs ) and not one by one"
627,A,How to define a boost factor to each term in each document during indexing? I want to insert another score factor in Lucene's similarity equation. The problem is that I can't just override Similarity class as it is unaware of the document and terms it is computing scores. For example in a document with the text below: The cat is in the top of the tree and he is going to stay there. I have an algorithm of my own that assigns for each one the terms in this document a score regarding how much each one of them are important to the document as whole. A possible score for each word is: cat: 0.789212 tree: 0.633423 top: 0.412315 stay: 0.123912 there: 0.0999842 going: 0.00988412 ... The score for each word is different from document to document. For example in another document cat could have score: 0.0023912 I want to add this score to the Lucene's scoring but I'm kind of lost on how to do that. Any tips? Use Lucene's Payload feature: From: http://www.lucidimagination.com/blog/2009/08/05/getting-started-with-payloads/ Add a Payload to one or more Tokens during indexing. Override the Similarity class to handle scoring payloads Use a Payload aware Query during your search
628,A,"Lucene index backup What is the best practice to backup a lucene index without taking the index offline (hot backup)? Lucene in action prescribes a way using SnapshotDeletionPolicy but does not explain much more. Tthat is while having acquired an IndexWriter with SnapshotDeletionPolicy can other IndexWriters write to the index? If not can ""this"" IndexWriter write to the index? Create a new index with a separate IndexWriter and use addIndexesNoOptimize() to merge the running index into the new one. This is very slow but it allows you keep the original index operational while doing the backup. However you cannot write to the index while merging. So even if it is online and you can query the index you cannot write to it during the backup. This is what I used to do but as you said it is not hot and it is slow. Thus the best solution is the cp -lr method since it only takes millis. yeah being pragmatic is better taking the index offline for a little while is preferable to a slow slow semi-hot copy :)  You don't have to stop your IndexWriter in order to take a backup of the index. Just use the SnapshotDeletionPolicy which lets you ""protect"" a given commit point (and all files it includes) from being deleted. Then copy the files in that commit point to your backup and finally release the commit. It's fine if the backup takes a while to run -- as long as you don't release the commit point with SnapshotDeletionPolicy the IndexWriter will not delete the files (even if eg they have since been merged together). This gives you a consistent backup which is a point-in-time image of the index without blocking ongoing indexing. I wrote about this in Lucene in Action (2nd edition) and there's paper excerpted from the book available (free) from http://www.manning.com/hatcher3 ""Hot Backups with Lucene"" that describes this in more detail. You can just use it always; there will be no real change to perf. Thank you for replying! In the book it is not clear though whether I can always use the SnapshotDeletionPolicy decorator for my IndexWriters. Since I will use just one IndexWriter for my application that will not be closing frequently can it always use SnapshotDeletionPolicy or this will have impact on the performance?  This answer depends upon (a) how big your index is and (b) what OS you are using. It is suitable for large indexes hosted on Unix operating systems and is based upon the Solr 1.3 replication strategy. Once a file has been created Lucene will not change it it will only delete it. Therefore you can use a hard link strategy to make a backup. The approach would be: stop indexing (and do a commit?) so that you can be sure you won't snapshot mid write create a hard link copy of your index files (using cp -lr) restart indexing The cp -lr will only copy the directory structure and not the files so even a 100Gb index should copy in less than a second. Your answers seems to be the most performant but: 1. works only on filesystems where hard links are supported 2. requires that all indexing operations are stopped. Could there be another way? IF your filesystem does not support hard links then you're going to have to copy the files which is slower but still works. Actually it doesn't require that indexing operations are stopped it just requires that no commits/writes are done while the copy is happening. Which I am afraid means that indexing should be stopped (searching is allowed). Anyway your answer is probably the best one might try.  In my opinion it would typically be enough to stop any ongoing indexing operation and simply take a file copy of your index files. Also look at the snapshooter script from Solr which can be found in apache-solr-1.4.1/src/scripts which essentially does: cp -lr indexLocation backupLocation Another options might be to have a look at the Directory.copy(..) routine for a progammatic approach (e.g. using the same Directory given as constructor parameter to the IndexWriter. You might also be interested in Snapshooter.java which does the equivalent of the script."
629,A,"how to delete documents using term in lucene I am trying to delete a document by using a term in lucene index. but the code that I made below isn't working. are there any suggestion of how can I perform deleting function in lucene index? public class DocumentDelete { public static void main(String[] args) { File indexDir = new File(""C:/Users/Raden/Documents/lucene/LuceneHibernate/adi""); Term term = new Term(FIELD_PATH ""compatible""); Directory directory = FSDirectory.getDirectory(indexDir); IndexReader indexReader = IndexReader.open(directory); indexReader.deleteDocuments(term); indexReader.close(); } } agreed unless of course FIELD_PATH is actually in your code and not a valid field string like ""title"" etc so anyway I had resorted to delete a document in an index in lucene is to use a tool name luke http://code.google.com/p/luke/ .as I am a bit loss in using lucene API to delete it.but maybe if you kind enough then you could give me a sample source code of how to delete documents in lucene index. :-) is that term even matching any documents? Do a search for that term first to see if you get any results back. Your code looks good hmm ... The code does look fine. Maybe the index has a lock file that you should delete. If this does not work please add to your question exactly how you see that the deletion fails. IndexReader indexReader = IndexReader.open(directory); // this one uses default readonly mode instead use this: IndexReader indexReader = IndexReader.open(directory false); // this will open the index in edit mode and you can delete the index. . . So you do not need any extra tool for deleting index contents. . ."
630,A,"Solr / SolrNet - Using wildcards for letter by letter search Hey Guys Im trying to implement some search functionality to an application were writing. Solr 1.4.1 running on Tomcat7 JDBC connection to a MS SQLServer with the View im indexing Solr has finished indexing and the index is working. To search and communicate with Solr i have created a little test WCF service (to be implemented with our main service later). The purpose is to implement a textfield in our main application. In this text field the users can start typing something like Paintbrush and gradually filter through the list of objects as more and more characters are input. This is working just fine and dandy with Solr up to a certain point. Im using the Wildcard asterisk in the end of my query and as such im throwing a lot of requests like p* pa* pain* paint* etc. at the server and its returning results just fine (quite impressively fast actually). The only problem is that once the user types the whole word the query is paintbrush* at which point solr returns 0 results. So it seems that query+wildcard can only be query+something and not query+nothing I managed to get this working under Lucene.Net but Solr isnt doing things the same way it seems. Any advice you can give me on implementing such a feature? there isn't much code to look at since im using SolrNet: http://pastebin.com/tXpe4YUe I figure it has something to do with the Analyzer and Parser but im not yet that into Solr to know where to look :) Stemming seems to be what caused the problem. I fixed it using a clone of text_ws instead of text for the type. My changes to scema.xml : http://pastebin.com/xaJZDgY4 Stemming is disabled and lowercase indexing is enabled. As long as all queries are in lower case they should always give results (if there at all). Issue seems to be that Analyzers dont work with Wildcards so the logic that would make Johnny the result of Johni or Johnni is ""broken"" when using wildcards. If your facing similiar problems and my solution here doesnt quite work you can add debugQuery=on to your query string and see a bit more about whats going on. That helped me narrow down the problem.  I wouldn't implement suggestions with prefix wildcard queries in Solr. There are other mechanisms better suited to do this. See: Simple Solr schema problem for autocomplete Solr TermsComponent: Usage of wildcards Looks interresting but it doesnt seem to do quite what i was interrested in. Here is the schema im using: http://pastebin.com/fF5Kisgd what i want is the ID of those that partially or fully match something in ""text"" which i ask for. Text is a concat of all the fields of a given row in my database (using a view) @mfriis: yes that is exactly what I'm talking about. Can you elaborate on how your case is different? @mfriis: you might want to use ngrams instead. Well it sounds like im on the right track anyway then. The output just seems plain weird: http://pastebin.com/H2M8aX2s im using this query: solr/terms?terms.fl=text&terms.regex=byg.*&terms.regex.flag=case_insensitive in my index i have multiple objects with ""bygning"" in its text field. Now what i need are all those objects listed with their ID and Type (the other 2 fields in my documents schema). The output here seems semi random (the names in the result list are known to me but partial at best)? I cant believe Solr cant do something that simple there must be something i am missing. I find it hard to believe i am the first who needs to do a wildcard search where the full word + wildcard still gives a result of the full word. @mfriis: not sure why you say that Solr can't do it I pointed out several ways to solve this."
631,A,"how to add synonym index on lucene I have a synonym table with list of synonyms comma separated. I want Lucene to consider these synonyms also while I indexing a text value. I am using StandardAnalyzer to Index. How can I make Lucene to make use of Synonym as well? Here is my code using StandardAnalyzer FSDirectory fsDircetory = FSDirectory.open(new File(""/home/lenovo/index"")); indexwriter = new IndexWriter(fsDircetory new StandardAnalyzer(Version.LUCENE_31) trueIndexWriter.MaxFieldLength.LIMITED); Document doc = new Document(); doc.add(new Field(""title""recipeVO.getTitle() Field.Store.YESField.Index.ANALYZED)); indexwriter.addDocument(doc); indexwriter.close() there has been a similar question that has been answered here: Synonyms using Lucene It is there But it doesn't have any implementation details. We can do with Wordnet But How we can build our own synonym Library on it? You just have to create/edit your own wordnet dictionary. You could use this tool (http://mac.softpedia.com/get/Development/Libraries/extjwnl.shtml) for example or just google for another one ;)"
632,A,"Why does this Lucene.Net query fail? I am trying to convert my search functionality to allow for fuzzy searches involving multiple words. My existing search code looks like:  // Split the search into seperate queries per word and combine them into one major query var finalQuery = new BooleanQuery(); string[] terms = searchString.Split(new[] { "" "" } StringSplitOptions.RemoveEmptyEntries); foreach (string term in terms) { // Setup the fields to search string[] searchfields = new string[] { // Various strings denoting the document fields available }; var parser = new MultiFieldQueryParser(Lucene.Net.Util.Version.LUCENE_29 searchfields new StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_29)); finalQuery.Add(parser.Parse(term) BooleanClause.Occur.MUST); } // Perform the search var directory = FSDirectory.Open(new DirectoryInfo(LuceneIndexBaseDirectory)); var searcher = new IndexSearcher(directory true); var hits = searcher.Search(finalQuery MAX_RESULTS); This works correctly and if I have an entity with the name field of ""My name is Andrew"" and I perform a search for ""Andrew Name"" Lucene correctly finds the correct document. Now I want to enable fuzzy searching so that ""Anderw Name"" is found correctly. I changed my method to use the following code:  const int MAX_RESULTS = 10000; const float MIN_SIMILARITY = 0.5f; const int PREFIX_LENGTH = 3; if (string.IsNullOrWhiteSpace(searchString)) throw new ArgumentException(""Provided search string is empty""); // Split the search into seperate queries per word and combine them into one major query var finalQuery = new BooleanQuery(); string[] terms = searchString.Split(new[] { "" "" } StringSplitOptions.RemoveEmptyEntries); foreach (string term in terms) { // Setup the fields to search string[] searchfields = new string[] { // Strings denoting document field names here }; // Create a subquery where the term must match at least one of the fields var subquery = new BooleanQuery(); foreach (string field in searchfields) { var queryTerm = new Term(field term); var fuzzyQuery = new FuzzyQuery(queryTerm MIN_SIMILARITY PREFIX_LENGTH); subquery.Add(fuzzyQuery BooleanClause.Occur.SHOULD); } // Add the subquery to the final query but make at least one subquery match must be found finalQuery.Add(subquery BooleanClause.Occur.MUST); } // Perform the search var directory = FSDirectory.Open(new DirectoryInfo(LuceneIndexBaseDirectory)); var searcher = new IndexSearcher(directory true); var hits = searcher.Search(finalQuery MAX_RESULTS); Unfortunately with this code if I submit the search query ""Andrew Name"" (same as before) I get zero results back. The core idea is that all terms must be found in at least one document field but each term can reside in different fields. Does anyone have any idea why my rewritten query fails? Final Edit: Ok it turns out I was over complicating this by a LOT and there was no need to change from my first approach. After reverting back to the first code snippet I enabled fuzzy searching by changing finalQuery.Add(parser.Parse(term) BooleanClause.Occur.MUST); to finalQuery.Add(parser.Parse(term.Replace(""~"" """") + ""~"") BooleanClause.Occur.MUST); What are the values of MIN_SIMILARITY and PREFIX_LENGTH? What's the value of finalQuery.ToString()? I added the constants I used into my post. What analyzer are you using when indexing? You want this line: var queryTerm = new Term(term); to look like this: var queryTerm = new Term(field term); Right now you're searching field term (which probably doesn't exist) for the empty string (which will never be found). Unfortunately that did not completely fix my issue. The previously mentioned search of `Andrew Name` when the Name field is ""My Name Is Andrew"" does not return any results.  Your code works for me if I rewrite the searchString to lower-case. I'm assuming that you're using the StandardAnalyzer when indexing and it will generate lower-case terms. You need to 1) pass your tokens through the same analyzer (to enable identical processing) 2) apply the same logic as the analyzer or 3) use an analyzer which matches the processing you do (WhitespaceAnalyzer). I'm using a `StandardAnalyzer` for indexing but now that I look at my code 2nd code block I'm not using any analyzer. However I don't immediately see how to use an Analyzer in my 2nd code block. I know how to use it in the first example because I am using a query parser but I'm not using one in the 2nd code block Use the QueryParser so you use the analyzer and get support for other queries (like phrases). Just iterate through the generated query and replace TermQuery with FuzzyQuery. There's a QueryVisitor you can use at http://devhost.se/blog/post/2011/04/21/A-QueryVisitor-for-Lucene.aspx I actually just realized I didn't need to use my 2nd method at all and instead I just needed to use the first approach but add ~ to the search terms (see my edit) I am going to mark this as the answer because it was your advice about the analyzer that actually put me on the right path to getting this working"
633,A,"How do i search 'and' with lucene? I am looking at the query syntax. and i could not figure out how to search 'and'. I tried ""a sentence with and and words after it"" i tried +and and \and. It always ignored it. How can i search 'and'? I am using lucene.net Have you tried AND in all caps? If you want to do an AND search you have a couple options. You can do +foo +bar or foo AND bar but foo and bar will not work. no i am not trying to do an and search and if you mean search and then no that does not work either (i ran it using AND and got no results) gotcha misunderstood the question.  Are you including 'and' in the index so its searchable? If your using the StandardAnalyzer to index your documents 'and' is included in the default stop words list. You can pass your own list of stop words as a string array to the constructor of the StandardAnalyzer if you want to include the word 'and' in the index."
634,A,"Query types within Lucene Lucene NOOB alert! I consider myself to be a human of at least reasonable intelligence however I am having enormous problems mentally grokking the query types within Lucene. In my particular instance I need to search a single string field in my document that is of only moedrate length (avg around 50 chars). I want the user to be able to type the start of words within the item they are searching for. And I also want to not have to dictate the order they provide the terms. Example field : ""generic brand strength"" Should match searches : ""generic brand strength"" ""brand generic strength"" ... ""gen bran str"" ""bran generic str"" ... etc. It is possible for me to store my information (each word in the example) in seperate fields if that would help but I am not convinced that it would. I am currently lost in a world of Fuzzy Wildcards and Multi-term Phrases. Can anyone clarify this whole scenario for me? (And yes I have looked extensively online for help but cannot find a decent resource). BTW I am using Lucene 2.9 but I don't think that really matters. You need not store each term within a separate field. Lucene creates tokens out of each term (if you are using a whitespace tokenizer) hence allows for great flexibility of search. To your question about: Example field : ""generic brand strength"" Should match searches : ""generic brand strength"" ""brand generic strength"" The above query will return both the results the latter with a lower score for obvious reasons. However ""gen bran str"" ""bran generic str"" ... etc. is tricky since it appears the terms are not standard ""stems"" in which case you can use a stemmer analyzer. The simplest approach would be to: Split your query phrase by the white space so you have a string[] Use a Booleanquery and create a query for each term appending a wildcard at the end. Something like: string[] terms = query.split("" ""); BooleanQuery bq = new BooleanQuery(); foreach(string term in terms) bq.Add(new Query(""FieldName"" term + ""*""...); There are better query types such as SpanQuery DisMax etc.  but since you mentioned a noob alert think the above is simplest (although prolly not most elegant) approach. HTH Yes they are identical. The Javadocs describe it thus: ""A Query that matches documents containing terms with a specified prefix. A PrefixQuery is built by QueryParser for input like app*."" +1 for the wildcard solution. Easiest although it might produce performance problems for large indexes. Is there a notable difference between using a wilcard query (by appending an asterisk) and the PrefixQuery? They seem identical to me. Thoughts? Do they basically boil down to the same thing?"
635,A,What is the VInt in Lucene? I want to know what is the VInt in Lucene ? I read this article  but i don't understand what is it and where does Lucene use it ? Why Lucene doesn't use simple integer or big integer ? Thanks . VInt is extremely space efficient. It could theoretically save upto 75% space. In Lucene many of the structures are list of integers. For example list of documents for a given term positions (and offsets) of the terms in documents among others. These lists form bulk of the lucene data. Think of Lucene indices for millions of documents that need tens of GBs of space. Shrinking space by more than half reduces disk space requirements. While savings of disk space may not be a big win given that disk space is cheap the real gain comes reduced disk IO. Disk IO for reading VInt data is lower than reading integers which automatically translates to better performance.  VInt refers to Lucene's variable-width integer encoding scheme. It encodes integers in one or more bytes using only the low seven bits of each byte. The high bit is set to zero for all bytes except the last which is how the length is encoded. I know this  but i want to know why lucene does this work ? Why it doesn't use simple integer(0 - ~4000000000) in 4 byte ?
636,A,"What are indexes in Lucene? What are the indexes in Lucene and how it works? I have gone through some articles on net and google but I could not understand the concept of the index documents etc fully. Please help if anyone can explain in simple terms the term index and the indexing. Thanks ! Lucene creates an inverted full-text index it splits the documents into words builds an index for each word. For Instance: Document 1: ""Apache Lucene Java"" Document 2: ""Java Library"" Inverted Index: Tokens Document Location apache 1 Library 2 Java 1 2 Lucene 1 Lets expand is further now lets consider Document with two Fields. Body and Title. Document doc = new Document() doc.add(new Field(""body"" ""This is my Test document"" Field.Store.YES Field.Index.TOKENIZED) doc.add(new Field(""title"" ""Test document"" Field.Store.YES Field.Index.UNTOKENIZED) You have the flexibility to tokenize or not tokenize a Field. Luncene has various analyzer using the StandardAnalyzer Analyzer analyzer = new StandardAnalyzer() above document would be tokenized ""my"" ""Test"" ""document"" ""test document""  Say you have a bunch of information you would like to make searchable. For example some HTML files some PDFs and some information stored in a database. When a user does a search you could write a search engine that trawls through this information and return results that match. However this is typically way too slow for large sets of data. So in advance of running our application we create an index of the information that needs to be searchable. The index contains a summary of each piece of information we would like to include in the search. In Lucene the summary for an information piece is called a document. A document contains a number of fields. When creating the index you decide which fields to include based on what you would like to be searchable. For example you may include a title an id category string and so forth. Once the fields are defined you create a document in the index for each information item (html pdf database entries etc). This process is called indexing. The search engine can now use the index to search for things. The index is highly optimized for the typical searches that we do. You can search for information in specific fields and do boolean logic. You can search for precise matches or fuzzy ones. And the search engine will weigh/score your documents in the index returning the most relevant first. Hope that helps at a high level."
637,A,"Group and count multiple fields at once Is it possible to merge these queries so the search criteria is executed only once? SELECT category count(*) FROM ads WHERE [search criteria] GROUP BY category SELECT state count(*) FROM ads WHERE [search criteria] GROUP BY state SELECT price_range count(*) FROM ads WHERE [search criteria] GROUP BY price_range ... Plus about 10 more COUNTS My aim is to create a filter similar to that is used in this website. I'm thinking about using some kind of search engine (eg. Lucene) instead of relational database. New ideas are welcome. If your use case includes full-text search in addition to the queries you listed Solr might be worth looking at. In incorporates faceted search over structured and text data and can do the kinds of counts you want reasonably efficiently. But it depends in part on how much data you have and what your use cases are. +1 I've been searching for Lucene which Solr is based on but haven't found how to do those counts without iterating over results. I've been searching for other no-sql solutions as well. Solr will do the counting for you; just specify which categories should be included in the results. You don't access Lucene directly when using Solr because Solr wraps that for you. Perhaps I am mis-understanding your request: can you say more?  you can do by using nested query and some tricks ;)  select categorycountCategorystatecountStateprice_rangecountPrice_range from (SELECT 1 pcolcategory count(*) countCategory FROM ads WHERE [search criteria] GROUP BY category) a inner join ( SELECT 1 pcolstate count(*) countState FROM ads WHERE [search criteria] GuROUP BY state ) b on a.pcol=b.pcol inner join (SELECT 1 pcolprice_range count(*) countPrice_range FROM ads WHERE [search criteria] GROUP BY price_range) ) on a.pcol=b.pcol  Eduardo Are you allowed to use a stored procedure? If so you can execute the common part first. SELECT ... FROM ads WHERE [search criteria] Then you can do your multiple queries on the smaller table. You can also store the results and return the whole thing as one table with three columns: type (category state price range etc) name (actually category/state/etc value) count Hi Jeanne. If I do so I will execute search criteria multiple times inside the stored procedure. Not if you only call the stored procedure once. The stored procedure executes the search criteria once and stores the results in a ""temporary table"" (more like a temporary variable.) The other queries run against the temporary set of data not by repeatedly executing the where clause. Note that if you are able to use analytic functions that is better. I didn't want to presume your database. I had misunderstood. Should I index that temp table? No need to index it. The temp table only exists for the scope of the stored procedure. It only contains the results that passed your where clause so you are past the point of indexing. If your where clause is going to be the same across users/calls to this query (and your database supports them) you might want to consider materialized views instead. Hi Jeanne. I believe I found a solution very close to yours. I create a key-value table and store: 1. object type (ad) 2. key(category state price range etc) 3. value (actually category state etc value). Count is done by SELECT COUNT(...) . This way I can store anything and combine like a faceted search.  What's the DBMS? In Oracle you can do that with analytic functions: SELECT category state price_range COUNT( 1 ) OVER ( PARTITION BY category ) AS category_count COUNT( 1 ) OVER ( PARTITION BY state) AS state_count COUNT( 1 ) OVER ( PARTITION BY price_range ) AS price_count FROM ads Very nice feature. Unfortunately it's MySQL. You probably have to go with a temp table than."
638,A,"how to integrate RAMDirectory into FSDirectory in lucene I had a question now this one regarding lucene. I was trying to make a lucene source code that can do indexing and store them first in a memory using RAMDirectory and then flush this index in a memory into a disk using FSDirectory. I had done some modifications of this code but to no avail. maybe some of you can help me out a bit. so what's the best way for me to integrate RAMDirectory in this source code before putting them in FSDirectory. any help would be appreciated though here is the source code. import org.apache.lucene.analysis.SimpleAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.store.FSDirectory; import java.io.File; import java.io.FileReader; import java.io.IOException; public class SimpleFileIndexer { public static void main(String[] args) throws Exception { File indexDir = new File(""C:/Users/Raden/Documents/lucene/LuceneHibernate/adi""); File dataDir = new File(""C:/Users/Raden/Documents/lucene/LuceneHibernate/adi""); String suffix = ""txt""; SimpleFileIndexer indexer = new SimpleFileIndexer(); int numIndex = indexer.index(indexDir dataDir suffix); System.out.println(""Total files indexed "" + numIndex); } private int index(File indexDir File dataDir String suffix) throws Exception { IndexWriter indexWriter = new IndexWriter( FSDirectory.open(indexDir) new SimpleAnalyzer() true IndexWriter.MaxFieldLength.LIMITED); indexWriter.setUseCompoundFile(false); indexDirectory(indexWriter dataDir suffix); int numIndexed = indexWriter.maxDoc(); indexWriter.optimize(); indexWriter.close(); return numIndexed; } private void indexDirectory(IndexWriter indexWriter File dataDir String suffix) throws IOException { File[] files = dataDir.listFiles(); for (int i = 0; i < files.length; i++) { File f = files[i]; if (f.isDirectory()) { indexDirectory(indexWriter f suffix); } else { indexFileWithIndexWriter(indexWriter f suffix); } } } private void indexFileWithIndexWriter(IndexWriter indexWriter File f String suffix) throws IOException { if (f.isHidden() || f.isDirectory() || !f.canRead() || !f.exists()) { return; } if (suffix != null && !f.getName().endsWith(suffix)) { return; } System.out.println(""Indexing file "" + f.getCanonicalPath()); Document doc = new Document(); doc.add(new Field(""contents"" new FileReader(f))); doc.add(new Field(""filename"" f.getCanonicalPath() Field.Store.YES Field.Index.ANALYZED)); indexWriter.addDocument(doc); } } Suggestion: add the lucene tag to your question. There is a number of knowledgeable Lucene developers in StackOverflow and they will find your question if you tag it properly. ok thanks for the suggestion. :-) I'm not really sure that you'll get any performance gain from doing this but you could do all the indexing on a RAMDirectory and then copy the directory to an FSDirectory. Like this: private int index(File indexDir File dataDir String suffix) throws Exception { RAMDirectory ramDir = new RAMDirectory(); // 1 IndexWriter indexWriter = new IndexWriter( ramDir // 2 new SimpleAnalyzer() true IndexWriter.MaxFieldLength.LIMITED); indexWriter.setUseCompoundFile(false); indexDirectory(indexWriter dataDir suffix); int numIndexed = indexWriter.maxDoc(); indexWriter.optimize(); indexWriter.close(); Directory.copy(ramDir FSDirectory.open(indexDir) false); // 3 return numIndexed; } this line is not recognize in eclipse. Directory.copy(ramDir FSDirectory.open(indexDir)); can we use Directory.copy to copy index to FSDirectory? I fixed the line and yes that's the idea. thanks you very much.it works perfectly. :-)"
639,A,Zend Lucene search related fields? I have a lot of paired fields (hoursDistance1 cityName1 hoursDistance2 cityName2 hoursDistance3 cityName3 etc.). What query do I need to search for so that Lucene scores based on both fields having the correct terms instead of just one of them? i.e. if I search for a city 3 hours from here with this name how do I get it to return results where hoursDistanceN is 3 hours from here AND cityNameN is this without scoring the other pairs of fields? IIUC you can do this by denormalizing your data: Create a Lucene document for each pair of fields e.g. if: hoursDistance1=3cityName1=London Create a document with the fields: hoursDistance=3cityName=LondonpairIndex=1 And then run a query like: hoursDistance=5 AND cityName=Leeds Creating documents for each set of pairs is not possible as they are part of a location document with each pair of fields relative to the location. I believe you need to explain this a little further. How did you create the fields? Maybe using a spatial search module is better.  You could create a document for each pair. So instead of id | hours1 | name1 | hours2 | name2 | ... You would have: id | pair_num = 1 | hours | name id | pair_num = 2 | hours | name ... Since you only want to search one pair at a time you shouldn't need to merge the results together or anything.
640,A,"How do you run Lucene on .net? Lucene is an excellent search engine but the .NET version is behind the official Java release (latest stable .NET release is 2.0 but the latest Java Lucene version is 2.4 which has more features). How do you get around this? Someone with more kudos than myself should edit the ""lucence"" to lucene Just fixed it :) One way I found which was surprised could work: Create a .NET DLL from a Java .jar file! Using IKVM you can download Lucene get the .jar file and run: ikvmc -target:library <path-to-lucene.jar> which generates a .NET dll like this: lucene-core-2.4.0.dll You can then just reference this DLL from your project and you're good to go! There are some java types you will need so also reference IKVM.OpenJDK.ClassLibrary.dll. Your code might look a bit like this: QueryParser parser = new QueryParser(""field1"" analyzer); java.util.Map boosts = new java.util.HashMap(); boosts.put(""field1"" new java.lang.Float(1.0)); boosts.put(""field2"" new java.lang.Float(10.0)); MultiFieldQueryParser multiParser = new MultiFieldQueryParser (new string[] { ""field1"" ""field2"" } analyzer boosts); multiParser.setDefaultOperator(QueryParser.Operator.OR); Query query = multiParser.parse(""ABC""); Hits hits = isearcher.search(query); I never knew you could have Java to .NET interoperability so easily. The best part is that C# and Java is ""almost"" source code compatible (where Lucene examples are concerned). Just replace System.Out with Console.Writeln :). ======= Update: When building libraries like the Lucene highlighter make sure you reference the core assembly (else you'll get warnings about missing classes). So the highlighter is built like this: ikvmc -target:library lucene-highlighter-2.4.0.jar -r:lucene-core-2.4.0.dll Good question. In this case I believe it actually creates a .NET dll that runs directly and is not interpreted. So lucene-core-2.4.0.dll is running through the CLR. IKVM may have other modes where it's doing on the fly interpretation which could be slow. From quick testing for our dataset etc. I don't see a performance difference between Lucene.NET and the IKVMC version. This is my first time learning of IKVM. Is performance ok? Because each instruction in the original java needs to go through TWO layers of VM right? The IKVM JVM and then the .NET CLR. And search is one thing you'd like to be as fast as possible. kurious..How did this work out? Is the performance OK? Thanks a lot .. I was not aware of this that jar files can be converted to .net dll  I converted the Lucene 2.4 from jar to dll through this way but now it gives me an error that 'Type or namespace Lucene could not be found'. I removed the old dll from the project and added reference for the new one. I really want to get rid of the old version as it took around 2 days and in the end during optimization it gave some error and now the index is not updateable :S. I read somewhere that Lucene 2.4 indexing speed is many times faster than the old versions if I use 2.3.1 from SVN will that be faster too?  Download the source and build it. I did this just last weekend and it was easy. No problem at all. The source is at version 2.3.1. I'm subscribed to the mailing list and judging from it Lucene.Net is being developed actively. It looks like the latest development version is 2.3 but the latest stable release is 2.0.0.4. I'm shipping 2.3 with my app (BugTracker.NET) and so far no complaints. Interesting -- I'd still prefer the latest version (given how easily it can be ported with IKVM) but thanks for the pointer!  Lucene.net is under development and now has three committers"
641,A,Lucene.NET: Retrieving all the Terms used in a particular Document Is there a way to itterate through all of the terms held against a particular document in a Lucene.NET index? Basically I want to be able to retrieve a Document from the Index based on it's ID and then find the frequency with which each Term is used in that Document. Does anyone know a way to do this? I can find the number of Documents that match a particular Term but not the Terms contained within a particular Document. Many thanks Tim In Lucene Java at least one of the options when indexing a document is storing the term frequency vector. The term frequency vector is simply a list of all the terms in a given field of a document and how often each of those terms was used. Getting the term frequency vector at runtime involves calling a method in the IndexReader with the Lucene ID of the document in question.
642,A,How to reload synonyms.txt in Solr? When I changed my synonyms.txt I only see the diferences when I do this: restart solr server indexed some item (empty commit?) Some knows a way to reload synonyms.txt file without restart server? Tks a lot. Reloading the core should also reload synonyms. I almost always map indexes to cores (even if it's only one core per instance) due to the enhanced flexibility it provides (as in this case).
643,A,"Java lucene standard analyzer`s default delimiters? i am looking for all the delimiters on which java lucene standard analyzer tokenizes the input string. need to know all delimiters that are by default used for tokenizing. I know (from Lucene in Action) that all characters which are not a-zA-Z or variatons of a-zA-Z that have diacritics are used as delimiters including numbers. So you might have Mc'Donald splitted in ""Mc"" ""Donald"" you might have ""Web2.0"" tokenized as ""Web"" and so on. The best is to do a test and enter all kinds of characters and then post your results here. thanks can i have a reference of this information hmmm it works thanx I'm sorry I think it's SimpleAnalyzer what I've described. StandardAnalyzer is more complex: http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/analysis/standard/StandardTokenizer.html"
644,A,"N-gram generation from a sentence How to generate an n-gram of a string like: String Input=""This is my car."" I want to generate n-gram with this input: Input Ngram size = 3 Output should be: This is my car This is is my my car This is my is my car Give some idea in Java how to implement that or if any library is available for it. I am trying to use this NGramTokenizer but its giving n-gram's of character sequence and I want n-grams of word sequence. This code returns an array of all Strings of the given length: public static String[] ngrams(String s int len) { String[] parts = s.split("" ""); String[] result = new String[parts.length - len + 1]; for(int i = 0; i < parts.length - len + 1; i++) { StringBuilder sb = new StringBuilder(); for(int k = 0; k < len; k++) { if(k > 0) sb.append(' '); sb.append(parts[i+k]); } result[i] = sb.toString(); } return result; } E.g. System.out.println(Arrays.toString(ngrams(""This is my car"" 2))); //--> [This is is my my car] System.out.println(Arrays.toString(ngrams(""This is my car"" 3))); //--> [This is my is my car] `ngrams(""This is my car"" -3)` (sorry couldn't resist) `ngrams(""This is my car"" -3)` works fine. `ngrams(""This is my car"" 6)` however results in a `NegativeArraySizeException`. What do you expect in these cases? I'd suggest to put a test at the beginning of the method and return an empty array. Generally I see few SO answers with a sophisticated error handling.  I believe this would do what you want: import java.util.*; public class Test { public static List<String> ngrams(int n String str) { List<String> ngrams = new ArrayList<String>(); String[] words = str.split("" ""); for (int i = 0; i < words.length - n + 1; i++) ngrams.add(concat(words i i+n)); return ngrams; } public static String concat(String[] words int start int end) { StringBuilder sb = new StringBuilder(); for (int i = start; i < end; i++) sb.append((i > start ? "" "" : """") + words[i]); return sb.toString(); } public static void main(String[] args) { for (int n = 1; n <= 3; n++) { for (String ngram : ngrams(n ""This is my car."")) System.out.println(ngram); System.out.println(); } } } Output: This is my car. This is is my my car. This is my is my car. An ""on-demand"" solution implemented as an Iterator: class NgramIterator implements Iterator<String> { String[] words; int pos = 0 n; public NgramIterator(int n String str) { this.n = n; words = str.split("" ""); } public boolean hasNext() { return pos < words.length - n + 1; } public String next() { StringBuilder sb = new StringBuilder(); for (int i = pos; i < pos + n; i++) sb.append((i > pos ? "" "" : """") + words[i]); pos++; return sb.toString(); } public void remove() { throw new UnsupportedOperationException(); } }   public static void CreateNgram(ArrayList<String> list int cutoff) { try { NGramModel ngramModel = new NGramModel(); POSModel model = new POSModelLoader().load(new File(""en-pos-maxent.bin"")); PerformanceMonitor perfMon = new PerformanceMonitor(System.err ""sent""); POSTaggerME tagger = new POSTaggerME(model); perfMon.start(); for(int i = 0; i<list.size(); i++) { String inputString = list.get(i); ObjectStream<String> lineStream = new PlainTextByLineStream(new StringReader(inputString)); String line; while ((line = lineStream.read()) != null) { String whitespaceTokenizerLine[] = WhitespaceTokenizer.INSTANCE.tokenize(line); String[] tags = tagger.tag(whitespaceTokenizerLine); POSSample sample = new POSSample(whitespaceTokenizerLine tags); perfMon.incrementCounter(); String words[] = sample.getSentence(); if(words.length > 0) { for(int k = 2; k< 4; k++) { ngramModel.add(new StringList(words) k k); } } } } ngramModel.cutoff(cutoff Integer.MAX_VALUE); Iterator<StringList> it = ngramModel.iterator(); while(it.hasNext()) { StringList strList = it.next(); System.out.println(strList.toString()); } perfMon.stopAndPrintFinalResult(); }catch(Exception e) { System.out.println(e.toString()); } } Here is my codes to create n-gram. In this case n = 2 3. n-gram of words sequence which smaller than cutoff value will ignore from result set. Input is list of sentences then it parse using a tool of OpenNLP  /** * * @param sentence should has at least one string * @param maxGramSize should be 1 at least * @return set of continuous word n-grams up to maxGramSize from the sentence */ public static List<String> generateNgramsUpto(String str int maxGramSize) { List<String> sentence = Arrays.asList(str.split(""[\\W+]"")); List<String> ngrams = new ArrayList<String>(); int ngramSize = 0; StringBuilder sb = null; //sentence becomes ngrams for (ListIterator<String> it = sentence.listIterator(); it.hasNext();) { String word = (String) it.next(); //1- add the word itself sb = new StringBuilder(word); ngrams.add(word); ngramSize=1; it.previous(); //2- insert prevs of the word and add those too while(it.hasPrevious() && ngramSize<maxGramSize){ sb.insert(0' '); sb.insert(0it.previous()); ngrams.add(sb.toString()); ngramSize++; } //go back to initial position while(ngramSize>0){ ngramSize--; it.next(); } } return ngrams; } Call: long startTime = System.currentTimeMillis(); ngrams = ToolSet.generateNgramsUpto(""This is my car."" 3); long stopTime = System.currentTimeMillis(); System.out.println(""My time = ""+(stopTime-startTime)+"" ms with ngramsize = ""+ngrams.size()); System.out.println(ngrams.toString()); Output: My time = 1 ms with ngramsize = 9 [This is This is my is my This is my car my car is my car]  You are looking for ShingleFilter. Update: The link points to version 3.0.2. This class may be in different package in newer version of Lucene."
645,A,"lucene or sql fulltext? I want to create a search website to search docs (all kinds of formats including pdf) images videos and audio. I also want to be able to filter my search results based on some criteria like author name date etc. I'm doing this in .NET so what's the easiest way to get up and running? SQL fulltext searching seems tempting because I'm familiar with sql and plus since I want to filter my search results it will be easy to store the filter fields for each item. I also need to get snippets from each search result Stored procedure for snippets: CREATE PROCEDURE SimpleCommentar @SearchTerm nvarchar(100) @Style nvarchar(200) AS BEGIN CREATE TABLE #match_docs ( doc_id bigint NOT NULL PRIMA ); INSERT INTO #match_docs ( doc_id ) SELECT DISTINCT Commentary_ID FROM Commentary WHERE FREETEXT ( Commentary @SearchTerm LANGUAGE N'English' ); DECLARE @db_id int = DB_ID() @table_id int = OBJECT_ID(N' @column_id int = ( SELECT column_id FROM sys.columns WHERE object_id = OBJECT_I AND name = N'Commentary' ); SELECT s.Commentary_ID t.Title MIN ( N'...' + SUBSTRING ( REPLACE ( c.Commentary s.Display_Term N'<span style=""' + @Style + '"">' + s.Display_Term + '</span>' ) s.Pos - 512 s.Length + 1024 ) + N'...' ) AS Snippet FROM ( SELECT DISTINCT c.Commentary_ID w.Display_Term PATINDEX ( N'%[^a-z]' + w.Display_Term + N'[^a-z]%' c.Commentary ) AS Pos LEN(w.Display_Term) AS Length FROM sys.dm_fts_index_keywords_by_document ( @db_id @table_id ) w INNER JOIN dbo.Commentary c ON w.document_id = c.Commentary_ID WHERE w.column_id = @column_id AND EXISTS ( SELECT 1 FROM #match_docs m WHERE m.doc_id = w.document_id ) AND EXISTS ( SELECT 1 FROM sys.dm_fts_parser ( N'FORMSOF(FREETEXT ""' + @SearchTerm + N'"")' 1033 0 1 ) p WHERE p.Display_Term = w.Display_Term ) ) s INNER JOIN dbo.Commentary c ON s.Commentary_ID = c.Commentary_ID INNER JOIN dbo.Book_Commentary bc ON c.Commentary_ID = bc.Commentary_ID INNER JOIN dbo.Book_Title bt ON bc.Book_ID = bt.Book_ID INNER JOIN dbo.Title t ON bt.Title_ID = t.Title_ID WHERE t.Is_Primary_Title = 1 GROUP BY s.Commentary_ID t.Title; DROP TABLE #match_docs; END;  If your primary concern is getting it up and running quickly and easily then SQL fulltext search is definitely the way to go. Lucene.NET has its advantages but it is by no means a walk in the park to set up correctly. The documentation is a bit lacking and there are a very limited number of examples on the web. thanks...do you know if it's possible to return snippets of search results with sql fulltext search? Yes you can do that. But you will need a stored procedure for that. Take a look at this sample:"
646,A,Using Zend Lucene to search Office 2003 or older files I know there are already objects supporting Office 2007 files but is there any native Office 2003 or earlier support ? I would recommend indexing the documents with Solr and Tika together and using JSON to search your Solr/Lucene index from PHP. See the ExtractingRequestHandler (Solr wiki page) article for more information.  There doesn't seem to be anything bundled with Zend_Search_Lucene for those. Still considering it can index HTML documents if you can find a way to convert your Office 2003 documents to HTML (at least for indexing -- keeping to original version alonside the HTML one for consultation) you might be able to index those...
647,A,Does Amazon has APIs to index and search through documents stored using Amazon S3? I have a lots of documents stored on Amazon S3. My questions are: Does Amazon provide any services/APIs using which I can index the contents of the document and search them (full text indexing and searching)? If it does could someone please point me to any link in the documentation. If it does not then could this be achieved with Lucene and Zend Framework? Have any one of you implemented this? Can I get some pointers? UPDATE: I do not intend to save my index on Amazon S3 rather I am looking forward to indexing the contents of the documents on S3 and serving them based on a search. You can see this question or this blog post if you want to do pure lucene or you can use Solr which is probably easier. See also this post. Zend has a PHP port of Lucene which ties in very well. You can look at the Zend documentation for how to use it.
648,A,"Lucene search results sort by custom order list (unique to each user) I have authenticated users in my application who have access to a shared database of up to 500000 items. Each of the users has their own public facing web site and needs the ability to prioritize the items on display (think upvote) on their own site. out of the 500000 items they may only have up to 200 prioritized items the order of the rest of the items is of less importance. Each of the users will prioritize the items differently. I initially asked a similar mysql question here http://stackoverflow.com/questions/1281484/mysql-results-sorted-by-list-which-is-unique-for-each-user and got a good answer but i believe a better option may be to opt for a non sql indexed solution. Can this be done in Lucene? is there another search technology which would be better for this. ps. Google implements a similar type setup with their search results where you can prioritize and exclude your own search results if you are logged in. Update: re-tagged with sphinx as i have been reading the documentation and i believe it may be able to do what i am looking for with ""per-document attribute values"" stored in memory - interested to hear any feedback on this from sphinx gurus You'll definitely want to store the id of item in each document object when building your index. There's a few ways to do the next step but an easy one would be take the prioritized items and add them to your search query something like this for each special item: ""OR item_id=%d+X"" where X is the amount of boost you'd like to use. You'll probably need to empirically tweak this number to make sure that just being ""upvoted"" doesn't put it to the top of a list searching for something totally unrelated. Doing it this way will at least prevent you from a lot of annoying postprocessing steps that would require you to iterate over the whole result set -- hopefully the proper sorting will be there right from querying the index. ok so im guessing i would store the prioritized list in mysql or similar and select this list by user_id ordered by prority. with this list i would then form the lucene search query string as you have suggested. will this still scale and work fast if there is say 200 items in their prority list"
649,A,Hibernate Search Paging + FullTextSearch + Criteria I am trying to do a search with some criteria FullTextQuery fullTextQuery = fullTextSession.createFullTextQuery(finalQuery KnowledgeBaseSolution.class).setCriteriaQuery(criteria); and then page it //Gives me around 700 results result.setResultCount(fullTextQuery.getResultSize()); //Some pages are empty fullTextQuery.setFirstResult(( (pageNumber - 1) * pageSize )); fullTextQuery.setMaxResults( pageSize ); result.setResults(fullTextQuery.list()); I suspect Lucene return full result of the full text search without taking the criteria into account and then hibernate search applies the criteria after therefore some page are empty (after filtering by criteria) What is proper way to do fullTextSearch with some criteria is it possible to apply the criteria before the lucene search? Or do I have to use pure Lucene (if so what's the point of Hibernate Search?) Thanks in advance Hi Roy. I too have hit this issue. Did you find a workaround? nope. I ended up do the paging myself Apparently you cannot use fullTextSearch and criteria and paging/sorting together. Unless you go to the Lucene level link text
650,A,Lucene.NET with SQL SERVER 2000 I have a SQL 2000 database with around 10 million rows and I need to make a query to get the product information based on the full / partial text search. Based on this I need to join back to other tables to check on my business process. I have this implemented using SQL proc but I can only validate around 6 rows a sec (without threads.. its a long business logic). I am trying to find a better ways to improve performance. Lucene.NET might help on this. I have couple of questions. Can you point me to right sources. While building index on Lucene how would I sync up with the SQL database and lucene DB? Do you think Lucene can give real performance gain? You can start with Mark Krellenstein's 'Search Engine versus DBMS' to see whether a full text search engine such as Lucene is the solution for you. In theory Lucene should be faster than SQL for textual search but your mileage may vary. You can do incremental updates with Lucene which are a bit similar to database replication. This keeps the Lucene index synchronized with the database. Thanks for your answer. I looked through quite a few tutorials and am at the point where I can index records of DB to the local file. As I said my DB is of around 10 million records Lucene takes a while if I build the index from scratch. My approach is : --Crate a windows service which looks for any update in database (each hour) and keeps the index in sync with database records. Say there are 2000 records added / hour is that going to adversely affect updating the index. Does the search slow down during index build? Well you need to somehow get the records into the Lucene index in the first place. The best way for this is to build the index from scratch offline. Once you have that you can use incremental updates like you suggest. I only have experience in Java Lucene with MySQL but I believe the issues are similar in your setting - 2000 records per hour or about 40 per minute seems reasonable for updates. It may slow down the search. See http://www.lucidimagination.com/Community/Hear-from-the-Experts/Articles/Scaling-Lucene-and-Solr  Here is an article on using LINQ to Lucene to work with SQL. This may point you in the right direction.
651,A,"Solr Copyfield on dynamic fields regexp issue I'm trying to copy some dynamic fields for spellchecking. However the . Below are the relevant fields from schema.xml: <dynamicField name=""*_text_fr"" stored=""false"" type=""text_fr"" multiValued=""true"" indexed=""true""/> <dynamicField name=""*_text_frs"" stored=""true"" type=""text_fr"" multiValued=""true"" indexed=""true""/> <dynamicField name=""*_text_frms"" stored=""true"" type=""text_fr"" multiValued=""true"" indexed=""true""/> It doesn't copy anything with: <copyField source=""*_text_fr*"" dest=""textSpellFr"" /> However it works with: <copyField source=""*_text_fr"" dest=""textSpellFr"" /> <copyField source=""*_text_frs"" dest=""textSpellFr"" /> <copyField source=""*_text_frms"" dest=""textSpellFr"" /> Why doesn't the first option work? DynamicField and CopyField names are not regular expressions. The * wildcard can only be used to indicate prefix or suffix but not both."
652,A,"Hibernate search : How to index B childs of a A parent class ? How to get only B objects that contains one specific object A using lucene? I have a problem with lucene indexation I insert one indexed entity in a manyToMany association but lucene doesn't index as I expected. @Entity @Indexed @Table(name=""level"") public class Level { ... @IndexedEmbedded private List<Course> courses = new ArrayList<Course>(); @ManyToMany(cascade=CascadeType.ALL fetch=FetchType.LAZY) @JoinTable(name = ""level_course"" joinColumns = { @JoinColumn(name = ""level_id"" nullable = false updatable = false) } inverseJoinColumns = { @JoinColumn(name = ""course_id"" nullable = false updatable = false) }) @OrderColumn(name=""corder"") public List<Course> getCourses() { return courses; } ... } @Entity @Indexed @Table(name=""course"") @FullTextFilterDef(name = ""filterLevel"" impl = LuceneFilterFactory.class cache=FilterCacheModeType.NONE) public class Course { ... @ContainedIn private List<Level> levels = new ArrayList<Level>(); @ManyToMany(cascade=CascadeType.ALL fetch=FetchType.LAZY mappedBy=""courses"") public List<Level> getLevels() { return levels; } } When I do : level.getCourses().add(myCourse1); entityManager.save(level); myCourse1 (with ID #10 for example) will be well created and attached to level (level is parent class Course is the child). Then instances of ""Course"" are well indexed but if I have a look to the indexes generated for Course I expected to find ""levels.id"" with value #10. But I don't find it. I need this kind of indexation because I use LuceneFilterFactory.class on Course to filter course by one level id. Maybe my use of @ContainedIn and @IndexEmbedded annotations is not good ? Or maybe I'm completely in the wrong way to do what I need. To simplify : I have 2 classes A and B with a manyToMany association between A and B. A is master on the relation. A and B are indexed. I'd like to use hibernate search for getting B objects that contains one A object in their manyToMany association. I don't want to get all B but only B objects that contains this specific A. How to do this ? Thanks for your help where do you set the other side of the bidirectional association (meaning updating the levels list in course?). you need to update both sides of the association. It would also help if you include the indexing code as well as the actual search you are trying to execute. Setting both sides of the relation is essential. It has nothing with Search to do but is just basic Hibernate (JPA) behavior/requirement. ok thanks hardy I didn't set the other side I tried to change the mapping by putting @IndexedEmbedded in the class Course over the property levels. And then I updated both sides of the association and you're right now I have the result I expected. But there is one important problem by setting both sides of the association : I didn't set the other side I tried to change the mapping by putting @IndexedEmbedded in the class Course over the property levels. And then I updated both sides of the association and you're right now I have the result I expected. But there is one important problem by setting both sides of the association : level.getCourses().add(myCourse1); myCourse1.getLevels().add(level); entityManager.save(level); ==> JPA will load two collections (level.courses and myCourse1.levels) or am I mistaken about the jpa collection loading ? Thanks for your help Have you tried to place the annotations consistently? Either all on the fields or all on the getters? I tried but it doesn't seem to be the problem... I've added more explanations in my question I hope this will help because I recognise my question wasn't very clear. !!! I valid hardy response upper : ""Setting both sides of the relation is essential. It has nothing with Search to do but is just basic Hibernate (JPA) behavior/requirement."" !!!"
653,A,Beginners Lucene tutorial I've never done anything in Java before but I'd like to use Lucene for the search on a site. I'm having trouble find a good step by step tutorial for a complete beginner at this. Can anyone recommend a good tutorial? Thanks Before diving into Lucene at least learn the basics of Java. Doing both at the same time is (most probably) not going to work. Do you want to write all the site in Java or just use Lucene search with your existing site in another programming language? I only want to index the site with java and then use php using the Zend library to access the index when a user searches the site. We've updated the LingPipe tutorials for Lucene. The latest covers Lucene 3.6. See: http://lingpipe-blog.com/2012/07/05/lucene-tutorial-updated-for-lucene-3-6/ this website might help you a bit.. http://www.lucenetutorial.com/lucene-in-5-minutes.html  If you're using Zend why aren't you using Zend's PHP port of lucene? See here for a tutorial on it. @user330936: Yes Java will be faster than PHP. But that's not unique to Lucene - everything will be faster in Java (which in turn is slower than C which is slower than assembly...) If you're going to use PHP for one part I think it's hard to make the claim that using a new language just for lucene is a good idea. I'm actually using Codeigniter but using the Zend Lucene within it. I read that using java for the crawler / indexer part was better than using PHP.  Along with user428747 answer you can also read this article. As well as this one (which is kind of old compared to the first one). On a side note if you want to use Lucene did you consider using Solr? It uses the lucene search library and extends it as you can read here. +1 for Solr. If you want to quickly set up a search server this is the way to go. Thanks for the links I'll get reading. I hadn't considered Solr but I will have a look as well.  maybe apache solr is better for you: http://lucene.apache.org/solr/  The classics: Lucene in Action +1 for Lucene in Action. Is by far the best beginner's tutorial IMHO.  This is not a direct reply to your question on Lucene tutorials (For that my answer is same as some of the other posters: Bob Carpenter's Lucene in 60 seconds tutorial on the Lingpipe blog). If you don't want to learn Java just for Lucene any full-text search database (Postgres/Mysql/etc) should solve your purpose. In particular Sphinx is recommended. This decision particularly relevant if you need your search app to have high performance / scalability (since you will be learning two things - Java and Lucene). Unless you have an in-house java expert it is better to fight one war than two at the same time.
654,A,"Lucene crawler (it needs to build lucene index) I am looking for Apache Lucene web crawler written in java if possible or in any other language. The crawler must use lucene and create a valid lucene index and document files so this is the reason why nutch is eliminated for example... Does anybody know does such a web crawler exist and can If answer is yes where I can find it. Tnx... What was your programming-related question? AFAIK lucene doesnot have a web-crawler chk this out http://java-source.net/open-source/crawlers Take a look at solr search server and nutch (crawler) both are related to the lucene project. Yes they are related but nutch is using modified lucene index's and solr is another type of application. Tnx anyway  What you're asking is two components: Web crawler Lucene-based automated indexer First a word of couragement: Been there done that. I'll tackle both of the components individually from the point of view of making your own since I don't believe that you could use Lucene to do something you've requested without really understanding what's going on underneath. Web crawler So you have a web site/directory you want to ""crawl"" through to collect specific resources. Assuming that it's any common web server which lists directory contents making a web crawler is easy: Just point it to the root of the directory and define rules for collecting the actual files such as ""ends with .txt"". Very simple stuff really. The actual implementation could be something like so: Use HttpClient to get the actual web pages/directory listings parse them in the way you find most efficient such as using XPath to select all the links from the fetched document or just parsing it with regex using Java's Pattern and Matcher classes readily available. If you decide to go the XPath route consider using JDOM for DOM handling and Jaxen for the actual XPath. Once you get the actual resources you want such as bunch of text files you need to identify the type of data to be able to know what to index and what you can safely ignore. For simplicity's sake I'm assuming these are plaintext files with no fields or anything and won't go deeper into that but if you have multiple fields to store I suggest you make your crawler to produce 1..n of specialized beans with accessors and mutators (bonus points: Make the bean immutable don't allow accessors to mutate the internal state of the bean create a copy constructor for the bean) to be used in the other component. In terms of API calls you should have something like HttpCrawler#getDocuments(String url) which returns a List<YourBean> to use in conjuction with the actual indexer. Lucene-based automated indexer Beyond the obvious stuff with Lucene such as setting up a directory and understanding its threading model (only one write operation is allowed at any time multiple reads can exist even when the index is being updated) you of course want to feed your beans to the index. The five minute tutorial I already linked to basically does exactly that look into the example addDoc(..) method and just replace the String with YourBean. Note that Lucene IndexWriter does have some cleanup methods which are handy to execute in a controlled manner for example calling IndexWriter#commit() only after a bunch of documents have been added to index is good for performance and then calling IndexWriter#optimize() to make sure the index isn't getting hugely bloated over time is a good idea too. Always remember to close the index too to avoid unnecessary LockObtainFailedExceptions to be thrown as with all IO in Java such operation should of course be done in the finally block. Caveats You need to remember to expire your Lucene index' contents every now and then too otherwise you'll never remove anything and it'll get bloated and eventually just dies because of its own internal complexity. Because of the threading model you most likely need to create a separate read/write abstraction layer for the index itself to ensure that only one instance can write to the index at any given time. Since the source data acquisition is done over HTTP you need to consider the validation of data and possible error situations such as server not available to avoid any kind of malformed indexing and client hangups. You need to know what you want to search from the index to be able to decide what you are going to put into it. Note that indexing by date must be done so that you split the date to say year month day hour minute second instead of millisecond value because when doing range queries from Lucene index the [0 to 5] actually gets transformed into +0 +1 +2 +3 +4 +5 which means the range query dies out very quickly because there's a maximum number of query sub parts. With this information I do believe you could make your own special Lucene indexer in less than a day three if you want to test it rigorously."
655,A,"Correctly indexing latitude and longitude values in Lucene Am working on a ""US based nearest city search within a given radius"" functionality using Lucene API. Am indexing city's lat and long values in Lucene as follows: doc.Add(new Field(""latitude"" paddedLatitude Field.Store.YES Field.Index.UN_TOKENIZED)); doc.Add(new Field(""longitude"" paddedLongitude Field.Store.YES Field.Index.UN_TOKENIZED)); Since Lucene only understands strings and not numbers am padding lat and long values. For example if original lat and long are 41.811846 and -87.820628 respectively after paddingvalues look like: paddedLatitude -->""0041.811846"" and paddedLongitude-->""-087.820628"" Am doing the same padding while building the nearest city query(using Lucene's ConstantScoreRangeQuery class). Given the fact that lat and long values could be decimal/negative numbers is this the right approach to index them so that I would get correct nearest cities in the search results when lucene would perform a number Range/comparison operation on these values? Thanks. Here's the bleeding edge about Searching Numerical Fields in Lucene by Uwe Schindler the expert on the subject. You may need to use the older (and slower) ConstantScoreRangeQuery because Lucene.net is a bit behind Lucene and the class NumericRangeQuery described in the link was not yet released in Java Lucene.  The linked article in Yuval F's answer made me realize I was wrong in an earlier answer which you seem to be relying on. You shouldn't index negative numbers as is especially in this case where some of the values are negative and some are positive. This article seems to have a pretty good discussion of spatial search. He uses some transformations to make all the values positive and he also touches on other subjects you should probably be aware of like distance calculations. One thing to remember if you're encoding the values is to encode them both for the indexing and when building the query. thanks...and what about decimal numbers?? Thanks again....I tried solution mentioned in above article link (http://sujitpal.blogspot.com/2008/02/spatial-search-with-lucene.html)but on executing the query Lucene.net throws an exception: Parameter name: latitude System.ArgumentException: The value provided is out of bounds. Parameter name: latitude Here is my query: latitude:[131450428 TO 132173263] longitude:[091694457 TO 092664286]"
656,A,"Displaying sample text from the Lucene Search Results Currently I am using Lucene version 3.0.2 to create a search application that is similar to a dictionary. One of the objects that I want to display is a sort of ""example"" where Lucene would look for a word in a book and then the sentences where the words were used are displayed. I've been reading the Lucene in Action book and it mentions something like this but looking through it I can't find other mentions. Is this something you can do with Lucene? If it is how is can you do it? Down boys down. I believe what you are looking for is a Highlighter. One possibility is to use the lucene.search.highlight package specifically the Highlighter. Another option is to use the lucene.search.vectorhighlight package specifically the FastVectorHighlighter. Both classes search a text document choose relevant snippets and display them with the matching terms highlighted. I have only used the first one which worked fine for my use-case. If you can pre-divide the book into shorter parts it would make highlighting faster. Thank you! I think these will work great... I could always divide the book into chapters."
657,A,"Lucene.NET & Facete Search Solution Hey just started using Lucene.NET any was wondering if anyone had a working example of Lucene.NET with a faceted search. I know this below link http://www.devatwork.nl/articles/lucenenet/faceted-search-and-drill-down-lucenenet/ Which looked great but all it does is tell me the number of results in the faceted search but not actually how I can retrieve the index and details of those results. i.e as in a normal search in Lucene.NET. I.e from that link he has the following snippet  private static void FacetedSearch(string indexPath string genre string term){ var searcher = new IndexSearcher(indexPath); // first get the BitArray result from the genre query var genreQuery = new TermQuery(new Term(""genre"" genre)); var genreQueryFilter = new QueryFilter(genreQuery); BitArray genreBitArray = genreQueryFilter.Bits(searcher.GetIndexReader()); Console.WriteLine(""There are "" + GetCardinality(genreBitArray) + "" document with the genre "" + genre); // Next perform a regular search and get its BitArray result Query searchQuery = MultiFieldQueryParser.Parse(term new[] {""title"" ""description""} new[] {BooleanClause.Occur.SHOULD BooleanClause.Occur.SHOULD} new StandardAnalyzer()); var searchQueryFilter = new QueryFilter(searchQuery); BitArray searchBitArray = searchQueryFilter.Bits(searcher.GetIndexReader()); Console.WriteLine(""There are "" + GetCardinality(searchBitArray) + "" document containing the term "" + term); // Now do the faceted search magic combine the two bit arrays using a binary AND operation BitArray combinedResults = searchBitArray.And(genreBitArray); Console.WriteLine(""There are "" + GetCardinality(combinedResults) + "" document containing the term "" + term + "" and which are in the genre "" + genre); } Which will tell me i.e there is 2 records for the search term ""Dublin"" and which are in genre ""Financial"" which is perfect but the article seems to skip the part where it says how I can retrieve the indexes of those results and display on screen. He does explain this in the link below for a normal search but not facete search.. i.e Normal Search  private static void Search(string indexPath string term) { // create searcher var searcher = new IndexSearcher(indexPath); // create a query which searches through the title and description the term can be in the title or the description Query searchQuery = MultiFieldQueryParser.Parse(term new[] {""title"" ""description""} new[] {BooleanClause.Occur.SHOULD BooleanClause.Occur.SHOULD} new StandardAnalyzer()); // perform the search Hits hits = searcher.Search(searchQuery); // loop through all the hits and show their title for (int hitIndex = 0; hitIndex &amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt; hits.Length(); hitIndex++) { // get the corresponding document Document hitDocument = hits.Doc(hitIndex); // write its title to the console Console.WriteLine(hitDocument.GetField(""title"").StringValue()); } } http://www.devatwork.nl/articles/lucenenet/search-basics-lucenenet/ Any help would be greatly appreciated Edit : Or should I do a search query and then do a Filter on the results ? The BitArray represents hits. Each 1 has an index that is equal to document id So 1001001 means that documents with position 0 3 and 6 in index match your search. You just have to retrieve them from lucene index. var searcher = new IndexSearcher(indexPath); // get document at position 0 var doc = searcher.Doc( 0 ); you have to get documents one by one Ah cool thanks mathieu can also explain how I can get those indexes ? Thanks for your help mathieu this sent me on the right track anyway Thats the part I don't get i.e how to retrieve them from Lucene. How can I pass the bit array indexes to something like Hits hits = searcher.Search(searchQuery); So that I can loop through the hits documents."
658,A,How to install for Solr 1.4 ( or 1.4.1 ) Extended Dismax (edismax) plugin and how to configure it? Im using Solr1.4  with dismax SearchHandler  I'm new to solr ;) it seems not supporting lucene syntax  it does not even match lowercase uppercase terms ( if you know how to do this it will be helpfull ). I want to try the edismax (Extended Dismax) with solr 1.4 or 1.4.1  I found it in solr 4.0 dev version there is not a lot of documentation about it . I'm using solr with windows server. Is it possible to patch my current version to use edismax? what about edismax configuration does it use the same params as the standard dismax? Since edismax is an experimental not-yet-finished feature you'll have to get the Lucene/Solr source code and build it. Again because this is not released your only documentation are the JIRA comments and source code.
659,A,"Is it possible to do a date based query time boost in Compass? I'm trying to get a query-time boost on recent items in a Compass index. I have tried using a property on my class mapping but this only seems to affect the boost during index-time not query-time. Any ideas?  DefaultCompassQuery query = (DefaultCompassQuery) compassBuilderQuery.toQuery(); query.setTypes( types.toArray( new Class[types.size()] ) ); LuceneSearchEngineQuery searchEngineQuery = (LuceneSearchEngineQuery) query.getSearchEngineQuery(); final SimpleDateFormat sdf = new SimpleDateFormat( ""yyyyMMddHHmmss"" ); final long timeInMillis = Calendar.getInstance().getTimeInMillis(); ValueSourceQuery valSrcQuery = new ValueSourceQuery( new ValueSource() { private static final long serialVersionUID = 1L; @Override public int hashCode() { return System.identityHashCode( this ); } @Override public DocValues getValues( final IndexReader reader ) throws IOException { return new DocValues() { @Override public float floatVal( int doc ) { try { Document document = reader.document( doc ); Field field = document.getField( ""date"" ); if (null != field) { Date parse = sdf.parse( field.stringValue() ); long t = timeInMillis - parse.getTime(); float f = (1.0f / (t * (1.0f / TimeUnit.DAYS.toMillis( 30 )) + 1.0f)); if (logger.isDebugEnabled()) { logger.debug( ""Date match: "" + parse.toString() ); logger.debug( ""Calculated date boost as: "" + f + "" for doc id: "" + doc ); } return f; } } catch (CorruptIndexException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } catch (ParseException e) { e.printStackTrace(); } return 1.0f; } @Override public String toString( int doc ) { return description() + ""="" + strVal( doc ); } }; } @Override public boolean equals( Object o ) { return this == o; } @Override public String description() { return ""[boost: date]""; } } ); CustomScoreQuery sq = new CustomScoreQuery( searchEngineQuery.getQuery() valSrcQuery ); searchEngineQuery.setQuery( sq ); Where 'date' is a date field on your document that you want to boost and compassBuilderQuery is a query you have generated with the compass query builder. You can also tweak the '30' up there to make the dates boosted less or more based on their age."
660,A,"Can a raw Lucene index be loaded by Solr? Some colleagues of mine have a large Java web app that uses a search system built with Lucene Java. What I'd like to do is have a nice HTTP-based API to access those existing search indexes. I've used Nutch before and really liked how simple the OpenSearch implementation made it to grab results as RSS. I've tried setting Solr's dataDir in solrconfig.xml hoping it would happily pick up the existing index files but it seems to just ignore them. My main question is: Can Solr be used to access Lucene indexes created elsewhere? Or might there be a better solution? Thanks for the heads up. Unfortunately nobody has given this approach a thumbs-up or a thumbs-down yet... possible duplicate: http://stackoverflow.com/questions/2195404/very-basic-dude-with-solr-lucene Success! With Pascal's suggestion of changes to schema.xml I got it working in no time. Thanks! Here are my complete steps for anyone interested: Downloaded Solr and copied dist/apache-solr-1.4.0.war to tomcat/webapps Copied example/solr/conf to /usr/local/solr/ Copied pre-existing Lucene index files to /usr/local/solr/data/index Set solr.home to /usr/local/solr In solrconfig.xml changed dataDir to /usr/local/solr/data (Solr looks for the index directory inside) Loaded my Lucene indexes into Luke for browsing (awesome tool) In the example schema.xml removed all fields and field types except for ""string"" In the example schema.xml added 14 field definitions corresponding to the 14 fields shown in Luke. Example: <field name=""docId"" type=""string"" indexed=""true"" stored=""true""/> In the example schema.xml changed uniqueKey to the field in my index that seemed to be a document id In the example schema.xml changed defaultSearchField to the field in my index that seemed to contain terms Started tomcat saw no exceptions finally and successfully ran some queries in localhost:8080/solr/admin This is just proof for me that it can work. Obviously there's a lot more configuration to be done.  I have never tried this but you would have to adjust the schema.xml to include all the fields of the documents that are in your Lucene index because Solr won't allow you to search for a field if it is not defined in schema.xml. The adjustment to schema.xml should also include defining the query-time analyzers to properly search in your field especially if the field where indexed using custom analyzers. In solrconfig.xml you may have to change settings in the indexDefaults and the mainIndex sections. But I'd be happy to read answers from people who actually did it. I'm looking at the index using Luke and it's not terribly complex. There are 14 fields all typed as strings. I'll give the configuration you suggested a try and report back. Thanks!"
661,A,"Full Text Search with multiple index and complex requirements We are building an application which will require us to index data for each of our users so that we can provide full text search on their data. Here are some notable things about the application: A) The data for every user is totally unrelated to every other user. This gives us few advantages: we can keep our indexes small in size. merging/compatcting fragmented index will take less time. if some indexes becomes inaccessible for whatever reason (corruption?) only those users gets affected. Other users are unaffected and the service is available for them. B) Each user can have few different types of data. We want to keep each type in separate folders for the same reasons as above. So our index hierarchy will look something like: /user1/type1/<index files> /user1/type2/<index files> /user2/type1/<index files> /user3/type3/<index files> C) Often probably with every itereation we'll add ""types"" of data that can be indexed. So we want to have an efficient/programmatic way to add schemas for different ""types"". We would like to avoid having fixed schema for indexing. I like Lucene's schema-less way of indexing stuff. D) The users can fire search queries which will search either: - Within a specific ""type"" for that user - Across all types for that user: in this case we want to fire a parallel query like Lucene has. (ParallelMultiSearcher) E) We require real time update for the index. This is a must. F) We are are planning to shard our index across multiple machines. For this also we want: if a shard becomes inaccessible only those users whose data are residing in that shard gets affected. Other users get uninterrupted service. We were considering Lucene Sphinx and Solr to do this. This is what we found: Sphinx: No efficient way to do A B C F. Or is there? Luecne: Everything looks possible as it is very low level. But we have to write wrappers to do F and build a communication layer between the web server and the search server. Solr: Not sure if we can do A B C easily. Can we? So my question is what is the best software for the above requirements? I am inclined more towards Solr and then Lucene if we get all the requirements. I can't see Solr being able to handle A or B as Solr's model is to have everything in one index (per shard core). Solr can handle C if you use the dynamic field types. Although Solr can do real time indexing it is not as fast as Lucene (even with Embedded Solr in my experience). This all points to Lucene being your only choice. bajafresh4life suggestion of using lots of cores is one didn't think of (for some reason I thought of core == shard on another server). Going strictly on your question specs (A to F) I think Lucene is the best choice as it gives you the most flexibility. However I'm not convinced you actually need separate index files for each user or need to worry about (A) as it seems like premature optimisation. Solr gives you a lot of free stuff over Lucene (Solr requires a lot less lines of code) but forces you to do things the Solr way. I've rewritten an app that used pure Lucene to using Solr and although it did take a while to understand and appreciate the Solr way of doing things the end result is a much better user experience. Does that change your stand on this now ?  I think Solr might work really well for you here. The key feature that Solr has that will work well for you in your sitiuation is the notion of cores. See http://wiki.apache.org/solr/CoreAdmin One way you can implement this is that each user/type combination can be a separate Solr core. This satisfies (A) and (B). The client can either direct the search at a single core or it can direct the search at multiple cores at once (and optional across different Solr servers) which is what you want when you search across a single user and all types. This satisfies (D) and (F). Or you can one core for each user with a ""type"" field that you can filter on. As for (C) Solr has the notion of dynamic fields. See http://wiki.apache.org/solr/SchemaXml#Dynamic_fields As far as (E) goes Solr doesn't have ""true"" real-time indexing yet. But if a lag of a few seconds is acceptable then Solr can handle that. Solr supports multiple cores over multiple nodes so it would work fine over multiple instances in EC2 Solr is designed for high volume so I wouldn't worry about large numbers of users. You can always scale out to multiple nodes. As for the # of cores question I don't think having a large number should be an issue although it's worth testing. I know there is some minimum memory overhead for keeping indexes open and you may need to up the ulimit for open file handles. Re: folder structure it probably won't allow for EXACTLY that structure but you may be able to get close to it. At least there will be one directory per core. do you think Solr wouldn't complain if there are fairly huge number of users ? What is the ideal number of cores can it have without impact on performance ? And can it support the folder structure mentioned in _B_ ? See http://wiki.apache.org/solr/LotsOfCores . Looks like the main issue is being able to open and close lots of cores efficiently. After reading that I am really scared to use Solr now... BTW we are going to use Amazon EC2 instances for this project... We would be having many instances and would prefer spreading index over instances..."
662,A,"Linq to Lucene error: ""Classes must define at least one field as a default search field"" I have the following attributes applied to my linq to sql class: [Document(MetadataType = typeof(SomeObjectMetadata))] public partial class SomeObject { } And this is the metadata code: public class SomeObjectMetadata { [Field(FieldIndex.Tokenized FieldStore.Yes IsKey = true)] private object ProductId { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes IsDefault = true)] private object Name { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Description { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Breadcrumb { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Tab1Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Tab2Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Tab3Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Tab4Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Tab5Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] private object Manufacturer { get; set; } } The index store writes just fine but when I try to search it I get the error shown in the title. Here's an example search:  var qry = from r in _dbi.Get<SomeObject>() where r.Description.Like(search) select r; Upon enumerating ""qry"" the Exception ""Classes must define at least one field as a default search field"" is thrown. should I delete such an idiot question? I would. And... I'm not the one that downvoted you. :) Maybe this will allow someone who made the same mistake to find an answer ;o) Downvotes well deserved. Wow I'm an idiot. The metadata fields were private. This fixes it of course: public class Catalog_ProductMetadata { [Field(FieldIndex.Tokenized FieldStore.Yes IsKey = true)] public object ProductId { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes IsDefault = true)] public object Name { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Description { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Breadcrumb { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Tab1Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Tab2Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Tab3Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Tab4Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Tab5Content { get; set; } [Field(FieldIndex.Tokenized FieldStore.Yes)] public object Manufacturer { get; set; } } +1 for making me laugh :)"
663,A,Lucene 3.1 payload I'm trying to figure out the way that payloads work in Lucene and I can't seem to grasp it. My situation is as follows: I need to index a document that has a single content field and attach to each token from the text within that field a payload (some 10 bytes). The analyzer I need to use is a basic whitespace analyzer. From the various articles I've been reading on the internet the way to do work with payloads would be to create my own Analyzer and attach the payload during the tokenizing step. I've come up with the following code for my new custom analyzer: public TokenStream tokenStream(String fieldName Reader reader) { TokenStream tokenStream = new WhitespaceTokenizer(Version.LUCENE_31 reader); OffsetAttribute offsetAttribute = tokenStream .getAttribute(OffsetAttribute.class); CharTermAttribute termAttribute = tokenStream .getAttribute(CharTermAttribute.class); if (!tokenStream.hasAttribute(PayloadAttribute.class)) { tokenStream.addAttribute(PayloadAttribute.class); } PayloadAttribute payloadAttribute = tokenStream .getAttribute(PayloadAttribute.class); try { while (tokenStream.incrementToken()) { int startOffset = offsetAttribute.startOffset(); int endOffset = offsetAttribute.endOffset(); String token; try{ token = (termAttribute.subSequence(startOffset endOffset)).toString(); } catch(IndexOutOfBoundsException ex){ token = new String(termAttribute.buffer()); } byte[] payloadBytes = payloadGenerator.generatePayload(token frequencyClassDigest); payloadAttribute.setPayload(new Payload(payloadBytes)); } tokenStream.reset(); return tokenStream; } catch (IOException e) { e.printStackTrace(); return null; } } The problems that I am having are the following: I can't correctly read the individual tokens. I'm not sure that by using the CharTermAttribute is the correct way to do it but I know that it just doesn't work. I need to get to the individual token in order to calculate the payload correctly but somehow the WithespaceTokenizer returns the individual words glued together (3 words at a time). I don't know if using the PayloadAttribute is the correct way to attach a payload to a token. Maybe you know of another way Where can I find some good tutorial on how to actually use Payloads in Lucene? I've tried searching the web and the only good article I was able to find was this: Lucene Payload tutorial however it doesn't exactly suit my needs. Thank you I can't seem to find a good tutorial Did you check http://sujitpal.blogspot.com/2010/10/custom-scoring-with-lucene-payloads.html ? You could encapsulate your payload generation logic inside a filter that would generate a payload for each token that comes through the filter. I've modeled this off Lucene's DelimitedPayloadTokenFilter. public final class PayloadGeneratorFilter extends TokenFilter { private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class); private final PayloadAttribute payAtt = addAttribute(PayloadAttribute.class); private final PayloadGenerator payloadGenerator; private final FrequencyClassDigest frequencyClassDigest; public PayloadGeneratorFilter(TokenStream input PayloadGenerator payloadGenerator FrequencyClassDigest frequencyClassDigest) { super(input); this.payloadGenerator = payloadGenerator; this.frequencyClassDigest = frequencyClassDigest; } @Override public boolean incrementToken() throws IOException { if (input.incrementToken()) { final char[] buffer = termAtt.buffer(); final int length = termAtt.length(); String token = buffer.toString(); byte[] payloadBytes = payloadGenerator.generatePayload(token frequencyClassDigest); payAtt.setPayload(new Payload(payloadBytes)); return true; } return false; } } This would make your analyzer code very simple: public class NLPPayloadAnalyzer extends Analyzer { private PayloadGenerator payloadGenerator; private FrequencyClassDigest frequencyClassDigest; public NLPPayloadAnalyzer(PayloadGenerator payloadGenerator FrequencyClassDigest frequencyClassDigest) { this.payloadGenerator = payloadGenerator; this.frequencyClassDigest = frequencyClassDigest; } public TokenStream tokenStream(String fieldName Reader reader) { TokenStream tokenStream = new WhitespaceTokenizer(Version.LUCENE_31 reader); tokenStream = new PayloadGeneratorFilter(tokenStream payloadGenerator frequencyClassDigest); return tokenStream; } } The alternative would be to pre-process your payloads and append them in the text that you send to Lucene and then use the DelimitedPayloadTokenFilter. text text text text would become text|1.0 text|2.2 text|0.5 text|10.5 http://sujitpal.blogspot.com/2010/10/denormalizing-maps-with-lucene-payloads.html is also a good resource. +1 for the hint to DelimitedPayloadTokenFilter
664,A,"Lucene stop phrases filter I'm trying to write a filter for Lucene similar to StopWordsFilter (thus implementing TokenFilter) but I need to remove phrases (sequence of tokens) instead of words. The ""stop phrases"" are represented themselves as a sequence of tokens: punctuation is not considered. I think I need to do some kind of buffering of the tokens in the token stream and when a full phrase is matched I discard all tokens in the buffer. What would be the best approach to implements a ""stop phrases"" filter given a stream of words like Lucene's TokenStream? In this thread I was given a solution: use Lucene's CachingTokenFilter as a starting point: That solution was actually the right way to go.  You'll really have to write your own Analyzer I should think since whether or not some sequence of words is a ""phrase"" is dependent on cues such as punctuation that are not available after tokenization. Actually punctuation can be discarded: I need to match phrases which can themselves be described as word tokens Please edit your question to make this clear."
665,A,Lucene locking exceptions I'm load testing a webservice which writes to a lucene index. If I make the same call repeatedly I get a org.apache.lucene.store.LockObtainFailedException: I assume this is because I'm trying to write to an index which is already locked by another thread and that thread waits. My question is what is the best way to solve this problem? Do I increase the waiting time or add the write requests to a queue? Please advise thanks. Does this happen after you have redeployed the webapp? The locks are released after the writer is done. There are just too many writes going to the one index that they're timing out. Why do you have multiple writers? IndexWriter is inherently thread-safe; you should have all your threads accessing the same writer. This will get rid of your locking issues. Yes I'm now queuing all my threads to the same writer. Thanks.
666,A,How to deal with constantly changing data and SOLR indexes? Afternoon guys I'm using a SOLR index for searching through items on my site. The search results contain an average rating of the item and an amount of comments the item has. The results can be sorted by both rating and num of comments. But obviously with the solr index these numbers aren't updated until the db (2million~ rows) is reindexed (done nightly probably). What would you guys think is the best way to approach this? Well i think you should change your db - index sync policy: First approach: when commiting database changes also post changes (a batch of them) to indexes. You should write a mapper tier to map your domain objects to solr docs (remember persists and if it goes ok then index -this works fine for us ;-)). If you want to achieve near real-time index updates you should see solutions like zoey (linkedin lucene-based searching framework) Second approach: take a look around delta import (and program more frecuently index updates). Excellent thanks for the reply Lici I'm going to take a good look at Zoie next week is the interface setup procedure similar to Solr? I'm not familiar with mapping domain object to solr docs do you have any links that could enlighten? Again massive thanks for your reply :) Zoey is a lower-lever solution than Solr. I think the best solution is to continue indexing via DIH and also design you domain --> SolrInputDocument mapping tier to get fresh updated results. Your mapping tier depends on the client technology chosen. I use SolrJ (Java-based). See : http://wiki.apache.org/solr/IntegratingSolr
667,A,"Handling + as a special character in Lucene search How do i make sure lucene gives me back relevant search results when my input string contains terms like c++? Lucene seems to ignore ++ characters. Code details: When I execute this lineI get a blank search query. queryField = multiFieldQueryParser.Parse(inpKeywords); keywordsQuery.Add(queryField BooleanClause.Occur.SHOULD); And here is my custom analyzer: public class CustomAnalyzer : Analyzer { private static readonly WhitespaceAnalyzer whitespaceAnalyzer = new WhitespaceAnalyzer(); public override TokenStream TokenStream(String fieldName System.IO.TextReader reader) { TokenStream result = whitespaceAnalyzer.TokenStream(fieldName reader); result = new StandardTokenizer(reader); result = new LowerCaseFilter(result); result = new StopFilter(result stop_words); return result; } } And I'm executing search query this way: indexSearcher.Search(searchQuery collector); I did try queryField = multiFieldQueryParser.Parse(QueryParser.Escape(inpKeywords));but it still does not work. Here is the query which get executed and returns zero hits. ""+(())"" Thanks. Try UTF-8 encoding your search queries. You can enable this as described in this article link is not working....  In addition to choosing the right analyzer you can use QueryParser.Escape(string s) to ensure all special characters are properly escaped. Because this is a static function you can use it even if you're using MultiFieldQueryParser. For example you can try something like this: queryField = multiFieldQueryParser.Parse(QueryParser.Escape(inpKeywords)); Thanks again for the response Jesse.I did try queryField = multiFieldQueryParser.Parse(QueryParser.Escape(inpKeywords));but it still does not work. Here is the query which get executed and returns zero hits. ""+(())"" thanks for ur answer.Please refer to my updated question. Hey Ed does it work for simple keywords without the Escape function? If so perhaps post more code relating to your Searcher and QueryParser objects. Also remember that you must search with the same Analyzer you use for indexing.  Since + is a special character it needs to be escaped. The list of all characters that need to be escaped is here (See bottom of the page.) You also need to be careful about the analyzer you use while indexing. For example StandardAnalyzer will skip +. You may need to use something like WhiteSpaceAnalyzer while indexing and searching which will preserve special characters in the tokenstream. Keep in mind that you need to use the same analyzer while indexing and searching. am using WhiteSpaceAnalyzer only...but still + characters are ignored"
668,A,How can I check Solr index using Luke How can I check my solr index using Luke? I tried pointing to my index location (solr/core1/data/index). But I am getting the error Unknown format version: -12 what version of Luke and what version of Solr are you using? I got that error with a nightly build of lucene (4.x) and an older luke (1.0.1) - the latest currently available from its site. Check out & build the latest luke from here: http://code.google.com/p/luke/source/checkout It reads latest lucene 4.x ok 3.x versions as well. Thank you kowd. If someone comes to this question this github repo (https://github.com/tarzanek/luke) is usually updated with newer versions of luke as lucene upgrades.  As Mauricio suggests this error means that your luke version is older than your solr version. Try updating to the newest version of Luke. I was using the latest version of Luke downloaded from the website.
669,A,"Luke like tool in C# Is there are Luke like tool for viewing lucene indexes in C# using the Lucene.NET api? I realize this is an old question. There is an alternative now. Luke.NET available on CodePlex: http://luke.codeplex.com/  Why can't you use the Java version of Luke? It should be compatible with indexes generated with Lucene.NET  If I could comment to a post I would here but... Bajafresh is correct - you can use the Java version of Luke with indexes built in C# with Lucene.net. I used this approach on a job last year with no troubles. Even dropped Luke on my Program Manager's desktop so she could get a feel for the ""new"" technology."
670,A,"is it possible to use negative query boost in lucene? I want to penalize some terms in query not to ignore them at all so ""MUST NOT"" operator will not work? Is it possible to use negative query boost with SHOULD in boolean query in lucene how does it work? Yes a query term boost is simply a multiplication factor so setting it to a negative value will have the affect you want. Here's a thread from the lucene mailing list discussing negative boost. In summary in is like a NOT but less strict in that matching documents will still appear in search results i.e. Any positive score (>0) will have the affect of increasing the default score. Any negative score (<0) will have the affect of decreasing the default score."
671,A,"Lucene Java opening too many files. Am I using IndexWriter properly? My Lucene Java implementation is eating up too many files. I followed the instructions in the Lucene Wiki about too many open files but that only helped slow the problem. Here is my code to add objects (PTicket) to the index: //This gets called when the bean is instantiated public void initializeIndex() { analyzer = new WhitespaceAnalyzer(Version.LUCENE_32); config = new IndexWriterConfig(Version.LUCENE_32 analyzer); } public void addAllToIndex(Collection<PTicket> records) { IndexWriter indexWriter = null; config = new IndexWriterConfig(Version.LUCENE_32 analyzer); try{ indexWriter = new IndexWriter(directory config); for(PTicket record : records) { Document doc = new Document(); StringBuffer documentText = new StringBuffer(); doc.add(new Field(""_id"" record.getIdAsString() Field.Store.YES Field.Index.ANALYZED)); doc.add(new Field(""_type"" record.getType() Field.Store.YES Field.Index.ANALYZED)); for(String key : record.getProps().keySet()) { List<String> vals = record.getProps().get(key); for(String val : vals) { addToDocument(doc key val); documentText.append(val).append("" ""); } } addToDocument(doc DOC_TEXT documentText.toString()); indexWriter.addDocument(doc); } indexWriter.optimize(); } catch (Exception e) { e.printStackTrace(); } finally { cleanup(indexWriter); } } private void cleanup(IndexWriter iw) { if(iw == null) { return; } try{ iw.close(); } catch (IOException ioe) { logger.error(""Error trying to close index writer""); logger.error(""{}"" ioe.getClass().getName()); logger.error(""{}"" ioe.getMessage()); } } private void addToDocument(Document doc String field String value) { doc.add(new Field(field value Field.Store.YES Field.Index.ANALYZED)); } EDIT TO ADD code for searching public Set<Object> searchIndex(AthenaSearch search) { try { Query q = new QueryParser(Version.LUCENE_32 DOC_TEXT analyzer).parse(query); //search is actually instantiated in initialization. Lucene recommends this. //IndexSearcher searcher = new IndexSearcher(directory true); TopDocs topDocs = searcher.search(q numResults); ScoreDoc[] hits = topDocs.scoreDocs; for(int i=start;i<hits.length;++i) { int docId = hits[i].doc; Document d = searcher.doc(docId); ids.add(d.get(""_id"")); } return ids; } catch (Exception e) { e.printStackTrace(); return null; } } This code is in a web application. 1) Is this the advised way to use IndexWriter (instantiating a new one on each add to index)? 2) I've read that raising ulimit will help but that just seems like a band-aid that won't address the actual problem. 3) Could the problem lie with IndexSearcher? just increase the number of filedescriptors on your server 1) Is this the advised way to use IndexWriter (instantiating a new one on each add to index)? i advise No there are constructors which will check if exists or create a new writer in the directory containing the index. problem 2 would be solved if you reuse the indexwriter. EDIT: Ok it seems in Lucene 3.2 the most but one constructors are deprecatedso the resue of Indexwriter can be achieved by using Enum IndexWriterConfig.OpenMode with value CREATE_OR_APPEND. also opening new writer and closing on each document add is not efficienti suggest reuse if you want to speed up indexing set the setRamBufferSize default value is 16MB so do it by trial and error method from the docs: Note that you can open an index with create=true even while readers are using the index. The old readers will continue to search the ""point in time"" snapshot they had opened and won't see the newly created index until they re-open. also reuse the IndexSearcheri cannot see the code for searching but Indexsearcher is threadsafe and can be used as Readonly as well also i suggest you to use MergeFactor on writer this is not necessary but will help on limiting the creation of inverted index files do it by trial and error method All of the constructors for IndexWriter have been deprecated in Lucene 3.2 except for the one I am using. I'll check on the IndexSearcher @gmoore: chk update Does anyone know why this construction method for IndexWriter is deprecated  I think we'd need to see your search code to be sure but I'd suspect that it is a problem with the index searcher. More specifically make sure that your index reader is being properly closed when you've finished with it. Good luck I'm not closing IndexSearcher because Lucene says that is okay. From their Wiki ""Make sure you only open one IndexSearcher and share it among all of the threads that are doing searches -- this is safe and it will minimize the number of files that are open concurently."" Thanks though.  This question is probably a duplicate of Too many open files Error on Lucene I am repeating here my answer for that. Use compound index to reduce file count. When this flag is set lucene will write a segment as single .cfs file instead of multiple files. This will reduce the number of files significantly. IndexWriter.setUseCompoundFile(true) The Lucene FAQ says that setUseCompoundFile is true by default since 1.4 http://wiki.apache.org/lucene-java/LuceneFAQ#Why_am_I_getting_an_IOException_that_says_.22Too_many_open_files.22.3F  The scientific correct answer would be: You can't really tell by this fragment of code. The more constructive answer would be: You have to make sure that there is only one IndexWriter is writing to the index at any given time and you therefor need some mechanism to make sure of that. So my answer depends of what you want to accomplish: do you want a deeper understanding of Lucene? or.. do you just want to build and use an index? If you answer is the latter you probably want to look at projects like Solr which hides all the index reading and writing. I just want to build and use an index and I don't want to use Solr. I'll look into the multiple IndexWriters."
672,A,"Lucene standard analyzer split on period How do I make Lucene's Standard Analyzer tokenize on the'.' char? For eg. on querying for ""B"" I need it to return the B in ""A.B.C"" as the result. I need to treat numbers the way the standard analyzer treats it and hence the Simple analyzer is not sufficient. It would be perfect if I could just specify to the standard Analyzer to tokenize on the'.' char too. If I had to write my own tokenizer with just this small extension how would I go about it? Thanks Nacha I believe the easiest is to create your own Analyzer. which will get tokens from StandardAnalyzer as input and further split tokens on dots keeping dotless tokens intact. The package summary gives some advice on how to do this. This blog post seems very relevant but uses an old version of Lucene so you will probably need to tweak it. Also see the Lucene FAQ. Thanks If i was writing my own filter that filters the Standard Analyzers resultant token stream to split those tokens that contain '.' how would i go about it? As far as I have seen filters have been used to skip tokens not to decompose one token into 2 tokens.."
673,A,"XML parser + Indexing data I need to index some xml documents with Lucene but before that i need to parse those XML and extract some info inside their tags. The XML looks like this: <?xml version=""1.0"" encoding=""UTF-8""?> <tt xml:lang=""es"" xmlns=""http://www.w3.org/2006/04/ttaf1"" xmlns:tts=""http://www.w3.org/2006/04/ttaf1#styling""> <head> <styling> <style id=""bl"" tts:fontWeight=""bold"" tts:color=""#FFFFFF"" tts:fontSize=""15"" tts:fontFamily=""sansSerif""/> </styling> </head> <body> <div xml:lang=""es""> <p begin=""00:00.50"" end=""00:04.02"" style=""bl"">Info</p> <p begin=""00:04.32"" end=""00:07.68"" style=""bl"">Different words<br />and phrases to index</p> <p begin=""00:11.76"" end=""00:16.04"" style=""bl"">Text</p> <p begin=""00:18.52"" end=""00:22.88"" style=""bl"">More and<br />more text</p> </div> </body> </tt> I need to extract only the timestamps inside the tags begin and end and then index the text inside the p tags. The goal is to query the text indexed and know in which timestamp gap are each hit. For example if i query the word ""Text"" the output should say something like: ""2 hits 00:11.76-00:16.04 00:18.52-00:22.88"" I started indexing the entire XML with Lucene. Now i want to parse the file but im not sure what is the best approximation to solve this problem. Any help or advice is welcome :) Thank you all! I can highly recommend storing all your XML in an eXist database which has Lucene built-in. I've been using this combination for a few months now and it solves a lot of search and retrieval problems quite easily. I've been looking the eXist database and seems to be a very good tool but what i need to do right now doesnt fit at all in this database. Anyway thanx for the info :D  I used the SAX library (i.e. a subclass of org.xml.sax.helpers.DefaultHandler ) to parse XML files extracted the desired information from each XML document into my own Document class and then indexed that Document instance. (The indirection was due to having multiple document formats that had to be parsed separately but indexed in the same index.) In your case if the contents of each of your <body> elements represents a logical document you can store the date information as payloads associated with specific tokens. Parse the XML to the <p> level enumerate the paragraph instances and for each instance add a new Field instance with the same name where the value is the text and the payload is the date information suitably represented. (Payloads are binary so for example you could store the two long values corresponding to the start and end times.) When you add multiple field instances with the same name to a document they get indexed as the same field but you can assign different payloads to each instance you can adjust the position of the start of the text etc. If you don't need the contents of each element as a single document you can treat each <p> as a separate document and then set the payload on that. Alternatively you can store dates as a separate field. I studied a little some parsers libraries and i think im going to use SAX since i need to parse all the tags but each of those tags have different values in their attributes. I'll be back if i find problems :D"
674,A,Grouping search results by fields How do i group search results returned by Lucene by fields? Thanks! already asked? http://stackoverflow.com/questions/342966/grouping-lucene-search-results#342975
675,A,How to apply default sorting in lucene on equal scores? Good day If I have for example the documents which have the following fields Person_name - Birthday Jordan - 2009-06-15 Marc - 2009-01-01 Marcos - 2009-01-01 Marcissh_something_something - 2009-06-15 Marcos - 2009-12-31 And upon searching for Person_name:Marc* I got the following scores (scores here are hypothetical) Person_name - Birthday - Score Jordan - 2009-06-15 - 0.0 Marc - 2009-01-01 - 1.0 Marcos - 2009-01-01 - 0.8 Marcissh_something_something - 2009-06-15 - 0.1 Marcos - 2009-12-31 - 0.8 How can I retrieve the result such that the result is first sorted by relevancy and then assuming same relevancy (score) sort by birthday descendingly....such that the result is Person_name - Birthday - Score Marc - 2009-01-01 - 1.0 Marcos - 2009-12-31 - 0.8 Marcos - 2009-01-01 - 0.8 Marcissh_something_something - 2009-06-15 - 0.1 Thanks I was going to recommend a ScoreDocComparator but I see it is deprecated now. You can use a FieldComparator. You need to create a TopFieldCollector and define its Sort according to your wishes. I believe this is rather new as I could not find a good example. Since I'm using lucene 2.4.1 I'll probably go with ScoreDocComparator...but how do I use it exactly? :-) In that case please see my answer to: http://stackoverflow.com/questions/8517/lucene-exact-ordering  Try to examine the search results and then sort those with equal score yourself. You can use a comparator for this which compares the score and then the natural fields of the search results.
676,A,"NoSQL Word Proximity Are there any NoSQL databases that support word proximity searching similar to lucene? I have a client that would like the flexibility of NoSQL with the search power of a Lucene or some other search tool. The average amount of data to be searched is 200GB Lucene is a NoSQL database.  Probably too late to be useful but check out MarkLogic. It's a document database with integrated full-text search (not bolt-on Lucene). You can see a quick demo via http://developer.marklogic.com/try/corona/index  If you can manage a .NET/Win solution also check out RavenDB - has lucene baked into it. If not Schild's answer is a good one. You can also use lucene separately with MongoDB but your app would have to maintain the index itself... I have also read that RavenDB will run on mono/linux with the Munin managed storage plugin.  Take a look at tjake's Solandra (former Lucandra). ""Solandra is a real-time distributed search engine built on Apache Solr and Apache Cassandra."" Solandra ""supports most out-of-the-box Solr functionality (search faceting highlights)"""
677,A,"looking for a db abstraction/substitute that actually works i am looking for a form of data storage that will answer a few requirements. i realize these requirements are non-standard and for now i'm using activerecord and ORM solutions like everyone else but this is my ""holy grail"" - if you know of anything like this i would be eternally grateful: pure PHP multiple repositories preferably file based for portability where i can instantiate by telling it ""use repository [X]"" - i don't want to pre-create repository [X] if i reference it it exists. zero database configuration - i don't want to create tables or export SQL dumps if it's referenced in my code it needs to be in the database auto-created without any fuss my code is my schema hierarchical not relational ideal structure would be just a freeform schema-less XML but since XML performs horribly with large trees it can't simply be an XML file. i have experimented with flat XML storage (with xpath and xquery) but it gags on a mid-sized repository and cripples the application. i have also experimented with key=>value pairs dropped into a SQLite database with a single generic table but that gags even faster and re-forming even the simplest record from key=>value pairs is a performance decimator. finally i experimented with lucene as implemented in the zend framework which was pretty close to ideal apart from the no-update part. any ideas anyone? i could have but that's not what i'm looking for. SQLite does create a repository simply by declaring it but table and fields management are the same as other rdbms which is no good to me. i really don't want to deal with DBs and DB migration and i don't mind paying with performance. You could have just asked for a SQLite ORM. Here are some links you may find useful: txtSQL Gladius DB Also have you considered using Berkeley DB? Some of the DB extensions listed in the PHP Manual are intended to be used on flat-file like databases. From your description it seems like PHP arrays should work perfectly: pure PHP multiple arrays file or memory based your code is your schema hierarchical You could use serialize() or var_export() functions to enable file storage. Arrays work up to a point the problem begins when you have massive amounts of data you need to index/search/sort. My ideal solution would be something like one or more of these: 1) an XML file i can do what i want with without the performance issues of XML. 2) a database i can throw objects into while following no schema or predefined structure and then be able to run queries on all objects as if they were all in the same table 3) redBean is not perfect but looks super cool if it supported SQLite i would probably say ""problem solved I can make this work"" i am familiar with both txtSQL and gladius but i may as well use SQLite. my question was about a persistent data store with behaviour **different** from a normal relational database. Berkeley DB (or rather Berkeley DB XML) looks promising but it requires **purchase** and **installation** which makes it unusable to me. @Nir: Answer updated maybe you should try to describe what you're after in more detail.  I've been having great fun with RedBean it's not quite designed for flatfiles but runs on PDO so it should be relatively easy to write a sqlite module for it. Not sure if it will work for your needs but definitely worth taking a look at. really cool project thanks. i think i can use it in my project maybe take a day or so to write a SQLite connector but even without SQLite it's just great. one year later i use redbean almost exclusively if you happen to be reading this thread: USE REDBEAN!!!!! it is absolutely freakin' awsome the best of the best!"
678,A,Zend_Lucene CJK support Does someone know if Zend_Lucene class support CJK (Chinese Japanese Korean). I want to use it on my own website the only problem it should work for both English and Japanese language. Also if someone has some ressource about CJK version of the Java version would be appreciated also. Thanks no one on this? Currently these are the only UTF-8 compatible analysers built into Zend_Lucene Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8 Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8Num Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8_CaseInsensitive Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8Num_CaseInsensitive You can use them by using the following code: Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_Text()); You can also build your own analyzer if you want. An alternative solution would be to build the index using Java Lucene and use that index within PHP since they're supposed to be compatible. I haven't tried this though. Zend_Search_Lucene was derived from the Apache Lucene project. The currently (starting from ZF 1.6) supported Lucene index format versions are 1.4 - 2.3 You can read more about this in the Zend Framework manual. link text Thanks for the answer I was thinking using the java version for building the index I don't really know if lucene play well with CJK but I'll try.
679,A,log4j Log Indexing using Solr We are finding it very hard to monitor the logs spread over a cluster of four managed servers. So I am trying to build a simple log4j appender which uses solrj api to store the logs in the solr server. The idea is to use leverage REST of solr to build a better GUI which could help us search the logs and the display the previous and the next 50 lines or so and tail the logs Being awful on front ends I am trying to cookup something with GWT (a prototype version). I am planning to host the project on googlecode under ASL. Greatly appreciate if you could throw some insights on Whether it makes sense to create a project like this ? Is using Solr for this an overkill? Any suggestions on web framework/tool which will help me build a tab-based front end for tailing. Sounds like a good idea for a project. There's already one that logs to file and indexes with lucene perhaps you can setup a gui that hooks into that index? Failing that i'd defo think about it as two modules 1. the appender and 2. the ui. You can use a combination of logstash (for shipping and filtering logs) + elasticsearch (for indexing and storage) + kibana (for a pretty GUI). This is great Stuff. Kibana looks very promising. Almost like splunk  Totally doable thing. Many folks have done the roll your own. A couple of useful links.. there is an online service www.loggly.com that does this. They are actually based on Solr as the core storage engine! Obviously they have built a proprietary interface. Another option is http://www.graylog2.org/. It is opensource. Not backed by Solr but still very cool! Thanks Eric. The links are really helpful.  The loggly folks have also built logstash which can be backed by quite a few things including lucene via elastic search. It can forward to graylog also.
680,A,"Avoid removal of current Lucene.NET index during rebuild I'm new to Lucene.NET but I'm using an open source tool built for Sitecore CMS that uses Lucene.NET to index lots of content from the CMS. I confirmed yesterday that when I rebuild my indexes the current index files wipe clean so anything that relies on the index gets no data for about 30-60 seconds (the amount of time for a full index rebuild). Is there a best practice or way to make Lucene.NET not overwrite the current index files until the new index is completely rebuilt? I'm basically thinking I'd like it to write to new temp index files and when the rebuild is done have those files overwrite the current index. Example of what I'm talking about: Build fresh index (~30 seconds) Index has about 500 documents Use code to access data in index and display on website Rebuild index (~30 seconds) Any code that now reads the index for data returns nothing because the index files are being overwritten; results in website not showing any data Rebuild complete: data now available again data back on website Thanks in advance You can open an index in append mode in which case your current index remains available until you flush or commit your new writes. I'm not familiar with that sitecore tool but I can answer how you would do it with pure Lucene.Net: you should use an NRT setup which means ""have one index writer and never close it."" Basically index writers have a ""virtual"" index in memory until it gets flushed to disk. So as long as you get your readers from the writer you'll always see the latest stuff even if it hasn't been flushed to disk yet.  I have no experience with ""Sitecore"" itself but here's my story. We've recently incorporated the index-based search (using Lucene.Net) for our eCommerce sub-system. The index update process for our case might take about half a hour (~50000 products themselves + lots of related information). To prevent a ""denial of service"" responses during the update of the index we first create a ""backup"" version of the it (simply copying index directory to another location) and all further requests are redirected to use this ""backup"" version. When the index update is completed we delete the backup in order for clients to start using the updated (or ""live"") version of the index. This is also helps in case of any unhandled exceptions that might occur during the update process becase you might end up in a situation of having no index at all (and in our case clients can always use the ""backup"" version). The API reference (Lucene 2.4) of the Lucene.Net.Index.IndexWriter object states the following: Note that you can open an index with create=true even while readers are using the index. The old readers will continue to search the ""point in time"" snapshot they had opened and won't see the newly created index until they re-open. So at least you shouldn't worry about the clients that are currently searching within your index. Hope this will help you to make a right decision."
681,A,"Why does 'delete document' in lucene 2.4 not work? Hi I want to delete a document in lucene 2.4 with java. My code is  Directory directory = FSDirectory.getDirectory(""c:/index""); IndexReader indexReader = IndexReader.open(directory); System.out.println(""num=""+indexReader.maxDoc()); indexReader.deleteDocuments(new Term(""name""""1"")); System.out.println(""num=""+indexReader.maxDoc()); output num=1 num=1 maxDoc() won't change until you optimize the index using an IndexWriter. At the very least you need to commit() or your delete may never even make it to disk. However numDocs() should return the number of non-deleted documents even before a commit or optimize. It's probably better practice (and certainly less confusing) to use an IndexWriter to add and delete documents and to open your IndexReaders read-only; 3.0 will open them read-only by default.  In my opinion it is best to use Indexwriter to delete the documents since Indexreader buffers the deletions and does not write changes to the index until close() is called on.; unless you use the same reference for search. The Lucene wiki states Generally it's best to use IndexWriter for deletions unless you must delete by document number you need your searches to immediately reflect the deletions or you must know how many documents were deleted for a given deleteDocuments invocation I can see you want the maxdoc value for the document in memory so its a better approach to use Indexwriter so the answer for your question is you should close the Indexreader object or use Indexwriter for deletions Thanks you very much sir faced same issue. I use indexWriter and even call commit close after deletion. Changes are not reflected in memory and disc. Update works fine"
682,A,"Lucene sorting chinese characters by strokes/radicals Anyone know if Lucene's sort by field feature will sort chinese characters by strokes/radicals or if there's a way to enable it do so? I couldn't find any relevant answer in their docs. I don't think there are other options except ""Unicode binary order"". Have a look at http://lucene.apache.org/java/3_0_3/api/contrib-collation/index.html which describes how to use Collator order. The package summary has some examples: http://lucene.apache.org/java/3_0_3/api/contrib-collation/org/apache/lucene/collation/package-summary.html Sorry this looks interesting but I'm not sure how to apply it to the sorting? The Sort class only allows me to specify the fields to be sorted by but not any collator or collationkeyfilter? (Sorry kind of new to Lucene)"
683,A,"Is it possible to user Solr facets with Solr dynamic fields? I am about to use Solr'd Dynamic Fields for the first time but my requirements state that those fields have to be facetable. I did quite a lot of googling and doc reading but I can't find a place that either confirms or denies the allegation :) Does anyone here know? Thanks in advance! Andre Can you explain how? The below question says ""No"" http://stackoverflow.com/questions/7512392/facet-dynamic-fields-with-apache-solr Yes you can facet on dynamic fields just fine. Thanks Mauricio! I have not had a chance to test this yet but I will have to in a couple of weeks and I will mark your answer as correct once I confirm it."
684,A,"elasticsearch / lucene highlight I'm using ElasticSearch to index documents. My mapping is: ""mongodocid"":{""boost"":1.0 ""store"": ""yes"" ""type"": ""string""} ""fulltext"": {""boost"": 1.0 ""index"": ""analyzed"" ""store"": ""yes"" ""type"": ""string"" ""term_vector"" : ""with_positions_offsets""}} To highlight the complete fulltext I am setting number_of_framgments to 0. If I do the following Lucene-like string query: '{""highlight"": {""pre_tags"": ""<b>"" ""fields"": {""fulltext"": {""number_of_fragments"": 0}} ""post_tags"": ""</b>""} ""query"": {""query_string"": {""query"": ""fulltext:test""}} ""size"": 100}' For some documents in the result set the length of the highlighted fulltext is smaller than the fulltext itself. Since I am setting number_of_fragments to 0 and pre_tags/post_tags are added this should not happen. Now comes the strange behaviour: If I only search for one of the failing elements by doing this: '{""highlight"": {""pre_tags"": ""<b>"" ""fields"": {""fulltext"": {""number_of_fragments"": 0}} ""post_tags"": ""</b>""} ""query"": {""query_string"": {""query"": ""fulltext:test AND mongodocid:4d0a861c2ebef6032c00b1ec""}} ""size"": 100} then all works fine. Any ideas? Sounds like issue which has been fixed in 0.14.0 (see #479). As of writing the 0.14.0 hasn't been released yet can you try master? works great with master. thanks"
685,A,"What is the best way to use Lucene from a Cocoa app? I'm interested in working with Lucene from a Cocoa application. I'm aware that there are many ways to do this but my question is ""which way is best?"" My investigations so far: LuceneKit is an Objective-C port of Lucene but is based on a version of Lucene that is ancient at this point and in trying to use it I've run into several major issues from the get go. (Improper subclass of NSDate; A basic query that works in Luke doesn't work with LuceneKit;) It appears to be a non-starter. CLucene looked like it might be viable but it fails a bunch of it's own tests on build including an intermittent concurrency related problem where half the time I run the tests they deadlock. Not inspiring. This still may be the answer but I'm very nervous considering my experience just building it and running its own tests. Current Apache Lucene via JNI - Having simply never called a Java library from C I'm unsure what's involved here. I certainly feel like the official Apache-curated incarnation of Lucene is likely to be the most mature and functional but having not done the C <-> Java JNI thing before I'm unclear how the effort involved would compare to working with CLucene. Maybe there are other options. I'm not necessarily looking for a first-class Objective-C interface (although I wouldn't turn one down either) just something functional and hopefully reasonably mature and reasonably performant. Anyone have any sage advice? From my experience using JNI (although not with Lucene) it's not too tricky to get something simple working but you can wind up writing a lot of fairly monotonous code wiring everything up. Another option you may want to consider is JCC which is used by the PyLucene project to generate a boilerplate C++ wrapping around the JNI itnerface which they then use to wrap a Python API around. I'm accepting this answer because it's the direction I've gone and I've been reasonably satisfied. However others reading this answer in the future should be warned this is not a straightforward or easy approach and doesn't present one with a ready path from ""want to use Lucene"" to ""using Lucene."" If anything I would say that the real benefit of working with JCC was that it was an awesome way to learn all the gory details of JNI in one fell swoop. Thanks for the tip!"
686,A,Error while copying Lucene index I've an asp.net web application which uses Lucene API for search. Here is the problem scenario: Events: User invokes a Lucene search query thru the web application. There is another windows service running which just copies the search index folder to another folder. When event 2 occurs after event 1 has occurred I am getting error below while copying the index: The process cannot access the file 'C:\Indexes\segments.gen' because it is being used by another process. What am i missing here? FYI am using System.IO.File.Copy with overwrite set to true to copy index files. I described how I used Lucene.NET in an ASP.NET application here: http://ifdefined.com/blog/post/Full-Text-Search-in-ASPNET-using-LuceneNET.aspx My code might not be right for a high volume website but for low volume it does seem to solve he problem you are having.  The problem is because the first event locks Lucine files You can create 2 copies of the same index and synchronize you processes to not prevent each other thanks for ur comments...could you please elaborate ur explanation?
687,A,Using Lucene Highlighter along with MultiFieldQueryParser Im using Lucene Highlighter to highlight the matches that I have found in a Lucene Index. Now my problem is that If I have to search multiple fields of a document and I need to display the matching text then how can I get in which field the hit has occurred? The code which I am using for the highlighter is basically the second function here If I do not know in which field the hit has occurred in then what field do I pass to the function defined above to get the matching fragments? Same problem here ... Did you find a solution ? Can you generate highlighting on each field separately? It will involve several calls but since they are on different fields there should be no loss of performance. You could combine the fields into a single text field and run your highlighter on it.
688,A,"Can I make Lucene return an unlimited number of search results? I am using Lucene 3.0.1 in a Java 5 environment. I've been researching this issue a little bit but the documentation hasn't given any direct answers. Using the search method TopFieldDocs search(Weight weight Filter filter int nDocs Sort sort) I always need to provide a maximum number of search results nDocs. What if I wanted to have all matching results? It feels like setting nDocs to Integer.MAX_VALUE is a kind of hacky way to do this (and would result in speed and memory performance drop?). Anyone else who has any idea? You are using a search method that returns the top n hits for a query. There are other (more low-level) methods that do not have the limitation and it says in the documentation that ""applications should only use this if they need all of the matching documents. The high-level search API (search(Query int)) is usually more efficient as it skips non-high-scoring hits."". So if you really need all documents you can use the low-level API. I doubt that it makes a big difference in performance to passing a really high limit to the high-level API. If you need all documents (and there really are a lot of them) it is going to be slow either way especially if sorting is involved. Thanks Thilo this was very helpful. I figured out another way to do it which is to use searcher.maxDoc() as nDoc but I figure that could also have an impact on the performance. What is the difference between the low vs. high level APIs?"
689,A,"How can I get DocId when adding a document in Lucene index? I am indexing a row of data from database in Lucene.Net. A row is equivalent of Document. I want to update my database with the DocId so that I can use the DocId in the results to be able to retrieve rows quickly. I currently first retrive the PK from the result docs which I think should be slower than retriving directly from the database using DocId. How can I find the DocId when adding a document to Lucene? Relying on Lucene's DocId is a bad policy as even Lucene tries to avoid this. I suggest you create your own DocId. In a database I would use an auto-increment field. If your application does not use a relational database you can create this type of field programmatically. Other than that I suggest you read Search Engine versus DBMS - I believe that only fields that may be searched should be stored in Lucene; The rest of the row belongs in a database so the sequence of events is: Using Lucene search for some text and get a DocId. Use the DocId to retrieve the full row from the database.  As Yuval stated leaking internal Lucene implementation details is bad especially since Lucene doc id's change when the index is mutated. If looking up the primary key using doc.get(""pk"") is too slow for you use a FieldCache to cache all the pk's in memory. Then the lookups will be plenty fast. Any sample code snippet to use FieldCache? I agree that relying on doc id is almost always poor design. However I have a particular use case in which I have a read-only index and need to do some processing outside of what's possible with a search query so I need to store the doc id of certain documents for later reference. Can you please elaborate on using FieldCache to do so?"
690,A,"Lucene.Net support phrases?: What is best approach to tokenize comma-delimited data (atomically) in fields during indexing? I have a database with a column I wish to index that has comma-delimited names e.g. User.FullNameList = ""Helen Ready Phil Collins Brad Paisley"" I prefer to tokenize each name atomically (name as a whole searchable entity). What is the best approach for this? Did I miss a simple option to set the tokenize delimiter? Do I have to subclass or write my own class that to roll my own tokenizer? Something else? ;) Or does Lucene.net not support phrases? Or is it smart enough to handle this use case automatically? I'm sure I'm not the first person to have to do this. Googling produced no noticeable solutions. * EDIT: using my example I want to store these name phrases in a single field: Helen Ready Phil Collins Brad Paisley NOT these individual words: Helen Ready Phil Collins Brad Paisley Hi Pete - did you get anywhere with this solution? Edit: Having read your clarification here is hopefully a more relevant answer: You did not miss an option to modify the separator character. You do need to roll your own tokenizer. I suggest you subclass CharTokenizer. You need to define isTokenChar() according to your spec meaning that anything but a comma is a token char. Yuval I want to index three full names rather than six individual words in a single field. I clarified my question and example above. Pete please see the new version of my answer.  You can split the string by comma yourself and either -- Index each name using the Keyword analyzer (non-tokenized) OR index each name using the standard analyzer and wrap your searches in quotes. Make sure to index a dummy term in between each name so that ""Ready Phil"" doesn't match the document"
691,A,Multi-Term Wildcard queries in Lucene? I'm using Zend_Search_Lucene the PHP port of Java Lucene. I currently have some code that will build a search query based on an array of strings finding results for which at least one index field matches each of the strings submitted. Simplified it looks like this: (Note: $words is an array constructed from user input.) $query = new Zend_Search_Lucene_Search_Query_Boolean(); foreach ($words as $word) { $term1 = new Zend_Search_Lucene_Index_Term($word $fieldname1); $term2 = new Zend_Search_Lucene_Index_term($word $fieldname2); $multiq = new Zend_Search_Lucene_Search_Query_MultiTerm(); $multiq->addTerm($term1); $multiq->addTerm($term2); $query->addSubquery($multiq true); } $hits = $index->find($query); What I would like to do is replace $word with ($word . '*') — appending an asterisk to the end of each word turning it into a wildcard term. But then $multiq would have to be a Zend_Search_Lucene_Search_Query_Wildcard instead of a Zend_Search_Lucene_Search_Query_MultiTerm and I don't think I would still be able to add multiple Index_Terms to each $multiq. Is there a way to construct a query that's both a Wildcard and a MultiTerm? Thanks! Not in the way you're hoping to achieve it unfortunately: Lucene supports single and multiple character wildcard searches within single terms (but not within phrase queries). and even if it were possible would probably not be a good idea: Wildcard range and fuzzy search queries may match too many terms. It may cause incredible search performance downgrade. I imagine the way to go if you insist on multiple wildcard terms would be two execute two separate searches one for each wildcarded term and bundle the results together.
692,A,"Doctrine Searchable Behavior vs Zend Lucene in symfony I need to search for keywords in 2 table simple thing I am undecided about using one or the other? any suggestion performance wise? Thanks! Lucene has been developed for fulltext search. Do you need this functionality? If you need only keywords functionality use Searchable Searchable is easer to use than Lucene. Also consider to use sfDoctrineActAsTaggablePlugin  As cuhuak mentioned Lucene is a fulltext search. Since it is written in Java you additionaly need a Java Server (Tomcat for example). If you don't want an additional server the Zend Lucene implementation (as noted in the comments by Matt Gibson) may also be worth looking into. With an additional server and if you only have a small host this MAY not be good performance wise. Also Lucene needs much more configuration and set-up than the Doctrine solutions. On the performance side of things: Lucene is built to index millions of words and search within these in milliseconds. With millions of words in your ""regular"" RDBMS things will get slow. You could set up a fulltextsearch in the database too but this again needs setup and knowledge what to do. So a generall thought: if the site is small (in terms of things to searc) go with the Doctrine approach if you plan to get serious amounts of data go with Lucene. @Matt thanks I wasn't fully aware of that. I updaded my anser to include a reference to Zend. Thanks for the info! Note that the [Zend version of Lucene](http://framework.zend.com/manual/en/zend.search.lucene.overview.html) is a full port to PHP5 not just an interface to Java. The current Symfony tutorial sample project Jobeet gives an example of plugging this in to Symfony for a PHP-only fulltext search solution."
693,A,"How to implement our own UID in Lucene? I wish to create an index with lets say the following fields : UID title owner content out of which I don't want UID to be searchable. [ like meta data ] I want the UID to behave like docID so that when I want to delete or update I'll use this. Is this possible ? How to do this ? You could mark is as non-searchable by adding it with Store.YES and Index.NO but that wont allow you easy updating/removal by using it. You'll need to index the field to allow replacing it (using IndexWriter.UpdateDocument(Term Document) where term = new Term(""UID"" ""..."")) so you need to use either Index.ANALYZED with a KeywordAnalyzer or Index.NOT_ANALYZED. You can also use the FieldCache if you have a single-valued field which a primary key usually is. However this makes it searchable. Summary: Store.NO (It can be retrieved using the FieldCache or a TermsEnum) Index.NOT_ANALYZED (The complete value will be indexed as a term including any whitespaces) You need it to be searchable to be able to find it when you need to remove it (update is really remove + insert). You could just have it stored non-searchable but then you need to iterate all documents to find which one has the stored uid you're looking for. so it is either I make it searchable or loose the update/delete functionality right ? No way to get 2 birds with one hit ?"
694,A,"Solr DatImportHandler multiple resuls of the same type? Hey guys some help here would as always be greatly appreciated. I'm indexing data from a db using Solr. Each row in the first table event_titles can have more than one start date associated with it contained in the table event_dates. Data-config is as follows; <entity name=""events"" query=""select idtitle_idnamesummarydescriptiontype from event_titles""> <entity name=""events"" query=""select start from event_dates where title_id = '${events.title_id}'""> </entity> </entity> Using the DIH Develpment Console I can see that it returns each date as it should but it only ever saves the first one for example; <lst name=""entity:event_dates""> <str name=""query""> select start from event_dates where title_id = '38947' </str> <str name=""time-taken"">0:0:0.10</str> <str>----------- row #1-------------</str> <date name=""start"">2010-04-25T23:00:00Z</date> <str>---------------------------------------------</str> <str>----------- row #2-------------</str> <date name=""start"">2010-04-26T23:00:00Z</date> <str>---------------------------------------------</str> <str>----------- row #3-------------</str> <date name=""start"">2010-04-27T23:00:00Z</date> <str>---------------------------------------------</str> </lst> But the result when you run a select is as follows.... ... <arr name=""start""> <date>2010-04-25T23:00:00Z</date> </arr> ... I would have though it would put all the returned dates into the start 'array'? Can anyone shed any light on whether this is even possible? Cheers! Fixed multiValued in schema should be set to true."
695,A,Lucene Indexing I have just started learning Lucene and would like to use it for indexing a table in an existing database. The way I have been thinking about this so far has been to 1. Create a 'Field' for every column in the table 2. Store all the Fields 3. 'ANALYZE' all the Fields except for the Field with the primary key 3. Each row in the table will be stored as a Lucene Document. While most of the columns in this table are small in size one happens to be huge. This column also happens to be the one containing bulk of the data on which searches will be performed. I am aware that Lucene does provide an option to not store a Field. However what would be the best way to go about this Store the field regardless of the size and if a hit is found for a search fetch the appropriate Field from Document OR Don't store the Field and if a hit is found for a search query the data base to get the relevant information out? While writing my question I also realize there may not be a one size fits all answer.. Thanks in advance for your response. For sure your system will be more responsive if you store everything on Lucene. Stored field does not affect the query time it will only make the size of your index bigger. And probably not that bigger if it is only a small portion of the rows that have a lot of data. So if the index size is not an issue for your system I would go with that. +1 for Pascal's response. You could also tokenize the large field and *not store* it. This way you can query(search) on the field get the relevant document/record identifier and retrieve the record from db. Thanks for your responses. If I opt not to store any Field I also would not be able to use Highlighting (Lucene contrib module) to highlight search hits? It could be done without storing the text but it is not the easy way. See http://www.lucidimagination.com/search/document/5ea8054ed8348e6f/highlight_arbitrary_text#60f592f5ff0de0c5 Oups in my previous comment I was referring to Solr. With plain Lucene yes I think you need to have the field stored.  I strongly disagree with a Pascal's answer. Index size can have major impact on search performance. The main reasons are: stored fields increase index size. It could be problem with relatively slow I/O system; stored fields are all loaded when you load Document in memory. This could be good stress for the GC stored fields are likely to impact reader reopen time. The final answer of course it depends. If the original data is already stored somewhere else it's good practice to retrieve it from original data store. Lucene should not be treated as the authoritative source of data. The data should be stored somewhere else anyway so do the keyword based query on a non stored text field then do the look up to get the actual data from the single source of truth.
696,A,"How can I read a Lucene document field tokens after they are analyzed? If I create a document and add a field that is both stored and analyzed how can I then read this field back as a list of tokens? I have the following:  Document doc = new Document(); doc.add(new Field(""url"" fileName Store.YES Index.NOT_ANALYZED)); doc.add(new Field(""text"" fileContent Store.YES Index.ANALYZED)); // add the document to the index writer.addDocument(doc); So the fileContext is a String containing a lot of text. It is analyzed whereby it is tokenized when it is stored in the index. However how can I get these tokens? I can retrieve the document from the index after it is stored and I can read the ""text"" field from the document but this is returned as a string. I would like to get the tokens if possible. My 'writer' is an IndexWriter instance and it uses a StandardAnalyzer. Any pointers would be very much welcomed. Thank you very much what exactly do you mean by ""tokens"" ? Check out document.getField(""name"").tokenStreamValue(). EDIT: Actually this question gives you the full solution using the above TokenStream. Excellent thank you so much for that. Regards Yaaaaayyyy! That post is good and is exactly what I need :D"
697,A,"How to find related items by tags in Lucene.NET My indexed documents have a field containing a pipe-delimited set of ids: a845497737704e8ab439dd410e7f1328| 0a2d7192f75148cca89b6df58fcf2e54| 204fce58c936434598f7bd7eccf11771 (ignore line breaks) This field represents a list of tags. The list may contain 0 to n tag Ids. When users of my site view a particular document I want to display a list of related documents. This list of related document must be determined by tags: Only documents with at least one matching tag should appear in the ""related documents"" list. Document with the most matching tags should appear at the top of the ""related documents"" list. I was thinking of using a WildcardQuery for this but queries starting with '*' are not allowed. Any suggestions? Setting aside for a minute the possible uses of Lucene for this task (which I am not overly familiar with) - consider checking out the LinkDatabase. Sitecore will behind the scenes track all your references to and from items. And since your multiple tags are indeed (I assume) selected from a meta hierarchy of tags represented as Sitecore Items somewhere - the LinkDatabase would be able to tell you all items referencing it. In some sort of pseudo code mockup this would then become  for each ID in tags get all documents referencing this tag for each document found if master-list contains document; increase usage-count else; add document to master list sort master-list by usage-count descending Forgive me that I am not more precise but am unavailable to mock up a fully working example right at this stage. You can find an article about the LinkDatabase here http://larsnielsen.blogspirit.com/tag/XSLT. Be aware that if you're tagging documents using a TreeListEx field there is a known flaw in earlier versions of Sitecore. Documented here: http://www.cassidy.dk/blog/sitecore/2008/12/treelistex-not-registering-links-in.html @Mark Cassidy: I have not used the Link Database before but I am going to try your approach. I wrote up a full article with an implementation of this pseudo-code - if for no other reason than just to assert to myself it could be done the way I envisioned ;-) You can find it here: http://www.cassidy.dk/blog/sitecore/2009/05/listing-related-articles-with-sitecore.html  Try this query on the tag field. +(tag1 OR tag2 OR ... tagN) where tag1 .. tagN are the tags of a document. This query will return documents with at least one tag match. The scoring automatically will take care to bring up the documents with highest number of matches as the final score is sum of individual scores. Also you need to realizes that if you want to find documents similar to tags of Doc1 you will find Doc1 coming at the top of the search results. So handle this case accordingly.  You can have the same field multiple times in a document. In this case you would add multiple ""tag"" fields at index time by splitting on |. Then when you search you just have to search on the ""tag"" field. I didn't know this was possible. Do you know of any SKD page that describes this? I usually use the Java versions documentation ""Several fields may be added with the same name"": http://lucene.apache.org/java/2_4_1/api/org/apache/lucene/document/Document.html#add(org.apache.lucene.document.Fieldable)) @Mike: Thanks I'll look into this.  Your pipe-delimited set of ids should really have been separated into individual fields when the documents were indexed. This way you could simply do a query for the desired tag sorting by relevance descending. That would not be very practical as the list may contain 0 to n Ids @ArnieZ: Please bare with me. What would not be practical? @Adam Paynter: What if there are 20 tags? Would I then have to search against 20 fields? @ArnieZ: I suspect you should be able to construct a BooleanQuery with 20 TermQuery clauses. It would still be one query. @Adam Paynter: The issue then is that some documents may contain 1 tagm some may contain 20 tags. I would have a search that tries to match against 19 tags that dont exist. @ArnieZ: When constructing a BooleanQuery you can specify if the clause (the TermQuery in this case) MUST MUST NOT or SHOULD exist. If you use SHOULD when building your BooleanQuery Lucene will still find the document even if only one of its 20 tags match. It will just score lowered than a document that matches 2 of its 20 tags etc... @Adam Paynter: I will investigate this. Thanks. @ArnieZ: I noticed a comment on Mike's answer. I just wanted to clarify: I meant that the pipe-delimited ids would be separated into multiple fields all having the same field name. @Adam Paynter: Ahh I see. I will try this now. @ArnieZ: :) The BooleanQuery is constructed using one BooleanClause per tag IN THE DOCUMENT THAT THE USER IS VIEWING. @Adam Paynter: Query parser came up with the following query: ""Tags:a845497737704e8ab439dd410e7f1328 Tags:0a2d7192f75148cca89b6df58fcf2e54 Tags:204fce58c936434598f7bd7eccf11771"" @Adam Paynter: The only result i get is the current document itself. Odd... @ArnieZ: Try building a query such as ""Tags:a845497737704e8ab439dd410e7f1328 OR Tags:0a2d7192f75148cca89b6df58fcf2e54 OR Tags:204fce58c936434598f7bd7eccf11771"" @Adam Paynter: It looks like in Lucene.NET my query syntax is equivalent to your query syntax. I will experiment with fewer tags... @Adam Paynter: Sadly this approach does not work for me. I am going to try the Link Database approach. Thanks for the help. @ArnieZ: Did you separate the ids into multiple fields? Where the fields specified as being indexed? @Adam Paynter: Yes and yes."
698,A,Search based on words of a phrase just like SO related questions I'm trying to get search results just like the stack overflow Related questions when you ask a new question. How can I do with lucene a search which would match all words or at least one or two words in my phrase ? There's a Lucene contribution called MoreLikeThis that does what you're looking for. Here's an article on using it: Using Lucene and MoreLikeThis to show related content. Is there an alternative without MoreLikeThis ?
699,A,"instant searching in petabyte of data I need to search over petabyte of data in CSV formate files. After indexing using LUCENE the size of the indexing file is doubler than the original file. Is it possible to reduce the indexed file size??? How to distribute LUCENE index files in HADOOP and how to use in searching environment? or is it necessary should i use solr to distribute the LUCENE index??? My requirement is doing instant search over petabyte of files.... What do you mean by ""instant"" ? If you want to do something in google/yahoo/any search engine style I would suggest you study their architecture before divind into a solution. yep similar google instant search. But the requirement here is searching the csv file based one the query and draw a chart as soon as you changed the query the chart also should change... Any decent off the shelf search engine (like Lucene) should be able to provide search functionality over the size of data you have. You may have to do a bit of work up front to design the indexes and configure how the search works but this is just config. You won't get instant results but you might be able to get very quick results. The speed will probably depend on how you set it up and what kind of hardware you run on. You mention that the indexes are larger than the original data. This is to be expected. Indexing usually includes some form of denormalisation. The size of the indexes is often a trade off with speed; the more ways you slice and dice the data in advance the quicker it is to find references. Lastly you mention distributing the indexes this is almost certainly not something you want to do. The practicalities of distributing many petabytes of data are pretty daunting. What you probably want is to have the indexes sat on a big fat computer somewhere and provide search services on the data (bring the query to the data don't take the data to the query). Thank you Qwerky. One more doubt how to integrate lucene with hadoop? I mean in which part can I use Hadoop in Lucene? since the petabyte of data distributed over Hadoop file system only.... Is it possible to use Map/Reduce in Lucene by integrating with hadoop?  Hadoop and Map Reduce are based on batch processing models. You're not going to get instant response speed out of them that's just not what the tool is designed to do. You might be able to speed up your indexing speed with Hadoop but it isn't going to do what you want for querying. Take a look at Lucandra which is a Cassandra based back end for Lucene. Cassandra is another distributed data store developed at Facebook if I recall designed for faster access time in a more query oriented access model than hadoop.  If you want to avoid changing your implementation you should decompose your lucene index into 10 20 or even more indices and query them in parallel. It worked in my case (I created 8 indices) I had 80 GB of data and I needed implement search which works on a developer machine (Intel Duo Core 3GB RAM)."
700,A,What are the techniques for making recent content relevant in lucene? I am implementing a search where recent content needs to be more relevant than older content. I do not want a filter or range but want to increase the relevancy of the youngest content. What are some solutions that have worked? It can be done with function queries. There's an example in the [Lucene in Action](http://www.manning.com/hatcher2/) book page 187. Don't know where there could be a good example online... If upgrading to Apache Solr is an option for you take a look at this. It unfortunately isn't. Any way to replicate the functionality? @jon077 Possibly [this post](http://stackoverflow.com/questions/4724451/boost-fresh-documents-with-lucene) can help you further Hmmm... so the idea is set a ground-zero date.. then continually increase the boost from that day forward (now-start)=boost.... thus future documents are always more relevant.  You could could add separate fields for month and date and make them part of the query. It would mean explicitly building out a date related section of the query but you could boost year: 2011 and month: 6 over other results. You could explicitly boost however much time is important and use unboosted wildcards for the rest.
701,A,Lucene search where a field MUST start with certain letters I'm trying to search for results within a range e.g. A TO C. However the results are coming in with results that contain letters within the range but I only want results that START with letters within the range. @Akeem: please don't change the whole question invalidating the existing answers. Instead create a new question. The query syntax for RangeQuery is [A TO C]. In this particular case you should receive all results starting with A B and terms containing only one letter C. Alternatively you could try A* OR B* OR C* query Yeah I should have added a comment to clarify that I edited the question to clarify. Apologies could you include sample output? I'd recommend building query object in your code not in Lucene parser to be sure that the error is not in the parser code. Easiest way -- During index time create another field that contains only the first letter. So if the field currently contains: Alpha Beta Charlie then index this in a separate field (non-analyzed): A B C Then use range query as usual myFieldFirstLetter:[A TO C]  I ended up using frange provided by the QParser plugin available for solr 1.4 {!frange l=A u=C}fieldname I got the info from here
702,A,"hit highlighting in lucene i am searching for strings indexed in lucene as documents. now i give it a long string to match. example: ""iamrohitbanga is a stackoverflow user"" search string documents: document 1: field value: rohit document 2: field value: banga now i use fuzzy matching to find the search strings in the documents. the 2 documents match. i want to retrieve the position at which the string rohit occurs in the search string. how to do it using lucene java api. also note that the fuzzy matching would lead to inexact matches also. but i am interested in the position word in the searched string. the answer to http://stackoverflow.com/questions/1311199/finding-the-position-of-search-hits-from-lucene refers to a website which requires us to download some files from http://www.iq-computing.de and this page does not load. so could you provide a solution? Probably this should help: http://lucene.apache.org/java/2_9_1/api/contrib-highlighter/index.html"
703,A,"How to find ""FooBar"" when seaching ""Foo Bar"" in Zend Lucene I'm building a search function for a php website using Zend Lucene and i'm having a problem. My web site is a Shop Director (something like that). For example i have a shop named ""FooBar"" but my visitors seach for ""Foo Bar"" and get zero results. Also if a shop is named ""Foo Bar"" and visitor seaches ""FooBar"" nothing is found. I tried to seach for "" foobar~ "" (fuzzy seach) but did not found articles named ""Foo Bar"" Is there a speciar way to build the index or to make the query? If you don't care about performance use WildcardQuery (performance is significantly worse): new WildcardQuery( new Term( ""propertyName"" ""Foo?Bar"" ) ); For zero or more characters use '*' for zero or one character use '?' If performance is important try using BooleanQuery. if the user searches for ""foobar"" and in the database i have ""foo bar"" there is no way for the script to know where to put that""?"" or ""*""  Option 1: Break the input query string in two parts at various points and search them. eg. In this case query would be (+fo +bar) OR (+foo +bar) OR (+foob +ar) The problem is this tokenization assumes there are two tokens in input query string. Also you may get extra possibly irrelevant results such as results of (+foob +ar) Option 2: Use n-gram tokenization while indexing and querying. While indexing the tokens for ""foo bar"" would be fo oo ba ar. While searching with foobar tokens would be fo oo ob ba ar. Searching with OR as operator will give you the documents with maximum n-gram matches at the top. This can achieved with NGramTokenizer Op. 2 sounds good have any idea how to use n-gram tokenization? thanks  Manually add index entries for most common name confusions. Get your customers to type them in on a special form.  Did you tried ""*foo* AND *bar*"" or ""*foo* OR *bar*""? It works in Ferret and I read it is based on Lucene. it works IF the queri is FOO BAR and in the database i have FOOBAR but if you are seaching for FOOBAR and in the DB you have FOO BAR it doesn't work Right my mistake... I have crazy idea: try to put '*' between every character ""f*o*o*b*a*r"" and set some string length limit (if str_len > 5). Or you can try to put spaces between down and upper cased letters - then you will seperate ""FooBar"" to ""Foo Bar"" - but user needs to put this string in camel case."
704,A,"Solr ShingleFilterFactory auto-suggest I'm using Solr ShingleFilterFactory for auto-suggestion. This is my field configuration : <fieldType name=""textSpellShingle"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.ShingleFilterFactory"" maxShingleSize=""4"" outputUnigrams=""true""/> <filter class=""solr.RemoveDuplicatesTokenFilterFactory""/> </analyzer> </fieldType> And my query : q=rows=0&facet=true&facet.field=spellingShingle&facet.prefix=har I get these answers : harry potter harrison ford etc... The problem is when I search ""pot"" or ""for""... I'm not getting any results. I'd like to suggest ""potter harry"" or ""ford harrison"". How can I do that? I think the results are depending on the content of your field spellingShingle. If both ""harry potter"" and ""potter harry"" are values of the field solr/lucene will return ""potter harry"" by typing ""pot"""
705,A,Having problem with solr query syntax Everyone I have started working with solr PHP from about 2 or 3 days this is working perfect so far but having some issues with the sorting and random records Also im using Lucene syntax Here is the query (cf_title: nokia OR cf_description: nokia) AND (cf_power_ad: 1 OR cf_power_deal: 1) AND cf_city : Lahore AND sort : cf_addeddate desc Can some one please help me with the sorting and random records? Thankyou!!! Sort order should be given not as part of the query but as separate parameter named sort. Please take a look at this: http://lucene.apache.org/solr/tutorial.html#Sorting To get random sort order you should use RndomSortField as described here: http://lucene.apache.org/solr/api/org/apache/solr/schema/RandomSortField.html
706,A,How to find similar/related text with Zend Lucene? Say I need to make searching for related titles just like stackoverflow does before you add your question or digg.com before submitting news. I didn't find a way how to do this with Zend Lucene. There are setSlop method for queries but as I understand it doesn't help. Is there any way to do this kind of searches? I figured that to do related search you should just pass the query string to the $index->find method. It will find not only the exact matches but also similar ones: $index->find('top 10 cars'); result: Top 10 Funniest Cars Top 11 Celebrities Cars Top 6 Barbeque Cars Top 10 Futuristic Concept Cars Top 5 Classic Oldest Cars Ever  The easiest way to do this is to submit the document's text as a query. Take the document's text tokenize it put an OR term in between each token and submit it as a Lucene query. I've done this before and it works reasonably well. Yeah but OR logic wouldn't help. Because it will find all documents that consist of any of these words whereas I need to find relevant documents.
707,A,"Solr Lucene - boost relevancy based on a 0 to 5 star rating? Quite new to Solr 1.4 - seems to be very powerful indeed. However I am stuck when trying to return search results in order of relevancy (score) and rating_value (a 0 to 5 star rating on each result). I've tried ordering search results by ""rating desc score desc"" and while this works it feels a bit basic. I would ultimately like to boost the relevancy of a search result based on how many stars it has been rated as (0 to 5). A 5-star result should therefore give the highest boost. I did try adding 'rating_value:1^0.1 rating_value:2^0.2' etc etc but this seems to massively boost answers that have no keyword match but do have a high star rating. Any help is VERY much appreciated! Thanks Seb If you are using the DISMAX request handler you should also consider boosting using the bq (boost query) field as this boost only affects documents that are already matched by the users query. You would predefine the bq field in solrconfig.xml inside the request handler e.g. <str name=""bq""> rating_value:1^0.1 rating_value:2^0.2 </str>  You are on the right track with adding the ""rating_value"" terms with boost values. However make sure when you are constructing your query that the keyword terms are ""MUST"" terms which will require the doc to contain that term in order for it to be returned. From there you can play with the relative boost values for each term. If the rating boost is too high you can give the keywords more boost and vice-versa. It's important to know that the absolute values of the boost is not comparable across fields i.e. giving keywords a boost of 20 and rating_value a boost of 19 will does not mean that keywords will be boosted more mainly because of length normalization. See Lucene's Similarity for more info. @seb835: a plus indicates that it is required. So yours would be `q=+foo rating_value:1^.01 rating_value:2^.02` Ah yes I see. My problem is that results are being returned that have no query keyword matching; they're being displayed simply because they have a rating. How would I manage your suggestion above of ""MUST"" terms with my query? ""q=foo rating_value:1^0.1 rating_value:2^0.2"" Yes thats the answer. My query is now as follows: q=+(foo bar nar) rating_value:1^0.1 ... Problem solved! Thanks very much everyone."
708,A,Lucene index updation and performance I am working on a job portal site and have been using Lucene for job search functionality. Users will be posting a number jobs on our site on a daily basis.We need to make sure that new job posted is searchable on the site as soon as possible. In this context how do I update Lucene index when a new job is posted or when an existing job is edited? Can lucene index updating and search work in parallel? Alsocan I know any tips/best practices with respect to Lucene indexingoptimizingperformance etc? Appreciate ur help! Thanks! My tip would be that using Lucene directly from the application layer is almost always the wrong approach. Unless an application specifically needs low-level index access I'd always recommend access at a higher level i.e. Solr or Elasticsearch. If your app is successful and you need to scale up managing your own index files will quickly become very time-consuming. I have used Lucene.Net on a web site similar to what you are doing. Yes you can do live indexes updating to keep everything up to date? What platform are you using Lucene on .NET Java? am using .net..any recommendations for indexing and optimization?Thanks @Craig do you have nay code you could show as to how you handle the updates and reads?  Make sure you create a new IndexSearcher as any additions after an IndexSearcher has been created are not visible to that instance. A better approach may be to ReOpen the IndexReader if you want to resuse the same index searcher.  Yes Lucene can search from and write to an index at the same time as long as no more than 1 IndexWriter writes to it. If you want the new records visible ASAP have the IndexWriter call the commit() function often (see IndexWriter's JavaDoc for details). These Wiki pages might also help: ImproveIndexingSpeed ImproveSearchingSpeed
709,A,"Lucene.Net TermQuery wildcard search I have a lucene index I am trying to do a wildcard search. In index i have a character like '234Test2343' I am trying to do the search like %Test%.. My lucene syntax looks like string catalogNumber=""test""; Term searchTerm = new Term(""FIELD"" ""*""+catalogNumber+""*""); Query query = new TermQuery(searchTerm); I don't get the results back. Any thoughts? Thanks You can use a WildCardQuery. A TermQuery looks for the literal asterisk rather than a wild card. Please note that a WildCardQuery's performance is usually very slow probably more so when using two wild cards as you do. Could you please add the search commands to your code snippet above? This way I may be able to answer the number of hits question. looks like it gives me back top 100 results? is there a way to get more than 100 results? @YuvalF the link is dead Edited with a new link."
710,A,Difference between Lucene stemmers: EnglishStemmer PorterStemmer LovinsStemmer Have anybody compared these stemmers from Lucene (package org.tartarus.snowball.ext): EnglishStemmer PorterStemmer LovinsStemmer? What are the strong/weak points of algorithms behind them? When each of them should be used? Or maybe there are some more algorithms available for english words stemming? Thanks. The Lovins stemmer is a very old algorithm that is not of much practical use since the Porter stemmer is much stronger. Based on some quick skimming of the source code it seems PorterStemmer implements Porter's original (1980) algorithm while EnglishStemmer implements his updated version which should be better. A stronger stemming algorithm (actually a lemmatizer) is available in the Stanford NLP tools. A Lucene-Stanford NLP by yours truly bridge is available here (API docs). See also Manning Raghavan & Schütze for general info about stemming and lemmatization.  I've tested the 3 Lucene stemmers available from org.apache.lucene.analysis.en version 4.4.0 which are EnglishMinimalStemFilter KStemFilter and PorterStemFilter in a document classification problem I'm working on. My results corroborate the claims made by the authors of Introduction to Information Retrieval that for small training corpora in document classification settings stemming is harmful and for large corpora stemming makes no difference. For search and indexing stemming can be more useful (see e.g. Jenkins & Smith) but even there the answer to your question depends on the details of what you're doing. There is no free lunch! At the end of the day nothing beats empirical tests of real code on real data. The only way you'll really know which is better is by running the stemmers for yourself in your application.
711,A,"adding a document to a Lucene index causes crash i'm trying to index an mp3 file with only one ID3 frame. using CLucene and TagLib. the following code works fine: ... TagLib::MPEG::File file(""/home/user/Depeche Mode - Personal Jesus.mp3""); if (file.ID3v2Tag()) { TagLib::ID3v2::FrameList frameList = file.ID3v2Tag()->frameList(); lucene::document::Document *document = new lucene::document::Document; TagLib::ID3v2::FrameList::ConstIterator frame = frameList.begin(); std::wstring field_name((*frame)->frameID().begin() (*frame)->frameID().end()); const wchar_t *fieldName = field_name.c_str(); const wchar_t *fieldValue = (*frame)->toString().toWString().c_str(); lucene::document::Field field(fieldName fieldValue true true true false); document->add(field); writer->addDocument(document); } ... but this one makes the application crash: ... TagLib::MPEG::File file(""/home/user/Depeche Mode - Personal Jesus.mp3""); if (file.ID3v2Tag()) { TagLib::ID3v2::FrameList frameList = file.ID3v2Tag()->frameList(); lucene::document::Document *document = new lucene::document::Document; for (TagLib::ID3v2::FrameList::ConstIterator frame = frameList.begin(); frame != frameList.end(); frame++) { std::wstring field_name((*frame)->frameID().begin() (*frame)->frameID().end()); const wchar_t *fieldName = field_name.c_str(); const wchar_t *fieldValue = (*frame)->toString().toWString().c_str(); lucene::document::Field field(fieldName fieldValue true true true false); document->add(field); } writer->addDocument(document); } ... why is that?! Don't you need to construct a new lucene::document::Field per tag you are adding? It seems like you are reusing the same address for this which is problematic. I guess a debugger could tell you more. even if it's problematic the field isn't reused in this case since the body of the for operator is executed once [one ID3 frame]  This is a scope issue - by the time you call writer->addDocument the fields you added to it are freed. Use this code instead: document->add(* new lucene::document::Field(fieldName fieldValue true true true false)); You may want to look at cl_demo and cl_test to see some code samples. it worked [without the first semicolon] but i don't understand why the fields are freed so quickly ;-) thank you anyway! As I said scopes: http://msdn.microsoft.com/en-us/library/b7kfh662%28VS.80%29.aspx. You're welcome."
712,A,"lucene encoding problem in zend framework i use of lucene search indexer . it work nice for english language but i use of persian in my site and it can`t index for this language for example ""سلام"" i use of this code for create document: public function __construct($class $key $title$contents $summary $createdBy $dateCreated) { $this->addField(Zend_Search_Lucene_Field::Keyword('docRef' ""$class:$key"")); $this->addField(Zend_Search_Lucene_Field::UnIndexed('class' $class)); $this->addField(Zend_Search_Lucene_Field::UnIndexed('key' $key)); $this->addField(Zend_Search_Lucene_Field::Keyword('title' $title 'utf-8')); $this->addField(Zend_Search_Lucene_Field::unStored('contents' $contents  'UTF-8')); $this->addField(Zend_Search_Lucene_Field::text('summary' $summary  'UTF-8')); $this->addField(Zend_Search_Lucene_Field::Keyword('dateCreated' $dateCreated)); } Add this (best place bootstrap)  Zend_Search_Lucene_Search_QueryParser::setDefaultEncoding('utf-8'); Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8_CaseInsensitive () );  I was having the same problem reported by @afsane and then I tried the solution provided by @ArneRie. It did solve my problem though after some testing I realized the first line was not needed (at least in my current setup). So the solution that worked for me was to explicitly set the default analyzer before creating my index: Zend_Search_Lucene_Analysis_Analyzer::setDefault( new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8_CaseInsensitive()); $index = Zend_Search_Lucene::create('/path/to/my/index'); I did not need to explicitly set the default analyzer before opening the index for querying though."
713,A,"lucene : how to get a line of occurence of query I have a number of text files. Each text files have data like this : <text> Big data... big data... </text> <text> another big data </text> <text> some other data </text> now I have to write a code with lucene that could retrieve the entire line when a search query matches like if i search for some data the entire third line should be filtered. <text> some other data </text> I've been able to do a little with spanQuery but that returns me only documents and the word positions. how do i get the ""real text"" from the text file ? Kindly give reference materials if available. Please see [this question](http://stackoverflow.com/questions/3652672/displaying-sample-text-from-the-lucene-search-results). I'm not sure what you mean. If it's always enough for you to retrieve only a single line then you may want to create one Document per line instead of per file. Then IndexReader.document will retrieve only the line in question. (Mapping back from lines to files will be more complicated of course.) I did something similar where I indexed email subject headers only. For evaluation I created sixty indexes of a few thousands Documents then ran a few hundred queries per index. That took about half a minute *in total* including loading time for the Java VM and several libraries. Performance depends on a lot of factors of course so YMMV. Awesome Idea.. Also i dont think Mapping back from lines to files will be difficult. I'll have a field in the document which points to the file That way it is made simple.. Please tell me about the performance.. Thanks.."
714,A,"Synonyms using Lucene What is the best way to handle synonyms (phrases) using Lucene? Especially when I need to execute queries like :a OR b OR c NOT d How about adding a new field called ""synonyms"" to each document while indexing? This field's value would have a list of all synonyms. It would be added to a document only when that document has any of the synonyms. I would then execute an ""OR"" search query which would look for search keyword in this field along with other fields. Can this approach work well for any kind of query? FYI The synonyms in my application are totally custom and not from English dictionary...ie. ""Global Leader in Finance"" could also mean ""Top Investment Bank"" or ""Fortune 500 Finance company"" etc etc. Please suggest. Thanks. You can get the Query object after parsing the input query string with QueryParser.parse(). In most of the cases the top-level query is boolean query with sub-queries as its children. You can recursively iterate on the query object. When you hit a TermQuery or PhraseQuery object you can get the (sub)query and replace that query object with a boolean query object consisting of its synoyms if any. Essentially you are transforming your original query a OR b AND c to (a OR synA) OR (b OR synB1 OR synB2) AND c Operating at query object ensure that you simply replace the leaf nodes of the query with new queries and don't fiddle with arbitrarily complex query hierarchy.  There is a contribution to the Lucene project called ""wordnet"". According to its documentation: This package uses synonyms defined by WordNet to build a Lucene index storing them which in turn can be used for query expansion. You normally run Syns2Index once to build the query index/""database"" and then call SynExpand.expand(...) to expand a query. It includes a sample of what it does: If you pass in the query ""big dog"" then it prints out: Query: big adult^0.9 bad^0.9 bighearted^0.9 boastful^0.9 boastfully^0.9 bounteous^0.9 bountiful^0.9 braggy^0.9 crowing^0.9 freehanded^0.9 giving^0.9 grown^0.9 grownup^0.9 handsome^0.9 large^0.9 liberal^0.9 magnanimous^0.9 momentous^0.9 openhanded^0.9 prominent^0.9 swelled^0.9 vainglorious^0.9 vauntingly^0.9 dog andiron^0.9 blackguard^0.9 bounder^0.9 cad^0.9 chase^0.9 click^0.9 detent^0.9 dogtooth^0.9 firedog^0.9 frank^0.9 frankfurter^0.9 frump^0.9 heel^0.9 hotdog^0.9 hound^0.9 pawl^0.9 tag^0.9 tail^0.9 track^0.9 trail^0.9 weenie^0.9 wiener^0.9 wienerwurst^0.9 You see that the original words (""big"" and ""dog"") have no weighting attached to them. The synonyms however have a weighting (0.9) that you can configure yourself. It comes bundled with the standard distribution of Lucene in the ""contrib"" directory. Thanks for ur inputs Adam...Could you please refer to my question again?I've now edited it. The WordNet module builds a Lucene index just like you are. This index that it builds is eventually used to expand queries. If you simply tried building this index from WordNet's dictionary I am sure you could easily tell what field names it is using for its index and add your own custom entries yourself.  I prefer to run a search using the whole phrase entered and weight anything returned heavier than the next series of searches. I then like to iterate through each word in the phrase and search with that with those results getting a lower score. I then aggregate the scores for all items returned more than once and sort the results accordingly. This may not be the 100% best way of doing this...but it has worked great for me in the past."
715,A,"Hibernet Search ShingleAnalyzerWrapper working example I am using hibernate-search-3.2.1.Final and would like to parse my input into shingles. From what i can see in the documentation ShingleAnalyzerWrapper seem to be exactly what I needed. I have tested with both WhitespaceAnalyzer StandardAnalyzer and SnowballAnalyzer as the default analyzer for the ShingleAnalyzerWrapper. Version luceneVersion = Version.LUCENE_29; SnowballAnalyzer keywordAnalyzer= new SnowballAnalyzer(luceneVersion ""English"" StopAnalyzer.ENGLISH_STOP_WORDS_SET); ShingleAnalyzerWrapper shingleAnalyzer = new ShingleAnalyzerWrapper(keywordAnalyzer 4); shingleAnalyzer.setOutputUnigrams(false); QueryParser keywordParser = new QueryParser(luceneVersion ""keyword"" keywordAnalyzer); Query keywordQuery = keywordParser.parse(QueryParser.escape(keyword.toLowerCase())); However the query came back empty. I was expecting keyword like ""hello world this is Lucene"" to result in shingles [hello world this is world this is lucene this is lucene] Let me know if my expectation and usage of ShingleAnalyzerWrapper is correct. Thanks Ryan Thanks Nikita! Yes it was an copy-n-paste error though the correct version still does produce the right results. Your link on AnalyzerUtils was a great help as I was able to use the following code to generate Shingles: ShingleAnalyzerWrapper shingleAnalyzer = new ShingleAnalyzerWrapper(4); shingleAnalyzer.setOutputUnigrams(false); TokenStream stream = shingleAnalyzer.tokenStream(""contents"" new StringReader(""red dress shoes with black laces"")); ArrayList tokenList = new ArrayList(); while (true) { Token token = null; try { token = stream.next(); } catch (IOException e) { e.printStackTrace(); } if (token == null) break; tokenList.add(token); } Which produces: [(red dress09type=shingle) (red dress shoes015type=shingleposIncr=0) (red dress shoes black026type=shingleposIncr=0) (dress shoes415type=shingle) (dress shoes black426type=shingleposIncr=0) (dress shoes black laces432type=shingleposIncr=0) (shoes black1026type=shingle) (shoes black laces1032type=shingleposIncr=0) (black laces2132type=shingle)] The problem was not with the ShingleAnalyzerWrapper itself but the QueryParser. I will need some more digging to figure out what's the underlying cause but you got me some where to start from.  Maybe it's a copy/paste error but in your code snippet the shingleAnalyzer is not actually being used because you're passing the variable keywordAnalyzer to the query parser. What analyzer are you using at indexing time? If you use an analyzer that filters out stop words as the delegate analyzer for ShingleAnalyzerWrapper stop words (""this"" and ""is"" in your example) will be dropped before the shingle analyzer has a chance to create shingles from them. A good way to debug analyzers is to use something like AnalyzerUtils described in ""Lucene in Action"". You can get the sample code here: http://java.codefetch.com/example/in/LuceneInAction/src/lia/analysis/AnalyzerUtils.java Nikita"
716,A,Reducing the memory size of Index for Lucene I use Lucene for searching the HTML documents. The issue I have is on increased size of index files I have abt 300-400MB size of HTML files but the index is running upto .98Gb. The reason I see because of specification we have. Like we index the same contents for four different fields which I guess is the problem ( we use same contents one case sensitive and other otherwise one casesensitive with special characters and other otherwise). Is there a way to reduce the size of index? Keeping the same requirements? Is there a different way we index the same and search differently to support all? I assume your problem is that you are storing these fields instead of just indexing them. So the solution is: don't store them. Ah that's my problem. that was a drastic change in size to abt 200MB.. Thanks a ton Xodarap I should have figured this though..
717,A,"Ruby alternative for Lucene I have heard about Lucene a lot that it's one of the best search engine libraries in Java. Is there any similar (as powerful) library for Ruby? CLucene is a cross-platform C++ port of Lucene. It can be wrapped and used also from every high-level language (there are also a few legacy Swift projects you could start with). See: http://sourceforge.net/projects/clucene http://clucene.git.sourceforge.net/git/gitweb.cgi?p=clucene/clucene;a=summary  Well there's Ferret which is a port of Lucene to Ruby. Also Lucene is very easy to use from JRuby if that's an option for you. Depending on your needs you might also want to take a look at Solr which is a higher-level front-end built on Lucene. There is a Ruby interface solr-ruby that interacts with Solr via HTTP.  Ferret is what you're looking for: ""Ferret is a high-performance full-featured text search engine library written for Ruby. It is inspired by Apache Lucene Java project."" Well you surely can put the high-performance in question. I find it deadly slow. Can you share any performance numbers? Like x seconds to return a search over y documents/rows/whatever? I have no personal experience with Ferret but judging by what I've read and this page: http://ferret.davebalmain.com/trac/wiki/MyFirstBenchmark it used to be slow but is now comparable with Java Lucene. There are performance numbers on that page. YMMV.  unfortunately in most cases ferret is not what you're looking for it's got recurring issues with re-indexing speed index corruption and segfaults on the server. I think most people are going to SOLR sphinx and Xapian. I recall seeing some Tsearch / postgres apps mentioned Tsearch seems to be a industrial-strength solution Take a look here http://stackoverflow.com/questions/1132284/full-text-searching-with-rails  I would try one of them in combination with sphinx. Thinking Sphinx http://freelancing-god.github.com/ts/en/rails3.html Riddle http://riddle.freelancing-gods.com/ http://blog.evanweaver.com/files/doc/fauna/ultrasphinx/files/README.html"
718,A,"Lucene IndexWriter slow to add documents I wrote a small loop which added 10000 documents into the IndexWriter and it took for ever to do it. Is there another way to index large volumes of documents? I ask because when this goes live it has to load in 15000 records. The other question is how do I prevent having to load in all the records again when the web application is restarted? Edit Here is the code i used; for (int t = 0; t < 10000; t++){ doc = new Document(); text = ""Value"" + t.toString(); doc.Add(new Field(""Value"" text Field.Store.YES Field.Index.TOKENIZED)); iwriter.AddDocument(doc); }; Edit 2  Analyzer analyzer = new StandardAnalyzer(); Directory directory = new RAMDirectory(); IndexWriter iwriter = new IndexWriter(directory analyzer true); iwriter.SetMaxFieldLength(25000); then the code to add the documents then;  iwriter.Close(); How long was forever? it took around 2.5 to 3 minutes. Is this to be expected? I should add that the docs contained a single field and that the field had ""value "" + t.toString() as its value. So very small Can we see the code you used to set the index writer up? @chibacity done. Just checking but you haven't got the debugger attached when you're running it have you? This severely affects performance when adding documents. On my machine (Lucene 2.0.0.4): Built with platform target x86: No debugger - 5.2 seconds Debugger attached - 113.8 seconds Built with platform target x64: No debugger - 6.0 seconds Debugger attached - 171.4 seconds Rough example of saving and loading an index to and from a RAMDirectory: const int DocumentCount = 10 * 1000; const string IndexFilePath = @""X:\Temp\tmp.idx""; Analyzer analyzer = new StandardAnalyzer(); Directory ramDirectory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDirectory analyzer true); for (int i = 0; i < DocumentCount; i++) { Document doc = new Document(); string text = ""Value"" + i; doc.Add(new Field(""Value"" text Field.Store.YES Field.Index.TOKENIZED)); indexWriter.AddDocument(doc); } indexWriter.Close(); //Save index FSDirectory fileDirectory = FSDirectory.GetDirectory(IndexFilePath true); IndexWriter fileIndexWriter = new IndexWriter(fileDirectory analyzer true); fileIndexWriter.AddIndexes(new[] { ramDirectory }); fileIndexWriter.Close(); //Load index FSDirectory newFileDirectory = FSDirectory.GetDirectory(IndexFilePath false); Directory newRamDirectory = new RAMDirectory(); IndexWriter newIndexWriter = new IndexWriter(newRamDirectory analyzer true); newIndexWriter.AddIndexes(new[] { newFileDirectory }); Console.WriteLine(""New index writer document count:{0}."" newIndexWriter.DocCount()); thanks for your help. i'm not quite getting the great times you are. mine is at 16secs which is acceptable i think. i think it's down to my hardwear now. +1 thank for this i'll look into this tonight. i suspect it may be the debugger now that you mention it. thanks for your help  You should do this way to get the best performance. on my machine i'm indexing 1000 document in 1 second 1) You should reuse (Document Field) not creating every time you add a document like this private static void IndexingThread(object contextObj) { Range<int> range = (Range<int>)contextObj; Document newDoc = new Document(); newDoc.Add(new Field(""title"" """" Field.Store.NO Field.Index.ANALYZED)); newDoc.Add(new Field(""body"" """" Field.Store.NO Field.Index.ANALYZED)); newDoc.Add(new Field(""newsdate"" """" Field.Store.YES Field.Index.NOT_ANALYZED_NO_NORMS)); newDoc.Add(new Field(""id"" """" Field.Store.YES Field.Index.NOT_ANALYZED_NO_NORMS)); for (int counter = range.Start; counter <= range.End; counter++) { newDoc.GetField(""title"").SetValue(Entities[counter].Title); newDoc.GetField(""body"").SetValue(Entities[counter].Body); newDoc.GetField(""newsdate"").SetValue(Entities[counter].NewsDate); newDoc.GetField(""id"").SetValue(Entities[counter].ID.ToString()); writer.AddDocument(newDoc); } } After that you could use threading and break your large collection into smaller ones and use the above code for each section for example if you have 10000 document you can create 10 Thread using ThreadPool and feed each section to one thread for indexing Then you will gain the best performance. +1 @Ehsan thank you for this. I'll try it out today."
719,A,"SOLR - PathHierarchyTokenizerFactory Facet Query I have been trying to perform a query on a field that is configured to be solr.PathHierarchyTokenizerFactory but the query just returns all records. It seems that doing a facet query is just not working. Does anyone have a way of accomplishing this? I'm using the PathHierarchy to implement category/subcategory facets. <fieldType name=""text_path"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer> <tokenizer class=""solr.PathHierarchyTokenizerFactory"" delimiter=""/"" /> </analyzer> </fieldType> <field name=""libraries"" type=""text_path"" indexed=""true"" stored=""true"" multiValued=""true"" /> And http://linux2:8984/solr/select?q=*:*&rows=0&fq=libraries:""/test/subtest""&facet=true&facet.field=libraries&f.libraries.facet.sort=true&f.libraries.facet.limit=-1&f.libraries.facet.mincount=-1 Thanks What happens if you remove the faceting parameters? Does it return all documents also? From my opinion faceting should not have an effect on the search results. It seems to me that the filter query you passed in the fq parameter is not working for some reason.  Change your text_path field definition to apply the PathHierarchyTokenizerFactory at index time only (example below). Your issue is that your queries are being processed by the tokenizer so that fq=libraries:""/test/subtest"" actually queries against fq=libraries:(/test/subtest OR /test). <fieldType name=""text_path"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.PathHierarchyTokenizerFactory"" delimiter=""/"" /> </analyzer> </fieldType> Note the analyzer type=""Index"" Small correction type=""index"" (lower case index)"
720,A,"What is a good Java Library to use for searching through several files for a list of search terms? Basically what I would like to do is search through a folder its subfolders for a list of search terms. It does not have to be highly optimized or anything like that. I would like the library to be able to ""Match Case"" match ""Whole Words Only"" etc. I think I could write something like this opening each file in a file and searching each word etc but I really want a short-cut. Is there some library that already does most of this? My dream code would be something like: ArrayList occurrences = SomeLibrary.parse(""directoryPath""""searchTerm""); Is there anything close to this high level? Thanks Grae Grae It goes like this: Lucene is a native Java search library. It has a somewhat steep learning curve. Solr is a search engine built using Lucene as a web application. It is much easier to learn and can be used via an HTTP interface or a Java interface called Solrj. If you prefer the minimal Java version you need Lucene. If you want the quickest-to-implement solution use Solr. Here's a Solr tutorial and a Lucene tutorial. Both approaches here require an indexing stage and a later retrieval stage. Your question seems to have a more grep-like flavor but I do not know a matching Java library for this. You also did not describe the file types - bare Lucene works with raw text. You may need Apache Tika to get text and metadata from your files.  I recomend Apache Solr. Easy to configure and it can index millions of documents. Solr make all possible optimizations in index and queries. Many documentation. And better of all is open. Solr is a search app which uses Lucene so I think brent777's later answer is a bit better. Still +1 for being first :) Every time I think I'm going to be first someone squeezes in an answer while I'm typing I've got to learn to type faster :(  I would not recommend using Lucene (or Solr) for these requirements. First of all there is no need for full-featured text search library that (to put it simply) does all kinds of magic to have very robust text search using all linguistic knowledge of stemming grammar and syntax tricks. While Lucene is a powerful you cannot have everything with Lucene with out-of-box functionality. As an example it is relatively easy to configure it to find apples with an ""apple"" term. Okay. But using the same configuration it will not find you ""123"" in ""12345"" string. And forget about ""non-readable"" texts like application logs. Lucene is a 'google' like engine it searches texts for humans from human-readable proper texts. To address all sorts of ""basic"" string matches you will need to write a custom processing code that integrates with Lucene functionality and it is not simple any more. With Java it is much simpler and quicker to write a BufferedReader scanner that recursively processes the files and folders and searches for exact or partial matches using String.match and String.contains operations. Thanks that sounds true. I guess I am surprised no one came up with that as a library.  Have you considered using Lucene? It can index and search through text files for search terms as you require. It is not difficult to integrate into your app either but not quite as simple as ""ArrayList occurrences = SomeLibrary.parse(""directoryPath""""searchTerm"");"" :) I don't think you will find a solution that simple. The performance of the search will also be good if you use Lucene. You could go a step further and use Solr (also an Apache product) but this may be overkill for you. If you decide to look into Lucene then this may be of some assistance to you. I did look at Lucene a bit but honestly I was a little overwhemed. I am looking for a ten minute type solution. However is lucene is the simplest I night have to use it. I like your link though looks pretty straightforward. Lucene is quite simple to use I currently use it in an app that I work on for full-text searching through database entities (I actually use a combination of Lucene and Hibernate Search). You should be able to get it up and running quite quickly and if you need help just let me know."
721,A,"""no inclosing instance error "" while getting top term frequencies for document from Lucene index I am trying to get the most occurring term frequencies for every particular document in Lucene index. I am trying to set the treshold of top occuring terms that I care about maybe 20 However I am getting the ""no inclosing instance of type DisplayTermVectors is accessible"" when calling Comparator... So to this function I pass vector of every document and max top terms i would like to know protected static Collection getTopTerms(TermFreqVector tfv int maxTerms){ String[] terms = tfv.getTerms(); int[] tFreqs = tfv.getTermFrequencies(); List result = new ArrayList(terms.length); for (int i = 0; i < tFreqs.length; i++) { TermFrq tf = new TermFrq(terms[i] tFreqs[i]); result.add(tf); } Collections.sort(result new FreqComparator()); if(maxTerms < result.size()){ result = result.subList(0 maxTerms); } return result; } /*Class for objects to hold the term/freq pairs*/ static class TermFrq{ private String term; private int freq; public TermFrq(String termint freq){ this.term = term; this.freq = freq; } public String getTerm(){ return this.term; } public int getFreq(){ return this.freq; } } /*Comparator to compare the objects by the frequency*/ class FreqComparator implements Comparator{ public int compare(Object pair1 Object pair2){ int f1 = ((TermFrq)pair1).getFreq(); int f2 = ((TermFrq)pair2).getFreq(); if(f1 > f2) return 1; else if(f1 < f2) return -1; else return 0; } } Explanations and corrections i will very much appreciate and also if someone else had experience with term frequency extraction and did it better way I am opened to all suggestions! Please help!!!! Thanx! I think you'd need to make your TermFrq public static class."
722,A,Solr query behaviour I have a Solr installation that contains recipes. Each recipe has multiple ingredients and I'm currently building a recipe search that you can type 'includes/excludes' and then I have a homebrew weight system that comes in after this. The query building is off however and so needs refining. // Works perfect - 109 results ingredients:chicken OR tomatoes OR bacon // Down to 7 results - Definitely wrong ingredients:chicken OR tomatoes OR bacon AND -ingredients:garlic I've tried building this query any which way but can't figure out an acceptable 'fuzzy filter' not so much I dont know how to apply that to my query it seems to be the same issue... have you tried all the suggestions? have you read the solr-user thread? this might help: http://stackoverflow.com/questions/634765/using-or-and-not-in-solr-query Try ingredients:chicken OR tomatoes OR bacon AND (-ingredients:garlic) I am assuming that you are you are using Solr 3.1 with edismax. I have found that enclosing negative queries in parenthesis works. I have not had the time to look into this in more detail and figure out if this is the expected behaviour or is it a bug. If you investigate this in more detail and confirm that this is a bug then please open a Jira issue here. Note that the query I suggested above will search for tomatoes / bacon in the default field(s) as per your config. If you want to search for them in ingredients only then use ingredients:(chicken OR tomatoes OR bacon) AND (-ingredients:garlic)  I would do: ingredients:((chicken OR tomatoes OR bacon) AND NOT garlic) This works for me. You can add all excludes like this: ingredients:((chicken OR tomatoes OR bacon) AND NOT (garlic OR peanuts OR spinach)) This worked perfectly for me. Thank you. You can see the search working over at http://www.guardian.co.uk/lifeandstyle/recipe-search/search
723,A,Can Lucene.NET be used with MVVM? Just a quick pre-emptive question before we start to seriously investigate using Lucene. Currently building a C# WPF application using MVVM and Microsoft Entity framework. My lead has brought up the point that this might represent a problem with us not being able to let Lucene directly access the database and therefore there might be difficulty in getting it to tie in with our BOL/DAL. Anyone have any experience of tying Lucene in with such a model? I have successfully used Lucene in a WPF application using MVVM and I didn't encounter any problems. Lucene should sit in your services layer. In the MVVM world Lucene is your Model. The ViewModel just calls into Lucene with queuries and formats the results in whatever way is needed for the View to present it. In my application I had a service that was responsible for pulling relevant data out of the database and adding it to the Lucene index. Excellent very reassuring to know that someone had used it in exactly the same way we wish too. :)
724,A,"Lucene javaUse span query to find hits per page I am using lucene as Search engine. When we use the standardAnalyserthe stop words are filtered out. That means when we search for a phrase that has stop words we can't find results for exact phrase. Example we search for ""This is a game"" it looks for ""This game"". (I look at it to work this way) I need to filter out stop words if someone is searching just for 'a' or 'the'etc (stop words) but not in phrase searches. EDIT: Looks like it works with QueryParser Object. However I cant get the results for hits per page in single document for phrase searches. I am looking to use SpanQuery for it any ideas on how to use it? Thanks Sharma When you filter stopwords the relative place of the words is not modified. For example ""this is a game"" results in: This at pos 0 game at pos 3 This way when you generate a phrase query (using the usual query parser) with the same stopwords it will work as expected. Some artifacts may appear however. The phrases ""this is a game"" ""this is some game"" will both match equally well for the query ""this is a game"". Regarding the SpanQuery... I don't quite understand the question. I would like to find the number of hits per page using span query object. Is that possible?"
725,A,"Compass Lucene indexing PhantomReadLock error I'm using MySQL to store Compass indexes with Tomcat on RHEL but when I shut down Tomcat and restart my indexing process this error is thrown while indexing and indexing fails: PhantomReadLock unable to obtain lock write.lock/customer-index Maybe because I shut down Tomcat this error occurs in Compass? After much Googling around I found that I need to manually delete the write.lock file but I couldn't find the lock file anywhere in my Tomcat temp folder. I did get a record with write.lock in my customer-index table in MySQL so I deleted that row and Compass started indexing properly and the PhantomReadLock error no longer occurs. I just want to clarify -- is this the correct way to solve this write.lock issue or do I need to do something else to delete the lock while shutting down Tomcat? I've had the same problem using Tomcat and I found this article : http://docs.ngdata.com/daisy-kb/443-daisy.html. What I learned is that if you just kill Tomcat when you shut it down (in eclipse you can do it ""properly"" or not) write.lock remains there whereas if you shut it down using the shutdown.sh/bat script the file is deleted and next restart is all clean. Even though this is an old question I hope it helps anyone finding it by chance."
726,A,"how to give batch of documents to lucene? I have lots of "".txt"" files in a single directory and I wants to give it to lucene for indexing. I read all files of directory and for each file make its document and then use indexwriter.addDocument(Document) to give these files to lucene. Is it possible to make all documents and give all of them to lucene?? I mean does lucene support this feature? no java doc will help you: http://lucene.apache.org/java/3_1_0/api/all/org/apache/lucene/index/IndexWriter.html  No you will have to add each document on its own. Furthermore I recommend using a configurable batch size to load just as many txt files and index them and carry on as long as there are more text files. This way you will not run into memory problems when you have bigger files.  This feature was added in lucene 3.2"
727,A,Does Solr support customize FunctionQuery? solr wiki FunctionQuery page shows solr supports sort by function Sort By Function <!> Solr3.1 It is now possible to sort the results by the output of a function. For instance if an application wanted to sort by distance it could do: http://localhost:8983/solr/select?q=*:*&sort=dist(2 point1 point2) desc but all solr available functions did not suit my case so does solr support customize functionquery? If solr supports please show me some examples. thanks:) Yes it is possible to add own/customized functionQuerys to solr. For that you have to expand the source code. For an example look at this: http://www.supermind.org/blog/756/how-to-write-a-custom-solr-functionquery I will take a look， thanks  There have been some changes in writing custom functions for solr 4. Check out http://www.dynamicalsoftware.com/solr/customize for a summary of what to sub class and why.
728,A,"DynamicFields in Solr In my current project I need to index all e-mails and their attachments from multiple mailboxes. I will use Solr but I don't know what is the best approach to build my index's structure. My first approach was: <fields> <field name=""id"" require=""true""/> <field name=""uid"" require=""true""/> //A lot of other fields <dynamicField name=""attachmentName_*"" require=""false""> <dynamicField name=""attachmentBody_*"" require=""false""> </fields> But now I am not really sure if it is the best structure. I don't think I can search for one term (e.g stackoverflow) and know where the term was (e.g. *attachmentBody_1* or *_2* or *_3* etc) with a single query. Anyone have a better suggestion to my index's structure? I found one possible solution. All I need to do is set attachmentBody as stored. This solution is not good enough because the index's space will dramatically increase but in my case there is no problem cause I will implement highlight feature too and those fields need to be stored.  You can use multiValued fields for attachmentName and attachmentBody. So you would have 2 regular fields instead of dynamic fields. You can then use highlighting to bring back the specific values that match with surrounding context. Another option would be to make each attachment a separate document and store something to identify which email it belongs to. The downside of this approach is that you may need to index any data from the email itself several times. But this is really only a problem if most of the email messages have more than one attachment. That way I would never be able to know in which file was the match. Anyway... to highlighting I need fields with store=""true"" so we are still with space problem. Your second option is a good one and i already thinked on that =)"
729,A,Help to get started with Apache Lucene in Java I am using hibernate and java. I want to search data from a database and from some sites it advice to use Lucene. Can someone explain how to get started with this or point to a good tutorial!! Does it works with database as well?? An official get started guide is always helpful for any project. Also check Hibernate Search if you want full text searching specific to Hibernate.  Here is simple tutorial Lucene tutorial and if you want to go deep I advise you to read Lucene in Action excellent exactly what I would have pointed out. I did the Lucene Tutorial myself to get started roughly a jear ago and it definitly helped a lot. Further Lucene in Action is a great book I can very much recommend it. Furthermore some reading on information retrievel (IR) might be helpfull to understand the basic concepts: http://nlp.stanford.edu/IR-book/information-retrieval-book.html Reading the first 6 chapters will give you quite a solid theoretical foundation  here: http://lucene.apache.org/java/3_0_3/gettingstarted.html While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.  Browse to Hibernate Search which should do everything you thought of (and all which you didn't yet) It is worth mentioning that Hibernate Search is built on top of Lucene. You may also want to check up Solr - http://lucene.apache.org/solr/ an easy entry point to Lucene.
730,A,"how to get all docs with acts_as_solr I'm doing something like this: Item.find_by_solr('name:ab*') and it says it returns 297 results: => #<ActsAsSolr::SearchResults:0xb6516858 @total_pages=1 @solr_data={:docs=>[doc1 doc2 doc3...]} :max_score=>1.6935261 :total_pages=>1 :total=>297} @current_page=1> Item.count_by_solr('name:ab*') also returns 297. Yet when iterate it only shows 10 items: Item.find_by_solr('reference_name:ab*').each do |i| puts i end I tried adding {:per_page=>80} and :limit=>:all but it still shows those 10. Any idea what I'm missing? as @mausch said Solr (and by extension acts_as_solr) defaults to 10 results. You can use the :limit option to increase this but it only takes a Fixnum not the :all symbol. So specify :limit with a Fixnum.  You have to specify the :offset parameter in your query. So to the see the next 10 entries(11th to 20th) of Item you have to do this  Item.find_by_solr('name:ab*' :offset => 10) And to see the next 10 entries you have to again increase the :offset parameter by 10. So the next 10 entries would look like this.  Item.find_by_solr('name:ab*' :offset => 20) 10 entries are fetched because the the default value of the :limit parameter is 10. We can change it to something else if we want to fetch more than 10 entries at a time.  # This query fetches 30 items offset by 30. (assuming more than 30 entries are found by this query) Item.find_by_solr('name:ab*' :limit => 30 :offset => 30)  I have in my sketchy notes that you can modify parser_methods.rb around l. 75 to return only AR ID's not the objects themselves. Worth a try in large datasets.  From the Solr FAQ: How can I get ALL the matching documents back? ... How can I return an unlimited number of rows? This is impractical in most cases. People typically only want to do this when they know they are dealing with an index whose size guarantees the result sets will be always be small enough that they can feasibly be transmitted in a manageable amount -- but if that's the case just specify what you consider a ""manageable amount"" as your rows param and get the best of both worlds (all the results when your assumption is right and a sanity cap on the result size if it turns out your assumptions are wrong) As for specifying a limit with acts_as_solr try something like :limit => 80 Did you read my question? I said I already tried :limit and :per_page and it didn't work. Yes I did. You mentioned :per_page=>80 and :limit=>:all but not :limit => Why should I assume that you tried it?"
731,A,"Zend Lucene - cannot search numbers Using Zend Lucene I cannot search numbers in description fields Added it like this: $doc->addField(Zend_Search_Lucene_Field::Text('description' $current_item['item_short_description'] 'utf-8')); Googling for this showed that applying following code should solve the problem but it did not..: Zend_Search_Lucene_Analysis_Analyzer::setDefault(new Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum_CaseInsensitive()); any thougts? I'm not sure about 'zend' but for deal with number in lucene you need use following technique: To place int to document use following: document.Add(new Field(FIELD_SPEC NumberTools.LongToString(YOUR_INT) Field.Store.YES Field.Index.UN_TOKENIZED)); To locate value use Term: Term(FIELD_SPEC NumberTools.LongToString(YOUR_INT)) How do I use it for description? I want so that numbers inside description are searchable. I don't just have a ""seprate integer"". @Pavel Dubinin - look at my note: ""To locate value use Term:..."" So when you need search you create this as part of BooleanQuery where Term is constructed using NumberTools. Little bit complicated if you use standard qury parser - in this case make preprocess by replacing all numbers in queries by value rendered NumberTools  Did you use that command before or after calling Zend_Search_Lucene::open()? Calling it beforehand definitely works.  You have to set the default analyzer twice: On the indexing process as well as on the searching process. Use the code line from above: Zend_Search_Lucene_Analysis_Analyzer::setDefault(new Zend_Search_Lucene_Analysis_Analyzer_Common_TextNum_CaseInsensitive());"
732,A,"Lucene AddIndexes (merge) - how to avoid duplicates? How do I make sure that when I merge a few temp indexes (that might or might not contain duplicate documents) I end up with one copy in the main index ? Thanks Here's a way: Provided that each document has an id and that duplicate documents have the same id: mark the indexes by I1..Im. for i in 1..m let Ci = all the indexes but Ii for all the documents Dj in Ii let cur_term = ""id:<Dj's id>"" for Ik in Ci Ik.deleteDocuments(cur_term) merge all indexes The gist is: delete all documents having the same id as the current document from the other indexes. After having done this for all indexes merge them. I know this is not elegant but I do not know a better algorithm. Thanks I was kinda hoping to avoid iterating through the entire temp index... What if the ID is same its contents are different. May be one should use some timeframe"
733,A,"How to sort by a field that has an alternative value if null in lucene? I want to sort my lucene(.net) search results by a date field (date1) but if date1 is not set I'd like to use date2. The traditional sort method is to sort by date1 and then sort the values that are the same by date 2. This would mean that whenever I did fall back to date2 these values would be at the top (or bottom) of the result set. I'd like to interleave the date2 values with the date1 values. In other words I want to sort on (date1 != null ? date1 : date2). Is this possible in lucene? I reckon I could do this in the index creation phase (just put the relevant date value in a new field) but I don't have enough control of the indexing process to be able to do this so would like a sorting solution. Any ideas? Thanks Matt Ugh I hate Lucene questions specific to a specific platform like .Net.. I got the same question but in another platform wish the solutions were platform independent :( I have an idea that may work: use Search(Query Filter Sort) In order to create the Sort object use the Sort(SortField[]) constructor Create a SortField for both date fields. for each of them use a ScoreDocComparator to handle the case of null values. In this case the Compare() function will return zero. Please see this blog post about using custom sort in Java Lucene. I believe it is not hard to translate this into Lucene.net.  Turns out it's very easy. You do have to implement ScoreDocComparator as Yuval suggested. However you only need to implement it once (I have a document with two dates I don't want to sort by date1 then date2 but rather by date1 if it's specified or date2 if not. Think of an actual date and a provisional date. I want to use the actual date if it's available but if not then the provisional date is good enough). Here's my code: public class ActualOrProvisionalDateSortComparator : ScoreDocComparator { private readonly StringIndex actualDates; private readonly StringIndex provisionalDates; public TxOrCreatedDateSortComparator(IndexReader reader FieldCache fieldCache) { actualDates = fieldCache.GetStringIndex(reader ""actualDate""); provisionalDates = fieldCache.GetStringIndex(reader ""provisionalDate""); } public int Compare(ScoreDoc i ScoreDoc j) { var date1 = GetValue(i.doc); var date2 = GetValue(j.doc); return date1.CompareTo(date2); } public IComparable SortValue(ScoreDoc i) { return GetValue(i.doc); } public int SortType() { return SortField.CUSTOM; } private string GetValue(int doc) { return actualDates.Lookup[actualDates.Order[doc]] ?? provisionalDates.Lookup[provisionalDates.Order[doc]]; } } My ActualOrProvisionalDateSortComparatorSource passes in FieldCache_Fields.DEFAULT and we're away!"
734,A,"Lucene indexing: Store and indexing modes explained I think I'm still not understanding the lucene indexing options. The following options are Store.Yes Store.No and Index.Tokenized Index.Un_Tokenized Index.No Index.No_Norms I don't really understand the store option. Why would you ever want to NOT store your field? Tokenizing is splitting up the content and removing the noise words/separators (like ""and"" ""or"" etc) I don't have a clue what norms could be. How are tokenized values stored? What happens if i store a value ""my string"" in ""fieldName""? Why doesn't a query fieldName:my string return anything? In case any Java users stumble upon this the same options in the March 2009 answer still exist in the Lucene 4.6.0 Java library but are deprecated. The current way to set these options is via FieldType.  Store.Yes Means that the value of the field will be stored in the index Store.No Means that the value of the field will NOT be stored in the index Store.Yes/No does not affect the indexing or searching with lucene. It just tells lucene if you want it to act as a datastore for the values in the field. If you use Store.Yes then when you search the value of that field will be included in your search result Documents. If you're storing your data in a database and only using the Lucene index for searching then you can get away with Store.No on all of your fields. However if you're using the index as storage as well then you'll want Store.Yes. Index.Tokenized Means that the field will be tokenized when it's indexed (you got that one). This is useful for long fields with multiple words. Index.Un_Tokenized Means that the field will not be analyzed and will be stored as a single value. This is useful for keyword/single-word and some short multi-word fields. Index.No Exactly what it says. The field will not be indexed and therefore unsearchable. However you can use Index.No along with Store.Yes to store a value that you don't want to be searchable. Index.No_Norms Same as Index.Un_Tokenized except for that a few bytes will be saved by not storing some Normalization data. This data is what is used for boosting and field-length normalization. For further reading the lucene javadocs are priceless (current API version 4.4.0): Field.Index Field.Store For your last question about why your query's not returning anything without knowing anymore about how you're indexing that field I'd say that it's because your fieldName qualifier is only attached to the 'my' string. To do the search for the phrase ""my string"" you want: fieldName:""my string"" A search for both the words ""my"" and ""string"" in the fieldName field: fieldName:(my string) Thanks that clears up a thing or two. Still not sure what I'm doing wrong with my indexing/searching though. But now I got a better view at what I'm doing. Well I (and the OP if I'm not mistaken) have been using Lucene.Net which is quite a bit behind. I don't recall which version the port is equivalent to at this point but those are the values that it has available. Are you using 2.4.1? Because those Field.Index values have been deprecated in favor of new names which are a bit clearer IMO. See http://lucene.apache.org/java/2_4_1/api/org/apache/lucene/document/Field.Index.html. As far as I know the Lucene.net version numbers match the Lucene version they're ported from With lucene 2.9.1 INDEX.TOKENIZED is deprecated. The documentation says it is just renamed to ANALYZER but I don't think the meaning has stayed the same. Anyone know any more details about INDEX.ANALYZER? The Field.Index.ANALYZED does tokenization which is the reason Field.Index.TOKENIZED now refers to it."
735,A,AnnotationSessionFactoryBean requires lucene classes. wtf? I am trying to add transaction support to an existing webapp via spring transactions. i recently changed my session factory class from LocalSessionFactoryBean to AnnotationSessionFactoryBean. now i get the following error when the webapp starts: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'txManager' defined in class path resource [context.xml]: Cannot resolve reference to bean 'sessionFactory' while setting bean property 'sessionFactory'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sessionFactory' defined in class path resource [context.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/lucene/analysis/standard/StandardAnalyzer at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:275) at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1245) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1010) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:472) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:409) at java.security.AccessController.doPrivileged(Native Method) so the root cause it nested exception is java.lang.NoClassDefFoundError: org/apache/lucene/analysis/standard/StandardAnalyzer i dont understand why now i need to include lucene in my webapp. i dont plan to use it for search and it seems very wrong to me that this error occurs. Can we see the stack trace Without seeing the full strack trace I can't be sure but my guess is that this has something to do with Hibernate Search which uses Lucene under the hood. The AnnotationSessionFactoryBean will attempt to auto-detect the presense of Hibernate Search on the classpath and will initialize it if it's found. Dependening on your environment it may get so far before failing to find Lucene. Have a grub around on your classpath see if Hibernate Search is there and see if you can remove it. Another option is to set the hibernate.search.autoregister_listeners hibernate property to false which should explicitly disable the registration of Hibernate Search.
736,A,Lucene problem with AND/OR Is there anyway I can guarantee that every document with all query terms always scores higher than documents with lesser query terms? Note that I don't want to stick with AND semantics. I still want to show results if there isn't any document that match all query terms. Doh. Comment retracted. Well you might be able to borrow the basic concept of DisMax... it's built on top of Lucene. @Frank I'm not using Solr... one (safe fast) thing you can try is to subclass DefaultSimilarity and adjust the computation of the coordination factor. The default computation is a basic fraction (so e.g. a document that only matches 2 out of 3 terms still gets 2/3 of the coordination factor as one that matches all 3). If this factor (matching all of the query terms) is important to you then I suggest you explicitly boost documents that match all of the query terms even more below is an example that cuts the score in half again for any document that doesn't match all the query terms. For example:  @Override public float coord(int overlap int maxOverlap) { return (overlap == maxOverlap) ? 1f : 0.5f * super.coord(overlap maxOverlap); } This factor is described in more detail here: Lucene Similarity javadocs
737,A,"Iterate through all undeleted Documents in a Lucene (.Net) index I want to get the count of all un-deleted documents of a Lucene (.Net 2.4) index and then read my stored fields of all or a range of these docs. After reading the Lucene help I'm not quite sure whether IndexReader.NumDocs() returns the count of all docs or only the undeleted ones. Can I simply iterate through IndexReader.Document[] and or does it contain deleted Documents? If NumDocs() and Docmuent[] does contain both deleted und undeleted docs I suppose I'll have to do something like this: int totalCount = reader.NumDocs(); int totalCountUndeleted = totalCount; for (int iDoc = 0; iDoc < totalCount; iDoc++) if (reader.IsDeleted(iDoc)) totalCountUndeleted--; for (int iDoc = 0; iDoc < totalCount; iDoc++) { if (!reader.IsDeleted(iDoc)) { Document doc = reader.Document(iDoc); // read fields } } Is this the right way or is there any other possible way? Thanks I found reader.NumDeletedDocs(). So getting totalCount is easier: int totalCount = reader.NumDocs() - reader.NumDeletedDocs(); IndexReader.NumDocs will give you number of active documents. IndexReader.MaxDoc is the number one greater than maximum document number in the index. Following code will read all the active documents in the index. int max = reader.MaxDoc(); for (int iDoc = 0; iDoc < max; iDoc++) { if (!reader.IsDeleted(iDoc)) { Document doc = reader.Document(iDoc); // read fields } }  This is the correct way. Until you optimize your index the documents will not be removed. Alternatively if you have a query like *:* which matches all document you can run that instead. The query method will probably be somewhat slower but maybe more standard. FYI: I compared both ways and the search with `*:*` was around 5 times faster than the iterating. (And to answer my own question: The formatting in comments can be done with ""`"") Thanks for your answer. I tried a normal search with "":"" (colon) as query (Query indexQuery = parser.Parse("":""); ) but it didn't work for me. (ParseException Cannot parse ':': Encountered """":"" "": """" at line 1 column 0. Was expecting one of: ... ... ""+"" ... ""-"" ... "":"" ... ""^"" ... ... ... ... ... ""["" ... ""{"" ... The Lucene docu states you can use a colon to specify the field to search in (""FIELDNAME:searchstring"") @gumo: sorry SO considers `*` a keyword. Can you true `*:*`? Thanks a lot. The search with (how to do the formatting in comments?) works for me thanks. I think that's the best way for me. @gumo: cool thanks for the info. I wouldn't have expected the query to be any faster much less 5 times faster. Good to know!"
738,A,"Solr sort by min of two fields? I want to sort a result set by the minimum of several fields. So after reading the functionquery documentation this is what I came up with: sort={!func}min(dvd_available_from_tdtdto_available_from_tdt)%20desc I also tried: sort=_val_:min(dvd_available_from_tdtdto_available_from_tdt)%20desc sort=_val_:""min(dvd_available_from_tdtdto_available_from_tdt)""%20desc sort=_val_:""min(dvd_available_from_tdtdto_available_from_tdt)%20desc"" sort=""{!func}min(dvd_available_from_tdtdto_available_from_tdt)""%20desc sort={!func}min(dvd_available_from_tdtdto_available_from_tdt)%20desc sort=""min(dvd_available_from_tdtdto_available_from_tdt)""%20desc and also some other placements of the quotes. But no matter what I always get this error: HTTP ERROR: 400 Missing sort order. Can anyobody point me in the right direction? Try using a query that matches all documents with a constant score plus a function. http://localhost:8983/solr/select/?q=%3A+_val_:price&version=2.2&start=0&rows=10&indent=on&debugQuery=true Also upgrading to Solr 3.3 is not that painful and there's all sorts of cool new toys like sorting by function. nice idea! my problem is that the there are numerous factors taht can be applied and the query parser is dismax. i already wrote an update shell script that i will run tomorrow. am i right in the assumption that i can just copy the data directory and keep the index? are they fully compatible? my tests on a developement system did not show any errors so far.  It seems to be available only in solr 3.1. I am running 1.4.1 http://wiki.apache.org/solr/FunctionQuery#Sort_By_Function"
739,A,Neo4j REST API - Index Node with multiple Properties I have multiple Nodes with a title a subtitle and a text property. Now when I index them (index/node/pages/title/foo) I can only search for the title property. How do I index the other two properties? You can add the subtitle just the same way (index/node/pages/subtitle/bar) but for the text property you might want a fulltext index. Documentation on indexing is here and how to create a fulltext index is there. At some point batch operations should make sense but this feature is considered experimental so far. Now it works thanks for the help!
740,A,"How to sort search results on multiple fields using a weighting function? I have a Lucene index where every document has several fields which contain numeric values. Now I would like to sort the search result on a weighted sum of this field. For example: field1=100 field2=002 field3=014 And the weighting function looks like: f(d) = field1 * 0.5 + field2 * 1.4 + field3 * 1.8 The results should be ordered by f(d) where d represents the document. The sorting function should be non-static and could differ from search to search because the constant factors are influenced by the user who performs the search. Has anyone an idea how to solve this or maybe an idea how to accomplish this goal in another way? Are you building the Query object yourself or is it being generated via a QueryParser? The Query is build manually not with the query parser. You could try implementing a custom ScoreDocComparator. For example: public class ScaledScoreDocComparator implements ScoreDocComparator { private int[][] values; private float[] scalars; public ScaledScoreDocComparator(IndexReader reader String[] fields float[] scalars) throws IOException { this.scalars = scalars; this.values = new int[fields.length][]; for (int i = 0; i < values.length; i++) { this.values[i] = FieldCache.DEFAULT.getInts(reader fields[i]); } } protected float score(ScoreDoc scoreDoc) { int doc = scoreDoc.doc; float score = 0; for (int i = 0; i < values.length; i++) { int value = values[i][doc]; float scalar = scalars[i]; score += (value * scalar); } return score; } @Override public int compare(ScoreDoc i ScoreDoc j) { float iScore = score(i); float jScore = score(j); return Float.compare(iScore jScore); } @Override public int sortType() { return SortField.CUSTOM; } @Override public Comparable<?> sortValue(ScoreDoc i) { float score = score(i); return Float.valueOf(score); } } Here is an example of ScaledScoreDocComparator in action. I believe it works in my test but I encourage you to prove it against your data. final String[] fields = new String[]{ ""field1"" ""field2"" ""field3"" }; final float[] scalars = new float[]{ 0.5f 1.4f 1.8f }; Sort sort = new Sort( new SortField( """" new SortComparatorSource() { public ScoreDocComparator newComparator(IndexReader reader String fieldName) throws IOException { return new ScaledScoreDocComparator(reader fields scalars); } } ) ); IndexSearcher indexSearcher = ...; Query query = ...; Filter filter = ...; // can be null int nDocs = 100; TopFieldDocs topFieldDocs = indexSearcher.search(query filter nDocs sort); ScoreDoc[] scoreDocs = topFieldDocs.scoreDocs; Bonus! It appears that the Lucene developers are deprecating the ScoreDocComparator interface (it's currently deprecated in the Subversion repository). Here is an example of the ScaledScoreDocComparator modified to adhere to ScoreDocComparator's successor FieldComparator: public class ScaledComparator extends FieldComparator { private String[] fields; private float[] scalars; private int[][] slotValues; private int[][] currentReaderValues; private int bottomSlot; public ScaledComparator(int numHits String[] fields float[] scalars) { this.fields = fields; this.scalars = scalars; this.slotValues = new int[this.fields.length][]; for (int fieldIndex = 0; fieldIndex < this.fields.length; fieldIndex++) { this.slotValues[fieldIndex] = new int[numHits]; } this.currentReaderValues = new int[this.fields.length][]; } protected float score(int[][] values int secondaryIndex) { float score = 0; for (int fieldIndex = 0; fieldIndex < fields.length; fieldIndex++) { int value = values[fieldIndex][secondaryIndex]; float scalar = scalars[fieldIndex]; score += (value * scalar); } return score; } protected float scoreSlot(int slot) { return score(slotValues slot); } protected float scoreDoc(int doc) { return score(currentReaderValues doc); } @Override public int compare(int slot1 int slot2) { float score1 = scoreSlot(slot1); float score2 = scoreSlot(slot2); return Float.compare(score1 score2); } @Override public int compareBottom(int doc) throws IOException { float bottomScore = scoreSlot(bottomSlot); float docScore = scoreDoc(doc); return Float.compare(bottomScore docScore); } @Override public void copy(int slot int doc) throws IOException { for (int fieldIndex = 0; fieldIndex < fields.length; fieldIndex++) { slotValues[fieldIndex][slot] = currentReaderValues[fieldIndex][doc]; } } @Override public void setBottom(int slot) { bottomSlot = slot; } @Override public void setNextReader(IndexReader reader int docBase int numSlotsFull) throws IOException { for (int fieldIndex = 0; fieldIndex < fields.length; fieldIndex++) { String field = fields[fieldIndex]; currentReaderValues[fieldIndex] = FieldCache.DEFAULT.getInts(reader field); } } @Override public int sortType() { return SortField.CUSTOM; } @Override public Comparable<?> value(int slot) { float score = scoreSlot(slot); return Float.valueOf(score); } } Using this new class is very similar to the original except that the definition of the sort object is a bit different: final String[] fields = new String[]{ ""field1"" ""field2"" ""field3"" }; final float[] scalars = new float[]{ 0.5f 1.4f 1.8f }; Sort sort = new Sort( new SortField( """" new FieldComparatorSource() { public FieldComparator newComparator(String fieldname int numHits int sortPos boolean reversed) throws IOException { return new ScaledComparator(numHits fields scalars); } } ) ); Adam can you post an update version that is compatible with lucene 4.7? in the latest release setNextReader takes AtomicReaderContext and returns FieldComparator. appreciate the help  I'm thinking one way to do this would be to accept these as parameters to your sorting function: number of fields array of documents list of weight factors(based on the number of fields) Calculate the weighing function for each document storing the result in a separate array in the same order as the document array. Then perform any sort you wish (quick sort would probably be best) making sure you are sorting not just the f(d) array but the document array as well. Return the sorted documents array and you're done.  Implement your own similarity class and override idf(Term Searcher) method. In this method you can return the score as follows. if (term.field.equals(""field1"") {  if (term.field.equals(""field1"") { score = 0.5 * Integer.parseInt(term.text()); } else if (term.field.equals(""field2"") { score = 1.4 * Integer.parseInt(term.text()); } // and so on return score; When you execute the query make sure it is on all the fields. That is query should look like field1:term field2:term field3:term The final score will also add some weights based on the query normalization. But that will not affect the relative ranking of the documents as per the equation given by you.  Create a wrapper which holds the rating and is comparable. Something like: public void sort(Datum[] data) { Rating[] ratings = new Rating[data.length]; for(int i=0;i<data.length;i++) rating[i] = new Rating(data[i]); Arrays.sort(rating); for(int i=0;i<data.length;i++) data[i] = rating[i].datum; } class Rating implements Comparable<Datum> { final double rating; final Datum datum; public Rating(Datum datum) { this.datum = datum; rating = datum.field1 * 0.5 + datum.field2 * 1.4 + datum.field3 * 1.8 } public int compareTo(Datum d) { return Double.compare(rating d.rating); } }"
741,A,"Using Solr for indexing multiple languages We're setting up a Solr to index documents where title field can be in various languages. After googling I found two options: Define different schema fields for every language i.e. title_en title_fr... applying different filters to each language then query one of title fields with a corresponding language. Creating different Solr cores to handle each language and make our app query correct Solr core. Which one is better? What are the ups and downs? Thanks It all depends on your requirements. I am assuming you dont need to query multiple languages in a single query. In that case splitting them into multiple cores would be a better idea since you can tweak around that core without affecting the other cores & index. With multiple languages there will be some tweaking or the other involved due to stemming spell check & other features (if you plan to use them). There is also an option of multiple solr webapps within a servlet container. So that could be an option you can look at. It all depends on the flexibility that you had with regards to downtime that you could take to fix any issues.  There's also a third alternative where you use a common set of fields for all languages but apply a filter to a field language. For instance if you have the fields text language you can put text contents for all languages in to the text field and use e.g. fq=language:english to only retrieve english documents. The downside of this approach is that you cannot use language specific features such as lemmatisation stemming etc. Define different schema fields for every language i.e. title_en title_fr... applying different filters to each language then query one of title fields with a corresponding language. This approach gives good flexibility but beware of high memory consumption and complexity when many languages are present. This can be mitigated using multiple solr servers. Creating different Solr cores to handle each language and make our app query correct Solr core. Definately a nice solution. But whether the separate administration and slight overhead will work for you is probably in relation to the number of languages you wish to use. Unless the first approach is applicable I would probably lean towards the second one unless the scalability of cores isn't desired. Either approach is fine though and I think it basicaly comes down to preference. Thanks for the info What if my index contains 200K records for 4 languages? Is this good candidate for your second approach (using fields for every language i.e. title_en title_fr ...) ? Thanks! 200k isn't all that much hence using separate fields seems fine Thanks for the comment!  If you use multiple cores and you need sharding one of the issue I can see is: you will need to do sharding on each language (core). You won't be able to do sharding on the whole index at once. If you use a single core maybe you lose space with text columns that are ""not full"" not sure about that."
742,A,"""Did you mean?"" feature in Lucene.net Can someone please let me know how do I implement ""Did you mean"" feature in Lucene.net? Thanks! Se this article on java.net is solves pretty much what you are after  Google's ""Did you mean?"" is (probably; they're secretive of course) implemented by consulting their query log. Look to see if people who searched for the query you're processing searched for something very similar soon after; if so it indicates they made a mistake and realized what they ought to be searching for. Since you probably don't have a huge query log you could approximate it. Take the query split up the terms see if there are any similar terms in the database (by edit distance whatever); replace your terms with those nearby terms and rerun the query. If you get more hits that was probably a better query. Suggest it to the user. (And since you've already got the hits and most people only look at the top 2 results show them those.) There's a simple explanation of what ""Did you mean"" does here http://norvig.com/spell-correct.html it's a very interesting read.  AFAIK Lucene supports proximity-search meaning that if you use something like: field:stirng~0.5 (it s a tilde-sign) will match ""string"". the float is how ""tolerant"" the search would be where 1.0 is exact match and 0.0 is match everything (sort of). Different parsers will however implement this differently. A proximity-search is much slower than a fuzzy-search (stri*) so use it with caution. In your case one would assume that if you find no matches on a regular search you try a proximity-search to see what you find and present ""did you mean"" based on the result somehow. Might be useful to cache this sort of lookups for very common mispellings for performance reasons.  Pasting this here because the links I posted in my answer no longer exist. http://web.archive.org/web/20090428031018/http://www.devatwork.nl/index.php/articles/lucenenet/introduction-to-lucenenet-lucenenet/ http://web.archive.org/web/20090402104554/http://www.devatwork.nl/index.php/articles/lucenenet/indexing-basics-lucenenet/  Take a look at google code project called semanticvectors. There's a decent amount of discussion on the Lucene mailing lists for doing functionality like what you're after using it - however it is written in java. You will probably have to parse and use some machine learning algorithms on your search logs to build a feature like this!  You should look into the SpellChecker module in the contrib dir. It's a port of Java lucene's SpellChecker module so its documentation should be helpful. (From the javadocs:) Example Usage: SpellChecker spellchecker = new SpellChecker(spellIndexDirectory); // To index a field of a user index: spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader a_field)); // To index a file containing words: spellchecker.indexDictionary(new PlainTextDictionary(new File(""myfile.txt""))); String[] suggestions = spellchecker.suggestSimilar(""misspelt"" 5); this is the right answer should be accepted! just what i was looking for ;) The SpellChecker module moved: https://svn.apache.org/repos/asf/lucene/lucene.net/trunk/C%23/contrib/SpellChecker.Net/"
743,A,"Reading document in Lucene My indexed document in Lucene has got multiple cities assigned to it...ie. doc.Add(new Field(""city"" city1.Trim() Field.Store.YES Field.Index.TOKENIZED)); doc.Add(new Field(""city"" city2.Trim() Field.Store.YES Field.Index.TOKENIZED)); etc how do i iterate thru them and read the values after executing the Lucene search query? Thanks. Call Document.getValues for stored fields with multiple values."
744,A,"Structured and Unstructured indexing - Lucene and Hbase I have a set of 200M documents I need to index. Every document has a free text and additional set of sparse metadata information (100+ columns). It seems that the right tool for free text indexing is Lucene while the right tool for structured sparse metadata is HBase. I would need to query the data and join between free text search results and the structured data results (e.g. get all books that has the phrase ""good morning"" in their textand were first published in 1980). What tools/mechanism should I look at to join structured and unstrcutured queries? Results may include millions of records (before and after the join) Thanks Saar While the hybrid approach may be better I believe it is possible for solr to have additional 'columns' for the document metadata and may save you headaches when you try to integrate querying and displaying the results from two sources. You will have to modify your crawler cfg to present all of the info combined and of course delete and reindex. Sorry don't have time to be more specific. good luck. A couple of things come to mind in addition to lucene on hbase: 1) Solr/Lucene can store multiple fields and each field can have different types. So your date range example is plausible wholly within Solr. 2) If you are talking about truly huge data sets that require a cluster also look at ElasticSearch: http://www.elasticsearch.org/ 3) Lily attempts to answer your exact question http://www.lilyproject.org/lily/index.html  Looks like HBase would like some Lucene action as well: https://issues.apache.org/jira/browse/HBASE-3529. from the comments this looks to be working just not included in standard Hbase atm."
745,A,"SOLR - how to do a fuzzy search on booleans If my index contains three boolean fields: a b and c... I would like to search for: ""a=True b=False c=True"" and SOLR should return all entries and their score should represent how good the whole query is matched. e.g. a=T b=F c=T score=1.0 a=T b=T c=T score=0.6 a=T b=T c=F score=0.5 is that possible ? Assuming true=1 false=0 a couple of ideas: Build every combination with its corresponding boost in the client e.g.: (a:1 AND b:0 AND c:1) OR (a:1 OR b:1 OR c:1)^0.6 OR... Use the dist function query e.g.: dist(1 abc 101) (requires Solr 1.5+) (I haven't used this you might have to multiply this by -1) Option 1 would be great but unfortunately I have over 90 of these boolean fields... But option 2 seems to be a real good choice (I'm using sqedist which doesn't take the square root and should suffice because I'm only using 1's and 0's)..."
746,A,Committed changes visibility in Lucene. Best practices I'm using Lucene 3.0.3. I've made a Spring bean aiming to encapsulate all operations on the same index. public class IndexOperations { private IndexWriter writer; private IndexReader reader; private IndexSearcher searcher; public void init() {...} public void destroy() {...} public void save(Document d) {...} public void delete(Document d) {...} public List<Document> list() {...} } In order to permit fast changes and searches I thought leaving writer reader and searcher open could be a good idea. But the problem is that commited changes on writer can't be seen by readers until reopen. And this operation can be costly so maybe is not a good idea for fast searches. What would be the best approach for this typical scenario? You should keep the writer always open but don't persist the reader/searcher. When you need a searcher just do IndexSearcher srch = new IndexSearcher(writer.getReader()); This way the searcher will get the most recent changes even if they aren't flushed to disk yet (giving you the best of both worlds). @Sinuhe: No this is the recommended way to do it. The writer will flush to disk as is necessary. @Xoradap: Good idea I didn't think about it. But wouldn't this be very costly when the index is big? @Xoradap: after several tests your method is great! Thanks. I would recommend to use `IndexReader#close()` and `IndexSearcher#close()` afterwards. But you may get to see uncommited changes with this method too.  For this sort of use-case I can highly recommend Compass which is a higher-level abstraction around Lucene. Specific to your question it provides better concurrent control plus transactions which obviates the need to manually control the reader/writer/searcher problem that you have. It's pretty clever stuff and to be honest it can be pretty baroque but it's a good solution to the problem. On the downside it's based on Lucene 2.9 rather than 3.0 so it's not as fast as 3.0 can be and it's no longer actively maintained but it's stable fairly well-documented and much easier to use than raw Lucene. I've read about Compass and Solr. I think they are not necessary for this very simple use cases and probably they're opening and closing these objects the same way I will do. About Compass I'm a little discouraged because of the reasons you say but maybe I'll give another try. Thanks. @Sinuhe: Sure it's not necessary. It'll make your job easier though. just a side note: elasticsearch is compass 3.0 :) try it. it uses a more recent lucene than solr does etc
747,A,"Remove results below a certain score threshold in Solr/Lucene? Is there a built-in functionalities in solr/lucene to filter the results if they fall below a certain score threshold? Let's say if I provide a score threshold of .2 then all documents with score less than .2 will be removed from my results. My intuition is that this is possible by updating/customizing solr or lucene. Could you point me to right direction on how to do this? Thanks in advance! Simon's answer is correct. But bear in mind that socres are relative and hence it's hard to pick a threshold for ""goodness"" of results. It's called normalized score (Scores As Percentages). You can use the following the following parameters to achieve that: ns = {!func}product(scale(product(query({!type=edismax v=$q})1)01)100) fq = {!frange l=20}$ns Where 20 is your 20% threshold. See also: how do I normalise a solr/lucene score? http://article.gmane.org/gmane.comp.jakarta.lucene.user/12076 http://article.gmane.org/gmane.comp.jakarta.lucene.user/10810  You could write your own Collector that would ignore collecting those documents that the scorer places below your threshold. Below is a simple example of this using Lucene.Net 2.9.1.2 and C#. You'll need to modify the example if you want to keep the calculated score. using System; using System.Collections.Generic; using Lucene.Net.Index; using Lucene.Net.Search; public class ScoreLimitingCollector : Collector { private readonly Single _lowerInclusiveScore; private readonly List<Int32> _docIds = new List<Int32>(); private Scorer _scorer; private Int32 _docBase; public IEnumerable<Int32> DocumentIds { get { return _docIds; } } public ScoreLimitingCollector(Single lowerInclusiveScore) { _lowerInclusiveScore = lowerInclusiveScore; } public override void SetScorer(Scorer scorer) { _scorer = scorer; } public override void Collect(Int32 doc) { var score = _scorer.Score(); if (_lowerInclusiveScore <= score) _docIds.Add(_docBase + doc); } public override void SetNextReader(IndexReader reader Int32 docBase) { _docBase = docBase; } public override bool AcceptsDocsOutOfOrder() { return true; } } Thanks Simon. This really gave me much better understanding on how to implement it. @Shashikant - thanks also for sharing your thoughts. I'll keep that in mind. I'll just be more cautious in setting the threshold so that there'll be small chance that I filter out relevant results."
748,A,"Solr- multivalued date field range queries to match ""any""/""count""? I'm using Solr as part of a property booking engine- my entries have a multivalued date field which stores the dates that the property is already booked and thus not available. I want to be able to query against this and return entries that have no dates within the window specified. I'm half way there- but right now Solr appears to be returning the entry if it has even one free date- I want it to only return entries that are totally empty within the range. Example of my entity: <doc> <arr name=""DateBlockDates""> <date>2011-02-25T00:00:00Z</date> <date>2011-02-26T00:00:00Z</date> <date>2011-02-27T00:00:00Z</date> </arr> </doc> The query works great in this instance: -DateBlockDates:[2011-02-25T00:00:00Z TO 2011-02-27T00:00:00Z] Because the entity does have date blocks for every one of those days. However when I run: -DateBlockDates:[2011-02-25T00:00:00Z TO 2011-02-28T00:00:00Z] The entity gets returned because it doesn't have an entry for 2011-02-28. To put my question into old-school SQL I want to do a ""count(DateBlockDates) = 0"". Any ideas? This is the best solution I've come up with- not using a range for the date query and instead using: -DateBlockDates:""2011-02-25T00:00:00.000Z"" AND -DateBlockDates:""2011-02-26T00:00:00.000Z"" AND -DateBlockDates:""2011-02-27T00:00:00.000Z"" AND -DateBlockDates:""2011-02-28T00:00:00.000Z"" It's messy but it works. Thankfully I'm going to be dealing with relatively small date ranges so the query won't get too long. But if there is a better solution out there I'd love to hear it. Hi Alastair did you ever find a better solution to this? I'm trying to store multiple datetime values and be able to do a date range search over them. consider transforming date/time to UTC (if not already) then to an integer value representing the seconds since the epoch. Should allow for more empirical date range queries"
749,A,Using Solr for multiple sites I have setup a Solr server now I have two sites that I want to index and search using SolrNet. How do I differentiate the two sites' content in Solr? You may want to take a look at this document: http://wiki.apache.org/solr/MultipleIndexes I think the best approach is to use Multiple Solr Cores. will the cores work if I use Nutch? yes they will. thanks Luciano  Another option is you can simply add a new field that indicates the item's Web site. For example you can add a field called type. Searches on website1.com would require you to filter on the type field. &fq=type:website1.com That way you only need to deal with one core and one schema.xml file. This works if the pages of both sites have a very similar field set and it will make it easier to search across both sites if you plan on doing that. http://wiki.apache.org/solr/MultipleIndexes#Flattening_Data_Into_a_Single_Index
750,A,"JOINS in Lucene Is there any way to implement JOINS in Lucene? Lucene isn't a relational database. What do you mean? @skaffman - I'm using Lucene as a full text search engine for an MS SQL DB. It occurs to me that I could do away with the SQL DB altogether if I simply stored all the data in fields within Lucene. However to do this I would need some way of joining docs together. Norris You can't do that you will need to flatten the relationship since lucene does not handle relations between documents. You can also use the new BlockJoinQuery; I described it in a blog post here: http://blog.mikemccandless.com/2012/01/searching-relational-content-with.html One of the problems with the BlockJoinQuery as I understand it is that Block require a full reindex of the elements within the block when one of the contained parent or children changes. This is exactly the kind of thing I would want to avoid with a ""join-concept"". So a BlockJoinQuery creates more query flexibility but doesn't offer a more atomic storage concept.  Lucene does not support relationships between documents but a join is nothing else but a specific combination of multiple AND within parenthesis but you will need to flatten the relationship first. Sample (SQL => Lucene): SQL: SELECT Order.* FROM Order JOIN Customer ON Order.CustomerID = Customer.ID WHERE Customer.Name = 'SomeName' AND Order.Nr = 400 Lucene: Make sure you have all the neccessary fields and their respective values on the document like: Customer.Name => ""Customer_Name"" and Order.Nr => ""Order_Nr"" The query would then be: ( Customer_Name:""SomeName"" AND Order_Nr:""400"" )  You can do a generic join by hand - run two searches get all results (instead of top N) sort them on your join key and intersect two ordered lists. But that's gonna thrash your heap real hard (if the lists even fit in it). There are possible optimizations but under very specific conditions. I.e. - you do a self-join and only use (random access) Filters for filtering no Queries. Then you can manually iterate terms on your two join fields (in parallel) intersect docId lists for each term filter them - and here's your join. There's an approach handling a popular use-case of simple parent-child relationships with relatively small numer of children per-document - https://issues.apache.org/jira/browse/LUCENE-2454 Unlike the flattening method mentioned by @ntziolis this approach correctly handles cases like: have a number of resumes each with multiple work_experience children and try finding someone who worked at company NNN in year YYY. If simply flattened you'll get back resumes for people that worked for NNN in any year & worked somewhere in year YYY. An alternative for handling simple parent-child cases is to flatten your doc indeed but ensure values for different children are separated by a big posIncrement gap and then use SpanNear query to prevent your several subqueries from matching across children. There was a few-years old LinkedIn presentation about this but I failed to find it.  https://issues.apache.org/jira/browse/SOLR-2272  Use joinutil. It allows query time joins. See: http://lucene.apache.org/core/4_0_0/join/org/apache/lucene/search/join/JoinUtil.html  There are some implementations on the top of Lucene that make those kind of joins among several different indexes possible. Numere (http://numere.stela.org.br/) enable that and make it possible to get results as a RDBMS result set.  Here is an example Numere provides an easy way to extract analytical data from Lucene indexes select a.type sum(a.value) as ""sales"" b.category count(distinct b.product_id) as ""total"" from a (index) inner join b (index) on (a.seq_id = b.seq_id) group by a.type b.category order by a.type asc b.category asc Join join = RequestFactory.newJoin(); // inner join a.seq_id = b.seq_id join.on(""seq_id"" Type.INTEGER).equal(""seq_id"" Type.INTEGER); // left { Request left = join.left(); left.repository(UtilTest.getPath(""indexes/md/master"")); left.addColumn(""type"").textType().asc(); left.addMeasure(""value"").alias(""sales"").intType().sum(); } // right { Request right = join.right(); right.repository(UtilTest.getPath(""indexes/md/detail"")); right.addColumn(""category"").textType().asc(); right.addMeasure(""product_id"").intType().alias(""total"").count_distinct(); } Processor processor = ProcessorFactory.newProcessor(); try { ResultPacket result = processor.execute(join); System.out.println(result); } finally { processor.close(); } Result: <?xml version='1.0' encoding='UTF-8' standalone='yes' ?> <DATAPACKET Version=""2.0""> <METADATA> <FIELDS> <FIELD attrname=""type"" fieldtype=""string"" WIDTH=""20"" /> <FIELD attrname=""category"" fieldtype=""string"" WIDTH=""20"" /> <FIELD attrname=""sales"" fieldtype=""i8"" /> <FIELD attrname=""total"" fieldtype=""i4"" /> </FIELDS> <PARAMS /> </METADATA> <ROWDATA> <ROW type=""Book"" category=""stand"" sales=""127003304"" total=""2"" /> <ROW type=""Computer"" category=""eletronic"" sales=""44765715835"" total=""896"" /> <ROW type=""Meat"" category=""food"" sales=""3193526428"" total=""110"" /> ... continue"
751,A,FastVectorHighlighter.Net returning null on GetBestFragment I have a large index on which Highlighter.Net works fine but FastVectorHighlighter returns null as a Best Fragment on Some documents. the searcher works fine. It is just the highlighter. The field has been indexed in the same manner for all documents so I fail to understand Why it highlights some documents but not all. Using Lucene.Net 2.9.2 built from trunk rev942061 are you setting FieldMatch to true? Yup I tried with both true/false value. I am also setting phrasehighlight to true (I need it). Still some documents are highlighted some are not what other way have you tried? I suspect it has to do with FieldMatch try both false/true.  Problem solved by the nice people at lucene-net-user. I was passing the document sequence number in the Hits object where I should have been passing the sequence number in the lucene index. Read full mailing list thread starting here
752,A,Lucene Term Vector Multivariate Bayes Model Expectation Maximization I am trying to implement an Expectation Maximization algorithm for document clustering. I am planning to use Lucene Term Vectors for finding similarity between 2 documents. There are 2 kinds of EM algos using naive Bayes: the multivariate model and the multinomial model. In simple terms the multinomial model uses the frequencies of different words in the documents which the multivariate model just uses the info of whether a word is present or not in the document(a boolean vector). I know that the term vectors in Lucene store the terms present in the current document along with their frequencies. This is exactly what is needed for the multinomial model. But the multivariate model requires the following: A vector which stores the presence or absence of a particular term. Thus all the terms in all the documents must be handled by this vector. As an example: doc1 : field CONTENT has the following terms : this is the world of pleasure. doc2 : field CONTENT has the following terms : this amazing world is full of sarcastic people. now the vector that I need should be < this is the world of pleasure amazing full sarcastic people > ( it contains all the words in all the documents ) for doc1 the value of this vector is <1 1 1 1 1 1 0 0 0 0> for doc2 the vakue of this vector is <1 1 0 1 0 0 1 1 1 1> Is there any way to generate such a boolean vector in Lucene? I would first generate the multinomial vectors and then process them (maybe their textual representation) to get the multivariate vectors. If the set of documents is not very small storing full vectors is wasteful. You should have a sparse representation because every document contains a small subset of the possible terms. This blog post describes generating feature vectors from Lucene/Solr documents although I do not think it goes much farther than what you already did. i too mailed the Lucene user list and they suggested the same approach of processing the term vectors to get the vectors that I need. Thanks for your time.
753,A,How to use multiple index directory in hibernate-search? I use hibernate-search. I want that each user can see only theirself index and use it for searching and can't use index of other users. How can I do this? Thanks There is one index per indexed class (class hierarchy) and there is no Lucene index sharding. The Lucene way of solving your problem is using a filter which will filter away all results which don't belong to a certain user. Check the SecurityFilter example of the Hibernate Search online docs.
754,A,"Using Lucene QueryAPI to access SQL Can you advise on whether I can use just the Query functionality from Lucene to generate SQL queries? Something like an SQLQueryBuilder? I have a massive SQL database of logs from a webserver cluster containing the original request and response strings plus some other useful/less bits and bobs. What I need to do is analyse the parameters in the original request and compare with the generated responses looking at ratios volatility variability consistency etc. This question does not relate to the analysis stage but only the retrieval of data from database which matches the parameters I'm interested in. So I could just do this in good old sql queries manually building the exact queries I need on a case-by-case basis. But that's kinda lame; I reckon we can be a bit smarter than that. Particularly as I can already see large numbers of similar but subtly different queries being useful. And as I'm hoping that I can expose a single search box via a web interface to non-technical end-users adding sql queries seems like a bad idea... and a recipe for permanent maintenance requests (and can I be the first to say er no thanks!). In an ideal world I expose a search form with the option to write simple queries like request:""someAttribute=\""someValue\"""" AND response=""some hoped for result"" AND daterange:30 which would then hopefully find all instances of requests which contain someAttribute=""someValue"" over the last 30 days. The results will then be put through standard statistical analyses on the given response text and printed out on-screen. At least that's the idea. Much of the actual logic to determine how to handle custom field definitions or special words I'll need to write myself and that's ok. And NB my non-technical end users are familiar enough with xml that they can handle a bit of attr=""value"" syntax at least for the first iteration of the tool :D In summary I want to: 1) allow users to use google-like search syntax (e.g. via Lucene's QueryAPI) to specify text to match in the logs 2) allow a layer to manipulate the query based on special words or fields (e.g. this layer could be during a Java object phase) 3) convert the final query into an sql query appropriate for my database schema 4) query the database and spit back the resultset for statistical analysis 5) pretty-print on website:) Am I completely barking up the wrong tree? It looks like it should be possible but I can't seem to find much on it. I've been googling for a bit on this for example trying ""Lucene SQLQueryBuilder"" as a possible start but didn't really find much by way of a lead. So my questions are: Has anyone tried using Lucene's QueryAPI like this before? Did it work? Any gotchas? Are there better query api libraries out there? Examples finished discussions and open-source implementations would be most helpful. Many thanks. NB: I don't think I want Lucene's search capabilities as such as I'm only ever looking for exact matches. I just need a query layer on top of the database. Lucene and SQL have very little in common as they're using totally different syntax (as HefferWolf mentioned) and different underlying data models. As you said yourself I'm afraid you're barking the wrong tree. There are however attempts such as Hibernate Search to bridge this gap. These are interesting experiments as such but I would be very careful to use any of that code in production. You could possibly use Full Text Search features available in some SQL databases or reindex all data in Lucene and use it without database. We use hibernate-search in production quite successfully even a mix of using lucene directly for search and using hibernate-search just for indexing works quite well. The inherent problem with this is atomicity. Each CRUD operation requires two independent subsystems to be kept in sync which is very hard to achieve (I know this because we boil in a similar soup too). How does Hibernate-Lucene search deal with e.g. Lucene index corruptions db transaction failures etc.?  I doubt you can reuse any code from lucene for this. Lucene does an internal rewrite of such queries but into a syntax which wouldn't be of much help for SQL I think. name: Phil AND lastname: Miller AND NOT age: 26 would be rewritten to +name Phil +lastname: Miller -age: 26 So I think you would have to write your on transition into a SQL Query syntax. But maybe you can use Lucene as such for this. Have a look into hibernate-search which is quite handy to easily create a lucene index of a sql table. I thought Lucene created a java object tree to represent the query? I'm assuming that would come _after_ the rewrite you're mentioning? I'm hoping that someone might be familiar with interacting with these objects. I looked at the hibernate search stuff; does that even work if the underlying sql structure has nothing to do with hibernate? no hibernate-search only works if you use hibernate for this."
755,A,C# Lucene get all the index I am working on a windows application using Lucene. I want to get all the indexed keywords and use them as a source for a auto-suggest on search field. How can I receive all the indexed keywords in Lucene? I am fairly new in C#. Code itself is appreciated. Thanks. Are you looking extract all terms from the index? private void GetIndexTerms(string indexFolder) { List<String> termlist = new ArrayList<String>(); IndexReader reader = IndexReader.open(indexFolder); TermEnum terms = reader.terms(); while (terms.next()) { Term term = terms.term(); String termText = term.text(); int frequency = reader.docFreq(term); termlist.add(termText); } reader.close(); } This is very helpful. Thank you.
756,A,"Optimize lucene search performance I have an application that store (titlebody) of a news as separate field in lucene document At search time i need to create a query that boost title over body. (title is more important in search) but it slow down the speed of searching. An optimization tip show me that I can combine these two fields into one and It absolutely speed up search and indexing but I loose scoring that i want to catch at searching (boost title over body) Is there anyway to combine the benefits ? Could you provide us with your benchmarks showing unacceptable performance using two separate fields? You probably want two separate fields to disallow phrase searches to overlap different fields like title:""hello"" body:""world"" would match the search ""hello world"" if you had a combined field of title + body even if no field contained the phrase. If you really want to store an field with combined data look into custom scorers which would allow you to build your own scoring routing using any algorithm (and field) you want. I leave it as an exercise for a real answer to actually write the example code. ;) The easiest way to boost title more than body and index them in the same field is to add title's text multiple times. Or you can use payloads and override Similarity. See: http://www.lucidimagination.com/blog/2009/08/05/getting-started-with-payloads/ I doubt either of these solutions will give you that much of a speed improvement though. adding title field is tricky idea Thank you Yes it absolutely speed things up look at the http://wiki.apache.org/lucene-java/ImproveIndexingSpeed In my case i don't need to store title or body I just want them to be indexed so the best approach would be combine them to speed indexing and searching  You could also try boosting at index time. For example Document doc = new Document(); Field f = new Field(...) f.setBoost(10f); // or choose a float value of choice doc.Add(f); But still unclear on why you have performance problems with searching with search time boosts. Usually no noticeable loss if any."
757,A,java lucene class not found problem I keep getting this error: java.lang.NoClassDefFoundError: org/apache/lucene/index/memory/MemoryIndex Is there any way i can go about making sure java/tomcat can find this class? This class does not exist in the core lucene jar you are using but in a contrib jar called lucene-memory-2.4.1.jar. Make sure you include this jar in your application as well. THANK YOU SO MUCH this solved the issue loaded it to the common lib for tomcat and now lucene correctly highlights phrases.. THANKS
758,A,"Indexing multilingual words in lucene I am trying to index in Lucene a field that could have RDF literal in different languages. Most of the approaches I have seen so far are: Use a single index where each document has a field per each language it uses or Use M indexes M being the number of languages in the corpus. Lucene 2.9+ has a feature called Payload that allows to attach attributes to term. Is anyone use this mechanism to store language (or other attributes such as datatypes) information ? How is performance compared to the two other approaches ? Any pointer on source code showing how it is done would help. Thanks. so basically lucene is a ranking algorithm it just looks at strings and compares them to other string. they can be encoded in different character encodings but their similarity is the same non the less. Just make sure you load the SnowBallAnalyzer with the supported langugage stemmer and you should get results. Like say Spanish or Chinese  It depends. Do you want to allow something like: ""Search all english text for 'foo'""? If so then you will need one field per language. Or do you want ""Search all text for 'foo' and present the user with which language the match was found in?"" If this is what you want then either payloads or separate fields will work. An alternative way to do it is to index all your text in one field then have another field saying the language of the document. (Assuming each document is in a single language.) Then your search would be something like +text:foo +language:english. In terms of efficiency: you probably want to avoid payloads since you would have to repeat the name of the language for every term and you can't search based on payloads (at least not easily). I want the case 2. I need to be able to present to user the language of its literal. If a field called prefLabel can lucene handle the indexing of the label being similar in different languages i.e. ""email""^en ""email""^fr for example ? Does the inverted index use the payload to distinguish the entry ? I noticed there is a class PayloadTermQuery that allows Payload to be queried. I am not sure if you claim is correct when you say that searcher ignores payload. @fellahst: You can think of a payload as ""whatever random crap you want to attach to the term."" The searcher ignores it. You can manually pull it out at the end though. @fellahst: Fair enough you can create your own `PayloadFunction`. But it isn't baked in to Lucene; payloads aren't indexed in the same way terms are. If you are concerned about performance payloads aren't the way to go."
759,A,"Trouble searching for two terms using lucene I have written a following code in my project: final IndexSearcher indexSearcher = new IndexSearcher(INDEXING_DIRECTORY true); final Query query = new QueryParser(Version.LUCENE_33 ""keywords"" new StandardAnalyzer(Version.LUCENE_33)).parse(""cats movies""); final TopScoreDocCollector collector = TopScoreDocCollector.create(10 true); indexSearcher.search(query collector); final ScoreDoc[] hits = collector.topDocs(0 10).scoreDocs; The task is very trivial. I for example have a stored record with an indexed ""keywords"" field. The keywords for example might be similar to ""Tons of movies with a funny cats"". The problem is that my code above will return records if search query will be ""funny cats"" (word order like in the indexed field) but will fail on ""cats movies"". How should I write my query so it would match any words order and if is possible searching for a similar words also? Please state your question in the title. why all the downvotes? seems a valid question? Replaced the word ""phrase"" with ""two terms"" in the question. Phrases is not the thing the op wants to search for. Most likely ""cats movies"" will be parsed as PhraseQuery. PhraseQueries respect ordering. What you want is to have a BooleanQuery with two TermQueries combined with AND. final Query query = new QueryParser(Version.LUCENE_33 ""keywords"" new StandardAnalyzer(Version.LUCENE_33)).parse(""+cats AND +movies""); Some more examples are listed here. Some may be already outdated. Similar words is a rather hard task because you need to have at least some sort of wordlist or database which aligns synonyms. Thank you. I'll try that at home :)"
760,A,"Lucene.NET result using sql like LIMIT? + 1k question I agree with this answer and like it but i really would rather have this solved before going live. So i am starting a bounty in hopes my ass isnt bitten later ;). With Lucene.NET 2.9.x any version using .NET. How might i search and limit/page the results similar to the limit keyword in SQLite and MySql? I'd like to find the top 20 docs that have the word 'Apple' and have a link to page 20 returning 20 results ignoring the first 400 docs with a higher score. Should i implement it this way (credit goes to Pascal Dimassimo answer below) 1k Question Hey Guys i currently have 999 questions so this will be my 1000th question! I just wanted to say thank you to all of you who answered my questions left me comments and overall help me learn programming and technologies years sooner then it would have taken me alone. I also want to mention Edward Tanguay who was leading the most question asked for a long time and more importantly ask great questions with many upvotes. I strike to get my quality as high as his. I also want to mention these guys who are asking many questions as well. ooo metal-gear-solid Masi Blankman The search method of the Searcher class has a parameter to limit the number of results returned for a query. Query query = parser.parse(""Apple""); TopDocs topDocs = searcher.search(query 20); But Lucene does not support pagination. You will have to redo your query and keep the results that fits the range that you need. See this question. Lucene is usually really fast when redoing the same search. You should test first to see how it goes for your app. I am now worried about what may happen if my app grows to handle hundred of thousands of docs. I'll drop the 2.9 requirement and hope there is an answer. I'll accept in a few days if there is no other solution. I'm disappointed lucene doesnt support this. I agree with Pascal paging is almost always the wrong way to go with Lucene. The time complexity of finding the top n documents grows with log(n) (see http://philosophyforprogrammers.blogspot.com/2010/09/lucene-performance.html) so increasing from 20 to 40 is really a very trivial change. Interesting article Xodarap. It looks like it is fast. Ok i'll be doing this when the bounty is over.  Searcher.search you use in second line has also signature: Search(Query query Filter filter HitCollector results) Use HitCollector to flush temporary result into fast and temporary storage. For example if user asks first 20 - you need to return it and in background thread start caching all another. Really you need to store only document's ID so for 1 millions result approximately 4Mb is expected. When result is in the storage it is simple to support paging."
761,A,How to include Meta Keywords and Meta Description in Sitecore Lucene search index? I have a Lucene search working great on an existing Sitecore website but recently I discovered the Meta tags don't seem to be included in the indexes. This is not a major issue but I am curious to know if there is a way to tell Lucene to include the Meta Description and Keywords content in its indexes. Being a Sitecore website my index definitions are set up in my web.config file. Any ideas? Can you paste your current index definition(s) from the web.config into the question please? Lucene only indexes the fields in your data templates. (It doesn't crawl your site) So if you want Lucene to index Meta Keywords and Meta Description then you need to make sure that your data templates have fields for both of those and then you have to set up your index in web.config to include those fields. Okay thanks. I have the meta data exposed as fields in my data templates so it must just be a matter of properly reconfiguring my index definitions in web.config. Thanks for the info!
762,A,Adding fuzziness to a lucene query Is there a simple way to add a fuzziness level to a user entered search query in lucene I'd like to avoid having to parse their entered text if possible. At present if they enter green boxes I use a multifield query parser with boosts which easily generates the following for example: +(title:green^10 title:boxes^10) +(category:green^3 category:boxes^3) What I'd like to then do is convert this to +(title:green^10~0.7 title:boxes^10~0.7) +(category:green^3~0.7 category:boxes^3~0.7) It looks like I'd need to parse the query and add the fuzziness to each term but I was wondering if maybe there's a simple way to add the fuzziness? Thanks Mike Another way is to subclass and override MultiFieldQueryParser.getFieldQuery having it call getFuzzyQuery. Thanks I'll give that a go Mike
763,A,"How well does Solr scale over large number of facet values? I'm using Solr and I want to facet over a field ""group"". Since ""group"" is created by users potentially there can be a huge number of values for ""group"". Would Solr be able to handle a use case like this? Or is Solr not really appropriate for facet fields with a large number of values? I understand that I can set facet.limit to restrict the number of values returned for a facet field. Would this help in my case? Say there are 100000 matching values for ""group"" in a search if I set facet.limit to 50. would that speed up the query or would the query still be slow because Solr still needs to process and sort through all the facet values and return the top 50 ones? Any tips on how to tune Solr for large number of facet values? Thanks. Since 1.4 solr handles facets with a large number of values pretty well as it uses a simple facet count by default. (facet.method is 'fc' by default). Prior to 1.4 solr was using a filter based faceted method (enum) which is definitely faster for faceting on attribute with small number of values. This method requires one filter per facet value. About facet.limit  think of it like as a way to navigate through the facet space (in conjunction with facet.offset) like you navigate through the result space with rows/offset. So a value of 10 ~ 50 is sensible. As with rows/offset and due to the nature of Solr you can expect the performance of facet.limit/facet.offset to degrade when the offset gets bigger but it should be perfectly fine if you stay within reasonable boundaries. By default solr outputs more frequent facets first. To sum up: Use Solr 1.4 Make sure facet.method is 'fc' (well that's the default anyway). Navigate through your facet space with facet.limit/facet.offset.  Don't misregard to enable cache faceting related parameters (try different cache sizes to chose the values that fit well to your system):  <filterCache class=""solr.FastLRUCache"" size=""4096"" initialSize=""4096"" autowarmCount=""4096""/> <queryResultCache class=""solr.LRUCache"" size=""5000"" initialSize=""5000"" autowarmCount=""5000""/>"
764,A,"Problems using Lucene Highlighter I am using Lucene Highlighter 2.4.1 for my application. I use the highlighter to get the best matching fragments and display them. I make a call to a function String[] getFragmentsWithHighlightedTerms(Analyzer analyzer Query query String fieldName String fieldContents int fragmentsNumber int fragmentSize). For example : String text = doc.get(""MetaData""); getFragmentsWithHighlightedTerms(analyzer query ""MetaData"" Text 5 100); The function getFragmentsWithHighlightedTerms() is defined as follows private static String[] getFragmentsWithHighlightedTerms( argument list here) { TokenStream stream = TokenSources.getTokenStream(fieldName fieldContents analyzer); SpanScorer scorer = new SpanScorer(query fieldName new CachingTokenFilter(stream)); Fragmenter fragmenter = new SimpleSpanFragmenter(scorer fragmentSize); Highlighter highlighter = new Highlighter(scorer); highlighter.setTextFragmenter(fragmenter); highlighter.setMaxDocCharsToAnalyze(Integer.MAX_VALUE); String[] fragments = highlighter.getBestFragments(stream fieldContents fragmentNumber); return fragments; } Now my trouble is that the highlighter.getBestFragments() method is returning duplicates. i.e If i display say the first 5 fragments no. 1 and 3 are same. I do not quite understand what is causing this. Is there a problem with the code? Does the duplicate fragment actually occur in the field content multiple times? Can you post the example query and content? Hi thanks for your reply. I found the bug which was in Index creation that was causing duplicate hits. I dont have the code in front of me but I think you are getting an array of arrays. So you would need to do this: item[] = fragments[0] fragment = item[0] or just get 1 item out the fragments array."
765,A,How can I exclude certain URLs in Solr / Lucene I have setup new instance of Solr indexing on a website. I want Solr NOT to index certain URL patterns. Is there any way of mentioning such exclude-pattern? Regards Paras You can do the filtering in Solr using an UpdateRequestProcessor. In that UpdateRequestProcessor you could decide whether or not to index the document if it matches or not your regex.  Do you have a crawler going about and collecting data? I would lean towards doing that logic in the crawler. Solr is more of respository and I don't think is the best place to put lots of indexing logic in. Eric  It can be done in the program index only if the pattern does not match the exclude pattern.
766,A,Solr on a .NET site I've got an ASP.NET site backed with a SQL Server database. I'm been using Lucene.NET to index and search the database. I'm adding faceted search navigation to the results page (the facets are a hiarchical category tree). I asked yesterday to make sure I was using the right technique for faceting. All I've gotten so far is a suggestion to use Solr but Solr does a lot of things I don't need. I would really like to know from anyone who is familiar with the Solr's source code if Solr's facet processing is terribly different from the one described here by Bert Willems. Bascially you have a Lucene filter for each facet you get the bits array from it and you count the set bits in the array. I'm thinking since mine is hiarchical to begin with I should be able to optimize this pretty well but I'm afraid I might be grossly under-estimating the impact of this design on search performance. If Solr is no quicker I'm not going to gain anything by using it. I'd recommend creating a prototype project modeling your faceting needs with Solr and benchmark it against Lucene.net. Even though faceting in Solr is very optimized (and gets new optimizations all the time like the parallel per-segment faceting method) when using Solr there is some overhead for example network roundtrips and response parsing. If your code already implements Lucene.NET performs adequately and you don't need any of Solr's additional features then there is no need to switch to Solr. But also consider that if you choose Solr you will get faceting performance boosts for free with each new version.
767,A,Lucene numDocs and doqFreq on custom similarity class im doing an aplication with Lucene (im a noob with it) and im facing some problems. My aplication uses the Lucene 2.4.0 library with a custom similaraty implementation (the jar is imported) In my app im calculating doqFreq and numDocs manually (im adding the values of all indexes and then i calculate a global value in order to use it on every query) and i want to use that values on a custom similarity implementation in order to calculate a new IDF. The problem is that I dont know how to use (or send) the new doqFreq and numDocs values from my app on that new similarty implementation as I dont want to change lucene´s code apart from this extra class. Any suggestions or examples? I read the docs but i dont now how to aproach this :s Thanks 1st i calculate docFreq and numDocs for each server and then i calculate a global value that i send to all to servers in order to calculate idf. Why do you have to calculate docFreq and numDocs manually? Sounds a like a maintenance headache. You can try extending IndexReader and overriding IndexReader.docFreq() and IndexReader.numDocs(). In this subtype you can supply that you are calculating manually. I'm not sure if there are other Lucene components that are dependent on those values so you might want to tread carefully here. Well i solved this problem by changing some parameters on my custom similarity implementation (a couple of functions) no problems yet. And as Yuval F says with that aproach i wont get the results that i want. Thank you This looks like a good idea. However be advised that Lucene calculates most of this stuff at indexing time so if you only modify the similarity calculations only during retrieval you may not get the effect you are looking for.
768,A,"(HibernateSearch) MultiFieldQueryParser different analyzer per field Some of my indexed fields use a Greek analyzer and I want to use an English analyzer for some other fields. My problem is: When searching for results (with a MultiFieldQueryParser currently) how can I use a different analyzer per field so that a Greek analyzer is used for Greek-indexed fields and an English analyzer is used for English-indexed fields? Here is the solution I found. Please comment. transaction.begin(); PerFieldAnalyzerWrapper wrapper = new PerFieldAnalyzerWrapper(new StandardAnalyzer(Version.LUCENE_30)); wrapper.addAnalyzer(""greekTitle"" new GreekAnalyzer(Version.LUCENE_30)); wrapper.addAnalyzer(""greekDescription"" new GreekAnalyzer(Version.LUCENE_30)); String[] fields = {""greekTitle"" ""greekDescription"" ""englishTitle"" ""englishDescription""}; QueryParser queryParser = new MultiFieldQueryParser(Version.LUCENE_30 fields wrapper); queryParser.setDefaultOperator(QueryParser.AND_OPERATOR); org.apache.lucene.search.Query query = queryParser.parse(QueryParser.escape(queryString)); javax.persistence.Query persistenceQuery = fullTextEntityManager.createFullTextQuery(query Item.class); @SuppressWarnings(""unchecked"") List<Item> result = persistenceQuery.getResultList(); transaction.commit(); return result;  You could build your query parser like this: Analyzer analyzer = fullTextSession.getSearchFactory().getAnalyzer(Item.class); QueryParser parser = new MultiFieldQueryParser(Version.LUCENE_31 fields analyzer); which would use the proper analyzer as defined in the annotations of your Item class: @Field(name = ""greekTitle"" analyzer = @Analyzer(impl = GreekAnalyzer.class)) public void getGreekTitle(){ //... } @Field(name = ""englishTitle"" analyzer = @Analyzer(impl = StandardAnalyzer.class)) public void getEnglishTitle(){ //... }"
769,A,Help needed bubbling up relevant records with most recent date I've got 5 records in Lucene index. a.Record 1 contains--tax analysis.Date field value is March 2009 b.Record 2 contains--Senior tax analyst.Date field value is Aug 2009 c.Record 3 contains--Senior tax analyst.Date field value is July 2009 d.Record 4 contains--tax analyst.Date field value is Feb 2009 e.Record 5 contains--Senior tax analyst.Date field value is Oct 2009 If the input keyword is senior tax analystthen search results should come up in the following order: a.Record 5--because this record is has got the most recent date and has got the matching phrase b.Record 2--because this record has got second most recent date and has got the matching phrase c.Record 3--because this record has got third most recent date and has got the matching phrase d.Record 4 e.Record 1 BasicallyI want to show the most relevant records grouped by date and sorted in descending order by date.And then I want to show remaining records sorted by descending value of relevancy. How do i achieve this with Lucene? Please help. Thanks for reading. Why haven't you accepted Yuval's answer? I believe you can do this by first collecting the search results and then sorting them using a CustomSorter. This blog entry explains custom sorting in Lucene Java. You have to translate this to .Net using .Net's ScoreDocComparator. Your compare() method will have to get the date fields from the documents and compare them. I would first try to get the proper order for the exact matches (Record 5 2 and 3). Later you can use the match as well in your comparator. You can generalize my answer to this question in order to do this.
770,A,"Lucene complex structure search Basically I do have pretty simple database that I'd like to index with Lucene. Domains are: // Person domain class Person { Set<Pair> keys; } // Pair domain class Pair { KeyItem keyItem; String value; } // KeyItem domain name is unique field within the DB (!!) class KeyItem{ String name; } I've tens of millions of profiles and hundreds of millions of Pairs however since most of KeyItem's ""name"" fields duplicates there are only few dozens KeyItem instances. Came up to that structure to save on KeyItem instances. Basically any Profile with any fields could be saved into that structure. Lets say we've profile with properties - name: Andrew Morton - eduction: University of New South Wales - country: Australia - occupation: Linux programmer. To store it we'll have single Profile instance 4 KeyItem instances: name educationcountry and occupation and 4 Pair instances with values: ""Andrew Morton"" ""University of New South Wales"" ""Australia"" and ""Linux Programmer"". All other profile will reference (all or some) same instances of KeyItem: name education country and occupation. My question is how to index all of that so I can search for Profile for some particular values of KeyItem::name and Pair::value. Ideally I'd like that kind of query to work: name:Andrew* AND occupation:Linux* Should I create custom Indexer and Searcher? Or I could use standard ones and just map KeyItem and Pair as Lucene components somehow? I believe you can use standard Lucene methodology. I would: Translate every profile to a Lucene Document. Translate every Pair to a Field in this Document. All Fields need to be indexed but not necessarily stored. Add a stored Field with a profile id to the Document. Search using name:value pairs similarly to your example. If you choose bare Lucene you will need a custom Indexer and Searcher but they are not hard to build. It may be easier for you to use Solr where you need less programming. However I do not know if Solr allows an open-ended schema like the one I described - I believe you have to predefine all field names so this may prevent you from using Solr. I'm using Compass plugin in Grails project. I was just wondered if that's possible to do using standard Compass annotations or XML definitions. I'd like to have as few code as possible.  Lucene returns the list of hit documents essentially based on the occurence of the keyword/s regardless of the type of query. The fundamental segment reader checks for the presence of keywords in the entire index database rather than in specific field of interest. Suggest to introduce a custom searcher that performs the following. 1.Read the short-listed documents using the document id. ( I guess the collect() method may be overridden to pass the document id from search() of IndexSearcher class ). 2.Get the field value and check the presence of expected keywords. 3.Subject the document for scoring only if the document meets your custom criteria. Note : The default standard searcher can be modified rather than writing a custom seacher from scratch."
771,A,"Field having multiple distinct values Am building a ""Book search"" API using Lucene. I need to index Book NameAuthor and Book category fields in Lucene index. A single book can fall under multiple distinct book categories...for example: BookName1 --fictionhumourphilosophy. BookName1 --fictionscience. BookName1 --humourbusiness. BookName4-humour and so on..... User should be able to search all the books under a particular category say ""homour"". Given this situation how do i index above fields and build the query in lucene? You can create a simple ""category"" field where you list all categrories for a book seperated by spaces. Then you can search something like: stock market AND category:(+""business"") Or if you want to search in more than one category stock market AND category:(+""business"" +""philosophy"")  I would use Solr instead - it's built on Lucene and managed by the ASF but is much much easier to use than Lucene especially for newcomers. If offers pretty much all the mainline features of Lucene (certainly everything you'll need for the project you describe) plus extra things like snapshotting replication schemas ... In Solr you would simply define the fields you want to index something like this in schema.xml: <field name=""book_id"" type=""string"" indexed=""true"" stored=""true"" required=""true"" multiValued='false'/> <field name=""book_name"" type=""text"" indexed=""true"" stored=""true"" required=""true"" multiValued='false' /> <field name=""book_authors"" type=""text"" indexed=""true"" stored=""true"" required=""true"" multiValued='true' /> <field name=""book_categories"" type=""textTight"" indexed=""true"" stored=""true"" required=""true"" multiValued='true' /> Note that the multiValued='true' attribute lets you effective pass an array or list to this field which gets split and indexed nicely by Solr. Once you have this start up Solr and you can ask queries like ""book_authors:Hemingway"" or ""book_categories:Romance book_categories:Mills"". There are several query handlers pre-written and configured for you to do things like parse complex queries (fuzzy matches boolean operations scoring boosts ...) and as Solr's API is exposed over HTTP all this is wrapped by a number of client libraries so you don't need to handle the low-level details of crafting queries yourself. There is lots of great documentation on their website to get you started.  You can have a field for a Lucene document occur multiple times. Create the document add the values for the the name and author then do the same for each category create new lucene document add name field and value add author field and value for each category: add category field and value add document to index When you search the index for a category it will return all documents that have a category field with the value you're after. The category should be a 'Keyword' field. I've written it in english because the specific code is slightly different per lucene version. wont this create multiple documents? what happens if you have 3 million records? and each book has 3-5 categories ? you are having between 9-15 million records.. i wonder if there is some other way of acheiving the same. No you would only have one document. It isn't like a database where you manage the schema AND the index. You have to relax and let Lucene handle the index it's really clever stuff. Will this work in Zend Lucene? This won't work in Zend_Search_Lucene: the source for Zend_Search_Lucene_Document::addField( $field ) { $this->_fields[$field->name] = $field; return $this; } How to test if all the categories are saved? When i write a query i only get the first category returned for a doc. Would the ranking function treat matches in several fields treat similar as matches for several tokens in the same field?"
772,A,Have you indexed nutch crawl results using elasticsearch before? Has anyone had any luck writing custom indexers for nutch to index the crawl results with elasticsearch? Or do you know of any that already exist? I wrote an ElasticSearch plugin that mocks the Solr api. Using this plugin and the standard Nutch Solr indexer you can easily send crawled data into ElasticSearch. Plugin and an example of how to use it with Nutch can be found on GitHub: https://github.com/mattweber/elasticsearch-mocksolrplugin  I know that Nutch will be adding pluggable backends and glad to see it. I had a need to integrate elasticsearch with Nutch 1.3. Code is posted here. Piggybacked off the (src/java/org/apache/nutch/indexer/solr) code. https://github.com/ctjmorgan/nutch-elasticsearch-indexer I am new to java so i dont know how to create a package on ubuntu and then rebuild it. I have installed nutch at the location /home/peter/nutch/ so i dont know where to copy the ivy files and java files. Also what settings have to be added to the ivy files??  Haven't done it but this is definitely doable but would require to piggyback the SOLR code (src/java/org/apache/nutch/indexer/solr) and adapt it to ElasticSearch. Would be a nice contrib to Nutch BTW That's the approach I've taken. I have written my own elasticsearch indexer and my own crawl process as well.
773,A,"How do I pass a list of 'allowed' IDs to filter a Lucene search? I need to return just the documents that a user has access to from a Lucene search. I can get a list of IDs from a database that make up the 'allowed' subset. How can I pass these to Lucene? The articles I've found on the web suggest I need to use a BitSet and FieldCache (am I right?) but I'm having trouble finding good examples. Does anyone have any? I'm using C# but any language would be great. Thanks. Once I knew I needed to use a custom filter asking this question got the right answer.  A simple way would be to build a MultiPhraseQuery with an array of all the matching IDs via MultiPhraseQuery.add(Term[] terms). You can build one of these things with thousands of terms and Lucene (as always) performs extremely well. This seemed promising but when using ""thousands"" of terms I hit the maxClauseCount of 1024 (by default). I know I can increase this but I was hoping that under the hood MultiPhraseQuery wouldn't actually split up into thousands of different clauses. Is there any way to avoid this? OK I ended up solving this by using a FieldCacheTermsFilter."
774,A,How to exclude searching specified fields using Zend Search (Lucene) I've built a search index using the PHP Zend Framework Search (based on Lucene). The search is for a buy/sell website. My search index includes the following fields: item-id (UnIndexed) item-title (Text) item-description (UnStored) item-tags (Text) item-price (keyword) seller-id (UnIndexed) seller-name (Text) I want the user to search the index filtering their search by either only searching for items or searching for sellers by name. If I do a search using Lucene's default search settings I will be searching all 5 item fields and the seller-name field. This is not what I want to happen. What I would like is when the user makes the search I want them to be required to select from a drop-down menu if they are searching for an item or for a seller-name. How can I tell the search query when searching for items to ignore the seller-name field? And when searching for seller-name's how can i tell the search query to not search across any of the item fields? Or is it better to create a separate index for the seller names? There's currently no way to explicitly not search a field in Lucene's query language or the Zend_Search_Lucene query constructing API. However you can explicitly list which fields you do want to search in a query. An example would be: seller-name: Joe McBob If using this approach you will have to explicitly list which fields you want to query and what to search in them. So if you also needed to search your item-title field with the same text you would have to duplicate the above but with the different field name. An example would be: seller-name: Joe McBob OR item-title: JoeMcBob You can of course do all this through the query building API that Zend_Search_Lucene provides as well. The manual has some good examples there. One thing worth noting here is that as you've discovered Zend_Search_Lucene will search ALL fields by default. This is one of the ways in which it differs from Java Lucene. You may however set a default field to query using the setDefaultSearchField static method of the Zend_Search_Lucene class. Thanks jason. I figured that's what I may have to do. I wanted to double check the knowledge base here to see if there was a better solution and you have satisfied my question!
775,A,Creating a search over a mySQL database I am trying to create a search for my website over a mySQL database. I started down the line of using Sphinx but was hesitant when learning that the index doesn't update in real time. I did see they have an almost real time update but I am concerned this doesn't fit my system well because new content is added to the database on a minute by minute basis. This new content needs to be added immediately and re-indexing after each update seems strange. I am currently looking into Solr which is built on Lucene but this also doesn't seem to fit my needs because it is more of a file based search instead of a database search. It also looks like an awful lot to configure for a relatively simple search. I also found this stackoverflow question but had a few problems with it as well. The first is that I am not searching through just one field but many. Also I am worried that searches done purely in SQL may be too slow over my database which will hopefully store in the hundreds of thousands of records if not more. If anyone has any opinions on any of the software I have mentioned or any that I haven't all ideas are welcome. I am using java for the back-end if that makes any difference. Thanks. Went with plain old Lucene and after a little bit of grunt work to get it setup it turned out to be the perfect choice. At their core databases are just files. What is wrong with a file based search? It sounds like Solr fulfills your requirements. If you use their example setup they provide in their download there isnt much to getting started. All you would need to do is configure your schema.xml for your data. To get real-time search you would need to add your documents to the solr index in real time. This is a simple to post to one of Solr's servlets or can be done through SolrJ (their java client). If you are searching over many columns I think Solr will be more efficient and easier to use than a database. It will also provide a richer feature set such as faceting and stemming. Nothing wrong with file based search. Just seemed like it might be a bit more difficult to set up as a result. But it is looking like Solr is the right choice at this point. Thanks for the info on real-time search with Solr.  There is also just plain Lucene and Xapian --- the latter has PHP-bindings.
776,A,"Lucene: Similarity class... how to define several similarity measures? For my experiment I need to define specific similarity metrics for each field of my collection documents. For example I need to measure the Description field similarity with tf.idf and Geolocation fields with Harvesine distance.. etc... I'm now studying the Similarity class. I was wondering if there is any good tutorial or example about this to procede faster... thanks EDIT: IIUC you have a similarity formula per field and you want to use it per document running against all other documents. You can use several options all at indexing time: Extend the DefaultSimilarity class. Extend the SimilarityDelegator class if you only need to modify part of the methods. In both methods you may make use of payloads to store term-specific information (could be useful for the lat-long data). After implementing a Similarity class using one of these methods use Similarity.setDefault(mySimilarity) to set this as the Similarity instance for indexing and searching. Only then index your text corpus which you can search later - you will probably have to extend the Searcher class as well to get the raw similarity. Having said that I believe this approach is wrong for your use case - Lucene is optimized to get a few similar documents not a score for every one so I predict the runtime will be prohibitive - Hope I am wrong but nevertheless I suggest you read Mining of Massive Datasets for a better approach - min hashes and shingling. Good luck. Patrick I will first quote Grant Ingersoll about modifying the Similarity class: ""Here be Dragons"". Customizing Lucene's Similarity class is hard. I have done this. It is not fun. Only do this if you absolutely have to. I suggest you should first read Grant's spatial search paper his findability paper and his 'debugging relevance' paper. These show other ways to get hits as needed. Here's my 2c: If your collection is not very small this is not a good use of Lucene. Lucene's similarity is planned to extract a few similar documents rather than compare to all other documents. I suggest you use Weka or Mahout's clustering for this. I will edit my answer along these lines. @Yuval F ok. I'm going to use as query a document of the collection in order to compare it to all other docs. So it is to compare documents. Each document has text fields geo fields and time/date field. I need to average the scores over all fields. I want to use my own scores formulas for geo and time/date. It is ok to use tf/idf for other fields instead. @Yuval F The material you passed me is not exactly what I'm looking for. I don't need to refine Lucene documents score or Findability. Also I don't need info about location aware service (it is interesting though). What I need to know is how to integrate multiple similarity measures into one: lucene already has a tf-idf score I can use for some fields. For other fields such as lat-long I need to use Harvesine distance (I already have a formula to use)... @Patrick - Can you please elaborate? Do you need similarity to rank documents in a Lucene fashion or do you need it for another purpose say as a feature to compare documents? @Yuval F Thanks for updating the answer. However in my question I've already mentioned the Similarity class. The issue with the similarity class is that only computeNorm()lengthNorm() and scorePayload() have field as input parameter. Therefore I cannot customize the score per field level but only per document level. @Yuval F I would like to give a try beacause my docs are very short. But what I actually need is to define different similarity measures per field. Can you help me ? @Patrick: Here it is. HTH."
777,A,Calculating similarity between and centroid of Lucene documents In order to perform a simple clustering algorithm on results that I get from Lucene I have to calculate Cosine similarity between 2 documents in Lucene I also need to be able to make a centroid document to represent the centroid of each cluster. All I can think of doing is building my own Vector Space model with tf-idf weighting using the TermFreqVectors and Overall Term frequencies to populate it. My question is: This is not an efficient approach is there a better way to do this? This feels a little unclear so any suggestions on how I can improve my question are also appreciated. in order to get similarity of one document to the other why not make a one query with the content of one document and run query against index? that way you will get score(cosine similarity values)  The short answer is: No. I have spent a lot of time (way way too much) looking into this and as far as I can see you can make your own Vector Space Model and work from that or use Mahout to generate a Mahout Vector which you can make comparisons between documents from. I am gonna go ahead and make my own so I'm marking this question answered! Sorry for bumping in like this but how did you solve the problem? Did you had to implement all by yourself or were you able to reuse some hidden existing components of Lucene? Thanks  Mark you may find Integrating Mahout with Lucene IR Math with Java or Vector Space Classifier Using Lucene useful. I already had a look at them but cheers anyway they are relevant links.
778,A,Solr/Lucene is it possible to order first by relevance and then by a second attribute? In Solr/Lucene is it possible to order first by relevance and then by a second attribute? As far as I can tell if I set an ordering parameter it totally overrides relevance and sorts by the ordering parameter(s). How can I have results sorted first by relevance and then in the case of two entries with exactly the same relevance giving the nod to the item that say comes first alphabetically. If it makes any difference I'm using Solr through Sunspot in Ruby on Rails. I am wondering if this would work well since relevance score would vary greatly. I wonder if solr can segment relevance results into many levels and then sort each of these by the second column. Solved my own problem! The keyword score can be passed to order the result by relevancy. So in Rails Sunspot terms: Article.search do keywords params[:query] order_by :score :desc order_by :name :asc end This might be late catchup but I tried your same solution and it's not working for me. Whenever I use score for sorting it uses the second sort option only. If I use score only it works fine. If I combine two other sort criteria that also works. is there anything else to do? Are you aware of gem version issues? Appreciate help
779,A,"How do I add an EdgeNGramTokenFilter to a Compass Query? I am building some auto-complete functionality using compass and I need to add an EdgeNGramTokenFilter to the compass query but I cannot see how I can add it. Is this possible? I managed to add the EdgeNGramTokenFilter filter by creating a provider class adding a reference to it in the compass.config.xml file by adding the following line within the <searchEngine> tags <analyzerFilter name=""lower"" type=""EdgeNGramTokenFilterProvider""/> Here is the class: import org.apache.lucene.analysis.TokenStream; import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter; import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.Side; import org.compass.core.CompassException; import org.compass.core.config.CompassSettings; import org.compass.core.lucene.engine.analyzer.LuceneAnalyzerTokenFilterProvider; public class EdgeNGramTokenFilterProvider implements LuceneAnalyzerTokenFilterProvider { public TokenStream createTokenFilter(TokenStream tokenStream) { return new EdgeNGramTokenFilter(tokenStream Side.FRONT 1 20); } public void configure(CompassSettings settings) throws CompassException { } }"
780,A,"Lucene Autocomplete with multiple words using Shingle filter I am trying to make a Lucene autocomplete using Lucene's Dictionary and spellcheck classes but so far only successful in making it work for single terms. I googled and found out that we need to make use of Shingle Matrix filter to get the work done.. Can someone experienced with Lucene show me a way to do it ? All I need is it has to generate words for autocomplete with phrases. For example if I have a doc like this : ""This is a long line with very long rant with too many words in it"" Then I should be able to generate words like ""long line"" ""long rant"" ""many words"" etc... Possible ? Thanks. Anything at all ? possible duplicate of [How to do query auto-completion/suggestions in Lucene?](http://stackoverflow.com/questions/120180/how-to-do-query-auto-completion-suggestions-in-lucene) See http://stackoverflow.com/questions/24968697/how-to-implements-auto-suggest-using-lucenes-new-analyzinginfixsuggester-api/25301811#25301811 for a complete example of how to do autocomplete with Lucene. You can write your own Analyzer implementing TokenStream function in inheriting Lucene.Net.Analysis.Analyzer class. There u can use this shingleFilter to get multiword from the tokenstream Code Stream: public override Lucene.Net.Analysis.TokenStream TokenStream(String fieldName System.IO.TextReader reader) { Lucene.Net.Analysis.TokenStream tokenStream = new Lucene.Net.Analysis.Standard.StandardTokenizer(Lucene.Net.Util.Version.LUCENE_30 reader); tokenStream = new Lucene.Net.Analysis.Shingle.ShingleFilter(tokenStream maxShingleSize); return tokenStream; } max Shingle size identifies max length of multi word unit  writer = new IndexWriter(dir new ShingleAnalyzerWrapper(new StandardAnalyzer( Version.LUCENE_CURRENT Collections.emptySet())3) false IndexWriter.MaxFieldLength.UNLIMITED); This did the job for me... Hi how did you get this to work? I've been trying to get multiple word did you mean suggestion feature. Can you outline the steps you took to implement this or even better post some more code?"
781,A,"Retreiving a task instance scheduled with ScheduledExecutorService I got a ScheduledExecutorService for task scheduling in a JEE environment. Some of those task are leaving resources opened when they are interrupted with ScheduledExecutorService.shutdownNow() (e.g. open files with a third-party lib like Lucene). I know that a thread may not stop his execution by itself: The must used way to stop a thread is cheeking the interrupt flag and stopping the method execution and if the thread is block (e.g wait() sleep() etc) or if doing some IO operation in a interruptible channel the Thread.interrupt() will make a InterruptedException rise. In both cases the finally block must be executed. See: http://download.oracle.com/javase/15.0/docs/api/java/lang/Thread.html#interrupt%28%29. Obviously I already tried to release the resources with a very well implemented finally block in the Task class but in some environments (e.g. CentOS) the finally block is not executed when the thread is interrupted. And then I found this very cool note in the official Java Documentation: Note: If the JVM exits while the try or catch code is being executed then the finally block may not execute. Likewise if the thread executing the try or catch code is interrupted or killed the finally block may not execute even though the application as a whole continues. So what I need is a reference to all the scheduled task in order to implement some public method in the Task classes that force the release of resources. Can I retrieve those references to the task classes from the ScheduledExecutorService? Or do you have some cool idea to resolve my problem in a better way? The first solution: Wrap it! Create a Wrapper class for the ScheduledExecutorService and add a property like this: private IdentityHashMap<ScheduledFuture<?> Runnable> taskList; With that we can access any Runnable object directly or by the ScheduledFuture related to it. For the instantiation of the wrapper I can get the ScheduledExecutorService from the Executors.newScheduledThreadPool() method and pass it to my wrapper. Another Solution: Extend it! Extend the ScheduledThreadPoolExecutor add the IdentityHashMap property and overwrite all the method that schedules or cancels jobs to add/remove the reference from the Map. The problem with both solutions? If the caller of your wrapper or extended class receive a SchedulerFuture<?> object cancel the job with the SchedulerFuture<?>.cancel() method is possible bypassing your ""capsule"". With the wrapper you can avoid passing the SchedulerFuture<?> reference to the caller but with the extended class you can't (if you create your own methods in the extended class you will get the same result as the wrapper but in a very confusing way). The elegant solution: Your own scheduler! Thanks to Kaj for pointing it ... Extend the ScheduledThreadPoolExecutor to overwrite the decorateTask() method Decorate the Runnable with one implementation of a ScheduledFuture interface Implement one custom cancel() method that actually cancels the thread but also manipulates the Runnable object to force the resource releasing. Check my blog post for the details and code exemples!!! do you suspect the VM to be exiting then? and not running the finally block? If so why not only exit the VM when all tasks have been cancelled (ie implement await termination on the executor)? It could just be a race condition..? This happens when a tomcat application is shutdown and the Lucene library is a shared resource in the server. So all the threads of the application are canceled but the VM still alive and the library still available to others web applications. In other hand when we shutdown the application we do: `shutdown()` after `awaitTermination(some minutes)` and then `shutdownNow()`. The problem: A Lucene index rebuild is a very long task (even hours) so if the index rebuild is in execution when `shutdownNow()` is called the thread is interrupted but the resources aren't released. hmmm I'd raise that as a bug with lucene! threads should respond properly to interrupt IMO (they should implement a correct interruption policy). You're having to work very hard to work around this it seems... What are you scheduling? What does the tasks look like? I find it very hard to believe that the finally block isn't executed. I would guess that it's the tasks that you have scheduled but that haven't started executing that are leaking resources (since their finally block won't be executed) Sounds like a really bad VM implementation on the CentOS if it really aren't executing those finally blocks. Haven't heard about that in any other VM implementation. One option that you can do instead of referencing all of the scheduled tasks is to subclass ScheduledThreadPoolExecutor and override the decorateTask methods so that they decorate the tasks with your classes and then intercept the cancel invokation. @ggarciao - you're sure the interruption is happening within the finally block? and you aren't doing anything like Thread.stop()? normal thread interruption should _always_ hit the finally block. also is it possible you are doing something inside the finally block which is sensitive to thread interruption and therefore skipping the rest of the finally block? My periodical scheduled task updates a Lucene Index (http://lucene.apache.org/java/docs/index.html). For us the problem with the finally block is incredible but our diagnostic is pretty good: in our CentOS the finally block is never reached (after thread interruption). Thanks for the advice about extending the pool executor. I will check how to do that. Not really I'm closing some Lucene readers writers and directories and also those operations are inside others try-catch blocks."
782,A,I'm using Railo 3.1 and CFSearch I want to distribute across several machines Can I rsync lucene indexes (every 5mins) to another pair of boxes and run the same app just for search? I'm not interested in doing App Server Clustering. Well it doesn't seem to work. cfsearch didn't work with copied files maybe it requires something in memory. Rebuilding indexes on each individual machine is good enough for us.
783,A,Is bulkIndexOnStartup needed when using mirrorChanges = true in Grails Searchable plugin? The mirrorChanges option will mirror all changes made through GORM/Hibernate so if I'm not making any external changes to the database then is bulkIndexOnStartup needed? The problem is that our data set is very large (>1M rows) and the bulk indexer may take 30+ minutes. When set to 'fork' lucene will crash if any changes are made though the GORM because lucene is not thread safe and the GORM transaction will attempt to update the index while the forked bulk index thread is running. When set to true the application will finish initializing for those 30+ minutes. have you actually seen the bulk indexer crash when forking it on startup? bulkIndexOnStartup should not be needed if you are mirroring changes. we have a much smaller dataset so we actually do bulkIndexOnStartup with 'fork' set as an option when we do releases as more of a maintenance task  If you are not modifying the data from another source then you don't need to set bulkIndexOnStartup to true specially if you set mirrorChanges to true. This is pretty much how we handle it in our application. We have a controller action that calls the indexer forked for a specific class so we can call it on demand in case we need to for example if for some reason we needed to update the database directly. We then call the indexer for a specific class and not the entire set of class in that way if we need to call it then it won't take that long.
784,A,Sunspot / Solr / Lucene : Find similar article Let's say we have a list of articles that are indexed by sunspot/solr/lucene (or any other search engine). How can be used to find similar articles with a given article? Should this be done with a resuming tool like: http://www.wordsfinder.com/api_Keyword_Extractor.php or termextract from http://developer.yahoo.com/yql/console or http://www.alchemyapi.com/api/demo.html ? Thank you all for your good answers. See this [answer](http://stackoverflow.com/questions/5122788/reducing-similar-top-results-in-solr-result-output/5123165#5123165) It seems you're looking for the MoreLikeThis feature.  What you are trying to do is very similar to the task I outlined in this answer. In brief you need to generate a summary for each document that you can use as the query to compare it with every other. A document summary could be as simple as the top N terms in that document (excluding stop words). You can generate top N terms from a Lucene document pretty easily without using any 3rd party tools there are plenty examples on SO and the web to do this.
785,A,Luke Lucene QueryParser Case Sensitivity In Luke if I enter the search expression docfile:Tomatoes.jpg* the parsed query is docfile:Tomatoes.jpg*. When the search expression is docfile:Tomatoes.jpg (no asterisk *) the parsed query is docfile:tomatoes.jpg with a lowercase 't'. Why? How can I change this? BTW using org.apache.lucene.analysis.standard.StandardAnalyzer. StandardAnalyzer uses LowerCaseFilter which means it lowercases your queries and data. This is described in the Javadocs http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/analysis/standard/StandardAnalyzer.html. If I remember correctly WhitespaceAnalyzer does not lowercase but verify it suits your needs http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/analysis/WhitespaceAnalyzer.html. StandardAnalyzer doesn't seem to be lowering the case of my data. docfile:Tomatoes.jpg* returns the expected results while docfile:Tomatoes.jpg returns 0. docfile:Tomatoes.jpg using WhitespaceAnalyzer or KeywordAnalyzer does return the expected results. The data was indexed with StandardAnalyzer. Not being too experienced with Lucene I don't know if it's ok to use a different analyzer while searching or not. Is this a common practice? It was indexed with StandardAnalyzer. Are you certain the data is not lowercased in the index? Did you run it through an analyzer? You usually should use the same analyzer your storing and searching. I am having the same issue items are indexed with standard analyzer and lowercase queries do not return results but same case queries do return results As said before docs have to be indexed with the same analyzer you are pulling data out of it.
786,A,"how to use a multiphrasequery? http://lucene.apache.org/java/2_3_1/api/core/org/apache/lucene/search/MultiPhraseQuery.html for the example ""Microsoft app*"" he says use IndexReader.term() but that returns TermEnum how do I put it in MultiPhraseQueryParser ? Edit : Or someone tell me how do I do a search on Microsoft app* in a better way over a 7.5 GB index!! You need to iterate on TermEnum to get the terms. You can iterate on the TermEnum to get terms starting with ""app"" as follows.  TermEnum te = reader.terms(new Term(""field"" ""app"")); List<Term> termList = new LinkedList<Term>(); while(te.next()) { Term t = te.term(); if (!t.field().equals(""field"") || !t.text().startsWith(""app"")) { break; } termList.add(t); } Term[] terms = termList.toArray(new Term[0]); Oh my God!!! I've 427800 docs!!! iterating over a 7.5 GB index would make it go crazy isn't it ?!!!! Since you are iterating only on small subset of terms it is quite fast. Try it out. Works fine :) Thank you :)"
787,A,Searching on date ranges with Lucene in Java? Is it possible to search on date ranges using Lucene in Java? How do I build Lucene search queries based on date fields and dates ranges? For example: between specified dates prior to a specified date after a specified date within the last 24 hours within the past week within the past month. [Edit] i'm using Lucene 2.4.1 and my system is really legacy and really poorly tested so i would like if possible not to have to upgrade Yes it is possible. If you need some sample code I'll find some for you - just ask in comment to this post. Have a look at Lucene in Action - you can find answer to this question and many others also. Lucene (before version 2.9 anyway) only stores String values and it only supports lexicographical range queries on that data. So if you want to store date/time data and performa range queries on it you need to explicitly format your data/time values in such a way as to make them lexicographically ordered. For example store your date/times as something like 2009-10-29T15:34:00 and then do range queries like [2009-10-29T15:00:00 TO 2009-10-29T16:00:00] As has been pointed out elsewhere Lucene 2.9 finally introduced support for range queries against non-string data making this all rather easier. Using `DateTools.dateToString(date Resolution.SECOND)` method can help you to produce date string in lucene form which is like `20110223220000` meaning `2011-02-23 22:00:00`.
788,A,"Profiling Lucene in Nutch I'm trying to profile Nutch using VisualVM. Lucene is the part of the Nutch core responsible for generating url indexes and for searching these indexes due to some query. I'm running Nutch through Apache Tomcat and I would like to determine how much time Nutch spends in various function calls (including Lucene calls) but when I try to profile using VisualVM I get a bunch of profiling data about Tomcat and not Nutch or Lucene. What am I doing wrong here? What do you mean you only get data about Tomcat? Since tomcat's the servelet you shouldn't expect to see nutch or lucene run in their own processes right? That's true what I'm looking for is when does the servlet use Nutch functions. I had the same experience trying to locate Lucene time inside Tomcat calls. What you have to do is: Use VisualVM 1.2.2. Choose the relevant process and press ""Profile"". Check the ""Settings"" checkbox. This should open a ""CPU settings"" tab with fields you can fill. Under ""Start profiling From classes:"" write an entrance point in your code (e.g. com.my.company.NutchUser) Uncheck ""Profile new runnables"". Choose ""Profile only classes:"" and under it write: org.apache.lucene.* org.apache.nutch.* Press the ""Profile CPU"" button. I believe if you do all that then run your process and take occasional snapshots you will be fine. Alternatively This guy suggests doing stack sampling instead of profiling. I have never done it but it sounds interesting. See my edited step 3. You should see the screen change once you check the ""Settings"" checkbox. This is extremely useful. The only question I have is about step 4. What do you mean? Where would I add this?"
789,A,"Boost factor in MultiFieldQueryParser Can I boost different fields in MultiFieldQueryParser with different factors? Also what is the maximum boost factor value I can assign to a field? Thanks a ton! Ed MultiFieldQueryParser has a constructor that accepts a map of boosts. You use it with something like this: String[] fields = new String[] { ""title"" ""keywords"" ""text"" }; HashMap<StringFloat> boosts = new HashMap<StringFloat>(); boosts.put(""title"" 10); boosts.put(""keywords"" 5); MultiFieldQueryParser queryParser = new MultiFieldQueryParser( fields new StandardAnalyzer() boosts ); As for the maximum boost I'm not sure but you shouldn't think about boost in absolute terms anyway. Just use a ratio of boosts that makes sense. Also see this question. The boosts parameter was available only in Lucene 2.4. If you can't upgrade you might consider copying the code into your own MyMutliFieldQueryParser. It's not that much code. You may have to port the code from java... I couldn't find lucene.net's source code online (svn.apache.org is down ATM). Hithanks for your answer....am using lucene.net version 2.0.0.4 I dont see MultiFieldQueryParser constructor accepting boost values. MultiFieldQueryParser multiFieldQueryParser = new MultiFieldQueryParser(fields _analyzer); May I know which Lucene version are u using? Thanks."
790,A,How to index and find numbers with Lucene.NET? I've implemented full text search for a web site using Lucene.NET (Version 2.0). Indexing and searching works well but I have one problem. If I look for numbers (phone numbers product numbers etc.) as search terms I don't get any resulting documents. I'm using the Lucene.Net.Analysis.SimpleAnalyzer Class. I guess I have to change Analyzer and/or Tokenizer. Any advice? Thank you! When you build up a Lucene Document you get to select different indexing options for each field. For fields you don't want tokenized you need to select the Field.Index.UN_TOKENIZED option. This will keep your phone numbers and product numbers in tact. I would also advise using the StandardAnalyzer as its doesn't strip numbers out like SimpleAnalyzer. It is also important you use the same analyzer for both indexing and searching to get consistent results.
791,A,"MySQL Full-Text Search Across Multiple Tables - Quick/Long Solution? I have been doing a bit of research on full-text searches as we realized a series of LIKE statements are terrible. My first find was MySQL full-text searches. I tried to implement this and it worked on one table failed when I was trying to join multiple tables and so I consulted stackoverflow's articles (look at the end for a list of the ones I've been to) I didn't see anything that clearly answered my questions. I'm trying to get this done literally in an hour or two (quick solution) but I also want to do a better long term solution. Here is my query: SELECT a.`product_id` a.`name` a.`slug` a.`description` b.`list_price` b.`price` c.`image` c.`swatch` e.`name` AS industry FROM `products` AS a LEFT JOIN `website_products` AS b ON (a.`product_id` = b.`product_id`) LEFT JOIN ( SELECT `product_id` `image` `swatch` FROM `product_images` WHERE `sequence` = 0) AS c ON (a.`product_id` = c.`product_id`) LEFT JOIN `brands` AS d ON (a.`brand_id` = d.`brand_id`) INNER JOIN `industries` AS e ON (a.`industry_id` = e.`industry_id`) WHERE b.`website_id` = 96 AND b.`status` = 1 AND b.`active` = 1 AND MATCH( a.`name` a.`sku` a.`description` d.`name` ) AGAINST ( 'ashley sofa' ) GROUP BY a.`product_id` ORDER BY b.`sequence` LIMIT 0 9 The error I get is: Incorrect arguments to MATCH If I remove d.name from the MATCH statement it works. I have a full-text index on that column. I saw one of the articles say to use an OR MATCH for this table but won't that lose the effectiveness of being able to rank them together or match them properly? Other places said to use UNIONs but I don't know how to do that properly. Any advice would be greatly appreciated. In the idea of a long term solution it seems that either Sphinx or Lucene is best. Now by no means and I a MySQL guru and I heard that Lucene is a bit more complicated to setup any recommendations or directions would be great. Articles: http://stackoverflow.com/questions/1117005/mysql-full-text-search-across-multiple-tables http://stackoverflow.com/questions/668371/mysql-fulltext-search-across-1-table http://stackoverflow.com/questions/2378366/mysql-how-to-make-multiple-table-fulltext-search http://stackoverflow.com/questions/737275/pros-cons-of-full-text-search-engine-lucene-sphinx-postgresql-full-text-searc http://stackoverflow.com/questions/1059253/searching-across-multiple-tables-best-practices I fixed the ""quick solution"" problem which can be found here: http://stackoverflow.com/questions/2891037/mysql-or-match-hangs-very-slow-on-multiple-tables For the short-term solution I suggest creating a table of just the full-text values as in this question. For the long-term solution please take a look at Solr. It is much easier to install than Lucene and yet gives you most of its functionality. I have also heard good things about Sphinx but have not personally ever used it. Great for the long-term solution! Looks like we will probably be using it for the short-term however that link simply pointed out that you can't link to 2 tables in the same MATCH so I need to figure out how I can make it work. When I use OR MATCH it simply takes forever (freezes essentially) and comes back like 15 minutes later"
792,A,"How does Solandra store documents? Does Solandra store documents as-they-are (in rows of a column family for example) with the Lucene index containing only pointer information... or are documents bound into (stored with) the index itself (which of course is stored in Cassandra)? If you mark a document field stored=true then the document is stored as is in the Docs ColumnFamily. The key however is essentially random so there is no way to look it up unless you know the internal solandra id. Is there a way to get a hold of this Solandra id? Would I have to get a hold of this ID during the write to the index - I'm assuming this would slow down things.  From http://blog.sematext.com/2010/02/09/lucandra-a-cassandra-based-lucene-backend/: Document Key Column Key Value ""indexName/documentId"" => { fieldName  value }"
793,A,OOM when using solr DIH hi all: There is an OOM error when I using DIH to execute a full import command The database is sql server 2008 and only 30k rows data in db How do I fix it? Thanks in advance for any help:) See this entry in the DataImportHandler FAQ. Hi Mauricio thanks for your help. I've read the wiki but another question is the importing speed is so slow if I use cursor to avoid OOM. There is 40 million data in db which I want to index is there a better way to slove this problem? @Illu: try writing the importing process yourself instead of using DIH.
794,A,"Querying a Lucene index file I'm trying to query a Lucene index file through QueryParser. However I would like to see the format of the index file before querying it. Is there a way to lookup the structure of a Lucene index file sort of like how I'm able to lookup the structure of a regular SQL table? The reason is that I haven't built this index file myself and would like to get my way around it before querying it. An important thing to remember is that Lucene doesn't have ""schemas"" - each document can have whatever fields (""columns"") it wants. So there is no ""structure"" of a lucene index like there is a structure of a relational db. @Xodarap - interesting is there some documentation where I can read more about it? you can look at the [Lucene index structure](http://www.ibm.com/developerworks/library/wa-lucene/) to see why it's not necessary - a better answer is probably just that Solr (which sits on top of Lucene) allows you to set up schemas. So it's not that schemas are necessarily ""bad"" or ""good"" just out of scope for Lucene. Luke - Lucene Index Toolbox Luke is a handy development and diagnostic tool which accesses already existing Lucene indexes and allows you to display and modify their content in several ways  You can use Luke or programmatically IndexReader.getFieldNames()."
795,A,Zend_Search_Lucene massive - similar to ZF-5545 issue EDIT: Solved with a hack for now. Added at line 473: if (isset($this->_termsFreqs[$termId][$docId])) { } This happens only when I'm searching for multiple words e.g.: +word1 +word2 + word3 I get this massive error: Notice: Undefined offset: 2 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 2 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 4 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 4 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 6 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 6 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 1 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 1 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 9 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Notice: Undefined offset: 9 in C:\wamp\www\project\library\Zend\Search\Lucene\Search\Query\MultiTerm.php on line 473 Funny thing is that the result set that is returned is correct so in production I could just turn off the error reporting and it would work like a charm but I don't want to do that. Similar issue is documented here: http://framework.zend.com/issues/browse/ZF-5545 And apparently there is no solution. I have also tried using UTF-8 compatible text analyzer (even though I have only Latin 1 characters in the index): Zend_Search_Lucene_Analysis_Analyzer::setDefault(new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8()); You have to put this condition to surpress the warning: if (array_key_exists($termId $this->_termsFreqs) && array_key_exists($docId $this->_termsFreqs[$termId])) { ... } But the question remains if this is helpful. There might be a logical error causing this undefined offset.  Undefined offset just means that it is trying to get an array value that doesn't exist. The solution is just to check array_key_exists first to make sure the key is set. From the source for the file mentioned in the error you'd need to add this if condition near line 473 (the second and sixth lines are the addition): foreach ($this->_terms as $termId => $term) { if (array_key_exists($termId$this->_weights)) { $score += $reader->getSimilarity()->tf($this->_termsFreqs[$termId][$docId]) * $this->_weights[$termId]->getValue() * $reader->norm($docId $term->field); } } Presently because $this->_weights[$termId]->getValue() is being multiplied by other values and then added to $score the result of the multiplication is 0 and nothing gets added thus the result comes out correctly. Adding the if will not change this as nothing will be added either way. Try adding && isset($docId) to the if? I have done that and I still get errors. It seems like the $docId is not defined.
796,A,What is the best Field Type/Encoding to store a number in a Zend Lucene Search Index? How would I index a price int field in a Zend Lucene Search Index? I am currently using: $doc->addField(Zend_Search_Lucene_Field::Keyword('price' $price 'utf-8')); Is this the correct way? Or should I be storing it specifically as a number somehow? I think you do it the right way. I'm not aware of better way to store it. Thanks for confirming it. I was having trouble finding an example of storing a non PK int so I wanted to double check I wasn't missing something (such as perhaps the encoding needs to be set to something else).
797,A,"Help needed ordering search results I've 3 records in Lucene index. Record 1 contains healthcare in title field. Record 2 contains healthcare and insurance in description field but not together. Record 3 contains healthcare insurance in company name field. When a user searches for healthcare insuranceI want to show records in the following order in search results... a.Record #3---because it contains both the words of the input together(ie.as a phrase) b.Record #1 c.Record #2 To put it another way exact match of all keywords should be given more weight than matches of individual keywords. How do i achieve this in lucene? Thanks. Record #1 doesn't contain one of the query term ""insurance"" and you would like it be ranked at #2. Is that correct? Rewrite the query with a phrase + slop factor. So if the query is: healthcare insurance you can rewrite it as: ""healthcare insurance""~100 Documents that have the words ""healthcare"" and ""insurance"" closer in proximity to each other will be scored higher. In this case since the slop factor is 100 documents that have both words but are more than 100 terms apart will not match. Rewriting the query involves manipulating the Term objects in a BooleanQuery. Take all the terms create a PhraseQuery and set a slop factor. Thanks for your valuable inputsbajafresh4life.I will try this approach.  You can use phrase + slop as bajafresh4life says but it will fail to match anything if the terms are more than slop apart. A slightly more complicated alternative is to construct a boolean query that explicitly searches for the phrase (with or without slop) and each of the terms in the phrase. E.g. ""healthcare insurance"" OR healthcare OR insurance Normal lucene relevance sort will give you what you want and won't fail in the way that the ""big slop"" approach will. You can also boost individual fields so that for example title is weighted more heavily than description or company name. This needs an even more complicated query but gives you a lot more control over the ordering... title:""healthcare insurance""^2 OR title:healthcare^2 OR title:insurance^2 OR description:""healthcare insurance"" OR ... It can be quite tricky to get the weights right and you may have to play around with them to get exactly what you want (e.g. in the example I just gave you might not want to boost the individual terms for title) but when you get it working its pretty nice :-)"
798,A,php lucene how to update and delete rows in index file There is time when users change the content of their post content field in actual database will get updated. how can I get the same field the index file updated as well? And when users delete a post how can I delete that post in the index file ? I used lucene search with Symfony and here is how I use it: // Called when an object is saved public function save(Doctrine_Connection $conn = null) { $conn = $conn ? $conn : $this->getTable()->getConnection(); $conn->beginTransaction(); try { $ret = parent::save($conn); $this->updateLuceneIndex(); $conn->commit(); return $ret; } catch (Exception $e) { $conn->rollBack(); throw $e; } } public function updateLuceneIndex() { $index = $this->getTable()->getLuceneIndex(); // remove existing entries foreach ($index->find('pk:' . $this->getId()) as $hit) { $index->delete($hit->id); } $doc = new Zend_Search_Lucene_Document(); // store job primary key to identify it in the search results $doc->addField(Zend_Search_Lucene_Field::UnIndexed('pk' $this->getId())); // index job fields $doc->addField(Zend_Search_Lucene_Field::unStored('title' Utils::stripAccent($this->getTitle()) 'utf-8')); $doc->addField(Zend_Search_Lucene_Field::unStored('summary' Utils::stripAccent($this->getSummary()) 'utf-8')); // add job to the index $index->addDocument($doc); $index->commit(); } // Called when an object is deleted public function delete(Doctrine_Connection $conn = null) { $index = $this->getTable()->getLuceneIndex(); foreach ($index->find('pk:' . $this->getId()) as $hit) { $index->delete($hit->id); } return parent::delete($conn); } And here is how I get my index: public static function getInstance() { return Doctrine_Core::getTable('Work'); } static public function getLuceneIndexFile() { return sfConfig::get('sf_data_dir') . '/indexes/work.' . sfConfig::get('sf_environment') . '.index'; } static public function getLuceneIndex() { ProjectConfiguration::registerZend(); if (file_exists($index = self::getLuceneIndexFile())) { return Zend_Search_Lucene::open($index); } else { return Zend_Search_Lucene::create($index); } } Hope it will help you ;) I have a small query on this code. How can you use find() in an unindexed field?
799,A,"Get starting and end index of a highlighted fragment in a searched field ""My search returns a highlighted fragment from a field. I want to know that in that field of particular searched document where does that fragment starts and ends ?"" for instance. consider i am searching ""highlighted fragment"" in above lines (consider the above para as single document). I am setting my fragmenter as : SimpleFragmenter fragmenter = new SimpleFragmenter(30); now the output of GetBestFragment is somewhat like : ""returns a highlighted fragment from"" Is it possible to get the starting and ending index of this fragment in the text above (say starting is 10 and ending is 45) I did just that a few months ago. You have to build custom Formatter and Encoder. Basically inside the highlighter the formatter processes the tokens chosen for highlighting while the encoder processes the rest of the tokens. In your case you need the encoder to emit the empty each time it is called and the formatter to emit the start index and the end index. They are indeed stored in the TokenGroup of the highlighted parts. Your highlighter should be constructed using these custom formatter and encoder. can you please post a code sample  The Highlighter does not returns that information when you use the methods getBestFragment. Behind the scene the Highlighter uses the TokenGroup class to get the start and the end index of each fragment. You could probably use that class. actually that hint was quite helpful. sorry for a really late response. Thanks"
800,A,"Is there a fast accurate Highlighter for Lucene? I've been using the (Java) Highlighter for Lucene (in the Sandbox package) for some time. However this isn't really very accurate when it comes to matching the correct terms in search results - it works well for simple queries for example searching for two separate words will highlight both code fragments in the results. However it doesn't act well with more complicated queries. In the simplest case phrase queries such as ""Stack Overflow"" will match all occurrences of Stack or Overflow in the highlighting which gives the impression to the user that it isn't working very well. I tried applying the fix here but that came with a lot of performance caveats and at the end of the day was just plain unusable. The performance is especially an issue on wildcard queries. This is due to the way that the highlighting works; instead of just working on the querystring and the text it parses it as Lucene would and then looks for all the matches that Lucene has made; unfortunately this means that for certain wildcard queries it can be looking for matches to 2000+ clauses on large documents and it's simply not fast enough. Is there any faster implementation of an accurate highlighter? The 'Highlighter for Lucene' link is currently broken. Thanks for pointing that out I've fixed the link. Now that Lucene 3.0.0 is out you should find that upgrading everything to that will make things just work. HOWEVER - beware that the highlighter now has a dependency on lucene-memory from contrib as well (this will only show up when highlighting for exact matches) I've been reading on the subject and came across spanQuery which would return to you the span of the matched term or terms in the field that matched.  You could look into using Solr. http://lucene.apache.org/solr Solr is a sort of generic search application that uses Lucene and supports highlighting. It's possible that the highlighting in Solr is usable as an API outside of Solr. You could also look at how Solr does it for inspiration. Thanks taking a look at Solr - I think I've always confused it with Nutch in the past and assumed they were the same thing silly me. I notice in the Solr docs it seems to separate out a PhraseHighlighter and a standard Highlighter so I'm not imbued with much confidence I'm afraid :( Unfortunately the solr highlighter just delegates to the highlighter in the Lucene Sandbox - it doesn't do anything clever :(  There is a new faster highlighter (needs to be patched in but will be part of release 2.9) https://issues.apache.org/jira/browse/LUCENE-1522 and a back-reference to this question Thanks for pointing that out Peter I'll give that a go and see if it's usable for us."
801,A,Where does the Symfony plugin 'sfLucene' get the information about my database schema from? A month ago I adjusted my database schema. I added a column called ordinal. I rebuilt my model and uploaded my changes. Everything works fine apart from my instance of sfLucene will not rebuild. I run symfony lucene-rebuild frontend But I get the error once it gets to the Model in question (others are fine): propel exception: unknown column ORDINAL I have tried clearing the Symfony cache but to no avail. I feel like Lucene has cached the database schema somewhere not sure where. Index rebuilding works fine on my local PC. Problem solved The databases.yml file on the server had the all: setting pointed to my test server. Unfortunately the test server is on the same box therefore MySQL login was succeeding. However the test database did not have the recently added fields. The command line (i.e. the symfony lucene-rebuild command) uses the all: setting in the databases.yml Ha ha - sorry @Raise! It was most definitely all down to you! Nice big tick for you ;)  Probably checks schema.yml or schema.xml You can regenerate/update it with the console command symfony propel:build-schema Older versions of symfony use this syntax symfony propel-build-schema Thanks Peter I have tried rebuilding my model and it has not helped the situation.
802,A,"Find all Lucene documents having a certain field I want to find all documents in the index that have a certain field regardless of the field's value. If at all possible using the query language not the API. Is there a way? I've done some experimenting and it seems the simplest way to achieve this is to create a QueryParser and call SetAllowLeadingWildcard( true ) and search for field:* like so: var qp = new QueryParser( Lucene.Net.Util.Version.LUCENE_29 field analyzer ); qp.SetAllowLeadingWildcard( true ); var query = qp.Parse( ""*"" ) ); (Note I am setting the default field of the QueryParser to field in its constructor hence the search for just ""*"" in Parse()). I cannot vouch for how efficient this method is over other methods but being the simplest method I can find I would expect it to be at least as efficient as field:[* TO *] and it avoids having to do hackish things like field:[0* TO z*] which may not account for all possible values such as values starting with non-alphanumeric characters.  If you know the type of data stored in your field you can try a range query. Per example if your field contain string data a query like field:[a* TO z*] would return all documents where there is a string value in that field. This should work. It would be slightly more complex in case the field has values starting with numbers or capital letters. Should be easy to do with an OR query. Good point! But if it starts with a capital a range starting with a* should catch it because the javadoc of TermRangeQuery states that it uses String.compareTo to determine if a string is part of the range. This looks good. Not sure about catching records starting with numbers but this is a good start. Thanks! The lexicographic order of strings defines numbers before letters so a range [0* TO z*] would catch all values starting by both letters and numbers. And capital letters appears before lower-case (in contrary to what I have said in my previous comment). Don't forget to check how your values are indexed: they may be all lower-cased! According to http://stackoverflow.com/questions/2686033/lucene-search-for-documents-that-have-a-particular-field/2726574#2726574 field:[* TO *] will also work however you may have to enable `SetAllowLeadingWildcard` on the `QueryParser`. Actually if you enabled that wouldn't simply field:* work?"
803,A,"Comparison of Lucene Analyzers Can someone please explain the difference between the different analyzers within Lucene? I am getting a maxClauseCount exception and I understand that I can avoid this by using a KeywordAnalyzer but I don't want to change from the StandardAnalyzer without understanding the issues surrounding analyzers. Thanks very much. In general any analyzer in Lucene is tokenizer + stemmer + stop-words filter. Tokenizer splits your text into chunks and since different analyzers may use different tokenizers you can get different output token streams i.e. sequences of chunks of text. For example KeywordAnalyzer you mentioned doesn't split the text at all and takes all the field as a single token. At the same time StandardAnalyzer (and most other analyzers) use spaces and punctuation as a split points. For example for phrase ""I am very happy"" it will produce list [""i"" ""am"" ""very"" ""happy""] (or something like that). For more information on specific analyzers/tokenizers see its Java Docs. Stemmers are used to get the base of a word in question. It heavily depends on the language used. For example for previous phrase in English there will be something like [""i"" ""be"" ""veri"" ""happi""] produced and for French ""Je suis très heureux"" some kind of French analyzer (like SnowballAnalyzer initialized with ""French"") will produce [""je"" ""être"" ""tre"" ""heur""]. Of course if you will use analyzer of one language to stem text in another rules from the other language will be used and stemmer may produce incorrect results. It isn't fail of all the system but search results then may be less accurate. KeywordAnalyzer do not use any stemmers it passes all the field unmodified. So if you are going to search some words in English text it isn't a good idea to use this analyzer. Stop words are the most frequent and almost useless words. Again it heavily depends on language. For English these words are ""a"" ""the"" ""I"" ""be"" ""have"" etc. Stop-words filters remove them from the token stream to lower noise in search results so finally our phrase ""I'm very happy"" with StandardAnalyzer will be transformed to list [""veri"" ""happi""]. And KeywordAnalyzer again do nothing. So KeywordAnalyzer is used for things like ID or phone numbers but not for usual text. And as for your maxClauseCount exception I believe you get it on searching. In this case most probably it is because of too complex search query. Try to split it to several queries or use more low level functions. Very clear description - thanks very much"
804,A,Rollback in lucene Is there a rollback in lucene? I'm saving & updating database repository & lucene repository simultaneously so that the lucene index & database are in sync.. ex.  CustomerRepository.add(customer); SupplierRepository.add(supplier); CustomerLuceneRepository.add(customer); SupplierLuceneRepository.add(supplier); // If this here fails i cannot rollback the customer above DataContext.SubmitChanges(); I am not completely sure on how you are using Lucene or how the C# version differs from the Java version but at least IndexWriter in Java contains methods for commit and rollback. If you could post some implementation code that would probably help. unfortunately i'm using lucene.net in C# and there's no commit & rollback method as i'd double checked. I don't know why they didn't include it.. Have you seen this blog post http://www.lybecker.com/blog/2009/12/03/lucene-net-and-transactions/? I checked the API documentation for Lucene.net 2.4.0 at http://lucene.apache.org/lucene.net/docs/2.4.0/ and IndexWriter seems to contain methods for commit and rollback. Hi Ponzao you are right. Those methods exist in 2.4 version. Currently im using 2.3. Need to get latest =).. thank you very much for pointing that out. Appreciate your help. No problem. Good thing it was just a version problem.
805,A,Store fields in database or in Lucene index file My domain object is pretty simple only have 20 properties(columns or fields whatever you call it) and does not have complex relationship. I need to index 5 of them for full text search and 3 need for sort. There might be 100000 records. To keep my application simple I'm thinking store all fields in Lucene index file to avoid introducing database. Will there be performance problem for this implementation? Thanks. Possible duplicate: http://stackoverflow.com/questions/3239198/lucene-indexing Depending on how you access stored fields they may all be loaded into memory (basically if you use a FieldCache everything will be cached into memory after the first use). And if you have a gig of storage which is taking up memory that's a gig less to use for your actual index. Depending on how much memory you have this may be a performance enhancement or a performance detriment. Thank you Xodarap. Then I think it's better not store them into index.
806,A,Lucene term boosting with sunspot-rails I'm having an issue with Lucene's Term [Boosting][1] query syntax specifically in Ruby on Rails via the sunspot_rails gem. This is whereby you can specify the weight of a specific term during a query and is not related to the weighting of a particular field. The HTML query generated by sunspot uses the qf parameter to specify the fields to be searched as configured and the q parameter for the query itself. When the caret is added to a search term to specify a boost (i.e. q=searchterm^5) it returns no results even though results would be returned without the boost term. If on the other hand I create an HTTP query manually and manually specify the field to search (q=title_texts:searchterm^5) results are returned and scores seem affected by the boost. In short it appears as though query term boosting doesn't work in conjunction with fields specified with qf. My application calls for search across several fields using the respective boosts associated to those fields conditionally in turn with boosting on individual terms of a query. Any insight? [1]: http://lucene.apache.org/java/2_9_1/queryparsersyntax.html#Boosting a Term Sunspot uses the dismax parser for fulltext search which eschews the usual Lucene query syntax in favor of a limited (but user-input-friendly) query syntax combined with a set of additional parameters (such as qf) that can be constructed by the client application to tune how search works. Sunspot provides support for per-field boost using the boost_fields method in the fulltext DSL: http://outoftime.github.com/sunspot/docs/classes/Sunspot/DSL/Fulltext.html#M000129 Thanks I see now that dismax is a subset of Lucene query syntax. Unfortunately I need to boost terms in the query not fields of the documents. Are there other query parsers that allow queries to be made across fields? You can use the fulltext block more than once performing queries on different fields in each boosting differently etc. -- would that fit your use case? I hadn't realized sunspot only supports dismax. Thanks to both responses but again I need to be able to specify both a boost per-field as well as with arbitrary terms of the incoming query. The latter seems to require a single field name be specified and the lucene parser over dismax.  The solution I have found is to use DisMax but adding the bq parameter with a boolean string with the boosted terms therein.
807,A,lucene dateToString stringToDate I am in a locale where the time is two hours ahead GMT+2. When I encode a date using new GregorianCalendar(ymddhms) and then use DateTools.dateToString with DAY resolution I end up getting the day before. Encoding 12:00 midnight 111970 I end up getting the 31st of january (22:00) which is clearly incorrect. The problem is even worse because stringToDate doesn't give me the same number back. Apparently this monstrous confusion is by design. What is the correct way to compensate for this so that a birth date can actually be searched correctly. Thanks Which constructor are you using to create the GregorianCalendar? If you don't specify the time zone it will just use the default which is the time zone of the machine you happen to be running the code on. Make sure the Date object you pass into dateToString is normalized to GMT correctly. On the search side of things you'll need to normalize the date queries to GMT as well. There is no way getting around normalization -- you don't know where you code is going to be executed so you'll need to anchor your dates by normalizing to GMT Great! If you think about it dateToString can't be symmetric with stringToDate because dateToString just normalizes to GMT and you lose the time zone info. stringToDate doesn't know what the original time zone is so it just assumes the string is in GMT when it's converted back. Thanks I figured it out exactly as you point out. The important thing (in my opinion) is that dateToString and stringToDate should be symmetric which they are not. I am sure opinions vary but in mine this is confusing. Anyway - your advice was spot on. Thank You
808,A,"How to do Polygon spatial search in Solr? We are using Solr 3.3 with Solr.NET and we have put a dynamic ""location_p"" location type field on our documents and now we need the ability to do spatial searches. I have got the radius searches (distance from a given point) working like this; {!geofilt sfield=location_p pt=33.882518712472255-84.05531775646972 d=1.7} Now we need the ability to do a Polygon squery to get all documents with the ""location_p"" field 'inside' a given set of Points (something along the lines of the Polygon search capabilities of ElasticSearch). This is really different than the BBox query filter as the points of the Polygon are not symmetrical more random based on user 'click' points. Any ideas or suggestions would be appreciated. As far as I know Solr doesn't currently implement polygon spatial search. There are a couple of efforts towards implementing this (SOLR-2155 SOLR-2268). Try applying one of these patches test it contribute to the project. There's also Spatial Solr plugin which implements polygon search but is only compatible with Solr 1.4. See also http://wiki.apache.org/incubator/SpatialProposal We just moved to ElasticSearch its using lucene as its index engine as well and supports GeoPolygons. You can use the Spatial Solr plugin 2.0 with Solr 3.x too. The jar you can download from [here](http://www.searchworkings.org/ssp) works out of the box with Solr 3.x. You can also easily upgrade from 1.0 to 2.0 version without the need to reindex. We are still using it despite of the spatial support provided by Solr."
809,A,Random Sorting Results in Lucene.Net 2.4 How do I sort my results in a random order. my code looks something like this at the moment: Dim searcher As IndexSearcher = New IndexSearcher(dir True) Dim collector As TopScoreDocCollector = TopScoreDocCollector.create(100 True) searcher.Search(query collector) Dim hits() As ScoreDoc = collector.TopDocs.scoreDocs For Each sDoc As ScoreDoc In hits 'get doc and return Next Since this is an IEnumerable you can use standard linq to randomize it. You can find an example here: public static IEnumerable<T> Randomize<T>(this IEnumerable<T> source) { Random rnd = new Random(); return source.OrderBy<T int>((item) => rnd.Next()); } If you want to do this inside of Lucene itself you can make your own sorter (although note that you will no longer be randomizing the top 100 results but rather randomizing all results).
810,A,Getting maximum value of field in solr I'd like to boost my query by the item's view count; I'd like to use something like view_count / max_view_count for this purpose to be able to measure how the item's view count relates to the biggest view count in the index. I know how to boost the results with a function query but how can I easily get the maximum view count? If anybody could provide an example it would be very helpful... There aren't any aggregate functions under solr in the way you might be thinking about them from SQL. The easiest way to do it is to have a two-step process: Get the max value via an appropriate query with a sort use it with the max() function So something like: q=*:*&sort=view_count desc&rows=1&fl=view_count ...to get an item with the max view_count which you record somewhere and then q=whatever&bq=div(view_count max(the_max_view_count 1)) Note that that max() function isn't doing an aggregate max; just getting the maximum of the max-view-count you pass in or 1 (to avoid divide-by-zero errors). If you have a multiValued field (which you can't sort on) you could also use the StatsComponent to get the max. Either way you would probably want to do this once not for every query (say every night at midnight or whatever once your data set settles down). Well thanks for your answer... Additionally: I think it's quite hard to find any good documentation on this at apache there's just the link above and one topic about functional queries... Can you probably recommend anything else?
811,A,Lucene. How to build a term-doc matrix I need to build that matrix but I can't find a way to compute normalized tf-idf for each cell. The normalization I would perform is cosine-normalization that is divide tf-idf (computed using DefaultSimilarity ) per 1/sqrt(sumOfSquaredtf-idf in the column). Does anyone know a way to perform that? Thanks in advance Antonio One way not using Lucene is described in Sujit Pal's blog. Alternatively you can build a Lucene index that has term vectors per field iterate over terms to get idf then iterate over term's documents to get tf.
812,A,"Solr/Lucene: Indexing facet values For example say I have the following facet: Colors Red (7825) Orange (2343) Green (843) Blue (5412) In my database colors would be a table and each color would have a primary key and a name/value. When indexing with Solr/Lucene in all of the examples I've seen the value is indexed and not the primary key. So if I filter by the color red I would get something like the following: http://www.example.com/search?color=Red I'm wondering is it wise to instead index the primary key and retrieve the values from the database when displaying the facet values? So I would instead get something like this: http://www.example.com/search?color=1 ""1"" representing the primary key of the color red. I'm wondering if I should take this approach since the values of many of my facets frequently change but the primary keys stay the same. Also the index is required to be in sync with the database. Does anymore have any experience with this? How do you think this will affect performance? Thanks in advance! short answer: yes it's ok. long answer: when I get home :-) If you expect your entities to change frequently it's easier to index the ID's and when you get your facet results do a lookup in the database to get the names of the colors. That way changes to colors wouldn't require affected documents to be updated in the index. In our system we index the ID's Lucene instead of the name of the entities exactly because of the reasons you stated. Also our entities have a bunch of properties associated with them which aren't indexed so we would have to hit the database to get them anyway. As far as performance goes the faceting of ID's won't be discernibly slower or faster. As far as the database lookups go it shouldn't be a big deal especially if you're only pulling down tens of facets at a time. You can always use caching to speed that up if it becomes an issue."
813,A,"How do I use native Lucene Query Syntax? I read that Lucene has an internal query language where one specifies : and you make combinations of these using boolean operators. I read all about it on their website and it works just fine in LUKE I can do things like field1:value1 AND field2:value2 and it will return seemingly correct results. My problem is how do I pass this who Lucene query into the API? I've seen QueryParser but I have to specifiy a field. Does this mean I still have to manually parse my input string fields values parenthesis etc or is there a way to feed the whole thing in and let lucene do it's thing? I'm using Lucene.NET but since it's a method by method port of the orignal java any advice is appreciated. Are you asking whether you need to force your user to enter the field? If so the query parser has a default field. Here's a little more info. As long as you have a default field that will do the job they don't need to specify fields. If you're asking how to get a Query object from the String you need the parse method. It understands about fields and the default field etc. mentioned earlier. You just need to make sure that the query parser and the index builder are both using the same analysis. Oh so the field you specify is just a default field in case none is specified? If a user searched for ""red white blue"" then the query that came out of parse would search the default field. If instead they searched for ""title:red title:white title:blue"" then the query would only look at the title field of the index. So yes (sorry I think I misinterpreted your comment at first)."
814,A,"How to create nested boolean query with lucene API (a AND (b OR c))? I have an indexed object with three fields (userId title description). I want to find all objects of a specific user where the title OR the description contains a given keyword. I have something like this (but that's obviously wrong): WildcardQuery nameQuery = new WildcardQuery(new Term(""name"" filter.getSearch())); WildcardQuery descQuery = new WildcardQuery(new Term(""description"" filter.getSearch())); TermQuery userQuery = new TermQuery(new Term(""user_id"" u.getId()+"""")); BooleanQuery booleanQuery = new BooleanQuery(); booleanQuery.add(new BooleanClause(name_query Occur.SHOULD)); booleanQuery.add(new BooleanClause(desc_query Occur.SHOULD)); booleanQuery.add(new BooleanClause(user_query Occur.MUST)); How wo modify the code to get all objects with the correct ID and the search phrase in title or description? I believe that you'll need to use the Query.mergeBooleanQueries method in order to create a single query that is the effective OR of the first two. So something like this at line 3: Query nameOrDescQuery = Query.mergeBooleanQueries(new Query[] { nameQuery descQuery }); and then create a new BooleanClause over this rather than the individual clauses. This should ensure you get the OR logic on your name/desc filters rather than the current AND logic.  I think that it will be something like this: BooleanQuery booleanQuery = new BooleanQuery(); TermQuery userQuery = new TermQuery(new Term(""user_id"" u.getId()+"""")); BooleanQuery innerBooleanQuery = new BooleanQuery(); innerBooleanQuery.add(new BooleanClause(name_query Occur.SHOULD)); innerBooleanQuery.add(new BooleanClause(desc_query Occur.SHOULD)); booleanQuery.add(new BooleanClause(userQuery  Occur.MUST)); booleanQuery.add(new BooleanClause(innerBooleanQuery Occur.MUST));"
815,A,"Problem with Lucene scoring I have a problem with Lucene's scoring function that I can't figure out. So far I've been able to write this code to reproduce it. package lucenebug; import java.util.Arrays; import java.util.List; import org.apache.lucene.analysis.SimpleAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.queryParser.QueryParser; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.Query; import org.apache.lucene.search.ScoreDoc; import org.apache.lucene.search.TopDocs; public class Test { private static final String TMP_LUCENEBUG_INDEX = ""/tmp/lucenebug_index""; public static void main(String[] args) throws Throwable { SimpleAnalyzer analyzer = new SimpleAnalyzer(); IndexWriter w = new IndexWriter(TMP_LUCENEBUG_INDEX analyzer true); List<String> names = Arrays .asList(new String[] { ""the rolling stones"" ""rolling stones (karaoke)"" ""the rolling stones tribute"" ""rolling stones tribute band"" ""karaoke - the rolling stones"" }); try { for (String name : names) { System.out.println(""#name: "" + name); Document doc = new Document(); doc.add(new Field(""name"" name Field.Store.YES Field.Index.TOKENIZED)); w.addDocument(doc); } System.out.println(""finished adding docs total size: "" + w.docCount()); } finally { w.close(); } IndexSearcher s = new IndexSearcher(TMP_LUCENEBUG_INDEX); QueryParser p = new QueryParser(""name"" analyzer); Query q = p.parse(""name:(rolling stones)""); System.out.println(""--------\nquery: "" + q); TopDocs topdocs = s.search(q null 10); for (ScoreDoc sd : topdocs.scoreDocs) { System.out.println("""" + sd.score + ""\t"" + s.doc(sd.doc).getField(""name"").stringValue()); } } } The output I get from running it is: finished adding docs total size: 5 -------- query: name:rolling name:stones 0.578186 the rolling stones 0.578186 rolling stones (karaoke) 0.578186 the rolling stones tribute 0.578186 rolling stones tribute band 0.578186 karaoke - the rolling stones I just can't understand why the rolling stones has the same relevance as the rolling stones tribute. According to lucene's documentation the more tokens a field has the smaller the normalization factor should be and therefore the rolling stones tribute should have a lower score than the rolling stones. Any ideas? What Lucene version are you using (you link the API doc of 2.4)? In Lucene 2.9 scores are not returned by default you have to provide a TopFieldCollector: http://www.gossamer-threads.com/lists/lucene/java-user/86309 @gossamer: I've run the same code against Lucene 2.3 2.4 and 2.9. Same results. The length normalization factor is calculated as 1 / sqrt(numTerms) (You can see this in DefaultSimilarity This result is not stored in the index directly. This value is multiplied by the boost value for the field specified. The final result is then encoded in 8 bits as explained in Similarity.encodeNorm() This is a lossy encoding which means fine details get lost. If you want to see length normalization in action try creating document with following sentence. the rolling stones tribute a b c d e f g h i j k This will create sufficient difference in the length normalization values which you could see. Now if your field have very few tokens as per the examples you have used you could set boost values for the documents/fields based on your own formula which is essentially higher boost for short field. Alternatively you could create custom Similarity and override legthNorm() method. That's right the 8-bit encoding was rounding up the `boost*lengthNorm` and causing the problem. Setting the field boost to 100 during indexation is a clean enough workaround for me. @Shashikant Kore Thanks!  I can reproduce it on Lucene 2.3.1 but do not know why this happens."
816,A,"Anyways of making a Lucene field stored as well as streamed through a reader Is there a way of creating a field in Lucene which can accept a reader or InputStream and also store the content of it? I want to store the data so that it can be used at the time of Highlighting and I want to stream the data because the content of the documents might be really large. I do not see a constructor for a field that allows me to use a reader as well as lets me store the value. Thanks If your documents are not too large just read them into memory first and then specify the resulting value as a String when adding a new Field. If the documents are large split them into manageable chunks and perform the above operation on each chunk. Make sure to use the same field name for each chunk so that Lucene will search over all values. Example: IndexWriter writer = ... String id = ... String[] lines = ... Document doc = new Document(); doc.add(new Field(""id"" id Store.YES Index.NOT_ANALYZED TermVector.NO ); for (String line: lines) { doc.add(new Field(""text"" line Store.YES Index.ANALYZED TermVector.WITH_POSITIONS); } writer.addDocument(doc); Lucene will automatically merge the values specified at each add with the same field name so that it can search over the pooled set. @sapan I added some sample code to the answer above. Thanks a lot it seems to be working. But wouldn't I get into issue when I want to show a cached copy or want to do highlighting? When you say keep the same field name what do you mean by it? Create multiple fields with the same ""name"" and add it to documents? Hello thanks for your useful information. What I still don't understand: why it is working like this. Because when I try to add some large elements as strings directly it fails because of the memory limits but this way it works despite I hold all the data in memory. I inspected the Lucene source code and I didn't find any kind of swap. What am I missing?"
817,A,splitting lucene index into two halves what is the best way to split an existing Lucene index into two halves i.e. each split should contain half of the total number of documents in the original index Just for clarity: you'd like to split the index *without* reindexing the documents right? yes you are right! I do not want to re-read the index and again use IndexWriter to buld two indices. Rather some automatic way of doing this seeked Can you provide some business rationale for this? The easiest way would be to read and loop through IndexReader class. A fairly robust mechanism is to use a checksum of the document modulo the number of indexes to decide which index it will go into.  Recent versions of Lucene have a dedicated tool to do this (IndexSplitter and MultiPassIndexSplitter under contrib/misc).  The easiest way to split an existing index (without reindexing all the documents) is to: Make another copy of the existing index (i.e. cp -r myindex mycopy) Open the first index and delete half the documents (range 0 to maxDoc / 2) Open the second index and delete the other half (range maxDoc / 2 to maxDoc) Optimize both indices This is probably not the most efficient way but it requires very little coding to do.
818,A,"Faster way to get distinct values from Lucene Query Currently I do like this: IndexSearcher searcher = new IndexSearcher(lucenePath); Hits hits = searcher.Search(query); Document doc; List<string> companyNames = new List<string>(); for (int i = 0; i < hits.Length(); i++) { doc = hits.Doc(i); companyNames.Add(doc.Get(""companyName"")); } searcher.Close(); companyNames = companyNames.Distinct<string>().Skip(offSet ?? 0).ToList(); return companyNames.Take(count??companyNames.Count()).ToList(); As you can see I first collect ALL the fields (several thousands) and then distinct them possibly skip some and take some out. I feel like there should be a better way to do this. I'm not sure there is honestly as Lucene doesn't provide 'distinct' functionality. I believe with SOLR you can use a facet search to achieve this but if you want this in Lucene you'd have to write some sort of facet functionality yourself. So as long as you don't run into any performance issues you should be ok this way. Ok thanks for letting me know.  public List<string> GetDistinctTermList(string fieldName) { List<string> list = new List<string>(); using (IndexReader reader = idxWriter.GetReader()) { TermEnum te = reader.Terms(new Term(fieldName)); if (te != null && te.Term != null && te.Term.Field == fieldName) { list.Add(te.Term.Text); while (te.Next()) { if (te.Term.Field != fieldName) break; list.Add(te.Term.Text); } } } return list; }  I suggest you to find a logic to skip this kind of iteration but if there is no solution in your context then you can get a performance gain with the following code 1) at Index time it is best to put the field that you want to iterate in first field Document doc = new Document(); Field companyField = new Field(...); doc.Add(companyField); ... 2) then you need to define a FieldSelector like this class CompanyNameFieldSelector : FieldSelector { public FieldSelectorResult Accept(string fieldName) { return (fieldName == ""companyName"" ? FieldSelectorResult.LOAD_AND_BREAK : FieldSelectorResult.NO_LOAD); } } 3) Then when you want to iterate and pick this field you should do something like this FieldSelector companySelector = new CompanyNameFieldSelector(); // when you iterate through your index doc = hits.Doc(i); doc.Get(""companyName"" companySelector); The performance of above code is much better than the code you provided cause it skip reading unnecessary document fields and save time.  Tying this question to an earlier question of yours (re: ""Too many clauses"") I think you should definitely be looking at term enumeration from the index reader. Cache the results (I used a sorted dictionary keyed on the field name with a list of terms as the data to a max of 100 terms per field) until the index reader becomes invalid and away you go. Or perhaps I should say that when faced with a similar problem to yours that's what I did. Hope this helps Could you elaborate on what you mean with ""Term Enumeration""? Do you mean enumerating all my documents and getting those fields so I can use C#'s StartsWith()? +1 for seeing the question behind the question Have a look at the Terms member function of the IndexReader class. BTW I found out a good deal about this kind of thing by having a look at the Luke source code. Very interesting! I'm not a big fan of Luke actually. I don't know why but it takes ages for each query to parse. Way slower then my own queries."
819,A,"Lucene: assigning custom ids? Can I assign my custom ids to Lucene indexed documents instead of automatically generate new ids ? I'm asking this because I already have ids in my collection. A specific field is used for that. thanks Yes - in fact this is the only way to do it. (Lucene can't generate IDs for you. The thing that it calls ""doc IDs"" are internal only and are subject to change at Lucene's whim.) You would just have a field called ""myID"" or whatever and mark it as stored. ok I see. So I use the internal ""doc"" id to get the document and print a specific field (in my case ""myId"") So I've added the myID field but how can I replace these ids ""doc"" I see in the results: [doc=0 score=10.122469 doc=1 score=3.5992513 doc=11 score=0.78047055] ? @Patrick: Those are internal Lucene IDs and you should ignore them. To get your field's value just use `doc.GetField(""myID"")` These results are the output of topDocs = searcher.search(booleanQuery 220000); topDocs.scoreDocs; How can I get the array of scoreDocs with my custom ids instead of the inner ones ? @Patrick: See any Lucene tutorial. For example step #4 in [Lucene in 5 minutes](http://www.lucenetutorial.com/lucene-in-5-minutes.html) will tell you this."
820,A,"Solr question about exact word search I have a question which closely relates to the below Solr - one word phrase search to avoid stemming In my schema I have a field <field name=""text"" type=""textgen"" indexed=""true"" stored=""true"" required=""true""/> This gives an exact match ie. stemming disabled eat = eat Is it possible while configured to textgen to search for other variants of the word eg. eat = eat eats eating eat~0 will give similar sounding words such as meat beat etc. but this is not what I want. I'm starting to think that the only way to acheive this is to add another field with somethign other then textgen but if there is a simpler way I am very interested to hear Thanks in advance Ruth If you use text as the field type then eat eats eaten and eating will all be stored as eat and a search for FieldName:eat will find all of them. If you change the field type to text-gen then the search for FieldName:eat will only find ""eat"" not eats eaten or eating.  Using copyfield statements is the normal approach in Solr. Since stemming is the answer to exactly what you're asking this is what I recommend you to use. You can set stored=false if you are worried about index size. You might also use lemmatisation which is the opposite of stemming - where you instead add a words all inflected forms. This is typically performed on the search query expanding e.g. eat to eat eats eating etc. The third alternative might be to use wildcard search although I wouldn't encourage it. Not least since it bypasses all schema configured filters for the target field."
821,A,"Calculate the score only based on the documents have more occurance of term in lucene I am started working on resume retrieval(document) component based on lucene.net engine. It works great and it fetches the document and score it based on the the idea behind the VSM is the more times a query term appears in a document relative to the number of times the term appears in all the documents in the collection the more relevant that document is to the query. Lucene's Practical Scoring Function is derived from the below. score(qd)=coord(qd)·queryNorm(q)· ∑( tf(t in d) ·idf(t)2 · t.getBoost() · norm(td) ) t in q in this tf(t in d) correlates to the term's frequency defined as the number of times term t appears in the currently scored document d. Documents that have more occurrences of a given term receive a higher score idf(t) stands for Inverse Document Frequency. This value correlates to the inverse of docFreq (the number of documents in which the term t appears). This means rarer terms give higher contribution to the total score. This is very great indeed in most of the situation but due to the fieldnorm calculation the result is not accurate fieldnorm aka ""field length norm"" value represents the length of that field in that doc (so shorter fields are automatically boosted up). Due to this we didn't get the accurate results. Say for an example i got 10000 documents in which 3000 documents got java and oracle keyword. And the no of times it appears vary on each document. assume doc A got 10 java 20 oracle among 1000 words and doc B got 2 java 2 oracle among 50 words if am searching for a query ""java and oracle"" lucene returns doc B with high score due to the length normalization. Due to the nature of the business we need to retrieve the documents got more search keyword occurrence should come first we don't really care about the length of the document. Because of this a Guy with a big resume with lot of keywords is been moved below in the result and some small resumes came up. To avoid that i need to disable length normalization. Can some one help me with this?? I have attached the Luke result image for your reference. In this image document with java 50 times and oracle 6 times moved down to 11 th position. But this document with java 24 times and oracle 5 times is a top scorer due to the fieldnorm. Hope i conveyed the info clear... If not please ask me i ll give more info You can disable length normalization with Field.setOmitNorms(true) long question for such a short answer ;) @serg10 key is so small for big vehicle but you cant start without it :) thanks Shashikant i ll try that.. :) Shasi it works as expected... you saved my day...... What are the cons of disabling filed norm?"
822,A,Tips/recommendations for using Lucene I'm working on a job portal using asp.net 3.5 I've used Lucene for job and resume search functionality. Would like to know tips/recommendations if any with respect to Lucene performance optimization scalability etc. Thanks a ton! One thing you should keep in mind is that it is very hard to cluster or replicate lucene indexes in large installations like fail over scenarios or distributed systems. So you should either have a good way to replicate your index jobs or the whole database.  I've documented how I used Lucene.NET (in BugTracker.NET) here: http://www.ifdefined.com/blog/post/2009/02/Full-Text-Search-in-ASPNET-using-LuceneNET.aspx  If you use a sort watch out for the size of the comparators. When sorts are used for each document returned by the searcher there will be a comparator object stored for each SortField in the Sort object. Depending on the size of the documents and the number of fields you want to sort on this can become a big headache.
823,A,"Indexing .PDF .XLS .DOC .PPT using Lucene.NET I've heard of Lucene.Net and I've heard of Apache Tika. The question is - how do I index these documents using C# vs Java? I think the issue is that there is no .Net equivalent of Tika which extracts relevant text from these document types. UPDATE - Feb 05 2011 Based on given responses it seems that the is not currently a native .Net equivalent of Tika. 2 interesting projects were mentioned that are each interesting in their own right: Xapian Project (http://xapian.org/) - An alternative to Lucene written in unmanaged code. The project claims to support ""swig"" which allows for C# bindings. Within the Xapian Project there is an out-of-the-box search engine called Omega. Omega uses a variety of open source components to extract text from various document types. IKVM.NET (http://www.ikvm.net/) - Allows Java to be run from .Net. An example of using IKVM to run Tika can be found here. Given the above 2 projects I see a couple of options. To extract the text I could either a) use the same components that Omega is using or b) use IKVM to run Tika. To me option b) seems cleaner as there are only 2 dependencies. The interesting part is that now there are several search engines that could probably be used from .Net. There is Xapian Lucene.Net or even Lucene (using IKVM). UPDATE - Feb 07 2011 Another answer came in recommending that I check out ifilters. As it turns out this is what MS uses for windows search so Office ifilters are readily available. Also there are some PDF ifilters out there. The downside is that they are implemented in unmanaged code so COM interop is necessary to use them. I found the below code snippit on a DotLucene.NET archive (no longer an active project): using System; using System.Diagnostics; using System.Runtime.InteropServices; using System.Text; namespace IFilter { [Flags] public enum IFILTER_INIT : uint { NONE = 0 CANON_PARAGRAPHS = 1 HARD_LINE_BREAKS = 2 CANON_HYPHENS = 4 CANON_SPACES = 8 APPLY_INDEX_ATTRIBUTES = 16 APPLY_CRAWL_ATTRIBUTES = 256 APPLY_OTHER_ATTRIBUTES = 32 INDEXING_ONLY = 64 SEARCH_LINKS = 128 FILTER_OWNED_VALUE_OK = 512 } public enum CHUNK_BREAKTYPE { CHUNK_NO_BREAK = 0 CHUNK_EOW = 1 CHUNK_EOS = 2 CHUNK_EOP = 3 CHUNK_EOC = 4 } [Flags] public enum CHUNKSTATE { CHUNK_TEXT = 0x1 CHUNK_VALUE = 0x2 CHUNK_FILTER_OWNED_VALUE = 0x4 } [StructLayout(LayoutKind.Sequential)] public struct PROPSPEC { public uint ulKind; public uint propid; public IntPtr lpwstr; } [StructLayout(LayoutKind.Sequential)] public struct FULLPROPSPEC { public Guid guidPropSet; public PROPSPEC psProperty; } [StructLayout(LayoutKind.Sequential)] public struct STAT_CHUNK { public uint idChunk; [MarshalAs(UnmanagedType.U4)] public CHUNK_BREAKTYPE breakType; [MarshalAs(UnmanagedType.U4)] public CHUNKSTATE flags; public uint locale; [MarshalAs(UnmanagedType.Struct)] public FULLPROPSPEC attribute; public uint idChunkSource; public uint cwcStartSource; public uint cwcLenSource; } [StructLayout(LayoutKind.Sequential)] public struct FILTERREGION { public uint idChunk; public uint cwcStart; public uint cwcExtent; } [ComImport] [Guid(""89BCB740-6119-101A-BCB7-00DD010655AF"")] [InterfaceType(ComInterfaceType.InterfaceIsIUnknown)] public interface IFilter { [PreserveSig] int Init([MarshalAs(UnmanagedType.U4)] IFILTER_INIT grfFlags uint cAttributes [MarshalAs(UnmanagedType.LPArray SizeParamIndex=1)] FULLPROPSPEC[] aAttributes ref uint pdwFlags); [PreserveSig] int GetChunk(out STAT_CHUNK pStat); [PreserveSig] int GetText(ref uint pcwcBuffer [MarshalAs(UnmanagedType.LPWStr)] StringBuilder buffer); void GetValue(ref UIntPtr ppPropValue); void BindRegion([MarshalAs(UnmanagedType.Struct)] FILTERREGION origPos ref Guid riid ref UIntPtr ppunk); } [ComImport] [Guid(""f07f3920-7b8c-11cf-9be8-00aa004b9986"")] public class CFilter { } public class IFilterConstants { public const uint PID_STG_DIRECTORY = 0x00000002; public const uint PID_STG_CLASSID = 0x00000003; public const uint PID_STG_STORAGETYPE = 0x00000004; public const uint PID_STG_VOLUME_ID = 0x00000005; public const uint PID_STG_PARENT_WORKID = 0x00000006; public const uint PID_STG_SECONDARYSTORE = 0x00000007; public const uint PID_STG_FILEINDEX = 0x00000008; public const uint PID_STG_LASTCHANGEUSN = 0x00000009; public const uint PID_STG_NAME = 0x0000000a; public const uint PID_STG_PATH = 0x0000000b; public const uint PID_STG_SIZE = 0x0000000c; public const uint PID_STG_ATTRIBUTES = 0x0000000d; public const uint PID_STG_WRITETIME = 0x0000000e; public const uint PID_STG_CREATETIME = 0x0000000f; public const uint PID_STG_ACCESSTIME = 0x00000010; public const uint PID_STG_CHANGETIME = 0x00000011; public const uint PID_STG_CONTENTS = 0x00000013; public const uint PID_STG_SHORTNAME = 0x00000014; public const int FILTER_E_END_OF_CHUNKS = (unchecked((int) 0x80041700)); public const int FILTER_E_NO_MORE_TEXT = (unchecked((int) 0x80041701)); public const int FILTER_E_NO_MORE_VALUES = (unchecked((int) 0x80041702)); public const int FILTER_E_NO_TEXT = (unchecked((int) 0x80041705)); public const int FILTER_E_NO_VALUES = (unchecked((int) 0x80041706)); public const int FILTER_S_LAST_TEXT = (unchecked((int) 0x00041709)); } /// /// IFilter return codes /// public enum IFilterReturnCodes : uint { /// /// Success /// S_OK = 0 /// /// The function was denied access to the filter file. /// E_ACCESSDENIED = 0x80070005 /// /// The function encountered an invalid handle probably due to a low-memory situation. /// E_HANDLE = 0x80070006 /// /// The function received an invalid parameter. /// E_INVALIDARG = 0x80070057 /// /// Out of memory /// E_OUTOFMEMORY = 0x8007000E /// /// Not implemented /// E_NOTIMPL = 0x80004001 /// /// Unknown error /// E_FAIL = 0x80000008 /// /// File not filtered due to password protection /// FILTER_E_PASSWORD = 0x8004170B /// /// The document format is not recognised by the filter /// FILTER_E_UNKNOWNFORMAT = 0x8004170C /// /// No text in current chunk /// FILTER_E_NO_TEXT = 0x80041705 /// /// No more chunks of text available in object /// FILTER_E_END_OF_CHUNKS = 0x80041700 /// /// No more text available in chunk /// FILTER_E_NO_MORE_TEXT = 0x80041701 /// /// No more property values available in chunk /// FILTER_E_NO_MORE_VALUES = 0x80041702 /// /// Unable to access object /// FILTER_E_ACCESS = 0x80041703 /// /// Moniker doesn't cover entire region /// FILTER_W_MONIKER_CLIPPED = 0x00041704 /// /// Unable to bind IFilter for embedded object /// FILTER_E_EMBEDDING_UNAVAILABLE = 0x80041707 /// /// Unable to bind IFilter for linked object /// FILTER_E_LINK_UNAVAILABLE = 0x80041708 /// /// This is the last text in the current chunk /// FILTER_S_LAST_TEXT = 0x00041709 /// /// This is the last value in the current chunk /// FILTER_S_LAST_VALUES = 0x0004170A } /// /// Convenience class which provides static methods to extract text from files using installed IFilters /// public class DefaultParser { public DefaultParser() { } [DllImport(""query.dll"" CharSet = CharSet.Unicode)] private extern static int LoadIFilter(string pwcsPath [MarshalAs(UnmanagedType.IUnknown)] object pUnkOuter ref IFilter ppIUnk); private static IFilter loadIFilter(string filename) { object outer = null; IFilter filter = null; // Try to load the corresponding IFilter int resultLoad = LoadIFilter(filename outer ref filter); if (resultLoad != (int) IFilterReturnCodes.S_OK) { return null; } return filter; } public static bool IsParseable(string filename) { return loadIFilter(filename) != null; } public static string Extract(string path) { StringBuilder sb = new StringBuilder(); IFilter filter = null; try { filter = loadIFilter(path); if (filter == null) return String.Empty; uint i = 0; STAT_CHUNK ps = new STAT_CHUNK(); IFILTER_INIT iflags = IFILTER_INIT.CANON_HYPHENS | IFILTER_INIT.CANON_PARAGRAPHS | IFILTER_INIT.CANON_SPACES | IFILTER_INIT.APPLY_CRAWL_ATTRIBUTES | IFILTER_INIT.APPLY_INDEX_ATTRIBUTES | IFILTER_INIT.APPLY_OTHER_ATTRIBUTES | IFILTER_INIT.HARD_LINE_BREAKS | IFILTER_INIT.SEARCH_LINKS | IFILTER_INIT.FILTER_OWNED_VALUE_OK; if (filter.Init(iflags 0 null ref i) != (int) IFilterReturnCodes.S_OK) throw new Exception(""Problem initializing an IFilter for:\n"" + path + "" \n\n""); while (filter.GetChunk(out ps) == (int) (IFilterReturnCodes.S_OK)) { if (ps.flags == CHUNKSTATE.CHUNK_TEXT) { IFilterReturnCodes scode = 0; while (scode == IFilterReturnCodes.S_OK || scode == IFilterReturnCodes.FILTER_S_LAST_TEXT) { uint pcwcBuffer = 65536; System.Text.StringBuilder sbBuffer = new System.Text.StringBuilder((int)pcwcBuffer); scode = (IFilterReturnCodes) filter.GetText(ref pcwcBuffer sbBuffer); if (pcwcBuffer > 0 && sbBuffer.Length > 0) { if (sbBuffer.Length < pcwcBuffer) // Should never happen but it happens ! pcwcBuffer = (uint)sbBuffer.Length; sb.Append(sbBuffer.ToString(0 (int) pcwcBuffer)); sb.Append("" ""); // ""\r\n"" } } } } } finally { if (filter != null) { Marshal.ReleaseComObject (filter); System.GC.Collect(); System.GC.WaitForPendingFinalizers(); } } return sb.ToString(); } } } At the moment this seems like the best way to extract text from documents using the .NET platform on a Windows server. Thanks everybody for your help. UPDATE - Mar 08 2011 While I still think that ifilters are a good way to go I think if you are looking to index documents using Lucene from .NET a very good alternative would be to use Solr. When I first started researching this topic I had never heard of Solr. So for those of you who have not either Solr is a stand-alone search service written in Java on top of Lucene. The idea is that you can fire up Solr on a firewalled machine and communicate with it via HTTP from your .NET application. Solr is truly written like a service and can do everything Lucene can do (including using Tika extract text from .PDF .XLS .DOC .PPT etc) and then some. Solr seems to have a very active community as well which is one thing I am not to sure of with regards to Lucene.NET. Other angle here is that Lucene indexes are binary compatible between java and .NET. So you could write the index with Tika and read it with C#.  You can also check out ifilters - there are a number of resources if you do a search for asp.net ifilters: http://www.codeproject.com/KB/cs/IFilter.aspx http://en.wikipedia.org/wiki/IFilters http://www.ifilter.org/ http://stackoverflow.com/questions/1535992/ifilter-or-sdk-for-many-file-types Of course there is added hassle if you are distributing this to client systems because you will either need to include the ifilters with your distribution and install those with your app on their machine or they will lack the ability to extract text from any files they don't have ifilters for. @dana I believe the code project link provides a wrapper. I think it's EPocalipse.IFilter.dll I am not distributing this to anybody. It would be for an application that my company would host. This looks interesting as it appears to be the underlying technology for windows search so you know MS will support Office formats. The only downside is you are using COM interop.  Apparently you can use Tika from .net (link) I have not tried this myself. Interesting. I have never heard about IKVM before but it sounds it could work. I supposed you could even use IKVM to run the Java version of Lucene? In any case thanks for the tip :)  This is one of the reasons I was dissatisfied with Lucene for a project I was working on. Xapian is a competing product and is orders of magnitude faster than Lucene in some cases and has other compelling features (well they were compelling to me at the time). The big issue? It's written in C++ and you have to interop to it. That's for indexing and retrieval. For the actual parsing of the text that's where Lucene really falls down -- you have to do it yourself. Xapian has an omega component that manages calling other third party components to extract data. In my limited testing it worked pretty darn well. I did not finish the project (more than POC) but I did write up my experience compiling it for 64 bit. Of course this was almost a year ago so things might have changed. If you dig into the Omega documentation you can see the tools that they use to parse documents. PDF (.pdf) if pdftotext is available (comes with xpdf) PostScript (.ps .eps .ai) if ps2pdf (from ghostscript) and pdftotext (comes with xpdf) are available OpenOffice/StarOffice documents (.sxc .stc .sxd .std .sxi .sti .sxm .sxw .sxg .stw) if unzip is available OpenDocument format documents (.odt .ods .odp .odg .odc .odf .odb .odi .odm .ott .ots .otp .otg .otc .otf .oti .oth) if unzip is available MS Word documents (.doc .dot) if antiword is available MS Excel documents (.xls .xlb .xlt) if xls2csv is available (comes with catdoc) MS Powerpoint documents (.ppt .pps) if catppt is available (comes with catdoc) MS Office 2007 documents (.docx .dotx .xlsx .xlst .pptx .potx .ppsx) if unzip is available Wordperfect documents (.wpd) if wpd2text is available (comes with libwpd) MS Works documents (.wps .wpt) if wps2text is available (comes with libwps) Compressed AbiWord documents (.zabw) if gzip is available Rich Text Format documents (.rtf) if unrtf is available Perl POD documentation (.pl .pm .pod) if pod2text is available TeX DVI files (.dvi) if catdvi is available DjVu files (.djv .djvu) if djvutxt is available XPS files (.xps) if unzip is available great write-up. It looks like they use something called ""swig"" to generate the c# binding. Is that new since you did your write-up or is that what you used? Also do you have to download the document processing tools separately (such as ""antiword"") or do they come with Xapian? @dana The win32 bindings and build files are here: http://www.flax.co.uk/xapian_binaries. They were using swig for Java when I looked at it but I didn't mess with that. Charlie Hull is a good guy and is on here if you run into any trouble. You can just download those tools separately -- they are all open source and use them with Lucene (which is what I was experimenting with when my project was pulled). So you could use antiword to extract the data out of a .doc file (by shelling a process in c#) and then stuffing that into Lucene."
824,A,Using Zend Lucene to search PDF files Is there a way to use Zend_Search_Lucene to search/index PDF documents? http://www.kapustabrothers.com/2008/01/20/indexing-pdf-documents-with-zend_search_lucene/ I don't know if it's only today but it gives me a broken link
825,A,lucene *.cfs numbers growth After every update of index in lucene *.cfs file number growth _2.cfs _5.cfs _7.cfs. Every time. Is it ok or maybe I forgot to close some objects in code or else? I guess that this is OK if you execute $index->optimize() after updating your index. Otherwise it is also OK but the quality of your index begins to corrupt. Thanks I call optimize and have only one file but it's name changed every time with incrementing number. that's OK. The thing with the incrementing number is unimportant  it seems that you need to Optimize the index it has its advantages 1) .CFS(Compound file) are merged into one big Segment file 2) MergeFactor is also a criteria why you have so much compound files set it to something around 15-20 we have found it giving good results in that range with decent memory usage
826,A,"How do I combine usage of db4o to store data and Lucene to index data for fast search? I'm new to both db4o and Lucene. Currently I'm using db4o to persist my data on an Android app. I need the capability to perform quick searches as well as provide suggestions to the user (e.g. auto complete suggestions). An SO poster mentioned using Lucene to index data and db4o to store it. Has anyone implemented this approach ? If yes I would appreciate if they share the overall approach? What are the alternatives? I would go to use lucene only as datastorage. no need for db4o or why would you use one? (just store the doc as json into a stored and none-indexed probably compressed field) I used Lucene to extract keywords from items to be stored in the database and store what I call 'keyword extension' objects that point to the corresponding domain objects. This made the domain objects findable by keyword (also allowing for stemming) and separated the keywords concerns. The database was built from a large static dataset (the USDA food nutrient database) so I didn't need to worry about changes during runtime. Thus this solution is limited in its current form ... The first part of the solution was to write a small chunk of code that takes some text and extracts both the keywords and corresponding stems (using Lucene's 'Snowball' stemming) into a map. You use this to extract the keywords/stems from some domain objects that you are storing in the database. I kept the original keywords around so that I could create some sort of statistics on the searches made. The second part was to construct objects I called 'keyword extensions' that store the stems as an array and the corresponding keywords as another array and have a pointer to the corresponding domain objects that had the keywords (I used arrays because they work more easily with DB4O). I also subclassed my KeywordExtension class to correspond to the particular domain objects's type - so for example I was storing a 'Nutrient' domain object and a corresponding 'NutrientKeywordExtension' object. The third part is to collect the user's entered search text again use the stemmer to extract the stems and search for the NutrientKeywordExtension objects with those stems. You can then grab the Nutrient objects that those extensions point to and finally present them as search results. As I said my database was static - it's created the first time the application runs. In a dynamic database you would need to worry about keeping the nutrients and corresponding keyword extensions in sync. One solution would be to merge the nutrient and nutrient keyword extension into one class if you don't mind having that stuff inside your domain objects (I don't like this). Otherwise you need to account for keyword extensions every time your create/edit/delete your domain objects. I hope this limited example helps. @Sam - thanks for responding. Can you give me an idea of the size of the index and how much time it took to build the initial index on the phone. @Soumyama the indexes in this case are embodied by the set of KeywordExtension objects. There is a lot more data in the database and I haven't worked out what space these particular objects take up. The majority of space I suspect is taken up the the 555726 nutrient entry objects in any case leading to a 45 MB database file. This is all on a Granite web application (Granite is our own open source Scala/Wicket/DB4O stack) not on a phone. It takes just over a minute on a 6-core desktop to generate the entire DB4O database from scratch. @Sam - that's helpful information. 45 MB is the DB4O db file size or the size of the Lucene index ? @Sam - thanks. Can you please tell me the size of the Lucene index ? @Soumya 45 MB is the total DB4O db file size @Soumya As I said before ""the indexes in this case are embodied by the set of KeywordExtension objects"" ... ""and I haven't worked out what space these particular objects take up. "". It is a fraction of the database size but without further work I don't know what fraction. All I can say is that there is here is one extension object per domain object. Not currently open source - it was really a test case for our Granite framework (which is open source). Will think about what to do with it. @Sam Is this an open source project? Are you going to release some code? The db4o community could benefit from what you did (it's awesome) @German - you can now see a basic web UI for the food nutrient database here: http://nutrients.ofthings.net/"
827,A,Problem with defining Solr_home Hallo I am configuring Solr. Everything ist working fine but one thing appears to be weird to me. I have Solr running on Tomcat. My Solr_home ist defined somewhere on my hard drive through JNDI. For some reason Solr creates on startup a new folder of solr inside of Tomcat's Webapps folder with the lib files and the index. That alone would make me think that I somehow defined Solr_home wrong but Solr is using the schema and config out of the real Solr_home. I just can't get any sense into that. Is that how Solr works or can anyone give me a hint on how I can force Solr to saving the index into my defined Solr_home? Have a look at setting the solr.data.dir. See here. The default is ./solr/data which I suppose is relative to the webapps path. Thank you for the quick answer. i just wasn't thinking off the config rerouting home
828,A,"Creating and updating Zend_Search_Lucene indexes I'm using Zend_Search_Lucene to create an index of articles to allow them to be searched on my website. Whenever a administrator updates/creates/deletes an article in the admin area the index is rebuilt: $config = Zend_Registry::get(""config""); $cache = $config->lucene->cache; $path = $cache . ""/articles""; try { $index = Zend_Search_Lucene::open($path); } catch (Zend_Search_Lucene_Exception $e) { $index = Zend_Search_Lucene::create($path); } $model = new Default_Model_Articles(); $select = $model->select(); $articles = $model->fetchAll($select); foreach ($articles as $article) { $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Text(""title"" $article->title)); $index->addDocument($doc); } $index->commit(); My question is this. Since I am reindexing the articles and handling deleted articles as well why would I not just use ""create"" every time (instead of ""open"" and update)? Using the above method I think the articles would be added with addDocument every time (so there would be duplicates). How would I prevent that? Is there a way to check if a Document exists already in the index? Also I don't think I fully understand how the indexing works when you ""open"" and update it. It seems to create new #.cfs (so I have _0.cfs _1.cfs _2.cfs) files in the index folder every time but when I use ""create"" it overwrites that file with a new #.cfs file with the # incremented (so for example just _2.cfs). Can you please explain what these segmented files are? Yes  you can check if a Document is already in the index have a look in this Manual Page. You can then delete this specific Document from the index via $index->delete($id); where $id is the return value of the termDocs method. After that you can simply add the new version of the Document. About the multiple index files that Lucene creates: Every time you modify an existing index Lucene does not realy change the existing files but adds partial indexes for every change you make. This is extremely bad for performance but there is a simple way around this. After every change you make to the index do this: $index->optimize(); - this will append all the partial files to the real index improving searchtimes dramatically. sold! thanks a lot."
829,A,How to index BigDecimal values in Lucene 3.0.1 I have some BigDecimal values which should be indexed for searching. Lucene has NumericField but it has setters only for long double float and int. I could store it as a String but then I would not benefit from NumericRangeQuery. How you have stored your BigDecimals? Any best practices to share? Steven Rowe provides interesting ideas in this post: http://www.lucidimagination.com/search/document/ad648772f8825a28/bigdecimal_values#2502f96055839c3d He says that his scheme could probably be used to represent all BigDecimal values. It seems easier to implement if you don't need negative values. Like mindas suggested you could extend AbstractField to implement this. There is also Yonik Seeley who says he has started some work in Solr for that with the class BCDUtils: http://www.lucidimagination.com/search/document/ad648772f8825a28/bigdecimal_values#cef1d0e25af063ef Hi the hyperlinks is not working.  If everything else fails considering extending AbstractField (similar to how NumericField extends it) TokenStream (similar to how NumericTokenStream extends it) and MultitermQuery (similar to how NumericRangeQuery extends it). All of three Numeric* classes are unfortunately final so they cannot be extended on their own :( The good news is that logic in these classes is fairly trivial and it should be easy to retrofit this for BigDecimals. The storing of data is trivial as even NumericField stores it in a String. From the javadoc: NOTE: This class is only used during indexing. When retrieving the stored field value from a Document instance after search you will get a conventional Fieldable instance where the numeric values are returned as Strings (according to toString(value) of the used data type). If you'll go that route try sending patch to Lucene developers or at least fill a JIRA request. Lucene devs are generally nice and open people so this could benefit others too.
830,A,"Should an index be optimised after incremental indexes in Lucene? We run full re-indexes every 7 days (i.e. creating the index from scratch) on our Lucene index and incremental indexes every 2 hours or so. Our index has around 700000 documents and a full index takes around 17 hours (which isn't a problem). When we do incremental indexes we only index content that has changed in the past two hours so it takes much less time - around half an hour. However we've noticed that a lot of this time (maybe 10 minutes) is spent running the IndexWriter.optimize() method. The LuceneFAQ mentions that: The IndexWriter class supports an optimize() method that compacts the index database and speeds up queries. You may want to use this method after performing a complete indexing of your document set or after incremental updates of the index. If your incremental update adds documents frequently you want to perform the optimization only once in a while to avoid the extra overhead of the optimization. ...but this doesn't seem to give any definition for what ""frequently"" means. Optimizing is CPU intensive and VERY IO-intensive so we'd rather not be doing it if we can get away with it. How much is the hit of running queries on an un-optimized index (I'm thinking especially in terms of query performance after a full re-index compared to after 20 incremental indexes where say 50000 documents have changed)? Should we be optimising after every incremental index or is the performance hit not worth it? An optimize operation reads and writes the entire index which is why it's so IO intensive! The idea behind optimize operations is to re-combine all the various segments in the Lucene index into one single segment which can greatly reduce query times as you don't have to open and search several files per query. If you're using the normal Lucene index file structure (rather than the combined structure) you get a new segment per commit operation; the same as your re-indexes I assume? I think Matt has great advice and I'd second everything he says - be driven by the data you have. I would actually go a step further and only optmize a) when you need to and b) when you have low query volume. As query performance is intimately tied to the number of segments in your index a simple ls -1 index/segments_* | count could be a useful indicator for when in optimization is really needed. Alternatively tracking the query performance and volume and kicking off an optimize when you reach unacceptable low performance with acceptably low volume would be a nicer solution.  In this mail Otis Gospodnetic advices against using optimize if your index is seeing constant updates. It's from 2007 but calling optimize() is in it's very nature an IO-heavy operation. You could consider using a more stepwise approach; a MergeScheduler  Mat since you seem to have a good idea how long your current process takes I suggest that you remove the optimize() and measure the impact. Do many of the documents change in those 2 hour windows? If only a small fraction (50000/700000 is about 7%) are incrementally re-indexed then I don't think you are getting much value out of an optimize(). Some ideas: Don't do an incremental optimize() at all. My experience says you are not seeing a huge query improvement anyway. Do the optimize() daily instead of 2-hourly. Do the optimize() during low-volume times (which is what the javadoc says). And make sure you take measurements. These kinds of changes can be a shot in the dark without them. These kinds of changes *are* shots in the dark without them. Cheers guess I was wondering whether people had experience of this before I dived in and started messing with a production system :) Mat: yes I realize you were looking for specific advice and I was being a little general. In my experience (I've been using Lucene for years) you will be fine without the optimize(). I've out-right removed the optimize() from on of our systems because of its overhead. @MattQuail so you never optimise the index even after full re-runs?"
831,A,"PatternTokenizerFactory and stopwords an document field in solr/lucene called COLORS has group of words like this: field1: blue/dark red/green field2: blue/yellow/orange [...] I need to run an faceted search over that to get all the colors and the count of each color. First I tried the PatternTokenizerFactory followd by the stopword-list: <analyzer> <tokenizer class=""solr.PatternTokenizerFactory"" pattern=""/"" /> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.TrimFilterFactory"" /> <filter class=""solr.StopFilterFactory"" ignoreCase=""true"" words=""stopwords"" enablePositionIncrements=""true"" /> </analyzer> Unfortunately the stopword list seams to be ignored. Stopwords are showing up in faceted search result. This SO question describes the same problem. Unfortunately the posted solution doen't work for me because i can not use the solr.StandardTokenizerFactory because the standard tokenizer also split tokens on whitspaces. That means ""dark red"" becomes ""dark"" and ""red"" which is wrong. Is there any way to use the pattern tokenizer? Thnak you for any kind of help! For your information: facet pattern tokenizer and stopwords will work in lucene / solr 4 :-)"
832,A,"solrj: how to store and retrieve List via multivalued field in index My use case is an index which holds titles of online media. The provider of the data associates a list of categories with each title. I am using SolrJ to populate the index via an annotated POJO class e.g. @Field(""title"") private String title; @Field(""categories"") private List<Category> categoryList; The associated POJO is public class Category { private Long id; private String name; ... } My question has two parts: a) is this possible via SolrJ - the docs only contain an example of @Field using a List of String so I assume the serialization/marshalling only supports simple types ? b) how would I set up the schema to hold this. I have a naive assumption I just need to set multiValued=true on the required field & it will all work by magic. I'm just starting to implement this so any response would be highly appreciated. The answer is as you thought: a) You have only simple types available. So you will have a List of the same type e.g. String. The point is you cant represent complex types inside the lucene document so you wont deserialize them as well. b) The problem is what you are trying is to represent relational thinking in a ""document store"". That will probably work only to a certain point. If you want to represent categories inside a lucene document just use the string it is not necessary to store a id as well. The only point to store an id as well is: if you want to do aside the search a lookup on a RDBMS. If you want to do this you need to make sure that the id and the category name is softlinked. This is not working for every 1:n relation. (Every 1:n relation where the n related table consists only of required fields is possible. If you have an optional field you need to put something like a filling emptyconstant in the field if possible). However if these 1:n relations are not sparse its possible actually if you maintain the order in which you add fields to the document. So the case with the category relation can be probably represented if you dont sort the lists. You may implement a method which returns this Category if you instantiate it with the values at position 0...n. So the solution would be if you want to have the first category it will be at position 0 of every list related to this category. Thanks for the answer. V. useful. :-)"
833,A,"Full Text Search like Google I would like to implement full-text-search in my off-line (android) application to search the user generated list of notes. I would like it to behave just like Google (since most people are already used to querying to Google) My initial requirements are: Fast: like Google or as fast as possible having 100000 documents with 200 hundred words each. Searching for two words should only return documents that contain both words (not just one word) (unless the OR operator is used) Case insensitive (aka: normalization): If I have the word 'Hello' and I search for 'hello' it should match. Diacritical mark insensitive: If I have the word 'así' a search for 'asi' should match. In Spanish many people incorrectly either do not put diacritical marks or fail in correctly putting them. Stop word elimination: To not have a huge index meaningless words like 'and' 'the' or 'for' should not be indexed at all. Dictionary substitution (aka: stem words): Similar words should be indexed as one. For example instances of 'hungrily' and 'hungry' should be replaced with 'hunger'. Phrase search: If I have the text 'Hello world!' a search of '""world hello""' should not match it but a search of '""hello world""' should match. Search all fields (in multifield documents) if no field specified (not just a default field) Auto-completion in search results while typing to give popular searches. (just like Google Suggest) How may I configure a full-text-search engine to behave as much as possible as Google? (I am mostly interested in Open Source Java and in particular Lucene) If you're pointed towards Lucene which handles many of these out of the box and is extensible can you be specific about what problem you're having? What are you searching? You do realize that Google employs http://en.wikipedia.org/wiki/Query_expansion and utilizes search history by other people. If everyone who searches for ""computer mice"" then end up clicking on yahoo.com (making it up) then over time yahoo.com will become the first search result for ""computer mice"". How many words do you have in your application? Brute force may be quite enough. I think Lucene can address your requirements. You should also consider using Solr which has similar functionality and is much easier to set up. I will discuss each requirement separately using Lucene. I believe Solr has similar mechanisms. Fast: like Google or as fast as possible having 100000 documents with 200 hundred words each. This is a reasonable index size both for Lucene and Solr enabling retrieval at several tens of milliseconds per query. Searching for two words should only return documents that contain both words (not just one word) (unless the OR operator is used) You can do that using a BooleanQuery with MUST as default in Lucene. The next four requirements can be handled by customizing a Lucene Analyzer: Case insensitive (aka: normalization): If I have the word 'Hello' and I search for 'hello' it should match. A LowerCaseFilter can be used for this. Diacritical mark insensitive: If I have the word 'así' a search for 'asi' should match. In Spanish many people incorrectly either do not put diacritical marks or fail in correctly putting them. This requires Unicode normalization followed by diacritic removal. You can build a custom Analyzer for this. Stop word elimination: To not have a huge index meaningless words like 'and' 'the' or 'for' should not be indexed at all. A StopFilter removes stop words in Lucene. Dictionary substitution (aka: stem words): Similar words should be indexed as one. For example instances of 'hungrily' and 'hungry' should be replaced with 'hunger'. Lucene has many Snowball Stemmers. One of them may be appropriate. Phrase search: If I have the text 'Hello world!' a search of '""world hello""' should not match it but a search of '""hello world""' should match. This is covered by the Lucene PhraseQuery specialized query. As you can see Lucene covers all of the required functionality. To get a more general picture I suggest the book Lucene in Action The Apache Lucene Wiki or The Lucid Imagination Site.  Unless you buy a search engine you have Lucene Nutch Apache Solr and few others.  A lot of these behaviors are default for Lucene. The first (including all terms) is not but you can force this behavior by setting the default operator: MultiFieldQueryParser parser = new MultiFieldQueryParser(fields new StandardAnalyzer()); parser.setDefaultOperator(QueryParser.AND_OPERATOR); I know that items 2 4 and 6 are possible and IIRC they happen by default. I'm not sure about items 3 and 5 but Lucene offers a ton of customization options so I'd suggest implementing a proof-of-concept with your data to see if it meets these requirements as well.  Buy a Google Search Appliance. Or as the comments say use Lucene like you already mentioned. A Google search appliance is not a good answer for searching data in an offline Android application. The android bit was added 7 hours after I posted my answer. 3 years later the search landscape has changed drastically.  HyperSQL is a pure-java SQL implementation that can be ran quite easily as can SQLite. You could use their full-text capabilities and querying to re-create the wheel but as the other commenters have pointed out an existing implementation is probably best."
834,A,Lucene / Solr: what request handlers to use for query strings in Chinese or Japanese? For my Solr server some of the query strings will be in Asian languages such as Chinese or Japanese. For such query strings would the Standard or Dismax request handler work? My understanding is that both the Standard and the Dismax handler tokenize the query string by whitespace. And that wouldn't work for Chinese or Japanese right? In that case what request handler should I use? And if I need to set up custom request handlers for those languages how do I do it? Thanks. Your queries will be parsed according to the analyzers of the fields you're querying whether you're using the standard Solr query parser or DisMax query parser. So in this case as Mauricio says the question is about how your strings of text are analyzed into tokens. For Chinese and Korean there is CJK which performs basic N-Gram analysis to break down text into byte pairs. It's not the best way to analyze in terms of relevance and index size but it works. For Japanese I highly recommend the new Kuromoji morphological analyzers in Solr and Lucene 3.6.0. It uses a dictionary and some other statistics to tokenize into real terms. That lets you do all sorts of really excellent quality Docs are sparse at the moment so check out these links… Kuromoji - Japanese morphological analyzer LUCENE-3305 Sample schema using Kuromoji analyzers on Websolr My presentation at the 20 Apr 2012 #herokujp meetup on full-text search with an emphasis on analyzing Japanese.  It's not about the request handler but the language analyzers. Lucene has a CJK package for this purpose. See here for info on using it in Solr. See also this thread for alternatives. see my comment for a bit more info on Japanese analysis :)
835,A,Lucene 2.2 arabic analyzer Is it possible to modify Lucene 2.2 to add Arabic analyzer and if anyone have done this already where can I get source/jar someone asked me before how to get arabic and persian support on lucene 2.4 so these were unofficially backported here: http://people.apache.org/~rmuir/ http://people.apache.org/~rmuir/lucene-analyzers-2.4.1_with_arabic_and_farsi.jar http://people.apache.org/~rmuir/arabicFarsiLucene241_contrib.patch http://people.apache.org/~rmuir/arabicFarsiLucene241_core.patch this would mean you only have to upgrade to 2.4.1 which might be easier than upgrading to 2.9 or 3.0. hope this helps  Alternatively you can try using lucene-hunspell for an analyzer. This is currently working with the Lucene trunk - I do not know whether it works with Lucene 3.0.1. Here is Robert Muir's explanation and a list of dictionaries including Arabic. I believe you could also back-port this. Shashikant's suggestion seems easier to implement while this one may be better quality.  Lucene 3.0.1 has Arabic Analyzer. It is in the contrib package. You can upgrade to Lucene 3.0.1 to get this working out of the box. You probably will not be able to use this as it is for Lucene 2.2 since TokenStream APIs have changed in this release. But back-porting changes to 2.2 shouldn't be very difficult in case you don't wish to migrate to latest Lucene release. the reason I thought of just adding Arabic analyzer to Lucene 2.2 and not upgrading to latest release is that I have to replace all the deprecated methods as it throws RuntimeException but in the end I guess I'll stick with the migration at latest release for maintainability reasons as I don't want to build my own jar every time a new feature in Lucene released
836,A,Effective search on a small text i have many small texts (lets say about 500 words) and 2 databases with roughly 10.000 entries each (keywords). i now want to process every text and find out which keywords (the ones saved in the 2 databases) are contained in the text. does anyone of you have a good approach on how to do this effectively? i wanted to process every text and index it (with lucene perhaps) before searching the database against ist. but i dont really know if lucene is the right tool for this. Your text needs to be indexed in some manner in order to search against it. You have two options: 1) Load your texts into a MySQL db and make the field/column full text searchable 2) As you say index with Lucene. Then read your keywords into a list loop over them and query against Lucene/MySQL. Give that your data sets are not large I would go with MySQL - it'll be much faster to set up.  Lucene is exactly the right tool for this task. One way to achieve your goal would be to use a RAMDirectory to index each text and then get the TermEnum from the index using the IndexReader. You can now match the terms against the keywords in your DB. Another approach would be to index every text as lucene document and then iterate over your keywords and get the termDocs for the current term => all texts that contain the current term/keyword. @Nicolas: I think what you mean is the TermFreqVector. The TermEnum gives you all terms in the index => all terms in the document you indexed using RAMDirectory. thanks i already thought about your first approach usind a ramdisk. but why do you recommend 'termenum'. as i understand it termenum comes in handy when you need the frequency of a given term in a text
837,A,FastVectorhighlighter with External Database I am using Lucene.NET 2.9 with one of my projects. I am using Lucene to create indexes for documents and search on those documents. A field in my document is text heavy and I have stored that into my MS SQL Database. So basically I search via lucene on its indexes and then fetch complete documents from MS SQL database. The problem I am facing is that I want to highlight my search query terms in results. For that I am using FastVectorHighlighter. Now this particular highlighter required Lucence DocId and field to highlight fields. The problem is that this particular text heavy field since is not stored in lucene database is not highlighted in my search results. Any suggestion on how to accomplish same. I either add the same field to my lucene database. It will resolve the problem but would make my database very heavy. Secondly if there is some alternative method to highlight the text it will give me very high flexibility. Thank you for reading question Naveen if you dont want to store the text in the Lucene index you should use the Highlighter contrib. Latest sources for it can be grabbed at https://svn.apache.org/repos/asf/incubator/lucene.net/trunk/src/contrib/Highlighter/ what all kind of queries does this support? and how is performance compared to fastvectorhighlight. Also does this need offset condition while creating index as required with fastvectorhighlight ? it supports almost all queries if I remember correctly the only one I had to implement support for was the FuzzyQuery. You dont need offsets in the index. It is significantly slower than FastVectorHighlighter tho since you need to retrieve the text from another source and tokenize it to get it Highlighted when I am searching for a phrase the highlighter adds formatting tags around each term rather than against the whole query. Is it possible to make it add formatting tags against the whole phrase. is SimpleSpanFragmenter not part of .NET port of lucene. I couldnt find it in any of the contrib modules. yeah I guess it would be much easier for me to do it on my client(via regex + js). Since at a given time I have about 20-30 results with each results having 300 characters I guess it would be easier. Do you feel this approach is right ? the suggested approach by the Highlighter package author is to postprocess phrases to merge the highlighted terms : http://mail-archives.apache.org/mod_mbox/lucene-java-user/200906.mbox/%3c4A27AEDC.6040902@gmail.com%3e Thanks ! Any idea about SimpleSpanFragmenter. I didnt find it in .net port I dont see it on the SVN it probably was not ported yet. Porting it should be simple tho it seems to be a small class doing the post processing on the client is in my opinion a good approach
838,A,"Is Lucene.Net suitable as the search engine for frequently changing content? Is Lucene.Net suitable as the search engine for frequently changing content? Or more specificically can anybody give a subjective opinion on how quickly lucene.net indexes can be updated. Any other approaches to searching frequently changing content would be great. We’re developing a forum. Forum posts will be frequently added to the forum repository. We think we need these posts to be added to lucene index very quickly (<0.5s) to become available to search. There’ll be about 5E6 posts in the repository initially. Assume search engine running on non-exotic server (I know this is very vague!). Other suggestions with regard to addressing the issue of searching frequently changing content appreciated. The forum posts need to be searchable on a variable number of named tags (tag name and value must match). A SQL based approach (based on Toxi schema) isn’t giving us the performance we’d like. Even though the question has already been answered you might want to consider a server to handle search (if out-of-process is an option) something like [Solr](http://lucene.apache.org/solr/) or [elasticsearch](http://www.elasticsearch.org/); both handle the creation and management of indexes very well as well as take care of things such as replication sharding etc which are important when dealing with large/multiple indexes. Out forums (http://episteme.arstechnica.com) use Lucene as the search backend so it's doable. Posts aren't indexed quite as quickly as you'd like but we could solve that by beefing up the indexing hardware and using a smarter caching strategy. The general answer to this question is: it depends what your write/update pattern is. Forums are relatively easy since most content is new and existing content is updated less frequently. For a forum I'd recommend having an ""archive"" index and a ""live"" index. The live index might include posts from the last day week year while the archive index will include a large body of posts that probably won't ever be touched again. So when someone creates a new post it will initially be indexed in the live index. At a later time some batch job would clear out the live index and reindex everything into the archive. Lucene's very good at querying across multiple indexes. You should abuse that ability. :)  Lucene.Net is extremely fast however there are many things that can slow down queries when used wrong. I strongly recommend reading the Lucene in Action book by Erik Hatcher and Otis Gospodnetić. It contains a very good chapter about performance testing and tuning."
839,A,"Search for short words with SOLR I am using SOLR along with NGramTokenizerFactory to help create search tokens for substrings of words NGramTokenizer is configured with a minimum word length of 3 This means that I can search for e.g. ""unb"" and then match the word ""unbelievable"". However I have a problem with short words like ""I"" and ""in"". These are not indexed by SOLR (I suspect it is because of NGramTokenizer) and therefore I cannot search for them. I don't want to reduce the minimum word length to 1 or 2 since this creates a huge search index. But I would like SOLR to include whole words whose length is already below this minimum. How can I do that? /Carsten First of all try to understand why your words don't get indexed by solr using the ""Analysis Tool"" http://localhost:8080/solr/admin/analysis.jsp Just put the field and the text you are searching for and see which analyser is filtering your short term. I suggest you to do so because you said you have only a ""suspect"" and you have to be certain about which analyser filters your data. Then why don't you just simply copy the term in another field without that analyser? In this way your terms will be indexed twice and will appear both as exact word and as n-gram. Then you have to deal with the scores of the two different fields. I hope this has helped you in some way. Some link for aggregation and copyfield attribute: Indexing data in multiple fields Using copy field tag Thanks for your suggestion. I have run the analysis against two words: A normal case - ""jeudan"" and the 1-letter word ""j"". Here are the results http://pastie.org/1000520 As you can see it IS actually the NGramTokenizer that is filtering out the 1-letter word - or in this the EdgeNGramTokenizer but I have tested with both. I could try what you suggest but I would rather let Solr do all the text-munging. I do a lot of field-specific searches so your suggestion would result in the need to rewrite those queries to look in two text-fields instead of one. Possible but counter-intuitive. Consider that it's typical in solr to have an aggregation field where you make the query and then a series of fields with different types and analyser. Simply use the copyfield tag to copy all your source field to the target. You don't have to change your queries. Well your answer actually solved this and other problems that I faced. I didn't know about the analysis tool. I ended up trying a few other filters and tokenizers through the analyser and ended up using the PhoneticFilter on both the index and query part. Very neat - thanks a lot!"
840,A,"What's a recommended strategy to replace ord() and rord() function queries in Solr? I'm using the rord() function in Solr queries in order to boost query results against a ""rank"" field using a syntax something like this: bf=rord(cur_rank)^1.8 The algorithm works well but recent changes in Solr indicate that using ord() and rord() is a memory hog now. From the changelog: Searching and sorting is now done on a per-segment basis meaning that the FieldCache entries used for sorting and for function queries are created and used per-segment and can be reused for segments that don't change between index updates. While generally beneficial this can lead to increased memory usage over 1.3 in certain scenarios: [...] 2) Certain function queries such as ord() and rord() require a top level FieldCache instance and can thus lead to increased memory usage. Consider replacing ord() and rord() with alternatives such as function queries based on ms() for date boosting. It mentions possible strategies for handling date-based boosting but how about for a number like ""rank"" where rank is a number between 1 and the total number of records? rord() seems ideal... any other strategies? The point of using segment-based field caches is to reduce the load time. If you want to get the value of a field after having added a new segment (which is done every time you commit) you only have to load a new field cache for the newly added segment. This is not possible with ord and rord which give you the ordinal for the whole index instead of the value for a single document. So the only solution for you would be to compute the boost based the value of the field ""cur_rank"" instead of its ord. This is how date boosting now works : it used to use the rord of the date field in order to compute the boost whereas it now uses the number of milliseconds between the value of the date field and now. See http://wiki.apache.org/solr/SolrRelevancyFAQ (""How can I boost the score of newer documents"") for more details. That's what we ended up doing."
841,A,Zend Framework - Is there a script to index static content from Views? I'm wanting to add a search box onto my website - which is built using Zend Framework 1.6 - using Zend Search Lucene. The majority of my content is static and held within Controller Views. Does anyone have a script that can index content from the views and add them into the Lucene search database so I can search them? Many thanks Matt I found this to be a great reference for creating a web crawler to build a Zend_Search_Lucene index using on existing HTML pages: http://static.zend.com/topics/Improve-your-PHP-Applications-Search-Capabilities-with-Lucene.pdf  Thanks for your post - i'll give it a try. If that works it shouldn't be too hard to recursively spider the links on the site adding each internal link to the index. I'll post back if I make some headway. Matt  Just an idea: you could use $doc = Zend_Search_Lucene_Document_Html::loadHTMLFile('http://my.host.domain/path2file'); $index->addDocument($doc); If allow_url_fopen is set to 1 the call to file_get_contents() used in Zend_Search_Lucene_Document_Html::__construct() should be able to open files via an url.
842,A,"Searching multiple fields with Lucene I'm having some trouble with a search I'm trying to implement. I need for a user to be able to enter a search query into a web interface and for the back-end Java to search for the query in a number of fields. An example of this might be best: Say I have a List containing ""Person"" objects. Say each object holds two String fields about the person: FirstName: Jack Surname: Smith FirstName Mary Surname: Jackson If a user enters ""jack"" I need the search to match both objects the first on Surname and the second on FirstName. I've been looking at using a MultiFieldQueryParser but can't get the fields set up right. Any help on this or pointing to a good tutorial would be greatly appreciated. MultiFieldQueryParser is what you want as you say. Make sure: The field names are always used consistently The same Analyzer is used on both fields and also on the query parser You won't find partial words by default so if you search for jack you won't find jackson. (You can search for jack* in that case.) Regarding field name I always set up an enum for my field names then use e.g. MyFieldEnum.firstname.name() when passing field names to Lucene so that if I make a spelling mistake the compiler can catch it and it's also a good place to put Javadoc so you can see what the fields are for and also a place where you can see the complete list of fields you wish to support in your Lucene documents. So when adding people to the index i can put: document.add(new Field(""FirstName"" ... ); within a for loop and it should be ok? That's right. Make sure the field is ""indexed"" (= searchable) i.e. pass this to the constructor of `Field`: http://lucene.apache.org/java/2_3_0/api/org/apache/lucene/document/Field.Index.html#TOKENIZED Great thank you very much for your help Adrian!"
843,A,"Solr Dismax Config for Boost Scoring I've seen many of this topics here but still confusing to implement it. In my case i need to do these: Search certain phrases in title & text and give title^3 text^1 based on result in #1 i need to boost the results by modified time i've tried these with different results: /solr/select ?q={!boost b=$dateboost v=$qq defType=dismax} &dateboost=recip(ms(NOW/HOURmodified)8640000011) &qq=video &qf=title^3+text &pf=title^3+text &debugQuery=true And Normal Query with different setting in solrconfig.xml <str name=""qf"">title^3 text</str> <str name=""pf"">title^3 text</str> <str name=""bf"">recip(ms(NOW/HOURmodified)8640000011)</str> I prefer to have the boost set by default in solrconfig thanks in advanced. Finally i used the first option since when using !boost the dismax handler config in solrconfig.xml is being ignored."
844,A,"Hibernate Search Lucene or any other alternative? I have a query which is doing ILIKE on some 11 string or text fields of table which is not big (500 000) but for ILIKE obviously too big search query takes round 20 seconds. Database is postgres 8.4 I need to implement this search to be much faster. What came to my mind: I made additional TVECTOR column assembled from all columns that need to be searched and created the full text index on it. The fulltext search was quite fast. But...I can not map this TVECTOR type in my .hbms. So this idea fell off (in any case i thaught it more as a temporary solution). Hibernate search. (Heard about it first time today) It seems promissing but I need experienced opinion on it since I dont wanna get into the new API possibly not the simplest one for something which could be done simpler. Lucene In any case this has happened now with this table but i would like to solution to be more generic and applied for future cases related to full text searches. All advices appreciated! Thanx A year ago I would have recommended Compass. It was good at what it does and technically still happily runs along in the application I developed and maintain. However there's no more development on Compass with efforts having switched to ElasticSearch. From that project's website I cannot quite determine if it's ready for the Big Time yet or even actually alive. So I'm switching to Hibernate Search which doesn't give me that good a feeling but that migration is still in its initial stages so I'll reserve judgement for a while longer.  I have used Lucene in the past to index database tables. The solution works great but remeber that you need to maintain the index. Either you update the index every time your objects are persisted or you have a daemon indexer that dump the database tables in your Lucene index. Have you considered Solr? It's built on top of Lucene and offers automatic indexing from a DB and a Rest API. thanx. we already use lucene for document indexing so i thaught better stick to the same library.How would it be possible with Lucene to for example I want to index some relations of objects? Do i must index entire tables or i could do particular columns that i need from main table and some of its relations? The way I did it was to use SELECT queries with JOINS to create a ""flat"" structure of my data so that I could run an indexer on them. This is one approach. You can also use stored procedures to flatten your data into a special table used for indexing purposes  All the projects are based on Lucene. If you want to implement a very advanced features I advice you to use Lucene directly. If not you may use Solr which is a powerful API on top of lucene that can help you index and search from DB. I give you an example : you have to make http calls to a web server. In java there is socket library that helps you do that but there is better : apache commons http client. It's exactly that come with built in libraries that implement the protocol. The same thing for Solr that have a built in API to manage indexes easy full text search with easy database integration and designed to be run of a servlet container. i will not need too advanced features i think but would like to avoid to use new library which we dont use so far. I am not sure I understood why you recommend Solr - which is in any case built on lucene? Could you bit more clarify please ? Thanx you!!!  I would strongly recommend Hibernate Search which provides a very easy to use bridge between Hibernate and Lucene. Rememeber you will be using both here. You simply annotate properties on your domain classes which you wish to be able to search over. Then when you update/insert/delete an entity which is enabled for searching Hibernate Search simply updates the relevant indexes. This will only happen if the transaction in which the database changes occurs was committed i.e. if it's rolled back the indexes will not be broken. So to answer your questions: Yes you can index specific columns on specific tables. You also have the ability to Tokenize the contents of the field so that you can match on parts of the field. It's not hard to use at all you simply work out which properties you wish to search on. Tell Hibernate where to keep its indexes. And then can use the EntityManager/Session interfaces to load the entities you have searched for. neat indeed thanx! @Julia You should only index the fields that you want to search on. You tell Hibernate Search what the @DocumentId (also the @Id) of the indexed entity is. Hibernate will then use this id to get the entity from the database (or the session cache) without you worrying about it. In effect Hibernate Search takes a search string and returns you domain entities matching that search. Neat huh? thanx for explanations one more short question I want to be able to search on few string fields. Does it make sense to store all the other fields to the index as well but not make searchable and then when i have hit that i get the object from there or should i just get the IDS and go to database to get them ?  I recommend Compass. It's an open source project built on top of Lucene that provider a simpler API (than Lucene). It integrates nicely with many common Java libraries and frameworks such as Spring and Hibernate.  Since you're already using Hibernate and Lucene Hibernate Search is an excellent choice. What Hibernate Search will primarily provide is a mechanism to have your Lucene indexes updated when data is changed and the ability to maximize what you already know about Hibernate to simplify your searches against the Lucene indexes. You'll be able to specify what specific fields in each entity you want to be indexed as well as adding multiple types of indexes as needed (e.g. stemmed and full text). You'll also be able to manage to index graph for associations so you can make fairly complex queries through Search/Lucene. I have found that it's best to rely on Hibernate Search for the text heavy searches but revert to plain old Hibernate for more traditional searching and for hydrating complex object graphs for result display."
845,A,"Lucene Query clause to always return true? Is there a Lucene clause to always return true? I am building my queries programmatically and I believe it'll be useful if I can insert a clause which will always evaluate to true for any document. For example I might be matching field foo against a list of values and I want to say ""if values is empty then select all the documents"". In SQL I can use ""where 1=1"" is there anything like this in Lucene? Or am I barking up the wrong tree? I think you are looking for MatchAllDocsQuery This is an efficient way to match any document."
846,A,"merge multiple solr facceted search into one i have an array of products. For each product I have to crate an solr faceted search. Example for the following ""products"": Computer TV MP3-Player by using faceted search I like to determine how often every product exists in field PRODUCT. With the following result Comupter (3) -apple -ibm -dell TV (5) -sony -toshiba [...] MP3-player (10) -[...] Right now i realize that by using one faceted search for every word/product. That works but the results returned in 400ms by using the following options: 'facet' => 'true' 'facet.field' => 'PRODUCT' 'facet.method' => 'enum' 'facet.limit'=>200 'facet.mincount'=>4 'fq' => 'PRODUCT:computer' <- by iterating an array with PHP i change the product (computertv...) on every iteration Unfortunately in real life there are not 3 product (like the example above) there are round about 100 products which are relevant. That means: the PHP script hast to request 100 solr searches with 400ms - so the script runs 40 seconds which is to long. I'm unable to run an unlimited/unrestricted faceted search for ""all"" products (without ""fq="") because there are thousend of products and I dont need the information only fro a every. Is there a way to realize a better performance for example be merge those multiple solr requests into one? Thank you! i doubt you will get results for `""TV""` You usually don't want to filter on a specific type value when faceting. The idea behind faceting is that it will do a ""group"" and ""count"" for all values in the faceted field (for all items that matches the original query). If you simply remove your fq-parameter you will see that in return you will get a list of all values in PRODUCT-field that occur at least 4 times and the count for each of those values. Sorry my question wasn't exact enough. Please take a look for the update of the example. What I would do is cache the taxonomy on the client and create a facet for PRODUCT getting the count for TV Computer etc. and then on the client side generate the list (form the cached taxonomy). This since you are not interested in the count on the second level. If you are I would suggest that you go with Karls suggestion and index the taxonomy and parse it in the client for every request.  I didn't quite get that but can't you just create one filter query for the products that are relevant to the query: facet' => 'true' 'facet.field' => 'PRODUCT' 'facet.method' => 'enum' 'facet.limit'=>200 'facet.mincount'=>4 'fq' => 'PRODUCT:(computer OR tv OR mp3-player)' And then do some processing on the returned results? sorry my question was not explicit enough. So i updated the example above. As you can see i'm interested in the frequency and also the manufacturer of an product. By running your example i get all the manufacturer at onces without the information which belongs to a specific product. do you control the schema? can you add fields? A way would be to store concatenated information such as Computer/Ibm etc (which would strictly speaking be a taxonomy) and then decode that in the client"
847,A,use a virtual file system with Lucene.NET Is there a way to use a virtual filesystem with Lucene.NET? Based on my (moderate) experience with Lucene I suspect the answer here is no; but just in case (...) barring that: Or is there an existing Contrib module or add-on for Lucene.NET that adds VFS support? Can you explain a bit more? Also what do you intend to solve with this? @Mauricio Lucene seems to require references to objects like System.IO.Directory to function. I am in an environment where I don't have direct access to the physical disk space I need to be able to hand it a virtualized file system provider that routes to an arbitrary store. @Mauricio for a very crude example I should be able to hand it a Stream (or a StreamProvider) instead of a Directory that it writes to. You can do this by implementing Lucene.Net.Store.Directory. The xmldoc for this abstract class is very didactic: A Directory is a flat list of files. Files may be written once when they are created. Once a file is created it may only be opened for read or deleted. Random access is permitted both when reading and writing. Java's i/o APIs not used directly but rather all i/o is through this API. This permits things such as: implementation of RAM-based indices; implementation indices stored in a database via JDBC; implementation of an index as a single file; Directory locking is implemented by an instance of LockFactory and can be changed for each Directory instance using setLockFactory. Here's an example of implementing a custom Directory to support Azure. Aha - I didn't dig deeply into that part of it. Great implementation example!
848,A,Indexing Multiple Tables in Lucene I want to use lucene.net to index records in our database. The records are stored in several different tables and tied together through a records table. Would it be better to index each table separately and tie the search results together in code or should I tie the records together coming out of the database and place them all in one index? Any other suggestions would be helpful as well. If you make a Lucene index that corresponds to each table then 1) you're going to have to do the work of performing the search against each index and 2) merging the search results in some magical fashion. Lucene is already set up to search documents with multiple fields (see MultiFieldQueryParser) and give you a unified result set. Even if you decide after making the index that you occasionally only want to search based on the data that came from a single table you can just use the normal QueryParser to search only the corresponding field of your documents.  Lucene isn't tied to database tables you select the information you want in a Lucene document. I would likely be better to let Lucene handle the merging and ranking of results rather than doing it yourself. I realize it's not tied to tables. I was wondering if I should leave the tables as they are and search them or pull them out the way I would want them and search that way. Regardless of the data structure in your tables you should create the Lucene document in a way that makes sense to you. ALl the data you want to search should be in it. They use one index. The pain of merging multiple results isn't worth it.
849,A,"Searching names with Apache Solr I've just ventured into the seemingly simple but extremely complex world of searching. For an application I am required to build a search mechanism for searching users by their names. After reading numerous posts and articles including: How can I use Lucene for personal name (first name last name) search? http://dublincore.org/documents/1998/02/03/name-representation/ what's the best way to search a social network by prioritizing a users relationships first? http://www.gossamer-threads.com/lists/lucene/java-user/120417 Lucene Index and Query Design Question - Searching People Lucene Fuzzy Search for customer names and partial address ... and a few others I cannot find at-the-moment. And getting at-least indexing and basic search working in my machine I have devised the following scheme for user searching: 1) Have a first second and third name field and index those with Solr 2) Use edismax as the requestParser for multi column searching 3) Use a combination of normalization filters such as: transliteration latin-to-ascii convesrion etc. 4) Finally use fuzzy search Evidently being very new to this I am unsure if the above is the best way to do it and would like to hear from experienced users who have a better idea than me in this field. I need to be able to match names in the following ways: 1) Accent folding: Jorn matches Jörn and vise versa 2) Alternative spellings: Karl matches Carl and vice versa 3) Shortened representations (I believe I do this with the SynonymFilterFactory): Sue matches Susanne etc. 4) Levenstein matching: Jonn matches John etc. 5) Soundex matching: Elin and Ellen Any guidance criticisms or comments are very welcome. Please let me know if this is possible ... or perhaps I'm just day-dreaming. :) EDIT I must also add that I also have a fullname field in case some people have long names as an example from one of the posts: Jon Paul or Del Carmen should also match Jon Paul Del Carmen And since this is a new project I can modify the schema and architecture any way I see fit so there are very limited restrictions. You'll get the best results by trying to resolve 1 issue at a time. (You probably know this I am just reminding you ;-) )... AND I would try resolving the easiest issues first. Your list looks to be sorted in reverse order of difficulty. I would start with #5 and work my way back to #1. Each of these issues rate a separate question here on S.O. Finally I think you'll get better help on the user forums at each projects main site. I am spent several hours a few months ago reading the lucene forum at apache.org and found it very encouraging and enlighting. Good Luck! Re accent folding as far as I know I don't really know swedish for instance but when you use a certain language analyzer on the text as long as that same analyzer is used on the query it'll make the indexed document searchable. I don't really know much though about language detection on a piece text. It sounds like you are catering for a corpus with searches that you need to match very loosely? If you are doing that you will want to choose your fields and set different boosts to rank your results. So have separate ""copied"" fields in solr: one field for exact full name (with filters) multivalued field with filters ASCIIFolding Lowercase... multivalued field with the SynonymFilterFactory ASCIIFolding Lowercase... PhoneticFilterFactory (with Caverphone or Double-Metaphone) See Also: more non-english Soundex discussion Synonyms for names I don't know if there is a public synonym db available. Fuzzy searching I've not found it useful it uses Levenshtein Distance. Other filters and indexing get more superior ""search relevant"" results. Unicode characters in names can be handled with the ASCIIFoldingFilterFactory You are describing solutions up front for expected use cases. If you want quality results plan on tuning your Search Relevance This tuning will be especially valuable when attempting to match on synonyms like MacDonald and McDonald (which has a larger Levenshtein distance than Carl and Karl). Thank you for your reply. Since I was in a hurry to implement this feature I removed some features and implemented it the following way (it's mostly similar to your suggestion): 1) Added a field with full name (as you said) 2) Created an analyzer with Lowercase ASCIIFolding and n-gram analyzer (for auto-suggest) 3) Added a SynonymFilter I also used the edismax query parser. Since your solution was close to my attempt; I'll mark this answer as correct. :) Thank you again for your time!  Found a nickname db not sure how good: http://www.peacockdata2.com/products/pdnickname/ Note that it's not free."
850,A,"Information Retrieval database formats? I'm looking for some documentation on how Information Retrieval systems (e.g. Lucene) store their indexes for speedy ""relevancy"" lookups. My Google-fu is failing me: I've found a page which describes Lucene's file format but it's more focused on how many bits each number is than on how the database is used in producing speedy queries. Surely someone has some useful bookmarks lying around that they can refer me to. Thanks! The Lucene index is an inverted index so any search on this topic should be relevant like: http://en.wikipedia.org/wiki/Inverted_index http://www.ibm.com/developerworks/library/wa-lucene/ True it's an inverted index but if I have a 10-term query is lucene really looking up each term in the inverted index intersecting the results and ranking them? In essence yes if you look at the Lucene scoring formula (http://lucene.apache.org/java/3_0_1/api/all/org/apache/lucene/search/Similarity.html) you'll see that each query terms is used to build a vector that is gonna be used to search in the index"
851,A,Nutch - how to crawl by small patches? I am stuck! Can`t get Nutch to crawl for me by small patches. I start it by bin/nutch crawl command with parameters -depth 7 and -topN 10000. And it never ends. Ends only when my HDD is empty. What i need to do: Start to crawl my seeds with possibility to go further on outlinks. Crawl 20000 pages then index them. Crawl another 20000 pages index them and merge with first index. Loop step 3 n times. Tried also with scripts found in wiki but all scripts i found don't go further. If i run them again they do everything from beginning. And in the end of script i have the same index i had when started to crawl. But i need to continue my crawl. Some help would be very usefull! You have to understand the Nutch generate/fetch/update cycles. The generate step of the cycle will take urls (you can set a max number with the topN parameter) from the crawl db and generate a new segment. Initially the crawl db will only contain the seed urls. The fetch step does the actual crawling. The actual content of the pages are stored in the segment. Finally the update step updates the crawl db with the results from the fetch (add new urls set the last fetch time for an url set the http status code of the fetch for an url etc). The crawl tool will run this cycle n times configurable with the depth parameter. After all cycles are complete the crawl tool will delete all indexes in the folder from which it is launch and create a new one from all the segments and the crawl db. So in order to do what you are asking you should probably not use the crawl tool but instead call the individual Nutch commands which is what the crawl tool is doing behind the scene. With that you will be able to control how many times you crawl and also make sure that the indexes are always merge and not delete at each iteration. I suggest you start with the script define here and change it to your needs. Thank you! Now i understand how Nutch works. One question: crawl db in initial step is only with seed urls. I crawled and got 100000 urls in my crawldb. When i start crawling again and do not use -topN parameter how much urls will nutch take to crawl from it`s crawldb? If you do not specify a topN parameter the generate command will take all the urls that are ready to be fetched and add them to the new segment. All the new urls that were discovered in the previous crawl will get fetched. For the urls that were already fetched they will be fetch again only if they are due according to the db.fetch.interval.default and db.fetch.interval.max parameters. so i even don't need to specify depth yes? Nutch will take 100000 urls in one depth. Is it right? yes depth only applied to the crawl command to specify the number of generate/fetch/update cycles to do. And yes the generate command should take all the urls ready to be fetched.
852,A,Hibernate Search Help Getting error when adding PhoneticAnalyzer?? post the exception I edited the question You are missing the library/jar that contains PhoneticAnalyzer in your classpath. You can download that library from here and add it to your classpath Yes that's was the problem but do u know how to use MultiFieldQueryParser with PhoneticAnalyzer @Noor - nope I am not aware of it. See if this link helps you https://forum.hibernate.org/viewtopic.php?f=9&t=1003107
853,A,"Lucene using Snowball and SpellChecker brings back strange values I am trying to get SpellChecker setup using Lucene.NET it all works fine other than situations similar to the following: I have text containing satellite in the index I analyze it using Snowball. I then create a SpellChecker index and get suggestions from it. The suggestion I get returned when passing in ""Satalite"" is ""satellit"". I am assuming this is because Snowball is stemming Satellite down to satellit and hence SpellChecker is returning that as the suggestion. Is there anyway around this so I can use the two together other than creating an additional field for non stemmed words just so the spell checker can check that? You are right this happens due to stemming. Unfortunately the stemmed words not meant only for search and outside search they can be meaningless. Even I don't know any other technique than storing it multiple times. That additional field can be configured to store as little information as possible to reduce the burden. I've added the new field to stop this from happening. If you add this as an answer I will accept it... As Shashikant mentioned above: You are right this happens due to stemming. Unfortunately the stemmed words not meant only for search and outside search they can be meaningless. Even I don't know any other technique than storing it multiple times. That additional field can be configured to store as little information as possible to reduce the burden. – Shashikant Kore Dec 2 at 14:08  Have you considered putting the words generated by the snowball filter in as synonyms? That is a direction I'm going... don't know how well it will work but seems plausible. Then spellchecker will return the right words but I can still do my searches and find the stemmed variant."
854,A,"How do I use StandardAnalyzer with TermQuery? I'm trying to produce something similar to what QueryParser in lucene does but without the parser i.e. run a string through StandardAnalyzer tokenize this and use TermQuery:s in a BooleanQuery to produce a query. My problem is that I only get Token:s from StandardAnalyzer and not Term:s. I can convert a Token to a term by just extracting the string from it with Token.term() but this is 2.4.x-only and it seems backwards because I need to add the field a second time. What is the proper way of producing a TermQuery with StandardAnalyzer? I'm using pylucene but I guess the answer is the same for Java etc. Here is the code I've come up with: from lucene import * def term_match(self phrase): query = BooleanQuery() sa = StandardAnalyzer() for token in sa.tokenStream(""contents"" StringReader(phrase)): term_query = TermQuery(Term(""contents"" token.term()) query.add(term_query) BooleanClause.Occur.SHOULD) The established way to get the token text is with token.termText() - that API's been there forever. And yes you'll need to specify a field name to both the Analyzer and the Term; I think that's considered normal. 8-) According to the API docs token.termText() is deprecated and they point me to instead using something like token.termBuffer()[0:token.termLength()] which works but seems even more awkward.  I've come across the same problem and using Lucene 2.9 API and Java my code snippet looks like this: final TokenStream tokenStream = new StandardAnalyzer(Version.LUCENE_29) .tokenStream( fieldName  new StringReader( value ) ); final List< String > result = new ArrayList< String >(); try { while ( tokenStream.incrementToken() ) { final TermAttribute term = ( TermAttribute ) tokenStream.getAttribute( TermAttribute.class ); result.add( term.term() ); }"
855,A,Using lucene in Tomcat Background I am assuming the following code is completely thread safe: // Called from a servlet when a user action results in the index needing to be updated public static void rebuildIndex() { FSDirectory dir = new NIOFSDirectory(new File(Configuration.getAttachmentFolder()) null); IndexWriter w = new IndexWriter(dir analyzer IndexWriter.MaxFieldLength.UNLIMITED); .... build index ... w.optimize(); w.commit(); w.close(); } The following code is called from the search servlet: // Called by the search servlet public void search() { FSDirectory dir = new NIOFSDirectory(new File(Configuration.getAttachmentFolder()) null); IndexReader indexReader = IndexReader.open(dirtrue); IndexSearcher searcher = new IndexSearcher(indexReader); .... do the search ... } Questions I am trying to work out the best/correct way to implement this to avoid multi-threading problems: Should the FSDirectory dir object be shared as some sort of static variable? Should the IndexSearcher searcher or the IndexReader indexReader objects be stored as a static variable and then simply replaced when the index is rebuilt? Im beginning to think I should declare the `FSDirectory dir` object only once. By doing this it might take care of locking. Not sure how to work this out. How often do you intend to update the index? If nightly you could probably use a simple mechanism. How many searcher threads do you want at once? Opening new IndexSearcher and IndexReader whenever somebody executes a search is very inefficient. Best to have a single reader that is shared by all calls to search(). This reader instance is closed and replaced whenever you need to rebuild the index. Of course this will need to be synchronized. This is elaborated more at: http://wiki.apache.org/lucene-java/UpdatingAnIndex  You should just move indexReader and searcher to the end of rebuildIndex() and of course synchronize on searcher in rebuildIndex() and in search(). But based on how often index has to be rebuilt and how quick search results have to be returned you might have to consider some caching of indexReader and searcher rather than search threads waiting everytime index is being rebuilt.
856,A,"Is there a way I can provide Lucene.NET with a list of predefined relevant terms? I know I can during search specify a ""boost factor"" to a term as described in http://lucene.apache.org/java/2_4_0/queryparsersyntax.html. My question is: Can I provide Lucene with a predefined table of relevance? For instance I could say that ""chair"" and ""table"" are relevant words with a boost factor of 4 and all subsequent searches would respect that. At index time you can use Payloads -- See http://www.lucidimagination.com/blog/2009/08/05/getting-started-with-payloads . Your special terms get encoded with a payload which at query time can be decoded and used to give an extra boost. Alternatively you might be able to extend the existing QueryParser to give your special terms an extra boost factor."
857,A,"I want read the single term in my index with Lucene I want read every single index. I want read and print to console the single term in my index. (I don't want view the contents with Luke). I must use the class IndexReader?? can someone help me? I try to do:  iReader = IndexReader.open(directory); int num = iReader.numDocs(); for ( int i = 0; i < num; i++) { if ( ! iReader.isDeleted( i)) { org.apache.lucene.document.Document d = iReader.document(i); System.out.println( ""d="" +d.getField(""title"").tokenStreamValue()); } } org.apache.lucene.document.Document doc = new org.apache.lucene.document.Document(); //aggiungo tutti i documenti Field title = new Field( ""title"" testDoc.title Field.Store.YES Field.Index.ANALYZED Field.TermVector.WITH_POSITIONS_OFFSETS); doc.add(title); Field content = new Field( ""content"" testDoc.content Field.Store.YES Field.Index.ANALYZED Field.TermVector.WITH_POSITIONS_OFFSETS); doc.add(content); iWriter.addDocument(doc); but d = null; where I wrong? I want to retrieve the term to the Field title that I indicized... Thanks very much I'm using Lucene.Net but I presume the logic is identical. There must be exactly one of StringValue() ReaderValue() and BinaryValue() set. Those not in use will either return null or throw an exception. In your case try reading StringValue() instead. See answer from Joel he's already provided some links to get you started. Just pass the document title through your analyzer and you'll get the resulting tokens. Simon if I use StringValue() will return the value of doc.title .. I want the token... Another solution would be to use IndexReader.GetTermFreqVector(documentId fieldName) but that requires that you index your field with TermVector.YES. I did not understand. Where do I do this operation? and what should I use? Thanks for your patience  Again I use Java but the principle will be the same. What you want to do is similar to enumerating term frequencies but you just care about distinct fields. This example and this example on how to count term frequencies in a Lucene index should get you going. Yes I find. It works! Thank to all :) @user568720: The code will answer your question. Just look at it. Thanks Joel but I want only retrieve the term in a index...the term frequencies I do not care...  To examine the index use IndexReader. The class has a method document(int) which you can use to find the individual documents that the index contains. The document then offers you all the fields that were created for that document. With the field you can either get it's value or the stream of tokens (i.e. the strings that end up in the index). [EDIT] If you delete documents the index will have holes. So you must add a check: org.apache.lucene.document.Document d = iReader.document(i); if( d == null ) continue; // <<-- You need this check System.out.println( ""d="" +d.getField(""title"").tokenStreamValue()); I Aaron thanks for your response. I modify my post above. Where is the mistake??"
858,A,"Installing PyLucene 3.0.3 on Ubuntu 10.04 I'm attempting to install PyLucene 3.0.3 on Ubuntu 10.04. This has proven considerably challenging but so far I've: Patched setuptools to allow building of JCC as instructed in the PyLucene docs. Built JCC via: cd pylucene-3.0.3-1/jcc; python setup.py build Built Lucene 3.0.3 via ant and installed the jar to /usr/share/java/lucene-core-3.0.3-dev.jar. Note I have Ubuntu's default Lucene package installed to /usr/share/java/lucene-core-2.9.2.jar which also symlinks to /usr/share/java/lucene-core.jar I'm now trying to ""make"" PyLucene but I get the error: cd lucene-java-3.0.3; -Dversion=3.0.3 /bin/sh: -Dversion=3.0.3: not found make: *** [lucene-java-3.0.3/build/lucene-core-3.0.3.jar] Error 127 The file pylucene-3.0.3-1/doc/documentation/install.html makes mention to ""edit Makefile to match your environment"" but I'm not sure what that means. The makefile seems to contain the same Lucene version number as the one I installed. How else do I need to edit my makefile in order to build PyLucene? Edit: After uncommenting a section in the makefile (thanks Torsten) for compiling under Ubuntu 8.10 (seriously 8.10?!) most of it seemed to compile fine but I still received an error. Several components reported ""BUILD SUCCESSFUL"" but the final build ended with: /usr/bin/python -m jcc --shared --jar lucene-java-3.0.3/build/lucene-core-3.0.3.jar --jar lucene-java-3.0.3/build/contrib/snowball/lucene-snowball-3.0.3.jar --jar lucene-java-3.0.3/build/contrib/analyzers/common/lucene-analyzers-3.0.3.jar --jar lucene-java-3.0.3/build/contrib/regex/lucene-regex-3.0.3.jar --jar lucene-java-3.0.3/build/contrib/memory/lucene-memory-3.0.3.jar --jar lucene-java-3.0.3/build/contrib/highlighter/lucene-highlighter-3.0.3.jar --jar lucene-java-3.0.3/build/contrib/queries/lucene-queries-3.0.3.jar --jar build/jar/extensions.jar --package java.lang java.lang.System java.lang.Runtime --package java.util java.util.Arrays java.text.SimpleDateFormat java.text.DecimalFormat java.text.Collator --package java.io java.io.StringReader java.io.InputStreamReader java.io.FileInputStream --exclude org.apache.lucene.queryParser.Token --exclude org.apache.lucene.queryParser.TokenMgrError --exclude org.apache.lucene.queryParser.QueryParserTokenManager --exclude org.apache.lucene.queryParser.ParseException --exclude org.apache.lucene.search.regex.JakartaRegexpCapabilities --exclude org.apache.regexp.RegexpTunnel --python lucene --mapping org.apache.lucene.document.Document 'get:(Ljava/lang/String;)Ljava/lang/String;' --mapping java.util.Properties 'getProperty:(Ljava/lang/String;)Ljava/lang/String;' --rename org.apache.lucene.search.highlight.SpanScorer=HighlighterSpanScorer --version 3.0.3 --module python/collections.py --files 200 --build /usr/bin/python: jcc is a package and cannot be directly executed make: *** [compile] Error 1 I did this before (but without installing Lucene's default package in Ubuntu). I don't know what exactly is Error 127 but in my case it helped to set NUM_FILES=200 from the original NUM_FILES=2 in my Makefile. For some reason when NUM_FILES=2 it creates really huge files in memory which ubuntu will not handle. With NUM_FILES=200 the chunks are smaller and installation worked for me in the end. For python 2.6 you also have to change the JCC setting in Makefile (see below). Here the part which was important for me in the Makefile: # Linux (Ubuntu 8.10 64-bit Python 2.5.2 OpenJDK 1.6 setuptools 0.6c9) PREFIX_PYTHON=/usr ANT=ant PYTHON=$(PREFIX_PYTHON)/bin/python JCC=$(PYTHON) -m jcc.__main__ --shared NUM_FILES=200 No problem. Glad I could help. Installing PyLucene should be really easier. I always spend half a day for it. :/ Sorry didn't see you had added `__main__`. That compiled perfectly. Thanks."
859,A,"Using Hibernate Search to Rate Result Is it possible for hibernate search to sort result according to best match after it has search result from the database Lucene has a sort functionality (which defaults to relevance). Hibernate search exposes this functionality (FullTextQuery.sort). if you do not want default behavior you could pass your own sort object. Sort sort = new Sort(new SortField(""name"")); searchQuery.setSort(sort); List results = searchQuery.list(); In your case default sort should be sufficient. Thanks doc_189!!  By default Hibernate Search will sort results based on the relevance of the results (as doc_180 mentioned) determined by the default Lucene scoring implementation. However if you are not satisfied with the way the way it is doing the ranking (e.g. you may want People entities to generally be ranked higher than all the other indexed entities that you have) then you could do one of two things: Apply a dynamic or static boost factor to the entities that should be considered more relevant (see the docs on the @Boost and @DynamicBoost annotations) or You could write your own custom scoring implementation by extending org.apache.lucene.search.Similarity (see Hibernate Search Advanced Features). The boost factor mentioned in point 1 is just one factor in this overall scoring algorithm."
860,A,"Version incompatibility between Lucene and Solr We have an index generated using the 3.0.2 Lucene jar. The Solr version I installed and configured is 1.4.1. In Solr's lib directory I see that Lucene 2.9.3 is used by Solr. At startup an exception is thrown: ""Incompatible format version: 2 expected 1 or lower"". I am guessing it is possibly due to incompatible Lucene versions - 3.0.2 was used to generate the index and 2.9.3 is being used to read the index. Am I missing a configuration step? Thanks in advance. You'll have to downgrade Lucene to 2.9.x or upgrade Solr to 3.x/4.x (trunk) which uses a more recent Lucene version."
861,A,"How to write Lucene queries for website search engine I plan to implement my website's search engine using Apache Solr. I have a search index built and one of its documents is: Virtua Fighter 2 Performing a search of: Virtua* returns all records starting with ""Virtua"" as expected. A search of ""Virtua Fighter 2"" returns an exact match. I would like a search of ""Virtua Fighter"" to return Virtua Fighter 2 in its result set. But a phrase search of Virtua Fighter omits Virtua Fighter 2 from its result sets. And I'm unable to use a wildcard in a phrase search-- ""Virtua Fighter*"" does not return any results. What type of query needs to be written to support this? Or what types of Lucene queries are used for simple website search engines? Consider a copy field that you use to do your searches against. You would have to create a new field type and use the appropriate TokenizerFactories [http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#TokenizerFactories] but I'm not versed enough to give a proper answer. I'm guessing you're using a Keyword analyzer for the titles? (Or another analyzer that doesn't split on tokens.) You should just use a Standard Analyzer then phrase queries will work fine. That was it. I was using a ""string"" field type for the titles which must have been defaulting to a keyword analyzer. I changed over to using a field type of ""text"" that Solr defines which uses the proper analyzers."
862,A,Lucene hot index backup using IndexReader instead of IndexWriter/SnapshotDeletionPolicy Are the following lines of code acceptable to get a hot backup of a lucene index or IndexWriter/SnapshotDeletionPolicy as described in Lucene index backup should be followed? Directory dir = ...; IndexReader reader = IndexReader.open(dir); IndexCommit commit = reader.getIndexCommit(); Collection<String> fileNames = commit.getFileNames(); //copy the files reader.close(); Even on a locked index you may open a reader on a commit point while a writer may still change the index. You need to use a SnapshotDeletionPolicy. Unless you have an unreleased snapshot the writer will be free to delete files as it pleases. This will only happen on flush/close so you might be able to get away with it most of the time but it won't always work. Note that the policy is owned by the writer so if you're trying to somehow use one process to back it up while another process writes this won't work.  If you have no IndexWriter writing to the index then the above code is fine. But an open IndexWriter against the index can easily delete the files referenced/still in use by this IndexReader (for example when a merge completes) and then your backup will fail.
863,A,"Why can I not search for a ""0"" field in Solr? From schema.xml: <field name=""myfield"" type=""integer"" indexed=""true"" stored=""false""/> The record with id 5 has myfield with value of 0 which I've confirmed by searching for plain id:5 and looking at the objectXml. A search for id:5 AND myfield:0 returns no records. A search for id:5 AND -myfield:1 however returns the record I am expecting. Why? -- Additional info: Definition for integer type: <fieldType name=""integer"" class=""solr.IntField"" omitNorms=""true""/> Solr version: 1.4 The schema snippet doesn't match your description -- you can't verify the value of a non-stored field by looking at results. Please verify that it matches your actual schema. Also please include the definition for your integer type and specify which Solr version you're running. I tried your case assuming you used the standard fieldtype for int: <fieldtype name=""integer"" class=""solr.TrieIntField"" ... /> That works fine. So i guess the field type definition of integer is somehow wrong. Check this definition.  Numeric fields require numeric queries. Try giving a type of something other than numeric for your field. When did 0 stop being a number? ...? When you specify a field type of numeric that means that you want to store it as a trie with a certain precision. This means that your number is *not* indexed verbatim; so you need a special type of query to query it (essentially it's a range query variant). ""Numeric query"" does not mean ""a normal query with a number in it"" it refers to a special Lucene class. Thanks for the explanation but it doesn't really help me. I have a field that is an integer. If I search for ""myfield:"" I find what I'm looking for. If I search for ""myfield:0"" I get nothing. You're saying I need a special type of query - ok can you give me an example?  What is the class that is bound to the ""integer"" field type? Does it treat 0 as a marker for not indexing? What does the index data on that document look like in the admin? The whole query is id:5 AND myfield:0. I am not trying to return myfield. I am searching by id and myfield. FWIW if I search for something else using myfield:1 it does return results. It just doesn't seem to like myfield:0.  I've solved the issue. It was a bug in the core of the application (third party CMS) and had nothing to do with Solr. The problem was that the vendor's code was verifying to see if the value being indexed was not false before indexing it. Unfortunately they were doing so like this: $value = strval($node); if ($value) Of course 0 evaluates to false even if it's the string ""0"". I changed it to: $value = strval($node); if ($value !== false) ... and now it works. Thank you for your efforts and sorry the problem ended up being something completely unrelated."
864,A,How to detect a date in a Lucene free text search query? We're using Lucene to develop a free text search box for data delivered to a user as in the case of an email Inbox. We'd like to allow for the box to handle dates for instance 5/1/2011. To make things easier we are limiting the current version of the feature to just two date formats: mm/dd/yy mm/dd/yyyy For our prototype we hacked the query analysis process to attempt to pre-process the query string to look for these two date patterns. This was about 2 years ago and we were on Lucene 2.4. Im curious to see if there are any tools in Lucene out-of-the-box to accept a DateFormat and return a TokenStream with any identified dates. Looking through the javadocs for Lucene 2.9 I found the class: org.apache.lucene.analysis.sinks.DateRecognizerSinkFilter which seems to do what I need but it implements a SinkFilter a concept which doesn't seem to be documented in the Lucene Wiki. Has anyone used this filter before and if so what is the most effective way to use it? There is a bit of sample code (which is admittedly over-complicated) in the documentation for TeeSinkTokenFilter. Note that the way the DateRecognizerSinkFilter is designed it does not store the actual date; it just detects that a token is a date that conforms to the specified format. What I would try is to re-implement the DateRecognizerSinkFilter class to take an array of DateFormat instances create a new Attribute class called DateAttribute (or some-such) and use the date recognizer subclass to set the parsed date into the DateAttribute if one of its formats matches. That way you can always test whether you have a valid date by interrogating the DateAttribute and localize the date formats to one class. Another advantage is that you won't have to handle multiple sinks thereby simplifying the code from the linked example. Great! You might also implement this as a regular in-line tokenizer that emits the original string and the date at the same position. That way if for example your date format had month or date names you could still search on those directly. Thanks for the reply! I'll give your solution a try and post my results.
865,A,"Lucene.Net PrefixQuery I´m development a suggest box for my site search service. I has to search fields like these: Visual Basic Enterprise Edition Visual C++ Visual J++ My code is:  Directory dir = Lucene.Net.Store.FSDirectory.GetDirectory(""Index"" false); IndexSearcher searcher = new Lucene.Net.Search.IndexSearcher( dirtrue); Term term = new Term(""nombreAnalizado"" _que); PrefixQuery query = new PrefixQuery(term); TopDocs topDocs = searcher.Search(query 10000); This code works well in this case: ""Enterprise"" will match ""Visual Basic Enterprise Edition"" But ""Enterprise E"" doesn´t match anything. I removed white spaces at indexing time and when the user is searching. Thanks. I think you should use the QueryParser and let it build the appropriate Query object instead of using something specific like the PrefixQuery. In Java: QueryParser parser = new QueryParser(Version.LUCENE_CURRENT ""nombreAnalizado"" new StandardAnalyzer(Version.LUCENE_CURRENT)); Query query = parser.parse(_que); Make sure you are using the same analyzer that you used for indexing. Works great thanks!!!"
866,A,What does NHibernate.Search ContainedIn attribute do? And how it differs from IndexedEmbedded? P.s. and what's the best source of information about NHibernate.Search? Answer The ContainedInAttribute is used in conjunction with the IndexedEmbeddedAttribute. The ContainedInAttribute is used as a sort of marker that points back to a class that uses and IndexedEmbeddedAttribute. This tells NHibernate.Search that when you update that class you want to update the parent's full text index. This is good when you update a child of a owning class and you want the owner's index also updated. below is an example of how to use this. [Indexed] class Parent { [IndexedEmbeded] public Child SomeChild { get; set; } } class Child { [ContainedIn] public Parent MyParent { get; set; } } Note: The ContainedIn attribute is useless if you are pointing to an owning parent that is not using an IndexEmbeded attribute. Answer Documentation Information is from hibernate search but most things here apply to NHibernate.Search as well. http://docs.jboss.org/hibernate/stable/search/reference/en/html/ or http://docs.jboss.org/hibernate/stable/search/reference/en/html_single/
867,A,"How does Lucene/Solr achieve high performance in multi-field / faceted search? Context This is a question mainly about Lucene (or possibly Solr) internals. The main topic is faceted search in which search can happen along multiple independent dimensions (facets) of objects (for example size speed price of a car). When implemented with relational database for a large number of facets multi-field indices are not useful since facets can be searched in any order so a specific ordered multi-index is used with low chance and creating all possible orderings of indices is unbearable. Solr is advertised to cope well with the faceted search task which if I think correctly has to be connected with Lucene (supposedly) performing well on multi-field queries (where fields of a document relate to facets of an object). Question The inverted index of Lucene can be stored in a relational database and naturally taking the intersections of the matching documents can also be trivially achieved with RDBMS using single-field indices. Therefore Lucene supposedly has some advanced technique for multi-field queries other than just taking the intersection of matching documents based on the inverted index. So the question is what is this technique/trick? More broadly: Why can Lucene/Solr achieve better faceted search performance theoretically than RDBMS could (if so)? Note: My first guess would be that Lucene would use some space partitioning method for partitioning a vector space built from the document fields as dimensions but as I understand Lucene is not purely vector space based. An explaining post can be found at: http://yonik.wordpress.com/2008/11/25/solr-faceted-search-performance-improvements/ The new method works by un-inverting the indexed field to be faceted allowing quick lookup of the terms in the field for any given document. It’s actually a hybrid approach – to save memory and increase speed terms that appear in many documents (over 5%) are not un-inverted instead the traditional set intersection logic is used to get the counts.  Faceting There are two answers for faceting because there are two types of faceting. I'm not certain that either of these are faster than an RDBMS. Enum faceting. Results of a query are a bit vector where the ith bit is 1 if the ith document was a match. The facet is also a bit vector so intersection is just a bitwise AND. I don't think this is a novel approach and most RDBMS's probably support it. Field Cache. This is just a normal (non-inverted) index. The SQL-style query that is run here is like: select facet count(*) from field_cache where docId in query_results group by facet Again I don't think this is anything that a normal RDBMS couldn't do. The index is a skip list with the docId as the key. Multi-term search This is where Lucene shines. Why Lucene's approach is so good is too long to post here but I can recommend this post on Lucene Performance or the papers linked therein. @ron: yes I don't know that Lucene's performance of faceted queries is remarkably good. But I'm not an expert in this area. Thank you. The post you point to writes ""The real optimizations of Lucene come from the fact that you never search for all documents which match the query but rather the top k."". In case of faceted search I think all matches have to be taken into account."
868,A,What is the subReader used for in Lucene? I don't know what the ReaderUtil.subReader of Lucene does. Does anybody know what it does? See the class definition here: ReaderUtil. Is it used to read each segment separately? A Lucene index is divided into segments. In short from each segment only a chunk of the index is read. And subreaders are the actual readers which work directly on a segment (one segment => one segment reader). And the IndexReader which clients use is an aggregated implementation that uses the subreaders to perform the actual work.
869,A,"Optimizing Lucene performance What are the various ways of optimizing Lucene performance? Shall I use caching API to store my lucene search query so that I save on the overhead of building the query again? Cheat. Use RAMDirectory to load the entire index into the ram. Afterwards everything is blazing fast. :)  I have found that the best answer to a performance question is to profile it. Guidelines are great but there is so many variables that can impact performance such as the size of your dataset the types of queries you are doing datatypes etc. Get the Netbeans profiler or something similar and try it out different ways. Use the articles linked to by Mitch but make sure you actually test what helps and what (often surprisingly) hurts. There is also a good chance that any performance differences you can get from Lucene will be minor compared to performance improvements in your code. The profiler will point that out as well.  Have you looked at Lucene Optimization Tip: Reuse Searcher Advanced Text Indexing with Lucene Should an index be optimised after incremental indexes in Lucene? The link you provide for Lucene Optimization Blog ""http://luceneoptimization.blogspot.com "" contains no posts. @gt_ebuddy : well it used to (obviously)! I will update. Thanks.  Lots of dead links in here. These (somewhat official) resources are where I would start: http://wiki.apache.org/lucene-java/ImproveIndexingSpeed http://wiki.apache.org/lucene-java/ImproveSearchingSpeed  Quick tips: Keep the size of the index small. Eliminate norms Term vectors when not needed. Set Store flag for a field only if it a must. Obvious but oft-repeated mistake. Create only one instance of Searcher and reuse. Keep in the index on fast disks. RAM if you are paranoid."
870,A,"Improving performance of Location based search using Lucene I'm using Lucene for a job search portal using .net. Am facing some performance related issues in the following use case. Use case is: When doing job search user can select job location(for exameple:AtlantaGA) and select radial distance (say 50 miles).The time required to return job search results from Lucene is pretty high. FYIwe are maintaining a sql server 2005 database where we store US and Canada based citystatelongitude and latitude.(contains a total of about 1 million records). Is there anyway I can improve the performace of this location based job search? Would you be able to elaborate on how exactly you are using Lucene to do location-based searches in relation to your database? When a user searches for ""AtlantaGA"" for example how does Lucene know which cities are within 50 miles of it? Does it have to first query the database? please see my comments in my answer below. } Thanks for ur comments.We have a .Net based distance API which takes location as input and returns nearest cities within a given radius.This collection is then given to Lucene for searching jobs. How big is your Lucene index? Just indexing every city and state in the US and Can. would be a very small index and should be lightning fast. What type of Query are you using to find the correct cities? So are you searching the Lucene index for the string ""Atlanta GA"" ? What are you indexing? Do you index ""Atlanta GA"" as one term or is it split into tokens? I index Atlanta in a field named ""city"" and GA in ""state"" in Lucene index. my index size is about 4 MB.Am using the following code for building query for nearest cities: foreach (string city in htNearestCities.Keys) { cityStateQuery = new BooleanQuery(); queryCity = queryParserCity.Parse(""\"""" + city + ""\""""); queryState = queryParserState.Parse(""\"""" + ((string[])htNearestCities[city])[1] + ""\""""); cityStateQuery.Add(queryCity BooleanClause.Occur.MUST); cityStateQuery.Add(queryState BooleanClause.Occur.MUST); findLocationQuery.Add(cityStateQuery BooleanClause.Occur.SHOULD); } 4MB? Lucene's a bit overkill for such a small data set. we are expecting millions of records to be indexed in Lucene down the line...  Basically you have two types of search parameters: textual and spatial. You can probably use one type to filter the results you got from the other. For example for someone looking for a .NET developer job near Atlanta GA you could either first retrieve all the .NET developer jobs and filter for location or retrieve all jobs around Atlanta and filter for .NET developer ones. I believe the first should be faster. You can also store the job locations directly in Lucene and incorporate them in the search. A rough draft is: Indexing: 1. When you receive a new 'wanted' ad find its geo-location using the database. 2. Store the location as a Lucene field in the ad's document. Retrieval: 1. Retrieve all jobs according to textual matches. 2. Use geometrical calculations for finding distances between the user's place and the job location. 3. Filter jobs according to distance. Lucene in Action has an example of spatial search similar in spirit. A second edition is in the making. Also check out Sujit Pal's suggestions for spatial search with Lucene and Patrick O'Leary's framework. There are also Locallucene and LocalSolr but I do not know how mature they are.  You may ultimately want to have lucene handle the spatial search by indexing tiles. But if you're certain the lucene query is slow not the finding of the cities then start by indexing the state and city together. Much like indexing multiple columns in a relational database: a 'state:city' field with values like 'GA:Atlanta'. Then the intersection isn't done at query time."
871,A,"Full text search on Google App Engine (Java) There are a few threads floating around on the topic but I think my use-case is somewhat different. What I want to do: Full text search component for my GAE/J app The index size is small: 25-50MB or so I do not need live updates to the index a periodic re-indexing is fine This is for auto-complete and the like so it needs to be extremely fast (I get the impression that implementing an inverted index in Datastore introduces considerable latency) My strategy so far (just planning haven't tried implementing anything yet): Use Lucene with RAMDirectory A periodic cron job creates the index serializes it to the Datastore stores an update id (or timestamp) Search servlet loads the index on startup and creates the RAMDirectory On each request the servlet checks the current update id and reloads the index as necessary The main thing I'm fuzzy on is how to synchronize in-memory data between instances - will this work or am I missing something? Also how far can I push it before I start having problems with memory use? I couldn't find anything on RAM quotas for GAE. (This index is small but I can think of more stuff I'd like to add) And of course any thoughts on better approaches? Well yeah that's kind of part of what I outlined above... Memory is separate between GAE instances so a straight-up RAM directory wouldn't be shared between them. You'd have to initialize it for each instance on startup. Which might not be so bad given the new standby servers and warmup requests. Recently GAE added ""text search"" service. Take a look at GAE Java Text Search Any idea of how to handle pagination of results in the GAE/J FTS API? I keep getting null cursors. Thanks.  Well as of GAE 1.5.0 looks like resident Backends can be used to create a search service. Of course there's no free quota for these.  For autocomplete perhaps you could store the top N matches for each prefix (basically what you'd put in the drop-down menu) in memcache? The memcache entities could be backed by entities in the datastore and reloaded if needed.  App Engine now includes a full-text search API (Experimental): https://developers.google.com/appengine/docs/java/search/  If you're okay with periodic rebuilds and your index is small your current approach sounds mostly okay. Instead of building the index online and serializing it to the datastore though why not build it offline and upload it with the app? Then you can instantiate it directly from the disk store and to push an update you deploy a new version of your app. I'm going to be updating about once an hour so bundling the index with the app doesn't seem like a good way to go."
872,A,"How to update a Lucene.NET index? I'm developing a Desktop Search Engine in Visual Basic 9 (VS2008) using Lucene.NET (v2.0). I use the following code to initialize the IndexWriter Private writer As IndexWriter writer = New IndexWriter(indexDirectory New StandardAnalyzer() False) writer.SetUseCompoundFile(True) If I select the same document folder (containing files to be indexed) twice two different entries for each file in that document folder are created in the index. I want the IndexWriter to discard any files that are already present in the Index. What should I do to ensure this? Are you trying to rebuild the index from scratch every time or are you trying to update specific entries in the index? Please clarify your question. Unless you're only modifying a small number of documents (say less than 10% of the total) it's almost certainly faster (your mileage may vary depending on stored/indexed fields etc) to reindex from scratch. That said I would always index to a temp directory and then move the new one into place when it's done. That way there's little downtime while the index is building and if something goes wrong you still have a good index.  There are optionslisted below which can be used as per requirements. See below code snap. [Source code in C# please convert it into vb.net] Lucene.Net.Documents.Document doc = ConvertToLuceneDocument(id data); Lucene.Net.Store.Directory dir = Lucene.Net.Store.FSDirectory.Open(new DirectoryInfo(UpdateConfiguration.IndexTextFiles)); Lucene.Net.Analysis.Analyzer analyzer = new Lucene.Net.Analysis.Standard.StandardAnalyzer(Lucene.Net.Util.Version.LUCENE_29); Lucene.Net.Index.IndexWriter indexWriter = new Lucene.Net.Index.IndexWriter(dir analyzer false Lucene.Net.Index.IndexWriter.MaxFieldLength.UNLIMITED); Lucene.Net.Index.Term idTerm = new Lucene.Net.Index.Term(""id"" id); foreach (FileInfo file in new DirectoryInfo(UpdateConfiguration.UpdatePath).EnumerateFiles()) { Scenario 1: Single step update. indexWriter.UpdateDocument(idTerm doc analyzer); Scenario 2: Delete a document and then Update the document indexWriter.DeleteDocuments(idTerm); indexWriter.AddDocument(doc); Scenario 3: Take necessary steps if a document does not exist. Lucene.Net.Index.IndexReader iReader = Lucene.Net.Index.IndexReader.Open(indexWriter.GetDirectory() true); Lucene.Net.Search.IndexSearcher iSearcher = new Lucene.Net.Search.IndexSearcher(iReader); int docCount = iSearcher.DocFreq(idTerm); iSearcher.Close(); iReader.Close(); if (docCount == 0) { //TODO: Take necessary steps //Possible Step 1: add document //indexWriter.AddDocument(doc); //Possible Step 2: raise the error for the unknown document } } indexWriter.Optimize(); indexWriter.Close();  To update a lucene index you need to delete the old entry and write in the new entry. So you need to use an IndexReader to find the current item use writer to delete it and then add your new item. The same will be true for multiple entries which I think is what you are trying to do.Just find all the entries delete them all and then write in the new entries. Could you point me to a demo that shows the use of IndexReader (to find the current item and) to update the Index?  One option is of course to remove a document and then to add the updated version of the document. Alternatively you can also use the UpdateDocument() method of the IndexWriter class: writer.UpdateDocument(new Term(""patient_id"" document.Get(""patient_id"")) document); This of course requires you to have a mechanism by which you can locate the document you want to update (""patient_id"" in this example). I have blogged more details with a more complete source code example.  There are many out-of-date examples out there on deleting with an id field. The code below will work with Lucene.NET 2.4. It's not necessary to open an IndexReader if you're already using an IndexWriter or to access IndexSearcher.Reader. You can use IndexWriter.DeleteDocuments(Term) but the tricky part is making sure you've stored your id field correctly in the first place. Be sure and use Field.Index.NOT_ANALYZED as the index setting on your id field when storing the document. This indexes the field without tokenizing it which is very important and none of the other Field.Index values will work when used this way: IndexWriter writer = new IndexWriter(""\MyIndexFolder"" new StandardAnalyzer()); var doc = new Document(); var idField = new Field(""id"" ""MyItemId"" Field.Store.YES Field.Index.NOT_ANALYZED); doc.Add(idField); writer.AddDocument(doc); writer.Commit(); Now you can easily delete or update the document using the same writer: Term idTerm = new Term(""id"" ""MyItemId""); writer.DeleteDocuments(idTerm); writer.Commit(); Even that signature for IndexWriter is obsolete now (and will be removed in Lucene 3.0). Suggested ctor would be new IndexWriter(directory analyzer maxFieldLength) and for analyzer again the signature is obsolete. Suggested one is new StandardAnalyzer(Version).  If you want to delete all content in the index and refill it you could use this statement writer = New IndexWriter(indexDirectory New StandardAnalyzer() True) The last parameter of the IndexWriter constructor determines whether a new index is created or whether an existing index is opened for the addition of new documents. simplest answer to clearing index while rebuilding thx  As Steve mentioned you need to use an instance of IndexReader and call its DeleteDocuments method. DeleteDocuments accepts either an instance of a Term object or Lucene's internal id of the document (it is generally not recommended to use the internal id as it can and will change as Lucene merges segments). The best way is to use a unique identifier that you've stored in the index specific to your application. For example in an index of patients in a doctor's office if you had a field called ""patient_id"" you could create a term and pass that as an argument to DeleteDocuments. See the following example (sorry C#): int patientID = 12; IndexReader indexReader = IndexReader.Open( indexDirectory ); indexReader.DeleteDocuments( new Term( ""patient_id"" patientID ) ); Then you could add the patient record again with an instance of IndexWriter. I learned a lot from this article http://www.codeproject.com/KB/library/IntroducingLucene.aspx. Hope this helps."
873,A,"nHibrnate.Search with nHibernate v2 I having trouble getting nHibernate.Search to create an Index. If I use 1.2.1.4 of nHibernate.dll & nHibernate.Search.dll then the index is created correctly and I can inspect it with Luke (a Lucene utility). A segments file is created as well as a Fragments file etc However when I use v 2 of nHibernate.dll & nHibernate.Search.dll then the index is not created correctly. Only a 1k segments file is created in the Index directory and Luke is unable to inspect it. The code I used in v1 was as follows: _configuration = new Configuration(); _configuration.Configure(); _configuration.AddAssembly(typeof (Contact).Assembly); _sessionFactory = _configuration.BuildSessionFactory(); SearchFactory.Initialize(_configuration _sessionFactory); and I have the following in the config file <property name=""hibernate.search.default.directory_provider"">NHibernate.Search.Storage.FSDirectoryProvider NHibernate.Search</property> <property name=""hibernate.search.default.indexBase"">~/Index</property> in version 2 there is no SearchFactory. The only similar thing I could find was SearchFactoryImpl.GetSearchFactory(_configuration); So I have set up the config as follows _configuration = new Configuration(); _configuration.Configure(); _configuration.AddAssembly(typeof (Contact).Assembly); _sessionFactory = _configuration.BuildSessionFactory(); _configuration.SetProperty(""hibernate.search.default.directory_provider"" ""NHibernate.Search.Store.FSDirectoryProvider NHibernate.Search""); _configuration.SetProperty(""hibernate.search.default.indexBase"" ""Index""); _configuration.SetProperty(""hibernate.search.analyzer"" ""Lucene.Net.Analysis.Standard.StandardAnalyzer Lucene.Net""); _configuration.SetListener(ListenerType.PostUpdate new FullTextIndexEventListener()); _configuration.SetListener(ListenerType.PostInsert new FullTextIndexEventListener()); _configuration.SetListener(ListenerType.PostDelete new FullTextIndexEventListener()); SearchFactoryImpl.GetSearchFactory(_configuration); This creates the bare bones of an Index but it is not viewable with Luke - which tells me it is corrupt I have also used the following code to try and create the index manually but again it only creates the segments file nothing else public void CreateIndex<T>(string rootIndexDirectory) { Type type = typeof (T); var info = new DirectoryInfo(Path.Combine(rootIndexDirectory type.Name)); // Recursively delete the index and files in there if (info.Exists) info.Delete(true); // Now recreate the index FSDirectory dir = FSDirectory.GetDirectory(Path.Combine(rootIndexDirectory type.Name) true); //Ioc.UrlProvider.MapPath(Path.Combine(rootIndexDirectory type.Name)) true); try { var writer = new IndexWriter(dir new StandardAnalyzer() true); writer.Close(); } finally { if (dir != null) dir.Close(); } using (ISession session = _sessionFactory.OpenSession()) { using (IFullTextSession fullTextSession = Search.CreateFullTextSession(session)) { foreach (var contact in _contacts) { //session.Save(contact); fullTextSession.Index(contact); } } } } So my question is - do I have to use v1.1.4 of nHibernate if I want to use nHibernate.Search? Or can I use v2? In which case what am I doing wrong? There is very little on the web about this. Anyone? Someone please change the title nhibrnate to nhibernate I have found a working example here: http://darioquintana.com.ar/blogging/?p=21 The v2 nHibernate.Search.dll in this project does contain a SearchFactory (albeit in a different namespace). The one I compiled from the SVN repository doesnt have this So all sorted good work dude. thanks!"
874,A,"Lucene Fuzzy Search for customer names and partial address I was going thru all the existing questions posts but couldn't get something much relevant. I have file with millions of records for person first name last name address1 address2 country code date of birth - I would like to check my list of customers with above file on daily basis (my customer list also get updated daily and file also gets updated daily). For first name and last name I would like fuzzy match (may be lucene fuzzyquery/levenshtein distance 90% match) and for remaining fields country and date of birth I wanted exact match. I am new to Lucene but by looking at number of posts looks like its possible. My questions are: How should I index my input file? I need to build index on combination of FN LN country DOB and use the index for search How I can use Fuzzy query of Lucene here? Is there any other way I can implement the same? Rushik here are a few ideas: Consider using Solr. It is much easier to start using it rather than bare Lucene. Build a Lucene/Solr index of the file. It appears that a document per customer is enough if you use a multi-valued field or two different fields for addresses. Do you have a unique id per person? To use Solr you need one. In Lucene you can get away without using a unique id. Store the country code as a ""keyword"". If you only require exact match for date of birth you may do the same. For range queries you will need another representation. I assume your customer list is smaller than the file. A possible policy would be to daily index the changes in the file (Here a unique id is really handy - otherwise you need to delete by query which may miss the mark). Then you can optimize the index and after that run a search for your updated customer list. What you describe is a BooleanQuery Whose clauses are fuzzy queries for the first and last names and term queries for the other fields. You can create the query programmaticaly or using the query parser. Consider using soundex for names as described here. Thanks Yuval yes I have unique ID per person in file thus I should be good. customer list is way smaller than the person file. I will try this solve and get back."
875,A,"Lucene Search doesn't find keyword indexed fields i save my fieldes with this code: class Places_Search_Document extends Zend_Search_Lucene_Document{ public function __construct($class $key $title$contents $summary $createdBy $dateCreated) { $this->addField(Zend_Search_Lucene_Field::Keyword('docRef' ""$class:$key"")); $this->addField(Zend_Search_Lucene_Field::UnIndexed('class' $class)); $this->addField(Zend_Search_Lucene_Field::UnIndexed('key' $key)); $this->addField(Zend_Search_Lucene_Field::Keyword('title' $title 'UTF-8')); $this->addField(Zend_Search_Lucene_Field::unStored('contents' $contents  'UTF-8')); $this->addField(Zend_Search_Lucene_Field::text('summary' $summary  'UTF-8')); //$this->addField(Zend_Search_Lucene_Field::UnIndexed('createdBy' $createdBy)); $this->addField(Zend_Search_Lucene_Field::Keyword('dateCreated' $dateCreated)); } } i search the word with this code: $index = Places_Search_Lucene::open(SearchIndexer::getIndexDirectory()); $term = new Zend_Search_Lucene_Index_Term($q); $query = new Zend_Search_Lucene_Search_Query_Wildcard($term); $results = $index->find($query); now it work perfect for unsorted and text fields  but it doesn`t search for keyword !! Are you sure you really want those fields to be keyword analyzed? The keyword analyzer puts the whole text of the field as one token which you rarely want. thanx a lot for your attention :) i found my mistake ;)"
876,A,"lucene index file randomly crash and need to reindex how you all deal wich such issue of occasionally need to reindex? what recommendation do you suggest to minimize this? Some background is needed: How large is what you are trying to index? What kind of crashes? (error messages)? In my experience with Lucene almost all crashes had to do with my programming errors... take liferay open source project for example. i want to list down measurements that able to minimize the crash If your Lucene instance is embedded in liferay I guess liferay should provide you with guidelines and logs about the Lucene indexes as well as ways to configure them. This is very different than using a bare Lucene index in your program. As a very general guideline I suggest you measure the overall number of documents you have in the index the number you index per hour and the optimize() interval. I believe a liferay user forum should give you a better answer. If you have a really large dataset then i would recommend you to maintain a second server that has a backup of your index so that you can have no single point of failure. You can synchronize this ""backup"" when ever you finish re-indexing and thus serve as a second search server that shares requests. If one fails then the other can back-it-up. This is a simple yet robust approach to your problem and recommended for small-to-medium setups."
877,A,".net lucene Multifield search I have created a index as Document doc = new Document(); doc.Add(new Field(""SearchKey"" (item.FullTextColumn ?? item.Code) Field.Store.NO Field.Index.TOKENIZED)); doc.Add(new Field(""Type"" item.Type.ToString() Field.Store.YES Field.Index.TOKENIZED)); doc.Add(new Field(""Name"" item.Name Field.Store.YES Field.Index.UN_TOKENIZED)); doc.Add(new Field(""Code"" item.Code ?? string.Empty Field.Store.YES Field.Index.UN_TOKENIZED)); etc and I am trying to search a term like ""Kansas City"" in ""SearchKey"" field and another filed ""Type"" must be ""Airport"" for that I am writing QueryParser parser = new QueryParser(""SearchKey"" analyzer); Query searchQuery = parser.Parse(text); TermQuery typeQuery = new TermQuery(new Term(""Type"" ""Airport"")); BooleanQuery filterQuery = new BooleanQuery(); filterQuery.Add(typeQuery BooleanClause.Occur.MUST); Filter f = new QueryFilter(filterQuery); Hits results = searcher.Search(searchQueryf); but it gives me NO result  if I remove 'f' from Hits results = searcher.Search(searchQueryf); then it gives result but ""Type"" field contain values other then ""Airport"" . any Idea where I am going wrong ? Looking at your code I think you need to add each query (one for the SearchKey and one for the Type) to the BooleanQuery like below. var standardLuceneAnalyzer = new StandardAnalyzer(); var query1 = new QueryParser(""SearchKey"" standardLuceneAnalyzer).Parse(""Kansas City*""); var query2 = new QueryParser(""Type"" standardLuceneAnalyzer).Parse(""Airport""); BooleanQuery filterQuery = new BooleanQuery(); filterQuery.Add(query1 BooleanClause.Occur.MUST); filterQuery.Add(query1 BooleanClause.Occur.MUST); TopDocs results = searcher.Search(filterQuery); I haven't tested the code but it should work."
878,A,"How can I check if a Lucene IndexWriter instance is valid/open? Sorry for the simple question but there doesn't seem to be any obvious way. According to the documentation it is recommended to keep a single instance of IndexWriter in memory that can be used again and again for updates as opposed to opening and closing one for each change (which is much more costly). However the documentation also states that the IndexWriter should be closed if an exception occurs (e.g. OutOfMemoryException). Therefore I need some way to check if my instance of IndexWriter is valid or not assuming it could be closed by some other operation. I realize I will get an exception if I try to use it when it's closed but rather than fail I would like to check to see if I need to create a new IndexWriter before I need it so no failure occurs. It seems to me a simple IsOpen property/method would be such an obvious addition. Thoughts: I suppose that if I could ensure that my class is the only thing capable of closing the IndexWriter I could simply set it to null when it was closed and just check to make sure it's not null whenever I go to use it however this would not handle cases where the IndexWriter closes itself (if such a case is possible now or in the future). Another thought is that I could wrap an attempt to use it in a try/catch block and use the exception to indicate that it needs to be reopened but this doesn't seem very efficient/elegant. What method would I use to test the validity of the IndexWriter? If you get an out of memory exception (or any kind of exception really) that's potentially a big problem. The changes you were trying to write probably won't get written and your index may even be corrupt depending on when/how the exception occurred. So an IndexWriter being in a faulted state is an exceptional circumstance and I would say it certainly warrants using exceptions i.e. try/catch. (I don't think your catch would be ""reopen the writer."" Depending on the exception you might need to reindex stuff possibly from scratch. Certainly you shouldn't expect to get exceptions in your writer as a regular occurrence.)  I suppose the IndexWriter would throw an AlreadyClosedException (see ensureOpen) if it is closed on access. So you could handle that in case the IndexWriter is not null. HTH"
879,A,"Lucene seems to be caching search results - why? In my project we use Lucene 2.4.1 for fulltext search. This is a J2EE project IndexSearcher is created once. In the background the index is refreshed every couple of minutes (when the content changes). Users can search the index through a search mechanism on the page. The problem is the results returned by Lucene seem to be cached somehow. This is scenario I noticed: I start the application and search for 'keyword' - 6 results are returned Index is refreshed using Luke I see that there are 8 results now to query 'keyword' I search again using the application again 6 results are returned. I analyzed our configuration and haven't found any caching anywhere. I have debugged the search and there is no caching in out code searcher.search returnes 6 results. Does Lucene cache results internally somehow? What properties etc. should I check? This is where lucene cache description is.  To see changes made by IndexWriters against an index for which you have an open IndexReader be sure to call IndexReader.reopen() to see the latest changes. Make sure also that your IndexWriter is committing the changes either through an explicit commit() a close() or having autoCommit set to true. I solved it by recreating IndexSearcher every time index is updated. This solved the issue. I think under the covers it does pretty much what you said. reopen() is more efficient as recreating it causes all the segment files to be read but reopen() knows to only read the segments that have been updated since the last open.  With versions prior to 2.9.0 Lucene cached automatically the results of queries. With later releases there's no caching unless you wrap your query in a QueryFilter and then wrap the result in a CachingWrapperFilter. You could consider switching to a release >= 2.9.0 if reopening the index becomes a problem  That's all I found about Lucene and caching. If that doesn't help I think you have to double check your application again if it doesn't resent an old query result. One of the issue you really don't need good luck!  One more note: In order to IndexReader find the real-time other threads updated documents when initialize IndexReader the parameter ""read-only"" has to be false. Otherwise method reopen() will not work."
880,A,ASP.NET search indexing building strategy This is what I'm planning to do and I'd appreciate anyone's input: I've built a forum in Asp.net MVC and now want to add Lucene.Net for search. My plan is to run an index builder thread every 5-10 minutes to update the search index with the changes made to the each discussion. The way it will work is I keep the date and time for the last run of the index builder thread in the search index. Then on every execution of the index builder I read this date back from the search then index any changes since that date and time. Once I'm done I then update the last run entry. Is this way good? Can someone suggest a better way to incrementally index changes in a forum app? You will need to maintain a timer... and if the indexing operation doesn't stop in 5 minutes another one will start indexing the same changes so you'll have to check for such condition as well. A slightly better way is to simply use a dedicated indexing Thread that stays alive. This Thread will fetch changes from the last run and process them as you describe but it will not wait. After the index operation finishes it'll re-start itself right away continually indexing as items are in. If there are no more items to index the Thread will then sleeps for 5 minutes (and then re-check for changes again when it wakes up). This way you can be sure that there will only be one client at a time modifying the indexes. It'll never take up a lot of CPU as might be the case if you mismanaged the timer somehow or you suddenly got a flood of posts and will scale as your forum grows without needing to adjust the indexing interval every now and then. You will need to monitor the Thread's health though. thanks for your response. I'm using Quartz.net to shcedule jobs to run in the background. I think I can fire a one off job in Quartz.net which never exits. What do you think about that? @Am sounds good :)... Haven't used Quartz before but I assumed that it can monitor the job's health and restart it if it crashes as well? ... Another thing is that you might want to keep some simple exception logs just in case. I especially chose quartz.net because it allows listening for different events (ie. start finish exception etc) which I've used for logging. I can then just simply check the logs and see which background thread is crashing. @Am It's a perfect fit then :)
881,A,"Lucene indexwriter destination folder I'm working on a small lucene project where i have to index a bunch of text files. so far i've managed to create the index i think. the code runs and i get a bunch of files called 0_.* fdt/fdx/fnm and more. what i want to know is can i pick a destination folder for the index to be created in? i'm following this Guide and i define a index folder and a files to index folder but i can't find any parameters in the indexwriter constructor that could achieve this. here's my code for creating the index public static void createIndex() throws CorruptIndexException LockObtainFailedException IOException { File[] files = FILES_TO_INDEX_DIRECTORY.listFiles(); Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_33); SimpleFSDirectory d = new SimpleFSDirectory(FILES_TO_INDEX_DIRECTORY); IndexWriter indexWriter = new IndexWriter(d analyzer IndexWriter.MaxFieldLength.LIMITED); for (File file : files) { Document document = new Document(); String path = file.getCanonicalPath(); byte[] bytes = path.getBytes(); document.add(new Field(FIELD_PATH bytes)); Reader reader = new FileReader(file); document.add(new Field(FIELD_CONTENTS reader)); indexWriter.addDocument(document); } indexWriter.optimize(); indexWriter.close(); } and i'm using type File instead of string for the directories public static File FILES_TO_INDEX_DIRECTORY = new File(""C:\\Users\\k\\Dropbox\\Public\\afgansprojekt\\RouteLogger\\Lucene\\FilesToIndex""); public static final File INDEX_DIRECTORY = new File(""C:\\Users\\k\\Dropbox\\Public\\afgansprojekt\\RouteLogger\\Lucene\\Index""); Actually you are setting the destination folder with SimpleFSDirectory d = new SimpleFSDirectory(FILES_TO_INDEX_DIRECTORY); Just change SimpleFSDirectory(FILES_TO_INDEX_DIRECTORY); to SimpleFSDirectory(INDEX_DIRECTORY);. Edit: File[] files = FILES_TO_INDEX_DIRECTORY.listFiles(); //this is where you set the files to index SimpleFSDirectory d = new SimpleFSDirectory(FILES_TO_INDEX_DIRECTORY); //here you are setting the index directory You should change this line to SimpleFSDirectory d = new SimpleFSDirectory(INDEX_DIRECTORY); missread the answer at first turns out it was correct"
882,A,"Java Lucene IndexReader not working correctly The story is this. I want to mimic the behavior of a relational database using a Lucene index in java. I need to be able to do searching(reading) and writing at the same time. For example I want to save Project information into an index. For simplicity let's say that the project has 2 fields - id and name. Now before adding a new project to the index I'm searching if a project with a given id is already present. For this I'm using an IndexSearcher. This operation completes with success (namely the IndexSearcher returns the internal doc id for the document that contains the project id I'm looking for). Now I want to actually read the value of this project ID so I'm using now an IndexReader to get the indexed Lucene document from which I can extract the project id field. The problem is that the IndexReader return a Document that has all of the fields NULL. So to repeat IndexSearcher works correctly IndexReader returns bogus stuff. I'm thinking that somehow this has to do with the fact that the document fields data does not get saved on the hard disk when the IndexWriter is flushed. The thing is that the first time I do this indexing operation IndexReader works good. However after a restart of my application the above mentioned situation happens. So I'm thinking that the first time around data floats in RAM but doesn't get flushed correctly (or totally since IndexSearcher works) on the hard drive. Maybe it will help if I give you the source code so here it is (you can safely ignore the tryGetIdFromMemory part I'm using that as an speed optimization trick): public class ProjectMetadataIndexer { private File indexFolder; private Directory directory; private IndexSearcher indexSearcher; private IndexReader indexReader; private IndexWriter indexWriter; private Version luceneVersion = Version.LUCENE_31; private Map<String Integer> inMemoryIdHolder; private final int memoryCapacity = 10000; public ProjectMetadataIndexer() throws IOException { inMemoryIdHolder = new HashMap<String Integer>(); indexFolder = new File(ConfigurationSingleton.getInstance() .getProjectMetaIndexFolder()); directory = FSDirectory.open(indexFolder); IndexWriterConfig config = new IndexWriterConfig(luceneVersion new WhitespaceAnalyzer(luceneVersion)); indexWriter = new IndexWriter(directory config); indexReader = IndexReader.open(indexWriter false); indexSearcher = new IndexSearcher(indexReader); } public int getProjectId(String projectName) throws IOException { int fromMemoryId = tryGetProjectIdFromMemory(projectName); if (fromMemoryId >= 0) { return fromMemoryId; } else { int projectId; Term projectNameTerm = new Term(""projectName"" projectName); TermQuery projectNameQuery = new TermQuery(projectNameTerm); BooleanQuery query = new BooleanQuery(); query.add(projectNameQuery Occur.MUST); TopDocs docs = indexSearcher.search(query 1); if (docs.totalHits == 0) { projectId = IDStore.getInstance().getProjectId(); indexMeta(projectId projectName); } else { int internalId = docs.scoreDocs[0].doc; indexWriter.close(); indexReader.close(); indexSearcher.close(); indexReader = IndexReader.open(directory); Document document = indexReader.document(internalId); List<Fieldable> fields = document.getFields(); System.out.println(document.get(""projectId"")); projectId = Integer.valueOf(document.get(""projectId"")); } storeInMemory(projectName projectId); return projectId; } } private int tryGetProjectIdFromMemory(String projectName) { String key = projectName; Integer id = inMemoryIdHolder.get(key); if (id == null) { return -1; } else { return id.intValue(); } } private void storeInMemory(String projectName int projectId) { if (inMemoryIdHolder.size() > memoryCapacity) { inMemoryIdHolder.clear(); } String key = projectName; inMemoryIdHolder.put(key projectId); } private void indexMeta(int projectId String projectName) throws CorruptIndexException IOException { Document document = new Document(); Field idField = new Field(""projectId"" String.valueOf(projectId) Store.NO Index.ANALYZED); document.add(idField); Field nameField = new Field(""projectName"" projectName Store.NO Index.ANALYZED); document.add(nameField); indexWriter.addDocument(document); } public void close() throws CorruptIndexException IOException { indexReader.close(); indexWriter.close(); } } To be more precise all the problems occur in this if: if (docs.totalHits == 0) { projectId = IDStore.getInstance().getProjectId(); indexMeta(projectId projectName); } else { int internalId = docs.scoreDocs[0].doc; Document document = indexReader.document(internalId); List<Fieldable> fields = document.getFields(); System.out.println(document.get(""projectId"")); projectId = Integer.valueOf(document.get(""projectId"")); } On the else branch... I don't know what is wrong. I had a hard time figuring out how to index/search numbers and I just wanted to say the following snippets of code really helped me out: projectId = Integer.valueOf(document.get(""projectId"")); //////////// Field idField = new Field(""projectId"" String.valueOf(projectId) Store.NO Index.ANALYZED); document.add(idField); Thanks!  Do you store the respective fields? If not the fields are ""only"" stored in the reverse index part i.e. the field value is mapped to the document but the document itself doesn't contain the field value. The part of the code where you save the document might be helpful. Yeah! That was it. Got confused by to much indexing. Thank you!"
883,A,"Extract terms from query for highlighting I'm extracting terms from the query calling ExtractTerms() on the Query object that I get as the result of QueryParser.Parse(). I get a HashTable but each item present as: Key - term:term Value - term:term Why are the key and the value the same? And more why is term value duplicated and separated by colon? Do highlighters only insert tags or to do anything else? I want not only to get text fragments but to highlight the source text (it's big enough). I try to get terms and by offsets to insert tags by hand. But I worry if this is the right solution. It is because .Net 2.0 doesnt have an equivalent to java's HashSet. The conversion to .Net uses Hashtables with the same value in key/value. The colon you see is just the result of Term.ToString() a Term is a fieldname + the term text your field name is probably ""term"". To highlight an entire document using the Highlighter contrib use the NullFragmenter  I think the answer to this question may help."
884,A,"ToTitleCase in solr to stop SCREAMING CAPS in Solr I'm using solr's faceting and i've run into a problem that i was hoping i could get around using filters. Basically some times a town name will come through to SOLR as ""CAMBRIDGE"" and sometime's it will come through as ""Cambridge"" I wanted to use a filter in Solr to stop the SCREAMING CAPS version of the town name. It seems there is a fitler to make all the text lower case. <!-- A text field that only sorts out casing for faceting --> <fieldType name=""text_facet"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> </analyzer> <analyzer type=""query""> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> </analyzer> </fieldType> I was wondering if anyone knew of a filter which will Ignore the First character of a word and apply lowercase to the rest of the characters. E.g. CAMBRIDGE >> Cambridge KingsTON Upon HULL >> Kingston Upon Hull etc Alternatively if it's easy to write your own filters.. some help on how to do that would be appreciated.. I'm not a Java person.. Thanks Perhaps you could make use of the solr.PatternReplaceCharFilterFactory? <fieldType name=""textCharNorm"" class=""solr.TextField""> <analyzer> <filter class=""solr.LowerCaseFilterFactory""/> <charFilter class=""solr.PatternReplaceCharFilterFactory"" pattern=""([^\s]{1})([^\s]*)"" replaceWith=""\U$1\L$2""/> </analyzer> </fieldType> Notice I haven't tested the code or solr.PatternReplaceCharFilterFactory so I'm not sure if it works. If you need to build your own filter this guide might be useful: http://robotlibrarian.billdueber.com/building-a-solr-text-filter-for-normalizing-data/ // John Thanks.. i'll check that out.  AFAIK there is no built-in filter like that. If you want to write it see LowerCaseFilterFactory and LowerCaseFilter for reference it doesn't seem to be very hard. Or you could do this client-side i.e. in SolrNet you could write a ISolrOperations decorator that does the necessary transformations after the real query using ToTitleCase. I'm using a very old version of SolrNet.. So i'll see if i can use the ISolrOperations. However I think it might be about time to learn java.. I know c# so the syntax shouldn't be a problem.. Thanks Mauricio ISolrOperations has been around since revision 1 :-) Anyway I recommend upgrading to the latest version... Nice i'll deffo do that."
885,A,"How to order search results by relevance and another field in Lucene.net I have a requirement to sort search results by relevance and another field. I need to do something similar to this: using Lucene.Net.Search; SortField[] fields = new[] { SortField.SCORE new SortField(""customField"") }; Sort sort = new Sort(fields); IndexSearcher searcher = GetSearcher(); Hits = searcher.Search(query sort); Except SortField.SCORE is an integer constant not a SortField. Lucene.net version 2.3.1.3. Has anyone come across this? Found an answer to this: SortField.FIELD_SCORE Not sure how or why I missed this... Same code as in the question but change `SortField.SCORE` to `SortField.FIELD_SCORE`. Can you write more informations about how to implement your solution?"
886,A,"How to change default conjunction with Lucene MultiFieldQueryParser I have some code using Lucene that leaves the default conjunction operator as OR and I want to change it to AND. Some of the code just uses a plain QueryParser and that's fine - I can just call setDefaultOperator on those instances. Unfortunately in one place the code uses a MultiFieldQueryParser and calls the static ""parse"" method (taking String String[] BooleanClause.Occur[] Analyzer) so it seems that setDefaultOperator can't help because it's an instance method. Is there a way to keep using the same parser but have the default conjunction changed? The MultiFieldQueryParser class extends the QueryParser class. Perhaps you could simply configure an instance of this class rather than relying on its static methods? If you really need to configure the BooleanClause.Occur values you could do it afterward. String queryString = ...; String[] fields = ...; Analyzer analyzer = ...; MultiFieldQueryParser queryParser = new MultiFieldQueryParser(fields analyzer); queryParser.setDefaultOperator(QueryParser.Operator.AND); Query query = queryParser.parse(queryString); // If you're not happy with MultiFieldQueryParser's default Occur (SHOULD) you can re-configure it afterward: if (query instanceof BooleanQuery) { BooleanClause.Occur[] flags = ...; BooleanQuery booleanQuery = (BooleanQuery) query; BooleanClause[] clauses = booleanQuery.getClauses(); for (int i = 0; i < clauses.length; i++) { clauses[i].setOccur(flags[i]); } } That's good thanks. The missing step was how to configure the Occur values afterwards. Another approach I'm toying with is that the code for MultiFieldQueryParser.parse is tiny so I might just paste that into my application and modify it. It creates QueryParser instances itself so I can just tweak it to set the default operator on them. Within the iterated for loop you can also use the following to change the conjunction per field: QueryParser parser = new QueryParser(Version.YOUR_VERSION fields[i] new YourAnalyzer()); parser.setDefaultOperator(QueryParser.Operator.AND); clause.setQuery(parser.parse(clause.getQuery().toString(fields[i])));"
887,A,"Is it possible to add custom metadata to a Lucene field? I've come to the point where I need to store some additional data about where a particular field comes from in my Lucene.Net index. Specifically I want to attach a guid to certain fields of a document when the field is added to the document and retrieve it again when I get the document from a search result. Is this possible? Edit: Okay let me clarify a bit by giving an example. Let's say I have an object that I want to allow the user to tag with custom tags like ""personal"" ""favorite"" ""some-project"". I do this by adding multiple ""tag"" fields to the document like so: doc.Add( new Field( ""tag"" ""personal"" ) ); doc.Add( new Field( ""tag"" ""favorite"" ) ); The problem is I now need to record some meta data about each individual tag itself specifically a guid representing where that tag came from (imagine it as a user id). Each tag could potentially have a different guid so I can't simply create a ""tag-guid"" field (unless the order of the values is preserved---see edit 2 below). I don't need this metadata to be indexed (and in fact I'd prefer it not to be to avoid getting hits on metadata) I just need to be able to retrieve it again from the document/field. doc.GetFields( ""tag"" )[0].Metadata... (I'm making up syntax here but I hope my point is clear now.) Edit 2: Since this is a completely different question I've posted a new question for this approach: Is the order of multi-valued fields in Lucene stable? Okay let's try another approach... The key problem area is the indeterminacy of the multiple field values under the same field name (e.g. ""tag""). If I could introduce or obtain some kind of determinacy here I might be able to store the metadata in another field. For example if I could rely on the order of the values of the field never changing I could use an index in the set of values to identify exactly which tag I am referring to. Is there any guarantee that the order I add the values to a field will remain the same when I retrieve the document at a later time? @Prescott the problem is I am adding multiple values for the same field (e.g. ""tag"") and I need to keep track of certain origin information for each tag so I have no way of just adding a new field to track it because I can't identify them uniquely. @chaiguy I'm having some trouble fully understanding what you mean - could you toss out a super simple example? @chaiguy Prescott is right you can add multiple fields to document only for storing - it is a common practice trying to clarify here: Assume the following definition Document { DocId Text Author CreatedDate }. Are you saying that you'd like to also add additional data to individual fields. In this case suppose you want to tag Author with a GUID so add a new field AuthorGUID for this document? @chaiguy - Just curious whats your use case for adding metadata to metadata? Intuitively I wouldn't think of tagging individual fields but the document as a whole See updated question--I can't just add a new field because fields support multiple values and I need metadata for *each* value in a single field. I suppose perhaps I could simply append it to the value somehow but would this mess up the indexing/searching? For example if I used new Field( ""tag"" ""favorite|metadata"" )... ? hmm how about something like new Field(""tags"" ""personal|date favoriate|date"") some what using Xodarap's idea of payloads below? @chaiguy - yes that *would* but using Xodarap's link to payloads you can make sure it doesn't could you just add it to your document: document.Add(new Field(""GUID"" ""guidvalue"" Field.Store.YES Field.Index.NO)); Depending on your search requirements for this index this may be possible. That way you can control the order of fields. It would require updating both fields as the tag list changes of course but the overhead may be worth it. doc.Add(new Field(""tags"" ""{personal}|{favorite}"")); doc.Add(new Field(""tagsref"" ""{1234}|{12345}"")); Note: using the {} allows you to qualify your search for uniqueness where similar values exist. Example: If values were stored as ""person|personal|personage"" searching for ""person"" would return a document that has any one of person personal or personage. By qualifying in curly brackets like so: ""{person}|{personal}|{personage}"" I can search for ""{person}"" and be sure it won't return false positives. Of course this assumes you don't use curly brackets in your values. This is what I ended up doing. Can you explain further what you mean by the {} characters? Is this just for searching or for storing field data as well (as you are doing)? Updated with a bit more detail on curly brackets.  I think you're asking about payloads. Edit: From your use case it sounds like you have no desire to use this metadata in your search you just want it there. (Basically you want to use Lucene as a database system.) So why can't you use a binary field? ExtraData ed = new ExtraData { Tag = ""tag"" Type = ""personal"" }; byte[] byteData = BinaryFormatter.Serialize(ed); // this isn't the correct code but you get the point doc.Add(new Field(""myData"" byteData Field.Store.YES)); Then you can deserialize it on retrieval. That seems more useful for weightings as Grant was using it for. I could see how you could expand it to tag pieces of your data but it seems more relevant for special handling within a field itself (ie ""Text Data"" -> ""Text|10 Data|5"" - adding different weights. In this case I think he wants something like ""Text Data""|GUID - so that GUID refers to the entire field not pieces of the field and not to the whole document I skimmed that page briefly but payloads seem *way* too complicated for what I need. It appears I would have to write a completely custom analyzer and generate token streams manually. I'm curious about these boosts though... would it be possible to segment the string value of a field such that I could attach metatdata and assign it a boost of 0 so that it would be ignored by queries? Is this supported by the standard analyzer or would I have to use another/write my own? @chaiguy: I've edited my answer: is this more of what you're looking for? You're correct in that I have no desire to use the metadata in a search but I *do* want to be able to use the field's value (e.g. the tag) in a search. I've updated the question with another approach. @chaiguy: Why can't you just have one field for searching and one for this metadata? I could if I can be sure the indexes will match up and not change on me as per my new approach. @chaiguy: you can store whatever you want in a binary field e.g. a hash table. You can be sure that this won't change."
888,A,How do I remove logically deleted documents from a Solr index? I am implementing Solr for a free text search for a project where the records available to be searched will need to be added and deleted on a large scale every day. Because of the scale I need to make sure that the size of the index is appropriate. On my test installation of Solr I index a set of 10 documents. Then I make a change in one of the document and want to replace the document with the same ID in the index. This works correctly and behaves as expected when I search. I am using this code to update the document: getSolrServer().deleteById(document.getIndexId()); getSolrServer().add(document.getSolrInputDocument()); getSolrServer().commit(); What I noticed though is that when I look at the stats page for the Solr server that the figures are not what I expect. After the initial index numDocs and maxDocs both equal 10 as expected. When I update the document however numDocs is still equal to 10 (expected) but maxDocs equals 11 (unexpected). When reading the documentation I see that maxDoc may be larger as the maxDoc count includes logically deleted documents that have not yet been removed from the index. So the question is how do I remove logically deleted documents from the index? If these documents still exist in the index do I run the risk of performance penalties when this is run with a very large volume of documents? Thanks :) You have to optimize your index. Note that an optimize is expansive you probably should not do it more than daily. Here is some more info on optimize: http://www.lucidimagination.com/search/document/CDRG_ch06_6.3.1.3 http://wiki.apache.org/solr/SolrPerformanceFactors#Optimization_Considerations Thanks - that was exactly what I needed :)
889,A,Lucene searching by numeric values I'm building a Java Lucene-based search system that on addition adds a certain number of meta-fields one of which is a sourceId field which denotes where the entry came from. I'm now trying to retrieve all documents from a particular source but the index doesn't appear to be able to find them. However if I search for a wildcard value the returned documents all have the correct value for this field. The lucene query I'm using is quite simple basically index-source-id:1 but that fails to return any hits if I search for content:a* I get dozens of documents all of which when asked return the value 1 for the index-source-id value which is correct. Any ideas? I have only worked with the PHP port however have you checked what text analyzer you are using? This FAQ seems to indicate that like the PHP version you need to use a diffrent one that doesn't remove digits. You can find a list of analyzers here Just to be sure you have set the id to be indexable? I have set the ID to be indexable yup. I was looking for a list of Analyzers but couldn't find one that said it particularly dealt with numbers it appears StandardAnalyzer does which I thought had been deprecated so perhaps that might help. Rebuilding the index and then searching with StandardAnalyzer instead of SimpleAnalyzer did the trick! For future reference you do not want to analyze (nor tokenize) id fields since they're supposed to be atomic by nature and as Einstein showed us with his buddies in Manhattan Project splitting atoms isn't a good thing to do...
890,A,"Using IndexReader IsLocked and Unlock methods Before calling AddDocument() on IndexWriter is it okay if I call IndexReader.IsLocked(myDirectory) and if it returns true then call IndexReader.Unlock(myDirectory) i.e. if(IndexReader.IsLocked(myDirectory)) { IndexReader.Unlock(myDirectory); } writer = new IndexWriter(myDirectory _analyzer true); writer.AddDocument(doc); I keep getting ""Lock obtain timed out."" errors in my code. To overcome this error I plan to this approach if it is okay. Getting the ""Lock obtain timed out"" error is a warning sign that something is wrong with the way you handle your index. If you have more than one IndexWriter writing to the index forcing unlock would likely cause your index to get corrupted. However in my experience it's easy to get those errors when you're working on the code since the occasional crashes and interrupted debug sessions can leave you index locked even though no process is writing to it anymore. If that is the case it would be OK to unlock the index at the start of the process. Don't call it every time before calling addDocument just once when creating the IndexWriter. In any case make sure you close all IndexWriters properly before exiting the process. thanks for ur valuable inputs...am a newbie to Lucene...just wanted to know the things i need to take care of when I'm exposing Lucene search API through a web application where concurrent users could be updating Lucene index at the same time....  Important point to remember with Lucene only one thread should be updating the index... so there are concurrent users on the website but one only user should be updating. If you do not handle that properly you will run into problems... You can have multiple reads/queries but not writes thanks for ur comments...can you please also look into this one? http://stackoverflow.com/questions/899542/problem-using-same-instance-of-indexsearcher-for-multiple-requests"
891,A,"Searching with Solr Sphinx or Lucene - ranking search results by clicks I want to implement a search that will rank results higher if previous similar searches have led users to click on a result. Is that possible with either Solr (Lucene) or Sphinx? With Sphinx you could use additional attribute clicks_count and use such query to rank clicked documents higher SELECT * clicks_count*1000 AS cc FROM your_index WHERE MATCH (""words to match"") ORDER BY cc DESC; to get only clicks wight into account or SELECT * weght() + clicks_count*10000 AS cc FROM your_index WHERE MATCH (""words to match"") ORDER BY cc DESC; to get match weight with clicks weight into account Of course you have update your counter 'clicks_count'.  I think tracking of user clicks is necessary. (if the higher ranking depends on user clicks) For ""ranking higher"" the results maybe the solr-elevator function could be helpful for your needs: http://wiki.apache.org/solr/QueryElevationComponent Probably the elevation-function is more helpful than the lucene boost function (in your case). http://lucene.apache.org/java/2_4_0/queryparsersyntax.html#Boosting%20a%20Term Finlay it depends of the kind of implementation i think.  It's certainly possible with Solr (Lucene) but not really feasible. What you'd have to do is: Track the clicks of users Normalize the search query to group similar queries and store them Reindex that into Solr If you ask me that sounds like a lot of work with a lot of pitfalls. It was a client request. Personally I don't think it makes a lot of sense. It might just be an indicator that some results (e.g. the headlines or short descriptions) are better. Still the underlying content could be completely irrelevant. That would then generate a lot of clicks results would get ranked higher and more users would click on presumably good results in vain. But thank you for your assessment. Oh right — I didn't even think about the aspect of a self-fulfilling prophecy: The first link is irrelevant but on top so it gets clicked. The algorithm would now tag it as relevant..."
892,A,"Lucene query permutation I have a question regarding performing a lucene query involving permutation. Say I have two fields: ""name"" and ""keyword"" and the user searches for ""joes pizza restaurant"". I want some part of that search to match the full contents of the ""name"" field and some part to match the full content of the keyword field. It should match all the the supplied terms and should match the entire contents of the fields. For example it could match: 1) name:""joes restaurant"" keyword:""pizza"" 2) name:""joes pizza"" keyword:""restaurant"" 3) name:""pizza restaurant"" keyword:""joes"" 4) name:""pizza"" keyword:""joes restaurant"" 5) name:""pizza joes"" keyword:""restaurant"" but it would not match 6) name:""big joes restaurant"" keyword:""pizza"" - because it's not a match on the full field 7) name:""joes pizza restaurant"" keyword:""nomatch"" - because at least one of the terms should match to the keyword field I've thought about possible ways to implement this by calculating all the permutations of the fields and using boolean queries however this doesn't scale very well as the number of terms increases. Anyone have any clues how to implement this sort of query efficiently? Let's divide your query into three parts: Both the 'name' field and the 'keyword' field should contain part of the query. Both matches should be to the full field. The union of the matches should cover the query completely. I would implement it this way: Create a boolean query composed of the tokens in the original query. Make it a disjunction of 'MUST' terms. e.g. in the example something like: (name:joes OR name:restaurant OR name:pizza) AND (keyword:joes OR keyword:restaurant OR keyword:pizza) Any document matching this query has a part of the original query in each field. (This could be a ConstantScoreQuery to save time). Take the set of matches from the first query. Extract the field contents as tokens and store them in String sets. Keep only the matches where the union of the sets equals the string set from your original query and the sets have an empty intersection. (This handles the covering - item 3 above). For your first example we will have the sets {""joes"" ""restaurant""} and {""pizza""} fulfilling both conditions. Take the set sizes from the matches left and compare them to the field lengths. For your first example we will have set sizes of 2 and 1 which should correspond to field lengths of 2 and 1 respectively. Note that my items 2 and 3 are not part of the regular Lucene scoring but rather external Java code.  Lucene docs recommend using separate field which is concatenation of 'name' and 'keyword' fields for queries spanning multiple fields. Do the search on this field. Combined with the answer to go this sounds like reasonable approach."
893,A,"Using JBoss Cache as directory for Apache Lucene Has anyone tried to store Lucene index in JBoss Cache? Are there any good implementations of Lucene Directory for it? I found sources only for this one but I can't find any documentation or testimonials on it. Basically what I would like to do is to store Lucene index in JBoss Cache and manipulate it with application written with GridGain support (GridGain supports JBoss Cache almost out of the box). Please share your thoughts. JBossCache isn't really suitable as a directory medium for lucene because it has no facility for file locking and its atomicity is fuzzy at best. None of this makes for a good consistent index. You may be interested to know what the Compass Project does (Compass does for Lucene what Hibernate does for JDBC). Compass supports the storage of the index using a number of 3rd party distribution technologies but JBoss Cache isn't one of them. I believe it was tried and abandoned because it just didn't work. Thanks. I'm using Compass for a long time and familiar with ""needle"" (I was using it with Terracotta) but storing really large and constantly changing index in Terracotta can be painful since free version allows only one active server and limits number of stored objects to Integer.MAX_VALUE (at the moment - it can be changed in the future). And of course I don't want to ""bind"" the system to the static topology. Also Compass doesn't provide interface to Lucene's HitCollector. Have you tried posting on the compass forum? It's pretty good for getting anseers to this sort of thing."
894,A,User Interface for Katta Index I m developing a User interface for Katta Index. For that I need to find the indexed field's names (and if possible its data type how it was indexed either normal string or Float or Int or long etc...) and number of documents the index contains.... --Thanks in advance According to the katta documentation ...a Katta index is basically just a folder with Lucene index sub folders... So I would try to use luke to view the index structure. I see this advice appears also under common problems in katta.
895,A,"In Lucene how do terms get used in calculating scores can I override it with a CustomScoreQuery? Has someone successfully overridden the scoring of documents in a query so that the ""relevancy"" of a term to the field contents can be determined through one's own function? If so was it by implementing a CustomScoreQuery and overriding the customScore(int float float)? I cannot seem to find a way to build either a custom sort or a custom scorer that can rank exact term matches much higher than other prefix term matches. Any suggestions would be appreciated. I don't know lucene directly but I can tell you that Solr an application based on lucene has got this feature: Boosting query via functions Let me know if it helps you. I'm accepting this because it's the only answer it is an answer about Solr though but it's somewhat helpful. Did you find a solution to your problem?"
896,A,How do I NOT analyze a clause in the lucene query parser? I'm using the Lucene query parser for a simple search front-end and I'm running into some problems. Each record that I'm storing has some fields that are analyzed and some fields that are not analyzed. When I try to use the query parser to construct a query that looks over both the analyzed an not analyzed fields the analyzer is processing both fields which means that the non-analyzed field will never get a match. Is there any way to tell the query parser to NOT analyze a field? You can use a PerFieldAnalyzerWrapper for defining specific analysis per field. The PerFieldAnalyzerWrapper should be used both for indexing and retrieval. Alternatively you can use Solr and define the analysis in the Solr schema.
897,A,"Zend_Search_Lucene changing term frequency problem I am trying to update the searching of terms of documents within my Lucene index. Currently the searches score on the number of times the term appears in the document. What I would like to do is score if the term exists rather than the number of times the term exists. So a document with the term in it once scores the same as a document with the term in it 100 times. I've tried to extend the Zend_Search_Lucene_Search_Similarity with my own class but to be honest I am not sure if this is working correctly as the scores are still quite low. class MySimilarity extends Zend_Search_Lucene_Search_Similarity{ //override the default frequency of searching public function tf($freq){ return 1.0; } public function lengthNorm($fieldName $numTerms) { return 1.0/sqrt($numTerms); } public function queryNorm($sumOfSquaredWeights) { return 1.0/sqrt($sumOfSquaredWeights); } public function sloppyFreq($distance) { return 1.0; } public function idfFreq($docFreq $numDocs) { return log($numDocs/(float)($docFreq+1)) + 1.0; } public function coord($overlap $maxOverlap) { return $overlap/(float)$maxOverlap; } } Now this is built from examples I have found when searching good old google. However the only real change I've done has been to the tf() function. Any help with this and I would be really greatful as at the moment it's really messing up my searches. Thanks Grant I would try two things to debug this: Build a really small index - two documents a single field in each the first having the word ""boat"" and the second the phrase ""boat boat"". Test your search on that. Try to override only the tf() function. This is the change you want. Overriding other parts such as the norm requires reindexing using the new similarity function. Make sure you actually need this before reindexing. Overall changing the tf() function seems the right thing to do. This provided that you only want a relative order and do not care about the absolute score. What would be the best way of getting the absolute score? Would it be idfFreq()??? Thanks Grant Thank Yuval those documents pointed me in the right direction. Why you need the absolute score? I suggest you read http://lucene.apache.org/java/2_4_0/scoring.html and http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/Similarity.html Java Lucene has a handy explain() function that describes why a document got its score. I couldn't find one in Zend but you may have better luck. Anyway for search you only need the proper order of documents hence the relative score is the important one."
898,A,"searching in solr for specific values with dismax I'm using the dismax handler to perform solr search over records (boosting some fields). In my index I have a RetailerId for each document as well as other fields. My query needs to search for documents that have this RetailerId as well as keywords: localhost:8983/solr/select?qt=dismax&q=RetailerId:(27 OR 92) AND socks What is the syntax for such a query? Thanks! if you want to filter by facet user eDismax (extended disMax) that way you can say for instance q= your query AND face_name:""facet value""  Dismax does not support boolean operators. For a query like the one you described you need to use the Standard Query Handler. UPDATE I have made a couple of tests and the fq parameter seems to work with dismax: /select?qt=dismax&q=socks&fq=RetailerId:(27 OR 92) That may be a problem. Is there any way to make this happen with Dismax? Can I add another param that filters on these for example facets etc? See my update about the fq param Magic thank you so much for your help. thx for the update good one +1 It's simple and it works thanks."
899,A,"Zend_Search_Lucene vs SOLR I have recenlty stumbled into Zend Lucene port of Lucene project. I have a little bit experience with SOLR so I would like to know what is the difference between two of them especially from performance and installation side. As much as I know SOLR requires Tomcat serverlet running in web hosting in order to work what about Zend Lucene library? I am also a bit confused what means ""being implemented on the top of Lucene""? The answer above takes some comments from this comparison between the two. For a more in-depth analysis I would recommend looking at this: Comparison between Solr and Zend Lucene.  Java Lucene and all its ports to other languages including Zend Lucene are search libraries. This means that in order to use Zend Lucene you have to wrap it in other (PHP) code that will integrate the search with the rest of your application. The code generally needs to manage indexing retrieval and usually some housekeeping of Lucene. You communicate with Zend Lucene using PHP function calls. Solr OTOH is a search server built on top of Lucene. This means that a Solr instance can run as a stand-alone server webapp inside a servlet container (That could be Tomcat Jetty or one of several other such programs). It is much easier to set up a Solr server than a Lucene application. You can do a lot with Solr without writing a single line of Java - just by tweaking some XML configuration files. Setting up a Solr server may take as few as several minutes. The default way to communicate with Solr is using HTTP calls. So basically Zend Lucene installation requires having a PHP server and proper indexing and retrieval using a PHP library. Solr installation requires running a Java servlet container and deploying a war file into it. Regarding performance Solr has many of Lucene caching and other parameters optimized. Also I believe Zend Lucene is slower than Java Lucene so my bet is that Solr would be faster but this really depends on the specific application. Yeah Zend Lucene is no match for Java performance-wise."
900,A,"Make lucene treat all terms in a field as a single term In my Lucene documents I have a field ""company"" where the company name is tokenized. I need the tokenization for a certain part of my application. But for this query I need to be able to create a PrefixQuery over the whole company field. Example: My Brand my brand brahmin farm brahmin farm Regularly querying for ""bra"" would return both documents because they both have a term starting with bra. The result I want though would only return the last entry because the first term starts with bra. Any suggestions? Create another indexed field where the company name is not tokenized. When necessary search on that field rather than the tokenized company name field. If you want fast searches you need to have index entries that point directly at the records of interest. There might be something that you can to with the proximity data to filter records but it will be slow. I see the problem as: how can a ""contains"" query over a complete field be performed efficiently? You might be able to minimize the increase in index size by creating (for each current field) a ""first term"" field and ""remaining terms"" field. This would eliminate duplication of the first term in two fields. For ""normal"" queries you look for query terms in either of these fields. For ""startswith"" queries you search only the ""first term"" field. But this seems like more trouble than it's worth. This would be a solution but would also increase my index quite a lot. I would have to duplicate all my fields that way (about 15) for 2500K+ records. I was hoping to find a way to simply do a startswith over a complete field  Use a SpanQuery to only search the first term position. A PrefixQuery wrapped by SpanMultiTermQueryWrapper wrapped by SpanPositionRangeQuery: <SpanPositionRangeQuery: spanPosRange(SpanMultiTermQueryWrapper(company:bra*) 0 1)>"
901,A,"What is the biggest size / number of documents of index - java lucene 3.0.2 on 32 bit OS I am playing around with lucene and 40GB of data (~500M of tuples 2 fields behaving like key - value). I have created -- a suprise -- a 35 GB index which does not work. Therefore I want to create a set of smaller indicies but for that I need information about maximum size. ""Does not work"": * I get no results when I search in the index. * I can not retrieve the first document in the index. * Luke (http://www.getopt.org/luke/) can not open my index and print out out an error message: out of index exception Can you please elaborate on ""index does not work""? What filesystem do you use? Are you absolutely sure that you have created a valid index? How exactly are you indexing your data? Theoretically you shouldn't be at the maximum by a long shot. Limitations When referring to term numbers Lucene's current implementation uses a Java int which means the maximum number of unique terms in any single index segment is 2147483648. This is technically not a limitation of the index file format just of Lucene's current implementation. Similarly Lucene uses a Java int to refer to document numbers and the index file format uses an Int32 on-disk to store document numbers. This is a limitation of both the index file format and the current implementation. Eventually these should be replaced with either UInt64 values or better yet VInt values which have no limit. http://lucene.apache.org/java/3_0_0/fileformats.html#Limitations I am using the index as a map (key value). The keys are indexed and normalized. The values are stored. My platform is Windows XP on NTFS.  Are you using MMapDirectory and a 32-bit VM? If so the address space is not enough to cover the whole index and that might have caused the problem. In that case you need to use SimpleFSDirectory or NIOFSDirectory instead. Note that functions like FSDirectory.open(File) return a FSDirectory which might or might not be a MMapDirectory."
902,A,"How to score a small set of docs in Lucene I would like to compute the scores for a small number of documents (rather than for the entire collection) for a given query. My attempt as follows returns 0 scores for each document even though the queries I test with were derived from the terms in the documents I am trying to score. I am using Lucene 3.0.3. List<Float> score(IndexReader reader Query query List<Integer> newDocs ) { List<Float> scores = new List<Float>(); IndexSearcher searcher = reader.getSearcher(); Collector collector = TopScoreDocCollector.create(newDocs.size() true); Weight weight = query.createWeight(searcher); Scorer scorer = weight.scorer(reader true true); collector.setScorer(scorer); float score = 0.0f; for(Integer d: newDocs) { scorer.advance(d); collector.collect(d); score = scorer.score(); System.out.println( ""doc: "" + d + ""; score="" + score); scores.add( new Float(score) ); } return scores; } I am obviously missing something in the setup of scoring but I cannot figure out from the Lucene source code what that might be. Thanks in advance Gene Why don't you use a Filter? See http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/search/Searcher.html#search%28org.apache.lucene.search.Query%20org.apache.lucene.search.Filter%20int%29 Yes. You should first try a high-level approach such as filtering before delving into the low-level scoring. Or we are missing something. In that case please tell us why you need to score just this set; Is this performance? or a special application? Still in both these cases I believe you can find an existing mechanism in Lucene that is easier to use. Yes I am looking to improve performance. I have a previously-cached result list of approx 1000 docs and I know that the newly-added docs are a likely match to this query. I would like to do the minimal amount of computation to update the search results. Thanks for the Filter tip. It seems to be the right way to go but I am having a problem actually getting results out of it. I created a SortedVIntList DocIdSet which I return in an anonymous Filter() class. When I step through the code the docid I put in (14186) appears to be two off from what the reader has (14188) which means the document never gets scored. Where am I going wrong? Use a filter and do a search with that filter. Then just iterate through the results as you would with a normal search - Lucene will handle the filtering. In general if you are looking at DocIds you're probably using a lower-level API than you need to and it's going to give you trouble."
903,A,"Getting exact matches in Lucene using the standard analyzer? Given 2 documents with the content as follows ""I love Lucene"" ""Lucene is nice"" I want to be able to query lucene only for those documents with Lucene in the beginning  i.e  everything that will match the regexp ""^Lucene .*"". Is there a way to do it  provided that I can't change the index  and it was analyzed using the standard analyzer? Sure take a look at SpanFirstQuery. Here is a good tutorial: http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/"
904,A,"How to combine BooleanQuery and Filter for Lucene? I'm wondering if there's a way to perform a filtered search in Lucene index with a combination of Filter and Query instead of a BooleanQuery? Since performing a query causes calculation of relevance rating we don't really need this when filtering documents by category or location. For example: we have a query ""happy new year"" which should be performed within ""Greeting Cards / XMas"" category. So what we really need is to do a search within the category and then order the results by a relevance rating based on a query text. If we try to do it with a BooleanQuery relevance rating calculation considers every term in a query (if I understand the mechanics correctly). See the api - note how you can pass in both a filter and a query. I wonder why I haven't noticed this before :) Thanks a lot."
905,A,"Logging Search Keywords in Solr / Lucene I'm new to Solr and am looking for a way to record searches (or keywords) to a log file or database so that I can then analyse for data visualisation. Can Solr do this already? Is this data accessible via. a Solr query? Thanks. Update 1 I'm starting to think I might need to write my own Solr analyzer? Good question... I hope you find a solution. what client platform are you using? The SolrLogging wiki page says you can use JDK logging (in Solr 1.0 to 1.3) or slf4j logging in Solr 1.4. About your own Solr analyzer - it depends on your needs. In many cases using your own analyzer helps for specific retrieval requirements. This would be very appropriate because you might want to use jdbc to record those findings. Depending on the ferocity of the traffic making a synchronous jdbc call during a search might be a bottleneck though. Might be better to log to a file and parse in a separate process.  Months later ... maybe someone is interested: http://karussell.wordpress.com/2010/10/27/feeding-solr-with-its-own-logs/ (you'll need to adapt the log parser if you are not using the default solr output format) BTW: check out logstash! http://code.google.com/p/logstash/  You can look at something like logstash to parse your log data.  I think it depends on what you are looking to log? Are you just looking to record the queries users are submitting as well as the results? If it's just ""what are folks searching for"" then you have that data in the q parameter that is logged by the servlet container. If you are using the default Jetty setup look at ./logs/*request.log. You will see lines like: 0:0:0:0:0:0:0:1%0 - - [21/01/2010:15:08:29 +0000] ""GET /solr/select/?q=*:*&qt=geo&lat=45&long=15&radius=10 HTTP/1.1"" 200 197 In this case you can parse out that the user was doing a q=: search! Use a tool like AWStats to parse your logs and do the analysis. It's at least a quick and easy way to get some information! Great answer cheers. This would be very quick to write a script for if one is not inclined to write java code (wrt @yuval answer). What are the last two numbers (Here: '200 197') of the log entry? - Thx RngTng the numbers is the response code HTTP 200 which means all okay and 197 is the time (I think) in milliseconds. Might be size of results as well not sure. how can I generate the ""request.log""?? thanks!! @EricPugh Found the request.log but that file is empty. Please advise."
906,A,"How do I get Average field length and Document length in Lucene? I am trying to implement BM25f scoring system on Lucene. I need to make a few minor changes to the original implementation given here for my needs I got lost at the part where he gets Average Field Length and document length... Could someone guide me as to how or where I get it from? I've browsed this issue a while ago I guess this implementation calculates average field and document length outside Lucene (while submitting documents for indexing for example). As far as I know Lucene doesn't have this feature (average field and document length). You can get field length from TermVector instances associated with documents' fields but that will increase your index size. This is probably the way to go unless you cannot afford a larger index. Of course you will still need to calculate the average yourself and store it elsewhere (or perhaps in a special document with a well-known external id that you just update when the statistics change). If you can store the data outside of the index one thing you can do is count the tokens when documents are tokenized and store the counts for averaging. If your document collection is static just dump the values for each field into a file & process after indexing. If the index needs to get updated with additions only you can store the number of documents and the average length per field and recompute the average. If documents are going to be removed and you need an accurate count you will need to re-parse the document being removed to know how many terms each field contained or get the length from the TermVector if you are using that. lets say I don't like the increase in index size then ? If you don't store term vectors you will have to count the number of terms in each field at indexing time and store those counts or averages somewhere. A ""special"" document is one possibility. Another possibility is to use some database or key-value store to store index statistics. Remember to update them if you add or delete documents though. Of course if your documents are all similar in length then being off a little bit won't hurt the BM25 stats."
907,A,"Sorting in lucene.net I got my lucene index with a field that needs to be sorted on. I have my query and I can make my Sort object. If I understand right from the javadoc I should be able to do query.SetSort(). But there seems to be no such method... Sure I'm missing something vital. Any suggestions? It looks like the actual method you want is e.g. Searcher.search(Query query Filter filter int n Sort sort). setSort is a method of Sort.  There are actually two important points. First the field must be indexed. Second pass the Sort object into the overloaded search method. Last time I looked the docs didn't do a very good job of pointing out the indexing part and certainly didn't explain why this is so. It took some digging to find out why. When a field is sortable the searcher creates an array with one element for each document in the index. It uses information from the term index to populate this array so that it can perform sorting very quickly. If you have a lot of documents it can use a lot of memory so don't make a field sortable unless there is a need. One more caveat: a sortable field must have no more than one value stored in each field. If there are multiple values Lucene doesn't know which to use as the sort key. the lucene documentation could indeed use some central documentation repository. I really like what the library brings to the table but there are some weird things I miss (like a ""contains"" decent startswith and endswith search). Thanks for the explanation. Not at work now but will check tomorow morning."
908,A,"How to index a string like ""aaa.bbb.ddd-fff"" in Lucene? I have to index a lot documents that contain reference numbers like ""aaa.bbb.ddd-fff"". The structure can change but it's always some arbitrary numbers or characters combined with ""/""""-""""_"" or some other delimiter. The users want to be able to search for any of the substrings like ""aaa"" or ""ddd"" and also for combinations like ""aaa.bbb"" or ""ddd-fff"". The best I have been able to come up with is to create my own token filter modeled after the synonym filter in ""Lucene in action"" which spits out multiple terms for each input. In my case I return ""aaa.bbb"" ""bbb.ddd""""bbb.ddd-fff"" and all other combinations of the substrings. This works pretty well but when I index large documents (100MB) that contain lots of such strings I tend to get out of memory exceptions because my filter returns multiple terms for each input string. Is there a better way to index these strings? I would try to build a token filter that: Extracts tokens delimited by the delimiters e.g. aaa bbb ddd fff. Extracts the delimiters as separate tokens. Maybe adds a separator token to prevent cross-number matches. For the query I would first try a boolean query with SHOULD terms. If this gives too many false positives I would change this to MUST. If this is still too much I would try a PhraseQuery."
909,A,"Controlling Solr score/sort I want to filter a property within a range but items that does not have the property should come last in the result. My solution was to set it to -1 if the property was not set. +(property:[10000000001 TO 10000000019] property:""-1""^0.5) This doesn't work since every document with property:-1 get a very high score for some reason. Is there a way to reliably control the sorting here? Boosting the range instead would mean I must boost every other term which I'd rather not do. The property with value -1 is boosted so removing it would give you better results. Furthermore if a property is missing just keep it missing. Don't set it to -1. You can sort the result with sort=property desc in the query or sort the property while indexing with sortMissingLast=true Thanks for the answer! What would the query look like if I don't set missing properties to `-1`? Would `+(property:[10000000001 TO 10000000019])` really return documents without `property` if I use `sortMissingLast`?"
910,A,Get a Date object out of Lucene Document I have indexed a date in lucene using DateTools.dateToString to store the date in a particular field. Is there any way to know if this was a date field and more importantly how to get the date out again? It's a fieldable with a long integer value. Thanks Lucene does not have strong-typing of fields so the same field could have a date in one record and a string in another record and a random integer in a third. It's up to your application to know what to look for in a particular field. You can use the DateTools.StringToDate method to convert from a string back to a date. I gathered as much after doing some more reading and checking. Perhaps in a later version. It would just be good to know which ones are dates since they are typically handled differently to other properties.
911,A,"FieldCache with frequently updating index Hi I have lucene index that is frequently updating with new records I have 5000000 records in my index and I'm caching one of my numeric fields using FieldCache. but after updating index it takes time to reload the FieldCache again (im reloading the cache cause documentation said DocID is not reliable) so how can I minimize this overhead by adding only newly added DocIDs to the FieldCache cause this capability turns to bottleneck in my application.  IndexReader reader = IndexReader.Open(diskDir); int[] dateArr = FieldCache_Fields.DEFAULT.GetInts(reader ""newsdate""); // This line takes 4 seconds to load the array dateArr = FieldCache_Fields.DEFAULT.GetInts(reader ""newsdate""); // this line takes 0 second as we expected // HERE we add some document to index and we need to reload the index to reflect changes reader = reader.Reopen(); dateArr = FieldCache_Fields.DEFAULT.GetInts(reader ""newsdate""); // This takes 4 second again to load the array I want a mechanism that minimize this time by adding only newly added documents to the index in our array there is a technique like this http://invertedindex.blogspot.com/2009/04/lucene-dociduid-mapping-and-payload.html to improve the performance but it still load all documents that we already have and i think there is no need to reload them all if we find a way to only adding newly added documents to the array The problem with your code is what I describe with inner/outer readers. You're passing the outer reader a DirectoryReader to the FieldCache. It thinks that the two readers are different and caches them separately. You need to use the innermost reader the segment reader to populate it per segment. This means that it will only load the changes after your call to Reopen. I'll post some code for this in a few minutes. Here's one way I've solved this problem. You'll need to create a background thread to construct IndexSearcher instances one at a time on some interval. Continue using your current IndexSearcher instance until a new one from the background thread is ready. Then swap out the new one to be your current one. Each instance acts as a snapshot of the index from the time that it was first opened. Note that the memory overhead for FieldCache doubles because you need two instances in memory at once. You can safely write to IndexWriter while this is happening. If you need to you can take this a step further by making index changes immediately available for search although it can get tricky. You'll need to associate a RAMDirectory with each snapshot instance above to keep the changes in memory. Then create a second IndexWriter that points to that RAMDirectory. For each index write you'll need to write to both IndexWriter instances. For searches you'll use a MultiSearcher across the RAMDirectory and your normal index on disk. The RAMDirectory can be thrown away once the IndexSearcher it was coupled with is no longer used. I'm glossing over some details here however that's the general idea. Hope this helps. suppose you have 1000 record in FSDirectory on disk and loading it using FieldCache and you have new 10 record in RAMDirectory as the way you explain above so we have two document with ID 0...10 cause each directory has its own docIDs and I cant create an integrated FieldCache with unique docID and i also optimize my index after 10 times adding records. in this case docID could change. The trick to the second part is that you'd be using a `MultiSearcher` across the `FSDirectory` and `RAMDirectory` such that the `FSDirectory` was opened before the `RAMDirectory` started getting changes. So it only appears to it that one of the two documents for a given ID exists. And the `MultiSearcher` handles merging the two when you perform searches. Unless you're using the `FieldCache` outside of search? I'd start with the first part though open a second `IndexSearcher` (or `IndexReader`) instance in the background to let it build the `FieldCache` and then swap it out. Yes i want to use FieldCache outside of search in CustomScoreQuery  The FieldCache uses weak references to index readers as keys for their cache. (By calling IndexReader.GetCacheKey which has been un-obsoleted.) A standard call to IndexReader.Open with a FSDirectory will use a pool of readers one for every segment. You should always pass the innermost reader to the FieldCache. Check out ReaderUtil for some helper stuff to retrieve the individual reader a document is contained within. Document ids wont change within a segment what they mean when describing it as unpredictable/volatile is that it will change between two index commits. Deleted documents could have been proned segments have been merged and such actions. A commit needs to remove the segment from disk (merged/optimized away) which means that new readers wont have the pooled segment reader and the garbage collection will remove it as soon as all older readers are closed. Never ever call FieldCache.PurgeAllCaches(). It's meant for testing not production use. Added 2011-04-03; example code using subreaders. var directory = FSDirectory.Open(new DirectoryInfo(""index"")); var reader = IndexReader.Open(directory readOnly: true); var documentId = 1337; // Grab all subreaders. var subReaders = new List<IndexReader>(); ReaderUtil.GatherSubReaders(subReaders reader); // Loop through all subreaders. While subReaderId is higher than the // maximum document id in the subreader go to next. var subReaderId = documentId; var subReader = subReaders.First(sub => { if (sub.MaxDoc() < subReaderId) { subReaderId -= sub.MaxDoc(); return false; } return true; }); var values = FieldCache_Fields.DEFAULT.GetInts(subReader ""newsdate""); var value = values[subReaderId]; Thanks Simon but i want to make sure if i add new document to my index the document id of that doc would never change on merge or optimize cause if it changes then the above solution doesn't meet my needs cause i want to provide only newly added doc to the FieldCache to prevent loading all docs again using FieldCache If i can make sure which segment reader remains intact during merge/optimize then i can surely load them according to your solution and reload other segment readers value and it is gradually improve the performance but it is still not ideal as i want While technically the readers are intact after a merge/optimize they are also obsoleted and replaced with the newly created segment. Could you provide some code examples that experience problems in your current setup? Thanks Simon your snippet code was perfect Hi Simon I have just read this post and I was wondering - how does your solution work if a merge is currently running on a background thread? @PiniSalim excellent. The result of the merge will not be seen until the reader is reopened. The old segments (and their field caches) will be handled by the garbage collector once all old reader instances are closed (or garbage collected). @Simon Svensson: here is a link to the question I posted. Hope to hear your opinion :) http://stackoverflow.com/questions/13110780/calling-commit-on-an-index-that-is-currently-been-merged-in-lucene You can use the FieldCache during searches without any problems. Searchers are read-only and wont cause any document id changes until the reader they are based on is reopened. Ask a new question if you have a reproducible problem with your Lucene.Net implementation. @Simon Svensson if you build the FieldCache from old reader doesnt it mean that the result array wont fit to the new merged segment? What I mean is 'GatherSubReaders' gives you readers that are not up to date in case of a merge that currently runs on background thread because the docIds might change at the end of the merge... Am I wrong? I would like to use your algorithm but I am not sure if I am missing something. The newly created segment from the merge wont be used until your reader has been reopened so document identifiers wont change [until the reader has been reopened]. The ReaderUtil class is part of the Lucene.Net assembly. I see. The problem (for me..) is that I want to use the FieldCache during the search so when I will open a reader - it will be updated AFTER merge and the DocIds wont fit to those that were genereted with the FieldCache_Fields.DEFAULT.GetInts. Will it solve my problem if I will call WaitForMerges() on my IndexWriter before calling GatherSubReaders and generating the FieldCache?"
912,A,"Solr Highlighting Problem Hi All I have a problem that when i Query Solr it matches results but when i enable highlighting on the results of this query the highlighting does not work.. My Query is +Contents:""item 503"" Contents is of type text and one important thing in text item 503 appear as ""item 503(c)"" can open parenthesis at the end create problem?? please help here is highlighting section in SolrSonfig.xml  <highlighting> <!-- Configure the standard fragmenter --> <!-- This could most likely be commented out in the ""default"" case --> <fragmenter name=""gap"" class=""org.apache.solr.highlight.GapFragmenter"" default=""true""> <lst name=""defaults""> <int name=""hl.fragsize"">100</int> </lst> </fragmenter> <!-- A regular-expression-based fragmenter (f.i. for sentence extraction) --> <fragmenter name=""regex"" class=""org.apache.solr.highlight.RegexFragmenter""> <lst name=""defaults""> <!-- slightly smaller fragsizes work better because of slop --> <int name=""hl.fragsize"">70</int> <!-- allow 50% slop on fragment sizes --> <float name=""hl.regex.slop"">0.5</float> <!-- a basic sentence pattern --> <str name=""hl.regex.pattern"">[-\w /\n\""']{20200}</str> </lst> </fragmenter> <!-- Configure the standard formatter --> <formatter name=""html"" class=""org.apache.solr.highlight.HtmlFormatter"" default=""true""> <lst name=""defaults""> <str name=""hl.simple.pre""><![CDATA[<em>]]></str> <str name=""hl.simple.post""><![CDATA[</em>]]></str> </lst> </formatter> </highlighting> and here is fieldtype definition in schema.xml <fieldtype name=""text"" class=""solr.TextField""> <analyzer> <tokenizer class=""solr.StandardTokenizerFactory"" luceneMatchVersion=""LUCENE_29""/> <filter class=""solr.StandardFilterFactory""/> <!-- <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.StopFilterFactory"" luceneMatchVersion=""LUCENE_29""/> <filter class=""solr.EnglishPorterFilterFactory""/>--> </analyzer> </fieldtype> and here is field definition <field name=""Contents"" type=""text"" indexed=""true"" stored=""true"" /> Regards Ahsan. Can you paste the relevant highlighting sections from solrconfig.xml? And your field definition in schema.xml. It's pretty wide open number of possibilities from what you have... Have you tried storing the term vectors too? If you're using the fast vector highlighter (which I think Solr might by default) you'll need those. One thing highlighting does not fail always it works most of the time but in above mentioned case it had searched the term but doesn't highlight it Secondly i have stored term vectors but as i know Fast Vector Highlighter will be available with Solr 1.5 Have you tried playing with the params? 1. Set slop to be really big 2. set regex to match .* A possibility here is that your field is just the phrase ""item 501(c)"" and so the minimum length on your regex (20) isn't getting satisfied. thanx very much its done.. FastVectorHighlighter isn't used by default as of today... @Xodarap I think it will be good if you write your answer at all?"
913,A,How to restrict search to component's field value with grails searchable plugin Using grails searchable plugin I would like to search for all products within a specific category using a query builder like: Products.search { must(queryString(params.q)) must(term('??????''Food')) } Using 'category.name' returns: Failed to find mapping for alias [category] and path [category.name] class Product { String name String desc Category category static searchable = { category component: true } } class Category { String name static hasMany = [products: Product] static searchable = true } Any ideas? Thanks. I think you can do something like: def results = Product.search { must(term('$/Product/category/name' params.categoryName)) must(queryString(params.q)) } Hope this helps! That works so does term('Product.category.name' params.categoryName)
914,A,"How to optimize Lucene.Net indexing I need to index around 10GB of data. Each of my ""documents"" is pretty small think basic info about a product about 20 fields of data most only a few words. Only 1 column is indexed the rest are stored. I'm grabbing the data from text files so that part is pretty fast. Current indexing speed is only about 40mb per hour. I've heard other people say they have achieved 100x faster than this. For smaller files (around 20mb) the indexing goes quite fast (5 minutes). However when I have it loop through all of my data files (about 50 files totalling 10gb) as time goes on the growth of the index seems to slow down a lot. Any ideas on how I can speed up the indexing or what the optimal indexing speed is? On a side note I've noticed the API in the .Net port does not seem to contain all of the same methods as the original in Java... Update--here are snippets of the indexing C# code: First I set thing up:  directory = FSDirectory.GetDirectory(@txtIndexFolder.Text true); iwriter = new IndexWriter(directory analyzer true); iwriter.SetMaxFieldLength(25000); iwriter.SetMergeFactor(1000); iwriter.SetMaxBufferedDocs(Convert.ToInt16(txtBuffer.Text)); Then read from a tab-delim data file:  using (System.IO.TextReader tr = System.IO.File.OpenText(File)) { string line; while ((line = tr.ReadLine()) != null) { string[] items = line.Split('\t'); Then create the fields and add the document to the index:  fldName = new Field(""Name"" items[4] Field.Store.YES Field.Index.NO); doc.Add(fldName); fldUPC = new Field(""UPC"" items[10] Field.Store.YES Field.Index.NO); doc.Add(fldUPC); string Contents = items[4] + "" "" + items[5] + "" "" + items[9] + "" "" + items[10] + "" "" + items[11] + "" "" + items[23] + "" "" + items[24]; fldContents = new Field(""Contents"" Contents Field.Store.NO Field.Index.TOKENIZED); doc.Add(fldContents); ... iwriter.AddDocument(doc); Once its completely done indexing:  iwriter.Optimize(); iwriter.Close(); That seems really slow. Can you post your indexing code? Creating ""Contents"" by concatenating multiple strings with '+' could be a problem if these strings are large. You could create it with StringBuilder. In fact you could add Field(""Contents"" item[4]) Field(""Contents"" item[5]) to the document to avoid creating single ""Contents"" string. ok I updated the question with the indexing code. Funny thing is now for the past two hours its been chugging away but hasn't updated the index files at all... actually just occurred to me...the index size -should- slow down as time goes on and the common words are already in it...I probably need a better way to track index progress. Total number documents roughly 10000000. StandardAnalyzer. Yes I am re-using IndexWriter and Document but was unable to re-use Field as the C# port does not seem to implement Field.SetValue. I actually just loaded this on a dedicated server with SetMaxBufferDocs at 25000. Its very fast in the beginning...but after 8 hrs now the index is only about 150mb. Can you elaborate on (a) total number of documents (b) analyzer used (c) length of indexed field? Also just to confirm are you re-using IndexWriter? Since I can't comment on the marked answer above related to a 3 year old version I would highly recommend installing the Visual Studio extension for NuGet Package Manager when adding Lucene.NET to your projects. It should add the most recent DLL version for you unless you need a specific later version.  Apparently I had downloaded a 3 yr old version of Lucene that is prominently linked to for some reason from the home page of the project...downloaded the most recent Lucene source code compiled used the new DLL fixed about everything. The documentation kinda sucks but the price is right and its real fast. From a helpful blog First things first you have to add the Lucene libraries to your project. On the Lucene.NET web site you’ll see the most recent release builds of Lucene. These are two years old. Do not grab them they have some bugs. There has not been an official release of Lucene for some time probably due to resource constraints of the maintainers. Use Subversion (or TortoiseSVN) to browse around and grab the most recently updated Lucene.NET code from the Apache SVN Repository. The solution and projects are Visual Studio 2005 and .NET 2.0 but I upgraded the projects to Visual Studio 2008 without any issues. I was able to build the solution without any errors. Go to the bin directory grab the Lucene.Net dll and add it to your project."
915,A,Lucene .NET fixed filenames in index directory possible? When building a Lucene .NET index it creates several randomly named files under the root index directory. My question is is there a way to have these files have a static or fixed name and just overwrite upon re-index or all be in one file? If you use cfs there will be just one file (random stuff.cfs)
916,A,"Wildcard for terms in phrase - Lucene Google's query syntax allows to search phrases like ""as * as a skyscraper"" where the asterisk can match any term (or terms). Is there a way to achieve the same thing in Lucene? The proximity operator ~ could be of use but it is not what I exactly want. Try a SpanNearQuery. Mark Miller's SpanQuery Blog Post explains how to use it and the examples are similar to what you describe. Perfect. SpanQuery is very expressive."
917,A,"Correct way to write a Tokenizer in Lucene I'm trying to analyze content of a Drupal database for collective intelligence purposes. So far I've been able to work out a simple example that tokenizes the various contents (mainly forum posts) and count tokens after removing stop words. The StandardTokenizer supplied with Lucene should be able to tokenize hostnames and emails but content can have also embedded html e.g: Pubblichiamo la presentazione di IBM riguardante DB2 per i vari sistemi operativi Linux UNIX e Windows.\r\n\r\nQuesto documento sta sulla piattaforma KM e lo potete scaricare a questo <a href=\'https://sfkm.griffon.local/sites/BSF%20KM/BSF/CC%20T/Specifiche/Eventi2008/IBM%20DB2%20for%20Linux%20UNIX%20e%20Windows.pdf\' target=blank>link</a>. This is tokenized badly in this way: pubblichiamo -> 1 presentazione -> 1 ibm -> 1 riguardante -> 1 db2 -> 1 vari -> 1 sistemi -> 1 operativi -> 1 linux -> 1 unix -> 1 windows -> 1 documento -> 1 piattaforma -> 1 km -> 1 potete -> 1 scaricare -> 1 href -> 1 https -> 1 sfkm.griffon.local -> 1 sites -> 1 bsf -> 1 20km/bsf -> 1 cc -> 1 20t/specifiche/eventi2008/ibm -> 1 20db2 -> 1 20for -> 1 20linux -> 1 20unix -> 1 20e -> 1 20windows.pdf -> 1 target -> 1 blank -> 1 link -> 1 What I would like to have is to keep links together and strip html tags (like <pre> or <strong>) that are useless. Should I write a Filter or a different Tokenizer? The Tokenizer should replace the standard one or can I mix them together? The hardest way would be to take StandardTokenizerImpl and copy it in a new file then add custom behaviour but I wouldn't like to go too deep in Lucene implementation for now (learning gradually). Maybe there is already something similar implemented but I've been unable to find it. EDIT: Looking at StandardTokenizerImpl makes me think that if I have to extend it by modifying the actual implementation it's not so convenient compared to using lex or flex and doing it by myself.. Generally when indexing documents that contain HTML markup with Lucene you should first parse out the HTML into a textual representation with the parts you want to leave and only then feed it to the Tokenizer to be indexed. See jGuru: How can I index HTML documents? for an FAQ explaining more of how to do this.  This is most easily achieved by pre processing the text before giving it to lucene to tokenize. Use an html parser like Jericho to convert your content into text with no html by stripping out tags you dont care about and extracting the text from those that you do. Jericho's TextExtractor is perfect for this and easy to use. String text = ""Pubblichiamo la presentazione di IBM riguardante DB2 per i vari sistemi operativi"" +""Linux UNIX e Windows.\r\n\r\nQuesto documento sta sulla piattaforma KM e lo potete"" +""scaricare a questo <a href=\'https://sfkm.griffon.local/sites/BSF%20KM/BSF/CC%20T/Specifiche/Eventi2008/IBM%20DB2%20for%20Linux%20UNIX%20e%20Windows.pdf\' target=blank>link</a>.""; TextExtractor te = new TextExtractor(new Source(text)){ @Override public boolean excludeElement(StartTag startTag) { return startTag.getName() != HTMLElementName.A; } }; System.out.println(te.toString()); This outputs: Pubblichiamo la presentazione di IBM riguardante DB2 per i vari sistemi operativiLinux UNIX e Windows. Questo documento sta sulla piattaforma KM e lo potetescaricare a questo link. You could use a custom Lucene Tokenizer with an html Filter but it's not the easiest solution - using Jericho will defn save you development time for this task. The existing html analysers for lucene probably don't want to do exactly what you want as they will keep all text on the page. The only caveat to this is that you will end up processing the text twice rather than all as one stream but unless you are handling Terabytes of data you aint gonna care about this performance consideration and dealing with performance is something best left untill you have your app fleshed out and have identified it as an issue anyway."
918,A,"Do documents in Lucene have to contain the same fields? I'm considering / working on implementing a search engine for our company's various content types and am attempting to wrap my head around Lucene (specifically the .net flavor). For the moment my primary question is whether or not documents one indexes have to contain the same fields. For instance: Document1: Title: ""I'm a document baby"" Body: ""Here are some important things"" Latitude: 26.12224 Longtitude: -65.23124 Brand: Toshiba Document2: Title: ""Another Document by Me"" Body: ""Lorem ipsum and all that jazz"" Category: Articles Author: Sir Loin ...and so forth Nothing in lucene forces uniformity. If you search on a field named 'fred' and not all docs have 'fred' that search will not find the fredless docs. You are my new hero.  It all depends on how you have indexed your documents in Lucene. All Documents must be added to the Index. You can use IndexWriter or write your own class to do that. Before adding a document to the Index you should break it up in name value pairs. Subsequently you can query Lucene for these name values using QueryParser. For example following query will return all documents with the phrases ""I'm a document baby"" in the title and ""Here are some important things"" in the body. title:(""I'm a document baby"") body:(""Here are some important things"") I have just shown a simple example but you can create a more powerful search query in many different ways. The classes which I have mentioned are from java but .net should be similar.  If you wish to index on a specific field I guess all documents must have the same fields. That's what my intuition has told be thusfar but I have not been able to find anything that concretely states one way or another."
919,A,"How to read a Lucene index? I'm working on a project for which I want to build a tag cloud by reading a Lucene index and pruning it down. I didn't set up the Lucene engine it was someone else in the team now I just want to read its index. Do you how to do that in Java? what you need to look for is how to use IndexReader class the .terms() method will give you back all the terms in the index. This looks even better! Is there any chance you know how to access the index if it's in the WEB-INF/index folder? I use OpenCMS and that's the default location. from the API docs. Concrete subclasses of IndexReader are usually constructed with a call to one of the static open() methods e.g. open(String).  Just do this: File indexDirectory = new File(""YourIndexLocation""); IndexReader reader = IndexReader.open(FSDirectory.open(indexDirectory)); return reader.maxDoc(); //return total docs in index Thank you finally some code!  You do it like this - IndexReader r = IndexReader.open( ""prdb_index""); int num = r.numDocs(); for ( int i = 0; i < num; i++) { if ( ! r.isDeleted( i)) { Document d = r.document( i); System.out.println( ""d="" +d); } } r.close();  Not sure what you mean by ""reading"" an Index: If you want to query it you can use IndexSearcher class. IndexReader allows you to open the index in read mode. If you want to view the contents of the index you can use Luke Thanks! Luke seems like the solution I was looking for! BTW you can initialize an `IndexSearch` like so: `IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(FSDirectory.open(new File(pathToIndex))));` `IndexSearcher` also has a constructor that accepts an `ExecutorService` you should look in to that for searching different segments in parallel."
920,A,"Filter index for searching in Lucene.net I am currently working on a project involving the Lucene library within C# however I have reached an issue with design of my project concerning the retrevial of documents within the index. The documents within my index have been created with several fields and i'd like to be able to filter between two of these fields and then search this subset for terms however I am still familiarising myself with lucene and am not fully sure if this is possible. I have learnt how to perform basic queries but I think I should be using lucenes filter class but i am not exactly sure how. I would be greatful if anyone could offer advice on this The project I am completing involves indexing email messages from various email accounts. the documents in my index have some of the following fields: Account: (e.g. fake@fake.com) Folder: (e.g. sent trash inbox...) Data: (the body of the email) I'd like to be able to filter my index so i can have a subset which only containts documents from a particular account and folder and then after this I'd like to be able to search the data field of this subset. giving more details about the queries you are performing and the data in the fields you want to apply a filter on would help on giving you advices from your additionnal input you dont need to use a filter but combine several conditions in a BooleanQuery do you use the QueryParser or you build your queries manually using the BooleanQuery class? it would be useful to know if you mean a query or a filter too. A query will do the direct lookup of data but a filter is used to retrieve a subset of data from a query. Is the filter necessary or are we talking unique individual calls and thus it would be a query? As @Jf Beaulac suggested you can do ""filtering"" with a BooleanQuery. private Query CreateFilteredQuery (string account string folder Query criteria) { BooleanQuery bq = new BooleanQuery(); bq.Add(new TermQuery (new Lucene.Net.Index.Term (""account"" account)) BooleanClause.Occur.MUST); bq.Add(new TermQuery (new Lucene.Net.Index.Term (""folder"" folder)) BooleanClause.Occur.MUST); bq.Add(criteria BooleanClause.Occur.MUST); return bq; } Query filteredQuery = CreateFilteredQuery (""fake@fake.com"" ""inbox"" myQueryParser.Parse (criteria)); var hits = myIndexSearcher.Search (filteredQuery); Here is a good question about the differences between queries and filters: Why do we use Filters while searching"
921,A,"Zend Lucene how to do query My create index function is as bellow function create() { Zend_Search_Lucene_Analysis_Analyzer::setDefault(new Zend_Search_Lucene_Analysis_Analyzer_Common_Utf8_CaseInsensitive ()); $index = Zend_Search_Lucene::create('data/index'); $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::Text('title' 'a cheap car in town milage under 3000''utf-8')); $doc->addField(Zend_Search_Lucene_Field::Text('detail''a cheap car in town milage under 3000''utf-8')); $doc->addField(Zend_Search_Lucene_Field::Text('category' 'milage under 3000''utf-8')); $index->addDocument($doc); } When user search for ""a cheap car in town milage under 3000"" I will split the query into 3 parts $query1 = ""a cheap car""; $query2 = ""in town""; $query3 = "" milage under 3000""; I want to search $query1 in field ""title"" $query2 in field ""detail"" $query3 in field ""category"". I also want to search number case insensitive and text case insensitive. How can I do it? I would need the actual code because I'm completely confused by the Zend Lucene docs I don't know which code or query to use. your query will be: $query = Zend_Search_Lucene_Search_QueryParser::parse(""title:($query1) detail:($query2) category:($query3)""; $hits = $index->find( $query); Index is case-insensitive unless explicitly set to case sensitive upon creation."
922,A,"How to get Lucene explanation for a SolrDocument with Solrj? I'm searching an Solr index with SolrJ and trying to get the Lucene explanation for logging it for further use. The code goes like this:  SolrServer server = new CommonsHttpSolrServer(""solr_url""); SolrQuery solrquery = new SolrQuery(); solrquery.set(""fl"" ""score id""); // id is a String field solrquery.set(""rows"" ""1000""); solrquery.set(""debugQuery"" ""on""); solrquery.setQuery(""query words here""); try { QueryResponse response = server.query(solrquery); SolrDocumentList docs = response.getResults(); Iterator<SolrDocument> dociterator = docs.iterator(); while (dociterator.hasNext()) { SolrDocument doc = dociterator.next(); String id = (String) doc.getFirstValue(idfield); Float relevance = (Float) doc.getFirstValue(""score""); String explanation = ???; } } catch (SolrServerException e) { e.printStackTrace(); } I figured that response.getEplainMap() would contain a map with the value like response.getEplainMap().get(id)  but it seems that the explainmap contains only the key null with the value of the last found document. Any ideas how to get the correct explanation? In my case there was a bug in the Solr index itself. Code below works now. Map<String String> explainmap = response.getExplainMap(); String explanation = explainmap.get(id); When creating an index and having problems like above make sure that the id field determined in schema.xml (e.g. <uniqueKey>id</uniqueKey>) contains correct data. In my case the id field I used in the code was not the same as Solr thought it was and it contained no data thus the explainmap had only one field with a key null.  Have you tried debugging a query from the admin console? This shows you the full output. QueryResponse has a couple of methods getDebugMap() and getExplainMap() that might prove useful. I haven't tested it in code but on the admin console when I debug a query I get the following; <?xml version=""1.0"" encoding=""UTF-8""?> <response> <lst name=""responseHeader""> <int name=""status"">0</int> <int name=""QTime"">0</int> <lst name=""params""> <str name=""q"">stuff</str> <str name=""start"">0</str> <str name=""indent"">on</str> <str name=""explainOther""/> <str name=""wt"">standard</str> <str name=""hl.fl""/> <str name=""fq""/> <str name=""version"">2.2</str> <str name=""qt"">standard</str> <str name=""debugQuery"">on</str> <str name=""fl"">*score</str> <str name=""rows"">1</str> </lst> </lst> <result name=""response"" numFound=""79"" start=""0"" maxScore=""4.050907""> <doc> <float name=""score"">4.050907</float> ..other bits of data </doc> </result> <lst name=""debug""> <str name=""rawquerystring"">stuff</str> <str name=""querystring"">stuff</str> <str name=""parsedquery"">MYSEARCHFIELD:stuff</str> <str name=""parsedquery_toString"">MYSEARCHFIELD:stuff</str> <lst name=""explain""> <str name=""6095""> <--- 6095 is the ID of the document 4.050907 = (MATCH) fieldWeight(MYSEARCHFIELD:stuff in 1292) product of: 1.4142135 = tf(termFreq(MYSEARCHFIELD:stuff )=2) 9.166156 = idf(docFreq=79 maxDocs=281583) 0.3125 = fieldNorm(field=MYSEARCHFIELD doc=1292) </str> </lst> ..timing stuff here </lst> </response> As mentioned in my own answer the ids were wrong (read: non-existent). However if I had read your answer before I figured out the problem by myself I would've seen my it from the debug query console since I believe the explain output didn't have the name/id -attribute at all like in your post. So in a way by posting that you solved my problem too :)  You can also get the explain information as a field in the document by passing in the special [explain] field (with square brackets) in the field list."
923,A,"Faster search in Lucene - Is there a way to keep the whole index in RAM? Is there a way of keeping the index in RAM instead of keeping it on the hard disk? We want to make searching faster. Check out the RAMDirectory documentation. Here's a basic usage example. This will only work if the index is small enough. And thank you for answering. What size would be small? The index is about 20 MB's and we don't expect it to grow more than 5 times. Is this small enough? You're welcome. A 100 MB is indeed small enough. You should still consider using a disk index with proper JVM stack size settings. This will save you having to re-index whenever you restart the application and may be as fast as the RAMDirectory. See also: http://www.lucidimagination.com/Community/Hear-from-the-Experts/Articles/Scaling-Lucene-and-Solr about search speed. Why does it reindex when I restart the application? I thought I could read the index on the hard disk into the RAM using RAMDirectory. Do I have to reindex everytime I want to read the index into the RAM? Thank you for the link. You are right. You can read it from the hard disk: http://lucene.apache.org/java/2_4_1/api/org/apache/lucene/store/RAMDirectory.html#RAMDirectory%28java.io.File%29  A RAM disk could be a solution for this. A mini-HOWTO is available at http://www.vanemery.com/Linux/Ramdisk/ramdisk.html. Mount the RAM disk as your index directory and you should be done. Thank you for your answer. We considered that. But we tend not to use that option as we don't have that much experience in the operating system. We think that it would give us less control and less ability to observe/monitor the index. Note that for large indexes ""hardware"" RAM disks are also available... basically device with a hard drive interface but filled with DRAM instead of platters.  Is there a way of keeping the index in RAM instead of keeping it on the hard disk? Using the RAMDirectory class SampleUsage here Also from the Lucene FAQs ImproveSearchingSpeed Generally for faster indexing performance it's best to flush by RAM usage instead of document count and use as large a RAM buffer as you can. Also check this question: EDIT: RE: RamDirectory As the API says RamDirectory is A memory-resident Directory implementation. it keeps only those index in RAM as specified by directory RAMDirecory RE:Caching In my knowledge Lucene caches search results by means of filters pls look @ CachingWrapperFilter and QueryWrapperFilter Thank you very much for your detailed answer. I have two further questions. First I read that Lucene does caching which would be keeping the index partially in RAM. But the RAMDirectory is different right? Does it keep all of the index in the RAM? Second I saw the setRAMBufferSizeMB what I understood was that it was for speeding up the indexing rather than searching. Does it also speed up searching? oops will edit my post From ""ImproveSearchingSpeed"": ""Open the IndexReader with readOnly=true"" do you know how to do this in Lucene 4+? I couldn't find any examples..."
924,A,Confused on picking best search engine Currently I am using custom made Java search engine. This Java engine uses Lucene to index MySql records. The are several problems with search engine: 1- Search results are not accurate. 2- It do not use weighting algorithms to make most matching result on top. 3- It consumes lot of memory. 4- Very hard to maintain for various reasons. So what I need to do is use ready made search engine either commercial or open source that could integrate with my system smoothly and resolve my problems (as much as possible). Could you please give me options and where to look. Thanks Wa'el submit your site to google (at this moment google is the champ) Solr: Solr is the popular blazing fast open source enterprise search platform from the Apache Lucene project. Its major features include powerful full-text search hit highlighting faceted search dynamic clustering database integration and rich document (e.g. Word PDF) handling. Solr is highly scalable providing distributed search and index replication and it powers the search and navigation features of many of the world's largest internet sites. Note that many of your problems might be due to misconfiguration or wrong queries rather than lucene not being good. yes however luence is indeed require lots of memoryand try to keep your collection small (as in smaller doc size) dun just anyhow index from column A to Z Solr works quite nicely out of the box. You can use the DataImportHandler for MySQL integration. If you have more difficulties you can ask more specific questions both here and in the Solr mailing list.  While I too like Solr I suggest you also consider Sphinx. It is said to be very easily integrated with MySQL. I would try Solr first and if it does not help try Sphinx as the second option. Yeah Sphinx in quite nice but it also crashes very regulary!
925,A,"Building dictionary of words from large text I have a text file containing posts in English/Italian. I would like to read the posts into a data matrix so that each row represents a post and each column a word. The cells in the matrix are the counts of how many times each word appears in the post. The dictionary should consist of all the words in the whole file or a non exhaustive English/Italian dictionary. I know this is a common essential preprocessing step for NLP. And I know it's pretty trivial to code it sill I'd like to use some NLP domain specific tool so I get stop-words trimmed etc.. Does anyone know of a tool\project that can perform this task? Someone mentioned apache lucene do you know if lucene index can be serialized to a data-structure similar to my needs? is known as Term-Document matrix. You can check out: bow - a veteran C library for text classification; I know it stores the matrix it may require some hacking to get it. Weka - a Java machine learning framework that can handle text and build the matrix Sujit Pal's blog post on building the term-document matrix from scratch If you insist on using Lucene you should create an index using term vectors and use something like a loop over getTermFreqVector() to get the matrix.  Maybe you want to look at GATE. It is an infrastructure for text-mining and processing. This is what GATE does (I got this from the site): open source software capable of solving almost any text processing problem a mature and extensive community of developers users educators students and scientists a defined and repeatable process for creating robust and maintainable text processing workflows in active use for all sorts of language processing tasks and applications including: voice of the customer; cancer research; drug research; decision support; recruitment; web mining; information extraction; semantic annotation the result of a €multi-million R&D programme running since 1995 funded by commercial users the EC BBSRC EPSRC AHRC JISC etc. used by corporations SMEs research labs and Universities worldwide the Eclipse of Natural Language Engineering the Lucene of Information Extraction the ISO 9001 of Text Mining  Thanks to @Mikos' comment I googled the term ""term-document matrix' and found TMG (Text to Matrix Generator). I found it suitable for my needs.  What you want is so simple that in most languages I would suggest you roll your own solution using an array of hash tables that map from strings to integers. For example in C#: foreach (var post in posts) { var row = new Dictionary<string int>(); foreach (var word in GetWordsFromPost(post)) { IncrementContentOfRow(row word); } } // ... private void IncrementContentOfRow(IDictionary<string int> row string word) { int oldValue; if (!row.TryGet(word out oldValue)) { oldValue = 0; } row[word] = oldValue + 1; } you're right :-)... still I was hoping to use some NLP domain specific tools so I get stop words trimmed. I will update my question I think GATE does most of that legwork for you (removing commonly-used words). @LiorH: Cool. @Vivin Paliath: Agreed if you want to do more than the question originally stated then GATE is probably a good way to go. @ealdent: I would always go that way because it's easier to test-drive code you own than code you don't but I get why the OP wants to go a different way. or you can use this solution but just throw in stop word removal yourself using one of the lists from http://en.wikipedia.org/wiki/Stop_words"
926,A,How to index pdf ppt xl files in lucene (java based or python or php any of these is fine)? Also I want to know how to add meta data while indexing so that i can boost some parameters There are several frameworks for extracting text suitable for Lucene indexing from rich text files (pdf ppt etc.) One of them is Apache Tika a sub-project of Lucene. Apache POI is a more general document handling project inside Apache. There are also some commercial alternatives.  You can use Apache Tika. Tika is a toolkit for detecting and extracting metadata and structured text content from various documents using existing parser libraries. Supported Document Formats HyperText Markup Language XML and derived formats Microsoft Office document formats OpenDocument Format Portable Document Format Electronic Publication Format Rich Text Format Compression and packaging formats Text formats Audio formats Image formats Video formats Java class files and archives The mbox format The code will look like this. Reader reader = new Tika().parse(stream);  see https://github.com/WolfgangFahl/pdfindexer for a java solution that uses PDFBox and Apache Lucene to split the PDF files page by page to text index these text-pages and create a resulting html index file that links to the pages in the pdf sources by using a corresponding open parameter.  Lucene indexes text not files - you'll need some other process for extracting the text out of the file and running Lucene over that.
927,A,"Lucene Indexing and searching I am trying to index a table in a database using Lucene. I use Lucene just for indexing the Fields are not stored. The table mentioned above has five columns (userid (PK) description report number reporttype report). I intend to use a combination of userid reportnumber and report type for getting data back from the database if Lucene finds a hit. One record in the table can span multiple rows for e.g. JQ123 SOMEDESCRIPTION 1 FIN content of fin report JQ123 AnotherDescription 2 MATH content of math report JQ123 YetAnotherDesc 3 MATH content of another math report JD456 MoreDesc 1 STAT content of stat report ..so on Some of the report types e.g. (MATH) have highly structured contents (XML stored as string in last column) and in the future I may want to flesh out some of the content as a Field of the document. My strategy so far has been to create a Lucene Document for every row and index it. My thinking behind it being that 1. It is easy and seems logical (to me) 2. if I end up extracting contents out of certain document types and making them in to Fields all that would be needed is an if statement that checks for report type and creates these new Fields. Here is the relevant code: public void createDocument(){ Document luceneDocument=new Document(); luceneDocument.add(new Field(""userid"" userID Field.Store.NO Field.Index.NOT_ANALYZED)); luceneDocument.add(new Field(""reportnumber"" reportNum Field.Store.NO Field.Index.NOT_ANALYZED)); luceneDocument.add(new Field(""reporttype"" reportType Field.Store.NO Field.Index.NOT_ANALYZED)); luceneDocument.add(new Field(""description"" description Field.Store.NO Field.Index.ANALYZED)); luceneDocument.add(new Field(""report"" report Field.Store.NO Field.Index.ANALYZED)); if(reporttype.equalsIgnoreCase(""MATH""){ luceneDocument.add(new Field(""more fields"" field content Field.Store.NO Field.Index.ANALYZED)); } indexwriter.add(luceneDocument) indexwriter.close } 1. Does having different Documents for the same record affect Lucene's search efficiency in any fashion? 2. Would this approach have any significant disk space over heads when compared to having one Document per record in Lucene (I do not store any Fields)? Thanks in advance for your response First note how the index is set up. Each term's index looks like: [term][docid][docid]... where the [docid]'s are IDs of documents which contain that term. So to answer your questions: If e.g. MATH and STATS contained the same term they would be listed twice here. And so the search would have to look at two documents when it should in theory only need to look at one. But this is a very minimal penalty. I assume you have to store at least an ID for each document so you will see a minor storage increase. It will be (length of id) * (number of documents per row). Again this is trivial. A more important problem is the fact that queries can't be normed appropriately. For example a search finds row #1 that matches in MATH and STATS and row #2 that matches only in MATH. You will need to manually rank row #1 higher because Lucene won't know that the two documents are actually the same row. In short: unless you have some absolutely massive index I wouldn't worry much about storage/performance. But I would worry about how you're going to score that query."
928,A,"Recommended title boost? I have a relatively simple Lucene index being served by Solr. The index consists of two major fields title and body and a few less-important fields. Most search engines give more relevance to results with matches in the title over the body. I'm going to start providing an index-time boost to the title field. My question is what values do people typically use for their title fields? 2? 4? 10? 100? Why use index time boost instead of search time? I suggest you divide the median body length by the median title length. This roughly gives you a factor M - for M appearances of a word in the body it will appear once in the title. Now use something like M*3. This is of course a rationalized heuristic and it is best you iterate over the values. See Grant Ingersoll's ""Debugging Relevance Issues in Search"" for a much more structured discussion."
929,A,"Indexing file paths or URIs in Lucene Some of the documents I store in Lucene have fields that contain file paths or URIs. I'd like users to be able to retrieve these documents if their query terms contain a path or URI segment. For example if the path is C:\home\user\research\whitepapers\analysis\detail.txt I'd like the user to be able to find it by queriying for path:whitepapers. Likewise if the URI is http://www.stackoverflow.com/questions/ask A query containing uri:questions would retrieve it. Do I need to use a special analyzer for these fields or will StandardAnaylzer do the job? Will I need to do any pre-processing of these fields? (To replace the forward slashes or backslashes with spaces for example?) Suggestions welcome! You can use StandardAnalyzer. I tested this by adding the following function to Lucene's TestStandardAnalyzer.java: public void testBackslashes() throws Exception { assertAnalyzesTo(a ""C:\\home\\user\\research\\whitepapers\\analysis\\detail.txt"" new String[]{""c""""home"" ""user"" ""research""""whitepapers"" ""analysis"" ""detail.txt""}); assertAnalyzesTo(a ""http://www.stackoverflow.com/questions/ask"" new String[]{""http"" ""www.stackoverflow.com""""questions""""ask""}); } This unit test passed using Lucene 2.9.1. You may want to try it with your specific Lucene distribution. I guess it does what you want while keeping domain names and file names unbroken. Did I mention that I like unit tests? Maybe you can use LetterTokenizer http://lucene.apache.org/java/2_2_0/api/org/apache/lucene/analysis/LetterTokenizer.html chained with some filter. LetterTokenizer divides text at non-letters. Thanks! Using the StandardAnalyzer to index path segments also works in Lucene.Net 2.4.0. Do you know of an out-of-the-box Lucene Analyzer that would break the domain name apart at the ""dots"" or separate the filename from its extension?"
930,A,"Solr: What are the benefits of length normalization/omitNorms=false? We're using Solr to search articles of various lengths. We index both descriptive metadata (title author category keywords etc) and the full article text. We do not boost relevance at index time - all boosts are done at query time (we use dismax coupled with various qf pf and bf boosts). Currently our fulltext field uses the standard omitNorms=false; and as a result all else equal shorter articles (2-3 column inch articles) will frequently have higher relevance than longer feature-length (multi-page) articles. In our case article length is a significant indicator of relevance and so I am considering setting omitNorms=true on our fulltext field. Questions: 1. Why is the default lucene/solr behavior to boost shorter field lengths over higher? What is the reasoning? 2. Why would I not want to omitNorms? I don't need to boost queries on this particular field nor use any kind of faceting on this field. Question 1: Boosting shorter field lengths over higher field lengths has to do with a fundamental concept of determining document relevancy called TF-IDF (see http://en.wikipedia.org/wiki/Tf%E2%80%93idf). As a short example consider your search returned two documents: the first is 100 words and the second is 1000 words. Each contains your search keyword just once. Since the keyword in the first document was 1% of the text the short document is judged to be more relevant to your search than the long document where the keyword you searched for was only 0.1% of the text. Question 2: It sounds like based on your requirements you might want to try omitting norms. However this may skew your search results in ways you don't expect. It could be that you have been benefiting from some of the nice properties of length normalization and didn't realize it. Another approach might be to actually store document length as some sort of tag field such as labeling documents as ""short"" ""medium"" and ""long"" and then boost documents that match on long or long and medium or whatever. This would also give your end users the ability to filter on document length when they search. Again when I mention nice properties of length normalization you might think of cases where a super long article exists that touches on 10 different topics 1 of which matches the user's search or a long article exists that talks about only 1 topic the one that was searched for. In this case you'd probably prefer the long article over the super long article (even if the super long article matched the search keyword more times). It all depends more on your data and your use cases. Mike thank you. This sounds like what I thought already - nice to get confirmation."
931,A,"How to do query auto-completion/suggestions in Lucene? I'm looking for a way to do query auto-completion/suggestions in Lucene. I've Googled around a bit and played around a bit but all of the examples I've seen seem to be setting up filters in Solr. We don't use Solr and aren't planning to move to using Solr in the near future and Solr is obviously just wrapping around Lucene anyway so I imagine there must be a way to do it! I've looked into using EdgeNGramFilter and I realise that I'd have to run the filter on the index fields and get the tokens out and then compare them against the inputted Query... I'm just struggling to make the connection between the two into a bit of code so help is much appreciated! To be clear on what I'm looking for (I realised I wasn't being overly clear sorry) - I'm looking for a solution where when searching for a term it'd return a list of suggested queries. When typing 'inter' into the search field it'll come back with a list of suggested queries such as 'internet' 'international' etc. Lucene now has some code specifically to do autocompletion/suggestion. See http://stackoverflow.com/questions/24968697/how-to-implements-auto-suggest-using-lucenes-new-analyzinginfixsuggester-api/25301811#25301811 for an answer describing how to use it. Based on @Alexandre Victoor's answer I wrote a little class based on the Lucene Spellchecker in the contrib package (and using the LuceneDictionary included in it) that does exactly what I want. This allows re-indexing from a single source index with a single field and provides suggestions for terms. Results are sorted by the number of matching documents with that term in the original index so more popular terms appear first. Seems to work pretty well :) import java.io.IOException; import java.io.Reader; import java.util.ArrayList; import java.util.HashMap; import java.util.Iterator; import java.util.List; import java.util.Map; import org.apache.lucene.analysis.Analyzer; import org.apache.lucene.analysis.ISOLatin1AccentFilter; import org.apache.lucene.analysis.LowerCaseFilter; import org.apache.lucene.analysis.StopFilter; import org.apache.lucene.analysis.TokenStream; import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter; import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.Side; import org.apache.lucene.analysis.standard.StandardFilter; import org.apache.lucene.analysis.standard.StandardTokenizer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.index.CorruptIndexException; import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.index.Term; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.Query; import org.apache.lucene.search.ScoreDoc; import org.apache.lucene.search.Sort; import org.apache.lucene.search.TermQuery; import org.apache.lucene.search.TopDocs; import org.apache.lucene.search.spell.LuceneDictionary; import org.apache.lucene.store.Directory; import org.apache.lucene.store.FSDirectory; /** * Search term auto-completer works for single terms (so use on the last term * of the query). * <p> * Returns more popular terms first. * * @author Mat Mannion M.Mannion@warwick.ac.uk */ public final class Autocompleter { private static final String GRAMMED_WORDS_FIELD = ""words""; private static final String SOURCE_WORD_FIELD = ""sourceWord""; private static final String COUNT_FIELD = ""count""; private static final String[] ENGLISH_STOP_WORDS = { ""a"" ""an"" ""and"" ""are"" ""as"" ""at"" ""be"" ""but"" ""by"" ""for"" ""i"" ""if"" ""in"" ""into"" ""is"" ""no"" ""not"" ""of"" ""on"" ""or"" ""s"" ""such"" ""t"" ""that"" ""the"" ""their"" ""then"" ""there"" ""these"" ""they"" ""this"" ""to"" ""was"" ""will"" ""with"" }; private final Directory autoCompleteDirectory; private IndexReader autoCompleteReader; private IndexSearcher autoCompleteSearcher; public Autocompleter(String autoCompleteDir) throws IOException { this.autoCompleteDirectory = FSDirectory.getDirectory(autoCompleteDir null); reOpenReader(); } public List<String> suggestTermsFor(String term) throws IOException { // get the top 5 terms for query Query query = new TermQuery(new Term(GRAMMED_WORDS_FIELD term)); Sort sort = new Sort(COUNT_FIELD true); TopDocs docs = autoCompleteSearcher.search(query null 5 sort); List<String> suggestions = new ArrayList<String>(); for (ScoreDoc doc : docs.scoreDocs) { suggestions.add(autoCompleteReader.document(doc.doc).get( SOURCE_WORD_FIELD)); } return suggestions; } @SuppressWarnings(""unchecked"") public void reIndex(Directory sourceDirectory String fieldToAutocomplete) throws CorruptIndexException IOException { // build a dictionary (from the spell package) IndexReader sourceReader = IndexReader.open(sourceDirectory); LuceneDictionary dict = new LuceneDictionary(sourceReader fieldToAutocomplete); // code from // org.apache.lucene.search.spell.SpellChecker.indexDictionary( // Dictionary) IndexReader.unlock(autoCompleteDirectory); // use a custom analyzer so we can do EdgeNGramFiltering IndexWriter writer = new IndexWriter(autoCompleteDirectory new Analyzer() { public TokenStream tokenStream(String fieldName Reader reader) { TokenStream result = new StandardTokenizer(reader); result = new StandardFilter(result); result = new LowerCaseFilter(result); result = new ISOLatin1AccentFilter(result); result = new StopFilter(result ENGLISH_STOP_WORDS); result = new EdgeNGramTokenFilter( result Side.FRONT1 20); return result; } } true); writer.setMergeFactor(300); writer.setMaxBufferedDocs(150); // go through every word storing the original word (incl. n-grams) // and the number of times it occurs Map<String Integer> wordsMap = new HashMap<String Integer>(); Iterator<String> iter = (Iterator<String>) dict.getWordsIterator(); while (iter.hasNext()) { String word = iter.next(); int len = word.length(); if (len < 3) { continue; // too short we bail but ""too long"" is fine... } if (wordsMap.containsKey(word)) { throw new IllegalStateException( ""This should never happen in Lucene 2.3.2""); // wordsMap.put(word wordsMap.get(word) + 1); } else { // use the number of documents this word appears in wordsMap.put(word sourceReader.docFreq(new Term( fieldToAutocomplete word))); } } for (String word : wordsMap.keySet()) { // ok index the word Document doc = new Document(); doc.add(new Field(SOURCE_WORD_FIELD word Field.Store.YES Field.Index.UN_TOKENIZED)); // orig term doc.add(new Field(GRAMMED_WORDS_FIELD word Field.Store.YES Field.Index.TOKENIZED)); // grammed doc.add(new Field(COUNT_FIELD Integer.toString(wordsMap.get(word)) Field.Store.NO Field.Index.UN_TOKENIZED)); // count writer.addDocument(doc); } sourceReader.close(); // close writer writer.optimize(); writer.close(); // re-open our reader reOpenReader(); } private void reOpenReader() throws CorruptIndexException IOException { if (autoCompleteReader == null) { autoCompleteReader = IndexReader.open(autoCompleteDirectory); } else { autoCompleteReader.reopen(); } autoCompleteSearcher = new IndexSearcher(autoCompleteReader); } public static void main(String[] args) throws Exception { Autocompleter autocomplete = new Autocompleter(""/index/autocomplete""); // run this to re-index from the current index shouldn't need to do // this very often // autocomplete.reIndex(FSDirectory.getDirectory(""/index/live"" null) // ""content""); String term = ""steve""; System.out.println(autocomplete.suggestTermsFor(term)); // prints [steve steven stevens stevenson stevenage] } } Note that this was created for an older version of Lucene. In the current version (4.4.0) the abstract method to implement on the Analyzer class is createComponents(String fieldName Reader reader). See http://lucene.apache.org/core/4_4_0/core/org/apache/lucene/analysis/Analyzer.html  In addition to the above (much appreciated) post re: c# conversion should you be using .NET 3.5 you'll need to include the code for the EdgeNGramTokenFilter - or at least I did - using Lucene 2.9.2 - this filter is missing from the .NET version as far as I could tell. I had to go and find the .NET 4 version online in 2.9.3 and port back - hope this makes the procedure less painful for someone... Edit : Please also note that the array returned by the SuggestTermsFor() function is sorted by count ascending you'll probably want to reverse it to get the most popular terms first in your list using System.IO; using System.Collections; using Lucene.Net.Analysis; using Lucene.Net.Analysis.Tokenattributes; using Lucene.Net.Util; namespace Lucene.Net.Analysis.NGram { /** * Tokenizes the given token into n-grams of given size(s). * <p> * This {@link TokenFilter} create n-grams from the beginning edge or ending edge of a input token. * </p> */ public class EdgeNGramTokenFilter : TokenFilter { public static Side DEFAULT_SIDE = Side.FRONT; public static int DEFAULT_MAX_GRAM_SIZE = 1; public static int DEFAULT_MIN_GRAM_SIZE = 1; // Replace this with an enum when the Java 1.5 upgrade is made the impl will be simplified /** Specifies which side of the input the n-gram should be generated from */ public class Side { private string label; /** Get the n-gram from the front of the input */ public static Side FRONT = new Side(""front""); /** Get the n-gram from the end of the input */ public static Side BACK = new Side(""back""); // Private ctor private Side(string label) { this.label = label; } public string getLabel() { return label; } // Get the appropriate Side from a string public static Side getSide(string sideName) { if (FRONT.getLabel().Equals(sideName)) { return FRONT; } else if (BACK.getLabel().Equals(sideName)) { return BACK; } return null; } } private int minGram; private int maxGram; private Side side; private char[] curTermBuffer; private int curTermLength; private int curGramSize; private int tokStart; private TermAttribute termAtt; private OffsetAttribute offsetAtt; protected EdgeNGramTokenFilter(TokenStream input) : base(input) { this.termAtt = (TermAttribute)AddAttribute(typeof(TermAttribute)); this.offsetAtt = (OffsetAttribute)AddAttribute(typeof(OffsetAttribute)); } /** * Creates EdgeNGramTokenFilter that can generate n-grams in the sizes of the given range * * @param input {@link TokenStream} holding the input to be tokenized * @param side the {@link Side} from which to chop off an n-gram * @param minGram the smallest n-gram to generate * @param maxGram the largest n-gram to generate */ public EdgeNGramTokenFilter(TokenStream input Side side int minGram int maxGram) : base(input) { if (side == null) { throw new System.ArgumentException(""sideLabel must be either front or back""); } if (minGram < 1) { throw new System.ArgumentException(""minGram must be greater than zero""); } if (minGram > maxGram) { throw new System.ArgumentException(""minGram must not be greater than maxGram""); } this.minGram = minGram; this.maxGram = maxGram; this.side = side; this.termAtt = (TermAttribute)AddAttribute(typeof(TermAttribute)); this.offsetAtt = (OffsetAttribute)AddAttribute(typeof(OffsetAttribute)); } /** * Creates EdgeNGramTokenFilter that can generate n-grams in the sizes of the given range * * @param input {@link TokenStream} holding the input to be tokenized * @param sideLabel the name of the {@link Side} from which to chop off an n-gram * @param minGram the smallest n-gram to generate * @param maxGram the largest n-gram to generate */ public EdgeNGramTokenFilter(TokenStream input string sideLabel int minGram int maxGram) : this(input Side.getSide(sideLabel) minGram maxGram) { } public override bool IncrementToken() { while (true) { if (curTermBuffer == null) { if (!input.IncrementToken()) { return false; } else { curTermBuffer = (char[])termAtt.TermBuffer().Clone(); curTermLength = termAtt.TermLength(); curGramSize = minGram; tokStart = offsetAtt.StartOffset(); } } if (curGramSize <= maxGram) { if (!(curGramSize > curTermLength // if the remaining input is too short we can't generate any n-grams || curGramSize > maxGram)) { // if we have hit the end of our n-gram size range quit // grab gramSize chars from front or back int start = side == Side.FRONT ? 0 : curTermLength - curGramSize; int end = start + curGramSize; ClearAttributes(); offsetAtt.SetOffset(tokStart + start tokStart + end); termAtt.SetTermBuffer(curTermBuffer start curGramSize); curGramSize++; return true; } } curTermBuffer = null; } } public override Token Next(Token reusableToken) { return base.Next(reusableToken); } public override Token Next() { return base.Next(); } public override void Reset() { base.Reset(); curTermBuffer = null; } } } does it matter how the directory has been previously indexed? Can I run this on a index that was created using the snowball analyzer? Or should I use a field that has not be analyzed at all?  There is complete new series of article which implement above code in complete console application and explore Sitecore enterprise search. I realize that Sitecore employs Lucene but how does this answer the question?  my code based on lucene 4.2，may help you import java.io.File; import java.io.IOException; import org.apache.lucene.analysis.miscellaneous.PerFieldAnalyzerWrapper; import org.apache.lucene.index.DirectoryReader; import org.apache.lucene.index.IndexWriterConfig; import org.apache.lucene.index.IndexWriterConfig.OpenMode; import org.apache.lucene.search.spell.Dictionary; import org.apache.lucene.search.spell.LuceneDictionary; import org.apache.lucene.search.spell.PlainTextDictionary; import org.apache.lucene.search.spell.SpellChecker; import org.apache.lucene.store.Directory; import org.apache.lucene.store.FSDirectory; import org.apache.lucene.store.IOContext; import org.apache.lucene.store.RAMDirectory; import org.apache.lucene.util.Version; import org.wltea4pinyin.analyzer.lucene.IKAnalyzer4PinYin; /** * * * @author <a href=""mailto:liu.gang@renren-inc.com""></a> * @version 2013-11-25上午11:13:59 */ public class LuceneSpellCheckerDemoService { private static final String INDEX_FILE = ""/Users/r/Documents/jar/luke/youtui/index""; private static final String INDEX_FILE_SPELL = ""/Users/r/Documents/jar/luke/spell""; private static final String INDEX_FIELD = ""app_name_quanpin""; public static void main(String args[]) { try { // PerFieldAnalyzerWrapper wrapper = new PerFieldAnalyzerWrapper(new IKAnalyzer4PinYin( true)); // read index conf IndexWriterConfig conf = new IndexWriterConfig(Version.LUCENE_42 wrapper); conf.setOpenMode(OpenMode.CREATE_OR_APPEND); // read dictionary Directory directory = FSDirectory.open(new File(INDEX_FILE)); RAMDirectory ramDir = new RAMDirectory(directory IOContext.READ); DirectoryReader indexReader = DirectoryReader.open(ramDir); Dictionary dic = new LuceneDictionary(indexReader INDEX_FIELD); SpellChecker sc = new SpellChecker(FSDirectory.open(new File(INDEX_FILE_SPELL))); //sc.indexDictionary(new PlainTextDictionary(new File(""myfile.txt"")) conf false); sc.indexDictionary(dic conf true); String[] strs = sc.suggestSimilar(""zhsiwusdazhanjiangshi"" 10); for (int i = 0; i < strs.length; i++) { System.out.println(strs[i]); } sc.close(); } catch (IOException e) { e.printStackTrace(); } } } Hi can you tell me what's the difference between Index_file and Index_file_spell are? Index_file is the index of documents.And Index_file_spell use Index_file to get the index which use to auto-completion/suggestions  Here's a transliteration of Mat's implementation into C# for Lucene.NET along with a snippet for wiring a text box using jQuery's autocomplete feature. <input id=""search-input"" name=""query"" placeholder=""Search database."" type=""text"" /> ... JQuery Autocomplete: // don't navigate away from the field when pressing tab on a selected item $( ""#search-input"" ).keydown(function (event) { if (event.keyCode === $.ui.keyCode.TAB && $(this).data(""autocomplete"").menu.active) { event.preventDefault(); } }); $( ""#search-input"" ).autocomplete({ source: '@Url.Action(""SuggestTerms"")' // <-- ASP.NET MVC Razor syntax minLength: 2 delay: 500 focus: function () { // prevent value inserted on focus return false; } select: function (event ui) { var terms = this.value.split(/\s+/); terms.pop(); // remove dropdown item terms.push(ui.item.value.trim()); // add completed item this.value = terms.join("" ""); return false; } }); ... here's the ASP.NET MVC Controller code:  // // GET: /MyApp/SuggestTerms?term=something public JsonResult SuggestTerms(string term) { if (string.IsNullOrWhiteSpace(term)) return Json(new string[] {}); term = term.Split().Last(); // Fetch suggestions string[] suggestions = SearchSvc.SuggestTermsFor(term).ToArray(); return Json(suggestions JsonRequestBehavior.AllowGet); } ... and here's Mat's code in C#: using System; using System.Collections.Generic; using System.Linq; using System.Text; using Lucene.Net.Store; using Lucene.Net.Index; using Lucene.Net.Search; using SpellChecker.Net.Search.Spell; using Lucene.Net.Analysis; using Lucene.Net.Analysis.Standard; using Lucene.Net.Analysis.NGram; using Lucene.Net.Documents; namespace Cipher.Services { /// <summary> /// Search term auto-completer works for single terms (so use on the last term of the query). /// Returns more popular terms first. /// <br/> /// Author: Mat Mannion M.Mannion@warwick.ac.uk /// <seealso cref=""http://stackoverflow.com/questions/120180/how-to-do-query-auto-completion-suggestions-in-lucene""/> /// </summary> /// public class SearchAutoComplete { public int MaxResults { get; set; } private class AutoCompleteAnalyzer : Analyzer { public override TokenStream TokenStream(string fieldName System.IO.TextReader reader) { TokenStream result = new StandardTokenizer(kLuceneVersion reader); result = new StandardFilter(result); result = new LowerCaseFilter(result); result = new ASCIIFoldingFilter(result); result = new StopFilter(false result StopFilter.MakeStopSet(kEnglishStopWords)); result = new EdgeNGramTokenFilter( result Lucene.Net.Analysis.NGram.EdgeNGramTokenFilter.Side.FRONT1 20); return result; } } private static readonly Lucene.Net.Util.Version kLuceneVersion = Lucene.Net.Util.Version.LUCENE_29; private static readonly String kGrammedWordsField = ""words""; private static readonly String kSourceWordField = ""sourceWord""; private static readonly String kCountField = ""count""; private static readonly String[] kEnglishStopWords = { ""a"" ""an"" ""and"" ""are"" ""as"" ""at"" ""be"" ""but"" ""by"" ""for"" ""i"" ""if"" ""in"" ""into"" ""is"" ""no"" ""not"" ""of"" ""on"" ""or"" ""s"" ""such"" ""t"" ""that"" ""the"" ""their"" ""then"" ""there"" ""these"" ""they"" ""this"" ""to"" ""was"" ""will"" ""with"" }; private readonly Directory m_directory; private IndexReader m_reader; private IndexSearcher m_searcher; public SearchAutoComplete(string autoCompleteDir) : this(FSDirectory.Open(new System.IO.DirectoryInfo(autoCompleteDir))) { } public SearchAutoComplete(Directory autoCompleteDir int maxResults = 8) { this.m_directory = autoCompleteDir; MaxResults = maxResults; ReplaceSearcher(); } /// <summary> /// Find terms matching the given partial word that appear in the highest number of documents.</summary> /// <param name=""term"">A word or part of a word</param> /// <returns>A list of suggested completions</returns> public IEnumerable<String> SuggestTermsFor(string term) { if (m_searcher == null) return new string[] { }; // get the top terms for query Query query = new TermQuery(new Term(kGrammedWordsField term.ToLower())); Sort sort = new Sort(new SortField(kCountField SortField.INT)); TopDocs docs = m_searcher.Search(query null MaxResults sort); string[] suggestions = docs.ScoreDocs.Select(doc => m_reader.Document(doc.doc).Get(kSourceWordField)).ToArray(); return suggestions; } /// <summary> /// Open the index in the given directory and create a new index of word frequency for the /// given index.</summary> /// <param name=""sourceDirectory"">Directory containing the index to count words in.</param> /// <param name=""fieldToAutocomplete"">The field in the index that should be analyzed.</param> public void BuildAutoCompleteIndex(Directory sourceDirectory String fieldToAutocomplete) { // build a dictionary (from the spell package) using (IndexReader sourceReader = IndexReader.Open(sourceDirectory true)) { LuceneDictionary dict = new LuceneDictionary(sourceReader fieldToAutocomplete); // code from // org.apache.lucene.search.spell.SpellChecker.indexDictionary( // Dictionary) //IndexWriter.Unlock(m_directory); // use a custom analyzer so we can do EdgeNGramFiltering var analyzer = new AutoCompleteAnalyzer(); using (var writer = new IndexWriter(m_directory analyzer true IndexWriter.MaxFieldLength.LIMITED)) { writer.SetMergeFactor(300); writer.SetMaxBufferedDocs(150); // go through every word storing the original word (incl. n-grams) // and the number of times it occurs foreach (string word in dict) { if (word.Length < 3) continue; // too short we bail but ""too long"" is fine... // ok index the word // use the number of documents this word appears in int freq = sourceReader.DocFreq(new Term(fieldToAutocomplete word)); var doc = MakeDocument(fieldToAutocomplete word freq); writer.AddDocument(doc); } writer.Optimize(); } } // re-open our reader ReplaceSearcher(); } private static Document MakeDocument(String fieldToAutocomplete string word int frequency) { var doc = new Document(); doc.Add(new Field(kSourceWordField word Field.Store.YES Field.Index.NOT_ANALYZED)); // orig term doc.Add(new Field(kGrammedWordsField word Field.Store.YES Field.Index.ANALYZED)); // grammed doc.Add(new Field(kCountField frequency.ToString() Field.Store.NO Field.Index.NOT_ANALYZED)); // count return doc; } private void ReplaceSearcher() { if (IndexReader.IndexExists(m_directory)) { if (m_reader == null) m_reader = IndexReader.Open(m_directory true); else m_reader.Reopen(); m_searcher = new IndexSearcher(m_reader); } else { m_searcher = null; } } } } Would it be possible for you to add a C# driver snippet that executes your code as well as the code to build the index? I can get your code to compile just fine but I have trouble figuring out how to build my Directory so that it can be queried by the code above. does it matter how the directory has been previously indexed? Can I run this on a index that was created using the snowball analyzer? Or should I use a field that has not be analyzed at all? (asked the same question above)  You can use the class PrefixQuery on a ""dictionary"" index. The class LuceneDictionary could be helpful too. Take a look at this article. It explains how to implement the feature ""Did you mean ?"" available in modern search engine such as Google. You may not need something as complex as described in the article. However the article explains how to use the Lucene spell package. One way to build a ""dictionary"" index would be to iterate on a LuceneDictionary. Hope it helps"
932,A,Does anyone know where decent documentation describing the Lucene index format IN DETAIL on the web is? I am mainly curious as to the inner workings of the engine itself. I couldnt find anything about the index format itself (IE in detail as though you were going to build your own compatible implementation) and how it works. I have poked through the code but its a little large to swallow for what must be described somewhere since there are so many compatible ports to other languages around. Can anyone provide a decent link? Have you seen this: http://lucene.apache.org/java/2_4_0/fileformats.html? It's the most detailed I've found. Although Lucene in Action does stop short of the detail in that link I found it a useful companion to keep a handle on the big picture concepts while understanding the nitty gritty. Thanks! Thats close enough to what I wanted. Much appreciated.
933,A,"Using SnowBallAnalyzer with PyLucene I'm trying to use SnowBallAnalyzer in PyLucene but I always get an error saying: InvalidArgsError when I try to create an instance of it like this: analyzer = SnowBallAnalyzer(""Spanish"") or analyzer = SnowBallAnalyzer(""Spanish"" STOPWORDS) What I really need is i.e if I search for ""Fútbol"" I should obtain the documents that have the word ""futbol"" or ""fútbol"". So... I would like to apply SnowBallAnalyzer to the text I would like to index an to the query. Any help will be appreciated. Thanks in advance. I don't know pylucene very well as I only work with the java version but as far as I know pylucene is accessing the java implementation. If this is the case you are missing the Version parameter in the constructor. SnowballAnalyzer(Version matchVersion String name String[] stopWords) As SnowballAnalyzer has been deprecated in lucene 3.1.0 I suggest you directly use the spanish analyzer. SpanishAnalyzer(Version matchVersion Set<?> stopwords)"
934,A,How to normalize Lucene scores? I need to normalize the Lucene scores between 0 and 1. For example a random query returns the following scores... 8.864665 2.792687 2.792687 2.792687 2.792687 0.49009037 0.33730242 0.33730242 0.33730242 0.33730242 What's the biggest score ? 10.0 ? thanks You can divide all scores with the maximum score to get scores between 0 and 1. However please note that the normalised scores should be used to compare the results of a single query only. It is not correct to compare the scores (normalised or not) of results from 2 different queries. In that case just multiply the 3 scores and sort the results based on that. There is no need to normalize the Lucene scores. (You can normalize if you want to the final ordering will not change) @nikhil500 ok. One last thing if I want to measure the difference between retrieval results using Lucene only and combined scores than the normalization is necessary right ? I mean the other scores have lower influence if Lucene scores are not normalized... No if you multiply the 3 scores then normalization will not have any effect on the final order of the results or the relative scores of the records in the result set. @nikhil500 really ? So if I have a bunch of queries how can I see which ones are performing better ? Please post some more details of how (and why) you want to compare the results of multiple queries. Scores across queries are not directly comparable but depending on your exact problem we may be able to come up with some solution. @nikhil500 My issue is that for each query I have to combine multiple scores (coming from other software) and they are all normalized (between 0 and 1) except for Lucene scores. Do you want to reorder the results from Lucene based on scores coming from other sources or do you want to merge results from other sources with the Lucene results? If you want to reorder then just go ahead and multiply the Lucene score with the external score. However if you want to merge results from external sources with Lucene results then it gets much more complicated - you need to somehow figure out a 'normalization factor' since it will be incorrect to assume that the top document from a Lucene result set is always scored 1 on a scale of 0 to 1. @nikhil500 The second one. And my question is how to do it indeed. Should I consider the query with the highest score and use that score for the normalization ? I need some help here Please edit your question and add some more info about what exactly you are trying to do - it is not clear why and how you want to merge results from multiple queries (Your question talks about normalizing scores of a single Lucene result set). Do you need to merge Lucene results for multiple queries with external results? If you can give some details of the exact use case then I can try to help solve the problem. @nikhil500 I don't want to merge results for multiple queries. I want to merge Lucene results with other software results per each query. i.e. I have query1 Lucene: 8.1 Score2: 0.98 Score3: 0.754 The problem is actually simple I need to assign a correct weight to Lucene scores or normalize Lucene results in order to avoid unbalanced results when I combine the scores.  There is no maximum score in Solr it depends on too many variables so it can't be predicted. But you can implement something called normalized score (Scores As Percentages) which is not recommended. See related links for more details: Is it possible to set a Solr Score threshold 'reasonably' independent of results returned? (i.e. Is Solr Scoring standardized in any way) how do I normalise a solr/lucene score? Remove results below a certain score threshold in Solr/Lucene?  There is no good standard way to normalize scores with lucene. Read this: ScoresAsPercentages and this explanation In your case the highest score is the score of the first result if the results are sorted by score. But this score will be different for every other query. See also how-do-i-normalise-a-solr-lucene-score My issue is that I have lucene scores + other scores (not related to Lucene) for each query results. The other scores are all normalized between 1 and 0. If I don't normalize Lucene scores in the same way I'm going to have unbalanced results... Have a look at http://lucene.apache.org/java/2_9_2/api/core/org/apache/lucene/search/Collector.html class. You might have to write your own Collector. Maybe using your other scores or a combination.
935,A,"Apache Lucene: How to get the first matching substring from a Document I could not find any info on the web and stackoverflow on how to get the first matching character subsequence from a Lucene Document. ATM i'm using this logic to retrieve results from Lucene:  Document doc=searcher.doc(hit.doc); String text=doc.get(""text""); if (text.length() > 80){ text=text.substring(080); } results.add(new SearchResult(doc.get(""url"") doc.get(""title"") text)); As you can see this just takes the first 80 chars of the searched text and wraps it together with some other data into a SearchResult object. Is it somehow possible to retrieve the first or even highest scoring subsequence of the text which actually contains any searchterms? It is called hit highlighter. This is probably a duplicate of another highlighter question  You need Lucene Highlighter. Here and here you can find some more info on it. Also note that there are several Highlighter implementations for both Lucene 2.x and Lucene 3.0. Take the one that fits your task better."
936,A,"what analzyer is good for my situation? hibernate search case We are running a search app for book. It is implemented by hibernate search. Book entity is defined as following: @Entity @Indexed public class Book{ @DocumentId private Integer UID; @Field private String title; @Field private String description; ...} If a user search book name say they input Microsoft access 2007 books with title or description contains microsoft access or 2007 returned. That is what we expected. Some of books are totally unrelated because of keyword 2007. I am looking for a solution to understand importance of each keywords. In that case 2007 is less important in search. But for that search there is no difference for microsoft access or 2007. The second user case: Is there a good analyzer that can use in indexing and querying to support multiple phrases? I thought the default analyzer of hibernate search just tokenize search words into single word? If search words is microsoft access 2007 results have best score if they contains ""microsoft access"" the other search example: ""salt lake city"" ""united states"" results are not expected if only match salt city or lake or at least they should be behind results with ""salt lake city"". Can anyone offer me some clues? thanks! Lucene should already discount terms that occur frequently and thus don't discriminate well among documents. If you want to increase that effect you have a few choices: Change the similarity function from the default and use the new function to weight terms differently Boost low-df (high idf) terms in the query by first looking up the number of documents that contain a given term and adjusting that term's weight accordingly Write a classifier that can a priori decide which terms are not going to be as effective (e.g. year numbers) and adjust their weight accordingly Use something like WordNet or Wikipedia as a source of phrases (e.g. leadership skills) that you index as a single token. This will involve a modified TokenStream as configured by your analyzer.  I don't know how to differentiate a good 2007 from a bad one. One thing you could do is to use a analyzer that ignores numbers for description but use a regular analyzer for title. That way only numbers in the title will be picked up. In practice it's not a whole analyzer but a simple filter that you can write and add to the analyzer stack. You can also index description twice once ignoring numbers and once not ignoring them. You can then play with the boost factor at query time to search both fields but give the one with numbers a low priority. Another solution is to ignore some number patterns in your custom filter (ie year-style numbers single digits numbers etc): these would be the most common type of noisy numbers that you would want ignored (that's what I would go for first I think). As for the phrase search simply use a PhraseQuery by Lucene or use the more friendly Hibernate Search DSL Query luceneQuery = mythQB .phrase() .onField(""history"") .matching(""Thou shalt not kill"") .createQuery(); The whole doc for the query DSL is here Emmanuel it is not something about number. I just made it as example. What I mean is when user input search words some of words are less important some of words are really important. Say ""Leadership Skills"". Leadership is more important than skills. We can assume user is looking for something about leadership and then skills. with me? so programming skills book will not show up at top. You want some kind of semantic search. Maybe another product will fit your bill better. For me the similitude algorithm is good enough in most cases to bring me leadership skills books at the top using the regular tokenized approach. At least good enough that I never had to explore semantic search engines."
937,A,Apache Lucene: how to convert collection index to another format? I need to convert an index generated by Apache Lucene into another collection representation. I currently have a collection of documents with many attributes. I need to create document pairs with similarity measures from it in order to pass them to classifiers. Do you know any tutorial I could use to perform this ? thanks The similarity measures need to be based on a query. i.e. you query your Lucene document set and you get back a set of documents with relative scores. If you want to compare every document with every other (is that right? it's hard to tell from the question) then you need to use a feature of each document as the basis for the queries. For example you could extract the top N terms (by frequency excluding stop words) from each document. If you have X documents then you will have X queries. Then you execute each of your X queries against the index and you get back relative similarities of each document with every other. This is a matrix you could use for classification. Another alternative would be to use the title or synopsis of each document as the basis for the query (again excluding stops). Also should I use MatchAllDocsQuery to get all similarity values with all collection docs ? Thanks you perfectly understood what I meant. So should I run a query for each document ? Successively I will save the results in a structured file to pass to a classifier. I actually already have a structured xml input and with description tags geolocation information for each document. For the description I will use tf.idf cosine similarity for geotags I need to implement Harvesine similarity. I dunno exactly how to integrate such similarity metrics.. I will use tf.idf only for now which should be implemented in Lucene. If you know any tutorial... very welcome since I don't have experience with Lucene... Yes the default Scoring function in Lucene uses tdf.if & cosine similarity so you can probably use it out of the box. You can customise it though. http://lucene.apache.org/java/2_4_0/scoring.html and also see http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/Similarity.html ok thnaks. What about complex queries ? You know I'm passing a document as query. THis means I have some several fields with text description (that I should process differently. i.e. stopwords only on few of them) then I have another numerical field with geo-coordinates. I should package all this stuff into my query.
938,A,Lucene/Solr aggregation I faced with large problem. Lucene can not aggregate data. What alternative solution available. After some improvement I have and need next : lucene data example product_id distri_id stock 1 d1 10 1 d2 20 1 d3 23 I need do query with next condition for example: prod_id =1 and distri_id = (d1 or d2) and stock(sum of d1 and d1) > 13. So I must aggregate data per dsitri and select where sum of stock more the some value. In other SQL terminology I must do GROUP BY distri HAVING sum(stock)>13. Can somebody suggest what to do in this case. Thanks! Best regards Artem Could you write how many rows of data you have? Could also add which db you tried? Postgresql. Indexing data comparable with tens millions You might want to have a look at field collapsing patch which I guess offers similar functionality. Also this says: <..>introduced Solr’s Result Grouping also called Field Collapsing that limits the number of documents shown for each “group” normally defined as the unique values in a field or function query. <..> You’ll need a recent nightly build of Solr 4.0-dev or the newly released LucidWorks Enterprise v1.6 our commercial version of Solr.  I suggest you use a database for problems solved by a database. Lucene wasn't designed for this. Unfortunately database is very very slow to search. What are you searching on? I provided only simple example to demonstrate problem. In complex search have difficult logic and if replace on to database queries it would work a long time. Lucene is my only salvation. In any case database is a fail solution for me. you're probably going to have to develop a hybrid solution with data like your example in a RDBMS including links to your docuements in Solr AND have your Solr Search INdex providing the search features. Also many databases are now adding search index type features but I have no experience with them. Also have asked about this on the Lucene/Solr specific support groups under apache.org ? Good luck!
939,A,"Lucene Sentence Search Is it possible to search for a phrase of the kind Searching is fun in Lucene? When I try to search with this Lucene ends up looking for the word fun alone. If you are using a QueryParser object to parse your query you can configure it to automatically assume the '+' operator that Aku spoke of in his answer (sorry Aku for not simply commenting but comments apparently don't support code formatting). For example: String defaultField = ...; Analyzer analyzer = ...; QueryParser queryParser = new QueryParser(defaultField analyzer); queryParser.setDefaultOperator(QueryParser.Operator.AND); Query query = queryParser.parse(""Searching is fun"");  Try to put in in quotes: ""Searching is fun"" or add '+' to required words +Searching +fun See ""Lucene - Query Parser Syntax"" for available options The link is dead"
940,A,"Problems with hyphen in Jackrabbit XPath query Firstly let me just say that I'm very new to JSR-170 and Jackrabbit/Lucene in general. I have the following XPath query: //*[@sling:resourceType=""users/user-profile"" and jcr:contains(*/*/*'sophie\-a')] order by @jcr:score descending I have a user named Sophie-Allen and a user named Sophie-Anne. Searching using the above query returns zero results where searching for 'sophie' alone returns both users. I understand that the hyphen means exclude in JSR-170 but I've escaped it (as you can see above). Why is this query not returning both users? Another strange thing is when I use asterisks (the hyphens are all escaped when executed): Searching for 'sophie-allen' returns Sophie-Allen's record. Searching for 'soph*' returns both Sophie-Allen and Sophie-Anne. Searching for 'sophie-a* returns nothing. Searching for 'sophie-allen*' returns nothing. I understand that with jcr:contains technically you don't need to use asterisks but looking at the above behaviour it seems to have some sort of effect. Is there something else that I'm missing with regards to hyphens and asterisks in XPath queries and searching a JCR? I've googled everything I can think of and read through the spec but can't seem to find anything that answers my question. Thanks in advance. Edit: It looks like a 'phrase query' doesn't work with jcr:contains (anymore?) as the default Lucene Analyzer tokenizes on the hyphen meaning it splits 'sophie-allen' to sophie and allen. Edit 2: I've tried using a custom analyzer and tokenizer as suggested by someone on the Jackrabbit Users list but that hasn't helped either Lucene is still taking the hyphen and omitting the results I want. @alejandro - np thanks. I'm changing your tags to `xpathengines` because besides this question relates to `jcr:contains` extension function there is no `order by` operator in XPath 1.0 or 2.0 You are correct that Lucene does split ""sophie-allen"" into two tokens but those tokens are adjacent. You said you've tried a phrase expression like this: ... jcr:contains(*/*/*'""sophie-a*""') ... This should work by finding the token ""sophie"" followed by another token containing 'a' as the first character. Because the same analyzer used during indexing should be used to tokenize this phrase expression the '-' character will still be used as a delimiter [1]. (Note that if you're specifying your XPath expression in Java code you'd have to escape the double-quote characters with a preceding backslash.) However if this does not work you might try taking out the hyphen in this expression. Because you're using wildcards the logic might be incorrectly tokenizing the wildcard expression. In other words try: ... jcr:contains(*/*/*'""sophie a*""') ... Of course without wildcards this would probably work (with or without the hyphen): ... jcr:contains(*/*/*'""sophie-allen""') ... Good luck! P.S. I've not verified that this works in Jackrabbit but it does work in ModeShape (which also uses Lucene). [1] The exact rules depend on the tokenizer. For example the StandardTokenizer filters out English stop words but tokenizes the '-' character except when there's a number in the token (in which case the whole token is interpreted as a product and is not split. Thanks for this I'll give it a whack and let you know. no dice on the double quotes. Actually nothing worked with jackrabbit. :(  While working on this with a colleague we discovered this JIRA for ModeShape incidentally logged by Randall (who answered here too). It turns out that the problem is caused by the fact that jackrabbit isn't handling a wildcard in a search term with a wildcard properly/too well. Randall had done a fix for ModeShape but my colleagues and project team nominated not to fix our problem at this stage as the use of Jackrabbit was not 100% certain. I'd like to associate the answer to this question to Randall but his post isn't the actual answer. I'll mark this post as the answer unless Randall comes along and posts something."
941,A,Does Zend Lucene need Java Lucene? When implementing Zend Lucene do we need to install Java on our server or not? Although I have not used it it appears that you do not need Java to use the Zend_Search_Lucene component. According to the documentation Zend_Search_Lucene is a fully PHP implementation of Lucene. However there is support for interoperability between the Java indexes and the PHP indexes. This is correct. Java is not required.  Zend implementation is the port of java to PHP of Lucene. This is a great approach of Zend. But the PHP version will lack for big indexes. Remember that if you have a big index php will need load it each time you call the script that make the search/insert into Lucene. The java version load the index when the JVM starts and keep it in memory to use it.
942,A,"How to handle numbers as both words and numerals (""one"" vs. ""1"") in Zend_Lucene I have news-article content which is being indexes using Lucene and interrogated using Zend_Lucene in PHP. The content frequently makes reference to UK television channels (e.g. BBC One) but I know that our users will often enter a search term of ""BBC 1"" or ""BBC1"" rather than ""BBC One"". Is there any ""standard"" approach to dealing with this numbers-as-words vs. numbers-as-numerals search issue? My choices seem to be to either amend the search term whenever I see numbers so for example I change a search terms of ""BBC1"" to ""BBC 1 One"" (or something similar) - or I amend the indexed content so that numerals are converted to words and vice-versa and both versions stored in the index. Please see this lucene FAQ entry it suggests to use a token filter to provide alias / aliasing of words: 26. How can I make 'pig' also match 'hog' ?: As far as I know Lucene does not provide a tokenzier that support term aliasing but you should be able to write one yourself. All you need is to write a TokenFilter that accepts a word pair mapping and uses it map the first word to the second. Again make sure to use the same analyzer both during the indexing and searching and don't forget to submit your code to the Lucene project so other can use it as well ;-) That's older information probably this is even more comfortable nowadays but probably worth the direction."
943,A,"Commons Digester: How to build complex XML-based queries with Apache Lucene? I need to build a XML-based query with Apache Lucene and Commons Digester. My docs have this format: <doc> <id>361492799</id> <title>Dan1</title> <description>We had another Flickr meetup in Rochester the biggest that Ive been to. 12 people showed up.Da he was to the right.</description> <time>18934934</time> <tags>flickrmeetup rochester dan totheright 200701</tags> <geo><latitude>324234</latitude><longitude>28342349</longitude></geo> <event>135961</event> </doc> And the query is actually also a document that I need to compare with the entire collection. Each attribute has a different similarity metric. For example ""description"" has tf-idf cosine similarity. ""Time"" is just the difference and ""latitude"" + ""longitude"" is compared using the haversine distance. For now I've only performed searches with simple textual queries such as ""word1 word2"". How can I build more complex queries instead ? Thanks I need to build a XML-based query with Apache Lucene and Commons Digester. This article should help you get started for analysing content from xml take a look at TIKA Apache Tika - a content analysis toolkit Apache Tika™ is a toolkit for detecting and extracting metadata and structured text content from various documents using existing parser libraries. THanks i've solved the first part of my problem which is the parsing of the data. I now need to build a query having the same (XML) structure of the docs. You know how can I do this ? thanks  Have you looked at SOLR? SOLR is basically Lucene+an entire XML based querying and indexing server. I actually don't need anymore a parser but to build a query with the same XML structure of the collection documents"
944,A,"Lucene sorting in (keyword alphabetical) order How do I sort my search results in the following order (query phrase alphabetical). To give an example if I have documents in my index each with one field (foodname). The food names of the documents are fat chicken chicken breast chicken lasagna If I query with the search word ""chicken"" I would like my results to be in the following order chicken breast chicken lasagna fat chicken Please note that the boosting factor for all these documents are the same in indexing stage. Your help is appreciated Thanks -Venu I'll just give a high level overview of what I'd do. Consider using a food ingredients / recipes types dictionary & assigning higher boosting for words in the index that matches items in food dictionary. And perhaps to go even further assign weights to certain keywords in your food dictionary. Here's an example of a food ingredients dictionary. @Joycechan thanks for answering. I am able to get the results. However the problem is that I have to give higher boost based on position during query time I was more so thinking along the lines of boosing certain keywords in the index. An implementation can look like this http://sujitpal.blogspot.com/2011/01/payloads-with-solr.html see this article in the solr wiki as well (How can I increase the score for specific documents) http://wiki.apache.org/solr/SolrRelevancyFAQ#How_can_I_increase_the_score_for_specific_documents"
945,A,"How to get Lucene Fuzzy Search result 's matching terms? how do you get the matching fuzzy term and its offset when using Lucene Fuzzy Search?  IndexSearcher mem = ....(some standard code) QueryParser parser = new QueryParser(Version.LUCENE_30 CONTENT_FIELD analyzer); TopDocs topDocs = mem.search(parser.parse(""wuzzy~"") 1); // the ~ triggers the fuzzy search as per ""Lucene In Action"" The fuzzy search works fine. If a document contains the term ""fuzzy"" or ""luzzy"" it is matched. How do I get which term matched and what are their offsets? I have made sure that all CONTENT_FIELDs are added with termVectorStored with positions and offsets . Are you looking for something along these lines? http://lucene.apache.org/java/3_0_0/api/contrib-highlighter/index.html No. I am not looking to hightlight text ;I need to do further text processing . Before doing further text processing  I need to figure out which term matched was it ""fuzzy"" or ""luzzy"" etc. as this is a fuzzy match. There was no straight forward way of doing this however I reconsidered Jared's suggestion and was able to get the solution working. I am documenting this here just in case someone else has the same issue. Create a class that implements org.apache.lucene.search.highlight.Formatter public class HitPositionCollector implements Formatter { // MatchOffset is a simple DTO private List<MatchOffset> matchList; public HitPositionCollector( { matchList = new ArrayList<MatchOffset>(); } // this ie where the term start and end offset as well as the actual term is captured @Override public String highlightTerm(String originalText TokenGroup tokenGroup) { if (tokenGroup.getTotalScore() <= 0) { } else { MatchOffset mo= new MatchOffset(tokenGroup.getToken(0).toString() tokenGroup.getStartOffset()tokenGroup.getEndOffset()); getMatchList().add(mo); } return originalText; } /** * @return the matchList */ public List<MatchOffset> getMatchList() { return matchList; } } Main Code public void testHitsWithHitPositionCollector() throws Exception { System.out.println("" .... testHitsWithHitPositionCollector""); String fuzzyStr = ""bro*""; QueryParser parser = new QueryParser(Version.LUCENE_30 ""f"" analyzer); Query fzyQry = parser.parse(fuzzyStr); TopDocs hits = searcher.search(fzyQry 10); QueryScorer scorer = new QueryScorer(fzyQry ""f""); HitPositionCollector myFormatter= new HitPositionCollector(); //Highlighter(Formatter formatter Scorer fragmentScorer) Highlighter highlighter = new Highlighter(myFormatterscorer); highlighter.setTextFragmenter( new SimpleSpanFragmenter(scorer) ); Analyzer analyzer2 = new SimpleAnalyzer(); int loopIndex=0; //for (ScoreDoc sd : hits.scoreDocs) { Document doc = searcher.doc( hits.scoreDocs[0].doc); String title = doc.get(""f""); TokenStream stream = TokenSources.getAnyTokenStream(searcher.getIndexReader() hits.scoreDocs[0].doc ""f"" doc analyzer2); String fragment = highlighter.getBestFragment(stream title); System.out.println(fragment); assertEquals(""the quick brown fox jumps over the lazy dog"" fragment); MatchOffset mo= myFormatter.getMatchList().get(loopIndex++); assertTrue(mo.getEndPos()==15); assertTrue(mo.getStartPos()==10); assertTrue(mo.getToken().equals(""brown"")); } Although this feels a bit _hackish_ (not your fault just feels as there should be a cleaner way) this is is the only working implementation I have found. Thank you!"
946,A,"which directory should i use for Lucene Index directory? I am new to Lucene and I wonder which directory is the best solution for indexing. My project is a Java-based web project which uses PostgreSQL for database. Searching is the most important part of the project. Therefore I decided to use Lucene but I couldn't decide directory for indexing. When I searching Lucene I found this article which says local file system is not good and it offers JDBCDirectory. Which directory should I use? local file directory jdbcdirectory (but i'm planning to use PostgreSQL and I don't know how it will be.) ramdirectory (I think this cannot be because of the too much data) and your suggestions? The article that you are referencing is specifically about clustered environments. I currently work on an enterprise web-app and I use the FS (file system) directory provider and it works great. If you do not have a ""complicated"" environment then I would recommend using it. The RAM directory provider is not a viable option for a production environment in my opinion so I wouldn't even consider that one. It is useful for testing though."
947,A,"Need to run Lucene Analyzer offline from servlet This might be quite a meaty question but my problem is that I have a Spring servlet that is used for real-time searching. I need to strip out the Lucene analyzer so that it runs offline and is called by the servlet everytime a query comes in to it rather than having the analyzer within the servlet. However I don't quite know how to do this and call the analyzer service from the servlet. Can anyone point me in the right direction? At the moment I have this: RAMDirectory ramDirectory = new RAMDirectory(); StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_31); IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_31 analyzer); IndexWriter indexWriter = new IndexWriter(ramDirectory config); Document document = new Document(); // TEST DATA document.add(new Field(""firstName"" ""John"" Field.Store.YES Field.Index.ANALYZED)); document.add(new Field(""occupation"" ""Engineer"" Field.Store.YES Field.Index.ANALYZED)); document.add(new Field(""firstName"" ""Mary"" Field.Store.YES Field.Index.ANALYZED)); document.add(new Field(""occupation"" ""Field Engineer"" Field.Store.YES Field.Index.ANALYZED)); document.add(new Field(""firstName"" ""Jamie"" Field.Store.YES Field.Index.ANALYZED)); document.add(new Field(""occupation"" ""Primary teacher"" Field.Store.YES Field.Index.ANALYZED)); // END TEST DATA indexWriter.addDocument(document); indexWriter.optimize(); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(ramDirectory); String[] fields = {""firstName"" ""occupation""}; MultiFieldQueryParser parser = new MultiFieldQueryParser(null fields analyzer); Query query = parser.parse(searchQuery); // Parsing of results here Thanks. Making sure I got the workflow you're looking for right: (1) servlet receives http request (2) offloads processing to a separate lucene process (3) get results from lucene (4) respond to user. Is that the general idea? Yep that's exactly it. I've got it working all within the servlet but I realise this is poor design in terms of performance. Please clarify: do you want to run Lucene as a separate process that listens on some socket and call it from your servlet or would you like to launch LuceneAnalyzer on-demand as a new process from your servlet? Hi sorry I want it to run as a separate process. Why do you find your current design poor in terms of performance? What problem are you running into exactly? I believe that what you're looking for is some form of inter-process communications. Two options that come to mind are: Synchronous: offload processing to another server (HTTP or plain TCP) that envelopes Lucene and responds with the query results. This will be useful for scaling your system since it lets you easily separate Lucene to another machine but other than that there are no immediate performance gains. Asynchronous: offload it to a queue (e.g. ZeroMQ) that some other process listens on runs Lucene then returns the result through some other queue. This is less advantageous if you need to respond to a user on the other end and in any case will be hard to implement with java servlets (though if anyone knows otherwise I'd love to hear about it)."
948,A,"Lucene custom scoring for numeric fields I would like to have in addition to standard term search with tf-idf similarity over text content field scoring based on ""similarity"" of numeric fields. This similarity will be depending on distance between the value in query and in document (e.g. gaussian with m= [user input] s= 0.5) I.e. let's say documents represent people and person document have two fields: description (full text) age (numeric). I want to find documents like description:(x y z) age:30 but age to be not the filter but rather part of score (for person of age 30 multiplier will be 1.0 for 25-year-old person 0.8 etc.) Can this be achieved in a sensible manner? EDIT: Finally I found out this can be done by wrapping ValueSourceQuery and TermQuery with CustomScoreQuery. See my solution below. EDIT 2: With fast-changing versions of Lucene I just want to add that it was tested on Lucene 3.0 (Java). Okay so here's (a bit verbose) proof-of-concept as a full JUnit test. Haven't tested its efficiency yet for large index but from what I've read probably after a warm-up it should perform well providing there's enough RAM available to cache numeric fields.  package tests; import org.apache.lucene.analysis.Analyzer; import org.apache.lucene.analysis.WhitespaceAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.document.NumericField; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.queryParser.QueryParser; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.Query; import org.apache.lucene.search.ScoreDoc; import org.apache.lucene.search.TopDocs; import org.apache.lucene.search.function.CustomScoreQuery; import org.apache.lucene.search.function.IntFieldSource; import org.apache.lucene.search.function.ValueSourceQuery; import org.apache.lucene.store.Directory; import org.apache.lucene.store.RAMDirectory; import org.apache.lucene.util.Version; import junit.framework.TestCase; public class AgeAndContentScoreQueryTest extends TestCase { public class AgeAndContentScoreQuery extends CustomScoreQuery { protected float peakX; protected float sigma; public AgeAndContentScoreQuery(Query subQuery ValueSourceQuery valSrcQuery float peakX float sigma) { super(subQuery valSrcQuery); this.setStrict(true); // do not normalize score values from ValueSourceQuery! this.peakX = peakX; // age for which the age-relevance is best this.sigma = sigma; } @Override public float customScore(int doc float subQueryScore float valSrcScore){ // subQueryScore is td-idf score from content query float contentScore = subQueryScore; // valSrcScore is a value of date-of-birth field represented as a float // let's convert age value to gaussian-like age relevance score float x = (2011 - valSrcScore); // age float ageScore = (float) Math.exp(-Math.pow(x - peakX 2) / 2*sigma*sigma); float finalScore = ageScore * contentScore; System.out.println(""#contentScore: "" + contentScore); System.out.println(""#ageValue: "" + (int)valSrcScore); System.out.println(""#ageScore: "" + ageScore); System.out.println(""#finalScore: "" + finalScore); System.out.println(""+++++++++++++++++""); return finalScore; } } protected Directory directory; protected Analyzer analyzer = new WhitespaceAnalyzer(); protected String fieldNameContent = ""content""; protected String fieldNameDOB = ""dob""; protected void setUp() throws Exception { directory = new RAMDirectory(); analyzer = new WhitespaceAnalyzer(); // indexed documents String[] contents = {""foo baz1"" ""foo baz2 baz3"" ""baz4""}; int[] dobs = {1991 1981 1987}; // date of birth IndexWriter writer = new IndexWriter(directory analyzer IndexWriter.MaxFieldLength.UNLIMITED); for (int i = 0; i < contents.length; i++) { Document doc = new Document(); doc.add(new Field(fieldNameContent contents[i] Field.Store.YES Field.Index.ANALYZED)); // store & index doc.add(new NumericField(fieldNameDOB Field.Store.YES true).setIntValue(dobs[i])); // store & index writer.addDocument(doc); } writer.close(); } public void testSearch() throws Exception { String inputTextQuery = ""foo bar""; float peak = 27.0f; float sigma = 0.1f; QueryParser parser = new QueryParser(Version.LUCENE_30 fieldNameContent analyzer); Query contentQuery = parser.parse(inputTextQuery); ValueSourceQuery dobQuery = new ValueSourceQuery( new IntFieldSource(fieldNameDOB) ); // or: FieldScoreQuery dobQuery = new FieldScoreQuery(fieldNameDOBType.INT); CustomScoreQuery finalQuery = new AgeAndContentScoreQuery(contentQuery dobQuery peak sigma); IndexSearcher searcher = new IndexSearcher(directory); TopDocs docs = searcher.search(finalQuery 10); System.out.println(""\nDocuments found:\n""); for(ScoreDoc match : docs.scoreDocs) { Document d = searcher.doc(match.doc); System.out.println(""CONTENT: "" + d.get(fieldNameContent) ); System.out.println(""D.O.B.: "" + d.get(fieldNameDOB) ); System.out.println(""SCORE: "" + match.score ); System.out.println(""-----------------""); } } } This can be generalized to arbitrary number of `ValueSourceQuery`-s as CustomScoreQuery has varargs constructor. Score method to override is then `public float customScore(int doc float subQueryScore float[] valSrcScore)`.  This can be achieved using Solr's FunctionQuery"
949,A,"Data structure for Pattern Matching on large data Problem Background I have a finite vocabulary containing of say 10 symbols [A-J]. What these symbols mean is not relevant to the question. They could be DNA bases phonemes words etc. An item is a sequence of symbols. In this problem all items are of the same length (say 6). E.g. A C B A D J I have a large (5M) table that contains counts of all items of length 6 sampled from some known data. E.g. A C B A D J 55 B C B I C F 923 A B C D E G 478 Given a new sequence with one unknown symbol my task is to guess the symbol. In the following example the missing symbol is ?. B C B ? C F A simple solution to guess ? is to look into my table and find the item with the largest count that fits the pattern B C B ? C F Questions What is a good data structure to store my item-frequency table so that I handle space-time reasonably efficiently? I prefer to use less memory if the computation at query time is reasonable. (I will be having many such tables and so the 5M number is just an approximation.) What are some implementation details that can make a big difference in processing speed? Things I have thought of: Make a string out of every sequence and use regexes to match. Caveat: 1. O(n) is unacceptable. (2) Regexes are slow. (3) Strings (in java at least) are bloated. Let Lucene handle the indexing. Turn off tfidf scoring. Use phrase-search. Potentially use the count values for scoring so that Lucene takes care of the sorting too. Use prefix and suffix tries to index each item. Use db (likely in-memory) with the entire data in one/separate column to handle the search. Updates In my actual application I will be working with sequences of length 5678910 stored separately. I simplified the problem by restricting it to a fixed length. Hence the constraint/preference to a solution that uses less memory. My vocabulary size can be assumed to be under 20. I would design it such that each item resides in it's own indexed column. So 6 columns + 1 column for the frequency. Querying and ordering from the DB should be very fast. The two constants in your description: 10 letters and length=6. Are they just examples or real values? Can number of letters of length be significantly bigger? @maxim1000. The constants can be considered close to real. Added Updates 12 Thanks. @arviman - I would suggest you learn more about how databases work. In this case using the indexes only saves you a factor of 2 on numbers of records from a full scan and adds enough complexity that it is a loss. Will there only ever be one unknown? @idz Yes. For now :-) I was among the ""everyone missing the obvious"" here. Just use any quick key/value lookup that is available to you. And look up all of your possible values. It is a small set and won't take long. Anything else short of storing your data 6 times will be slower. If you had a large possible vocabulary then my previous answer would be appropriate. Here is my old (and bad) answer. I would stick them in a database with multiple concatenated indexes. How many is up to you. At a minimum I would have 2. I would have an index on (col1 col2 col3 col4 col5 col6) and (col4 col5 col6 col1 col2 col3). This would mean that no matter which column was missing there would be a way to get your data and only look through at most 1/1000 of the records. If you wish you could instead index (col1 col2 col3 col4 col5 col6) (col3 col4 col5 col6 col1 col2) and (col5 col6 col1 col2 col3 col4) to limit your search to 1/10000 of the data. This uses half again as much memory but is 10 times faster. (Warning I will not guarantee that MySQL will successfully figure out which index it should use. I'd hope that other databases would get it right but haven't tested it.) If you wished to not use a database you could use balanced binary trees exactly as I was suggesting using indexes above. For any given search pick the tree that has the missing element as deep as possible. Do a range search. Filter the returned data for just the rows of interest. This is in fact exactly what a good database should do above with those indexes.  Decision with tries seems to be the best one: with number of string occurrence on leaves you can easily design function that will return all possible strings with one missing character in O(log n) time and then you just iterate over this small number of strings searching for the max number of occurrences. If you use chars from A to Z there will be at most 26 such strings so iterating will not take a lot. AFAIK Lucene uses such mechanism internally for its wildcards search so you can concatenate your chars index them with KeywordAnalyzer (to omit stemming) and then search as ""ACB?DJ"". The only restriction here is that Lucene cannot handle searches with first ""?"" but you can bypass it by adding one extra char at the beginning (just trick to bypass Lucene checks) or by having one more index for reversed words (will increase performance for words with wildcard as a first char a lot). And finally if you first have to calculate number of occurrences anyway you can use some machine learning schemes such as decision trees to handle all the work. There were cases when decision trees were used to compress database and speed up search so you can do the same. Use lines as instances position of chars as attributes and chars themselves as attribute values. Then run some algorithm like C4.5 (you can use Weka's implementation called J48) with minimal pruning and run classification - the algorithm will do the rest! You can now start queries with wildcards. See [QueryParser.setAllowLeadingWildcard](http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/queryParser/QueryParser.html#setAllowLeadingWildcard(boolean))  Based on the comment that there will only be 1 unknown you can do the following: But your data in a hashtable. When you need to look up a pattern generate all the wildcard combinations since your vocabulary is limited this would mean at most looking up 20 patterns. This sounds like a hack but if you consider the performance implications of other methods it is hard to beat. The hash table lookup is O(1) 20 lookups is O(1) too. This method is not advisable if the number of wildcards could increase although it may still perform well for 2 or 3. A double array trie would also work and may reduce the amount of space to store your strings but the performance would suffer. Hash table isn't always O(1): with a large data set a number of collisions will occur resulting in a longer lookup. Also note that computing hash also takes some time. On the other hand tries are extremely fast for string lookup (that's why they are used so often in string manipulation intensive applications like Lucene): for a string of 6 chars without wildcards they will involve strictly 6 char comparisons and for a string with 1 wildcard - at most (when the wildcard is at first position) (6-1) * N comparisons where N is the number of possible values for chars. @ffriend the problem with tries is that taking the non-wildcard example for simplicity each of the 6 char comparisons has to be done against chars that are distributed (or scattered if you prefer) in memory; it is highly improbable that they all reside in a single cache line. This is the reason why a hash implementation *for this specific problem* would most likely outperform a trie. Lucene has to support a far greater variety of query types and the design decisions made there were appropriate. though I believe actual performance of both methods will depend on many factors (such as cache size hash table size speed of memory access CPU performance etc.) you words really make sense so +1 for answer. This seems to be the best option so far. If you do not update the table frequently you could also keep some additional statistics for example perhaps some symbols cannot appear in some positions. This might allow you to squeeze out more performance.  The db would be easy solution but another solution is a tree where each node chooses one character and leaf would contain array of possible results and counts. Then it would only take 5 steps in the tree to match one string. But creating the tree would take N*C time where N is number of items and C is number of characters in each item. Wildcards is just a node in the tree that will simultaniously remove one character from input but keeps the possible results intact.  In order to uniquely characterize a new sequence two pieces of information are necessary: the sequence (string) of five known symbols and the position of the unknown symbol. If your alphabet has 10 symbols then there can be no more than 10^5 = 100000 unique five-symbol strings. Depending on your memory resources this may be small enough to fit into a hashtable whose entries provide a lookup structure to find the best (position symbol) combination. For example: --------- | BCBCF | --> { 0: <heap of symbols partially ordered by frequency> ... } --------- This should allow for a fairly efficient lookup for a new sequence: concatenate the known symbols look up the sequence in the hashtable find the position of the unknown character and then return the symbol that is at the top of the corresponding heap. If you can guarantee the lookup structure will be stable (no new input) before any lookup is performed then you can squeeze a bit more efficiency out of it by replacing each of the position-indexed heaps with the single symbol that would have been at the top of the heap. (The heap structure is only necessary during the lookup phase if new information will come in that may change the symbol frequencies.) This is possibly workable but you would need to have 6 such hash tables. One for each missing column. @btilly: Not so because while ""ABC?DE"" and ""A?BCDE"" will hash to the same entry the sub-structure of that entry provides a way to get the best symbol for the position of the unknown. No need for separate tables. By doing this won't I be creating 6 keys to index a single 6-length values in my data? E.g. ABCDEF has to be mapped via keys: ABCDE ABCDF ABCEF ABDEF ACDEF BCDEF. This will greatly increase indexing time and memory. Also look at updates that modify the problem slightly."
950,A,"indexing services for websites: lucene and other options Just looking for some search and indexing services for our sites and wondered if you guys could recommend anything? our requirements: The service can either index via http or direct access to our database. It's gotta be just really simple to use and set up provide a simple API so we can get the results programmatically and do what we want with it ideally free or very cheap so far we've looked at Yahoo Boss and Lucene. Any pros cons or opinions for those? Lucene's looking good. We're a .NET house so LINQ to Luncene looks cool and of course the .NET port Lucene.NET. But by all means it can be any technology. Cool thanks for your help! I would recommend Solr. Lucene technology but server-side. +1. Solr is fantastic. Queries can be invoked over HTTP and data is returned in XML or JSON form. Recommend it highly. cool thanks guys. I'm not totally clear on what you mean by ""but server-side""? cool cheers Patrick means that you only need to install Solr on one server machine and then you can use it over HTTP without any Lucene code running on the client. Therefore if your client understands JSON and can make HTTP calls this is enough to create an application. You do not need to handle the nitty-gritty of Lucene and believe me most people do not WANT to handle the nitty-gritty of Lucene... cool thanks Yuval. Performance is also a very big issue for us...trafficking large amounts of data over http is a concern...what do you guys think?"
951,A,"Is there a open-search solution for python? lucene-like would be preferred. thanks How about Sphinx? http://www.sphinxsearch.com/ It has Python bindings included. I don't have comparision with other solutions like Lucene but I'm using Sphinx for CRM and it works very well indexing emails notes etc.  Why you need lucene-like when you can use lucene (PyLucene) :) http://lucene.apache.org/pylucene/ It is great and builds against the latest build of lucene quote from site: PyLucene is a Python extension for accessing Java Lucene. Its goal is to allow you to use Lucene's text indexing and searching capabilities from Python. It is API compatible with the latest version of Java Lucene version 2.9.0 as of October 13th 2009. PyLucene is not a Lucene port but a Python wrapper around Java Lucene. PyLucene embeds a Java VM with Lucene into a Python process. The PyLucene Python extension a Python module called lucene is machine-generated by JCC. PyLucene is built with JCC a C++ code generator that makes it possible to call into Java classes from Python via Java's Native Invocation Interface (JNI). Sources for JCC are included with the PyLucene sources. thanks for the reply. ""PyLucene is not a Lucene port but a Python wrapper around Java Lucene. PyLucene embeds a Java VM with Lucene into a Python process."" May there be some performance problems? I' ll try it. I have used it and there aren't performance problems but that depends on what you expect we were able to index 1000 docs per sec easily  See SolPython and solrpy What is solrpy? solrpy is a python client for solr an enterprise search server built on top of lucene. solrpy allows you to add documents to a solr instance and then to perform queries and gather search results from solr using your favorite programming language--python.  You can also check ElasticSearch it has native JSON interface so integrating with it in python should be simpler. Seems like Simon Willison thinks it got potential... I looked at solr and pylucene. ElasticSearch looks like the only option I have to work with lucene and python with an easy setup and without embellishments. Unluckily whoosh is not an option for me as I need some features which are currently supported only by lucene.  Xapian is an excellent Lucene-alternative with fairly good Python-bindings which is also easier to install than pylucene.  How about python bindings for Lucene?"
952,A,"Specific performance differences of certain features in Java and C# I am currently doing some performance tests on Hibernate / Hibernate.Search with Lucene in Java and the pendent in C#. Currently I am using the newest releases of both versions. I had some interesting results in the tests but what i am now doing is to find some performance tests or better some explanation of possible performance differences in both languages. For example: What I know is that generics in C# are more efficient because they don't use boxing at runtime because they are type-safe. Furthermore two different Lists with referenced types can share the JIT-Code in C#. So in C# there is now boxing-overhead. (knowledge came from this article) This is an example I searched for. But I would like to know if there are more differences in those two language that could cause performance differences. But I need some references as well since these test will be summarised in a scientific paper. I have found many papers / books about differences but not much about performance differences in specific shared features of those two languages. But important to say is that I am not just searching for some tests that are made but also deeper explanations why one languages is faster than the other in that feature. Thanks for any hints and help! So C# has richer runtime info without more runtime cost. Free lunch! Any comparison of performance of languages is just a speculation by definition. Performance of Java and C# application depends on a lot of features. First of all it depends on VMs which run the applications. So you are comparing two different virtual machines for different languages. But there is still difficulties just in comparing Java virtual machines. So the right question is ""what differences between those VMs and implementation of some common libraries"". And my answer is ""A LOT"". thx for the answere but since i am comparing not just the language but more the framework lucene / hibernate in both languages. Comparing performances of languages is a speculation but not comparing a framework which exists in two different languages. It is more about: find out which existing framework version is better. C#'s Lucene.NET or Java's Lucene. And so I need to have a closer look at the languages or the VM's to find out possible bottlenecks. And then have a look if these bottlenecks are used in these framworks!  C# language contains many more abstractions and that theoretically can make the compiler/plataform understand more what the programmer wants and then apply more optimizations like the one you mentioned about statically type-safe generics. On the other hand platform maturity is very important. JVM is usually much faster than .NET runtime because garbage collection and optimization are much more sophisticated and evolved in Java than in .NET as of today. Even in the absense of good language abstractions a platform can identify program patterns and optimize accordingly. But because they are similar in nature and use similar architectures when deciding between C# or Java performance is a very small issue. Normally when one have performance problems he/she can select other algorithms perform tunning or simply use better hardware. If you want to write a paper about language performance I suggest you include C++ and talk about code optimization to better use hardware resources (such as special instructions) because these is what produce top performance today. Libraries can have assembly-crafted routines to squeeze CPU for some mass data operations. Interpreted languages can make more load-specific and hardware-specific optimizations but that said the remaining optimizations are pretty much the ones of static code analysis. http://en.wikipedia.org/wiki/Program_optimization +1: Where you need the speed or features of C++ you can use these from C# and Java narrowing the difference further. Ok I know that I could include another language like C++. But the main issue in my paper is not just comparing the performance of Java and C# but to compare Lucene in Java and C#. Since this framework use there own algorithms I want to find some points to look at why those to language versions for Lucene could have performance differences. I don't want just to do it by looking into the framework code but also to have a look on the architecture of those languages. So using another language is not an option for me. But your answere is pretty good already!"
953,A,"Solr: fieldNorm different per document with no document boost I want my search results to order by score which they are doing but the score is being calculated improperly. This is to say not necessarily improperly but differently than expected and I'm not sure why. My goal is to remove whatever is changing the score. If I perform a search that matches on two objects (where ObjectA is expected to have a higher score than ObjectB) ObjectB is being returned first. Let's say for this example that my query is a single term: ""apples"". ObjectA's title: ""apples are apples"" (2/3 terms) ObjectA's description: ""There were apples in the apples-apples and now the apples went all apples all over the apples!"" (6/18 terms) ObjectB's title: ""apples are great"" (1/3 terms) ObjectB's description: ""There were apples in the apples-room and now the apples went all bad all over the apples!"" (4/18 terms) The title field has no boost (or rather a boost of 1) and the description field has a boost of 0.8. I have not specified a document boost through solrconfig.xml or through the query that I'm passing through. If there is another way to specify a document boost there is the chance that I'm missing one. After analyzing the explain printout it looks like ObjectA is properly calculating a higher score than ObjectB just like I want except for one difference: ObjectB's title fieldNorm is always higher than ObjectA's. Here follows the explain printout. Just so you know: the title field is mditem5_tns and the description field is mditem7_tns: ObjectB: 1.3327172 = (MATCH) sum of: 1.0352166 = (MATCH) max plus 0.1 times others of: 0.9766194 = (MATCH) weight(mditem5_tns:appl in 0) product of: 0.53929156 = queryWeight(mditem5_tns:appl) product of: 1.8109303 = idf(docFreq=3 maxDocs=9) 0.2977981 = queryNorm 1.8109303 = (MATCH) fieldWeight(mditem5_tns:appl in 0) product of: 1.0 = tf(termFreq(mditem5_tns:appl)=1) 1.8109303 = idf(docFreq=3 maxDocs=9) 1.0 = fieldNorm(field=mditem5_tns doc=0) 0.58597165 = (MATCH) weight(mditem7_tns:appl^0.8 in 0) product of: 0.43143326 = queryWeight(mditem7_tns:appl^0.8) product of: 0.8 = boost 1.8109303 = idf(docFreq=3 maxDocs=9) 0.2977981 = queryNorm 1.3581977 = (MATCH) fieldWeight(mditem7_tns:appl in 0) product of: 2.0 = tf(termFreq(mditem7_tns:appl)=4) 1.8109303 = idf(docFreq=3 maxDocs=9) 0.375 = fieldNorm(field=mditem7_tns doc=0) 0.2975006 = (MATCH) FunctionQuery(1000.0/(1.0*float(top(rord(lastmodified)))+1000.0)) product of: 0.999001 = 1000.0/(1.0*float(1)+1000.0) 1.0 = boost 0.2977981 = queryNorm ObjectA: 1.2324848 = (MATCH) sum of: 0.93498427 = (MATCH) max plus 0.1 times others of: 0.8632177 = (MATCH) weight(mditem5_tns:appl in 0) product of: 0.53929156 = queryWeight(mditem5_tns:appl) product of: 1.8109303 = idf(docFreq=3 maxDocs=9) 0.2977981 = queryNorm 1.6006513 = (MATCH) fieldWeight(mditem5_tns:appl in 0) product of: 1.4142135 = tf(termFreq(mditem5_tns:appl)=2) 1.8109303 = idf(docFreq=3 maxDocs=9) 0.625 = fieldNorm(field=mditem5_tns doc=0) 0.7176658 = (MATCH) weight(mditem7_tns:appl^0.8 in 0) product of: 0.43143326 = queryWeight(mditem7_tns:appl^0.8) product of: 0.8 = boost 1.8109303 = idf(docFreq=3 maxDocs=9) 0.2977981 = queryNorm 1.6634457 = (MATCH) fieldWeight(mditem7_tns:appl in 0) product of: 2.4494898 = tf(termFreq(mditem7_tns:appl)=6) 1.8109303 = idf(docFreq=3 maxDocs=9) 0.375 = fieldNorm(field=mditem7_tns doc=0) 0.2975006 = (MATCH) FunctionQuery(1000.0/(1.0*float(top(rord(lastmodified)))+1000.0)) product of: 0.999001 = 1000.0/(1.0*float(1)+1000.0) 1.0 = boost 0.2977981 = queryNorm The problem is caused by the stemmer. It expands ""apples are apples"" to ""apples appl are apples appl"" thus making the field longer. As document B only contains 1 term that is being expanded by the stemmer the field stays shorter then document A. This results in different fieldNorms. Could you elaborate or possibly provide a link? Why would the ""stemmer"" be expanding my field to something that it *isn't*? That seems counter-intuitive! :) Unless the first ""appl"" you wrote was supposed to be ""apple""? Having just looked into stemming that would make sense if ""apples"" is being broken down into its root form. So - let me know if I have this right - you're saying that if I change all references to ""apple"" and search for ""apple"" only I should get the results in the order I want? I edited my post so it should be clearer now. The stemmer uses ""appl"" as root form for ""apple"" and ""apples"". So if you disable stemming you should get the result you expect. You can also exclude terms from being stemmed by adding them to protwords.txt and change the schema.xml  FieldNOrm is computed of 3 components - index-time boost on the field index-time boost on the document and field length. Assuming that you are not supplying any index-time boost the difference must be field length. Thus since lengthNorm is higher for shorter field values for B to have a higher fieldNorm value for the title it must have smaller number of tokens in the title than A. See the following pages for a detailed explanation of Lucene scoring: http://lucene.apache.org/java/2_4_0/scoring.html http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/Similarity.html +1 for lots of insight - thanks! Unfortunately however you'll notice in my post that I stated what the fields (and their lengths) are. Both objects have titles with 3 tokens and descriptions with 18 tokens. ObjectA's title has 2/3 tokens matching ObjectB has 1/3 matching and the matching descriptions are respectively 6/18 and 4/18. So if I understand what you're saying the lengthNorm should not be having any effect. May I ask - how would I go about setting index-time boosts? Sorry - I thought your example was made up and not the actual values. In that case you are right in that field length shouldn't be a factor. You can set boosts in Solr in a variety of ways - If you are using SolrJ I believe there is a ""setBoost"" method on the SolrInputDocument. But if Doc B was getting a boost the fieldNorm should be higher in the description field as well. You also might want to check out Luke - it allows you to reconstruct your indexed field data so you can see what really gets indexed. Nope not made up - just testing data. :) I'll take a look at the code and see if anything suspicious is happening with index-time boosts. I'll probably also check out Luke. Thanks for the help."
954,A,Force Solr to read from an updated index I have a lucene index that i build and update using raw lucene indexers. I was wondering if there is a way to force solr to re-read the index without restarting the solr instance. Ive tried the update?commit=true but it doesnt seem to matter. The only way i can be sure solr -re-reads the index is by a total restart which of course is not ideal in a production environment. If you are using a multi-core setup you can just reload that single core. AFAIK while the core is being reloaded the requests to that core are queued. and if im not?? I don't know if you can send a reload when you are not using cores. But there is nothing wrong with using a multi-core setup with only one core. It does not add any significant overhead.
955,A,Using stop words with WhitespaceAnalyzer Lucene's StandardAnalyzer removes dots from string/acronyms when indexing it. I want Lucene to retain dots and hence I'm using WhitespaceAnalyzer class. I can give my list of stop words to StandardAnalyzer...but how do i give it to WhitespaceAnalyzer? Thanks for reading. Create your own analyzer by extending WhiteSpaceAnalyzer and override tokenStream method as follows. public TokenStream tokenStream(String fieldName Reader reader) { TokenStream result = super.tokenStream(fieldName reader); result = new StopFilter(result stopSet); return result; } Here the stopSet is the Set of stop words which you could get by adding a constructor to your analyzer which accepts a list of stop words. You may also wish to override reusableTokenStream() method in similar fashion if you plan to reuse the TokenStream. could you please have a loot at my answer and comment: http://stackoverflow.com/questions/899542/problem-using-same-instance-of-indexsearcher-for-multiple-requests/1014501#1014501 @Shashikant Kore - Any inputs for question - http://stackoverflow.com/questions/14554850/solrj-query-get-the-most-relevant-record-first
956,A,"Sitecore Multisite Lucene Search Relevance I currently have a multisite setup of Sitecore 6.4. I had installed the LuceneSearch module on the sites but have been requested to display results for the search across all sites. I have altered the 'Root' in the index definition on the web.config to point to the root containing all sites and this successfully returns results across all sites. However these results need to be weighted to display the current context site's results above the others. Is there any easy way to achieve this? I have been toying with the idea of creating a separate index for each site and then looping through all the sites returning the results from the current site's index first but I suspect there must be a better idea. you can index the path of each item and add it as lucene field then make a prefix query with the start path of each site. NOTE: when you index the path replace (""/"" or space ) with something else like '#' and do the same when you build the query. for advance use of lucene search look to : http://sitecoreblog.alexshyba.com/2010/11/sitecore-searcher-and-advanced-database.html  As far as I know the boost values are set when you create or re-build an index so you won't be able to set a boost value based on a context site. Just a thought but using the AdvancedDatabaseCrawler SharedSource module you could add a new dynamic field called 'site' and store the site id or site name for each item you are indexing. You could then search the index twice. Once to get all the results for the current context site and then a second search where you get all the results where the site field does not equal the context site."
957,A,"Lucene Standard Analyzer vs Snowball Just getting started with Lucene.Net. I indexed 100000 rows using standard analyzer ran some test queries and noticed plural queries don't return results if the original term was singular. I understand snowball analyzer adds stemming support which sounds nice. However I'm wondering if there are any drawbacks to gong with snowball over standard? Am I losing anything by going with it? Are there any other analyzers out there to consider? If you use the snowball analyzer you should get results for singular/plural because snowball will normalize them into the same form. Are you sure that you use the same analyzer for creating an index and querying it? Yes by using a stemmer such as Snowball you are losing information about the original form of your text. Sometimes this will be useful sometimes not. For example Snowball will stem ""organization"" into ""organ"" so a search for ""organization"" will return results with ""organ"" without any scoring penalty. Whether or not this is appropriate to you depends on your content and on the type of queries you are supporting (for example are the searches very basic or are users very sophisticated and using your search to accurately filter down the results). You may also want to look into less aggressive stemmers such as KStem. I just figured out you can also do a fuzzy search like this ""kangaroos~"" that will return singular versions of the word as well although it seems to take a bit longer to process the query. @alchemical: I would really recommend against doing that. ~ is a very slow operator and if your user does stuff like search for a phrase you're kinda screwed. Why is it so bad if you ""kangaroos"" is stored as ""kangaroo""? OK that's good to know -- to use KStem do you need Solr? Do you need to work with Lucene source code to integrate it in? Know this is a bit old but do you know if the normal analyser does stemming at all or is it only stop-words? Wasn't able to figure it out :(  I just finished an analyzer that performs lemmatization. That's similar to stemming except that it uses context to determine a word's type (noun verb etc.) and uses that information to derive the stem. It also keeps the original form of the word in the index. Maybe my library can be of use to you. It requires Lucene Java though and I'm not aware of any C#/.NET lemmatizers.  The snowball analyzer will increase your recall because it is much more aggressive than standard analyzer. So you need to evaluate your search results to see if for your data you need to increase recall or precision."
958,A,"Lucene - How to index a value with special characters I have a value I am trying to index that looks like this: Test (Test) Using a StandardAnalyzer I attempted to add it to my document using: Field.Store.YES Field.Index.TOKENIZED When I do a search with the value of 'Test (Test)' my QueryParser generates the following tags: +Name:test +Name:test This operates as I expect because I am not escaping special characters. However if I do QueryParser.Escape('Test (Test)') while indexing my value it creates the terms: [test] and [test] Then when I do a search like such:  QueryParser.Escape('Test (Test)') I get the same two terms (as I expect). The problem is if I have two documents indexed with the names: Test Test (Test) It matches on both. If I specify a search value of 'Test (Test)' then I want to just get the second document. I am curious as to why escaping the special characters does not preserve them in the created terms. Is there an alternate Analyzer I should look at? I looked at WhitespaceAnalyzer and KeywordAnalyzer. WhitespanceAnalyzer is case sensitive and KeywordAnalyzer stores it as a single term of: [Test (Test)] Which means that if I do a search for just 'Test' I will not be able to return both documents. Any ideas on how to implement this? It doesn't seem like it should be that difficult. If you search for 'Test (Test)' and you want to retrieve documents that contains that exact expression you must enclose the search expression between ""..."" so that Lucene knows that you want to do a phrase search. See the Lucene documentation for details: http://lucene.apache.org/java/3_0_1/queryparsersyntax.html#Terms"
959,A,"Does a pom.xml.template tell me everything I need to know to use the project as a dependency I'm trying to add the lucene sandbox contribution called term-highlighter to my pom.xml. I'm not really that familiar with Maven but the code has a pom.xml.template which seems to imply if I add a dependency that looks like: <dependency> <groupId>org.apache.lucene</groupId> <artifactId>lucene-highlighter</artifactId> </dependency> It might work. Can someone help me out in adding a lucene-community project to my pom.xml file? Thanks for the comments it turns out that adding the version was all I needed and I just guessed it should match the lucene-core version I was using.: <dependency> <groupId>org.apache.lucene</groupId> <artifactId>lucene-highlighter</artifactId> <version>2.3.1</version> </dependency> You have it right but you probably want to add the version as well: From The Maven 5 minute tutorial <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""> <modelVersion>4.0.0</modelVersion> <groupId>com.mycompany.app</groupId> <artifactId>my-app</artifactId> <packaging>jar</packaging> <version>1.0-SNAPSHOT</version> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> </project>  You have to add the version number but you only have to do it once in a project structure. That is if the version number is defined in a parent pom you don't have to give the version number again. (But you don't even have to provide the dependency in this case since the dependency will be inherited anyways.)"
960,A,"Grails searchable plugin In my Grails app I'm using the Searchable plugin for searching/indexing. I want to write a Compass/Lucene query that involves multiple domain classes. Within that query when I want to refer to the id of a class I can't simply use 'id' because all classes have an 'id' property. Currently I work around this problem by adding the following property to a class Foo public Long getFooId() { return id } static transients = ['fooId'] Then when I want to refer to the id of Foo within a query I use 'fooId'. Is there a way I can provide an alias for a property in the searchable mapping rather than adding a property to the class? You can give a more specific name to your id property. See this page for how to do this.  I finally discovered that this is the way to do it: static searchable = { id: name 'fooId' }  Thanks! This would work really well that is if I could get past the OOM errors the app server seems to throw each time Searchable plugin is installed. These are bubbling up via : org.compass.gps.CompassGpsException: Failed to index execution exception; nested exception is java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space Has anyone configured their Searchable.groovy in grails-app/conf to perform a strict file:/// or mmap:// only configuration using no heap and indexing say once or twice per day? Search is beyond a nice to have - but the cost of using the database mirroring in Grails (with Oracle 10g) seems memory intensive. Really small amount of domains to search (4) small database maybe 1-2gb for this application. Turns out this was just since there were multiple Grails apps deployed in same Tomcat instance -- just increased PermSize and set the index to bulkIndexOnStartup = ""fork"" and worked straightaway."
961,A,"Lucene .NET IndexWriter DeleteDocuments Not Working Here is the code: Try Dim util As New IndexerUtil() Dim dir As Lucene.Net.Store.Directory = FSDirectory.Open(New DirectoryInfo(util.getIndexDir())) Dim indexWriter As New IndexWriter(dir New SimpleAnalyzer() indexWriter.MaxFieldLength.UNLIMITED) Dim numDocs As Integer = indexWriter.NumDocs() indexWriter.DeleteDocuments(New Term(""id"" insightId)) indexWriter.Optimize() indexWriter.Commit() indexWriter.Close() numDocs = indexWriter.NumDocs() Catch ex As Exception LOG.Error(""Could not remove insight "" + insightId + "" from index"" ex) End Try numDocs = 85 both times I also have a little gui app I wrote which reads the index and prints the docs out in a nice format. The doc with the id field that equals insightId definitely exists and STILL exists after the ""deletion"". Here is how the id field is being created doc.Add(New Field(""id"" insightID Field.Store.YES Field.Index.ANALYZED)) //insightID is an integer How are you creating the id field when you build the index? Could you post the code of that? Also does the code throw any exceptions? As you have probably discovered with your more recent post your ID column is not being indexed correctly because SimpleAnalyzer uses LetterTokenizer which only returns letters. Consider using the KeywordAnalyzer instead for the id field. I thought so but I wanted to be sure. Thanks :) That fixed it thanks! If the id field is an integer and it's probably only used for keeping track of documents wouldn't it be better to store it as `NOT_ANALYZED` field instead? This way he wouldn't have to worry about analyzers at all. @George you're probably right. The OP will still need an appropriate analyzer if he'll be parsing a query that contains `id:1234` but that's usually in a much different area of code.  You should create a new IndexWriter instead of counting documents on a closed one. Yeah I realized that after posting this. However that still does not change the fact the document(s) are NOT being deleted.  Since SimpleAnalyzer converts input text to lowercase you will have lowercased terms in the index. Are you sure that ""insightId"" is also lowercased? The insightId is an integer value (I convert it to a string) so there is no upper/lower case."
962,A,"Lucene Query WITHOUT Operators I am trying to use Lucene to search for names in a database. However some of the names contain words like ""NOT"" and ""OR"" and even ""-"" minus symbols. I still want the different tokens inside the names to be broken up using an Analyzer and searched upon as a boolean combination of terms but I do not want Lucene to interpret any of the ""NOT""/""OR"" terms as operators (instead I want them to be searched upon like normal terms). One way to accomplish what I am talking about would be to manually run the Analyzer on the search query and then manually construct a boolean query based on all the resulting tokens. Is this the best way? I get the impression that analyzer's were designed to be used in conjunction with the query parser and I feel like there should be a built-in way to accomplish what I am trying to do. Anyone know the best way to do this? Your own suggested approach of constructing a BooleanQuery from a TokenStream makes complete sense. The QueryParser API is really just intended for parsing structured queries using a specific syntax - if you are not leveraging the query parser syntax I see no reason to use QueryParser over a manually constructed BooleanQuery. However if you are using a StandardAnalyzer (or another analyzer with a StopFilter) to index your fields words like ""AND"" ""NOT"" and ""OR"" will not be indexed and cannot be searched on. So in that case you could just as easily strip those words and operators like ""-"" and ""+"" from your queries using a regular expression. I would sooner recommend the BooleanQuery approach however."
963,A,"Is there a pure Python Lucene? The ruby folks have Ferret. Someone know of any similar initiative for Python? We're using PyLucene at current but I'd like to investigate moving to pure Python searching. Probably not an answer to the question but Elasticsearch implements a simple web interface on top of Lucene and PyES is a python wrapper over Elasticsearch. I have used pyES comfortably but some advanced features present in Lucene are still missing from Elasticsearch. For accessing Lucene indices I found (and am trying out) `plush`: https://pypi.python.org/pypi/plush/0.3.0 By the way the old Ferret URL redirects now to http://www.chandanweb.com/solutions/web-applications.html - I've replaced the URL with the new github page https://github.com/dbalmain/ferret :) any reason for going for pure python? For some applications pure Python is overrated. Take a look at Xapian. Thanks for the Xapian mention. Not what I need right now but I'll sure keep it in mind for later.  Whoosh is a new project which is similar to lucene but is pure python. Just used whoosh for a project and it really was easy to use. No messing around at all - just worked.  After weeks of searching for this I found a nice Python solution: repoze.catalog. It's not strictly Python-only because it uses ZODB for storage but it seems a better dependency to me than something like SOLR.  lupy was a lucene port to pure python.The lupy people suggest that you use PyLucene. Sorry. Maybe you can use the Java sources in combination with Jython. It's interesting that Ferret seems to be very appreciated and used while Lupy was abandoned. Well PyLucene seems to cater to a similar community. Also some people are even ready to do their full-text searches in Java because of Lucene ;-) Sphinx have good python community with Pythonic API.  +1 to the Xapian and Pyndexter answers. Ferret is actually written in C with Ruby bindings on top. A pure Ruby search engine would be even slower than a pure Python one. I would love to see ""someone else"" write a Cython/Pyrex layer for Python interface to Ferret but won't do it myself because why bother when there are Python bindings for Xapian. Thanks. I used the term ""pure"" in a dirty way. =) If I can install it with easy_setup of the like I'm happy.  I recently found pyndexter. It provides abstract interface to various different backend full-text search engines/indexers. And it ships with a default pure-python implementation. These things can be disastrously slow though in Python. I came here looking for something to access Lucene indices in python I'm not too concerned about speed at this point. I just don't want to be tied to Java. So thanks for the pynter.  For non-pure Python Sphinx Search with Python API works the fastest. From the benchmarks from multiple blogs Sphinx Search is way faster than Lucene uses way less memory and it is in C. I am developing a multi-document search engine based on it using python and web2py as framework.  The only one pure-python (not involving even C extension) search solution I know of is Nucular. It's slow (much slower than PyLucene) and unstable yet. We moved from PyLucene-based home baked search and indexing to Solr but YMMV."
964,A,Get starting with Zend Lucene I decided to use Zend Lucene to search for keywords into my .pdf and .doc files but really need a push the documentation available from the official site is higher a bit from a big newbie like me. May I find a volunteer that give me first steps to get started just three or four first steps will be highly appreciated! For information: I'm using standard PHP and Netbeans 6.8 as an IDE zero experience with Zend Framework. Accept my Regards! Dany90. I'd be interested to hear how this works out for you... the last time I used Zend_Lucene... searching was very slow. Which part of the Zend Documentation is complicated for you? Where does it hang you could need a push? You can include just the parts of the framework you need. You wont need to use the entire stack. The best place to get started is here: http://framework.zend.com/manual/en/zend.search.lucene.html But need to download the whole framework? The minimal download package will be fine. It will include the entire framework but you only need to include the parts you want to use in your code. Thanks datasage I suppose that it's integration with Netbeans is simple!
965,A,"Inconsistent Apache Solr query results I'm new to Apache Solr and trying to make a query using search terms against a field called ""normalizedContents"" and of type ""text"". All of the search terms must exist in the field. Problem is I'm getting inconsistent results. For example the solr index has only one document with normalizedContents field with value = ""EDOUARD SERGE WILFRID EDOS0004 UNE MENTION COMPLEMENTAIRE"" I tried these queries in solr's web interface: normalizedContents:(edouard AND une) returns the result normalizedContents:(edouar* AND une) returns the result normalizedContents:(EDOUAR* AND une) doesn't return anything normalizedContents:(edouar AND une) doesn't return anything normalizedContents:(edouar* AND un) returns the result (although there's no ""un"" word) normalizedContents:(edouar* AND uned) returns the result (although there's no ""uned"" word) Here's the declaration of normalizedContents in schema.xml: <field name=""normalizedContents"" type=""text"" indexed=""true"" stored=""true"" multiValued=""false""/> So wildcards and AND operator do not follow the expected behavior. What am I doing wrong ? Thanks. do you have any token filter applied to normalizedContents? third line can be solved with LowerCase filter last two with a WS filter (whitespace). By default the field type text does stemming on the content (solr.SnowballPorterFilterFactory). Thus 'un' and 'uned' match une. Then you might not have the solr.LowerCaseFilterFactory filter on both query and index analyzer therefore EDUAR* does not match. And the 4th doesnt match as edouard is not stemmed to edouar. If you want exact matches you should copy the data in another field that has a type with a more limited set of filters. E.g. only a solr.WhitespaceTokenizerFactory Posting the <fieldType name=""text""> section from your schema might be helpful to understand everything."
966,A,Solr Filter for Logging/Analysing Queries I'm using Solr and wish to be able to log queries/tokens to a database for analytics for both marketing and suggested search functionality. I'm guessing that a Lucene filter would be a good way of doing this maybe putting a filter in the query filter chain for a given field type which logs to a specified database. I'm wondering the best way to do this to minimize the impact on Solr response times. If anybody has done this in the past and is willing to share their solution that would be fantastic. Perhaps the easiest way is to just analyze the access log and consume that data into a report. This could be done offline and would have no affect on response times. Access log does not provide any if on what's been found. This log is produced in HTTP layer. Technically the OP asked to log just the queries not the results so using the access log should get you the info you need but you need to parse it.
967,A,What should go in my Lucene document? I use Lucene.net to index content and documents etc.. on our CMS. This has worked well so far but now I've got to take account of the following additions to web pages: Publish date Expiry date Page 'is active' User authorisation So the search results should only show pages that are within the Publish / Expiry window are 'active' and that the current user is authorised to view. Should I include the above information in the Lucene index? It will make the queries a little more complicated but the hits collection will only return 'valid' documents which will make paging the results a lot easier. On the other hand I'll be repeating information that is already in the CMS database so I'll be risking the integrity of my data and I'll have update the index whenever anything in the above list is changed as well as the actual content itself. Anyone else had this problem? How did you solve it? Thanks. Edit: I may need to use a 'FieldCache' (mentioned here) to pass the 'valid' doc ids into the lucene search? ..so the search results should only show pages that are within the Publish / Expiry window are 'active' and that the current user is authorised to view. There are a few ways to handle the authorization issue. You could maintain multiple indexes (one per permission level) filter the results with the query (by storing permission required) or filter the results before you display them. If there are only a few levels I think that I would maintain separate indexes - it seems safest. As for 'is active' - can you just rebuild your index with that in mind? Just rebuild your index in the background every so often and only add active content. You may have too much info to make that feasible - but Lucene is VERY fast. My preference would be to use a win service to periodically rebuild the index (lucene docs < 10000) but app requirements dictate that changes made to content / pages etc.. are reflected ASAP in the search results Yeah. Stupid requirements anyway :) Unless you get a chance to update your index periodically - it looks like you are stuck filtering your results after you get them out of the index.  Query the CMS database first and build a BitSet with all the matching documents (you'll need a FieldCache to translate between your app's doc ID's with Lucene's internal doc ID's). Then you can run your Lucene query on your index using a Filter (wrapping the BitSet). You keep all mutable data in your database (where it belongs) and you don't have to worry about updating or rebuilding your index. This will run very fast as well. P.S. I've only used the Java version of Lucene but this should work fine in Lucene.NET
968,A,"How to sort by Lucene.Net field and ignore common stop words such as 'a' and 'the'? I've found how to sort query results by a given field in a Lucene.Net index instead of by score; all it takes is a field that is indexed but not tokenized. However what I haven't been able to figure out is how to sort that field while ignoring stop words such as ""a"" and ""the"" so that the following book titles for example would sort in ascending order like so: The Cat in the Hat Horton Hears a Who Is such a thing possible and if yes how? I'm using Lucene.Net 2.3.1.2. When you create your index create a field that only contains the words you wish to sort on then when retrieving sort on that field but display the full title. Well that's the trick isn't it? You can't sort by a tokenized field and its the tokenizing that analyzes the field for stop words and punctuation as I understand it. So how to strip those stop words but keep the field un-tokenized? In your code strip out the stop words. You'll have to maintain your own list.  For search I found search lucene .net index with sort option link interesting to solve ur problem  It's been a while since I used Lucene but my guess would be to add an extra field for sorting and storing the value in there with the stop words already stripped. You can probably use the same analyzers to generate this value.  I wrap the results returned by Lucene into my own collection of custom objects. Then I can populate it with extra info/context information (and use things like the highlighter class to pull out a snippet of the matches) plus add paging. If you took a similar route you could create a ""result"" class/object add something like a SortBy property and grab whatever field you wanted to sort by strip out any stop words then save it in this property. Now just sort the collection based on that property instead. I think that's how it would have to be done yes. I do create a collection of custom objects with the Lucene results so it shouldn't be too hard. Thanks.  There seems to be a catch-22 in that you must tokenize a field with an analyzer in order to strip punctuation and stop words but you can't sort on tokenized fields. How then to strip the stop words without tokenizing? Don't rely on Lucene to strip them do it yourself."
969,A,"What is the correct way to rebuild Lucene's index I have a forum like web application written in Asp.net MVC. I'm trying to implement Lucene.net as the search engine. When I build my index every now and then I get exceptions related to Lucene not being able to rename the deletable file. I think it's because I empty the index every time I want to rebuild it. Here is the code that deals with indexing: public class SearchService : ISearchService { Directory IndexFileLocation; IndexWriter Writer; IndexReader Reader; Analyzer Analyzer; public SearchService(String indexLocation) { IndexFileLocation = FSDirectory.GetDirectory(indexLocation System.IO.Directory.Exists(indexLocation) == false); Reader = IndexReader.Open(IndexFileLocation); Writer = new IndexWriter(IndexFileLocation Analyzer IndexFileLocation.List().Length == 0); Analyzer = new StandardAnalyzer(); } public void ClearIndex() { var DocumentCount = Writer.DocCount(); if (DocumentCount == 0) return; for (int i = 0; i < DocumentCount; i++) Reader.DeleteDocument(i); } public void AddToSearchIndex(ISearchableData Data) { Document Doc = new Document(); foreach (var Entry in Data) { Field field = new Field(Entry.Key Entry.Value Lucene.Net.Documents.Field.Store.NO Lucene.Net.Documents.Field.Index.TOKENIZED Lucene.Net.Documents.Field.TermVector.WITH_POSITIONS_OFFSETS); Doc.Add(field); } Field KeyField = new Field( SearchField.Key.ToString() Data.Key Lucene.Net.Documents.Field.Store.YES Lucene.Net.Documents.Field.Index.NO); Doc.Add(KeyField); Writer.AddDocument(Doc); } public void Dispose() { Writer.Optimize(); Writer.Close(); Reader.Close(); } } And here is the code that executes it all:  private void btnRebuildIndex_Click(object sender EventArgs e) { using (var SearchService = new SearchService(Application.StartupPath + @""\indexs\"")) { SearchService.ClearIndex(); } using (var SearchService = new SearchService(Application.StartupPath + @""\indexs\"")) { Int32 BatchSize = 50; Int32 Current = 0; var TotalQuestions = SubmissionService.GetQuestionsCount(); while (Current < TotalQuestions) { var Questions = SubmissionService.ListQuestions(Current BatchSize ""Id"" Qsparx.SortOrder.Asc); foreach (var Question in Questions) { SearchService.AddToSearchIndex(Question.ToSearchableData()); } Current += BatchSize; } } } Why does Lucene complain about renaming the ""deletable"" file? Not sure why you are recreating the index everytime. You can append to the index thus: Writer = new IndexWriter(IndexFileLocation Analyzerfalse); The false flag at the end tells the IndexWriter to open in append mode(i.e. not overwrite). That might make your problem go away. IndexFileLocation.List().Length == 0 will evaluate to true only when no index files exist  It turned out if no index files exist then creating an IndexReader before an IndexWriter is not a good idea. I also realized even though the AddDocument method of IndexWriter has two overloads (one w/ and one w/o Analyzer parameter) only the one with analyzer parameter works for me."
970,A,Finding the start and end of a match with Lucene I would like to find the start and end positions of a match from a lucene (Version 3.0.2 for Java) query. It seems like I should be able to get this info from Highlighter or FastVectorHighligher but these classes seem only return a text fragment with the relevant text highlighted. Is there any way to get this info either with a Highlighter or from the ScoreDoc itself? Update: I found this related question: http://stackoverflow.com/questions/1311199/finding-the-position-of-search-hits-from-lucene But I think the answer by Allasso won't work for me because my queries are phrases not individual terms. If I were you I'd just take code from FastVectorHighlighter. Relevant code is in FieldTermStack:  List<string> termSet = fieldQuery.getTermSet(fieldName); VectorHighlightMapper tfv = new VectorHighlightMapper(termSet); reader.GetTermFreqVector(docId fieldName tfv); // <-- look at this line string[] terms = tfv.GetTerms(); foreach (String term in terms) { if (!termSet.Contains(term)) continue; int index = tfv.IndexOf(term); TermVectorOffsetInfo[] tvois = tfv.GetOffsets(index); if (tvois == null) return; // just return to make null snippets int[] poss = tfv.GetTermPositions(index); if (poss == null) return; // just return to make null snippets for (int i = 0; i < tvois.Length; i++) termList.AddLast(new TermInfo(term tvois[i].GetStartOffset() tvois[i].GetEndOffset() poss[i])); The major thing there is reader.GetTermFreqVector(). Like I said FastVectorHighlighter already does some legwork that I would just copy but if you want that GetTermPositions call should do everything you need. I should have specified that I'm using Lucene Java 3.0.2. Still I will look at the code for FastVectorHighlighter is see if I can get what I need from there. @Mike: Sorry I figured c# syntax was close enough to java. In any case the TermPositionsVector should do what you want. Since you want to highlight phrases it will be a bit tougher (you'll need to find ones which are right next to each other) but it shouldn't be too bad.
971,A,"Need help regarding Lucene index/query I want to have a ""citystate"" field in Lucene index which will store various city state values like: Chicago IL Boston MA San Diego CA How do i store these values(shud it be tokenized or non-tokenized?) in Lucene and how do I generate a query (should it be phrasequery or termquery or something else?) which gets me all records whose citystate contain: Chicago IL OR Boston MA OR San Diego CA ?? I would appreciate if i can get help with the code as well. Thanks. Shouldnt city state be normalized further into two separate fields ?  It depends. Will you ever want to search by city alone or by state alone? In this case you need to tokenize. If not do not tokenize. Check out the KeywordAnalyzer though - it may suit you. As to your second question. Suppose you call the field 'citystate'. You can then use a query such as: citystate:Chicago IL OR citystate:BostonMA OR citystate:San Diego CA. The programmatic version is a BooleanQuery composed out of several TermQueryes."
972,A,How can I set up a Lucene index in Sitecore that handles security correctly? I have a number of different roles in Sitecore. And I have set security permissions on my content items so that different roles can only access certain content items. It seems that Lucene will just index all of the content. And when I query Lucene it doesn't pay any attention to the security. Is there any way to get Lucene to only return items that the current Extranet user has access to? Thanks Corey Not to my knowledge. But when working through the Hits collection you would normally have a loop similar to this: for ( int i = 0; i < hits.Length() && i < Context.Current.Settings.MaxSearchResultsToProcess; i++ ) { Item item = Index.GetItem( hits.Doc( i ) Context.Current.Database ); if ( item != null ) { indexResultater.Add( item ); } } And since this runs in context of your current user no results would be added to your results if the user cannot access them. Actually in my code I do it a little differently. Instead of iterating through thousands of results I only iterate through the current page of items. So if my search returned 1475 hits and I was on page 2 of the result set then I would only pull hits 11-20. However I suppose I could try it the way that you suggested. It just means that I would have to always iterate through all 1475 hits and then get just the ones I have access to. Then after that I could just return items 11-20. Hopefully it won't be too slow. In reality it isn't usually too bad. A couple of things you can consider is not returning more than YYY results anyway from a search. 1475 would be what I consider too many I usually never ask Lucene to process more than around 200-300 hits for any given query. Sitecore's Item cache will help you (a lot) for subsequent page requests and finally you do have the option of caching the result for page browsing.
973,A,Running Lire image search inside Solr -- how? I'd like to use Lire image search from within Solr. Lire is built on top of Lucene: http://www.semanticmetadata.net/lire/ What's the best way to integrate Lire in Solr? I'd especially appreciate links to any success stories or sample code in this case. Newly available as of late 2013: LireSolr a Solr plugin package for Lire by the Lire authors. A live demo is available: http://demo-itec.uni-klu.ac.at/liredemo/ … images can be searched with several Lire classifiers e.g. Auto Color Correlogram PHOG et al. I’ve personally mirrored it on GitHub: https://github.com/fish2000/liresolr  I do not know of any succesful implementations of this library within Solr but since both Solr and the library are build on top of Lucene you might be able to build a small plugin to implement this functionality. Solr wiki: Plugins An example plugin Next to these two links I would advise you to download the Solr source code and take a look at the official request handlers. I used the above links and the source code to write a plugin of my own. Will start down this road -- thanks for providing a starting point.
974,A,asking about index verification tools for lucene I would like to ask about lucene index. I mean I created a simple program that created lucene indexes and stored it in a folder. also I had use a diagnostic tools name Luke to be able to lurk inside lucene index and find out its content. and I know that lucene is a standard framework when it come to building a search engine. but I just wanted to be sure that lucene indexes every term that existed in a file. I mean is there a way for me or some tools out there to verify that the index contains in lucene indexes is dependable? and not a single term went missing there? I know that this is subjective question but I just wanted to hear your two cents. thanks though. :-) tl;dr: how can we know that the index in lucene is correct? You could always build a small program that will perform the same analysis you use when indexing your content. Then for all the terms query your index to make sure that the document is among the results. Repeat for all the content. But personally I would not waste time on this. If you can open your index in Luke and if you can make a couple of queries everything is most probably fine. Often the real question is whether or not the analysis you did on the content will be appropriate for the queries that will be made against your index. You have to make sure that your index will have a good balance between recall and precision. thanks though.I think what you're saying is right. :-)
975,A,"Lucene/Solr Searching problem? I have a problem that i want to search in the specific locations in the indexed text let we have a lucene document that contains text as <Cover> This document contains following items 1. Business overview. 2. Risk Factors. 3. Management </Cover> <BusinessOverview> our business is xyz </BusinessOverview> <RiskFactors> we have xyz risk factors </RiskFactors> <Management> we have xyz type of management </Mangement> now in above code html tags(could be any thing) divide main document in sections now i want to have a functionality if user give some text to search and does not mention any specific section the text should be searched in whole document but user if user specify some section along with text to search the search should be done only in that particular section. I want to know is this type of search is possible with solr/lucene. Regards Ahsan Your schema should reflect that need ; the data sent to the indexer would have then to match this schema properly. Once done you'll be able to query against scpcific fields. You could also use an xml importer. schema wise i have to index section separately right? but my requirement is i have to group the results on the basis of main document.  I have never worked with solr but lucene itself has very flexible query language see this link. So answer is yes it is possible. can u please give a sample query for that  You can use the <copyField> option to have a ""field of fields"" se here: http://wiki.apache.org/solr/FAQ#How_do_I_use_copyField_with_wildcards.3F http://www.ibm.com/developerworks/java/library/j-solr1/"
976,A,"Solr WordDelimiterFilter + Lucene Highlighter I am trying to get the Highlighter class from Lucene to work properly with tokens coming from Solr's WordDelimiterFilter. It works 90% of the time but if the matching text contains a '' such as ""1500"" the output is incorrect: Expected: 'test 1500 this' Observed: 'test 11500 this' I am not currently sure whether it is Highlighter messing up the recombination or WordDelimiterFilter messing up the tokenization but something is unhappy. Here are the relevant dependencies from my pom: org.apache.lucene lucene-core 2.9.3 jar compile org.apache.lucene lucene-highlighter 2.9.3 jar compile org.apache.solr solr-core 1.4.0 jar compile And here is a simple JUnit test class demonstrating the problem: package test.lucene; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.io.IOException; import java.io.Reader; import java.util.HashMap; import org.apache.lucene.analysis.Analyzer; import org.apache.lucene.analysis.TokenStream; import org.apache.lucene.queryParser.ParseException; import org.apache.lucene.queryParser.QueryParser; import org.apache.lucene.search.Query; import org.apache.lucene.search.highlight.Highlighter; import org.apache.lucene.search.highlight.InvalidTokenOffsetsException; import org.apache.lucene.search.highlight.QueryScorer; import org.apache.lucene.search.highlight.SimpleFragmenter; import org.apache.lucene.search.highlight.SimpleHTMLFormatter; import org.apache.lucene.util.Version; import org.apache.solr.analysis.StandardTokenizerFactory; import org.apache.solr.analysis.WordDelimiterFilterFactory; import org.junit.Test; public class HighlighterTester { private static final String PRE_TAG = ""<b>""; private static final String POST_TAG = ""</b>""; private static String[] highlightField( Query query String fieldName String text ) throws IOException InvalidTokenOffsetsException { SimpleHTMLFormatter formatter = new SimpleHTMLFormatter( PRE_TAG POST_TAG ); Highlighter highlighter = new Highlighter( formatter new QueryScorer( query fieldName ) ); highlighter.setTextFragmenter( new SimpleFragmenter( Integer.MAX_VALUE ) ); return highlighter.getBestFragments( getAnalyzer() fieldName text 10 ); } private static Analyzer getAnalyzer() { return new Analyzer() { @Override public TokenStream tokenStream( String fieldName Reader reader ) { // Start with a StandardTokenizer TokenStream stream = new StandardTokenizerFactory().create( reader ); // Chain on a WordDelimiterFilter WordDelimiterFilterFactory wordDelimiterFilterFactory = new WordDelimiterFilterFactory(); HashMap<String String> arguments = new HashMap<String String>(); arguments.put( ""generateWordParts"" ""1"" ); arguments.put( ""generateNumberParts"" ""1"" ); arguments.put( ""catenateWords"" ""1"" ); arguments.put( ""catenateNumbers"" ""1"" ); arguments.put( ""catenateAll"" ""0"" ); wordDelimiterFilterFactory.init( arguments ); return wordDelimiterFilterFactory.create( stream ); } }; } @Test public void TestHighlighter() throws ParseException IOException InvalidTokenOffsetsException { String fieldName = ""text""; String text = ""test 1500 this""; String queryString = ""1500""; String expected = ""test "" + PRE_TAG + ""1500"" + POST_TAG + "" this""; QueryParser parser = new QueryParser( Version.LUCENE_29 fieldName getAnalyzer() ); Query q = parser.parse( queryString ); String[] observed = highlightField( q fieldName text ); for ( int i = 0; i < observed.length; i++ ) { System.out.println( ""\t"" + i + "": '"" + observed[i] + ""'"" ); } if ( observed.length > 0 ) { System.out.println( ""Expected: '"" + expected + ""'\n"" + ""Observed: '"" + observed[0] + ""'"" ); assertEquals( expected observed[0] ); } else { assertTrue( ""No matches found"" false ); } } } Anyone have any ideas or suggestions? After further investigation this appears to be a bug in the Lucene Highlighter code. As you can see here: public class TokenGroup { ... protected boolean isDistinct() { return offsetAtt.startOffset() >= endOffset; } ... The code attempts to determine if a group of tokens is distinct by checking to see if the start offset is greater than the previous end offset. The problem with this approach is illustrated by this issue. If you were to step through the tokens you would see that they are as follows: 0-4: 'test' 'test' 5-6: '1' '1' 7-10: '500' '500' 5-10: '1500' '1500' 11-15: 'this' 'this' From this you can see that the third token starts after the end of the second but the fourth starts the same place as the second. The intended outcome would be to group tokens 2 3 and 4 but per this implementation token 3 is seen as separate from 2 so 2 shows up by itself then 3 and 4 get grouped leaving this outcome: Expected: 'test <b>1500</b> this' Observed: 'test 1<b>1500</b> this' I'm not sure this can be accomplished without 2 passes one to get all the indexes and a second to combine them. Also I'm not sure what the implications would be outside of this specific case. Does anyone have any ideas here? EDIT Here is the final source code I came up with. It will group things correctly. It also appears to be MUCH simpler than the Lucene Highlighter implementation but admittedly does not handle different levels of scoring as my application only needs a yes/no as to whether a fragment of text gets highlighted. Its also worth noting that I am using their QueryScorer to score the text fragments which does have the weakness of being Term oriented rather than Phrase oriented which means the search string ""grammatical or spelling"" would end up with highlighting that looks something like this ""grammatical or spelling"" as the or would most likely get dropped by your analyzer. Anyway here is my source: public TextFragments<E> getTextFragments( TokenStream tokenStream String text Scorer scorer ) throws IOException InvalidTokenOffsetsException { OffsetAttribute offsetAtt = (OffsetAttribute) tokenStream.addAttribute( OffsetAttribute.class ); TermAttribute termAtt = (TermAttribute) tokenStream.addAttribute( TermAttribute.class ); TokenStream newStream = scorer.init( tokenStream ); if ( newStream != null ) { tokenStream = newStream; } TokenGroups tgs = new TokenGroups(); scorer.startFragment( null ); while ( tokenStream.incrementToken() ) { tgs.add( offsetAtt.startOffset() offsetAtt.endOffset() scorer.getTokenScore() ); if ( log.isTraceEnabled() ) { log.trace( new StringBuilder() .append( scorer.getTokenScore() ) .append( "" "" ) .append( offsetAtt.startOffset() ) .append( ""-"" ) .append( offsetAtt.endOffset() ) .append( "": '"" ) .append( termAtt.term() ) .append( ""' '"" ) .append( text.substring( offsetAtt.startOffset() offsetAtt.endOffset() ) ) .append( ""'"" ) .toString() ); } } return tgs.fragment( text ); } private class TokenGroup { private int startIndex; private int endIndex; private float score; public TokenGroup( int startIndex int endIndex float score ) { this.startIndex = startIndex; this.endIndex = endIndex; this.score = score; } } private class TokenGroups implements Iterable<TokenGroup> { private List<TokenGroup> tgs; public TokenGroups() { tgs = new ArrayList<TokenGroup>(); } public void add( int startIndex int endIndex float score ) { add( new TokenGroup( startIndex endIndex score ) ); } public void add( TokenGroup tg ) { for ( int i = tgs.size() - 1; i >= 0; i-- ) { if ( tg.startIndex < tgs.get( i ).endIndex ) { tg = merge( tg tgs.remove( i ) ); } else { break; } } tgs.add( tg ); } private TokenGroup merge( TokenGroup tg1 TokenGroup tg2 ) { return new TokenGroup( Math.min( tg1.startIndex tg2.startIndex ) Math.max( tg1.endIndex tg2.endIndex ) Math.max( tg1.score tg2.score ) ); } private TextFragments<E> fragment( String text ) { TextFragments<E> fragments = new TextFragments<E>(); int lastEndIndex = 0; for ( TokenGroup tg : this ) { if ( tg.startIndex > lastEndIndex ) { fragments.add( text.substring( lastEndIndex tg.startIndex ) textModeNormal ); } fragments.add( text.substring( tg.startIndex tg.endIndex ) tg.score > 0 ? textModeHighlighted : textModeNormal ); lastEndIndex = tg.endIndex; } if ( lastEndIndex < ( text.length() - 1 ) ) { fragments.add( text.substring( lastEndIndex ) textModeNormal ); } return fragments; } @Override public Iterator<TokenGroup> iterator() { return tgs.iterator(); } } Have you sent a patch to the mailing list? I just faced this bug too and wasn't sure what it was. Would be awesome to get it fixed. Sorry Dan no I haven't. The reason is I cant figure out where. I cant even find the bugzilla for Lucene let alone Lucene Highlighter. Do you have the address of the mailing list? Please post here and when I see it next I will see if I can submit this suggestion. https://issues.apache.org/jira/browse/Lucene and https://issues.apache.org/jira/browse/SOLR — looks like this was addressed three weeks ago: https://issues.apache.org/jira/browse/LUCENE-2874  Here's a possible cause. Your highlighter needs to use the same Analyzer used for search. IIUC Your code uses a default analyzer for the highlighting even though it uses a specialized analyzer for parsing the query. I believe you need to change the Fragmenter to work with your specific TokenStream. The code is using the same analyzer in both cases. You can see above that it is actually created by the same helper method (getAnalyzer) both in the QueryParser constructor (for the query tokenizer) and by the highlighter.getBestFragments (for the text tokenizer). This works except in the case with the  as this question indicates. I do think I have found the problem and it is a bug in Lucene Highlighter. I will post the answer below."
977,A,"Question related to phrase search in lucene/solr? I have question is it possible to perform a phrase search with wild cards in solr/lucene as if i have two queries both have exactly same results. One is: +Contents:""change market"" and the other is: +Contents:""change* market"" I assumed the second should match ""changes market"" but it does not return any matches. You can do this in Lucene with ComplexPhraseQueryParser. Solr has facility to plug in custom query parser with QParserPlugin. You can possibly use these two to have desired functionality with Solr as well.  IMO it is not possible to search for wild cards with in phrase. You might want to consider using two queries with proximity search.(q=change* market&qs=1) http://wiki.apache.org/solr/SolrRelevancyFAQ#How_can_I_search_for_one_term_near_another_term_.28say.2C_.22batman.22_and_.22movie.22.29 I had tried wht you r saying but unfortunately it doesn't work as i guess it will get documents having change* or market in the contents. @Ahsan: it depends on default operator in schema you can set it to 'AND' to get the desired result."
978,A,"Which one is better for efficient free text search Hibernate Search or Lucene? We are developing a web application using Spring MVC Spring and Hibernate. We need to add efficient free text search capabilities to our applications. For this we are thinking of using either Hibernate Search (it uses Lucene under the hood) or directly lucene. What is the best option for us as we are already using hibernate in our application? What are the pros and cons of one over the other? Thanks. The other way of using Lucene is to get the middlman API which is known as SOLR. SOLR will connect to Lucene and perfom HTTP calls for search. Please note that you will need to build and Parse the XML what Solr consumes. All the functionality of Lucene is exponse via SOLR and should be really helpful.  You said it yourself - you'll be using Lucene one way or the other. The raw Lucene API isn't very easy to use. It's much more low-level than Hibernate Search. if you're already using Hibernate then it's a no-brainer - use Hibernate Search to implement your text search functionality.  disclaimer: I'm one of the developers of Hibernate Search. The goal of the project is not to compete with Lucene nor Solr but to facilitate as much as possible integration with Hibernate applications to avoid having to maintain the two worlds in sync and duplicate all mapping and CRUD operations. While we provide some common helpers and a nice encapsulation Hibernate Search can also hand you over a direct reference to the Lucene API so in case you find yourself needing to use the ""raw"" Lucene API you will never be stuck. Also for writing to the index Hibernate Search provides a common pattern which will solve most of known requirements but in case you have very non-standard requirements you can get full control of the written Documents. Solr is a good alternative but as it is a separate server you have to interact with it via REST APIs which is quite different with it's pros and cons. Having a second service to manage is not always wanted and of course the remote invocations will never be as efficient as direct references to Lucene and to all it's internal filters and caches. Not all functionality of Lucene can be exposed via a remote API and if you need to do some ""low level"" operation if this is not implemented in Solr you won't be able to do it (without patching Solr). Still Solr is very cute especially when you want to share the index with other non-Java applications and so we might add a Solr backend for Hibernate Search to eventually keep a Solr server in synch (especially if there's interest for it and possibly some help). Finally the Lucene API is really hard core stuff. We spend a lot of effort to make the best use of it to provide top performance while exposing a stable API to people using Hibernate Search basically until now all releases have been backwards compatible to provide a ""drop-in"" performance boost to use latest greatest tricks from Lucene - which actually changes API quite often; these changes are always exciting but be prepared to maintain that in your application if you don't use a proper abstraction."
979,A,"Is it possible to have SOLR MoreLikeThis use different fields for model and matches? Let's say I have documents with two fields A and B. I'd like to use SOLR's MoreLikeThis but with a twist: I'm most interested in boosting documents whose A field is like my model document's B field. (That is extract MLT's 'interesting terms' from the model B field but only collect MLT results based on the A field.) I don't see a way to use the mlt.fl fields or mlt.qf boosts to achieve this effect in a single query. (It seems mlt.fl specifies fields used for both discovery of 'interesting terms' and matching to those terms.) Am I missing some option? Or will I have to extract the 'interesting terms' myself and swap the 'field:term' details? (Other ideas in this same vein appreciated as well.) Two options I see are: Use a copyField - index your original document with a copy of field A named B and then query using B. Extend MoreLikeThisHandler and change the fields you query. The first option costs a bit of programming (mostly configuration changes) and some memory consumption. The second involves more programming but no memory footprint increase. Hope one of them suits your needs. Not sure how (1) making an extra copy of the fields helps: the fields already exist. I just need the interesting-terms-extraction to come from one but then be compared against the other. Yes (2) would be the ultimate fallback... though I think the desired effect can be accomplished without code changes just with a properly-formulated series of queries. See a following self-answer for details.  I now think there are two ways to achieve the desired effect (without customizing the MLT source code). First option: Do an initial MLT query with the MLT handler adding the parameter &mlt.interestingTerms=details. This includes the list of terms that were deemed interesting ranked with their relative boosts. The usual behavior uses those discovered terms against the same mlt.fl fields to find similar documents. For example the response will include something like: ""interestingTerms"": [""field_b:foo""5.0""field_b:bar""2.9085307""field_b:baz""1.67070794] (Since the only thing about this initial query that's interesting is the interestingTerms throwing in an fq that rules out all docs could help it skip unnecessary scoring work.) Explicitly re-composing that interestingTerms info into a new OR query field_a:foo^5.0 field_a:bar^2.9085307 field_a:baz^1.67070794 amounts to using the B field example text to find documents that are similar in field A and may be mimicking exactly the kind of query default MLT does on its usual model field. Second option: Grab the model document's actual field B text and feed it directly as a ContentStream body to be used in lieu of a query for specifying the model document. Then target mlt.fl at field A for the sake of collecting similar results. For example a fragment of the parameters might be …&stream.body=foo bar baz&mlt.fl=field_a&…. Again the net effect being that model text originally from field_b is finding documents similar only in field_a."
980,A,"How do I delete a Lucene index without affecting other non-index files in the directory? I am wanting to write back an in-memory Lucene index to disk overtop of the originally-loaded index. Currently if I call Directory.Copy( _ramDirectory _fileSystemDirectory false ) it simply adds the new files to the directory but leaves the old (stale) ones there. I tried calling: new IndexWriter( _fsd _analyzer true IndexWriter.MaxFieldLength.UNLIMITED ).Close(); ...(to create a new empty index in the directory) but this has strange behavior and sometimes results in the entire index being wiped clean on the next run of the program. Is there any way I can simply get a list of the files a file system index is currently using so I can delete them manually? I don't want to blindly erase all files in the directory in case there are some non-index files there. Apparently FSDirectory.ListAll() lists all files in the physical directory whether or not they are actually part of the index. Is there any way I can tell if a particular file is used/created by the index? I mean I can't even check file extensions due to Lucene's bizarre file naming conventions. I'd definitely recommend that you dont mix other files in a Lucene index folder. The best solution would be to create a new index using the IndexWriter constructor that has the create parameter which will create a new index at the location. Then you use the IndexWriter.AddIndexesNoOptimize(Directory[] dirs) method to add your RamDirectory to the FSDirectory Even if you use CFS I think you'll still have segments.gen segments* and *.del (at least). you are right i edited my answer as per your comment I totally agree but in my application the user can customize the location of the index thus they could (accidentally) set it to some existing important folder--I really don't want an update to the index to completely wipe that folder clean. Marked as answer because it is actually the best answer to the question as stated though Xodarap's point may actually make the question irrelevant (in my case at least).  If you're using Lucene 2.9 or greater all IndexWriters use a behind-the-scenes RAM directory which will probably be faster than you making your own RAM directory and then attempting to manually flush to disk. See the FAQ about NRT. If you really want to use your own RAM directory open the existing (non-RAM) index and then do IndexWriter.DeleteAll() and optimize. Hmm interesting! Didn't realize that. That might actually make things a lot easier--I was planning to check the size of the index and use a file system directory if the index was above a certain size--does this ""behind the scenes"" IndexWriter do that for me? @chaiguy: You can see `IndexWriter.SetMaxBufferedDocs` and `IndexWriter.SetRAMBufferSizeMB`."
981,A,"Lucene: getting the full collection documents as results When I perform a query in Lucene (topDocs = searcher.search(booleanQuery 220000);) I get 170 hits as retrieved doc. Which is correct but I would like to have the full list of docs in the results even if the scores are very low. Is there a way to force lucene to get the full list of documents of all my collection and not just the relevant ones ? Or maybe it means that all other docs score is 0 ? thanks Since Lucene 3.x you can use TotalHitCountCollector to retrieve the total hits of a query. Then you can retrieve all documents for your query with the total hit count. Be careful with the case without any hits. TotalHitCountCollector collector = new TotalHitCountCollector(); searcher.search(booleanQuery collector); topDocs = searcher.search(booleanQuery Math.max(1 collector.getTotalHits()));  It should work if you search for '*' and allow leading * in wildcard queries. Just did a test in Luke on a 501 document index which returns 501 results for this query. I actually don't want to search for * I still have my query but I want to get all documents for results even if the scores are very low. It you can tell me that the not retrieved document have all score exactly =0 then the issue is solved. I can add them by myself on the bottom. Update: The query would better be booleanQuery + "" OR *"". This way documents matching the original query would get a higher score than the others but the others get a score > 0 as well and thus are returned. Example: If i look for `*beer*` I get one document with score 1. If I look for `*beer* or *` I get all documents with the top doc having score 1.4142 and the others (that would have had score 0 in the previous query) have score 03536. I've added the following lines but I still get 172 results only. rest = new TermQuery(new Term(""*""""*"")); booleanQuery.add(rest BooleanClause.Occur.SHOULD); I've also tried new Term(""title""""*""); where title is an existing field but it is the same. I'm not sure about TermQuery Syntax since we're build the query string manually and then use the QueryParser e.g. QueryParser.parse(""content: *beer* OR content: *""); You might need to call setAllowLeadingWildcard(true) on the query parser to allow that. Did you add the other query terms using BooleanClause.Occur.MUST? If so this might prevent the other documents from being returned.  You can add some field to all docs like test:1 and then search like [your_query] OR test:1.  Lucene does not do any filtering based on score. If a query has 170 hits then it means that only 170 documents matched the query. The rest of the documents did not match and can be presumed to have a score of 0. Yup I am sure this is how Lucene or any other full-text search engine works. There are multiple ways of getting the remaining docs at the bottom of the list - Thomas has explained one way of getting it done in another answer to this question. Alternately you can fire a second query which is the negative of the original query to get all the documents which did not match the original query. I see are you sure 100% about your statement ? :) I mean it would be perfect problem solved. Uhm well I still need to include them in the final list.. is there a way to get all remaining collection docs and add them on the bottom of the list ?"
982,A,"Katta luke integration I m using Katta for distributed Lucene Index. Is it possible to use LUKE for Katta index if so how? Thanks in advance... possible duplicate of [User Interface for Katta Index](http://stackoverflow.com/questions/4929634/user-interface-for-katta-index) Surendhar I already answered a very similar question you asked. I believe you need not ask this again. Also it is considered common courtesy to accept answers which are useful. but i didn't get any IDEA ""http://katta.sourceforge.net/documentation/common-problems"" they mentioned only for Lucene Index not for Katta Index Fair enough. They basically say that a Katta index is a folder containing Lucene indexes. So what you need to do is: Install Luke. Point it at the sub-folders of the Katta index. See what these indexes' structure is. ya got it thanx dude... :)"
983,A,"postprocess solr's faceted search result I'm not sure how to handle the following issues. So i hope to get here some ideas or something like that. I'm using lucene with solr. Every document (which is indexed in lucene) has an date-field an an topic - field (with some keywords) By using faceted search i'm able to calculate the frequency of every keyword at an specific date. Example 1 (pseudo code): 1st search where date=today: web=>70 apple=>35 blue=>32 2nd search where date=yesterday: web=>65 blue=>55 apple=>5 But now i would like to combine the results into one solr/lucene query in order to calculate which word-frequency grows very strong and witch doesn't. An result could be: Example 2: one search merging both querys from example 1 web=>(7065) <- growth +769% blue=>(3255) <- growth -4181% apple=>(345) <- growth +680% Is it possible (and useful) to do this consolidation (and calclulation) inside solr or is it better to start 2 solr querys (see example 1) an postprocess the results with PHP? Than you! If you have the facet values a priori you could do this with facet queries i.e. something like facet.query=category:web AND date:[2011-06-14T00:00:00Z TO 2011-06-14T23:59:59Z]&facet.query=category:web AND date:[2011-06-13T00:00:00Z TO 2011-06-13T23:59:59Z]&... so you would do the cartesian product of facet values * dates. Otherwise to do this inside Solr I think you'd have to write some custom Java faceting code. Or do it client-side with multiple queries as you mentioned. Thank you for that answer. But that does not exactly fit my needs. Because the ""term"" (for example) ""web"" is an result of the firs query (the result of example 1) . I can consolidate the results of the querys from example 1 by using facet.date.start facet.date.end and facet.date.gap. But how to postprocess? If this is only possible be writing own java facetting code - so postprocessing in PHP would be the better way for me. Thank you. I don't think you can use facet.date.start for this you need a cartesian product of facet queries. @The Bndr: facet.query docs: http://wiki.apache.org/solr/SimpleFacetParameters#facet.query_:_Arbitrary_Query_Faceting . fq is filter query it's nothing like a facet query in fact it's not for faceting at all. following your example i get strange results. It seams that solr doen't recognize ""facet.query"" keyword. Could this be possible? On the other hand: (fq=) works fq= and facet.query is the same right? Finally my solr installation seams to use the AND operator between the ""fq"". That mens it doesn't count the category on the first and 2nd day it looks like solar searches for documents which are at the 1st and 2nd day at the same time which results in ' numFound=""0""' Thank you! It looks like there is some ""basic knowledge"" missing on my side! ;-) Thank you for supporting me - greetings from Europe."
984,A,Can anyone suggest some good tutorials for Lucene? can anyone suggest me some good tutorials on Lucene. I was reading Lucene in Action but it seems to be a old edition of current lucene. Most of the methods are deprecated. Where to start? I am googling around a bit. Thanks Kapil The second edition of Lucene in Action is available in electronic format here  I found this website useful http://www.avajava.com/tutorials/lessons/how-do-i-perform-a-range-query.html  A quick start tutorial can be found at here.  Lucid imagination is a company offering support and consulting related to Lucene. See their Lucene tutorial. They also have a search engine indexing lots of Lucene related material.  It's true that there has been changes but they're not as substantial as it might seem. The most radical change that I can think of is that the api for the IndexSearcher.search() method has changed but it really isn't that difficult to adapt your code to the new usage. In general the old methods are still there but are marked as deprecated. This is a good thing because the reference manual specifies what you should use instead. Lucene in Action is a great book. Take a look at the second edition recommended by KenE if possible but otherwise I think you'll get a long way using the first edition the manual and some common sense. As I said the changes are not as daunting as you could imagine.
985,A,"Lucene: Fastest way to return the document occurance of a phrase? I am trying to use Lucene (actually PyLucene!) to find out how many documents contain my exact phrase. My code currently looks like this... but it runs rather slow. Does anyone know a faster way to return document counts? phraseList = [""some phrase 1"" ""some phrase 2""] #etc a list of phrases... countsearcher = IndexSearcher(SimpleFSDirectory(File(STORE_DIR)) True) analyzer = StandardAnalyzer(Version.LUCENE_CURRENT) for phrase in phraseList: query = QueryParser(Version.LUCENE_CURRENT ""contents"" analyzer).parse(""\"""" + phrase + ""\"""") scoreDocs = countsearcher.search(query 200).scoreDocs print ""count is: "" + str(len(scoreDocs)) Typically writing custom hit collector is the fastest way to count the number of hits using a bitset as illustrated in javadoc of Collector. Other method is to get TopDocs with number of results specified as one. TopDocs topDocs = searcher.search(query filter 1); topDocs.totalHits will give you the total number of results. I'm not sure if this is as fast as it involves calculating scores which is skipped in aforementioned method. These solutions are applicable for Java. You have to check equivalent technique in Python."
986,A,Lucene BooleanQuery How to use booleanQuery with StandardAnalyzer in Lucene Search? BooleanQuery. BooleanQuery is a container of Boolean clauses that are optional required or prohibited subqueries. You can normally add a clause to BooleanQuery making use of an API method that looks like: public void add(Query query boolean required boolean prohibited)  I presume you are referring to parsing boolean queries using the QueryParser object correct? The Lucene query syntax documentation should have everything you need. It Works. Thank You
987,A,Hibernate Search with index in a different database I have a database which is readonly (I only have the access to view) but I have to index this database for search. The DAO layer to this table is now using a generic DAO approach with Hibernate+JPA. Is it possible to add hibernate search to this view and store the index in a separate database? I am aware that I may lose the capability of post- indexing. But it is ok I will do full indexing manually. Configuration: Spring 2.5+Hibernate 3 (Or should I use compass or lucene directly?) Please Advise Thanks Roy Hibernate Search and Compass both use Lucene under the covers and Lucene can store its index data in various forms including in memory on disk or in a database. If you choose to store it in a database then there's no reason that needs to be the same database as the data you're indexing. However if there's no concrete need to keep the index in a database then local disk-based storage will be easier and probably faster.
988,A,Solr partial document index update I'm using Solr and Solr:Cell plugin to index and search rich text documents and metadata. DEFINITION: solr_document = tuple(rich_text_document metadata1 metadata2) I want to reindex some solr_documents when metadata changes but only the parts in the solr_document that chaged not the whole solr_documnt because parsing and extracting text from rich text documents is computing expensive and pointless since the rich text document was not modified. Does Solr support partial document index updates? Aditional: I'm using Solr via sunspot in a Rails application. One of the main sunspot developers says here that: Solr does not support the concept of partial updates -- under the hood updating a document actually consists of removing it from the index and then re-adding it. So Sunspot does have to construct the full document each time anything changes; it's an unfortunate limitation from a performance standpoint but it's pretty fundamental to the way Solr and Lucene work. Is there anything that can be done maybe Solr:Cell allows something? I can't answer for sure but Lucene doesn't provide this ability so I'm doubtful any extensions on top of it do. Does Solr support partial document index updates? Nope. Check out the FAQ. Is there anything that can be done? Yes IIRC there was an issue in the project JIRA about it. Look it up ask what's missing contribute to the effort of implementing it.
989,A,Lucene Indexing and searching with Map/Reduce Possible Duplicate: instant searching in petabyte of data… How to use HADOOP's Map/Reduce in Lucene Indexing and searching????? can you specify your question a little bit please? http://stackoverflow.com/questions/4791602/instant-searching-in-petabyte-of-data if you have alredy posted thy question _why_ do you repost it? The closest thing I could find for you is Katta: Katta is a distributed application running on many commodity hardware servers very similar to Hadoop MapReduce Hadoop DFS HBase Bigtable or Hypertable. (...) Katta supports distributed scoring for its lucene implementation - this is because we do not expect that term distribution is fully balanced over all shards. Each search query that is done in Katta ends up being two network roundtrips: first we get the document frequencies for a query from all the nodes and on the second trip pass this value and the search query to all nodes. Please note that we also provide a simple count method that just counts documents matching the query but does that within one network roundtrip.
990,A,"Lucene wildcard matching fails on chemical notations(?) Using Hibernate Search Annotations (mostly just @Field(index = Index.TOKENIZED)) I've indexed a number of fields related to a persisted class of mine called Compound. I've setup text search over all the indexed fields using the MultiFieldQueryParser which has so far worked fine. Among the fields indexed and searchable is a field called compoundName with sample values: 3-Hydroxyflavone 64'-Dihydroxyflavone When I search for either of these values in full the related Compound instances are returned. However problems occur when I use the partial name and introduce wildcards: searching for 3-Hydroxyflav* still gives the correct hit but searching for 64'-Dihydroxyflav* fails to find anything. Now as I'm quite new to Lucene / Hibernate-search I'm not quite sure where to look at this point.. I think it might have something to do with the ' present in the second query but I don't know how to proceed.. Should I look into Tokenizers / Analyzers / QueryParsers or something else entirely? Or can anyone tell me how I can get the second wildcard search to match preferably without breaking the MultiField-search behavior? I'm using Hibernate-Search 3.1.0.GA & Lucene-core 2.9.3. Some relevant code bits to illustrate my current approach: Relevant parts of the indexed Compound class: @Entity @Indexed @Data @EqualsAndHashCode(callSuper = false of = { ""inchikey"" }) public class Compound extends DomainObject { @NaturalId @NotEmpty @Length(max = 30) @Field(index = Index.TOKENIZED) private String inchikey; @ManyToOne @IndexedEmbedded private ChemicalClass chemicalClass; @Field(index = Index.TOKENIZED) private String commonName; ... } How I currently search over the indexed fields: String[] searchfields = Compound.getSearchfields(); MultiFieldQueryParser parser = new MultiFieldQueryParser(Version.LUCENE_29 searchfields new StandardAnalyzer(Version.LUCENE_29)); FullTextSession fullTextSession = Search.getFullTextSession(getSession()); FullTextQuery fullTextQuery = fullTextSession.createFullTextQuery(parser.parse(""searchterms"") Compound.class); List<Compound> hits = fullTextQuery.list(); I wrote my own analyzer: import java.util.Set; import java.util.regex.Pattern; import org.apache.lucene.index.memory.PatternAnalyzer; import org.apache.lucene.util.Version; public class ChemicalNameAnalyzer extends PatternAnalyzer { private static Version version = Version.LUCENE_29; private static Pattern pattern = compilePattern(); private static boolean toLowerCase = true; private static Set stopWords = null; public ChemicalNameAnalyzer(){ super(version pattern toLowerCase stopWords); } public static Pattern compilePattern() { StringBuilder sb = new StringBuilder(); sb.append(""(-{01}\\(-{01})"");//Matches an optional dash followed by an opening round bracket followed by an optional dash sb.append(""|"");//""OR"" (regex alternation) sb.append(""(-{01}\\)-{01})""); sb.append(""|"");//""OR"" (regex alternation) sb.append(""((?<=([a-zA-Z]{2}))-(?=([^a-zA-Z])))"");//Matches a dash (""-"") preceded by two or more letters and succeeded by a non-letter return Pattern.compile(sb.toString()); } }  Use WhitespaceAnalyzer instead of StandardAnalyzer. It will just split at whitespace and not at commas hyphens etc. (It will not lowercase them though so you will need to build your own chain of whitespace + lowercase assuming you want your search to be case-insensitive). If you need to do things differently for different fields you can use a PerFieldAnalyzer. You can't just set it to un-tokenized because that will interpret your entire body of text as one token. Hmm this sounds promissing but initial testing so far has not yet resolved the issue.. I might have to try with the chain of filters & PerFieldAnalyzer.. Will report back once I do.  I think your problem is a combination of analyzer and query language problems. It is hard to say what exactly causes the problem. To find this out I recommend you inspect you index using the Lucene index tool Luke. Since in your Hibernate Search configuration you are not using a custom analyzer the default - StandardAnalyzer - is used. This would be consistent with the fact that you use StandardAnalyzer in the constructor of MultiFieldQueryParser (always use the same analyzer for indexing and searching!). What I am not so sure of is how ""64'-Dihydroxyflavone"" gets tokenized by StandardAnalyzer. That the first thing you have to find out. For example the javadoc says: Splits words at hyphens unless there's a number in the token in which case the whole token is interpreted as a product number and is not split. It might be that you need to write your own analyzer which tokenizes your chemical names the way you need it for your use cases. Next the query parser. Make sure you understand the query syntax - Lucene query syntax. Some characters have special meaning for example a '-'. It could be that your query is parsed the wrong way. Either way first step os to find out how your chemical names get tokenized. Hope that helps. FYI I quickly checked how the StandardAnalyzer tokenizes your examples. ""3-Hydroxyflavone"" seems to fall under the product rule mentioned above. It becomes a single token ""3-hydroxyflavone"". ""64'-Dihydroxyflavone"" on the other hand becomes two tokens ""64"" and ""dihydroxyflavone"". It appears the StandardTokenizer splits words at apostrophes.. That at least pinpoints the problem but it will take me some time to fix this.. :) Thanks for the help! Wow thanks! I was just trying to use Luke here to test the same.. Gues this means I need to use an alternate Analyzer? (I've tried to set the field to UN_TOKENIZED but that even breaks the first search example..)"
991,A,"Tags and attributes in Lucene shared across documents My app needs to keep an index of files in which the files are known by tags and attributes suggesting a Lucene (Java) document schema like: tags: i s (indexed stored) attributes: i s content: i fileId: i s (The actual file is looked up by id in sqlite.) However while a file has only one set of tags/attributes it may have multiple versions of its content (each identified by a versionId). The only real solution it seems is one document type with one document for each version such that the tags and attributes are redundant across many documents: tags: i s attributes: i s content: i versionId: i s fileId: i s My concern about this schema is whether it will be performant enough and compact enough. So here are my questions: If I understand Lucene's indexing scheme correctly when the same long string is indexed as a field in many documents this doesn't really bulk out the index compared to if it were indexed just once. Correct? If I create a single Term object make it stored and then add it to many documents does the full string data get duplicated for each document in the index? If this is the case am I just best off putting the actual storage of the tags/attributes into sql? As far as I can tell the only info that comes back in query results is the documents themselves ordered by score. To determine which fields satisfied the query for a matched document must I do separate queries on the fields for each document or what? Understand that this is just a client-side app so concurrent access is a non-issue and index updates will be quite infrequent (every time the user retags or edits/creates a file). I'm mainly concerned about real-time response for a single user and to some extent about index size (though more for conserving memory rather than disk space). MORE BACKGROUND I considered some alternative document schema but rejected them. My initial instinct was to avoid data duplication by splitting documents into two types one type for representing a file: tags: i s attributes: i s fileId: i s ...but then one document type for representing the versions of files: content: i fileId: i s versionId: i s There are a number of problems with this: First it requires doing separate queries for content and tags/attributes and then matching content results to files: for each version document in my results I must look at its fileId to then look up the corresponding file document in a separate query. While this is a standard relational technique my understanding is that it's a rather awkward and slow thing to do in Lucene. Second for a query requiring both ""pizza"" and ""hot dog"" I want to get back the file versions that include both those terms in either the tags/attributes or content or ""hot dog"" in one and ""pizza"" in the other. By splitting the tags/attributes from their content this becomes very tricky (and likely expensive). So maybe I can just keep content and tags/attributes together by keeping multiple content fields: tags: i s attributes: i s content: i (multiple fields) fileId: i s The question is whether I can identify a content field so I can know which version content produced the hit. I could name each content field differently corresponding to the version id: tags: i s attributes: i s content {versionId}: i content {versionId}: i content {versionId}: i # etc. fileId: i s Even if I could identify the content field(s) that caused the document to match the query consolidating the versions messes up the scoring. If I understand Lucene's indexing scheme correctly when the same long string is indexed as a field in many documents this doesn't really bulk out the index compared to if it were indexed just once. Correct? If I create a single Term object make it stored and then add it to many documents does the full string data get duplicated for each document in the index? If this is the case am I just best off putting the actual storage of the tags/attributes into sql? As far as I can tell the only info that comes back in query results is the documents themselves ordered by score. To determine which fields satisfied the query for a matched document must I do separate queries on the fields for each document or what? Correct. Lucene stores a dictionary mapping strings to numerical identifiers so the memory consumed is only to store the identifier several times. I think you are safe storing the tags and attributes in Lucene. You do not need separate queries - once you hold a Document object you can use e.g. getField() to get the relevant field information. Since you are concerned about Lucene performance I suggest you read Scaling Lucene and Solr which covers lots of performance tips. Thanks yuval 3) I understand I can inspect the Document fields but I want to get the set of fields in the Document that matched the query. My impression is that you're supposed to use filters and scoring in the query rather than sorting that stuff out after the fact. This is a subtle issue. Lucene does not give this information as a default. I suggest you read: http://www.lucidimagination.com/Community/Hear-from-the-Experts/Articles/Debugging-Relevance-Issues-Search and go on to explore Lucene explanations and highlighting. HTH"
992,A,Extending / changing how Zend_Search_Lucene searches I am currently using Zend_Search_Lucene to index and search a number of documents currently at around a 1000 or so. What I would like to do is change how the engine scores hits on a document from the current default. Zend_Search_Lucene scores on the frequency of number of hits within a document so a document that has 10 matches of the word PHP will score higher than a document with only 3 matches of PHP. What I am trying to do is pass a number of key words and score depending on the hits of those keywords. e.g. I pass 5 key words sayPHP MySQL Javascript HTML and CSS that I search against the index. One document has 3 matches to those key words and one document has all 4 matches the 4 matches scores the highest. The number of instances of those words in the document do not concern me. Now I've had a quick look at Zend_Search_Lucene_Search_Similarity however I have to confess that I am not sure (or that bright) to know how to use this to achieve what I am after. Is what I want to do possible using Lucene or is there a better solution out there? For what I've understood in the Zend_Search_Lucene_Search_Similarity section of the manual I'd start by extending the default similarity class to override the tf (term frequency) method so that it doesn't alter the score: class MySimilarity extends Zend_Search_Lucene_Search_Similarity { public function tf($freq) { return 1.0; // overriding default sqrt($freq); } } This way the number of matches shouldn't be taken into account. Do you think this would be enough? Then set it to be the default similarity algorithm before indexing: Zend_Search_Lucene_Search_Similarity::setDefault(new MySimilarity()); This has improved the scoring of the documents somewhat but there is still a bit to go which I think can also be helped with a bit of boosting of key terms. Thanks again.
993,A,"Is it possible to make Lucene queries in alfresco that find nodes based on their parent/children properties is it possible to make Lucene query in alfresco that finds nodes based on their parent/children properties? For example i want to find all the nodes that have the property ""foo"" set to '1' and have nodes associated to them by a child association with the property ""baz"" set to '2' (maybe specifing somehow the name of their child association) something like @crl\:numeroAtto:""6555"" AND @crl\:firmatario:""Marco rossi"" Where ""numeroAtto"" is a property of the parent node and ""firmatario"" is a property of the child. The association type is ""firmatari"" (It's not in the query because i don't know how to use it) To be even clearer i'm trying to tell lucene: ""Find all nodes that have the property numeroAtto set to 6555 and that have children (association type with the children: firmatari) with the property ""firmatario"" set to Marco rossi. Thanx in advance (In the SQL world this is known as an INNER or OUTER JOIN.) There is no direct lucene way to do this. Another idea: the first would return all of the parent nodes and then build searches based on the root of each returned node.  You can't search on associations so what we do is not to build slow queries. But add a new d:text property of the association on the parent Type. So it's searchable through Lucune. To make it fully working create a Java Behaviour which checks on content update. And when 'your' association is found it adds it to the d:text property. This way lucene searches are very quick."
994,A,"In a Lucene / Lucene.net search how do I count the number of hits per document? When searching a bunch of documents I can easily find the number of documents which match my search criteria: Hits hits = Searcher.Search(query); int DocumentCount = hits.Length(); How do I determine the total number of hits within the documents? For example let's say I search for ""congress"" and I get 2 documents back. How can I get the number of times ""congress"" occurs in each document? For example let's say ""congress"" occurs 2 times in document #1 and 3 times in document #2. The result I'm looking for is 5. This is Lucene Java also. If your query/search criteria can be written as a SpanQuery then you can do something like this: IndexReader indexReader = // define your index reader here SpanQuery spanQuery = // define your span query here Spans spans = spanQuery.getSpans(indexReader); int occurrenceCount = 0; while (spans.next()) { occurrenceCount++; } // now occurrenceCount contains the total number of occurrences of the word/phrase/etc across all documents in the index  This is Lucene Java but should work for Lucene.NET: List docIds = // doc ids for documents that matched the query // sorted in ascending order int totalFreq = 0; TermDocs termDocs = reader.termDocs(); termDocs.seek(new Term(""my_field"" ""congress"")); for (int id : docIds) { termDocs.skipTo(id); totalFreq += termDocs.freq(); } Do you want the # of times the phrase appears in each doc or each individual word? @bajafresh4life: What about if the phrase was two words like ""apple tree""?"
995,A,"find similar documents before add User fill multi-field form (document) with date time title and description. Check if similar documents are stored in Solr before document saved User can choose save this document or not. How to implement in Solr ""find similar documents""? in Lucene: FuzzyLikeThisQuery MoreLikeThis? but in Solr? P.S. I use django-hastack Solr also has a MoreLikeThis component."
996,A,"keyword/phrase density from a mysql database I have a LAMP setup with the mysql database essentially being a catalog of products. Since the database changes frequently as new products are added it's cumbersome to manually maintain a list of keywords and popular phrases. The need to keep a keyword/phrase list is twofold: (1) for google adwords and other marketing initiatives and (2) for link structure on my site. I've been using the Zend Lucene port as the backbone for all searching on my site. Is it possible to do things like determine keyword density and/or phrase density using Lucene? What about another search engine? For further clarity of what I'm looking for let's say I have a catalog of laptops. I might have various models of Dell Inspiron Dell Latitude Macbook Gateway Lenovo and Acer laptops. For a keyword density report I'd like to see that the words ""laptop"" and ""notebook"" are popular as well as perhaps ""Dell Inspiron"" or ""Dell Inspiron laptops"" or ""Lenovo laptops."" Can anyone recommend something to get started? I'm sorta eying the whole search module world like Lucene Sphinx Solr etc. since it's already indexing data but I don't know if I'm going down the wrong path. Thanks! Lucene is capable of giving you a list of (keyword frequency) pairs. See this question or this blog post. Seems like you can do this in PHP: search for termDocs() in this page: http://framework.zend.com/manual/en/zend.search.lucene.best-practice.html Thanks I noticed this post is referring to pure Lucene (java) and I'm using the Lucene port bundled with Zend. So hopefully I can do everything natively in php otherwise I may need to scrub off the java cobwebs. I ended up writing my own script for determining keyword density wasn't too hard with python. I'll go ahead and accept this as the right answer..."
997,A,"Situations to prefer Apache Lucene over Solr? There are several advantages to use Solr 1.4 (out-of-the-box facetting search grouping replication http administration vs. luke ...). Even if I embed a search-functionality in my Java application I could use SolrJ to avoid the HTTP trade-off when using Solr. Is SolrJ recommended at all? So when would you recommend to use ""pure-Lucene""? Does it have a better performance or requires less RAM? Is it better unit-testable? PS: I am aware of this question. here are other 'comparisons' http://www.lucenetutorial.com/lucene-vs-solr.html and http://www.lucidimagination.com/solutions/software/choosing-lucene-solr have another look at http://www.findbestopensource.com/article-detail/lucene-vs-solr If you have a web application use Solr - I've tried integrating both and Solr is easier. Otherwise if you don't need Solr's features (the one that comes to mind as being most important is faceted search) then use Lucene. Did you use SolrJ or HTTP approach? I tried to embed lucene in a webapp and it was quite easy. I used Solrj so I didn't need to make HTTP requests from within the application. Honestly I cannot remember what made it difficult so maybe I was doing something dumb somewhere. Thanks for the reply. What about unit-testing is it easy to setup a RAMDirectory like I can do with lucene? I haven't tried it but apparently it is possible: http://search.lucidimagination.com/search/out?u=https%3A%2F%2Fissues.apache.org%2Fjira%2Fbrowse%2FSOLR-465 Lucene supports faceted search but it is really different than what Solr provides. At time of upsert you write a taxonomy (think of it as a secondary inverted index) for each document. The query is different too. You use facet terms and collect facet results.  I'm surprised nobody mentioned NRT - Near Real Time search available with Lucene but not with Solr (yet). really? here is the link http://wiki.apache.org/lucene-java/NearRealtimeSearch ... i thought it is available for solr too @Karussell: see https://issues.apache.org/jira/browse/SOLR-1606 Thanks Mauricio!  Use Solr if you are more concerned about scalability than performance and use Lucene if you are more concerned about performance than scalability. This question is 4 years old btw ... look into ElasticSearch!  If you want to completely embed your search functionality within your application and do not want to maintain a separate process like Solr using Lucene is probably preferable. Per example a desktop application might need some search functionality (like the Eclipse IDE that uses Lucene for searching its documentation). You probably don't want this kind of application to launch a heavy process like Solr. What do you mean with heavy? In terms of CPU/RAM or the maintaining stuff? In terms of physical resources yes. And there is the startup time of Solr that would probably be unacceptable in a desktop application. But I have never experiment with the EmbeddedSolrServer. It might a interesting way to embed Solr.  Here is one situation where I have to use Lucene. Given a set of documents find out the most common terms in them. Here I need to access term vectors of each document (using low-level APIs of TermVectorMapper). With Lucene it's quite easy. Another use case is for very specialized ordering of search results. For exmaple I want a search for an author name (who has writen multiple books) to result into one book from each store in the first 10 results. In this case I will find results from each book store and to show final results I will pick one result from each book store. Here you are essentially doing multiple searches to generate final results. Having access to low-level APIs of lucene definitely helps. One more reason to go for Lucene was to get new goodies ASAP. This no longer is true as both of them have been merged and there will be synchronous releases. TVMapper is core to Lucene. Why go through extra layer when you can directly read from the source? And I'm not exactly looking for grouping. I want all results from each of the book store but I want the order to be a close approximation of round-robin with some additional criteria. Regarding the TermVectorMapper -> Do you know if it is possible with Solr? Regarding the search-order example: couldn't this be done with Solr's grouping feature: http://blog.jteam.nl/2009/10/20/result-grouping-field-collapsing-with-solr/"
998,A,"Lucene and Special Characters I am using Lucene.Net 2.0 to index some fields from a database table. One of the fields is a 'Name' field which allows special characters. When I perform a search it does not find my document that contains a term with special characters. I index my field as such: Directory DALDirectory = FSDirectory.GetDirectory(@""C:\Indexes\Name"" false); Analyzer analyzer = new StandardAnalyzer(); IndexWriter indexWriter = new IndexWriter(DALDirectory analyzer true IndexWriter.MaxFieldLength.UNLIMITED); Document doc = new Document(); doc.Add(new Field(""Name"" ""Test (Test)"" Field.Store.YES Field.Index.TOKENIZED)); indexWriter.AddDocument(doc); indexWriter.Optimize(); indexWriter.Close(); And I search doing the following: value = value.Trim().ToLower(); value = QueryParser.Escape(value); Query searchQuery = new TermQuery(new Term(field value)); Searcher searcher = new IndexSearcher(DALDirectory); TopDocCollector collector = new TopDocCollector(searcher.MaxDoc()); searcher.Search(searchQuery collector); ScoreDoc[] hits = collector.TopDocs().scoreDocs; If I perform a search for field as 'Name' and value as 'Test' it finds the document. If I perform the same search as 'Name' and value as 'Test (Test)' then it does not find the document. Even more strange if I remove the QueryParser.Escape line do a search for a GUID (which of course contains hyphens) it finds documents where the GUID value matches but performing the same search with the value as 'Test (Test)' still yields no results. I am unsure what I am doing wrong. I am using the QueryParser.Escape method to escape the special characters and am storing the field and searching by the Lucene.Net's examples. Any thoughts? While index you have tokenized the field. So your input String creates two tokens ""test"" and ""test"". For search you are constructing query by hand ie using TermQuery instead of QueryParser which would have tokenized the field. For the entire match you need to index field UN_TOKENIZED. Here the input string is taken as a single token. The single token created ""Test (Test)."" In that case your current search code will work. You have to watch the case of input string carefully to make sure if you are indexing lower case text you have to do the same while searching. It is generally good practice to use same analyzer during indexing and searching. You can use KeywordAnalyer to generate single token from the input string.  StandardAnalyzer strips out the special characters during indexing. You can pass in a list of explicit stopwords (excluding the ones you want in). Should I consider another Analyzer to use to achieve my goal? What about switching between Tokenized to Un_Tokenized when storing fields with special characters? well if you don't tokenize the field you cannot ""search"" on it. You have a couple of choices write your own analyzer (is very simple) or pass the list of stop words to StandardAnalyzer. something like: Hashtable htStopwords = new Hashtable(); Analyzer analyzer = new StandardAnalyzer(htStopwords); you can also look at StopAnalyzer or SimpleAnalyzer...they might help. The problem is that you could end up having a lot of noise words. But if that is not an issue...."
999,A,"How to get a Token from a Lucene TokenStream? I'm trying to use Apache Lucene for tokenizing and I am baffled at the process to obtain Tokens from a TokenStream. The worst part is that I'm looking at the comments in the JavaDocs that address my question. http://lucene.apache.org/java/3_0_1/api/core/org/apache/lucene/analysis/TokenStream.html#incrementToken%28%29 Somehow an AttributeSource is supposed to be used rather than Tokens. I'm totally at a loss. Can anyone explain how to get token-like information from a TokenStream? There are two variations in the OP question: What is ""the process to obtain Tokens from a TokenStream""? ""Can anyone explain how to get token-like information from a TokenStream?"" Recent versions of the Lucene documentation for Token say (emphasis added): NOTE: As of 2.9 ... it is not necessary to use Token anymore with the new TokenStream API it can be used as convenience class that implements all Attributes which is especially useful to easily switch from the old to the new TokenStream API. And TokenStream says its API: ... has moved from being Token-based to Attribute-based ... the preferred way to store the information of a Token is to use AttributeImpls. The other answers to this question cover #2 above: how to get token-like information from a TokenStream in the ""new"" recommended way using attributes. Reading through the documentation the Lucene developers suggest that this change was made in part to reduce the number of individual objects created at a time. But as some people have pointed out in the comments of those answers they don't directly answer #1: how do you get a Token if you really want/need that type? With the same API change that makes TokenStream an AttributeSource Token now implements Attribute and can be used with TokenStream.addAttribute just like the other answers show for CharTermAttribute and OffsetAttribute. So they really did answer that part of the original question they simply didn't show it. It is important that while this approach will allow you to access Token while you're looping it is still only a single object no matter how many logical tokens are in the stream. Every call to incrementToken() will change the state of the Token returned from addAttribute; So if your goal is to build a collection of different Token objects to be used outside the loop then you will need to do extra work to make a new Token object as a (deep?) copy.  This is how it should be (a clean version of Adam's answer): TokenStream stream = analyzer.tokenStream(null new StringReader(text)); CharTermAttribute cattr = stream.addAttribute(CharTermAttribute.class); stream.reset(); while (stream.incrementToken()) { System.out.println(cattr.toString()); } stream.end(); stream.close(); Your code did not function properly until I added a stream.reset() before the while loop. I am using Lucene 4.0 so that may be a recent change. Refer to the example near the bottom of this page: http://lucene.apache.org/core/4_0_0-BETA/core/org/apache/lucene/analysis/package-summary.html Thanks you saved me hours of work and head banging :) Tried to edit to add the reset() call which avoids an NPE inside Lucene at incrementToken() but all but one peer rejected the edit as incorrect. The Lucene docs explictly say that ""The consumer calls reset()"" prior to ""The consumer calls incrementToken()"" in the [TokenStream API](http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/analysis/TokenStream.html) Also had to call `reset()` with Lucene 4.3 so I took the liberty of adding it Extremely useful thanks. maybe the question is odd but finally is not very clear how to obtain the next **Token** (not the next string)?  Yeah it's a little convoluted (compared to the good ol' way) but this should do it: TokenStream tokenStream = analyzer.tokenStream(fieldName reader); OffsetAttribute offsetAttribute = tokenStream.getAttribute(OffsetAttribute.class); TermAttribute termAttribute = tokenStream.getAttribute(TermAttribute.class); while (tokenStream.incrementToken()) { int startOffset = offsetAttribute.startOffset(); int endOffset = offsetAttribute.endOffset(); String term = termAttribute.term(); } Edit: The new way According to Donotello TermAttribute has been deprecated in favor of CharTermAttribute. According to jpountz (and Lucene's documentation) addAttribute is more desirable than getAttribute. TokenStream tokenStream = analyzer.tokenStream(fieldName reader); OffsetAttribute offsetAttribute = tokenStream.addAttribute(OffsetAttribute.class); CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class); tokenStream.reset(); while (tokenStream.incrementToken()) { int startOffset = offsetAttribute.startOffset(); int endOffset = offsetAttribute.endOffset(); String term = charTermAttribute.toString(); } Thanks for this! This API does not seem very intuitive so your example is doubly helpful! +1. @Donotello: Thank you very much for that! Now TermAttribute is depricated. As I can see we can use something like `CharTermAttributeImpl.toString()` instead You should use addAttribute rather than getAttribute. From lucene javadocs: ""It is recommended to always use addAttribute(java.lang.Class) even in consumers of TokenStreams because you cannot know if a specific TokenStream really uses a specific Attribute"" http://lucene.apache.org/core/old_versioned_docs/versions/3_5_0/api/all/org/apache/lucene/util/AttributeSource.html#getAttribute(java.lang.Class) @jpountz: Thanks for the tip! I have modified the answer accordingly. Had to call `reset()` with Lucene 4.3 so took the liberty of adding it Finally I don't see the answer on the post question: ""How to get a **Token** from a Lucene TokenStream?"" @serhio: I added a supplementary answer that hopefully addresses your concern You are missing `tokenStream.end()` and `tokenStream.close()` required by the [TokenStream workflow](http://lucene.apache.org/core/4_7_0/core/org/apache/lucene/analysis/TokenStream.html)."
1000,A,"Lucene distinct result Please help me get distinct result by certain field. I tried go by the many way googled... But can't get ti. I tried add to HashSet tried DuplicateFilter. Think about Collector without success. But any result. I use Java lucene-2.9.3. Example: some_id description 1 bbb aaa 1 aaa ccc 2 aaa ddd 2 fff aaa And if I search by description in result I must get distinct some_id (12). Maybe somebody have solution or have code example. Thanks to advance. Using a hashset what did you try? Er what do you exactly mean by distinct result? That the word turnip might appear in 50000 documents but you only want to know if it appears at all? I provided example what i want distinct. hashset store unique value. so i get value and add into hashset. in result unique value but really bad in terms of performance. As far as I know there is no ""native distinct"" support in lucene. So you have to write your own logic in java to consolidate the results.  The very-new (still only a patch) grouping module on https://issues.apache.org/jira/browse/LUCENE-1421 might be relevant here. It enables you to group all hits according to a certain field. For example if you group by ""author"" then all documents having the same author are in the same group. Well I still need to port the patch back to 3.x. Once that's done likely (though I'm not sure!) using it in 2.9.x wouldn't be so bad... it's fairly standalone. Thanks Michael. Excellent work! Could you please suggestcan I use your patch for version 2.9.3? I don't ready jump to 3.2. Thanks one more."
1001,A,"Reindexing a large SQL Server database to Lucene We have a web service method which accepts some data and puts it in Lucene index. We use it to index new and updated entries from our asp.net web app. These entries are stored in a large SQL Server table (20M rows and growing) and I need a way to be able to reindex the whole table in case if current index gets deleted or corrupted. I'm not sure what's the optimal way to retrieve chunks of data from a large table. Currently we use the fact that the table has PK which is autoincrement so we get chunks of 1000 rows until it starts to return nothing. Kind of like (in pseudo language): i = 0 while (true) { SELECT col1 col2 col3 FROM mytable WHERE pk between i and i + 1000 .... if result is empty 20 times in a row break .... .... otherwise send result to web service to reindex .... i = i + 1000 } This way we don't need to SELECT COUNT(*) which would be a big performance killer and we just move up the pk values until we stop getting any results. This has it's con: if we have a hole greater than 20000 values somewhere in the table it will stop indexing assuming it reached the end but that's a tradeoff we have to live for now. Can anyone suggest a more efficient way of getting data from a table to index? I would assume we are not the first ones facing this problem - search engines are widely used nowadays :) I think you need to clarify your question. Is a ""lucene index"" lucene.apache.org? The index being corrupted -- is this a SQL server index? Are you concerned about having sequential numbers in your 20M row table? Also your idea to use IDENT_CURRENT() is interesting but won't necessarily tell you the number of rows in the table - any failed inserts can create phantom gaps in the sequence. @EBarr: I really don't need number of rows but rather first and last row numbers so I can loop from first to last id and grab a 1000 rows each iteration. If there is a gap I will get less than 1000 rows or none - not a big deal. @EBarr: I was talking about Apache's Lucene index. I actually just figured it out - I can use IDENT_CURRENT(table_name) to get the last generated id and use that instead of MAX() or Count() - this method should blow the other two away :)  Why is a COUNT(*) a performance killer? What about MAX(id)? I'm thinking that a index would provide the information needed for those queries. You do have an index on your primary key right?  For what we do with Lucene we rarely need to reindex everything. I can't remember coming across any case when all index would be corrupted (Lucene is actually quite safe/good at this) but it has been many times when individual items needed to be reindexed because of one reason or another. I'd say the most frequent reindexing patterns would be: reindex items by given id (or set of ids) reindex items by given period of time The latter of course requires separate db index on the relevant date field(s) which should be a bit costly for 20M+ records but we decided to go for it (our biggest deployment had up to 10M records) as disk space is cheap these days anyway. EDIT: added few explanations as per question author's comment. If the source data structure changes requiring reindexing of all records our approach is to roll out new code which ensures all new data is correct (basically forms correct Lucene Document from this moment). Then after we can reindex things in batches (either manually or by hand) by providing relevant period ranges. This to certain extent also applies to Lucene version changes too. I've already had problems with Lucene index in our test environment when we decided to make some schema changes that turned out to be incompatible with existing index and that's how we realized we need ability to do a full reindex. Added explanation how we handle these."
1002,A,"How to query PDF in Solr? I added PDF document to Solr curl ""http://localhost:8983/solr/update/extract?literal.id=doc2&captureAttr=true&defaultField=text&fmap.div=foo_t&capture=div"" -F ""tutorial=@a.pdf"" and I would like to query it for word ""errors"" http://localhost:8983/solr/select/?q=errors&version=2.2&start=0&rows=10&indent=on I get no results. However if I query it for word ""java"" I get one page(the added PDF text) and in visible text there is word ""errors"". P.S. Im new to Lucene and Solr and I dont understand why not every word in that pdf is searchable. Try extractOnly=true to see what Solr is extracting from the PDF. Take a look at this similar question and answer Try using the analyzer to see which words are inserted in the index."
1003,A,In Solr what is the difference between the NOT and - (minus) operators? In Solr is there a difference between the NOT and - (minus) operators? If so what is it? Solr documentation references the Lucene Query Parser Syntax and it is vague on this matter. The two operators seem to function the same way but it's not clear. The Lucene QueryParser code says they're equivalent.  To expand on Mauricio's answer (because the QueryParser class is some of the most confusing code I've ever read) if you look at lines 145-152 you'll see:  case MINUS: jj_consume_token(MINUS); ret = MOD_NOT; break; case NOT: jj_consume_token(NOT); ret = MOD_NOT; break; So they are both considered MOD_NOTs. I'm accepting your answer b/c you explained it.
1004,A,"Lucene.NET - sorting by int In the latest version of Lucene (or Lucene.NET) what is the proper way to get the search results back in sorted order? I have a document like this: var document = new Lucene.Document(); document.AddField(""Text"" ""foobar""); document.AddField(""CreationDate"" DateTime.Now.Ticks.ToString()); // store the date as an int indexWriter.AddDocument(document); Now I want do a search and get my results back in order of most recent. How can I do a search that orders results by CreationDate? All the documentation I see is for old Lucene versions that use now-deprecated APIs. After doing some research and poking around with the API I've finally found some non-deprecated APIs (as of v2.9 and v3.0) that will allow you to order by date: // Find all docs whose .Text contains ""hello"" ordered by .CreationDate. var query = new QueryParser(Lucene.Net.Util.Version.LUCENE_29 ""Text"" new StandardAnalyzer()).Parse(""hello""); var indexDirectory = FSDirectory.Open(new DirectoryInfo(""c:\\foo"")); var searcher = new IndexSearcher(indexDirectory true); try { var sort = new Sort(new SortField(""CreationTime"" SortField.LONG)); var filter = new QueryWrapperFilter(query); var results = searcher.Search(query  1000 sort); foreach (var hit in results.scoreDocs) { Document document = searcher.Doc(hit.doc); Console.WriteLine(""\tFound match: {0}"" document.Get(""Text"")); } } finally { searcher.Close(); } Note I'm sorting the creation date with the LONG comparison. That's because I store the creation date as DateTime.Now.Ticks which is a System.Int64 or long in C#. first comprehensible solution to sorting in Lucene that I've found I think you're missing a null for the filter in the searcher.Search() method call..."
1005,A,lucene query size- does this scale? query for '1 OR 2 OR 3 .. OR N' Suppose I have a lucene query 'id1 OR id2 OR id3 ... idN'. How well does that scale as N increases? The situation I'm looking at would be similar to someone doing a text search on products in their shopping cart but they may have hundreds or thousands of items their shopping cart. The user wants to do a text search across all products in their shopping cart. Could I do a text query against all available products then limit the items returned with a OR clause of product IDs in their cart? As @Shashikant Kore mentioned the limit is 1024 by default. If you have a very large collection of text you might want to look at the MoreLikeThis implementation - it uses some neat heuristics to generate a representative query from the content you have.  The maximum number of clauses in a boolean query is 1024 by default. You can increase this limit. There would be performance penalty though. I suppose it would be efficient if you use Filters instead. I was referring to TermsFilter that @Kai Chan pointed in the next answer. Thanks. I am learning Lucene and noticed filters might also solve this I need to look into this. Are you referring to a filter applied during tokenization or another kind? Could you describe the performance difference (why is that approach more performant)?  Use FilteredQuery during search time. Its constructor takes a query and a filter. Create the query from what the user enters (take a look at QueryParser). Create the filter from the list of product IDs (take a look at TermsFilter).  There's a limit on the amount of boolean statements in your query.  As some people already answered there are practical limitations. However if you are interested in the theory there is really no difference between doing a bunch of OR'd terms versus a single term with a lot of possible results. If p is the number of postings (term/doc pairs) which match your query and you want to find the k best matches the query will run in O(p log k). See Doug's paper Space Optimizations for Total Ranking. If you have q query terms OR'd together and t terms in your index total it will actually be something like O(q log t + p log k) but for most applications p log k will dominate that. (This formula came from the fact that it takes log t time to find the posting stream and you have to do it once per query term.) That's great thanks.
1006,A,Forgot to close the Lucene IndexWriter after adding Documents to the index I had a program running for 2 days to build a Lucene index for around 160 million text files and after the program ended I tried searching the index and found the index was not correctly built indexReader.numDocs() returned 0. I checked the index directory it looked good all the index data seemed to be there the directory is 1.5 Gigabytes in size. I checked my code and found that I forgot to call indexWriter.optimize() and indexWriter.close() I want to know if it is possible to re-optimize() the index so I don't need to rebuild the whole index from scratch? I don't really want the program to take another 2 days. How do you know index was corrupt? try opening in LUKE http://www.getopt.org/luke/  see if it can show the Documents! Calling IndexWriter.optimize() is not necessary and can be called at a later time by reopening the index. It just optimizes the documents in the index for better read performance and doesn't otherwise affect anything. If you forgot to call IndexWriter.close() however then your index might not be complete. Since you processed so many documents it likely flushed most of them so hopefully you only need to re-index the last ones. Use Luke as suggested for a UI to quickly browse the index to see what state it's in. Thanks for your reply. I think I need to re-index all the files cause I have no idea what documents are not flushed I need the index to be exact. You could iterate through the documents in the index to determine which ones exist before you re-index everything. See http://stackoverflow.com/questions/2311845/is-it-possible-to-iterate-through-documents-stored-in-lucene-index
1007,A,"field cross search in lucene Hi: I have two documents: title body Lucene In Action A high-performance full-featured text search engine library. Lucene Practice Use lucene in your application NowI search ""lucene performance"" using private String[] f = { ""title"" ""body""}; private Occur[] should = { Occur.SHOULD Occur.SHOULD}; Query q = MultiFieldQueryParser.parse(Version.LUCENE_29 ""lucene performance"" f shouldnew IKAnalyzer()); Then I get two hits: ""Lucene In Action"" and ""Lucene Practice"". However I do not want the ""Lucene practice"" in the search result. That's to sayI just want the documents who own all my search terms can be returnedthe ""lucene parctice"" does not contain the term ""performance""so it should not be returned. Any ideas? Lucene cannot match across fields. That is to say for the query ""a b"" it won't match ""a"" in title and ""b"" in body. For that you need to create another field say all_text which has title and body both indexed. Also when you are searching for ""lucene performance"" I suppose you are looking for documents that have both the terms - lucene as well as performance. By default the boolean operator is OR. You need to specify default operator as AND to match all the terms in the query. (Otherwise in this case the query ""lucene performance"" will start returning matches that talk about database performance.) For English Characters searchingyou are right. And I am looking for some ways to solve the Chinese Characters serching. Thank you anyway."
1008,A,"is it mandatory to optimize the lucene index after write? Currently i am calling the optimize method of the indexwriter after the completions of the write. Since my data set is huge it took long time ( and needs more space (2*actual size)) to optimize the index. I am very much concerned about this because lot of documents included frequently in the index. So is it ok to turn off optimize? What are the performance implications like how much slower the querying when its not optmized? Cheers You know your data best so I would suggest you perform some tests to measure how fast your queries run with and without the optimize step. According to the javadocs ""in environments with frequent updates optimize is best done during low volume times if at all"". You should only optimize when necessary. If only 5% of your documents have changed since the last optimize then it is not necessary so get a feel of how frequently your documents change. Maybe you can optimise less often say once every few hours or once a day. Also take a look at this thread in which they advise against calling optimize at all in an environment whose indices are constantly updated and instead choose to set a low mergeFactor. thanks for the link.. :)  The Lucene FAQ says: What is index optimization and when should I use it? The IndexWriter class supports an optimize() method that compacts the index database and speeds up queries. You may want to use this method after performing a complete indexing of your document set or after incremental updates of the index. If your incremental update adds documents frequently you want to perform the optimization only once in a while to avoid the extra overhead of the optimization. If I decide not to optimize the index when will the deleted documents actually get deleted? Documents that are deleted are marked as deleted. However the space they consume in the index does not get reclaimed until the index is optimized. That space will also eventually be reclaimed as more documents are added to the index even if the index does not get optimized. thats really helpful....... :)"
1009,A,"search with a combination of structured criteria and freetext keyword/phrase - NOSQL vs Lucene/Sphinx we have a eMall application based mainly around a ~500k rows MySQL master table (with detail tables storing non searchable fields and other related tables with shop info etc). Users can today search based on specific structured product data (e.g. brand category price specific shop etc). We would also like to support keyword search in combination with the structured data. We also want to improve the performance of our application and are considering our infrastructure options to achieve both the functional requirement of keyword search and the technical requirement of improved speed: Lucene Sphinx etc to index all products? A NoSQL db (mongo couch etc) used as an intermediate cache layer in front of MySQL? A NOSQL db to replace MySQL? A combination of the above? In the case of Lucene and Sphinx - how flexible are they in terms of combining structured criteria? Or would we need to first run a text search and then filter the results with a second structured query on mySQL? Any hints or leasons learned from your own experiences would be more than welcome! thanks in advance I have been using Sphinx for full text searching similar to your requirements (searching based on freetext and structured attributes) with a few GB of data & 5M rows in MySQL. I am very satisfied with the performance and reliability (not even a single downtime). The advantage in using Sphinx is it is targeted to use with MySQL so it is really easy to setup. Normally you can have the whole system ready in less than an hour so why not give it a try?  I suggest you use Solr - It enables keyword search based on Lucene. You can use facets and filters for your structured product data. 500 K items seem a size Solr can handle rather easily. It can be considered a NoSQL DB and it is easier to use than pure Lucene. You can go over the relevant considerations in Full Text Search Engine versus DBMS. See this link to get know more about solr capabilities - http://www.ibm.com/developerworks/java/library/j-solr1/ and lucene query language: http://lucene.apache.org/java/3_0_2/queryparsersyntax.html Many thanks for your response. I guess we will have to experiment with Solr and Sphinx. We will give Sphinx a try first since it seems to be easier to integrate with what we already have but Solr might also be a suitable solution. +1 for Solr. You can get all that you want done and then some. My only suggestion keep the moving parts to minimum. Just don't add Nosql-ish things because they are the ""in"" thing. FWIW Solr should plenty suffice."
1010,A,"Prevent ""Too Many Clauses"" on lucene query In my tests I suddenly bumped into a Too Many Clauses exception when trying to get the hits from a boolean query that consisted of a termquery and a wildcard query. I searched around the net and on the found resources they suggest to increase the BooleanQuery.SetMaxClauseCount(). This sounds fishy to me.. To what should I up it? How can I rely that this new magic number will be sufficient for my query? How far can I increment this number before all hell breaks loose? In general I feel this is not a solution. There must be a deeper problem.. The query was +{+companyName:mercedes +paintCode:a*} and the index has ~2.5M documents. the paintCode:a* part of the query is a prefix query for any paintCode beginning with an ""a"". Is that what you're aiming for? Lucene expands prefix queries into a boolean query containing all the possible terms that match the prefix. In your case apparently there are more than 1024 possible paintCodes that begin with an ""a"". If it sounds to you like prefix queries are useless you're not far from the truth. I would suggest you change your indexing scheme to avoid using a Prefix Query. I'm not sure what you're trying to accomplish with your example but if you want to search for paint codes by first letter make a paintCodeFirstLetter field and search by that field. ADDED If you're desperate and are willing to accept partial results you can build your own Lucene version from source. You need to make changes to the files PrefixQuery.java and MultiTermQuery.java both under org/apache/lucene/search. In the rewrite method of both classes change the line query.add(tq BooleanClause.Occur.SHOULD); // add to query to try { query.add(tq BooleanClause.Occur.SHOULD); // add to query } catch (TooManyClauses e) { break; } I did this for my own project and it works. If you really don't like the idea of changing Lucene you could write your own PrefixQuery variant and your own QueryParser but I don't think it's much better. It's the backend for an autocomplete dropdown. Isn't there a way to make it just return the results it's already found? Hmm so no default options there then.. That's kind of o downer. Thanks for the response. For an auto-complete field why not use the terms enumerations returned from the index reader and maintain your own cached list? You'll need to check the index reader every now and then to refresh the list but at least you'll have complete flexibility... Well actually that's not that bad an idea.. A bit stupid to distribute my searching over both Lucene and my own filtering but it's the best thing I can think of now.  It seems like you are using this on a field that is sort of a Keyword type (meaning there will not be multiple tokens in your data source field). There is a suggestion here that seems pretty elegant to me: http://grokbase.com/t/lucene.apache.org/java-user/2007/11/substring-indexing-to-avoid-toomanyclauses-exception/12f7s7kzp2emktbn66tdmfpcxfya The basic idea is to break down your term into multiple fields with increasing length until you are pretty sure you will not hit the clause limit. Example: Imagine a paintCode like this: ""a4c2d3"" When indexing this value you create the following field values in your document: [paintCode]: ""a4c2d3"" [paintCode1n]: ""a"" [paintCode2n]: ""a4"" [paintCode3n]: ""a4c"" By the time you query the number of characters in your term decide which field to search on. This means that you will perform a prefix query only for terms with more of 3 characters which greatly decreases the internal result count preventing the infamous TooManyBooleanClausesException. Apparently this also speeds up the searching process. You can easily automate a process that breaks down the terms automatically and fills the documents with values according to a name scheme during indexing. Some issues may arise if you have multiple tokens for each field. You can find more details in the article"
1011,A,Best cross-language analyzer to use with lucene index I'm looking for feedback on which analyzer to use with an index that has documents from multiple languages. Currently I am using the simpleanalyzer as it seems to handle the broadest amount of languages. Most of the documents to be indexed will be english but there will be the occasional double-byte language indexed as well. Are there any other suggestions or should I just stick with the simpleanalyzer. Thanks Purely anecdotal evidence but we use a (customised but not in any relevant way) version of StandardAnalyzer for our system. Our documents may not only be in different languages to each other but documents may contain chunks of different languages (for example imagine an article written in Japanese with comments in English) so language-sniffing is difficult. The majority of our documents are in English but significant numbers are in Chinese and Japanese with a smaller number in French Spanish Portuguese and Korean. End result? We use StandardAnalyzer and have very few complaints from people using the system in non-Roman languages about the way our searching works. Our system is somewhat 'enforced' on its users by the way so it's not like people are not complaining but moving elsewhere; if they're unhappy we generally know. So based on the fact that I'm not swamped with user complaints (very occasional ones mainly about Chinese but nothing serious and they're easily explained) it seems to be 'good enough' for many cases.  I've used the StandardAnalyzer with non-English words and it works ok. It even deals with accented characters. If the language is CJK (Chinese Japanese Korean) Russian or German it may have problems but I suspect most of the problems will be related to the stemming of words. If you don't have stemming enabled it will probably be adequate.  First  you should find that what is your your language ? For example my documents are in english  japanes or persian . you can find that your document language by process on UTF-8 characters . Then  when you find that your document is in which language  you can analyze it with specific analyzer .  From your description I presume you have document of multiple languages but each document has text in only one language. For this case you can use Nutch's language identification to get the language of the document. Then use respective language analyzer to index. To get the correct results for search you need apply language identification to the search query and use that analyzer. The upside here is you will be able to use language-specific stemmer & stopwords pushing the quality of search up. The extra overhead while indexing should be acceptable. The search queries where language identification fails to identify correct language may suffer though. I have used this couple of years back and the results were better than expected. For CJK you can apply similar technique but the tools might be different. I like your suggestions as well more advanced but I might migrate to this. It offers an excellent balance.  SimpleAnalyzer really is simple all it does is lower-case the terms. I'd have thought that the StandardAnalyzer would give better results than SimpleAnalyzer even with non-english language data. You could perhaps improve it slightly by supplying a custom list of stop words in addition to the default english-language ones.  The correct answer depends on your main language (if any). For best cross-language IR performance I'd go with a 4/5-grams analyzer it has shown to work great on many languages. It might even work better than SimpleAnalyzer for English too. See http://www.eecs.qmul.ac.uk/~christof/html/publications/inrt142.pdf for example. I have looked into this but from another angle. It seems like there isn't a catch-all analyzer - each language needs its own approach for the best results.
1012,A,"How to query lucene with ""like"" operator? The wildcard * can only be used at the end of a word like user*. I want to query with a like %user% how to do that? Somewhat similar issue : [http://stackoverflow.com/questions/468279/lucene-net-leading-wildcard-character-throws-an-error](http://stackoverflow.com/questions/468279/lucene-net-leading-wildcard-character-throws-an-error) The trouble with LIKE queries is that they are expensive in terms of time taken to execute. You can set up QueryParser to allow leading wildcards with the following: QueryParser.setAllowLeadingWildcard(true) And this will allow you to do searches like: *user* But this will take a long time to execute. Sometimes when people say they want a LIKE query what they actually want is a fuzzy query. This would allow you to do the following search: user~ Which would match the terms users and fuser. You can specify an edit distance between the term in your query and the terms you want matched using a float value between 0 and 1. For example user~0.8 would match more terms than user~0.5. I suggest you also take a look at regex query which supports regular expression syntax for Lucene searches. It may be closer to what you really need. Perhaps something like: .*user.*  Since Lucene 2.1 you can use QueryParser.setAllowLeadingWildcard(true); but this can kill performance. The LuceneFAQ has some more info for this.  When you think about it it is not entirely unsurprising that lucene's support for wildcarding is (normally) restricted to a wildcard at the end of a word pattern. Keyword search engines works by creating a reverse index of all words in the corpus which is sorted in word order. When you do a normal non-wildcard search the engine makes use of the fact that index entries are sorted to locate the entry or entries for your word in O(logN) steps where N is the number of words or entries. For a word pattern with a suffix wildcard the same thing happens to find the first matching word and other matches are found by scanning the entries until the fixed part of the pattern no longer matches. However for a word pattern with a wildcard prefix and a wildcard suffix the engine would have to look at all entries in the index. This would be O(N) ... unless the engine built a whole stack of secondary indexes for matching literal substrings of words. (And that would make indexing a whole lot more expensive). And for more complex patterns (e.g. regexes) the problem would be even worse for the search engine.  Lucene provides the ReverseStringFilter that allows to do leading wildcard search like *user. It works by indexing all terms in reverse order. But I think there is no way to do something similar to 'LIKE %user%'. thank you Interesting. It does in effect mean that you need to set up your index in advance to allow leading wildcards. And from looking at the bug (https://issues.apache.org/jira/browse/LUCENE-1398) it seems that you can only specify a leading wildcard but not a trailing one in the same term (because then you're back to the same problem)."
1013,A,"How to setup Lucene/Solr for a B2B web app? Given: 1 database per client (business customer) 5000 clients Clients have between 2 to 2000 users (avg is ~100 users/client) 100k to 10 million records per database Users need to search those records often (it's the best way to navigate their data) Possibly relevant info: Several new clients each week (any time during business hours) Multiple web servers and database servers (users can login via any web server) Let's stay agnostic of language or sql brand since Lucene (and Solr) have a breadth of support For Example: Joel Spolsky said in Podcast #11 that his hosted web app product FogBugz On-Demand uses Lucene. He has thousands of on-demand clients. And each client gets their own database. They use an index per client and store it in the client's database. I'm not sure on the details. And I'm not sure if this is a serious mod to Lucene. The Question: How would you setup Lucene search so that each client can only search within its database? How would you setup the index(es)? Where do you store the index(es)? Would you need to add a filter to all search queries? If a client cancelled how would you delete their (part of the) index? (this may be trivial--not sure yet) Possible Solutions: Make an index for each client (database) Pro: Search is faster (than one-index-for-all method). Indices are relative to the size of the client's data. Con: I'm not sure what this entails nor do I know if this is beyond Lucene's scope. Have a single gigantic index with a database_name field. Always include database_name as a filter. Pro: Not sure. Maybe good for tech support or billing dept to search all databases for info. Con: Search is slower (than index-per-client method). Flawed security if query filter removed. One last thing: I would also accept an answer that uses Solr (the extension of Lucene). Perhaps it's better suited for this problem. Not sure. You summoned me from the FogBugz StackExchange. My name is Jude I'm the current search architect for FogBugz. Here's a rough outline of how the FogBugz On Demand search architecture is set up[1]: For reasons related to data portability security etc. we keep all of our On Demand databases and indices separate. While we do use Lucene (Lucene.NET actually) we've modded its backend fairly substantially so that it can store its index entirely in the database. Additionally a local cache is maintained on each webhost so that unnecessary database hits can be avoided whenever possible. Our filters are almost entirely database-side (since they're used by aspects of FogBugz outside of search) so our search parser separates queries into full-text and non-full-text components executes the lookups and combines the results. This is a little unfortunate as it voids many useful optimizations that Lucene is capable of making. There are a few benefits to what we've done. Managing the accounts is quite simple since client data and their index are stored in the same place. There are some negatives too though such as a set of really pesky edge case searches which underperform our minimum standards. Retrospectively our search was cool and well done for its time. If I were to do it again however I would discourage this approach. Simply unless your search domain is very special or you're willing to dedicate a developer to blazingly fast search you're probably going to be outperformed by an excellent product like ElasticSearch Solr or Xapian. If I were doing this today unless my search domain was extremely specific I would probably use ElasticSearch Solr or Xapian for my database-backed full-text search solution. As for which that depends on your auxiliary needs (platform type of queries extensibility tolerance for one set of quirks over another etc.) On the topic of one large index versus many(!) scattered indices: Both can work. I think the decision really lies with what kind of architecture you're looking to build and what kind of performance you need. You can be pretty flexible if you decide that a 2-second search response is reasonable but once you start saying that anything over 200ms is unacceptable your options start disappearing pretty quickly. While maintaining a single large search index for all of your clients can be vastly more efficient than handling lots of small indices it's not necessarily faster (as you pointed out). I personally feel that in a secure environment the benefit of keeping your client data separated is not to be underestimated. When your index gets corrupted it won't bring all search to a halt; silly little bugs won't expose sensitive data; user accounts stay modular- it's easier to extract a set of accounts and plop them onto a new server; etc. I'm not sure if that answered your question but I hope that I at least satisfied your curiosity :-) [1]: In 2013 FogBugz began powering its search and filtering capabilities with ElasticSearch. We like it. To all-- I accepted @Blinky's answer because he has been there done that--with almost the exact same scenario as I face. @Mikos and Shalin offered great suggestions too. And I will consider all their great advice when implementing search on my web app. Jude I appreciate your answer your effort and simply that you took time out of your busy schedule for this. I will keep your advice in mind along with Shalin and @Mikos. Thank you so much.  Shalin Shekhar Mangar answered me on the Solr-user mailing list and by private email. Shalin is a contributor to Solr and an author of the upcoming book Solr in Action. His reply on the mailing list: How would you setup the index(es)? I'd look at setting up multiple cores for each client. You may need to setup slaves as well depending on search traffic. Where do you store the index(es)? Setting up 5K cores on one box will not work. So you will need to partition the clients into multiple boxes each having a subset of cores. Would you need to add a filter to all search queries? Nope but you will need to send the query to the correct host (perhaps a mapping DB will help) If a client cancelled how would you delete their (part of the) index? (this may be trivial--not sure yet) With different cores for each client this'd be pretty easy. His reply by email: I've worked on a similar use-case in the past and we used the multi-core approach with some heavy optimizations on the Solr side. See http://wiki.apache.org/solr/LotsOfCores - I haven't been able to push these changes into Solr yet. I'll try out his approach with a small subset of clients. If Solr doesn't work well I will wait for his ""LotsOfCores"" change to be pushed. His change might go in the next Solr release (within the next few months?).  I am still unclear on what exactly from the 5K databases users are searching for why you need Lucene and the data sizes in each database. But I will take a whack anyway: You should be looking at Multicore Solr (each core = 1 index) and you have a unique URL to query. Authentication will still be a problem and one (hackish) way to approach it would be to make the URL hard to guess. Your webservers can query the Solr instance/core depending on what they have access to. I'd suggest staying away from the filter approach and creating one huge index combining all databases. HTH Thanks @Mikos I will look into the multi-core Solr. Yes I am vague on the type of data stored. But I can say that clients have 100k to 10 mil records. Right now my ""search engine"" consists of dynamic sql queries--which is slow and limiting. I read Lucene is better than full-text catalogs--faster and more scalable. Happy to help. I have done a similar effort recently but if you database fields contain plenty of text then Lucene/Solr will blow your socks off (cf. dyn. sql) plus you also get faceting as a bonus to better filter the results. Just a couple of lessons learned: 1. Do not store the entire record in the index (is tempting to do so) only store what you absolutely need to  such as the record identifier (a db record => a document in Lucene). 2. Once your search is performed use the record ids to retrieve the records from individual db. I found this approach worked best in my case. HTH"
1014,A,How Lucene Cache The Data? How Lucene cache the data ? Does it use memory cache ? or It uses static file for caching data ? Cache _what_ data? Caching of search result Lucene allows for some low-level caching of certain data structures (like the FieldCache) but this is more to allow for custom functionality. If you're looking to cache search results you might want to look into Solr. It comes with all kinds of caches out of the box. See http://wiki.apache.org/solr/SolrCaching . But I have found that OS buffer caching (which happens automatically) makes Lucene searches fast enough. Without providing any more detail about your situation it's difficult to recommend a good caching strategy for you.
1015,A,"Semantic analysis using Solr I'm considering about adding semantic analysis to my Solr installation but I don't exactly know where to start. Basically I'd like Solr to be able to find ""similar"" words (taken from the body of the indexed documents). For example if I search for ""music"" I should be able to query the semantic engine and obtain ""rock"" ""pop"" etc. (of course if these words appeared near to music in some of the indexed documents). I found this project but I don't know if it is the correct place to start: http://code.google.com/p/semanticvectors/ You may use the Lucene Wordnet contrib package to look for synonyms. Optimizing Findability in Lucene and Solr gives other ways to expand queries.  Semantic indexing is a good place to start. However in my experience these kind of technologies don't work that well in practice. You often end up with very bizarre results. Also because of Google people have a certain expectation of how keyword search should behave - i.e. your search term should appear in the matching document."
1016,A,"Do Lucene and Sphinx support prefix matching? If not how do you make this work with them and which is better? e.g. when searching for ""mi"" i would like results with ""microsoft"" to potentially show up in a result even though there is no ""keyword"" like ""mi"" specifically. Relevant: http://stackoverflow.com/questions/737275/pros-cons-of-full-text-search-engine-lucene-sphinx-postgresql-full-text-searc Yes and Yes. Lucene has PrefixQuery: BooleanQuery query = new BooleanQuery(); for (String token : tokenize(queryString)) { query.add(new PrefixQuery(new Term(LABEL_FIELD_NAME token)) Occur.MUST); } return query; You can also use the Lucene query parser syntax and define the prefix search by using a wildcard exam*. The query parser syntax works if you want to deploy a separate Lucene search server Solr that is called using a HTTP API In Sphinx it seams you have to do the following: Set minimum prefix length to a value larger than 0 Enable wildcard syntax Generate a query string with a willdcard exam* I've used the wildcard syntax to provide autocomplete and google live type results with sphinx. You'd start typing ""New "" and it would offer ""New York Times New York Post"" etc. It was fast enough to provide this with ajax."
1017,A,"Is the order of multi-valued fields in Lucene stable? Suppose I add several values to a Document under the same field name: doc.Add( new Field( ""tag"" ""one"" ) ); doc.Add( new Field( ""tag"" ""two"" ) ); doc.Add( new Field( ""tag"" ""three"" ) ); doc.Add( new Field( ""tag"" ""four"" ) ); If I then later retrieve these fields from a new instance of Document (from a search result) am I guaranteed that the order of the Fields in the array will remain the same? Field[] fields = doc.GetFields( ""tag"" ); Debug.Assert( fields[0].StringValue() == ""one"" ); Debug.Assert( fields[1].StringValue() == ""two"" ); Debug.Assert( fields[2].StringValue() == ""three"" ); Debug.Assert( fields[3].StringValue() == ""four"" ); Current code does but states no guarantees whatsoever so it may change at any time. I wouldn't depend on it. Hmm well it's all I have to go on right now so I may have to. I won't update the Lucene engine without making sure it is still so."
1018,A,"Where do I begin learning Lucene.NET Solr Hadoop and MapReduce? I'm a .NET developer and I need to learn Lucene so we can run a very large scale search service that removes entries that the end user doesn't have access to. (ie a User can search for all documents with clearance level 3 or higher but not clearance level 2 or 1) Where do I start learning which products should I consider? To be honest I'm a little overwhelmed but I'm determined to figure it all out... eventually. +1 for the question. I'm really interested in MapReduce. You seem to be confused about what exactly each project (Lucene/Solr/Hadoop/etc) does. So the first thing to do would be understanding the purpose of each project. Read the docs and blogs about them. If possible buy and read books about them. For example MapReduce and Hadoop have nothing to do with your security requirements. Hadoop is a platform for distributed scalable computing. But Solr is scalable on its own. You might want to use Hadoop to distribute a crawler though (e.g. Nutch).  If you want a book that covers all the basics of Lucene consider ""Lucene in Action"". Even though the code samples are Java you can easily port them to .NET. Of course there also are tonnes of resources on the web such as SO and the Lucene mailing lists which should help you along. For project you describe you should look at Solr since it abstracts out lots of the issues of scalability etc. and via Solrnet can easily integrate into your .NET app. To restrict access by a level your index documents should contain a field called ""Level"" (say) and in the background of your user query you append the ""Level:Level-1"" query using a boolean query construct. At this stage my recommendation would be to stay away from Hadoop (Apache Map-reduce implementation) for your project and stick with Solr. If you are however keen to learn about it. It too has a very useful book you guessed it ""Hadoop In Action"" (also from Manning Publications). Since I have very large amounts of data that has to be indexed in realtime perhaps I need Hadoop to process and index the data and Solr to allow users to read the data? (Via REST?) They are Apples and Oranges. For most enterprise end applications Solr should suffice and scales well. Hadoop is a distributed computing platform used by organizations like Yahoo for their search indexes. Hadoop is also used for high-performance Machine learning tasks Apache Mahout is one such project. Bottom-line: since you indicated you are newbie my recommendation would be to stick to Solr. Unless I missing something I think should more than suffice for your requirements. Perhaps you are putting the cart before the horse. Could you define ""large amounts of data""? Also it'd be wise to check if Solr scaling approaches (qv. http://bit.ly/90WhVo) work for you before hypothetical problems and associated solutions. In most cases the approaches in the link above should suffice a-plenty.... Thanks! Can you help me understand the difference between Hadoop and Solr? Do they serve the same requirement in different ways?"
1019,A,Search engine Lucene vs Database search I am using a MySQL database and have been using database driven search. Any advantages and disadvantages of database engines and Lucene search engine? I would like to have suggestions about when and where to use them? Use Lucene when you want to index textual Documents (of any length) and search for Text within those documents returning a ranked list of documents that matched the search query. The classic example is search engines like Google that uses text indexers like Lucene to index and query the content of web pages. The advantages of using Lucene over a database like Mysql for indexing and searching text are: for the developer - tools to analyse parse and index textual information (e.g. stemming plurals synonyms tokenisation) in multiple languages. Lucene also scales very well for text search. for the user - quality search results. Lucene uses a very good similarity function (to compare the search query against each document) at the heart of which are the Cosine Similarity and Inverse Term/Document frequency. This results in good search results with very little tweaking required upfront. Lots of useful info on Lucene here.  I suggest you read Full Text Search Engines vs. DBMS. A one-liner would be: If the bulk of your use case is full text search use Lucene. If the bulk of your use case is joins and other relational operations use a database. You may use a hybrid solution for a more complicated use case. i have seen 'search our site' in lots of sites. If I am to search the content of sites then which would be better? Searching a site is full-text search. Therefore Lucene is better. Better still use Solr: http://lucene.apache.org/solr/ @YuvalF if the database is not large and the user of that site is not so many is DBMS based full text query enough? since sometimes if that guy want to use lucene he must develop it by himself ... @hugemeow - sure. A DBMS's full-text search will be fine for small use cases. If you want to use Lucene you can use Solr or ElasticSearch - both much easier to start working with than bare Lucene and providing at least 90% of Lucene's functionality. I have also heard nice things about Sphinx though have never used it myself. I have updated the link. Also see my Quora answer about a related subject where I have added the NoSql option: http://www.quora.com/ElasticSearch/Does-it-make-sense-to-use-Lucene-based-products-ElasticSearch-Solr-as-a-datastore Can you update the link? The current one seems broken.  Lucene search has a advantage of indexing. This post can help you understand lucene. i think we can also add index on database table. but i don't have a clue about what is it? You should look at this: http://databases.aspfaq.com/database/should-i-index-my-database-table-s-and-if-so-how.html This will help you understand indexing.  We used Sql Server at work to make some queries which used Fulltext search. In case of big amounts of data Sql makes an inner join between result set returned by FullText search and the rest of the query which might be slow if database is running on the low powered machine (2GB ram for 20 GB of data). Switching the same query to Lucene improved speed considerably.
1020,A,Neo4j indexing with Lucene and query with SOLR I am using Neo4j with Lucene indexing which works pretty good with nodes maybe not so well with relationships. But My question is: is there a way to use SOLR to query on Neo4j since it uses Lucene? is there a plugin somewhere or do I have to create my own? Thanks for your answers What is the problem with index for relationships? Have you looked at the integrated index framework http://wiki.neo4j.org/content/Index_Framework#Relationship_indexes ? Regarding you question I haven't looked into Solr much. / Mattias main contributor to lucene integration for Neo4j.  Doesn't seem to be released yet but see http://lists.neo4j.org/pipermail/user/2010-January/002372.html
1021,A,Implementing Lucene without an Analyzer for the used language of content? Does it make sense? For my client it's too expensive to develop the Analyzer for Croatian language I did not find any existing ones...so my question is...do I tell them to drop the idea of Lucene for Croatian content? Thanks! What requirements? Diacritics? Stemming? Synonyms? Sematext's Morphological Analyzer claims to support Croatian.  Robert Muir Chris Male and others built a Lucene Morphological Analyzer based on Hunspell. The code is here. Croatian is one of the supported languages in the list. There may be licensing issues as hunspell is GPL I think but it is well worth checking.
1022,A,Wildcard search in Solr I am having a problem doing wildcard searches in lucene syntax using the edismax handler. I have Solr 4.0 nightly build from the trunk. A general search like 'computer' returns results but 'com*er' doesn't return any results. Similary a search like 'co?mput?r' returns no results. The only type of wildcard searches working currrently is ones with trailing wildcards(like compute? or comput*). I want to be able to do searches with wildcards at the beginning (*puter) and in between (com*er). Could someone please tell me what I am doing wrong and how to fix it. Thanks. Regards Imran. Leading wildcard won't work unless you activate the reverse string filter. To use it in Solr add a ReversedWildcardFilterFactory to the analyzer of the field that you want to search with a leading wildcard. As for your other queries that return 0 result try using luke to see how your terms are being analyzed (stored in the index). Don't forget to take into account the effect of stemming if you are using it. Thanks for your response. Could you tell me how to activate reverse String filter? See my updates on how to use it in solr. Please tell us how it went because I have never actually used it...  With edismax leading wildcards are no problem. I just retested it. Wildcards in middle of term are no problem either. Looks like there is something else wrong. Are you sure you are using edismax? Thanks for responding. The edismax handler I am using is: http://drupal.org/files/issues/713142-solrconfig-1_0.patch I have Solr 4.0 nightly and I'm querying the existing Index after setting QueryType as 'edismax'. Please could you point out what I am doing wrong? Thanks. A bit hard to answer that but how big is your war-file? My drupal war doesn't support edismax actually. I'm not using any war-file. I'm starting the Solr server via the jar (start.jar) and using SolrJ to Index and Search. While searching I'm setting the QueryType as 'edismax'. Normal text searches work perfectly only wildcards are not working. Any idea whats wrong? Hey I figured it out. I was using stemming. Got rid of that and it started working perfectly. Thanks for your help. :)
1023,A,"Solr - equivalent to LIKE in SQL I've been playing around with creating a collection in Apache Solr via ColdFusion 9 from a database result set. I would like to do a search which would be as follows in SQL: select * from events where eventName like 'Meet%' In SQL this will match partially on a word and return the row. I am trying to do this using a Solr collection and <cfsearch> in CF like so: <cfsearch collection=""#myCollection#"" criteria=""Meet*"" name=""results"" /> However I am not getting the data back unless I specify the full word despite the use of the wildcard. The docs say the wildcard is not allowed at the start of the search but it doesn't say it's not allowed at the end. In fact for me it doesn't work anywhere! <!--- No results --> <cfsearch collection=""#myCollection#"" criteria=""Meet*"" name=""results"" /> <!--- No results --> <cfsearch collection=""#myCollection#"" criteria=""Meet*g"" name=""results"" /> <!--- No results --> <cfsearch collection=""#myCollection#"" criteria=""Meeti?g"" name=""results"" /> <!--- Yes - results! --> <cfsearch collection=""#myCollection#"" criteria=""Meeting"" name=""results"" /> Has anyone implemented a wildcard Solr search using <cfsearch>? If so can you point me in the right direction on this please? Have you had a look at this post about wildcard searching in Solr? As long as you are using the correct query parser i.e. one that supports wildcard queries then you should be able to do the 'Meet%' query using 'Meet*'. I'm using ColdFusion which should support wildcard operations. Please see the question. The * operator is one of the examples I give. Ok sounds good. Have you given the * operator a try yet?  Try ""meet*"" rather than ""Meet*"". I've found that wildcards will only work with lower case strings so whenever a search query contains an asterisk I LCase() the string before passing it to Solr. This is the answer! No wonder I was getting confusing results. I wish the docs would mention that - in fact I will post a comment there now."
1024,A,"how to index a folder using lucene.net Sir I am trying to develop a search engine in asp.net using lucene.net. I go through many tutorials and pages to get the appropriate results but i couldn't. Actually I have a folder with some files(docpptpdfexcel etc..) and i want to search within that folder only for contents and if the results are not found within that folder then ask user to search on web. for example i have a folder with thousands of files @ C:\test and if user searched for ""miller"" then it should search into every document. if results are found then it should display results like that Searched text file no of occurences miller C:\test\1\file.doc 5 miller C:\test\1\11\new.doc 2 please help me i am not getting appropriate results . Waiting for your reply... what have you tried and at what step is it failing/are you getting unexpected results? I'd suggest that you use the code from http://www.searcharoo.net. This is a lucene search engine that does almost exactly what you want and is open source. You will need to modify it slightly for your purposes. thanx a lot but I have problem in indexing a folder or where we can add folder path to which it has to monitor. Kindly Explain.............  Lucene / Lucene.NET is just an indexing engine you still have to extract the text from the file types that you want to support yourself -on Windows you can use the IFilter interface for many file types if you have Acrobat Reader 7+ installed there should be built in support for IFilter for PDF files. As for the indexing part itself there are many many samples out there. Also see this thread What's a good method for extracting text from a PDF using C# or classic ASP (VBScript)?"
1025,A,"Lucene: searching/filtering by field's value length I need some help doing a search. Say I have a really simple document structure just 1 field labeled name. I need to retrieve all the names whose length is more or less than a specified value. By length I mean String.length(). A range filter seems close in concept but I couldn't find a good example to write my specific case. Thanks for the help. Creating a NumericField using the length is a reasonable suggestion you can then use a RangeQuery to extract your results I got the same answer from lucene users list. I'm going to wait a bit more and go that way. If you care about the points you can add it as an answer and I'll mark this as resolved. The easiest solution appears to be to create a second field that contains the length. I'm not sure how the index for the `name` field could be used to efficiently query by its length. I guess I could create that field index by it and then do a range filter/query. Seems to me that there should be a more direct solution but this is a good suggestion. There could be a more direct solution I'm only a newbie with Lucene (that's why I wrote my suggestion as a comment rather than an answer) but this is certainly what I'd do. Add a NumericField using the length then use a RangeQuery. See NumericField javadoc's for an example.  This is a classic example of a MultiTermQuery. It's not in the box but easy to implement. Take a look at WildCardQuery which extends MultiTermQuery. This does something very similar. Just use a different FilterredTermEnum like this one which uses the length of the term.text to filter the terms (not the term text itself). The magic happens here (this code is in the custom term enumerator at the bottom of my post): protected internal override bool TermCompare(Term term) { if (field == term.Field()) { System.String searchText = term.Text(); if (searchText.Length >= text.Length()) { return true; } } endEnum = true; return false; } The above code looks through all the terms for the field and checks their lengths against the length of the term passed in the constructor. It yields true for any field that is at least that long. public class MinLengthQuery : MultiTermQuery { public MinLengthQuery(Term term) : base(term) { } protected internal override FilteredTermEnum GetEnum(IndexReader reader) { return new MinLengthTermEnum(reader GetTerm()); } } This class does all the work: public class MinLengthTermEnum : FilteredTermEnum { internal Term searchTerm; internal System.String field = """"; internal System.String text = """"; internal System.String pre = """"; internal int preLen = 0; internal bool endEnum = false; public MinLengthTermEnum(IndexReader reader Term term):base() { searchTerm = term; field = searchTerm.Field(); text = searchTerm.Text(); SetEnum(reader.Terms(new Term(searchTerm.Field() """"))); } protected internal override bool TermCompare(Term term) { if (field == term.Field()) { System.String searchText = term.Text(); if (searchText.Length >= text.Length()) { return true; } } endEnum = true; return false; } public override float Difference() { return 1.0f; } public override bool EndEnum() { return endEnum; } public override void Close() { base.Close(); searchTerm = null; field = null; text = null; } } (I'm a lucene.net guy but the translation ought be be easy enough... It would probably be easier to start with your version of Lucene's source code for WildCardQuery and TermEnum and work from it). Thanks a lot for your detailed answer! Converting it to java was indeed easy enough. However I think there's a problem: this assumes that the terms are ordered by the enumerated criteria and I don't think that's the case. If the index has the following content: aaaa aaaabbbb bbbb ...and the query asks for lengths less than 5 it seems like this will stop enumerating at the 2nd element and miss the 3rd. I tweaked your endEnum() method to use return actualEnum.term()==null; and that works however seems like that transformed an index search into a linear search for that term. I tested it with a simple query and a small database (~17k docs) and adding this criterion changed the query time from <10ms to 284ms. I could add a length field and index by it but that is now very close to the initial suggestion. What do you think? Looks like I don't understand that stuff as well as I thought. I'm shooting from the hip here but shouldn't it make sure it's looking over the correct field... otherwise won't it continue on to the next field?(`actualEnum.term().field() == expectedField || actualEnum.term()==null`). I thought the TermEnumerator would start at one field go through all the terms for that field then advance to the next field... until it reaches the last term of the last field. I've now realized that I was referring to documents and the enumerator is about terms. This is turning too time consuming for me. I'm going to try the first suggestion and if that doesn't work I'll get back trying this. Still thanks a lot for your help!"
1026,A,"Lucene DuplicateFilter question Why DuplicateFilter doesn't work together with other filters? For example if a little remake of the test DuplicateFilterTest then the impression that the filter is not applied to other filters and first trims results:  public void testKeepsLastFilter() throws Throwable { DuplicateFilter df = new DuplicateFilter(KEY_FIELD); df.setKeepMode(DuplicateFilter.KM_USE_LAST_OCCURRENCE); Query q = new ConstantScoreQuery(new ChainedFilter(new Filter[]{ new QueryWrapperFilter(tq) // new QueryWrapperFilter(new TermQuery(new Term(""text"" ""out""))) // works right it is the last document. new QueryWrapperFilter(new TermQuery(new Term(""text"" ""now""))) // why it doesn't work? It is the third document but hits count is 0. } ChainedFilter.AND)); // this varians doesn't hit too: // ScoreDoc[] hits = searcher.search(new FilteredQuery(tq df) new QueryWrapperFilter(new TermQuery(new Term(""text"" ""now""))) 1000).scoreDocs; // ScoreDoc[] hits = searcher.search(new FilteredQuery(tq new QueryWrapperFilter(new TermQuery(new Term(""text"" ""now"")))) df 1000).scoreDocs; ScoreDoc[] hits = searcher.search(q df 1000).scoreDocs; assertTrue(""Filtered searching should have found some matches"" hits.length > 0); for (int i = 0; i < hits.length; i++) { Document d = searcher.doc(hits[i].doc); String url = d.get(KEY_FIELD); TermDocs td = reader.termDocs(new Term(KEY_FIELD url)); int lastDoc = 0; while (td.next()) { lastDoc = td.doc(); } assertEquals(""Duplicate urls should return last doc"" lastDoc hits[i].doc); } } DuplicateFilter independently constructs a filter which chooses either the first or last occurence of all documents containing each key. This can be cached with minimal memory overhead. Your second filter independently selects some other documents. The two choices may not coincide. To filter duplicates according to some arbitrary subset of all docs would probably need to use a field cache to be performant and this is where things get expensive RAM-wise"
1027,A,"multiple fieldables in document causes nullpointerexception When i add more than one Fieldable to my Document i get a nullpointer exception(without any exception description) when i try to add the document to the indexwriter. i only changed the fieldable method called stringvalue to return a string. Is it not allowed to add more fieldables or am i missing something ? code that might be relevant  File[] files = FILES_TO_INDEX_DIRECTORY.listFiles(); Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_33); SimpleFSDirectory d = new SimpleFSDirectory(INDEX_DIRECTORY); IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_33 analyzer); IndexWriter indexWriter = new IndexWriter(d iwc); for (File file : files) { try { final String path = file.getCanonicalPath(); final String name = file.getName(); Fieldable field1 = new Fieldable() { removed } Fieldable field2 = new Fieldable() {also removed} Document document = new Document(); document.add(field1); document.add(field2); List<Fieldable> minliste = document.getFields(); indexWriter.addDocument(document); //this is where it fails } Fieldable : Fieldable field1 = new Fieldable() { @Override public int getBinaryLength() { return 0; } @Override public int getBinaryOffset() { return 0; } @Override public byte[] getBinaryValue() { return null; } @Override public byte[] getBinaryValue(byte[] arg0) { return null; } @Override public float getBoost() { return 0; } @Override public boolean getOmitNorms() { return false; } @Override public boolean getOmitTermFreqAndPositions() { return false; } @Override public boolean isBinary() { return false; } @Override public boolean isIndexed() { return false; } @Override public boolean isLazy() { return false; } @Override public boolean isStoreOffsetWithTermVector() { return false; } @Override public boolean isStorePositionWithTermVector() { return false; } @Override public boolean isStored() { return false; } @Override public boolean isTermVectorStored() { return false; } @Override public boolean isTokenized() { return false; } @Override public String name() { //TODO return ""path""; } @Override public Reader readerValue() { return null; } @Override public void setBoost(float arg0) { } @Override public void setOmitNorms(boolean arg0) { } @Override public void setOmitTermFreqAndPositions(boolean arg0) { } @Override public String stringValue() { //TODO return path; } @Override public TokenStream tokenStreamValue() { return null; } }; You're implementing your Fieldables from scratch. I suspect that the problem is hidden somewhere in your removed lines - maybe one of the implemented methods return null where it should return a real value. Try to add instances of Field instead and check if you still have the same error Use a debugger an add a breakpoint on NullPointerException that should help finding the real cause of your problem. Now that you've added the implementation of your Fieldable I'm pretty sure that the NPE occurs because you return some null values in your implementation. A hot candidate is your implementation of readerValue() which returns null. JavaDoc explains that the Reader that is returned by this method can be used at index time to generate index tokens. An indexing is what happens automatically when you add more then one Fieldable to a document. Give this a try: @Override public Reader readerValue() { return new StringReader(path); } Change your implementation and avoid return null unless the JavaDoc clearly says that null is a legal return value. (and: even if Field is deprecated: use it for a test to see if the problem is related to your Fieldables. If your code works with Field objects then you know where to look) Field has been drepecated for Fieldable i've debugged at stepped through the process and confirmed that the document isn't null and it contains the fieldables. i've added the code for the Fieldable just in case. update followed your advice and went back to Fields and just ignore the deprecation warnings. it works this way but i'm still a bit frustrated that i can't figure out what's going on with fieldables. Easy as I wrote: Something inside the lucene framework (indexer I guess) calls methods on your `Fieldables` expects a value but receives `null` tries to use `null` as an objects and runs into an NPE. Look at my last edit added some code. that doesn't make sense to me when it doesn't fail until i add more than one fieldable to the document. i tried changing the two null returning methods in fieldable to return som muck data got the same error. It makes sense. Look at the JavaDoc for [Document#add](http://lucene.apache.org/java/3_0_0/api/core/org/apache/lucene/document/Document.html). Lucene will index *if* you add more then one `Fieldable`. On the other hand no indexing if you only add one."
1028,A,How to boost Solr relevancy score by inverse of geodist() So I've implemented and successfully used Solr 4. I've got to say that Solr 4 is awesome! Anyway I successfully sorted by distance and used a geofilter to limit the results to a certain area. What I would like to do now is boost the relevancy score by the inverse of the distance. This page talks about it but doesn't say how to do it (http://wiki.apache.org/solr/SpatialSearch) I've tried the following but it gives me an error: http://localhost:8983/solr/select/?q={!boost b=recip(geodist() 1 1000 1000)}... The error I get is: org.apache.lucene.queryParser.ParseException: Expected identifier at pos 27 str='{!boost b=recip(geodist() 1 10 in ... Any help would be appreciated. Thanks! http://wiki.apache.org/solr/SpatialSearch#How_to_boost_closest_results You still need to specify the main part of your query after the boost function: q={!boost b=recip(geodist()110001000)}foo:bar&... If you're only interested in boosting by the inverse of the distance you can use a wildcard query: q={!boost b=recip(geodist()110001000)}*&... ...or use the function query parser: q={!func}recip(geodist()110001000)&... You also need to specify the lat/long values and spatial field to query against either as arguments of the geodist function: q={!boost b=recip(geodist(50.1 -0.86 myGeoField)110001000)}foo:bar&... ...or factored out as query string parameters: q={!boost b=recip(geodist()110001000)}foo:bar&sfield=myGeoField&pt=50.1-0.86 Thank you! Yes the part of the query I was messing up on was specifying the lat/long and the spatial field! Thanks again! Just FYI In solr 3.x you will need to remove spaces in between the commas for the parameters @PsychoDad Holy moly that is one nasty gotcha. Solved a problem for me thanks heaps! I have edited the answer to remove the gotcha spaces.
1029,A,"Lucene ignore keywords in search term This seems like it should be simple but I can't figure out how to get Lucene to ignore the AND OR and NOT keywords - the query parser throws a parse error when it gets one. I have a query builder class that splits the search term so that it searches on the words themselves as well as on n-grams in the word. I'm using Lucene in Java. So in a search for say ""ANDERSON COOPER"" the query string looks like: name: (ANDERSON COOPER ""ANDERSON COOPER"")^5 gram4: ( ANDE NDER DERS ERSO RSON SONC ONCO NCOO COOP OOPE OPER) the query parser throws an error when it gets those ANDs. Ideally I'd like the parser to just ignore AND OR NOT altogether and I'll use the && || and ! equivalents if I need them - do I have to modify the code in the QueryParser class itself to get this? Or is there an easier way? I could also just insert an escape character for these cases if that is the best way to do it but adding \ before the word AND doesn't seem to do anything. You can wrap the AND in quotes like this: ""AND"". Is that easy? A regex could probably do that easily if you know exactly what your queries look like. The parser shouldn't have a problem with it and the PhraseQuery will be rewritten as a term query so it will be a small constant-time performance difference big-oh O(1). The regex could probably look like this: \b(AND|OR|NOT)\b Which would be replaced with ""$1"" That's exactly what I needed thanks! I had tried wrapping the name in quotes but each of the n-grams need to be in quotes as well. About as easy a solution as a I could have hoped for."
1030,A,"How to use NGramTokenizerFactory or NGramFilterFactory? Recently I am studying how to store and index using Solr. I want to do facet.prefix search. With whitespace tokenizer ""Where are you"" will be splited into three words and indexed. If I search facet.prefix=""where are"" no result will be returned. I google and found NGramFilterFactory can help me. But when I apply this filter factory I found the result is ""w h e ... wh .."" which split the sentence by character not by token word. I use the parameters maxGramSize and minGramSize set to 1 and 3. Does the NGramFilterFactory work right? Should I add some other parameters? Is there some other filter factories which can help me? Thanks! Hi Mauricio I want to use facet.prefix for autocompletion. With default method the three words will be indexed separately. When doing facet.prefix search of course searching ""w"" will return ""where"" but searching ""where "" nothing will be returned. So I want to add the tokens for indexed. Not quite an answer for you but a clarification. NGram works on individual characters. It can take the word ""cat"" and slice it into tokens like ""c""""a""""t""""ca""""at"" and ""cat"". It *looks* like you may be wanting what is called a shingle tokenizer which works much the same but at a word level instead of a character level. Yes that's exactly how ngram works. In what context are you using facet.prefix? Facets should only be applied to non tokenized fields like strings. if you want that results will be displayed for ""what are"" use no tokenizer at all for that field (or a copyField directive). I guess that you want to use facet.prefix for autocompletion. you can do this look here. for the ngramtokenizer check this out."
1031,A,"how do i get all the similar documents without any search terms i need to develop a search application  where many documents are indexed with different fields and a id field which is unique for each of the document . Fields are not stored just indexed except for id field i need to find out for each document  the documents similar to this here all i have is unique id field of current document  i dont have any other fields of current document to form Terms and query the index for finding similar documents like current one. How do i do this ? any help greatly appreciated . I believe the simplest way to do this is to use Solr and use Solr's MoreLikeThisHandler. You can use a query likehttp://localhost:8983/solr/select?q=unique_id:2722&mlt=true&mlt.fl=manucat&mlt.mindf=1&mlt.mintf=1&fl=idscore hey yeah thanx i recollect now  there is complete worked example of ""MoreLikethis "" in Manning Lucene in action book  thanq very much  let me try that  Do you have any control over how these documents are indexed? You can index with term vectors and at query time look up the term vector for the document construct a query using the terms and submit the query. yes i have access to indexing too  i would try this  thanq !"
1032,A,"Most efficient way to generate a list of Unigrams from a text field in MongoDB I need to generate a vector of unigrams i.e. a vector of all the unique words which appear in a specific text field that I have stored as part of a broader JSON object in MongoDB. I'm not really sure what's the easiest and most efficient way to generate this vector. I was thinking of writing a simple Java app which could handle the tokenization (using something like OpenNLP) however I think that a better approach may be to try to tackle this using Mongo's Map-Reduce feature... However I'm not really sure how I could go about this. Another option would be to use Apache Lucene indexing but it would mean I'd still need to export this data in one by one. Which is really the same issue I would have with the custom Java or Ruby approach... Map reduce sounds good however the Mongo data is growing by the day as more document are inserted. This isn't really a one off task as there are new documents being added all the time. Updates are very rare. I really don't want to run a Map-Reduce over the millions of documents every time I want to update my Unigram vector as I fear this will be very inefficient use of resources... What would be the most efficient way to generate the unigram vector and then keep it updated? Thanks! Anyone help ??? Since you have not provided a sample document (object) format take this as a sample collection called 'stories'. { ""_id"" : ObjectId(""4eafd693627b738f69f8f1e3"") ""body"" : ""There was a king"" ""author"" : ""tom"" } { ""_id"" : ObjectId(""4eafd69c627b738f69f8f1e4"") ""body"" : ""There was a queen"" ""author"" : ""tom"" } { ""_id"" : ObjectId(""4eafd72c627b738f69f8f1e5"") ""body"" : ""There was a queen"" ""author"" : ""tom"" } { ""_id"" : ObjectId(""4eafd74e627b738f69f8f1e6"") ""body"" : ""There was a jack"" ""author"" : ""tom"" } { ""_id"" : ObjectId(""4eafd785627b738f69f8f1e7"") ""body"" : ""There was a humpty and dumpty . Humtpy was tall . Dumpty was short ."" ""author"" : ""jane"" } { ""_id"" : ObjectId(""4eafd7cc627b738f69f8f1e8"") ""body"" : ""There was a cat called Mini . Mini was clever cat . "" ""author"" : ""jane"" } For the given dataset you can use the following javascript code to get to your solution. The collection ""authors_unigrams"" contains the result. All the code is supposed to be run using mongo console (http://www.mongodb.org/display/DOCS/mongo+-+The+Interactive+Shell). First we need to mark of all the new documents that have come afresh into the 'stories' collection. We do it using following command. It will add a new attribute called ""mr_status"" into each document and assign value ""inprocess"". Later we will see that map-reduce operation will only take those documents in account which are having the value ""inprocess"" for the field ""mr_status"". This way we can avoid reconsidering all the documents for map-reduce operation that have been already considered in any of the previous attempt making the operation efficient as asked. db.stories.update({mr_status:{$exists:false}}{$set:{mr_status:""inprocess""}}falsetrue); Second we define both map() and reduce() function. var map = function() { uniqueWords = function (words){ var arrWords = words.split("" ""); var arrNewWords = []; var seenWords = {}; for(var i=0;i<arrWords.length;i++) { if (!seenWords[arrWords[i]]) { seenWords[arrWords[i]]=true; arrNewWords.push(arrWords[i]); } } return arrNewWords; } var unigrams = uniqueWords(this.body) ; emit(this.author {unigrams:unigrams}); }; var reduce = function(keyvalues){ Array.prototype.uniqueMerge = function( a ) { for ( var nonDuplicates = [] i = 0 l = a.length; i<l; ++i ) { if ( this.indexOf( a[i] ) === -1 ) { nonDuplicates.push( a[i] ); } } return this.concat( nonDuplicates ) }; unigrams = []; values.forEach(function(i){ unigrams = unigrams.uniqueMerge(i.unigrams); }); return { unigrams:unigrams}; }; Third we actually run the map-reduce function. var result = db.stories.mapReduce( map reduce {query:{author:{$exists:true}mr_status:""inprocess""} out: {reduce:""authors_unigrams""} }); Fourth we mark all the records that have been considered for map-reduce in last run as processed by setting ""mr_status"" as ""processed"". db.stories.update({mr_status:""inprocess""}{$set:{mr_status:""processed""}}falsetrue); Optionally you can see the result collection ""authors_unigrams"" by firing following command. db.authors_unigrams.find();"
1033,A,"Strategies for keeping a Lucene Index up to date with domain model changes Was looking to get peoples thoughts on keeping a Lucene index up to date as changes are made to the domain model objects of an application. The application in question is a Java/J2EE based web app that uses Hibernate. The way I currently have things working is that the Hibernate mapped model objects all implement a common ""Indexable"" interface that can return a set of key/value pairs that are recorded in Lucene. Whenever a CRUD operation is performed involving such an object I send it via JMS queue into a message driven bean that records in Lucene the primary key of the object and the key/value pairs returned from the index( ) method of the Indexable object that was provided. My main worries about this scheme is if the MDB gets behind and can't keep up with the indexing operations that are coming in or if some sort of error/exception stops an object from being index. The result is an out-of-date index for either a sort or long period of time. Basically I was just wondering what kind of strategies others had come up with for this sort of thing. Not necessarily looking for one correct answer but am imagining a list of ""whiteboard"" sort of ideas to get my brain thinking about alternatives. Change the message: just provide the primary key and the current date not the key/value pairs. Your mdb fetches the entity by primary key and calls index(). After indexing you set a value ""updated"" in your index to the message date. You update your index only if the message date is after the ""updated"" field of the index. This way you can't get behind because you always fetch the current key/value pairs first. As an alternative: have a look at http://www.compass-project.org."
1034,A,"Counting number of Regex query matches in Document field Using Lucene I can figure out how to create a document put values in respected fields and then proceed to use a searcher to search the indexed document for matches. However I am now more concerned with the number of matches in a particular field of each document. Just knowing there is a match is fine but I would like to know how many times the pattern was found in the field. Example. Document doc = new Document(); doc.add(new Field(""TNAME"" ""table_one"" Field.Store.YES Field.Index.NOT_ANALYZED)); doc.add(new Field(""CNAME"" ""column_one"" Field.Store.YES Field.Index.NOT_ANALYZED)); doc.add(new Field(""DATA"" ""This would be the data found in this particular field of a single document"" Field.Store.NO Field.Index.ANALYZED)); If I wanted to preform a document search querying the ""DATA"" field to figure out the number of times ^d.* pattern is met how would I do so? (giving the result of 2 for the above document). OK I found a way to count the number terms that match a particular regex: IndexReader reader = IndexReader.open(directory); RegexTermEnum regexTermEnum = new RegexTermEnum(reader new Term(""field"" ""^d.*"") new JavaUtilRegexCapabilities()); However I am still at a loss as to how to search a full index and find the frequency of a regex pattern match in a field per document. I guess the two pieces of information I need are: 1) Which document in the index has 1 or more matches for the query. 2) The number of times that regex query was found in each document/field. Simple Answer found: IndexSearcher searcher = new IndexSearcher(directory); IndexReader reader = searcher.getIndexReader(); RegexTermEnum regexTermEnum = new RegexTermEnum(reader new Term( ""field"" ""d.*"") new JavaUtilRegexCapabilities()); do { System.out.println(""Next:""); System.out.println(""\tDoc Freq: "" + regexTermEnum.docFreq()); if (regexTermEnum.term() != null) { System.out.println(""\t""+regexTermEnum.term()); TermDocs td = reader.termDocs(regexTermEnum.term()); while(td.next()){ System.out.println(""Found ""+ td.freq()+"" matches in document "" + reader.document(td.doc()).get(""name"")); } } } while (regexTermEnum.next()); System.out.println(""End."");"
1035,A,How to store Search History I am building a set of 'Now-Trending' kind of visualizations to showcase the trending searches/ trending documents within my system. The idea to show the top queries that came to my system/ most viewed results etc. I was wondering what would be the most effective and scalable Java based backend for this. If it's a database what should be the schema like? Or is it wise to maintain this info within a Lucene index? Presently for the prototype I store them in a flat file in an unstructured format. A schema-less backend might be preferable if you plan on capturing data ad-hoc or are unsure of your data needs in the future. Additionally a scalable solution (horizontally) would support growth in the dataset. With regards to your question about whether to store this data in a search engine here's a great article going over that concept with some examples. http://www.elasticsearch.org/blog/2011/05/13/data-visualization-with-elasticsearch-and-protovis.html  You might try storing this kind of data in a key-value store such as Redis. Redis has efficient atomic methods for incrementing counters that you can use for accruing votes for queries.
1036,A,how to use Lucene highlighter with phraseQuery? How to use Lucene's Highlighter with phraseQuery ? I did a google search and I am getting confused with spanScorer QueryScorer and few things like that.. The Lucene version I am using is 3.0.3 My requirements are - Doing a multi field query - Doing a wildcard search - Doing a phrase query All the above needs to be highlighted. How do I achieve this ? So I found the answer to my own question... After a LOT of hair pulling and google searching I found this : http://www.gossamer-threads.com/lists/lucene/java-...g=highlight%20wildcard;#116172 Specifically setting up the QueryScorer with this : qs.setExpandMultiTermQuery(true); did the job of highlighting wildcard searches. As for the multi field stuff I did ask it to highlight looping through my array of fields. So that was ugly but works. Phrase query words with QueryScorer easily.
1037,A,How to know when Lucene Index generation process is completed I've a .net windows service which generates Lucene search indexes every night. I first get all the records from the database and add it to Lucene index using IndexWriter's AddDocument method and then call Optimize method before returning from the method. Since the records fetched are faily large indexing takes around 2-3 minutes to complete. As you already knowLucene generates intermediate segment files while it is generating the index and it compresses the whole index into 3 files when Optimize is called. Is there anyway I can know that this index generation process is finished by Lucene and index is avaialable for search? I need to know this because I want to call another method when process is completed. You generally don't need to run an optimize every time you generate indexes but if its only taking 2-3 minutes then I guess it isn't hurting anything. Our 82million record lucene/solr index is so large I'm scared to know how long an optimize would take. You can check for the existance of the write.lock file. http://wiki.apache.org/lucene-java/LuceneFAQ#head-733eab8f4000ba0f6c9f4ea052dea77d3d541857  I don't understand why you would need to know when Lucene finishes indexing. You can perform searches while Lucene is indexing. In fact I think you can search while it's optimizing. I personally do not like the idea of searching for the lock file. Can you not set a boolean and toggle it after you call writer.optimize()? You can need this to find out when a built index can be replicated so other machines will have a local copy for example.
1038,A,"Using Lucene to search for email addresses I want to use Lucene (in particular Lucene.NET) to search for email address domains. E.g. I want to search for ""@gmail.com"" to find all emails sent to a gmail address. Running a Lucene query for ""*@gmail.com"" results in an error asterisks cannot be at the start of queries. Running a query for ""@gmail.com"" doesn't return any matches because ""foo@gmail.com"" is seen as a whole word and you cannot search for just parts of a word. How can I do this? No one gave a satisfactory answer so we started poking around Lucene documentation and discovered we can accomplish this using custom Analyzers and Tokenizers. The answer is this: create a WhitespaceAndAtSymbolTokenizer and a WhitespaceAndAtSymbolAnalyzer then recreate your index using this analyzer. Once you do this a search for ""@gmail.com"" will return all gmail addresses because it's seen as a separate word thanks to the Tokenizer we just created. Here's the source code it's actually very simple: class WhitespaceAndAtSymbolTokenizer : CharTokenizer { public WhitespaceAndAtSymbolTokenizer(TextReader input) : base(input) { } protected override bool IsTokenChar(char c) { // Make whitespace characters and the @ symbol be indicators of new words. return !(char.IsWhiteSpace(c) || c == '@'); } } internal class WhitespaceAndAtSymbolAnalyzer : Analyzer { public override TokenStream TokenStream(string fieldName TextReader reader) { return new WhitespaceAndAtSymbolTokenizer(reader); } } That's it! Now you just need to rebuild your index and do all searches using this new Analyzer. For example to write documents to your index: IndexWriter index = new IndexWriter(indexDirectory new WhitespaceAndAtSymbolAnalyzer()); index.AddDocument(myDocument); Performing searches should use the analyzer as well: IndexSearcher searcher = new IndexSearcher(indexDirectory); Query query = new QueryParser(""TheFieldNameToSearch"" new WhitespaceAndAtSymbolAnalyzer()).Parse(""@gmail.com""); Hits hits = query.Search(query); I'd also pass the tokenizer through a LowerCaseFilter or perhaps a customized LowerCaseFilter that would only lowercase the @GmAil.COM token  I see you have your solution but mine would have avoided this and added a field to the documents you're indexing called email_domain into which I would have added the parsed out domain of the email address. It might sound silly but the amount of storage associated with this is pretty minimal. If you feel like getting fancier say some domain had many subdomains you could instead make a field into which the reversed domain went so you'd store com.gmail com.company.department or ae.eim so you could find all the United Arab Emirates related addresses with a prefix query of 'ae.'  There also is setAllowLeadingWildcard But be careful. This could get very performance expensive (thats why it is disabled by default). Maybe in some cases this would be an easy solution but I would prefer a custom Tokenizer as stated by Judah Himango too.  You could a separate field that indexes the email address reversed: Index 'foo@gmail.com' as 'moc.liamg@oof' Which enables you to do a query for ""moc.liamg@*"" Hmm. That sounds really hackish."
1039,A,"Help needed figuring out reason for maxClauseCount is set to 1024 error I've two sets of search indexes. TestIndex (used in our test environment) and ProdIndex(used in PRODUCTION environment). Lucene search query: +date:[20090410184806 TO 20091007184806] works fine for test index but gives this error message for Prod index. ""maxClauseCount is set to 1024"" If I execute following line just before executing search query then I do not get this error. BooleanQuery.SetMaxClauseCount(Int16.MaxValue); searcher.Search(myQuery collector); Am I missing something here?Why am not getting this error in test index?The schema for two indexes are same.They only differ wrt to number of records/data.PROD index has got higher number of records(around 1300) than those in test one (around 950). Thanks for reading. chk http://wiki.apache.org/lucene-java/LuceneFAQ#Why_am_I_getting_a_TooManyClauses_exception.3F I had the same problem. My solution was to catch BooleanQuery.TooManyClauses and dynamically increase maxClauseCount. Here is some code that is similar to what I have in production. Good Luck Randy  private static Hits searchIndex(Searcher searcher Query query) throws IOException { boolean retry = true; while (retry) { try { retry = false; Hits myHits = searcher.search(query); return myHits; } catch (BooleanQuery.TooManyClauses e) { // Double the number of boolean queries allowed. // The default is in org.apache.lucene.search.BooleanQuery and is 1024. String defaultQueries = Integer.toString(BooleanQuery.getMaxClauseCount()); int oldQueries = Integer.parseInt(System.getProperty(""org.apache.lucene.maxClauseCount"" defaultQueries)); int newQueries = oldQueries * 2; log.error(""Too many hits for query: "" + oldQueries + "". Increasing to "" + newQueries e); System.setProperty(""org.apache.lucene.maxClauseCount"" Integer.toString(newQueries)); BooleanQuery.setMaxClauseCount(newQueries); retry = true; } } } Nanshi the reason is because it *cannot* be calculated. There is no way to find out how many items are included in the range until *after* you do a query but then that query is transformed into a Boolean Query where the exception is thrown. This is a good solution but why not count the clauses before then and set it correctly instead of after catch the exception? Will this affect performance if a big amount of data being processed? Thanks Randy! I have a difference case that i constructed Boolean queries myself so i was able to count it before running a search. Where would I put this code?  The range query essentially gets transformed to a boolean query with one clause for every possible value ORed together. For example the query +price:[10 to 13] is tranformed to a boolean query +(price:10 price:11 price:12 price:13) assuming all the values 10-13 exist in the index. I suppose all of your 1300 values fall in the range you have given. So boolean query has 1300 clauses which is higher than the default value of 1024. In the test index the limit of 1024 is not reached as there are only 950 values. The downside is the performance of query degrades with the count of unique timestamps. But it is not that bad. You can try it out and check if the perfromance is acceptable. You should mostly be fine. Lucene 2.9 (Java) has improved range queries dramatically. I am not sure when this will be ported to .Net version. Meanwhile there are other tricks you can use for date queries. Typically it involves breaking date into year month and day. This needs lot of work to translate user query to the underlying lucene format. Try searching for ""lucene date query"" to get interesting ideas. Thanks Shashikant for your answer.What is the solution to resolve this issue? BooleanQuery.SetMaxClauseCount(Int16.MaxValue); is supposedly a very expensive call. Thanks. In the meantime you can design your date field differently - could you restrict it to days in a single year? (thus restricting it to 365 values)? Or split a data into year month and day and use a more complex query? I know this is inelegant but it may work."
1040,A,Meaning of Fuzzy Parameter in Lucene As stated in the Lucene documentation there is a parameter that enables for specifying similarity required for a match. The value of is between 0 and 1 with a value closer to 1 only terms with a higher similarity will be matched. For example: roam~0.8 Know I wonder whether this parameter is meant in a relative sence i.e. for a string that is longer the string edit distance might be higher and there is still a match. Or is this a meant as an absolute value i.e. only up to x substitutions/deletion/insertions are allowed to make a match happen? A search for term~sim will find all terms which have an edit distance of less than length(term) * (1- sim). So roam~0.8 will find all terms with an edit distance of less than 4*(1-.8)=.8 of roam. EDIT: The term must be longer than 1/(1 - sim). So a search for roam~.8 won't do anything fuzzy because things with a similarity of .8 must have a length of at least 5. Thanks. And do you know what are the cost for substitutions/deletion/insertions are they equally 1 as suggested by Levenshtein or are they slightly different? @bertolami: yes it is an exact Levenshtein implementation. You can see the method `Similarity` in the `FuzzyTermEnum` class for the code.
1041,A,Is there a quick solution to query a Lucene index from Node.js? I'm developing a server with node.js and would like to re-use the index files I generated with Lucene. Do you guys know a quick way to query this index from node.js? there is a node-solr module https://github.com/gsf/node-solr @generalhenry you should post that as an answer! there is a node-solr module github.com/gsf/node-solr
1042,A,"Lucene 3 iterating over all hits I'm in the process of updating a tool that uses a Lucene index. As part of this update we are moving from Lucene 2.0.0 to 3.0.2. For the most part this has been entirely straightforward. However in one instance I cant seem to find a straightforward conversion. Basically I have a simple query and I need to iterate over all hits. In Lucene 2 this was simple e.g.: Hits hits = indexSearcher.search(query); for(int i=0 ; i<hits.length() ; i++){ // Process hit } In Lucene 3 the API for IndexSearcher has changed significantly and although I can bash together something that works it is only by getting the top X documents and making sure that X is sufficiently large. While the number of hits (in my case) is typically between zero and ten there are anomalous situation where they could number much higher. Having a fixed limit therefor feels wrong. Furthermore setting the limit really high causes OOME which means that space for all X possible hits is allocated immediately. As this operation is carried out alot something reasonably efficient is desired. Edit: Currently I've got the following to work: TopDocs hits = indexSearcher.search(query MAX_HITS); for (int i=0 ; i<hits.totalHits ; i++) { // Process hit } This works fine except that a) what if there are more hits then MAX_HITS ? and b) if MAX_HITS is large then I'm wasting memory as room for each hit is allocated before the search is performed. As most of the time there will only be a few hits I don't mind doing follow up searches to get the subsequent hits but I cant seem to find a way to do that. IndexSearcher has a method docFreq(Term). Invoking it does not seem to have a performance penalty and its output is then a suitable input parameter for the number of documents to get. E.g. int freq = searcher.docFreq(new Term(FIELD value)); TopDocs hits = indexSearcher.search(query freq); for (int i=0 ; i<hits.totalHits ; i++) { // Process hit } This works because my query is essentially a TermQuery. If it was a more complex query then this wouldn't be suitable.  Why don't you use Searcher.search(Query query int n) ? You can specify the number of results you want back and you can use the TopDocs object that is returned to iterate through the results. Using Hits to process long result sets was a bad idea because in the background the hits object would run more searches to fill in results that it didn't already have. TopDocs only contains ids and scores so you shouldn't have a memory problem even for large n. That is basically what I'm currently doing. But what if I need result number n+1? Just ask for N + M where M is some kind of constant value. I think you're worrying too much about memory here; TopDocs only contains scores and id's which is almost no memory at all even for large N. If you don't believe me run a profiler to find out. I think the ""problem"" with Hits was more that it _didn't_ run the additional queries in the background. If it had maybe it wouldn't have performed so poorly.  How about using NumDocs from the index reader as the maximum number of results. Do watch out for the edge case of zero documents in the index though... Hope this helps  @Kris - I ran into this issue as well this worked for me. Try this: TopDocs tp = ms.search(query 1); TopDocs hits = indexSearcher.search(query tp.totalHits); for (int i=0 ; i<hits.totalHits ; i++) { // Process hit } According to Uwe in the link below tp.totalHits "".. will still count all hits but return only 1. "" See full details in link from java-user lucene apache mail archives - http://www.gossamer-threads.com/lists/lucene/java-user/95032 Could you please paste here the relevant part of the solution instead of just a link? Thanks"
1043,A,Any good way to handling repeats when using Lucene indexing? I am using Lucene to index my documents. In my case each document is rather in small size but having a large quantity (~2GB). And in each document there are many repeating words or terms. I am wondering if it is the right way for me to do index using Lucene or what preprocessing I should do on the document before indexing. The following are a couple of examples of my documents (each column is a field the first row is the field name and starting from 2nd row each row is one document): ID category track keywords id1 cat1 track1 mode=heat treatment;repeat=true;Note=This is an apple id2 cat1 track2 mode=cold treatment;repeat=true;Note=This is an orange I want to index all documents perform a search on the 3 fields (category track and keywords) and return the unique id1. If I directly index this will the repeating terms affect the searching performance? Do you have a good idea how I should do the indexing and searching? Thanks a lot in advance. Repeated terms may affect the search performance by forcing the scorer to consider a large set of documents. If you have terms that are not that discriminating between documents I suggest preprocessing the documents in order to remove these terms. However you may want to start by indexing everything (say for a sample of 10000-20000 documents) and see how you fare with regard to relevance and performance. From the way you describe this you will need to index the category track and keywords fields maybe using a KeywordAnalyzer for the category and track fields. You only need to store the id field. You may want a custom analyzer for the keywords field or alternatively to preprocess it before the actual indexing. +1 and I second bajafresh4life's comment +1 for try indexing everything first then optimize later. 2GB is not that much data and Lucene is pretty fast
1044,A,"Lucene - how to make multiple words within query AND instead of OR together I am using Zend Search Lucene and if the user types in Vibrant Bouquet into the search box results with the word ""Vibrant"" or the word ""bouquet"" (or both obviously) are returned. What I want is only to return those results with both words. Obviously I can do this by typing AND between the words but as far as I can tell Lucene implicitly puts OR between each word as it is; is it possible to change this so that it implicitly puts AND between each word so that the default behaviour for searches is to find all words not just one or more? I could do this with a string replace on the search term to replace spaces with AND but that could cause issues with stopping the user typing in more complex queries. I've searched google for you : The default boolean operator may be set or retrieved with the Zend_Search_Lucene_Search_QueryParser::setDefaultOperator($operator) and Zend_Search_Lucene_Search_QueryParser::getDefaultOperator() methods respectively. These methods operate with the Zend_Search_Lucene_Search_QueryParser::B_AND and Zend_Search_Lucene_Search_QueryParser::B_OR constants. http://framework.zend.com/manual/en/zend.search.lucene.query-language.html Thanks I was looking on the query construction api page.  I try this code and this not work: Zend_Search_Lucene_Search_QueryParser::setDefaultOperator('AND'); but this code work: Zend_Search_Lucene_Search_QueryParser::setDefaultOperator(Zend_Search_Lucene_Search_QueryParser::B_AND); and for more flexibility: $term='zand search'; Zend_Search_Lucene_Search_QueryParser::setDefaultOperator(Zend_Search_Lucene_Search_QueryParser::B_AND); $query = Zend_Search_Lucene_Search_QueryParser::parse($term.""*""'UTF-8'); $results = $index->find($query ); in result for example: (some world)zend search(some world) zend search lucence for get more flexcibility use Query Language: Query Language"
1045,A,"How to make Lucene match all words in query? I am using Lucene to allow a user to search for words in a large number of documents. Lucene seems to default to returning all documents containing any of the words entered. Is it possible to change this behaviour? I know that '+' can be use to force a term to be included but I would like to make that the default action. Ideally I would like functionality similar to Google's: '-' to exclude words and ""abc xyz"" to group words. Just to clarify I also thought of inserting '+' into all spaces in the query. I just wanted to avoid detecting grouped terms (brackets quotes etc) and potentially breaking the query. Is there another approach? The behavior is hard-coded in method addClause(List int int Query) of class org.apache.lucene.queryParser.QueryParser so the only way to change the behavior (other than the workarounds above) is to change that method. The end of the method looks like this: if (required && !prohibited) clauses.addElement(new BooleanClause(q BooleanClause.Occur.MUST)); else if (!required && !prohibited) clauses.addElement(new BooleanClause(q BooleanClause.Occur.SHOULD)); else if (!required && prohibited) clauses.addElement(new BooleanClause(q BooleanClause.Occur.MUST_NOT)); else throw new RuntimeException(""Clause cannot be both required and prohibited""); Changing ""SHOULD"" to ""MUST"" should make clauses (e.g. words) required by default.  This looks similar to the Lucene Sentence Search question. If you're interested this is how I answered that question: String defaultField = ...; Analyzer analyzer = ...; QueryParser queryParser = new QueryParser(defaultField analyzer); queryParser.setDefaultOperator(QueryParser.Operator.AND); Query query = queryParser.parse(""Searching is fun""); +1 I have been looking for this and glad that I found it in SO  Like Adam said there's no need to do anything to the query string. QueryParser's setDefaultOperator does exactly what you're asking for.  Why not just preparse the user search input and adjust it to fit your criteria using the Lucene query syntax before passing it on to Lucene. Alternatively you could just create some help documentation on how to use the standard syntax to create a specific query and let the user decide how the query should be performed.  Lucene has a extensive query language as described here that describes everything you want except for + being the default but that's something you can simple handle by replacing spaces with +. So the only thing you need to do is define the format you want people to enter their search queries in (I would strongly advise to adhere to the default Lucene syntax) and then you can write the transformations from your own syntax to the Lucene syntax."
1046,A,"Lucene nightly build javadoc? Can I read the javadoc for Lucene nightly build 4.0 ? thanks Er. google for ""lucene nightly build javadoc"". http://lucene.apache.org/java/docs/api/index.html JA Hm. http://www.gossamer-threads.com/lists/lucene/java-user/118545"
1047,A,"influencing solr search results with a field value I've recently started experimenting with solr. My data is indexed and searchable. My problem is in the sorting. I have 3 fields: Author Title Sales. What I would like to do is search against the author & title fields but have the sales value influence the score so that matches with higher sales move toward the top even if the initial match score is not the highest. Simply sorting by sales does not produce valid results as a result with a near 0 score for the search term but a lot of sales in general could end up above a perfect match for the term that has never been sold. What I am seeing is results that while great term matches are not necessarily the product I want showing at the top of the list. Any help is greatly appreciated! You can also use Index-time boosting. And here's detailed info on using function queries to influence scoring.  If you're using the dismax handler you can add a boost function (bf) with the field you want to boost on e.g. http://...?q=foo&bf=""fieldValue(sales)^1.5"" ...to make the value of the sales figure give a bump. You can of course make the function more complex if you want to munge the sales data in some way. More info is easily found. You may also just want to do this at index time since the sales data isn't going to be changing on the fly."
1048,A,"Umbraco Examine - how to sort search results? I am trying to sort my search results by a custom Umbraco property I have created - let's call it sortDate. Inside my IndexSet in config/ExamineIndex.config I have this: <IndexUserFields> <add Name=""sortDate"" EnableSorting=""true"" Type=""DateTime"" /> ... In my Search user control I am constructing a criteria and filter and using them to search like so: var criteria = ExamineManager.Instance.SearchProviderCollection[""MySearcher""].CreateSearchCriteria( UmbracoExamine.IndexTypes.Content); var filter = criteria.GroupedOr(new string[] { ""sortDate"" ""someThing"" ""someThingElse"" ""bodyText"" } SearchTerm.ToLower()).Compile(); var MySearchResults = ExamineManager.Instance.SearchProviderCollection[""MySearcher""].Search(filter).Distinct(); I'm guessing I need to add something to specify how Lucene should sort this on my filter? This is Umbraco 4.6.1 if that matters :) OK not sure how I missed this but it looks like you can just do: filter.OrderBy( new string[] { ""sortDate"" } ); Thanks Elwyn earlier one of the post suggested to use _ or __ infront of name. But without any prefix I can see the results sorted as needed. Datetime value is converted to a numeric value for sorting i.e. 20121029134700 etc."
1049,A,Can I integrate Solr with Sharepoint with out using Lucene Connector Framework Can I integrate Solr with Sharepoint with out using Lucene Connector Framework. if so should I make Solr Index the Sharepoint's underlying database ? Will this produce successful search results ? To get text into Solr you need to be able to extract it. The Lucene Connector Framework is meant to do just that. If you somehow get the raw text another way you may use the DataImportHandler to import information. If you index text correctly and configure the schema well you should be able to get successful search results. Where does Apache Tika filter fit in here ? I am not sure that it does. Looking at Tika's supported document formats: http://tika.apache.org/formats.html it does support several Microsoft document formats. I do not know which of these are relevant to Sharepoint. Ideally Tika lets you extract text and meta-data out of a file in a supported format so you can tailor the repository handling (file system directories crawling) yourself. But why not use the LCF? Yes i went ahead with using LCFwill LCF take care of full text search ? AFAIU LCF handles getting the text into Lucene or Solr. From there on you need to handle the searches yourself which is not hard (I guess easier with Solr as most things are). See this presentation about LCF from a talk given last week: http://lucene-eurocon.org/slides/Lucene-Connectors-Framework-Introduction_Karl-Wright.pdf
1050,A,"lucene get matched terms in query what is the best way to find out which terms in a query matched against a given document returned as a hit in lucene? I have tried a weird method involving hit highlighting package in lucene contrib and also a method that searches for every word in the query against the top most document (""docId: xy AND description: each_word_in_query""). Do not get satisfactory results? hit highlighting does not report some of the words that matched for a document other than the first one. i am not sure if the second approach is the best alternative. Not tried yet but have a look at the implementation of org.apache.lucene.search.highlight.QueryTermExtractor.  The method explain in the Searcher is a nice way to see which part of a query was matched and how it affects the overall score. Example taken from the book Lucene In Action 2nd Edition: public class Explainer { public static void main(String[] args) throws Exception { if (args.length != 2) { System.err.println(""Usage: Explainer <index dir> <query>""); System.exit(1); } String indexDir = args[0]; String queryExpression = args[1]; Directory directory = FSDirectory.open(new File(indexDir)); QueryParser parser = new QueryParser(Version.LUCENE_CURRENT ""contents"" new SimpleAnalyzer()); Query query = parser.parse(queryExpression); System.out.println(""Query: "" + queryExpression); IndexSearcher searcher = new IndexSearcher(directory); TopDocs topDocs = searcher.search(query 10); for (int i = 0; i < topDocs.totalHits; i++) { ScoreDoc match = topDocs.scoreDocs[i]; Explanation explanation = searcher.explain(query match.doc); System.out.println(""----------""); Document doc = searcher.doc(match.doc); System.out.println(doc.get(""title"")); System.out.println(explanation.toString()); } } } This will explain the score of each document that matches the query. could you give some example code does it work with fuzzy matching also. I want to find out the terms in query so if ""dogs"" matches with ""dog"" in the query. I want to identify that it was the term ""dog"" in the query that matched."
1051,A,SOLR and Natural Language Parsing - Can I use it? hey guys my requirements are pretty similar to this: Requirements http://stackoverflow.com/questions/90580/word-frequency-algorithm-for-natural-language-processing Using Solr While the answer for that question is excellent I was wondering if I could make use of all the time I spent getting to know SOLR for my NLP. I thought of SOLR because: It's got a bunch of tokenizers and performs a lot of NLP. It's pretty use to use out of the box. It's restful distributed app so it's easy to hook up I've spent some time with it so using could save me time. Can I use Solr? Although the above reasons are good I don't know SOLR THAT well so I need to know if it would be appropriate for my requirements. Ideal Usage Ideally I'd like to configure SOLR and then be able to send SOLR some text and retrieve the indexed tonkenized content. Context So you guys know I'm working on a small component of a bigger recommendation engine. I guess you can use Solr and combine it with other tools. Tokenization stop word removal stemming and even synonyms come out of the box with Solr. If you need named entity recognition or base noun-phrase extraction you need to use OpenNLP or an equivalent tool as a pre-processing stage. You will probably need term vectors for your retrieval purposes. Integrating Apache Mahout with Apache Lucene and Solr may be useful as it discusses Lucene and Solr integration with a machine learning (including recommendation) engine. Other then that feel free to ask further more specific questions. thanks yuval great answer. I've had a chat with my superiors and for whatever reason we can't have Java running on the server. So for my purposes using the underlying lucene library (the .NET Port) would server me just as well right? cheers! Thanks Andy. Sure you can use Lucene.Net. The core functionality is there. It is always a little behind the Java version though but I guess you can do a lot with it. As to Lucene versus Solr using bare Lucene requires handling a lot more integration and configuration details. Please see this question: http://stackoverflow.com/questions/1749314/is-solr-available-for-net and this blog post: http://www.lucidimagination.com/blog/2010/05/26/migrating-from-lucene-to-solr/  There is a special request handler designed to apply parsing to filter our less relevant search results. It is based on machine learning of constituency parse trees obtained by OpenNLP. Please see the blog http://search-engineering.blogspot.com and the paper http://dx.doi.org/10.1016/j.datak.2012.07.003 This SOLR search request handler will be available as a part of OpenNLP Similarity component  You can actually configure Solr to use NLP algorithms both when indexing documents and at search time. The first phase (indexing time) can be done using/writing Solr UpdateRequestProcessor plugins for analyzing fields texts while the second phase can be implemented writing a custom QParserPlugin which analyzes the query hit by the user. I've presented an approach for implementing natural language search in Solr at Lucene Eurocon 2011 which takes advantage of Apache UIMA for running (open source) NLP algorithms. You can take a look at the slides and at the video of the talk. Hope this helps. Tommaso  In this Google code project http://code.google.com/p/relevance-based-on-parse-trees you can use the linguistic-based request handler in the package opennlp.tools.similarity.apps.solr public class SyntGenRequestHandler extends SearchHandler where the search results obtained by SearchHandler are re-ranked based on similarity of parse trees.
1052,A,"decreasing memory usage of indexing a Lucene document Right now my documents in lucene can have very very large values in one field (from 0 to say hundreds of MB). I am using Lucene 3.1.0 I create documents like this: doc = new Document(); Field field = new Field(fieldname VERYLARGEVALUE store tokenize storevector); doc.add(field); Where VERYLARGEVALUE is a String in memory. I am thinking that maybe writing VERYLARGEVALUE to a file while it is being created (it is created by extracting text from a number of sources so it is incremental) and then using: Field field = Field(String name Reader reader Field.TermVector termVector); doc.add(field); Where reader reads from the File I wrote VERYLARGEVALUE to. Will this decrease the memory requirement or VERYLARGEVALUE will be eventually read to memory sooner or later? java.io.Reader implementations were designed to efficiently read character streams by reading portions of the stream into memory. (See the read(char[] cbuf) API.) So I'd say ""yes"" using a Reader would decrease your memory overhead surely but here the devil is in how the Reader is used when reading the doc and later indexing it. If the whole String is read into memory at some point it is no good to me in my case. The Reader content seems to be getting added to the index. So long as you call commit() frequently (and don't use a memory based index) memory requirements should be manageable.  Looking through the Lucene code the Reader you pass into Field ultimately gets passed to the TokenStream that tokenizes your data (namely in DocInverterPerField). So your plan should definitely save memory since it'll stream directly from that reader to do its indexing. You'll like want to use a BufferedReader on top of the FileReader for better performance."
1053,A,"SOLR/LUCENE Experts please help me design a simple keyword search from PDF index? I dabbled with solr but couldn't figure out a way to tailor it to my reuqirement. What I have : A bunch of PDF files. A set of keywords. What I am trying to achieve : Index the PDF files (solrcell - done) Search for a keyword (works ok) Tailor the output to spit out the names of the PDF files an excerpt where the keyword occurred (No clue/idea how to) Tried manipulating ResponseHandler/Schema.xml/Solrconfig.xml to no avail. Lucene/solr experts do you think what I am trying to achieve is possible? I put my existing code on github @ https://github.com/ThinkCode/solr_search (which is mostly solr's default example with minor modifications to the fields (all the content is stored in one content field). Notable changes in schema.xml being : Schema.xml : <solrQueryParser defaultOperator=""AND""/> <field name=""id"" type=""string"" indexed=""true"" stored=""true"" required=""true"" /> <field name=""content"" type=""text_general"" indexed=""true"" stored=""true"" multiValued=""true"" termVectors=""true"" termPositions=""true"" termOffsets=""true""/> <dynamicField name=""*"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true"" termVectors=""true"" termPositions=""true"" termOffsets=""true""/> <solrQueryParser defaultOperator=""AND""/> <copyField source=""*"" dest=""content""/> Current Output : (query) http://localhost:8983/solr/select/?q=Java+Servlet&version=2.2&start=0&rows=10&indent=on <response><lst name=""responseHeader""><int name=""status"">0</int><int name=""QTime"">13</int><lst name=""params""><str name=""indent"">on</str><str name=""start"">0</str><str name=""q"">Java Servlet</str><str name=""version"">2.2</str><str name=""rows"">10</str></lst></lst> <result name=""response"" numFound=""1"" start=""0""><doc><arr name=""content_type""><str>application/pdf</str></arr><str name=""id"">tutorial.pdf</str><str name=""subject"">Solr</str><arr name=""title""><str>Solr tutorial</str></arr></doc></result></response> What I am looking for is 'extracted fragment (line) where the keyword was found'. In the query provided I search for 'Java Servlet' and it returned the document. I am interested in the context 'Solr can run in any Java Servlet Container of your choice' to be returned in the output xml. I put the code on github @ https://github.com/ThinkCode/solr_search and the schema file is at https://github.com/ThinkCode/solr_search/blob/master/apachesolr330/example/solr/conf/schema.xml ok cool......... I don't mean to be rude but you'll have to be much more specific than this... otherwise it's a ""plz send me the codez / do my job for free"" kind of question which is not welcome on stackoverflow. Yes it's possible. Can you post what you have so far or where concretely you're having trouble? I updated the question with a sample. I am not looking for someone who can do the job for me! I am looking for hints/leads which will help me research in the right direction. Its been less than a week since I stumbled upon solr. Thanks! To get snippets of text around the matched keywords see http://wiki.apache.org/solr/HighlightingParameters To get the filename of the indexed PDF as part of the response simply add a field with that information (it should be a string field non-indexed stored). Of course you have to populate this new field at index-time.  A standalone solution using PDF Box and Apache Lucene is available at: * https://github.com/WolfgangFahl/pdfindexer It will create a HTML file with links to the corresponding pages in the PDF file where the keywords were found."
1054,A,"Zend_Search_Lucene Help EDIT: I have managed to solve the problem by using: +""lorem ipsum"" +type:photo +""lorem ipsum"" +type:video Another problem though is that the index is returning correct results but with wrong id (id is a primary key). More specifically id fields returned are 1 less than real ids (id - 1) in the database which I use to build the index. That's very strange. What's wrong with these search queries: ""lorem ipsum"" AND +type:photo ""lorem ipsum"" AND +type:video First query is supposed to find only results with type = photo second one searches only videos. But they are both returning both photos and videos. Here is how I build the index:  // create media index $index = Zend_Search_Lucene::create('/data/media_index'); // get all media $media = $this->_getTable('Media')->get(); // iterate through media and build index foreach ($media as $m) { $doc = new Zend_Search_Lucene_Document(); $doc->addField(Zend_Search_Lucene_Field::UnIndexed('id' $m->id)); $doc->addField(Zend_Search_Lucene_Field::UnIndexed('thumb_path' $m->thumb_path)); $doc->addField(Zend_Search_Lucene_Field::Keyword('title' $m->title)); $doc->addField(Zend_Search_Lucene_Field::UnStored('description' $m->description)); $doc->addField(Zend_Search_Lucene_Field::Keyword('type' $m->type)); $index->addDocument($doc); } // commit the index $index->commit(); And here is how I search it:  $index = Zend_Search_Lucene::open('/data/media_index'); $this->view->photos = $index->find('""lorem ipsum"" AND +type:photo'); $this->view->videos = $index->find('""lorem ipsum"" AND +type:video'); Any ideas? I just ran some tests on my own search index and the problem seems to be in the query itself and not the code. The ""AND"" in the query is an operator and so is the ""+"". The query parser seems to be confused by the double operator logic with no term between. This was a block quote I found in their docs: If the AND/OR/NOT style is used then an AND or OR operator must be present between all query terms. Each term may also be preceded by NOT operator. The AND operator has higher precedence than the OR operator. This differs from Java Lucene behavior. Now running your query through the parser this was the Search_Query object: string '""lorem ipsum"" AND +type:photo' (length=29) object(Zend_Search_Lucene_Search_Query_MultiTerm)[230] private '_terms' => array 0 => object(Zend_Search_Lucene_Index_Term)[236] public 'field' => null public 'text' => string 'lorem' (length=5) 1 => object(Zend_Search_Lucene_Index_Term)[237] public 'field' => null public 'text' => string 'ipsum' (length=5) 2 => object(Zend_Search_Lucene_Index_Term)[238] public 'field' => null public 'text' => string 'and' (length=3) 3 => object(Zend_Search_Lucene_Index_Term)[239] public 'field' => null public 'text' => string 'type' (length=4) 4 => object(Zend_Search_Lucene_Index_Term)[240] public 'field' => null public 'text' => string 'photo' (length=5) Changing the query up a bit removing the AND or removing the + and only using 1. string '""lorem ipsum"" +type:photo' (length=25) string '""lorem ipsum"" AND type:photo' (length=28) object(Zend_Search_Lucene_Search_Query_Boolean)[227] private '_subqueries' => array 0 => object(Zend_Search_Lucene_Search_Query_Phrase)[230] private '_terms' => array 0 => object(Zend_Search_Lucene_Index_Term)[233] public 'field' => null public 'text' => string 'lorem' (length=5) 1 => object(Zend_Search_Lucene_Index_Term)[234] public 'field' => null public 'text' => string 'ipsum' (length=5) 1 => object(Zend_Search_Lucene_Search_Query_Term)[235] private '_term' => object(Zend_Search_Lucene_Index_Term)[232] public 'field' => string 'type' (length=4) public 'text' => string 'photo' (length=5) The only difference: AND:  private '_signs' => array 0 => boolean true 1 => boolean true +:  private '_signs' => array 0 => null 1 => boolean true The AND operator requires that both of the search queries are required in the result where as the + only requires the value on the right be required. So just change up the query to ""lorem ipsum"" AND type:photo And you should get the results you are looking for. I think that did it. There's one more problem though the search is returning results starting with id = 0 (id is a primary key) and id in the database starts with 1. It seems like the index is returning id - 1. Why is that? I am creating the index correctly as far as I can see. I have edited my first post. Check out this post: http://stackoverflow.com/questions/674817/zend-framework-lucene-boolean-google-like-search I didn't realize it either but it appears ID is a reserved word for lucene. In my link builder I am using SLUGs to link to them a URL-Encoded version of the title. So like the post below it says ID must be used by the lucene index already try just renaming the Keyword in your index to `media_id` or something along those lines.  About hte ""id problem"" i would guess that ""id"" is internal variable used to access each result. So I would recommend to rename the field to sth. like ""entryId"" and then use $resultItem->entryId Yeah that was it :) Thanks"
1055,A,"Disabling scoring in Lucene(.NET) When searching is there a way to disable scoring for any query? The scenario is that the user refines his query by trying different combinations of words phrases etc. and needs realtime (well reasonably fast at least) responses on the number of hits. Search time slows down a lot when there are millions of hits due to scoring but the user really doesn't care about all these documents. As soon as he sees there are 1M+ hits he will start adding additional words to the query. A ""Sort by relevance"" option would allow him to do this quickly while turning scoring back on when the number of hits is reasonable. Is this possible? I'm using Lucene.NET 2.9.2 but AFAIK it is identical to the Java version. In Lucene 2.9 you can use a custom Collector that can do what you want (get hits without scoring). http://hudson.zones.apache.org/hudson/job/Lucene-trunk/javadoc/all/org/apache/lucene/search/Collector.html That doesn't surprise me. In order to find the # of docs for a query Lucene has to do the work of looking up the docs that matched the terms. If real-time performance is really important for you maybe look into splitting your indexes and searching them in parallel Thanks for info. I tried creating a NullCollector (does nothing) and it does indeed work. Unfortunately the search time is the same as with the default collector. The time is not spent in the collector it is spent in the TermScorer which still calls my NullCollector for every document in the result set.  Try ConstantScoreQuery. This only returns the hits without scoring them. It works.. and its the easiest solution too.."
1056,A,How Lucene reduces CPU usage over full-text search of sql server? I had read article about how stackoverflow reduced its CPU usage by using lucene. But my question is how ? Is it due to caching of lucene ? and if yes. If we implement a sqlserver fts with memcache. will it be same as lucene ? or does Lucene uses different data structures for search ? Lucene is using indexing and full text search - it's more than caching. SQL is a set-based relational language. It's not built for ad hoc queries of documents. The technology is completely different. @dyffymo Is using fts of sqlserver/ sqlite http://www.sqlite.org/draft/fts3.html will not create index and full-text search ? Is that the version SO is using? The fact that SO explicitly said they were using Lucene suggests to me that there was no other way for them to get the functionality it provides. Your citation means adding what Lucene does into a SQL relational engine. It's not native to the technology. @dyffymo ok  thanks. why I asked was that I need to choose one from database fts over lucene in my project. so I thought that lucene will reduce cpu usages over sqlite fts search.
1057,A,"Can I use Lucene for business application search? I have a typical enterprise/business application that I am developing that includes orders salespersons contacts reference data etc... There will be at least 100 or more users on the system at a time who are entering new data changing data etc. I need to provide search capability across the application for almost all tables. One option is to do table queries such as ""select * from salespersons where name contains 'searchtest'"" or something similar. But I was wondering if I can use Lucene(.net) for this instead. The main thing is that the search needs to reflect changes within a few seconds. So if a user enters a order for example and then immediately searches for it right after then it needs to show up in the search list. (i.e. I can't have an index job every hour or half hour or nightly etc). Is this something that would work well or is there a better option? Yes you certainly can use Lucene for this use case. I see some downsides: You'll be replicating much of the information in the index (and you'll have implement something to keep the index and database in synch which might not be trivial.) You'll be hitting the database very often (or be delaying the inserts or just creating more load depending on the way you choose to build it) to build this index. Near realtime search is implemented only in the latest version of official Lucene. I'm not aware of the status of Lucene.net at this respect. And a (big) upside: Lucene will most likely outperform in both performance and results quality the database fulltext indexing. The answers to this question might help http://stackoverflow.com/questions/1002255/lucene-net-best-practices  I have implemented something almost identical to what you describe. The table to be indexed was huge (>5 hours to index with lucene) and the requirement was that the search would reflect changes in the DB within 5 minutes. There are two approaches I considered (I implemented the first one): Index the table incrementally. Every row had a timestamp (last modified). Every 5 minutes a cron job would start a java process that read the rows modified since the last run create a plain-text version of them and then update the lucene index. The incremental indexing would lock the table for 200-300 msces for about 1000 table rows. Obviously this depends on your system database schema etc. However my experience is that it is definitely practical to implement this. And the search operations are orders-of-magnitude faster with lucene than with the query. Use a dedicated thread to do the indexing. Whenever something changes in the DB the code that actually runs the SQL query should send a message (through a LinkedBlockinQueue) to the thread that updates the lucene index. That way your updateDB() method at the main thread returns immediately after the DB has been updated and does not have to wait for the lucene indexing process whereas the indexing happens as soon as possible (usually a few msecs later). One downside with this is that lucene uses locks stored in the disk. So I assume there is an overhead of updating the indexing for every single row (I haven't run any benchmark though). A workaround would be to keep a buffer of updates on your indexing thread and flush them to disk every few seconds (again the performance of this depends on the ratio of updates vs searches on the index)"
1058,A,"How to set Lucene standard analyzer for PhraseQuery search? I'm under the impression from a variety of tutorials out there on Lucene that if I do something like: IndexWriter writer = new IndexWriter(indexPath new StandardAnalyzer(Version.LUCENE_CURRENT) true IndexWriter.MaxFieldLength.LIMITED); Document doc = new Document(); Field title = new Field(""title"" titlefield Field.Store.YES Field.Index.ANALYZED); doc.add(title); writer.addDocument(doc); writer.optimize(); writer.close(); IndexReader ireader = IndexReader.open(indexPath); IndexSearcher indexsearcher = new IndexSearcher(ireader); Term term1 = new Term(""title"" ""one""); Term term2 = new Term(""title"" ""two""); PhraseQuery query = new PhraseQuery(); query.add(term1); query.add(term2); query.setSlop(2); that Lucene should return all queries for the title field containing ""one"" and ""two"" within 2 words of each other. But I don't get any results because I'm not using the StandardAnalyzer to search. How can do a proximity search in Lucene then? Does the following queryParser allow for proximity searches (using the tilde?) QueryParser queryParser = new QueryParser(""title""new StandardAnalyzer()); Query query = queryParser.parse(""test""); yes when you parse a query using QueryParser you will be able to do proximity searching. In general it is always recommended to use the same analyser for indexing and searching. BR Chris"
1059,A,"Lucene query returns things I'm not expecting I'm querying a Lucene index file which structure I didn't build. This index contains Documents with fields structured like this: As you can see the 'type' field is always empty however the 'all' field contains data formatted in a way so that is searchable and it contains a type=ta sort of syntax. The weird thing is that when I query this index using type:ta it actually outputs something even though the type field is always empty. What's happening here? EDIT After googling a bit more I found out a weird concept (at least for me coming from SQL database background) that data can be stored (Store.YES and Store.NO) in different ways . Lucene indexing: Store and indexing modes explained This is a very unusual concept for me as I don't find many reasons to NOT store data. What's the reason behind using Store.NO? I will most likely always want to have the data there even though I'm not displaying it anywhere... I mean if data is indexed it must be stored anyhow right? What's the reason behind using Store.NO? Consider the queries: What documents contain the term 'foo'? What terms does document '1234' contain? An index for the first will map term -> document. The second will map document -> term. Most people only want to use Lucene for the first type of query so they only build the first type of index (Store.NO). If you want to do the second type of query you'll need to build both types of indices. This takes up more space. (It is in theory possible to loop through all terms and figure out the document without actually building this index but it's really slow.) ""Reverse index"" might be a more appropriate name than ""store.""  what lucene query syntax: there are a lot of steering chars try type:'ta' quoted thoe .."
1060,A,"Hibernate Search and Relations I have an object called MyItemBean which can have 0 or more associated KeywordBean objects. The resulting classes look like this: @Entity public class MyItemBean { ...stuff... @ManyToMany(targetEntity = KeywordBean.class cascade = CascadeType.PERSIST) @JoinTable(name = ""tbl_item_keyword"" joinColumns = @JoinColumn(name = ""item_id"") inverseJoinColumns = @JoinColumn(name = ""keyword_id"")) private List<KeywordBean> keywords = null; ...more stuff... } @Entity public class KeywordBean { ...stuff... private String value=null; ...more stuff... } I'm using JBoss Seam/Hibernate Search to index these objects so I can perform search queries against them. I'd like to be able to search for MyItemBean instances with a given keyword value. This relation however is unidirectional because I apply KeywordBean objects to more than just MyItemBean. I've looked in the Hibernate Search documentation for examples on how to index relations but all of the examples they provide are bi-directional. Can anyone tell me what annotations I need to apply on MyItemBean.keywords to index the keyword values properly? The annotation to use is IndexedEmbedded. It works fine with unidirectional associations as well. A problem can occur if you are changing the value of KeywordBean Hibernate Search does not have a way to update the index for the MyItemBean instances which reference the changed KeywordBean. In bidirectional relations you can use @ContainedIn to fix this problem but you don't really need this. Provided on your usecase this index updating constraint might not be an issue. Maybe your KeywordBean is not changing. Or if it changes you can re-index all *KeywordBean*s which are affected manually. --Hardy So where does the @IndexEmbedded annotation go? Would it go on KeywordBean at class-level? Or MyItemBean at property level for keywords? It belongs onto the keyword property in MyItemBean"
1061,A,"Why lucene doesn't require composite index but relation database does? Lucene stores index for each field separetly. So when we perform query ""fld1:a AND fld2:b"" we iterate over Termdocs for first term and second term. This can't be faster. In case of database two separete indexes for fld1 and fld2 will work slow and only one will be used. In that case DB requres composite key for fld1 and fld2. My question is. Why Can't DB utilize Lucene index algorithm for executing Boolean queries if it as fast as DB index and dosn't requires different combinations of columns? Some details of Lucene Boolean Query search: It utilize interface TermDoc. The main idea in using two methods boolean skipTo(int) and boolean next(). So it is doesn't depend on term order(popular or not popular term) because count of those method calls will be always as most infrequent term(due to skipTo method). So there are no need in hierarchical composite index it will not bring any additional performance. TermDocs t1 = searcher.docs(fld1:a); TermDocs t2 = searcher.docs(fld2:b); int doc = -1; t1.next(); t2.next(); while(t1.doc()!=-1 && t2.doc()!=-1) { if(t1.doc()<t2.doc()) { if(!t1.skipTo(t2.doc)) return; } if(t2.doc()<t1.doc()) { if(!t2.skipTo(t1.doc)) return; } if(t1.doc()==t2.doc()) { println(""found doc:""+t1.doc()); t1.next() } } I don't know for sure but wouldn't Lucene spawn 2 threads to search fld1 and fld2 concurrently? Very broadly speaking search engine indexes only use RAM resources while relational DBs assume that the index will be swapped in and out from disk to RAM as needed. Avail RAM is a much more of a hard limit on the size of a search engine instance (running in prod for instance) i.e. the number of terms documents etc that can be retrieved in milliseconds. While a DB has an extra layer that temporarily brings in fld1Index and fld2Index for the current query (plus system optimizations). Good Luck! `In case of database two separete indexes for fld1 and fld2 will work slow and only one will be used.` This is only really true of MySQL. Postgres doesn't have this problem. @yara : Thanks for the clarification. Good luck on solving your problem! Are you sure? It is also true for Db2. Well I definitely know it's not a problem for postgres =D http://www.postgresql.org/docs/8.3/static/indexes-bitmap-scans.html @shellter No lucene dosn't search in parallel. It utilize interface TermDoc(http://lucene.apache.org/java/2_3_2/api/org/apache/lucene/index/TermDocs.html). The main idea in using two methods skipTo and next. So it is doesn't depend on term order(popular or not popular term) because count of those method calls will be always as most infrequent term(due to skipTo method). So there are no need in hierarchical composite index it will not bring any additional performance. I think @Frank Farmer's comment gives you most of your answer: it's perfectly possible for an RDB to use multiple indexes even if they aren't ""composite"". A more specific question has a harder answer: why don't RDBs use Lucene's multi-index-search paradigm? Recall that Lucene uses an inverted index with a skip list; recall also that these are only efficient if the index is extremely sparse and the number of terms is very high. In the type of column where you're likely to do a query like where a = b the number of possible bs is probably pretty small and hence the index will be relatively dense. So it makes more sense to use bitmaps (like PostgreSQL does) and gain the speedup of bit-level parallelism than to store it as a skip list and deal with pointer-chasing. I should note that even Lucene uses bitmaps when combining filters with queries so we might equivalently ask why Lucene doesn't use Lucene's search. My guess is that bitmaps are smaller and therefore more likely to fit in memory. To the best of my knowledge this is not a huge performance gain so you probably can't make a very strong argument for either bitmaps or skip lists in the general case. But if I had to guess why the PostgreSQL devs went the bitmap route I think it would be this. Thanks I now realize why Lucene skipTo works so fast - just because it is skip list. Now the problems seems clear to me."
1062,A,"Problem in using Solr WordDelimiterFilter I am doing some test using WordDelimiterFilter in Solr but it doesn't preserve the protected list of words which I pass to it. Would you please inspect the code and the output example and suggest which part is missing or used badly? with running this code: private static Analyzer getWordDelimiterAnalyzer() { return new Analyzer() { @Override public TokenStream tokenStream(String fieldName Reader reader) { TokenStream stream = new StandardTokenizer(Version.LUCENE_32 reader); WordDelimiterFilterFactory wordDelimiterFilterFactory = new WordDelimiterFilterFactory(); HashMap<String String> args = new HashMap<String String>(); args.put(""generateWordParts"" ""1""); args.put(""generateNumberParts"" ""1""); args.put(""catenateWords"" ""1""); args.put(""catenateNumbers"" ""1""); args.put(""catenateAll"" ""0""); args.put(""luceneMatchVersion"" Version.LUCENE_32.name()); args.put(""language"" ""English""); args.put(""protected"" ""protected.txt""); wordDelimiterFilterFactory.init(args); ResourceLoader loader = new SolrResourceLoader(null null); wordDelimiterFilterFactory.inform(loader); /*List<String> protectedWords = new ArrayList<String>(); protectedWords.add(""good bye""); protectedWords.add(""hello world""); wordDelimiterFilterFactory.inform(new LinesMockSolrResourceLoader(protectedWords)); */ return wordDelimiterFilterFactory.create(stream); } }; } input text: hello world good bye what is your plan for future? protected strings: good bye hello world output: (hellostartOffset=0endOffset=5positionIncrement=1type=) (worldstartOffset=6endOffset=11positionIncrement=1type=) (goodstartOffset=12endOffset=16positionIncrement=1type=) (byestartOffset=17endOffset=20positionIncrement=1type=) (whatstartOffset=21endOffset=25positionIncrement=1type=) (isstartOffset=26endOffset=28positionIncrement=1type=) (yourstartOffset=29endOffset=33positionIncrement=1type=) (planstartOffset=34endOffset=38positionIncrement=1type=) (forstartOffset=39endOffset=42positionIncrement=1type=) (futurestartOffset=43endOffset=49positionIncrement=1type=) You are using a standard tokenizer which at least tokenizes on a whitespace level so you will always have ""hello world"" be split to ""hello"" and ""world"". TokenStream stream = new StandardTokenizer(Version.LUCENE_32 reader); See Lucene Documentation: public final class StandardTokenizer extends Tokenizer A grammar-based tokenizer constructed with JFlex This should be a good tokenizer for most European-language documents: Splits words at punctuation characters removing punctuation. However a dot that's not followed by whitespace is considered part of a token. Splits words at hyphens unless there's a number in the token in which case the whole token is interpreted as a product number and is not split. Recognizes email addresses and internet hostnames as one token. The word delimiter protected word list is meant for something like: ISBN2345677 to be split in ISBN 2345677 text2html not to be split in text 2 html (because text2html was added to protected words) If you really want to do something like you mentioned you may use the KeywordTokenizer. But you have to do the complete splitting by yourself. You may have a look at `CommonGramsFilterFactor`. thank you very much it seems I've demanded a different functionality from what WordDelimiterFilter is expected to do. KeywordTokenizer emit whole the text as a single token. It is not what I want Is there any filter ot tokenizer which would protect keywords containing whitespaces from splitting?"
1063,A,"create new core directories in SOLR on the fly i am using solr 1.4.1 for building a distributed search engine but i dont want to use only one index file - i want to create new core ""index""-directories on the fly in my java code. i found following rest api to create new cores using an EXISTING core directory (http://wiki.apache.org/solr/CoreAdmin). http://localhost:8983/solr/admin/cores?action=CREATE&name=coreX&instanceDir=path_to_instance_directory&config=config_file_name.xml&schema=schem_file_name.xml&dataDir=data is there a way to create a new core without an extisting core directory? has solr such a function? via rest or in the solrj-api? thanks. It's not currently possible to programmatically submit your schema and config to Solr to create a new core. Here's the JIRA issue about it. As mentioned in the comments you can work around it by using something like WebDAV or scp or sftp."
1064,A,"NoSQL (MongoDB) vs Lucene (or Solr) as your database With the NoSQL movement growing based on document-based databases I've looked at MongoDB lately. I have noticed a striking similarity with how to treat items as ""Documents"" just like Lucene does (and users of Solr). So the question: Why would you want to use NoSQL (MongoDB Cassandra CouchDB etc) over Lucene (or Solr) as your ""database""? What I am (and I am sure others are) looking for in an answer is some deep-dive comparisons of them. Let's skip over relational database discussions all together as they serve a different purpose. Lucene gives some serious advantages such as powerful searching and weight systems. Not to mention facets in Solr (which Solr is being integrated into Lucene soon yay!). You can use Lucene documents to store IDs and access the documents as such just like MongoDB. Mix it with Solr and you now get a WebService-based load balanced solution. You can even throw in a comparison of out-of-proc cache providers such as Velocity or MemCached when talking about similar data storing and scalability of MongoDB. The restrictions around MongoDB reminds me of using MemCached but I can use Microsoft's Velocity and have more grouping and list collection power over MongoDB (I think). Can't get any faster or scalable than caching data in memory. Even Lucene has a memory provider. MongoDB (and others) do have some advantages such as the ease of use of their API. New up a document create an id and store it. Done. Nice and easy. Thank you in advance! @Philip: It's a hypothetical question. Why not use Lucene as your document storage? You get a lot more searching power and scalability (when mixed with Solr making Lucene even easier to use). See http://stackoverflow.com/questions/2546494/is-mongodb-a-valid-alternative-to-relational-db-lucene Thank you but that does not answer my question: which is why would I use MongoDB instead of Lucene for my database? They both handle documents but Lucene has some very powerful search options. +1 though for actually finding a related question. I search several times on Stackoverflow and did not come up with a near comparison. How are you using Lucene that it provides functionality similar to MongoDB? Are you tying it to a relational DB for storage? This is a great question something I have pondered over quite a bit. I will summarize my lessons learned: You can easily use Lucene/Solr in lieu of MongoDB for pretty much all situations but not vice versa. Grant Ingersoll's post sums it up here. MongoDB etc. seem to serve a purpose where there is no requirement of searching and/or faceting. It appears to be a simpler and arguably easier transition for programmers detoxing from the RDBMS world. Unless one's used to it Lucene & Solr have a steeper learning curve. There aren't many examples of using Lucene/Solr as a datastore but Guardian has made some headway and summarize this in an excellent slide-deck but they too are non-committal on totally jumping on Solr bandwagon and ""investigating"" combining Solr with CouchDB. Finally I will offer our experience unfortunately cannot reveal much about the business-case. We work on the scale of several TB of data a near real-time application. After investigating various combinations decided to stick with Solr. No regrets thus far (6-months & counting) and see no reason to switch to some other. Summary: if you do not have a search requirement Mongo offers a simple & powerful approach. However if search is key to your offering you are likely better off sticking to one tech (Solr/Lucene) and optimizing the heck out of it - fewer moving parts. My 2 cents hope that helped. +1 That is an excellent answer! I've give time for others to chime in before marking this as the answer. But I cannot see anyone else giving more. Thank you very much! Solr does not have map-reduce built-in but you can combine with Hadoop. http://architects.dzone.com/articles/solr-hadoop-big-data-love Solr has no map reduce functionality. Therefore reporting stats computation of scores etc. are not possible! Use Solr only if you have/ can threat your data as text data Map-reduce no but it does have the ability to run a query in parallel across multiple solr servers and aggregate those results. So while it doesn't have general purpose map-reduce it has already written what you would be writing with map-reduce which is parallel search queries. @Roo: Would it be an option to use Lucene as a main DB and create aggregate indexes with MongoDB somehow? Or doesn't that make sense? And Mikos: great answer and +1 for the real-world experience mention.  From my experience with both Mongo is great for simple straight-forward usage. The main Mongo disadvantage we've suffered is the poor performance on unanticipated queries (you cannot created mongo indexes for all the possible filter/sort combinations you simple can't). And here where Lucene/Solr prevails big time especially with the FilterQuery caching Performance is outstanding. Gave you a +1 for the retro answer with Lucene's newest features. :)  We use MongoDB and Solr together and they perform well. You can find my blog post here where i described how we use this technologies together. Here's an excerpt: [...] However we observe that query performance of Solr decreases when index size increases. We realized that the best solution is to use both Solr and Mongo DB together. Then we integrate Solr with MongoDB by storing contents into the MongoDB and creating index using Solr for full-text search. We only store the unique id for each document in Solr index and retrieve actual content from MongoDB after searching on Solr. Getting documents from MongoDB is faster than Solr because there is no analyzers scoring etc. [...] Good blog post. Yes this is exactly how I've used Lucene in the past with older SQL and MySql datastores (storing IDs in Lucene and retrieving the complex types from the datastore). Technically though this question was to explore the differences between the two - not exactly how to use the ""best of both worlds."" +1 for using it that way as it's really the only real way to use massive amounts of data. Thanks for your response. I know that the question is about choosing Nosql over Lucene but here I want to show that instead of choosing one over other using them in a hybrid manner will give the better result. Do you remember (now 1.5 years later) roughly the size of the Solr database when the query performance had decreased so much so you started thinking about adding MongoDB? (Was it 10000 docs or 10000000 docs?) Very helpful. I work in GIS and so being able to combine full-text with spatial search in this way is very intriguing. We already use MongoDB and Postgres and I have been thinking about Solr for a while.  Also please note that some people have integrated Solr/Lucene into Mongo by having all indexes be stored in Solr and also monitoring oplog operations and cascading relevant updates into Solr. With this hybrid approach you can really have the best of both worlds with capabilities such as full text search and fast reads with a reliable datastore that can also have blazing write speed. It's a bit technical to setup but there are lots of oplog tailers that can integrate into solr. Check out what rangespan did in this article. http://denormalised.com/home/mongodb-pub-sub-using-the-replication-oplog.html +1 Awesome retro answer to my 2 year question! I will take a look at that. Very very cool man. Ha ha I didn't realize it was that old we have been looking at doing the same thing so I'm glad it helped. If I understood you correctly the reason you use MongoDB (in addition to Solr) is that MongoDB has faster insertion + read speed? Did you also indicate that MongoDB has a more reliable datastore? (Or were you referring to Solr?) — What did you start with initially? Only MongoDB only Solr or both Mongo + Solr?  You can't partially update a document in solr. You have to re-post all of the fields in order to update a document. And performance matters. If you do not commit your change to solr does not take effect if you commit every time performance suffers. There is no transaction in solr. As solr has these disadvantages some times nosql is a better choice. MongoDB does not have transactions either. Solr or Lucene have realtime search so committing is not an issue. @user183037 in MongoDB any updates within a document is Atomic. And FYI Lucene doesn't have transactions (in your sense) either This answer has become incorrect. Solr 4+ does support partial updates and soft commits / near real time do away with most of the issues of ""old-style"" Solr commits.  @mauricio-scheffer mentioned Solr 4 - for those interested in that LucidWorks is describing Solr 4 as ""the NoSQL Search Server"" and there's a video at http://www.lucidworks.com/webinar-solr-4-the-nosql-search-server/ where they go into detail on the NoSQL(ish) features. (The -ish is for their version of schemaless actually being a dynamic schema.) +1 Nice updated answer to a retro question.  Since no one else mentioned it let me add that MongoDB is schema-less whereas Solr enforces a schema. So if the fields of your documents are likely to change that's one reason to choose MongoDB over Solr. that IMHO is not quite true. Solr does have a schema as defined in `schema.xml` BUT it does also have 'dynamic fields' ie fields whose types are determined via wild cards so you can have all fields matching say `*_i` indexed as integer fields. when adding documents you can then have documents conaining fields like `count_i` `foo_i` `bar_i` that are all understood as integer fields without appearing in `schema.xml` literally. pretty schema-less i'd say. see http://www.youtube.com/watch?v=WYVM6Wz-XTw for more. I have to come back and bump this up with a +1 because that is true - schema changes in Solr has always been in a PITA to keep in sync with other data stores."
1065,A,"SQL Server 2008 Full Text Search (FTS) versus Lucene.NET I know there have been questions in the past about SQL 2005 versus Lucene.NET but since 2008 came out and they made a lot of changes to it and was wondering if anyone can give me pros/cons (or link to an article). One consideration that you need to keep in mind is what kind of search constraints you have in addition to the full-text constraint. If you are doing constraints that lucene can't provide then you will almost certainly want to use FTS. One of the nice things about 2008 is that they improved the integration of FTS with standard sql server queries so performance should be better with mixed database and FT constraints than it was in 2005.  SQL Server FTS is going to be easier to manage for a small deployment. Since FTS is integrated with the DB the RDBMS handles updating the index automatically. The con here is that you don't have an obvious scaling solution short of replicating DB's. So if you don't need to scale SQL Server FTS is probably ""safer"". Politically most shops are going to be more comfortable with a pure SQL Server solution. On the Lucene side I would favor SOLR over straight-up Lucene. With either solution you have to do more work yourself updating the index when the data changes as well as mapping data yourself to the SOLR/Lucene index. The pros are that you can easily scale by adding additional indexes. You could run these indexes on very lean linux servers which eliminates some license costs. If you take the Lucene/SOLR route I would aim to put ALL the data you need directly into the index rather than putting pointers back to the DB in the index. You can include data in the index that is not searchable so for example you could have pre-built HTML or XML stored in the index and serve it up as a search result. With this approach your DB could be down but you are still able to serve up search results in a disconnected mode. I've never seen a head-to-head performance comparison between SQL Server 2008 and Lucene but would love to see one.  I built a medium-size knowledge base (maybe 2GB of indexed text) on top of SQL Server 2005's FTS in 2006 and have now moved it to 2008's iFTS. Both situations have worked well for me but the move from 2005 to 2008 was actually an improvement for me. My situation was NOT like StackOverflow's in the sense that I was indexing data that was only refreshed nightly however I was trying to join search results from multiple CONTAINSTABLE statements back in to each other and to relational tables. In 2005's FTS this meant each CONTAINSTABLE would have to execute its search on the index return the full results and then have the DB engine join those results to the relational tables (this was all transparent to me but it was happening and was expensive to the queries). 2008's iFTS improved this situation because the database integration allows the multiple CONTAINSTABLE results to become part of the query plan which made a lot of searches more efficient. I think that both 2005 and 2008's FTS engines as well as Lucene.NET have architectural tradeoffs that are going to align better or worse to a lot of project circumstances - I just got lucky that the upgrade worked in my favor. I can completely see why 2008's iFTS wouldn't work in the same configuration as 2005's for the highly OLTP nature of a use case like StackOverflow.com. However I would not discount the possibility that the 2008 iFTS could be isolated from the heavy insert transaction load... but it also sounds like it could be as much work to accomplish that as move to Lucene.NET ... and the cool factor of Lucene.NET is hard to ignore ;) Anyway for me the ease and efficiency of SQL 2008's iFTS in the majority of situations probably edges out Lucene's 'cool' factor (though it is easy to use I've never used it in a production system so I'm reserving comment on that). I would be interesting in knowing how much more efficient Lucene is (has turned out to be? is it implemented now?) in StackOverflow or similar situations.  we use both full-text-search possibilities but in my opinion it depends on the data itself and your needs. we scale with web-servers and therefore i like lucene because i don't have that much load on the sql-server. for starting at null and wanting to have a full-textsearch i would prefer the sql-server solution because i think it is really fast to get results if you want lucene you have to implement more at start (and also get some know-how).  This might help: http://blog.stackoverflow.com/2008/11/sql-2008-full-text-search-problems/ Haven't used SQL Server 2008 personally though based on that blog entry it looks like the full-text search functionality is slower than it was in 2005."
1066,A,"determine which value produced a hit in SOLR multivalued field type If I have a multiValued field type of text and I put values [catdoggreenblue] in it. Is there a way to tell when I execute a query against that field for dog that it was in the 1st element position for that multiValued field? Assumption: client does not have any pre-knowledge of what the field type of the field being queried is. (i.e. Solr must provide the answer and the client can't post process the return doc to figure it out because it would not know how SOLR matched the query to the result). Disclosure: I posted to solr-user list and am getting no traction so I post here now. @Mauricio - I don't want to match only a certain position I want to know which position matched. http://old.nabble.com/determine-which-value-produced-a-hit-in-multivalued-field-type-td27281182.html I don't understand why you want to match only a certain position in the multiValued field... The Lucene API allows for this but I'm not sure if Solr does out of the box. In Lucene you can use the IndexReader.termPositions(Term term) method.  Hopefully I understand your question correctly. If you want to get field index or value there is an ugly workaround: You could add the index directly in the value e.g. store ""1; car"" ""2; test"" and so on. Then use highlighting. When reading the returned fields simply skip the text before the semicolon. But if you want to query only one type: You can avoid the multivalue approach and simply store it as item_i and query via item_1. To query against all items regardless the type you need to use the copyField directive in the schema.xml  Currently there's no out-of-the-box functionality provided in Solr which tells you the position of a value in a multiValue field. oops I forgot to +1 (+1) This may be the true answer? Is it possible to run the field through the query and index analyzers or extended versions of them as a client side post process? The key is that the client can produce the same hit that querying the server would."
1067,A,Configure best bets or result promotion in Solr/Lucene I'm using Solr/Lucene as search engine for my application. I require that some results (known as best bets) show every time the user asks for certain queries. Does anyone knows how to configure this in Solr/Lucene? Thanks. Take a look at the QueryElevationComponent. Thanks that is exactly what I was looking for.
1068,A,"how to make lucene be case-insensitive By default word ""Word"" and ""word"" are not the same. How can I make Lucene be case-insensitive? By using http://lucene.apache.org/java/2_4_1/api/org/apache/lucene/analysis/LowerCaseFilter.html.  Add LowerCaseFilterFactory to your fieldType for that field in Schema.xml. Example <fieldType name=""text"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""1"" catenateNumbers=""1"" catenateAll=""0"" splitOnCaseChange=""1"" preserveOriginal=""1"" /> <filter class=""solr.LowerCaseFilterFactory""/> </analyzer> <analyzer type=""query""> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <filter class=""solr.WordDelimiterFilterFactory"" generateWordParts=""1"" generateNumberParts=""1"" catenateWords=""0"" catenateNumbers=""0"" catenateAll=""0"" splitOnCaseChange=""1""/> <filter class=""solr.LowerCaseFilterFactory""/> </analyzer> </fieldType>  The StandardAnalyzer applies a LowerCaseFilter that would make ""Word"" and ""word"" the same. You could simply pass that to your uses of IndexWriter and QueryParser. E.g. a few line snippets: Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_30); IndexWriter writer = new IndexWriter(dir analyzer true mlf); QueryParser parser = new QueryParser(Version.LUCENE_30 field analyzer);  The easiest approach is lowercasing all searchable content as well as the queries. See the LowerCaseFilter documentation. You could also use Wildcard queries for case insensitive search since it bypasses the Analyzer. You can store content in different fields to capture different case configurations if preferred."
1069,A,"Best full text search alternative to ms sql c++ solution What is the best full text search alternative to ms sql? (which works with ms sql) I'm looking for something similar to Lucene and Lucene.NET but without the .NET and Java requirements. I would also like to find a solution that is usable in commercial applications. I prefer [Xapian](http://xapian.org/) - pure C++ Search Engine Library. One note of caution on [Lucene.NET](http://incubator.apache.org/lucene.net/): it tends to lag behind Apache's [Java Lucene](http://lucene.apache.org/) (the ""official"" Lucene). Lucene.NET is a port of Java Lucene and the community around Java Lucene is larger. For obvious reasons this can sometimes be a problem. Solr is based on Lucene but accessible via HTTP so it can be used from any platform.  Sphinx is one of the best solutions. It's written in C++ and has amazing performance. but it's not for ms sql Lucene is not for MSSQL either you can 2 choices: index documents prior passing it to DB or let indexing engine go through DB itself. Sphinx can't crawl MSSQL automatically as far as I know but it has best indexing performance.  DT Search is hands down the best search tool I have used. They have a number of solutions available. Their Engine will run on Native Win32 Linux or .NET. It will index pretty much every kind of document you might have (Excel PDF Word etc.) I did some benchmarks comparisons a while ago and it was the easiest to use and had the best performance.  I second Sphinx but Lucene is also not so bad despite the Java. :) If you are not dealing with too much data spread out etc. then also look into MySQL's FULLTEXT. We are using it to search across a 20 GB database.  Take a look at CLucene - It's a well maintained C++ port of java Lucene. It's currently licenced under LGPL and we use it in our commercial application. Performance is incredible however you do have to get your head around some of the strange API conventions."
1070,A,Array as index for Zend_Search_Lucene_Field This is what works: $doc->addField(Zend_Search_Lucene_Field::text('Image' $item['Image']['localName'])); which indexes the field and it's accessible later in my view helper as this: $item['Image'] However what I want (without a work-around in my view helper) is for it to be accessible like this: $item['Image']['localName'] ... This doesn't work: $doc->addField(Zend_Search_Lucene_Field::text(array('Image' => 'localName') $item['Image']['localName'])); Is this even possible? Zend_Search_Lucene_Field::text expects the first parameter to be a string not an array. Therefore the proposed method is not possible.
1071,A,"Deleting document by Term from lucene The following code does not delete the document by Term as expected:  RAMDirectory idx = new RAMDirectory(); IndexWriter writer = new IndexWriter(idx new SnowballAnalyzer(Version.LUCENE_30 ""English"") IndexWriter.MaxFieldLength.LIMITED); Document doc = new Document(); doc.add(new Field(""title"" ""mydoc"" Field.Store.YES Field.Index.NO)); doc.add(new Field(""content"" ""some content deleteme"" Field.Store.YES Field.Inde x.ANALYZED)); writer.addDocument(doc); Document doc2 = new Document(); doc2.add(new Field(""title"" ""mydoc2"" Field.Store.YES Field.Index.NO)); doc2.add(new Field(""content"" ""other content don't deleteme"" Field.Store.YES Field.I ndex.ANALYZED)); writer.addDocument(doc2); writer.optimize(); writer.close(); /* IndexReader reader = IndexReader.open(idx false); int docs_up_for_deletion = reader.docFreq(new Term(""title"")); int before = reader.numDocs(); int docs_deleted = reader.deleteDocuments(new Term(""title"" ""mydoc"")); reader.close(); */ IndexWriter writer2 = new IndexWriter(idx new SnowballAnalyzer(Version.LUCENE_30 ""English"") IndexWriter.MaxFieldLength.LIMITED); int before = writer2.numDocs(); writer2.deleteDocuments(new Term(""title"" ""mydoc"")); writer2.commit(); writer2.optimize(); int after = writer2.numDocs(); writer2.close(); int docs_deleted = before - after; I've tried deleting with the IndexReader and IndexWriter and neither works. I've also tried adding another IndexReader search after the above code just in case the number only gets updated after closing writer2 (mentioned in this FAQ) but that doesn't help. Doing a writer.deleteAll() works just not the delete by Term. I found an old reference to the fact that only fields of type Field.Keyword can be deleted but this is no longer a valid field type in Lucene 3.x Your title field is not indexed. Change new Field(""title"" ""mydoc"" Field.Store.YES Field.Index.NO) to new Field(""title"" ""mydoc"" Field.Store.YES Field.Index.ANALYZED) or new Field(""title"" ""mydoc"" Field.Store.YES Field.Index.NOT_ANALYZED) depending on whether or not you want your field analyzed."
1072,A,Lucene / Lucene.NET - Document.SetBoost() values? I know it takes in a float but what are some typical values for various levels of boosting within a result? For example: If I wanted to boost a document's weighting by 10% then I should set it 1.1? For 20% then 1.2? What happens if I start setting boosts to values like 75.0? or 500.0? Edit: Fixed Formatting The important thing to remember about boosting is not to approach it in isolation you need to consider it as part of a global strategy make a list of each criteria used to effect the relevancy and then order those criteria. Define a relationship between each of those criteria. Are you regularly re-indexing or are you just adding new documents if you are regularly re-indexing you can afford to tune your document boost criteria if not you need to think it through thoroughly beforehand.  Please see the Lucene Similarity Documentation for the formula. In principle all other factors being equal setting a document's boost to 1.1 will indeed give it a score that is 10% higher as compared to an identical document with a boost of 1.0. If you have a set of documents that should be intrinsically preferred in searches this may be a good idea. Note that document boost is an indexing-time attribute making it impossible to change the document's boost without reindexing it. There are other important factors in scoring - including term match scores norms etc. See Debugging Relevance Issues in Search for details. But be aware that document and field boosts end up encoded in a single byte with a 3-bit mantissa -- so any difference less than 25% may end up completely unnoticed.  Adding to what Yuval has said. This value is function of field boost & document boost. The boost values are encoded in a single byte. So the precision might be lost while storing this value. Debugging with Searcher.Explain() would help you get the right amount of boost. If you want the boost value to be preserved (it's useful for example when you want to recreate index from current index) you may add it in a stored field.
1073,A,Using Solr and Zends Lucene port together Afternoon chaps After my adventures with Zend-Lucene-Search and discovering it isn't all its cracked up to be when indexing large datasets I've turned to Solr (thanks to Bill Karwin for that :) ) I've got Solr indexing the db far far quicker now taking just over 8 minutes to index a table of just over 1.7million rows - which I'm very pleased with. However when I come to try and search the index with the Zend port I run into the following error; Fatal error: Uncaught exception 'Zend_Search_Lucene_Exception' with message 'Unsupported segments file format' in /var/www/Zend/Search/Lucene.php:407 Stack trace: #0 /var/www/Zend/Search/Lucene.php(555): Zend_Search_Lucene->_readSegmentsFile() #1 /var/www/z_search.php(12): Zend_Search_Lucene->__construct('tmp/feeds_index') #2 {main} thrown in /var/www/Zend/Search/Lucene.php on line 407 I've tried to have a search around but can't seem to find anything about this problem everyone just seems to be able to get them to work? Any help as always much appreciated :) Thanks Tom I confirmed on my machine that a Lucene index created through Solr cannot be read by Zend_Search_Lucene. Zend_Search_Lucene throws that exception when it detects a Lucene index format that it doesn't support. Looking at the code Zend currently supports formats pre-2.1 2.1 and 2.3. Solr creates an index in format FORMAT_HAS_PROX which as far as I can tell is used by Lucene 2.9 and higher. I think once you get it running you're bound to be happier with it. The only other suggestion I have is to try to see if you can force Solr to create the Lucene index in 2.3 format. But I don't know how one could do that. Ahhh rubbish. Is there no work around then I presume? I'm guessing they'll be no updated Zend code out anytime soon either. Looks like I'll be pushing for that Jetty/Tomcat server.  Never used Zend before but I've used Lucene/Solr. Are you using the same version of Lucene for both the Solr indexing and the Zend port? Check to see what Lucene jar file is being used for each. If they're different then Solr might be producing a Lucene index that isn't compatible with the Zend port. Chances are that Solr's index versuin is more advanced than Zend's. You may want to consider going an extra step using Solr for search as well and communicating with PHP via an HTTP interface such as XML or JSON. We had considered that the only problem being that we're unsure on the possibilities of running Jetty/Tomcat on our live server. The plan was to index the db locally then upload it every x days. I'll look into the Lucene versions for both Zend and Solr and make sure they're singing from the same hymn sheet.
1074,A,"Solr date boost and sort by relevant results NOT working properly I am implementing Solr dismax search and also using this function recip(ms(NOWPubDate)3.16e-1110001000) for date boost. Everthing is working fine but only got one problem. if search keywords are repeated in the Title they get more score than recent results. e.g. 1) Title = solr lucene Date = 1 day old 2) Title = solr lucene is best love solr lucene Date = 15 days old If user searched for 'solr lucene' then #2 comes at first position only because keywords are repeated in the Title. I have got too many records which are12 or 3 days old and they have even the exact same title ""SOLR LUCENE"" but those records doesn't come on first page only because old records have keywords repeated in the Title. I don't want to sort the results entirely by date. Currently i am sorting it like this. sort= score desc date asc You shouldn't use an order clause if you are using boost. If you like to give the date more relevance so pimp your boost function. It's up to you who big is the date influence for the order of the search result is. It also depends on the dismax-handler you are using: {!edismax boost=recip(pow(ms(NOWPubDate)<val>)3.16e-1111)} Put an value instead of the <val> placeholder between 0 and 2 where 0 is nearly ""order by date"" and 2 is order by relevance. Not sure if this works for dismax but it works for standard solr search handler (with other syntax than the example above) and edismax. Thank u so much. I will try this!"
1075,A,"How to instruct StandardAnalyzer in Lucene to not to remove stop words? Simple question : How to make Lucene's StandardAnalyzer not to remove stop words when analyzing my sentence ? The answer is version-dependent. For Lucene 3.0.3 (current) you need to construct the StandardAnalyzer with an empty set of stop words using something like this: Analyzer ana = new StandardAnalyzer(LUCENE_30 Collections.emptySet()); Thought so... Was waiting if someone would say ""there is something in API to do so"" :) I guess I'll go with your answer then :) @Joel @Yuval : Accepted the answer :) and also upvoted :) Don't forget to mark the answer correct then! @Shrinath@Joel: Thank you both  Update: the answer is version-dependent. For Lucene 4.0 use: Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_40 CharArraySet.EMPTY_SET); Note that the StandardAnalyzer is not in the lucene-core jar but in lucene-analyzers-common-4.0.0.jar"
1076,A,Lucene Index optimization Is there a progmmatic way in Lucene to know if the index is optimized or not? Thanks. IndexReader.isOptimized
1077,A,"Indexing columns in a single text doc using Lucene HI All I am planning to index a single document which has tab separated data as below:  Name ID email address So when somebody searches for the 'Name' you should get his IDemail and address as the response. The same holds good for other columns as well. I am planning to use Lucene for this. But from whatever little I have read about Lucene it talks about indexing multiple text docs. Can somebody guide me through a tutorial or link where i can build an index with my requirements. Thank you :) You would just have each line be a ""document."" This doesn't really sound like a good use for lucene though - why can't you just use a normal (relational) database? Well.. the data is in this format.. Is treating every line as a document and the content within as the Field the only way out ? Also Xodarap why do you say that it is not a good use of Lucene ? Even if we have the same in a relational DB wouldn't Lucene beat it in performance ? Pretty new to Lucene just trying to learn.. Thank you :) @paypalcomp: Don't let the word ""document"" mislead you. You can think of each ""document"" as a single search result - you want each line to be a search result --> each line is a ""document."" wrt performance: Lucene is very good at free text search. So if you have large amounts of free text lucene will be good. If you just want to search by a discrete value a relational db will probably be faster and it will certainly be easier to use. @paypalcomp: also if you are just starting with lucene you may want to look into Solr instead. It is a wrapper around lucene which makes it much easier to use. Great.. thanks you so much :) ill have a look at Solr.. thanks for your time again .."
1078,A,"Lucene wildcard queries I have this question relating to Lucene. I have a form and I get a text from it and I want to perform a full text search in several fields. Suppose I get from the input the text ""textToLook"". I have a Lucene Analyzer with several filters. One of them is lowerCaseFilter so when I create the index words will be lowercased. Imagine I want to search into two fields field1 and field2 so the lucene query would be something like this (note that 'textToLook' now is 'texttolook'): field1: texttolook* field2:texttolook* In my class I have something like this to create the query. I works when there is no wildcard. String text = ""textToLook""; String[] fields = {""field1"" ""field2""}; //analyser is the same as the one used for indexing Analyzer analyzer = fullTextEntityManager.getSearchFactory().getAnalyzer(""customAnalyzer""); MultiFieldQueryParser parser = new MultiFieldQueryParser(fields analyzer); org.apache.lucene.search.Query queryTextoLibre = parser.parse(text); With this code the query would be: field1: texttolook field2:texttolook but If I set text to ""textToLook*"" I get field1: textToLook* field2:textToLook* which won't find correctly as the indexes are in lowercase. I have read in lucene website this: "" Wildcard Prefix and Fuzzy queries are not passed through the Analyzer which is the component that performs operations such as stemming and lowercasing"" My problem cannot be solved by setting the behaviour case insensitive cause my analyzer has other fields which for examples remove some suffixes of words. I think I can solve the problem by getting how the text would be after going through the filters of my analyzer then I could add the ""*"" and then I could build the Query with MultiFieldQueryParser. So in this example I woud get ""textToLower"" and after being passed to to these filters I could get ""texttolower"". After this I could make ""textotolower*"". But is there any way to get the value of my text variable after going through all my analyzer's filters? How can I get all the filters of my analyzer? Is this possible? Thanks Can you use QueryParser.setLowercaseExpandedTerms(true)? http://wiki.apache.org/lucene-java/LuceneFAQ#Are_Wildcard.2C_Prefix.2C_and_Fuzzy_queries_case_sensitive.3F ** EDIT ** Okay I understand your issue now. You actually want the wildcarded term to be stemmed before it's run through the wildcard query. You can subclass QueryParser and override protected Query getWildcardQuery(String field String termStr) throws ParseException to run termStr through the analyzer before the WildcardQuery is constructed. This might not be what the user expects though. There's a reason why they've decided not to run wildcarded terms through the analyzer per the faq: The reason for skipping the Analyzer is that if you were searching for ""dogs*"" you would not want ""dogs"" first stemmed to ""dog"" since that would then match ""dog*"" which is not the intended query. I had already seen this but it doesn't solve the problem. I have more filters than the lowercase one. As I mentioned one of them eliminates suffixes of words so if I index for example ""changeable"" it would be index as ""change"" so if I would search for ""changes"" in the query should get the root of the word (""change"") and look for ""change*"" and these words would match."
1079,A,"How to find a match within a single term using Lucene I am using the Lucene search engine but it only seems to find matches that occur at the beginning of terms. For example: Searching for ""one"" would match ""onematch"" or ""one day a time"" but not ""loneranger"". The Lucene doc says it doesnt support wildcards at the front of a search string so I am not sure whether Lucene even searches inter-term matches or only can match documents that start with the search term. Is this a problem with how I have created my index how I am building my search query or just a limitation of Lucene? Found some info in another post here on Stack Overflow [LUCENE.NET] Leading wildcard throws an error"" You can set the SetAllowLeadingWildcardCharacters property on your Query Parser to allow leading wildcards during your search. This will of course have the obvious large performance impact but will allow user to find matches within a search term.  Your query should be ""Query query = new WildcardQuery(new Term(""contents"" ""*one *""));"" where contents is the field name in which you are searching. ""one"" should be enclosed with asterisk mark. I have given space in the query after *one but there should not be any space. without space the * is not displaying that is why I added star.  Lucene will find a document if the search term appears anywhere within it but it doesn't allow you to do wildcard queries where the wildcard is on the front of the search term because it performs horribly. If that is functionality you care about you will either have to do some low-level Lucene hacking change a config flag (thanks for the interesting link) find a third-party library that has already done that hacking or find a different search implementation (for small enough datasets the built in search from a lot of RDBMS engines is sufficient)."
1080,A,"Lucene SpanNearQuery partial matching Given a document {'foo' 'bar' 'baz'} I want to match using SpanNearQuery with the tokens {'baz' 'extra'} But this fails. How do I go around this? Sample test (using lucene 2.9.1) with the following results: givenSingleMatch - PASS givenTwoMatches - PASS givenThreeMatches - PASS givenSingleMatch_andExtraTerm - FAIL ... import org.apache.lucene.analysis.standard.StandardAnalyzer; import org.apache.lucene.document.Document; import org.apache.lucene.document.Field; import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.IndexWriter; import org.apache.lucene.index.Term; import org.apache.lucene.search.IndexSearcher; import org.apache.lucene.search.TopDocs; import org.apache.lucene.search.spans.SpanNearQuery; import org.apache.lucene.search.spans.SpanQuery; import org.apache.lucene.search.spans.SpanTermQuery; import org.apache.lucene.store.RAMDirectory; import org.apache.lucene.util.Version; import org.junit.After; import org.junit.Assert; import org.junit.Before; import org.junit.Test; import java.io.IOException; public class SpanNearQueryTest { private RAMDirectory directory = null; private static final String BAZ = ""baz""; private static final String BAR = ""bar""; private static final String FOO = ""foo""; private static final String TERM_FIELD = ""text""; @Before public void given() throws IOException { directory = new RAMDirectory(); IndexWriter writer = new IndexWriter( directory new StandardAnalyzer(Version.LUCENE_29) IndexWriter.MaxFieldLength.UNLIMITED); Document doc = new Document(); doc.add(new Field(TERM_FIELD FOO Field.Store.NO Field.Index.ANALYZED)); doc.add(new Field(TERM_FIELD BAR Field.Store.NO Field.Index.ANALYZED)); doc.add(new Field(TERM_FIELD BAZ Field.Store.NO Field.Index.ANALYZED)); writer.addDocument(doc); writer.commit(); writer.optimize(); writer.close(); } @After public void cleanup() { directory.close(); } @Test public void givenSingleMatch() throws IOException { SpanNearQuery spanNearQuery = new SpanNearQuery( new SpanQuery[] { new SpanTermQuery(new Term(TERM_FIELD FOO)) } Integer.MAX_VALUE false); TopDocs topDocs = new IndexSearcher(IndexReader.open(directory)).search(spanNearQuery 100); Assert.assertEquals(""Should have made a match."" 1 topDocs.scoreDocs.length); } @Test public void givenTwoMatches() throws IOException { SpanNearQuery spanNearQuery = new SpanNearQuery( new SpanQuery[] { new SpanTermQuery(new Term(TERM_FIELD FOO)) new SpanTermQuery(new Term(TERM_FIELD BAR)) } Integer.MAX_VALUE false); TopDocs topDocs = new IndexSearcher(IndexReader.open(directory)).search(spanNearQuery 100); Assert.assertEquals(""Should have made a match."" 1 topDocs.scoreDocs.length); } @Test public void givenThreeMatches() throws IOException { SpanNearQuery spanNearQuery = new SpanNearQuery( new SpanQuery[] { new SpanTermQuery(new Term(TERM_FIELD FOO)) new SpanTermQuery(new Term(TERM_FIELD BAR)) new SpanTermQuery(new Term(TERM_FIELD BAZ)) } Integer.MAX_VALUE false); TopDocs topDocs = new IndexSearcher(IndexReader.open(directory)).search(spanNearQuery 100); Assert.assertEquals(""Should have made a match."" 1 topDocs.scoreDocs.length); } @Test public void givenSingleMatch_andExtraTerm() throws IOException { SpanNearQuery spanNearQuery = new SpanNearQuery( new SpanQuery[] { new SpanTermQuery(new Term(TERM_FIELD BAZ)) new SpanTermQuery(new Term(TERM_FIELD ""EXTRA"")) } Integer.MAX_VALUE false); TopDocs topDocs = new IndexSearcher(IndexReader.open(directory)).search(spanNearQuery 100); Assert.assertEquals(""Should have made a match."" 1 topDocs.scoreDocs.length); } } Note: All tokens are in a single field. Thanks danben for point out that missing information. SpanNearQuery lets you find terms that are within a certain distance of each other. Example (from http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/): Say we want to find lucene within 5 positions of doug with doug following lucene (order matters) – you could use the following SpanQuery: new SpanNearQuery(new SpanQuery[] { new SpanTermQuery(new Term(FIELD ""lucene"")) new SpanTermQuery(new Term(FIELD ""doug""))} 5 true); In this sample text Lucene is within 3 of Doug But for your example the only match I can see is that both your query and the target document have ""cd"" (and I am making the assumption that all of those terms are in a single field). In that case you don't need to use any special query type. Using the standard mechanisms you will get some non-zero weighting based on the fact that they both contain the same term in the same field. Edit 3 - in response to latest comment the answer is that you cannot use SpanNearQuery to do anything other than that which it is intended for which is to find out whether multiple terms in a document occur within a certain number of places of each other. I can't tell what your specific use case / expected results are (feel free to post it) but in the last case if you only want to find out whether one or more of (""BAZ"" ""EXTRA"") is in the document a BooleanQuery will work just fine. Edit 4 - now that you have posted your use case I understand what it is you want to do. Here is how you can do it: use a BooleanQuery as mentioned above to combine the individual terms you want as well as the SpanNearQuery and set a boost on the SpanNearQuery. So the query in text form would look like: BAZ OR EXTRA OR ""BAZ EXTRA""~100^5 (as an example - this would match all documents containing either ""BAZ"" or ""EXTRA"" but assign a higher score to documents where the terms ""BAZ"" and ""EXTRA occur within 100 places of each other; adjust the position and boost as you like. This example is from the Solr cookbook so it may not parse in Lucene or may give undesirable results. That's ok because in the next section I show you how to build this using the API). Programmatically you would construct this as follows: Query top = new BooleanQuery(); // Construct the terms since they will be used more than once Term bazTerm = new Term(""Field"" ""BAZ""); Term extraTerm = new Term(""Field"" ""EXTRA""); // Add each term as ""should"" since we want a partial match top.add(new TermQuery(bazTerm) BooleanClause.Occur.SHOULD); top.add(new TermQuery(extraTerm) BooleanClause.Occur.SHOULD); // Construct the SpanNearQuery with slop 100 - a document will get a boost only // if BAZ and EXTRA occur within 100 places of each other. The final parameter means // that BAZ must occur before EXTRA. SpanNearQuery spanQuery = new SpanNearQuery( new SpanQuery[] { new SpanTermQuery(bazTerm) new SpanTermQuery(extraTerm) } 100 true); // Give it a boost of 5 since it is more important that the words are together spanQuery.setBoost(5f); // Add it as ""should"" since we want a match even when we don't have proximity top.add(spanQuery BooleanClause.Occur.SHOULD); Hope that helps! In the future try to start off by posting exactly what results you are expecting - even if it is obvious to you it may not be to the reader and being explicit can avoid having to go back and forth so many times. Re edit 2: Yes exactly :) which brings us back to my original question which is how do I do partial matching using SpanNearQuery (or some proximity-aware query). Re edit3: Re SpanNearQuery - thanks. Which is why I state it does not work and which is why I ask how to go around it? Re my specific use case: it is what it is :) Given the terms I need to make matches wherein I give a higher score if they're together (means it's most likely what the user is searching for). Yet I need it to be lax enough such that if not all the terms are found they are returned in the search result (but still higher proximity means higher score). Thanks! That's what I did as well (except that my boost factor is equal to the number of tokens to compensate for the higher scores or-queries usually give out). Sometimes though the results makes sense and sometimes it does not. I guess I need to find out what those other factors are. Thanks! Re your tip: Will keep that in mind. Thank you for your patience ! :-) The in-line image explaining distance is a nice touch. I modified my post again and clarified the requested information - the first 3 passes and the fourth fails. That's what I initially assumed as well. However the document in question does not get returned from my search. Maybe you could post some code showing how you're searching? Kindly see a simplified version of the problem I'm pertaining to."
1081,A,How can I retrieve non-stored Lucene field values? When searching only stored fields are returned from a search. For debugging reasons I need to see the unstored fields too. Is there a way via the API? Thanks! P.S.: I know Luke unfortunately I can't use it in my case. If the unstored fields were stored… they'd be called stored fields right? For unstored fields all you can see are the tokenized keywords as they were indexed and that requires un-inverting the inverted index. Using the IndexReader API you can enumerate all of the unique terms in a particular field. Then for each term you can enumerate the documents that contain the term. This tells you roughly the value of specified field of a given document. Depending on the analysis performed on the field during indexing this may allow you to reconstruct the original field exactly or merely give you an rough idea of what it may have contained. This seems correct. I've read sone Luke source code and they do it exactly like this. Was hoping for something faster... Anyway thanks!
1082,A,CakePHP with Lucene I am trying to implement Lucene with cakephp and following this guide http://jamienay.com/2010/01/zend_search_lucene-datasource-for-cakephp/ Am getting this error ConnectionManager::loadDataSource - Unable to import DataSource class .ZendSearchLuceneSource i have placed the Vendor files in app/vendors/Zend/ Added this in the bootstrap.php ini_set('include_path' ini_get('include_path') . ':' . CAKE_CORE_INCLUDE_PATH . DS . '/vendors'); /** * AutoLoading Zend Vendor Files */ function __autoload($path) { if(substr($path 0 5) == 'Zend_') { include str_replace('_' '/' $path) . '.php'; } return $path; } added this to the Database Config var $zendSearchLucene = array( 'datasource' => 'ZendSearchLucene' 'indexFile' => 'lucene' // stored in the cache dir. 'driver' => '' 'source' => 'search_indices' ); Add created a model called search.php <?php class Search extends AppModel { var $useDbConfig = 'zendSearchLucene'; } ?> Right now i have created a controller called search too like this <?php class SearchController extends AppController { var $name = 'Search'; function index(){ } } ?> when i visit site/search am getting that error. have done this also copied zend_search_lucene.php to models/datasources Not sure if this is still relevant to you but I have just begun using the same datasource and came across the same issues. I updated the datasource for Cake 1.3 and it should work now. Have a look at my fork of the project at Github: https://github.com/deceze/zend_search_lucene_source If you find any problems with it please open tickets for them. I'll see if I can get around to fixing them. The datasource is a good basis but may need some updating and extension. thanks a lot. will look into it and get abck to you :D
1083,A,"mahout lucene document clustering howto? I'm reading that i can create mahout vectors from a lucene index that can be used to apply the mahout clustering algorithms. http://cwiki.apache.org/confluence/display/MAHOUT/Creating+Vectors+from+Text I would like to apply K-means clustering algorithm in the documents in my Lucene index but it is not clear how can i apply this algorithm (or hierarchical clustering) to extract meaningful clusters with these documents. In this page http://cwiki.apache.org/confluence/display/MAHOUT/k-Means says that the algorithm accepts two input directories: one for the data points and one for the initial clusters. My data points are the documents? How can i ""declare"" that these are my documents (or their vectors)  simply take them and do the clustering? sorry in advance for my poor grammar Thank you @ maiky You can read more about reading the output and using clusterdump utility in this page -> https://cwiki.apache.org/confluence/display/MAHOUT/Cluster+Dumper  If you have vectors you can run KMeansDriver. Here is the help for the same. Usage: [--input <input> --clusters <clusters> --output <output> --distance <distance> --convergence <convergence> --max <max> --numReduce <numReduce> --k <k> --vectorClass <vectorClass> --overwrite --help] Options --input (-i) input The Path for input Vectors. Must be a SequenceFile of Writable Vector --clusters (-c) clusters The input centroids as Vectors. Must be a SequenceFile of Writable Cluster/Canopy. If k is also specified then a random set of vectors will be selected and written out to this path first --output (-o) output The Path to put the output in --distance (-m) distance The Distance Measure to use. Default is SquaredEuclidean --convergence (-d) convergence The threshold below which the clusters are considered to be converged. Default is 0.5 --max (-x) max The maximum number of iterations to perform. Default is 20 --numReduce (-r) numReduce The number of reduce tasks --k (-k) k The k in k-Means. If specified then a random selection of k Vectors will be chosen as the Centroid and written to the clusters output path. --vectorClass (-v) vectorClass The Vector implementation class name. Default is SparseVector.class --overwrite (-w) If set overwrite the output directory --help (-h) Print out help Update: Get the result directory from HDFS to local fs. Then use ClusterDumper utility to get the cluster and list of documents in that cluster. yes that is i cant understand. What is the output? how can i view in the output that for example documents 5 and 8 are on the same cluster?  A pretty good howto is here: integrating apache mahout with apache lucene"
1084,A,"storing n+1 objects in a solr document I'm struggling to work out how the best way to store n+1 object in a solr document. I am storing a CV/resume document in a solr document. I am looking at storing two different data types ""education"" and ""employment"" If we look at education the object looks like this: { ""establishment"" => 'Oxford' ""Subject"" => 'Computing' ""Type"" => 'Degree' ""Grade"" => '2:1' } A CV can have n+1 of these objects depending on the contents of the CV. The search needs to be able to see that when I search for CV with Establishment = Oxford & Subject = Computing & Grade = 2:1 that it matches this object not a different establishment with the same subject and grade. A multivalue I don't think would help or is possible to store n+1 of these types of objects. My question is how to set up solr to be able to store this type of data against one ""CV"" Solr document so that it is search able as part of a general search of the index? You essentially want to turn Solr into a relational database. I.e. you want to enforce some structure on your documents rather than having them just be a bag of words. If you need relations then you need relations. The only way I can think of accomplishing this is to index education objects separately and then have a ""foreign key"" from the resume. Alternatively it seems likely that your ""n"" will be pretty small. So you could just include each resume in the index multiple times once with each education listing. This might throw off scoring a bit but ymmv. Looks like the only way of doing this is to add it to the index multiple time. Thanks for your help"
1085,A,"Document search on partial words I am looking for a document search engine (like Xapian Whoosh Lucene Solr Sphinx or others) which is capable of searching partial terms. For example when searching for the term ""brit"" the search engine should return documents containing either ""britney"" or ""britain"" or in general any document containing a word matching r*brit* Tangentially I noticed most engines use TF-IDF (Term frequency-Inverse document frequency) or its derivatives which are based on full terms and not partial terms. Are there any other techniques that have been successfully implemented besides TF-IDF for document retrieval? Thanks for the suggestion shelter. Added more tags. Any reason you have not read the documentation of the various engines. Lucene (and therefore Solr) support wildcard searches: http://wiki.apache.org/lucene-java/LuceneFAQ#What_wildcard_search_support_is_available_from_Lucene I reccomend that you add a search engine tag to your question lucene Xapian or at least search-engine. Search is a general tag people that are into search-engines may get tired reading all sorts of weird requests for non search-engine related questions. Good Luck! With lucene you would be able to implement this in several ways: 1.) You can use wildcard queries *brit* (You would have to set your query parser to allow leading wild cards) 2.) You can create an additional field containing N-Grams of all the terms. (http://lucene.apache.org/java/3_1_0/api/all/org/apache/lucene/analysis/ngram/package-summary.html). This would result in larger indexes but would be in many cases faster (search speed). 3.) You can use fuzzy search to handle typing mistakes in the query. e.g. someone typed britnei but wanted to find britney. For wildcard queries and fuzzy search have a look at the query syntax: http://lucene.apache.org/java/3_1_0/queryparsersyntax.html br chris"
1086,A,"Indexing and searching MySQL with solr (I have put ' in the XML below to make it display) Hi all I want to index my MySQL db table with solr. I have installed the necessary java components/adaptors etc. My database is called 'test_db' and the table in it is called 'table_tb'. The table contains 2 columns (fields) -Field 1 is called 'ID' and is an autoincremented primary key integer -Field 2 is called 'COLA' and is text The table has two rows (records) ID=1 and ID=2 with some text against each corresponding to the second column. I have setup the following conf files (they are in the correct directories): data-config.xml <dataConfig> <dataSource type=""JdbcDataSource"" driver=""com.mysql.jdbc.Driver"" url=""jdbc:mysql://localhost/test_db"" user=""username"" password=""db_pwd""/> <document name=""doc""> <entity name=""test_tb"" query=""select ID from test_tb""> <field column=""ID"" name=""ID"" />   <field column=""COLA"" name=""COLA"" /> </entity> </document> </dataConfig> solrconfig.xml <requestHandler name=""/dataimport"" class=""org.apache.solr.handler.dataimport.DataImportHandler""> <lst name=""defaults""> <str name=""config"">data-config.xml</str> </lst> </requestHandler> schema.xml  <fields> <field name=""ID"" type=""int"" indexed=""true"" stored=""true"" required=""true""/> <field name=""COLA"" type=""string"" indexed=""true"" stored=""true"" required=""true""/>  </fields> <uniqueKey>ID</uniqueKey> ""[URL]:8983/solr/dataimport?command=full-import"" in my browser I get the following outputs: (1) browser output (xml) <response> − <lst name=""responseHeader""> <int name=""status"">0</int> <int name=""QTime"">1</int> </lst> − <lst name=""initArgs""> − <lst name=""defaults""> <str name=""config"">data-config.xml</str> </lst> </lst> <str name=""command"">full-import</str> <str name=""status"">idle</str> <str name=""importResponse""/> − <lst name=""statusMessages""> <str name=""Total Requests made to DataSource"">1</str> <str name=""Total Rows Fetched"">2</str> <str name=""Total Documents Skipped"">0</str> <str name=""Full Dump Started"">2010-08-03 16:15:51</str> − <str name=""""> Indexing completed. Added/Updated: 0 documents. Deleted 0 documents. </str> <str name=""Committed"">2010-08-03 16:15:51</str> <str name=""Optimized"">2010-08-03 16:15:51</str> <str name=""Total Documents Processed"">0</str> <str name=""Total Documents Failed"">2</str> <str name=""Time taken "">0:0:0.32</str> </lst> − <str name=""WARNING""> This response format is experimental. It is likely to change in the future. </str> </response> Suggesting 2 records were read but not indexed Server-side output WARNING: Error creating document : SolrInputDocument[{ID=ID(1.0)={1}}] org.apache.solr.common.SolrException: Document [null] missing required field: id WARNING: Error creating document : SolrInputDocument[{ID=ID(1.0)={2}}] org.apache.solr.common.SolrException: Document [null] missing required field: id Does anyone know what I am doing wrong? Thanks in advance for any help!!! use the **Code Sample** button to format XML I'm having a similar problem. Where did you see those error messages? @Muc the errors are output in the tomcat logs: yourTomcatdir/logs/catalina.somedate.log You need to have synchronized: scheme.xml and what is in data-config.xml (definition of fields needs to be the same)  <entity name=""test_tb"" query=""select ID from test_tb""> <field column=""ID"" name=""ID"" /> <field column=""COLA"" name=""COLA"" /> </entity> it should be query=""select * from test_tb"" i think. if you execute the query select id from test_tb you will get only one column while you want two.  A field 'id' was present elsewhere in the document. I commented this out and it worked."
1087,A,"Unable to restrict custom Sitecore Lucene index to /sitecore/content/Home I am trying to create a new Lucene index on a site running Sitecore 6.3.1. I used the existing ""system"" index as a guide and I was successfully able to create a new index on web and master to index all items in the Sitecore content tree. Where I am running into difficulty however is limiting which part of the content tree the database crawler indexes. Currently the search index contains items from everywhere in the content tree (content items media library items layouts templates etc.). I would like to limit the index to only items in /sitecore/content/Home. I have created a file at ~/App_Config/Include/Search Indexes/website.config and I have pasted relevant sections below: <?xml version=""1.0"" encoding=""utf-8"" ?> <configuration> <sitecore> <!-- This works as expected.... --> <databases> <database id=""web""> <indexes hint=""list:AddIndex""> <index path=""indexes/index[@id='website']"" /> </indexes> </database> <!-- ... similar entry for master database goes here ... --> </databases> <!-- So does this.... --> <indexes> <index id=""website"" singleInstance=""true"" type=""Sitecore.Data.Indexing.Index Sitecore.Kernel""> <param desc=""name"">$(id)</param> <fields hint=""raw:AddField""> <!-- ... field descriptions go here ... --> </fields> </index> </indexes> <!-- This works... mostly. The ""__website"" directory does get created but the Root directive is getting ignored. --> <search> <configuration type=""Sitecore.Search.SearchConfiguration Sitecore.Kernel"" singleInstance=""true""> <indexes hint=""list:AddIndex""> <index id=""website"" singleInstance=""true"" type=""Sitecore.Search.Index Sitecore.Kernel""> <param desc=""name"">$(id)</param> <param desc=""folder"">__$(id)</param> <Analyzer ref=""search/analyzer"" /> <locations hint=""list:AddCrawler""> <web type=""Sitecore.Search.Crawlers.DatabaseCrawler Sitecore.Kernel""> <Database>web</Database> <Root>/sitecore/content/home</Root> <Tags>content</Tags> </web> <!-- ... similar entry for master database goes here ... --> </locations> </index> </indexes> </configuration> </search> </sitecore> </configuration> A couple of notes: This is not from my web.config file; I created a separate file so that I could distribute config changes via Sitecore packages. The index was added to both master and web; I omitted the references to master for brevity. Sitecore is definitely processing the entries for configuration/sitecore/search/configuration. I can see them when I go to http://localhost/sitecore/admin/showconfig.aspx and if I change one of the tag values to something invalid (e.g. <Root>/nothere</Root>) Sitecore throws an Exception on the next page load. I have reviewed the index contents in IndexViewer and the wrong items are definitely getting indexed (for example document #0 in the index is the /sitecore node). Where am I going wrong? What changes do I need to make to my configuration file to get the search indexer to ignore items outside /sitecore/content/Home? This was originally posted on the [SDN forums](http://sdn.sitecore.net/forum/ShowPost.aspx?PostID=34197). I was able to solve the problem using the Advanced Database Crawler. Switching out the configuration/search/configuration block with the code provided in Alex's presentation (see above link) made everything start to work more or less automagically."
1088,A,"the store attribute of a lucene field There is a constructor of lucene Field: Field(String name String value Store store Index index) For example I can create a new filed by: Field f1 = new Field(""text"" ""The text content"" Field.Store.YES Field.Index.ANALYZED); NowI am not exactly sure of the meaning of the fourth parameter: Index If I set it to Index.Noso is it needful to add this field as a ""field""? Since in my opiniononce a attribute is declared as a field it should be Indexedif not then why do you declare it as a field? BTWwhat is the difference between query and search? As mentioned in Lucene FAQ:  What is the different between Stored Tokenized Indexed and Vector? - Stored = as-is value stored in the Lucene index - Tokenized = field is analyzed using the specified Analyzer - the tokens emitted are indexed - Indexed = the text (either as-is with keyword fields or the tokens from tokenized fields) is made searchable (aka inverted) - Vectored = term frequency per document is stored in the index in an easily retrievable fashion. You can just index field content without store it the field is also searchable just can't highlight the result because highlight requires original message content which should Store. So why do one create a field without indexing it? For example store the postid index the post content then you could load those posts by postid after search lucene.  Stored fields are what is returned when you ask Lucene to give you back a document. They hold the original value of a field with no analysis. You can use them to present the document to the users (not necessarily all fields). Stored fields that are not indexed are useful to store meta-data about a document that the user won't use to query the index. An example might be a database id where a document comes from. This id will never be used by the user since they does not know about it so it is generally useless to index it. But if you store it so you can use it to gather extra information from your db at runtime. The difference between a query and a search is rather subjective. For myself a search is really the general act of searching in the index while a query is the actual query string used to search the index."
1089,A,Is there any recommended IndexSearcher method? I'm using Lucene search API in a web based application. Which method of Lucene's IndexSearcher class is recommended to use?Is any method faster than other? 1.IndexSearcher(Directory directory) 2.IndexSearcher(IndexReader r) 3.IndexSearcher(String path) Thanks for reading. You have 19 open questions and you haven't accepted a single answer. Please consider voting up and accepting answers The constructor which accepts Directory and path to index internally use the constructor that accpets IndexReader. So there is no performance advantage of one over others. Keep in mind that if you create searcher with IndexReader you have to close the reader explicitly after you close the searcher.  It's all about convenience. If you just want to create an IndexSearcher use the one that accepts a path. If you already have a Directory object use the one that accepts a Directory. And if you have an IndexReader... you get the point. Just remember that if you provided an IndexReader you're expected to close it yourself after you've closed the IndexSearcher. I highly recommend grabbing a copy of the Lucene source code. It is very readable and can answer a lot of these questions. Can you please answer this one? http://stackoverflow.com/questions/899542/problem-using-same-instance-of-indexsearcher-for-multiple-requests
1090,A,"Lucene Search with Unicode Characters I have indexed a database of some texts and the database texts are of unicode encoding. When I search an english word with lucene search everything goes OK. But when I use a non-English query like: ""تو"" it gives me the following exception: Exception in thread ""main"" org.apache.lucene.queryParser.ParseException: Cannot parse '??': '' or '?' not allowed as first character in WildcardQuery at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:187) at Search.main(Search.java:151) Caused by: org.apache.lucene.queryParser.ParseException: '' or '?' not allowed as first character in WildcardQuery at org.apache.lucene.queryParser.QueryParser.getWildcardQuery(QueryParser.java:923) at org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1347) at org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1250) at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:1178) at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:1167) at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:182) ... 1 more What should I do? Thank you. two points here - What is the encoding type of your srouce file (*.java). Make sure it is utf-8 The default encoding of Java is likely to be something other than utf8. Make sure you specify the encoding like: InputStreamReader( new FileInputStream(filename) ""UTF-8"");` Good idea but I have checked that before. It is not the source of problem. I checked it again. You are right. This was the problem."
1091,A,"How to programmatically add an index column for NHibernate Search (Lucene.net) without using FieldAttribute I'm trying to find out how to programmatically (i.e. without using the FieldAttribute) add an index column for NHibernate Search (Lucene.net). I'm having inheritance issues due to the fact that the FieldAttribute is not automatically inherited. The following code illustrates what I want to do. class A { [Field(Index.Tokenized)] public virtual string P1 { get { return ""P1""; } } } class B : A { public override string P1 { get { return ""P1+""; } } } I expected the override of P1 to be indexed but it didn't. When I inspected the FieldAttribute class I found that it didn't have Inherited = true specified in the AttributeUsage attribute. I then added a FieldAttribute to the overridden property but that resulted in NHibernate Search bailing out with an exception stating that an item with the same key has already been added to a dictionary. I figure that's because there's two equally named properties both with a FieldAttribute on them in the type chain and it accepts only one. So how do I programmatically solve this by not using FieldAttribute? I've just started a Fluent NHibernate.Search mapping interface similar to FluentNHibarnate which allow you to map your entities without attributes. public class BookSearchMap : DocumentMap<Book> { public BookSearchMap() { Id(p => p.BookId).Field(""BookId"").Bridge().Guid(); Name(""Book""); Boost(500); Analyzer<StandardAnalyzer>(); Map(x => x.Title) .Analyzer<StandardAnalyzer>() .Boost(500); Map(x => x.Description) .Boost(500) .Name(""Description"") .Store().Yes() .Index().Tokenized(); } } You should take a look on the project site hosted on codeplex. http://fnhsearch.codeplex.com/  Non-attribute mapping was recently implemented take a look at this blog post. Great I didn't catch that edit but I'm delighted to see that patch got in!"
1092,A,"Troubleshoot Java Lucene ignoring Field We're currently using Lucene 2.1.0 for our site search and we've hit a difficult problem: one of our index fields is being ignored during a targeted search. Here is the code for adding the field to a document in our index: // Add market_local to index contactDocument.add( new Field( ""market_local""  StringUtils.objectToString( currClip.get( ""market_local"" ) )  Field.Store.YES  Field.Index.UN_TOKENIZED ) ); Running a query ( * ) against the index will return the following results: Result 1: title: Foo Bar market_local: Local Result 2: title: Bar Foo market_local: National Running a targeted query: +( market_local:Local ) won't find any results. I realize this is a highly specific question I'm just trying to get information on where to start debugging this issue as I'm a Lucene newbie. UPDATE Installed Luke checking out latest index... the Field *market_local* is available in searches so if I execute something like: market_local:Local The search works correctly (in Luke). I'm going over our Analyzer code now is there any way I could chalk this issue up to the fact that our search application is using Lucene 2.1.0 and the latest version of Luke is using 2.3.0? For debugging Lucene the best tool to use is Luke which lets you poke around in the index itself to see what got indexed carry out searches etc. I recommend downloading it pointing it at your index and seeing what's in there. My eyes bleed... Luke is ugly! (But it does the trick. Thanks. +1)  Another simple thing to do would be to use a debugger or logging statement to check the value of StringUtils.objectToString(currClip.get(""market_local"")) to make sure it is what you think it is.  Luke is bundled with Lucene but you can tell Luke to use another version of Lucene. Say ""lucene-core-2.1.0.jar"" contains Lucene 2.1.0 that you want to use and ""luke.jar"" contains Luke with Lucene 2.3.0. Then you can start Luke with the following command. java -classpath lucene-core-2.1.0.jar;luke.jar org.getopt.luke.Luke (The trick is to put your version of Lucene before Luke on the classpath. Also This is on Windows. On Unix replace "";"" with "":"".) As you can check in Luke +( market_local:Local ) gets rewritten to market_local:Local if the rewrite(IndexReader) method of the Query object is called. The two queries should be equivalent so there might be a bug in 2.1. If you have to use 2.1 you can try to manually call that method before passing the Query object to the IndexSearcher.  The section on ""Why am I getting no hits?"" in the Lucene FAQ has some suggestions you might find useful. You're using Field.Index.UN_TOKENIZED so no Analyzer will be used for indexing (I think). If you're using an Analyzer when you're searching then that might be the root of your problem - the indexing and searching Analyzers should be the same to make sure you get the right hits."
1093,A,"Boosting Multi-Value Fields I have a set of documents containing scored items that I'd like to index. Our data structure looks like: Document ID Text List<RelatedScore> RelatedScore ID Score My first thought was to add each RelatedScore as a multi-value field using the Boost property of the Field to modify the value of the particular score when searching. foreach (var relatedScore in document.RelatedScores) { var field = new Field(""RelatedScore"" relatedScore.ID Field.Store.YES Field.Index.UN_TOKENIZED); field.SetBoost(relatedScore.Score); luceneDoc.Add(field); } However it appears that the ""Norm"" that is calculated applies to the entire multi-field - all the RelatedScore"" values for a document will end up having the same score. Is there a mechanism in Lucene to allow for this functionality? I would rather not create another index just to account for this - it feels like there should be a way using a single index. If there isn't a means to accomplish this a few ideas that we have to compensate are : Insert the multi-value field items in order of descending value. Then somehow add a positional-aware analysis to assign higher boost/score to the first items in the field. Add a high value score multiple times to the field. So a RelatedScore with Score==1 might be added three times while a RelatedScore with Score==.3 would only be added once. Both of these will result in a loss of search fidelity on these fields yes but they may be good enough. Any thoughts on this? This appears to be a use case for Payloads. I'm not sure if this is available in Lucene.NET as I've only used the Java version. Another hacky way to do this if the absolute values of the scores aren't that important is to discretize them (place them in buckets based on value) and create a field for each bucket. So if you have scores that range from 1 to 100 create say 10 buckets called RelatedScore0_10 RelatedScore10_20 etc and for any document that has a RelatedScore in that bucket add a ""true"" value in that field. Then for every search that gets executed tack on an OR query like: (RelatedScore0_10:true^1 RelatedScore10_20:true^2 ...) The nice thing about this is that you can tweak the boost values for each one of your buckets on the fly. Otherwise you'd need to reindex to change the field norm (boost) values for each field. I was able to use the Payload to store the score and use it via a custom similarity object. Once I started looking in this direction I found Grant Ingersoll recently posted an article on exactly this topic here: http://www.lucidimagination.com/blog/2009/08/05/getting-started-with-payloads/  If you use Lucene.Net you might not have payloads functionality yet. What you can do is convert 0-100 relevancy score to a bucket from 1-10 (integer division by 10) then add each indexed value that many times (but only store value once). Then if you search for that field lucene built-in scoring will take into account frequency of indexed field (it will be indexed 1-10 times based on relevance). Therefore results can be sorted by variable relevance. foreach (var relatedScore in document.RelatedScores) { // get bucket for relevance... int bucket=relatedScore.Score / 10; var field = new Field(""RelatedScore"" relatedScore.ID Field.Store.YES Field.Index.UN_TOKENIZED); luceneDoc.Add(field); // add more instances of field but only store the first one above... for(int i=0;i<bucket;i++) { luceneDoc.Add(new Field(""RelatedScore"" relatedScore.ID Field.Store.NO Field.Index.UN_TOKENIZED)); } }"
1094,A,"Lucene exact ordering I've had this long term issue in not quite understanding how to implement a decent Lucene sort or ranking. Say I have a list of cities and their populations. If someone searches ""new"" or ""london"" I want the list of prefix matches ordered by population and I have that working with a prefix search and an sort by field reversed where there is a population field IE New Mexico New York; or London Londonderry. However I also always want the exact matching name to be at the top. So in the case of ""London"" the list should show ""London London Londonderry"" where the first London is in the UK and the second London is in Connecticut even if Londonderry has a higher population than London CT. Does anyone have a single query solution? dlamblinlet me see if I get this correctly: You want to make a prefix-based query and then sort the results by population and maybe combine the sort order with preference for exact matches. I suggest you separate the search from the sort and use a CustomSorter for the sorting: Here's a blog entry describing a custom sorter. The classic Lucene book describes this well. Thank you for your blog post explaining how to implement a sort comparator that conveniently does not require defining 2 classes. However because the sort comparator can only work on two documents without knowing the search term it cannot rank the results as I've described them in my question. How would the sort comparator know that the name field ""london"" exactly matches the search term ""london"" if it cannot access the search term? I think you can do the following: The class implementing the ScoreDocComparator interface (AZ09Comparator in the blog example) will have a ""search term"" member to be set when running the query. The comparing method (compare() in the blog example) can access this field during the time it is called and rank a document with an exact match higher than another not having an exact match. Dang that's what I get for not thinking it through (though it's been a while since I was in front of that code). Now this makes a lot more sense and is helpful.  API for Sortcomparator says There is a distinct Comparable for each unique term in the field - if some documents have the same term in the field the cache array will have entries which reference the same Comparable You can apply a FieldSortedHitQueue to the sortcomparator which has a Comparator field for which the api says ... Stores a comparator corresponding to each field being sorted by. Thus the term can be sorted accordingly  My current solution is to create an exact searcher and a prefix searcher both sorted by reverse population and then copy out all my hits starting from the exact hits moving to the prefix hits. It makes paging my results slightly more annoying than I think it should be. Also I used a hash to eliminate duplicates but later changed the prefix searcher into a boolean query of a prefix search (MUST) with an exact search (MUST NOT) to have Lucene remove the duplicates. Though this seemed even more wasteful. Edit: Moved to a comment (since the feature now exists): Yuval F Thank you for your blog post ... How would the sort comparator know that the name field ""london"" exactly matches the search term ""london"" if it cannot access the search term?"
1095,A,"Lucene Analyzer to Use With Special Characters and Punctuation? I have a Lucene index that has several documents in it. Each document has multiple fields such as: Id Project Name Description The Id field will be a unique identifier such as a GUID Project is a user's ProjectID and a user can only view documents for their project and Name and Description contain text that can have special characters. When a user performs a search on the Name field I want to be able to attempt to match the best I can such as: First Will return both: First.Last and First.Middle.Last Name can also be something like: Test (NameTest) Where if a user types in 'Test' 'Name' or '(NameTest)' then they can find the result. However if I say that Project is 'ProjectA' then that needs to be an exact match (case insensitive search). The same goes with the Id field. Which fields should I set up as Tokenized and which as Untokenized? Also is there a good Analyzer I should consider to make this happen? I am stuck trying to decide the best route to implement the desired searching. Your ID field should be untokenized for simple reason it does not appear it can be tokenized (whitespace based) unless you write your own tokenizer. You can Tokenize all your other fields. Perform a phrase query on the project name look up PhraseQuery or enclose your project name in double quotes (which will make it match exactly). Example: ""\""My Fancy Project""\"" For the name field a simple query should work fine. Unsure if there are situations where you want a combination of fields. In that situation look up BooleanQuery (which allows you to combine different queries boolean-ly) I do plan on being able to do a boolean query across both Name and Description for something like 'test'. In that case I want to return all documents that contain test in either field. I would like my queries scoped by a project Id. Example: (name or description contains 'test') AND project id = 3 (exact match) I presume project Id would be untokenized and name and description would be tokenized using a standard analyzer. Would a standard booleanquery using the QueryParser class achieve my goal? Yes the above should work. If your project id is likely to be just a number or some identifier (a ""term"" in Lucene terms) you can use a TermQuery. I followed what you said but am running into a bit of a hiccup. When inserting a tokenized field I escape the special characters. When performing the search using a QueryParser I escape the search value before performing the search using a StandardAnalyzer. One problem is that if I have 2 objects in my index and their names are 'Test' and 'Test (Test)' respectively when I perform a search for 'Test (Test)' and escape the special characters I get back both objects. I know it is creating 2 terms 'Test' and '\\(Test\\)' from my input but it doesn't make sense why it gets both. I should add that I picture it would perform the 'AND' operation on the terms to match documents with a Field/Value pair that met all the Term criteria."
1096,A,"ask for a design pattern We are working on web application with a search model. In the search servlet it capture the request parameters from the client and then build a hibernate-search query for searching. Now the problem is the parameters from the clien are mutable!. All the parameters we accepts are listed as following: 1)keyword. The keyword(s) using for searchinga search request can be processed even just this parameter is passed. Valid example: /search?keyword=""test"" 2)lowleftXlowleftYupperrightXupperrightY. These four parameter must occur at the same time or never.Since these four paramers are used for a TermRangeQuery in lucene.If one of them occurthe rest three must be occur also. Andthese four parameter can occur with the ""keyword"" at the sametime. Valid example: /search?lowleftX=10&lowleftY=10&upperrightX=40&upperrightY=30 /search?lowleftX=10&lowleftY=10&upperrightX=40&upperrightY=30&keyword=""test"" 3)category This is used to limit the search scope(just search within the special category). 4)startlimit These two parameters are using for paging. 5)returnFields The returnFields which will be retivived from the index(if it is Stored in the index) and return to the client. So I have no idea about how to build the query using the estimate syntax(if....else....if...). Anyone can tell me how ? I have no idea what you mean with ""estimate syntax"" but it seems to me that point 1 -3 are the actual Lucene query. You would have to inspect the parameters and decide depending on the name and number of parameters which type of query you have. Using the different sub classes of Query in particular BooleanQuery you then build an appropriate Lucene query and use it to create a Hibernate Search FullTextQuery. On this fulltext query you specify the start and limit paramters. If you are using projections to retrieve the field values directly from the index you also set the projected field names on the fulltext query. I hope this helps a little."
1097,A,"Is it possible to re-generate Lucene index in background? Sometimes there is need to re-generate a lucene index e.g. when something changes in the Compass mapping or in the way boosts are applied or if something went corrupt for whatever reason. In my case generation of the index takes about 5 to 6 hours clearing the index before leads to data not being complete for this interval. I. e. doing a search in this time returns an incomplete result. Is there any standard way to have lucene generate the index in the background? E.g. write index to a temporary directory and (when indexing is finished without exceptions etc) replace the existing index with the new one? Of course one could implement this ""manually"" but does one have to? Sounds like a common use case to me. Best regards + Thanks for your opinion Peter :) We have a similar problem. Our data is indexed in Lucene but the original source is DB and content repo. So if an index goes out of sync (or data type changes etc.) we simply iterate over all existing entries in the index and re-generate the data so each document gets updated. It is not really a complex thing to do.  I had a similar experience; there were certain parameters to the Analyzer which would get changed from time to time; obviously if that was the case the entire index needs to get rebuilt. (I won't go into the details suffice to say I had the same requirement!) I did what you suggested in your question. There were three directories ""old"" ""current"" and ""new"". Queries from the live site went against ""current"" always. The index recreation process was: Recursive delete on the ""old"" and ""new"" directories Create the new index into the ""new"" directory (in my case takes about 6 hrs) Rename ""current"" to ""old""; and ""new"" to ""current"" Recursive delete the ""old"" directory An analysis of what happens when the process crashes - if it crashes in the 1st step the next time it will just carry on. If it crashes in the 2nd step then the ""new"" directory will get deleted next run. The 3rd step is very fast - renaming a directory is fast and atomic. Crashing in the 4th step doesn't matter it'll just get cleaned up next run. The careful observer will note that in step 3 the system could crash between renaming the current directory away and moving the new directory in. This is unlikely to happen as directory rename is so fast. The system has been in production for a few years and this has never happened (yet?).  I think the usual way to do this is to use solr's replication functionality. In your case though the master and slave would be on the same machine but just pointed at different directories."
1098,A,"Lucene search and underscores When I use Luke to search my Lucene index using a standard analyzer I can see the field I am searchng for contains values of the form MY_VALUE. When I search for field:""MY_VALUE"" however the query is parsed as field:""my value"" Is there a simple way to escape the underscore (_) character so that it will search for it? EDIT: 4/1/2010 11:08AM PST I think there is a bug in the tokenizer for Lucene 2.9.1 and it was probably there before. Load up Luke and try to search for ""BB_HHH_FFFF5_SSSS"" when there is a number the following tokens are returned: ""bb hhh_ffff5_ssss"" After some testing I've found that this is because of the number. If I input ""BB_HHH_FFFF_SSSS"" I get ""bb hhh ffff ssss"" At this point I'm leaning towards a tokenizer bug unless the presence of the number is supposed to have this behavior but I fail to see why. Can anyone confirm this? I don't think you'll be able to use the standard analyser for this use case. Judging what I think your requirements are the keyword analyser should work fine for little effort (the whole field becomes a single term). I think some of the confusion arises when looking at the field with luke. The stored value is not what's used by queries what you need are the terms. I suspect that when you look at the terms stored for your field they'll be ""my"" and ""value"". Hope this helps  It doesn't look like you used the StandardAnalyzer to index that field. In Luke you'll need to select the analyzer that you used to index that field in order to match MY_VALUE correctly. Incidentally you might be able to match MY_VALUE by using the KeywordAnalyzer. If you indexed using the Standard Analyzer then your index will contain ""my"" and ""value"" as two different tokens. Try searching for ""my value"" (including the quotes) and you might get results. No I did use the standard analyzer as the indexer which is why this is weird. I would double-check which analyzer you're using for indexing. If you've used the StandardAnalyzer for indexing it's impossible to have MY_VALUE as a term since StandardAnalyzer always splits on underscores."
1099,A,"What is the best way to search multiple sources simultaneously? I'm writing a phonebook search that will query multiple remote sources but I'm wondering how it's best to approach this task. The easiest way to do this is to take the query start a thread per remote source query (limiting max results to say 10) waiting for the results from all threads and aggregating the list into a total of 10 entries and returning them. BUT...which of the remote source is more important if all sources return at least 10 results so then I would have to do a search on the search results. While this would yield accurate information it seems inefficient and unlikely to scale up well. Is there a solution commercial or open source that I could use and extend or is there a clever algorithm I can use that I've missed? Thanks To be honest I haven't seen a ready solution but this is why we programmers exist: to create a solution if one is not readily availble :-) The way I would do it is similar to what you describe: using threads - if this is a web application then ajax is your friend for speed and usability for a desktop app gui representation is not even an issue. It sounds like you can't determine or guess upfront which source is the best in terms of reliability speed & number of results. So you need to setup you program so that it determines best results on the fly. Let's say you have 10 data sources and therfore 10 threads. When you fire up your threads - wait for the first one to return with results > 0. This is going to be you ""master"" result. As other threads return you can compare them to your ""master"" result and add new results. There is really no way to avoid this if you want to provide unique results. You can start displaying results as soon as you have your first thread. You don't have to update your screen right away with all the new results as they come in but if takes some time user may become agitated. You can just have some sort of indicator that shows that more results are available if you have more than 10 for instance. If you only have a few sources like 10 and you limit the number of results per source you are waiting for to like 10 it really shouldn't take that much time to sort through them in any programming language. Also make sure you can recover if your remote sources are not available. If let's say you are waiting for all 10 sources to come back to display data - you may be in for a long wait if one of the sources is down. The other approach is to f00l user. Sort of like airfare search sites do - where they make you want a few seconds while they collect and sort results. I really like Kayak.com's implementation - as it make me feel like it's doing something unlike some other sites. Hope that helps.  John I believe what you want is federated search. I suggest you check out Solr as a framework for this. I agree with Nick that you will have to evaluate the relative quality of the different sources yourself and build a merge function. Solr has some infrastructure for this as this email thread shows."
1100,A,"Searching and sorting by language I am testing Lucene.NET for our searching requirements and I've got couple of questions. We have documents in XML format. Every document contains multi-language text. The number of languages and the languages itself vary from document to document. See example below: <document>This is a sample document which is describing a <word lang=""de"">tisch</word> a <word lang=""en"">table</word> and a <word lang=""en"">desk</word>.</document> The keywords of a document are tagged with a special element and language attribute. When I am creating lucene index I extract the text content from the XML and pairs of language and keyword (I am not sure if I have to) like this: This is a sample document which is describing a tisch a table and a desk. de - tisch en - table en - desk I don't know exactly how to create an index that I will be able to search for example: - all the documents that contains word tisch in German (and not the document which contains word tisch in other languages). And also I want to specifiy sorting at runtime: I want to sort by user specified language order (depending on a user interface). For example if we have two documents: <document>This is a sample document which is describing a <word lang=""de"">tisch</word>.</document> <document>This is a another sample document which is describing a <word lang=""en"">table</word>.</document> and a user on an English interface searches by ""tisch OR table"" I want to get the second result first. Any information or advice is appreciated. Many thanks! You have a design decision to make where the options are: Use a single index where each document has a field per each language it uses or Use M indexes M being the number of languages in the corpus. If you use the multi-index approach it will be easier to restrict search to a specific language or set of languages - just search the indexes for these languages not using the other languages. Also sorting by language becomes easier. Therefore if you do not have an ""AND"" search that requires keywords from different languages appear in the same document I would suggest the M-index approach. Based on your example I assume that the part of the documents not specially tagged is in English. If this is so you can add the document text to the English index as a separate field; The other indexes need only store a document id which will make them lighter."
1101,A,"Is it possible to run several map task in one JVM? I want to share large in memory static data(RAM lucene index) for my map tasks in Hadoop? Is there way for several map/reduce tasks to share same JVM? To my best knowledge there is no easy way for multiple map tasks (Hadoop) to share static data structures. This is actually a known problem for current Map Reduce model. The reason that current implementation doesn't share static datas across map tasks is because Hadoop is designed to be highly reliable. As a result if a task fails it will only crash its own JVM. It will not impact the execution of other JVMs. I am currently working on a prototype that can distribute the work of a single JVM across multiple cores (essentially you just need one JVM to utilize multi cores). This way you can reduce the duplication of in memory data structures without costing CPU utilization. The next step for me is to develop a version of Hadoop that can run multiple Map tasks within one JVM which is exactly what you are asking for. There is an interesting post here https://issues.apache.org/jira/browse/MAPREDUCE-2123  In $HADOOP_HOME/conf/mapred-site.xml add the follow property <property> <name>mapred.job.reuse.jvm.num.tasks</name> <value>#</value> </property> The # can be set to a number to specify how many times the JVM is to be reused (default is 1) or set to -1 for no limit on the reuse amount.  Shameless plug I go over using static objects with JVM reuse to accomplish what you describe here: http://chasebradford.wordpress.com/2011/02/05/distributed-cache-static-objects-and-fast-setup/ Another option although more complicated is to use distributed cache with a read-only memory mapped file. That way you can share the resource across the JVM processes as well.  Jobs can enable task JVMs to be reused by specifying the job configuration mapred.job.reuse.jvm.num.tasks. If the value is 1 (the default) then JVMs are not reused (i.e. 1 task per JVM). If it is -1 there is no limit to the number of tasks a JVM can run (of the same job). One can also specify some value greater than 1 using the api. The JVM will be cleared after a task completes. This parameter only provides better runtime for jobs that are not ""long-running"" since the jvm instantiation is very expensive. You could not share any ressources over task instances. Thanks one more question. Does those tasks also share some Class-loader so all statics resources will be loaded only once? (Or may it works like tomcat in that way there are almost no reason to share JVM...)"
1102,A,How to count term frequency for set of documents? i have a Lucene-Index with following documents: doc1 := { caldari jita shield planet } doc2 := { gallente dodixie armor planet } doc3 := { amarr laser armor planet } doc4 := { minmatar rens space } doc5 := { jove space secret planet } so these 5 documents use 14 different terms: [ caldari jita shield planet gallente dodixie armor amarr laser minmatar rens jove space secret ] the frequency of each term: [ 1 1 1 4 1 1 2 1 1 1 1 1 2 1 ] for easy reading: [ caldari:1 jita:1 shield:1 planet:4 gallente:1 dodixie:1 armor:2 amarr:1 laser:1 minmatar:1 rens:1 jove:1 space:2 secret:1 ] What i do want to know now is how to obtain the term frequency vector for a set of documents? for example: Set<Documents> docs := [ doc2 doc3 ] termFrequencies = magicFunction(docs); System.out.pring( termFrequencies ); would result in the ouput: [ caldari:0 jita:0 shield:0 planet:2 gallente:1 dodixie:1 armor:2 amarr:1 laser:1 minmatar:0 rens:0 jove:0 space:0 secret:0 ] remove all zeros: [ planet:2 gallente:1 dodixie:1 armor:2 amarr:1 laser:1 ] Notice that the result vetor contains only the term frequencies of the set of documents. NOT the overall frequencies of the whole index! The term 'planet' is present 4 times in the whole index but the source set of documents only contains it 2 times. A naive implementation would be to just iterate over all documents in the docs set create a map and count each term. But i need a solution that would also work with a document set size of 100.000 or 500.000. Is there a feature in Lucene i can use to obtain this term vector? If there is no such feature how would a data structure look like someone can create at index time to obtain such a term vector easily and fast? I'm not that Lucene expert so i'am sorry if the solution is obvious or trivial. Maybe worth to mention: the solution should work fast enough for a web application applied to client search queries. So you have 500K documents how big is your term list? I know exactly what you're trying to accomplish too bad I don't have an answer to your question :) @Justin: i have around 2.000 different terms absolute max in a few years maybe 10.000 but for sure not more. hi ManBugra I also have a similar requirement. Did you find any way to solve the problem of getting the count on a set of documents ? @ManBugra : could you plz share  how to count term frequency? Go here: http://lucene.apache.org/java/3_0_1/api/core/index.html and check this method org.apache.lucene.index.IndexReader.getTermFreqVectors(int docno); you will have to know the document id. This is an internal lucene id and it usually changes on every index update (that has deletes :-)). I believe there is a similar method for lucene 2.x.x  I don't know Lucene however; your naive implementation will scale provided you don't read the entire document into memory at one time (i.e use an on-line parser). English text is about 83% redundant so your biggest document will have a map with 85000 entries in it. Use one map per thread (and one thread per file pooled obviouly) and you will scale just fine. Update: If your term list does not change frequently; you might try building a search tree out of the characters in your term list or building a perfect hash function (http://www.gnu.org/software/gperf/) to speed up file parsing (mapping from search terms to target strings). Probably just a big HashMap would perform about as well.
1103,A,"re-indexing objects with lucene I have a question still related to my objects in memory. I initialize the model as follows: model.init("""" + ""<root>"" + "" <objA nb='5' attrA1='string(val1)' attrA2='int(0)'>"" + "" <objB nb='8' attrB1='string(val2)' attrB2='int(102)' />"" + "" <objQ nb='4' attrD3='int(250)' attrD6='string(ok)' />"" + "" </objA>"" + "" <objC nb='1' attrC1='int(1)'/>"" + "" <objD nb='1' attrD1='string(valx)'/>"" + "" <objZ nb='8' attrB1='string(val2)' attrB2='int(102)' />"" + "" <objR nb='4' attrR1='string(val3)' />"" + "" <mark nb='2' attrR9='string(valeur)'>"" + "" <ttt name='string(qsa)'>"" + "" <aaa></aaa>"" + "" </ttt>"" + "" </mark>"" + ""</root>""); rootType = model.getRootType(); Then I index all the elements with their attributes; After that I modify some of them (I change the value of their attributes I delete some objects I create some news). How to detect that I modified the object and re-index it with lucene? I'll be really grateful for help. How to detect that I modified the object This is generally done by having a ""modified"" flag that is set to true by all setters and set to false whenever is's indexed. and re-index it with lucene? IIRC Lucene cannot modify indexed documents so you have to delete and re-add them."
1104,A,"Adjust Lucene search result score by weight particular fields with same name I'm currently using Lucene as our full text search engine. But we need sorting the search result according to a particular field. For example if we have the following three documents in our index with exactly contents excepts the id field.  val document01 = new Document() val field0100 = new Field(""id"" ""1"" Field.Store.YES Field.Index.ANALYZED) val field0101 = new Field(""contents"" ""This is a test: Linux"" Field.Store.YES Field.Index.ANALYZED) val field0102 = new Field(""contents"" ""This is a test: Windows"" Field.Store.YES Field.Index.ANALYZED) document01.add(field0100) document01.add(field0101) document01.add(field0102) val document02 = new Document() val field0200 = new Field(""id"" ""2"" Field.Store.YES Field.Index.ANALYZED) val field0201 = new Field(""contents"" ""This is a test: Linux"" Field.Store.YES Field.Index.ANALYZED) val field0202 = new Field(""contents"" ""This is a test: Windows"" Field.Store.YES Field.Index.ANALYZED) document02.add(field0200) document02.add(field0201) document02.add(field0202) val document03 = new Document() val field0300 = new Field(""id"" ""3"" Field.Store.YES Field.Index.ANALYZED) val field0301 = new Field(""contents"" ""This is a test: Linux"" Field.Store.YES Field.Index.ANALYZED) val field0302 = new Field(""contents"" ""This is a test: Windows"" Field.Store.YES Field.Index.ANALYZED) document03.add(field0300) document03.add(field0301) document03.add(field0302) Now when I search Linux using IndexSearcher I got the following result: Document<storedindexedtokenized<id:1> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:2> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:3> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> When I search Windows I get same result with same ordering. Document<storedindexedtokenized<id:1> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:2> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:3> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> The question is that is it possible weight a particular fields when building index? For example I would like make field0201 has higher score if its been matched when search. In other words when I search Linux I would like get the result in the following order: Document<storedindexedtokenized<id:2> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:1> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:3> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> And when I search for Windows it still remains the original ordering like the following: Document<storedindexedtokenized<id:1> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:2> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> Document<storedindexedtokenized<id:3> storedindexedtokenized<contents:This is a test: Linux> storedindexedtokenized<contents:This is a test: Windows>> I tried using field0201.setBoost() but it will change the ordering of search result both when I search Linux or Windows. It looks like the documents all contain the same data except for the id. Why would you have an expectation that the scores are different? @huynhjil Because the contents come from different source. I would like a field come from particular source have higher score if it is matched with search terms. In other words it should sort with the using (score lucene calculated the source of field) pair. Would it be possible for you to use the Sort instance that you pass to a TopFieldCollector to do your sorting? ...or do you explicitly want to do it with the score of your fields (This would only work if the contents are not the same)? @csupning I'm think that I TopFieldCollector is not feasible solution. Since I could not find a way to store extra information to a field so I could not sort by the extra information. What I need to achieve is when the search result hit two field has same content but come from different source(ex one from source A one from source B) the field come from source B will have higher score. But if the field is not matched with search request then it should not affect the ordering of search result. I think it should be possible if you put your data for different sources in fields with different names. You can set a boost at index time but if you use the same name I think the boost would apply to all fields with the same name - based on the setBoost javadoc. So if you do this instead: val field0201 = new Field(""content-high"" ""This is a test: Linux"" ...) field0201.setBoost(1.5f) val field0202 = new Field(""content-low"" ""This is a test: Windows"" ...) And then query with content-high:Linux content-low:Linux (using a boolean query with two should clauses both set to term Linux) then the boost for content-high should increase the document score if the match is in that field. Use explain to see whether that works. Thanks this seems work well!"
1105,A,"Numeric Range Query I read that for handling date range query NumericRangeQuery is better than TermRangeQuery in ""Lucene in action"" But i couldnot find the reason. i want to know the reason behind it. I used TermRangeQuery and NumericRangequery both for handling date range query and i found that searching is fast via NumericRangeQuery. My second point is to query using NumericRangeQuery i have to create indexes using NumericField by which i can create indexes upto milisecond but what if i want to reduce my resolution upto hour or day. Explanation by @Xodarap about Numeric field is correct. Essentially the precision is dropped for the numbers to reduce the actual term space. Also I suppose TermRangeQuery uses String comparison whereas NumericRange query is working with integers. That should squeeze some more performance. You can index at any desirable resolution - millisecond to day. Date.getTime() gives you milliseconds since epoch. You can divide this number by 1000 to get time with resolution at second. Or you can divide by 60000 to get resolution at minute. And so on. Note that the precision parameter will affect this. Even if you pass in the seconds if you have precision = 4 and you're using a long it will store only about 500 different values. So if you have multiple values per day every day for two years you will be unable to store more than the day no matter how precise the values you pass in are. @Xodarap: Precision step has no bearing on how many different values you can store. Values are stored exactly as they're provided. Precision steps simply define how quickly queries can find a range of numbers. The lower the precision number the faster the query but the larger the index. The higher the precision number the slower the query but the smaller the index. Good explanation at: http://stackoverflow.com/a/19236339/1145177  Why is numeric so much faster than term? As you have noted there is a ""precision step"". This means that numbers are only stored to a certain precision which means that there is a (very) limited number of terms. According to the documentation it is rare to have more than 300 terms in an index. Check out the wikipedia article on Tries if you are interested in the theory. How can you reduce precision? The NumericField class has a ""precision"" parameter in the constructor. Note that the range query also has a precision parameter and they must be the same. That JavaDoc page has a link to a paper written about the implementation explaining more of what precision means."
1106,A,"Good tutorial for lucene 3.03? All that I am getting in google is tutorials for lucene less than 3.0. I want to use indexing and searching from ""files"". Not ""in-memory"". Something equivalent to this or this in version greater than 3.0 ? I used this : http://today.java.net/pub/a/today/2003/07/30/LuceneIntro.html This seemed to be a nice guide to work my way through..  Try Bob Carpenter's Lucene 3.0 Tutorial."
1107,A,"All of these words feature I have a ""description"" field indexed in Lucene.This field contains a book's description. How do i achieve ""All of these words"" functionality on this field using BooleanQuery class? For example if a user types in ""top selling book"" then it should return books which have all of these words in its description. Thanks! I believe if you add all query parts (one per term) via BooleanQuery.add(Query BooleanClause.Occur) and set that second parameter to the constant BooleanClause.Occur.MUST then you should get what you want. The equivalent query syntax would be ""+term1+term2 +term3 ..."".  There are two pieces to get this to work: You need the incoming documents to be analysed properly so that individual words are tokenised and indexed separately The user query needs to be tokenised and the tokens combined with the AND operator. For #1 there are a number of Analyzers and Tokenizers that come with Lucene - have a look in the org.apache.lucene.analysis package. There are options for many different languages stemming stopwords and so on. For #2 there are again a lot of query parsers that come with Lucene mainly in the org.apache.lucene.queryParser packagage. MultiFieldQueryParser might be good for you: to require every term to be present just call QueryParser.setDefaultOperator(QueryParser.AND_OPERATOR) Lucene in Action although a few versions old is still accurate and extremely useful for more information on analysis and query parsing."
1108,A,Searching secured/gated content with Solr Does anything exist within Solr/Lucene for securing/gating content based on some kind of security mechanism? I'm considering Solr as an enterprise search alternative to the Google Mini/Google Search Appliance but I will need support for secured content which the Google options do provide. Does anyone have any experience/suggestions on how to do this with Solr? The usual way to solve this in my experience is to index security information (ACL:s and deny-ACL:s or similar) and then do filter queries on it. In short this requires: Your connector must be able to extract security information Your client/search GUI must be able to look up the current users authorities This solution assumes that you have some sort of middleware that handles user request and builds query parameters from them. If you need to let your users interact with Solr directly look here. For a good discussion on this topic - see http://www.searchtechnologies.com/search-engine-security.html
1109,A,"Lucene IndexWriter thread safety Lucene encourages the reuse of an IndexWriter from multiple threads. Given that two threads might have a reference to the IndexWriter if thread A calls close on the writer thread B would be left with a useless writer. But to my understanding lucene somehow knows that another thread uses the same writer and defers its closure. Is this indeed the case? How does lucene track that another thread uses the writer? EDIT Judging from the answers it is not correct to close the IndexWriter. But this poses a new issue: If one keeps an IndexWriter open essentially blocks access to this index from another JVM (eg in case of a cluster or a shared index between many applications). If one thread closes IndexWriter while other threads are still using it you'll get unpredictable results. We try to have the other threads hit AlreadyClosedException but this is just best effort (not guaranteed). EG you can easily hit NullPointerException too. So you must synchronize externally to make sure you don't do this. Recently (only in Lucene's trunk right now to be 4.0 eventually) a big thread bottleneck inside IndexWriter was fixed allowing segment flushes to run concurrently (previously they were single threaded). On apps running with many indexing threads on concurrent hardware this can give a big boost in indexing throughput. See http://blog.mikemccandless.com/2011/05/265-indexing-speedup-with-lucenes.html for details.  Are you referring to the waitForMerges flag on the IndexWriter.close() method? Closes the index with or without waiting for currently running merges to finish. This is only meaningful when using a MergeScheduler that runs merges in background threads. Lucene generally uses background threads to consolidate fragmented writes that have occurred across multiple threads - the writes themselves happen immediately but the consolidation happens asynchronously. When closing the writer you should allow it to finish the consolidation process otherwise: it is dangerous to always call close(false) especially when IndexWriter is not open for very long because this can result in ""merge starvation"" whereby long merges will never have a chance to finish. This will cause too many segments in your index over time. So the writer doesn't ""know"" about your threads in the sense that you meant. So if two threads are using the same writer and one thread closes it then the other thread is indeed left with a useless writer? @yannisf: Well yes I suppose so but that's the same for any mutable shared object - one thread can render the shared object useless. This doesn't seem like a special case.  The threadafety and reuse of IndexWriter means you can have multiple threads all using that instance to create/update/delete documents. If you close indexwriter in one thread though it will indeed muck everyone else up."
1110,A,"Sitecore 6.4 + Lucene - Template inheritance After a few hours of figuring out Lucene I gave up. Hope you can help. Setting In our current Sitecore tree we have template 2 template inheritance levels. (Let's say Vehicle -> Car -> Formula One / Vehicle -> Boat). What I would need to fetch through Lucene now is every ContentItem inheritting either directly or indirectly from a certain Template. For instance: Get vehicles -> returns: ""Generic"" Vehicles Cars F1 Cars and boats Get cars -> returns: Cars F1 Cars What I thought was using the _templates-field in Lucene ie: Get Vehicles -> _templates contains Vehicle-template-guid. Get Cars -> _templates contains Car-template-guid. The problem The main problem I experience is that Content Items of the F1 Car-template contain only the Cars and F1 Car in the _templates-field and not the generic ""Vehicle"". I probably misinterpret the _templates-field but am wondering if there's any other options. Sitecore configuration: Content items: sitecore content Home ACategory A Child 1 A Child 1.1 A Child 2 Templates (levels = inheritance): Main entity Child entity 1 Child entity 1.1 Child entity 2 Category What Lucene approach are you taking? The ""old"" Lucene approach or the ""new"" one? Are you using the [Advanced Database Crawler](http://sitecoreblog.alexshyba.com/2010/11/sitecore-searcher-and-advanced-database.html)? We're working on a custom index maintained by a custom crawler inheriting Sitecore.Search.Crawlers.DatabaseCrawler. This crawler adds a few of our custom fields to the item as well. - Currently the solution I'm trying is writing item.Template.ID.Guid and (recursively) each baseTemplate.Guid to a new field. I think this would work (it's pretty similar as the _path-field only for template inheritance instead) I'll let you know. Create an new indexer which inherits Sitecore.Data.Indexing.Index and override the AddFields method as below: protected override void AddFields(Item item Document document) { // Add base fields base.AddFields(item document); // Add all inherited templates id to a field string TEMPLATE-PATH=""get template path for this item here""; document.Add(new Field(""template-path"" TEMPLATE-PATH Field.Store.NO Field.Index.TOKENIZED)); } Then you can search it by ""template-path"" field. As mentioned @ original post's comments I'm using the new Sitecore Lucene approach. What you mention is however similar to how I solved the thing: added a new field including the entire template hierarchy. Thanks Mark and chiesa going to mark this one as answer. What I mean is  not new crawler but a new indexer. In the config you can edit the index as below"
1111,A,"Implementing a custom IReadOnlyMappingManager in solrnet I am trying to implement a custom IReadOnlyMappingManager in solrnet to allow us to use our own Attribute types for decorating the properties of the documents that represent our solr index records. As I only need to replace the implementation of the GetFields and GetUniqueKey methods the current implementation is as follows: public class CustomMappingManager : AttributesMappingManager { public new ICollection<KeyValuePair<PropertyInfo string>> GetFields(Type type) { IEnumerable<KeyValuePair<PropertyInfo IndexFieldAttribute[]>> mappedProperties = this.GetPropertiesWithAttribute<IndexFieldAttribute>(type); IEnumerable<KeyValuePair<PropertyInfo string>> fields = from mapping in mappedProperties select new KeyValuePair<PropertyInfo string>(mapping.Key mapping.Value[0].FieldName ?? mapping.Key.Name); return new List<KeyValuePair<PropertyInfo string>>(fields); } public new KeyValuePair<PropertyInfo string> GetUniqueKey(Type type) { KeyValuePair<PropertyInfo string> uniqueKey; IEnumerable<KeyValuePair<PropertyInfo IndexUniqueKeyAttribute[]>> mappedProperties = this.GetPropertiesWithAttribute<IndexUniqueKeyAttribute>(type); IEnumerable<KeyValuePair<PropertyInfo string>> fields = from mapping in mappedProperties select new KeyValuePair<PropertyInfo string>(mapping.Key mapping.Value[0].FieldName ?? mapping.Key.Name); uniqueKey = fields.FirstOrDefault(); return uniqueKey; } } This type has been successfully wired up using structuremap and the mappingManager in my concrete instance of ISolrOperations is an instance of this CustomMappingManager type. I have followed the stack trace right down to the Viistors in the solrnet implementation that do the real work; these have the CustomMappingManager instance as intended. Unfortunately the GetFields and GetUniqueKey methods on this type never get called and my documents are always empty. Any ideas very welcome. I think the intention was to remove the dependency on SolrNet types; it does not confer any extra functionality. I picked this up from some research code a colleague did several months ago and having spent a couple of days with it now I'm not convinced that there's any need for the isolation. Just out of curiosity why do you want to use your own attributes? Do they have any extra information? I have resolved this. The approach in the question was wrong way to go about it. Here is the equivalent portion of the working code for the CustomMappingManager implementation: public class CustomMappingManager : IReadOnlyMappingManager { public ICollection<SolrFieldModel> GetFields(Type type) { IEnumerable<KeyValuePair<PropertyInfo IndexFieldAttribute[]>> mappedProperties = this.GetPropertiesWithAttribute<IndexFieldAttribute>(type); IEnumerable<SolrFieldModel> fields = from mapping in mappedProperties select new SolrFieldModel() { Property = mapping.Key FieldName = mapping.Value[0].FieldName ?? mapping.Key.Name }; return new List<SolrFieldModel>(fields); } public SolrFieldModel GetUniqueKey(Type type) { SolrFieldModel uniqueKey; IEnumerable<KeyValuePair<PropertyInfo IndexUniqueKeyAttribute[]>> mappedProperties = this.GetPropertiesWithAttribute<IndexUniqueKeyAttribute>(type); IEnumerable<SolrFieldModel> fields = from mapping in mappedProperties select new SolrFieldModel() { Property = mapping.Key FieldName = mapping.Value[0].FieldName ?? mapping.Key.Name }; uniqueKey = fields.FirstOrDefault(); if (uniqueKey == null) { throw new Exception(""Index document has no unique key attribute""); } return uniqueKey; } }"
1112,A,"Lucene: output elaborated data by adding IR information to it I need to process a database in order to add meta-information such as td-idf weights to the documents terms. Successively I need to create document pairs with similarity measures such as td-idf cosine similarity etc... I'm planning to use Apache Lucene for this task. I'm actually not interested in the retrieval or running a query but in indexing the data and elaborate them in order to generate an output file with the above mentioned document pairs and similarity scores. The next step would be to pass these results to a Weka classifier. Can I easily do it with Lucene ? thanks Try Integrating Apache Mahout with Apache Lucene and Solr. Replace the places that say ""Mahout"" with ""Weka"". Good Luck."
1113,A,Lucene(.Net) geometric (lat/long) queries? I'm writing an ASP.NET MVC site where I plan to use Lucene.Net for searching. Some of my site content has a location attribute and I'm thinking of incorporating location into the searching functionality. Does functionality exist in Lucene where if I were to define latitude and longitude fields for my documents the user inputs some coordinates and the system returns the closest hits to those coordinates? If so could you point me to some samples on how to use those features? There is a project in contrib called Spatial.Net. Lucene in Action has a sample implementation but looking at the test file (TestCartesian.cs) is probably just as good. Technical documentation can be found here.
1114,A,CompassQuery - leave only keywords no aliases or operators I have following piece of code: CompassQuery suggested = hits.getSuggestedQuery() String displayedSuggestion = suggested.toString(); If just do that I get nasty string with aliases and query operators: +(alias:item) field:keyword1 OR field:keyword2 How do I get simple sitring like this?: keyword1 keyword2 I had this same problem myself recently. The only way I could make it work reliably was code like this: import static org.apache.commons.lang.StringUtils.*; import java.util.ArrayList; import java.util.List; import org.apache.lucene.index.Term; import org.apache.lucene.search.Query; import org.compass.core.CompassQuery; import org.compass.core.engine.SearchEngineException; import org.compass.core.engine.SearchEngineQuery; import org.compass.core.impl.DefaultCompassQuery; import org.compass.core.lucene.engine.LuceneSearchEngineQuery; import org.compass.core.lucene.engine.queryparser.QueryParserUtils; public class SearchTermExtractor { public String extract(CompassQuery compassQuery final String fieldName) { SearchEngineQuery searchEngineQuery = ((DefaultCompassQuery)compassQuery).getSearchEngineQuery(); Query luceneQuery = ((LuceneSearchEngineQuery)searchEngineQuery).getQuery(); final List<String> suggestedTerms = new ArrayList<String>(); // Use a visitor to extract the terms QueryParserUtils.visit(luceneQuery new QueryParserUtils.QueryTermVisitor() { @Override public Term replaceTerm(Term term) throws SearchEngineException { if (fieldName.equals(term.field())) { suggestedTerms.add(term.text()); } // Just return the original term we're not trying to modify the query itself return term; } }); return join(suggestedTerms ' '); // join is from Commons Lang StringUtils } } The fieldName parameter is the field used for the terms you want to extract (field in your example). @Mat: Yes. Every term in Compass/Lucene has a field name and that's the name you need to specify. Your keywords will all use a specific field name.
1115,A,Getting the Vector Space Model (tf-idf) from a query on a lucene index I need to get the Vector Space Model(with tf-idf weighting) from the results of a lucene query and cant figure out how to do it. It seems like it should be simple and at this stage maybe one of you guys can point me in the right direction. I have been trying to figure out how to do this for a good while and either I haven't copped how the stuff i have read is what i need yet (more than likely) or a solution hasn't been posted to my particular problem. I even tried computing the VSM myself direct from the query results but my solution has hideous complexity. Edit: For anyone else who stumbles upon this there is a solution @ the much clearer question here What i need can be gotten by the IndexReader.getTermFreqVector(String field int docid) method. Unfortunately this doesn't work for me as the index I am working off hasn't stored the term frequency vectors so I guess I'm still looking for more help on this! To answer this question you can compute a TF-IDF weighted vector space model for a set of lucene results using the IndexReader.getTermFreqVector() and Searcher.docFreq() classes. There is no way of directly getting the VSM for a set of results in Lucene. (this is all as far as I can tell)  If I understand correctly from your comment you want the compute VSM cosine similarity between documents rather than between a query and a document. I don't know exactly how to do this but I'd point you to the Lucene API page for the Similarity class. You'd probably have to derive and use a custom subclass of Similarity that changes the coord and queryNorm members and find a way to turn documents into query objects. (No guarantees; I'm just trying to figure out this scoring myself.) Yep thats what I'm looking for I'll have a fresh look at the similarity class. Thanks for your help.  Maybe I'm misunderstanding what you're trying to do but Lucene's scoring uses the vector space model. If you want more details for how the scores are calculated given a document and a query use Searcher.explain(Query query int doc) . Submit the text of each document as the query and you'll get the cosine similarity for that document with every other document in your index. When you transform the text of the document into a query make sure each term is an OR term.
1116,A,"Fastest way to count all results in Lucene (java) What is the fastest way to count all results for a given Query in Lucene? TopDocs.totalHits implement and manage a Filter using QueryFilter implement a custom 'counting' Collector. This simply increments a count in the collect(int doc) method and returns true for the acceptsDocOutOfOrder() method. All other methods are NOOPS. Since 1. will do scoring on all docs and 2. could have an upfront hit due to loading of the FieldCache I assume the answer is 3. It just seems odd that Lucene doesn't provide such a collector out of the box? You are right that #3 will be quicker but I don't think it's because of scoring. There is a much faster way skip to the bottom if you don't care about the reasoning behind this. The performance loss of #1 comes from the fact that the TopDocs collector will keep the docs in a priority queue which means that you will lose some time sorting them by score. (You will also eat up some memory but since you're storing just a heap of int+float pairs it's probably pretty minimal.) As to why Lucene doesn't provide this out of the box: you generally don't want to find all results. That's why when you search you say to only find the top n results. There are strong theoretical reasons for this. Even Google says ""Showing 25 of about n results."" So my advice to you is the following: if you have a reasonable number of results then using TopDocs.totalHits won't be too bad performance-wise. If the totalHits method gives you problems I don't think that a custom collector will be much better. (TopDocs.totalHits will run in n log n time and the custom collector will be linear. Depending on your set up the log n coefficient may be relevant or it may not.) So if you absolutely need this functionality and TopDocs.totalHits is too slow I would recommend looking at the document frequency of the search terms. You could assume that frequency is independent (so p(A and B)=p(A)*p(B)) and make a pretty good guess from there. It will be very fast because it's just a constant-time lookup for each term. Thanks for the answer. We will go with a TotalHitCountCollector at this stage. Our data set is still small enough to accurately count. I will keep the term frequency approach you describe in mind though - that does indeed sound the fastest approach. I wonder how Google is doing this. Clearly it isn't really returning the ""top 25"" results. If it were then it should know the total number of results as a side-effect of checking all the other results to discover that they were not in the top 25. My theory would be that it is returning 25 essentially arbitrary ""worthy of being up the top"" results.  The codes should be here now: http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/search/TotalHitCountCollector.java Thanks for that. Looks like it will be in an upcoming Lucene release? yes this is the branch for upcoming 3.x releases (next target is 3.1) That link returns a 404"
1117,A,boosting a term in lucene's query I want to know when a term is boosted in lucene query how does scores change? I mean what is the scoring algorithm of lucene for scoring documents when query has one or more term that boosted? You find the complete answer here: http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/Similarity.html ∑ ( tf(t in d) · idf(t)^2 · t.getBoost() · norm(td) ) Each term in the query is summed up with The term frequency of one query term in a document The inverse document frequency to the power of 2 (if appliable the fieldtype and its attributes are here important) The boost thats what you ask And the norm for the term t in d (if appliable can be disabled in the schema) The easy answer is its multiplicated to the term weight.
1118,A,"Lucene.NET: Best way to process keyword snippets from document text I'm using Lucene.Net to implement a search website (to search PDFs). Once the keyword is entered I display the results and when one of the result items is clicked I want to take the user to a ""details"" page where I want to display snippets from that PDF document everywhere the keyword is found. So my question is what's the best way to gather these snippets from that document? Do I just take the selected item id re-query on just that document and let Lucene's highlighter give me the collection of snippets? Or since I already have the text content for each result record would it be better to manually process the snippets using C# string manipulation? If it is 1. could you please point me to an example of how to write a query to search a single document in Lucene? Thanks. You should probably use Lucene Highlight package because your query and document will need tokenized using the same analyzer which was used to index the document. Using C# directly via string methods can work but you'd have to use the same tokenizing logic to match the query terms the document text (such as stemming stop words etc.). If you are storing the full text of the document in the index then using the highlighter is simple. You could also fetch the document text from someplace else if you dont store the text in the index. You will need to pass the same query used in the initial search and include an exact match for the document you want to highlight for example by appending a required clause to the query for that documents unique ID. The query used for the single document should have 2 required clauses the first clause is the original query used to find the document initially and the other clause is some unique identifier for that single document. That way the highlighter can use the same query to generate highlighted snippets. Perfect. Thanks for your answer."
1119,A,"Lucene.net with IndexSearcher/IndexWriter in a Web Application I'm currently running Lucene.net in a web application and am wondering about the best method performance-wise. I currently have it set up so that all index writes get processed together in a scheduled process along with optimizing the index. However for searching - I'm currently opening and closing the searcher per search which I know isn't ideal. What do you think would be the best approach in this situation? I'll need to close and reopen the index searcher once the updates/optimization is processed so the scheduled process (which is a windows console app) needs to communicate it's finished to the web application. Just out of curiosity who says that it is not ideal to open/close the searcher per search? Is that really a bad thing? Just noticed this now - see: http://wiki.apache.org/lucene-java/ImproveSearchingSpeed ""Use one instance of IndexSearcher. Share a single IndexSearcher across queries and across threads in your application. "" I just integrated Lucene.NET into BugTracker.NET. I'm not sure that what I did is the best but it seems to be working well. I create the index at app startup. I create a searcher and keep it around so that the index isn't reloaded with each search. All threads share the same searcher. When the searcher searches it grabs a lock. Meanwhile I have an IndexWriter that updates the index when there is a data change. It is just changing a little bit so it does its task quick. When it runs it grabs the same lock destroys the searcher updates the index and the re-recreates the searcher. The new searcher stays around until the next update of the index. The searcher always is working with an up-to-date index. You can get the BugTracker.NET source and look at the files my_lucene.cs and search_text.aspx. It's all in those two files and there isn't that much code. I took a quick look at your lucene code - I see: protected static Lucene.Net.Search.Searcher searcher = null; Which just sits in your app_code folder - won't there be a new instance of searcher per user logging on to your system? Ah nevermind static variables are shared across the application context Isn't that a very bad idea to lock the searcher each time a search is made? If someone executes a long-running search wont everyone else be delayed until that first search is done? What exactly is the purpose of this?  You could call to the readers IsCurrent() method to check if there is a new version of the index available and if it's then reopen it. Couldn't be the best way but is easy enough and if your requirements are not very big it will be sufficient."
1120,A,"How to handle search term concatenations in SOLR We are currently replacing our product search from mysql to a SOLR backend. Our customer often search for terms like 'startrek online' 'starwars' 'redsteel' or even 'grandtheftauto'. Is there a method in SOLR to either expand or spellcheck these searches into syllables eg.'star trek online' 'star wars' 'red steel' 'grand theft auto'? you can spellcheck via spellcheck.txt file in the conf dir You can try modifying your search terms using Levenshtein but also making use of SoundEx / Metaphone to improve matches. http://web.elctech.com/2008/04/13/advanced-solr-filters-with-phonetics/ http://web.elctech.com/2009/07/06/solr-vs-sphinx-fuzzy-search/  You can use a synonym file. Take a look into this documenation site (solr.SynonymFilterFactory): <fieldtype name=""syn"" class=""solr.TextField""> <analyzer> <tokenizer class=""solr.WhitespaceTokenizerFactory""/> <filter class=""solr.SynonymFilterFactory"" synonyms=""syn.txt"" ignoreCase=""true"" expand=""false""/> </analyzer> </fieldtype> For the searchquery splitting the WordDelimiterFilterFactory could match partially your needs but maybe the synomymfilter is easier and better (+ probably faster). you could try to ask this on the mailing list. Maybe there are hacks or patches for this scenario. I was hoping for a more genric solution than the synonym file. But I will give the LetterDelimitedFilterFactory a try."
1121,A,"Lucene Indexing stuck after ~1million files (nrm file getting larger and larger...) Does anyone know why this is happening? I'm doing basic indexing + SAX parsing of XML files and adding each path as a new field in the document. I got to like 1.5million files and its stuck on this one file for 30 minutes and the .nrm (normalizing file??) gets larger and larger. I don't know why this is happening my IndexWriter is of the form: writer = new IndexWriter(dir new StandardAnalyzer(Version.LUCENE_30) IndexWriter.MaxFieldLength.UNLIMITED) Is this not optimal to use for large indices? Why is it frozen on this one file? I've ran it multiple times with over 1 million XML files and its persistently getting stuck on different XML files (not just this one in particular--whose structure is fine). Edit: So let's say I'm indexing files 2000 at a time with separate java commands. After the indexing is complete and I call the indexwriter close method am I missing anything if I want to rewrite to this index ? Should I optimize the index? I think I recall Lucene in Action saying to optimize if you won't be writing to it for a while. Actually this method worked for 1.8 million files but after I tried to add more in batches of 2000 this NRM file and another one wrote to around 70GB! Why would memory run out from the JVM if the java Lucene indexing function is called only in batches of 2000? It doesn't seem like a garbage colelcting problem unless I'm needing to add something to the Lucene code before I close the index writer. Edit 2: I have about 4 million XML files that look like : <?xml version=""1.0"" encoding=""UTF-8""?> <person> <name>Candice Archie </name> <filmography> <direct> <movie> <title>Lucid (2006) (V) </title> <year>2006 </year> </movie> </direct> <write> <movie> <title>Lucid (2006) (V) </title> <year>2006 </year> </movie> </write> <edit> <movie> <title>Lucid (2006) (V) </title> <year>2006 </year> </movie> </edit> <produce> <movie> <title>Lucid (2006) (V) </title> <year>2006 </year> </movie> </produce> </filmography> </person> I parse these XML files and add the contents into a field of the path for example /person/produce/filmography/movie/title  Lucid (2006) (V) The thing is I'm looking to compute statistics of frequency of a given term within field instances of a document for each document in the index (then sum over this value over all documents)... so if there were two instances of /person/produce/filmography/movie/title and they both contained ""Lucid"" I'd want two. the tf(t in d) Lucene gives would give 3 if there was another path (ex: /person/name: Lucid) but does not do this for terms within similar fields within a document. The heart of the Lucene Indexing is doing this: public void endElement( String namespaceURIString localNameString qName ) throws SAXException { if(this.ignoreTags.contains(localName)){ ignoredArea = false; return; } String newContent = content.toString().trim(); if(!empty.equals(newContent) && newContent.length()>1) { StringBuffer stb = new StringBuffer(); for(int i=0; i<currpathname.size();i++){ //System.out.println(i + ""th iteration of loop. value:"" + currpathname.get(i).toString() + k + ""th call.""); stb.append(""/""); stb.append(currpathname.get(i)); } //stb.append(""0""); if(big.get(stb.toString())==null){ big.put(stb.toString() 1); } else{ big.put(stb.toString()big.get(stb.toString())+1); } if(map.get(stb.toString())==null){ map.put(stb.toString()0); stb.append(map.get(stb.toString())); //ADDED THIS FOR ZERO } else { map.put(stb.toString()map.get(stb.toString())+1); stb.append(map.get(stb.toString())); } doc.add(new Field(stb.toString()newContentField.Store.YES Field.Index.ANALYZED)); seenPaths.add(stb); //System.out.println(stb.toString());// This will print all fields indexed for each document (separating nonunique [e.x.: /person/name0 /person/name1] //System.out.println(newContent); } currpathname.pop(); content.delete(0content.length()); //clear content //This method adds to the Lucene index the field of the unfolded Stack variable currpathname and the value in content (whitespace trimmed). } Map and BigMap are hashmaps (don't worry about bigmap it's used for something else. map is instantiated whenever a new XML file (Document object) is instantiated. There is a method endDocument() that adds the document after all the startElement endElement and character methods are called (These are Xerces Parser methods)  public void endDocument( ) throws SAXException { try { numIndexed++; writer.addDocument(doc); } catch (CorruptIndexException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } Sorry for the long post--thanks for your help! Also I don't think the server is the problem. I ran the code on 4 million files at once and it ran out of heap memory even when i used Xmx12000M Xms12000M it's a powerful server so it can definitely handle this... Edit 3: Hello again! Thanks and you're right. Lucene probably wasn't made to do this. We are actually going to do other experiments but I think I solved the problem with the help of your ideas and some others. First I stopped normalizing the fields and that shrunk the index's size many times. Also I played with the mergedocs and rambuffer methods and upped them. The indexing greatly improved. I'm gonna mark the question answered with your help:) Thanks. Try indexing in batches. The bellow code should give you an idea how to do it. Also I would recommend to check out the latest edition of Lucene in Action. Most likely you are overloading the garbage collector (assuming there is no memory leaks which are hard to find) which should eventually give your out of memory error.  private static final int FETCH_SIZE = 100; private static final int BATCH_SIZE = 1000; //Scrollable results will avoid loading too many objects in memory ScrollableResults scroll = query.scroll(ScrollMode.FORWARD_ONLY); int batch = 0; scroll.beforeFirst(); while (scroll.next()) { batch++; index(scroll.get(0)); //index each element if (batch % BATCH_SIZE == 0) { //flushToIndexes(); //apply changes to indexes //optimize(); //clear(); //free memory since the queue is processed } } Added more once again! Thank you :) Thank you for your reply! please look at my edit. I was hoping you could give me some more help..."
1122,A,"Lucene query parsing behaviour - joining query parts with AND Let's say we have a Lucene index having few documents indexed using StopAnalyzer.ENGLISH_STOP_WORDS_SET. A user is issuing two queries: foo:bar baz:""there is"" Let's assume that the first query yields some results because there are documents matching that query. The second query yields 0 results. The reason for this is because when baz:""there is"" is parsed it ends up as a void query as both there and is are stopwords (technically speaking this is converted to an empty BooleanQuery having no clauses). So far so good. However any of the following combined queries +foo:bar +baz:""there is"" foo:bar AND baz:""there is"" behave exactly the same way as query +foo:bar that is brings back some results - all despite the second AND part which yields no results. One might argue that when ANDing both conditions have to be met but they aren't. It seems contradictory as an atomic query component has different impact on the overall query depending on the context. Is there any logical explanation for this? Can this be addressed in any way preferably without writing own QueryAnalyzer? Can this be classified as a Lucene bug? If this makes any difference observed behaviour happens under Lucene v3.0.2. This question was also posted on Lucene Java users mailing list no answers came so far. I think it is perfectly fine. You can imagine the result for an empty query being the whole document collection. However this result is omitted for practical reasons. Sone basically you're ANDing with superset not an empty set. E: You can think of it in a way that additional keywords refine the result set. This makes most sense when you take prefix search into account. The shorter your prefix is the more matches there are. The most extreme case would be the empty query matching the whole document collection It should but it can't. ""Bring back the whole collection"" will kill performance for any decent collection. For the same reason many search applications enforce a minimum size of prefixes. Bringing back the whole collection means to take the whole index and to merge all doclists (and aggregate them and so on). So while baz:""there is"" shoould return the whole collection just like the query """" it doesn't because a user never requires this result and its computation is craziely expensive I'm afraid I have to disagree again. QueryParser can play smart and convert such a query into MatchAllDocsQuery (which is available by default in Lucene) also it's not the querying but data retrieving that is expensive. What seems most logical to me is not to return any records at all (your answer was suggesting to bring back everything that's why I asked). But in this case `baz:""there is""` should then also bring back the whole collection right? - but it isn't and that's the point. I disagree with refinement argument because refinement is something totally different from AND.  I would suggest not using the StopAnalyzer if you want to be able to search for phrases like ""there is"". StopAnalyzer is essentially a lossy optimization method and unless you are indexing huge text documents it's probably not worth it.  Erick Ericksson from Lucene mailing list answered one part of this question: But imagine the impact of what you're requesting. If all stop words get removed then no query would ever match yours. Which would be very counter-intuitive IMO. Your users have no clue that you've removed stopwords so they'll sit there saying ""Look I KNOW that ""bar"" was in foo and I KNOW that ""there is"" was in baz why the heck didn't this cursed system find my doc? So it looks like the only sensible way is to cease using stopwords or reduce the stopword set."
1123,A,"How can I use Lucene to search for Xml documents? I'm using Lucene to search through an index of XML documents. I'm supposed to look for documents that have certain words inside certain tags. What would be the best way to go about this? I tried to use RegexQuery with something like ""tag.*?word.*?tag"" but that returned no results. To clarify and example of an XML: <?xml version=""1.0"" encoding=""utf-8""?> <Legislation> <ENTRY COLNAME=""COL1""> <LegBody_1_1 ID=""KEY_3""> <ParagraphNum REFID=""284:1"" JUMP_LINK_KEY=""0"">1. </ParagraphNum>In the following pragraphs - </LegBody_1_1> <LegBody_1_2 ID=""KEY_4""> <Term>""Legal Guardian"" </Term> <Definition> - a person to whom legal title to property is entrusted to use for another's benefit; </Definition> </LegBody_1_2> <LegBody_1_2 ID=""KEY_5""> <Term>""Authority"" </Term> <Definition> - Any civil servant appointed by the department head or minister; </Definition> </LegBody_1_2> .... more tags.. </Legislation> A search looking for the word ""legal"" in the tag ""definition"" (""definition.*?legal.*?definition"") should return this document. Any ideas? I'd also explore native XML databases. eXist-db (http://exist-db.org) has Lucene built in so you can keep your XML intact and query the structure with XQuery while applying Lucene indexes.  I'd have a look at Parsing indexing and searching XML with Digester and Lucene."
1124,A,Command line lucene app Luke is a great app for working with lucene indexes. Does it have a command line interface? Are there alternatives? Do the alternatives offer a command line interface? What kind of actions do you want to execute against your index? Mostly just test queries. Sure. I'd love to have an answer. Maybe the Solr Luke Handler is something for you. http://wiki.apache.org/solr/LukeRequestHandler lucli looks like a command-line alternative. It comes with the Lucene package. Look under the contrib folder.
1125,A,"Storing users' data in lucene or querying rdbms? all. I'm struggling with lucene and not sure how it's better to do: i've got users' data for their profiles - some of them(3-4 fields) are storing in lucene.But on query results i need also to show user's age/name/etc. I don't think it's reasonable to save all of these fields(additional which are not participate in the search process) in lucene but querying rdmbs will either take some time so my question is how it's better to do? Thanks. This blog post tries to give you tools to choose between a full text search engine and a database. A compromise is to index all searchable fields and store an id you can use to retrieve a record from the database using a database key. Thanks i've chosen the same schema.  Indexing all profile fields with lucene gives better search experience to end users as it will search over all fields and do appropriate ranking. In RDBMS i dont know abt full text search over multiple columns and ranking. In such case i have always preferred Lucene. you are also required to sync index with rdms. it's true but if a client want to search only on some of the profile fields it's imho not reasonable to index _all of them. And it's not a problem to sync index with rdbms.  Apart from taking more disk space using ""stored"" field in the index does not impact performance of queries. I would go with that."
1126,A,"Using Scriptransformer to send a HTTP Request in Solr I am using solr to index RSS feeds and I am using DataImportHandler to parse the urls and then index them. Now I have implemented a web service that takes a url and creates an thumbnail image and stores it in a local directory. So here is what I want to do: After the url is parsed I want to send a Http request to the web service with the URL. ScriptTransformer seemed the way to go and here is how my data-config.xml file looks.  <dataConfig> <script> <![CDATA[ function sendURLRequest(row){ var url = new java.net.URL(""http://***********/GenerateThumbnail?url=http://money.cnn.com/2011/07/20/news/economy/debt_ceiling_deal/index.htm?cnn=yes""); url.openConnection().connect(); return row; } ]]> </script> <dataSource type=""JdbcDataSource"" name=""dbSource"" driver=""com.mysql.jdbc.Driver"" url=""jdbc:mysql://localhost/solr_sources"" user=""root"" password=""******""/> <document> <entity name=""rssFeedItems"" rootEntity=""false"" dataSource=""dbSource"" query=""select url from rss_feeds""> <entity name=""rssFeeds"" dataSource=""urlSource"" url=""${rssFeedItems.url}"" transformer=""script:sendURLRequest"" processor=""XPathEntityProcessor"" forEach=""/rss/channel/item""> <field column=""title"" xpath=""/rss/channel/item/title""/> <field column=""link"" xpath=""/rss/channel/item/link"" /> <field column=""description"" xpath=""/rss/channel/item/description"" /> <field column=""date_published"" xpath=""/rss/channel/item/pubDate""/> </entity> </entity> ................. ................ As you can see from the data-config file I am currently testing to see if this would work by hard coding a dummy URL. url.openConnection().connect(); Should make the HTTP Request. But the image is not generated. I see no compile errors. I tried the example script of printing out a message var v = new java.lang.Runnable() { run: function() { print('********************PRINTING************************'); } } v.run(); And it worked. I even played around with the function names to force it throw some compile errors and it did throw errors which shows that it is able to create the objects of class type URL and URL Connection. Any suggestions? I think you need to do more than just connect() to the URL to issue an HTTP GET. Maybe try: var url = new java.net.URL(""http://***********/GenerateThumbnail?url=http://money.cnn.com/2011/07/20/news/economy/debt_ceiling_deal/index.htm?cnn=yes""); var connection = url.openConnection(); connection.connect(); connection.getContent(); return row; I just did a little experiment because I was curious and found that url.openConnection().connect() didn't even actually open a connection to my test server. It wasn't until I called getContent() that the client connected and issued an HTTP request. Perhaps for the HTTP protocol the java URL library doesn't see a need to open a stateful connection and therefore doesn't connect until the data is requested (as opposed to if URL was used to access something like an FTP address). Thanks for your answer."
1127,A,"Sitecore Lucene index directory exists but no segments files Following the instructions in this SDN document (PDF) I added a custom Lucene index to a Sitecore 6.3 website (or so I thought): In the <indexes> section in web.config I added my index definition:  <indexes> ... <index id=""website"" singleInstance=""true"" type=""Sitecore.Data.Indexing.Index Sitecore.Kernel""> <param desc=""name"">$(id)</param> <fields hint=""raw:AddField""> <field target=""created"">__created</field> <field target=""name"">@name</field> <field target=""body"">body</field> <field target=""template"" storage=""keyword"">@tid</field> <field target=""id"" storage=""unindexed"">@id</field> </fields> </index> </indexes> I added the index to the master database:  <database id=""master"" ...> ... <indexes hint=""list:AddIndex""> ... <index path=""indexes/index[@id='website']"" /> </indexes> </database> Since the master database already has HistoryEngine set up (by default) I did not make any additional configuration changes. I can verify that the system index is getting populated correctly as the search application in the Sitecore Desktop is able to find items and the ~/Data/indexes/master/system directory is chock full of Lucene index files. However although the ~/Data/indexes/master/website directory was created automatically there are no index files in it and attempting to perform a search in my sublayout results in the following System.IO.FileNotFoundException (path abbreviated for simplicity): no segments* file found in Sitecore.Data.Indexing.FSDirectory@~/Data/indexes/master/website: files: What additional changes do I need to make to get Sitecore to recognize the new index? After doing more some experimentation/research I discovered that there was actually no additional configuration necessary. Following the instructions in this blog post I simply rebuilt the search index for the master database and everything started working! I will have to keep an eye on it though; I thought the search index would be updated automatically every 5 minutes (based on the value of the Indexing.UpdateInterval setting)."
1128,A,"Why are Solr Filters disable bypassed when doing a fuzzy searches? My Solr setup contains the Filter LowerCaseFilterFactory for indexing and querying so - the index only contains lowercase words and - every query is supposed to be turned lowercase before used for searching. When doing a non-fuzzy search like ""mix:barba"" they are working fine: - barba finds Barba and BARBA - Barba finds Barba and BARBA Problem is as soon as I do fuzzy searches those Filters seem bypassed: - barba~0.8 finds Barba and BARBA - Barba~0.8 finds nothing. My interpretion of those results: when doing fuzzy searches the query is not turned lowercase anymore (so already lowercase queries work fine because the index is also lowercase but uppercase queries cannot match anything anymore). How can I prevent fuzzy search from turning off LowerCaseFilterFactory? Thanks! schema.xml (shortened to the most relevant parts): http://pastie.org/853448 Look here. I had the same problem: http://stackoverflow.com/questions/1840347/solr-wildcard-search-with-capital-letter Great many thanks for the reply! Though it's quite a bummer that I now have to implement this upstream..."
1129,A,"Solandra vs. ElasticSearch We are working with a Cassandra database that will store data in the petabyte range. We are thinking of using either ElasticSearch or Solandra but we are having a fun time deciding between which to use. I'm wondering if the our database might get too large. I know ElasticSearch is scalable but to what extent - especially with a Cassandra database. Solandra on the other hand is made for Cassandra and is highly scalable but again to what extent? Both are scalable but how scalable using Cassandra? Have a look at this presentation that kimchy (the ElasticSearch lead developer) made at Berlin Buzzwords 2011: http://berlinbuzzwords.de/sites/berlinbuzzwords.de/files/elasticsearch-bbuzz2011.pdf Have a look into this nice discussion: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/3f99e682887f98e4  Solandra is being used in the 10s of Terabytes range. Are you saying you want to index a PB of data in solandra or a subset? I think if you want 1 big index with a PB of data you are stretching the limits. but If you want a PB of indexes then this will scale the same as Cassandra. How many nodes are you planning to run? how much disk per node? This pretty much answers my question. ""10s of Terabyte range"" is really what I was asking. ALSO: Does Solandra store documents as-they-are (in rows of a column family for example) with the Lucene index containing only pointer information... or are documents bound into (stored with) the index itself (which of course is stored in Cassandra). If it ends up being a dumb/unclear question my apologies in advance."
1130,A,"Lucene Index and Query Design Question - Searching People I have recently just started working with Lucene (specifically Lucene.Net) and have successfully created several indicies and have no problem with any of them. Previously having worked with Endeca I find that Lucene is lightweight powerful and has a much lower learning curve (due mostly to a concise API). However I have one specific index/query situation which I am having problems wrapping my head around. What I have is a person directory. People can be searched for in this application with the goal of returning both exact and approximate matches. Right now in the index I concatenate the ""FirstName"" and ""LastName"" into a single field called ""FullName"" adding a space between the two. So FirstName:Jon with LastName:Smith yield FullName:Jon Smith. I do anticipate the possibility of middle names and possibly suffix but that is not important at the moment. I would like to do the equivalent of a fuzzy search on the name so someone searching for ""John Smith"" would still get back ""Jon Smith"". I had thought about a multisearch however this becomes more involved if his name was actually ""Jon Del Carmen"" or ""Jon Paul Del Carmen"". I have nothing in what the user types in to delineate the first name or last name pieces. The only thought that I have is that I could replace spaces in the concatenated value with a character that would not be discarded. If I did this when I built the document for the index and also when I parsed the query I could treat it as one larger word right? Is there another way to do this that would work for both simple names (""Jon Smith"") and also more complex names (""Jon Paul Del Carmen"")? Any advice would truly be appreciated. Thanks in advance! Edit: Additional detail follows. In Luke I put in the following query: FullName:jonn smith~ It is being parsed as: FullName:jonn CreatedOn:smith~0.5 With an Explanation of: BooleanQuery:boost=1.0000 clauses=2 maxClauses=1024 Clause 0: SHOULD TermQuery:boost=1.0000 Term: field='FullName' text='jonn' Cluase 1: SHOULD FuzzyQuery: boost=1.0000 prefixLen=0 minSimilarity=0.5000 org.apache.lucene.search.FuzzyTermEnum: diff=-1.0000 FilteredTermEnum: Exception null ""CreatedOn"" is another Field in the index. I tried putting quotes around the term ""jonn smith"" but it then treats it like a phrasequery instead. I am sure that the problem is that I am just not doing something right but being so green at all of this I am not sure what that something truly is. I'm a little confused. This seems like a pretty straight forward query. What does it matter if the name has 2 pieces or 10 pieces? Using a fuzzy and/or span query if they get close to the name it will return in the results. When using Luke or running from code something like FullName:John Smith~ is for some reason only using ""John"" and discarding everything after the space. I'll include the Explanation in my message so you can see if anything is obviously missing. Is my problem that I should be doing the query like: FullName:john\ smith~? It seems that escaping the space is causing it to correct properly. Disregard my previous comment. Escaping the space does show it parsing correctly in Luke but still returning no hits. My problem was with how I was building the index. What I ended up doing was making sure that it was not tokenizing the FullName and the query started returning the correct results. The Explain results from above were due to an ID10T error on my part and is now returning correctly."
