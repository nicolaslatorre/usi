,T0,T1,T2,T3,T4,Text
3180,"Anyone soloing using fogbugz? Is there anyone working solo and using fogbugz out there? I'm interested in personal experience/overhead versus paper. I am involved in several projects and get pretty hammered with lots of details to keep track of... Any experience welcome. (Yes I know Mr. Joel is on the stackoverflow team... I still want good answers :) Go to http://www.fogbugz.com/ then at the bottom under ""Try It"" sign up. under Settings => Your FogBugz Hosted Account it should either already say ""Payment Information: Using Student and Startup Edition."" or there should be some option/link to turn on the Student and Startup Edition. And yes it's not only for Students and Startups I asked their support :-) Disclaimer: I'm not affiliated with FogCreek and Joel did not just deposit money in my account. The tip about the ""Startup Edition"" was very helpful!  When I was working for myself doing my consulting business I signed up for a hosted account and honestly I couldn't have done without it. What I liked most about it was it took 30 seconds to sign up for an account and I was then able to integrate source control using sourcegear vault (which is an excellent source control product and free for single developers) set up projects clients releases and versions and monitor my progress constantly. One thing that totally blew me away was that I ended up completely abandoning outlook for all work related correspondence. I could manage all my client interactions from within fogbugz and it all just worked amazingly well. In terms of overhead one of the nice things you could do was turn anything into a case. Anything that came up in your mind while you were coding you simply created a new email sent it to fogbugz and it was instantly added as an item for review later. I would strongly recommend you get yourself one of the hosted accounts and give it a whirl  I use it as well and quite frankly wouldn't want to work without it. I've always had some kind of issue tracker available for the projects I work on and thus am quite used to updating it. With FB6 the process is now even better. Since FB also integrates with Subversion the source control tool I use for my projects the process is really good and I have two-way links between the two systems now. I can click on a case number in the Subversion logs and go to the case in FB or see the revisions bound to a case inside FB.  In addition to the benefits already mentioned another nice feature of using FogBugz is BugzScout which you can use to report errors from your app and log them into FogBugz automatically. If you're a one person team chances are there are some bugs in your code you've never seen during your own testing so it's nice to have those bugs found ""in the wild"" automatically reported and logged for you. +1 for making me aware of the BugzScout facility. Great stuff.  Yea FogBugz is great for process-light quick and easy task management. It seems especially well suited for soloing where you don't need or want a lot of complexity in that area. By the way if you want to keep track of what you're doing at the computer all day check out TimeSprite which integrates with FogBugz. It's a Windows app that logs your active window and then categorizes your activity based on the window title / activity type mappings you define as you go. (You can also just tell it what you're working on.) And if you're a FogBugz user you can associate your work with a FogBugz case and it will upload your time intervals for that case. This makes accurate recording of elapsed time pretty painless and about as accurate as you can get which in turn improves FogBugz predictive powers in its evidence-based scheduling. Also when soloing I find that such specific logging of my time keeps me on task in the way a meandering manager otherwise might. (I'm not affiliated with TimeSprite in any way.)  I use it especially since the hosted Version of FugBugz is free for up to 2 people. I found it a lot nicer than paper as I'm working on multiple projects and my paper tends to get rather messy once you start making annotations or if you want to re-organize and shuffle tasks around mark them as complete only to see that they are not complete after all... Plus the Visual Studio integration is really neat something paper just cannot compete with. Also if you lay the project to rest for 6 months and come back all your tasks and notes are still there whereas with paper you may need to search all the old documents and notes again if you did not discard it. But that is just the point of view from someone who is not really good at staying organized :-) If you are a really tidy and organized person paper may work better for you than it does for me. Bonus suggestion: Run Fogbugz on a second PC (or a small Laptop like the eeePC) so that you always have it at your fingertips. The main problem with Task tracking programs - be it FogBugz Outlook Excel or just notepad - is that they take up screen space and my two monitors are usually full with Visual Studio e-Mail Web Browsers some Notepads etc.  I think it's great that Joel et al. let people use FogBugs hosted for free on their own. It's a great business strategy because the users become fans (it is great software after all) and then they recommend it to their businesses or customers.",fogbugz,0.014913864907611688,0.0022686297190140574,0.09453871034940346,0.8828731199449088,0.005405675079061967
1383,"What is unit testing? I saw many questions asking 'how' to unit test in a specific language but no question asking 'what' 'why' and 'when'. What is it? What does it do for me? Why should I use it? When should I use it (also when not)? What are some common pitfalls and misconceptions Clean code talks: http://www.youtube.com/watch?v=wEhu57pih5w Uber there are a lot of resources available and the [Wikipedia article](http://en.wikipedia.org/wiki/Unit_testing) is certainly a great place to start. Once you've got a basic idea perhaps some more specific questions can help you decide for yourself if you want to use unit testing. That is not 1 specific question. That is 5 broad questions. Good thing you asked this question reading an answer of a working programmer is much better and to the point in compared to reading online resources. Unit testing is a practice to make sure that the function or module which you are going to implement is going to behave as expected (requirements) and also to make sure how it behaves in scenarios like boundary conditions and invalid input. xUnit NUnit mbUnit etc. are tools which help you in writing the tests.  Test Driven Development has sort of taken over the term Unit Test. As an old timer I will mention the more generic definition of it. Unit Test also means testing a single component in a larger system. This single component could be a dll exe class library etc. It could even be a single system in a multi-system application. So ultimately Unit Test ends up being the testing of whatever you want to call a single piece of a larger system. You would then move up to integrated or system testing by testing how all the components work together.  I don't disagree with Dan (although a better choice may just be not to answer)...but... Unit testing is the process of writing code to test the behavior and functionality of your system. Obviously tests improve the quality of your code but that's just a superficial benefit of unit testing. The real benefit are to Make it easier to change the technical implementation while making sure you don't change the behavior (refactoring). Properly unit tested code can be aggressively refactored/cleaned up with little chance of breaking anything without noticing it. Give developers confidence when adding behavior or making fixes. Document your code Indicate areas of your code that are tightly coupled. It's hard to unit test code that's tightly coupled Provide a means to use your API and look for difficulties early on Indicates methods and classes that aren't very cohesive You should unit test because its in your interest to deliver a maintainable and quality product to your client. I'd suggest you use it for any system or part of a system which models real-world behavior. In other words it's particularly well suited for enterprise development. I would not use it for throw-away/utility programs. I would not use it for parts of a system that are problematic to test (UI is a common example but that isn't always the case) The greatest pitfall is that developers test too large a unit or they consider a method a unit. This is particularly true if you don't understand Inversion of Control - in which case your unit tests will always turn into end-to-end integration testing. Unit test should test individual behaviors - and most methods have many behaviors. The greatest misconception is that programmers shouldn't test. Only bad or lazy programmers believe that. Should the guy building your roof not test it? Should the doctor replacement a heart valve not test the new valve? Only a programmer can test that his code does what he intended it to do (QA can test edge cases - how code behaves when its told to do things the programmer didn't intend and the client can do acceptance test - does the code do what what the client paid for it to do) +1 especially for the last paragraph!  Use Testivus. All you need to know is right there :)  Unit testing is about writing code that tests your application code. The Unit part of the name is about the intention to test small units of code (one method for example) at a time. xUnit is there to help with this testing - they are frameworks that assist with this. Part of that is automated test runners that tell you what test fail and which ones pass. They also have facilities to setup common code that you need in each test before hand and tear it down when all tests have finished. You can have a test to check that an expected exception has been thrown without having to write the whole try catch block yourself. How exactly does it help? @Alon - expanded on my answer.  Unit-testing is the testing of a unit of code (e.g. a single function) without the need for the infrastructure that that unit of code relies on. i.e. test it in isolation. If for example the function that you're testing connects to a database and does an update in a unit test you might not want to do that update. You would if it were an integration test but in this case it's not. So a unit test would exercise the functionality enclosed in the ""function"" you're testing without side effects of the database update. Say your function retrieved some numbers from a database and then performed a standard deviation calculation. What are you trying to test here? That the standard deviation is calculated correctly or that the data is returned from the database? In a unit test you just want to test that the standard deviation is calculated correctly. In an integration test you want to test the standard deviation calculation and the database retrieval.  I was never taught unit testing at university and it took me a while to ""get"" it. I read about it went ""ah right automated testing that could be cool I guess"" and then I forgot about it. It took quite a bit longer before I really figured out the point: Let's say you're working on a large system and you write a small module. It compiles you put it through its paces it works great you move on to the next task. Nine months down the line and two versions later someone else makes a change to some seemingly unrelated part of the program and it breaks the module. Worse they test their changes and their code works but they don't test your module; hell they may not even know your module exists. And now you've got a problem: broken code is in the trunk and nobody even knows. The best case is an internal tester finds it before you ship but fixing code that late in the game is expensive. And if no internal tester finds it...well that can get very expensive indeed. The solution is unit tests. They'll catch problems when you write code - which is fine - but you could have done that by hand. The real payoff is that they'll catch problems nine months down the line when you're now working on a completely different project but a summer intern thinks it'll look tidier if those parameters were in alphabetical order - and then the unit test you wrote way back fails and someone throws things at the intern until he changes the parameter order back. That's the ""why"" of unit tests. :-)  LibrarIES like NUnit xUnit or JUnit are just mandatory if you want to develop your projects using the TDD approach popularized by Kent Beck: You can read Introduction to Test Driven Development (TDD) or Kent Beck's book Test Driven Development: By Example. Then if you want to be sure your tests cover a ""good"" part of your code you can use software like NCover JCover PartCover or whatever. They'll tell you the coverage percentage of your code. Depending on how much you're adept at TDD you'll know if you've practiced it well enough :)  Unit testing is roughly speaking testing bits of your code in isolation with test code. The immediate advantages that come to mind are: Running the tests becomes automate-able and repeatable You can test at a much more granular level than point-and-click testing via a GUI Note that if your test code writes to a file opens a database connection or does something over the network it's more appropriately categorized as an integration test. Integration tests are a good thing but should not be confused with unit tests. Unit test code should be short sweet and quick to execute. Another way to look at unit testing is that you write the tests first. This is known as Test-Driven Development (TDD for short). TDD brings additional advantages: You don't write speculative ""I might need this in the future"" code -- just enough to make the tests pass The code you've written is always covered by tests By writing the test first you're forced into thinking about how you want to call the code which usually improves the design of the code in the long run. If you're not doing unit testing now I recommend you get started on it. Get a good book practically any xUnit-book will do because the concepts are very much transferable between them. Sometimes writing unit tests can be painful. When it gets that way try to find someone to help you and resist the temptation to ""just write the damn code"". Unit testing is a lot like washing the dishes. It's not always pleasant but it keeps your metaphorical kitchen clean and you really want it to be clean. :) Edit: One misconception comes to mind although I'm not sure if it's so common. I've heard a project manager say that unit tests made the team write all the code twice. If it looks and feels that way well you're doing it wrong. Not only does writing the tests usually speed up development but it also gives you a convenient ""now I'm done"" indicator that you wouldn't have otherwise. that was beautiful.  Chipping in on the philosophical pros of unit testing and TDD here are a few of they key ""lightbulb"" observations which struck me on my tentative first steps on the road to TDD enlightenment (none original or necessarily news)... TDD does NOT mean writing twice the amount of code. Test code is typically fairly quick and painless to write and is a key part of your design process and critically. TDD helps you to realize when to stop coding! Your tests give you confidence that you've done enough for now and can stop tweaking and move on to the next thing. The tests and the code work together to achieve better code. Your code could be bad / buggy. Your TEST could be bad / buggy. In TDD you are banking on the chances of BOTH being bad / buggy being fairly low. Often its the test that needs fixing but that's still a good outcome. TDD helps with coding constipation. You know that feeling that you have so much to do you barely know where to start? It's Friday afternoon if you just procrastinate for a couple more hours... TDD allows you to flesh out very quickly what you think you need to do and gets your coding moving quickly. Also like lab rats I think we all respond to that big green light and work harder to see it again! In a similar vein these designer types can SEE what they're working on. They can wander off for a juice / cigarette / iphone break and return to a monitor that immediately gives them a visual cue as to where they got to. TDD gives us something similar. It's easier to see where we got to when life intervenes... I think it was Fowler who said: ""Imperfect tests run frequently are much better than perfect tests that are never written at all"". I interprete this as giving me permission to write tests where I think they'll be most useful even if the rest of my code coverage is woefully incomplete. TDD helps in all kinds of surprising ways down the line. Good unit tests can help document what something is supposed to do they can help you migrate code from one project to another and give you an unwarranted feeling of superiority over your non-testing colleagues :) This presentation is an excellent introduction to all the yummy goodness testing entails. +1 for ""your tests might be wrong""! I'd give another +1 if I could for using tests as markers - leaving yourself a failing test on a Friday afternoon's an awesome way to get you quickly into the groove come Monday morning.  I use unit tests to save time. When building business logic (or data access) testing functionality can often involve typing stuff into a lot of screens that may or may not be finished yet. Automating these tests saves time. For me unit tests are a kind of modularised test harness. There is usually at least one test per public function. I write additional tests to cover various behaviours. All the special cases that you thought of when developing the code can be recorded in the code in the unit tests. The unit tests also become a source of examples on how to use the code. It is a lot faster for me to discover that my new code breaks something in my unit tests then to check in the code and have some front-end developer find a problem. For data access testing I try to write tests that either have no change or clean up after themselves. Unit tests aren’t going to be able to solve all the testing requirements. They will be able to save development time and test core parts of the application.  I would like to recommend the xUnit Testing Patterns book by Gerard Meszaros. It's large but is a great resource on unit testing. Here is a link to his web site where he discusses the basics of unit testing. http://xunitpatterns.com/XUnitBasics.html  I went to a presentation on unit testing at FoxForward 2007 and was told never to unit test anything that works with data. After all if you test on live data the results are unpredictable and if you don't test on live data you're not actually testing the code you wrote. Unfortunately that's most of the coding I do these days. :-) I did take a shot at TDD recently when I was writing a routine to save and restore settings. First I verified that I could create the storage object. Then that it had the method I needed to call. Then that I could call it. Then that I could pass it parameters. Then that I could pass it specific parameters. And so on until I was finally verifying that it would save the specified setting allow me to change it and then restore it for several different syntaxes. I didn't get to the end because I needed-the-routine-now-dammit but it was a good exercise.  I think the point that you don't understand is that unit testing frameworks like NUnit (and the like) will help you in automating small to medium-sized tests. Usually you can run the tests in a GUI (that's the case with NUnit for instance) by simply clicking a button and then - hopefully - see the progress bar stay green. If it turns red the framework shows you which test failed and what exactly went wrong. In a normal unit test you often use assertions e.g. Assert.AreEqual(expectedValue actualValue ""some description"") - so if the two values are unequal you will see an error saying ""some description: expected <expectedValue> but was <actualValue>"". So as a conclusion unit testing will make testing faster and a lot more comfortable for developers. You can run all the unit tests before committing new code so that you don't break the build process of other developers on the same project.  What do you do if you are given a pile of crap and seem like you are stuck in a perpetual state of cleanup that you know with the addition of any new feature or code can break the current set because the current software is like a house of cards? How can we do unit testing then? You start small. The project I just got into had no unit testing until a few months ago. When coverage was that low we would simply pick a file that had no coverage and click ""add tests"". Right now we're up to over 40% and we've managed to pick off most of the low-hanging fruit. (The best part is that even at this low level of coverage we've already run into many instances of the code doing the wrong thing and the testing caught it. That's a huge motivator to push people to add more testing.)  First of all whether speaking about Unit testing or any other kinds of automated testing (Integration Load UI testing etc.) the key difference from what you suggest is that it is automated repeatable and it doesn't require any human resources to be consumed (= nobody has to perform the tests they usually run at a press of a button).  The main difference of unit testing as opposed to ""just opening a new project and test this specific code"" is that it's automated thus repeatable. If you test your code manually it may convince you that the code is working perfectly - in its current state. But what about a week later when you made a slight modification in it? Are you willing to retest it again by hand whenever anything changes in your code? Most probably not :-( But if you can run your tests anytime with a single click exactly the same way within a few seconds then they will show you immediately whenever something is broken. And if you also integrate the unit tests into your automated build process they will alert you to bugs even in cases where a seemingly completely unrelated change broke something in a distant part of the codebase - when it would not even occur to you that there is a need to retest that particular functionality. This is the main advantage of unit tests over hand testing. But wait there is more: unit tests shorten the development feedback loop dramatically: with a separate testing department it may take weeks for you to know that there is a bug in your code by which time you have already forgotten much of the context thus it may take you hours to find and fix the bug; OTOH with unit tests the feedback cycle is measured in seconds and the bug fix process is typically along the lines of an ""oh sh*t I forgot to check for that condition here"" :-) unit tests effectively document (your understanding of) the behaviour of your code unit testing forces you to reevaluate your design choices which results in simpler cleaner design Unit testing frameworks in turn make it easy for you to write and run your tests. i like your answer much more than the accepted one +1 In addition my favourite part about test code (especially when given a new codebase): It demonstrates the expected use of the code under test.  This is my take on it. I would say unit testing is the practice of writing software tests to verify that your real software does what it is meant to. This started with jUnit in the Java world and has become a best practice in PHP as well with SimpleTest and phpUnit. It's a core practice of Extreme Programming and helps you to be sure that your software still works as intended after editing. If you have sufficient test coverage you can do major refactoring bug fixing or add features rapidly with much less fear of introducing other problems. It's most effective when all unit tests can be run automatically. Unit testing is generally associated with OO development. The basic idea is to create a script which sets up the environment for your code and then exercises it; you write assertions specify the intended output that you should receive and then execute your test script using a framework such as those mentioned above. The framework will run all the tests against your code and then report back success or failure of each test. phpUnit is run from the Linux command line by default though there are HTTP interfaces available for it. SimpleTest is web-based by nature and is much easier to get up and running IMO. In combination with xDebug phpUnit can give you automated statistics for code coverage which some people find very useful. Some teams write hooks from their subversion repository so that unit tests are run automatically whenever you commit changes. It's good practice to keep your unit tests in the same repository as your application.",unit-testing glossary,0.021129332257432222,0.015863721057362383,0.004120550396451055,0.9488664429151425,0.01001995337361185
1401,"ASP.Net Custom Client-Side Validation I have a custom validation function in JavaScript in a user control on a .Net 2.0 web site which checks to see that the fee paid is not in excess of the fee amount due. I've placed the validator code in the ascx file and I have also tried using Page.ClientScript.RegisterClientScriptBlock() and in both cases the validation fires but cannot find the JavaScript function. The output in Firefox's error console is ""feeAmountCheck is not defined"". Here is the function (this was taken directly from firefox->view source) <script type=""text/javascript""> function feeAmountCheck(source arguments) { var amountDue = document.getElementById('ctl00_footerContentHolder_Fees1_FeeDue'); var amountPaid = document.getElementById('ctl00_footerContentHolder_Fees1_FeePaid'); if (amountDue.value > 0 && amountDue >= amountPaid) { arguments.IsValid = true; } else { arguments.IsValid = false; } return arguments; } </script> Any ideas as to why the function isn't being found? How can I remedy this without having to add the function to my master page or consuming page? Try changing the argument names to ""sender"" and ""args"". And after you have it working switch the call over to ScriptManager.RegisterClientScriptBlock regardless of AJAX use.  While I would still like an answer to why my JS wasn't being recognized the solution I found in the meantime (and should have done in the first place) is to use an Asp:CompareValidator instead of an Asp:CustomValidator.  Also you could use: var amountDue = document.getElementById('<%=YourControlName.ClientID%>'); That will automatically resolve the client id for the element without you having to figure out that it's called 'ctl00_footerContentHolder_Fees1_FeeDue'.  When you're using .Net 2.0 and Ajax - you should use: ScriptManager.RegisterClientScriptBlock It will work better in Ajax environments then the old Page.ClientScript version",asp.net javascript validation,0.001485162075887588,0.7580876499870924,0.17889840068295973,0.05659771221527929,0.004931075038780909
3903,"Is this a good way to determine OS Architecture? Since the WMI class Win32_OperatingSystem only includes OSArchitecture in Windows Vista I quickly wrote up a method using the registry to try and determine whether or not the current system is a 32 or 64bit system. private Boolean is64BitOperatingSystem() {  RegistryKey localEnvironment = Registry.LocalMachine.OpenSubKey(""SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment"");  String processorArchitecture = (String) localEnvironment.GetValue(""PROCESSOR_ARCHITECTURE"");  if (processorArchitecture.Equals(""x86"")) {  return false;  }  else {  return true;  } } It's worked out pretty well for us so far but I'm not sure how much I like looking through the registry. Is this a pretty standard practice or is there a better method? Edit: Wow that code looks a lot prettier in the preview. I'll consider linking to a pastebin or something next time. The easiest way to test for 64-bit under .NET is to check the value of IntPtr.Size. EDIT: Doh! This will tell you whether or not the current process is 64-bit not the OS as a whole. Sorry!  Take a look at Raymond Chens solution: How to detect programmatically whether you are running on 64-bit Windows and here's the PINVOKE for .NET: IsWow64Process (kernel32) Update: I'd take issue with checking for 'x86'. Who's to say what intel's or AMD's next 32 bit processor may be designated as. The probability is low but it is a risk. You should ask the OS to determine this via the correct API's not by querying what could be a OS version/platform specific value that may be considered opaque to the outside world. Ask yourself the questions 1 - is the registry entry concerned properly documented by MS 2 - If it is do they provide a definitive list of possible values that is guaranteed to permit you as a developer to make the informed decision between whether you are running 32 bit or 64 bit. If the answer is no then call the API's yeah it's a but more long winded but it is documented and definitive.  The easiest way to test for 64-bit under .NET is to check the value of IntPtr.Size. I believe the value of IntPtr.Size is 4 for a 32bit app that's running under WOW isn't it? Edit: @Edit: Yeah. :)  Looking into the registry is perfectly valid so long as you can be sure that the user of the application will always have access to what you need.",c# windows registry,0.00981297284136161,0.07352014431113028,0.0027256501796376407,0.8244645673047506,0.08947666536311989
4954,"What are good regular expressions? I have worked for 5 years mainly in java desktop applications accessing Oracle databases and I have never used regular expressions. Now I enter Stack Overflow and I see a lot of questions about them; I feel like I missed something. For what do you use regular expressions? P.S. sorry for my bad english Don't forget to read the Javadocs for java.util.regex.Pattern. It's a good reference. Also http://perldoc.perl.org/perlre.html If you're just starting out with regular expressions I heartily recommend a tool like The Regex Coach: http://www.weitz.de/regex-coach/ also heard good things about RegexBuddy: http://www.regexbuddy.com/  Regular Expressions (or Regex) are used to pattern match in strings. You can thus pull out all email addresses from a piece of text because it follows a specific pattern. In some cases regular expressions are enclosed in forward-slashes and after the second slash are placed options such as case-insensitivity. Here's a good one :) /(bb|[^b]{2})/i Spoken it can read ""2 be or not 2 be"". The first part are the (brackets) they are split by the pipe | character which equates to an or statement so (a|b) matches ""a"" or ""b"". The first half of the piped area matches ""bb"". The second half's name I don't know but it's the square brackets they match anything that is not ""b"" that's why there is a roof symbol thingie (technical term) there. The squiggly brackets match a count of the things before them in this case two characters that are not ""b"". After the second / is an ""i"" which makes it case insensitive. Use of the start and end slashes is environment specific sometimes you do and sometimes you do not. Two links that I think you will find handy for this are regular-expressions.info Wikipedia - Regular expression It's a good description but Mike's real-world example is preferable to the punning '2b' one. Would be nice to combine the two.  Consider an example in Ruby: puts ""Matched!"" unless /\d{3}-\d{4}/.match(""555-1234"").nil? puts ""Didn't match!"" if /\d{3}-\d{4}/.match(""Not phone number"").nil? The ""/\d{3}-\d{4}/"" is the regular expression and as you can see it is a VERY concise way of finding a match in a string. Furthermore using groups you can extract information as such: match = /([^@]*)@(.*)/.match(""myaddress@domain.com"") name = match[1] domain = match[2] Here the parenthesis in the regular expression mark a capturing group so you can see exactly WHAT the data is that you matched so you can do further processing. This is just the tip of the iceberg... there are many many different things you can do in a regular expression that makes processing text REALLY easy.  Coolest regular expression ever: /^1?$|^(11+?)\1+$/ It tests if a number is prime. And it works!! N.B.: to make it work a bit of set-up is needed; the number that we want to test has to be converted into a string of “1”s first then we can apply the expression to test if the string does not contain a prime number of “1”s: def is_prime(n) str = ""1"" * n return str !~ /^1?$|^(11+?)\1+$/ end There’s a detailled and very approachable explanation over at Avinash Meetoo’s blog. It's clever but hardly appropriate for a beginner! :) That doesn't seem to work... or I'm missing something. @Copas: It absolutely works. Have you read the how-to and explanation that I’ve linked?  If you want to learn about regular expressions I recommend Mastering Regular Expressions. It goes all the way from the very basic concepts all the way up to talking about how different engines work underneath. The last 4 chapters also gives a dedicated chapter to each of PHP .Net Perl and Java. I learned a lot from it and still use it as a reference.  These RE's are specific to Visual Studio and C++ but I've found them helpful at times: Find all occurrences of ""routineName"" with non-default params passed: routineName\(:a+\) Conversely to find all occurrences of ""routineName"" with only defaults: routineName\(\) To find code enabled (or disabled) in a debug build: \#if.DEBUG Note that this will catch all the variants: ifdef if defined ifndef if !defined  A regular expression (regex or regexp for short) is a special text string for describing a search pattern. You can think of regular expressions as wildcards on steroids. You are probably familiar with wildcard notations such as *.txt to find all text files in a file manager. The regex equivalent is .*\.txt$. A great resource for regular expressions: http://www.regular-expressions.info  As you may know Oracle now has regular expressions: http://www.oracle.com/technology/oramag/webcolumns/2003/techarticles/rischert_regexp_pt1.html. I have used the new functionality in a few queries but it hasn't been as useful as in other contexts. The reason I believe is that regular expressions are best suited for finding structured data buried within unstructured data. For instance I might use a regex to find Oracle messages that are stuffed in log file. It isn't possible to know where the messages are--only what they look like. So a regex is the best solution to that problem. When you work with a relational database the data is usually pre-structured so a regex doesn't shine in that context.  Validating strong passwords: This one will validate a password with a length of 5 to 10 alphanumerical characters with at least one upper case one lower case and one digit: ^(?=.*[A-Z])(?=.*[a-z])(?=.*[0-9])[a-zA-Z0-9]{510}$",regex,0.051386548228458014,0.04586561275341745,0.6583770234264719,0.13723762450098792,0.1071331910906647
833,"Editing database records by multiple users I have designed database tables (normalised on an MS SQL server) and created a standalone windows front end for an application that will be used by a handful of users to add and edit information. We will add a web interface to allow searching accross our production area at a later date. I am concerned that if two users start editing the same record then the last to commit the update would be the 'winner' and important information may be lost. A number of solutions come to mind but I'm not sure if I am going to create a bigger headache. Do nothing and hope that two users are never going to be editing the same record at the same time. - Might never happed but what if it does? Editing routine could store a copy of the original data as well as the updates and then compare when the user has finished editing. If they differ show user and comfirm update - Would require two copies of data to be stored. Add last updated DATETIME column and check it matches when we update if not then show differences. - requires new column in each of the relevant tables. Create an editing table that registers when users start editing a record that will be checked and prevent other users from editing same record. - would require carful thought of program flow to prevent deadlocks and records becoming locked if a user crashes out of the program. Are there any better solutions or should I go for one of these? @ Mark Harrison : SQL Server does not support that syntax (SELECT ... FOR UPDATE). The SQL Server equivalent is the SELECT statement hint UPDLOCK. See SQL Server Books Online for more information.  Another option is to test that the values in the record your changing are the still the same as they where when you started: SELECT customer_nm customer_nm AS customer_nm_orig FROM demo_customer WHERE customer_id = @p_customer_id (display the customer_nm field and the user changes it) UPDATE demo_customer SET customer_nm = @p_customer_name_new WHERE customer_id = @p_customer_id AND customer_name = @p_customer_nm_old IF @@ROWCOUNT = 0 RAISERROR( 'Update failed: Data changed' ); You don't have to add a new column to your table (and keep it up to date) but you do have to create more verbose SQL statements and pass ""new"" and ""old"" fields to the stored procedure. It also has the advantage that your not locking the records - because we all know that records will end up staying locked when they should not be...  If you expect infrequent collisions Optimistic Concurrency is probably your best bet. Scott Mitchell wrote a comprehensive tutorial on implementing that pattern: Implementing Optimistic Concurrency  With me the best way i have a column lastupdate (timetamp datatype). when select and update just compare this value another advance of this solution is that you can use this column to track down the time data has change. I think it is not good if you just create a colum like isLock for check update.  SELECT FOR UPDATE and equivalents are good providing you hold the lock for a microscopic amount of time but for a macroscopic amount (e.g. the user has the data loaded and hasn't pressed 'save' you should use optimistic concurrency as above. (Which I always think is misnamed - it's more pessimistic than 'last writer wins' which is usually the only other alternative considered.)  The database will do this for you. Look at ""select ... for update"" which is designed just for this kind of thing. It will give you a write lock on the selected rows which you can then commit or roll back.  A classic approach is as follows: add a boolean field  ""locked"" to each table. set this to false by default. when a user starts editing you do this: lock the row (or the whole table if you can't lock the row) check the flag on the row you want to edit if the flag is true then inform the user that they cannot edit that row at the moment else set the flag to true release the lock when saving the record set the flag back to false For such cases (application/browser crash) you can add ForceUnlock method to the application which will forcibly set lock to False. Good point @SilverNight - can you post the correct solution? This is not good enought if set the lock=true if the application or the browser is crash then  the record is dead lock forever.",sql-server database,6.172261109216524E-4,0.0025632515588030623,0.0012986922533367556,0.1817980546991449,0.8137227753777936
4230,The Difference Between a DataGrid and a GridView in ASP.NET? I've been doing ASP.NET development for a little while now and I've used both the GridView and the DataGrid controls before for various things but I never could find a really good reason to use one or the other. I'd like to know: What is the difference between these 2 ASP.NET controls? What are the advantages or disadvantages of both? Is one any faster? Newer? Easier to maintain? The intellisense summary for the controls doesn't seem to describe any difference between the two. They both can view edit and sort data and automatically generate columns at runtime. Edit: Visual Studio 2008 no longer lists DataGrid as an available control in the toolbox. It is still available (for legacy support I assume) if you type it in by hand though. The DataGrid was originally in .NET 1.0. The GridView was introduced (and replaced the DataGrid) in .NET 2.0. They provide nearly identical functionality.  some basic diffrence between gridview and details view the GridView control also has a number of new features and advantages over the DataGrid control which include: · Richer design-time capabilities. · Improved data source binding capabilities. · Automatic handling of sorting paging updates and deletes. · Additional column types and design-time column operations. · A Customized pager user interface (UI) with the PagerTemplate property. Differences between the GridView control and the DataGrid control include: · Different custom-paging support. · Different event models.  One key difference security wise is that DataGrid uses BoundColumn which does not HtmlEncode the bound data. There is no property to turn HtmlEncoding on or off either so you need to do it in code somehow. GridView uses BoundField which does HtmlEncode by default on the bound data and it has a HtmlEncode property if you need to turn it off.  DataGrid was an ASP.NET 1.1 control still supported. GridView arrived in 2.0 made certain tasks simpler added different databinding features: This link has a comparison of DataGrid and GridView features - http://msdn.microsoft.com/en-gb/magazine/cc163933.aspx  The key difference is in the ViewState management IIRC. The DataGrid requires ViewState turned on in order to have edit and sort capabilities.  The GridView control is the successor to the DataGrid control. Like the DataGrid control the GridView control was designed to display data in an HTML table. When bound to a data source the DataGrid and GridView controls each display a row from a DataSource as a row in an output table. Both the DataGrid and GridView controls are derived from the WebControl class. Although it has a similar object model to that of the DataGrid control the GridView control also has a number of new features and advantages over the DataGrid control which include: Richer design-time capabilities. Improved data source binding capabilities. Automatic handling of sorting paging updates and deletes. Additional column types and design-time column operations. A Customized pager user interface (UI) with the PagerTemplate property. Differences between the GridView control and the DataGrid control include: Different custom-paging support. Different event models. Sorting paging and in-place editing of data requires additional coding when using the DataGrid control. The GridView control enables you to add sorting paging and editing capabilities without writing any code. Instead you can automate these tasks along with other common tasks such as data binding to a data source by setting properties on the control.  If you're working in Visual Studio 2008 / .NET 3.5 you probably shouldn't use either. Use the ListView - it gives you the features of the GridView combined with the styling flexibility of a repeater.,asp.net,6.274111706434905E-4,0.17174431368710838,0.0013201224195699566,0.7252169477102928,0.10109120501238539
3589,Backup SQL Schema Only? I need to create a backup of a SQL Server 2005 Database that's only the structure...no records just the schema. Is there any way to do this? EDIT: I'm trying to create a backup file to use with old processes so a script wouldn't work for my purposes sorry Toad for SQL Server does this nicely if you're considering a commercial product.  Use a 3 step process: Generate a script from the working database Create a new database from that script Create a backup of the new database  Why not just use SQL Management Studio to create a complete script of your database and the objects?  I make heavy use of this tool: SQLBalance for MySQL Unfortunately; its windows only... but works like a charm to move databases around data or no data merge or compare.,sql sql-server backup,0.0028478360779729726,0.030551664177075458,0.005992070957130802,0.3894026913601158,0.571205737427705
1108,"How does database indexing work? Given that indexing is so important as your dataset increases in size can someone explain how indexing works at a database agnostic level? For information on queries to index a field check out http://stackoverflow.com/questions/1156/how-do-i-index-a-database-field Here is a very well explained tutorial of index. I recommend reading it. Use the Index Luke Note that link-only answers are discouraged (links tend to get stale over time). Please consider editing your answer and adding a synopsis here. that link is thorough This link is a wide definitions.. great article... :)  Why is it needed? When data is stored on disk based storage devices it is stored as blocks of data. These blocks are accessed in their entirety making them the atomic disk access operation. Disk blocks are structured in much the same way as linked lists; both contain a section for data a pointer to the location of the next node (or block) and both need not be stored contiguously. Due to the fact that a number of records can only be sorted on one field we can state that searching on a field that isn’t sorted requires a Linear Search which requires N/2 block accesses (on average) where N is the number of blocks that the table spans. If that field is a non-key field (i.e. doesn’t contain unique entries) then the entire table space must be searched at N block accesses. Whereas with a sorted field a Binary Search may be used this has log2 N block accesses. Also since the data is sorted given a non-key field the rest of the table doesn’t need to be searched for duplicate values once a higher value is found. Thus the performance increase is substantial. What is indexing? Indexing is a way of sorting a number of records on multiple fields. Creating an index on a field in a table creates another data structure which holds the field value and pointer to the record it relates to. This index structure is then sorted allowing Binary Searches to be performed on it. The downside to indexing is that these indexes require additional space on the disk since the indexes are stored together in a table using the MyISAM engine this file can quickly reach the size limits of the underlying file system if many fields within the same table are indexed. How does it work? Firstly let’s outline a sample database table schema;  Field name Data type Size on disk id (Primary key) Unsigned INT 4 bytes firstName Char(50) 50 bytes lastName Char(50) 50 bytes emailAddress Char(100) 100 bytes Note: char was used in place of varchar to allow for an accurate size on disk value. This sample database contains five million rows and is unindexed. The performance of several queries will now be analyzed. These are a query using the id (a sorted key field) and one using the firstName (a non-key unsorted field). Example 1 Given our sample database of r = 5000000 records of a fixed size giving a record length of R = 204 bytes and they are stored in a table using the MyISAM engine which is using the default block size B = 1024 bytes. The blocking factor of the table would be bfr = (B/R) = 1024/204 = 5 records per disk block. The total number of blocks required to hold the table is N = (r/bfr) = 5000000/5 = 1000000 blocks. A linear search on the id field would require an average of N/2 = 500000 block accesses to find a value given that the id field is a key field. But since the id field is also sorted a binary search can be conducted requiring an average of log2 1000000 = 19.93 = 20 block accesses. Instantly we can see this is a drastic improvement. Now the firstName field is neither sorted so a binary search is impossible nor are the values unique and thus the table will require searching to the end for an exact N = 1000000 block accesses. It is this situation that indexing aims to correct. Given that an index record contains only the indexed field and a pointer to the original record it stands to reason that it will be smaller than the multi-field record that it points to. So the index itself requires fewer disk blocks that the original table which therefore requires fewer block accesses to iterate through. The schema for an index on the firstName field is outlined below;  Field name Data type Size on disk firstName Char(50) 50 bytes (record pointer) Special 4 bytes Note: Pointers in MySQL are 2 3 4 or 5 bytes in length depending on the size of the table. Example 2 Given our sample database of r = 5000000 records with an index record length of R = 54 bytes and using the default block size B = 1024 bytes. The blocking factor of the index would be bfr = (B/R) = 1024/54 = 18 records per disk block. The total number of blocks required to hold the table is N = (r/bfr) = 5000000/18 = 277778 blocks. Now a search using the firstName field can utilise the index to increase performance. This allows for a binary search of the index with an average of log2 277778 = 18.08 = 19 block accesses. To find the address of the actual record which requires a further block access to read bringing the total to 19 + 1 = 20 block accesses a far cry from the 277778 block accesses required by the non-indexed table. When should it be used? Given that creating an index requires additional disk space (277778 blocks extra from the above example) and that too many indexes can cause issues arising from the file systems size limits careful thought must be used to select the correct fields to index. Since indexes are only used to speed up the searching for a matching field within the records it stands to reason that indexing fields used only for output would be simply a waste of disk space and processing time when doing an insert or delete operation and thus should be avoided. Also given the nature of a binary search the cardinality or uniqueness of the data is important. Indexing on a field with a cardinality of 2 would split the data in half whereas a cardinality of 1000 would return approximately 1000 records. With such a low cardinality the effectiveness is reduced to a linear sort and the query optimizer will avoid using the index if the cardinality is less than 30% of the record number effectively making the index a waste of space. Hi Xenph . thanks for your answer . Its awesome !!! Why linear search only require N/2 block access ? lets suppose a table with 20 records and each block able to hold 5 record. So we have 4 blocks .. Doing a linear search in the worst case will access all the blocks. isn´t ? @CHAPa it's N/2 in the average case. You are correct in assuming that in the worst case it's N. @Xenph thanks for reply ! So why did u use N/2 ? Do you have any resource about database indexes ? +1 for your post ! really awesome ! @XenphYan : I don't get this. `""query optimizer will avoid using the index if the cardinality is ` **greater** `than 30% of the record number""` I thought to use index cardinality should be higher. Is it incorrect? @Bhathiya You're correct. I've updated the answer to reflect this. Awesom explanation! I have one important question. If I have a non-unique field that is not sorted and if the cardinality of the field is 2 would the index table of this field have just 2 rows? If yes would there be multiple pointers for each of these 2 rows pointing to all the rows that have this value? If no would the index table have as many rows as there are for this field in the original table? binary search can be done when the data is unique am i right? although you mentioned that minimum cardinality is important the algorithm wouldn't be a simple binary search how would this approximation (~log2 n) affect the process time? @CheckRaise The fact that the question has been viewed more than 100243 times speak for itself. SO is a knowledge sharing forum at the first place and you will find the problem in it when you think otherwise. This is also a great read: http://kylebanker.com/blog/2010/09/21/the-joy-of-mongodb-indexes/ @XenphYan - So an index is just a way to sort data in a column and keep that sort order handy for quickly accessing the column elements ? If we update a non-indexed column then performance should not be affected right ? Related question - http://stackoverflow.com/questions/16124690/lesser-the-number-of-indexes-means-faster-inserts-updates-and-deletes ""it stands to reason that indexing fields used only for output would be simply a waste"" This is not entirely true if you consider covering indexes. A covering index will respond to queries by pulling values only from the index without needing to look up matching records. @CHAPa: He has already explained it but since you didn't get it yet here is a detailed explanation. Example: Worst Case: Total records=20 & I'm searching 20th element. In Linear search no. of comparisons=N. But there can also be other cases like say you are searching for 4th element so that will take only 4 comparisons! Best Case: finding 1st element bingo! you get it right at the first shot! To compare algorithms you cannot assume best case/worst caseyou need average case. Avg case is element you want is sitting in the middle-how many comparisons would you make to reach the middle? N/2. @AbhishekShivkumar:Great question!I think the index table will have as many rows as there are in the data table. And as this field will have only 2 values(boolean with true/false) & say you want a record with value truethen you can only halve the result set in first pass in second pass all your records have value true so there is no basis to differentiatenow you have to search the data table in linear fashion-hence he said cardinality should be considered while deciding the indexed column. In this caseit's worthless to index on such a column. Hope I'm correct :) shouldn't the number of block accesses in the average case be `(N+1)/2`. If we sum the number of block accesses for all possible cases and divide it by the number of cases then we have `N*(N+1)/(2*n)` which comes out to be `(N+1)/2`. Higher cardinality fields can make use of bitmap indexes. It is designed specifically for fields which hold lots of duplicate values e.g. a gender field. Finally clarified. I have one more question. If a table is small and will stay small (example a country list) it looks like not indexing at all is both a win in time and disk space. On huge tables indexing CHAR fields should be avoided if possible especially if searches on CHAR fields are rather rare. That's probably why big forums only allow one keyword search every minute. I think there are a few typos in this answer for example in the sentence: ""a far cry from the 277778 block accesses required by the non-indexed table."" doesn't the author mean 1000000 block accesses? 277778 is the number of blocks required by the index itself. There seems to be a couple of other inaccuracies too :( The answer doesn't seem to explain that the index itself is sorted? I.e. they just jump to this explanation: ""Now a search using the firstName field can utilise the index to increase performance. This allows for a binary search of the index with an average of log2 277778 = 18.08 = 19 block accesses"" - the index MUST be sorted in order for binary search to be possible but I can't see that explained anywhere. It would also be great if there was an explanation of how the indexing works on multiple fields the answer only explains indexing of a single field the firstName  The first time I read this it was very helpful to me. Thank you. Since then I gained some insight about the downside of creating indexes: if you write into a table (UPDATE or INSERT) with one index you have actually two writing operations in the file system. One for the table data and another one for the index data (and the resorting of it (and - if clustered - the resorting of the table data)). If table and index are located on the same hard disk this costs more time. Thus a table without an index (a heap)  would allow for quicker write operations. (if you had two indexes you would end up with three write operations and so on) However defining two different locations on two different hard disks for index data and table data can decrease/eliminate the problem of increased cost of time. This requires definition of additional file groups with according files on the desired hard disks and definition of table/index location as desired. Another problem with indexes is their fragmentation over time as data is inserted. REORGANIZE helps you must write routines to have it done. In certain scenarios a heap is more helpful than a table with indexes e.g:- If you have lots of rivalling writes but only one nightly read outside business hours for reporting. Also a differentiation between clustered and non-clustered indexes is rather important. Helped me:- What do Clustered and Non clustered index actually mean? I think these indexing issues can be resolved by maintaining two different databases just as Master and Slave. Where Master can be used to insert or update records. Without indexing. And slave can be used to read with proper indexing right??? no wrong sorry. not just the content of the tables must be updated but also the index structure and content (b-tree nodes). your concept of master and slave makes no sense here. what can be feasable though is replicating or mirroring to a second database on which analytics take place to take that workload away from the first database. that second database would hold copies of data *and* indexes on that data. Ya...! Try to read my comment and understand it properly. I also said the same I referred to master and slave (whatever) as ""eplicating or mirroring to a second database on which analytics take place to take that workload away from the first database. that second database would hold copies of data and indexes on that data"" the second database - to which mirroring or replicating is done the slave - would experience all the data manipulation as the first one does. with each dml-operation the indexes on that second database would experience ""these indexing issues"". i don't see the gain in that where ever the indexes are needed and built for quick analysis they need to be kept up to date.  Although the other answers are very good I would say that: An index is just a data structure that makes the searching faster for a specific column in a database. This structure is usually a b-tree but it can also be a hash table or some other logic structure. For more information I recommend this webpage: http://www.programmerinterview.com/index.php/database-sql/what-is-an-index/",database indexing article,1.782124385790449E-4,0.008942531512988584,0.002718527493831141,0.0032761265307119907,0.9848846020238892
3802,How do you typeset code elements in normal text? What is the best way to typeset a function with arguments for readibility brevity and accuracy? I tend to put empty parentheses after the function name like func() even if there are actually arguments for the function. I have trouble including the arguments and still feeling like the paragraph is readable. Any thoughts on best practices for this? I usually take that approach but if I feel like it's going to cause confusion I'll use ellipses like: myFunction(...) I guess if I were good I would use those any time I was omitting parameters from a function in text.  I would simply be a little more careful with the name of my variables and parameters most people will then be able to guess much more accurately what type of data you want to hold in it.,language-agnostic format,0.11405419096477998,0.6766083266585879,0.036259340306686025,0.15920721523702083,0.013870926832925151
1709,"How to pass enumerated values to a web service My dilemma is basically how to share an enumeration between two applications. The users upload documents through a front-end application that is on the web. This application calls a web service of the back-end application and passes the document to it. The back-end app saves the document and inserts a row in the Document table. The document type (7 possible document types: Invoice Contract etc.) is passed as a parameter to the web service's UploadDocument method. The question is what should the type (and possible values) of this parameter be? Since you need to hardcode these values in both applications I think it is O.K. to use a descriptive string (Invoice Contract WorkOrder SignedWorkOrder). Is it maybe a better approach to create a DocumentTypes enumeration in the first application and to reproduce it also in the second application and then pass the corresponding integer value to the web service between them? I'd suggest against passing an integer between them simply for purposes of readability and debugging. Say you're going through your logs and you see a bunch of 500 errors for DocumentType=4. Now you've got to go look up which DocumentType is 4. Or if one of the applications refers to a number that doesn't exist in the other perhaps due to mismatched versions. It's a bit more code and it rubs the static typing part of the brain a bit raw but in protocols on top of HTTP the received wisdom is to side with legible strings over opaque enumerations.  If you are consuming your Web service from a .NET page/application you should be able to access the enumeration after you add your Web reference to the project that is consuming the service.  There are some fairly good reasons for not using enums on an interface boundary like that. Consider Dare's post on the subject.  I've noticed that when using ""Add Service Reference"" as opposed to ""Add Web Reference"" from VS.net the actual enum values come across as well as the enum names. This is really annoying as I need to support both 2.0 and 3.5 clients. I end up having to go into the 2.0 generated web service proxy code and manually adding the enum values every time I make a change!  If you are not working with .NET to .NET SOAP you can still define an enumerator provided both endpoints are using WSDL. <s:simpleType name=""MyEnum""> <s:restriction base=""s:string""> <s:enumeration value=""Wow""/> <s:enumeration value=""This""/> <s:enumeration value=""Is""/> <s:enumeration value=""Really""/> <s:enumeration value=""Simple""/> </s:restriction> </s:simpleType> Its up to the WSDL -> Proxy generator tool to parse that into a enum equivalent in the client language.  I would still use enumeration internally but would expect consumers to pass me only the name not the numeric value itself. just some silly example to illustrate: public enum DocumentType {  Invoice  Contract  WorkOrder  SignedWorkOrder } [WebMethod] public void UploadDocument(string type byte[] data) {  DocumentType docType = (DocumentType)Enum.Parse(typeof(DocumentType) type); }  In .NET enumeration values are (by default) serialized into xml with the name. For instances where you can have multiple values (flags) then it puts a space between the values. This works because the enumeration doesn't contain spaces so you can get the value again by splitting the string (ie. ""Invoice Contract SignedWorkOrder"" using lubos's example). You can control the serialization of values of in asp.net web services using the XmlEnumAttribute or using the EnumMember attribute when using WCF.  I can only speak about .net but if you have an ASP.net Webservice you should be able to add an enumeration directly to it. When you then use the ""Add Web Reference"" in your Client Application the resulting Class should include that enum But this is from the top of my head i'm pretty sure i've done it in the past but I can't say for sure. The 'Add Web Reference' does create enums but only those that are referenced in some method. I can manually add enums to the automatically generated Reference.cs file and all works well. Maybe this is separate question but is there any way to get the 'Add Web Reference' to add all enums in a referenced assembly (but not used in the code) without hacking the Reference.cs file manually? @Dave Web Services encapsulate methods so if there are no methods using an Enum there is no point - from a Web Service Perspective - to have it referenced. I recommend asking a separate question with a description what you want to do/why you want an unreferenced enum in it.",web-services application-integration,6.516039356359426E-4,0.5982386323381292,0.19416935436681465,0.0848135320800307,0.12212687727938949
19,"What is the fastest way to get the value of π? Solutions are welcome in any language. :-) I'm looking for the fastest way to obtain the value of π as a personal challenge. More specifically I'm using ways that don't involve using #defined constants like M_PI or hard-coding the number in. The program below tests the various ways I know of. The inline assembly version is in theory the fastest option though clearly not portable; I've included it as a baseline to compare the other versions against. In my tests with built-ins the 4 * atan(1) version is fastest on GCC 4.2 because it auto-folds the atan(1) into a constant. With -fno-builtin specified the atan2(0 -1) version is fastest. Here's the main testing program (pitimes.c): #include <math.h> #include <stdio.h> #include <time.h> #define ITERS 10000000 #define TESTWITH(x) { \ diff = 0.0; \ time1 = clock(); \ for (i = 0; i < ITERS; ++i) \ diff += (x) - M_PI; \ time2 = clock(); \ printf(""%s\t=> %e time => %f\n"" #x diff diffclock(time2 time1)); \ } static inline double diffclock(clock_t time1 clock_t time0) { return (double) (time1 - time0) / CLOCKS_PER_SEC; } int main() { int i; clock_t time1 time2; double diff; /* Warmup. The atan2 case catches GCC's atan folding (which would * optimise the ``4 * atan(1) - M_PI'' to a no-op) if -fno-builtin * is not used. */ TESTWITH(4 * atan(1)) TESTWITH(4 * atan2(1 1)) #if defined(__GNUC__) && (defined(__i386__) || defined(__amd64__)) extern double fldpi(); TESTWITH(fldpi()) #endif /* Actual tests start here. */ TESTWITH(atan2(0 -1)) TESTWITH(acos(-1)) TESTWITH(2 * asin(1)) TESTWITH(4 * atan2(1 1)) TESTWITH(4 * atan(1)) return 0; } And the inline assembly stuff (fldpi.c) noting that it will only work for x86 and x64 systems: double fldpi() { double pi; asm(""fldpi"" : ""=t"" (pi)); return pi; } And a build script that builds all the configurations I'm testing (build.sh): #!/bin/sh gcc -O3 -Wall -c -m32 -o fldpi-32.o fldpi.c gcc -O3 -Wall -c -m64 -o fldpi-64.o fldpi.c gcc -O3 -Wall -ffast-math -m32 -o pitimes1-32 pitimes.c fldpi-32.o gcc -O3 -Wall -m32 -o pitimes2-32 pitimes.c fldpi-32.o -lm gcc -O3 -Wall -fno-builtin -m32 -o pitimes3-32 pitimes.c fldpi-32.o -lm gcc -O3 -Wall -ffast-math -m64 -o pitimes1-64 pitimes.c fldpi-64.o -lm gcc -O3 -Wall -m64 -o pitimes2-64 pitimes.c fldpi-64.o -lm gcc -O3 -Wall -fno-builtin -m64 -o pitimes3-64 pitimes.c fldpi-64.o -lm Apart from testing between various compiler flags (I've compared 32-bit against 64-bit too because the optimisations are different) I've also tried switching the order of the tests around. The atan2(0 -1) version still comes out top every time though. *boggle* If it's a personal challenge why are you asking us for the solution? =) I asked the question initially because I was one of the very first people in the private beta and Jeff was asking people to populate the site with questions. This was one I was dealing with at the time to solve this problem: http://stackoverflow.com/questions/1053 [continues] [continued] However aside from that specific purpose (which is kinda frivolous anyway) this question itself is frivolous and was not intended to be a serious question at all. In fact of the 4 questions I've so for posted only one is serious. :-P @erik: Not all languages have a built-in constant like `M_PI`. I was trying to find an ""authoritative"" way to get a (floating-point) value of pi that (in theory) works across a variety of languages (and/or their built-in libraries). My current preferred method is using `atan2(0 -1)` but perhaps there are better ways. There's gotta to be a way to do it in C++ metaprogramming. The run time will be really good but the compile time won't be. @David: http://stackoverflow.com/questions/19/fastest-way-to-get-value-of-pi/1947163#1947163 Why do you consider using atan(1) different from using M_PI? I'd understand why you want to do this if you only used arithmetic operations but with atan I don't see the point. codegolf much?? the question is: why would you _not_ want to use a constant? e.g. either defined by a library or by yourself? Computing Pi is a waste of CPU cycles as this problem has been solved over and over and over again to a number of significant digits much greater than needed for daily computations There is only one solution which is faster than pre-calculate constant PI: pre-calculate all the values appear in formulas e.g. when circumference needed you may pre-calculate 2*PI instead of multiplying every time the PI by 2 in runtime. @Doorhandle Well you're talking to [user #3](http://codegolf.stackexchange.com/users/3) on [Code Golf SE](http://codegolf.stackexchange.com/) and the poster of [Stack Overflow code golf](http://stackoverflow.com/q/62188/13) so. ;-) Code Golf much? Please close the thread by accepting an answer considering there are respectful amount of good answers. @Zeus Really?! You're going to accept-rate-police a 100k+ user who's been on the site since day 1 of the private beta? @ChrisJester-Young Nope. That's not the intention.. I recently started reading more into the rules.. and thought I would do my part on the threads that I visit to remind users that might have forgotten coz of the long time frame. I'm in no way trying to be a police here. Apologies if i came across rude. @Zeus In this specific case my question was actually intended to be a ""fun"" micro-optimisation question (which these days would probably be a better fit for [Programming Puzzles & Code Golf](http://codegolf.stackexchange.com/)) but the general premise of ""fastest way to calculate pi"" seemed to be useful enough to keep this question here. So at some stage I will probably reevaluate whether I should just accept the best algorithmic answer (probably nlucaroni's one) without regard to whether it's related to micro-optimisation. @HopelessN00b In the dialect of English I speak ""optimisation"" is [spelt](http://en.wiktionary.org/wiki/spelt#Verb) with an ""s"" not a ""z"" (which is pronounced as ""zed"" BTW not ""zee"" ;-)). (This is not the first time I've had to revert this sort of edit too if you look at the review history.) nlucaroni's answer has reached 100 upvotes (congrats) so it's probably a good point to green-tick it. Enjoy! (Though since it's community wiki and all it is generating no rep so not even sure if nlucaroni will even notice this.) If you want to compute an approximation of the value of π (for some reason) you should try a binary extraction algorithm. Bellard's improvement of BBP gives does PI in O(N^2). If you want to obtain an approximation of the value of π to do calculations then: PI = 3.141592654 Granted that's only an approximation and not entirely accurate. It's off by a little more than 0.00000000004102. (four ten-trillionths about 4/10000000000). If you want to do math with π then get yourself a pencil and paper or a computer algebra package and use π's exact value π. If you really want a formula this one is fun: π = -i ln(-1) I was just being ""cute"" with Euler's formula :P Your formula depends on how you define ln in the complex plane. It has to be non-contiguous along one line in the complex plane and it's quite common for that line to be the negative real axis.  Just came across this one that should be here for completeness: calculate PI in Piet It has the rather nice property that the precision can be improved making the program bigger. Here's some insight into the language itself I always loved this Piet program. Shows how Piet is clearly more practical than BF ;-)  If you are willing to use an approximation 355 / 113 is good for 6 decimal digits and has the added advantage of being usable with integer expressions. That's not as important these days as ""floating point math co-processor"" ceased to have any meaning but it was quite important once.  There's actually a whole book dedicated (amongst other things) to fast methods for the computation of \pi: 'Pi and the AGM' by Jonathan and Peter Borwein (available on Amazon). I studied the AGM and related algorithms quite a bit: it's quite interesting (though sometimes non-trivial). Note that to implement most modern algorithms to compute \pi you will need a multiprecision arithmetic library (GMP is quite a good choice though it's been a while since I last used it). The time-complexity of the best algorithms is in O(M(n)log(n)) where M(n) is the time-complexity for the multiplication of two n-bit integers (M(n)=O(n log(n) log(log(n))) using FFT-based algorithms which are usually needed when computing digits of \pi and such an algorithm is implemented in GMP). Note that even though the mathematics behind the algorithms might not be trivial the algorithms themselves are usually a few lines of pseudo-code and their implementation is usually very straightforward (if you chose not to write your own multiprecision arithmetic :-) ).  The Monte Carlo method as mentioned applies some great concepts but it is clearly not the fastest --not by a long shot not by any reasonable usefulness. Also it all depends on what kind of accuracy you are looking for. The fastest pi I know of is the digits hard coded. Looking at Pi and Pi[PDF] there are a lot of formulas. Here is a method that converges quickly (~14digits per iteration). The current fastest application PiFast uses this formula with the FFT. I'll just write the formula since the code is straight forward. This formula was almost found by Ramanujan and discovered by Chudnovsky. It is actually how he calculated several billion digits of the number --so it isn't a method to disregard. The formula will overflow quickly since we are dividing factorials it would be advantageous then to delay such calculating to remove terms. where Below is the Brent–Salamin algorithm. Wikipedia mentions that when a and b are 'close enough' then (a+b)^2/4t will be an approximation of pi. I'm not sure what 'close enough' means but from my tests one iteration got 2digits two got 7 and three had 15 of course this is with doubles so it might have error based on it's representation and the 'true' calculation could be more accurate. let pi_2 iters = let rec loop_ a b t p i = if i = 0 then abtp else let a_n = (a +. b) /. 2.0 and b_n = sqrt (a*.b) and p_n = 2.0 *. p in let t_n = t -. (p *. (a -. a_n) *. (a -. a_n)) in loop_ a_n b_n t_n p_n (i - 1) in let abtp = loop_ (1.0) (1.0 /. (sqrt 2.0)) (1.0/.4.0) (1.0) iters in (a +. b) *. (a +. b) /. (4.0 *. t) Lastly how about some pi golf (800 digits)? 160 characters! int a=10000bc=2800def[2801]g;main(){for(;b-c;)f[b++]=a/5;for(;d=0g=c*2;c-=14printf(""%.4d""e+d/a)e=d%a)for(b=c;d+=f[b]*af[b]=d%--gd/=g----b;d*=b);} The image was Chudnovsky`s Formula; mentioned in the link just north of the image. I replaced those long variables to k1...6. in my experience 'close enough' usually means there's a taylor series approximation involved. @nlucaroni - I don't recall what image you originally linked above can you find it or something similar to replace it with and update your post? The stack overflow image upload interface would be a good longer term solution... Oh no! All those edits made your post community-moded. :-( I know! all I wanted was a nice complete answer. I just thought of things late and I can't spell right! Woot! Very nice. :-) Assuming you are trying to implement the first one yourself wouldn't sqr(k3) be a problem? I'm pretty sure it would end up an irrational number that you will have to estimate (IIRC all roots that are not whole numbers are irrational). Everything else looks pretty straight-forward if you are using infinite precision arithmetic but that square root is a deal breaker. The second one includes a sqrt as well.  I really like this program which approximates pi by looking at its own area :-) IOCCC 1998 : westley.c If you replace _ with -F<00||--F-OO-- it should be easier to follow :-) Ow. That hurts my brain. or if you replace _ with ""if (previous character is '-') { OO--; } F--;"" I knew the guy who wrote it. Very nice guy quiet not the sort of person you'd expect to come up with lots of weird C like that. Gah! That's utterly insane. it prints 0.25 here -.- +1 litb it prints 0.25 for me too This program was great in 1998 but was broken because modern preprocessors are more liberal with inserting spaces around macro expansions to prevent things like this from working. It is a relic unfortunately. Pass `--traditional-cpp` to *cpp* to get the intended behavior.  With doubles: 4.0 * (4.0 * Math.Atan(0.2) - Math.Atan(1.0 / 239.0)) This will be accurate up to 14 decimal places enough to fill a double (the inaccuracy is probably because the rest of the decimals in the arc tangents are truncated). Also Seth it's 3.141592653589793238463 not 64.  The following answers precisely how to do this in the fastest possible way -- with the least computing effort. Even if you don't like the answer you have to admit that it is indeed the fastest way to get the value of PI. The FASTEST way to get the value of Pi is: chose your favorite programming language load it's Math library and find that Pi is already defined there!! ready to use it.. in case you don't have a Math library at hand.. the SECOND FASTEST way (more universal solution) is: look up Pi on the Internet e.g. here: http://www.eveandersson.com/pi/digits/1000000 (1 million digits .. what's your floating point precision? ) or here: http://3.141592653589793238462643383279502884197169399375105820974944592.com/ or here: http://en.wikipedia.org/wiki/Pi It's really fast to find the digits you need for whatever precision arithmetic you would like to use and by defining a constant you can make sure that you don't waste precious CPU time. Not only is this a partly humorous answer but in reality if anybody would go ahead and compute the value of Pi in a real application .. that would be a pretty big waste of CPU time wouldn't it? At least I don't see a real application for trying to re-compute this. Dear Moderator: please note that the OP asked: ""Fastest Way to get the value of PI"" Funny! and so true!  Use the Machin-like formula 176 * arctan (1/57) + 28 * arctan (1/239) - 48 * arctan (1/682) + 96 * arctan(1/12943) [; \left( 176 \arctan \frac{1}{57} + 28 \arctan \frac{1}{239} - 48 \arctan \frac{1}{682} + 96 \arctan \frac{1}{12943}\right) ;] for you TeX the World people. Implemented in Scheme for instance: (+ (- (+ (* 176 (atan (/ 1 57))) (* 28 (atan (/ 1 239)))) (* 48 (atan (/ 1 682)))) (* 96 (atan (/ 1 12943))))  The BBP formula allows you to compute the nth digit - in base 2 (or 16) - without having to even bother with the previous n-1 digits first :)  Brent's method posted above by Chris is very good; Brent generally is a giant in the field of arbitrary-precision arithmetic. If all you want is the Nth digit the famous BBP formula is useful in hex The Brent method wasn't posted by me; it was posted by Andrea and I just happened to be the last person who edited the post. :-) But I agree that post deserves an upvote.  This version (in Delphi) is nothing special but it is at least faster than the version Nick Hodge posted on his blog :). On my machine it takes about 16 seconds to do a billion iterations giving a value of 3.1415926525879 (the accurate part is in bold). program calcpi; {$APPTYPE CONSOLE} uses SysUtils; var start finish: TDateTime; function CalculatePi(iterations: integer): double; var numerator denominator i: integer; sum: double; begin { PI may be approximated with this formula: 4 * (1 - 1/3 + 1/5 - 1/7 + 1/9 - 1/11 .......) //} numerator := 1; denominator := 1; sum := 0; for i := 1 to iterations do begin sum := sum + (numerator/denominator); denominator := denominator + 2; numerator := -numerator; end; Result := 4 * sum; end; begin try start := Now; WriteLn(FloatToStr(CalculatePi(StrToInt(ParamStr(1))))); finish := Now; WriteLn('Seconds:' + FormatDateTime('hh:mm:ss.zz'finish-start)); except on E:Exception do Writeln(E.Classname ': ' E.Message); end; end.  If this article is true then the algorithm that Bellard has created could be one of the speediest available. He has created pi to 2.7 TRILLION digits using a DESKTOP PC! ...and he has published his work here Good work Bellard You are a pioneer! http://www.theregister.co.uk/2010/01/06/very_long_pi/ Bellard pioneered in many many ways...first there was LZEXE quite possibly the first executable compressor (think what UPX does then flip back in time to the '80s) and of course now both QEMU and FFMPEG are widely used. Oh and his IOCCC entry.... :-P  If by fastest you mean fastest to type in the code here's the golfscript solution: ;''6666-2%{2+.2/@*\/10.3??2*+}*`1000<~\;  This is a ""classic"" method very easy to implement. This implementation in python (not so fast language) does it: from math import pi from time import time precision = 10**6 # higher value -> higher precision # lower value -> higher speed t = time() calc = 0 for k in xrange(0 precision): calc += ((-1)**k) / (2*k+1.) calc *= 4. # this is just a little optimization t = time()-t print ""Calculated: %.40f"" % calc print ""Costant pi: %.40f"" % pi print ""Difference: %.40f"" % abs(calc-pi) print ""Time elapsed: %s"" % repr(t) You can find more information here. Anyway the fastest way to get a precise as-much-as-you-want value of pi in python is: from gmpy import pi print pi(3000) # the rule is the same as # the precision on the previous code here is the piece of source for the gmpy pi method I don't think the code is as much useful as the comment in this case: static char doc_pi[]=""\ pi(n): returns pi with n bits of precision in an mpf object\n\ ""; /* This function was originally from netlib package bmp by * Richard P. Brent. Paulo Cesar Pereira de Andrade converted * it to C and used it in his LISP interpreter. * * Original comments: * * sets mp pi = 3.14159... to the available precision. * uses the gauss-legendre algorithm. * this method requires time o(ln(t)m(t)) so it is slower * than mppi if m(t) = o(t**2) but would be faster for * large t if a faster multiplication algorithm were used * (see comments in mpmul). * for a description of the method see - multiple-precision * zero-finding and the complexity of elementary function * evaluation (by r. p. brent) in analytic computational * complexity (edited by j. f. traub) academic press 1976 151-176. * rounding options not implemented no guard digits used. */ static PyObject * Pygmpy_pi(PyObject *self PyObject *args) { PympfObject *pi; int precision; mpf_t r_i2 r_i3 r_i4; mpf_t ix; ONE_ARG(""pi"" ""i"" &precision); if(!(pi = Pympf_new(precision))) { return NULL; } mpf_set_si(pi->f 1); mpf_init(ix); mpf_set_ui(ix 1); mpf_init2(r_i2 precision); mpf_init2(r_i3 precision); mpf_set_d(r_i3 0.25); mpf_init2(r_i4 precision); mpf_set_d(r_i4 0.5); mpf_sqrt(r_i4 r_i4); for (;;) { mpf_set(r_i2 pi->f); mpf_add(pi->f pi->f r_i4); mpf_div_ui(pi->f pi->f 2); mpf_mul(r_i4 r_i2 r_i4); mpf_sub(r_i2 pi->f r_i2); mpf_mul(r_i2 r_i2 r_i2); mpf_mul(r_i2 r_i2 ix); mpf_sub(r_i3 r_i3 r_i2); mpf_sqrt(r_i4 r_i4); mpf_mul_ui(ix ix 2); /* Check for convergence */ if (!(mpf_cmp_si(r_i2 0) && mpf_get_prec(r_i2) >= (unsigned)precision)) { mpf_mul(pi->f pi->f r_i4); mpf_div(pi->f pi->f r_i3); break; } } mpf_clear(ix); mpf_clear(r_i2); mpf_clear(r_i3); mpf_clear(r_i4); return (PyObject*)pi; } EDIT: I had some problem with cut and paste and identation anyway you can find the source here.  Instead of defining pi as a constant I always use acos(-1). cos(-1) or acos(-1)? :-P That (the latter) is one of the test cases in my original code. It's among my preferred (along with atan2(0 -1) which really is the same as acos(-1) except that acos is usually implemented in terms of atan2) but some compilers optimise for 4 * atan(1)!  Calculate PI at compile-time with D. ( Copied from DSource.org ) /** Calculate pi at compile time * * Compile with dmd -c pi.d */ module calcpi; import meta.math; import meta.conv; /** real evaluateSeries!(real x real metafunction!(real y int n) term) * * Evaluate a power series at compile time. * * Given a metafunction of the form * real term!(real y int n) * which gives the nth term of a convergent series at the point y * (where the first term is n==1) and a real number x * this metafunction calculates the infinite sum at the point x * by adding terms until the sum doesn't change any more. */ template evaluateSeries(real x alias term int n=1 real sumsofar=0.0) { static if (n>1 && sumsofar == sumsofar + term!(x n+1)) { const real evaluateSeries = sumsofar; } else { const real evaluateSeries = evaluateSeries!(x term n+1 sumsofar + term!(x n)); } } /*** Calculate atan(x) at compile time. * * Uses the Maclaurin formula * atan(z) = z - z^3/3 + Z^5/5 - Z^7/7 + ... */ template atan(real z) { const real atan = evaluateSeries!(z atanTerm); } template atanTerm(real x int n) { const real atanTerm = (n & 1 ? 1 : -1) * pow!(x 2*n-1)/(2*n-1); } /// Machin's formula for pi /// pi/4 = 4 atan(1/5) - atan(1/239). pragma(msg ""PI = "" ~ fcvt!(4.0 * (4*atan!(1/5.0) - atan!(1/239.0))) ); Unfortunately tangents are arctangents are based on pi somewhat invalidating this calculation. I just copied it from DSource.org  Back in the old days with small word sizes and slow or non-existent floating-point operations we used to do stuff like this: /* Return approximation of n * PI; n is integer */ #define pi_times(n) (((n) * 22) / 7) For applications that don't require a lot of precision (video games for example) this is very fast and is accurate enough. For more accuracy use `355 / 113`. Very accurate for the size of numbers involved.  In the interests of completeness a C++ template version which in an optimised build will compute PI at compile time and will inline to a single value. #include <iostream> template<int I> struct sign { enum {value = (I % 2) == 0 ? 1 : -1}; }; template<int I int J> struct pi_calc { inline static double value () { return (pi_calc<I-1 J>::value () + pi_calc<I-1 J+1>::value ()) / 2.0; } }; template<int J> struct pi_calc<0 J> { inline static double value () { return (sign<J>::value * 4.0) / (2.0 * J + 1.0) + pi_calc<0 J-1>::value (); } }; template<> struct pi_calc<0 0> { inline static double value () { return 4.0; } }; template<int I> struct pi { inline static double value () { return pi_calc<I I>::value (); } }; int main () { std::cout.precision (12); const double pi_value = pi<10>::value (); std::cout << ""pi ~ "" << pi_value << std::endl; return 0; } Note for I > 10 optimised builds can be slow as can non-optimised runs. I think for 12 iterations there are around 80k calls to value() (without memoization). Thank you very much. The question needed one of these. Well that's accurate to 9dp's. Are you objecting to something or just making an observation? I run this and get ""pi ~ 3.14159265383""  Pick a better algorithm. This one is more work but converges fast.  Here's a general description of a technique for calculating pi that I learnt in high school. I only share this because I think it is simple enough that anyone can remember it indefinitely plus it teaches you the concept of ""Monte-Carlo"" methods -- which are statistical methods of arriving at answers that don't immediately appear to be deducible through random processes. Draw a square and inscribe a quadrant (one quarter of a semi-circle) inside that square (a quadrant with radius equal to the side of the square so it fills as much of the square as possible) Now throw a dart at the square and record where it lands -- that is choose a random point anywhere inside the square. Of course it landed inside the square but is it inside the semi-circle? Record this fact. Repeat this process many times -- and you will find there is a ratio of the number of points inside the semi-circle versus the total number thrown call this ratio x. Since the area of the square is r times r you can deduce that the area of the semi circle is x times r times r (that is x times r squared). Hence x times 4 will give you pi. This is not a quick method to use. But it's a nice example of a Monte Carlo method. And if you look around you may find that many problems otherwise outside your computational skills can be solved by such methods. This is the method we used to calculate Pi in a java project in school. Just used a randomizer to come up with the xy coordinates and the more 'darts' we threw the closer to Pi we came.  Pi is exactly 3! [Prof. Frink (Simpsons)] Joke but here's one in C# (.NET-Framework required). using System; using System.Text; class Program { static void Main(string[] args) { int Digits = 100; BigNumber x = new BigNumber(Digits); BigNumber y = new BigNumber(Digits); x.ArcTan(16 5); y.ArcTan(4 239); x.Subtract(y); string pi = x.ToString(); Console.WriteLine(pi); } } public class BigNumber { private UInt32[] number; private int size; private int maxDigits; public BigNumber(int maxDigits) { this.maxDigits = maxDigits; this.size = (int)Math.Ceiling((float)maxDigits * 0.104) + 2; number = new UInt32[size]; } public BigNumber(int maxDigits UInt32 intPart) : this(maxDigits) { number[0] = intPart; for (int i = 1; i < size; i++) { number[i] = 0; } } private void VerifySameSize(BigNumber value) { if (Object.ReferenceEquals(this value)) throw new Exception(""BigNumbers cannot operate on themselves""); if (value.size != this.size) throw new Exception(""BigNumbers must have the same size""); } public void Add(BigNumber value) { VerifySameSize(value); int index = size - 1; while (index >= 0 && value.number[index] == 0) index--; UInt32 carry = 0; while (index >= 0) { UInt64 result = (UInt64)number[index] + value.number[index] + carry; number[index] = (UInt32)result; if (result >= 0x100000000U) carry = 1; else carry = 0; index--; } } public void Subtract(BigNumber value) { VerifySameSize(value); int index = size - 1; while (index >= 0 && value.number[index] == 0) index--; UInt32 borrow = 0; while (index >= 0) { UInt64 result = 0x100000000U + (UInt64)number[index] - value.number[index] - borrow; number[index] = (UInt32)result; if (result >= 0x100000000U) borrow = 0; else borrow = 1; index--; } } public void Multiply(UInt32 value) { int index = size - 1; while (index >= 0 && number[index] == 0) index--; UInt32 carry = 0; while (index >= 0) { UInt64 result = (UInt64)number[index] * value + carry; number[index] = (UInt32)result; carry = (UInt32)(result >> 32); index--; } } public void Divide(UInt32 value) { int index = 0; while (index < size && number[index] == 0) index++; UInt32 carry = 0; while (index < size) { UInt64 result = number[index] + ((UInt64)carry << 32); number[index] = (UInt32)(result / (UInt64)value); carry = (UInt32)(result % (UInt64)value); index++; } } public void Assign(BigNumber value) { VerifySameSize(value); for (int i = 0; i < size; i++) { number[i] = value.number[i]; } } public override string ToString() { BigNumber temp = new BigNumber(maxDigits); temp.Assign(this); StringBuilder sb = new StringBuilder(); sb.Append(temp.number[0]); sb.Append(System.Globalization.CultureInfo.CurrentCulture.NumberFormat.CurrencyDecimalSeparator); int digitCount = 0; while (digitCount < maxDigits) { temp.number[0] = 0; temp.Multiply(100000); sb.AppendFormat(""{0:D5}"" temp.number[0]); digitCount += 5; } return sb.ToString(); } public bool IsZero() { foreach (UInt32 item in number) { if (item != 0) return false; } return true; } public void ArcTan(UInt32 multiplicand UInt32 reciprocal) { BigNumber X = new BigNumber(maxDigits multiplicand); X.Divide(reciprocal); reciprocal *= reciprocal; this.Assign(X); BigNumber term = new BigNumber(maxDigits); UInt32 divisor = 1; bool subtractTerm = true; while (true) { X.Divide(reciprocal); term.Assign(X); divisor += 2; term.Divide(divisor); if (term.IsZero()) break; if (subtractTerm) this.Subtract(term); else this.Add(term); subtractTerm = !subtractTerm; } } }",performance algorithm language-agnostic unix pi,0.8866620977676911,0.02543146485385304,0.002999160022693335,0.07553146367621533,0.009375813679547143
918,"How to learn Python: Good Example Code? I have been dabbling in Python for a couple months now read the online docs/tutorial and started playing with Django a bit as well... and I feel like I am starting to leave my formative toddler years and ready for some more serious code. Most everything I see is generally encapsulated in a single script or so large and unwieldy I don't know where to start. I would really like to see an excellent example of some proper Python code style organization etc. and ideally a WHY associated with some of those decisions. Any ideas for where I should look next? I would prefer a simple console app with at most just a few extra .py files (maybe one package for good measure)... Oh and one more thing: The reason this is coming up is I have quite a bit of experience in Java and .NET where it is generally preferred to have one class definition per source file. That rarely seems to be the case in the python world (at least in my very limited exposure) and it just doesn't smell right to me. So maybe this is more Enterprise Python that hobbyist python or something. So 16 answers and not a single one answered the original question. One can only assume that python has a lot of good resources but almost no one follows them. I'm also having a hard time finding a well organized small package to look at. Try Dive into Python which is available to read online for free. There seems to be a problem with this link: *""The requested resource / is no longer available on this server and there is no forwarding address. Please remove all references to this resource.*"" @Alenanno I've updated the link to point to a mirror. Thanks! :) I noticed it so also seeing the message I thought it would have been better to notify here. @vsync The site looks to be back up. The link is dead :(  Well I breezed through the PDF and a few of the sample source files - and like most Python code I encountered almost all the class definitions and program execution were all lumped into one single .py script. Not quite what I was looking for. I am completely willing to accept that this is the way most (all?) python code is commonly written and in my own experience have enjoyed either insanely packed scripts with everything including the kitchen sink or countless import statements at the top of the file to locally scope class/methods. It just seems like I am going about this wrong or perhaps Python isn't really designed with the large app in mind and there will be pain.  PEP-8 is the style guide for Python and gives a good guidance on the style and code structure once you know the Python language. To answer your statement about things being in one big file I do think that Python tends to do it a little more that way particularly for libraries because it's a relatively dense language and the whitespacing makes a large file not feel quite so daunting. Most Python apps I've worked with (and larger libraries like Django SQLAlchemy etc.) do break their code up into multiple files and smaller modules. While it's not quite as segmented as Java with its one class per file there is still a very modular feel to the code. In the end you should do whatever you feel comfortable with.  If you want to learn Python fast without installing anything on your computer at all or setting up an IDE just go to Learn Python and start the tutorial. It's a community website I've created and hopefully more and more tutorials will be available for everyone. Very cool web site - thanks for the heads up!  Some more advanced features (decorators generators etc.) once you are beyond the basics common to every language (loops objects etc.) are also covered in the Google talk Advanced Python or Understanding Python  I've been really impressed with the Python Essential Reference so far. It's one of those great reference books that allows an experienced programmer to quickly learn a new language. The first 120 odd pages are a (fairly) quick rundown of the syntax of the language. The rest of the book is just a reference of the Python Standard Library.  I never really learn to use a language until I try to use it properly for something. It is all well and good going through thesetutorials but the examples are always written to demonstrate a function of the language when in reality you really want to try to make the language solve your problem. In essence the best way is to find a problem you need solving and dive right in. Good luck  i suggest you to start with: The Python Challenge because it really shows you the strengths of Python. So you can get a clue that every Language got its own strengths and weaknesses.  It seems that what you are looking for is a something mildly challenging and somewhat useful to do with python? Try some of these things: Level 1: Have you tried putting a GUI on top of a simple console program? There are several packages that allow you to do this with minimal effort. Metaslash Covers most all of them. Level 2: Google Code Jam. The Competition itself is long underway but you can still try the qualification round questions. Try doing them in python. They can be done with any amount of complexity you want one two three files. Its a good way to learn to build algorithms in python. Level 3: Write your own extension for something in python. I would try maybe a small scaling plugin for GIMP. Documentation can be found here. This will be pretty challenging and provide you good experience to contributing to open source! thanks for google code jam.  Read the Python Standard Library! Many modules are written in pure Python and they are readily available on your machine if you have Python installed. Sure some of the modules show their age and haven't been updated much along the way so they might not utilize newer features of the language. But at least the library is written in a very Pythonic style. After scrolling...and scrolling...finally somebody recommended the source!  Generators are one of the coolest new additions to Python. In order to learn more about them and to see practical code examples of advanced generator usage take a look at this presentation on Generator Tricks for System Programmers. It was an eye-opener for me.  I have found The Python Challenge a great resource for learning python. Its an online game in which you go solving riddles using python programming and you get tons of source code examples but the condition to see the code examples is that you solve each riddle first. The code examples are really other player's solutions by the way. I find that I learn a lot more from the code examples given in the solutions when I already tried to solve myself.  Peter Norvig presents two short and beautiful useful code samples written in Python: Solving Every Sudoku Puzzle How to Write a Spelling Corrector He also recently wrote a book chapter with python code: Natural Language Corpus Data: Beautiful Data @hughdbrown is it a coincidence that both articles were written by the head of R&D at a [quaint little internet company](http://www.google.com)? Peter Norvig had an awesome resume long before he landed at Google. Not sure what you think the coincidence is -- that I like them *and* Peter Norvig is the head of R&D (as in I like them because of his position)? Wow! These are two of my favorite online articles and have been for years and you've picked exactly these two out of all the possibilities!  Python is such a simple language (once you learn the basics) that generally you don't need to read reams of code to work out how to do things. I second Brian's suggestion of Dive into Python (and upmodded it to represent my support) to learn the syntax and the pythonic way of doing things and would like to add a link to The Python Library Reference to learn what's available under python. Keeping things simple works best in the early days and large open source projects rarely serve as simple examples and much simpler stuff isn't as easy to find. Hang around long enough and I'm sure Stack Overflow will be the resource you want :)  You're right that there's no substitute for looking at code but I've found Code Like a Pythonista to be useful for those kinds of questions. It has small sections on module structure and package organization at the end. Edit: The Python Tutorial section on modules is also worth reading.  There are plenty of large Python apps you mention one in your question: Django. Why not browse the Django source to see one way of laying out a larger project. For laying out classes and modules one pattern I've seen repeated in bigger Python projects is to put several related classes into a single file then several related files into a single directory. Put an __init__.py file in that directory and import all of the code in that directory into a single namespace. An example of this in the Django source: /trunk/django/forms/ Then if you wanted to use say the CharField class in another file you could just use import forms field = forms.CharField()",python,0.07211004573009415,0.14249731976179636,0.023872684145748474,0.7320506077955079,0.029469342566852935
1936,"How to RedirectToAction in ASP.NET MVC without losing request data Using ASP.NET MVC there are situations (such as form submission) that may require a RedirectToAction. One such situation is when you encounter validation errors after a form submission and need to redirect back to the form but would like the URL to reflect the URL of the form not the action page it submits to. As I require the form to contain the originally POSTed data for user convenience as well as validation purposes how can I pass the data through the RedirectToAction()? If I use the viewData parameter my POST parameters will be changed to GET parameters. Here's a question that is similar (on the same topic) but different than this one. Anyway it still may be of interest to those interested in this question: [http://stackoverflow.com/questions/129335/how-do-you-redirecttoaction-using-post-intead-of-get](http://stackoverflow.com/questions/129335/how-do-you-redirecttoaction-using-post-intead-of-get) If you want to pass data to the redirected action the method you could use is: return RedirectToAction(""ModelUpdated"" new {id = 1}); // The definition of the action method like public ActionResult ModelUpdated(int id);  Keep in mind that TempData stores the form collection in session. If you don't like that behavior you can implement the new ITempDataProvider interface and use some other mechanism for storing temp data. I wouldn't do that unless you know for a fact (via measurement and profiling) that the use of Session state is hurting you.  There is another way which avoids tempdata. The pattern I like involves creating 1 action for both the original render and re-render of the invalid form. It goes something like this: var form = new FooForm(); if (request.UrlReferrer == request.Url) { // Fill form with previous request's data } if (Request.IsPost()) { if (!form.IsValid) { ViewData[""ValidationErrors""] = ... } else { // update model model.something = foo.something; // handoff to post update action return RedirectToAction(""ModelUpdated"" ... etc); } } // By default render 1 view until form is a valid post ViewData[""Form""] = form; return View(); That's the pattern more or less. A little pseudoy. With this you can create 1 view to handle rendering the form re-displaying the values (since the form will be filled with previous values) and showing error messages. When the posting to this action if its valid it transfers control over to another action. I'm trying to make this pattern easy in the .net validation framework as we build out support for MVC. I use TempData as well the problem as I understand it with your solution Deeno is that if the user was to refresh the page after posting invalid data they would receive a ""Would you like to resubmit the form data"" confirmation. Using the TempData solution as MattMitchell says eliminates this problem. Cool. Seems someone noticed this basic idea with preview 5 too.  Take alook at MVCContrib you can do this:  using MvcContrib.Filters; [ModelStateToTempData] public class MyController : Controller { // ... }  The solution is to use the TempData property to store the desired Request components. For instance: public ActionResult Send() {  TempData[""form""] = Request.Form;  return this.RedirectToAction(a => a.Form()); } Then in your ""Form"" action you can go: public ActionResult Form() {  /* Declare viewData etc. */  if (TempData[""form""] != null)  {  /* Cast TempData[""form""] to  System.Collections.Specialized.NameValueCollection  and use it */  }  return View(""Form"" viewData);  } RedirectToAction(a => a.Form()) doesn't compile for me using MVCv2 are you using MVCv3? This was pre-MVC1 (2008). The lambdas were removed at some stage I believe (there is a codeplex project MVCContrib that reintroduces them although you may as well use their helper attribute at that stage: http://stackoverflow.com/questions/1936/how-to-redirecttoaction-in-asp-net-mvc-without-losing-request-data/718653#718653). In mvc2 you should be able to go RedirectToAction(""Form"") i.e. name as a string although I loosely recall that MVC 2 or 3 introduced a helper attribute similar to that in MVCContrib if you are keen to go searching.",c# asp.net-mvc,6.516039356359426E-4,0.8767251067920102,0.06135272808880989,0.059107088284287845,0.0021634728992561475
4,"When setting a form's opacity should I use a decimal or double? I want to use a track-bar to change a form's opacity. This is my code: decimal trans = trackBar1.Value / 5000; this.Opacity = trans; When I try to build it I get this error: Cannot implicitly convert type 'decimal' to 'double'. I tried making trans a double but then the control doesn't work. This code has worked fine for me in VB.NET in the past. Also Decimal can't represent as wide a value as a Double. Decimal can only go up to +/-7.9228162514264337593543950335E+28; whereas a Double can go up to +/-1.79769313486232E+308 It sounds like this.Opacity is a double value and the compiler doesn't like you trying to cram a decimal value into it.  this.Opacity = trackBar1.Value / 5000d;  Assuming you are using WinForms Form.Opacity is of type double so you should use: double trans = trackBar1.Value / 5000.0; this.Opacity = trans; Unless you need the value elsewhere it's simpler to write: this.Opacity = trackBar1.Value / 5000.0;  Your code worked fine in VB.NET because it implicitly does any casts while C# has both implicit and explicit ones. In C# the conversion from decimal to double is explicit as you lose accuracy. For instance 1.1 can't be accurately expressed as a double but can as a decimal (see ""Floating point numbers - more inaccurate than you think"" for the reason why). In VB the conversion was added for you by the compiler: decimal trans = trackBar1.Value / 5000m; this.Opacity = (double) trans; That (double) has to be explicitly stated in C# but can be implied by VB's more 'forgiving' compiler.  Since Opacity is a double value I would just use a double from the outset and not cast at all but be sure to use a double when dividing so you don't loose any precision Opacity = trackBar1.Value / 5000.0;  The best solution is: this.Opacity = decimal.ToDouble(trackBar1.Value/5000);  An explicit cast to double isn't necessary. double trans = (double)trackBar1.Value / 5000.0; Identifying the constant as 5000.0 (or as 5000d) is sufficient: double trans = trackBar1.Value / 5000.0; double trans = trackBar1.Value / 5000d;  A more generic answer for the generic question ""Decimal vs Double?"": Decimal for monetary calculations to preserve the precision Double for scientific calculations that do not get affected by small differences. Since Double is a type which is native to the CPU (internal representation is stored in base 2) calculations made with Double perform better then Decimal (which is represented in base 10 internally).  In my opinion it is desirable to be as explicit as possible. This adds clarity to the code and aids your fellow programmers who may eventually read it. In addition to (or instead of) appending a .0 to the number you can use decimal.ToDouble(). Here are some examples: // Example 1 double transperancy = trackBar1.Value/5000; this.Opacity = decimal.ToDouble(transperancy); // Example 2 - with inline temp this.Opacity = decimal.ToDouble(trackBar1.Value/5000);  You should use 5000.0 instead of 5000.  You have two problems. First Opacity requires a double not a decimal value. The compiler is telling you that while there is a conversion between decimal and double it is an explicit conversion that you need to specify in order for it to work. The second is that TrackBar.Value is an integer value and dividing an int by an int results in an int no matter what type of variable you assign it to. In this case there is an implicit cast from int to decimal or double - because there is no loss of precision when you do the cast - so the compiler doesn't complain but the value you get is always 0 presumably since trackBar.Value is always less than 5000. The solution is to change your code to use double (the native type for Opacity) and do floating point arithmetic by explicitly making the constant a double - which will have the effect of promoting the arithmetic - or casting trackBar.Value to double which will do the same thing - or both. Oh and you don't need the intermediate variable unless it used elsewhere. My guess is the compiler would optimize it away anyway.  trackBar.Opacity = (double)trackBar.Value / 5000.0;  Why are you dividing by 5000? Just set the TrackBar's Minimum and Maximum values between 0 and 100 and then divide the Value by 100 for the Opacity percentage. Minimum 20 example below prevents the form from becoming completely invisible: private void Form1_Load(object sender System.EventArgs e) { TrackBar1.Minimum = 20; TrackBar1.Maximum = 100; TrackBar1.LargeChange = 10; TrackBar1.SmallChange = 1; TrackBar1.TickFrequency = 5; } private void TrackBar1_Scroll(object sender System.EventArgs e) { this.Opacity = TrackBar1.Value / 100; }  The Opacity property is of double type: double trans = trackBar1.Value / 5000.0; this.Opacity = trans; or simply: this.Opacity = trackBar1.Value / 5000.0; or: this.Opacity = trackBar1.Value / 5000d; Notice that I am using 5000.0 (or 5000d) to force a double division because trackBar1.Value is an integer and it would perform an integer division and the result would be an integer.",c# winforms type-conversion opacity,0.11683115326230033,0.8761946743511091,0.0013769252071513156,0.0034244651971986986,0.002172781982240626
3374,OpenVPN Config file to prevent timeout I am using OpenVPN with an *.opvn config file under Windows XP to connect to a VPN server. Everything works fine I connect successfully but after 2 minutes or so of inactivity my connection dies. Does anyone know if this is a: OpenVPN config problem Windows problem Client network problem Server VPN config problem Server network problem I have included my *.opvn config file below: #OpenVPN Server conf tls-client client dev tun proto udp tun-mtu 1400 remote XXX.XXX.XXX.X XXXX pkcs12 XXXX.p12 cipher BF-CBC verb 3 ns-cert-type server I have tried the adding following options to my *.ovpn file with no success: ping 30 keepalive 10 60 If possible have you tried using TCP as a protocol? I've had all sorts of connecting as well as staying connected when using UDP. My config is below and not much different than yours but I can stay connected for hours at a time. I'm using IPCop 1.4.16 with the ZERINA OpenVPN server: #OpenVPN Client conf tls-client client dev tun proto tcp-client tun-mtu 1400 remote XXX.XXX.XXX.XXX 1194 pkcs12 XXXX.p12 cipher BF-CBC comp-lzo verb 3 ns-cert-type server I wasn't able to switch to tcp as the server wasn't configured for this. I ended up solving my problem by having my terminal send a null packet every 20 seconds which kept the connection alive.,windows vpn openvpn,0.001329381989825764,0.005520732835779435,0.9817794814231076,0.006956553940910897,0.0044138498103764474
3004,"Using ASP.NET Dynamic Data / LINQ to SQL how do you have two table fields have a relationship to the same foreign key? I am using ASP.NET Dynamic Data for a project and I have a table that has two seperate fields that link to the same foreign key in a different table. This relationship works fine in SQL Server. However in the LINQ to SQL model in the ASP.NET Dynamic Data model only the first field's relationship is reflected. If I attempt to add the second relationship manually it complains that it ""Cannot create an association ""ForeignTable_BaseTable"". The same property is listed more than once: ""Id""."" This MSDN article gives such helpful advice as: Examine the message and note the property specified in the message. Click OK to dismiss the message box. Inspect the Association Properties and remove the duplicate entries. Click OK. The solution is to delete and re-add BOTH tables to the LINQ to SQL diagram not just the one you have added the second field and keys to. Alternatively it appears you can make two associations using the LINQ to SQL interface - just don't try and bundle them into a single association.",asp.net dynamic-data,0.0019905538696535522,0.5710604639786132,0.004188281805687756,0.02350464772438907,0.3992560526216565
1535,"Reducing duplicate error handling code in C#? I've never been completely happy with the way exception handling works there's a lot exceptions and try/catch brings to the table (stack unwinding etc.) but it seems to break a lot of the OO model in the process. Anyway here's the problem: Let's say you have some class which wraps or includes networked file IO operations (e.g. reading and writing to some file at some particular UNC path somewhere). For various reasons you don't want those IO operations to fail so if you detect that they fail you retry them and you keep retrying them until they succeed or you reach a timeout. I already have a convenient RetryTimer class which I can instantiate and use to sleep the current thread between retries and determine when the timeout period has elapsed etc. The problem is that you have a bunch of IO operations in several methods of this class and you need to wrap each of them in try-catch / retry logic. Here's an example code snippet: RetryTimer fileIORetryTimer = new RetryTimer(TimeSpan.FromHours(10)); bool success = false; while (!success) {  try  {  // do some file IO which may succeed or fail  success = true;  }  catch (IOException e)  {  if (fileIORetryTimer.HasExceededRetryTimeout)  {  throw e;  }  fileIORetryTimer.SleepUntilNextRetry();  } } So how do you avoid duplicating most of this code for every file IO operation throughout the class? My solution was to use anonymous delegate blocks and a single method in the class which executed the delegate block passed to it. This allowed me to do things like this in other methods: this.RetryFileIO( delegate()  {  // some code block  } ); I like this somewhat but it leaves a lot to be desired. I'd like to hear how other people would solve this sort of problem. Just a general FYI: It is [almost *always* better](http://philosopherdeveloper.wordpress.com/2010/05/05/re-throwing-caught-exceptions/) to simply `throw;` instead of `throw e;` This looks like an excellent opportunity to have a look at Aspect Oriented Programming. Here is a good article on AOP in .NET. The general idea is that you'd extract the cross-functional concern (i.e. Retry for x hours) into a separate class and then you'd annotate any methods that need to modify their behaviour in that way. Here's how it might look (with a nice extension method on Int32) [RetryFor( 10.Hours() )] public void DeleteArchive() { //.. code to just delete the archive }  Just wondering what do you feel your method leaves to be desired? You could replace the anonymous delegate with a.. named? delegate something like  public delegate void IoOperation(params string[] parameters);  public void FileDeleteOperation(params string[] fileName)  {  File.Delete(fileName[0]);  }  public void FileCopyOperation(params string[] fileNames)  {  File.Copy(fileNames[0] fileNames[1]);  }  public void RetryFileIO(IoOperation operation params string[] parameters)  {  RetryTimer fileIORetryTimer = new RetryTimer(TimeSpan.FromHours(10));  bool success = false;  while (!success)  {  try  {  operation(parameters);  success = true;  }  catch (IOException e)  {  if (fileIORetryTimer.HasExceededRetryTimeout)  {  throw;  }  fileIORetryTimer.SleepUntilNextRetry();  }  }  }  public void Foo()  {  this.RetryFileIO(FileDeleteOperation ""L:\file.to.delete"" );  this.RetryFileIO(FileCopyOperation ""L:\file.to.copy.source"" ""L:\file.to.copy.destination"" );  }  You could also use a more OO approach: Create a base class that does the error handling and calls an abstract method to perform the concrete work. (Template Method pattern) Create concrete classes for each operation. This has the advantage of naming each type of operation you perform and gives you a Command pattern - operations have been represented as objects.  Here's what I did recently. It has probably been done elsewhere better but it seems pretty clean and reusable. I have a utility method that looks like this:  public delegate void WorkMethod(); static public void DoAndRetry(WorkMethod wm int maxRetries) { int curRetries = 0; do { try { wm.Invoke(); return; } catch (Exception e) { curRetries++; if (curRetries > maxRetries) { throw new Exception(""Maximum retries reached"" e); } } } while (true); } Then in my application I use c#'s Lamda expression syntax to keep things tidy: Utility.DoAndRetry( () => ie.GoTo(url) 5); This calls my method and retries up to 5 times. At the fifth attempt the original exception is rethrown inside of a retry exception. But why the custom `WorkMethod` delegate instead of `Action`?",c# exception error-handling,7.027917169927406E-4,0.6590971447396929,0.024583607587649728,0.1284439994217919,0.18717245653387274
3234,"Displaying ad content from Respose.WriteFile()/ Response.ContentType How would one display any add content from a ""dynamic"" aspx page? Currently I am working on using the System.Web.HttpResponse ""Page.Response"" to write a file that is stored on a web server to a web request. This would allow people to hit a url to the type http://www.foo.com?Image=test.jpg and have the image display in their browser. So as you may know this revolves around the use of Response.ContentType. By using Response.ContentType = ""application/octet-stream""; I am able to display images of type gif/jpeg/png (all i have tested so far) bit trying to display .swf or .ico files gives me a nice little error. using Response.ContentType = ""application/x-shockwave-flash""; I can get flash files to play but then the images are messed. So how do i easily choose the contenttype? This is ugly but the best way is to look at the file and set the content type as appropriate: switch ( fileExtension ) { case ""pdf"": Response.ContentType = ""application/pdf""; break; case ""swf"": Response.ContentType = ""application/x-shockwave-flash""; break; case ""gif"": Response.ContentType = ""image/gif""; break; case ""jpeg"": Response.ContentType = ""image/jpg""; break; case ""jpg"": Response.ContentType = ""image/jpg""; break; case ""png"": Response.ContentType = ""image/png""; break; case ""mp4"": Response.ContentType = ""video/mp4""; break; case ""mpeg"": Response.ContentType = ""video/mpeg""; break; case ""mov"": Response.ContentType = ""video/quicktime""; break; case ""wmv"": case ""avi"": Response.ContentType = ""video/x-ms-wmv""; break; //and so on default: Response.ContentType = ""application/octet-stream""; break; }  This is part of a solution I use on a local intranet. Some of the variables you will have to collect yourself as I pull them from a database but you may pull them from somewhere else. The only extra but I've got in there is a function called getMimeType which connects to the database and pulls back the correct mine type based on file extension. This defaults to application/octet-stream if none is found. // Clear the response buffer incase there is anything already in it. Response.Clear(); Response.Buffer = true; // Read the original file from disk FileStream myFileStream = new FileStream(sPath FileMode.Open); long FileSize = myFileStream.Length; byte[] Buffer = new byte[(int)FileSize]; myFileStream.Read(Buffer 0 (int)FileSize); myFileStream.Close(); // Tell the browse stuff about the file Response.AddHeader(""Content-Length"" FileSize.ToString()); Response.AddHeader(""Content-Disposition"" ""inline; filename="" + sFilename.Replace("" """"_"")); Response.ContentType = getMimeType(sExtention oConnection); // Send the data to the browser Response.BinaryWrite(Buffer); Response.End();  Yup Keith ugly but true. I ended up placing the MIME types that we would use into a database and then pull them out when I was publishing a file. I still can't believe that there is no autoritive list of types out there or that there is no mention of what is available in MSDN. I found this site that provided some help.",c#,7.44050593679382E-4,0.012874455223406953,0.0064578008179221125,0.018570339313191502,0.9613533540518
4046,"Example of a build.xml for an EAR that deploys in WebSphere 6 I'm trying to convince my providers to use ANT instead of Rational Application Development so anyone can recompile recheck redeploy the solution anyplace anytime anyhow. :P I started a build.xml for a project that generates a JAR file but stopped there and I need real examples to compare notes. My good friends! I don't have anyone close to chat about this! This is my build.xml so far. (*) I edited my question based in the suggestion of to use pastebin.ca My Environment: Fedora 8; WAS 6.1 (as installed with Rational Application Developer 7) The documentation is very poor in this area and there is a dearth of practical examples. Using the WebSphere Application Server (WAS) Ant tasks To run as described here you need to run them from your server profile bin directory using the ws_ant.sh or ws_ant.bat commands. <?xml version=""1.0""?> <project name=""project"" default=""wasListApps"" basedir="".""> <description> Script for listing installed apps. Example run from: /opt/IBM/SDP70/runtimes/base_v61/profiles/AppSrv01/bin </description> <property name=""was_home"" value=""/opt/IBM/SDP70/runtimes/base_v61/""> </property> <path id=""was.runtime""> <fileset dir=""${was_home}/lib""> <include name=""**/*.jar"" /> </fileset> <fileset dir=""${was_home}/plugins""> <include name=""**/*.jar"" /> </fileset> </path> <property name=""was_cp"" value=""${toString:was.runtime}""></property> <property environment=""env""></property> <target name=""wasListApps""> <taskdef name=""wsListApp"" classname=""com.ibm.websphere.ant.tasks.ListApplications"" classpath=""${was_cp}""> </taskdef> <wsListApp wasHome=""${was_home}"" /> </target> </project> Command: ./ws_ant.sh -buildfile ~/IBM/rationalsdp7.0/workspace/mywebappDeploy/applist.xml A Deployment Script <?xml version=""1.0""?> <project name=""project"" default=""default"" basedir="".""> <description> Build/Deploy an EAR to WebSphere Application Server 6.1 </description> <property name=""was_home"" value=""/opt/IBM/SDP70/runtimes/base_v61/"" /> <path id=""was.runtime""> <fileset dir=""${was_home}/lib""> <include name=""**/*.jar"" /> </fileset> <fileset dir=""${was_home}/plugins""> <include name=""**/*.jar"" /> </fileset> </path> <property name=""was_cp"" value=""${toString:was.runtime}"" /> <property environment=""env"" /> <property name=""ear"" value=""${env.HOME}/IBM/rationalsdp7.0/workspace/mywebappDeploy/mywebappEAR.ear"" /> <target name=""default"" depends=""deployEar""> </target> <target name=""generateWar"" depends=""compileWarClasses""> <jar destfile=""mywebapp.war""> <fileset dir=""../mywebapp/WebContent""> </fileset> </jar> </target> <target name=""compileWarClasses""> <echo message=""was_cp=${was_cp}"" /> <javac srcdir=""../mywebapp/src"" destdir=""../mywebapp/WebContent/WEB-INF/classes"" classpath=""${was_cp}""> </javac> </target> <target name=""generateEar"" depends=""generateWar""> <mkdir dir=""./earbin/META-INF""/> <move file=""mywebapp.war"" todir=""./earbin"" /> <copy file=""../mywebappEAR/META-INF/application.xml"" todir=""./earbin/META-INF"" /> <jar destfile=""${ear}""> <fileset dir=""./earbin"" /> </jar> </target> <!-- http://publib.boulder.ibm.com/infocenter/wasinfo/v6r1/index.jsp?topic=/com.ibm.websphere.javadoc.doc/public_html/api/com/ibm/websphere/ant/tasks/package-summary.html --> <target name=""deployEar"" depends=""generateEar""> <taskdef name=""wsInstallApp"" classname=""com.ibm.websphere.ant.tasks.InstallApplication"" classpath=""${was_cp}""/> <wsInstallApp ear=""${ear}"" failonerror=""true"" debug=""true"" taskname="""" washome=""${was_home}"" /> </target> </project> Notes: You can only run this once! You cannot install if the app name is in use - see other tasks like wsUninstallApp It probably won't start the app either You need to run this on the server and the script is quite fragile Alternatives I would probably use Java Management Extensions (JMX). You could write a file-upload servlet that accepts an EAR and uses the deployment MBeans to deploy the EAR on the server. You would just POST the file over HTTP. This would avoid any WAS API dependencies on your dev/build machine and could be independent of any one project. The WAS deployment JMX MBeans can't be used by a remote JSE app? (assuming libs on the classpath of course) @bluefoot I don't know/remember - I wrote this ~5 years ago - but I assume you'd need to get the bytes over to the server to provide a local path to the [AppManagement](http://pic.dhe.ibm.com/infocenter/wasinfo/v6r1/topic/com.ibm.websphere.javadoc.doc/public_html/mbeandocs/AppManagement.html) MBean anyway. Extra info & samples here: http://illegalargumentexception.blogspot.com/2008/08/ant-automated-deployment-to-websphere.html  Here is some of the same functionality if you don't have the WAS ant tasks available or don't want to run was_ant.bat. This relies on wsadmin.bat existing in the path. <property name=""websphere.home.dir"" value=""${env.WS6_HOME}"" /> <property name=""was.server.name"" value=""server1"" /> <property name=""wsadmin.base.command"" value=""wsadmin.bat"" /> <property name=""ws.list.command"" value=""$AdminApp list"" /> <property name=""ws.install.command"" value=""$AdminApp install"" /> <property name=""ws.uninstall.command"" value=""$AdminApp uninstall"" /> <property name=""ws.save.command"" value=""$AdminConfig save"" /> <property name=""ws.setManager.command"" value=""set appManager [$AdminControl queryNames cell=${env.COMPUTERNAME}Node01Cellnode=${env.COMPUTERNAME}Node01type=ApplicationManagerprocess=${was.server.name}*]"" /> <property name=""ws.startapp.command"" value=""$AdminControl invoke $appManager startApplication"" /> <property name=""ws.stopapp.command"" value=""$AdminControl invoke $appManager stopApplication"" /> <property name=""ws.conn.type"" value=""SOAP"" /> <property name=""ws.host.name"" value=""localhost"" /> <property name=""ws.port.name"" value=""8880"" /> <property name=""ws.user.name"" value=""username"" /> <property name=""ws.password.name"" value=""password"" /> <property name=""app.deployed.name"" value=""${artifact.filename}"" /> <property name=""app.contextroot.name"" value=""/${artifact.filename}"" /> <target name=""websphere-list-applications""> <exec dir=""${websphere.home.dir}/bin"" executable=""${wsadmin.base.command}"" output=""waslist.txt"" logError=""true""> <arg line=""-conntype ${ws.conn.type}"" /> <arg line=""-host ${ws.host.name}"" /> <arg line=""-port ${ws.port.name}"" /> <arg line=""-username ${ws.user.name}"" /> <arg line=""-password ${ws.password.name}"" /> <arg line=""-c"" /> <arg value=""${ws.list.command}"" /> </exec> </target> <target name=""websphere-install-application"" depends=""websphere-uninstall-application""> <exec executable=""${websphere.home.dir}/bin/${wsadmin.base.command}"" logError=""true"" outputproperty=""websphere.install.output"" failonerror=""true""> <arg line=""-conntype ${ws.conn.type}"" /> <arg line=""-host ${ws.host.name}"" /> <arg line=""-port ${ws.port.name}"" /> <arg line=""-username ${ws.user.name}"" /> <arg line=""-password ${ws.password.name}"" /> <arg line=""-c"" /> <arg value=""${ws.install.command} ${dist.dir}/${artifact.filename}.war {-appname ${app.deployed.name} -server ${was.server.name} -contextroot ${app.contextroot.name}}"" /> <arg line=""-c"" /> <arg value=""${ws.save.command}"" /> <arg line=""-c"" /> <arg value=""${ws.setManager.command}"" /> <arg line=""-c"" /> <arg value=""${ws.startapp.command} ${app.deployed.name}"" /> <arg line=""-c"" /> <arg value=""${ws.save.command}"" /> </exec> <echo message=""${websphere.install.output}"" /> </target> <target name=""websphere-uninstall-application""> <exec executable=""${websphere.home.dir}/bin/${wsadmin.base.command}"" logError=""true"" outputproperty=""websphere.uninstall.output"" failonerror=""false""> <arg line=""-conntype ${ws.conn.type}"" /> <arg line=""-host ${ws.host.name}"" /> <arg line=""-port ${ws.port.name}"" /> <arg line=""-username ${ws.user.name}"" /> <arg line=""-password ${ws.password.name}"" /> <arg line=""-c"" /> <arg value=""${ws.setManager.command}"" /> <arg line=""-c"" /> <arg value=""${ws.stopapp.command} ${app.deployed.name}"" /> <arg line=""-c"" /> <arg value=""${ws.save.command}"" /> <arg line=""-c"" /> <arg value=""${ws.uninstall.command} ${app.deployed.name}"" /> <arg line=""-c"" /> <arg value=""${ws.save.command}"" /> </exec> <echo message=""${websphere.uninstall.output}"" /> </target>  If you just want to play around why not use the netbeans IDE to generate your ear files. If you create an enterprise project it will automatically generate the ant files for you. Good for prototyping and just getting started :-) There is even a was plugin which allows automated deployment however this seems very shakey!  a good start point could be this maven pluggin not for use it or maybe yes but this maven is build over ant task. If you see WAS5+Plugin+Mojo.zip\src\main\scripts\was5.build.xml Or as said ""McDowell"" you can use ""WebSphere Application Server (WAS) Ant tasks"" but directly as ANT task. <path id=""classpath""> <fileset file=""com.ibm.websphere.v61_6.1.100.ws_runtime.jar""/> </path> <taskdef name=""wsStartApp"" classname=""com.ibm.websphere.ant.tasks.StartApplication"" classpathref=""classpath"" /> <taskdef name=""wsStopApp"" classname=""com.ibm.websphere.ant.tasks.StopApplication"" classpathref=""classpath"" /> <taskdef name=""wsInstallApp"" classname=""com.ibm.websphere.ant.tasks.InstallApplication"" classpathref=""classpath"" /> <taskdef name=""wsUninstallApp"" classname=""com.ibm.websphere.ant.tasks.UninstallApplication"" classpathref=""classpath"" /> <target name=""startWebApp1"" depends=""installEar""> <wsStartApp wasHome=""${wasHome.dir}"" application=""${remoteAppName}"" server=""${clusterServerName}"" conntype=""${remoteProdConnType}"" host=""${remoteProdHostName}"" port=""${remoteProdPort}"" user=""${remoteProdUserId}"" password=""${remoteProdPassword}"" /> </target> <target name=""stopWebApp1"" depends=""prepare""> <wsStopApp wasHome=""${wasHome.dir}"" application=""${remoteAppName}"" server=""${clusterServerName}"" conntype=""${remoteConnType}"" host=""${remoteHostName}"" port=""${remotePort}"" user=""${remoteUserId}"" password=""${remotePassword}""/> </target> <target name=""uninstallEar"" depends=""stopWebApp1""> <wsUninstallApp wasHome=""${wasHome.dir}"" application=""${remoteAppName}"" options=""-cell uatNetwork -cluster DOL"" conntype=""${remoteConnType}"" host=""${remoteHostName}"" port=""${remoteDmgrPort}"" user=""${remoteUserId}"" password=""${remotePassword}""/> </target> <target name=""installEar"" depends=""prepare""> <wsInstallApp ear=""${existingEar.dir}/${existingEar}"" wasHome=""${wasHome.dir}"" options=""${install_app_options}"" conntype=""${remoteConnType}"" host=""${remoteHostName}"" port=""${remoteDmgrPort}"" user=""${remoteUserId}"" password=""${remotePassword}"" /> </target> Another useful link could be this.",java-ee deployment ant websphere ear,1.7905167329377317E-4,7.435759319890762E-4,0.9940140304426126,0.004468849774337886,5.94492177766669E-4
2518,"Using multiple SQLite databases at once I have 2 SQLite databases one downloaded from a server (server.db) and one used as storage on the client (client.db). I need to perform various sync queries on the client database using data from the server database. For example I want to delete all rows in the client.db tRole table and repopulate with all rows in the server.db tRole table. Another example I want to delete all rows in the client.db tFile table where the fileID is not in the server.db tFile table. In SQL Server you can just prefix the table with the name of the database. Is there anyway to do this in SQLite using Adobe Air? SQLite databases exist independently so there's not way to do this from the database level. You will have to write your own code to do this. ""There's not way to do this from the database level."" How so? SQLite has an `attach` keyword that allows you attach another database. And Theo's answer shows that Air seem to have a corresponding API call. Is there some distinction I'm missing when you specifically say ""from the database level""?  I just looked at the AIR SQL API and there's an attach method on SQLConnection it looks exactly what you need. I haven't tested this but according to the documentation it should work: var connection : SQLConnection = new SQLConnection(); connection.open(firstDbFile); connection.attach(secondDbFile ""otherDb""); var statement : SQLStatement = new SQLStatement(); statement.connection = connection; statement.text = ""INSERT INTO main.myTable SELECT * FROM otherDb.myTable""; statement.execute(); There may be errors in that code snipplet I haven't worked much with the AIR SQL API lately. Notice that the tables of the database opened with open are available using main.tableName any attached database can be given any name at all (otherDb in the example above).  this code can be workit is write of me:  package lib.tools { import flash.utils.ByteArray; public class getConn { import flash.data.SQLConnection; import flash.data.SQLStatement; import flash.data.SQLResult; import flash.data.SQLMode; import flash.events.SQLErrorEvent; import flash.events.SQLEvent; import flash.filesystem.File; import mx.core.UIComponent; import flash.data.SQLConnection; public var Conn:SQLConnection; /* <br>wirten by vivid msn:guanyangchen@126.com */ public function getConn(database:Array) { Conn=new SQLConnection(); var Key:ByteArray=new ByteArray(); ; Key.writeUTFBytes(""Some16ByteString""); Conn.addEventListener(SQLErrorEvent.ERROR createError); var dbFile:File =File.applicationDirectory.resolvePath(database[0]); Conn.open(dbFile); if(database.length>1){ for(var i:Number=1;i<database.length;i++){ var DBname:String=database[i] Conn.attach(DBname.split(""\."")[0]File.applicationDirectory.resolvePath(DBname)); } } /* <br>wirten by vivid msn:guanyangchen@126.com */ Conn.open(dbFile SQLMode.CREATE false 1024 Key); } /* <br>wirten by vivid msn:guanyangchen@126.com */ private function createError(event:SQLErrorEvent):void { trace(""Error code:"" event.error.details); trace(""Details:"" event.error.message); } /* <br>wirten by vivid msn:guanyangchen@126.com */ public function Rs(sql:Array):Object{ var stmt:SQLStatement = new SQLStatement(); Conn.begin(); stmt.sqlConnection = Conn; try{ for(var i:String in sql){ stmt.text = sql[i]; stmt.execute(); } Conn.commit(); }catch (error:SQLErrorEvent){ createError(error); Conn.rollback(); }; var result:Object =stmt.getResult(); return result; } } }  It's possible to open multiple databases at once in Sqlite but it's doubtful if can be done when working from Flex/AIR. In the command line client you run ATTACH DATABASE path/to/other.db AS otherDb and then you can refer to tables in that database as otherDb.tableName just as in MySQL or SQL Server. Tables in an attached database can be referred to using the syntax database-name.table-name. ATTACH DATABASE documentation at sqlite.org",actionscript-3 flex sqlite air adobe,5.752068962625133E-4,0.002388751783811543,0.012556531628027036,0.014356264654074713,0.9701232450378243
717,"Why doesn't VFP .NET OLEdb provider work in 64 bit Windows? I wrote a windows service using VB that read some legacy data from Visual Foxpro Databases to be inserted in SQL 2005. The problem is this use to run fine in Windows server 2003 32-Bits but the client recently moved to Windows 2003 64-Bits and now the service won't work. I'm getting a message the the VFP .NET OLEdb provider is not found. I researched and everything seems to point out that there is no solution. Any Help please... You'll need to compile with the target CPU set to x86 to force your code to use the 32 bit version of the VFP OLE Db provider. Microsoft has stated that there are no plans on releasing a 64-bit edition of the Visual FoxPro OLE Db provider. For what's worth Microsoft has also stated that VFP 9 is the final version of Visual FoxPro and support will end in 2015. If you need the OLE DB provider for VFP 9 you can get it here.  Sybase Anywhere has a OLEDB provider for VFP tables. It states in the page that the server supports 64 bit Windows don't know about the OLEDB provider: Support 64-bit Windows and Linux Servers In order to further enhance scalability support for the x86_64 architecture was added to the Advantage Database Servers for Windows and Linux. On computers with an x86_64 processor and a 64 bit Operating System the Advantage Database Server will now be able to use memory in excess of 4GB. The extra memory will allow more users to access the server concurrently and increase the amount of information the server can cache when processing queries. I didn't try it by myself but some people of the VFP newsgroups reports that it works OK. Link to the Advantage Server / VFP Page  Have you tried changing the target CPU to x86 instead of ""Any CPU"" in the advanced compiler options? I know that this solves some problems with other OLEDB providers by forcing the use of the 32-bit version. @Paul: I'm running into this problem also. However my sln has about 10 projects. Do I need to change this setting for ALL the projects or just the ""startup"" project? Thanks! Thanks no worries I got it to work. But for anyone else reading this I changed the setting only in the startup project and it did the trick. Seems like this should be a solution setting instead of a project setting. Oh well. @Ken: Sorry. I don't know for sure and I can't easily get a test set up. My guess would be that the startup project + any projects that use the OLEDB provider that is causing the issue.",.net database oledb legacy vfp,0.0010755477032426846,0.01153850549547226,0.037622587231757626,0.6703878046628947,0.2793755549066328
1453,"Using object property as default for method property I'm trying to do this (which produces an unexpected T_VARIABLE error): public function createShipment($startZip $endZip $weight = $this->getDefaultWeight()){} I don't want to put a magic number in there for weight since the object I am using has a ""defaultWeight"" parameter that all new shipments get if you don't specify a weight. I can't put the defaultWeight in the shipment itself because it changes from shipment group to shipment group. Is there a better way to do it than the following? public function createShipment($startZip $endZip weight = 0){  if($weight <= 0){  $weight = $this->getDefaultWeight();  } } try using a static class member. class Shipment { public static $DefaultWeight = '0'; public function createShipment($startZip$endZip$weight=Shipment::DefaultWeight){ //you function } }  This will allow you to pass a weight of 0 and still work properly. Notice the === operator this checks to see if weight matches ""null"" in both value and type (as opposed to == which is just value so 0 == null == false). PHP: public function createShipment($startZip $endZip $weight=null){  if ($weight === null)  $weight = $this->getDefaultWeight(); } [@pix0r](#2213) That is a good point however if you look at the original code if the weight is passed as 0 it uses the default weight.  Neat trick with boolean OR operator: public function createShipment($startZip $endZip $weight = 0){ $weight or $weight = $this->getDefaultWeight(); ... }  This isn't much better: public function createShipment($startZip $endZip $weight=null){  $weight = !$weight ? $this->getDefaultWeight() : $weight; } // or... public function createShipment($startZip $endZip $weight=null){  if ( !$weight )  $weight = $this->getDefaultWeight(); }",php parameters error-handling,0.9605309737898694,0.007664589329863645,0.01601858419354587,0.009657980325106383,0.006127872361614781
4519,"Using Xming X Window Server over a VPN I have the Xming X Window Server installed on a laptop running Windows XP to connect to some UNIX development servers. It works fine when I connect directly to the company network in the office. However it does not work when I connect to the network remotely over a VPN. When I start Xming when connected remotely none of my terminal windows are displayed. I think it may have something to do with the DISPLAY environment variable not being set correctly to the IP address of the laptop when it is connected. I've noticed that when I do an ipconfig whilst connected remotely that my laptop has two IP addresses the one assigned to it from the company network and the local IP address I've set up for it on my ""local network"" from my modem/router. Are there some configuration changes I need to make in Xming to support its use through the VPN ? You may have better luck doing X11 Forwarding through SSH rather than fiddling with your DISPLAY variable directly. X11 Forwarding with SSH is secure and uses the existing SSH connection to tunnel so working through a VPN should be no problem. Fortunately this is fairly straightforward with Xming. If you open your connection from within Xming (e.g. the plink option) I believe it sets up X11 forwarding by default. If you connect using another SSH client (e.g. PuTTY) then you simply need to enable X11 forwarding (e.g. 'ssh -X user@host'). In PuTTY the option is under Connection -> SSH -> X11 -> click on 'Enable X11 Forwarding'. Make sure Xming is running in the background on your laptop and do the standard X test 'xclock'. If you get a message like 'X connection to localhost:19.0 broken (explicit kill or server shutdown).' then Xming is most likely not running. Also make sure you're not explicitly setting your DISPLAY variable in any startup scripts; SSH will set up an alias (something like localhost:10 or in the example above localhost:19) for the X11 tunnel and automatically set DISPLAY to that value. Overwriting DISPLAY will obviously mean you will no longer be pointing to the correct X11 tunnel. The flip side of this is that other terminals that don't have SSH X11 Forwarding set can use the same DISPLAY value and take advantage of the tunnel. I tend to prefer the PuTTY option but several of my coworkers use plink from within Xming.  I had nothing but problems with Xming. When I could get it to work it was extremely slow (this is over a VPN). IMO X is not designed to run over slow connections its too chatty. And by slow connection I mean anything less then a LAN connection. My solution was to use x11vnc. It lets you access your existing X11 session through VNC. I just ssh into my box through the VPN and launch: $ x11vnc -display :0 That way I can access everything I had opened during the day. Then when I don't I just exit (Ctrl-C) in the terminal to close x11vnc. Xming is terribly slow over a LAN too... :-/  Chances are it's either X authentication the X server binding to an interface or your DISPLAY variable. I don't use Xming myself but there are some general phenomenon to check for. One test you can do to manually verify the DISPLAY variable is correct is: Start your VPN. Run ipconfig to be sure you have the two IP addresses you mentioned (your local IP and your VPN IP). Start Xming. Run 'netstat -n' to see how it's binding to the interface. You should see something that either says localIP:6000 or VPNIP:6000. It may not be 6000 but chances are it will be something like that. If there's no VPNIP:6000 it may be binding only to your localIP or even 127.0.0.1. That will probably not work over the VPN. Check if there are some Xming settings to make it bind to other or all interfaces. If you see VPNIP:6000 or something similar take note of what it says and remote shell into your UNIX host (hopefully something like ssh if not whatever you have to get a text terminal). On the UNIX terminal type 'echo $DISPLAY'. If there is nothing displayed try 'export DISPLAY=VPNIP:0.0' where VPNIP is your VPN IP address and 0.0 is the port you saw in step 3 minus 6000 with .0 at the end (i.e. 6000 = 0.0 6010 = 10.0). On the UNIX host run something like 'xclock' or 'xterm' to see if it runs. The error message should be informative. It will tell you that it either couldn't connect to the host (a connectivity problem) or authentication failed (you'll need to coordinate Xauth on your host and local machine or Xhosts on your local machine). Opening Xhosts (with + for all hosts or something similar) isn't too bad if you have a locally protected network and you're going over a VPN. Hopefully this will get you started tracking down the problem. Another option that is often useful as it works over a VPN or simple ssh connectivity is ssh tunneling or X11 forwarding over ssh. This simulates connectivity to the X server on your local box by redirecting a port on your UNIX host to the local port on your X server box. Your display will typically be something like localhost:10.0 for the local 6010 port. X can be ornery to set up but it usually works great once you get the hang of it.  putty + XMing - I had to set the DISPLAY environment variable manually to get things running (alongside with checking ""Enable X11 forwarding"" in putty - Connection/SSH/X11) export DISPLAY=0:10.0 (it was set to ""localhost:10.0"" which did not work)  Haven't have the exact problem but I think you need to look at the xhost and make sure that the vpn remote is allowed to send data to the x server. This link might help: http://www.straightrunning.com/XmingNotes/trouble.php  Thanks for the help @Stephen and @Greg Castle using it I've managed to resolve my problem. To provide a basic guide for others (from scratch): Using Xwindows on a Windows PC to connect to a UNIX server over a VPN What you need to start with: The Putty Telnet/SSH client download putty.exe (for free) from: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html The Xming X server download Xming (for free) from: http://sourceforge.net/project/showfiles.php?group_id=156984 What to do: Install both of the above on your Windows PC From the Windows start menu select: Programs -> Xming -> Xming Run the Putty.exe program in the location you downloaded it to In the PuTTY configuration screen do the following: Set the IP address to be the IP address of your UNIX server Select the SSH Protocol radio-button Click the SSH : Tunnels category in the left hand pane of the configuration screen Click the Enable X11 forwarding check-box Click the Open button Logon as usual to your UNIX server Check the directory containing the X windows utilities are in your path e.g. /usr/X/bin on Solaris Run your X Windows commands in your putty window and they will spawn new windows on your desktop",unix open-source hardware vpn xming,3.696782473546908E-4,0.0015352207675928603,0.9268736521032124,0.0627019401587313,0.008519508723108781
2756,"Lightweight IDE for Linux Even though I have a robust and fast computer (Pentium Dual Core 2.0 with 2Gb RAM) I'm always searching for lightweight software to have on it so it runs fast even when many apps are up and running simultaneously. On the last few weeks I've been migrating gradually to Linux and want to install a free lightweight yet useful IDE to program on C++ and PHP. Sintax highlighting and code completition tips are must-haves. So I'd like to receive some suggestions from you guys. any of the popular editors can be turned into an ide. I use Vi on the console and have used various gui editors over the years. This doesn't just go for linux I use Crimson Editor on windows as a C/python/z80asm ide.  I'm not sure exactly what you mean by 'lightweight' but here are a few popular IDEs for linux: Anjuta for Gtk/Gnome KDevelop or Quanta for KDE CodeBlocks runs on Windows/Mac/Linux and is written in C++ None one of these are Java so they automatically have an edge over Eclipse for performance ;) Another option is MonoDevelop which is geared towards .Net/Gtk# programming but also includes C++ support.  I would say Bluefish not an I.D.E but a nice lightweight code editor with syntax highlighting and code completion (and many others) for quite an array of languages (among them C and Php).  You can look at jEdit if you are using or have Java installed. jEdit (wikipedia article) Again it's a 'smart editor' rather than an IDE. Seems to know how to handle most languages and once its started it is pretty smart still Java but less resource hungry than Netbeans and Eclipse.  How has no one mentioned Code::Blocks! Not only is it a fantastic Open Source IDE for C++ but it's fully cross platform so if you need to work on a Windows or Mac box for a bit you can use the exact same IDE and exact same project files to do so! Which is great for cross-compiling!  what about eclipse with linuxtools?  Nobody mentioned Kate. It's easier than vi for start (and has nice vi-mode for those who want to migrate to vi) has more options than gedit (And better syntax highlighting). It also has kioslaves support (nice for remote server PHP development) and it's only a little bit more CPU-demanding than gedit. It can also have built-in console (extremely helpful if you want to quick grep through files or compile the project). There are also features like: basic code completion advanced indentation and block selection operations good and very clean (to read) find/replace with regexp comment-out on ctrl+d (it comments out one line or one function if used on function header) and a lot more...  I bounce about between Mac Windows and Ubuntu and while Emacs used to be my editor of choice I'm finding that in my old age I prefer to something GUI-based (using command-line for the shell is still fine by me). My preferred editor is Komodo Edit which the advantages of: Being free (as in beer) Available for Mac Windows and Linux Syntax highlighting for a boatload of languages including C++ and PHP (I'm using it for Ruby Python and PHP myself) Code completion even for classes I defined myself Ability to ""remote save"" via FTP SFTP or SCP Support for organizing your files into projects Tabs and other interface niceties I'm not sure how lightweight it is but it certainly feels snappier than Eclipse!  If you are taking your time switching to linux I'd switch to emacs or vim at some point as well. There will always be a resource or a document describing exactly the problem you are having with either of them and generally a solution is just a few more clicks down the road. Emacs may be easier at the beginning because of modeless editing... but don't let modal editing scare you away from Vim. The key with either Vim or Emacs is knowing it could probably take you the better part of the day just to figure out what you want them to do let alone how to get them to do that. Once they work for you though you'll see why mostly everyone is in one of two camps. General hints: Setting up a Makefile for your project is almost always worth it. Using cscope and or ctags will make your life easier. Vim hints: :make :cn :cp OmniCompletion using BufRead autoloads to set what :make should do depending on file type Emacs hints: ecb is fun M-x dired M-. M- M-* M-x complete-tag for etags M-x compile (add-hook 'mylanguage-mode-hook '(lambda () (setq my-customizations t))) And check out other people's customizations for examples of what other people do.  emacs has been used by linux programmers for decades. It features syntax highlighting it's fast and there are a million tutorials out there you can find. If you go with Emacs be sure to check out CEDET this collection of tools provides things such as code completion display of types of methods code browsing and code generation. http://cedet.sourceforge.net/  gedit Syntax highlighting Fast lightweight Tabs GUI  Console editors such as emacs and vi are more lightweight than their GUI counterparts and (at least those two are) just as capable as any other IDE (syntax highlighting mouse support ctags autocompletion ... all the way to gdb integration). The learning curve might be somewhat steep and you might have to do some customization but its all worth it. Also vi is present on every installation of unix-like operating system. Amongst X applications there are gedit which comes with GNOME and has many of these IDE features (see for example this blog entry) Geany - really fast depends only on GTK and with even more features including code folding. These would be lightweight IDEs as opposed to heavyweights like Anjuta KDevelop Eclipse or NetBeans. +1 for Geany i used it too  This is a really religious question - just choose the one you like. Every editor has it's pros/cons and you need to decide which set suits best to you. There are many IDEs out there that can use various editors like Pida.  Vim (or Emacs varying on religion) will always be my first answer to this question over any point-and-click IDE. As they write in The Pragmatic Programmer Choose an editor know it thoroughly and use it for all editing tasks. [...] The editor will be an extension of your hand; the keys will sing as they slice their way through text and thought. That's our goal. Make sure that the editor you choose is available on all platforms you use. Vim is configurable extensible programmable and can be turned into an IDE with all the regular features. Lately I've been using Eclim to ""bring Eclipse functionality to the Vim editor"" (projects better java support etc.) making it a complete platform with advanced IDE features. A big +1 for eclim. It's made my life much better. I like the thread here which seems to be: ""Start with any reasonable editor and you can build an IDE around it which is by definition light weight.""  Joey I believe anything is lighter than Eclipse! :o) IMHO eclipse/zend ide have the most clunkiest interfaces i've encountered in my lifetime.",php c++ linux ide freeware,3.770094146622865E-4,0.033791383851225376,0.04293458071525259,0.9117296637520775,0.011167362266782197
1625,"XML Editing/Viewing Software What software is recommended for working with and editing large XML schemas? I'm looking for both Windows and Linux software (doesn't have to be cross platform just want suggestions for both) that help with dealing with huge XML files. The oXygen XML Editor a great IDE for Windows bit expensive tho.  I may be old fashioned but I prefer my text editor. I use emacs and it has a fairly decent xml mode. Most good text editors will have decent syntax hi-lighting and tag matching facilities. Your IDE might already do it (IntelliJ idea does and I believe Eclipse does as well). Good text editors will be able to deal with huge files but some text editors may not be able to handle them. How big are we talking about?  +1 for XML Spy I've used both the stand alone product and the visual studio plugin and I've been impressed. In terms of FOSS I use Notepad++  I use nxml-mode in GNU Emacs for editing xml including very big files. And i use it for a long time - it quick provide on-the-fly validation of xml  and provide completion functionality for tag & attributes names  XML Copy Editor - Windows and Linux Fast free and supports XML schema validation. Official Website http://xml-copy-editor.sourceforge.net/ How to install in Ubuntu http://ubuntuforums.org/showthread.php?t=1640003  I use Notepad++ as my editor. You can also add plugins for dealing with XML specifically.  You need at least a decent text editor as a baseline emacs with nxml mode as mentioned before is a very good choice. However as the schema becomes larger and larger you may loose the overview especially when you author an XML Schema document which can be very verbose. You'll need some sort of visualization: XML Spy is ok Oxygen is great but expensive but as it turns out on Windows you have almost all needed features in XMLPad which is freeware. When you start editing instance XML documents (and even editing XML Schemas) you need on the fly validation against a schema and if possible auto-completion of attributes and elements. Emacs only supports on the fly validation and auto-completion with a relax NG based schema (but any XSD can be converted to a relax NG schema). If you have any choice in the matter consider using Relax NG as your schema syntax it is much more readable and maintainable. XMLPad looks like the best alternative to VS I've seem until now. The only annoyance is that there seems to be no way to customize the layout. The bottom and right panes just take up space in my case.  Altova's XML Spy is a great editor but not necesarily the cheapest option out there.  I work a lot with XML and have found Oxygen to be a great editor. It's cross-platform and has a graphical schema editor but since I use DTDs and not schemas I can't vouch for the schema editor's quality. The rest of the editing package (such as the XML editor and XSLT debugger) is solid so it could be worth a try.  For Windows I found Microsoft's own free XML Notepad 2007 to be a great simple to use editor with a nice selection of features. Used it for both reviewing my XML output when developing and editing broken iTunes' libraries. ;) Requires .net 2.0  I agree that your text editor is probably your best bet. I do know some people who swear by XMLSpy if you need something that's tailored specifically for dealing with XML files in a visual way. I bet you could find some F/OSS work-alikse but I'm not aware of any.  Altova's XMLSpy is probably the best available. It offers different views of your data/schemas XPath tools and produces good diagrams among other things. It does cost quite a bit though. It's a mature product so you don't tend to run into limitations as quickly as you do with some other tools. Liquid XML is a pretty good but relatively new alternative. It's a nice app to use and there's even a free version available! This is a tool worth keeping an eye on. Both of these products have a handy feature which produces sample XML files based on your schema. In contrast Oracle's JDeveloper (based on Borland Jbuilder I believe) tries to provide a decent schema editor but falls short in that it sometimes produces invalid schema files. I stopped using it soon after noticing this. I highly recommend checking out IBM's XML Schema Quality Checker. This command line tool validates your schema against WC3's XML Schema language. This is a good idea even if you've built your schema using another tool.  I highly recommend Stylus Studio if you have any need for a long term broadly capable XML IDE. I've used it mostly for XSLT development but it supports development of almost everything XML related you would want to do. It's Windows only (very annoying).  I am using Cooktop (also available on tucows) and I'm very happy about XPath testing feature. Cooktop is an editor and development environment for XML DTD and XSLT documents Cooktop is a Windows application Best of all it's free! Features Color-coded XML DTD and XSLT editing Check well-formedness and validate Stylesheet testing with almost any XSLT engine XPATH testing Customizable ""Code Bits"" library XML formatting via Tidy Small download small footprint Link is broken... @Paulocoghi: added tucows alternate download.  Recently I was editing XSLT files with Eclipse but for some reason Eclipse wouldn't do any auto-completion anymore. So I switched to Emacs's brilliant nxml-mode and I'm not sure I'm going back. You get auto-completion that's really easy to use and it's very fast. The only glitch is that you must provide a RELAX NG version of your document's schema but there are tools out there that generate one for you from your DTD or Schema. Check out http://www.xmlhack.com/read.php?item=2061 for more. For non-free software I second the recommendations for OxygenXML.  FirstObject XML Editor. http://www.firstobject.com/dn_editor.htm Its free written in C++ optimized for working with very large xml files. While it is relatively limited in functionality it can load 100MB+ unformatted files in seconds indent them and locate specific elements using the tree view. By using the 'Refresh' option you can also synchronise the tree with the text view. It's in the UNIX spirit of having a simple tool doing a specific job very well.  Open source XML editors examined - it is a little bit outdated though.",windows xml linux,3.5335897912101166E-4,0.003790844897134498,0.014683869334038986,0.9637349248325896,0.01743700195711594
4665,"Verifying files for testing I was working with quality yesterday doing some formal testing. In their procedure they were verifying all files on the test machine were pulled from the release. The way they were verifying these files were the same was by checking the size and the date/time stamp windows put on them in Windows Explorer. These happened to be off for another reason which I was able to find out why. Is this a valid way to verify a file is the same? I didn't think so and started to argue but I am younger here so thought I shouldn't push it too far. I wanted to argue they should do a binary compare on the file to verify its contents are exact. In my experience time/date stamps and size attributes don't always act as expected. Any thoughts??? Hashing is very good. But the other slightly lower tech alternative is to run a diff tool like WinMerge or TextWrangler and compare the two versions of each file. Boring and there's room for human error. Best of all use version control to ensure the files you're testing are the files you edited and the ones you're going to launch. We have checkout folders from our repo as the staging and live sites so once you've committed the changes from your working copy you can be 100% sure that the files you test push to staging and then live are the same because you just run ""svn update"" on each box and check the revision number. Oh and if you need to roll back in a hurry (it happens to us all sometime or another) you just run svn update again with the -r switch and go back to a previous revision virtually instantly.  You should do a CRC check on each file... from the wiki: Cyclic redundancy check a type of hash function used to produce a checksum in order to detect errors in transmission or storage. It produces an almost unique value based on the contents of the file. CRC-32 only has good hamming distances for fairly small files (< 128K) over that size the do not have enough entropy to be used reliably for file comparison.  The only 100% way to figure out if two files are equal is to do a binary comparison of the two. If you can live with the risk of false positives (ie. two files which aren't 100% identical but your code says they are) then the digest and checksum algorithms can be used to lessen the work particularly if the files lives on two different machines with less than optimal bandwidth so that a binary comparison is infeasible. The digest and checksum algorithms all have chances of false positives but the exact chance varies with the algorithm. General rule is that the more crypto-made it is and the more bits it outputs the less chance of a false positive. Even the CRC-32 algorithm is fairly good to use and it should be easy to find code examples on the internet that implements it. If you only do a size/timestamp comparison then I'm sorry to say that this is easy to circumvent and won't actually give you much of a certainty that the files are the same or different. It depends though if you know that in your world timestamps are kept and only changed when the file is modified then you can use it otherwise it holds no guarantee.  I would do something like an md5sum hash on the files and compare that to the known hashes from the release. They will be more accurate than just date/time comparisons and should be able to be automated more.  The normal way is to compute a hash of the two files and compare that. MD5 and SHA1 are typical hash algorithms. md5sum should be installed by default on most unix type machines and Wikipedia's md5sum article has links to some windows implementations.",windows testing,0.08573089373433775,0.029890986458705523,0.0016984923930363654,0.7526139303459544,0.13006569706796597
2775,"How to remove the time portion of a datetime value (SQL Server)? Here's what I use: SELECT CAST(FLOOR(CAST(getdate() as FLOAT)) as DATETIME) I'm thinking there may be a better/more elegant way. Requirements: It has to be as fast as possible (the less casting the better) The final result has to be a datetime type not a string SELECT CONVERT(VARCHAR(10)[YOUR COLUMN NAME]105) [YOURTABLENAME]  Itzik Ben-Gan in DATETIME Calculations Part 1 (SQL Server Magazine February 2007) shows three methods of performing such a conversion (slowest to fastest; the difference between second and third method is small): SELECT CAST(CONVERT(char(8) GETDATE() 112) AS datetime) SELECT DATEADD(day DATEDIFF(day 0 GETDATE()) 0) SELECT CAST(CAST(GETDATE() - 0.50000004 AS int) AS datetime) Your technique (casting to float) is suggested by a reader in the April issue of the magazine. According to him it has performance comparable to that of second technique presented above. This is now fastest in SQL 2008: `Convert(date GetDate())`. If we're going to use this method I would prefer `SELECT CAST(CAST(GETDATE() - '12:00:00.003' AS int) AS datetime)` instead as it means something to me and is imo much easier to remember. In my opinion casting to float is not best. Please [see my answer](http://stackoverflow.com/questions/2775/whats-the-best-way-to-remove-the-time-portion-of-a-datetime-value-sql-server/3696991#3696991) @Emtucifor I agree that the 3rd method is very obscure because of the *0.50000004* value but **it's the fastest one and your tests confirm that**. Thus it satisfies the *as fast as possible* requirement. @Emtucifor Also here's what the article I linked says about the *0.50000004* value: *Although this expression is short (and efficient as I'll demonstrate shortly) **I have to say that I feel uneasy with it**. I'm not sure I can put my finger on exactly why—maybe because it's too technical and you can't see datetime-related logic in it.*  Your CAST-FLOOR-CAST already seems to be the optimum way at least on MS SQL Server 2005. Some other solutions I've seen have a string-conversion like Select Convert(varchar(11) getdate()101) in them which is slower by a factor of 10. We use the method suggested by Michael Stum in one of our products and it works like a charm. This is not the optimum way by quite a bit. Please see [my answer](http://stackoverflow.com/questions/2775/whats-the-best-way-to-remove-the-time-portion-of-a-datetime-value-sql-server/3696991#3696991) on this same page.  SQL Server 2008 has a new date data type and this simplifies this problem to: SELECT CAST(CAST(GETDATE() AS date) AS datetime)  What Is Really Best? I've seen inconsistent claims about what's fastest for truncating the time from a date in SQL Server and some people even said they did testing but my experience has been different. So let's do some more stringent testing and let everyone have the script so if I make any mistakes people can correct me. Float Conversions Are Not Accurate First I would stay away from converting datetime to float because it does not convert correctly. You may get away with doing the time-removal thing accurately but I think it's a bad idea to use it because it implicitly communicates to developers that this is a safe operation and it is not. Take a look: declare @d datetime; set @d = '2010-09-12 00:00:00.003'; select Convert(datetime Convert(float @d)); -- result: 2010-09-12 00:00:00.000 -- oops This is not something we should be teaching people in our code or in our examples online. Also it is not even the fastest way! Proof – Performance Testing If you want to perform some tests yourself to see how the different methods really do stack up then you'll need this setup script to run the tests farther down: create table AllDay (Tm datetime NOT NULL CONSTRAINT PK_AllDay PRIMARY KEY CLUSTERED); declare @d datetime; set @d = DateDiff(Day 0 GetDate()); insert AllDay select @d; while @@ROWCOUNT != 0 insert AllDay select * from ( select Tm = DateAdd(ms (select Max(DateDiff(ms @d Tm)) from AllDay) + 3 Tm) from AllDay ) X where Tm < DateAdd(Day 1 @d); exec sp_spaceused AllDay; -- 25920000 rows Please note that this creates a 427.57 MB table in your database and will take something like 15-30 minutes to run. If your database is small and set to 10% growth it will take longer than if you size big enough first. Now for the actual performance testing script. Please note that it's purposeful to not return rows back to the client as this is crazy expensive on 26 million rows and would hide the performance differences between the methods. Performance Results set statistics time on; -- (All queries are the same on io: logical reads 54712) GO declare @dd date @d datetime @di int @df float @dv varchar(10); -- Round trip back to datetime select @d = CONVERT(date Tm) from AllDay; -- CPU time = 21234 ms elapsed time = 22301 ms. select @d = CAST(Tm - 0.50000004 AS int) from AllDay; -- CPU = 23031 ms elapsed = 24091 ms. select @d = DATEDIFF(DAY 0 Tm) from AllDay; -- CPU = 23782 ms elapsed = 24818 ms. select @d = FLOOR(CAST(Tm as float)) from AllDay; -- CPU = 36891 ms elapsed = 38414 ms. select @d = CONVERT(VARCHAR(8) Tm 112) from AllDay; -- CPU = 102984 ms elapsed = 109897 ms. select @d = CONVERT(CHAR(8) Tm 112) from AllDay; -- CPU = 103390 ms elapsed = 108236 ms. select @d = CONVERT(VARCHAR(10) Tm 101) from AllDay; -- CPU = 123375 ms elapsed = 135179 ms. -- Only to another type but not back select @dd = Tm from AllDay; -- CPU time = 19891 ms elapsed time = 20937 ms. select @di = CAST(Tm - 0.50000004 AS int) from AllDay; -- CPU = 21453 ms elapsed = 23079 ms. select @di = DATEDIFF(DAY 0 Tm) from AllDay; -- CPU = 23218 ms elapsed = 24700 ms select @df = FLOOR(CAST(Tm as float)) from AllDay; -- CPU = 29312 ms elapsed = 31101 ms. select @dv = CONVERT(VARCHAR(8) Tm 112) from AllDay; -- CPU = 64016 ms elapsed = 67815 ms. select @dv = CONVERT(CHAR(8) Tm 112) from AllDay; -- CPU = 64297 ms elapsed = 67987 ms. select @dv = CONVERT(VARCHAR(10) Tm 101) from AllDay; -- CPU = 65609 ms elapsed = 68173 ms. GO set statistics time off; Some Rambling Analysis Some notes about this. First of all if just performing a GROUP BY or a comparison there's no need to convert back to datetime. So you can save some CPU by avoiding that unless you need the final value for display purposes. You can even GROUP BY the unconverted value and put the conversion only in the SELECT clause: select Convert(datetime DateDiff(dd 0 Tm)) from (select '2010-09-12 00:00:00.003') X (Tm) group by DateDiff(dd 0 Tm) Also see how the numeric conversions only take slightly more time to convert back to datetime but the varchar conversion almost doubles? This reveals the portion of the CPU that is devoted to date calculation in the queries. There are parts of the CPU usage that don't involve date calculation and this appears to be something close to 19875 ms in the above queries. Then the conversion takes some additional amount so if there are two conversions that amount is used up approximately twice. More examination reveals that compared to Convert( 112) the Convert( 101) query has some additional CPU expense (since it uses a longer varchar?) because the second conversion back to date doesn't cost as much as the initial conversion to varchar but with Convert( 112) it is closer to the same 20000 ms CPU base cost. Here are those calculations on the CPU time that I used for the above analysis:  method round single base ----------- ------ ------ ----- date 21324 19891 18458 int 23031 21453 19875 datediff 23782 23218 22654 float 36891 29312 21733 varchar-112 102984 64016 25048 varchar-101 123375 65609 7843 round is the CPU time for a round trip back to datetime. single is CPU time for a single conversion to the alternate data type (the one that has the side effect of removing the time portion). base is the calculation of subtracting from round the difference between the two invocations: single - (round - single). It's a ballpark figure that assumes the conversion to and from that data type and datetime is approximately the same in either direction. It appears this assumption is not perfect but is close because the values are all close to 20000 ms with only one exception. One more interesting thing is that the base cost is nearly equal to the single Convert(date) method (which has to be almost 0 cost as the server can internally extract the integer day portion right out of the first four bytes of the datetime data type). Conclusion So what it looks like is that the single-direction varchar conversion method takes about 1.8 μs and the single-direction DateDiff method takes about 0.18 μs. I'm basing this on the most conservative ""base CPU"" time in my testing of 18458 ms total for 25920000 rows so 23218 ms / 25920000 = 0.18 μs. The apparent 10x improvement seems like a lot but it is frankly pretty small until you are dealing with hundreds of thousands of rows (617k rows = 1 second savings). Even given this small absolute improvement in my opinion the DateAdd method wins because it is the best combination of performance and clarity. The answer that requires a ""magic number"" of 0.50000004 is going to bite someone some day (five zeroes or six???) plus it's harder to understand. Additional Notes When I get some time I'm going to change 0.50000004 to '12:00:00.003' and see how it does. It is converted to the same datetime value and I find it much easier to remember. For those interested the above tests were run on a server where @@Version returns the following: Microsoft SQL Server 2008 (RTM) - 10.0.1600.22 (Intel X86) Jul 9 2008 14:43:34 Copyright (c) 1988-2008 Microsoft Corporation Standard Edition on Windows NT 5.2 (Build 3790: Service Pack 2) +1 great answer but isn't convert to date work faster than datediff? @Roman If you are working with SQL Server 2008 and up yes converting to `date` data type is fastest as shown in my tests above. @Denis in Oracle there's the simple trunc() function. If you use round I think you'll find that time after 12 noon are rounded to the next day instead of the current date. +1 What version of SQL Server did you test this on by the way? It looks like you have *single* and *round* backwards in your table. Also is there any difference in time if you use `char` instead of `varchar`? @Gabe thanks fixed. Char appears to be exactly the same as varchar. In Oracle there's `select round(sysdate) from dual` and we definitely need that in Sql Server. +1 I liked the point that float conversions are not accurate @Rick thanks! In fact you opened my eyes as to the meaning of this keyword in the context of date-time.",sql-server datetime date-conversion,0.01404552706625266,0.01659462759789522,0.00227428216436806,0.08751803270426102,0.879567530467223
2770,"Global Exception Handling for winforms control When working on ASP.NET 1.1 projects I always used the Global.asax to catch all errors. I'm looking for a similar way to catch all exceptions in a Windows Forms user control which ends up being a hosted IE control. What is the proper way to go about doing something like this? Also have a look at [my question](http://stackoverflow.com/questions/944/unhandled-exception-handler-in-net-11) for some of the pitfalls (links to a couple of coding horror blog entries). Code from MSDN: http://msdn.microsoft.com/en-us/library/system.appdomain.unhandledexception.aspx?cs-save-lang=1&cs-lang=vb#code-snippet-2 Sub Main() Dim currentDomain As AppDomain = AppDomain.CurrentDomain AddHandler currentDomain.UnhandledException AddressOf MyHandler Try Throw New Exception(""1"") Catch e As Exception Console.WriteLine(""Catch clause caught : "" + e.Message) Console.WriteLine() End Try Throw New Exception(""2"") End Sub Sub MyHandler(sender As Object args As UnhandledExceptionEventArgs) Dim e As Exception = DirectCast(args.ExceptionObject Exception) Console.WriteLine(""MyHandler caught : "" + e.Message) Console.WriteLine(""Runtime terminating: {0}"" args.IsTerminating) End Sub  Currently in my winforms app I have handlers for Application.ThreadException as above but also AppDomain.CurrentDomain.UnhandledException Most exceptions arrive via the ThreadException handler but the AppDomain one has also caught a few in my experience Sample code from MSDN showing how to catch both types of unhandled exceptions: [msdn](http://msdn.microsoft.com/en-us/library/system.windows.forms.application.threadexception.aspx)  You need to handle the System.Windows.Forms.Application.ThreadException event for Windows Forms. This article really helped me: http://bytes.com/forum/thread236199.html.  If you're using VB.NET you can tap into the very convenient ApplicationEvents.vb. This file comes for free with a VB.NET WinForms project and contains a method for handling unhandled exceptions. To get to this nifty file it's ""Project Properties >> Application >> Application Events"" If you're not using VB.NET then yeah it's handling Application.ThreadException.",winforms error-handling user-controls,0.047541520366769015,0.8317453085598586,0.0024728939090155797,0.06797182687249063,0.05026845029186616
1505,"How do I give my web sites an icon for iPhone? How do I set the icon that appears on the iPhone for the web sites I create? I also found the `apple-touch-icon` information on the [Safari Web Content Guide](http://developer.apple.com/safari/library/documentation/AppleApplications/Reference/SafariWebContent/ConfiguringWebApplications/ConfiguringWebApplications.html) as well as `apple-mobile-web-app-capable` meta tag for full screen mode. possible duplicate of [What size should apple-touch-icon.png be for iPad and iPhone 4?](http://stackoverflow.com/questions/2997437/what-size-should-apple-touch-icon-png-be-for-ipad-and-iphone-4) I've made a suggestion that Stack Overflow implement an apple-touch-icon: Add an apple-touch-icon for Safari on iPhone  And for the sake of completeness a link to Scott Hanselman's posting which contains some additional tips as well: Add Home Screen iPhone Icons and Adjust the ViewPort  See the Specifying a Webpage Icon for Web Clip section of the Configuring Web Applications page of the Safari Web Content Guide in the Safari Reference Library... Specifying a Webpage Icon for Web Clip iPhone OS Note: The Web Clip feature is available in iPhone OS 1.1.3 and later. The apple-touch-icon-precomposed.png filename is available in iPhone OS 2.0 and later. You may want users to be able to add your web application or webpage link to the Home screen. These links represented by an icon are called Web Clips. Follow these simple steps to specify an icon to represent your web application or webpage on iPhone OS. To specify an icon for the entire website (every page on the website) place an icon file in PNG format in the root document folder called apple-touch-icon.png or apple-touch-icon-precomposed.png. If you use apple-touch-icon-precomposed.png as the filename Safari on iPhone OS won’t add any effects to the icon. To specify an icon for a single webpage or replace the website icon with a webpage-specific icon add a link element to the webpage as in: <link rel=""apple-touch-icon"" href=""/custom_icon.png""/> In the above example replace custom_icon.png with your icon filename. If you don’t want Safari on iPhone OS to add any effects to the icon replace apple-touch-icon with apple-touch-icon-precomposed. See ""Create an Icon for Your Web Application or Webpage"" in iPhone Human Interface Guidelines for Web Applications for webpage icon metrics.  From the Apple Developer Connection Safari Web Content Guide for iPhone page Specifying a Webpage Icon for Web Clip... The user can add a web application or webpage link to the Home screen. These links represented by an icon are called web clips. Follow these simple steps to specify an icon to represent your web application or webpage on iPhone. To specify an icon for the entire website (every page on the website) place an icon file in PNG format in the root document folder called apple-touch-icon.png. To specify an icon for a single webpage or replace the website icon with a webpage-specific icon add a link element to the webpage as in: <link rel=""apple-touch-icon"" href=""/custom_icon.png""/> In the above example replace custom_icon.png with your icon filename. See ""Create an Icon for Your Web Application or Webpage"" in iPhone Human Interface Guidelines in iPhone Human Interface Guidelines for webpage icon metrics. Note: The web clip feature is available in iPhone 1.1.3 and later. The link's dead.",iphone html favicon apple-touch-icon,4.963611593264563E-4,0.0020613167409152806,0.017362680389641713,0.009124738855773532,0.970954902854343
470,Homegrown consumption of web services I've been writing a few web services for a .net app now I'm ready to consume them. I've seen numerous examples where there is homegrown code for consuming the service as opposed to using the auto generated methods Visual Studio creates when adding the web reference. Is there some advantage to this? No what you're doing is fine. Don't let those people confuse you. If you've written the web services with .net then the reference proxies generated by .net are going to be quite suitable. The situation you describe (where you are both producer and consumer) is the ideal situation. If you need to connect to a web services that is unknown at compile time then you would want a more dynamic approach where you deduce the 'shape' of the web service. But start by using the auto generated proxy class and don't worry about it until you hit a limitation. And when you do -- come back to stack overflow ;-) Just watch out if you're planning on using SSL. The proxies that are generated automatically won't allow you to change the encryption method on the service. For example on an Oracle/Java web service I had to consume recently I needed to build my proxy manually so that I could specify the ServicePointManager SecurityProtocol as SSL3. ServicePointManager.SecurityProtocol = SecurityProtocolType.Ssl3;,.net web-services,0.12137320317006452,0.08794694736364234,0.45514521850752704,0.06360177878195071,0.2719328521768154
1576,"What should a longtime Windows user know when starting to use Linux? We've finally moved our websites to a decent host and for the first time we have Shell Access. I know very little about using Linux I can navigate through the file system read files with Vim and I'm aware of the man command and I have been able to work out solutions to problems as they show up (eventually) but I know I'm unaware of a lot. Edit: We currently only use the host to hold our live sites I'm sure that we use it more effectively but I'm not sure where to start. So with Web Development in mind: What are the essential commands that every Linux user should know about? What are the most useful commands that I should look into? less [file name] - shows contents of file screen by screen I'd recommend that over reading the file through vim. Also pico and emacs are a little more friendly than vim for noobs.  Personally I found slicehost a big help with setting up webservers on Linux and this eventually helped me make the transition to Linux as my desktop OS. They have tons of articles covering various distros security concerns and web stacks and should be helpful even if you're not hosting with them. http://articles.slicehost.com/  If you only have shell access to your host a number of issues are already taken care of for you (you don't have to maintain the system yourself). The useful commands depend on what you primarily want to do such as interacting with your source control system via command line (you do use source control don't you?) You already know how to use vim and navigate through the filesystem using cd and ls so that is a great start. Most useful commands: ls list files in current directory (like Windows dir) cd change directory cp copying file(s) example: $> cp {file1} {file2}  $> cp /home/jms/file1.txt /home/jms/file1-copy.txt mv moving or renaming file(s) example - rename file1.txt: $> mv {file1} {file2} $> mv /home/jms/file1.txt /home/jms/file_1_new_name.txt example - move file1.txt: $> mv /home/jms/file1.txt /home/jms/myfiles/file1.txt man see the manual pages for a command example: $> man woman $> Segmentation fault (core dumped) find search through directories recursively (and optionally perform some action for each match) grep search for pattern matches wc word count / character count / line count example: counting the files in a the current directory (uses ls and wc) $> ls | wc -l example: count the files that contain .txt in your home directory (uses find grep and wc) $> find /home/jms | grep *.txt | wc -l less lightweight file viewer head see the first few lines of a file tail see the last few lines of a file (useful for realtime logging) example: monitor a logfile as logging occurs while an application is running $> tail -f /var/log/somelogfile.log passwd change your password example: will act on current user and prompt for old/new password $> passwd example: will change password for the user named someuser $> passwd someuser ssh secure shell for logging into remote systems touch set file ""last modified"" time to now (creates a new file if none exists) rm remove a file can also remove files and directories recursively mkdir / rmdir create or remove a directory df check free disk space on volumes du check used disk space on a directory (recursively) ln make a new file/directory that is a ""link"" to another (such as a symbolic link) example (symbolic link): $> ln -s /path/to/destination kill kill/stop a running process chmod chown change permissions / ownership for files. sudo run a command with superuser (ie ""root"") privileges your web host may not give you permission to do this vi a text editor included with every linux installation A number of these items you will have an easier time learning by experimentation. A very comprehensive guide to bash scripting might also be of use. Eh. For searching for files i'd just use `locate ""search string""` unless i needed the powers of regex. Le sigh sounds like a bug at the sanitiser or Markdown.... What about creating a new directory? (I probably use that more often than some of ones mentioned) a few more which come in really handy : sed (edit streams as they pipe in and out) cut (get fields from delimiter separated lines) comm (compare two sorted files line by line) @Ray Vega - added a few more including mkdir/rmdir @Jean I don't really use sed cut or comm would you really think they are necessary for linux beginners? For searching use find / | grep ""search string""  A command I use quite frequently that I don't see any reference to is $ top It serves as a terminal/console equivalent to Windows Task Manager letting you see the state of the system in terms of workload and resource availability; listing what tasks are running and how much impact they are having.  Find out the distro they run (e.g. Ubuntu) and focus on that first. Use VirtualBox or another VM to play around especially networking and the Daemons that your host is using.  Treebeard's Unix Cheat Sheet  There are many answers here but none of them include the incredibly useful tool that I simply cannot live without: strace I cannot count the number of times that I have run a program it has crashed for an obscure reason or given an obscure error message and I have been able to ascertain the problem immediately and resolve it. Here is a concrete example I was called in to discover why after an upgrade php started ignoring its configuration on the webserver. No matter what settings they put in the config file they weren't being observed and the website was down. So I ran 'php' saw no errors ascertained that it was looking at the correct config file using 'phpinfo()' so I decided ""lets see what it's actually doing"". $ strace -e open php 2>&1 | grep php.ini open(""/usr/bin/php.ini"" O_RDONLY) = -1 ENOENT (No such file or directory) open(""/etc/php.ini"" O_RDONLY) = -1 EACCES (Permission denied) doing a quick ls -la solves the mystery: $ ls -la /etc/php.ini -rw------- 1 root root 44990 2008-05-09 00:26 /etc/php.ini php.ini wasn't readable to the user running the webserver. Somehow its permissions had been altered in the upgrade. I would never have found that out without using strace.  This article on linuxguide.it is a fairly comprehensive list of commands but here are a few of the ones I find I use most frequently: ls -la (lists the contents of the current directory including hidden files) tar cvzf output_file.tar.gz directory/ (tars and gzips all of the files in directory/ into output_file.tar.gz) tar cvzf file.tar.gz (unzips and untars the file restoring the original directory structure and keeping all permissions intact) man binary (displays the man page for binary) chmod (changes the permissions on a specified file use man chmod for more details) Hope that helped at least a little bit check out the link above for a better list.  There is the common ls cp ln mv grep man kill ps. One thing I would definitely recommend apt-get and aptitude to be able to uninstall some program or library you really need. I would also recommend reading the Advanced Bash Scripting Guide to help you automate anything that you happen to do often.  If you're going to be doing development on Linux I'd recommend skimming ""The Art of Unix Programming"" by Eric S. Raymond. This book is less about the details and more about the differing philosophies that are behind Windows and Linux. It will help you think like a Unix developer and make all the weird things more understandable. As Eric says in the introduction: You should read this book if you are a non-Unix programmer who has figured out that the Unix tradition might have something to teach you. We believe you're right and that the Unix philosophy can be exported to other operating systems. So we will pay more attention to non-Unix environments (especially Microsoft operating systems) than is usual in a Unix book; and when tools and case studies are portable we say so.  The Window Key on your keyboard is not gonna work.  Oh there was a fun thing on old Linux distributions: rm -rf * I just wanted to delete all files inside the current folder. But I managed to delete the whole disk. This is because ""*"" included ""."" and "".."" too. This was changed in newer Linux releases. But be careful on old distributions. ;) I've had a couple of Linux boxes commit seppuku (running rm -Rf /* as root) when they were being decommissioned. It's surprising how long they can run without a file system before the inevitable happens. Not anymore. But they used to. `*` should not include `.` and `..`; there may however be a shell option you can set that makes it include them. Any distro/OS which changed this behavior by default is completely broken.  more/less: view a file in your command prompt. more myfile.txt grep: find keywords using regular expressions in a file group of files or recursively in all files grep hello myfile.txt grep hello *.txt grep -r hello . understand pipe (the '|' character)... it lets you chain things together nicely. grep -r hello . | grep txt: (will recursively grep for hello then grep those results for ""txt:"" which if you look at grep output you will see the above means it was found in a file ending in txt) su will let you ""switch user"" with no argument will be to root with an argument will be to another user sudo will let you execute a command as root if you are on the sudo list sudo more /path/to/root/only/accessible.file Those are the first few I can think of off the top of my head that should prove really helpful but I'm sure there are many more that are useful.  Before you start to migrate to Linux read this: Linux is not Windows.  Baby steps... Most of my command line stuff is integrating with our code. svn up svn commit svn merge and so on. Personally what I've found very useful is a basic knowledge of bash scripting ruby scripting and (absurdly) colordiff Command line stuff requires a pile of typing but that's one of the main reasons why scripting exists. For example I can't for the life of me read svn diff output on the commandline. Piping it to the colordiff utility makes a world of difference. It's then one more step to create a bash function to save you typing svn diff | colordiff | less -R all the time. If you read man colordiff you'll see they define a function in your ~/.bashrc file mine goes something like this function sdiff() { svn diff $@ | colordiff | less -R; } The $@ means 'All arguments' otherwise arguments are $1 $2 etc and the rest should be fairly self explanatory. You can then apply this knowledge you now have to other repetitive actions and before you know it you'll be a guru. Sometimes bash gets a bit hairy (like when you want to do anything more complicated than piping a few programs together) and in those cases I've found just writing simple ruby scripts to be the way out. Ruby is awesome. Good Luck! :-)  I don't want to get into a religious war here but regardless of your eventual text editor preference pico and nano are the easiest to jump into because the essential keyboard commands are all displayed for you at the bottom of the screen. I've long been told that vi would be worth using but for when you just need to change one line in a config file its nice not to have to spend 15 minutes trying to remember how to save and quit. That said if you do find yourself in vi you can press escape to enter 'command mode' (or whatever it is called) and :wq will save and quit. I only mention this because of all things Linux related trying to exit vi is the thing that frustrated me the most until I learnt how! You can just press ESC and then type `:x` to save and quit Vi/m IIRC  Now that you are on a stable OS. I recommend that you still boot your new machine at least once a month or so. I know with the old Windows machine you probably rebooted weekly but don't do what a friend did on an hp-ux machine and run 2 years without rebooting. It really does speed things up if you reboot once a month or so even if everything is running well. :)  This is a little more broad range & general of a suggestion but I found Unix System V: A Practical Guide (Mark G. Sobell) was a great...I repeat...great book to learn the basics from (albeit it was my academic textbook). Like I said you might be more interested in something a little more distro-specific but for the general Unix goods I can't recommend this reference enough.  In a pathname the separators go like this ""/"" and not like this ""\"". Also spaces and/or capitalization sucks in a pathname. Spaces are sometimes a pain but I've never found a situation where capitalization ever caused a problem. the Tab key always does the trick :)  If you've had to type a long command and for whatever reason you need to type it again there's a neat (well it's neat to me anyway) trick. Providing you remember at least a part of command on the line you typed use history | [part of the line] This will return a numbered list of the command in the history buffer (which generally stashes about 1000 lines) which contain [part of the line]. You can then quickly re-run it by typing ![linenumber] I think you meant: `history | grep [part of the line]`  An extremely helpful command that doesn't seem to get mentioned a lot is apropos which lets you search the man pages. The man pages are great for figuring out how a command works but troublesome when you aren't sure what command(s) will do what you want to do. Keep in mind when moving to Linux that you will want to read the man pages thoroughly and often. Beyond that another laundry list of commands isn't going to help a lot (there are already some useful suggestions in this area); but I can say that it's important to get used to piping and redirection. If possible pick up books on shell scripting and perl - it's no joke that they are the glue and duct tape that hold pretty much every *nix system together. Learn to use vi effectively since it's the standard on linux systems for editing configuration files. Finally if you're not already get comfortable with regular expressions and globs since they are used a lot in the linux world.  Fosswire Unix/Linux Command Reference My advice would be to install a distribution (or use a LiveCD) on one of your own machines and play around with it first and then once you're confident enough to move on fiddling with the production server. I think it would be even safer to play around with Linux in a VM. Because running Linux from a LiveCD it might mount your other hard drives and you might delete files on them by mistake.  Seeing that this seems to be a web host you will probably want to know: How to start/stop/restart the webserver (Distro/server dependent. Try /etc/init.d/apache restart to restart.) How to check the logs (Distro/server dependent. Try less /var/www/apache.log) How to access MySQL directly (mysql -u myusername -p and mysqladmin) How to upload/download files (Probably using SFTP on the client end) How to edit the webserver configuration (Probably nano /etc/apache2/httpd.conf) How to check/edit UNIX permissions and ownership (ls -l to check and chmod XXXX files and chown newowner.newgroup files to change) These are all using complete guesses for file locations etc. and assume that you are using Apache. A bit of investigation is probably needed for your particular setup. A special mention goes to permissions. This causes much of the headaches with running CGI's and security. Below is a rough guide to what permissions should be set. PHP files: readable by web-server (chmod 640 filename.php) CGI scripts: executable by webserver (chmod 750 filename.cgi) Static web files: readable by webserver (chmod 640 filename.html) Directories: executable by webserver (chmod 750 directoryname) These settings assume that the webserver process is running as a user who belongs to the same group as the webserver files (this is likely the setup on a managed host). A final headache that may cause trouble is that Linux is case sensitive. When serving static files from Apache by default you will need to include any weird capitalisation. It's generally a good idea to stick to lower-case and underscores/hyphens for naming directories and files. For checking log files in real time : tail -f /var/www/apache.log  mc (midnight commander) is quite helpful - it's a text-mode Norton commander clone useful for getting an overview of the file system copying/moving files has an integrated text viewer/editor which is a bit more friendly than vi and much more. It's not installed by default on all systems though.  Bash is the default shell under linux and its job control features are nice. Bash supports fg (foreground) bg (background) and the classic '&' (run in the background). Use the jobs command to see the currently running process in the shell. Check out the bash man page for more info. I believe there is a job control section that discusses these commands. Keep in mind that most of the core bash commands (and much much more) are available on windows via cygwin. You can play around with all this command-line stuff without leaving the comfort of your windows environment if you choose. Once you become versed in the fundamental unix shell tools you will find that cygwin allows you to transfer these skills to windows systems.  Don't forget you can pipe things to the more command which will give you a single page of output and prompt you to continue. I use this most often with ls -la | more (emulates the standard dir command) and I also sometimes pipe (|) grep searches to more.  A more philosophical answer: Unix (in all flavors) is built on a single big idea: Difficult problems can be solved by solving easy problems first. You might recognize this as the Bottom-Up approach. But Unix takes this philosophy to an extreme. It's like living in a foreign country: you need to enculturate. For instance suppose you are working on a system that requires three separate servers. Every morning you type in three different commands to start them up. At some point (hopefully on day two) you decide it's silly to type those same three commands each time you want to start work. You might be tempted to write an application that controls those servers but that's not the Unix way. Instead you should put those three commands in a script and move on. Later when you are ready to go into production you want to show the startup process to your customer. It's a bit embarrassing to show off the startup script so you write a little GUI that has a startup button for the customer to press. That button simply calls the script you wrote back on day two. Problem solved! If that scenario strikes you as odd or horrifying you are still thinking like a Windows programmer. And in some ways it is horrifying: a little helper script has become part of production code almost by accident. But this sort of thing happens all the time on Unix systems and it mostly works. There's not much point in listing commands that you need. If you use the command line long enough you'll figure out what you need fairly quickly. Instead I'd focus on trying to do everything in the shell for a while. (I use ksh exclusively for writing scripts and bash for my interactive shell. They're similar but geared toward slightly different uses.) I don't believe anyone who does not understand pipelines can be said to be a Unix programmer. A fairly common idiom that I found helpful to grokking pipelines was the output of find piped into xargs. Right now for instance I need to remove a bunch of files that I own. First I find the files: $ find . -user jericson Then I pipe the results to xargs to remove them: $ find . -user jericson | xargs rm Be sure to look at the -i option for xargs as well. Note that I broke the problem into two smaller problems and solved each one separately. I actually ran the find command by itself to be sure I was looking at correct files. Then I piped the results to the second command as input. Using the pipeline avoids an intermediary file.  Let's see... the basics: cd ls mv cp mkdir rm cat man the good ones: ps chmod chgrp sudo (or su) pwd grep head tail xargs less find awk sed ssh/scp ping touch locate/updatedb ln kill echo wc passwd tar diff df du free reboot and the operators: | > >> && and || ... and all the useful associated command line options therein. I could probably come up with another 20 if I thought about it more.  I have found these mapping tables between Windows Shell (DOS) and Unix/Bash commands to be very useful: UNIX For DOS Users  You said ""Shell Access"" to the Linux Host right? So you will be making access from a standard Windows Desktop. The very first things you have to get are: PuTTY WinSCP and NotePad++ All of these 3 software are free extremely reliable and necessary for your survival. And the best part is these 3 are from separate vendors you can install these three separately yet these 3 get ""integrated"". PuTTY allows you to make the actual Shell Access WinSCP allows you to look at the files on the Linux box very conveniently and even transfer files fix their permissions etc. NotePad++ allows you to edit the text files while ensuring that you are not messing up the EOL characters. Besides comes with a host of plugins so you could edit files with a lot of syntax based colorization. Once you get hold of the above 3 you should quickly learn the use of ssh public & private keys & pagent. That will allow you to secure your servers so that access can be made to your servers without passwords but on the basis of ""keys"". I have seen quite a few Windows users find this password-less entry quite amazing and use this method myself. While on the Linux Command prompt the use of short-cuts like ^r are priceless use google to search them out. Depending upon the Linux distribution that you use you might need to quickly get the hang of APT or RPM. And to learn without messing up the real servers install one of the free virtualization software on your Windows Desktop and have fun installing and messing up Linux virtual machines. There's hardly any difference between a physical linux box and a virtualized system. Atleast none AFAIK from the O/S learning perspective. It will make it very easy to learn things like configuring network disks services software etc.  Oh I also use less to look at log files. It's a WHOLE LOT BETTER if you know some shortcuts (these also work in vim) /regex - Search for the next text which matches regex n - Jump to the next match for the regex you just typed N - Jump to previous match ESC - Cancel searches and get yourself out of trouble g - Jump to start of file G - Jump to end of file 72g - Jump to line 72  In Windows when you run a program by typing just the program name (without the path) Windows will search the current working directory and then the PATH. In Linux programs must be in your path for you to execute them without specifying the path (absolute or relative). So if you want to run a program that's in the current directory you have a couple of options (assuming the program name is program): Run ./program. Add ""."" to the PATH then run program This was got me the first few days/weeks I started using Linux.  A lot of the posts here are really great. Here are my favorites whenver I am asked this: Get a book on bash scripting. In there it will walk you through most of the basics of the command line as well as give you a feel for what you will need moving forward. Read through a book like 'Linux in a nutshell' which is just a listing of all the commands and how they are used. Go on IRC and joining #linuxhelp channel to ask questions. If you have no real attachment to windows: switch to linux. Don't just 'dual boot'- take your windows or other machine and put it in a corner and use linux only for a period of at least 6 months. When doing this don't look for the gui tools first.  Vim - If you haven't already you should definitely run through the vimtutor tutorial. It will help you really use all of the power that Vim provides. You can get usually access it just by typing 'vimutor' at the command line. ack - ack is replacement(sort of) for grep written in Perl providing a ton of features specifically for programmers. It is amazing and much easier to use than grep. It should come with every copy of Linux but sadly it doesn't. As a bonus it works on Windows too! git - the greatest version control system ever imho. Even if you are working by yourself the simple repository setup and branching/merging make git a great choice for version control. If you are unfamiliar with git the kernel.org tutorial is a decent place to start. !! - Repeats the previous command. For example if you wanted to delete some file that required sudo'ing to delete but forget to type the sudo you could do the following: > rm somefile # won't do anything since it needs root privileges to delete > sudo !! # the same as typing 'sudo rm somefile'  As a relatively new Linux user myself there are a few commands and tools that I have come to rely on. Find - I use find every day along with xargs. Find is very flexible and its useful to start out small and start building on top of that. Using xargs and find you can execute commands on the files you've found. I often use this to zip files I've recently edited or search through files using the next tool I'd suggest. grep - I use grep to search through log files and code mostly. Grep is a very powerful tool and can be used with Regular Expressions for very powerful searching. Grep is very powerful is fairly easy to start out with. Once you get comfortable with it keep adding more complex/specific searches to help get better using grep. Cron - Cron is a powerful scheduling service that lets you run commands scripts programs at specific times intervals etc. Cron is used to rotate logs run backups you name it. If you want to run something regularly learn how to use cron. A solid text editor - I use Vim as my text editor but this is more of a choice thing. I'd suggest that you try out several and find one you like the most and stick with it. I find its easiest to just skip the more simple editors and use more powerful ones so you can grow and learn the more advanced features as you get better. It also helps to learn some basic shell scripting and have a basic understanding of your distribution's file system.",linux bash unix shell,9.841261500915567E-5,0.022409405729660888,0.6110503726192786,0.3389377064039811,0.02750410263207018
227,"What's the best way to generate a tag cloud from an array using h1 through h6 for sizing? I have the following arrays: $artist = array(""the roots"" ""michael jackson"" ""billy idol"" ""more"" ""and more"" ""and_YET_MORE""); $count = array(5 3 9 1 1 3); I want to generate a tag cloud that will have artists with a higher number in $count enclosed in h6 tags and the lowest enclosed h1 tags. As a helper in Rails: def tag_cloud (strings counts) max = counts.max strings.map { |a| ""<span style='font-size:#{((counts[strings.index(a)] * 4.0)/max).ceil}em'>#{a}</span> "" } end Call this from the view: <%= tag_cloud($artists $counts) %> This outputs elements in an array that will be converted to a string in the view to ultimately render like so: <span style='font-size:3em'>the roots</span> <span style='font-size:2em'>michael jackson</span> <span style='font-size:4em'>billy idol</span> <span style='font-size:1em'>more</span> <span style='font-size:1em'>and more</span> <span style='font-size:2em'>and_YET_MORE</span> It would be better to have a 'class' attribute and reference the classes in a style sheet as mentioned by Brendan above. Much better than using h1-h6 semantically and there's less style baggage with a <span>. why did someone give it a -1?  This method is for SQL/PostgreSQL fanatics. It does the entire job in the database and it prints text with ""slugified"" link. It uses Doctrine ORM just for the sql call I'm not using objects. Suppose we have 10 sizes: public function getAllForTagCloud($fontSizes = 10) { $sql = sprintf(""SELECT count(tag) as tagcounttagslug floor((count(*) * %d )/(select max(t) from (select count(tag) as t from magazine_tag group by tag) t)::numeric(62)) as ranking from magazine_tag mt group by tagslug"" $fontSizes); $q = Doctrine_Manager::getInstance()->getCurrentConnection(); return $q->execute($sql); } then you print them with some CSS class from .tagranking10 (the best) to .tagranking1 (the worst): <?php foreach ($allTags as $tag): ?> <span class=""<?php echo 'tagrank'.$tag['ranking'] ?>""> <?php echo sprintf('<a rel=""tag"" href=""/search/by/tag/%s"">%s</a>' $tag['slug'] $tag['tag'] ); ?> </span> <?php endforeach; ?> and this is the CSS: /* put your size of choice */ .tagrank1{font-size: 0.3em;} .tagrank2{font-size: 0.4em;} .tagrank3{font-size: 0.5em;} /* go on till tagrank10 */ This method displays all tags. If you have a lot of them you probably don't want your tag cloud to become a tag storm. In that case you would append an HAVING TO clause to your SQL query: -- minimum tag count is 8 -- HAVING count(tag) > 7 That's all  Personally I would do something like this: <?php $data = array($rating[0] => array('word0' 'word1' 'word2') $rating[1] => array('word3' 'word4' 'word8'...)); //assums that $rating is an array with the weight of each word so the more popular words would have a higher value in rating usort($data); //sort the $data variable this should give us the most popular words first $size = '1'; foreach($data as $rank) { $i=0; while($i<$count($rank)) { echo ""<h"" . $size . "">"" . $rank[$i] . ""</h"" . $size . "">""; $i++; } $size++; } ?> Assuming I'm not a complete idiot this should work. But it is untested. Couple of things: lose the $ in front of count in the while loop; put $size++ inside the while loop. The way you have it now $count($rank) will be a fatal error and $size will not get incremented until everything is finished.  kevind wrote: @Ryan That's correct but it actually makes the tags with the least number larger. This code has been tested: I actually meant to mention this in my answer- the original poster specified higher frequencies in higher-number tags but HTML uses lower numbers for more significant headings. I wrote my code to spec. ;P If you consider the date of this answer you might notice that it occurred on the second day of the beta. This is long before the commenting feature was implemented. Comment on his answer next time please. Don't make a non-answer answer.  You will want to add a logarithmic function to it too. (taken from tagadelic my Drupal module to create tag clouds http://drupal.org/project/tagadelic): db_query('SELECT COUNT(*) AS count id name FROM ... ORDER BY count DESC'); $steps = 6; $tags = array(); $min = 1e9; $max = -1e9; while ($tag = db_fetch_object($result)) { $tag->number_of_posts = $tag->count; #sets the amount of items a certain tag has attached to it $tag->count = log($tag->count); $min = min($min $tag->count); $max = max($max $tag->count); $tags[$tag->tid] = $tag; } // Note: we need to ensure the range is slightly too large to make sure even // the largest element is rounded down. $range = max(.01 $max - $min) * 1.0001; foreach ($tags as $key => $value) { $tags[$key]->weight = 1 + floor($steps * ($value->count - $min) / $range); } Then in your view or template: foreach ($tags as $tag) { $output .= ""<h$tag->weight>$tag->name</h$tag->weight>"" } nice one! ;)  @Ryan That's correct but it actually makes the tags with the least number larger. This code has been tested: $artist = array(""the roots""""michael jackson""""billy idol""""more""""and more""""and_YET_MORE""); $count = array(539113); $highest = max($count); for ($x = 0; $x < count($artist); $x++) {  $normalized = ($highest - $count[$x]+1) / $highest;  $heading = ceil($normalized * 6); // 6 heading types  echo ""<h$heading>{$artist[$x]}</h$heading>""; }  Perhaps this is a little academic and OT but hX tags are probably not the best choice for a tag cloud for reasons of document structure and all that sort of thing ... Maybe spans or an ol with appropriate class attributes (plus some css)?  Have used this snippet for a while credit is prism-perfect.net. Doesnt use H tags though <div id=""tags""> <div class=""title"">Popular Searches</div> <?php // Snippet taken from [prism-perfect.net] include ""/path/to/public_html/search/settings/database.php""; include ""/path/to/public_html/search/settings/conf.php""; $query = ""SELECT query AS tag COUNT(*) AS quantity FROM sphider_query_log WHERE results > 0 GROUP BY query ORDER BY query ASC LIMIT 10""; $result = mysql_query($query) or die(mysql_error()); while ($row = mysql_fetch_array($result)) { $tags[$row['tag']] = $row['quantity']; } // change these font sizes if you will $max_size = 30; // max font size in % $min_size = 11; // min font size in % // get the largest and smallest array values $max_qty = max(array_values($tags)); $min_qty = min(array_values($tags)); // find the range of values $spread = $max_qty - $min_qty; if (0 == $spread) { // we don't want to divide by zero $spread = 1; } // determine the font-size increment // this is the increase per tag quantity (times used) $step = ($max_size - $min_size)/($spread); // loop through our tag array foreach ($tags as $key => $value) { // calculate CSS font-size // find the $value in excess of $min_qty // multiply by the font-size increment ($size) // and add the $min_size set above $size = $min_size + (($value - $min_qty) * $step); // uncomment if you want sizes in whole %: // $size = ceil($size); // you'll need to put the link destination in place of the /search/search.php... // (assuming your tag links to some sort of details page) echo '<a href=""/search/search.php?query='.$key.'&search=1"" style=""font-size: '.$size.'px""'; // perhaps adjust this title attribute for the things that are tagged echo ' title=""'.$value.' things tagged with '.$key.'""'; echo '>'.$key.'</a> '; // notice the space at the end of the link } ?> </div> This seems a good approach to me. If your data is in an array just skip the database part. I'd recommend you store the artist name & count in a single associative array. To make that work with the above code use something like: $tags = array(""the roots"" => 5""michael jackson"" = 3""billy idol"" => 9""madonna"" => 1); I would agree don't use H tags since it messes up your semantics. Spans would be my choice. Finally a helper exists in Zend Framework that may just do what you need. See http://framework.zend.com/manual/en/zend.tag.html sorry no formatting in the above comment it seems!  Off the top of my head... $artist = array(""the roots""""michael jackson""""billy idol""""more""""and more""""and_YET_MORE""); $count = array(539113); $highest = max($count); for (int $x = 0; x < count($artist); $x++) {  $normalized = $count[$x] / $highest;  $heading = ceil($normalized * 6); // 6 heading types  echo ""<h"".$heading."">"".$artist[$x].""</h"".$heading."">""; }  I actually meant to mention this in my answer- the original poster specified higher frequencies in higher-number tags but HTML uses lower numbers for more significant headings. I wrote my code to spec. ;P AH I see now it seemed counter-intuitive to me that the higher the count the smaller the tag? My mistake. @TheSoftwareJedi Comments did not exist on StackOverflow before Sept. 2008. DOH! Thanks I forget... Comment on his answer next time please. Don't make a non-answer answer",php arrays cloud tag-cloud,2.3311809822756683E-4,0.8746591803328855,4.904988025031354E-4,0.002752680477806987,0.1218645222885768
4891,"What point should someone decide to switch Database Systems When developing whether its Web or Desktop at which point should a developer switch from SQLite MySQL MS SQL etc BrianLy hit the nail on the head but I'd also add that you may end up using different databases at different levels of development. It's not uncommon for developers to use SQLite on their workstation when they're coding against their personal development server and then have the staging and/or production sites using a different database tool. Of course if you're using extensions or capabilities specific to a certain database tool (say PostGIS in PostGreSQL) then obviously that wouldn't work.  It depends on what you are doing. You might switch if: You need more scalability or better performance - say from SQLite to SQL Server or Oracle. You need access to more specific datatypes. You need to support a customer that only runs a particular database. You need better DBA tools. Your application is using a different platform where your database no longer runs or it's libraries do not run. There are many more reasons for switching and it all depends on your requirements.  You should switch databases at milestone 2.3433 3ps prior to the left branch of dendrite 8151215. You should switch databases when you have a reason to do so would be my advice. If your existing database is performing to your expectations supports the load that is being placed on it by your production systems has the features you require in your applications and you aren't bored with it why change? However if you find your application isn't scaling or you are designing an application that has high load or scalability requirements and your research tells you your current database platform is weak in that area or as was already mentioned you need some spatial analysis or feature that a particular database has well there you go. Another consideration might be taking up the use of a database agnostic ORM tool that can allow you to experiment freely with different database platforms with a simple configuration setting. That was the trigger for us to consider trying out something new in the DB department. If our application can handle any DB the ORM can handle why pay licensing fees on a commercial database when an open source DB works just as well for the levels of performance we require? The bottom line though is that with databases or any other technology I think there are no ""business rules"" that will tell you when it is time to switch - your scenario will tell you it is time to switch because something in your solution won't be quite right and if you aren't at that point no need to change.",sql database,0.0011486564598548494,0.012322818257605755,0.025074701401567586,0.4440623642682994,0.5173914596126723
4393,"Drop all tables whose names begin with a certain string I'd like a script to drop all tables whose name begins with a given string. I'm sure this can be done with some dynamic sql and the INFORMATION_SCHEMA tables. If anyone has a script or can knock one up quickly please post it. If no-one posts an answer before I figure it out myself I'll post my solution. SELECT 'if object_id(''' + TABLE_NAME + ''') is not null begin drop table ""' + TABLE_NAME + '"" end;' FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME LIKE '[prefix]%'  CREATE PROCEDURE usp_GenerateDROP @Pattern AS varchar(255) @PrintQuery AS bit @ExecQuery AS bit AS BEGIN DECLARE @sql AS varchar(max) SELECT @sql = COALESCE(@sql '') + 'DROP TABLE [' + TABLE_NAME + ']' + CHAR(13) + CHAR(10) FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME LIKE @Pattern IF @PrintQuery = 1 PRINT @sql IF @ExecQuery = 1 EXEC (@sql) END  SELECT 'DROP TABLE ""' + TABLE_NAME + '""' FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME LIKE '[prefix]%' This will generate a script. I might add to remove the brackets when replacing ""prefix"" with your target prefix. MYSQL: SELECT concat('DROP TABLE 'TABLE_NAME"";"") as data FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME LIKE '[prefix]%' --- for those who like me found this thread The result contains also views  Xenph Yan's answe was far cleaner than mine but here is mine all the same. declare @startStr as Varchar (20) set @startStr = 'tableName' declare @startStrLen as int select @startStrLen = LEN(@startStr) select 'DROP TABLE ' + name from sysobjects where type = 'U' and left(name@startStrLen) = @startStr Just change tableName to the characters that you want to search with.  You may need to modify the query to include the owner if there's more than one in the database. declare @cmd varchar(4000) declare cmds cursor for select 'drop table [' + Table_Name + ']' from INFORMATION_SCHEMA.TABLES where Table_Name like 'prefix%' open cmds while 1=1 begin fetch cmds into @cmd if @@fetch_status != 0 break exec(@cmd) end close cmds; deallocate cmds This is cleaner than using a two-step approach of generate script plus run. But one advantage of the script generation is that it gives you the chance to review the entirety of what's going to be run before it's actually run. I know that if I were going to do this against a production database I'd be as careful as possible. In SQL Server 2005 I had to change the last two lines to `close cmds; deallocate cmds`. You may have to run this script several times because of foreign key constraints between master and detail tables.  select 'DROP TABLE ' + name from sysobjects where type = 'U' and sysobjects.name like '%test%' -- Test is the table name What about FKs? this doesn't actually execute anything just return a bunch of commands.  Thanks Curt that's the same sort of solution that I was midway through myself. Yours is nicer than mine though - it lends itself to easy modification. I added a union to the select and wiped out some views as well ;) declare @cmd varchar(4000) declare cmds cursor for Select 'drop table [' + Table_Name + ']' From INFORMATION_SCHEMA.TABLES Where Table_Name like 'prefix%' union Select 'drop view [' + Table_Name + ']' From INFORMATION_SCHEMA.VIEWS Where Table_Name like 'prefix%' open cmds while 1=1 begin  fetch cmds into @cmd  if @@fetch_status != 0 break  exec(@cmd) end close local deallocate local Don't worry it's not a production database - this is just for easy clean-up of my dev db while I try stuff out.  This will get you the tables in foreign key order and avoid dropping some of the tables created by SQL Server :) . The t.Ordinal value will slice the tables into dependency layers. WITH TablesCTE(SchemaName TableName TableID Ordinal) AS ( SELECT OBJECT_SCHEMA_NAME(so.object_id) AS SchemaName OBJECT_NAME(so.object_id) AS TableName so.object_id AS TableID 0 AS Ordinal FROM sys.objects AS so WHERE so.type = 'U' AND so.is_ms_Shipped = 0 AND TableName LIKE 'MyPrefix%' UNION ALL SELECT OBJECT_SCHEMA_NAME(so.object_id) AS SchemaName OBJECT_NAME(so.object_id) AS TableName so.object_id AS TableID tt.Ordinal + 1 AS Ordinal FROM sys.objects AS so INNER JOIN sys.foreign_keys AS f ON f.parent_object_id = so.object_id AND f.parent_object_id != f.referenced_object_id INNER JOIN TablesCTE AS tt ON f.referenced_object_id = tt.TableID WHERE so.type = 'U' AND so.is_ms_Shipped = 0 AND TableName LIKE 'MyPrefix%' ) SELECT DISTINCT t.Ordinal t.SchemaName t.TableName t.TableID FROM TablesCTE AS t INNER JOIN ( SELECT itt.SchemaName AS SchemaName itt.TableName AS TableName itt.TableID AS TableID Max(itt.Ordinal) AS Ordinal FROM TablesCTE AS itt GROUP BY itt.SchemaName itt.TableName itt.TableID ) AS tt ON t.TableID = tt.TableID AND t.Ordinal = tt.Ordinal ORDER BY t.Ordinal DESC t.TableName ASC Thanks to http://stackoverflow.com/questions/352176/sqlserver-how-to-sort-table-names-ordered-by-their-foreign-key-dependency",sql sql-server scripting dynamic-sql,4.4417434213395163E-4,0.001844592370136615,9.34577729381793E-4,0.03152955276933253,0.9652471027890152
4303,"Why should I practice Test Driven Development and how should I start? Lots of people talk about writing tests for their code before they start writing their code. This practice is generally known as Test Driven Development or TDD for short. What benefits do I gain from writing software this way? How do I get started with this practice? Please see my answer to the same question already asked. [A elaborated answer to this question](http://stackoverflow.com/questions/62625/how-do-you-know-what-to-test-when-writing-unit-tests#69259) Benefits You figure out how to compartmentalize your code You figure out exactly what you want your code to do You know how it supposed to act and down the road if refactoring breaks anything Gets you in the habit of making sure your code always knows what it is supposed to do Getting Started Just do it. Write a test case for what you want to do and then write code that should pass the test. If you pass your test great you can move on to writing cases where your code will always fail (2+2 should not equal 5 for example). Once all of your tests pass write your actual business logic to do whatever you want to do. If you are starting from scratch make sure you find a good testing suite that is easy to use. I like PHP so PHPUnit or SimpleTest work well. Almost all of the popular languages have some xUnit testing suite available to help build and automate testing. By the way ""compartmentalize your code"" lead you to a very good architecture ""for free"". When you break your codes in pieces to test it you end up with a better architecture. It is quite free if you have a bit of experience as software architect.  Good starter: Getting Started with Tdd in Java using Eclipse by Brett L. Schuchert Is a set of screencasts about TDD in Java and in C#. It is starting from the scratch and teaching basics of TDD.  In my opinion the single greatest thing is that it clearly allows you to see if your code does what it is supposed to. This may seem obvious but it is super easy to run astray of your original goals as I have found out in the past :p  The benefits part has recently been covered as for where to start....on a small enterprisey system where there aren't too many unknowns so the risks are low. If you don't already know a testing framework (like NUnit) start by learning that. Otherwise start by writing your first test :)  There are a lot of benefits: You get immediate feedback on if your code is working so you can find bugs faster By seeing the test go from red to green you know that you have both a working regression test and working code You gain confidence to refactor existing code which means you can clean up code without worrying what it might break At the end you have a suite of regression tests that can be run during automated builds to give you greater confidence that your codebase is solid The best way to start is to just start. There is a great book by Kent Beck all about Test Driven Development. Just start with new code don't worry about old code... whenever you feel you need to refactor some code write a test for the existing functionality then refactor it and make sure the tests stay green. Also read this great article. the link to the last article (Tips for Unit Testing) expired. Here is the link to the new article: http://devver.wordpress.com/2008/07/07/tips-for-unit-testing/",testing tdd,9.914134612279224E-4,0.004117197984570331,0.0020860118507104388,0.9764762330117187,0.016329143691772578
4824,"String.indexOf function in C Is there a C library function that will return the index of a character in a string? So far all I've found are functions like strstr that will return the found char * not it's location in the original string. If you are not totally tied to pure C and can use string.h there is strchr() See here I'm not sure what you mean by ""pure C"" but strchr() and string.h have been part of Standard C for 20 years.  You can use strstr to accomplish what you want. Example: char *a = ""Hello World!""; char *b = strstr(a ""World""); int position = b - a; printf(""the offset is %i\n"" position); This produces the result: the offset is 6 UB if there is no match (subtraction of pointers not within the same object).  strstr returns a pointer to the found character so you could use pointer arithmetic: (Note: this code not tested for its ability to compile it's one step away from pseudocode.) char * source = ""test string""; /* assume source address is */ /* 0x10 for example */ char * found = strstr( source ""in"" ); /* should return 0x18 */ if (found != NULL) /* strstr returns NULL if item not found */ { int index = found - source; /* index is 8 */ /* source[8] gets you ""i"" */ } Undefined behavior if the character is not found. Also you should not use `strstr` (with or without the typo) if `strchr` would suffice. @R.. - I fixed the typo and added error checking to the pseudocode. Also `strchr` does not work for finding multi-character sequences does it? Indeed but OP asked for a character not a substring. Of course if you interpret character broadly to include multibyte characters `strstr` is the right function to use. @R.. - Since the OP was talking about a character as in text not as in the `char` storage name I gave a solution that has a hope of working for a character like 輪. You object? No objection. :-)  I think that size_t strcspn ( const char * str1 const char * str2 ); is what you want. Here is an example pulled from here: /* strcspn example */ #include <stdio.h> #include <string.h> int main () { char str[] = ""fcba73""; char keys[] = ""1234567890""; int i; i = strcspn (strkeys); printf (""The first number in str is at position %d.\n""i+1); return 0; } The above comment is very important - doing it as in the example DOES NOT mimic indexOf() so the original answer is pretty much incorrect. `strcspn` is more like `String.IndexOfAny()` - searching for any of the characters in the `keys` array. But yeah it'll do the trick  EDIT: strchr is better only for one char. Pointer aritmetics says ""Hellow!"": char *pos = strchr (myString '#'); int pos = pos ? pos - myString : -1; Important: strchr () returns NULL if no string is found did you run out of variable names? ;-)",c string,0.6925157704494362,0.013115253704943748,0.01864887031001855,0.19931641168962935,0.0764036938459722
382,What is the meaning of the type safety warning in certain Java generics casts? What is the meaning of the Java warning? Type safety: The cast from Object to List is actually checking against the erased type List I get this warning when I try to cast an Object to a type with generic information such as in the following code: Object object = getMyList(); List<Integer> list = (List<Integer>) object; If you want to get rid of the yellow underline in Eclipse but don't want to add `@SuppressWarning` you have the option to ignore the warning in the preferences. In the preferences window go to _Java > Compiler > Errors/Warnings_ then under the _Generic types_ section change the option **Unchecked generic type operation** to **Ignore** This warning is there because Java is not actually storing type information at run-time in an object that uses generics. Thus if object is actually a List<String> there will be no ClassCastException at run-time except until an item is accessed from the list that doesn't match the generic type defined in the variable. This can cause further complications if items are added to the list with this incorrect generic type information. Any code still holding a reference to the list but with the correct generic type information will now have an inconsistent list. To remove the warning try: List<?> list = (List<?>) object; However note that you will not be able to use certain methods such as add because the compiler doesn't know if you are trying to add an object of incorrect type. The above will work in a lot of situations but if you have to use add or some similarly restricted method you will just have to suffer the yellow underline in Eclipse (or a SuppressWarning annotation).,java generics warnings casting type-safety,0.03236079311058932,0.8483343743124694,0.0032853060087903643,0.0903023985131442,0.025717128055006627
429,How do you schedule tasks in Windows? I run into a situation every couple of months where I need to develop a solution that's scheduled to run periodically but I haven't found a way to handle this in Windows that I'm entirely happy with. Every time I do it I end up feeling corralled into using a Windows Service which seems to be an unacceptable amount of overhead for something I could do with a cron job in Unix. Am I missing something here? What have you used to schedule an application in Windows? @Karl Yes I am and I should have been clearer about something. I've been steered away from Scheduled Tasks from the people I work with towards Services but have never really gotten a clear reason for this. I haven't been a heavy Windows user in about 10 years and have only recently started developing for the platform so I'm rather unfamiliar with the ups and downs of certain parts of it. @sasb The at command is new to me and seems like it may be what I'm looking for. The links suggest it's for Windows 2000 and that it's been superseded by SCHTASKS. I'm guessing this is the command line window into the Scheduled Tasks piece in Windows. I've found the timer library from this codeproject page extremely useful. It offers a wide variety of flexible schedule patterns include composable ones that efficiently use one timer. It seems to scale well since I've had hundereds of timers running off it. Also it's easy to add tasks with very customizable schedules (e.g. every tuesday between 10am and 7pm every 15 seconds).  Hate to point out the obvious but you are familiar with Windows Scheduled Tasks right? How To Schedule Tasks in Windows XP How to use the Windows Task Scheduler A scheduled task is really the way to go for programs that start execute and stop (like running a backup script every day at 2am). Services are meant for programs that always run in the background (like IIS or SQL Server). The nice thing about a schedule task is it'll run any old executable on your given schedule. You don't need to do anything special (except make sure the executable runs without user interaction). Services require a special type of program and typically require that you code extra-carefully as this will run forever so even a small memory leak can lead to problems over time. Scheduled tasks are like crons.  Use the at command AT.exe How To Use the AT Command to Schedule Tasks SCHTASKS is a CLI for Windows Scheduled Tasks.  You could use Task Scheduler API which comes with Windows: Task Scheduler Reference,windows windows-services scheduled-tasks,0.007711194761046625,0.01761390400020068,0.7250115898070253,0.21951051187665435,0.030152799555073054
2134,"Do sealed classes really offer performance Benefits? I have come across a lot of optimization tips which say that you should mark your classes as sealed to get extra performance benefits. I ran some tests to check the performance differential and found none. Am I doing something wrong? Am I missing the case where sealed classes will give better results? Has anyone run tests and seen a difference? Help me learn :) I don't think sealed classes were intended to give a perf increase. The fact that they do may be incidental. In addition to that profile your app after you've refactored it to use sealed classes and determine if it was worth the effort. Locking down your extensibility to make an unneeded micro-optimization will cost you in the long run. Of course if you profiled and it lets you hit your perf benchmarks (rather than perf for the sake of perf) then you can make a decision as a team if it is worth the money spent. If you have sealed classes for non-perf reasons then keep em :) Have you tried with reflection? I read somewhere that instantiating by reflection is faster with sealed classes See this related question on the same topic regarding Java/JVM: http://stackoverflow.com/questions/3961881/why-defining-class-as-final-improves-jvm-performance To my knowledge there is none. Sealed is there for a different reason - to block extensibility which may be usefull / needed in a lot of cases. Performance optimization was not a goal here. ... but if you think about the compiler: if your class is sealed you know the address of a method you call on your class at compile time. If your class isn't sealed you have to resolve the method at run-time because you might need to call an override. It's certainly going to be negligible but I could see that there would be *some* difference. Yes but that sort of does not translate into real benefits as the OP has pointed out. Architectural differences are / may be a lot more relevant. Seals clas should provide perf-benefit only in presence of virtual/override method. And if the JIT have some inline caching mechanism it is less perf-benefit. Sealed classes should provide a performance improvement. Since a sealed class cannot be derived any virtual members can be turned into non-virtual members. Of course we're talking really small gains. I wouldn't mark a class as sealed just to get a performance improvement unless profiling revealed it to be a problem. They *should* but it appears that they do not. If the CLR had the concept of non-null reference types then a sealed class really would be better as the compiler could emit `call` instead of `callvirt`... I'd love non-null reference types for many many other reasons too... sigh :-(  I consider ""sealed"" classes the normal case and I ALWAYS have a reason to omit the ""sealed"" keyword. The most important reasons for me are: a) Better compile time checks (casting to interfaces not implemented will be detected at compile time not only at runtime) and top reason: b) Abuse of my classes is not possible that way I wish Microsoft would have made ""sealed"" the standard not ""unsealed"".  Run this code and you'll see that sealed classes are 2 times faster: class Program { static void Main(string[] args) { Console.ReadLine(); var watch = new Stopwatch(); watch.Start(); for (int i = 0; i < 10000000; i++) { new SealedClass().GetName(); } watch.Stop(); Console.WriteLine(""Sealed class : {0}"" watch.Elapsed.ToString()); watch.Start(); for (int i = 0; i < 10000000; i++) { new NonSealedClass().GetName(); } watch.Stop(); Console.WriteLine(""NonSealed class : {0}"" watch.Elapsed.ToString()); Console.ReadKey(); } } sealed class SealedClass { public string GetName() { return ""SealedClass""; } } class NonSealedClass { public string GetName() { return ""NonSealedClass""; } } output: Sealed class : 00:00:00.1897568 NonSealed class : 00:00:00.3826678 RuslanG You forgot to call watch.Reset() after running first test. o_O :] There's a couple of problems with this. First off you're not resetting the stopwatch between the first and second test. Second the way you're calling the method means that all op codes will be call not callvirt so the type doesn't matter.  sealed classes will be at least a tiny bit faster but sometimes can be waayyy faster... if the JIT Optimizer can inline calls that would have otherwise been virtual calls. So where there's oft-called methods that are small enough to be inlined definitely consider sealing the class. However the best reason to seal a class is to say ""I didn't design this to be inherited from so I'm not going to let you get burned by assuming it was designed to be so and I'm not going to burn myself by getting locked into an implementation because I let you derive from it."" I know some here have said they hate sealed classes because they want the opportunity to derive from anything... but that is OFTEN not the most maintainable choice... because exposing a class to derivation locks you in a lot more than not exposing all that. Its similar to saying ""I loathe classes that have private members... I often can't make the class do what I want because I don't have access."" Encapsulation is important... sealing is one form of encapsulation. The C# compiler will still use `callvirt` (virtual method call) for instance methods on sealed classes because it still has to do a null-object check on them. Regarding inlining the CLR JIT can (and does) inline virtual method calls for both sealed and non-sealed classes... so yeah. The performance thing is a myth.  As I know there is no guarantee of performance benefit. But there is a chance to decrease performance penalty under some specific condition with sealed method. (sealed class makes all methods to be sealed.) But it's up to compiler implementation and execution environment. Details Many of modern CPUs use long pipeline structure to increase performance. Because CPU is incredibly faster than memory CPU has to prefetch code from memory to accelerate pipeline. If the code is not ready at proper time the pipelines will be idle. There is a big obstacle called dynamic dispatch which disrupts this 'prefetching' optimization. You can understand this as just a conditional branching. // Value of `v` is unknown // and can be resolved only at runtime. // CPU cannot know code to prefetch // so just prefetch one of a() or b(). // This is *speculative execution*. int v = random(); if (v==1) a(); else b(); CPU cannot prefetch next code to execute in this case because the next code position is unknown until the condition is resolved. So this makes hazard causes pipeline idle. And performance penalty by idle is huge in regular. Similar thing happen in case of method overriding. Compiler may determine proper method overriding for current method call but sometimes it's impossible. In this case proper method can be determined only at runtime. This is also a case of dynamic dispatch and a main reason of dynamically-typed languages are generally slower than statically-typed languages. Some CPU (including recent Intel's x86 chips) uses technique called speculative execution to utilize pipeline even on the situation. Just prefetch one of execution path. But hit rate of this technique is not so high. And speculation failure causes pipeline stall which also makes huge performance penalty. (this is completely by CPU implementation. some mobile CPU is known as does not this kind of optimization to save energy) Basically C# is a statically compiled language. But not always. I don't know exact condition and this is entirely up to compiler implementation. Some compilers can eliminate possibility of dynamic dispatch by preventing method overriding if the method is marked as sealed. Stupid compilers may not. This is the performance benefit of the sealed. This answer (Why is processing a sorted array faster than an unsorted array?) is describing the branch prediction a lot better. One advantage of non virtual or sealed function is that they can be inlined in more situations. Pentium class CPUs prefetch indirect dispatch directly. Sometimes function pointer redirection is faster than if(unguessable) for this reason.  The answer is no sealed classes do not perform better than non-sealed. The issue comes down to the call vs callvirt IL op codes. Call is faster than callvirt and callvirt is mainly used when you don't know if the object has been subclassed. So people assume that if you seal a class all the op codes will change from calvirts to calls and will be faster. Unfortunately callvirt does other things that make it useful too like checking for null references. This means that even if a class is sealed the reference might still be null and thus a callvirt is needed. You can get around this (without needing to seal the class) but it becomes a bit pointless. Structs use call because they cannot be subclassed and are never null. See this question for more information: Call and callvirt @BrianKennedy Inlining doesn't provide that much of a performance boost. If it did everyone would be telling you to seal your classes. And most methods cannot be inlined due to complexity. Uhh... I realize this is old but this isn't quite right... the big win with sealed classes is when the JIT Optimizer can inline the call... in that case the sealed class can be a huge win. Why this answer is wrong. From Mono changelog: ""Devirtualization optimization for sealed classes and methods improving IronPython 2.0 pystone performance by 4%. Other programs can expect similar improvement [Rodrigo]."". Sealed classes can improve performance but as always it depends on situation. To anyone who thinks this is wrong - go test it. Any potential speed improvement will be minimal at best. @Smilediver It can improve performance but only if you have a bad JIT (no idea how good the .NET JITs are nowadays though - used to be quite bad in that regard). Hotspot for example will inline virtual calls and deoptimize later if necessary - hence you pay the additional overhead only if you actually subclass the class (and even then not necessarily). -1 The JIT does not necessarily have to generate the same machine code for the same IL opcodes. Null checking and virtual call are orthogonal and separate steps of callvirt. In the case of sealed types the JIT compiler can still optimise part of callvirt. The same goes when the JIT compiler can guarantee that a reference won’t be null. This should be the accepted answer. AFAIK the circumstances where `call` is used are: in the situation `new T().Method()` for `struct` methods for non-virtual calls to `virtual` methods (such as `base.Virtual()`) or for `static` methods. Everywhere else uses `callvirt`.  The JITter will sometimes use non-virtual calls to methods in sealed classes since there is no way they can be extended further. There are complex rules regarding calling type virtual/nonvirtual and I don't know them all so I can't really outline them for you but if you google for sealed classes and virtual methods you might find some articles on the topic. Note that any kind of performance benefit you would obtain from this level of optimization should be regarded as last-resort always optimize on the algorithmic level before you optimize on the code-level. Here's one link mentioning this: Rambling on the sealed keyword the 'rambling' link is interesting in that it sounds like technical goodness but is in fact nonsense. Read the comments on the article for more info. Summary: the 3 reasons given are Versioning Performance and Security/Predictability - [see next comment] [continued] Versioning only applies when there are no subclasses duh but extend this argument to every class and suddenly you have no inheritance and guess what the language is no longer object-oriented (but merely object-based)! [see next] [continued] the performance example is a joke: optimizing a virtual method call; why would a sealed class have a virtual method in the first place since it cannot be subclassed? Finally the Security/Predictability argument is clearly fatuous: 'you cannot use it so it's secure/predictable'. LOL! Did I mention that I loathe sealed classes? There's nothing worse than getting 95% done with an application and discovering that the one feature the customer wants that the library doesn't provide logically belongs in a subclass of a sealed base class. Grrrrr! @Steven A. Lowe - I think what Jeffrey Richter was trying to say in a slightly roundabout way is that if you leave your class unsealed you need to think about how derived classes can/will use it and if you don't have the time or inclination to do this properly then seal it as it's less likely to cause breaking changes in others' code in future. That's not nonsense at all it's good common sense. A sealed class might have a virtual method since it might derive from a class that declares it. When you then later declare a variable of the sealed descendant class and call that method the compiler might emit a direct call to the known implementation since it knows there is no way that this might be different than what's in the known-at-compile-time vtable for that class. As for sealed/unsealed that's a different discussion and I agree with the reasons for making classes sealed by default.  @Vaibhav what kind of tests did you execute to measure performance? I guess one would have to use Rotor and to drill into CLI and understand how a sealed class would improve performance. SSCLI (Rotor) SSCLI: Shared Source Common Language Infrastructure The Common Language Infrastructure (CLI) is the ECMA standard that describes the core of the .NET Framework. The Shared Source CLI (SSCLI) also known as Rotor is a compressed archive of the source code to a working implementation of the ECMA CLI and the ECMA C# language specification technologies at the heart of Microsoft’s .NET architecture. The test included creating a class hierarchy with some methods which were doing dummy work (string manipulation mostly). Some of these methods were virtual. They were calling each other here and there. Then calling these methods 100 10000 and 100000 times... and measuring the time elapsed. Then running these after marking the classes as sealed. and measuring again. No difference in them.  Marking a class sealed should have no performance impact. There are cases where csc might have to emit a callvirt opcode instead of a call opcode. However it seems those cases are rare. And it seems to me that the JIT should be able to emit the same non-virtual function call for callvirt that it would for call if it knows that the class doesn't have any subclasses (yet). If only one implementation of the method exists there's no point loading its address from a vtable—just call the one implementation directly. For that matter the JIT can even inline the function. It's a bit of a gamble on the JIT's part because if a subclass is later loaded the JIT will have to throw away that machine code and compile the code again emitting a real virtual call. My guess is this doesn't happen often in practice. (And yes VM designers really do aggressively pursue these tiny performance wins.) I wonder why this got a downvote.  <off-topic-rant> I loathe sealed classes. Even if the performance benefits are astounding (which I doubt) they destroy the object-oriented model by preventing reuse via inheritance. For example the Thread class is sealed. While I can see that one might want threads to be as efficient as possible I can also imagine scenarios where being able to subclass Thread would have great benefits. Class authors if you must seal your classes for ""performance"" reasons please provide an interface at the very least so we don't have to wrap-and-replace everywhere that we need a feature you forgot. Example: SafeThread had to wrap the Thread class because Thread is sealed and there is no IThread interface; SafeThread automatically traps unhandled exceptions on threads something completely missing from the Thread class. [and no the unhandled exception events do not pick up unhandled exceptions in secondary threads]. </off-topic-rant> @StevenA.Lowe: Is there any reason the same approach couldn't have been used in 1.1 (replacing `Action` with a custom delegate type that takes a parameter of type `Exception`)? Why would `SafeThread` need to be its own class? Why not just have a static factory method somewhere which accepts an `Action` for the thread to be run and an `Action` which gets run after it completes normally or abnormally? Such a method could simply return a `Thread`. @supercat: that would be one useful solution now. SafeThread was first written in .NET 1.1! @supercat: maybe didn't think about it - and i wanted exception-thrown and operation-completed events to hook into Since we're doing off-topic rants here just like you _loathe_ sealed classes I _loathe_ swallowed exceptions. There is nothing worse than when something goes titsup but the program carries on. JavaScript is my favourite. You make a change to some code and suddenly clicking a button does absolutely nothing. Great! ASP.NET and UpdatePanel is another one; seriously if my button handler throws it's a Big Deal and it needs to CRASH so that I know there's something that needs fixing! A button that does nothing is *more* useless than a button that brings up a crash screen! I don't seal my classes for performance reasons. I seal them for design reasons. Designing for inheritance is hard and that effort will be wasted most of the time. I totally agree about providing interfaces though - that's a *far* superior solution to unsealing classes. @[Jon Skeet]: do you write classes for libraries that other developers will be using? I don't think that desigining for inheritance is hard what am I missing? Also interfaces are great but they're no substitute for inheritance they're just better than nothing. Encapsulation is generally a better solution than inheritance. To take your specific thread example trapping thread exceptions breaks the Liskov Substitution Principle because you've changed the documented behaviour of the Thread class so even if you could derive from it it would not be reasonable to say that you could use SafeThread everywhere you could use Thread. In this case you would be better to encapsulate Thread into another class which has different documented behaviour which you are able to do. Sometimes things are sealed for your own good. @[Greg Beech]: opinion not fact - being able to inherit from Thread to fix a heinous oversight in its design is NOT a bad thing ;-) And I think you're overstating LSP - the provabable property q(x) in this case is 'an unhandled exception destroys the program' which is not a ""desirable property"" :-) -1 for not understanding that too much of code flexibility usually only leads to abuse. @Turing interesting set of assumptions; I'm guessing that you drive a slot-car ;-) No but I have had my share of crappy code where I enabled other developers to abuse my stuff by not sealing it or by allowing corner cases. Most of my code nowadays is assertions and other contract related stuff. And I'm quite open about the fact that I do this only to be a pain in the ***.  I made the following test program and then decompiled it using Reflector to see what MSIL code was emitted. public class NormalClass { public void WriteIt(string x) { Console.WriteLine(""NormalClass""); Console.WriteLine(x); } } public sealed class SealedClass { public void WriteIt(string x) { Console.WriteLine(""SealedClass""); Console.WriteLine(x); } } public static void CallNormal() { var n = new NormalClass(); n.WriteIt(""a string""); } public static void CallSealed() { var n = new SealedClass(); n.WriteIt(""a string""); } In all cases the C# compiler (Visual studio 2010 in Release build configuration) emits identical MSIL which is as follows:  L_0000: newobj instance void <NormalClass or SealedClass>::.ctor() L_0005: stloc.0 L_0006: ldloc.0 L_0007: ldstr ""a string"" L_000c: callvirt instance void <NormalClass or SealedClass>::WriteIt(string) L_0011: ret The oft-quoted reason that people say sealed provides performance benefits is that the compiler knows the class isn't overriden and thus can use call instead of callvirt as it doesn't have to check for virtuals etc. As proven above this is not true. My next thought was that even though the MSIL is identical perhaps the JIT compiler treats sealed classes differently? I ran a release build under the visual studio debugger and viewed the decompiled x86 output. In both cases the x86 code was identical with the exception of class names and function memory addresses (which of course must be different). Here it is // var n = new NormalClass(); 00000000 push ebp 00000001 mov ebpesp 00000003 sub esp8 00000006 cmp dword ptr ds:[00585314h]0 0000000d je 00000014 0000000f call 70032C33 00000014 xor edxedx 00000016 mov dword ptr [ebp-4]edx 00000019 mov ecx588230h 0000001e call FFEEEBC0 00000023 mov dword ptr [ebp-8]eax 00000026 mov ecxdword ptr [ebp-8] 00000029 call dword ptr ds:[00588260h] 0000002f mov eaxdword ptr [ebp-8] 00000032 mov dword ptr [ebp-4]eax // n.WriteIt(""a string""); 00000035 mov edxdword ptr ds:[033220DCh] 0000003b mov ecxdword ptr [ebp-4] 0000003e cmp dword ptr [ecx]ecx 00000040 call dword ptr ds:[0058827Ch] // } 00000046 nop 00000047 mov espebp 00000049 pop ebp 0000004a ret I then thought perhaps running under the debugger causes it to perform less aggressive optimization? I then ran a standalone release build executable outside of any debugging environments and used WinDBG + SOS to break in after the program had completed and view the dissasembly of the JIT compiled x86 code. As you can see from the code below when running outside the debugger the JIT compiler is more aggressive and it has inlined the WriteIt method straight into the caller. The crucial thing however is that it was identical when calling a sealed vs non-sealed class. There is no difference whatsoever between a sealed or nonsealed class. Here it is when calling a normal class: Normal JIT generated code Begin 003c00b0 size 39 003c00b0 55 push ebp 003c00b1 8bec mov ebpesp 003c00b3 b994391800 mov ecx183994h (MT: ScratchConsoleApplicationFX4.NormalClass) 003c00b8 e8631fdbff call 00172020 (JitHelp: CORINFO_HELP_NEWSFAST) 003c00bd e80e70106f call mscorlib_ni+0x2570d0 (6f4c70d0) (System.Console.get_Out() mdToken: 060008fd) 003c00c2 8bc8 mov ecxeax 003c00c4 8b1530203003 mov edxdword ptr ds:[3302030h] (""NormalClass"") 003c00ca 8b01 mov eaxdword ptr [ecx] 003c00cc 8b403c mov eaxdword ptr [eax+3Ch] 003c00cf ff5010 call dword ptr [eax+10h] 003c00d2 e8f96f106f call mscorlib_ni+0x2570d0 (6f4c70d0) (System.Console.get_Out() mdToken: 060008fd) 003c00d7 8bc8 mov ecxeax 003c00d9 8b1534203003 mov edxdword ptr ds:[3302034h] (""a string"") 003c00df 8b01 mov eaxdword ptr [ecx] 003c00e1 8b403c mov eaxdword ptr [eax+3Ch] 003c00e4 ff5010 call dword ptr [eax+10h] 003c00e7 5d pop ebp 003c00e8 c3 ret Vs a sealed class: Normal JIT generated code Begin 003c0100 size 39 003c0100 55 push ebp 003c0101 8bec mov ebpesp 003c0103 b90c3a1800 mov ecx183A0Ch (MT: ScratchConsoleApplicationFX4.SealedClass) 003c0108 e8131fdbff call 00172020 (JitHelp: CORINFO_HELP_NEWSFAST) 003c010d e8be6f106f call mscorlib_ni+0x2570d0 (6f4c70d0) (System.Console.get_Out() mdToken: 060008fd) 003c0112 8bc8 mov ecxeax 003c0114 8b1538203003 mov edxdword ptr ds:[3302038h] (""SealedClass"") 003c011a 8b01 mov eaxdword ptr [ecx] 003c011c 8b403c mov eaxdword ptr [eax+3Ch] 003c011f ff5010 call dword ptr [eax+10h] 003c0122 e8a96f106f call mscorlib_ni+0x2570d0 (6f4c70d0) (System.Console.get_Out() mdToken: 060008fd) 003c0127 8bc8 mov ecxeax 003c0129 8b1534203003 mov edxdword ptr ds:[3302034h] (""a string"") 003c012f 8b01 mov eaxdword ptr [ecx] 003c0131 8b403c mov eaxdword ptr [eax+3Ch] 003c0134 ff5010 call dword ptr [eax+10h] 003c0137 5d pop ebp 003c0138 c3 ret To me this provides solid proof that there cannot be any performance improvement between calling methods on sealed vs non-sealed classes... I think I'm happy now :-) Any reason why you posted this answer twice instead of closing the other as a duplicate? They look like valid duplicates to me although this isn't my area. What about ngen'd code?",.net optimization frameworks performance,0.902424549241836,0.011872397898523054,0.0010710798429733096,0.08422223389818274,4.0973911848485046E-4
328,"PHP Session Security What are some guidelines for maintaining responsible session security with PHP? There's information all over the web and it's about time it all landed in one place! This is pretty trivial and obvious but be sure to session_destroy after every use. This can be difficult to implement if the user does not log out explicitly so a timer can be set to do this. Here is a good tutorial on setTimer() and clearTimer().  One guideline is to call session_regenerate_id every time a session's security level changes. This helps prevent session hijacking.  You need to be sure the session data are safe. By looking at your php.ini or using phpinfo() you can find you session settings. session.savepath_ tells you where they are saved. Check the permission of the folder and of its parents. It shouldn't be public (/tmp) or be accessible by other websites on your shared server. Assuming you still want to use php session You can set php to use an other folder by changing session.savepath_ or save the data in the database by changing session.savehandler_ . You might be able to set session.savepath_ in your php.ini (some providers allow it) or for apache + modphp in a .htaccess file in your site root folder: phpvalue session.savepath ""/home/example.com/html/session"". You can also set it at run time with _sessionsavepath() . Check Chris Shiflett's tutorial or ZendSessionSaveHandler_DbTable to set and alternative session handler.  My two (or more) cents: Trust no one Filter input escape output (cookie session data are your input too) Avoid XSS (keep your HTML well formed take a look at PHPTAL or HTMLPurifier) Defense in depth Do not expose data There is a tiny but good book on this topic: Essential PHP Security by Chris Shiflett. On the home page of the book you will find some interesting code examples and sample chapters. You may use technique mentioned above (IP & UserAgent) described here: How to avoid identity theft +1 for XSS-prevention. Without that it's impossible to protect against CSRF and thus somebody can ""ride"" the session without even getting the session ID.  php.ini session.cookie_httponly = 1 change session name from default PHPSESSID eq Apache add header: X-XSS-Protection 1 Can you elaborate? httpd.conf -> Header set X-XSS-Protection ""1"" Be aware that `X-XSS-Protection` isn't really useful at all. In fact the protecting algorithm itself could actually be exploited making it worse than before.  The main problem with PHP sessions and security (besides session hijacking) comes with what environment you are in. By default PHP stores the session data in a file in the OS's temp directory. Without any special thought or planning this is a world readable directory so all of your session information is public to anyone with access to the server. As for maintaining sessions over multiple servers. At that point it would be better to switch PHP to user handled sessions where it calls your provided functions to CRUD (create read update delete) the session data. At that point you could store the session information in a database or memcache like solution so that all application servers have access to the data. Storing your own sessions may also be advantageous if you are on a shared server because it will let you store it in the database which you often times have more control over then the filesystem.  I would check both IP and User Agent to see if they change if ($_SESSION['user_agent'] != $_SERVER['HTTP_USER_AGENT']     || $_SESSION['user_ip'] != $_SERVER['REMOTE_ADDR']) {     //Something fishy is going on here? } I believe the user_agent can also change when toggling between compatibly mode in IE8. It's also very easy to fake. Yep but what about users that had static IP eq GSM and is changed every half hour. So stored IP in Session + host name WHEN IP != REMOTE_ADDR check host and compare hostanmes eq. 12.12.12.holand.nl-> when is holand.nl == true. But some host had IP based hostname Then need compare mask 88.99.XX.XX @jasondavis There is a browser called Chrome. IP can legitimately change if user is behind load-balanced proxy farm. And user_agent can change every time a user upgrades their browser. @scotts I agree with the IP part but for the browser upgrade you would set the session when they login so I don't see how they would upgrade there browser without creating a new session once they login again.  Using IP address isn't really the best idea in my experience. For example; my office has two IP addresses that get used depending on load and we constantly run into issues using IP addresses. Instead I've opted for storing the sessions in a separate database for the domains on my servers. This way no one on the file system has access to that session info. This was really helpful with phpBB before 3.0 (they've since fixed this) but it's still a good idea I think.  I set my sessions up like this- on the log in page: $_SESSION['fingerprint'] = md5($_SERVER['HTTP_USER_AGENT'] . PHRASE . $_SERVER['REMOTE_ADDR']); (phrase defined on a config page) then on the header that is throughout the rest of the site: session_start(); if ($_SESSION['fingerprint'] != md5($_SERVER['HTTP_USER_AGENT'] . PHRASE . $_SERVER['REMOTE_ADDR'])) { session_destroy(); header('Location: http://website login page/'); exit(); }  I think one of the major problems (which is being addressed in PHP 6) is register_globals. Right now one of the standard methods used to avoid register_globals is to use the $_REQUEST $_GET or $_POST arrays. The ""correct"" way to do it (as of 5.2 although it's a little buggy there but stable as of 6 which is coming soon) is through filters. So instead of: $username = $_POST[""username""]; you would do: $username = filter_input(INPUT_POST 'username' FILTER_SANITIZE_STRING); or even just: $username = filter_input(INPUT_POST 'username'); This has no relation to the question at all. Really? Then why in the accepted answer do they mention not to use register globals? Wouldn't as far as most run-of-the-mill developers are concerned register globals and form variable handling fall under the umbrella of ""sessions"" even if it isn't technically part of the ""session"" object? I agree this does not *fully* answer the question but it is definitely PART of the answer to the question. Again this fleshes out a bullet point in the accepted answer ""Don't use register globals"". This tells what to do instead. -1 This does not answer the question.  This session fixation paper has very good pointers where attack may come. See also session fixation page at Wikipedia.  There are a couple of things to do in order to keep your session secure: Use SSL when authenticating users or performing sensitive operations. Regenerate the session id whenever the security level changes (such as logging in). You can even regenerate the session id every request if you wish. Have sessions time out Don't use register globals Store authentication details on the server. That is don't send details such as username in the cookie. Check the $_SERVER['HTTP_USER_AGENT']. This adds a small barrier to session hijacking. You can also check the IP address. But this causes problems for users that have changing IP address due to load balancing on multiple internet connections etc (which is the case in our environment here). Lock down access to the sessions on the file system or use custom session handling For sensitive operations consider requiring logged in users to provide their authenication details again Don't regenerate session on every request. It's susceptible to race conditions and you'll lose session sooner or later. @grom Doesn't Chrome change it's user-agent automatically when it upgrades silently in the background while the user is using the browser? In this way you are blocking out real users for no real good reason. Don't forget that enhanced usability is also enhanced security. Using SSL only for some operations is not enough unless you have separate sessions for encrypted and unencrypted traffic. If you use single session over HTTPS and HTTP attacker will steal it on first non-HTTPS request. I agree with porneL. Also for number 6 if an attacker has your session id wouldn't they also have access to your user agent? If you regenerate the session id then the session id that an attacker steals on a non-HTTPS request is useless. -1 the user agent is trivial to spoof. What you are describing wastes code and is not a security system. Damn i wish i could give you another -1 for use of ssl. At no point can the cookie be leaked over http thats laid out in OWASP A3. @The Rook it may be a trivial barrier (the attacker can capture a victim's user-agent using their own site) and relies on security through obscurity but it is still one extra barrier. If the User-Agent HTTP was to change during the session use it would be extremely suspicious and most likely an attack. I never said you can use it alone. If you combine it with the other techniques you have a much more secure site. @grom I think its like putting a piece of scotch tape across your door and saying it will prevent people from breaking in. @The Rook yes the User Agent can be spoofed. Its just one small little barrier. And what do you mean by at no point the cookie can be leaked over http. Yes it can be stolen. http is plain text. @grom the only barrier is the one in your mind you are stopping no attack. If you're checking the user agent you'll block all requests from IE8 users when they toggle compatibility mode. See the fun I had tracking down this problem in my own code: http://serverfault.com/questions/200018/http-302-problem-on-ie7. I'm taking the user agent check out because it's such a trivial thing to spoof as others have said.  If you you use session_set_save_handler() you can set your own session handler. For example you could store your sessions in the database. Refer to the php.net comments for examples of a database session handler. DB sessions are also good if you have multiple servers otherwise if you are using file based sessions you would need to make sure that each webserver had access to the same filesystem to read/write the sessions.",security php,2.508017273649085E-4,0.007637800596443629,0.9157583618744756,0.01615400438818563,0.06019903141353029
13,"Determining a web user's time zone Is there a standard way for a Web Server to determine what time zone offset a user is in? Perhaps from an HTTP header? Or part of the user-agent description? You might consider making John Isaacks's the correct answer... His solution is a lot simpler to put it lightly. Ask the user. If you get the time zone from the user's computer and it is set wrong then what? Then the user probably doesn't care? I agree with catphive. The simplest answer should be The Answer @MartinArgerami: for posterity the only sane way of dealing with time is keeping everything in UTC until the last possible moment because it avoids risks of mixups (since everything is in UTC there's no need for most applications to be timezone _and DST_ aware). Dates and times should ideally only be translated to a local timezone during display. Even though OP asked for a server-side solution a client-side solution is much more appropriate. A simple way to do it is by using: new Date().getTimezoneOffset(); Why did you repost an identical answer (by John Isaacks) from 2 years ago: http://stackoverflow.com/a/1809974/836407 ? Also it only returns the *current* time zone offset - not the *time zone*. See [the timezone tag wiki](http://stackoverflow.com/tags/timezone/info).  I still have not seen a detailed answer here that gets the time zone. You shouldn't need to geocode by IP address or use PHP (lol) or incorrectly guess from an offset. Firstly a time zone is not just an offset from GMT. It is an area of land in which the time rules are set by local standards. Some countries have daylight savings and will switch on DST at differing times. It's usually important to get the actual zone not just the current offset. If you intend to store this timezone for instance in user preferences you want the zone and not just the offset. For realtime conversions it won't matter much. Now to get the time zone with javascript you can use this: >> new Date().toTimeString(); ""15:46:04 GMT+1200 (New Zealand Standard Time)"" //Use some regular expression to extract the time. However I found it easier to simply use this robust plugin which returns the Olsen formatted timezone: https://github.com/scottwater/jquery.detect_timezone  One possible option is to use the Date header field which is defined in RFC 7231 and is supposed to include the timezone. Of course it is not guaranteed that the value is really the client's timezone but it can be a convenient starting point.  With PHP date function you will get the date time of server on which site is located. The only way to get user time is to use JavaScript. But I suggest you to if your site have registration required then best way is to ask user while registration as compulsory field. You can list various time zones in register page and save that in database. After this if user login to site then you can set default time zone for that session as per users’ selected time zone. You can set any specific time zone using PHP function date_default_timezone_set. This set the specified time zone for users. Basically users’ time zone is goes to client side so we must use JavaScript for this. Below is the script to get users’ time zone using PHP and JavaScript. <?php #http://www.php.net/manual/en/timezones.php List of Time Zones function showclienttime() { if(!isset($_COOKIE['GMT_bias'])) { ?> <script type=""text/javascript""> var Cookies = {}; Cookies.create = function (name value days) { if (days) { var date = new Date(); date.setTime(date.getTime() + (days * 24 * 60 * 60 * 1000)); var expires = ""; expires="" + date.toGMTString(); } else { var expires = """"; } document.cookie = name + ""="" + value + expires + ""; path=/""; this[name] = value; } var now = new Date(); Cookies.create(""GMT_bias""now.getTimezoneOffset()1); window.location = ""<?php echo $_SERVER['PHP_SELF'];?>""; </script> <?php } else { $fct_clientbias = $_COOKIE['GMT_bias']; } $fct_servertimedata = gettimeofday(); $fct_servertime = $fct_servertimedata['sec']; $fct_serverbias = $fct_servertimedata['minuteswest']; $fct_totalbias = $fct_serverbias – $fct_clientbias; $fct_totalbias = $fct_totalbias * 60; $fct_clienttimestamp = $fct_servertime + $fct_totalbias; $fct_time = time(); $fct_year = strftime(""%Y"" $fct_clienttimestamp); $fct_month = strftime(""%B"" $fct_clienttimestamp); $fct_day = strftime(""%d"" $fct_clienttimestamp); $fct_hour = strftime(""%I"" $fct_clienttimestamp); $fct_minute = strftime(""%M"" $fct_clienttimestamp); $fct_second = strftime(""%S"" $fct_clienttimestamp); $fct_am_pm = strftime(""%p"" $fct_clienttimestamp); echo $fct_day."" "".$fct_month."" "".$fct_year."" ( "".$fct_hour."":"".$fct_minute."":"".$fct_second."" "".$fct_am_pm."" )""; } showclienttime(); ?> But as per my point of view it’s better to ask to the users if registration is mandatory in your project.  The magic all seems to be in visitortime.getTimezoneOffset() That's cool I didn't know about that. Does it work in IE etc? From there you should be able to use JS to ajax set cookies whatever. I'd probably go the cookie route myself. You'll need to allow the user to change it though. We tried to use geolocation (via maxmind) to do this a while ago and it was wrong reasonably often - enough to make it not worth doing so we just let the user set it in their profile and show a notice to users who haven't set theirs yet.  The most popular (==standard?) way of determining the time zone I've seen around is simply asking the users themselves. If your website requires subscription this could be saved in the users' profile data. For anon users the dates could be displayed as UTC or GMT or some such. I'm not trying to be a smart aleck. It's just that sometimes some problems have finer solutions outside of any programming context. What about when a user is downloading an .ics file that should have a start time specific to their location (e.g. 9-11am across the country)? They shouldn't HAVE to say what their time zone is imo. Why not accept this as the correct answer? The answer given by Unkwntech even not work for non-IE browsers. @Ishmaeel: but users do travel internationally and they shouldnt need to tell their timezone each time they login from some non-native timezone Personally if I traveled that much and the timestamps in my webapp were mission critical *I* wouldn't trust the webapp to guess my current timezone via JS/ActiveX gimmicks. I would tell the webapp to display the timestamps in UTC/GMT. I also wouldn't be changing the timezone settings on my laptop each time I got off the plane so the webapp would not have a chance to guess correctly anyway. This doesn't answer the question which clearly implies he's looking for a technological solution. @gWiz OP is asking for a standard solution. This is pretty standard. +1 - Definitely if you care and it matters to your app's functionality ask the user.  Don't use IP address to definitively determine location (and hense timezone)-- that's because with NAT Proxies (increasingly popular) and VPNs IP addresses do not necessarily realistically reflect the user's actual location but the location at which the servers implementing those protocols reside. Similar to how Area Codes are no longer useful for locating a telephone user given the popularity of Number Portability. IP and other techniques shown above are useful for suggesting a default that the user can adjust/correct.  Here is a better JS one (cross-browser): https://bitbucket.org/pellepim/jstimezonedetect  Using Unkwntech's approach I wrote a function using jQuery and PHP. This is tested and does work! On the PHP page where you are want to have the timezone as a variable have this snippet of code somewhere near the top of the page: <?php session_start(); $timezone = $_SESSION['time']; ?> This will read the session variable ""time"" which we are now about to create. On the same page in the  you need to first of all include jQuery: <script type=""text/javascript"" src=""http://code.jquery.com/jquery-latest.min.js""></script> Also in the  below the jQuery paste this: <script type=""text/javascript""> $(document).ready(function() { if(""<?php echo $timezone; ?>"".length==0){ var visitortime = new Date(); var visitortimezone = ""GMT "" + -visitortime.getTimezoneOffset()/60; $.ajax({ type: ""GET"" url: ""http://domain.com/timezone.php"" data: 'time='+ visitortimezone success: function(){ location.reload(); } }); } }); </script> You may or may not have noticed but you need to change the url to your actual domain. One last thing. You are probably wondering what the heck timezone.php is. Well it is simply this: (create a new file called timezone.php and point to it with the above url) <?php session_start(); $_SESSION['time'] = $_GET['time']; ?> If this works correctly it will first load the page execute the JavaScript and reload the page. You will then be able to read the $timezone variable and use it to your pleasure! It returns the current UTC/GMT time zone offset (GMT -7) or whatever timezone you are in. i do like this but i might have something that checks the current $_SESSION['time'] and only get the javascript to reload if its different  Try this php code <?php $ip = $_SERVER['REMOTE_ADDR']; $json = file_get_contents(""http://api.easyjquery.com/ips/?ip="".$ip.""&full=true""); $json = json_decode($jsontrue); $timezone = $json['LocalTimeZone']; ?> Dead link (now directs to a godaddy parking page)  Here is a robust JavaScript solution to determine the time zone the browser is in. >>> var timezone = jstz.determine(); >>> timezone.name(); ""Europe/London"" http://www.pageloom.com/automatic-timezone-detection-with-javascript IMHO - This is the best answer.  Easy just use the JavaScript getTimezoneOffset function like so: -new Date().getTimezoneOffset()/60; This is a Javascript function not a PHP function. Also it only returns the *current* time zone offset - not the *time zone*. See [the timezone tag wiki](http://stackoverflow.com/tags/timezone/info).  Here is a more complete way. Get the timezone offset for the user Test some days on DLS boundaries to determine if they are in a zone that uses DLS. Getting TZ and DST from JS This worked for me! Read the comments under the blog post for a couple updates to the code. This should be the accepted answer! This will still only return the *standard offset* for the time zone such as +02:00. It will not give you enough information to determine the *time zone* of the user such as `Africa/Johannesburg` or `Europe/Istanbul`. See [the timezone tag wiki](http://stackoverflow.com/tags/timezone/info).  Javascript is the easiest way to get the client's local time. I would suggest using an XMLHttpRequest to send back the local time and if that fails fall back to the timezone detected based on their IP address. As far as geolocation I've used MaxMind GeoIP on several projects and it works well though I'm not sure if they provide timezone data. It's a service you pay for and they provide monthly updates to your database. They provide wrappers in several web languages. I have voted this answer up because the latitude and longitude obtained from databases like GeoIP (which has a free version available as of now) can be combined with databases that convert such a coordinate to a time zone. I think GeoNames has a latter such database. See also [this question](http://stackoverflow.com/questions/5584602).  Here is an article (with source code) that explains how to determine and use localized time in an ASP.NET (VB.NET C#) application: It's About Time In short the described approach relies on the JavaScript getTimezoneOffset function which returns the value that is saved in the session cookie and used by code-behind to adjust time values between GMT and local time. The nice thing is that the user does not need to specify the time zone (the code does it automatically). There is more involved (this is why I link to the article) but provided code makes it really easy to use. I suspect that you can convert the logic to PHP and other languages (as long as you understand ASP.NET). The link is dead. I think this is the alternative link: http://www.devproconnections.com/article/aspnet2/it-s-about-time-122778 The article is available as PDF form here too: https://app.box.com/shared/bfvvmidtyg  To submit it as an HTTP header on AJAX requests with jQuery $.ajaxSetup({ beforeSend: function(xhr settings) { xhr.setRequestHeader(""X-TZ-Offset"" -new Date().getTimezoneOffset()/60); } }); This only returns the *current* time zone offset - not the *time zone*. See [the timezone tag wiki](http://stackoverflow.com/tags/timezone/info).  -new Date().getTimezoneOffset()/60; getTimezoneOffset() will subtract your time from GMT and return the number of minutes. So if you live in GMT-8 it will return 480. To put this into hours divide by 60. Also notice that the sign is the opposite of what you need -- it's calculating GMT's offset from your time zone not your time zone's offset from GMT. To fix this simply multiply by -1. What about users who use cell phones that have browsers without javascript support? I like the question the user asks about HTTP headers user agent... is there a way to make this work server side as accurate as possible? That's strange. For me this returns `-4` but I'm in `GET` which ought to be `+4` - what am I missing? I've checked that my OS is set to the correct timezone. getTimezoneOffset() returns number of minutes when you subtract your current time from GMT time. So if you live in California that's PST which is GMT-8 so you're 8 hours behind. Subtract that from GMT using getTimezoneOffset() and it will return 480 minutes. If you want to find your offset multiply that by -1 and divide by 60. This doesn't account for Daylight Saving Time / Summer Time but the link posted by Joseph Lust does: http://stackoverflow.com/a/5492192/462162 This doesn't always work for DST. Get timezone offset does exactly what it says. It get's the offset. A time ZONE is actually a geographical area. This won't work for daylight savings since you don't know which hemisphere the user lives in or if their country even has daylight savings. Why not use this instead: `>>> date.toTimeString() ""15:46:04 GMT+1200 (New Zealand Standard Time)""` Will this work for timezones like `GMT +1`? (Non-negative.) I beg to differ with Keyo. The definition of getTimezoneOffset() (according to the ECMA standard http://www.ecma-international.org/ecma-262/5.1/#sec-15.9.5.26) is ""Returns the difference between local time and UTC time in minutes."" - in other words it should take daylight savings into account. Mozilla's documentation says ""Daylight saving time prevents this value from being a constant even for a given locale."" @xgretsch: It gets the user's *current* offset from GMT. That's fine if you're presenting another time that occurs on the same day (unless the current date is a changeover date where it could be wrong). However there are many timezones which have the same offset from GMT and they may have different changeover dates or not use daylight savings. @Keyo: `toTimeString`'s output is implementation-defined: http://www.ecma-international.org/ecma-262/5.1/#sec-15.9.5.4. There is no guarantee that it will include a time zone name. There is also no guarantee that if it *does* include a time zone name that it maps to anything the server knows about. It may work in some browsers some of the time but I wouldn't like to rely on it. @Mike Dimmick: While true the question itself asks if there is a way to get the time zone offset not the actual time zone itself so this should be correct. @Mike Dimmick: Ah I see your point. I guess there isn't a simple answer to ""what's the user's time zone offset"" - you need to ask ""what's the user's time zone offset at a particular point in time"". I'm beginning to think that the only robust way of doing this is one of those horrible solutions where you set the time zone print out the time in local time and compare what you got! One thing to note here; some places (such as Newfoundland in Canada) have timezones that are off by half an hour so after dividing by 60 your answer may not be an integer. this is what makes the most sense to me. is there any reason this shouldn't be used or is everyone above just making the problem more difficult than they need to. @jordanstephens I am not an expert so I do not know if there are circumstances where this would not work but it worked fine for me.  Anyone know of any services that will match IP to geographic location Well lucky for you that answer can be found on our very own stackoverflow website: http://stackoverflow.com/questions/1033/ip-to-country spoiler: http://www.hostip.info/use.html @AdamLassek: possibly interesting information but can you explain why? @AndréCaron It says I'm in Irvine CA when in fact I am actually in Omaha NE. @AdamLassek OK so it works for you but you have nothing to support it being _totally_ wrong then. @AndréCaron It's totally wrong for me so I am advising skepticism of that site's accuracy. I just went to that website and it was totally wrong :-/  There are no HTTP headers that will report the clients timezone so far although it has been suggested to include it in the HTTP specification. If it was me I would probably try to fetch the timezone using clientside JavaScript and then submit it to the server using Ajax or something.  Here's how I do it. This will set the PHP default timezone to the user's local timezone. Just paste the following on the top of all your pages: <?php session_start(); if(!isset($_SESSION['timezone'])) { if(!isset($_REQUEST['offset'])) { ?> <script> var d = new Date() var offset= -d.getTimezoneOffset()/60; location.href = ""<?php echo $_SERVER['PHP_SELF']; ?>?offset=""+offset; </script> <?php } else { $zonelist = array('Kwajalein' => -12.00 'Pacific/Midway' => -11.00 'Pacific/Honolulu' => -10.00 'America/Anchorage' => -9.00 'America/Los_Angeles' => -8.00 'America/Denver' => -7.00 'America/Tegucigalpa' => -6.00 'America/New_York' => -5.00 'America/Caracas' => -4.30 'America/Halifax' => -4.00 'America/St_Johns' => -3.30 'America/Argentina/Buenos_Aires' => -3.00 'America/Sao_Paulo' => -3.00 'Atlantic/South_Georgia' => -2.00 'Atlantic/Azores' => -1.00 'Europe/Dublin' => 0 'Europe/Belgrade' => 1.00 'Europe/Minsk' => 2.00 'Asia/Kuwait' => 3.00 'Asia/Tehran' => 3.30 'Asia/Muscat' => 4.00 'Asia/Yekaterinburg' => 5.00 'Asia/Kolkata' => 5.30 'Asia/Katmandu' => 5.45 'Asia/Dhaka' => 6.00 'Asia/Rangoon' => 6.30 'Asia/Krasnoyarsk' => 7.00 'Asia/Brunei' => 8.00 'Asia/Seoul' => 9.00 'Australia/Darwin' => 9.30 'Australia/Canberra' => 10.00 'Asia/Magadan' => 11.00 'Pacific/Fiji' => 12.00 'Pacific/Tongatapu' => 13.00); $index = array_keys($zonelist $_REQUEST['offset']); $_SESSION['timezone'] = $index[0]; } } date_default_timezone_set($_SESSION['timezone']); //rest of your code goes here ?>  If you happen to be using OpenID for authentication Simple Registration Extension would solve the problem for authenticated users (You'll need to convert from tz to numeric). Another option would be to infer the time zone from the user agent's country preference. This is a somewhat crude method (won't work for en-US) but makes a good approximation.  javascript: function maketimus(timestampz) { var linktime = new Date(timestampz * 1000); var linkday = linktime.getDate(); var freakingmonths=new Array(); freakingmonths[0]=""jan""; freakingmonths[1]=""feb""; freakingmonths[2]=""mar""; freakingmonths[3]=""apr""; freakingmonths[4]=""may""; freakingmonths[5]=""jun""; freakingmonths[6]=""jul""; freakingmonths[7]=""aug""; freakingmonths[8]=""sep""; freakingmonths[9]=""oct""; freakingmonths[10]=""nov""; freakingmonths[11]=""dec""; var linkmonthnum = linktime.getMonth(); var linkmonth = freakingmonths[linkmonthnum]; var linkyear = linktime.getFullYear(); var linkhour = linktime.getHours(); var linkminute = linktime.getMinutes(); if (linkminute < 10) {linkminute = ""0"" + linkminute;} var fomratedtime = linkday + linkmonth + linkyear + "" "" + linkhour + "":"" + linkminute + ""h""; return fomratedtime; } simply provide your times in UNIX Timestamp format to this function javascript already knows the timezone of the user. like this: php: echo '<script type=""text/javascript""> var eltimio = maketimus('.$unix_timestamp_ofshiz.'); document.write(eltimio); </script><noscript>pls enable javascript</noscript>'; this will always show the times correctly based on the timezone the person has set on his computer clock no need to ask anything to anyone and save it into places thank god!  timezone.js: function ajaxpage() { var url = ""timezone.php""; var visitortime = new Date(); var time = visitortime.getTimezoneOffset()/60; var page_request = false; if (window.XMLHttpRequest) { page_request = new XMLHttpRequest(); } else if (window.ActiveXObject) { try { page_request = new ActiveXObject(""Msxml2.XMLHTTP""); } catch (e) { try{ page_request = new ActiveXObject(""Microsoft.XMLHTTP""); } catch (e) { } } } else { return false; } page_request.onreadystatechange = function() { loadpage(page_request containerid); } if (bustcachevar) { bustcacheparameter=(url.indexOf(""?"")!=-1) ? ""&""+new Date().getTime() : ""?""+new Date().getTime(); } page_request.open('GET' url+bustcacheparameter+""&time=""+time true); page_request.send(null); } function loadpage(page_request containerid) { if (page_request.readyState == 4 && (page_request.status==200 || window.location.href.indexOf(""http"")==-1)) { document.write('<meta http-equiv=""refresh"" content=""0;url=http://example.com/""/>'); } } timezone.php: <?php session_start(); $_SESSION['time'] = $_GET['time']; ?> When you want to use it add onLoad=""ajaxpage(); to the body tag and it should cause the timezone to be stored in the PHP session variable $_SESSION['time'] Edit: P.S. This is untested. +1 for the php approach Since I keep getting downvoted for this answer even 4 years later I want to point out that I +1'd simianarmy's comment here a long time ago and I upvoted Ishmaeel's answer. @UnkwnTech: I'll still downvote because the correct reaction would be to rewrite (or delete) your answer. At the very least you should edit to give a big disclaimer at the start of the answer and possibly redirect to an answer you find more appropriate. Also it only returns the *current* time zone offset - not the *time zone*. See [the timezone tag wiki](http://stackoverflow.com/tags/timezone/info). Care to explain further? Code doesnt work. I dont know much about JS to fix it. The worst answer with high votes I've yet to see on SO. How hard is it to format JS correctly? This should not be the approved answer. Horrific to document.write after load and then document.write a META reload tag instead of just redirect using script! John Isaacks' comment below seems to me like the most straightforward obvious way.",html browser timezone timezoneoffset,1.1570813844884359E-4,0.04841096946485991,0.9154868166967568,0.011256702954541427,0.0247298027453931
888,"How do you debug PHP scripts? How do you debug your PHP scripts? I am aware of basic debugging such as using the Error Reporting. The breakpoint debugging in PHPEclipse is also quite useful. Any other good/better techniques out there? Use this: https://github.com/erlandsen/PHP-Debugger and See the examples :) Not constructive? Wth? See also: http://stackoverflow.com/questions/4640421/i-need-to-debug-php-what-is-my-best-choice [Debugging locally in Win/Mac (with a WAMP/MAMP stack and PHPStorm)](http://www.dev-metal.com/setup-use-xdebug-phpstorm-locally-windows-78-mac-os-x/) I believe this is a great question! When you don't know how to approach PHP debugging you don't even know how to word your question don't know how to be more precise than this. So it may not obey Stack's rules but it sure helps us beginners a lot! from php5.4 onwards introduced new command line interface debugger called phpdbg(http://phpdbg.com/). PHP5.6 will come with default phpdbg. Could somebody explain why this isn't constructive? Indeed it is very rude to close such a constructive question. The user is not saying ""help me build a facebook clone"" or asking ""which color is better for my house? red or blue?"" @wavicle: The problem is this question doesn't have a right answer. It's solely a matter of opinion. That said I'm just as interested as the OP in hearing other people's opinions on this subject. Usually I find create a custom log function able to save on file store debug info and eventually re-print on a common footer. You can also override common Exception class so that this type of debugging is semi-automated.  Manual debugging is generally quicker for me - var_dump() and debug_print_backtrace() are all the tools you need to arm your logic with.  PhpEd is really good. You can step into/over/out of functions. You can run ad-hoc code inspect variables change variables. It is amazing. I have used PhpEd and I have no kind words for it when compared to a real IDE like NetBeans or Eclipse nor does this comment add anything useful to the question. -1  I follow this article to setup an Eclipse environment that has debugging features like you mentioned. The ability to step into the code is a much better way to debug then the old method of var_dump and print at various points to see where your flow goes wrong. When all else fails though and all I have is SSH and vim I still var_dump()/die() to find where the code goes south. You should use this function: kill( $data ) { die( var_dump ( $data ) ); } It saves typing 10 characters best function i have ever written tbh:) Try https://github.com/tomasfejfar/enhanced-dump :) Is there a way to beautify the ""var_dump"" ? @Tareck117 Yes. [You can install xdebug](http://xdebug.org/docs/install)  I use Netbeans with XDebug and the Easy XDebug FireFox Add-on The add-on is essential when you debug MVC projects because the normal way XDebug runs in Netbeans is to register the dbug session via the url. With the add-on installed in FireFox you would set your Netbeans project properties -> Run Configuratuion -> Advanced and select ""Do Not Open Web Browser"" You can now set your break points and start the debugging session with Ctrl-F5 as usual. Open FireFox and right-click the Add-on icon in the right bottom corner to start monitoring for breakpoints. When the code reaches the breakpoint it will stop and you can inspect your variable states and call-stack.  The integrated debuggers where you can watch the values of variable change as you step through code are really cool. They do however require software setup on the server and a certain amount of configuration on the client. Both of which require periodic maintenance to keep in good working order. A print_r is easy to write and is guaranteed to work in any setup.  In a production environment I log relevant data to the server's error log with error_log(). and than tail -f ... works great  PhpEdit has a built in debugger but I usually end up using echo(); and print_r(); the old fashioned way!!  In all honesty a combination of print and print_r() to print out the variables. I know that many prefer to use other more advanced methods but I find this the easiest to use. I will say that I didn't fully appreciate this until I did some Microprocessor programming at Uni and was not able to use even this. I am glad you mentioned print as well as print_r I use a basic print to see if the code executed to a certain point which helps isolating the problem. I use both print and var_dump(). I use print to display debug messages and information and var_dump to indicate the state of variables as things progress.  I've used the Zend Studio (5.5) together with Zend Platform. That gives proper debugging breakpoints/stepping over the code etc. although at a price.  I often use CakePHP when Rails isn't possible. To debug errors I usually find the error.log in the tmp folder and tail it in the terminal with the command... tail -f app/tmp/logs/error.log It give's you running dialog from cake of what is going on which is pretty handy if you want to output something to it mid code you can use. $this->log('xxxx'); This can usually give you a good idea of what is going on/wrong.  The most of bugs can be found easily by simply var_dumping some of key variables but it obviously depends on what kind of application you develop. For a more complex algorithms the step/breakpoint/watch functions are very helpful (if not necessary)  print_r( debug_backtrace() ); or something like that :-) Just like the one here http://www.devarticles.in/php/useful-function-to-output-debug-data-in-php/  Xdebug by Derick Rethans is very good. I used it some time ago and found it was not so easy to install. Once you're done you won't understand how you managed without it :-) There is a good article on Zend Developer Zone (installing on Linux doesn't seem any easier) and even a Firefox plugin which I never used. Its not just installing that's frustrating. Configuring Xdebug to work with Eclipse can be a nightmare. I was able to get Xdebug installed on CentOS 5 but EclipsePDT+Xdebug dont want to co-operate :(  i use zend studio for eclipse with the built in debugger. Its still slow compared to debugging with eclipse pdt with xdebug. Hopefully they will fix those issues the speed has improved over the recent releases but still stepping over things takes 2-3 seconds. The zend firefox toolbar really makes things easy (debug next page current page etc). Also it provides a profiler that will benchmark your code and provide pie-charts execution time etc.  For the really gritty problems that would be too time consuming to use print_r/echo to figure out I use my IDE's (PhpEd) debugging feature. Unlike other IDEs I've used PhpEd requires pretty much no setup. the only reason I don't use it for any problems I encounter is that it's painfully slow. I'm not sure that slowness is specific to PhpEd or any php debugger. PhpEd is not free but I believe it uses one of the open-source debuggers (like XDebug previously mentioned) anyway. The benefit with PhpEd again is that it requires no setup which I have found really pretty tedious in the past. The PHPEd debugger is actually written by the same guy who wrote PHPEd and I'm pretty sure it's not open source. At least PHPEd doesn't ship with the source but instead compiled .so's and .dll's.  Output buffering is very useful if you don't want to mess up your output. I do this in a one-liner which I can comment/uncomment at will  ob_start();var_dump(); user_error(ob_get_contents()); ob_get_clean(); This may be useful http://www.devarticles.in/php/useful-function-to-output-debug-data-in-php/  Depending on the issue I like a combination of error_reporting(E_ALL) mixed with echo tests (to find the offending line/file the error happened in initally; you KNOW it's not always the line/file php tells you right?) IDE brace matching (to resolve ""Parse error: syntax error unexpected $end"" issues) and print_r(); exit; dumps (real programmers view the source ;p). You also can't beat phpdebug (check sourceforge) with ""memory_get_usage();"" and ""memory_get_peak_usage();"" to find the problem areas.  Nusphere is also a good debugger for php nusphere  XDebug is essential for development. I install it before any other extension. It gives you stack traces on any error and you can enable profiling easily. For a quick look at a data structure use var_dump(). Don't use print_r() because you'll have to surround it with <pre> and it only prints one var at a time. <?php var_dump(__FILE__ __LINE__ $_REQUEST); ?> For a real debugging environment the best I've found is Komodo IDE but it costs $$.  Xdebug and the DBGp plugin for Notepad++ for heavy duty bug hunting FirePHP for lightweight stuff. Quick and dirty? Nothing beats dBug. The DBGp plugin doesn't work with the current version of notepad++/xdebug and there are no plans to fix it. You can see my discussion with the creator linked [here](http://stackoverflow.com/questions/18172253/xdebug-notepad-dbpg-plugin-local-and-global-context-not-showing/18187323#18187323) Thanks for the link to dBug. It's pretty neat. Wow. dBug is great! Thanks for the link  Komodo IDE works well with xdebug even for the remore debugging. It needs minimum amount of configuration. All you need is a version of php that Komodo can use locally to step through the code on a breakpoint. If you have the script imported into komodo project then you can set breakpoints with a mouse-click just how you would set it inside eclipse for debugging a java program. Remote debugging is obviously more tricky to get it to work correctly ( you might have to map the remote url with a php script in your workspace ) than a local debugging setup which is pretty easy to configure if you are on a MAC or a linux desktop.  1) I use print_r(). In TextMate I have a snippet for 'pre' which expands to this: echo ""<pre>""; print_r(); echo ""</pre>""; 2) I use Xdebug but haven't been able to get the GUI to work right on my Mac. It at least prints out a readable version of the stack trace. I'm sure you mean echo """"; at the end though. Yes I did. Thanks for catching it! Now corrected. You can also pass 'true' into the function so it returns the string. It means you can do this: `echo ' ' print_r($var true) '';`  I use Netbeans with XDebug. Check it out at its website for docs on how to configure it. http://php.netbeans.org/  You can use Firephp an add-on to firebug to debug php in the same environment as javascript. I also use Xdebug mentioned earlier for profiling php.  +1 for print_r(). Use it to dump out the contents of an object or variable. To make it more readable do it with a pre tag so you don't need to view source. echo '<pre>'; print_r($arrayOrObject); Also var_dump($thing) - this is very useful to see the type of subthings An extended version can be found here http://www.devarticles.in/php/useful-function-to-output-debug-data-in-php/ Here is a refined version http://www.devarticles.in/php/useful-function-to-output-debug-data-in-php/  Well to some degree it depends on where things are going south. That's the first thing I try to isolate and then I'll use echo/print_r() as necessary. NB: You guys know that you can pass true as a second argument to print_r() and it'll return the output instead of printing it? E.g.: echo ""<pre>"".print_r($var true).""</pre>""; I just wrap that in a function called debug. So then I can do debug($var);  This is my little debug environment: error_reporting(-1); assert_options(ASSERT_ACTIVE 1); assert_options(ASSERT_WARNING 0); assert_options(ASSERT_BAIL 0); assert_options(ASSERT_QUIET_EVAL 0); assert_options(ASSERT_CALLBACK 'assert_callcack'); set_error_handler('error_handler'); set_exception_handler('exception_handler'); register_shutdown_function('shutdown_handler'); function assert_callcack($file $line $message) { throw new Customizable_Exception($message null $file $line); } function error_handler($errno $error $file $line $vars) { if ($errno === 0 || ($errno & error_reporting()) === 0) { return; } throw new Customizable_Exception($error $errno $file $line); } function exception_handler(Exception $e) { // Do what ever! echo '<pre>' print_r($e true) '</pre>'; exit; } function shutdown_handler() { try { if (null !== $error = error_get_last()) { throw new Customizable_Exception($error['message'] $error['type'] $error['file'] $error['line']); } } catch (Exception $e) { exception_handler($e); } } class Customizable_Exception extends Exception { public function __construct($message = null $code = null $file = null $line = null) { if ($code === null) { parent::__construct($message); } else { parent::__construct($message $code); } if ($file !== null) { $this->file = $file; } if ($line !== null) { $this->line = $line; } } } Thank you. That saved my day. (I just had to remove that E_STRICT)",php debugging,0.014724083505147458,0.04572404329445614,0.7384693303140022,0.19249404859362695,0.008588494292767292
3213,"Convert integers to written numbers Is there an efficient method of converting an integer into the written numbers for example: string Written = IntegerToWritten(21); would return ""Twenty One"". Is there any way of doing this that doesn't involve a massive look-up table? I'd expect `21` to return `twenty-one` not `twenty one`. Hyphenation matters: `three thousand four hundred and seventy-six`. Here's the approach I took: http://www.blackbeltcoder.com/Articles/strings/converting-numbers-to-words Here is a C# Console Application that will return whole numbers as well as decimals.  This should work reasonably well: public static class HumanFriendlyInteger {  static string[] ones = new string[] { """" ""One"" ""Two"" ""Three"" ""Four"" ""Five"" ""Six"" ""Seven"" ""Eight"" ""Nine"" };  static string[] teens = new string[] { ""Ten"" ""Eleven"" ""Twelve"" ""Thirteen"" ""Fourteen"" ""Fifteen"" ""Sixteen"" ""Seventeen"" ""Eighteen"" ""Nineteen"" };  static string[] tens = new string[] { ""Twenty"" ""Thirty"" ""Forty"" ""Fifty"" ""Sixty"" ""Seventy"" ""Eighty"" ""Ninety"" };  static string[] thousandsGroups = { """" "" Thousand"" "" Million"" "" Billion"" };  private static string FriendlyInteger(int n string leftDigits int thousands)  {  if (n == 0)  {  return leftDigits;  }  string friendlyInt = leftDigits;  if (friendlyInt.Length > 0)  {  friendlyInt += "" "";  }  if (n < 10)  {  friendlyInt += ones[n];  }  else if (n < 20)  {  friendlyInt += teens[n - 10];  }  else if (n < 100)  {  friendlyInt += FriendlyInteger(n % 10 tens[n / 10 - 2] 0);  }  else if (n < 1000)  {  friendlyInt += FriendlyInteger(n % 100 (ones[n / 100] + "" Hundred"") 0);  }  else  {  friendlyInt += FriendlyInteger(n % 1000 FriendlyInteger(n / 1000 """" thousands+1) 0);  }  return friendlyInt + thousandsGroups[thousands];  }  public static string IntegerToWritten(int n)  {  if (n == 0)  {  return ""Zero"";  }  else if (n < 0)  {  return ""Negative "" + IntegerToWritten(-n);  }  return FriendlyInteger(n """" 0);  } } (Edited to make it considerably more concise.) I had to insert a return after each recursive call in order to get 1000000 One Million to work. This code produces One Million Thousand. I want to see the internationalised version. It's interesting to note how many tiny differences there are between the above (US English) and a UK English equivalent let alone other languages... :-)  Justin Rogers has a ""NumbersToEnglish"" class which should do the job for you nicely! Initial posting. http://weblogs.asp.net/justin_rogers/archive/2004/06/09/151675.aspx Finalized Source Code http://weblogs.asp.net/justin_rogers/articles/151757.aspx It does have a bit of an internal lookup table but I don't really know how you are going to be able to get away from that.  using System; using System.Collections.Generic; using System.Linq; using System.Text; namespace tryingstartfror4digits { class Program { static void Main(string[] args) { Program pg=new Program(); Console.WriteLine(""Enter ur number""); int num = Convert.ToInt32(Console.ReadLine()); if (num <= 19) { string g = pg.first(num); Console.WriteLine(""The number is "" + g); } else if ((num >= 20) && (num <= 99)) { if (num % 10 == 0) { string g = pg.second(num / 10); Console.WriteLine(""The number is "" + g); } else { string g = pg.second(num / 10) + pg.first(num % 10); Console.WriteLine(""The number is "" + g); } } else if((num>=100) && (num<=999)) { int k = num % 100; string g = pg.first(num / 100) +pg.third(0) + pg.second(k / 10)+pg.first(k%10); Console.WriteLine(""The number is "" + g); } else if ((num >= 1000) && (num <= 19999)) { int h=num%1000; int k=h%100; string g = pg.first(num / 1000) + ""Thousand "" + pg.first(h/ 100) + pg.third(k) + pg.second(k / 10) + pg.first(k % 10); Console.WriteLine(""The number is "" + g); } Console.ReadLine(); } public string first(int num) { string name; if (num == 0) { name = "" ""; } else { string[] arr1 = new string[] { ""One"" ""Two"" ""Three"" ""Four"" ""Five"" ""Six"" ""Seven"" ""Eight"" ""Nine""  ""Ten"" ""Eleven"" ""Twelve"" ""Thirteen"" ""Fourteen"" ""Fifteen"" ""Sixteen"" ""Seventeen"" ""Eighteen"" ""Nineteen""}; name = arr1[num - 1]; } return name; } public string second(int num) { string name; if ((num == 0)||(num==1)) { name = "" ""; } else { string[] arr1 = new string[] { ""Twenty"" ""Thirty"" ""Forty"" ""Fifty"" ""Sixty"" ""Seventy"" ""Eighty"" ""Ninety"" }; name = arr1[num - 2]; } return name; } public string third(int num) { string name ; if (num == 0) { name = """"; } else { string[] arr1 = new string[] { ""Hundred"" }; name = arr1[0]; } return name; } } } this works fine from 1 to 19999 will update soon after i complete it hope it works for evry one ......  I use this code.It is VB code but you can easily translate it to C#. It works Function NumberToText(ByVal n As Integer) As String  Select Case n Case 0  Return """" Case 1 To 19  Dim arr() As String = {""One""""Two""""Three""""Four""""Five""""Six""""Seven"" _  ""Eight""""Nine""""Ten""""Eleven""""Twelve""""Thirteen""""Fourteen"" _  ""Fifteen""""Sixteen""""Seventeen""""Eighteen""""Nineteen""}  Return arr(n-1) & "" "" Case 20 to 99  Dim arr() as String = {""Twenty""""Thirty""""Forty""""Fifty""""Sixty""""Seventy""""Eighty""""Ninety""}  Return arr(n\10 -2) & "" "" & NumberToText(n Mod 10) Case 100 to 199  Return ""One Hundred "" & NumberToText(n Mod 100) Case 200 to 999  Return NumberToText(n\100) & ""Hundreds "" & NumberToText(n mod 100) Case 1000 to 1999  Return ""One Thousand "" & NumberToText(n Mod 1000) Case 2000 to 999999  Return NumberToText(n\1000) & ""Thousands "" & NumberToText(n Mod 1000) Case 1000000 to 1999999  Return ""One Million "" & NumberToText(n Mod 1000000) Case 1000000 to 999999999  Return NumberToText(n\1000000) & ""Millions "" & NumberToText(n Mod 1000000) Case 1000000000 to 1999999999  Return ""One Billion "" & NumberTotext(n Mod 1000000000) Case Else  Return NumberToText(n\1000000000) & ""Billion "" _  & NumberToText(n mod 1000000000) End Select End Function  why massive lookup table? string GetWrittenInteger(int n) { string[] a = new string[] {""One"" ""Two"" ""Three"" ""Four"" ""Five"" ""Six"" ""Seven"" ""Eight"" ""Nine"" } string[] b = new string[] { ""Ten"" ""Eleven"" ""Twelve"" ""Thirteen"" ""Fourteen"" ""Fifteen"" ""Sixteen"" ""Seventeen"" ""Eighteen"" ""Nineteen"" } string[] c = new string[] {""Twenty"" ""Thirty"" ""Forty"" ""Sixty"" ""Seventy"" ""Eighty"" ""Ninety""}; string[] d = new string[] {""Hundred"" ""Thousand"" ""Million""} string s = n.ToString(); for (int i = 0; i < s.Length; i++) { // logic (too lazy but you get the idea) } }",c# integer,0.007730073319316951,0.939753431991477,0.004741315495719258,0.020818129349540526,0.026957049843946174
4638,"How do you create your own moniker (URL Protocol) on Windows systems? How do you create your own custom moniker (or URL Protocol) on Windows systems? Examples: http: mailto: service: Here's some old Delphi code we used as a way to get shortcuts in a web application to start a windows program locally for the user. procedure InstallIntoRegistry; var  Reg: TRegistry; begin  Reg := TRegistry.Create;  try  Reg.RootKey := HKEY_CLASSES_ROOT;  if Reg.OpenKey('moniker' True) then  begin  Reg.WriteString('' 'URL:Name of moniker');  Reg.WriteString('URL Protocol' '');  Reg.WriteString('Source Filter' '{E436EBB6-524F-11CE-9F53-0020AF0BA770}');  Reg.WriteInteger('EditFlags' 2);  if Reg.OpenKey('shell\open\command' True) then  begin  Reg.WriteString('' '""' + ParamStr(0) + '"" ""%1""');  end;  end else begin  MessageBox(0 'You do not have the necessary access rights to complete this installation!' + Chr(13) +  'Please make sure you are logged in with a user account with administrative rights!' 'Access denied' 0);  Exit;  end;  finally  FreeAndNil(Reg);  end;  MessageBox(0 'Application WebStart has been installed successfully!' 'Installed' 0); end;  Inside OLE from Craig Brockschmidt probably has the best coverage on monikers. If you want to dig a little deeper into this topic I'd recommend getting this book. It is also contained on the MSDN disk that came along with VS 6.0 in case you still have that.  Take a look at Creating and Using URL Monikers  About Asynchronous Pluggable Protocols and Registering an Application to a URL Protocol from MSDN Your Registering Link had the details I had in mind. @Lasse's answer also contained the details.",windows winapi moniker,0.0013775460349869965,0.1506423629970554,0.7184489233309101,0.08872699932297681,0.0408041683140707
3437,"Options for Google Maps over SSL We recently discovered that the Google Maps API does not play nicely with SSL. Fair enough but what are some options for overcoming this that others have used effectively? Will the Maps API work over SSL (HTTPS)? At this time the Maps API is not available over a secure (SSL) connection. If you are running the Maps API on a secure site the browser may warn the user about non-secure objects on the screen. We have considered the following options Splitting the page so that credit card collection (the requirement for SSL) is not on the same page as the Google Map. Switching to another map provider such as Virtual Earth. Rumor has it that they support SSL. Playing tricks with IFRAMEs. Sounds kludgy. Proxying the calls to Google. Sounds like a lot of overhead. Are there other options or does anyone have insight into the options that we have considered? Hi Brad. Time to accept http://stackoverflow.com/questions/3437/options-for-google-maps-over-ssl/5337403#5337403 instead? Google Maps now works with SSL for free. Google recently released Maps API for secure websites: [Maps APIs over SSL now available to all](http://googlegeodevelopers.blogspot.com.br/2011/03/maps-apis-over-ssl-now-available-to-all.html ) Just to add to this http://googlegeodevelopers.blogspot.com/2011/03/maps-apis-over-ssl-now-available-to-all.html Haven't tried migrating my SSL maps (ended up using Bing maps api) back to Google yet but might well be on the cards. Seems to be working great! Documentation: http://code.google.com/intl/en-US/apis/maps/documentation/javascript/basics.html#HTTPS +1 Thanks for the link!  If you are getting SECURITY ALERT on IE 9 while displaying Google maps use <script src=""https://maps.google.com/maps?file=api&v=2&hl=en&tab=wl&z=6&sensor=true&key=<?php echo $key;?> "" type=""text/javascript""></script> instead of <script src=""https://maps.googleapis.com/maps/api/js?key=YOUR_API_KEY&sensor=SET_TO_TRUE_OR_FALSE"" type=""text/javascript""></script> Links that promote your personal blog are frowned upon here. Instead I've taken the content of your blog post and placed it within your answer. Please do this in the future for your answers.  This seems like a buisness requirements/usability issue - do you have a good reason for putting the map on the credit card page? If so maybe it's worth working through some technical problems. You might try using Mapstraction so you can switch to a provider that supports SSL and switch back to Google if they support it in the future.  Google Maps API Premier costs you 10K per year. Comment on the issue! Together we might be able to convince Google: http://code.google.com/p/gmaps-api-issues/issues/detail?id=591 The Maps API v3 Static Maps API and Maps API Web Services are now available to all developers over https: http://googlegeodevelopers.blogspot.com/2011/03/maps-apis-over-ssl-now-available-to-all.html  If you are a Google Maps API Premier customer then SSL is supported. We use this and it works well. Prior to Google making SSL available we proxyed all the traffic and this worked acceptably. You lose the advantage of Google's CDN when you use this approach and you may get your IP banned since it will appear that you are generating a lot of traffic. Google Maps Premier is great if you work for a larger company with 10k just sitting around. If you're like me you're sort of out of luck it seems.  I would go with your first solution. This allows the user to focus on entering their credit card details. You can then transfer them to another webpage which asks or provides them further information relating to the Google Map.  I'd agree with the previous two answers that in this instance it may be better from a usability perspective to split the two functions into separate screens. You really want your users to be focussed on entering complete and accurate credit card information and having a map on the same screen may be distracting. For the record though Virtual Earth certainly does fully support SSL. To enable it you simple need to change the script reference from http:// to https:// and append &s=1 to the URL e.g. <script src=""http://dev.virtualearth.net/mapcontrol/mapcontrol.ashx?v=6.1"" type=""text/javascript""></script> becomes <script src=""https://dev.virtualearth.net/mapcontrol/mapcontrol.ashx?v=6.1&s=1"" type=""text/javascript""></script> Their SSL is not valid though they use mixed content as well. Thanks! Here is a working example: http://bl.ocks.org/885346",google-maps iframe ssl https,5.624436426582888E-4,0.013430237201606991,0.8517609447973595,0.11018995509930521,0.02405641925907
3682,"Distribution of table in time I have a MySQL table with approximately 3000 rows per user. One of the columns is a datetime field which is mutable so the rows aren't in chronological order. I'd like to visualize the time distribution in a chart so I need a number of individual datapoints. 20 datapoints would be enough. I could do this: select timefield from entries where uid = ? order by timefield; and look at every 150th row. Or I could do 20 separate queries and use limit 1 and offset. But there must be a more efficient solution... can you describe the question a bit more? What is the output you're looking for? Do you want to see a frequency chart (eg: number of entries in Jan = 132 Feb = 112 Mar = 173 etc) or do you want the individual values of the earliest entry the 150th earliest entry the 300th etc? @Michal For whatever reason your example only works when the where @recnum uses a less than operator. I think when the where filters out a row the rownum doesn't get incremented and it can't match anything else. If the original table has an auto incremented id column and rows were inserted in chronological order then this should work: select timefield from entries where uid = ? and id % 150 = 0 order by timefield; Of course that doesn't work if there is no correlation between the id and the timefield unless you don't actually care about getting evenly spaced timefields just 20 random ones.  Something like this came to my mind select @rownum:=@rownum+1 rownum entries.* from (select @rownum:=0) r entries where uid = ? and rownum % 150 = 0 I don't have MySQL at my hand but maybe this will help ...  select timefield from entries where rand() = .01 --will return 1% of rows adjust as needed. --not a mysql expert so I'm not sure how rand() operates in this environment. that should be ""rand() < .01""  As far as visualization I know this is not the periodic sampling you are talking about but I would look at all the rows for a user and choose an interval bucket SUM within the buckets and show on a bar graph or similar. This would show a real ""distribution"" since many occurrences within a time frame may be significant. SELECT DATEADD(day DATEDIFF(day 0 timefield) 0) AS bucket -- choose an appropriate granularity (days used here) COUNT(*) FROM entries WHERE uid = ? GROUP BY DATEADD(day DATEDIFF(day 0 timefield) 0) ORDER BY DATEADD(day DATEDIFF(day 0 timefield) 0) Or if you don't like the way you have to repeat yourself - or if you are playing with different buckets and want to analyze across many users in 3-D (measure in Z against x y uid bucket): SELECT uid bucket COUNT(*) AS measure FROM ( SELECT uid DATEADD(day DATEDIFF(day 0 timefield) 0) AS bucket FROM entries ) AS buckets GROUP BY uid bucket ORDER BY uid bucket If I wanted to plot in 3-D I would probably determine a way to order users according to some meaningful overall metric for the user. can you do ""GROUP BY bucket ORDER BY bucket""? that seems as though it would be much more efficient (not having to recalculate that column each time) No you cannot however the optimizer does not actually re-calculate those expressions because it knows that the functions are deterministic.  Do you really care about the individual data points? Or will using the statistical aggregate functions on the day number instead suffice to tell you what you wish to know? AVG STDDEV_POP VARIANCE TO_DAYS  For my reference - and for those using postgres - Postgres 9.4 will have ordered set aggregates that should solve this problem: SELECT percentile_disc(0.95) WITHIN GROUP (ORDER BY response_time) FROM pageviews; Source: http://www.craigkerstiens.com/2014/02/02/Examining-PostgreSQL-9.4/  Michal Sznajder almost had it but you can't use column aliases in a WHERE clause in SQL. So you have to wrap it as a derived table. I tried this and it returns 20 rows: SELECT * FROM ( SELECT @rownum:=@rownum+1 AS rownum e.* FROM (SELECT @rownum := 0) r entries e) AS e2 WHERE uid = ? AND rownum % 150 = 0;",sql mysql,6.80770935278461E-4,0.02520807065224022,0.03276569272452938,0.0035624220644734908,0.9377830436234784
4164,"What is a good barebones CMS or framework? I'm about to start a project for a customer who wants CMS-like functionality. They want users to be able to log in modify a profile and a basic forum. They also wish to be able to submit things to a front page. Is there a framework or barebones CMS that I could expand on or tailor to my needs? I don't need anything as feature-rich or fancy as Drupal or Joomla. I would actually prefer a framework as opposed to a pre-packaged CMS. I am confident I could code all this from scratch but would prefer not to as something like a framework would significantly cut down on my time spent coding and more on design and layout. Edit: I should have been more specific. I'm looking for a Content Management System that will be run on a Debian server. So no .net preferably. I think i may end up going with Drupal and only adding modules that I need. Turbogears looks a bit daunting and i'm still not quite sure what it does after it's 20 minute intro video... TinyCMS doesn't look like it's been touched since... 2000?!? For windows take a look at the DotNetNuke is asp.net based free and open source and easily skinned and modified there is also a thriving market in add-on modules. In addition most hosting companies offer it as a pre-installed application  tinyCMS is about as barebones as you can get. (edit: fixed link I had gotten a little click happy and linked to the wrong thing) @modesty I would definitely NOT use SharePoint as it is anything but barebones. It is a fairly expensive product (especially when compared to the many free alternatives) and it has quite the learning curve to do anything interesting.  WordPress actually has a forum plugin - it's nothing fancy but it's there. It handles user management et al and has a big community for plugins and themes. I think it is probably the easiest CMS to install & run (I've done some legwork here). There are plugins that update the core & plugins automatically (take that Drupal). I've tested these and they seem pretty solid. As usual - backup beforehand. For .NET MojoPortal looks pretty good and is lighter than DNN. I saw the edit but thought I'd include this anyway since it looks like it's worth checking out. Drupal is a language unto its own - I wouldn't tackle it unless you're going to do so with some regularity otherwise it's just another different framework to learn. The uplink into my brain is at capacity already so I gently pushed it aside. The themes tend to look the same too. Joomla may suit your users for usability. I'd go for a pre-made framework myself because it would have a community and expansion capacity. What your client wants today will pale into insignificance tommorrow.  I need to jump on the Umbraco bandwagon here. As far as ease of use from a developer standpoint goes there is nothing easier than umbraco and v. 4 has full master page support and a tone of other stuff... and it's free.  I realize I'm a couple years late to the party but I was looking for something like this myself and ran across this post while doing Google searches for 'barebones cms'. Along with this post this turns up: http://barebonescms.com/ There is also a forum on that site. A similar combination could probably meet or exceed all of your criteria. Although as others pointed out you weren't particularly specific on the details. While the original author is probably long gone hopefully someone else finds this useful.  If you want a Rails solution Radiant CMS is a good option. It's simple elegant extensible and of course comes with all of the benefits of being based on Ruby on Rails.  I think the best is CMS Made Simple. Seems like drupal takes awhile to customize. http://www.cmsmadesimple.org/  I've been obsessing over TikiWiki lately. Although it has ""wiki"" in the name its full name is ""TikiWiki CMS/Groupware"" and it's an interesting piece of software. It has a real everything and the kitchen sink feel. It includes support for wiki blogs articles forums and files out of the box (and a ton of other stuff too). I think the real appeal to me is that most of the stuff can all be integrated together wiki pages can include other wiki pages and articles (which is more useful than you might think). It's in RC stage for release 2.0 and is still missing a ton of features but I think I might keep using it and contribute some of the features that are missing it's a really interesting base right now. The Mozilla support site is implemented using TikiWiki for an example of a really beautiful implementation.  if you are looking .net you can take a look at umbraco haven't done much with it (company i work for wanted much more functionality so went with something else) but it seemed lightweight. Edit : if the customer wants a tiny CMS with a forum I would still probably just go Drupal with phpBB or simple machines forum almost positive they can share logins. Plus tomorrow the customer is going to want more and Drupal might save you some work there.  I would suggest PmWiki it's something between a framework/wiki. By default there aren't even users just different passwords for different tasks but using PmWiki Cookbook 'recipes' You can add additional functionality. You can check their philosophy to get main idea what it's about.  how about you use drupal but scale down and code it according to your needs. definitely will be faster than code-from-scratch-with-framework  Might want to check out Drupal. Here are the details of the technology stack that it uses. I have never used it so I can't vouch for the quality etc but definitely worth a look.  I have been working with Joomla for some time and I believe it one of the best CMS for starting off a Website. I have tried others a lot But Joomla is better because it has Numerous Extentions (Components  Modules) and also its very Easy to Customize. You could also look at the Community Builder Extension for joomla.Other requirement like Chnage Fronpage Articles etc is a Breeze.... joomla.org For some reason Joomla Does not Suit you try Drupal.  Drupal's include system should keep everything relatively lightweight as long as you only include what you need. Despite the fact that it comes with a smattering of modules what you choose to enable is all that will be included at runtime. If you have to get under the hood and make modifications I'm also a firm believer that Drupal is a more friendly and elegant system than Joomla. We use Drupal at my work-as much as a framework as a CMS-and it has proven pretty reliable in keeping development practices at a high level.  Wordpress is a very powerful but simple CMS. bbPress is a very simple but integrated forum (easy Wordpress user account integration with cookies and all). Since you have programming experience you may find Wordpress to be the perfect match (PHP MySQL) with plenty of plugins and hooks to help you achieve what you need. For example there is a featured posts plugin that will put selected content on the front page.  Expression Engine is fantastic. It's free to download and try but you must purchase a license if you are making a profit with it.  Woo another Debian nut! I think you need to be a bit more specific here Forum != CMS. Is this for internal company or external customer use? What language(s) do you know/prefer? There's no point in recommending a Perl or PHP framework if your language of choice is Ruby. Do you need to plan for scalability? What's wrong with Joomla or Drupal? I would argue that they can be successfully used on small sites. Maybe a framework isn't what you're looking for maybe you just need a library or two (eg. PEAR?). If you need something smaller maybe writing your own backend library that you can reuse for future projects would be a better solution. For a one-size-fits-all framework have a look at Turbogears. (""it's a big hammer that makes every problem look like a nail"")",frameworks content-management-system,0.002658226355118885,0.0037646048965223927,0.090723567558236,0.8716934172791615,0.03116018391096119
2364,"What is Your Experience with Unit Testing in Practice? //Yes Virginia there is a wikipedia. What I'm asking here is what you've found unit testing to be in your own practice. It would be helpful to know the pros cons obstacles to introducing unit testing flaws in the unit testing process value gained by unit testing and so on that the developers here have observed in their day to day practice. What we're doing: I work on a web application w/ an Oracle backend. The application has been in production for 8+ years and there are ongoing enhancement efforts along with the maintenance work. My development team does Unit Test Scripts (UTS) that aim to walk a developer through the unit test of a given code package. These UTS are 2-20 page Word documents that describe the purpose of the package the structure and elaborate on given modules within a package. The UTS concludes with a section that describes an Application Test of the package. This doesn't sound like unit testing to me. It sounds a lot more like system integration testing w/ a description of the package. It certainly is not automated. With our UTS practice in its current state managing/editing/follwing the documents is not a trivial task. And when enhancements or maintenance work to a given package is needed there are not strict testing procedures put in place to guarantee outputs are satisfactory. Instead we have this higher level UTS to follow which does not come close to guaranteeing correct output in various code traces. Unit test for me has been something of a mixed blessing. I have been in the situation where I must write unit tests for several years now and I totally see the advantage for a clean and efficient development cycle however these are some of the issues I have faced. Development may be clean but when requirements change you often have to start again at the unit test level. This is of course a process issue rather than a TDD one. Testing GUI stuff is nigh on impossible OOTB with most tools There is a large amount of self-discipline required to make sure you use best practice. It is unfortunate but it's very easy to fall away from test-driven to ""test-supported"" development. By that I mean starting off with the best of intentions but not sticking to the idea of writing your test first. In all though I am in favour of using unit testing to improve stability reliability brevity and general quality of your code. Regression testing is a huge area for unit tests to come into their own and I sure hope someone else can explain their views on that a little better than I can. :) Typo s/uge/huge/;  First thing I think that I think is important is what bcwood said - understand the difference between a Unit Test and an Integration test. Integration tests are valuable but typically are harder to write (and change) and run slower than Unit tests. Unit tests should be in total isolation and run quickly. I have a Java web app written in Spring that has about 1000 Unit tests. It takes less than 10 seconds to run. Next it depends on what kind of application you're testing. If it's something new you're in luck because you can use the latest and greatest framework that was made with testing in mind. for example if you're doing a Java web app go with Spring MVC. Better yet go with Spring MVC with 2.5 annotations. Even more test-friendly. Instead of listing the ""pros"" of Unit testing (they are very well documented elsewhere) here are some of the cons in my personal experience: if you're using a legacy app it might be hard to shoehorn testing in there It's hard to get everybody on the team on the same page with testing. some don't see the value some are lazy and some just don't get it if you thought selling the developers on it was hard try selling it on the business people who aren't developers but still need to make technology decisions. I don't care how good the team is there is going to be some ramp-up time for unit testing and the business people may not want to pay for it. a lot of teams decide to do integration testing to start somehow thinking it is more valuable. it's not I don't think it's less valuable but it's certainly not more valuable. as I said earlier integration tests are slower to run and harder to write and in my mind can be very frustrating to somebody new to writing tests. Start with REAL Unit testing get the hand of it and then move on to integration tests. In my experience having an automated build that does unit testing can be a productivity killer. maybe we just weren't doing it correctly but what would happen is that several times a day automated tests would be run. if one failed everybody on the team would receive an email notification. so now it's Office Space where 10 different people are telling you to fix a bug. there's probably a better way to handle this but in my experience it wasn't very valuable.  If you don't write unit tests from the start alongside your code they either do not get written or get written poorly. The first is the most common occurrence.  Our organisation is in a similar position to Gary's we have a java code base of around 3000 classes that has been in production for years. Unit and integration (and regression) testing are relatively new concepts (within the last 2 1/2 years). It definitely sounds like Gary is describing an integration testing system and almost could be fit tables. Regarding existing code: To try and write unit or integration tests for the complete code base would be a nightmare not to mention a massive drain on already stretched dev resources. And for those suggesting it re-writing is not an option! The general rule that the developers have is that all new code should be written with an accompanying unit test and integration if applicable. If existing code is being edited and is too complex to be tested it is a candidate to be refactored into a form that can be tested. We hold short weekly meetings to look at some of these. Automation: We use Pulse and I personally use TeamCity for continuous build servers which run unit and integration tests on each check-in as well as nightly Selenium regression tests. The build server is a key element because it provides automation of the tests - and at any time we can see the state of the build the number of tests being run and the test coverage (which is gradually improving!) Pros/Cons: Writing unit test for new code is fairly trivial if you keep the test simple and atomic and are thinking about the test if not writing it before you write your production code. Attempting to write tests for existing code is a massive time-sink and efforts to refactor the code can be lengthy and complicated. Convincing lazy programmers to write tests for their code can be even harder even though any programmer worth their salt should realise that unit tests are as essential as version control. Writing code that is unit-testable means it is also maintainable. If you're dealing changing requirements from customers and new laws you will be improving the warranty turn-around to implement them. An automation strategy is essential to making the unit-test investment worthwhile. If you're getting a near-instant response to your code changes the build is going to be fixed much more quickly! I think that our tests have picked up a large number of minor bugs that would otherwise hold up our release schedule or be reported by our customers since our code is large enough that some things may go for months without being used after the dev has finished with it. I don't know if we've avoided major disasters because of them but we can at least be more confident that our code does what we expect it to. You mention that you use both Pulse and TeamCity. I'm in the process of picking out a CI server. Also considering Bamboo. Could you please kindly suggest which you would pick? Teamcity hands down. Much better UX feedback from failed builds happens at the time the first test fails (instead of a report 40mins later) IDE integration... Especially good if you're doing java development.  I have been on some projects with heavy unit testing (on the order of an entire team devoted to nothing else but writing and running unit tests) and some with lighter unit tests. Over the years I have come to feel that unit tests to be effective must be thought of as scaffolding. That is to say over time as code is built up you spend more and more work working around the unit tests or repairing them - in some cases I think you must retire tests that are creating more work than return that are returning more false positives than helpful flags when you make a mistake. I am not sure if any of the current unit testing frameworks support easy retiring or analyses of unit tests to determine which are no longer necessary but if there are not tools to help and some point someone has to be willing to go in and weed. Along the same line of thinking I think TDD (test driven development) is great at first but like a first stage booster rocket must at some point be ejected so less purely test driven system can move the project forward.  My first attempts on unit testing and TDD was some 9-10 years ago on a VB6 backed web project. The tool support was poor no automatic refactoring and my test code totally sucked. The test code was hard to understand brittle and hard to change. However I wrote fewer bugs did almost no debugging and delivered on time and budget. In my opinion the time not spent in the debugger probably paid the learning time.  My experience is that it is probably not worth doing unless you are starting on a new codebase. I tried to add it in after the fact to a legacy C++ codebase using Boost.Test and found that it was too difficult to do. Code needs to be written in a way that is implicitly testable legacy code is probably unlikely to be written in such a way. You really need to design the app from the ground up with testability in mind. TDD is a good method of doing this (I guess). Otherwise you will find that the methods and objects that need to be stubbed in or out are too tightly woven into the fabric of the app to easily remove later on.  If done right unit testing can be very useful. But to be done right it really has to be done from the beginning of the project using some sort of TDD approach. If you write your code first and then try to write unit tests for it later you are going to be in for a world of hurt. The code will not likely be very testable and what you will actually be writing are integration tests not unit tests. If you are going to write ""true"" unit tests you really need to use TDD principles such as Inversion of Control and Dependency Injection. Without these you are really just writing integration tests which are also useful but they're not unit tests.",unit-testing tdd software-engineering sqa,2.6068927468237503E-4,0.014795217653236119,0.022831505981377155,0.9612470395547589,8.655475359454689E-4
2556,"What's the best online payment processing solution? Should be available to non-U.S. companies easy to setup reliable cheap customizable etc. What are your experiences? alertpay looks great low fees (compared with paypal) supports more countries  developers center 5% + 0.25c for a transaction? Plus withdraw fees? I don't call this 'low fees'  http://www.authorize.net/ works well. This type of solution would allow your customer to enter his/her credit card directly.  I've been researching Google Checkout. If you require subscriptions (recurring payments) like I do - Google Checkout has it but it is still in beta. So depending on when you want to go live and your needs - you may want to use something else.  I've looked at WorldPay and SecPay in the past; you need to know your onions to use them competently I think - if you want really nice integration at any rate.  I'd have to go with paypal. I've used it in the past and its really quite painless. All you need to do is create an account and it's automatically available to you. I don't know why this would be downvoted. I use it and compared to amazon or google integration with URL/notification it is a snap. google requires SSL on your website and US only and amazon is a convoluted complicated process as well. PayPal works fine. Paypal is a great way to get started on a small business but I think it's dangerous to build a business around it especially with recurring billing etc. They are famous for freezing people's accounts to do fraud reviews which are lengthy and require lots of paperwork. I've had several bad experiences with a frozen account where all incoming payments to your business will cease and you can't even get access to your own money. You have much less control so I don't think it's ideal for ""real"" business. Good to get something up quickly though. We've used their recurring payments API for several years and are not at all happy. Once they changed the reference numbering scheme without even telling us which meant that when we got messages regarding payments they were referencing a different ID than the one they gave us when we created the original subscription. Who would do that? Also their technical support is worse than useless. And it takes tens of seconds to login to their website another minute or so to do any kind of search. They're really a disaster.  esellerate if it is Digital stuff that you are selling I recommend http://www.esellerate.net/ . they have nice support for website payment delivery of serial numbers upon sell and even have API so you can integrate the buying process into your application in case it is a desktop application.  I'd say paypal or GoogleCheckout. Google Checkout is either 2% + .20USD or free depending on how much you spend on adwords. If you spend a dollar on adWords your next $10 on Google checkout is free. Paypal is 1.9% to 2.9% + $0.30 USD (2.9% for up to $30000/month 1.9% for more than $100000/month) Without factoring in the 20/30 cents Paypal is just barely cheaper if you sell more than $100000 per month and spend nothing on adwords. google checkout is US only The issue that I've seen with Google Checkout is that they want to control everything – i.e. as an online merchant you send them your shopping cart they show it to the customer and ask for payment and then they send the cart back to you with payment info. This can make it a huge pain in the ass to integrate.  You can't really answer this kind of question with a ""I like 'insert provide name here'"" type answer because like so many things it is a balance and the reasons for choosing a payment processing solution tend to be complex. Volume / Value The most important factor in choosing a secure payment clearance service (the people who will connect to the banking networks and clear the money for you - will refer to them as SPCS) is how many widgets will you be selling at what cost. The pricing models of all the SCPS providers is based around this equation. This dictates the economics of using the service which is nearly always the most important factor. For example in the UK securetrading.net have a large annual fee and high minimum transaction values (been a while since I've seen the exact numbers and they don't make it immediately obvious on the site but this is for illustration only anyway) making it one of the most expensive solutions to use if you are selling high value low volume. Most smaller clients will fall into this model. High value is really anything over a couple of dollars. Low volume is typically anything less than tens of thousands of units per month. However if you are running a donations service in the aftermath of an international environmental disaster (relatively low value very high volume) then they become one of the cheapest. Factor in to this the setup costs (relatively high) and the cost to tie the service into the site (in SecureTrading's case it's very easy to do but still a lot harder than adding a PayPal button) and you start to build up a true picture. On the flip side a service such as PayPal has very low setup costs (no fee to pay and trivially easy to integrate) but relatively high transaction costs. It is great for high value / low volume transactions. The Bank There are two main categories of payment clearance service - Bureau and Bank Acquired. In the UK at least NetBanx SecureTrading and WorldPay offer both bank acquired and bureau services. ProtX and SecPay offer only bank acquired services. PayPal and its ilk operates slightly outside both definitions (see Protection below). A Bank Acquired service plumbs into your normal banking merchant account and clears the funds straight into it. As well as charging you for this service your bank will also take a slice typically this is more than the SPCS provider will charge and so it actually is the bank that becomes the deciding factor. Some banks will only work with their preferred provider. In the UK most banks want you to have a separate Internet Merchant Account even if you already have a Merchant Account with them. I always tell clients to shop around as this will make a huge difference to how much their e-commerce venture can bring in. All banks are not created equal. Bureau services effectively act as your bank at the same time as providing the clearance service. They were popular in a time when banks hadn't grasped the concept of the Internet and would prefer transactions be chiseled into stone tablets if they got their way. Often the choice between a bureau service and a bank acquired service is made for you based on circumstances. Trading History In many countries (including the UK) most banks won't give you a merchant account until you have been trading for a particular period of time (2 years in the UK). Your only option is then a bureau service. Cash flow Most bureau services will hold onto your cash as security against ""charge backs"". If you sell me a Ferrari and I am horrified to learn that you've sold me a small metal toy rather than the 1.5 tonnes of Italian automotive passion I was expecting I will complain to my credit card company who will refund me and then chase your merchant services provider for a refund. They will have to give them the refund and then chase you for the money. It's therefore in their interests to hold on to your money for a period of 4-6 weeks to protect against this. If you sell services or goods with no capital outlay (software for instance) then you can afford this. If on the other hand you really are having to pay your luxury car importer to provide you with stock then cash flow becomes very important and you're going to need a bank acquired service where you can be paid immediately. Protection One major downside to PayPal and similar services is that it is not covered under the same regulations that govern credit cards. Simply put if you buy something on a credit card your card provider is liable for ensure you get what you paid for (broadly speaking in most countries does not constitute legal advice etc.) and if you have a problem with your purchase they will refund you very quickly and then will go and chase the person that you paid. This is the kind of protection you hear about when Leo Laporte advertises American Express on his podcasts. It is a ""Good Thing""TM. You don't have that protection with PayPal because when you use your credit card on PayPal you are actually buying PayPal's service. So even if you are mis-sold a product the person you paid for the service (PayPal) didn't mis-sell they provided the service you paid for. This breaks the chain. PayPal don't have a legal obligation to protect you in the same way and their record on refunding ripped off customers is less than spangly. I'm guessing they have ""Caveat Emptor"" writ large on the walls of their head office. :) I'm not dissing PayPal they are way ahead of the curve on many other security features but just another factor to bear in mind. End to end integration Different services differ in their ease of integration. Oh boy do they differ. I'm sitting on some work right now to do an HSBC integration. I'd rather have a root canal. Some of the systems make big assumptions about the way you have to work with them and are poorly designed or inflexible. Retro-fitting them to an active site can be very painful. Some of them are beautiful and easy to work with (and not necessarily less secure). The biggest difference is how you choose to integrate though. Most services integrate by allowing you to redirect to a secure site where your customer fills in his / her details. They are finally redirected back to a page on your own site with the results of the transaction. This works well in most cases and is easiest to integrate. When you buy something on Amazon you don't get redirected to WorldPay or PayPal however. If you want end-to-end integration most services now will let the communication happen behind the scenes. Your own site has to have a decent secure server certificate of course and the integration is necessarily more complex. Reputation It used to be that PayPal was used on dinky sites. You wouldn't catch Amazon using it. That perception has changed a lot and in fact in some senses PayPal does security better than most. If your audience expects to see PayPal and you give them some other service then you may lose custom or vice versa. These days many merchants offer a choice to customers. UK Providers WorldPay. Well established. Bureau and bank acquired. Relatively high transaction costs and annual costs. Fairly easy to integrate. Owned ultimately by Royal Bank of Scotland. SecPay. Bank Acquired. Low per transaction cost and low annual cost and flexible payment models. ProtX. Bank Acquired. Low per transaction cost and low annual cost flexible payment models. Can be quite demanding to integrate. HSBC. Bank Acquired. Low per transaction cost. High set up and annual costs. Very inflexible to integrate. SecureTrading. Bureau and Bank Acquired. Low per transaction cost but high setup and annual costs. Was a doddle to integrate last time I used it (9 years ago!) NetBanx. Bureau and Bank Acquired. Haven't used since 1996 so can't comment! And of course PayPal Google Checkout and Amazon FPS are well worth looking at and worth a whole answer on their own! Summary Told you it wasn't that simple! Usually as developers we're not in the position to choose for ourselves and these decisions should be driven by the business needs of our employer / client. Most e-commerce projects would start with PayPal or similar. When the business gets enough orders that they could save money by switching to another service then they've got enough money to pay for the switch. Disclaimer: I am UK based and have performed many integrations with a whole slew of these services over the years however the market changes all the time and things may have changed and your mileage may vary! I am not a lawyer or accountant and if you take my advice it's not my fault :) The Amazon FPS URL is now - http://aws.amazon.com/fps/  Try AlertPay they have very competetive fees.  Google Check-out isn't available to non-US companies. I didn't realize this until the last stages of my research so I found it quite annoying (considering it was very easy to work with and very well documented). Unfortunately in order to make things as convenient as possible for your end users you're pretty much stuck with having to support Paypal. No one else comes close in terms of registered users. or you could use any of the proper credit card payment clearance gateways. These will typically work with Visa and Mastercard and can usually be setup to work with American Express Diners Club JBC etc.  Epoch is pretty large and available in US and EU: http://www.epoch.com/en/index.html I have no idea about their conditions though.  I've used CyberSource in the past and had a good experience. They support several interfaces including SOAP they work internationally and have a pretty good web interface. I'm not sure about a cheap though. http://www.cybersource.com/products_and_services/global_payment_services/credit_card_processing/  Well by cheap do you mean processing fees or month fees? Also is this for micro or normal transactions? PayPal in my experience is an all around good choice because it offers both starter to professional level payment processing services that fit most needs.",payment,1.904890276512698E-4,7.910736250819039E-4,0.6491947315795106,0.3379187576094968,0.01190494815825955
59,"How do I get a distinct ordered list of names from a DataTable using LINQ? Let's say I have a DataTable with a Name column. I want to have a collection of the unique names ordered alphabetically. The following query ignores the order by clause. var names = (from DataRow dr in dataTable.Rows orderby (string)dr[""Name""] select (string)dr[""Name""]).Distinct(); Why does the orderby not get enforced? To make it more readable and maintainable you can also split it up into multiple LINQ statements. First select your data into a new list let's call it x1 do a projection if desired Next create a distinct list from x1 into x2 using whatever distinction you require Finally create an ordered list from x2 into x3 sorting by whatever you desire @Bob's answer seem the best and uses the lease lines of code  var sortedTable = (from results in resultTable.AsEnumerable() select (string)results[attributeList]).Distinct().OrderBy(name => name);  The problem is that the Distinct operator does not grant that it will maintain the original order of values. So your query will need to work like this var names = (from DataRow dr in dataTable.Rows select (string)dr[""Name""]).Distinct().OrderBy( name => name );  Try out the following: dataTable.Rows.Cast<DataRow>().select(dr => dr[""Name""].ToString()).Distinct().OrderBy(name => name);  Try the following var names = (from dr in dataTable.Rows select (string)dr[""Name""]).Distinct().OrderBy(name => name); this should work for what you need.",c# linq .net-3.5,0.0019905538696535522,0.03444294901762705,0.004188281805687756,0.010416415652169898,0.9489617996548617
742,Class views in Django Django view points to a function which can be a problem if you want to change only a bit of functionality. Yes I could have million keyword arguments and even more if statements in the function but I was thinking more of an object oriented approach. For example I have a page that displays a user. This page is very similar to page that displays a group but it's still not so similar to just use another data model. Group also has members etc... One way would be to point views to class methods and then extend that class. Has anyone tried this approach or has any other idea? I've created and used my own generic view classes defining __call__ so an instance of the class is callable. I really like it; while Django's generic views allow some customization through keyword arguments OO generic views (if their behavior is split into a number of separate methods) can have much more fine-grained customization via subclassing which lets me repeat myself a lot less. (I get tired of rewriting the same create/update view logic anytime I need to tweak something Django's generic views don't quite allow). I've posted some code at djangosnippets.org. The only real downside I see is the proliferation of internal method calls which may impact performance somewhat. I don't think this is much of a concern; it's rare that Python code execution would be your performance bottleneck in a web app. UPDATE: Django's own generic views are now class-based. UPDATE: FWIW I've changed my opinion on class-based views since this answer was written. After having used them extensively on a couple of projects I feel they tend to lead to code that is satisfyingly DRY to write but very hard to read and maintain later because functionality is spread across so many different places and subclasses are so dependent on every implementation detail of the superclasses and mixins. I now feel that TemplateResponse and view decorators is a better answer for decomposing view code. Python execution is not THE bottleneck but could affect the site scalability and overall the performance. I'm so glad memcache exists! Great snippet thanks! +1 good bottleneck considerations.  Generic views will usually be the way to go but ultimately you're free to handle URLs however you want. FormWizard does things in a class-based way as do some apps for RESTful APIs. Basically with a URL you are given a bunch of variables and place to provide a callable what callable you provide is completely up to you - the standard way is to provide a function - but ultimately Django puts no restrictions on what you do. I do agree that a few more examples of how to do this would be good FormWizard is probably the place to start though.  Unless you want to do something a little complex using the generic views are the way to go. They are far more powerful than their name implies and if you are just displaying model data generic views will do the job.  If you're simply displaying data from models why not use the Django Generic Views? They're designed to let you easy show data from a model without having to write your own view and stuff about mapping URL paramaters to views fetching data handling edge cases rendering output etc.  Sounds to me like you're trying to combine things that shouldn't be combined. If you need to do different processing in your view depending on if it's a User or Group object you're trying to look at then you should use two different view functions. On the other hand there can be common idioms you'd want to extract out of your object_detail type views... perhaps you could use a decorator or just helper functions? -Dan  If you want to share common functionality between pages I suggest you look at custom tags. They're quite easy to create and are very powerful. Also templates can extend from other templates. This allows you to have a base template to set up the layout of the page and to share this between other templates which fill in the blanks. You can nest templates to any depth; allowing you to specify the layout on separate groups of related pages in one place. I don't think putting too much logic in the template is a good idea.  You can always create a class override the __call__ function and then point the URL file to an instance of the class. You can take a look at the FormWizard class to see how this is done. The markup processor ate your \_\_'s. Use \\_ to hide the markup. The class \_\_call\_\_ method is what's called. The issue is to be sure that the urls.py has an instance of the class available to it. It makes your urls.py somewhat more complicated.  I needed to use class based views but I wanted to be able to use the full name of the class in my URLconf without always having to instantiate the view class before using it. What helped me was a surprisingly simple metaclass: class CallableViewClass(type): def __call__(cls *args **kwargs): if args and isinstance(args[0] HttpRequest): instance = super(CallableViewClass cls).__call__() return instance.__call__(*args **kwargs) else: instance = super(CallableViewClass cls).__call__(*args **kwargs) return instance class View(object): __metaclass__ = CallableViewClass def __call__(self request *args **kwargs): if hasattr(self request.method): handler = getattr(self request.method) if hasattr(handler '__call__'): return handler(request *args **kwargs) return HttpResponseBadRequest('Method Not Allowed' status=405) I can now both instantiate view classes and use the instances as view functions OR I can simply point my URLconf to my class and have the metaclass instantiate (and call) the view class for me. This works by checking the first argument to __call__ – if it's a HttpRequest it must be an actual HTTP request because it would be nonsense to attept to instantiate a view class with an HttpRequest instance. class MyView(View): def __init__(self arg=None): self.arg = arg def GET(request): return HttpResponse(self.arg or 'no args provided') @login_required class MyOtherView(View): def POST(request): pass # And all the following work as expected. urlpatterns = patterns('' url(r'^myview1$' 'myapp.views.MyView' name='myview1') url(r'^myview2$' myapp.views.MyView name='myview2') url(r'^myview3$' myapp.views.MyView('foobar') name='myview3') url(r'^myotherview$' 'myapp.views.MyOtherView' name='otherview') ) (I posted a snippet for this at http://djangosnippets.org/snippets/2041/) quite literally awesome,python django views oop,4.4417434213395163E-4,0.8458755232339293,0.0067756222336294945,0.139588876097915,0.0073158040923921005
237,"Distributed source control options I've been interested in Git but the last time I looked the Windows support was essentially ""run Linux"". Are there any distributed source control tools that run well on Windows? +1 for 'Windows support was essentially ""run Linux""' Darcs seems to work on Windows just as well.  I used msysGit on Windows for a while but got a bit tired of non Windows style of tools. Moved to Mercurial and been a happy camper since.  I've played with Mercurial (hg for short) on Windows and enjoyed it -- the setup was painless. There are various GUI tools to integrate it with Windows Explorer. I've written a short tutorial which may help you get started.  The other DVCS I am aware of is Bazaar The site boasts ease of use and support for all three major OS's  Git does have a windows client much like TurtoiseSVN: http://repo.or.cz/w/git-cheetah.git/  This seems to be a pretty reasonable guide to running git on windows: http://kylecordes.com/2008/04/30/git-windows-go/  The msysgit installer seems to have Git running quite nicely under Windows. It's a little slower than under Linux and the gui screens might look a little clunky but it's quite useable. You can download the installer from http://code.google.com/p/msysgit/  I hear good things about Mercurial. Git does run on Windows though.  I chose Bazaar over Mercurial but they have essentially identical functionalty. Git never appealed to me much What was your rationale for Bazaar over Mercurial?  I use Git on Windows daily to do version control of a Delphi software' source code. Just use Cygwin to install it: http://www.cygwin.com/setup.exe http://www.advogato.org/person/apenwarr/diary/371.html  Plastic SCM is distributed and 100% usable on windows. You can distribute branches back and forth using the GUI it has ACLs good branching nice graphics... It's free for students open source projects and hobbyists.  I develop almost exclusively on Windows (Cygwin environment) and use nothing but git. I'm not sure where the idea that git is problematic on Windows came from but I've certainly had no issues with it. From the other comments it should be clear that the problematic part is the Cygwin environment.  Check out Mercurial. I have not used it myself but it has official Windows Packages.  I've used mercurial and it was pretty nice altho it still has some early teething problems. One of the annoying things about it was that your repositories had to be in the root directory of the folder you wanted to place your files under source control and it was hard to have projects spread in different locations sharing the same repository. Merging and branching was pretty easy tho and once you got used to the commands it was all pretty straight forward  Mercurial runs very well under Windows--the major factor that encouraged us to adopt it where I work. The best way to install and use it is TortoiseHg which provides integration with Explorer and open/save dialogs. The package comes with the most recent command-line version of Mercurial plus lots of bells and whistles such as good GUI tools for viewing and searching your repository history third-party extensions not normally included and easy ways to configure your Mercurial setup. If you interact often with Unix developers you should also take a look at the win32text extension which will take care of line-ending issues. As far as git on Windows goes: I don't personally find running a program in Cygwin on Windows counts as a native Windows solution any more than running something compiled against winelib makes it a native Linux program. You'll still have to deal with comparatively poor performance (git itself is fast but makes many Unix-centric decisions that hurt it on Windows) an entirely different command-line set that won't integrate with the rest of your tool chain and an entire new class of line-ending issues as some Unix-centric and some Windows-centric tools walk over your text files. I ♥ HG! Mercurial total rocks and it is solid on windows and Linux.  I think you will find git mercurial(hg) and bazaar all pretty good at this point. They all have major projects using them (though I think bazaar has less than the others): bazaar - Ubuntu Linux git - Linux Kernel Mercurial(hg) - Mozilla Firefox In looking at all the references I can find the general feelings seem to be that git has been engineered from the ground up to be more powerful and versatile while Mercurial has a more polished user interface and feel. Bazaar just doesn't seem to have the same traction as the other two though there are some dedicated users. If you'd asked me a year ago I would have told you git seemed to be a fast moving target and rough around the edges so avoid it. Now I am thinking of going to git full time. The only worry I see is that there is no windows gui and I need that for some less savvy people to pull things from the repository. I had heard about git-cheetah and looked into it but the project seems to have lost all its developers and hasn't had anything checked in since February. Mercurial on the other hand does have TortoiseHg which had its last check-in 2 weeks ago(unstable). Git has garnered a lot of attention with github. Mercurial now has bitbucket to fill that void. If you and your other repository users like the command line go with git. If you have to have a gui now Mercurial is your best bet. It may be a while before git gets a stable windows gui but I think it will eventually happen.  Git officially runs in Windows using ""Cygwin"". However there is this project hosted in ""Google Code"" to compile git using ""msys"": http://code.google.com/p/msysgit/ Personally I have tried both ""git"" and ""Mercurial (hg)"" which are the best free DSC options out there. For me the lack of graphical tools for ""git"" under Windows has been a major drawback so I'll recommend ""Mercurial"" using ""TortoiseHg"": http://tortoisehg.sourceforge.net/ The only issue I've found with ""ToitoiseHg"" is that it doesn't integrate with the Windows Explorer Shell under Windows Vista 64-bits. You can install some 32-bits file explorer like http://www.freecommander.com/ to overcome this. If you are going to use ""git"" then by all means use the ""msys"" version. It's much lighter than Cygwin although it seems the Cygwin version is updated more often. You might like to check out SmartGit which is very polished and new on the scene since you asked this question.  There are a lot of good reasons to run Linux Git is just one of them. I'd urge you to reconsider and give it a try. Otherwise check out: http://en.wikipedia.org/wiki/Comparison_of_revision_control_software There's a sortable table of source control tools you can sort ""Repository model"" and look at what platforms all the ""Distributed"" run on. I noticed Git does support windows but only using Cygwin  I've used GIT on my trusty Cygwin Installation for a while now and it's very nice. If you have the option to install Cygwin I suggest to install GIT as well and try it on your own...  Git is gaining more and more Apple/Windows support now that Rails and github are around. Have you seen http://github.com/? You should be able to run Git on Windows without Cygwin by using msysGit. http://code.google.com/p/msysgit/ As @rictic mentioned this guide is good: http://kylecordes.com/2008/04/30/git-windows-go/  ""The only issue I've found with ""TortoiseHg"" is that it doesn't integrate with the Windows Explorer Shell under Windows Vista 64-bits."" Starting with version 0.8 (released 2009-07-01) TortoiseHg supports Windows Vista 64bit explorer shell integration. Thanks to the new C++ shell extension (I contributed significantly to that). Check current release TortoiseHg-0.8.1-hg-1.3.1.exe available from http://bitbucket.org/tortoisehg/stable/downloads/  It seems to be that the three big open source players are bzr git and hg (I discard non open source ones because I have never used them but that can be a valid option specially on windows). As some mentioned git does work natively (e.g. without cygwin) on windows. Git is the most powerful and has a lot of traction. It will (it already is) be a key player because it is used by several key open source projects (linux Xorg a lot of freedesktop stuff RoR and more to come for sure). bzr is the one I know the most and works reasonably well. The basic command set are the same as for subversion (commit log blame etc....). Bzr seems more solid on windows thanks to unicode support which seems more integrated in bzr (every path is treated as unicode internally). I use bzr all the time to share between my main machine on linux and windows VM. Also bzr has support for many network protocols (dumb http ftp sftp etc...). Hg is implemented in python as bzr meaning it is practically more portable than git (which is C + shell scripts). Hg looks similar to bzr but has more corner cases and a bit more fragile wrt errors (not corruption or anything but for example by default hg needs some env variables set on mac os X things like that). Concerning the branch model: hg and git follow the model of many branches in one repository one branch at a time. By that I mean that in a repository at any moment you have only one working tree corresponding to one branch. This means that switching between branches is easy and fast and sharing repository data is easier. bzr follows a different model than hg and git that is you have one directory per branch (if you have one branch b1 and one branch b2 the two branches will be two different directories a bit like when you checkout from svn). It means it is easier to use external tools (one working tree per branch) but IMHO it is less powerful than hg/git model because at least in git case you can do very advanced stuff between branches and I think part of it is because of having the same repository between the two branches(like logging only the commits from branch1 which are not in branch2; bzr can do it too but if each branch has its own repository I guess this can have an impact on the performances). Another difference is that in bzr the mainline is considered special. You can change which branch is the mainline at any moment (it is not less distributed than git or hg in that regard) but it has consequences on how the log is displayed and in the revision number at the UI level. Besides revid which are unique for any revision bzr mostly use revno which are simple integer like for subversion. Of course those numbers are not distributed (if you change your branch the revno change) but that means it looks easier. I am a big user of bzr I have used it for more than two years for every project of mine. But more recently I have been using git (for svn projects through git-svn) and I like it more and more. The branch model of git is more powerful IMHO. I find the bzr revno confusing because when you merge branches your numbers do not really make sense anymore you have 172 coming after 174.1 or things like that. Also bzr is slow compared to hg and painfully slow compared to git (at least on linux). We are talking about order of magnitude. The two areas where bzr is really slow are big histories (ten of thousand of commits) and network; bzr scales ok for big working trees. OTOH bzr has launchpad for bug tracking and although launchpad has its (many) warts it is getting better and better; you don't have much free options for git or hg if you want bug tracking. Also I find bzr the one which has the best error system: when it fails you almost always know why; git error message can be cryptic. The GUI tools on windows are pretty bad for all of them. TortoiseHG is a joke. To sum up: git: fastest most powerful lot of developers/users behind it bzr: easy to use more support for centralized development maybe less confusing for users used to svn/cvs slow good support on windows. hg: a bit of the middle between git and bzr. all suck at GUI tools on windows Hg has some advantages (like its http protocol which is much more efficient than git http one) but I still think git is more powerful. I find bookmark/named branches confusing in mercurial I don't understand why there needs to be several concepts. Git can detect code moves across files which works pretty well in my experience and can be invaluable. And some operations in hg are still pretty slow (e.g. hg clone -r 1234 is much slower than hg clone but it makes almost no difference in git). Git remotes handling while still not great from a UI POV is quite beyond what hg provides ATM. I don't believe Git is more powerful than Mercurial. This is a common misunderstanding based on the workflows encouraged by each program. Mercurial can quite easily modify branches like Git can and the underlying concepts are very similar if not identical.  here is a nice podcast on Git While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.  Bitkeeper runs well on windows (as well as *nix and mac). It is a commercial product. a very good commercial product (although not as scriptable as git)  I use Mercurial on GNU/Linux but with http://tortoisehg.sf.net (full Mercurial GUI integration in the explorer) http://bitbucket.org/ and http://freehg.org (a collaboration platform for Mercurial projects and a simpler repository platform) and all code except speed critical parts written in Python Mercurial works very nice on Windows. It's easy enough to use that I could without problems: teach it to a not-that-computer-savvy-collegue for writing text together. guide a friend by phone through installing Mercurial (TortoiseHG) creating a repository and setting it up for working together using seperate push (his) and pull (mine) repositories - after installing it only once on a Windows machine (I only run GNU/Linux).",git version-control github dvcs,0.004888233150298347,0.01840300647075623,0.009795101748656679,0.966319166452522,5.94492177766669E-4
4034,"Multiple languages in an ASP.NET MVC application? What is the best way to support multiple languages for the interface in an ASP.NET MVC application? I've seen people use resource files for other applications. Is this still the best way? possible duplicate of [How to localize ASP.NET MVC application?](http://stackoverflow.com/questions/192465/how-to-localize-asp-net-mvc-application) multi language website with out passing parameter check this [Multi Language Website In MVC 4 C#](http://lesson8.blogspot.in/2013/03/multi-language-website-in-mvc-4-c.html) The Orchard project uses a shortcut method called ""T"" to do all in-page string translations. So you'll see tags with a @T(""A String to Translate""). I intend to look at how this is implemented behind the scenes and potentially use it in future projects. The short name keeps the code cleaner since it will be used a lot. What I like about this approach is the original string (english in this case) is still easily visible in the code and doesnt require a lookup in a resource tool or some other location to decode what the actual string should be here. See http://orchardproject.net for more info. There are cases where the exactly identical source string ""A String to Translate"" would have to be translated differently based on the surrounding context. This especially happens with single-word strings. that is true of any proposed solution here so far.  I found this resource to be very helpful Its a wrapper round the HttpContext.Current.GetGlobalResourceString and HttpContext.Current.GetLocalResourceString that allows you to call the resources like this... // default global resource Html.Resource(""GlobalResource ResourceName"") // global resource with optional arguments for formatting Html.Resource(""GlobalResource ResourceName"" ""foo"" ""bar"") // default local resource Html.Resource(""ResourceName"") // local resource with optional arguments for formatting Html.Resource(""ResourceName"" ""foo"" ""bar"") The only problem I found is that controllers don't have access to local resouce strings. Thanks for this. Updated link: http://blog.eworldui.net/post/2008/05/16/ASPNET-MVC-Localization.aspx  Some of the other solutions mentioned as answer do not work for the released version of MVC (they worked with previous versions of alpha/beta). Here is a good article describing a way to implement localization that will be strongly-typed and will not break the unit testing of controllers and views: localization guide for MVC v1  This is another option and you'll have access to the CurrentUICulture in the controller: Check MVC3-multi-language  If you're using the default view engines then local resources work in the views. However if you need to grab resource strings within a controller action you can't get local resources and have to use global resources. This makes sense when you think about it because local resources are local to an aspx page and in the controller you haven't even selected your view. Yes this does make sense for controllers. But what about View Models? They are (often) specific to the view and it would make sense to reference view specific resources in the view model code. I wonder if there is a mechanism to make a resource file local to a view and its view model... will this also work in razor view engine?  Yes resources are still the best way to support multiple languages in the .NET environment. Because they are easy to reference and even easier to add new languages. Site.resx Site.en.resx Site.en-US.resx Site.fr.resx etc... So you are right still use the resource files. How should these be placed for this to work?",asp.net-mvc internationalization multilingual,7.093474718743448E-4,0.7678550899084089,0.02014884653399307,0.1996033596974763,0.01168335638824747
3975,How do I know which SQL Server 2005 index recommendations to implement if any? We're in the process of upgrading one of our SQL Server instances from 2000 to 2005. I installed the performance dashboard (http://www.microsoft.com/downloads/details.aspx?FamilyId=1d3a4a0d-7e0c-4730-8204-e419218c1efc&displaylang=en) for access to some high level reporting. One of the reports shows missing (recommended) indexes. I think it's based on some system view that is maintained by the query optimizer. My question is what is the best way to determine when to take an index recommendation. I know that it doesn't make sense to apply all of the optimizer's suggestions. I see a lot of advice that basically says to try the index and to keep it if performance improves and to drop it if performances degrades or stays the same. I wondering if there is a better way to make the decision and what best practices exist on this subject. Your best researching the most common type of queries that happen on your database and creating indexes based on that research. For example if there is a table which stores website hits which is written to very very often but hardly even read from. Then don't index the table in away. If how ever you have a list of users which is access more often than is written to then I would firstly create a clustered index on the column that is access the most usually the primary key. I would then create an index on commonly search columns and those which are use in order by clauses.  First thing to be aware of: When you upgrade from 2000 to 2005 (by using detach and attach) make sure that you: Set compability to 90 Rebuild the indexes Run update statistics with full scan If you don't do this you will get suboptimal plans. IF the table is mostly write you want as few indexes as possible IF the table is used for a lot of read queries you have to make sure that the WHERE clause is covered by indexes.  The advice you got is right. Try them all one by one. There is NO substitute for testing when it comes to performance. Unless you prove it you haven't done anything.,sql-server sql-server-2005,0.04666554971813563,0.014778351947609526,0.0028984651371618065,0.2336486111090524,0.7020090220880406
4782,How much database performance overhead when using LINQ? How much database performance overhead is involved with using C# and LINQ compared to custom optimized queries loaded with mostly low-level C both with a SQL Server 2008 backend? I'm specifically thinking here of a case where you have a fairly data-intensive program and will be doing a data refresh or update at least once per screen and will have 50-100 simultaneous users. Everything you need to know is right here (it's a five part series): That information is pretty old (beta 2). Also the author acknowledges that the normal ado.net queries he used did not make use of prepared statements. The point is that it isn't a good representation of dlinq performance. In my experience the overhead is minimal provided that the person writing the queries knows what he/she is doing and take the usual precautions to ensure the generated queries are optimal that the necessary indexes are in place etc etc. In other words the database impact should be the same; there is a minimal but usually negligible overhead on the app side. That said... there is one exception to this; if a single query generates multiple aggregates the L2S provider translates it to a large query with one sub-query per aggregate. For a large table this can have a significant I/O impact as the db I/O cost for the query grows by magnitudes for each new aggregate in the query. The workaround for that is of course to move the aggregates to stored proc or view. Matt Warren has some sample code for an alternative query provider that translate that kind of queries in a more efficient way. Resources: https://connect.microsoft.com/VisualStudio/feedback/ViewFeedback.aspx?FeedbackID=334211 http://blogs.msdn.com/mattwar/archive/2008/07/08/linq-building-an-iqueryable-provider-part-x.aspx  Thanks Stu. Bottom line seems to be that LINQ to SQL probably doesn't have a significant database performance overhead with the newer versions if you are able to use a compiled select and the slower functions of updating are likely to be faster unless you have a REALLY sharp expert doing most of the coding.,sql-server linq performance linq-to-sql,0.024936509146977915,0.01290774127720004,0.03417602365164524,0.21198500346854393,0.7159947224556328
3630,"SQLite vs MySQL SQLite is a single-file based database and MySQL is a normal database. That's great but I'm not sure which is faster where or better for what...what are the pros and cons of each option? SQLite is not a flat-file database it stores data in structured files with indexes. A flat-file database would use ""flat"" files e.g. fixed-record or CSV. SQLite is not a flat-file database and MySQL is not a ""normal"" database. They are both implementations of SQL. SQLite is no less ""relational"" than MySQL. It might be better to say that SQLite is an *embedded* database while MySQL is a *standalone* database. You don't embed the database engine of MySQL inside your app because it is too big; rather you access the database via a running instance over some access interface and doesn't require a host application of any sort. @LarryLustig: It is a bit less relational than MySQL you can start by the distinct types of JOINs supported. @StevenHaryanto: It doesn't seem to be free though while SQLite is public domain. For Php use: Mysql is good if you perform often INSERT or UPDATE queries. sqlLite is better for SELECT queries (file access is always better) moreover you can store your sqlite DB in memory! Very very effective and very good in production! I am working on this project: ""We run a Java program to collect some news coming from RSS let's say 10 000 news. Every night at 00:00 we store these news in a sqlite. Then we only perform READ queries to the sqlite it's fast easy simple scalable bcoz we simply copy/paste the .db file on severals server! My conclusion is to use sqlite like a read only cache. For client application use: sqlite is really nice bcoz you have a real DB (SQL queries) w/o a server running on the user's computer. Before I remember having used in Visual Basic ini files or reg DB hu dirty! I think that's an often overlooked feature of SQLite. Stupid simple DB backups and moves. No config points no connection checks just a file.  Another difference: SQLite supports transactions without the overhead of InnoDB. I would consider SQLite for a website running on a VPS with very little memory.  It seems for a huge majority of sites using MySQL SQLite would be more than adequate. It just seems to be a mindset that ""if it's anything resembling production I have to use MySQL!"" I would say that if you don't have to do any performance-fiddling with MySQL you can get away with using SQLite.. OTOH that mindset of ""we'll just go with what is considered enterprise-y without devoting any time to analyzing actual requirements"" extends into the product. I know 2 projects the original aging project and its supposed replacement where the original project works with ~30 tables in a BDB-like database and the replacement project has ~600 MySQL tables. I'll call it self-fulfilling crap: ""we're an important enterprise-y project so we'll need a real database"". @dbr: I second what you said - most people could do more with SQLite without the need for MySQL. We recently had a project based on SQLite and it still works to this day without a glitch. I think it's again down to the mindset that if it's not MySQL then it's not database enough.  But SQLite handles multi-user fine if all you are doing is reading. Reading does not require a lock. So SQLite can run any well trafficed site/app that doesn't require modifying records. Or where 1 person is doing the editing. Like a blog with no comments. It's small available on all platforms and free. Also open source and the code is well documented. So change what you want. I think SQLite is fit for mass production. Look at Firefox iTunes etc etc. And to the OP: Compared to any other SQL server MySQL is easy peasy to set up. I mean com'on on Windows you install answer a few questions and you off. Pretty much the same on Mac or any Linux destro. Can you please define ""well trafficed""? Any number? @DanMan absolutely not possible because it depends on how good your hardware is.  Despite the various answers here it seems to me that the balance has changed slightly when SQLite introduced WAL mode. From what I have been able to discover this allows simulataneous updates without getting (as much) lock contention. The big downsite of sqlite has been that during a transaction that involves updates to the database the entire database is locked rather than the much finer grained locking of other databases. With WAL mode each user is effectively able to see a consistent view of data even if other people are writing to the database - and therefore the locks that sqlite applies can be applied less frequently. The documentation about when the WAL is re-encorporated into the main database is not as clear as it might be and it turns out that that the last connection to close will write it back (as well as the other mechanisms provided). In a scenario where the sqlite database is supporting a web site provided there is a time when there is not a web page request in progress the wal gets re-incorporated. The 100K hits/day figure gives about a 1.15 sec per hit so unless the database uses lots of queries per page at the end of most page requests or just after a burst if that is how they come the WAL will be written back. That is of course if it needs to be - most hits are likely to be of a read only nature. The other thing that seems to be important with a sqlite based web site is to ensure all the queries are encased in a single transaction covering the entire page display. Some tests on my desktop computer showed about 7 inserts per second when each one was a transaction and 1000/second when they were all encased in a single transaction. With the above caveats about what slows SQLite down - the upside is that its a library with the code running in the process that calls it compared to mysql where there is an interprocess communication for each sql statement. This should make sqlite faster especially when complex joins etc are done in code rather than sql. What I really like is the simplicity of backup up and restoring the database. It is slightly simplistic to say you just need to copy the file since a transaction may be going on when you are doing that - but there is a backup api which is used by the command line utility sqlite3 /path/to/live.db '.backup /backup/path.db' to get a consistent snapshot in the cases where you can't stop the processes doing the updates. +1 Good answer. A lot of the answers above are based on SQLite before WAL was introduced. It really does help to improve the performance of the database writes. You can tune when the WAL file is merged back to the db. By default it merges when it reaches a certain size I am pretty sure. @devshorts: True. Furthermore lets not forget that MyISAM also locks the whole table (and you don't have transactions). InnoDB has transactions and row-level locking but it's a huge memory hog. While SQLite has database-locking you could get away by using two simple techniques: enclose write queries in transactions to minimize lock contention (SQLite wins vs MyISAM) and use the `ATTACH DATABASE` command to separate tables into distinct DB files.  SQLite is extremely fast for read-heavy operations. This is particularly true on an OS like Linux which caches commonly-read files into RAM if you do reads almost exclusively then you can get much better performance out of SQLite than MySQL (or any other DBMS for that matter) because you avoid massive amounts of overhead. It's as fast as simply reading a static file (because that's exactly what it is). SQLite does file-level (i.e. entire-database) locking though. This means that any time any user writes to the database everyone else is locked out until that operation complete. This absolutely destroys performance on write-heavy sites where updates are the norm. MySQL on the other hand does table-level or row-level locking which allows multiple simultaneous writes. MySQL was explicitly built for a multi-user environment while SQLite was explicitly not. For most reader-centric sites (e.g. CMS/blog) SQLite will be faster or at least fast enough. Forums tend to do a lot of arguably unnecessary updates (e.g. fastidiously recording page views) which makes it a poor fit for SQLite (and arguably MySQL) unless you can factor out that silly behavior. Using SQLite for an e-commerce site is probably asking for trouble as well. source for ""extremely fast for read-heavy operations."" ? and does it mean faster then mysql? @DanielMagnusson In a locking-free simple data-model scenario sqlite beats the pants off of mysql (and pretty much everything else for that matter). The overhead is very very low. Just the fact that it's a library rather than a service alone means that it's not even fair to compare the two.  Sqlite is best for standalone application like android applications whereas MySql is best suited for web applications with large incoming traffic and high concurrency.  I use sqlite as development db for website and then deploy to mysql on production. This is easier to setup and since data are stored in simple flat file you can copy / move them like you want (great when you try to make major structure change but want revert back option). I also use sqlite as desktop apps file save format for anything that look/sound/smell like ""save""/""save as""/""load""/""import""/""export"".  SQLite is great for testing and prototyping or for embedding in applications. MySQL is worthy of (large scale) production environments. This site has guidance on when to use SQLite Here is my personal summary: SQLite: easier to setup great for temporary (testing databases) great for rapid development great for embedding in an application doesn't have user management doesn't have many performance features doesn't scale well. MySQL: far more difficult/complex to set up better options for performance tuning can scale well if tuned properly can manage users permissions etc. This answer could be much better for all its upvotes. None of the bullet-points are given with examples so it sounds quite... arbitrary. The link to the SQLite page is more descriptive. I believe what was meant by 'not fit for production' is that in most production environments multiple machines are involved. In a situation where you may have multiple servers mirroring or having different database sets that are queried from additional servers MySQL has advantages since it's designed to be accessed remotely. You should put there ""MySQL is fast SQLite3 is slow"". The speed difference is considerably large. I used it when my site had 60k users but when i had 70k/day visitors it was impossible for the server to handle. But i used a slightly modded version of SQLite. So... I'm a little bit sceptic about their 100k/day statement. I'd disagree with the ""not fit for production"" comment for SQLite. For its intended purpose (little concurrency) SQLite is perfectly suitable for production use. I wouldn't run a public-facing web site with it but there are lots of applications where SQLite is a perfect fit. According to the SQLite page it's appropriate for sites that get up to 100k hits/day. That covers a lot of ground. ""Not fit for production"" is quite a generalization. SQLite is used by Google Adobe Mozilla Opera and many others. Even if you are trying to say to say that it is not fit for ""production"" use on a multi user environment you are incorrect. I think the generalization is OK here though perhaps the wording could be better? MySQL is *more* oriented towards enterprise production use than SQLite. Maybe would be better to say ""less fit for large scale production"" or ""more fit for large scale production"" SQLite is perfectly fit for even heavy usage applications that are read-mostly. You run into concurrency issue with simultaneous updates and even there SQLite can support a ""reasonable"" load as long as the updates are not of long duration. Again not to sound repetitive - I just want to generalize about which is a better tool for various jobs.  Justin's answer seems to be evaluating from the perspective of a multiuser app (and is a good evaluation). It's good to note though that sqlite has a lot of ""single user"" production applications. By going single user you get rid of the security and concurrency issues. This allows you access to data via SQL without the overhead of running a server. In practical terms they are great for ""personal databases"". Adium X the sorta-pidgin-port for Mac OS X uses sqlite for its chat logs. I've not personally confirmed this but my understanding is that the ""awesome bar"" in Firefox 3 is implemented using sqlite. Also Mac OS X has an entire data storage API that's built on top of sqlite (which now that I think about it is probably why Adium X is using it). I believe the security issues are addressed at the OS layer (unix file permissions etc). So while sqlite is not appropriate for large multiuser production applications it works quite well for single user production apps.  SQLite is being used a lot in client-side data stores: Firefox uses it extensively various apps Apple wrote for the iPhone use it yum on Linux was rewritten to use it. It's probably more flexible (especially in data structures and indexing) and easy to use than the Berkeley DB's and custom binary formats that some of these things previously depended upon. All those things have something in common: only one process/thread will probably want to write to the database at a time and a relatively small number of things are going to want to read from it. SQLite blocks all other IO on the table during a write which isn't so much for multi-user/multi-threaded the whole table when you start doing an update. If you prototype with SQLite be careful. It's ""weakly typed"" by default -- you can put a string into an integer column unless you enable strict affinity mode and I'm not sure if that's been implemented yet. Aren't sqlite locks per-database (and not per-table unless separate files are used for tables)? Strict affinity has still not been implemented. But you can emulate it by giving every column constraints like `CHECK(typeof(x) = 'integer')`. I forget why (Portability for the non-abstracted portions of my SQLAlchemy schema I think) but I was doing that using `CHECK((x + 0) = x)` instead.  From personal experience I'd say SQLite is production worthy just not when you're running a web site like stack overflow 8^D I have two applications that use SQLite for the primary data source since they are database style applications. It is FAR easier to deploy this than the Microsoft equivalent and providing updates is as simple as zipping up the file and having the user download and unpack it. In addition you can use it for serializing basic objects without the hassle of versioning/updates. I will admit part of my dilemma most likely stems from taking my first crack at things but I had developed a custom object I wanted serialized to a file followed all the recommended norms and then had my application not be able to read previous versions when I added a new field. With SQLite you can modify to your hearts delight and not break anything.  To echo what @jaredg said - SQLite won't handle multi-user or even multi-threaded use since writes lock the database. That means you can't even read from the database while it's being updated. More on my experiences with it here. Just wanted to chime in here that SQLite has write ahead logging now. It doesn't need to lock the whole db on writes just the write ahead file. If your query isn't in the write ahead file then you aren't blocked by a write and if it is then you wait for that write to finish before getting the data. It made a big difference in concurrent applications and its easy to set with a pragma command Sqlite does handle multi user and multi threaded reading. tuinstoel - when you say it handles it does it handle it by locking the entire database when any thread writes to the database requiring all other threads to wait or retry unti they succeed? That's all it did when I last checked. I just wanted to say that you can do multi-user and multi-threaded reading (select).  Check Appropriate Uses For SQLite on the SQLite homepage. I think it is quite reasonable and it is hard to add anything more. @Cashcow: you misunderstand: when I said why recreate the wheel I meant why copy/paste the information when it's clearly spelled out on that site and unlikely to ever disappear @jcollum is it actually reinventing the wheel or just recognising you do not use the same type of wheel on a truck that you use on a bicycle. @CashCow: I don't understand. @jcollum Different types of vehicles require different types of wheel. And different applications require different types of database. Maybe you could elaborate by adding some particularly useful points from that article to your answer. One day that link might go away. Thanks.  We heavily use both SQLite and MySQL in production. The SQLite databases get used where we have a large amount of mostly read-only data. We build this datasource from a large number of flat files held in our subversion repository and then distribute copies of the data to production nodes which require access to it. Profiling SQLite is much trickier than with MySQL - particularly if you're wanting to get data from your production nodes. This is something you'd have to do in your application. It's also less than straightforward to have SQLite's query planner tell you what it's going to do with certain queries which makes optimisation tricky.  SQLite is a single-tier database it embedded into standalone software. That way end users don't have to install database software separately. At the same time allows developers enjoy the beaut of rdbms worry less on read/write/manipulate/search/sort/query/... data. MySql is a multi-tier database users/applications are connecting to a centralized database system. Some data are meant to be together and only work if they store at the same place... At least logically. Any other ""differences/pros/cons"" are probable irrelevant and mostly incorrect.  First you need to understand what MySQL is. It's actually a process doing writes and reads on multiple files with very optimized algorithms. In SQLite you are the process and because by default SQLite is much faster (being almost the same as the native read/write to file) if you are smart and implement only what you need (using multiple files) you will get better performance and in the end better app. MySQL is an easy way and it has all the features but 70% of them you don't need and they slow it down. However it has all what enterprise needs and you know it will be always up to the task. However I've done all my project (big enterprise projects) in SQLite and the latest one actually had too much writes (70 processes each one doing few hundreds of writes every second) which was too much for MySQL process to handle. Switched it over to multiple SQLite databases and problem solved. CPU and memory usage minimum compared to MySQL. yeah have fun maintaining that. It sounds horrible from an adminstration backup and support plan. What? How exactly is mysql cluster spread over multiple instances easy for administration backup and support plan? I was obviously pointing out how capable people can do it and how people behind mysql could think. Again not you capable people who want and know how to write specific reliable software 24/7.  MySQL and SQLite are two platforms for different applications. SQLite is perfect for standalone apps or databases and queries that are very light. MySQL is perfect for client-server apps and more complex deploys.  There is an excellent interview with D. Richard Hipp creator of SQLite on FLOSS Weekly. In this interview he discusses when and when not to use SQLite among many other things. FLOSS Weekly 26: SQLite",mysql database sqlite comparison,9.954059157885398E-4,0.010913726885091207,0.028788555127215155,0.24260823071537138,0.7166940813565338
4630,How can I Java webstart multiple dependent native libraries? Example: I have two shared objects (same should apply to .dlls). The first shared object is from a third-party library we'll call it libA.so. I have wrapped some of this with JNI and created my own library libB.so. Now libB depends on libA. When webstarting both libraries are places in some webstart working area. My java code attempts to load libB. At this point the system loader will attempt to load libA which is not in the system library path (java.library.path won't help this). The end result is that libB has an unsatisfied link and cannot be used. I have tried loading libA before libB but that still does not work. Seems the OS wants to do that loading for me. Is there any way I can make this work other than statically compiling? Static compilation proved to be the only way to webstart multiple dependent native libraries.  I'm not sure if this would be handled exactly the same way for webstart but we ran into this situation in a desktop application when dealing with a set of native libraries (dlls in our case). Loading libA before libB should work unless one of those libraries has a dependency that is unaccounted for and not in the path. My understanding is that once it gets to a system loadLibrary call (i.e. Java has found the library in its java.library.path and is now telling the OS to load it) - it is completely dependent on the operating system to find any dependent libraries because at that point it is the operating system that is loading the library for the process and the OS only knows how to look in the system path. That seems hard to set in the case of a Webstart app but there is a way around this that does not involve static compiling. You may be able to shuffle where your libraries are - I am unsure If you use a custom classloader you can override loadLibrary and findLibrary so that it can locate your libraries from within a jar in your classpath and if you also make it aware of your native library dependencies (i.e. libB depends on libA depends on libX then when loading libB you can catch yourself and ensure you load libA first and in checking that notice and load libX first. Then the OS doesn't try to find a library that isn't in your path. It's klunky and a bit painful but ensuring Java finds them and loads them all in the correct order can work. Classloaders cannot fix the problem since the OS resolves the native dependecies.  Are both native libraries packaged into a signed jar which is listed as <nativelib ...> In the JNLP file?,java jni java-web-start nativelibrary,0.008636044402383102,0.911750238756883,0.002398746675166379,0.05843776181672059,0.018777208348846854
1070,"Process size on UNIX What is the correct way to get the process size on Solaris HP-UX and AIX? Should we use top or ps -o vsz or something else? The exact definitions of ""vsize"" ""rss"" ""rprvt"" ""rshrd"" and other obscure-looking abbreviations vary from OS to OS. The manual pages for the ""top"" and ""ps"" commands will have some sort of description but all such descriptions are simplified greatly (or are based on long-extinct kernel implementations). ""Process size"" as a concept is fiendishly difficult to pin down in the general case. Answers in specific instances depend heavily on the actual memory management implementation in the OS and are rarely as satisfying as the tidy ""process size"" concept that exists in the minds of most users (and most developers). For example none of those numbers (nor likely any combination of them) can be used to tell you exactly how many such processes can run at once in a given amount of free memory. But really your best bet is to come at it from that end: why do you want this number and what will you use it for? Given that information I think you'll get more useful answers.  On Solaris you can get detailed information on a process's memory usage with the pmap command. In particular pmap -x <pid> shows you how much of a process's memory is shared and how much is specifically used by that process. This is useful for working out the ""marginal"" memory usage of a process -- with this technique you can avoid double-counting shared libraries.  Yes you are right to lock at the VSZ. ""ps u"" will give you the VSZ and RSS which are the virtual memory size and resident set size. The RSS is how much physical memory has been allocated to the process and the VSZ is the virtual memory size of the process. If you have several copies of a program running a lot of the memory in the VSZ will be shared between those processes. The VSZ number is useless if what you are interested in is memory consumption. VSZ measures how much of the process's *virtual* memory space has been marked by the process as memory that should be mapped by the operating system if the process happens to touch it. But it has nothing to do with whether that memory has actually been touched and used. VSZ is an internal detail about how a process does memory allocation — how big a chunk of unused memory it grabs at once. Look at RSS for the count of memory pages it has actually started using.  I summed up the resident set size for all processes like this (as root): ps ax -o rss | awk '{rss += $1;} END { print rss}'",unix size,9.979186062559426E-4,0.7587150428289695,0.028345641097074388,0.18894362485925942,0.022997772608440662
3196,"SQL query count and group by If I have data like this: +---+----+ |Key|Name| +---+----+ |1 |Dan | +---+----+ |2 |Tom | +---+----+ |3 |Jon | +---+----+ |4 |Tom | +---+----+ |5 |Sam | +---+----+ |6 |Dan | +---+----+ What is the SQL query to bring back the records where Name is repeated 2 or more times? So the result I would want is  +---+ |Tom| +---+ |Dan| +---+ This could also be accomplished by joining the table with itself SELECT DISTINCT t1.name FROM tbl t1 INNER JOIN tbl t2 ON t1.name = t2.name WHERE t1.key != t2.key;  select name from table group by name having count(name) > 1  Couldn't be simpler... Select Name Count(Name) As Count From Table Group By Name Having Count(Name) > 1 Order By Count(Name) Desc This could also be extended to delete duplicates: Delete From Table Where Key In (  Select  Max(Key)  From  Table  Group By  Name  Having Count(Name) > 1) For delete you should leave only the Min(key) and delete other entries. Query should be modified as ""...where Key NOT IN ( select Min(Key) ....""",sql,0.002361438536776977,0.41350513237834513,0.0204955144768588,0.012357226654911893,0.5512806879531073
1024,"How do I fix 'Unprocessed view path found' error with ExceptionNotifier plugin in rails 2.1? After upgrading a rails 1.2 website to 2.1 the ExceptionNotifier plugin no longer works complaining about this error: ActionView::TemplateFinder::InvalidViewPath: Unprocessed view path found: ""/path/to/appname/vendor/plugins/exception_notification/lib/../views"". Set your view paths with #append_view_path #prepend_view_path or #view_paths=. What causes it and how do I fix it? You ought to upgrade to the newest Exception Notification plugin which is in its new home at GitHub.  This was caused by a change in rails 2.1 which prevents rails from loading views from any arbitrary path for security reasons. There is now an updated version of the plugin on github so the solution is to use that. The old solution here for posterity To work around it edit init.rb under your vendor/plugins/exception_notification directory and add the following code to the end ActionController::Base.class_eval do append_view_path File.dirname(__FILE__) + '/lib/../views' end This adds the ExceptionNotifier plugins' views folder to the list so it is allowed to load them. thanks i have to keep old plugin so the old solution si pretty handy for me",ruby-on-rails ruby exception plugins,0.0018234900435536716,0.3432859405005198,0.6273042264879749,0.009542183470048114,0.018044159497903566
80,"SQLStatement.execute() - multiple queries in one statement I've written a database generation script in SQL and want to execute it in my Adobe AIR application: Create Table tRole ( roleID integer Primary Key roleName varchar(40) ); Create Table tFile ( fileID integer Primary Key fileName varchar(50) fileDescription varchar(500) thumbnailID integer fileFormatID integer categoryID integer isFavorite boolean dateAdded date globalAccessCount integer lastAccessTime date downloadComplete boolean isNew boolean isSpotlight boolean duration varchar(30) ); Create Table tCategory ( categoryID integer Primary Key categoryName varchar(50) parent_categoryID integer ); ... I execute this in Adobe AIR using the following methods: public static function RunSqlFromFile(fileName:String):void { var file:File = File.applicationDirectory.resolvePath(fileName); var stream:FileStream = new FileStream(); stream.open(file FileMode.READ) var strSql:String = stream.readUTFBytes(stream.bytesAvailable); NonQuery(strSql); } public static function NonQuery(strSQL:String):void { var sqlConnection:SQLConnection = new SQLConnection(); sqlConnection.open(File.applicationStorageDirectory.resolvePath(DBPATH); var sqlStatement:SQLStatement = new SQLStatement(); sqlStatement.text = strSQL; sqlStatement.sqlConnection = sqlConnection; try { sqlStatement.execute(); } catch (error:SQLError) { Alert.show(error.toString()); } } No errors are generated however only tRole exists. It seems that it only looks at the first query (up to the semicolon- if I remove it the query fails). Is there a way to call multiple queries in one statement? What about making your delimiter something a little more complex like "";\n"" which would not show up all that often. You just have to ensure when creating the file you have a line return or two in there. I end up putting two ""\n\n"" into the creation of my files which works well.  The SQLite API has a function called something like sqlite_prepare which takes one statement and prepares it for execution essentially parsing the SQL and storing it in memory. This means that the SQL only has to be sent once to the database engine even though the statement is executed many times. Anyway a statement is a single SQL query that's just the rule. The AIR SQL API doesn't allow sending raw SQL to SQLite only single statements and the reason is likely that AIR uses the sqlite_prepare function when it talks to SQLite.  I wound up using this. It is a kind of a hack but it actually works pretty well. The only thing is you have to be very careful with your semicolons. : D var strSql:String = stream.readUTFBytes(stream.bytesAvailable); var i:Number = 0; var strSqlSplit:Array = strSql.split("";""); for (i = 0; i < strSqlSplit.length; i++){ NonQuery(strSqlSplit[i].toString()); } Just realized how badly this would fail if a semicolon appeared in a varchar field.",flex actionscript-3 air,0.006213911838312715,0.1004916031003217,0.0017259754196975244,0.004292566310266594,0.8872759433314015
1037,"Displaying Flash content in a C# WinForms application What is the best way to display Flash content in a C# WinForms application? I would like to create a user control (similar to the current PictureBox) that will be able to display images and flash content. It would be great to be able to load the flash content from a stream of sorts rather than a file on disk. I upvoted Sven's answer but just a little note: using the WebBrowser component is quite manageable and in fact it's intended for exactly this kind of use (embedding in application UIs). You can point the browser control at a URL sure but you can also exactly specify the content it contains respond to events and so forth. There's even an embedded resource protocol/scheme (res://) which you can use to reference embedded resources. But obviously if the Flash component itself is clean that's a better way to go :) While I haven't used a flash object inside a windows form application myself I do know that it's possible. In Visual studio on your toolbox choose to add a new component. Then in the new window that appears choose the ""COM Components"" tab to get a list in which you can find the ""Shockwave Flash Object"" Once added to the toolbox simply use the control as you would use any other ""standard"" control from visual studio. three simple commands are available to interact with the control: AxShockwaveFlash1.Stop() AxShockwaveFlash1.Movie = FilePath & ""\FileName.swf"" AxShockwaveFlash1.Play() which I think are all self explanatory. It would be great to be able to load the flash content from a stream of sorts rather than a file on disk. I just saw you are also looking for a means to load the content from a stream and because I'm not really sure that is possible with the shockwave flash object I will give you another option (two actually). the first is the one I would advise you to use only when necessary as it uses the full blown ""webbrowser component"" (also available as an extra toolbox item) which is like trying to shoot a fly with a bazooka. of course it will work as the control will act as a real browser window (actually the internet explorer browser) but its not really meant to be used in the way you need it. the second option is to use something I just discovered while looking for more information about playing flash content inside a windows form. F-IN-BOX is a commercial solution that will also play content from a given website URL. (The link provided will direct you to the .NET code you have to use). Note: in VS 2013 option in toolbox called ""Choose Items..."" when right click in toolbox.  Sven you reached the same conclusion as I did: I found the Shockwave Flash Object all be it from a slightly different route but was stumped on how to load the files from somewhere other than file on disk/URL. The F-IN-BOX although just a wrapper of the Shockwave Flash Object seems to provide much more functionality which may just help me! Shooting flys with bazookas may be fun but an embeded web brower is not the path that I am looking for. :) There was a link on Adobe's site that talked about ""Embedding and Communicating with the Macromedia Flash Player in C# Windows Applications"" but they seem to have removed it :( Broken Link ... Sorry about the broken link looks like Adobe has removed the page.",c# winforms flash adobe macromedia,8.247482491550279E-4,0.19864804889412924,0.007158195911968072,0.2483445788543375,0.5450244280904102
6,"Why doesn't the percentage width child in absolutely positioned parent work? I have an absolutely positioned div containing several children one of which is a relatively positioned div. When I use a percentage-based width on the child div it collapses to 0 width on IE7 but not on Firefox or Safari. If I use pixel width it works. If the parent is relatively positioned the percentage width on the child works. Is there something I'm missing here? Is there an easy fix for this besides the pixel-based width on the child? Is there an area of the CSS specification that covers this? Does the parent div have a defined width either pixel or percentage? Not 100% sure but I think in IE7 the parent div needs a defined width for child percentage divs to work correctly.  Here is some sample code. I think this is what you are looking for. The following displays exactly the same in Firefox 3 (mac) and IE7. <!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""> <html xmlns=""http://www.w3.org/1999/xhtml""> <head> <style> #absdiv { position: absolute; left: 100px; top: 100px; width: 80%; height: 60%; background: #999; } #pctchild { width: 60%; height: 40%; background: #CCC; } #reldiv { position: relative; left: 20px; top: 20px; height: 25px; width: 40%; background: red; } </style> </head> <body> <div id=""absdiv""> <div id=""reldiv""></div> <div id=""pctchild""></div> </div> </body> </html>  I think this has something to do with the way the hasLayout property is implemented in the older browser. Have you tried your code in IE8 to see if works in there too? IE8 has a Debugger (F12) and can also run in IE7 mode.  Why doesn’t the percentage width child in absolutely positioned parent work in IE7? Because it's Internet Exploder Is there something I'm missing here? That is to raise your co-worker's / clients' awareness that IE sucks. Is there an easy fix besides the pixel-based width on the child? Use em units as they are more useful when creating liquid layouts as you can use them for padding and margins as well as font sizes. So your white space grows and shrinks proportionally to your text if it is resized (which is really what you need). I don't think percentages give a finer control than ems; there's nothing to stop you specifying in hundredths of ems (0.01 em) and the browser will interpret as it sees fit. Is there an area of the CSS specification that covers this? None as far as I remember em's and %'s were intended for font sizes alone back at CSS 1.0.  IE prior to 8 has a temporal aspect to its box model that most notably creates a problem with percentage based widths. In your case here an absolutely positioned div by default has no width. Its width will be worked out based on the pixel width of its content and will be calculated after the contents are rendered. So at the point IE encounters and renders your relatively positioned div its parent has a width of 0 hence why it itself collapses to 0. If you would like a more in depth discussion of this along with lots of working examples have a gander here.",html css css3 internet-explorer-7,0.005777639036135257,0.855705791321096,0.03169438610667269,0.08924502701754793,0.01757715651854809
3881,"IllegalArgumentException or NullPointerException for a null parameter? I have a simple setter method for a property and null is not appropriate for this particular property. I have always been torn in this situation: should I throw an IllegalArgumentException or a NullPointerException? From the javadocs both seem appropriate. Is there some kind of an understood standard? Or is this just one of those things that you should do whatever you prefer and both are really correct? Holy war question! It doesn't matter too much either way. isn't the current trend not to check for null anyway? I was all in favour of throwing IllegalArgumentException for null parameters until today when I noticed the java.util.Objects.requireNonNull method in Java 7. With that method instead of doing: if (param == null) { throw new IllegalArgumentException(""param cannot be null.""); } you can do: Objects.requireNonNull(param); and it will throw a NullPointerException if the parameter you pass it is null. Given that that method is right bang in the middle of java.util I take its existence to be a pretty strong indication that throwing NullPointerException is ""the Java way of doing things"". I think I'm decided at any rate. Note that the arguments about hard debugging are bogus because you can of course provide a message to NullPointerException saying what was null and why it shouldn't be null. Just like with IllegalArgumentException. One added advantage of NullPointerException is that in highly performance critical code you could dispense with an explicit check for null (and a NullPointerException with a friendly error message) and just rely on the NullPointerException you'll get automatically when you call a method on the null parameter. Provided you call a method quickly (i.e. fail fast) then you have essentially the same effect just not quite as user friendly for the developer. Most times it's probably better to check explicitly and throw with a useful message to indicate which parameter was null but it's nice to have the option of changing that if performance dictates without breaking the published contract of the method/constructor. Interesting find in Java7 Guava `Preconditions.checkNotNull(arg)` also throws NPE. This isn't really adding more weight to NullPointerException for illegal null ARGUMENTS. Both the JDK requireNonNull() and Guava checkNotNull() can be called anywhere in the code with any object. You could call them inside a loop for example. requireNotNull() and checkNotNull() could not possibly assume to be invoked with some method arguments and throw IllegalArgumentException. Note that Guava also has Preconditions.checkArgument() which does throw IllegalArgumentException. A fair point by Bogdan though I suspect the typical (and generally intended) use of requireNonNull is for argument checking. If you needed to check that something wasn't null in a loop I'd have thought an assertion would be the typical way.  I wanted to single out Null arguments from other illegal arguments so I derived an exception from IAE named NullArgumentException. Without even needing to read the exception message I know that a null argument was passed into a method and by reading the message I find out which argument was null. I still catch the NullArgumentException with an IAE handler but in my logs is where I can see the difference quickly. I have adopted the ""throw new IllegalArgumentException(""foo == null"")"" approach. You need to log the variable name anyway (to be certain that you are looking at the right sttatement etc)  Apache Commons Lang has a NullArgumentException that does a number of the things discussed here: it extends IllegalArgumentException and its sole constructor takes the name of the argument which should have been non-null. While I feel that throwing something like a NullArgumentException or IllegalArgumentException more accurately describes the exceptional circumstances my colleagues and I have chosen to defer to Bloch's advice on the subject. Note they removed it from commons-lang3: http://apache-commons.680414.n4.nabble.com/commons-lang3-NullArgumentException-missing-td4222254.html  In this case IllegalArgumentException conveys clear information to the user using your API that the "" should not be null"". As other forum users pointed out you could use NPE if you want to as long as you convey the right information to the user using your API. GaryF and tweakt dropped ""Effective Java"" (which I swear by) references which recommends using NPE. And looking at how other good APIs are constructed is the best way to see how to construct your API. Another good example is to look at the Spring APIs. For example org.springframework.beans.BeanUtils.instantiateClass(Constructor ctor Object[] args) has a Assert.notNull(ctor ""Constructor must not be null"") line. org.springframework.util.Assert.notNull(Object object String message) method checks to see if the argument (object) passed in is null and if it is it throws a new IllegalArgumentException(message) which is then caught in the org.springframework.beans.BeanUtils.instantiateClass(...) method.  You should throw an IllegalArgumentException as it will make it obvious to the programmer that he has done something invalid. Developers are so used to seeing NPE thrown by the VM that any programmer would not immediately realize his error and would start looking around randomly or worse blame your code for being 'buggy'. Sorry if a programmer looks around ""randomly"" upon getting any kind of exception... changing the name of an exception isn't going to help much.  The accepted practice if to use the IllegalArgumentException( String message ) to declare a parameter to be invalid and give as much detail as possible... So to say that a parameters was found to be null while exception non-null you would do something like this: if( variable == null ) throw new IllegalArgumentException(""The object 'variable' cannot be null""); You have virtually no reason to implicitly use the ""NullPointerException"". The NullPointerException is an exception thrown by the Java Virtual Machine when you try to execute code on null reference (Like toString()).  Throwing an exception that's exclusive to null arguments (whether NullPointerException or a custom type) makes automated null testing more reliable. This automated testing can be done with reflection and a set of default values as in Guava's NullPointerTester. For example NullPointerTester would attempt to call the following method... Foo(String string List<?> list) { checkArgument(string.length() > 0); // missing null check for list! this.string = string; this.list = list; } ...with two lists of arguments: """" null and null ImmutableList.of(). It would test that each of these calls throws the expected NullPointerException. For this implementation passing a null list does not produce NullPointerException. It does however happen to produce an IllegalArgumentException because NullPointerTester happens to use a default string of """". If NullPointerTester expects only NullPointerException for null values it catches the bug. If it expects IllegalArgumentException it misses it.  Some collections assume that null is rejected using NullPointerException rather than IllegalArgumentException. For example if you compare a set containing null to a set that rejects null the first set will call containsAll on the other and catch its NullPointerException -- but not IllegalArgumentException. (I'm looking at the implementation of AbstractSet.equals.) You could reasonably argue that using unchecked exceptions in this way is an antipattern that comparing collections that contain null to collections that can't contain null is a likely bug that really should produce an exception or that putting null in a collection at all is a bad idea. Nevertheless unless you're willing to say that equals should throw an exception in such a case you're stuck remembering that NullPointerException is required in certain circumstances but not in others. (""IAE before NPE except after 'c'..."")  You should be using IllegalArgumentException (IAE) not NullPointerException (NPE) for the following reasons: First the NPE JavaDoc explicitly lists the cases where NPE is appropriate. Notice that all of them are thrown by the runtime when null is used inappropriately. In contrast the IAE JavaDoc couldn't be more clear: ""Thrown to indicate that a method has been passed an illegal or inappropriate argument."" Yup that's you! Second when you see an NPE in a stack trace what do you assume? Probably that someone dereferenced a null. When you see IAE you assume the caller of the method at the top of the stack passed in an illegal value. Again the latter assumption is true the former is misleading. Third since IAE is clearly designed for validating parameters you have to assume it as the default choice of exception so why would you choose NPE instead? Certainly not for different behavior -- do you really expect calling code to catch NPE's separately from IAE and do something different as a result? Are you trying to communicate a more specific error message? But you can do that in the exception message text anyway as you should for all other incorrect parameters. Fourth all other incorrect parameter data will be IAE so why not be consistent? Why is it that an illegal null is so special that it deserves a separate exception from all other types of illegal arguments? Finally I accept the argument given by other answers that parts of the Java API use NPE in this manner. However the Java API is inconsistent with everything from exception types to naming conventions so I think just blindly copying (your favorite part) of the Java API isn't a good enough argument to trump these other considerations. Effective Java 2nd Edition Item 60: ""Arguably all erroneous method invocations boil down to an illegal argument or illegal state but other exceptions are standardly used for certain kinds of illegal arguments and states. If a caller passes null in some parameter for which null values are prohibited convention dictates that NullPointerException be thrown rather than IllegalArgumentException. Similarly if a caller passes an out-of-range value in a parameter representing an index into a sequence IndexOutOfBoundsException should be thrown rather than IllegalArgumentException."" The JavaDoc for NPE also states the following: ""Applications should throw instances of this class to indicate other illegal uses of the null object."" This one could be more clear :( Unfortunately the validation methods `Validate.notNull` (commons lang) and `Preconditions.checkNotNull` (guava) both throw NPE :-( Although Guava also has Preconditions.checkArgument() throws IllegalArgumentException ... ""When you see IAE you assume the caller of the method at the top of the stack passed in an illegal value. Again the latter assumption is true the former is misleading."" -- It's not at all misleading quite the contrary. Had you not explicitly checked for a null argument value you would have gotten an NPE when you used it. The check is just to make the details of the logic error explicit. There is no *logical* difference between a program that throws an NPE upon derefing null and a program that throws an exception in lieu of derefing null. The exception message should make the cause clear. ""Although Guava also has Preconditions.checkArgument() throws IllegalArgumentException"" -- yes so? That takes a boolean condition; it isn't for use with null arguments. +1 for excellent reasoning  It's a ""Holy War"" style question. In others words both alternatives are good but people will have their preferences which they will defend to the death.  If it's a setter method and null is being passed to it I think it would make more sense to throw an IllegalArgumentException. A NullPointerException seems to make more sense in the case where you're attempting to actually use the null. So if you're using it and it's null NullPointer. If it's being passed in and it's null IllegalArgument.  If it's a ""setter"" or somewhere I'm getting a member to use later I tend to use IllegalArgumentException. If it's something I'm going to use (dereference) right now in the method I throw a NullPointerException proactively. I like this better than letting the runtime do it because I can provide a helpful message (seems like the runtime could do this too but that's a rant for another day). If I'm overriding a method I use whatever the overridden method uses.  In general a developer should never throw a NullPointerException. This exception is thrown by the runtime when code attempts to dereference a variable who's value is null. Therefore if your method wants to explicitly disallow null as opposed to just happening to have a null value raise a NullPointerException you should throw an IllegalArgumentException. JavaDoc on NPE has another opinion: ""Applications should throw instances of this class to indicate other illegal uses of the null object."". Don't be so categorical  I tend to follow the design of JDK libraries especially Collections and Concurrency (Joshua Bloch Doug Lea those guys know how to design solid APIs). Anyway many APIs in the JDK pro-actively throws NullPointerException. For example the Javadoc for Map.containsKey states: @throws NullPointerException if the key is null and this map does not permit null keys (optional). It's perfectly valid to throw your own NPE. The convention is to include the parameter name which was null in the message of the exception. The pattern goes: public void someMethod(Object mustNotBeNull) { if (mustNotBeNull == null) { throw new NullPointerException(""mustNotBeNull must not be null""); } } Whatever you do don't allow a bad value to get set and throw an exception later when other code attempts to use it. That makes debugging a nightmare. You should always the follow the ""fail-fast"" principle. Food for thought: Maybe the reason that NullPointerException doesn't extend IllegalArgumentException is that the former can occur in cases not involving method arguments. @Gili: maybe this problem exists in the first place because Java does not support multiple inheritance. If Java supports MI you'd be able to throw an exception that inherits from both IllegalArgumentException and NullPointerException. The message needs not to include the argument since it would be always null giving: ""null must not be null"" not very useful. :-) Otherwise I agree you can make a ""rich"" NPE with a meaningful message. I have to agree - follow the standard API when in doubt. Not everything in the API is optimal mind you but still it's maintained and iterated by hundreds of developers and used by millions of programmers. Now in Java 7 we have another example of the NPE being used in this way; the Objects.requireNonNull(T obj) method - clearly specified for checking that object references are non-null clearly specified for doing parameter validation in methods/constructors and clearly specified to throw an NPE if the object is null. End of  If you choose to throw a NPE and you are using the argument in your method it might be redundant and expensive to explicitly check for a null. I think the VM already does that for you. The run time wont include a meaningful msg. Actually this comment could derive some other opinion. Let the VM speak in `NPE` yet the programmers speak in `IAE` before the VM if they want to. Expensive? I don't think that == null is that expensive... Beside the null argument can be just stored for latter use and will throw an exception long after the method call making the error harder to track. Or you can create an expensive object before using the null argument and so on. Early detection seems to be a good option.  Voted up Jason Cohen's argument because it was well presented. Let me dismember it step by step. ;-) The NPE JavaDoc explicitly says ""other illegal uses of the null object"". If it was just limited to situations where the runtime encounters a null when it shouldn't all such cases could be defined far more succinctly. Can't help it if you assume the wrong thing but assuming encapsulation is applied properly you really shouldn't care or notice whether a null was dereferenced inappropriately vs. whether a method detected an inappropriate null and fired an exception off. I'd choose NPE over IAE for multiple reasons It is more specific about the nature of the illegal operation Logic that mistakenly allows nulls tends to be very different from logic that mistakenly allows illegal values. For example if I'm validating data entered by a user if I get value that is unacceptable the source of that error is with the end user of the application. If I get a null that's programmer error. Invalid values can cause things like stack overflows out of memory errors parsing exceptions etc. Indeed most errors generally present at some point as an invalid value in some method call. For this reason I see IAE as actually the MOST GENERAL of all exceptions under RuntimeException. Actually other invalid arguments can result in all kinds of other exceptions. UnknownHostException FileNotFoundException a variety of syntax error exceptions IndexOutOfBoundsException authentication failures etc. etc. In general I feel NPE is much maligned because traditionally has been associated with code that fails to follow the fail fast principle. That plus the JDK's failure to populate NPE's with a message string really has created a strong negative sentiment that isn't well founded. Indeed the difference between NPE and IAE from a runtime perspective is strictly the name. From that perspective the more precise you are with the name the more clarity you give to the caller. +1 Yours is well presented *and* correct. Well presented but conveniently wrong on critical facts and logic = sophistry / intellectual dishonesty. The difference between most unchecked exceptions is just the name.  I'm not a Java developer but just from the sound of it it seems like an IllegalArgumentException is called for if you don't want null to be an allowed value and the NullPointerException would be thrown if you were trying to use a variable that turns out to be null. The following answers to this question make persuasive arguments that NullPointerException is the correct exception: http://stackoverflow.com/a/8160/372926 ; http://stackoverflow.com/a/8196334/372926 ; and http://stackoverflow.com/a/6358/372926.  The standard is to throw the NullPointerException. The generally infallible ""Effective Java"" discusses this briefly in Item 42 (in the first edition) or Item 60 (in the second edition) ""Favor the use of standard exceptions"": ""Arguably all erroneous method invocations boil down to an illegal argument or illegal state but other exceptions are standardly used for certain kinds of illegal arguments and states. If a caller passes null in some parameter for which null values are prohibited convention dictates that NullPointerException be thrown rather than IllegalArgumentException."" The exception trace shows the point of the exception so if the difference in the type of the exception causes hell for you or is ""the difference that helps you"" you are doing something very wrong. @ThorbjørnRavnAndersen the stack trace will point you at the exact same spot anyway? certainly doesn't make *that* much of a difference @djechlin it allows you or others to triage the problem early without having to wake a programmer to look at the precise source first. As the original answerer here let me say it just doesn't matter that much. It certainly doesn't warrant 6 years of conversation. Pick one whichever you like and be consistent. The standard as I pointed out is NPE. If you prefer IAE for whatever reasons go for it. Just be consistent. Fantasies and sophistry. Just below MB writes ""Note that the arguments about hard debugging are bogus because you can of course provide a message to NullPointerException saying what was null and why it shouldn't be null. Just like with IllegalArgumentException."" Also these points were covered by Christopher Smith  among others 4 1/2 years ago ... continuing to bring up bogus points is trolling. @JimBalter If nullpointerexceptions are never thrown by _your_ code but only by the JVM when resolving a `.` or an array index you can immediately say whether this is bad data caught by your code or this is unforeseen data resulting in a code malfunction without having to look at the source first. In a production failure where downtime may be expensive this may allow the initial investigator to react correctly without having to wake up a vendor programmer first. I don't necessarily agree with the standard (I could actually go either way on the issue) but that IS what the standard usage throughout the JDK is hence Effective Java making the case for it. I think this is a case of choosing whether or not to follow the standard or do the thing you feel is right. Unless you have a very good reason (and this certainly may qualify) it's best to follow standard practice. I strongly disagree. NullPointerExceptions should only be thrown if the JVM accidentially follows a null reference. That is the difference that helps you when called in to look at code at 3 in the morning. Actually the NullPointerException is supposed to be thrown for ""illegal uses of the null reference"". Indeed encapsulation means you should *know* whether it was caused by the JVM actually dereferencing a bad pointer or the app just rejecting it. Agree with Thorbjorn do yourself a favor and avoid debugging hell.  I think you should definitely throw a IllegalArgumentException and thus fail-fast. Let other developers know by marking it in the JavaDocs and also define constraints on your methods so that they see what happens when they pass an invalid objects. I wrote about this a couple of weeks ago if you want to follow up. This does not actually answer the question. The OP knows that fail-fast is important but wants to know which exception is the better choice under the described circumstances.  The definitions from the links to the two exceptions above are IllegalArgumentException: Thrown to indicate that a method has been passed an illegal or inappropriate argument. NullPointerException: Thrown when an application attempts to use null in a case where an object is required. The big difference here is the IllegalArgumentException is supposed to be used when checking that an argument to a method is valid. NullPointerException is supposed to be used whenever an object being ""used"" when it is null. I hope that helps put the two in perspective. The salient bit is that it is the *application* that is using null not the runtime. So there is a fairly large overlap between ""when a method has been passed an illegal or inappropriate argument"" and ""when an application is using null"". In theory if an app passes a null for a field that requires non-null both criteria are being met.  Actually the question of throwing IllegalArgumentException or NullPointerException is in my humble view only a ""holy war"" for a minority with an incomlete understanding of exception handling in Java. In general the rules are simple and as follows: argument constraint violations must be indicated as fast as possible (-> fast fail) in order to avoid illegal states which are much harder to debug in case of an invalid null pointer for whatever reason throw NullPointerException in case of an illegal array/collection index throw ArrayIndexOutOfBounds in case of a negative array/collection size throw NegativeArraySizeException in case of an illegal argument that is not covered by the above and for which you don't have another more specific exception type throw IllegalArgumentException as a wastebasket on the other hand in case of a constraint violation WITHIN A FIELD that could not be avoided by fast fail for some valid reason catch and rethrow as IllegalStateException or a more specific checked exception. Never let pass the original NullPointerException ArrayIndexOutOfBounds etc in this case! There are at least three very good reasons against the case of mapping all kinds of argument constraint violations to IllegalArgumentException with the third probably being so severe as to mark the practice bad style: (1) A programmer cannot a safely assume that all cases of argument constraint violations result in IllegalArgumentException because the large majority of standard classes use this exception rather as a wastebasket if there is no more specific kind of exception available. Trying to map all cases of argument constraint violations to IllegalArgumentException in your API only leads to programmer frustration using your classes as the standard libraries mostly follow different rules that violate yours and most of your API users will use them as well! (2) Mapping the exceptions actually results in a different kind of anomaly caused by single inheritance: All Java exceptions are classes and therefore support single inheritance only. Therefore there is no way to create an exception that is truly say both a NullPointerException and an IllegalArgumentException as subclasses can only inherit from one or the other. Throwing an IllegalArgumentException in case of a null argument therefore makes it harder for API users to distinguish between problems whenever a program tries to programmatically correct the problem for example by feeding default values into a call repeat! (3) Mapping actually creates the danger of bug masking: In order to map argument constraint violations into IllegalArgumentException you'll need to code an outer try-catch within every method that has any constrained arguments. However simply catching RuntimeException in this catch block is out of the question because that risks mapping documented RuntimeExceptions thrown by libery methods used within yours into IllegalArgumentException even if they are no caused by argument constraint violations. So you need to be very specific but even that effort doesn't protect you from the case that you accidentally map an undocumented runtime exception of another API (i.e. a bug) into an IllegalArgumentException of your API. Even the most careful mapping therefore risks masking programming errors of other library makers as argument constraint violations of your method's users which is simply hillareous behavior! With the standard practice on the other hand the rules stay simple and exception causes stay unmasked and specific. For the method caller the rules are easy as well: - if you encounter a documented runtime exception of any kind because you passed an illegal value either repeat the call with a default (for this specific exceptions are neccessary) or correct your code - if on the other hand you enccounter a runtime exception that is not documented to happen for a given set of arguments file a bug report to the method's makers to ensure that either their code or their documentation is fixed.  the dichotomy... Are they non-overlapping? Only non-overlapping parts of a whole can make a dichotomy. As i see it: throw new IllegalArgumentException(new NullPointerException(NULL_ARGUMENT_IN_METHOD_BAD_BOY_BAD)); This would double the overhead for exception creation and wouldn't really help as catching `NullPointerException` would do nothing. The only thing which could help is `IllegalNullPointerArgumentException extends IllegalArgumentException NullPointerException` but we have no multiple inheritance.  Couldn't agree more with what's being said. Fail early fail fast. Pretty good Exception mantra. The question about which Exception to throw is mostly a matter of personal taste. In my mind IllegalArgumentException seems more specific than using a NPE since it's telling me that the problem was with an argument I passed to the method and not with a value that may have been generated while performing the method. My 2 Cents",java exception null nullpointerexception illegalargumentexception,0.8900835681863724,0.025897723817892966,2.61996438535865E-4,0.07924963443459002,0.00450707712260886
4612,"CSharpCodeProvider Compilation Performance Is CompileAssemblyFromDom faster than CompileAssemblyFromSource? It should be as it presumably bypasses the compiler front-end. CompileAssemblyFromDom compiles to a .cs file which is then run through the normal C# compiler. Example: using System; using System.Collections.Generic; using System.Linq; using System.Text; using Microsoft.CSharp; using System.CodeDom; using System.IO; using System.CodeDom.Compiler; using System.Reflection; namespace CodeDomQuestion { class Program { private static void Main(string[] args) { Program p = new Program(); p.dotest(""C:\\fs.exe""); } public void dotest(string outputname) { CSharpCodeProvider cscProvider = new CSharpCodeProvider(); CompilerParameters cp = new CompilerParameters(); cp.MainClass = null; cp.GenerateExecutable = true; cp.OutputAssembly = outputname; CodeNamespace ns = new CodeNamespace(""StackOverflowd""); CodeTypeDeclaration type = new CodeTypeDeclaration(); type.IsClass = true; type.Name = ""MainClass""; type.TypeAttributes = TypeAttributes.Public; ns.Types.Add(type); CodeMemberMethod cmm = new CodeMemberMethod(); cmm.Attributes = MemberAttributes.Static; cmm.Name = ""Main""; cmm.Statements.Add(new CodeSnippetExpression(""System.Console.WriteLine('f'zxcvv)"")); type.Members.Add(cmm); CodeCompileUnit ccu = new CodeCompileUnit(); ccu.Namespaces.Add(ns); CompilerResults results = cscProvider.CompileAssemblyFromDom(cp ccu); foreach (CompilerError err in results.Errors) Console.WriteLine(err.ErrorText + "" - "" + err.FileName + "":"" + err.Line); Console.WriteLine(); } } } which shows errors in a (now nonexistent) temp file: ) expected - c:\Documents and Settings\jacob\Local Settings\Temp\x59n9yb-.0.cs:17 ; expected - c:\Documents and Settings\jacob\Local Settings\Temp\x59n9yb-.0.cs:17 Invalid expression term ')' - c:\Documents and Settings\jacob\Local Settings\Tem p\x59n9yb-.0.cs:17 So I guess the answer is ""no"" Absolutely correct albeit disappointing. CodeDOM is converted into C# text saved to a temporary file and then the C# compiler (which is developed in C++) is called. I don't know if this is the case for Mono but sadly the CodeDOM is actually slower than writing C# directly. Supposedly csc.exe is being rewritten in C# so in the future there may be a managed way that lets you pass an AST directly to the compiler. However as .NET 3.5 stands now there is currently no way to bypass the compiler frontend other than emitting the IL assembly or bytecode yourself.  I've tried finding the ultimate compiler call earlier and I gave up. There's quite a number of layers of interfaces and virtual classes for my patience. I don't think the source reader part of the compiler ends up with a DOM tree but intuitively I would agree with you. The work necessary to transform the DOM to IL should be much less than reading C# source code.",c# performance compiler,0.7356557177127655,0.14713818811160773,0.09151487818640386,0.01670122933850305,0.008989986650719942
4884,"Javascript keyboard events primer? (or rather: help me with my custom dropdown) I need help finishing my custom built ajax [div] based dynamic dropdown. Basically I have an [input] box which; onkeyup runs an Ajax search which returns a bunch of results in divs and are drawn back in using innerHTML. These divs all have highlights onmouseover so a typical successfull search yields the following structure (pardon the semicode): [input] [div id=results] //this gets overwritten contantly by my AJAX function [div id=result1 onmouseover=highlight onclick=input.value=result1] [div id=result2 onmouseover=highlight onclick=input.value=result2] [div id=result2 onmouseover=highlight onclick=input.value=result2] [/div] It works.. beautifully! looks elegant and is way more complete than any regular dropdown (those results div bring in a lot of information). However I'm missing the most of important functions behind regular HTML elements that is I can't keyboard down or up between ""options"". How do I do this? I know javascript handles keyboard events but; I haven't been able to find a good guide on how to do this. (of course the follow up question to this will eventually end up being: ""can I use to trigger that onclick event) What you need to do is attach event listeners to the div with id=""results"". You can do this by adding onkeyup onkeydown etc. attributes to the div when you create it or you can attach these using JavaScript. My recommendation would be that you use an AJAX library like YUI jQuery Prototype etc. for two reasons: It sounds like you are trying to create an Auto Complete control which is something most AJAX libaries should provide. If you can use an existing component you'll save yourself a lot of time. Even if you don't want to use the control provided by a library all libraries provide event libraries that help to hide the differences between the event APIs provided by different browsers. Forget addEvent use Yahoo!’s Event Utility provides a good summary of what an event library should provide for you. I'm pretty sure that the event libraries provided by jQuery Prototype et. al. provide similar features. If that article goes over your head have a look at this documentation first and then re-read the original article (I found the article made much more sense after I'd used the event library). A couple of other things: Using JavaScript gives you much more control than writing onkeyup etc. attributes into your HTML. Unless you want to do something really simple I would use JavaScript. If you write your own code to handle keyboard events a good key code reference is really handy.  Off the top of my head I would think that you'd need to maintain some form of a data structure in the JavaScript that reflects the items in the current dropdown list. You'd also need a reference to the currently active/selected item. Each time keyup or keydown is fired update the reference to the active/selected item in the data structure. To provide highlighting information on the UI add or remove a class name that is styled via CSS based on if the item is active/selected or not. Also this isn't a biggy but innerHTML is not really standard (look into createTextNode() createElement() and appendChild() for standard ways of creating data). You may also want to see about attaching event handlers in the JavaScript rather than doing so in an HTML attribute.",javascript events keyboard,7.514027083328701E-4,0.9072505047348038,0.001581010361002087,0.0879222547293585,0.002494827466502744
2639,"What are some web-based knowledge-base solutions? I've used a WordPress blog and a Screwturn Wiki (at two separate jobs) to store private company-specific KB info but I'm looking for something that was created to be a knowledge base. Specifically I'd like to see: Free/low cost Simple method for users to subscribe to KB (or just sections) to get updates Ability to do page versioning/audit changes Limit access to certain pages for certain users Very simple method of posting/editing articles Very simple method of adding images to articles Excellent (fast accurate) searching abilities Ability to rate and comment on articles I liked using the Wordpress blog because it allowed me to use Live Writer to add/edit articles and images but it didn't have page versioning (that I could see). I like using Screwturn wiki because of it's ability to track article versions and I like it's clean look but some non-technical people balk at the input and editing. I know you asked this some time ago but I am researching this issue right now. I am interested in knowing what you ended up deciding to use? I am considering KBPublisher as I want it to be on a LAMP server and self hosted. Cerberus - it's more a full featured Help Desk/Issue Tracking system but it has a nice KB solution built in. It can be free but they do have a low cost pay version that is also very good.  We've been using a combination of TWiki OpenGrok for the codebase usenet LotusNotes based system As long as there is a google search appliance pointed at these things I think it's ok to have any or many versions as long as people use them  I second Luke's answer. I can Recommend Confluence and here is why: I tested extensively many commercial and free Wiki based solutions. Not a single one is a winner on all accounts including confluence. Let me try to make your quest a little shorter by summarizing what I have learned to be a pain and what is important: WYSIWYG is a most have feature for the Enterprise. A wiki without it skip it Saying that in reality WYSIWYG doesn't work perfectly. It is more of a feature you must have to get the casual users not be afraid of the monster and start using it. But you and anyone that wants to seriously create content will very quickly get used to the wiki markup. it is faster and more reliable. You need good permissions controls (who can see edit etc' a page). confluence has good but I have my complaints (to complicated to be put here) You will want a good export feature. Most will give you a single page ""PDF"" export but you need much more. For example lets say you have an FAQ you want to export the entire FAQ right? will that work? Macros: you want a community creating macros. You asked for example about the ability to rate pages here is a link to a Macro for Confluence that lets you do that Structure: you want to be able to say that a page is a child of a different page and be able to browse the data. The wikipedia model of orphaned pages with no sturcture will not work in the Enterprise. (think FAQ you want to have a hierarchy no?) Ability to easily attache picture to be embedded in the body of the page/article. In confluence you need to upload the image and then can embed it it could be a little better (CTR+V) but I guess this is easy enough for 80% of the users. At the end of the day remember that a Wiki will be valuable to you the more flexible it is. It needs to be a ""blank"" canvas and your imagination is then used to ""build"" the application. In Confluence I found 3 different ""best practices"" on how to create a FAQ. That means I can implement MANY things. Some examples (I use my Wiki for) FAQ: any error problem is logged. Used by PS and ENG. reduced internal support time dramatically Track account status: I implemetned sophisticated ""dashboard"" that you can see at a glance which customer is at what state the software version they have who in the company 'owns"" the custoemr etc' Product: all documentation installation instructions the ""what's new"" etc Technical documentation DB structure and what the tables mean HR: contact list Document repository My runner up (15 month ago) was free Deki_Wiki time has passed so I don't know if this would be still my runner up. good luck!  Personally I use MediaWiki for this purpose. I've tried a number of other free and paid wikis (including Confluence) and have always been impressed with MediaWiki's simplicity and ease of use. I have MediaWiki installed on a thumb drive (using XAMPP from PortableApps) which I use mostly as a personal knowledge base/code snippet repository. I can take it with me wherever I go and view/edit it from any computer I'm using.  I think Drupal is a very possible choice. It has a lot of built-in support for book-type information capturing. And there is a rich collection of user generated modules which you can use to enhance the features. I think it has almost all the features you ask for out of the box. Drupal CMS Benefits  I've also been investigating wiki software for use as a KB but it is tricky to find something that is easy to use for non-technical people. There are many wikis that attempt to provide WYSIWYG editing but most of the software I've found generates nasty inefficient html markup from the WYSIWYG editor. One notable exception to this is Confluence which generates wiki syntax from a WYSIWYG editor. This still isn't perfect (show me a WYSIWYG editor that is) but is a pretty good compromise between retaining simple wiki syntax for those who like it and allowing non-technical users to contribute content. The only problem is that Confluence isn't free ($1200 for 25 user license). Edit: I also tried DekiWiki and while the UI is nice it doesn't seem to be quite ready for primetime (suffers terribly from the bad WYSIWYG output disease mentioned above). Also seems like they lack direction as there are so many different ways of accomplishing the same task.",language-agnostic planning,4.4678402752249756E-4,0.03710760657708569,9.400687125689592E-4,0.8630786308136299,0.098426909869193
180,"Function for creating color wheels This is something I've pseudo-solved many times and never quite found a solution that's stuck with me. The problem is to come up with a way to generate N colors that are as distinguishable as possible where N is a parameter. Some related resources: ColorBrewer - Sets of colours designed to be maximally distinguishable for use on maps. Escaping RGBland: Selecting Colors for Statistical Graphics - A technical report describing a set of algorithms for generating good (i.e. maximally distinguishable) colour sets in the hcl colour space. Escaping RGBland is a must to read reference for picking perceptually distinguishable color palettes.  Last I checked JFreeChart has this precise algorithm and as it is open source you can check out what it does. I do know that the colors I get do not seem to be randomly spaced along some circle or sphere but rather chosen more specifically.  It would be best to find colors maximally distant in a ""perceptually uniform"" colorspace e.g. CIELAB (using Euclidean distance between L* a* b* coordinates as your distance metric) and then converting to the colorspace of your choice. Perceptually uniformity comes from tweaking the colorspace to approximate the non-linearities in the human visual system.  To achieve ""most distinguishable"" we need to use a perceptual color space like Lab (or any other perceptually linear color space) and not RGB. Also we can quantize this space to reduce the size of the space. Generate the full 3D space with all possible quantized entries and run the K-means algorithm with k=N. The resulting centers/ ""means"" should be approximately most distinguishabl from each other.  My first thought on this is ""how generate N vectors in a space that maximize distance from each other."" You can see that the RGB (or any other scale you use that forms a basis in color space) are just vectors. Take a look at Random Point Picking. Hope this is a good start for you! Once you have a set of vectors that are maximized a part you can save them in a hash table or something for later and just perform random rotations on them to get all the colors you desire that are maximally apart from each other! Edit: Thinking about this problem more it would be better to map the colors in a linear manor possibly (000) --> (255255255) lexicographically and then distribute them evenly. I really don't know how well this will work but it should since lets say: n = 10 we know we have 16777216 colors (256^3). We can use buckles algorithm 515 to find the lexicographically indexed color.. You'll probably have to edit the algorithm to avoid overflow and probably add some minor speed improvements.  I know this an old post but I found it while looking for a PHP solution to the topic and finally came with a simple solution: function random_color($i = null $n = 10 $sat = .5 $br = .7) { $i = is_null($i) ? mt_rand(0$n) : $i; $rgb = hsv2rgb(array($i*(360/$n) $sat $br)); for ($i=0 ; $i<=2 ; $i++) $rgb[$i] = dechex(ceil($rgb[$i])); return implode('' $rgb); } function hsv2rgb($c) { list($h$s$v)=$c; if ($s==0) return array($v$v$v); else { $h=($h%=360)/60; $i=floor($h); $f=$h-$i; $q[0]=$q[1]=$v*(1-$s); $q[2]=$v*(1-$s*(1-$f)); $q[3]=$q[4]=$v; $q[5]=$v*(1-$s*$f); return(array($q[($i+4)%6]*255$q[($i+2)%6]*255$q[$i%6]*255)); //[1] } } So just call the random_color() function where $i identifies the color $n the number of possible colors $sat the saturation and $br the brightness. You need to add 180 degrees to the hue of your color maintaining saturation and value. Post a new question for this paste the link here and I'll explain further! On `random_color()` `$i` is the ""seed"" to generate the hue should be a number from 0 to `$n` if you input no seed (NULL) the function picks a random one. `$n` is the amount of possible colors for a given saturation and brightness i.e. the number of colors in the palette. We're basically splitting the 360 hue degrees into `$n` and using `$i` as a multiplier. In other words higher `$n` will give you more colors lower `$n` will give you less colors but more different to each other. `$i` will identify the color and will always be the same if you keep using this function. I hope that helps. Can you explain what ""i"" is in this case? The question asked for N numbers. What is the ""i"" paramater? I see! Thanks for the explanation. One more thing...any suggestions for what to do if I have a background color and I want to be as far away from that as possible for all the colors?  Here is some code to allocate RGB colors evenly around a HSL color wheel of specified luminosity. class cColorPicker { public: void Pick( vector<DWORD>&v_picked_cols int count int bright = 50 ); private: DWORD HSL2RGB( int h int s int v ); unsigned char ToRGB1(float rm1 float rm2 float rh); }; /** Evenly allocate RGB colors around HSL color wheel @param[out] v_picked_cols a vector of colors in RGB format @param[in] count number of colors required @param[in] bright 0 is all black 100 is all white defaults to 50 based on Fig 3 of http://epub.wu-wien.ac.at/dyn/virlib/wp/eng/mediate/epub-wu-01_c87.pdf?ID=epub-wu-01_c87 */ void cColorPicker::Pick( vector<DWORD>&v_picked_cols int count int bright ) { v_picked_cols.clear(); for( int k_hue = 0; k_hue < 360; k_hue += 360/count ) v_picked_cols.push_back( HSV2RGB( k_hue 100 bright ) ); } /** Convert HSL to RGB based on http://www.codeguru.com/code/legacy/gdi/colorapp_src.zip */ DWORD cColorPicker::HSL2RGB( int h int s int l ) { DWORD ret = 0; unsigned char rgb; float saturation = s / 100.0f; float luminance = l / 100.f; float hue = (float)h; if (saturation == 0.0) { r = g = b = unsigned char(luminance * 255.0); } else { float rm1 rm2; if (luminance <= 0.5f) rm2 = luminance + luminance * saturation; else rm2 = luminance + saturation - luminance * saturation; rm1 = 2.0f * luminance - rm2; r = ToRGB1(rm1 rm2 hue + 120.0f); g = ToRGB1(rm1 rm2 hue); b = ToRGB1(rm1 rm2 hue - 120.0f); } ret = ((DWORD)(((BYTE)(r)|((WORD)((BYTE)(g))<<8))|(((DWORD)(BYTE)(b))<<16))); return ret; } unsigned char cColorPicker::ToRGB1(float rm1 float rm2 float rh) { if (rh > 360.0f) rh -= 360.0f; else if (rh < 0.0f) rh += 360.0f; if (rh < 60.0f) rm1 = rm1 + (rm2 - rm1) * rh / 60.0f; else if (rh < 180.0f) rm1 = rm2; else if (rh < 240.0f) rm1 = rm1 + (rm2 - rm1) * (240.0f - rh) / 60.0f; return static_cast<unsigned char>(rm1 * 255); } int _tmain(int argc _TCHAR* argv[]) { vector<DWORD> myCols; cColorPicker colpick; colpick.Pick( myCols 20 ); for( int k = 0; k < (int)myCols.size(); k++ ) printf(""%d: %d %d %d\n"" k+1 ( myCols[k] & 0xFF0000 ) >>16 ( myCols[k] & 0xFF00 ) >>8 ( myCols[k] & 0xFF ) ); return 0; } I would really love a Java solution to this!!! what if I want to include that the colors be as distinct from a background color I provide? AFAIK it is straightforward to port code from C++ to Java Calculate the 'distance' between the generated colors and your background color. Do not use the color that is closest to your background. not when I don't understand all the bit shifting stuff among other things :/ I have provided URLs that link to explanations of what the code does.  I've read somewhere the human eye can't distinguish between less than 4 values apart. so This is something to keep in mind. The following algorithm does not compensate for this. I'm not sure this is exactly what you want but this is one way to randomly generate non-repeating color values: (beware inconsistent pseudo-code ahead) //colors entered as 0-255 [R G B] colors = []; //holds final colors to be used rand = new Random(); //assumes n is less than 16777216 randomGen(int n){  while (len(colors) < n){  //generate a random number between 0255 for each color  newRed = rand.next(256);  newGreen = rand.next(256);  newBlue = rand.next(256);  temp = [newRed newGreen newBlue];  //only adds new colors to the array  if temp not in colors {  colors.append(temp);  }  } } One way you could optimize this for better visibility would be to compare the distance between each new color and all the colors in the array: for item in color{  itemSq = (item[0]^2 + item[1]^2 + item[2]^2])^(.5);  tempSq = (temp[0]^2 + temp[1]^2 + temp[2]^2])^(.5);  dist = itemSq - tempSq;  dist = abs(dist); } //NUMBER can be your chosen distance apart. if dist < NUMBER and temp not in colors {  colors.append(temp); } But this approach would significantly slow down your algorithm. Another way would be to scrap the randomness and systematically go through every 4 values and add a color to an array in the above example.  Isn't it also a factor which order you set up the colors? Like if you use Dillie-Os idea you need to mix the colors as much as possible. 0 64 128 256 is from one to the next. but 0 256 64 128 in a wheel would be more ""apart"" Does this make sense?",algorithm language-agnostic colors color-space,0.008679030370556125,0.026403590276134734,6.688976036900098E-4,0.0016635737020640668,0.9625849080475551
1829,"How do I make a menu that does not require the user to press [enter] to make a selection? I've got a menu in Python. That part was easy. I'm using raw_input() to get the selection from the user. The problem is that raw_input (and input) require the user to press Enter after they make a selection. Is there any way to make the program act immediately upon a keystroke? Here's what I've got so far: import sys print """"""Menu 1) Say Foo 2) Say Bar"""""" answer = raw_input(""Make a selection> "") if ""1"" in answer: print ""foo"" elif ""2"" in answer: print ""bar"" It would be great to have something like print menu while lastKey = """": lastKey = check_for_recent_keystrokes() if ""1"" in lastKey: #do stuff... On Linux: set raw mode select and read the keystroke restore normal settings  import sys import select import termios import tty def getkey(): old_settings = termios.tcgetattr(sys.stdin) tty.setraw(sys.stdin.fileno()) select.select([sys.stdin] [] [] 0) answer = sys.stdin.read(1) termios.tcsetattr(sys.stdin termios.TCSADRAIN old_settings) return answer print """"""Menu 1) Say Foo 2) Say Bar"""""" answer=getkey() if ""1"" in answer: print ""foo"" elif ""2"" in answer: print ""bar""  Wow that took forever. Ok here's what I've ended up with #!C:\python25\python.exe import msvcrt print """"""Menu 1) Say Foo 2) Say Bar"""""" while 1:  char = msvcrt.getch()  if char == chr(27): #escape  break  if char == ""1"":  print ""foo""  break  if char == ""2"":  print ""Bar""  break It fails hard using IDLE the python...thing...that comes with python. But once I tried it in DOS (er CMD.exe) as a real program then it ran fine. No one try it in IDLE unless you have Task Manager handy. I've already forgotten how I lived with menus that arn't super-instant responsive.  The reason msvcrt fails in IDLE is because IDLE is not accessing the library that runs msvcrt. Whereas when you run the program natively in cmd.exe it works nicely. For the same reason that your program blows up on Mac and Linux terminals. But I guess if you're going to be using this specifically for windows more power to ya.  On Windows: import msvcrt answer=msvcrt.getch()",python,0.0010755477032426846,0.6621542314317923,0.0022630369091315348,0.33093612156568414,0.0035710623901492413
3544,"What is the best way to deploy a VB.NET application? Generally when I use ClickOnce when I build a VB.NET program but it has a few downsides. I've never really used anything else so I'm not sure what my options are. Downsides to ClickOnce: Consists of multiple files - Seems easier to distribute one file than manageing a bunch of file and the downloader to download those files. You have to build it again for CD installations (for when the end user dosn't have internet) Program does not end up in Program Files - It ends up hidden away in some application catch folder making it much harder to shortcut to. Pros to ClickOnce: It works. Magically. And it's built into VisualStudio 2008 express. Makes it easy to upgrade the application. Does Windows Installer do these things as well? I know it dosen't have any of the ClickOnce cons but It would be nice to know if it also has the ClickOnce pros. Update: I ended up using Wix 2 (Wix 3 was available but at the time I did the project no one had a competent tutorial). It was nice because it supported the three things I (eventually) needed. An optional start-up-with-windows shortcut a start-up-when-the-installer-is-done option and three paragraphs of text that my boss thinks will keep uses from clicking the wrong option. Have you seen WiX yet? http://wix.sourceforge.net/ It builds windows installers using an XML file and has additional libraries to use if you want to fancify your installers and the like. I'll admit the learning curve for me was medium-high in getting things started but afterwards I was able to build a second installer without any hassles. It will handle updates and other items if you so desire and you can apply folder permissions and the like to the installers. It also gives you greater control on where exactly you want to install files and is compatible with all the standardized Windows folder conventions so you can specify ""PROGRAM_DATA"" or something to that effect and the installer knows to put it in C:\Documents and Settings\All Users\Application Data or C:\ProgramData depending on if you're running XP or Vista. The rumor is that Office 2007 and Visual Studio 2008 used WiX to create their installer but I haven't been able to verify that anywhere. I do believe is is developed by some Microsoft folks on the inside. Sounds interesting. Will have to check it out.  Creating an installer project with a dependency on your EXE (which in turn depends on whatever it needs) is a fairly straightforward process - but you'll need at least VS Standard Edition for that. Inside the installer project you can create custom tasks and dialog steps that allow you to do anything you code up. What's missing is the auto-upgrade and version-checking magic you get with ClickOnce. You can still build it in it's just not automatic.  I agree with Joseph my experience with ClickOnce is its great for the vast majority of projects especially in a corporate environment where it makes build publish and deployment easy. Implementing the ""forced upgrade"" to ensure users have the latest version when running is so much easier in ClickOnce and a main reason for my usage of it. Issues with ClickOnce: In a corporate environment it has issues with proxy servers and the workarounds are less than ideal. I've had to deploy a few apps in those cases from UNC paths...but you can't do that all the time. Its ""sandbox"" is great until you want to find the executable or create a desktop shortcut. Have not deployed out of 2008 yet so not sure if those issues still exist.  ClickOnce can be problematic if you have 3rd party components that need to be installed along with your product. You can skirt this to some extent by creating installers for the components however with ClickOnce deployment you have to create the logic to update said component installers. I've in a previous life used Wise For Windows Installer to create installation packages. While creating upgrades with it were not automatic like ClickOnce is they were more precise and less headache filled when it came to other components that needed to be registered/added.  I don't believe there is any easy way to make a Windows Installer project have the ease or upgradability of ClickOnce. I use ClickOnce for all the internal .NET apps I develop (with the exception of Console Apps). I find that in an enterprise environment the ease of deployment outweighs the lack of flexibility.",vb.net visual-studio installer clickonce,0.01771334196120444,0.006960602977837724,0.07388937779710737,0.8822179161677558,0.019218761096094626
249,"Accessing a remote form in php I want to gather info from a user on a local php page (that I control) then use that info to query a form on another site (that I don't control). How do I do that? Here's the code I ended up using using curl: $ch = curl_init(); curl_setopt($ch CURLOPT_URL $formurl); curl_setopt($ch CURLOPT_POST 1); curl_setopt($ch CURLOPT_POSTFIELDS ""user_id=$id&password=$pw""); curl_setopt($ch CURLOPT_SSL_VERIFYPEER false); // required to work with https:// sites. curl_setopt($ch CURLOPT_RETURNTRANSFER true); // curl_exec() returns page instead of printing it. $page = curl_exec($ch); if ($page == false) {  $curlerror = curl_error($ch);  print ""<br>Errors: $curlerror<br>\n""; } else {  print ""<br>curl call succeeded<br>\n"";  print $page; } curl_close($ch);  There is a curl extension for PHP in Windows as well.  What version of PHP are you using? I will assume 5.x. PHP includes easy to use classes for HTTP operations. The documentation can be found here: http://us3.php.net/manual/en/ref.http.php There are several ways to approach this: one is capturing the input on your local form and POST-ing the data to the remote form using http_post_fields(). http://us3.php.net/manual/en/function.http-post-fields.php  Keep in mind when posting data from one server to another that foreign server may ignore your request. So be sure to test against that and if you are unsuccessful contact the person who has access to configure the server to accept requests from your local server.  Have your local form post the data to your local processing script. Then use something like curl to programmatically post the data to the remote server and receieve a response. You will then have to parse the reponse in some way to retrieve meaningful information.  If you are using linux I suggest curl but here is a generic class i use http://willwharton.com/http.phps that works on linux and windows as it can use both curl and fsockopen() $http = new Http(); $http->setMethod('POST'); $http->addParam('aaaa'  'bbb'); $http->addParam('ccccc'  'ddddd'); $http->execute('http://www.namecheap.com/myaccount'); $raw = $http->result; This is exactly what I needed 4 hours ago before I started on the ugly javascript forms and iframe solution i just completed :p",php forms,9.914134612279224E-4,0.5451703800185311,0.36713394189772014,0.03778155574797952,0.0489227088745413
4850,C# and Arrow Keys I am new to C# and am doing some work in an existing application. I have a DirectX viewport that has components in it that I want to be able to position using arrow keys. Currently I am overriding ProcessCmdKey and catching arrow input and send an OnKeyPress event. This works but I want to be able to use modifiers(ALT+CTRL+SHIFT). As soon as I am holding a modifier and press an arrow no events are triggered that I am listening to. Does anyone have any ideas or suggestions on where I should go with this? Within your overridden ProcessCmdKey how are you determining which key has been pressed? The value of keyData (the second parameter) will change dependant on the key pressed and any modifier keys so for example pressing the left arrow will return code 37 shift-left will return 65573 ctrl-left 131109 and alt-left 262181. You can extract the modifiers and the key pressed by ANDing with appropriate enum values: protected override bool ProcessCmdKey(ref Message msg Keys keyData) { bool shiftPressed = (keyData & Keys.Shift) != 0; Keys unmodifiedKey = (keyData & Keys.KeyCode); // rest of code goes here }  I upvoted Tokabi's answer but for comparing keys there is some additional advice on StackOverflow.com here. Here are some functions which I used to help simplify everything.  public Keys UnmodifiedKey(Keys key) { return key & Keys.KeyCode; } public bool KeyPressed(Keys key Keys test) { return UnmodifiedKey(key) == test; } public bool ModifierKeyPressed(Keys key Keys test) { return (key & test) == test; } public bool ControlPressed(Keys key) { return ModifierKeyPressed(key Keys.Control); } public bool AltPressed(Keys key) { return ModifierKeyPressed(key Keys.Alt); } public bool ShiftPressed(Keys key) { return ModifierKeyPressed(key Keys.Shift); } protected override bool ProcessCmdKey(ref Message msg Keys keyData) { if (KeyPressed(keyData Keys.Left) && AltPressed(keyData)) { int n = code.Text.IndexOfPrev('<' code.SelectionStart); if (n < 0) return false; if (ShiftPressed(keyData)) { code.ExpandSelectionLeftTo(n); } else { code.SelectionStart = n; code.SelectionLength = 0; } return true; } else if (KeyPressed(keyData Keys.Right) && AltPressed(keyData)) { if (ShiftPressed(keyData)) { int n = code.Text.IndexOf('>' code.SelectionEnd() + 1); if (n < 0) return false; code.ExpandSelectionRightTo(n + 1); } else { int n = code.Text.IndexOf('<' code.SelectionStart + 1); if (n < 0) return false; code.SelectionStart = n; code.SelectionLength = 0; } return true; } return base.ProcessCmdKey(ref msg keyData); } Plus one million to you for writing readable code!,c# user-interface directx,9.031070278866064E-4,0.9835341916598929,0.0019002081737906156,0.004725889775844126,0.008936603362585857
1537,"What is software engineering? The term ""software engineering"" is often used without fully being considered. Since the field is relatively young compared to other mature professional disciplines the definition is arguably still be worked out and at the very least it is often understood differently by different populations despite being ""defined"" by IEEE or the like. So how's software engineering being defined by SO users? What is software engineering? According to wikipedia: http://en.wikipedia.org/wiki/Software_Engineering My answer: ""A combination of tools methods and techniques used in the optimal design implementation and maintenance of software systems.""  In its simplest form Software Engineering is simply the application of Computer Science. However it usually goes beyond this. When I took 'Software Engineering' in College it was much more like a mix of Programming and Management. For example Extreme Programming is a ""software engineering methodology"" (wikipedia) Ryan was correct as well Software Engineering also has to do with the consequences and implications of computer science. Both the ACM and IEEE of standards of Ethics that should be followed. (If this is of interest to you I would suggest the book A Gift of Fire) In summary a Software Engineer is someone who can take the theory of Computer Science and apply practically while having the foresight to realize the implications of his/her work  As a software engineering student I believe I can answer this question. Software engineering is the application of engineering principles to software systems. This involves designing constructing and maintaining a low-cost high-reliability system that meets the customer needs and then delivering all versions on time and budget. Software engineering is composed of a number of domains including requirements design construction testing maintenance configuration management quality engineering management tools and methods and processes. These domains are derived from mathematics and statistics computer science cognitive sciences telecommunications and networking project management quality engineering and various other engineering disciplines. If you want to know I would read the Software Engineering Body of Knowledge. I would like to say that yes software engineering is an engineering discipline. +1 for good answer. One of my software engineering lecturers has written a nice essay on the topic of differences between SE and CS... http://www.cs.auckland.ac.nz/~ewan/essays/se_vs_cs/  As a computer engineering student I take issue with people throwing the term ""engineering"" around so loosely. An engineer doesn't simply apply science in a practical way; he also carries a level of responsibility for his actions to ensure the well-being of society the environment etc. Very few ""software engineers"" have such responsibility. If Windows Vista crashes on 25% of the computer it's installed on you won't see an engineer standing in front of a discipline committee. I honestly cannot believe this answer has gotten so many upvotes. I'd like to add the comment the industry uses ""Software Engineer"" loosely for programmers and developers so your perception is correct but the term is very close to the same responsibilities of an engineer with a huge focus on the human aspect. I assume Ryan does not understand what Software Engineering as a science truly entails nor does much of the software industry. This is a big problem right now IMHO. @ThomasOwens I completely agree this is where it is obvious but it transcends all Software. Hard to use web sites that crash often cause people no ends of frustration. Is that better or worse then driving to the store and working with people who care? A website that does not care in its design is just like a person who does not care in their affect but to a much larger audience. Neither will an electronics engineer stand before one if a cell phone crashes and needs to be restarted. Neither will a mechanical engineer if the grandfather clock mechanism breaks. Whats your point? I think more than ""very few"" software engineers have a responsibility to society. How about software engineers working on software systems in medical devices aircraft military systems things like the FAA radar system...lives are on the line in all of those.  Software Engineer is just a title used by recruiters to lure unsuspecting victims into their lair. Don't get hung up on it the fashion for Software Engineer has reached old age and will be pensioned off just a soon as one of the myriad of phrases used to describe what we do makes a quantum leap. What's the betting that it's something even less useful to describe what you do to industry outsiders?  I would say that software engineering is the intelligent application of the knowledge base of software science (what computer science is generally focused on). Unfortunately I think it is often mistaken that this is an immature new field of engineering. There is a huge body of knowledge and best practices - it is simply a complex field with more specialists than many other engineering fields. Where I live engineering is a regulated profession - though they way I understand it engineering and professional engineering are quite different. Engineering is using your brain to solve problems to move a project forward. Professional engineering is an acceptance of responsibility and a recognition of skill when using your brain to solve problems to move a project forward. Of course stackoverflow is a great resource for all of us - engineers scientists hackers testers coders builders managers all!  In Germany an engineer (Ingenieur) no matter of what profession has certain professional “privileges”. For example engineers are permitted to install high voltage equipment something ordinary people aren't allowed to do unless supervised by a certified professional. It is simply assumed that engineers knows what they're doing and can take responsibility for their actions. For that reason attaining a degree in software engineering is very uncommon in Germany. The “throwing … around” of the term can't happen here. Any software engineer really is a fully-fledged engineer. Alright got it! ;) So you are saying that there is no categories and only one title for the craft. @Secko No. There *are* categories (e.g. software engineer electrical engineer etc.) but they all have the degree of “diploma in engineering”. Conversely studying computer science usually gives you a *different* degree (either “Diplominformatik” i.e. diploma in computer science or a master degree). To attain the degree of engineer you need to study for this specifically. But I don’t know how much these two differ merely that there are potentially different requirements for the final examinations.  Perhaps 'Software Engineering' is not the best term since it is not a engineering discipline in the purest sense but it is an attempt to provide formal methods to the process of creating software in the interest of better success and quality. The need for such a discipline was identified in the late 1960s when it became obvious that software projects were taking too much time and money and failing at an unacceptable rate. The field flourished in the 1970s but unfortunately an unacceptable number of projects still fail to to be completed on time or at all. Please see my blog entry 'What Ever Happened to Software Engineering?'  To me software engineering is an engineering discipline and I say from a background of more mechanically minded engineering. The context is different the particular activities will change but many of the processes for building any type of system are common. I don't have massive experience but to me software engineering covers the all aspects of the whole software development life-cycle. You've got to think a little bit about everything to give yourself the best possible chance at producing the highest quality project within budget and on time. Don't think about how to handle QA and it'll probably cost you in some way. Fail to think about and optimise your methodology and it'll cost you. Fail to get the requirements right and it'll really cost you. You might not fail outright but a project can always be completed better. Most people won't do all of these things; some will specialise as analysts developers test engineers or something else. But to be a software engineer you've got to be willing to stick your head out of your aspect of the project and at least understand the importance of all the other parts and roles needed to engineer quality software.  I think it is the use of engineering principles of combining optimzing and managing tools methods personnel and other resources to build an effectiveefficient and applicable software.",software-engineering,0.06386928693909748,0.02906528719777124,6.356789104640163E-4,0.891521330078753,0.014908416873914289
2348,"What is the best way to iterate through an array in Classic Asp VBScript? Is: For i = LBound(arr) To UBound(arr) The best way? What is the point in asking for LBound? Surely that is always 0 isn't it? Why not use For Each? That way you don't need to care what the LBound and UBound are. Dim x y z x = Array(1 2 3) For Each y In x z = DoSomethingWith(y) Next You cannot use `For Each` to directly edit array cells since `y` in your example is a copy of array cell not it's reference. For array editing `For i = LBound(arr) To UBound(arr)` will allow one to refer to cell directly using `arr(i)` and edit cell content (like `arr(i) = Trim(arr(i))`).  I've always used For Each...  Option Base doesn't seem to work in ASP Yes it only does in VB6 like I said. My point is that this usage comes from VB6 programmers who are also ASP (VBScript) programmers.  LBound may not always be 0. Whilst it is not possible to create an array that has anything other than a 0 Lower bound VBScript it is still possible to retrieve an array of variants from a COM component which may have specified a different LBound. That said I've never come across one that has done anything like that.  It is actually a matter of taste. For i ... UBound is a tad faster but ForEach is cleaner.  There is a good reason NOT to use ""For i = LBound(arr) To UBound(arr)"" ""dim arr(10)"" allocates eleven members of the array 0 through 10 (assuming the VB6 default Option Base). Many VB6 programmers assume the array is one-based and never use the allocated arr(0). You can remove a potential source of bugs by using ""For i = 1 To UBound(arr)"" or ""For i = 0 To UBound(arr)"" because then it is clear whether arr(0) is being used. ""For each"" makes a copy of each array element rather than a pointer. This has two problems. First when you try to assign a value to an array element it doesn't happen. This code assigns a value of 47 to the variable i but does not affect the elements of arr. for each i in arr i = 47 next i Second you don't know the index of an array element in a ""for each"" and you are not guaranteed the sequence of elements (although it seems to be in order.)  Probably it comes from VB6. Because with Option Base statement in VB6 you can alter the lower bound of arrays like this: Option Base 1 Also in VB6 you can alter the lower bound of a specific array like this: Dim myArray(4 To 42) As String",arrays asp-classic vbscript,0.04278981877735536,0.8939166307029539,0.0026577378537434525,0.02322056784260715,0.037415244823340035
972,"Adding a Method to an Existing Object I've read that it is possible to add a method to an existing object (e.g. not in the class definition) in python I think this is called Monkey Patching (or in some cases Duck Punching). I understand that it's not always a good decision to do so. But how might one do this? And if you don't know python can your language of choice do this? If so how? UPDATE 8/04/2008 00:21:01 EST: That looks like a good answer John Downey I tried it but it appears that it ends up being not a true method. Your example defines the new patch function with an argument of self but if you write actual code that way the now patched class method asks for an argument named self (it doesn't automagically recognize it as the identity class which is what would happen if defined within the class definition) meaning you have to call class.patch(class) instead of just class.patch() if you want the same functionality as a true method. It looks like python isn't really treating it as a method but more just as a variable which happens to be a function (and as such is callable). Is there any way to attach an actual method to a class? Oh and Ryan that isn't exactly what I was looking for (it isn't builtin functionality) but it is quite cool nonetheless. Why wouldn't this work with c extensions? See [Q9883906](http://stackoverflow.com/questions/9883906/monkey-patching-c-extension-in-python) I also don't know Python but this struck me as something that should be rather easy to find in Google. [I was right.](http://pypi.python.org/pypi/monkey) (Note I don't actually know if this is what you want but it sure sounds like it.) Module new is deprecated since python 2.6 and removed in 3.0 use types see http://docs.python.org/library/new.html In the example below I've deliberately removed return value from patch_me() function. I think that giving return value may make one believe that patch returns a new object which is not true - it modifies the incoming one. Probably this can facilitate a more disciplined use of monkeypatching. import types class A(object):#but seems to work for old style objects too pass def patch_me(target): def method(targetx): print ""x=""x print ""called from"" target target.method = types.MethodType(methodtarget) #add more if needed a = A() print a #out: <__main__.A object at 0x2b73ac88bfd0> patch_me(a) #patch instance a.method(5) #out: x= 5 #out: called from <__main__.A object at 0x2b73ac88bfd0> patch_me(A) A.method(6) #can patch class too #out: x= 6 #out: called from <class '__main__.A'>  If it can be of any help I recently released a Python library named Gorilla to make the process of monkey patching more convenient. Using a function needle() to patch a module named guineapig goes as follows: import gorilla import guineapig @gorilla.patch(guineapig) def needle(): print(""awesome"") But it also takes care of more interesting use cases as shown in the FAQ from the documentation. The code is available on GitHub.  You can use lambda to bind a method to an instance: def run(self): print self._instanceString class A(object): def __init__(self): self._instanceString = ""This is instance string"" a = A() a.run = lambda: run(a) a.run() This is instance string Process finished with exit code 0  Consolidating Jason Pratt's and the community wiki answers with a look at the results of different methods of binding: Especially note how adding the binding function as a class method works but the referencing scope is incorrect. #!/usr/bin/python -u import types import inspect ## dynamically adding methods to a unique instance of a class # get a list of a class's method type attributes def listattr(c): for m in [(n v) for n v in inspect.getmembers(c inspect.ismethod) if isinstance(vtypes.MethodType)]: print m[0] m[1] # externally bind a function as a method of an instance of a class def ADDMETHOD(c method name): c.__dict__[name] = types.MethodType(method c) class C(): r = 10 # class attribute variable to test bound scope def __init__(self): pass #internally bind a function as a method of self's class -- note that this one has issues! def addmethod(self method name): self.__dict__[name] = types.MethodType( method self.__class__ ) # predfined function to compare with def f0(self x): print 'f0\tx = %d\tr = %d' % ( x self.r) a = C() # created before modified instnace b = C() # modified instnace def f1(self x): # bind internally print 'f1\tx = %d\tr = %d' % ( x self.r ) def f2( self x): # add to class instance's .__dict__ as method type print 'f2\tx = %d\tr = %d' % ( x self.r ) def f3( self x): # assign to class as method type print 'f3\tx = %d\tr = %d' % ( x self.r ) def f4( self x): # add to class instance's .__dict__ using a general function print 'f4\tx = %d\tr = %d' % ( x self.r ) b.addmethod(f1 'f1') b.__dict__['f2'] = types.MethodType( f2 b) b.f3 = types.MethodType( f3 b) ADDMETHOD(b f4 'f4') b.f0(0) # OUT: f0 x = 0 r = 10 b.f1(1) # OUT: f1 x = 1 r = 10 b.f2(2) # OUT: f2 x = 2 r = 10 b.f3(3) # OUT: f3 x = 3 r = 10 b.f4(4) # OUT: f4 x = 4 r = 10 k = 2 print 'changing b.r from {0} to {1}'.format(b.r k) b.r = k print 'new b.r = {0}'.format(b.r) b.f0(0) # OUT: f0 x = 0 r = 2 b.f1(1) # OUT: f1 x = 1 r = 10 !!!!!!!!! b.f2(2) # OUT: f2 x = 2 r = 2 b.f3(3) # OUT: f3 x = 3 r = 2 b.f4(4) # OUT: f4 x = 4 r = 2 c = C() # created after modifying instance # let's have a look at each instance's method type attributes print '\nattributes of a:' listattr(a) # OUT: # attributes of a: # __init__ <bound method C.__init__ of <__main__.C instance at 0x000000000230FD88>> # addmethod <bound method C.addmethod of <__main__.C instance at 0x000000000230FD88>> # f0 <bound method C.f0 of <__main__.C instance at 0x000000000230FD88>> print '\nattributes of b:' listattr(b) # OUT: # attributes of b: # __init__ <bound method C.__init__ of <__main__.C instance at 0x000000000230FE08>> # addmethod <bound method C.addmethod of <__main__.C instance at 0x000000000230FE08>> # f0 <bound method C.f0 of <__main__.C instance at 0x000000000230FE08>> # f1 <bound method ?.f1 of <class __main__.C at 0x000000000237AB28>> # f2 <bound method ?.f2 of <__main__.C instance at 0x000000000230FE08>> # f3 <bound method ?.f3 of <__main__.C instance at 0x000000000230FE08>> # f4 <bound method ?.f4 of <__main__.C instance at 0x000000000230FE08>> print '\nattributes of c:' listattr(c) # OUT: # attributes of c: # __init__ <bound method C.__init__ of <__main__.C instance at 0x0000000002313108>> # addmethod <bound method C.addmethod of <__main__.C instance at 0x0000000002313108>> # f0 <bound method C.f0 of <__main__.C instance at 0x0000000002313108>> Personally I prefer the external ADDMETHOD function route as it allows me to dynamically assign new method names within an iterator as well. def y(self x): pass d = C() for i in range(15): ADDMETHOD(d y 'f%d' % i) print '\nattributes of d:' listattr(d) # OUT: # attributes of d: # __init__ <bound method C.__init__ of <__main__.C instance at 0x0000000002303508>> # addmethod <bound method C.addmethod of <__main__.C instance at 0x0000000002303508>> # f0 <bound method C.f0 of <__main__.C instance at 0x0000000002303508>> # f1 <bound method ?.y of <__main__.C instance at 0x0000000002303508>> # f2 <bound method ?.y of <__main__.C instance at 0x0000000002303508>> # f3 <bound method ?.y of <__main__.C instance at 0x0000000002303508>> # f4 <bound method ?.y of <__main__.C instance at 0x0000000002303508>>  In Python monkey patching generally works by overwriting a class or functions signature with your own. Below is an example from the Zope Wiki: from SomeOtherProduct.SomeModule import SomeClass def speak(self):  return ""ook ook eee eee eee!"" SomeClass.speak = speak That code will overwrite/create a method called speak on the class. In Jeff Atwood's recent post on monkey patching. He shows an example in C# 3.0 which is the current language I use for work. @Troy that is false. You only need `types.MethodType` if you're attaching the method to an instance. If you're attaching the method to a class then it automatically propagates to all instances. But it influences *all* instances of the class not just one. That modifies the class but it doesn't modify existing instances. For that you need import types; myinstance.newmethodname = types.MethodType(mymethodname myinstance).  What you're looking for is setattr I believe. Use this to set an attribute on an object. >>> def printme(s): print repr(s) >>> class A: pass >>> setattr(A'printme'printme) >>> a = A() >>> a.printme() # s becomes the implicit 'self' variable < __ main __ . A instance at 0xABCDEFG> This is patching the class `A` not the instance `a`. Is there a reason to use `setattr(A'printme'printme)` instead of simply `A.printme = printme`?  Since this question asked for non-Python versions here's JavaScript: a.methodname = function () { console.log(""Yay a new method!"") }  You guys should really look at forbidden fruit it's a python library that provides support to monkey patching ANY python class even strings.  I think that the above answers missed the key point. Let's have a class with a method: class A(object): def m(self): pass Now let's play with it in ipython: In [2]: A.m Out[2]: <unbound method A.m> Ok so m() somehow becomes an unbound method of A. But is it really like that? In [5]: A.__dict__['m'] Out[5]: <function m at 0xa66b8b4> It turns out that m() is just a function reference to which is added to A class dictionary - there's no magic. Then why A.m gives us an unbound method? It's because the dot is not translated to a simple dictionary lookup. It's de facto a call of A.__class__.__getattribute__(A 'm'): In [11]: class MetaA(type): ....: def __getattribute__(self attr_name): ....: print str(self) '-' attr_name In [12]: class A(object): ....: __metaclass__ = MetaA In [23]: A.m <class '__main__.A'> - m <class '__main__.A'> - m Now I'm not sure out of the top of my head why the last line is printed twice but still it's clear what's going on there. Now what the default __getattribute__ does is that it checks if the attribute is a so-called descriptor or not i.e. if it implements a special __get__ method. If it implements that method then what is returned is the result of calling that __get__ method. Going back to the first version of out A class this is what we have: In [28]: A.__dict__['m'].__get__(None A) Out[28]: <unbound method A.m> And because Python functions implement the descriptor protocol if they are called on behalf an object they bound themselves to that object in their __get__ method. Ok so how to add a method to an existing object? Assuming you don't mind patching class it's as simple as: B.m = m Then B.m ""becomes"" an unbound method thanks to the descriptor magic. And if you want to add a method just to a single object then you have to emulate the machinery yourself by using types.MethodType: b.m = types.MethodType(m b) By the way: In [2]: A.m Out[2]: <unbound method A.m> In [59]: type(A.m) Out[59]: <type 'instancemethod'> In [60]: type(b.m) Out[60]: <type 'instancemethod'> In [61]: types.MethodType Out[61]: <type 'instancemethod'>  There are at least two ways for attach a method to an instance without types.MethodType: >>> class A: ... def m(self): ... print 'im m invoked with: ' self >>> a = A() >>> a.m() im m invoked with: <__main__.A instance at 0x973ec6c> >>> a.m <bound method A.m of <__main__.A instance at 0x973ec6c>> >>> >>> def foo(firstargument): ... print 'im foo invoked with: ' firstargument >>> foo <function foo at 0x978548c> 1: >>> a.foo = foo.__get__(a A) # or foo.__get__(a type(a)) >>> a.foo() im foo invoked with: <__main__.A instance at 0x973ec6c> >>> a.foo <bound method A.foo of <__main__.A instance at 0x973ec6c>> 2: >>> instancemethod = type(A.m) >>> instancemethod <type 'instancemethod'> >>> a.foo2 = instancemethod(foo a type(a)) >>> a.foo2() im foo invoked with: <__main__.A instance at 0x973ec6c> >>> a.foo2 <bound method instance.foo of <__main__.A instance at 0x973ec6c>> Useful links: Data model - invoking descriptors Descriptor HowTo Guide - invoking descriptors  In Python there is a difference between functions and bound methods. >>> def foo(): ... print ""foo"" ... >>> class A: ... def bar( self ): ... print ""bar"" ... >>> a = A() >>> foo <function foo at 0x00A98D70> >>> a.bar <bound method A.bar of <__main__.A instance at 0x00A9BC88>> >>> Bound methods have been ""bound"" (how descriptive) to an instance and that instance will be passed as the first argument whenever the method is called. Callables that are attributes of a class (as opposed to an instance) are still unbound though so you can modify the class definition whenever you want: >>> def fooFighters( self ): ... print ""fooFighters"" ... >>> A.fooFighters = fooFighters >>> a2 = A() >>> a2.fooFighters <bound method A.fooFighters of <__main__.A instance at 0x00A9BEB8>> >>> a2.fooFighters() fooFighters Previously defined instances are updated as well (as long as they haven't overridden the attribute themselves): >>> a.fooFighters() fooFighters The problem comes when you want to attach a method to a single instance: >>> def barFighters( self ): ... print ""barFighters"" ... >>> a.barFighters = barFighters >>> a.barFighters() Traceback (most recent call last): File ""<stdin>"" line 1 in <module> TypeError: barFighters() takes exactly 1 argument (0 given) The function is not automatically bound when it's attached directly to an instance: >>> a.barFighters <function barFighters at 0x00A98EF0> To bind it we can use the MethodType function in the types module: >>> import types >>> a.barFighters = types.MethodType( barFighters a ) >>> a.barFighters <bound method ?.barFighters of <__main__.A instance at 0x00A9BC88>> >>> a.barFighters() barFighters This time other instances of the class have not been affected: >>> a2.barFighters() Traceback (most recent call last): File ""<stdin>"" line 1 in <module> AttributeError: A instance has no attribute 'barFighters' More information can be found by reading about descriptors and metaclass programming. This helped explain why my code was doing what it was doing. I did not know there was a difference between assigning a method to an attribute versus assigning to an attribute as part of the construction of the class. So I ended up moving those assignments into the init because I don't want the methods to be bound to the class. Thanks There is something here that I think is incorrect. The answer says that when you add an attribute to a class instances of the class are updated. This is not true. The reason that `a.fooFighters()` works in the example is that the method resolution order in python is to look in an object's class _before_ looking in the object itself. awesome I finally 'get' the name of the Foo Fighters!  What Jason Pratt posted is correct. >>> class Test(object): ... def a(self): ... pass ... >>> def b(self): ... pass ... >>> Test.b = b >>> type(b) <type 'function'> >>> type(Test.a) <type 'instancemethod'> >>> type(Test.b) <type 'instancemethod'> As you can see Python doesn't consider b() any different than a(). In Python all methods are just variables that happen to be functions. You are patching the class `Test` not an instance of it.  I don't know Python syntax but I know Ruby can do it and it is rather trivial. Let's say you want to add a method to Array that prints the length to standard out: class Array  def print_length  puts length  end end If you don't want to modify the whole class you can just add the method to a single instance of the array and no other arrays will have the method: array = [1 2 3] def array.print_length  puts length end Just be aware of the issues involved in using this feature. Jeff Atwood actually wrote about it not too long ago.",python oop monkeypatching,0.004653183198536442,0.9737632151960472,4.7239666438063225E-4,0.020365765061290543,7.454398797450537E-4
308,"Is there a version control system for database structure changes? I often run into the following problem. I work on some changes to a project that require new tables or columns in the database. I make the database modifications and continue my work. Usually I remember to write down the changes so that they can be replicated on the live system. However I don't always remember what I've changed and I don't always remember to write it down. So I make a push to the live system and get a big obvious error that there is no NewColumnX ugh. Regardless of the fact that this may not be the best practice for this situation is there a version control system for databases? I don't care about the specific database technology. I just want to know if one exists. If it happens to work with MS SQL Server then great. [Here is another discussion on DB versioning.](http://stackoverflow.com/questions/173/how-do-i-version-my-ms-sql-database-in-svn) MyBatis (formerly iBatis) has a schema migration tool for use on the command line. It is written in java though can be used with any project. To achieve a good database change management practice we need to identify a few key goals. Thus the MyBatis Schema Migration System (or MyBatis Migrations for short) seeks to: Work with any database new or existing Leverage the source control system (e.g. Subversion) Enable concurrent developers or teams to work independently Allow conflicts very visible and easily manageable Allow for forward and backward migration (evolve devolve respectively) Make the current status of the database easily accessible and comprehensible Enable migrations despite access privileges or bureaucracy Work with any methodology Encourages good consistent practices  We've used MS Team System Database Edition with pretty good success. It integrates with TFS version control and Visual Studio more-or-less seamlessly and allows us to manages stored procs views etc. easily. Conflict resolution can be a pain but version history is complete once it's done. Thereafter migrations to QA and production are extremely simple. It's fair to say that it's a version 1.0 product though and is not without a few issues.  Take a look at dbdeploy.  I wonder that no one mentioned the open source tool liquibase which is Java based and should work for nearly every database which supports jdbc. Compared to rails it uses xml instead ruby to perform the schema changes. Although I dislike xml for domain specific languages the very cool advantage of xml is that liquibase knows how to roll back certain operations like <createTable tableName=""USER""> <column name=""firstname"" type=""varchar(255)""/> </createTable> So you don't need to handle this of your own Pure sql statements or data imports are also supported. we use liquibase but we use 3 different approach for the different information: 1. structure (table views ...): historical changelog 2. codes (procedures pl/sql functions): changelog with only one changeset marked with runalways=true runonchange = true 3. code tables other meta ""constants"" stored in tables: the same approach as for codes only one changeset delete from insert all info  Try SQL Examiner http://www.sqlaccessories.com/SQL_Examiner/Database_Version_Control.aspx  In the absence of a VCS for table changes I've been logging them in a wiki. At least then I can see when and why it was changed. It's far from perfect as not everyone is doing it and we have multiple product versions in use but better than nothing.  Redgate has a product called SQL Source Control. It integrates with TFS SVN SourceGear Vault Vault Pro Mercurial Perforce and Git.  For Oracle I use Toad which can dump a schema to a number of discrete files (e.g. one file per table). I have some scripts that manage this collection in Perforce but I think it should be easily doable in just about any revision control system.  Two book recommendations: ""Refactoring Databases"" by Ambler and Sadalage and ""Agile Database Techniques"" by Ambler. Someone mentioned Rails Migrations. I think they work great even outside of Rails applications. I used them on an ASP application with SQL Server which we were in the process of moving to Rails. You check the migration scripts themselves into the VCS. Here's a post by Pragmatic Dave Thomas on the subject.  I've done this off and on for years -- managing (or trying to manage) schema versions. The best approaches depend on the tools you have. If you can get the Quest Software tool ""Schema Manager"" you'll be in good shape. Oracle has its own inferior tool that is also called ""Schema Manager"" (confusing much?) that I don't recommend. Without an automated tool (see other comments here about Data Dude) then you'll be using scripts and DDL files directly. Pick an approach document it and follow it rigorously. I like having the ability to re-create the database at any given moment so I prefer to have a full DDL export of the entire database (if I'm the DBA) or of the developer schema (if I'm in product-development mode).  Have your initial create table statements in version controller then add alter table statements but never edit files just more alter files ideally named sequentially or even as a ""change set"" so you can find all the changes for a particular deployment. The hardiest part that I can see is tracking dependencies eg for a particular deployment table B might need to be updated before table A.  Take a look at the oracle package DBMS_METADATA. In particular the following methods are particularly useful: DBMS_METADATA.GET_DDL DBMS_METADATA.SET_TRANSFORM_PARAM DBMS_METADATA.GET_GRANTED_DDL. Once you are familiar with how they work (pretty self explanitory) you can write a simple script to dump the results of those methods into text files that can be put under source control. Good luck! Not sure if there is something this simple for MSSQL.  There's a PHP5 ""database migration framework"" called Ruckusing. I haven't used it but the examples show the idea if you use the language to create the database as and when needed you only have to track source files.  In Ruby on Rails there's a concept of a migration -- a quick script to change the database. You generate a migration file which has rules to increase the db version (such as adding a column) and rules to downgrade the version (such as removing a column). Each migration is numbered and a table keeps track of your current db version. To _migrate up_ you run a command called ""db:migrate"" which looks at your version and applies the needed scripts. You can migrate down in a similar way. The migration scripts themselves are kept in a version control system -- whenever you change the database you check in a new script and any developer can apply it to bring their local db to the latest version. This is the choice for Ruby projects. The nearest equivalent to this design in java is mybatis schema migrations. For .NET the equivalent is http://code.google.com/p/migratordotnet. They're all excellent tools for this job IMO.  I'm a bit old-school in that I use source files for creating the database. There are actually 2 files - project-database.sql and project-updates.sql - the first for the schema and persistant data and the second for modifications. Of course both are under source control. When the database changes I first update the main schema in project-database.sql then copy the relevant info to the project-updates.sql for instance ALTER TABLE statements. I can then apply the updates to the development database test iterate until done well. Then check in files test again and apply to production. Also I usually have a table in the db - Config - such as: SQL CREATE TABLE Config ( cfg_tag VARCHAR(50) cfg_value VARCHAR(100) ); INSERT INTO Config(cfg_tag cfg_value) VALUES ( 'db_version' '$Revision: $') ( 'db_revision' '$Revision: $'); Then I add the following to the update section: UPDATE Config SET cfg_value='$Revision: $' WHERE cfg_tag='db_revision'; The db_version only gets changed when the database is recreated and the db_revision gives me an indication how far the db is off the baseline. I could keep the updates in their own separate files but I chose to mash them all together and use cut&paste to extract relevant sections. A bit more housekeeping is in order i.e. remove ':' from $Revision 1.1 $ to freeze them.  I'd recommend one of two approaches. First invest in PowerDesigner from Sybase. Enterprise Edition. It allows you to design Physical datamodels and a whole lot more. But it comes with a repository that allows you to check in your models. Each new check in can be a new version it can compare any version to any other version and even to what is in your database at that time. It will then present a list of every difference and ask which should be migrated… and then it builds the script to do it. It’s not cheap but it’s a bargain at twice the price and it’s ROI is about 6 months. The other idea is to turn on DDL auditing (works in Oracle). This will create a table with every change you make. If you query the changes from the timestamp you last moved your database changes to prod to right now you’ll have an ordered list of everything you’ve done. A few where clauses to eliminate zero-sum changes like create table foo; followed by drop table foo; and you can EASILY build a mod script. Why keep the changes in a wiki that’s double the work. Let the database track them for you.  you might be interesting in readind this article about mysql database versioning  ER Studio allows you to reverse your database schema into the tool and you can then compare it to live databases. Example: Reverse your development schema into ER Studio -- compare it to production and it will list all of the differences. It can script the changes or just push them through automatically. Once you have a schema in ER Studio you can either save the creation script or save it as a proprietary binary and save it in version control. If you ever want to go back to a past version of the scheme just check it out and push it to your db platform.  Schema Compare for Oracle is a tool specifically designed to migrate changes from our Oracle database to another. Please visit the URL below for the download link where you will be able to use the software for a fully functional trial. http://www.red-gate.com/Products/schema_compare_for_oracle/index.htm  PLSQL Developer a tool from All Arround Automations has a plugin for repositories that works OK ( but not great) with Visual Source Safe. From the web: The Version Control Plug-In provides a tight integration between the PL/SQL Developer IDE >>and any Version Control System that supports the Microsoft SCC Interface Specification. >>This includes most popular Version Control Systems such as Microsoft Visual SourceSafe >>Merant PVCS and MKS Source Integrity. http://www.allroundautomations.com/plsvcs.html  Most database engines should support dumping your database into a file. I know MySQL does anyway. This will just be a text file so you could submit that to Subversion or whatever you use. It'd be easy to run a diff on the files too. Yeah but diffing SQL files won't give you the neccessary scripts to upgrade your dev/prod db from one revision to another  I write my db release scripts in parallel with coding and keep the release scripts in a project specific section in SS. If I make a change to the code that requires a db change then I update the release script at the same time. Prior to release I run the release script on a clean dev db (copied structure wise from production) and do my final testing on it.  I highly recommend SQL delta. I just use it to generate the diff scripts when i'm done coding my feature and check those scripts into my source control tool (Mercurial :)) They have both an SQL server & Oracle version.  If you're using SQL Server it would be hard to beat Data Dude (aka the Database Edition of Visual Studio). Once you get the hang of it doing a schema compare between your source controlled version of the database and the version in production is a breeze. And with a click you can generate your diff DDL. There's an instructional video on MSDN that's very helpful. I know about DBMS_METADATA and Toad but if someone could come up with a Data Dude for Oracle then life would be really sweet.  I suggest reading http://www.codinghorror.com/blog/archives/001050.html Thanks! I read that at the time of posting but had since forgotten about it.",sql database version-control,1.9664651136136262E-4,0.00728156696332248,4.1375971695260474E-4,0.6294194730763012,0.3626885537320624
2332,"How do I turn on line numbers by default in TextWrangler on the Mac? I am fed up having to turn them on every time I open the application. Go to TextWrangler > Preferences. Choose Text Status Display in the category pane then check the option ""Show line numbers"" and close the preferences. This should now be on by default when you open existing documents.",osx textwrangler,0.007105384646575591,0.07622676292718336,0.014950287802482419,0.831406956507191,0.07031060811656768
3432,"Multiple Updates in MySQL I know that you can insert multiple rows at once is there a way to update multiple rows at once (as in in one query) in MySQL? Edit: For example I have the following Name   id  Col1  Col2 Row1   1    6     1 Row2   2    2     3 Row3   3    9     5 Row4   4    16    8 I want to combine all the following Updates into one query UPDATE table SET Col1 = 1 WHERE id = 1; UPDATE table SET Col1 = 2 WHERE id = 2; UPDATE table SET Col2 = 3 WHERE id = 3; UPDATE table SET Col1 = 10 WHERE id = 4; UPDATE table SET Col2 = 12 WHERE id = 4; I'm not sure I understand. When you want to update multiple rows do you want to update them all with the same value? in that case: UPDATE mytable SET valueField = 'NewValue' WHERE idField IN (1356515841251) or did you want to set multiple rows to different values?  Since you have dynamic values you need to use an IF or CASE for the columns to be updated. It gets kinda ugly but it should work. Using your example you could do it like:  UPDATE table SET Col1 = CASE id WHEN 1 THEN 1 WHEN 2 THEN 2 WHEN 4 THEN 10 ELSE Col1 END Col2 = CASE id WHEN 3 THEN 3 WHEN 4 THEN 12 ELSE Col2 END WHERE id IN (1 2 3 4);  UPDATE tableName SET col1='000' WHERE id='3' OR id='5' This should achieve what you'r looking for. Just add more id's. I have tested it.  You may also be interested in using joins on updates which is possible as well. Update someTable Set someValue = 4 From someTable s Inner Join anotherTable a on s.id = a.id Where a.id = 4 -- Only updates someValue in someTable who has a foreign key on anotherTable with a value of 4. Edit: If the values you are updating aren't coming from somewhere else in the database you'll need to issue multiple update queries.  Yes that's possible - you can use INSERT ... ON DUPLICATE KEY UPDATE. Using your example: INSERT INTO table (idCol1Col2) VALUES (111)(223)(393)(41012) ON DUPLICATE KEY UPDATE Col1=VALUES(Col1)Col2=VALUES(Col2); Note: this answer also assumes ID is the primary key FANTASTIC!! just awesome. Saved our day after several hours :-) So you have to select values first? Doesn't sound like an optimized update. Second answer seems to be more robust. @JayapalChandran you should use INSERT IGNORE together with ON DUPLICATE KEY UPDATE. http://dev.mysql.com/doc/refman/5.5/en/insert.html @HaralanDobrev ok. it has been some years and i am off that project. anyway... having it in mind. @HaralanDobrev Using INSERT IGNORE still inserts the non duplicated records. which Jayapal wanted to avoid. INSERT IGNORE just turns any errors into warning :( http://stackoverflow.com/questions/548541/insert-ignore-vs-insert-on-duplicate-key-update/548570#comment361469_548575 @JM4: this answer assumes ID is the primary key OR a unique key (but of course most probably the primary key) If there is no duplicates then i dont want that row to be inserted. what should id do? because i am fetching information from another site which maintains tables with id's. I am inserting values with respect to that id. if the site has new records then i will end up inserting only the ids and count except all other information. if and only if there is an entry for the id then it should update else it should skip. what shall i do? so if it is like insert into skip null or skip empty on duplicate update ... then that would be nice.  UPDATE table1 table2 SET table1.col1='value' table2.col1='value' WHERE table1.col3='567' AND table2.col6='567' This should work for ya. There is a reference in the MySQL manual for multiple tables.  The question is old yet I'd like to extend the topic with another answer. My point is the easiest way to achieve it is just to wrap multiple queries with a transaction. The accepted answer INSERT ... ON DUPLICATE KEY UPDATE is a nice hack but one should be aware of its drawbacks and limitations: As being said if you happen to launch the query with rows whose primary keys don't exist in the table the query inserts new ""half-baked"" records. Probably it's not what you want If you have a table with a not null field without default value and don't want to touch this field in the query you'll get ""Field 'fieldname' doesn't have a default value"" MySQL warning even if you don't insert a single row at all. It will get you into trouble if you decide to be strict and turn mysql warnings into runtime exceptions in your app. I made some performance tests for three of suggested variants including the INSERT ... ON DUPLICATE KEY UPDATE variant a variant with ""case / when / then"" clause and a naive approach with transaction. You may get the python code and results here. The overall conclusion is that the variant with case statement turns out to be twice as fast as two other variants but it's quite hard to write correct and injection-safe code for it so I personally stick to the simplest approach: using transactions. +1|By far the most useful answer around here. Thanks. Using transactions very nice (and simple) tip!  You can alias the same table to give you the id's you want to insert by (if you are doing a row-by-row update: UPDATE table1 tab1 table1 tab2 -- alias references the same table SET col1 = 1 col2 = 2 . . . WHERE tab1.id = tab2.id; Additionally It should seem obvious that you can also update from other tables as well. In this case the update doubles as a ""SELECT"" statement giving you the data from the table you are specifying. You are explicitly stating in your query the update values so the second table is unaffected.  UPDATE `your_table` SET `something` = IF(`id`=""1""""new_value1""`something`) `smth2` = IF(`id`=""1"" ""nv1""`smth2`) `something` = IF(`id`=""2""""new_value2""`something`) `smth2` = IF(`id`=""2"" ""nv2""`smth2`) `something` = IF(`id`=""4""""new_value3""`something`) `smth2` = IF(`id`=""4"" ""nv3""`smth2`) `something` = IF(`id`=""6""""new_value4""`something`) `smth2` = IF(`id`=""6"" ""nv4""`smth2`) `something` = IF(`id`=""3""""new_value5""`something`) `smth2` = IF(`id`=""3"" ""nv5""`smth2`) `something` = IF(`id`=""5""""new_value6""`something`) `smth2` = IF(`id`=""5"" ""nv6""`smth2`) // You just building it in php like $q = 'UPDATE `your_table` SET '; foreach($data as $dat){ $q .= ' `something` = IF(`id`=""'.$dat->id.'""""'.$dat->value.'""`something`) `smth2` = IF(`id`=""'.$dat->id.'"" ""'.$dat->value2.'""`smth2`)'; } $q = substr($q0-1); So you can update hole table with one query  Use a temporary table // Reorder items function update_items_tempdb(&$items) { shuffle($items); $table_name = uniqid('tmp_test_'); $sql = ""CREATE TEMPORARY TABLE `$table_name` ("" ."" `id` int(10) unsigned NOT NULL AUTO_INCREMENT"" ."" `position` int(10) unsigned NOT NULL"" ."" PRIMARY KEY (`id`)"" ."") ENGINE = MEMORY""; query($sql); $i = 0; $sql = ''; foreach ($items as &$item) { $item->position = $i++; $sql .= ($sql ? ' ' : '').""({$item->id} {$item->position})""; } if ($sql) { query(""INSERT INTO `$table_name` (id position) VALUES $sql""); $sql = ""UPDATE `test` `$table_name` SET `test`.position = `$table_name`.position"" ."" WHERE `$table_name`.id = `test`.id""; query($sql); } query(""DROP TABLE `$table_name`""); }  Not sure why another useful option is not yet mentioned: UPDATE my_table m JOIN ( SELECT 1 as id 10 as _col1 20 as _col2 UNION ALL SELECT 2 5 10 UNION ALL SELECT 3 15 30 ) vals ON m.id = vals.id SET col1 = _col1 col2 = __col2;  There is a setting you can alter called 'multi statement' that disables MySQL's 'safety mechanism' implemented to prevent (more than one) injection command. Typical to MySQL's 'brilliant' implementation it also prevents user from doing efficient queries. Here (http://dev.mysql.com/doc/refman/5.1/en/mysql-set-server-option.html) is some info on the C implementation of the setting. If you're using PHP you can use mysqli to do multi statements (I think php has shipped with mysqli for a while now) $con = new mysqli('localhost''user1''password''my_database'); $query = ""Update MyTable SET col1='some value' WHERE id=1 LIMIT 1;""; $query .= ""UPDATE MyTable SET col1='other value' WHERE id=2 LIMIT 1;""; //etc $con->multi_query($query); $con->close(); Hope that helps. This is the same as sending the queries separately. The only difference is that you send it all in one network packet but the UPDATEs will be still processed as separate queries. Better is to wrap them in one transaction then the changes will be commited to the table at once.  Yes ..it is possible using INSERT ON DUPLICATE KEY UPDATE sql statement.. syntax: INSERT INTO table_name (abc) VALUES (123)(456) ON DUPLICATE KEY UPDATE a=VALUES(a)b=VALUES(b)c=VALUES(c)  The following will update all rows in one table Update Table Set Column1 = 'New Value' The next one will update all rows where the value of Column2 is more than 5 Update Table Set Column1 = 'New Value' Where Column2 > 5 There is all Unkwntech's example of updating more than one table UPDATE table1 table2 SET table1.col1 = 'value' table2.col1 = 'value' WHERE table1.col3 = '567' AND table2.col6='567'",mysql sql sql-update,2.9623288358906406E-4,0.007073558294149437,0.023996679647224208,0.013236855135655572,0.9553966740393818
1010,"How to get the value of built encoded ViewState? I need to grab the base64-encoded representation of the ViewState. Obviously this would not be available until fairly late in the request lifecycle which is OK. For example if the output of the page includes: <input type=""hidden"" name=""__VIEWSTATE"" id=""__VIEWSTATE"" value=""/wEPDwUJODU0Njc5MD...=="" /> I need a way on the server side to get the value ""/wEPDwUJODU0Njc5MD...=="" To clarify I need this value when the page is being rendered not on PostBack. e.g. I need to know the ViewState value that is being sent to the client not the ViewState I'm getting back from them. See this blog post where the author describes a method for overriding the default behavior for generating the ViewState and instead shows how to save it on the server Session object. In ASP.NET 2.0 ViewState is saved by a descendant of PageStatePersister class. This class is an abstract class for saving and loading ViewsState and there are two implemented descendants of this class in .Net Framework named HiddenFieldPageStatePersister and SessionPageStatePersister. By default HiddenFieldPageStatePersister is used to save/load ViewState information but we can easily get the SessionPageStatePersister to work and save ViewState in Session object. Although I did not test his code it seems to show exactly what you want: a way to gain access to ViewState code while still on the server before postback.  I enabled compression following similar articles to those posted above. The key to accessing the ViewState before the application sends it was overriding this method; protected override void SavePageStateToPersistenceMedium(object viewState) You can call the base method within this override and then add whatever additional logic you require to handle the ViewState.  Rex I suspect a good place to start looking is solutions that compress the ViewState -- they're grabbing ViewState on the server before it's sent down to the client and gzipping it. That's exactly where you want to be. Scott Hanselman on ViewState Compression (2005) ViewState Compression with System.IO.Compression (2007)",c# asp.net,0.001429331283325961,0.08112058872062466,0.7548552333376515,0.08266436172080878,0.07993048493758913
3017,"How to generate getters and setters in Visual Studio? By ""generate"" I mean auto-generation of the code necessary for a particuliar selected (set of) variable(s). But any more explicit explication or comment on good practice is welcome. If you're using ReSharper go into the ReSharper menu --> Code --> Generate ... (or hit Alt+Ins inside the surrounding class) and you'll get all the options for generating getters and/or setters you can think of :-)  By generate do you mean auto-generate? If that's not what you mean: Visual Studio 2008 has the easiest implementation for this: public PropertyType PropertyName { get; set; } In the background this creates an implied instance variable to which your property is stored and retrieved. However if you want to put in more logic in your Properties you will have to have an instance variable for it: private PropertyType _property; public PropertyType PropertyName {  get  {  //logic here  return _property;  }  set  {  //logic here  _property = value;  }  } Previous versions of Visual Studio always used this longhand method as well. Is not the standard in C# is the lower dash after the member name `property_` instead of before `_property` ? @Julen - I've always seen the _property convention.  If you are using Visual Studio 2005 and up you can create a setter/getter real fast using the insert snippet command. Right click on your code click on Insert Snippet (Ctrl+kx) and then choose ""prop"" form the list. Hope this helps.  Visual Studio also has a feature that will generate a Property from a private variable. If you right-click on a variable in the context menu that pops up click on the ""Refactor"" item. Then choose encapsulate field. This will create a getter/setter property for a variable. I'm not too big a fan of this technique as it is a little bit awkward to use if you have to create a lot of getters/setters and it puts the property directly below the private field which bugs me because I usually have all of my private fields grouped together and this Visual Studio feature breaks my class' formatting. Unfortunately this option is not given by the express editions of Visual Studio. there is shourtcut too - I preffer this way :) - CTRL+R+E  In addition to the 'prop' snippet and auto-properties there is a refactor option to let you select an existing field and expose it via a property. Also if you don't like the 'prop' implementation you can create your own snippets. Additionally a 3rd party refactoring tool like resharper will give you even more features and make it easier to create more advanced snippets. I'd recommend Resharper if you can afford it. http://msdn.microsoft.com/en-us/library/f7d3wz0k(VS.80).aspx http://www.jetbrains.com/  you can also use ""propfull"" and hit TAB twice variable and property with get and set will be generate. this is the more useful one and I always forget it I'm doing XML serialization and spawning a ton of properties all over the show. +100septillion upvotes if I could. Thanks!Edit- EVEN BETTER autohotkey script + ^this = productivity over 90000!  Rather than using ctrl+kx you can also just type prop and then hit tab twice totally nifty. Thanks! And don't forget ""propg"" + tab which generates the same but with a private setter.  I don't have Visual Studio installed on my machine anymore (and I'm using Linux) but I do remember that there was an wizard hidden somewhere inside one of the menus that gave access to a class builder. With this wizard you could define all your classes' details including methods and attributes. If I remember well there was an option through which you could ask VS to create the setters and getters automatically for you. I know it's quite vague but check it out and you might find it.",c# visual-studio setter getter,7.228328759869963E-4,0.5923427918097425,0.006273646950117564,0.35073326160231333,0.04992746676183961
564,"What is the difference between an int and an Integer in Java and C#? I was just sitting at my local Borders sipping coffee and reading More Joel on Software (for free) when I came across Joel Spolsky saying something about a particular type of programmer knowing the difference between an int and an Integer in Java/C# (Object Oriented Programming Languages). After a quick 'brain check' I realized to my dismay that I didn't know the answer. C# doesn't have an Integer type. There's no such thing as an Integer in C. Don't you mean C#? Regarding Java 1.5 and autoboxing there is an important ""quirk"" that comes to play when comparing Integer objects. In Java Integer objects with the values -128 to 127 are immutable (that is for one particular integer value say 23 all Integer objects instantiated through your program with the value 23 points to the exact same object). Example this returns true: Integer i1 = new Integer(127); Integer i2 = new Integer(127); System.out.println(i1 == i2); // true While this returns false: Integer i1 = new Integer(128); Integer i2 = new Integer(128); System.out.println(i1 == i2); // false The == compares by reference (does the variables point to the same object). This result may or may not differ depending on what JVM you are using. The specification autoboxing for Java 1.5 requires that integers (-128 to 127) always box to the same wrapper object. A solution? =) One should always use the Integer.equals() method when comparing Integer objects. System.out.println(i1.equals(i2)); // true More info at java.net Example at bexhuff.com Objects created with the new operator will always return false when compared with ==. Andreas is confusing Integer.valueOf(int) with new Integer(int)  There are many reasons to use wrapper classes: We get extra behavior (for instance we can use methods) We can store null values whereas in primitives we cannot Collections support storing objects and not primitives.  In platforms like Java ints are primitives while Integer is an object which holds a integer field. The important distinction is that primitives are always passed around by value and by definition are immutable. Any operation involving a primitive variable always returns a new value. On the other hand objects are passed around by reference. One could argue that the point to the object (AKA the reference) is also being passed around by value but the contents are not.  I'll just post here since some of the other posts are slightly inaccurate in relation to C#. Correct: int is an alias for System.Int32. Wrong: float is not an alias for System.Float but for System.Single Basically int is a reserved keyword in the C# programming language and is an alias for the System.Int32 value type. float and Float is not the same however as the right system type for ''float'' is System.Single. There are some types like this that has reserved keywords that doesn't seem to match the type names directly. In C# there is no difference between ''int'' and ''System.Int32'' or any of the other pairs or keywords/system types except for when defining enums. With enums you can specify the storage size to use and in this case you can only use the reserved keyword and not the system runtime type name. Wether the value in the int will be stored on the stack in memory or as a referenced heap object depends on the context and how you use it. This declaration in a method: int i; defines a variable i of type System.Int32 living in a register or on the stack depending on optimizations. The same declaration in a type (struct or class) defines a member field. The same declaration in a method argument list defines a parameter with the same storage options as for a local variable. (note that this paragraph is not valid if you start pulling iterator methods into the mix these are different beasts altogether) To get a heap object you can use boxing: object o = i; this will create a boxed copy of the contents of i on the heap. In IL you can access methods on the heap object directly but in C# you need to cast it back to an int which will create another copy. Thus the object on the heap cannot easily be changed in C# without creating a new boxed copy of a new int value. (Ugh this paragraph doesn't read all that easily.)  I'll add to the excellent answers given above and talk about boxing and unboxing and how this applies to Java (although C# has it too). I'll use just Java terminology because I am more au fait with that. As the answers mentioned int is just a number (called the unboxed type) whereas Integer is an object (which contains the number hence a boxed type). In Java terms that means (apart from not being able to call methods on int) you cannot store int or other non-object types in collections (List Map etc.). In order to store them you must first box them up in its corresponding boxed type. Java 5 onwards have something called auto-boxing and auto-unboxing which allow the boxing/unboxing to be done behind the scenes. Compare and contrast: Java 5 version: Deque<Integer> queue; void add(int n) {  queue.add(n); } int remove() {  return queue.remove(); } Java 1.4 or earlier (no generics either): Deque queue; void add(int n) {  queue.add(Integer.valueOf(n)); } int remove() {  return ((Integer) queue.remove()).intValue(); } It must be noted that despite the brevity in the Java 5 version both versions generate identical bytecode. Thus although auto-boxing and auto-unboxing is very convenient because you write less code these operations do happen behind the scenes with the same runtime costs so you still have to be aware of their existence. Hope this helps! Deque isn't in java 1.5 or 1.4. It was added in 1.6.  In both Java and C# 'int' is 4-byte signed integer. Unlike Java C# Provides both signed and unsigned integer values.As Java and C# are object object-oriented  some operations in these languages do not map directly onto instructions provided by the runtime and so needs to be defined as part of an object of some type. C# provides System.Int32 which is a value type using a part of memory that belongs to the reference type on the heap. java provides java.lang.Integer which is a reference type operating on int. The methods in Integer can't be compiled directly to runtime instructions.So we box an int value to convert it into an instance of Integer and use the methods which expects instance of some type (like toStringparseIntvalueOf etc). In C# variable int refers to System.Int32.Any 4-byte value in memory can be interpreted as a primitive int that can be manipulated by instance of System.Int32.So int is an alias for System.Int32.When using integer-related methods like int.Parse() int.ToString() etc integer is compiled into the FCL System.Int32 struct calling the respective methods like Int32.Parse() Int32.ToString().  In Java there are two basic types in the JVM. 1) Primitive types and 2) Reference Types. int is a primitive type and Integer is a class type (which is kind of reference type). Primitive values do not share state with other primitive values. A variable whose type is a primitive type always holds a primitive value of that type. int aNumber = 4; int anotherNum = aNumber; aNumber += 6; System.out.println(anotherNum); // Prints 4 An object is a dynamically created class instance or an array. The reference values (often just references) are pointers to these objects and a special null reference which refers to no object. There may be many references to the same object. Integer aNumber = Integer.valueOf(4); Integer anotherNumber = aNumber; // anotherNumber references the // same object as aNumber Also in Java everything is passed by value. With objects the value that is passed is the reference to the object. So another difference between int and Integer in java is how they are passed in method calls. For example in public int add(int a int b) { return a + b; } final int two = 2; int sum = add(1 two); The variable two is passed as the primitive integer type 2. Whereas in public int add(Integer a Integer b) { return a.intValue() + b.intValue(); } final Integer two = Integer.valueOf(2); int sum = add(Integer.valueOf(1) two); The variable two is passed as a reference to an object that holds the integer value 2. @WolfmanDragon: Pass by reference would work like so: public void increment(int x) { x = x + 1; } int a = 1; increment(a); // a is now 2 When increment is called it passes a reference (pointer) to variable a. And the increment function directly modifies variable a. And for object types it would work as follows: public void increment(Integer x) { x = Integer.valueOf(x.intValue() + 1); } Integer a = Integer.valueOf(1); increment(a); // a is now 2 Do you see the difference now? By your definition there is no pass by reference. Anything that is passed must have a value(even null is a value) even if it is just a value of the pointer otherwise it is just an empty set. By CS terms pass by reference is passing the value of the the reference(pointer). I'm a little confused.?  In Java the 'int' type is a primitive  whereas the 'Integer' type is an object. In C# the 'int' type is the same as System.Int32 and is a value type (ie more like the java 'int'). An integer (just like any other value types) can be boxed (""wrapped"") into an object. The differences between objects and primitives are somewhat beyond the scope of this question but to summarize: Objects provide facilities for polymorphism are passed by reference (or more accurately have references passed by value) and are allocated from the heap. Conversely primitives are immutable types that are passed by value and are often allocated from the stack. ‒1. This may accurately describe how Java handles this but for C# it is plain wrong. I don't know Java but there is not type `Integer` but `Int32``Int64` and they are all struct which is value type. Primitive means in C# that types are defined in FCL (Framework Class Library) by CLR team and that's why they're called primitive. In this case even Date obj is called primitive type. `int` is a keyword which is exactly the same thing as `Int32` and `single` is `Int16` and etc. They become objects when they are boxed however this is not what OP said. Why is this voted up? The answer is wrong. It's not quite right for Java and not even close to right for C#. Someone who reads this will know *less* about the topic than they did before. I took the liberty of adding the C# aspect to this answer (instead of creating a new answer). The statement that ""objects [...] are passed by reference"" is confusing and incorrect IMO. It's more accurate to say that ""Object references are passed by value."" (Also primitives aren't always allocated from the stack - consider a primitive field within an object...) In C# at least int is a language keyword that is equivalent to the Int32 CLR (actually CTS) type. @Jon Skeet i'm aware that you are seen as a C# Guru but due to the way the ENGLISH LANGUAGE works you can just say passed by reference as it is exatcly the same meaning not programming related. Yes I'm aware that passing by reference in C# just causes the reference to be passed by ref. Sorry the English language does not make ""pass something by reference"" and ""pass a reference to something by value"" equivalent statements nor do these have equivalent meanings in a programming language context.  One more thing that I don't see in previous answers: In Java the primitive wrappers classes like Integer Double Float Boolean... and String are suposed to be invariant so that when you pass an instance of those classes the invoked method couldn't alter your data in any way in opositión with most of other classes which internal data could be altered by its public methods. So that this classes only has 'getter' methods no 'setters' besides the constructor. In a java program String literals are stored in a separate portion of heap memory only a instance for literal to save memory reusing those instances  Well in Java an int is a primitive while an Integer is an Object. Meaning if you made a new Integer: Integer i = new Integer(6); You could call some method on i: String s = i.toString();//sets s the string representation of i Whereas with an int: int i = 6; You cannot call any methods on it because it is simply a primitive. So: String s = i.toString();//will not work!!! would produce an error because int is not an object. int is one of the few primitives in Java (along with char and some others). I'm not 100% sure but I'm thinking that the Integer object more or less just has an int property and a whole bunch of methods to interact with that property (like the toString() method for example). So Integer is a fancy way to work with an int (Just as perhaps String is a fancy way to work with a group of chars). I know that Java isn't C but since I've never programmed in C this is the closest I could come to the answer. Hope this helps! Integer object javadoc Integer Ojbect vs. int primitive comparison I don't know Java but there is not type Integer but Int32Int64 and they are all struct which is value type. Primitive means in C# that types are defined in FCL (Framework Class Library) by CLR team and that's why they're called primitive. In this case even Date obj is called primitive type. in C# int is a synonym for Int32 see http://stackoverflow.com/questions/62503/c-int-or-int32-should-i-care  have you ever programmed before then (int) is one of the primitive types you can set for your variables (just like char float ...). but Integer is a wrapper class that you can use it to do some functions on an int variable (e.g convert it to string or vise versa...)  but keep note that methods in the wrapper classes are static so you can use them anytime without creating an instance of Integer class. as a recap :  int x; Integer y; x and y are both variables of type int but y is wrapped by an Integer class and has several methods that you usebut i case you need to call some functions of Integer wrapper class you can do it simply.  Integer.toString(x); but be aware that both x and y are corect but if you want to use them just as a primitive type use the simple form (used for defining x).  This has already been answered for Java here's the C# answer: ""Integer"" is not a valid type name in C# and ""int"" is just an alias for System.Int32. Also unlike in Java (or C++) there aren't any special primitive types in C# every instance of a type in C# (including int) is an object. Here's some demonstrative code: void DoStuff() {  System.Console.WriteLine( SomeMethod((int)5) );  System.Console.WriteLine( GetTypeName<int>() ); } string SomeMethod(object someParameter) {  return string.Format(""Some text {0}"" someParameter.ToString()); } string GetTypeName<T>() {  return (typeof (T)).FullName; } To be clear in C# int as well as System.Int32 are not objects. They are value types and treated much differently by the CLR than objects. Actually in C# Int32 is an object. It is a valuetype struct object that derives from system.object. It is not treated especially differently from other value objects as an ""int"" would be in Java.  In C# int is just an alias for System.Int32 string for System.String double for System.Double etc... Personally I prefer int string double etc. because they don't require a using System; statement :) A silly reason I know... And it should be added C#'s int/Int32 are *not* the same as Java's Integer.  An int and Integer in Java and C# are two different terms used to represent different things. An int is one of the the primitive data types that can be assigned to a variable that can store exactly one value of its declared type at a time. For example( int number = 7; ) where int is the datatype assigned to the variable number which holds the value 7. So an int is just a primitive not an Object. While an Integer is a wrapper class for a primitive data type which has static methods. Integer can be used as an argument to a method which requires an object where as int can be used as an argument to a method which requires an integer value that can be used for arithmetic expression. For example ( Integer number = new Integer(5); )  int is used to declare primitive variable e.g. int i=10; Integer is used to create reference variable of class Integer Integer a = new Integer();",c# java integer int,2.1529774590926728E-4,0.9323717281730769,4.530033804884225E-4,0.045010840918176837,0.02194912978234856
1854,"Python: What OS am I running on? What do I need to look at to see if I'm on Windows Unix etc? see (http://bugs.python.org/issue12326) for details! /usr/bin/python3.2 def cls(): from subprocess import call from platform import system os = system() if os == 'Linux': call('clear' shell = True) elif os == 'Windows': call('cls' shell = True) Welcome on SO here it is a good practice to explain why to use your solution and not just how. That will make your answer more valuable and help further reader to have a better understanding of how you do it. I also suggest that you have a look on our FAQ : http://stackoverflow.com/faq. Good answer maybe even on par with the original answer. But you could explain why.  >>> import platform >>> platform.system()  I am using the WLST tool that comes with weblogic and it doesn't implement the platform package. wls:/offline> import os wls:/offline> print os.name java wls:/offline> import sys wls:/offline> print sys.platform 'java1.5.0_11' Apart from patching the system javaos.py (issue with os.system() on windows 2003 with jdk1.5) (which I can't do I have to use weblogic out of the box) this is what I use: def iswindows(): os = java.lang.System.getProperty( ""os.name"" ) return os.lower().find(""win"") > -1  Dang -- lbrandy beat me to the punch but that doesn't mean I can't provide you with the system results for Vista! >>> import os >>> os.name 'nt' >>> import platform >>> platform.system() 'Windows' >>> platform.release() 'Vista'  For the record here's the results on Mac: >>> import os >>> os.name 'posix' >>> import platform >>> platform.system() 'Darwin' >>> platform.release() '8.11.1'  I do this import sys print sys.platform Docs here : sys.platform. Everything you need is probably in the sys module.  in the same vein.... import platform is_windows=(platform.system().lower().find(""win"") > -1) if(is_windows): lv_dll=LV_dll(""my_so_dll.dll"") else: lv_dll=LV_dll(""./my_so_dll.so"") This is problematic if you are on a Mac since platform.system() returns ""Darwin"" on a Mac and ""Darwin"".lower().find(""win"") = 3.  If you not looking for the kernel version etc but looking for the linux distribution you may want to use the following in python2.6+ >>> import platform >>> print platform.linux_distribution() ('CentOS Linux' '6.0' 'Final') >>> print platform.linux_distribution()[0] CentOS Linux >>> print platform.linux_distribution()[1] 6.0 in python2.4 >>> import platform >>> print platform.dist() ('centos' '6.0' 'Final') >>> print platform.dist()[0] centos >>> print platform.dist()[1] 6.0 Obviously this will work only if you are running this on linux. If you want to have more generic script across platforms you can mix this with code samples given in other answers.  For Jython the only way to get os name I found is to check os.name Java property (tried with sys os and platform modules for Jython 2.5.3 on WinXP): def get_os_platform(): """"""return platform name but for Jython it uses os.name Java property"""""" ver = sys.platform.lower() if ver.startswith('java'): import java.lang ver = java.lang.System.getProperty(""os.name"").lower() print('platform: %s' % (ver)) return ver  A comparison between the different methods and what they return on different operating systems can be found here: OS_flavor_name_version Methods that are compared: import platform import sys def linux_distribution(): try: return platform.linux_distribution() except: return ""N/A"" print(""""""Python version: %s dist: %s linux_distribution: %s system: %s machine: %s platform: %s uname: %s version: %s mac_ver: %s """""" % ( sys.version.split('\n') str(platform.dist()) linux_distribution() platform.system() platform.machine() platform.platform() platform.uname() platform.version() platform.mac_ver() ))  >>> import os >>> print os.name posix >>> import platform >>> platform.system() 'Linux' >>> platform.release() '2.6.22-15-generic' See: platform — Access to underlying platform’s identifying data @mcepl what's not correct? are you replying to a deleted comment? could you clarify? I guess so (I don't remember). Somebody probably claimed it doesn't work with jython. @mcepl Then a @... tag would have been useful.  You can also use sys.platform if you already have imported sys and you don't want to import another module >>> import sys >>> sys.platform 'linux2'  Interesting results on windows 8: >>> import os >>> os.name 'nt' >>> import platform >>> platform.system() 'Windows' >>> platform.release() 'post2008Server' Edit: That's a bug",python,5.908493891773921E-4,0.05684281355903113,0.0012431935574916632,0.9315915179380723,0.009731625556227584
4913,"How to make a button appear as if it is pressed? Using VS2008 C# .Net 2 and Winforms how can I make a regular Button look ""pressed""? Imagine this button is an on/off switch. ToolStripButton has the Checked property but the regular Button does not. One method you can used to obtain this option is by placing a ""CheckBox"" object and changing its ""Appearance"" from ""Normal"" to ""Button"" this will give you the same functionality that I believe you are looking for.  I think you may need a ToggleButton. You can take a look at third party vendors of WinForms components such as Telerik DevExpress ComponentFactory ViBlend which provide such control. They all provide toggle buttons.  You could probably also use the ControlPaint class for this.",c# .net winforms gui button,0.004420562457856503,0.42528131909864436,0.5034224299774264,0.05219841818883336,0.014677270277239482
4544,"Http Auth in a Firefox 3 bookmarklet Im trying to create a bookmarklet for posting del.icio.us bookmarks to a seperate account. I tested it from the command line like: wget -O - --no-check-certificate \ ""https://seconduser:thepassword@api.del.icio.us/v1/posts/add?url=http://seet.dk&description=test"" and this works great I then wanted to create a bookmarklet in my firefox. I googled and found bits and pieces and ended up with: javascript:void( open('https://seconduser:password@api.del.icio.us/v1/posts/add?url=' +encodeURIComponent(location.href) +'&description='+encodeURIComponent(document.title) 'delicious''toolbar=nowidth=500height=250' ) ); but all that happens is that i get this from del.icio.us: <?xml version=""1.0"" standalone=""yes""?> <result code=""access denied"" /> <!-- fe04.api.del.ac4.yahoo.net uncompressed/chunked Thu Aug 7 02:02:54 PDT 2008 --> If I then go to the address bar and presses enter it changes to: <?xml version='1.0' standalone='yes'?> <result code=""done"" /> <!-- fe02.api.del.ac4.yahoo.net uncompressed/chunked Thu Aug 7 02:07:45 PDT 2008 --> Any ideas how to get it to work directly from the bookmarks? Does calling the method twice work? Seems to me that your authentication is being approved after the content arrives so then a second attempt now works because you have the correct cookies.  I see what you mean. I've just tried calling https://seconduser:password@api.del.icio.us then https://api.del.icio.us in the same window and neither works. I hope someone more knowledgeable can help?  I'd recommend checking out the iMacros addon for Firefox. I use it to login to a local web server and after logging in navigate directly to a certain page. The code I have looks like this but it allows you to record your own macros: VERSION BUILD=6000814 RECORDER=FX TAB T=1 URL GOTO=<http://10.20.2.4/login> TAG POS=1 TYPE=INPUT:TEXT FORM=NAME:introduce ATTR=NAME:initials CONTENT=username-goes-here SET !ENCRYPTION NO TAG POS=1 TYPE=INPUT:PASSWORD FORM=NAME:introduce ATTR=NAME:password CONTENT=password-goes-here TAG POS=1 TYPE=INPUT:SUBMIT FORM=NAME:introduce ATTR=NAME:Submit&&VALUE:Go URL GOTO=<http://10.20.2.4/timecard> I middle click on it and it opens a new tab and runs the macro taking me directly to the page I want logged in with the account I specified.  Does calling the method twice work? I tested it just now and that didnt make a difference. To me it seems like firefox i preventing the http auth?  @travis Looks very nice! I will sure take a look into it. I can think of several places I can use that I never got round to sniff the traffic but found out that a php site on my own server with http-auth worked fine so i figured it was something with delicious. I then created a php page that does a wget of the delicious api and everything works fine :)  Can you sniff the traffic to find what's actually being sent? Is it sending any auth data at all and it's incorrect or being presented in a form the server doesn't like or is it never being sent by firefox at all?",javascript firefox delicious-api,8.524857172629724E-4,0.23335506489186308,0.7304748142410196,0.02127671043300656,0.014040924716847925
4387,"Best Multi-Language Documentation Generator What is the best documentation generator? I want to something that will easily add templates for documenting functions classes etc. I know there are several tools out there -- from Visual Studio plugins to external applications that take code files as input. which is the best? (If language-specific specify) are there any documentation tools that could be used for multiple languages (e.g. VB.net and JavaScript) Bonus points if they're free / open source. I use Doxygen too. I even created a shortcut to document methods with ctrl+shift+d in VAX.  There are several good options but I also cast a vote for Doxygen.  http://www.stack.nl/~dimitri/doxygen/ is probably a good place to start. From the site: ""Doxygen is a documentation system for C++ C Java Objective-C Python IDL (Corba and Microsoft flavors) Fortran VHDL PHP C# and to some extent D."" For JavaScript there's http://jsdoc.sourceforge.net/ but it's not multi-language. For VB.NET there's http://vbdox.sourceforge.net/ also not multi-language. I accepted this answer because it was thorough but I really would still like to see a single solution for both .net and JS  I vote for Doxygen as well. I am using Doxygen for C#.NET and it generates class diagrams inheritance diagrams etc. Here is an informative blog post.  As others have said Doxygen is probably your best bet. It works across many different languages and just as importantly it works with a number of different commenting styles. For example in C# you can continue to use the xml documentation in order to get the intellisense benefits that come with it as well as the ability to also use other tools such as the sandcastle help file builder. In other languages you're still free to use more concise styles. It can generate output in a number of different ways including html man page ps pdf (with LaTex) and a few others I believe.  I really like Doxygen. It works for ""C++ C Java Objective-C Python IDL (Corba and Microsoft flavors) Fortran VHDL PHP C# and to some extent D."" It supports several output formats and it will also generate hierarchical and ""call"" charts using the dot program.",documentation generator,0.047969001919499994,0.611069263320667,0.0029794244483974178,0.32397019579084363,0.014012114520591885
947,"How to use a mutex in Visual Basic I have imported the kernel32 library so I have the createMutex function available but I am not quite sure of the various parameters and return values. This is classic Visual Basic not Visual Basic.NET but I can probably work with either language in the form of an answer. Here's the VB6 declarations for CreateMutex - I just copied them from the API viewer which you should have as part of your VB6 installation. VB6 marshalls strings to null-terminated ANSI using the current code page. Public Type SECURITY_ATTRIBUTES nLength As Long lpSecurityDescriptor As Long bInheritHandle As Long End Type Public Declare Function CreateMutex Lib ""kernel32"" Alias ""CreateMutexA"" _ (lpMutexAttributes As SECURITY_ATTRIBUTES ByVal bInitialOwner As Long _ ByVal lpName As String) As Long Bear in mind that if you create a mutex from the VB6 IDE the mutex belongs to the IDE and won't be destroyed when you stop running your program - only when you close the IDE.  The VB code looks something like this: hMutex = CreateMutex(ByVal 0& 1 ByVal 0&) The first parameter is a pointer to an SECURITY_ATTRIBUTES structure. If you don't know what it is you don't need it. Pass NULL (0). The second parameter is TRUE (non-zero or 1) if the calling thread should take ownership of the mutex. FALSE otherwise. The third parameter is the mutex name and may be NULL (0) as shown. If you need a named mutex pass the name (anything unique) in. Not sure whether the VB wrapper marshals the length-prefixed VB string type (BSTR) over to a null-terminated Ascii/Unicode string if not you'll need to do that and numerous examples are out there. Good luck!  Well based on the documentation (http://msdn.microsoft.com/en-us/library/ms682411(VS.85).aspx) it looks like: Security attributes (can pass null) Whether it's initially owned (can pass false) The name of it HTH",vb6,0.0015613988962998786,0.7354032621934081,0.06488409443736924,0.17243411641791603,0.025717128055006627
1528,"Hiding inherited members I'm looking for some way to effectively hide inherited members. I have a library of classes which inherit from common base classes. Some of the more recent descendant classes inherit dependency properties which have become vestigial and can be a little confusing when using IntelliSense or using the classes in a visual designer. These classes are all controls that are written to be compiled for either WPF or Silverlight 2.0. I know about ICustomTypeDescriptor and ICustomPropertyProvider but I'm pretty certain those can't be used in Silverlight. It's not as much a functional issue as a usability issue. What should I do? Update Some of the properties that I would really like to hide come from ancestors that are not my own and because of a specific tool I'm designing for I can't do member hiding with the new operator. (I know it's ridiculous) To fully hide and mark not to use including intellisense which I believe is what most readers expect ... [Obsolete(""Not applicable in this class."")] [DesignerSerializationVisibility(DesignerSerializationVisibility.Hidden)] [Browsable(false) EditorBrowsable(EditorBrowsableState.Never)]  Override them like Michael Suggests above and to prevent folks from using the overridden (sp?) methods mark them as obsolete: [Obsolete(""These are not supported in this class."" true)] public override  void dontcallmeanymore() { } If the second parm is set to true a compiler error will be generated if anyone tries to call that method and the string in the first parm is the message. If parm2 is false only a compiler warning will be generated.  I know there's been several answers to this and it's quite old now but the simplest method to do this is just declare them as new private. Consider an example I am currently working on where I have an API that makes available every method in a 3rd party DLL. I have to take their methods but I want to use a .Net property instead of a ""getThisValue"" and ""setThisValue"" method. So I build a second class inherit the first make a property that uses the get and set methods and then override the original get and set methods as private. They're still available to anyone wanting to build something different on them but if they just want to use the engine I'm building then they'll be able to use properties instead of methods. Using the double class method gets rid of any restrictions on being unable to use the new declaration to hide the members. You simply can't use override if the members are marked as virtual. public class APIClass { private static const string DllName = ""external.dll""; [DllImport(DllName)] public extern unsafe uint external_setSomething(int x uint y); [DllImport(DllName)] public extern unsafe uint external_getSomething(int x uint* y); public enum valueEnum { On = 0x01000000; Off = 0x00000000; OnWithOptions = 0x01010000; OffWithOptions = 0x00010000; } } public class APIUsageClass : APIClass { public int Identifier; private APIClass m_internalInstance = new APIClass(); public valueEnum Something { get { unsafe { valueEnum y; fixed (valueEnum* yPtr = &y) { m_internalInstance.external_getSomething(Identifier yPtr); } return y; } } set { m_internalInstance.external_setSomething(Identifier value); } } new private uint external_setSomething(int x float y) { return 0; } new private unsafe uint external_getSomething(int x float* y) { return 0; } } Now valueEnum is available to both classes but only the property is visible in the APIUsageClass class. The APIClass class is still available for people who want to extend the original API or use it in a different way and the APIUsageClass is available for those who want something more simple. Ultimately what I'll be doing is making the APIClass internal and only expose my inherited class.  While you cannot prevent usage of those inherited members to my knowledge you should be able to hide them from IntelliSense using the EditorBrowsableAttribute: Using System.ComponentModel; [EditorBrowsable(EditorBrowsableState.Never)] private string MyHiddenString = ""Muahahahahahahahaha""; Edit: Just saw this in the documentation comments which makes it kinda useless for this purpose: There is a prominent note that states that this attribute ""does not suppress members from a class in the same assembly"". That is true but not complete. Actually the attribute does not suppress members from a class in the same solution.  I think you're best least hackish way is to consider composition as opposed to inheritance. Or you could create an interface that has the members you want have your derived class implement that interface and program against the interface.  I tested all of the proposed solutions and they do not really hide new members. But this one DOES: [DesignerSerializationVisibility(DesignerSerializationVisibility.Hidden)] public new string MyHiddenProperty { get { return _myHiddenProperty; } } But in code-behide it's still accessible so add as well Obsolete Attribute [Obsolete(""This property is not supported in this class"" true)] [DesignerSerializationVisibility(DesignerSerializationVisibility.Hidden)] public new string MyHiddenProperty { get { return _myHiddenProperty; } }  One potential thing you can do is contain the object rather than extend from the other class. This will give you the most flexibility in terms of exposing what you want to expose but if you absolutely need the object to be of that type it is not the ideal solution (however you could expose the object from a getter). Thus: public class MyClass : BaseClass {  // Your stuff here } Becomes: public class MyClass {  private BaseClass baseClass;  public void ExposeThisMethod()  {  baseClass.ExposeThisMethod();  } } Or: public class MyClass {  private BaseClass baseClass;  public BaseClass BaseClass  {  get  {  return baseClass;  }  } }",c# wpf silverlight polymorphism dependency-properties,0.8185754736261586,0.09073484584271045,0.0013422717052844603,0.0872293099186828,0.002118098907163789
594,cx_Oracle - what is the best way to iterate over a result set? There are several ways to iterate over a result set which way is the best? There are exceptions to this but the general rule of thumb is: if there is more than one way to do it it's probably because each way is suitable for different situations. Otherwise there would only be one way. There are three ways to iterate over a result set. The best way in terms of both readability and performance is usually to use the built-in cursor iterator. curs.execute('select * from people') for row in curs: print row You can fetch all the rows into a list but this can have some bad side effects if the result set is large. You have to wait for the entire result set to be returned to your client process. You may eat up a lot of memory in your client to hold the built-up list. It may take a while for Python to construct and deconstruct the list which you are going to immediately discard anyways. for row in curs.fetchall(): print row Finally you can loop over the result set fetching one row at a time. In general there's no particular advantage in doing this over using the iterator. If there is something in your programming logic that seems to indicate there is an advantage in doing this perhaps you should reconsider your programming logic. row = curs.fetchone() while row: print row row = curs.fetchone() I think SScursor is for MySQL. But anything that has a fetchall() will probably have the same memory usage as it returns a list of all the rows returned. about the second method what if you use a SScursor ? will it stil eat up a lot of memory?  There's also the way psyco-pg seems to do it... From what I gather it seems to create dictionary-like row-proxies to map key lookup into the memory block returned by the query. In that case fetching the whole answer and working with a similar proxy-factory over the rows seems like useful idea. Come to think of it though it feels more like Lua than Python. Also this should be applicable to all PEP-249 DBAPI2.0 interfaces not just Oracle or did you mean just fastest using Oracle?  My preferred way is the cursor iterator but setting first the arraysize property of the cursor. curs.execute('select * from people') curs.arraysize = 256 for row in curs: print row In this example cx_Oracle will fetch rows from Oracle 256 rows at a time reducing the number of network round trips that need to be performed In my tests (on a database connected by LAN) this actually seemed to give identical (even slower in a couple iterations) speed as compared to doing 'fetchone()' repeatedly. I was doing it with about 12000 entries... Very odd!,python sql database oracle cx-oracle,0.02779090946269143,0.2050245745488855,0.008835098781694061,0.018713364860206336,0.7396360523465226
1306,"How do I use (n)curses in Ruby? I'd like to create a progress bar to indicate the status of an a batch job in Ruby. I've read some tutorials / libraries on using ncurses none of which were particularly helpful in explaining how create an ""animated"" progress bar in the terminal or using curses with Ruby. I'm already aware of using a separate thread to monitor the progress of a given job I'm just not sure how to proceed with drawing a progress bar. Update ProgressBar class was incredibly straight-forward perfectly solved my problem. You might be able to get some implementation ideas from the Ruby/ProgressBar library which generates text progress bars. I stumbled across it a couple of months back but haven't made any use of it. Take note: Current as of 2012 implementations of ***ncurses*** in the ruby language use the [ffi-ncurses](https://github.com/seanohalpin/ffi-ncurses) gem. ***Curses*** is included in the STDLIB of ruby.  On windows curses works out of the box ncurses doesn't and for a progress bar curses should be sufficient. So use curses instead of ncurses. Also both curses and ncurses are wafer-thin wrappers around the c library - that means you don't really need Ruby-specific tutorials. However on the site for the PickAxe you can download all the code examples for the book. The file ""ex1423.rb"" contains a curses demo which plays Pong - that should give you plenty of material to get you going.  Very late answer and sorry for self promotion but I created library to show progress in terminal.  Personally I think curses is overkill in this case. While the curses lib is nice (and I frequently use it myself) it's a PITA to relearn every time I haven't needed it for 12 months which has to be the sign of a bad interface design. If for some reason you can't get on with the progress bar lib Joey suggested roll your own and release it under a pretty free licence for instant kudos :) @MeNoMore ""PITA"" is not code. It is an acronym for ""pain in the arse"". Do not use code formatting for such things. @Andrew Barber :) you got me laughing now i rreally didnt know that with PITA as ""pain in the arse"" just learned something where i come from PITA is something totally different (Hint: even there it isnt code :) you can google it) thank you anyway",ruby unix curses,0.0010605475100226125,0.4925339878165905,0.016178037781755836,0.4867061685461482,0.0035212583454828144
705,"Embedded Database for .net that can run off a network I was (and still am) looking for an embedded database to be used in a .net (c#) application. The caveat: The Application (or at least the database) is stored on a Network drive but only used by 1 user at a time. Now my first idea was SQL Server Compact edition. That is really nicely integreated but it can not run off a network. Firebird seems to have the same issue but the .net Integration seems to be not really first-class and is largely undocumented. Blackfish SQL looks interesting but there is no trial of the .net Version. Pricing is also OK. Any other suggestions of something that works well with .net and runs off a network without the need of actually installing a server software? Interesting that this post is not considered to be ""opinion based"" in contrast to my post http://stackoverflow.com/questions/20229964/multi-user-application-without-need-to-install-anything-embedded-database-that. Anyway - i think all of the proposed embedded databases works for one user on server (including SQL CE) some of them allow concurrent reads (like SQLite) but only one (at least the only one I have found and tested that it WORKS!) that allows concurrent writes is VistaDB Have you considered an OODB? From the various open sources alternatives I recommend db4o (sorry for the self promotion :)) which can run either embedded or in client/server mode. Best Adriano  Check out VistaDB. They have a very good product the server version (3.4) is in Beta and is very close to release.  You can use the firebird embeded it's just a dll that you will need to ship with you app. About things being undocumented that's not really true the firebird .NET driver implements the ADO Interfaces so if you know ADO you can work with Firebird basically instead of SQLConnection you will use FBConnection and so on but my advice is to write a data access layer and use just interfaces on your code something like this: using FirebirdSql.Data.FirebirdClient; public static IDbConnection MyConnection() {  FbConnection cn = new FbConnection(""..."");  return cn; } This example is very simple but you will not need much more than that. We use firebird for our all application without any problems you should at least try it out.  Why not use SQL Server 2005 Express edition? It really depends on what you mean by ""embedded"" - but you can redistribute SQLServer2005E with your applications and the user never has to know it's there. Embedding SQL Server Express in Applications Embedding SQL Server Express into Custom Applications @CodingTheWheel: Because it needs installation on a server and the user will notice a service running in the background latest when the alarm bells of every network security tool go off. Embedded means that it's part of the application or a separate .dll but that it does not require any installation does not try to do stuff in the registry and does not leave any files behind when you delete it except for the database. SQL Server Express is not embedded Microsoft is using ""embedding"" as ""Packaging with your application but still being a separate thing with dependencies"". There are many u  It sounds like ADO/Access is perfect for your needs. It's baked into the MS stack well seasoned and multi-user. You can programatically create a DB like so: Dim catalog as New ADOX.Catalog Catalog.Create(""Provider=Microsoft.Jet.OLEDB.4.0;Data Source=\\server\path\to\db.mdb"") You can then use standard ADO.NET methods to interact with the database.  I'm puzzled. You're asking for an embeded database - where the database itself is stored on the server. that translates to storing the data file on a network share. You then say that SQL Compact Edition won't work... except that if one looks at this document: Word Document: Choosing Between SQL Server 2005 Compact Edition and SQL Server 2005 Express Edition On page 8 you have a nice big green tick next to ""Data file storage on a network share"". So it seems to me that you're first thought was the right one. That's funny because SQL Server specifically tells me it won't: http://img512.imageshack.us/img512/6082/sqlceerror.jpg - maybe it's a misunderstanding as creating seems to not be possible while accessing could work. However for my purposes creating the database on Application Launch was critical. Reality trumps documentation in almost every case - question then arises as to why the docs say the ""wrong"" thing (or at least how the discrepancy arises). [FX:Browse Browse] Information is fairly thin on the ground in MSDN You _CAN_ store your database in SQL Server on a network store but only 1 SQL Server can access it. And you are then running in attached mode which is slower than dirt. Just because you can doesn't mean you should. He was asking about each app accessing the database directly not through a server app. Jason without wishing to be rude about half of that comment is wrong because we're not talking about a multi-user server version at any point just the compact version which is basically a .DLL (unlike Express or the other ""server"" versions).  I'd recommend Advantage Database Server (www.advantagedatabase.com). It's a mature embedded DB with great support and accessible from many development languages in addition to .NET. The ""local"" version is free runs within your application in the form of a DLL requires no installation on the server/network share and supports all major DB features. You can store the DB and/or application files all on the network; it doesn't care where the data is. Disclaimer: I am an engineer in the ADS R&D group. I promise it rocks :)  There's also Valentina. I cam e across this product when I was working on some Real Basic project. The RB version is very good.  SQLite came to my mind while reading your question and I'm quite sure that it's possible to access it from a network drive if you keep yourself to the constraint of 1 user at a time. SQLite on .NET - Get up and running in 3 minutes @Sven: SQLite actually has rather comprehensive file locking and a SQLite database can definitely be accessed by multiple users at a single time. On filesystems that support it SQLite will even use byte-range instead of whole-file locking to improve the performance of multiple simultaneous uses of the same database. It's not Access; it's pretty robust. @ChrisHanson It allows only read access for multiple users at the same time I've tested it during the past weeks and SQLite is really a great product. It is not a full flavorued RDBMS of course but it has all the features needed to get the job done.  A little late to the post here.. And VistaDB is already mentioned but I wanted to point out that VistaDB is 100% managed (since your post was tagged .net). It can run from a shared network drive and is 1MB xcopy deployed. Since you mention SQL CE we also support T-SQL Syntax and datatypes (in fact more than SQL CE) and have updateable views TSQL Procs and other things missing in SQL CE. One more good thing about Vista (comparing to others) is that it supports concurrent writes for database file that is located on a network location - for me it was decision making feature",.net database embedded-database,0.015175182069326606,0.0015579420849582458,7.893440306059477E-4,0.2831637473981376,0.6993137844169717
2702,"How do I use T-SQL Group By I know I need to have (although I don't know why) an Order By clause on the end of a SQL query that uses any aggregate functions like count sum avg etc: select count(userID) userName from users group by userName When else would GROUP BY be useful and what are the performance ramifications? Note that 'GROUP BY' does not order the result set. If you require a particular order then add ORDER BY too Counting the number of times tags are used might be a google example: Select TagNameCount(*) As TimesUsed From Tags Group By TagName Order TimesUsed If your simply wanting a distinct value of tags I would prefer to use the distant statement. Select Distinct TagName From Tags Order By TagName Asc  GROUP BY also helps when you want to generate a report that will average or sum a bunch of data. You can GROUP By the Department ID and the SUM all the sales revenue or AVG the count of sales for each month.  Group By forces the entire set to be populated before records are returned (since it is an implicit sort). For that reason (and many others) never use a Group By in a subquery.  GROUP BY is similar to DISTINCT in that it groups multiple records into one. This example borrowed from http://www.devguru.com/technologies/t-sql/7080.asp lists distinct products in the Products table. SELECT Product FROM Products GROUP BY Product Product ------------- Desktop Laptop Mouse Network Card Hard Drive Software Book Accessory The advantage of GROUP BY over DISTINCT is that it can give you granular control when used with a HAVING clause. SELECT Product count(Product) as ProdCnt FROM Products GROUP BY Product HAVING count(Product) > 2 Product ProdCnt -------------------- Desktop 10 Laptop 5 Mouse 3 Network Card 9 Software 6 very wonderfully simple example it is a +1  To retrieve the number of widgets from each widget category that has more than 5 widgets you could do this: SELECT WidgetCategory count(*) FROM Widgets GROUP BY WidgetCategory HAVING count(*) > 5 The ""having"" clause is something people often forget about instead opting to retrieve all their data to the client and iterating through it there.",sql group-by,0.001157397837391792,0.05807712815371322,0.1698572072645495,0.06693728361028112,0.7039709831340644
3136,How to setup a crontab to execute at specific time How can i set up my crontab to execute X script at 11:59PM every day without emailing me or creating any logs? Right now my crontab looks something like this @daily /path/to/script.sh You will with the above response receive email with any text written to stderr. Some people redirect that away too and make sure that the script writes a log instead. ... 2>&1 ....  When you do crontab -e try this: 59 23 * * * /usr/sbin/myscript > /dev/null That means: At 59 Minutes and 23 Hours on every day (*) on every month on every weekday execute myscript. See man crontab for some more info and examples.  Following up on svrist's answer depending on your shell the 2>&1 should go after > /dev/null or you will still see the output from stderr. The following will silence both stdout and stderr: 59 23 * * * /usr/sbin/myscript > /dev/null 2>&1 The following silences stdout but stderr will still appear (via stdout): 59 23 * * * /usr/sbin/myscript 2>&1 > /dev/null The Advanced Bash Scripting Guide's chapter on IO redirection is a good reference--search for 2>&1 to see a couple of examples.,cron settings crontab,0.0022233518271935677,0.009233261419079777,0.9110510509601181,0.07011030330334994,0.007382032490258629
800,"Object Oriented vs. Relational Databases Object oriented databases seem like a really cool idea to me no need to worry about mapping your domain model to your database model no messing around with SQL or ORM tools. The way I understand it relational DBs offer some advantages when there is massive amounts of data and searching and indexing need to be done. To my mind 99% of websites are not massive and enterprise issues never need to be thought about so why aren't OO DBs more widely used? AskOxford.com: Which is the correct spelling: 'oriented' or 'orientated'? It really doesn't matter: it's a matter of personal taste. Orientated is currently preferred use in general British use. Oriented is prevalent in technical use and in the US. If you use a ORM like Hibernate aren't you actually building a sort of Mini-OODB on top of your SQL DBMS? Maybe that's one of the reasons many people feel little need for a OODB. The reason object relational databases are not very widely used is due to the object-relational impedance mismatch. The two paradigms just don't quite fit . Read more about this on [Wikipedia](http://en.wikipedia.org/wiki/Object-Relational_impedance_mismatch) the object-relational impedance mismatch is exactly the reason why one *would* use an OO database object relational databses are not very widely used? I thought SQL Server Oracle etc... are quite wide-spread. I worked on a project using an OO database. It had it's perks but the biggest downside besides performance and poorly documented API was that it was impossible to see what data was actually stored in the database without writing code. All the developers would've killed for: SHOW TABLES; SHOW COLUMNS FROM table; SELECT * FROM table; In the end we scrapped that platform for another. @Slauma but it is easy to choose a good RDB but much harder to choose a good OODB. Totally right. This one was terrible! ;) Was obviously a poor OODB. Poor RDBs exist too.  Simon Munro: They apply very well to niche situations. Those are generally caching type applications and complex objects for smaller sets of data (think 911 dispatcher type application) They do not handle schema changes well at all. You pretty much have to write code to mutate each object. Imagine writing objects that serialize to disk as binaries - if you change your class you lose the ability to load old files. Object databases have the same problem You can't 'report' off object databases in a traditional way. Every report has to be a program that loads the objects and does something with them so there is no concept of 'list all where...' Point 1 is valid but the other two are not (at least not in general). Ad 2: First SQL handles schema changes badly (typed columns have to update both client program and server). The other part about not being able to load old instances once you modify their class is simply not true. For example in Common Lisp CLOS OODBMS you can add slots at will and easily implement deletion and change protocols to load old classes. Ad 3: Having a standardized query language is pretty much orthogonal to whether the underlying system is relational or not. skyper: You need to design your schema so that schema changes can be done without changes to the data and this is well known. A common way of doing this is to provide views. I'll grant that this is not neccessarily automatic and does require actual knowledge of SQL (Which you do seem to have only you're addressing the naive perspective whereas I'm addressing the expert perspective). If this is well known then it must have gone past me. Got any resources on this?  I used db4o for an experimental website forum application. It wasn't a commercial project but was designed from the start to deal with huge forums - 1 million plus posts. I realised people wouldn't want to migrate their database to another rdbms after 1 million or more posts. Sadly db4o couldn't manage this amount of string data floating around and took around 10 seconds to query. It may have improved since then but my general feeling with db4o sql lite and others is they are designed for embedded scenarios or small sets of data. For example phone applications mocking and < 50000 rows apps that don't use large blogs or strings. Having said this db4o outperforms Hibernate with mySql and others. Here's forum thread about my issue  I too don't have any experience with OODB's but I'd be very wary about throwing one out even for a small project. 2 reasons: You may (and if your project grows will) need to access your database from things other than your main app. This includes things like reporting services replication so on and so forth. Also for most projects the database isn't the bottleneck or point of difference. It's just a place to shove your data so there is really no need to worry about it. Just take the path of least resistance (SQL) and focus your efforts on the actual core features of your app or site.  With this database benchmark software (GNU GPL) you can test many famous different databases. I think it's suitable to get some info about different database performance. Access to data in OODBMS can be faster because joins are often not needed (as in a tabular implementation of a relational database). This is because an object can be retrieved directly without a search by following pointers. (It could however be argued that ""joining"" is a higher-level abstraction of pointer following.) OODBMS are faster than relational DBMS because data isn’t stored in relational rows and columns but as objects. Objects have a many to many relationship and are accessed by the use of pointers. Pointers are linked to objects to establish relationships. Another benefit of OODBMS is that it can be programmed with small procedural differences without affecting the entire system. This is most helpful for those organizations that have data relationships that aren’t entirely clear or need to change these relations to satisfy the new business requirements. This ability to change relationships leads to another benefit which is that relational DBMS can’t handle complex data models while OODBMS can ....  I think nobody has mentioned the object-relational databases (i.e. relational databases with object-oriented extensions). In fact most of the DBMS vendors (e.g. Oracle) nowadays are object-relational. In Oracle you can combine in the same database relational tables and object tables (tables where each row is an object of a given type). Object types (i.e. classes) can have inheritance relationships methods... In my opinion this is the best option: take the best of both worlds.  To my mind 99% of websites are not massive and enterprise issues never need to be thought about so why arn't OO DBs more widely used? I think you give the answer yourself. If you have a very small app no massive data - so no need to think about performance - than you can as well use an ORM tool. It's easy to set up a relational database with standard configuration and get started using hibernate or something. Your statement implies that RDB+ORM is easier than OODB without the need of ORM. This is questionable (and in my own experience: wrong).  I feel it often boils down to people thinking of the relational model being the de facto standard because that's what everyone's used for the past X years. ** Disclaimer: I'm in my 20's; the following comes from research notes and not experience so may be inaccurate.* It seems that in the early days of databases around the 1950/60s network object and relational databases all had an equal footing. Relational won out simply due to marketing (thanks to IBM and a few direct competitors) and since then has had all the focus and funds pushed in that direction leading to the big advances seen over the other models. Then during the 70s we saw the emergence of object databases in research before being launched into the mainstream mid 80s hitting their stride in the mid 90s at around the same time IBM brought out lotus notes server (later lotus domino) which brought into the mainstream a new type of database: the document db. This in conjunction with lotus notes did very well. As did the Object databases. For a while anyway. Then the marketing guys started up again. Lots of noise about moving how moving legacy relational systems was easier to move to new RDBMSs rather than other models (even though the stuff talking to the dbs was OO - this didn't matter). And that's where we are today. People use RDBMSs because... people use RDBMSs. If people were to start using the other data persistence models and push for the features they need rather than complain and move back to RDBMSs we might see a similar growth of mature stable platforms. We're seeing some steps towards this now with some of the newer OpenSource projects but its not enough. People often forget is that the data they're storing often doesn't sit well in an RDBMS but they'll do anything they can to fudge it into one. We often see cases where we need to work with a lot of semi-structured loosely-related data. We need to store it index it and search it quickly. Currently there are few if any (none come to mind) solutions to this. So we grunt switch our mind to relational mode and start to draft our schemas as best we can knowing that we're imposing a structure that a few months down the line won't bare any resemblance to the data we started with. CouchDB has started down a path with a solution for this. It stores semi-structured data in documents and allowa you to create views to retrieve this data with its relationships. The best thing to do is to look at your data work out how you're going to use that data the look at what would be the best way to store it. So when working with: lots of numbers (eg. stock/share/pricing information) and need to run aggregated sums/reports on this data stick with relational databases. a huge pile of tuples and you're doing simple look-ups a key/value-db is unbeatable. a ""real-world"" model with which you need persistence look no further than an OODB. ""small"" bits of related/meta data around a key element such as contact/address-book information or a product catalog then a document database is probably the way to go. working with a lot of content/copy/words (eg. web pages blog posts technical documents which reference other tech docs) specifically in the ""web"" arena you can't really beat a good XML db (specifically for inter-relating documents the XInclude spec for linking document fragments does a wonderful job of allowing updates to referenced data). Whats also worth bearing in mind is how you're going to work with that data. As with most things with development picking X or Y because they're cool doesn't mean they work well. XML gets a bad reputation because people don't work with it in the right way. It works best when its managed along-side its other related technologies: XQuery & XPath for querying XSL for translation/templating XInclude for linking and merging data fragments and XUpdate for manipulation. Wrap all this in a native xml database and you've got a very nice environment for working with XML.  A database containing simply data is easier to understand and manipulate than a database containing code and data. Your code is going to be buggy. If you are using a relational database and you fix a bug and the patch does not touch the schema then you can just deploy the patch without touching the database and you're done. Lots of fixes can be done this way and they are fast and painless and usually fairly low risk. This is nice and productive so you end up designing your systems to maximise the chance that bugs can be fixed without touching the schema typically by thinking carefully about the schema. A bunch of CREATE TABLE and CREATE VIEW statements is a concise way to describe a data structure and there are lots of programmers out there who can efficeintly grok a relational schema. If you have to fix a bug that changes the schema then you have to take a deep breath and migrate the production database. People learn how to do migrations with relational databases and it is manageable since you are simply updating data tables and the migration process does not have to worry about code versioning. Anything that makes the migration more complex is going to be complicate an already risky situation. If on the other hand you are using an object database then a high proportion of your bug fixes are going to alter the contents of the database and so you're doing the risky stressful database migrations much more frequently. Object models vary in important details for different platforms. Therefore using an object database will tend to tie the database access code to one specific platform. The popular relational databases can be accessed by just about anything. Gemstone of course. It is only called an OODB if it stores the code b.t.w. You might want to ask yourself why there are so many errors in databases. Which OODB contains ""code""? The code of class methods is usually never stored in the database.  This might sound like a stupid answer but... They're not widely used because they're not widely used - I think that the issue with people not using OODBs is that more people feel comfortable with SQL etc. (because they already know it well and because they know that everyone uses it)  Things that are initially easy usually grow into big problems. (Great Dane puppies grow in to very big dogs.) A good program design that cannot scale is usually not a good idea. E.F. (Ted) Codd the genius that invented the idea of a relational database in 1970 published a paper called ""A Relational Model of Data for Large Shared Data Banks"". He introduced the concept of a table (he called it a relation originally) for storing data. He also pioneered the idea of having a query language to be able to quickly pull the data out. What is relevant to the object database model is that the reason the relational structure was invented is to get away from hierarchical databases. The old IMS model (IBMs hierarchical DB) suffered from many of the same problems that Object databases do. It is based around the programming environment and ease for the programmer. Which does sound good at first but data is usually reused by reports as well as other systems. The relational model focuses on the structure of the data and access of the data not the program using the data. To make it the most painfully obvious on reuse try reporting off of hierarchical or object database data. It is a simple test of the accessibility of the data and shows its flaws. Don’t look for short cuts to good design. Learn OO learn SQL and relational database design and learn structured programming. You will gain more than experience you will develop good judgment. ""The old IMS model (IBMs hierarchical DB) suffered from many of the same problems that Object databases do. It is based around the programming environment and ease for the programmer."" yes I'm tired of people who don't understand data trying to design to make things easier for them not the users. And the relational database is a massive failure at good design (when compared to an OODB). Terrible data quality but you can access it...  My short answer because people are more familiar with relational databases and if they work is there a need to learn something new? I'm not saying that's a good thing but at University I was not taught OODBs I've seen at least 5 times as much on the web about Relational than I have about OODBs.  Object Oriented databases are good at storing objects and obviously allow objects to be related to each other in simple ways. But in a real world application the interesting stuff is about the relationships between the data; where the results of the query relate tables in interesting ways that go beyond that attibutes of a single object. I think many ORMs / Object Oriented databases don't handle this kind of data mashup very efficiently.  In the vein of portability I'd use relational. Moving a website from one host to another would be a nightmare with some niche DBMS. Almost all hosting providers offer MSSQL and MySQL these days.  I find the following arguments flawed: I've used such and such OODB and it was inferior- thuse every OODB is inferior Use RDB as everyone uses RDB - millions of Chinese can't be wrong Object databases are proven inferior (slow buggy can't handle massive data) - by anonymous authority I don't know much about OODB - they must be inferior There was a time when RDBs faced the same criticism and there are still COBOL dbs around. Nothing in IT is a silver bullet and you should chose your tools wisely. Technologies change and every once in a while something better is produced. Try things out. The question asked why OODBs are not widely used rather than asked people to prove that OODBs should not be used. While you are correct that the arguments are fallacies they are nonetheless the kind of fear uncertainty and doubt that tend to stop people using OODBs.  I don't know much about OO databases (beyond when I heard them mentioned in the SO podcast and what I just read here) but it seems to me that if you have issues with massive amounts of data in an OO DB I would be leery of using one for any site that I thought was going to grow to any sort of large size (like this one) unless it was a straight forward process to migrate over to a relational database when the OO DB started slogging down to a snails pace because of data overload... Plus small DBs are so easy to develop and maintain that it just doesn't seem worth it to learn OO DB stuff just for a little DB. So if they are easier to build/design for large DBs but too slow to use with a large DB then they are self defeating because the very kind of DB you might be motivated to use them for is the very DB they should be avoided for. Then again I know next to nothing about OO DB so I could be wrong about everything I just said. I would love to hear from someone who has actually used them who could set me straight on my (probable) misconceptions. I wish there was some way to mark this as a comment rather than an answer... Please see my reply at http://stackoverflow.com/questions/61520/what-are-the-pros-and-cons-of-object-databases  Relational databases are proven technology that works and scales and does everything required of it. Objects are great for defining and using within the software that doesn't mean they work well as a container for data. Relational databases are also getting easier to use Microsoft is making a valid attempt with LINQ. It's a not broke don't fix it situation. :-)  OODBs are not widely used because they are really slow when compared to relational DB's. Recently I discovered and studied about Object-Relational Databases (ORDBs) which use good points from both worlds. On your database you can create objects with methods and attributes just like you do on your favorite programming language and then you create tables which are made only by those objects with SQL commands like... CREATE TABLE of TArtist; ... or you can use those objects as a column of an old-school relational table with SQL commands like... CREATE TABLE Records ( id number(5) name varchar2(40) singer TArtist ); The best part of it is to create select queries without dozens of joins. Here's an example of a query on the table above: SELECT r.name r.singer.name from Records r; It can be a bit tricky to create 1-N or N-N references between tables but the gain of simplicity when making selects may be worth it. Anyway you still have to create code to map objects from and to the database just as you do now. I plan to begin using it on personal projects because I think it will be an important topic in a mid-term future. Two very known databases that have ORDB capabilities are Oracle and Postgre. OODBs are tuned for what OODBs tent to be used for and RDBMs are tuned for what RDBMs tend to be used for. So ""slow"" depends on your workload and the ""shape"" of your data -1 For complex models OODBs are orders of magnitude faster than RDBMs. Oh really? Our teacher taught us the other way around! It's time for me to run a search on the topic. Thanks for the tip. -1 The general statement about OODB performance is really terribly wrong because ""it depends...""  i have made an answer here before Relational db SQL and standards easy to model can use only standard and vendor types referential integrity (matematically solid relational set theory) lot of tools and database implementations data separate from program storage management and high end infrastructure support transaction and concurrency management done within Relational Model is Value-Based ie rows are identified by primary keys Cons no custom type no extensible data types impedance mismatch cannot express nested relationship cannot use complex entities as a single unit need to define keys and various types of relationships at the data model level write procedures for versioning transactions if needed Object DB High performance Faster as no joins required Inherent versioning mechanism Navigational interface for operations (like graph traversal) Object Query Language retrieve objects declaratively complex data types object identity ie. equals() in which object identity is independent of value and updates facilitates object sharing classes and hierarchies (inheritance and encapsulation) support for relationships integrated with a persistence language like ODL support for atomicity support for nested relationships semantic modelling Cons No mathematical foundation as RDB (refer Codd) cons of object orientation persistence difficult for complex structures some data must be transient Object-Relational databases (You might have seen UDTs!) support for complex data types like collection multisets etc object oriented data modelling extended SQL and rich types support for UDT inhertance powerful query language Different approaches (OO Relational DB or OODB) may be necessary for different applications References The advantage of using relational databases for large corpora Relational Database Relational Database OODMS manifesto ODMG Benefits of a relational database The Object-Oriented Database System Manifesto Object Oriented Database Systems Object Relational Databases in DBMS Completeness Criteria for Object-Relational Database Systems Comparisons http://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems http://en.wikipedia.org/wiki/Comparison_of_object_database_management_systems http://en.wikipedia.org/wiki/Comparison_of_object-relational_database_management_systems  Why we abandoned Object Databases A while back I was part of a Solaris/C++ project that used the Objectivity object database. We eventually switched the project over to Sybase. This was about 10 years ago so I'm sure a lot has changed since then but a few of the observations still apply. The application was a carrier-class telecom system. The basic functionality was nice. You reference an object and it magically pops into memory. If your programming problem boils down to having a large graph of objects (as opposed to tabular data) it is a definite win. There were some stability-related ""early adopter"" problems. Seeing as the company is still in business it's safe to assume they are fixed. We wanted to allow the client to define their own schema. This was a huge problem since you give the object database a specification and it spits out a header file which defines your object. This is definitely not conducive to post-compiling schema definition which is a definite strength of SQL databases. The customer perceived it as being a new experimental technology. If you're familiar with the telecom world you know that's not a recommendation. SQL databases have tons of tools for working with schemas database administration UI generation backup etc. We were having to write every little piece of that ourselves. I had a bad experience working with some of the early-adopter objectologists in that group. If you're familiar with the DailyWTF these were the TrueFalseFileNotFound guys. While attempting to address the client customization part our lead objectologist came up with some data classes called Rows and Columns. That was when it became apparent that for our particular application we were just a whole lot better off going with a relational database. Anyways that was my experience with Object DBs I would love to hear some other (hopefully happier!) experiences with them. @Mitch: yes he was severely promoted and given a very stern raise. @Slauma: People and organizations tend to be a lot more conservative with their data storage than with their displays. Absolutely hilarious that he came up with some classes called Rows and Columns. A clear indicator. Glad you guys recognized it put on the breaks! Love the term ""lead objectologist""! I bet that penny dropped hard when ""our lead objectologist came up with some data classes called Rows and Columns""? :) ""...This was about 10 years ago..."" degrades this answer to an anecdote. It is like someone asks in 2008 ""Why does not everyone use flatscreen monitors?"" and gets the answer ""Because 10 years ago they were just too expensive"".  I worked with OODBs a few years ago and... They apply very well to niche situations. Those are generally caching type applications and complex objects for smaller sets of data (think 911 dispatcher type application) They do not handle schema changes well at all. You pretty much have to write code to mutate each object. Imagine writing objects that serialize to disk as binaries - if you change your class you lose the ability to load old files. Object databases have the same problem You can't 'report' off object databases in a traditional way. Every report has to be a program that loads the objects and does something with them so there is no concept of 'list all where...' In my experience point (2) is not correct at least not for all OODBs. Automatic schema versioning is an integral part for OODBs and usually you don't have to write a single line of conversion code. It's handled by the database itself.  ""The Large Hadron Collider at CERN in Switzerland uses an Objectivity DB. The database is currently being tested in the hundreds of terabytes at data rates up to 35 MB/second."" ie: The Single Most expensive experiment in Human history uses a OODBMS. Maturity and Simplicity are the two main factors in the lack of OODBMSs being used. In general terms OODBMS is a Teenager compared to the weathered and seasoned RDBMSs. People trust that proven technology behind RDBMSs and are wary of the Upstart Youngin' with its new approach. SQL is a simple means of communicating with the ""Data Manager"" (the MS of RDBMS) whereas OO equivalents are immature and people find it hard to use them to communicate their intentions to the ""Data Manager"". The reason OO is adopted for the LHC is that these old proven methods reach their innate capacity to perform. A capacity which under normal situations is never really required. The LHC requires huge throughput and storage which RDBMSs can achieve but are sub-par to OODBMSs past a certain point. Under normal situations every fiscal transactions of every human being and an itemized inventory of every interpersonal communication during an instance of time; is insignificant compared to the Systemic demands of observing the unpredictable events of the Quantum realm. Oh and MySQL is cheap ... like free beer. Everyone likes free beer. For the record CERN also uses Oracle extensively and MySQL and a whole bunch of other products too. Being really smart the CERN guys are able to understand that different types of database suit different usages. The Large Hadron Collider - isn't that the big expensive tube that didn't work? ;) @_ande_turner_: +1... SQL DBs also show their limitations and slow performances when you have big amount of data like when you're Google. Bigtables saves the day because SQL simply can't handle the load.",database oop object-database,0.009735314419212987,4.058050122436205E-4,2.0560441052149906E-4,0.37702014880712,0.6126331273509019
1083,"Bayesian filtering for spam I was wondering if there is any good and clean oo implementation of bayesian filtering for spam and text classification? For learning purposes. Don't waste your time on SPAM filtering usages. Spammers easily bypass Bayesian filtering by adding random text to their spam emails. FIX: OKOK Bayesian filtering can be useful when trained personally. However at a corporate level or above its probably useless. Random text won't (significantly) affect a Bayesian classification. The remarkable accuracy of filters like SpamBayes and PopFile should demonstrate this. OK Bayesian classification can be useful for end user usage to classify ""good"" emails. I will fix my answer.  Check out Chapter 6 of Programming Collective Intelligence  Here is an implementation of Bayesian filtering in C#: A Naive Bayesian Spam Filter for C# (hosted on CodeProject).  In French but you should be able to find the download link :) PHP Naive Bayesian Filter  I definitely recommend Weka which is an Open Source Data Mining Software written in Java: Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing classification regression clustering association rules and visualization. It is also well-suited for developing new machine learning schemes. As mentioned above it ships with a bunch of different classifiers like SVM Winnow C4.5 Naive Bayes (of course) and many more (see the API doc). Note that a lot of classifiers are known to have much better perfomance than Naive Bayes in the field of spam detection or text classification. Furthermore Weka brings you a very powerful GUI…  Maybe https://ci-bayes.dev.java.net/ or http://www.cs.cmu.edu/~javabayes/Home/node2.html? I never played with it either.  nBayes - another C# implementation hosted on CodePlex",artificial-intelligence email-spam bayesian,0.07725828476443015,0.004806507304786369,0.6188524401340759,0.2800197670059886,0.01906300079071906
1848,"Locating Text within image I am currently working on a project and my goal is to locate text in an image. OCR'ing the text is not my intention as of yet. I want to basically obtain the bounds of text within an image. I am using the AForge.Net imaging component for manipulation. Any assistance in some sense or another? Update 2/5/09: I've since went along another route in my project. However I did attempt to obtain text using MODI (Microsoft Office Document Imaging). It allows you to OCR an image and pull text from it with some ease. This is an active area of research. There are literally oodles of academic papers on the subject. It's going to be difficult to give you assistance especially w/o more deatails. Are you looking for specific types of text? Fonts? English-only? Are you familiar with the academic literature? ""Text detection"" is a standard problem in any OCR (optical character recognition) system and consequently there are lots of bits of code on the interwebs that deal with it. I could start listing piles of links from google but I suggest you just do a search for ""text detection"" and start reading :). There is ample example code available as well.  If you're ok with using an online API for this the API at http://www.wisetrend.com/wisetrend_ocr_cloud.shtml can do text detection in addition to just OCR.  Stroke width transform can do that for you. That's at least what MS developed for their mobile phone OS. A discussion on the implementation is here at http://stackoverflow.com/  recognizing text inside an image is indeed a hot topic for researchers in that field but only begun to grow out of control when captcha's became the ""norm"" in terms of defense against spam bots. Why use captcha's as protection? well because it is/was very hard to locate (and read) text inside an image! The reason why I mention captcha's is because the most advancement* is made within that tiny area and I think that your solution could be best found there. especially because captcha's are indeed about locating text (or something that resembles text) inside a cluttered image and afterwards trying to read the letters correctly. so if you can find yourself a good open source captcha breaking tool you probably have all you need to continue your quest... You could probably even throw away the most dificult code that handles the character recognition itself because those OCR's are used to read distorted text something you don't have to do. *: advancement in terms of visible usable and practical information for a ""non-researcher"" The other reason to mention captcha's is that is probably what his ""project"" is. :-)",c# image image-processing artificial-intelligence,0.4996267290838286,0.10886239163569753,0.0023808994569574976,0.3481717991182622,0.04095818070525425
1898,"CSV File Imports in .Net I realize this is a newbie question but I'm looking for a simple solution - it seems like there should be one. What's the best way to import a CSV file into a strongly-typed data structure? Again simple = better. This is a duplicate of http://stackoverflow.com/questions/1103495/is-there-a-proper-way-to-read-csv-files Considering this was created a year earlier than 1103495 I think that question is a duplicate of this one. Thanks Matt. I was just trying to link them together not indicate which one came first. You'll see I have exactly the same text on the other question pointing at this one.Is there a better way to tie two questions together? [Top 6 ways to parse .CSV? High Performance!](http://izlooite.blogspot.com/2011/06/top-6-ways-to-parse-csv-high.html) I agree with @NotMyself. FileHelpers is well tested and handles all kinds of edge cases that you'll eventually have to deal with if you do it yourself. Take a look at what FileHelpers does and only write your own if you're absolutely sure that either (1) you will never need to handle the edge cases FileHelpers does or (2) you love writing this kind of stuff and are going to be overjoyed when you have to parse stuff like this: 1""Bill""""Smith""""Supervisor"" ""No Comment"" 2  'Drake'  'O'Malley'""Janitor Oops I'm not quoted and I'm on a new line! +1 for the Irish thrower in #2.  Brian gives a nice solution for converting it to a strongly typed collection. Most of the CSV parsing methods given don't take into account escaping fields or some of the other subtleties of CSV files (like trimming fields). Here is the code I personally use. It's a bit rough around the edges and has pretty much no error reporting. public static IList<IList<string>> Parse(string content) {  IList<IList<string>> records = new List<IList<string>>();  StringReader stringReader = new StringReader(content);  bool inQoutedString = false;  IList<string> record = new List<string>();  StringBuilder fieldBuilder = new StringBuilder();  while (stringReader.Peek() != -1)  {  char readChar = (char)stringReader.Read();  if (readChar == '\n' || (readChar == '\r' && stringReader.Peek() == '\n'))  {  // If it's a \r\n combo consume the \n part and throw it away.  if (readChar == '\r')  {  stringReader.Read();  }  if (inQoutedString)  {  if (readChar == '\r')  {  fieldBuilder.Append('\r');  }  fieldBuilder.Append('\n');  }  else  {  record.Add(fieldBuilder.ToString().TrimEnd());  fieldBuilder = new StringBuilder();  records.Add(record);  record = new List<string>();  inQoutedString = false;  }  }  else if (fieldBuilder.Length == 0 && !inQoutedString)  {  if (char.IsWhiteSpace(readChar))  {  // Ignore leading whitespace  }  else if (readChar == '""')  {  inQoutedString = true;  }  else if (readChar == '')  {  record.Add(fieldBuilder.ToString().TrimEnd());  fieldBuilder = new StringBuilder();  }  else  {  fieldBuilder.Append(readChar);  }  }  else if (readChar == '')  {  if (inQoutedString)  {  fieldBuilder.Append('');  }  else  {  record.Add(fieldBuilder.ToString().TrimEnd());  fieldBuilder = new StringBuilder();  }  }  else if (readChar == '""')  {  if (inQoutedString)  {  if (stringReader.Peek() == '""')  {  stringReader.Read();  fieldBuilder.Append('""');  }  else  {  inQoutedString = false;  }  }  else  {  fieldBuilder.Append(readChar);  }  }  else  {  fieldBuilder.Append(readChar);  }  }  record.Add(fieldBuilder.ToString().TrimEnd());  records.Add(record);  return records; } Note that this doesn't handle the edge case of fields not being deliminated by double quotes but meerley having a quoted string inside of it. See this post for a bit of a better expanation as well as some links to some proper libraries.  Check out FileHelpers. @dangph @Zeus @John @Martin Section 6b says you are allowed to ""use a suitable shared library mechanism for linking with the library. A suitable mechanism is one that (1) uses at run time a copy of the library already present on the user's computer system rather than copying library functions into the executable and (2) will operate properly with a modified version of the library if the user installs one as long as the modified version is interface-compatible with the version that the work was made with."" A .Net DLL assembly meets those criteria & your own source need not be released. The fact that the legality of this is complicated enough to cause this discussion probably rules it out for a lot of people. It certainly shows the situation is uncertain. Most people don't have time or money to get ""the leagal team"" in to verify the situation. That said unless you are going to incorporate the libary in a packaged solution ship ship to 1000s of people it is unlikly anyone will ever enforce the licence anyway. the licensing objection is nothing but FUD. For comparison NHibernate is also LGPL and it has been used in countless commercial apps. So there's nothing to worry about. Unfortunately this is LGPL which is less than ideal in a corporate environment... @John why do you say that? LGPL doesn't require you to release any code unless you modify the library itself. (In which case it would make sense to submit a patch anyway.) +1 Just implemented this...awesome @dangph I don't think that's quite true. http://www.opensource.org/licenses/lgpl-2.1.php states ""However linking a ""work that uses the Library"" with the Library creates an executable that is a derivative of the Library ... The executable is therefore covered by this License. Section 6 states terms for distribution of such executables. "" @Zeus I still don't think you have to release the source of your ""work that uses the Library"". You need to release ""object code and/or source code"". I'm not sure what that means in a .Net environment. But you are right. The requirements of section 6 are extremely onerous. What a ridiculous license. @dangph I agree I'm not sure quite what to make of it! Another problem with FileHelpers is that development on it seems to have completely stalled since about 2007. And unfortunately it contains bugs. (Probably it would work fine for simple cases.) Even though it's open source it's not clear that the author is accepting patches. FileHelpers homepage says: ""FileHelpers Library is @Copyright 2005-2006 to Marcos Meli but it's source code and the binaries are free for commercial and non commercial use.""  I typed in some code. The result in the datagridviewer looked good. It parses a single line of text to an arraylist of objects.  enum quotestatus { none firstquote secondquote } public static System.Collections.ArrayList Parse(string linestring delimiter) { System.Collections.ArrayList ar = new System.Collections.ArrayList(); StringBuilder field = new StringBuilder(); quotestatus status = quotestatus.none; foreach (char ch in line.ToCharArray()) { string chOmsch = ""char""; if (ch == Convert.ToChar(delimiter)) { if (status== quotestatus.firstquote) { chOmsch = ""char""; } else { chOmsch = ""delimiter""; } } if (ch == Convert.ToChar(34)) { chOmsch = ""quotes""; if (status == quotestatus.firstquote) { status = quotestatus.secondquote; } if (status == quotestatus.none ) { status = quotestatus.firstquote; } } switch (chOmsch) { case ""char"": field.Append(ch); break; case ""delimiter"": ar.Add(field.ToString()); field.Clear(); break; case ""quotes"": if (status==quotestatus.firstquote) { field.Clear(); } if (status== quotestatus.secondquote) { status =quotestatus.none; } break; } } if (field.Length != 0) { ar.Add(field.ToString()); } return ar; }  I had to use a CSV parser in .NET for a project this summer and settled on the Microsoft Jet Text Driver. You specify a folder using a connection string then query a file using a SQL Select statement. You can specify strong types using a schema.ini file. I didn't do this at first but then I was getting bad results where the type of the data wasn't immediately apparent such as IP numbers or an entry like ""XYQ 3.9 SP1"". One limitation I ran into is that it cannot handle column names above 64 characters; it truncates. This shouldn't be a problem except I was dealing with very poorly designed input data. It returns an ADO.NET DataSet. This was the best solution I found. I would be wary of rolling my own CSV parser since I would probably miss some of the end cases and I didn't find any other free CSV parsing packages for .NET out there. EDIT: Also there can only be one schema.ini file per directory so I dynamically appended to it to strongly type the needed columns. It will only strongly-type the columns specified and infer for any unspecified field. I really appreciated this as I was dealing with importing a fluid 70+ column CSV and didn't want to specify each column only the misbehaving ones. Why not the VB.NET built in CSV parser? http://msdn.microsoft.com/en-us/library/microsoft.visualbasic.fileio.textfieldparser.aspx  Microsoft's TextFieldParser is stable and follows RFC 4180 for CSV files. Don't be put off by the Microsoft.VisualBasic namespace; it's a standard component in the .NET Framework just add a reference to the global Microsoft.VisualBasic assembly. If you're compiling for Windows (as opposed to Mono) and don't anticipate having to parse ""broken"" (non-RFC-compliant) CSV files then this would be the obvious choice as it's free unrestricted stable and actively supported most of which cannot be said for FileHelpers. See also: How to: Read From Comma-Delimited Text Files in Visual Basic for a VB code example. There's actually nothing VB-specific about this class other than its unfortunately-named namespace. I would definitely choose this library if I only needed a ""simple"" CSV parser because there's nothing to download distribute or worry about in general. To that end I've edited the VB-focused phrasing out of this answer. @Aaronaught I think your edits are mostly an improvement. Although that RFC is not necessarily authoritative as many CSV writers do not comply with it e.g. Excel [does not always use a comma](http://office.microsoft.com/en-us/excel-help/import-or-export-text-txt-or-csv-files-HP010099725.aspx#BMchange_the_separator_in_all_.csv_text) in ""CSV"" files. Also didn't my previous answer already say the class could be used from C#? The `TextFieldParser` will work for tab-delimited and other weird Excel-generated cruft too. I realize that your previous answer wasn't claiming that the library was VB-specific it just came across to me as implying that it was really *meant* for VB and not *intended* to be used from C# which I don't think is the case - there are some really useful classes in MSVB.  If you can guarantee that there are no commas in the data then the simplest way would probably be to use String.split. For example: String[] values = myString.Split(''); myObject.StringField = values[0]; myObject.IntField = Int32.Parse(values[1]); There may be libraries you could use to help but that's probably as simple as you can get. Just make sure you can't have commas in the data otherwise you will need to parse it better. very bad on memory usage and lots of overhead. Small should be less thank a few kilobytes. Definitely not good for a 10mb csv! this is not an optimal solution  I was bored so i modified some stuff i wrote. It try's to encapsulate the parsing in an OO manner whle cutting down on the amount of iterations through the file it only iterates once at the top foreach. using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.IO; namespace ConsoleApplication1 { class Program { static void Main(string[] args) { // usage: // note this wont run as getting streams is not Implemented // but will get you started CSVFileParser fileParser = new CSVFileParser(); // TO Do: configure fileparser PersonParser personParser = new PersonParser(fileParser); List<Person> persons = new List<Person>(); // if the file is large and there is a good way to limit // without having to reparse the whole file you can use a // linq query if you desire foreach (Person person in personParser.GetPersons()) { persons.Add(person); } // now we have a list of Person objects } } public abstract class CSVParser { protected String[] deliniators = { """" }; protected internal IEnumerable<String[]> GetRecords() { Stream stream = GetStream(); StreamReader reader = new StreamReader(stream); String[] aRecord; while (!reader.EndOfStream) { aRecord = reader.ReadLine().Split(deliniators StringSplitOptions.None); yield return aRecord; } } protected abstract Stream GetStream(); } public class CSVFileParser : CSVParser { // to do: add logic to get a stream from a file protected override Stream GetStream() { throw new NotImplementedException(); } } public class CSVWebParser : CSVParser { // to do: add logic to get a stream from a web request protected override Stream GetStream() { throw new NotImplementedException(); } } public class Person { public String Name { get; set; } public String Address { get; set; } public DateTime DOB { get; set; } } public class PersonParser { public PersonParser(CSVParser parser) { this.Parser = parser; } public CSVParser Parser { get; set; } public IEnumerable<Person> GetPersons() { foreach (String[] record in this.Parser.GetRecords()) { yield return new Person() { Name = record[0] Address = record[1] DOB = DateTime.Parse(record[2]) }; } } } }  If you're expecting fairly complex scenarios for CSV parsing don't even think up of rolling our own parser. There are a lot of excellent tools out there like FileHelpers or even ones from CodeProject. The point is this is a fairly common problem and you could bet that a lot of software developers have already thought about and solved this problem.  Use an OleDB connection.  String sConnectionString = ""Provider=Microsoft.Jet.OLEDB.4.0;Data Source=C:\\InputDirectory\\;Extended Properties='text;HDR=Yes;FMT=Delimited'""; OleDbConnection objConn = new OleDbConnection(sConnectionString); objConn.Open(); DataTable dt = new DataTable(); OleDbCommand objCmdSelect = new OleDbCommand(""SELECT * FROM file.csv"" objConn); OleDbDataAdapter objAdapter1 = new OleDbDataAdapter(); objAdapter1.SelectCommand = objCmdSelect; objAdapter1.Fill(dt); objConn.Close(); This requires file system access. As far as i know there is no way to make OLEDB work with in-memory streams :( @UserControl of course it requires file system access. He asked about importing a CSV file I'm not complaining. In fact I'd prefer OLEDB solution over the rest but I was frustrated so many times when needed to parse CSV in ASP.NET applications so wanted to note it.  A good simple way to do it is to open the file and read each line into an array linked list data-structure-of-your-choice. Be careful about handling the first line though. This may be over your head but there seems to be a direct way to access them as well using a connection string. Why not try using python instead of c# or vb? It has a nice CSV module to import that does all the heavy lifting for you. EDIT: @NotMyself - just because we commented poorly on your trolling question please don't vote down our answers. sasb and i appreciate it. Don't jump to python from VB for the sake of a CSV parser. There's one in VB. Although weirdly it seems to have been ignored in the answers to this question. http://msdn.microsoft.com/en-us/library/microsoft.visualbasic.fileio.textfieldparser.aspx  There are two articles on CodeProject that provide code for a solution one that uses StreamReader and one that imports CSV data using the Microsoft Text Driver.  If the file is small: Read each line tokenize what you've read and assign each value to its respective place in the data structure",c# vb.net file csv import,0.7717559778373622,0.019053523598674703,0.021111541140985034,0.07044800490228857,0.11763095252068949
1064,How To Display 100 Floating Cubes Using DirectX OR OpenGL? I'd like to display ~100 floating cubes using DirectX or OpenGL. I'm looking for either some sample source code or a description of the technique. I know this kind of thing is easy for you accomplished '3D' gurus out there but I have enough trouble getting even one cube to display correctly. I've combed the net for a good series of tutorials and although they talk about how to do 3D primitives what I can't find is information on how to do large numbers of 3D primitives - cubes spheres pyramids and so forth. Yeah if you were being efficient you'd throw everything into the same vertex buffer but I don't think drawing 100 cubes will push any GPU produced in the past 5 years so you should be fine following the suggestions above. Write a basic pass through vertex shader shade however you desire in the pixel shader. Either pass in a world matrix and do the translation in the vertex shader or just compute the world space vertex positions on the CPU side (do this if your cubes are going to stay fixed). You could get fancy and do geometry instancing etc but just get the basics going first.  Just use glTranslatef (or the DirectX equivalent) to draw a cube using the same code but moving the relative point where you draw it. Maybe there's a better way to do it though I'm fairly new to OpenGL. Be sure to set your viewpoint so you can see them all.  You say you have enough trouble getting one cube to display... so I am not sure if you have got one to display or not. Basically... put your code for writing a cube in one function then just call that function 100 times. void DrawCube() {  //code to draw the cube } void DisplayCubes() {  for(int i = 0; i < 10; ++i)  {  for(int j = 0; j < 10; ++j)  {  glPushMatrix();  //alter these values depending on the size of your cubes.  //This call makes sure that your cubes aren't drawn overtop of each other  glTranslatef(i*5.0 j*5.0 0);  DrawCube();  glPopMatrix();  }  } } That is the basic outline for how you could go about doing this. If you want something more efficient take a look into Display Lists sometime once you have the basics figured out :),language-agnostic opengl 3d directx,0.03865840620712157,0.7972811775682356,0.0029794244483974178,0.11913709011851822,0.04194390165772716
146,"How do I track file downloads I have a website that plays mp3s in a flash player. If a user clicks 'play' the flash player automatically downloads an mp3 and starts playing it. Is there an easy way to track how many times a particular song clip (or any binary file) has been downloaded? Is the play link a link to the actual mp3 file or to some javascript code that pops up a player? If the latter you can easily add your own logging code in there to track the number of hits to it. If the former you'll need something that can track the web server log itself and make that distinction. My hosting plan comes with webalizer which does this nicely. It's javascript code so that answers that. However it would be nice to know how to track downloads using the other method (without switching hosts). If your song / binary file was served by apache you can easily grep the access_log to find out the number of downloads. A simple post-logrotate script can grep the logs and maintain your count statistics in a db. This has the performance advantage by not being in your live request code path. Doing non-critical things like stats offline is a good idea to scale your website to large number of users.  Use bash: grep mp3 /var/log/httpd/access_log | wc the simpler the better  Is the play link a link to the actual mp3 file or to some javascript code that pops up a player? If the latter you can easily add your own logging code in there to track the number of hits to it. If the former you'll need something that can track the web server log itself and make that distinction. My hosting plan comes with webalizer which does this nicely.  Is there a database for your music library? If there is any server code that runs when downloading the mp3 then you can add extra code there to increment the play count. You could also have javascript make a second request to increment the play count but this could lead to people/robots falsely incrementing counts. I used to work for an internet-radio site and we used separate tables to track the time every song was played. Our streams were powered by a perl script running icecast so we triggered a database request every time a new track started playing. Then to compute the play count we would run a query to count how many times a song's id was in the play log.  The funny thing is i wrote a php media gallery for all my music 2 days ago. I had a similar problem. Im using http://musicplayer.sourceforge.net/ for the player. and the playlis are built via php. all music request go there a script called xfer.php?file=WHATEVER $filename = base64_url_decode($_REQUEST['file']); header(""Cache-Control: public""); header(""Content-Description: File Transfer""); header('Content-disposition: attachment; filename='.basename($filename)); header(""Content-Transfer-Encoding: binary""); header('Content-Length: '. filesize($filename)); // Put either file counting code here. either a db or static files // readfile($filename); //and spit the user the file function base64_url_decode($input) { return base64_decode(strtr($input '-_' '+/=')); } And when you call files use something like function base64_url_encode($input) { return strtr(base64_encode($input) '+/=' '-_'); } http://us.php.net/manual/en/function.base64-encode.php If you are using some javascript or a flash player (JW player for example) that requires the actual link to be an mp3 file or whatever you can append the text ""&type=.mp3"" so the final linke becomes something like ""www.example.com/xfer.php?file=34842ffjfjxfh&type=.mp3"". That way it looks like it ends with an mp3 extension without affecting the file link. this will blow up the server's memory limits if the files are too big and your traffic is high.. something i've experienced myself. how to fix the ""directory traversal vulnerability"" ? @anarchOi: The easiest way would be to compare the GET parameter *(`$_REQUEST['file']`)* against a whitelist of known-good file names. For instance a listing of all files in the directory you store your files. Make sure you **only** use that directory for storing files you want to be downloadable... there is a directory traversal vulnerability in this script! An attacker can pass in xfer.php?file=../../../passwd or whatever else they want! Be careful!!!  Use your httpd log files. Install http://awstats.sourceforge.net/  You could even set up an Apache .htaccess directive that converts *.mp3 requests into the querystring dubayou is working with. It might be an elegant way to keep the direct request and still be able to slipstream log function into the response.",php apache download analytics,5.328833019840402E-4,0.019731982124527914,0.7614455749252284,0.048337922245440916,0.16995163740281877
3625,What's the Developer Express equivalent of System.Windows.Forms.LinkButton? I can't seem to find Developer Express' version of the LinkButton. (The Windows Forms linkbutton not the ASP.NET linkbutton.) HyperLinkEdit doesn't seem to be what I'm looking for since it looks like a TextEdit/TextBox. Anyone know what their version of it is? I'm using the latest DevX controls: 8.2.1. You should probably just use the standard ASP.Net LinkButton unless it's really missing something you need.  The control is called the HyperLinkEdit. You have to adjust the properties to get it to behave like the System.Windows.Forms control like so:  control.BorderStyle = BorderStyles.NoBorder; control.Properties.Appearance.BackColor = Color.Transparent; control.Properties.AppearanceFocused.BackColor = Color.Transparent; control.Properties.ReadOnly = true; So ... why not just use the standard LinkButton?,devexpress,0.002696367556206114,0.7380898158281927,0.023402447034925337,0.1914006620474229,0.044410707533252984
